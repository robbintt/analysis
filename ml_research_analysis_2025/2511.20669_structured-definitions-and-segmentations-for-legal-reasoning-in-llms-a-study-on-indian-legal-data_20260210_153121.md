---
ver: rpa2
title: 'Structured Definitions and Segmentations for Legal Reasoning in LLMs: A Study
  on Indian Legal Data'
arxiv_id: '2511.20669'
source_url: https://arxiv.org/abs/2511.20669
tags:
- legal
- reasoning
- judgment
- rhetorical
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of applying large language models
  (LLMs) to legal judgment prediction tasks, particularly in the Indian legal context.
  The authors propose a structured approach that enhances LLMs' performance by incorporating
  rhetorical roles, definitions, and chain-of-thought reasoning.
---

# Structured Definitions and Segmentations for Legal Reasoning in LLMs: A Study on Indian Legal Data

## Quick Facts
- **arXiv ID**: 2511.20669
- **Source URL**: https://arxiv.org/abs/2511.20669
- **Reference count**: 40
- **Primary result**: Incorporating definitions of rhetorical roles improves LLM legal judgment prediction by 1.5-4.36% F1 on Indian legal datasets.

## Executive Summary
This paper addresses the challenge of applying large language models (LLMs) to legal judgment prediction tasks, particularly in the Indian legal context. The authors propose a structured approach that enhances LLMs' performance by incorporating rhetorical roles, definitions, and chain-of-thought reasoning. They conduct experiments using three Indian legal judgment prediction datasets, testing various configurations of their method. The results demonstrate that incorporating definitions of rhetorical roles significantly improves model performance, with a minimum increase of 1.5% and a maximum improvement of 4.36% in F1 score compared to the baseline.

## Method Summary
The authors propose a structured prompting approach for zero-shot legal judgment prediction on Indian legal data. The method involves segmenting legal documents by rhetorical roles (facts, arguments, precedents, decisions), adding explicit definitions of these roles to familiarize the model with legal terminology, and optionally using chain-of-thought reasoning through multi-stage prompting. They test various configurations (D, R, C, D/R, D/C, D/R/C) across three Indian legal datasets, measuring performance using macro F1, false positive rate (FPR), and false negative rate (FNR). The approach is implemented with Llama-3.1-8B and other LLMs in a zero-shot setting without fine-tuning.

## Key Results
- D/R configuration (Definitions + Rhetorical Roles) achieved the best overall performance, with F1 improvements of 1.5-4.36% over baseline
- Full D/R/C configuration underperformed simpler configurations, suggesting component interactions may introduce noise
- PREAMBLE role was critical for model performance as it identifies competing parties
- Llama-3.1-8B and o3-mini showed strongest legal alignment among tested models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Reorganizing legal documents by rhetorical roles (facts, arguments, precedents, decisions) improves model processing of complex legal texts in zero-shot settings.
- **Mechanism**: Explicit structural markers reduce the cognitive load on models by signaling which sentences serve which functional purpose. The preamble role is particularly critical because it identifies the competing parties—information the model cannot reliably infer from unstructured text.
- **Core assumption**: Models struggle to infer document structure implicitly; explicit segmentation allows attention mechanisms to weight information more appropriately.
- **Evidence anchors**:
  - [abstract]: "reorganizing documents based on rhetorical roles to assess how structured information affects long context processing and model decisions"
  - [section 4.3]: "The absence of PREAMBLE negatively affected the model's performance, likely because the model relies on meta-data such as the competing parties' names typically found in the PREAMBLE"
  - [corpus]: LegalSeg (arXiv:2502.05836) provides 7,000+ documents with rhetorical role annotations; MARRO (arXiv:2503.10659) addresses rhetorical role labeling challenges.

### Mechanism 2
- **Claim**: Injecting explicit definitions of rhetorical roles at prompt start improves model understanding of legal terminology without fine-tuning.
- **Mechanism**: Definitions serve as in-context learning signals that bridge vocabulary mismatch between general pretraining data and specialized legal language. This is a lightweight alternative to domain alignment.
- **Core assumption**: A significant portion of model error stems from terminology gap, not reasoning incapacity.
- **Evidence anchors**:
  - [abstract]: "defining rhetorical roles to familiarize the model with legal terminology"
  - [section 6.1]: "definitions of rhetorical roles significantly contributed to the model's enhanced performance by augmenting its understanding of the complex structure of legal language"
  - [corpus]: Limited direct corpus evidence on definition injection specifically; related work on prompt complexity (Mu et al., 2024) cited in paper.

### Mechanism 3
- **Claim**: Multi-stage chain-of-thought prompting (ANALYSIS → RATIO → RPC) can improve judgment prediction, but combining all components (D/R/C) underperforms simpler configurations.
- **Mechanism**: Recursive prompting where each stage output feeds the next, emulating hierarchical legal reasoning. However, the paper finds that shorter combinations (D/R or C alone) outperform the full stack—suggesting component interactions may introduce noise.
- **Core assumption**: Legal reasoning has a decomposable structure models can follow; however, more components ≠ better performance due to prompt complexity tradeoffs.
- **Evidence anchors**:
  - [section 3]: "The ANALYSIS and the initial input are then fed back into the LLM to produce the RATIO. Afterwards, the original input, ANALYSIS, and RATIO are all input into the LLM to generate the RPC"
  - [section 6.1]: "the ablation involving all components did not yield the best performance outcomes, while shorter combinations proved more effective"
  - [corpus]: NyayaRAG (arXiv:2508.00709) explores RAG-based LJP under Indian common law; Legal Assist AI (arXiv:2505.22003) applies transformers to Indian legal assistance.

## Foundational Learning

- **Concept: Rhetorical Roles in Legal Documents**
  - **Why needed here**: The entire method depends on understanding that legal judgments have functional sentence types—Facts (FAC), Ruling by Lower Court (RLC), Arguments (ARG), Precedents (PRE), Analysis (ANALYSIS), Ratio (RATIO), Final Decision (RPC). Without this, you cannot implement segmentation or interpret the D/R configuration.
  - **Quick check question**: Given a sentence from a judgment—"The appellant contends that the lower court erred in admitting witness testimony"—which rhetorical role does it most likely belong to?

- **Concept: In-Context Learning (ICL) for Domain Adaptation**
  - **Why needed here**: The paper's approach relies on zero-shot ICL via prompt engineering rather than fine-tuning. Understanding ICL's strengths (no weight updates, fast iteration) and limitations (context window constraints, variable effectiveness) is essential for diagnosing why D/R worked better than D/R/C.
  - **Quick check question**: Why might adding more components to a prompt (D/R/C) degrade performance compared to a simpler configuration (D/R)?

- **Concept: False Positive/Negative Rates in Legal Context**
  - **Why needed here**: The paper explicitly uses FPR and FNR (not just F1) due to the legal doctrine of "presumption of innocence." A system that wrongly predicts guilt (high FPR) has different ethical implications than one that misses convictions (high FNR).
  - **Quick check question**: In legal judgment prediction, which error type carries greater risk of harm to defendants, and how should this inform metric selection?

## Architecture Onboarding

- **Component map**:
  Input Document → [Rhetorical Role Segmenter] → Segmented Document → [Definition Component] → Enriched Prompt → LLM → [Chain Stage 1: ANALYSIS] → [Chain Stage 2: RATIO] (optional) → [Chain Stage 3: RPC] (optional) → Binary Output (YES/NO)

- **Critical path**:
  1. Obtain or build rhetorical role annotations (paper uses existing datasets from Kalamkar et al. and Bambroo et al.)
  2. Format segmented roles as `[ROLE_NAME]\n{sentences}\n\n` blocks
  3. Prepend definitions (D) if using D or D/R or D/C configurations
  4. Execute single-pass (no chain) or multi-pass (C) inference
  5. Extract binary decision; verify consistency between explanation and output

- **Design tradeoffs**:
  - D/R vs D/R/C: Paper shows D/R achieves best F1/FPR/FNR balance on Dataset 1; adding chain (C) helps on PredEx but hurts on smaller datasets. Start with D/R.
  - Model selection: Llama-3.1-8B and o3-mini show strongest legal alignment; Phi-3 and Mistral 7B underperform or produce indecisive outputs.
  - Context window: D/R/C prompts exceeded 10k tokens—requires models with large context (128k+ recommended).

- **Failure signatures**:
  - Indecisive outputs (Mistral): Model refuses binary decision; indicates insufficient evidence or prompt ambiguity.
  - Inconsistent explanation-decision pairs: Analysis favors plaintiff but output is NO. Paper added explicit consistency-check step to mitigate.
  - High FPR with low FNR: Model biased toward finding for plaintiff; check if preamble (party identification) was removed.

- **First 3 experiments**:
  1. **Baseline replication**: Run "None" configuration (unstructured input, no definitions, no chain) on Dataset 1 to establish F1/FPR/FNR baseline for your model.
  2. **D/R ablation**: Add rhetorical role segmentation + definitions; expect +1.5-4.36% F1 improvement if results replicate.
  3. **Component interaction test**: Compare D/R vs D/R/C on your data to verify paper's finding that full stack underperforms—this validates whether prompt complexity tradeoff generalizes to your domain.

## Open Questions the Paper Calls Out

- **Open Question 1**: To what extent does fine-tuning or few-shot learning on rhetorical roles improve performance over the zero-shot structured prompting approach?
  - **Basis in paper**: [explicit] The authors state in Section 8 that future work should "explore few-shot and fine-tuning methods to enhance the model’s understanding of legal terminology."
  - **Why unresolved**: The current study was restricted by GPU memory limitations, confining the methodology to zero-shot inference.
  - **What evidence would resolve it**: A comparative study evaluating the F1 scores of fine-tuned LLMs against the zero-shot D/R (Definitions/Roles) baseline on the same datasets.

- **Open Question 2**: Can the structured prompting framework be adapted to predict complex, non-binary outcomes such as partial appeals?
  - **Basis in paper**: [explicit] Section 8 notes that "instead of only considering binary outcomes, we can explore partial appeals to make systems more useful and robust."
  - **Why unresolved**: The current research simplified the task to binary classification (Yes/No) to resolve annotation disagreements and simplify model evaluation.
  - **What evidence would resolve it**: Extending the dataset annotation to include partial verdict labels and measuring the model's ability to distinguish them from full verdicts.

- **Open Question 3**: How does the temporal evolution of legal statutes and precedents affect the model's judgment prediction accuracy?
  - **Basis in paper**: [explicit] The authors suggest in the Limitations section that obtaining a larger dataset would allow them to "examine the temporal aspects of the legal domain."
  - **Why unresolved**: The current datasets and experiments do not account for the time dimension or changes in legal standards over the years.
  - **What evidence would resolve it**: A longitudinal analysis of model performance on cases separated by significant time gaps or statutory changes.

## Limitations

- **Definition specificity gap**: Exact definition text and complete list of rhetorical role headers used in experiments are not provided, requiring reconstruction from related papers.
- **Generalizability boundary**: Results may be specific to Indian common law and the three test datasets; performance on other jurisdictions or document types is untested.
- **Metric interpretation**: While FPR/FNR are justified for legal contexts, the tradeoff between them is not fully explored across datasets.

## Confidence

- **High**: The core finding that D/R configuration outperforms other ablations on Dataset 1 (44 cases) and that D/R/C underperforms simpler configs.
- **Medium**: The generalizability of the 1.5-4.36% F1 improvement range across different Indian legal domains and models.
- **Low**: The exact wording of definitions and the full set of rhetorical role headers, which were not specified in the paper.

## Next Checks

1. **Definition fidelity test**: Reconstruct and test multiple variants of rhetorical role definitions to identify the specific wording that maximizes performance.
2. **Jurisdiction transfer**: Apply the D/R configuration to non-Indian legal documents (e.g., US/UK judgments) to assess cross-jurisdictional robustness.
3. **Error analysis drill-down**: Examine false positive vs. false negative distributions to validate the legal reasoning behind FPR/FNR metric choices and identify systematic biases.