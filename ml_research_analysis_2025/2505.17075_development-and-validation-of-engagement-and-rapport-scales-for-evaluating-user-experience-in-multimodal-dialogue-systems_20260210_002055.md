---
ver: rpa2
title: Development and Validation of Engagement and Rapport Scales for Evaluating
  User Experience in Multimodal Dialogue Systems
arxiv_id: '2505.17075'
source_url: https://arxiv.org/abs/2505.17075
tags:
- engagement
- rapport
- dialogue
- were
- items
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study developed and validated engagement and rapport scales
  for evaluating user experience in multimodal dialogue systems within foreign language
  learning contexts. Based on theories from educational psychology, social psychology,
  and second language acquisition, the researchers designed 21 questionnaire items
  to measure behavioral, cognitive, emotional, and social engagement, as well as face
  management, mutual attentiveness, and coordination aspects of rapport.
---

# Development and Validation of Engagement and Rapport Scales for Evaluating User Experience in Multimodal Dialogue Systems

## Quick Facts
- arXiv ID: 2505.17075
- Source URL: https://arxiv.org/abs/2505.17075
- Reference count: 18
- This study developed and validated engagement and rapport scales for evaluating user experience in multimodal dialogue systems within foreign language learning contexts.

## Executive Summary
This study developed and validated 21-item questionnaire scales to measure user engagement and rapport during interactions with multimodal dialogue systems in foreign language learning contexts. Based on theories from educational psychology, social psychology, and second language acquisition, the scales evaluate four engagement dimensions (behavioral, cognitive, emotional, social) and three rapport dimensions (face management, mutual attentiveness, coordination). After 71 Japanese learners of English completed roleplay and discussion tasks with both human tutors and a dialogue agent, the scales demonstrated strong structural validity through confirmatory factor analyses and successfully differentiated between human and AI interlocutors across all measured dimensions.

## Method Summary
The study recruited 74 Japanese English learners, excluding 3 for not completing the experiment, resulting in 71 participants. Participants engaged in two task types (role-play: deadline extension request, discussion: SNS debate) with either a GPT-4 based dialogue agent or one of five trained human tutors. Each interaction followed a 3-phase structure (Introduction, Main Task, Closing) with the agent using phase-specific prompts and completion tokens. Immediately after each task, participants completed a 21-item questionnaire on a 5-point Likert scale measuring engagement (12 items across 4 sub-dimensions) and rapport (9 items across 3 sub-dimensions). The questionnaire included 2 reverse-scored items to detect acquiescence bias. Data analysis included calculating Cronbach's alpha for internal consistency, confirmatory factor analysis to test structural validity, and t-tests to compare human vs. agent scores.

## Key Results
- Human tutors scored significantly higher than the AI agent across all engagement and rapport dimensions (average 4.45 vs. 3.60 in role-play, 4.44 vs. 3.53 in discussion)
- The multi-factor engagement model demonstrated strong structural validity with SRMR values below 0.08
- Cronbach's alpha showed high internal consistency for social engagement (0.863-0.931) and rapport items (0.726-0.932), though cognitive engagement showed lower reliability
- The scales successfully differentiated between human-human and human-agent interaction quality

## Why This Works (Mechanism)

### Mechanism 1: Multi-Factor Construct Modeling
- **Claim:** Evaluating user experience (UX) in dialogue systems as distinct, correlated sub-constructs (Behavioral, Cognitive, Emotional, Social) rather than a unidimensional "satisfaction" score appears to yield higher structural validity.
- **Mechanism:** The paper employs Confirmatory Factor Analysis (CFA) to validate a "Hypothesis Model" where Engagement and Rapport are second-order factors comprising specific first-order dimensions (e.g., Face Management, Mutual Attentiveness). By comparing this against a baseline unidimensional model, the analysis isolates specific psychological attitudes (e.g., "I felt respected" vs. "I tried hard") rather than conflating them.
- **Core assumption:** Users cognitively distinguish between their internal effort (Cognitive Engagement) and their relationship with the interlocutor (Social Engagement/Rapport) when answering questionnaires.
- **Evidence anchors:**
  - [abstract] The abstract notes the scales "demonstrated structural validity through confirmatory factor analyses," distinguishing behavioral, cognitive, emotional, and social components.
  - [section] Section 4.2 (Results) reports that the hypothesized correlated four-factor model for engagement generally outperformed the baseline model, evidenced by increased CFI/TLI and decreased SRMR.
  - [corpus] Neighbor paper "Navigating the Synchrony-Stability Frontier in Adaptive Chatbots" supports the need to balance distinct variables (synchrony vs. stability), aligning with the need for multi-factor evaluation over single metrics.
- **Break condition:** If future studies find high collinearity between dimensions (e.g., Emotional and Social Engagement always scoring identically), the granularity of this multi-factor model may be unnecessary overhead.

### Mechanism 2: The "Human-Gap" Sensitivity
- **Claim:** The validated scales function effectively as instruments to quantify the qualitative deficit between human-human and human-agent interaction, specifically in social dimensions.
- **Mechanism:** The questionnaire items operationalize "Rapport" and "Social Engagement" through social psychology constructs (e.g., Face Management, Mutual Attentiveness). The mechanism relies on the user's subjective perception of social friction; if the agent fails to simulate "care" or "respect" (Face Management), the scale captures this as a statistically significant score reduction compared to human baselines.
- **Core assumption:** Lower scores for agents reflect a genuine lack of social capability in the agent, rather than user bias against interacting with a machine.
- **Evidence anchors:**
  - [abstract] The results showed human tutors scored higher than the AI agent across all dimensions (p < 0.05), specifically highlighting gaps in quality of experience.
  - [section] Section 4.3 notes that in discussion tasks, human tutors averaged 4.44 vs 3.53 for the agent, demonstrating the scale's sensitivity to interaction quality.
  - [corpus] "Human vs. Agent in Task-Oriented Conversations" (arXiv:2509.17619) reinforces the importance of comparative evaluation frameworks, though specific corpus evidence for this exact scale is weak.
- **Break condition:** If agents achieve statistical parity with human tutors on these specific "social" items, the scale may lose its discriminative power for SOTA (State-of-the-Art) systems.

### Mechanism 3: Task-Context Reliability Variance
- **Claim:** The internal consistency of measuring psychological constructs like "Engagement" is contingent upon the interlocutor type (Human vs. Agent), specifically affecting "Cognitive Engagement."
- **Mechanism:** The paper identifies a drop in Cronbach's alpha for Cognitive Engagement when interacting with humans (α = 0.238). This suggests the mechanism of "mental effort" differs fundamentally between struggling with a machine (high consistency in effort perception) and conversing with a human, where social nuances may obscure the user's self-assessment of "focus."
- **Core assumption:** The low alpha is not a failure of the scale design but a reflection of the differing psychological demands of human vs. agent interlocution.
- **Evidence anchors:**
  - [section] Section 4.1 explicitly states, "the value for cognitive engagement was significantly lower when the dialogue partner was a human (α = 0.238)."
  - [corpus] "Enhancing User Engagement in Socially-Driven Dialogue" suggests engagement is subtle and context-dependent, supporting the idea that reliability varies by interaction type.
- **Break condition:** If the low alpha persists across different agent architectures, the "Cognitive Engagement" items themselves may be ill-defined for dialogue contexts.

## Foundational Learning

- **Concept: Confirmatory Factor Analysis (CFA)**
  - **Why needed here:** The paper relies on CFA to prove that its 21 questions actually map to the theoretical concepts (Engagement vs. Rapport). Without understanding CFA, one cannot verify if the evaluation metric is valid or just noise.
  - **Quick check question:** Does a high Comparative Fit Index (CFI) indicate that the questionnaire items accurately measure the intended latent constructs (Engagement/Rapport) or just correlate with each other?

- **Concept: Construct Validity vs. Reliability**
  - **Why needed here:** The paper distinguishes between *structural validity* (does the model fit the theory?) and *internal consistency* (do items yield similar results?). High reliability does not guarantee validity (e.g., a broken ruler is reliable but invalid).
  - **Quick check question:** If the "Cognitive Engagement" scale has low Cronbach's alpha (reliability) with humans, can we still trust its validity as a measure of mental effort in that context?

- **Concept: Multidimensionality of Engagement**
  - **Why needed here:** The system evaluates "Engagement" not as a single score but as a vector of Behavioral, Cognitive, Emotional, and Social states. An engineer must understand that a system can fail at "Social Engagement" while succeeding at "Behavioral Engagement."
  - **Quick check question:** If a user completes all tasks (High Behavioral) but feels anxious (Low Emotional), how should the aggregate Engagement score be interpreted?

## Architecture Onboarding

- **Component map:** Dialogue Core (GPT-4 LLM) -> State Manager (Phase-transition logic) -> Evaluation Layer (21-item Qualtrics questionnaire) -> Interface (Multimodal Dialogue Agent)

- **Critical path:**
  1.  **Prompt Configuration:** Define phase-specific prompts (e.g., situational settings, rules) for the LLM.
  2.  **Interaction Loop:** User speaks -> System transcribes/understands -> LLM generates response -> System speaks.
  3.  **Phase Transition:** Manager detects "phase completion token" -> moves to next stage.
  4.  **Post-Interaction Data Capture:** Collect Likert scale responses (1-5) for the 21 items immediately after the Closing phase.

- **Design tradeoffs:**
  - **LLM vs. Scenario DB:** The system traded the high reliability of a scripted Scenario Database for the flexibility of an LLM to handle "reciprocal" discussion tasks. This improves task capability but risks hallucination or off-topic behavior.
  - **Reverse Scoring:** The study used reverse-scored items (e.g., "Did you feel stressed?") to detect acquiescence bias, but found they lowered model fit, suggesting they may introduce noise in online survey formats.

- **Failure signatures:**
  - **Low Alpha in Cognitive Dimension:** If replicating with human controls, expect low reliability in "Cognitive Engagement" items.
  - **Reverse-Item Inconsistency:** Watch for users incorrectly answering reverse-coded questions (e.g., rating "stress" high when they meant "enjoyment"), which skews factor analysis.
  - **Phase Stagnation:** The system relies on the LLM emitting a "phase completion token." If the LLM fails to generate this, the dialogue manager may stall.

- **First 3 experiments:**
  1.  **Scale Reduction:** Remove the two problematic reverse-scored items and re-run CFA to verify if model fit improves significantly without losing construct coverage.
  2.  **Cross-Domain Validation:** Apply the exact questionnaire to a non-learning task (e.g., customer service) to test if the "Social Engagement" factor remains distinct or collapses into "Rapport."
  3.  **Behavioral Correlation:** Correlate objective metrics (e.g., turn-taking frequency, latency) with "Behavioral Engagement" scores to determine if the subjective scale can be approximated automatically.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Are the developed engagement and rapport scales valid for dialogue tasks outside of foreign language learning, such as counseling?
- **Basis in paper:** [explicit] The authors state, "more research is necessary to examine whether these questionnaires can be applied for dialogue tasks other than English learning."
- **Why unresolved:** The current study validated the scales only within the specific context of English conversation tasks (role-play and discussion).
- **What evidence would resolve it:** A validation study demonstrating similar psychometric properties (structural validity, reliability) when the scales are applied to counseling or other non-learning dialogue domains.

### Open Question 2
- **Question:** Can engagement and rapport scores be accurately estimated automatically from dialogue data?
- **Basis in paper:** [explicit] The authors propose future work should focus on "developing methods for automatically estimating engagement and rapport scores."
- **Why unresolved:** The current methodology relies on post-interaction questionnaires, which are resource-intensive to collect and prevent real-time evaluation.
- **What evidence would resolve it:** The development of a predictive model (e.g., using machine learning) that utilizes acoustic and linguistic dialogue features to predict user scores without administering the questionnaire.

### Open Question 3
- **Question:** Which specific system behaviors causally drive higher engagement and rapport scores?
- **Basis in paper:** [explicit] The conclusion highlights the need for "systems that provide evidence of system behaviors that influence these scores."
- **Why unresolved:** This study compared a generic AI agent to human tutors but did not isolate which specific conversational attributes (e.g., turn-taking, empathy) drove the score differences.
- **What evidence would resolve it:** Controlled experiments (e.g., A/B testing) manipulating specific agent behaviors to measure their impact on the subscale scores (e.g., Face Management, Emotional Engagement).

### Open Question 4
- **Question:** Does the removal of reversed questionnaire items improve the structural validity of the scales?
- **Basis in paper:** [inferred] The authors noted that reversed items showed lower factor loadings, possibly because participants misinterpreted them, and suggested that "constructing a model with higher fit might have been achieved without the reversed items."
- **Why unresolved:** The study retained these items in the final analysis despite the noise, leaving the optimization of the questionnaire composition as an open methodological issue.
- **What evidence would resolve it:** A follow-up Confirmatory Factor Analysis (CFA) comparing the current model fit against a modified model where the problematic reversed items are removed.

## Limitations
- The scale's validity for non-educational dialogue tasks remains untested, as validation was conducted exclusively within foreign language learning contexts
- The cognitive engagement dimension showed concerning reliability issues (α = 0.238) when interacting with human tutors, suggesting the scale may not measure the intended construct consistently across interlocutor types
- The GPT-4 system prompts and phase transition logic are not fully specified, making exact replication of the dialogue agent challenging

## Confidence
- **High Confidence:** The structural validity of the multi-factor engagement model (behavioral, cognitive, emotional, social) is well-supported by CFA results and differentiates human vs. AI interactions effectively
- **Medium Confidence:** The rapport scale demonstrates acceptable reliability (α = 0.726-0.932) but requires more diverse testing across different dialogue domains to confirm generalizability
- **Low Confidence:** The cognitive engagement dimension's low reliability with human interlocutors raises questions about whether the scale measures the intended construct or is confounded by interaction type

## Next Checks
1. **Cross-Domain Validation:** Apply the questionnaire to customer service or casual conversation tasks to test if social engagement remains distinct from rapport in non-educational contexts
2. **Scale Refinement:** Remove the two problematic reverse-scored items and re-run CFA to verify if model fit improves without losing construct coverage
3. **Behavioral Correlation:** Correlate objective interaction metrics (turn-taking frequency, response latency) with subjective engagement scores to determine if the scales can be approximated through automatic behavioral analysis