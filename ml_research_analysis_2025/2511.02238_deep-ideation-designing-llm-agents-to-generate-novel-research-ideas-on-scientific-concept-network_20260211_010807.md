---
ver: rpa2
title: 'Deep Ideation: Designing LLM Agents to Generate Novel Research Ideas on Scientific
  Concept Network'
arxiv_id: '2511.02238'
source_url: https://arxiv.org/abs/2511.02238
tags:
- idea
- research
- keyword
- keywords
- scientific
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Deep Ideation, a framework that integrates
  large language models (LLMs) with a scientific concept network to generate novel
  research ideas. The framework constructs a keyword co-occurrence network from approximately
  100,000 AI papers and employs an iterative explore-expand-evolve workflow guided
  by a critic model trained on real-world reviewer feedback.
---

# Deep Ideation: Designing LLM Agents to Generate Novel Research Ideas on Scientific Concept Network

## Quick Facts
- **arXiv ID**: 2511.02238
- **Source URL**: https://arxiv.org/abs/2511.02238
- **Reference count**: 39
- **Primary result**: Framework generates novel research ideas with 10.67% quality improvement over baselines, exceeding top conference acceptance levels

## Executive Summary
Deep Ideation presents a framework that combines large language models with a scientific concept network to automate research idea generation. The system constructs a keyword co-occurrence network from approximately 100,000 AI papers and uses an iterative explore-expand-evolve workflow guided by a critic model trained on real-world reviewer feedback. Experiments across four AI domains demonstrate significant improvements in idea quality compared to baseline methods.

The framework addresses the challenge of research ideation by leveraging structured scientific knowledge and iterative refinement processes. Human evaluation confirms the practical value of generated ideas, while ablation studies validate the effectiveness of individual workflow components. The approach shows promise for accelerating research discovery by systematically exploring the space of possible research directions within established scientific domains.

## Method Summary
The Deep Ideation framework integrates LLMs with a scientific concept network through a three-stage iterative workflow. It begins by constructing a keyword co-occurrence network from a corpus of AI papers, then uses this network to guide LLM-based ideation through explore, expand, and evolve phases. A critic model trained on reviewer feedback evaluates and ranks generated ideas, enabling iterative refinement. The system operates across multiple AI domains including computer vision, natural language processing, and machine learning, using domain-specific concept networks to maintain relevance while exploring novel combinations of ideas.

## Key Results
- Deep Ideation improves idea quality by 10.67% compared to baseline methods
- Generated ideas exceed top conference acceptance levels in automated evaluations
- Human evaluation by PhD students confirms practical value of generated ideas

## Why This Works (Mechanism)
The framework's effectiveness stems from combining structured scientific knowledge with iterative refinement. By constructing a keyword co-occurrence network from existing research, it grounds ideation in established scientific concepts while enabling systematic exploration of novel combinations. The critic model trained on real reviewer feedback provides domain-specific quality assessment that guides the iterative refinement process, ensuring generated ideas meet academic standards while maintaining novelty.

## Foundational Learning
- **Keyword co-occurrence networks**: Maps relationships between scientific concepts by analyzing how frequently keywords appear together in papers - needed to capture domain structure and identify promising conceptual connections
- **Iterative explore-expand-evolve workflow**: Systematically generates, refines, and evolves research ideas through multiple cycles - needed to balance novelty with feasibility and relevance
- **Critic model training**: Uses real reviewer feedback to create automated evaluation metrics - needed to ensure generated ideas meet academic quality standards
- **LLM-guided ideation**: Employs large language models to generate and manipulate research concepts - needed to handle the complexity and creativity required for novel idea generation
- **Domain-specific concept networks**: Creates separate networks for different AI subfields - needed to maintain domain relevance while exploring cross-cutting ideas
- **Automated quality metrics**: Measures novelty, diversity, and feasibility of generated ideas - needed for systematic comparison and optimization

## Architecture Onboarding

**Component map**: Keyword network construction -> LLM ideation module -> Critic model evaluation -> Iterative refinement loop -> Final idea ranking

**Critical path**: Keyword network -> Explore phase -> Critic evaluation -> Evolve phase -> Output

**Design tradeoffs**: 
- Uses keyword co-occurrence instead of full-text analysis for scalability
- Employs automated metrics alongside human evaluation for comprehensive assessment
- Balances exploration of novel ideas with grounding in established concepts
- Iterates through refinement cycles rather than single-pass generation

**Failure signatures**:
- Poor keyword network coverage leading to limited idea diversity
- Critic model bias toward conventional ideas reducing novelty
- LLM generation stuck in local optima without sufficient exploration
- Metric misalignment causing evaluation of suboptimal ideas

**3 first experiments**:
1. Test keyword network construction with varying corpus sizes (10k, 50k, 100k papers)
2. Evaluate critic model performance on held-out reviewer feedback
3. Compare iterative vs single-pass ideation workflows on idea quality

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relies heavily on automated metrics that may not capture true research impact
- Focus on AI domains limits generalizability to other scientific fields
- Human evaluation sample size of 10 PhD students may not represent broader researcher perspectives

## Confidence
**High**: Framework architecture is clearly described and technically sound; 10.67% improvement over baselines is reported with specific metrics
**Medium**: Claims about exceeding top conference acceptance levels need more detailed benchmarking data; human evaluation has limited sample size
**Low**: Generalization to non-AI domains is not demonstrated; long-term effectiveness in actual research settings remains unverified

## Next Checks
1. Conduct a longitudinal study tracking the actual development and publication outcomes of ideas generated by Deep Ideation versus human-generated ideas over 2-3 years
2. Expand human evaluation to include researchers from diverse scientific domains beyond AI, with larger sample sizes and multiple rounds of blind review
3. Perform stress testing of the keyword network by deliberately introducing emerging concepts and measuring how well the framework adapts to novel terminology and interdisciplinary connections