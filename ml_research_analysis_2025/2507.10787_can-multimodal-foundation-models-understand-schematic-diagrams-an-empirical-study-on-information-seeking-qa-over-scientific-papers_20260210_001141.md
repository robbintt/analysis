---
ver: rpa2
title: Can Multimodal Foundation Models Understand Schematic Diagrams? An Empirical
  Study on Information-Seeking QA over Scientific Papers
arxiv_id: '2507.10787'
source_url: https://arxiv.org/abs/2507.10787
tags:
- question
- module
- bounding
- highlighted
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MISS-QA, the first benchmark designed to
  evaluate models' ability to interpret schematic diagrams in scientific literature.
  The benchmark includes 1,500 expert-annotated examples over 465 scientific papers,
  focusing on information-seeking questions that require understanding highlighted
  visual elements within diagrams and reasoning over the broader paper context.
---

# Can Multimodal Foundation Models Understand Schematic Diagrams? An Empirical Study on Information-Seeking QA over Scientific Papers

## Quick Facts
- arXiv ID: 2507.10787
- Source URL: https://arxiv.org/abs/2507.10787
- Authors: Yilun Zhao; Chengye Wang; Chuhan Li; Arman Cohan
- Reference count: 18
- Primary result: Introduced MISS-QA benchmark with 1,500 expert-annotated examples; found significant performance gap between human experts (89.0%) and best open-source model (61.6%) on schematic diagram understanding

## Executive Summary
This paper introduces MISS-QA, the first benchmark designed to evaluate models' ability to interpret schematic diagrams in scientific literature. The benchmark includes 1,500 expert-annotated examples over 465 scientific papers, focusing on information-seeking questions that require understanding highlighted visual elements within diagrams and reasoning over the broader paper context. The evaluation of 18 frontier multimodal foundation models reveals a significant performance gap between human experts (89.0% accuracy) and current models, with the best open-source model (Qwen2.5-VL-72B) achieving only 61.6% accuracy. Detailed error analysis shows models struggle with diagram interpretation, retrieving relevant context, and reasoning correctly, while also exhibiting overconfidence in responding to unanswerable questions. These findings highlight the need for improved capabilities in multimodal scientific literature understanding.

## Method Summary
The authors constructed MISS-QA by collecting 465 recent scientific papers and having expert annotators create 1,500 question-answer pairs that require interpreting schematic diagrams with highlighted regions. Each question references a specific color-coded bounding box in the diagram, forcing models to establish visual-to-text grounding. The benchmark includes 26.5% unanswerable questions to test answerability detection. Models were evaluated using LLM-as-judge (GPT-4.1) and human expert validation, with chain-of-thought prompting applied to all model inputs.

## Key Results
- Human experts achieve 89.0% accuracy on MISS-QA test set
- Best open-source model (Qwen2.5-VL-72B) reaches 61.6% accuracy
- Proprietary models (O4-mini, GPT-4.1) significantly outperform open-source models (78.3%, 77.8% accuracy)
- Models struggle most with unanswerable questions, achieving only 33-47% accuracy versus 85% for humans
- Error analysis reveals primary failure modes: visual grounding failures, context retrieval issues, reasoning errors, and overconfidence

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Forcing models to interpret highlighted visual elements before accessing textual context creates a necessary reasoning bottleneck that exposes genuine multimodal understanding capabilities.
- **Mechanism:** The annotation protocol requires annotators to place colored bounding boxes on schematic diagrams and reference them in questions ("the module highlighted by [color] bounding box"). This design eliminates textual shortcuts and forces visual-to-text grounding as a prerequisite for context retrieval.
- **Core assumption:** Models that perform well can establish correspondences between visual regions and their conceptual/functional roles as described in paper text.
- **Evidence anchors:** [Page 5] Annotators apply bounding boxes "forcing models to interpret the figure rather than relying on textual clues within the question itself" and [Page 5] The benchmark "ensures that the questions really necessitate interpretation of schematic diagram within the paper"
- **Break condition:** If models can answer questions by pattern-matching on question text alone (ignoring the highlighted regions), the mechanism fails. The low scores on unanswerable questions (33-47% for most models vs. 85% for humans) suggest the mechanism is working but exposing a real capability gap.

### Mechanism 2
- **Claim:** Requiring answerability detection (knowing when information is insufficient) creates a proxy for measuring context comprehension depth and hallucination propensity.
- **Mechanism:** The 26.5% unanswerable questions force models to distinguish between "I cannot find this in the paper" and "I can construct a plausible answer from parametric knowledge." Models that succeed must perform exhaustive context search and negative verification.
- **Core assumption:** Models with stronger context comprehension will more accurately determine when evidence is absent, rather than defaulting to generation.
- **Evidence anchors:** [Page 7] Confusion matrices show models like GPT-4o achieve only 34.7% accuracy on unanswerable questions and [Page 6] "Most models... continue to struggle with unanswerable questions. These models often exhibit overconfidence"
- **Break condition:** If models achieve high accuracy on answerable questions but low accuracy on unanswerable ones (as observed), it indicates successful context retrieval but poor uncertainty calibration.

### Mechanism 3
- **Claim:** Performance gaps between proprietary and open-source models on schematic interpretation correlate with the degree of scientific multimodal data exposure during training.
- **Mechanism:** O4-mini and GPT-4.1 achieve 78.3% and 77.8% accuracy respectively, while Qwen2.5-VL-72B reaches 61.6%. The gap may reflect proprietary models' access to larger/more diverse scientific corpora, or architectural differences in vision-language fusion strategies.
- **Core assumption:** The selected papers (July-November 2024) are post-training-cutoff for most open-source models, but this does not fully explain the gap.
- **Evidence anchors:** [Page 4] Papers are selected "after the cutoff date for most open-source pretraining corpora" and [Page 7] "Notable improvements can be observed within the same model families: Qwen2.5-VL-72B outperforms its predecessor Qwen2-VL-72B by 7.4%"
- **Break condition:** If the gap disappears when controlling for model scale (parameters, compute), then the mechanism is not about training data but about capacity. The 8B vs 72B comparison within model families supports scale sensitivity.

## Foundational Learning

- **Concept: Vision-Language Grounding**
  - **Why needed here:** The benchmark's core innovation is requiring models to ground textual references ("module highlighted in red") to specific visual regions in schematics, then connect those regions to paper content.
  - **Quick check question:** Given a schematic diagram with a labeled component and a paper section describing that component, can you identify the three-way mapping between visual region, textual label, and functional description?

- **Concept: Information-Seeking Question Answering**
  - **Why needed here:** MISS-QA questions simulate real researcher scenarios (design rationale, implementation details, literature background) rather than factoid extraction, requiring synthesis across paper sections.
  - **Quick check question:** For a paper with a methodology diagram and results section, what additional information beyond the diagram is needed to answer "Why was this module designed this way?"

- **Concept: Uncertainty Calibration in Generative Models**
  - **Why needed here:** The 26.5% unanswerable questions test whether models can refuse to answer when evidence is insufficient, rather than hallucinating plausible responses.
  - **Quick check question:** If a model answers a question about a paper's methodology but the relevant information only appears in a different paper by the same authors, should the model answer or refuse?

## Architecture Onboarding

- **Component map:** Input: Schematic diagram (image) + Caption + Full paper text + Question → Visual Encoder → Text Encoder → Cross-Modal Grounding → Context Retrieval → Answerability Assessment → Reasoning & Generation
- **Critical path:** The visual grounding step (identifying what "module highlighted in red" refers to) must succeed before context retrieval can begin. Error analysis shows this is a primary failure point.
- **Design tradeoffs:** Chain-of-thought prompting (used in experiments) improves reasoning but increases inference cost; Larger context windows (128K in Qwen2.5-VL vs 32K in earlier models) help but don't solve grounding failures; LLM-as-judge evaluation (GPT-4.1) is scalable but may inherit biases; human evaluation is gold standard but costly
- **Failure signatures:** 1. Visual grounding failure: Model describes diagram generically without connecting highlighted region to specific concepts; 2. Context retrieval failure: Model finds wrong paper section or uses parametric knowledge instead of paper content; 3. Reasoning error: Model finds correct information but draws incorrect conclusions; 4. Overconfidence: Model generates detailed answer when question is unanswerable; 5. Visual overreliance: Model ignores paper context and answers from diagram alone
- **First 3 experiments:** 1. Baseline reproduction: Run Qwen2.5-VL-7B and Qwen2.5-VL-72B on the testmini split (500 examples) to verify reported 42.1% vs 61.6% gap; analyze whether errors concentrate in specific question types; 2. Ablate visual grounding: Provide models with questions where bounding box references are replaced with explicit module names (e.g., "the Retriever module" instead of "the module highlighted in red"); measure performance change to quantify grounding difficulty; 3. Answerability calibration: Fine-tune a binary classifier on top of a frozen multimodal model to predict answerability from attention patterns over the paper context; evaluate whether this reduces overconfidence on unanswerable questions

## Open Questions the Paper Calls Out
- How can models be improved to better interpret schematic diagrams in scientific literature?
- What specific architectural or training data improvements would most effectively close the performance gap with human experts?
- Can uncertainty calibration be improved to reduce overconfidence on unanswerable questions?
- How do different vision-language fusion strategies affect schematic diagram understanding performance?

## Limitations
- Benchmark covers only three schematic diagram types (flowcharts, functional diagrams, and block diagrams) from 465 papers, limiting generalizability
- Human expert baseline consists of just three annotators, raising questions about inter-rater reliability
- Performance gap between models and humans may reflect task design difficulty rather than just model limitations
- Limited transparency about pretraining data composition prevents definitive analysis of why proprietary models outperform open-source models

## Confidence

| Level | Assessment |
|-------|------------|
| High (4) | Benchmark construction methodology is sound with clear annotation protocols and substantial dataset addressing genuine evaluation gap |
| Medium (3) | Error analysis patterns are consistent with observed performance distributions, though exact failure attribution could benefit from more granular analysis |
| Low (2) | Interpretation of proprietary vs open-source performance gap remains speculative due to limited transparency about pretraining data |

## Next Checks

1. **Generalization Testing:** Evaluate the same models on schematic diagrams from scientific domains not represented in the MISS-QA corpus (e.g., biological pathway diagrams, chemical reaction schemes) to assess whether performance patterns hold across visual diagram types.

2. **Grounding Mechanism Isolation:** Design controlled experiments where visual highlighting is progressively removed or replaced with explicit textual labels, measuring how performance changes to quantify the specific contribution of visual-to-text grounding versus general multimodal reasoning capabilities.

3. **Answerability Calibration Benchmark:** Develop a dedicated evaluation protocol for uncertainty calibration, where models are tested on systematically varied levels of information sufficiency (clearly sufficient, clearly insufficient, borderline cases) to determine whether overconfidence stems from architectural limitations or training data biases.