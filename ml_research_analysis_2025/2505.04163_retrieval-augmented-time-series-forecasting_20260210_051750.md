---
ver: rpa2
title: Retrieval Augmented Time Series Forecasting
arxiv_id: '2505.04163'
source_url: https://arxiv.org/abs/2505.04163
tags:
- retrieval
- time
- series
- forecasting
- patterns
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RAFT introduces retrieval-augmented forecasting for time series,
  addressing the challenge of complex, non-stationary patterns. The method retrieves
  historical patches from the training set that are most similar to the input query,
  then uses the subsequent values of these patches to enhance predictions.
---

# Retrieval Augmented Time Series Forecasting

## Quick Facts
- arXiv ID: 2505.04163
- Source URL: https://arxiv.org/abs/2505.04163
- Reference count: 40
- Primary result: RAFT achieves average win ratio of 86% over contemporary baselines on ten benchmark datasets

## Executive Summary
RAFT introduces retrieval-augmented forecasting for time series, addressing the challenge of complex, non-stationary patterns. The method retrieves historical patches from the training set that are most similar to the input query, then uses the subsequent values of these patches to enhance predictions. This approach reduces the burden on models to memorize all patterns by directly providing relevant historical information at inference time. Empirical evaluations on ten benchmark datasets show RAFT consistently outperforms contemporary baselines.

## Method Summary
RAFT addresses time series forecasting by retrieving historical patterns most similar to the current input query. At inference, it computes Pearson correlation between the normalized input and all training key patches, selects the top-m most similar keys, and aggregates their corresponding value patches using softmax-weighted correlations. The method employs multi-period downsampling (P={1,2,4}) to capture patterns at different temporal scales, and uses offset normalization by subtracting the final timestep value to focus on directional similarity rather than absolute magnitudes. The aggregated retrieval result is concatenated with the processed input and passed to a predictor.

## Key Results
- RAFT achieves an average win ratio of 86% over contemporary baselines on ten benchmark datasets
- Multi-period retrieval consistently outperforms single-period alternatives on 5 of 8 datasets in ablation studies
- Pearson correlation with offset normalization outperforms cosine similarity, L2 distance, and learned projection alternatives across most datasets
- RAFT reduces computational burden by retrieving relevant patterns rather than requiring models to memorize all possible patterns

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieving historical patterns similar to the current input provides predictive signal because similar past trajectories tend to have similar continuations.
- Core assumption: Pattern similarity in the lookback window implies similarity in subsequent evolution—if two historical segments look alike, what followed one provides useful signal for what will follow the other.
- Evidence anchors: Spearman's correlation of 0.60 between key similarity and value similarity validates this trend across datasets.
- Break condition: If the time series exhibits regime changes where similar past patterns no longer predict future behavior, or if patterns are genuinely non-recurrent, retrieval provides noise rather than signal.

### Mechanism 2
- Claim: Multi-period downsampling enables retrieval at different temporal scales, capturing both local dynamics and global trends that single-scale retrieval would miss.
- Core assumption: Time series contain distinct patterns at different frequencies (e.g., hourly fluctuations vs. daily cycles vs. weekly trends), and accessing history at each scale improves the aggregate prediction.
- Evidence anchors: Ablation shows "With One Period" underperforms full multi-period retrieval on 5 of 8 datasets.
- Break condition: If the chosen periods do not align with actual periodicity in the data, or if downsampling destroys critical high-frequency signal, multi-period retrieval adds computation without benefit.

### Mechanism 3
- Claim: Offsetting by the final time step value (xL) before similarity computation makes pattern matching robust to scale and baseline shifts.
- Core assumption: The "shape" of recent changes matters more for prediction than absolute values, and scale-invariant comparison is appropriate for the target domain.
- Evidence anchors: Pearson correlation outperforms cosine similarity, L2 distance, and learned projection across most datasets.
- Break condition: If absolute values carry predictive information (e.g., threshold effects, saturation regimes), offset normalization discards useful signal.

## Foundational Learning

- Concept: Sliding Window Patch Extraction
  - Why needed here: RAFT constructs the retrieval database by segmenting the entire training time series into overlapping patches using a sliding window. Without understanding this, the key/value structure is opaque.
  - Quick check question: Given a time series of length 1000 with lookback L=96, forecast horizon F=96, and stride 1, how many key-value pairs are in the candidate set?

- Concept: Attention-Weighted Aggregation
  - Why needed here: The retrieval result is not a single retrieved patch but a softmax-weighted combination of the top-m value patches. This is structurally analogous to attention but operates over the entire training history rather than within a sequence.
  - Quick check question: If top-m keys have raw correlations [0.8, 0.6, 0.4] and temperature τ=0.1, what are the resulting attention weights?

- Concept: Distribution Shift in Non-Stationary Time Series
  - Why needed here: The paper explicitly motivates RAFT as addressing non-stationary patterns where "patterns may lack inherent temporal correlation" (Section 1). Understanding why standard models struggle here clarifies when retrieval helps.
  - Quick check question: Why might a model trained on data from 2020–2022 fail to predict 2024 patterns even if they resemble 2019 patterns?

## Architecture Onboarding

- Component map: Input (x) → Offset normalization → Linear projection f → Multi-period downsampling (P={1,2,4}) → Per-period retrieval: Pearson correlation → Top-m selection → Softmax → Weighted sum of values → Linear projections g(p) → Sum aggregation → Concatenate [f(x̂), Σg(p)(ṽ(p))] → Linear predictor h → Add offset → Prediction (y)

- Critical path: Pre-compute all key-value pairs from training data (one-time O(N²) cost, can increase stride to reduce) → At inference: normalize query, compute correlations across all keys for each period, select top-m, aggregate values → The retrieval result quality directly determines performance—Section 5.1 shows higher value similarity correlates with larger MSE reduction

- Design tradeoffs:
  - **Number of retrievals (m)**: Higher m incorporates more diverse patterns but dilutes signal if low-similarity patches are included. Paper uses 1–20 depending on dataset.
  - **Stride for candidate generation**: Stride=1 gives maximum coverage but O(N²) pre-computation. Stride=8 reduces time ~8× with modest MSE increase.
  - **Temperature (τ)**: Lower τ makes selection sharper (closer to hard top-1); higher τ smooths toward uniform. Paper finds τ=0.1 optimal.
  - **Lookback window (L)**: Longer lookback improves performance for linear architectures, consistent with prior work.

- Failure signatures:
  - **Random retrieval baseline**: Table 8 shows random retrieval performs worse than no retrieval on most datasets—retrieval must be similarity-based.
  - **Short training history**: Table 12 shows gains diminish when training data is limited, though retrieval still helps.
  - **Exchange Rate dataset**: RAFT underperforms TimeMixer (Table 1), suggesting retrieval may not suit domains with highly stochastic, non-recurrent dynamics.

- First 3 experiments:
  1. **Sanity check**: Implement retrieval on a single period with m=5, τ=0.1, L=96 on ETTh1. Compare: (a) random retrieval, (b) no retrieval, (c) similarity-based retrieval. Verify that (c) > (b) > (a).
  2. **Ablation on m and τ**: On a held-out validation split, grid search m∈{1,5,10,20} and τ∈{0.01,0.1,1,10}. Plot MSE vs. each parameter to confirm paper's reported sensitivity patterns.
  3. **Multi-period validation**: Compare single-period (P={1}) vs. multi-period (P={1,2,4}) retrieval. On datasets where the paper shows large gaps (ETTh1, Electricity), verify that multi-period aggregation is the driver, not just longer training.

## Open Questions the Paper Calls Out
None

## Limitations
- The core assumption that similar past patterns predict future behavior may break down under regime changes or non-recurrent patterns
- Multi-period period selection remains heuristic without theoretical grounding for optimal choices
- Offset normalization assumes scale-invariant patterns are optimal, but domains requiring absolute value prediction may lose signal

## Confidence
- **High confidence**: Retrieval consistently improves performance over no retrieval and random retrieval baselines
- **Medium confidence**: Multi-period retrieval provides consistent gains, though period selection remains heuristic
- **Medium confidence**: Pearson correlation with offset normalization outperforms alternatives, but corpus evidence is weak
- **Low confidence**: Claims about reducing model capacity burden are plausible but not directly validated against architectural comparisons

## Next Checks
1. **Distribution shift test**: Evaluate RAFT on data where patterns from 2019-2021 are queried against 2024 outcomes to quantify degradation under regime change
2. **Period sensitivity analysis**: Systematically vary P beyond {1,2,4} (e.g., {1,3,5}) across all datasets to test whether optimal periods are domain-specific or generalizable
3. **Scale-dependency validation**: Remove offset normalization on a subset of datasets to measure performance impact when absolute values are predictive, testing the scale-invariance assumption