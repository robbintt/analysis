---
ver: rpa2
title: 'ReVision: A Dataset and Baseline VLM for Privacy-Preserving Task-Oriented
  Visual Instruction Rewriting'
arxiv_id: '2502.14780'
source_url: https://arxiv.org/abs/2502.14780
tags:
- multimodal
- instruction
- dataset
- rewriting
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ReVision, a novel approach for privacy-preserving
  multimodal interaction through visual instruction rewriting. The method converts
  multimodal instructions into text-only commands, enabling lightweight on-device
  vision-language models (250M parameters) to process visual queries without transmitting
  sensitive image data to servers.
---

# ReVision: A Dataset and Baseline VLM for Privacy-Preserving Task-Oriented Visual Instruction Rewriting

## Quick Facts
- **arXiv ID**: 2502.14780
- **Source URL**: https://arxiv.org/abs/2502.14780
- **Reference count**: 7
- **Primary result**: Privacy-preserving visual instruction rewriting using 250M parameter on-device VLMs

## Executive Summary
ReVision introduces a novel approach for privacy-preserving multimodal interaction through visual instruction rewriting. The system converts multimodal instructions into text-only commands, enabling lightweight on-device vision-language models to process visual queries without transmitting sensitive image data to servers. A curated dataset of 39,000+ examples across 15+ domains was created, and a compact vision-language model was pretrained on image captioning datasets and fine-tuned for instruction rewriting.

## Method Summary
The approach employs a compact vision-language model (250M parameters) that processes visual inputs on-device and generates text-only instructions for server-side processing. The model was pretrained on image captioning datasets and fine-tuned on the curated dataset of 39,000+ examples across 15+ domains. The system supports 8-bit quantization, reducing the model size to under 500MB while maintaining performance. The instruction rewriting process bridges the gap between large-scale multimodal AI and privacy-centric, on-device execution, ensuring secure, real-time interaction with AR/VR and smartphone interfaces.

## Key Results
- 8-bit quantized model achieves strong performance across NLG metrics (BLEU, METEOR, ROUGE)
- Compact 250M parameter model effectively rewrites visual instructions with semantic parsing accuracy
- Model size under 500MB enables practical on-device deployment

## Why This Works (Mechanism)
The approach works by converting multimodal inputs into text-only representations that can be processed by lightweight on-device models, eliminating the need to transmit sensitive visual data to servers. The 8-bit quantization enables practical deployment while maintaining sufficient performance for instruction rewriting tasks.

## Foundational Learning
- **Vision-Language Model Quantization**: 8-bit quantization reduces model size while preserving accuracy - needed for on-device deployment, quick check: model size under 500MB
- **Instruction Rewriting**: Converting multimodal instructions to text-only commands - needed for privacy preservation, quick check: semantic parsing accuracy maintained
- **Dataset Curation**: Creating domain-specific examples across 15+ domains - needed for model training, quick check: 39,000+ examples with sufficient diversity
- **On-Device Processing**: Local execution of VLMs - needed for privacy, quick check: inference latency meets real-time requirements

## Architecture Onboarding
**Component Map**: Visual Input -> VLM (250M) -> Text-Only Instruction -> Server Processing

**Critical Path**: Visual input processing through quantized VLM to generate rewritten text instructions

**Design Tradeoffs**: Model size vs. performance (250M parameters with 8-bit quantization vs. larger models), privacy vs. accuracy (on-device processing vs. server-side multimodal processing)

**Failure Signatures**: Degraded performance on complex visual scenes, latency issues with larger input resolutions, semantic parsing errors in rewritten instructions

**First Experiments**:
1. Measure inference latency across different input resolutions
2. Test model performance on out-of-domain visual instructions
3. Evaluate robustness to adversarial visual inputs

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focuses on benchmark metrics without extensive real-world deployment testing in sensitive environments
- 39,000+ example dataset may not capture all edge cases and adversarial scenarios
- Performance trade-offs between compact model and larger state-of-the-art VLMs remain incompletely characterized

## Confidence
**High confidence**: Technical feasibility of converting multimodal instructions to text-only commands using on-device VLMs
**Medium confidence**: Effectiveness of 8-bit quantized model achieving strong NLG performance
**Medium confidence**: Claims of ensuring secure, real-time interaction require further empirical validation

## Next Checks
1. Deploy the quantized model in a controlled real-world setting with actual privacy-sensitive visual data to measure performance degradation, latency, and privacy guarantees
2. Conduct adversarial testing of the instruction rewriting system with deliberately obfuscated or misleading visual inputs
3. Perform comparative user studies measuring task completion success rates between compact model's rewritten instructions and original multimodal instructions across multiple privacy-sensitive domains