---
ver: rpa2
title: 'OncoReason: Structuring Clinical Reasoning in LLMs for Robust and Interpretable
  Survival Prediction'
arxiv_id: '2510.17532'
source_url: https://arxiv.org/abs/2510.17532
tags:
- reasoning
- clinical
- survival
- cancer
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces OncoReason, a framework for enhancing cancer
  outcome prediction in large language models (LLMs) by incorporating structured clinical
  reasoning. The method combines supervised fine-tuning with Chain-of-Thought (CoT)
  prompting and Group Relative Policy Optimization (GRPO) to jointly predict survival
  status, survival time, and generate interpretable rationales.
---

# OncoReason: Structuring Clinical Reasoning in LLMs for Robust and Interpretable Survival Prediction

## Quick Facts
- **arXiv ID**: 2510.17532
- **Source URL**: https://arxiv.org/abs/2510.17532
- **Reference count**: 40
- **Key outcome**: CoT prompting improves F1-score by +6.0 and reduces MAE by 12% in cancer survival prediction

## Executive Summary
OncoReason introduces a framework for enhancing cancer outcome prediction in large language models (LLMs) by incorporating structured clinical reasoning. The method combines supervised fine-tuning with Chain-of-Thought (CoT) prompting and Group Relative Policy Optimization (GRPO) to jointly predict survival status, survival time, and generate interpretable rationales. Experiments on the MSK-CHORD dataset demonstrate significant improvements in both predictive accuracy and interpretability, achieving state-of-the-art performance across multiple metrics.

## Method Summary
OncoReason uses cancer-type-specific attribute selection to construct natural language prompts from structured patient data. Models are first fine-tuned with supervised learning using CoT traces generated by DeepSeek R1, then further aligned using GRPO with a composite reward function. The framework employs LLaMA3-8B and Med42-8B as backbone models, training on 24,950 MSK-CHORD records. The approach jointly performs binary survival classification, continuous survival time regression, and rationale generation through autoregressive language modeling.

## Key Results
- Chain-of-Thought prompting improves F1-score from 0.77 to 0.83 (+6.0 points)
- GRPO reduces missing predictions from 1,911 to 0 compared to baseline models
- Significant correlation between reasoning quality (BLEU) and prediction accuracy (Macro-F1)

## Why This Works (Mechanism)

### Mechanism 1
Chain-of-Thought prompting improves both predictive accuracy and interpretability by forcing models to generate intermediate reasoning steps (6.1 steps on average) before outputting predictions. This decomposition regularizes the model by requiring explicit clinical justification, constraining the output space beyond direct feature-to-label mappings. The structured reasoning trace creates a path that appears to prevent overfitting to spurious correlations.

### Mechanism 2
GRPO reinforcement learning further aligns reasoning traces with expert-like trajectories through relative advantages computed across sampled outputs. The cold-start SFT phase using 5K diverse CoT exemplars provides stable initialization. The composite reward function explicitly enforces XML-like structure, reducing format failures to zero while maintaining clinical accuracy.

### Mechanism 3
Joint multi-task learning (classification + regression + rationale generation) improves performance across all tasks compared to single-task baselines. The autoregressive formulation couples prediction and explanation, enforcing consistency where reasoning must justify the prediction. This coupling prevents the model from independently optimizing each output, creating task interdependence that benefits overall performance.

## Foundational Learning

- **Autoregressive Language Modeling**
  - Why needed here: The framework tokenizes all outputs (status, months, reasoning) as text sequences, requiring next-token prediction understanding
  - Quick check question: Can you explain why the model outputs "1:DECEASED" and "27.9" as text tokens rather than classification logits and regression values?

- **Reinforcement Learning from Human Feedback (RLHF) fundamentals**
  - Why needed here: GRPO is a variant of policy optimization that replaces the value critic with group-relative rewards
  - Quick check question: Why does GRPO compute advantages as (r_i - mean) / std over a group of sampled outputs rather than using a learned value function?

- **Chain-of-Thought Prompting**
  - Why needed here: The paper builds on CoT as the core reasoning mechanism
  - Quick check question: What happens if you remove the <reasoning> tags from the prompt during inference—would the model still generate step-by-step traces?

## Architecture Onboarding

- **Component map**: MSK-CHORD (JSON records) -> Cancer-type-specific attribute selector -> Natural language prompt constructor -> [Training path] -> DeepSeek R1 (teacher) -> CoT traces -> Cold-start SFT (5K diverse exemplars via K-means on hidden states) -> GRPO fine-tuning (policy optimization with composite reward) -> [Inference] LLaMa3-8B / Med42-8B -> <reasoning> + <answer>

- **Critical path**:
  1. Prompt construction is cancer-type-specific (breast prioritizes HER2/CA15-3; NSCLC prioritizes smoking/PD-L1). Errors here propagate.
  2. CoT trace quality from DeepSeek R1 determines distillation quality. The paper evaluates traces via BERT-based relevance/coherence but does not use human review.
  3. Cold-start exemplar selection (K=5000 clusters) determines GRPO initialization stability.

- **Design tradeoffs**:
  - 8B parameter models vs. larger teachers (DeepSeek R1): Smaller student models for deployment feasibility may lose reasoning capacity
  - Autoregressive output vs. structured heads: Tokenizing floats as text (e.g., "27.9") simplifies training but may reduce numerical precision compared to regression heads
  - Composite reward weighting: The paper does not specify weights for R_correct vs. R_format; this is a tuning risk

- **Failure signatures**:
  - Baseline models (MEDITRON, OPENBIO) fail to produce valid CoT outputs for 1800+ samples under CoT prompting—indicating architectural incompatibility with structured reasoning
  - If GRPO models produce well-formatted but clinically implausible reasoning, check whether format rewards dominate
  - Missing predictions → likely context length issues or tokenization failures on structured inputs

- **First 3 experiments**:
  1. **Ablate CoT during inference**: Compare model performance when CoT traces are removed from prompts at test time vs. retained. This tests whether reasoning is functional or decorative.
  2. **Reward component ablation**: Train GRPO variants with only R_correct, only R_format, and full composite reward to isolate which components drive BLEU/F1 improvements.
  3. **Cold-start size sensitivity**: Vary K (1000, 2500, 5000, 10000 cluster centers) to determine whether cold-start diversity or quantity matters more for GRPO stability.

## Open Questions the Paper Calls Out

### Open Question 1
Can the OncoReason framework generalize to diverse institutional datasets beyond MSK-CHORD, given potential differences in clinical documentation practices, patient populations, and outcome distributions? The paper evaluates exclusively on MSK-CHORD from Memorial Sloan Kettering, but domain generalizability persists as an ongoing challenge for clinical LLMs.

### Open Question 2
How do clinicians assess the clinical validity and utility of the generated CoT rationales compared to standard reference explanations? The paper uses BLEU, ROUGE, and BERTScore for rationale evaluation, not human expert assessment, though interpretability is stated as essential for clinical decision-making.

### Open Question 3
What specific architectural modifications would enable biomedical LLMs to reliably generate structured reasoning traces without requiring extensive fine-tuning? The paper notes baseline models fail to produce valid reasoning traces due to "limited context length and poor support for structured reasoning" but does not investigate specific architectural factors.

## Limitations
- The paper does not establish whether generated reasoning traces represent causally valid clinical reasoning versus post-hoc rationalization
- Limited clinical validation with only automated metrics (BLEU, ROUGE, BERTScore) rather than human expert review
- Potential reward alignment risk where format adherence may dominate clinical accuracy in the composite reward function

## Confidence

**High Confidence**:
- Performance improvements on MSK-CHORD dataset (F1 +6.0, MAE -12% with CoT)
- Zero missing predictions with GRPO vs. 1,911 failures for baselines
- Correlation between reasoning quality (BLEU) and prediction accuracy (Macro-F1)

**Medium Confidence**:
- Claims about clinical interpretability and reasoning quality
- Generalizability of CoT+GRPO approach to other clinical prediction tasks
- Assumption that task coupling (joint multi-task learning) is beneficial rather than harmful

**Low Confidence**:
- That CoT traces represent causally grounded clinical reasoning
- That format rewards don't compromise clinical accuracy
- That the reasoning decomposition is the optimal or only valid approach

## Next Checks

1. **CoT Trace Validation Experiment**: Remove or corrupt CoT traces during inference while keeping model architecture and training intact. Compare performance to full CoT inference. If performance degrades similarly to non-CoT models, this validates that CoT traces are functionally important rather than decorative rationalization.

2. **Reward Component Ablation Study**: Train three GRPO variants: (a) only correctness reward, (b) only format reward, (c) full composite reward. Compare all three on clinical accuracy metrics (F1, MAE) and interpretability metrics (BLEU, ROUGE). This isolates whether format adherence is driving improvements or whether clinical correctness rewards are sufficient.

3. **Cross-Dataset Generalization Test**: Evaluate OncoReason models on an external oncology survival dataset (e.g., SEER or TCGA) without additional fine-tuning. Measure performance drop and reasoning quality degradation. This tests whether the framework generalizes beyond the MSK-CHORD distribution or overfits to its specific patterns.