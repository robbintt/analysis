---
ver: rpa2
title: Evaluating Latent Knowledge of Public Tabular Datasets in Large Language Models
arxiv_id: '2510.20351'
source_url: https://arxiv.org/abs/2510.20351
tags:
- contamination
- datasets
- data
- llms
- tabular
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether Large Language Models (LLMs) exhibit
  contamination effects on widely used tabular datasets like Adult Income and Titanic.
  Through controlled probing experiments involving completion and existence tasks
  on both real and obfuscated data variants, the authors reveal that contamination
  effects occur exclusively for datasets with strong semantic cues (meaningful column
  names and interpretable categories).
---

# Evaluating Latent Knowledge of Public Tabular Datasets in Large Language Models

## Quick Facts
- arXiv ID: 2510.20351
- Source URL: https://arxiv.org/abs/2510.20351
- Reference count: 16
- Primary result: Contamination effects in LLMs occur only for datasets with strong semantic cues; syntactic memorization is not detected.

## Executive Summary
This study investigates whether Large Language Models exhibit contamination effects on widely used tabular datasets like Adult Income and Titanic. Through controlled probing experiments involving completion and existence tasks on both real and obfuscated data variants, the authors reveal that contamination effects occur exclusively for datasets with strong semantic cues (meaningful column names and interpretable categories). In contrast, when such cues are removed, performance drops sharply to near-random levels. No evidence of syntactic contamination (verbatim memorization) was found. The results indicate that LLMs' apparent competence on tabular reasoning tasks stems primarily from semantic familiarity rather than genuine generalization, highlighting the need for contamination-aware evaluation protocols in future LLM assessments.

## Method Summary
The authors employ zero-shot probing tasks to detect dataset contamination in LLMs. Two tasks are used: completion (predicting masked attributes from candidates) and existence (identifying real records among perturbed variants). Three dataset variants are tested: real (original), like (synthetic marginal sampling preserving feature distributions), and obfuscated (semantic labels replaced with abstract tokens). Models are evaluated across llama_8B, mistral_7B, and qwen variants (7B-32B) with 20% attribute masking and 5-way candidate selection. Performance is compared against a 20% random baseline with statistical significance testing.

## Key Results
- Semantic contamination effects occur only for datasets with meaningful column names and interpretable categories
- Performance drops to near-random levels when semantic cues are removed through obfuscation
- No evidence of syntactic contamination (verbatim memorization) was found across all tested datasets and models
- Credit dataset showed elevated obfuscated accuracy (0.46-0.69), suggesting possible statistical regularity exploitation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs exhibit contamination through semantic familiarity, not verbatim memorization, when prior exposure to tabular datasets exists.
- Mechan: Meaningful column names and categorical values (e.g., "occupation," "education") activate conceptual associations learned during pretraining, enabling above-chance completion without explicit record recall.
- Core assumption: The training corpus contained discussions, tutorials, or derived descriptions of widely-used benchmarks like Adult and Titanic.
- Evidence anchors:
  - [abstract] "contamination effects emerge exclusively for datasets containing strong semantic cues—for instance, meaningful column names or interpretable value categories"
  - [section 5] "all evaluated models achieved accuracies substantially above the random-guess baseline (≈ 20%)... plausibly stems from indirect exposure to publicly available resources"
  - [corpus] Related work on contamination detection (Golchin & Surdeanu) probes LLMs for benchmark instances, supporting the broader contamination investigation paradigm, but corpus evidence specific to semantic-vs-syntactic tabular contamination is limited.
- Break condition: If column names are obfuscated to abstract symbols (c01, c02), performance drops to near-random, severing the semantic pathway.

### Mechanism 2
- Claim: The existence task fails systematically because models lack syntactic-level memorization of specific tabular tuples.
- Mechan: The existence task requires discriminating genuine records from perturbed variants—a recognition task that demands exact pattern storage. The near-random performance (often ~20% for 5-way) indicates no stored instance-level representations.
- Core assumption: Verbatim memorization would manifest as above-chance discrimination even without semantic cues.
- Evidence anchors:
  - [section 5] "the existence task, which directly tests for the memorisation of specific records, was consistently failed across all models and datasets"
  - [table 1] AE values cluster near 0.20 baseline across all model-dataset combinations
  - [corpus] Weak direct corpus support; related benchmarks focus on retrieval rather than existence discrimination.
- Break condition: If models were trained on data dumps containing literal CSV rows, existence accuracy should exceed chance—this was not observed.

### Mechanism 3
- Claim: Statistical regularity exploitation can persist even under obfuscation for some datasets, suggesting emergent pattern inference rather than contamination.
- Mechan: For the credit dataset, obfuscated accuracy remains elevated (e.g., qwen_14B at 0.57). This may reflect the model inferring from residual numerical distributions and inter-feature correlations rather than recalled semantics.
- Core assumption: Obfuscation fully removes semantic content; residual performance must come from structural reasoning.
- Evidence anchors:
  - [section 5] "We hypothesise that this effect reflects the models' ability to exploit latent statistical regularities... rather than memorised contamination"
  - [table 1] Credit obf AC: 0.46–0.69 across models
  - [corpus] Latte paper transfers LLM latent knowledge for tabular learning, suggesting LLMs can extract structural patterns, but this is indirect support.
- Break condition: If this reflects undetected semantic leakage rather than reasoning, then further obfuscation (e.g., permuting column order, normalizing values) should degrade performance.

## Foundational Learning

- Concept: **Syntactic vs. Semantic Contamination**
  - Why needed here: The paper's central distinction determines whether inflated benchmarks stem from verbatim data inclusion or learned conceptual associations.
  - Quick check question: Can you explain why n-gram overlap detection fails to identify semantic contamination?

- Concept: **Marginal Distribution Sampling**
  - Why needed here: Understanding how "like" datasets preserve single-feature statistics while destroying inter-feature dependencies is essential for interpreting the control experiments.
  - Quick check question: If you sample each feature independently from its marginal, what statistical property of the original dataset is guaranteed to be lost?

- Concept: **Zero-Shot Probing**
  - Why needed here: The methodology relies on querying models without fine-tuning to isolate pre-existing knowledge; confusing this with few-shot evaluation would invalidate interpretation.
  - Quick check question: Why must no examples be provided when testing for prior dataset knowledge?

## Architecture Onboarding

- Component map: Completion Task Module -> Existence Task Module -> Dataset Variant Pipeline -> Evaluation Harness
- Critical path:
  1. Load dataset → generate real/like/obf variants
  2. For each variant, construct completion and existence prompts
  3. Query each model, collect 5-way accuracy
  4. Compare against 0.20 random baseline with statistical significance testing (α=0.001)

- Design tradeoffs:
  - **Masking ratio (20%)**: Higher ratios increase task difficulty but may introduce noise; lower ratios reduce sensitivity to contamination.
  - **Number of candidates (5)**: More candidates lower random baseline but increase prompt length and cost; fewer candidates raise baseline and reduce discrimination power.
  - **Entropy-based feature selection**: Focuses probing on informative attributes but may miss contamination in low-variance features.

- Failure signatures:
  - Completion accuracy at ~20% on real semantic datasets → no detectable contamination OR model too small/weaker architecture.
  - Existence accuracy above ~30% on any dataset → potential syntactic memorization (unexpected per paper findings).
  - Obfuscated performance matching real performance → semantic cues not the contamination pathway; investigate structural leakage.

- First 3 experiments:
  1. Reproduce the Adult dataset completion task across all three variants (real, like, obf) with a single model to validate the semantic contamination signature (AC should drop from ~0.5–0.7 on real to ~0.2 on obf).
  2. Test existence task on gamma dataset as a non-semantic control to confirm near-random AE (~0.17–0.20) baseline behavior.
  3. Introduce a new obfuscation level—permute column order in addition to label replacement—to test whether credit's residual obfuscated performance persists under stronger disruption.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do contamination patterns, particularly syntactic memorization, emerge in models with significantly larger parameter counts (e.g., >70B–100B parameters)?
- Basis in paper: [explicit] The authors explicitly note in the Limitations section that larger models "may exhibit distinct contamination dynamics due to their increased memorization capacity" and list evaluating larger LLMs as a primary direction for future work.
- Why unresolved: The study restricted its evaluation to models up to 32B parameters (Llama 8B, Mistral 7B, Qwen 32B), leaving the behavior of frontier-scale models unknown.
- What evidence would resolve it: Applying the existence and completion tasks to models exceeding 70B parameters to check for the emergence of syntactic (verbatim) contamination.

### Open Question 2
- Question: Does semantic contamination merely inflate recall of specific associations, or does it actively bias the model's reasoning and generalization capabilities on downstream tasks?
- Basis in paper: [explicit] The authors propose assessing "how this classification [contaminated vs. non-contaminated] impacts LLM performance across various machine learning tasks" to determine if contamination biases reasoning.
- Why unresolved: The current work focuses on detecting contamination via probing tasks rather than measuring the causal impact of that contamination on the validity of reasoning in complex downstream applications.
- What evidence would resolve it: A comparative analysis of model performance on "clean" vs. "contaminated" datasets across diverse reasoning tasks to isolate the bias introduced by prior exposure.

### Open Question 3
- Question: What mechanisms allow models to maintain high accuracy on the "Credit" dataset even when feature names are obfuscated, contrary to the trend seen in other semantic datasets?
- Basis in paper: [inferred] The authors highlight an "intriguing exception" where models perform well on the obfuscated Credit dataset, hypothesizing "emergent reasoning" or the exploitation of "latent statistical regularities," but they do not confirm the cause.
- Why unresolved: The paper identifies the anomaly but lacks an ablation study to distinguish whether the model is utilizing structural patterns (reasoning) or detecting artifacts specific to that dataset's distribution.
- What evidence would resolve it: An analysis of feature importance and attention patterns on the obfuscated Credit dataset to determine if the model relies on valid logical inference or spurious statistical correlations.

### Open Question 4
- Question: How can evaluation protocols effectively disentangle "semantic leakage" (familiarity with concepts) from authentic tabular reasoning without resorting to extreme obfuscation?
- Basis in paper: [explicit] The paper concludes by urging the development of "contamination-aware evaluation protocols" and "strategies to disentangle semantic leakage from authentic reasoning ability."
- Why unresolved: While obfuscation removes contamination, it also removes the semantic context necessary for authentic natural language reasoning; current methods struggle to separate the two in realistic settings.
- What evidence would resolve it: The development and validation of new benchmark designs or statistical correction methods that can account for semantic pre-knowledge while still testing reasoning fidelity.

## Limitations

- The methodology cannot definitively rule out all forms of structural leakage, as statistical patterns may be learned independently of semantic cues
- The boundary cases (like credit dataset) introduce uncertainty about whether all contamination pathways have been identified
- The study does not explore whether models might learn to recognize tabular formatting patterns or numerical distributions as contamination signatures

## Confidence

- **High Confidence**: The syntactic contamination finding (no verbatim memorization) is strongly supported by the systematic failure of existence tasks across all datasets and models
- **Medium Confidence**: The semantic contamination mechanism is well-demonstrated for datasets with strong semantic cues, but boundary cases introduce uncertainty
- **Medium Confidence**: The statistical regularity exploitation hypothesis for credit dataset performance is plausible but not definitively proven

## Next Checks

1. **Extended Obfuscation Protocol**: Test credit dataset performance under additional obfuscation levels including column order permutation and numerical normalization to determine if residual performance persists under more aggressive disruption.

2. **Cross-Dataset Semantic Transfer**: Evaluate whether models trained on semantic datasets show improved performance on semantically-similar but previously unseen datasets, helping distinguish genuine semantic knowledge from contamination.

3. **Structural Pattern Analysis**: Conduct feature importance analysis to determine whether credit dataset performance relies on specific inter-feature correlations or general numerical distribution patterns, providing evidence for or against the statistical regularity hypothesis.