---
ver: rpa2
title: Optimal Implicit Bias in Linear Regression
arxiv_id: '2506.17187'
source_url: https://arxiv.org/abs/2506.17187
tags:
- convex
- have
- optimal
- which
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of finding the optimal implicit
  bias that leads to the best generalization performance in over-parameterized linear
  regression. The authors analyze interpolators obtained by minimizing convex potentials
  subject to interpolation constraints and characterize their precise asymptotic generalization
  error in terms of the over-parameterization ratio, noise variance, eigenspectrum
  of data covariance, and distribution of the true parameter.
---

# Optimal Implicit Bias in Linear Regression

## Quick Facts
- arXiv ID: 2506.17187
- Source URL: https://arxiv.org/abs/2506.17187
- Authors: Kanumuri Nithin Varma; Babak Hassibi
- Reference count: 40
- Primary result: Characterization of optimal convex potentials that minimize asymptotic generalization error in over-parameterized linear regression

## Executive Summary
This paper addresses the problem of finding optimal implicit bias in over-parameterized linear regression by analyzing interpolators obtained through minimizing separable convex potentials. The authors establish that the asymptotic generalization error of any such interpolator can be characterized by solving a system of two nonlinear equations, and further derive a fundamental lower bound on this error using Fisher information. The key theoretical contribution is constructing the optimal convex potential that achieves this lower bound under certain conditions involving log-concavity of convolved prior densities.

## Method Summary
The authors analyze linear regression in the over-parameterized regime (n > m) where multiple interpolators exist. They characterize the generalization performance of interpolators obtained by minimizing separable convex potentials subject to interpolation constraints using the Convex Gaussian Minimax Theorem (CGMT). The analysis reduces to solving a system of two nonlinear equations involving the Moreau envelope of the potential. The fundamental lower bound is derived using Fisher information and Stam's inequality, with the optimal potential constructed from the negative Moreau envelope of a log-density under log-concavity conditions.

## Key Results
- The asymptotic generalization error of any separable convex potential interpolator is characterized by solving a system of two nonlinear equations
- A fundamental lower bound on generalization error exists, expressed in terms of Fisher information of Gaussian-convolved priors
- The optimal convex potential achieving this bound can be constructed when the convolved prior density is log-concave
- For Gaussian priors, the optimal potential reduces to a weighted ridge regression objective

## Why This Works (Mechanism)

### Mechanism 1: Convex Potential Minimization Induces Algorithmic Implicit Bias
- Claim: Minimizing a separable convex potential Ψ(β) subject to interpolation constraints selects a specific global optimum with distinct generalization properties.
- Mechanism: In the over-parameterized regime (n > m), the training loss has infinitely many global optima that interpolate the data. The optimizer (specifically Stochastic Mirror Descent with potential Ψ) converges to the unique solution that minimizes Ψ(β) among all interpolators. This solution's generalization error can be characterized.
- Core assumption: The initialization β₀ satisfies ∇Ψ(β₀) = 0. The convex potential is separable: Ψ(β) = Σᵢ ψ(βᵢ, Σᵢᵢ).
- Evidence anchors:
  - [abstract]: "To find the optimal implicit bias, we provide a precise asymptotic analysis of the generalization performance of interpolators obtained from the minimization of convex functions/potentials for over-parameterized linear regression..."
  - [section 1]: "it was shown that when SMD is initialized at β₀ such that β₀ := arg minβ Ψ(β)..., it converges to the solution ̂β := arg min Ψ(β) s.t yi = xᵢᵀβ..."
  - [corpus]: Evidence is primarily from the paper itself; corpus neighbors discuss related implicit bias topics but not this specific mechanism.
- Break condition: The initialization condition (∇Ψ(β₀) = 0) is not met. The potential is not strictly convex.

### Mechanism 2: Asymptotic Generalization Error Determined by System of Non-Linear Equations
- Claim: The asymptotic generalization error of the interpolator is determined by a system of two non-linear equations involving the Moreau envelope of the potential.
- Mechanism: Using the Convex Gaussian Minimax Theorem (CGMT), the high-dimensional optimization problem is reduced to a scalar minimax optimization. The solution to this scalar problem yields the asymptotic generalization error α²ψ via the solution (αψ, uψ) to the system of equations (6).
- Core assumption: High-dimensional asymptotics (n, m → ∞, m/n → δ). Gaussian features and noise (Assumption 2). The distribution of eigenvalues and true parameter follows Assumption 3.
- Evidence anchors:
  - [abstract]: "The authors analyze interpolators... and characterize their precise asymptotic generalization error..."
  - [Theorem 3.1]: "if we assume that αψ, uψ are the unique solutions to (6), then... r(̂β) → α²ψ."
  - [corpus]: Evidence is primarily from the paper itself. Related work on precise asymptotics is cited but the specific system is novel.
- Break condition: The assumptions of CGMT are violated (e.g., non-Gaussian data without universality). The system of equations has no unique solution.

### Mechanism 3: Fundamental Lower Bound via Fisher Information
- Claim: There exists a fundamental lower bound on the generalization error for any interpolator obtained via convex potential minimization. This bound is tight and can be achieved by an optimal potential.
- Mechanism: The lower bound α²* is derived using properties of Fisher information and Stam's inequality. It depends on the weighted Fisher information of a Gaussian-convolved prior. The optimal potential Ψ* is constructed from the negative Moreau envelope of the log-density of this convolved prior. If this density is log-concave, Ψ* is convex and achieves the bound.
- Core assumption: The distribution of the true parameter and the data covariance spectrum are known. The convolved prior density PVα*(v|λ) is log-concave.
- Evidence anchors:
  - [abstract]: "Finally, we find the optimal convex implicit bias that achieves this lower bound under certain sufficient conditions involving the log-concavity..."
  - [Theorem 3.4 & 3.6]: Derivation of the lower bound and construction of the optimal potential.
  - [corpus]: Evidence is primarily from the paper itself.
- Break condition: The log-concavity condition is not met, so the constructed optimal potential is not convex and the lower bound may not be achievable within the class of convex potentials.

## Foundational Learning

- Concept: **Over-parameterization and Interpolation**
  - Why needed here: This is the core regime the paper analyzes. It explains why there are multiple global optima and why the implicit bias of an optimizer matters.
  - Quick check question: If you have 1000 parameters and only 100 data points in a linear model, why does the solution to min ||Xβ - y||² have infinitely many solutions?

- Concept: **Convex Conjugate and Moreau Envelope**
  - Why needed here: The primary theoretical tools used to characterize the generalization error and construct the optimal potential. The key equations involve the Moreau envelope and its derivatives.
  - Quick check question: For a function f(x), what is its convex conjugate f*(y)? What does the Moreau envelope represent, intuitively?

- Concept: **Fisher Information of a Location Family**
  - Why needed here: The fundamental lower bound on generalization error is expressed in terms of the Fisher information of a Gaussian-convolved prior distribution. Understanding its properties (e.g., Cramér-Rao, Stam's inequality) is essential for the result.
  - Quick check question: State the Cramér-Rao bound. How does Fisher information behave under the convolution of two independent random variables?

## Architecture Onboarding

- Component map:
  1. **Data Model**: Linear regression with non-isotropic Gaussian features (X), additive Gaussian noise (z), and a true parameter vector (β*) with a known prior.
  2. **Optimization Problem**: Minimize a separable convex potential Ψ(β) subject to the interpolation constraint Xβ = y. Solved by Stochastic Mirror Descent (SMD).
  3. **Analysis Framework (CGMT)**: Reduces the high-dimensional problem to a low-dimensional scalar minimax problem, yielding a system of two non-linear equations for the error.
  4. **Optimal Bias Construction**: A two-step process: first, find the lower bound α* via a scalar equation involving Fisher information; second, construct the optimal potential Ψ* using the negative Moreau envelope of a log-density, checking for convexity.

- Critical path:
  1. Specify the problem (δ, σ², P_Λ,B).
  2. Choose a convex potential Ψ.
  3. Solve the system of non-linear equations (6) to find αψ, the asymptotic generalization error.
  4. To find the best potential, solve the single non-linear equation (10) to find the lower bound α*.
  5. Construct the optimal potential Ψ* using equation (13) and verify its convexity via log-concavity of PVα*.

- Design tradeoffs:
  - **Generality vs. Tractability**: The analysis holds for general separable convex potentials, but computing the optimal one requires knowledge of the prior and eigen-spectrum, and the log-concavity condition must be checked.
  - **Theoretical vs. Practical**: The optimal potential is derived for the asymptotic limit. In practice, finite-sample effects and non-Gaussian data may affect performance.
  - **Computation**: Solving the system of non-linear equations for a given potential is non-trivial but tractable. Verifying log-concavity can be complex.

- Failure signatures:
  - **No Convex Optimal Potential**: The constructed Ψ* is not convex, meaning the theoretical lower bound is not achievable by a convex potential minimizer.
  - **Non-Unique Solutions**: The system of equations (6) has multiple solutions, making the asymptotic error ambiguous. This can happen if the potential is not strictly convex.
  - **Asymptotic Mismatch**: For small n and m, the finite-sample generalization error deviates from the asymptotic prediction α²ψ.

- First 3 experiments:
  1. **Validate Asymptotics for a Fixed Potential**: Choose a standard potential (e.g., ℓ2-norm) and a signal prior (e.g., Gaussian). Simulate linear regression for increasing problem sizes (n, m) with a fixed ratio δ. Compare the empirical generalization error to the theoretical limit α²ψ obtained by solving (6). This validates the core analysis framework.
  2. **Compare Potentials and Find the Lower Bound**: For a given signal prior and data spectrum (e.g., sparse-Gaussian B, bi-level Λ), simulate and compare the generalization error for several potentials (ℓ1, ℓ2, ℓ3, ℓ∞). Plot these results against the theoretical lower bound α²* to see which potential comes closest to optimal.
  3. **Test the Optimal Potential Construction**: For a signal prior where log-concavity holds (e.g., Gaussian B), compute the optimal potential Ψ* analytically (as in Corollary 3.8). Implement SMD with this potential and verify that its generalization error approaches the theoretical lower bound α²*. This validates the optimality construction.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the analysis of optimal implicit bias be extended to finite-sample (non-asymptotic) settings?
- Basis in paper: [explicit] Section 6 states that extending results to "non-asymptotic settings" is an "important future direction."
- Why unresolved: The current methodology relies on the Convex Gaussian Minimax Theorem (CGMT) within a proportional asymptotic limit ($m, n \to \infty$).
- What evidence would resolve it: Derivation of generalization error bounds and optimal potentials that hold for fixed dimensions $m$ and $n$.

### Open Question 2
- Question: How can the optimal implicit bias characterization be generalized to non-linear models?
- Basis in paper: [explicit] Section 6 notes it would be "interesting to generalize these results to non-linear models."
- Why unresolved: The current theoretical framework fundamentally relies on the linearity of the regression model.
- What evidence would resolve it: A theoretical derivation of optimal potentials for non-linear architectures (e.g., deep networks) outside of linear regimes like the Neural Tangent Kernel (NTK).

### Open Question 3
- Question: What is the role of optimal implicit bias in classification problems?
- Basis in paper: [explicit] Section 6 suggests it would be interesting to "study the role of implicit bias in classification problems."
- Why unresolved: The paper focuses exclusively on the regression setting with square loss, and the optimality conditions may differ for classification losses.
- What evidence would resolve it: Characterization of the asymptotic generalization error and optimal convex potentials for interpolating classifiers.

### Open Question 4
- Question: Do the optimal implicit bias results rigorously hold for non-Gaussian data distributions?
- Basis in paper: [inferred] Section 5 notes that while universality is conceivable, it "remains to be shown" for the Gaussian assumptions used in the analysis.
- Why unresolved: The proofs rely on Gaussian comparison lemmas, and the extension to sub-Gaussian or general distributions is not formally proven.
- What evidence would resolve it: A rigorous proof demonstrating that the derived generalization errors and optimal potentials are invariant under sub-Gaussian data distributions.

## Limitations
- Analysis is limited to separable convex potentials and the high-dimensional asymptotic regime
- Achievability of the fundamental lower bound requires log-concavity of convolved prior densities, which may not hold
- Numerical simulations are synthetic and focus on Gaussian features and noise; performance on real data remains untested
- The optimal potential construction requires knowledge of the true parameter distribution and data covariance spectrum

## Confidence
- Asymptotic characterization of generalization error via system (6): **High**
- Existence of a fundamental lower bound via Fisher information: **High**
- Achievability of the lower bound by a convex potential: **Medium** (conditional on log-concavity)
- Numerical simulations validating theory: **Medium** (synthetic data only)

## Next Checks
1. **Verify log-concavity for sparse-Gaussian signals**: For B ~ pδ₀ + (1-p)N(0,σ_b²), analytically or numerically check whether PVα(v|λ) is log-concave over its support for relevant α and λ values. If not, characterize the achievable gap between the best convex potential and the theoretical lower bound.

2. **Finite-sample correction study**: For moderate n and m (e.g., n=1000, m=500), compare empirical generalization error to the asymptotic prediction α². Quantify the finite-sample bias and test if a simple correction term improves prediction accuracy.

3. **Robustness to non-Gaussian features**: Repeat the core experiments with features drawn from t-distribution or uniform distribution. Assess whether the asymptotic generalization error from the Gaussian assumption still accurately predicts empirical performance.