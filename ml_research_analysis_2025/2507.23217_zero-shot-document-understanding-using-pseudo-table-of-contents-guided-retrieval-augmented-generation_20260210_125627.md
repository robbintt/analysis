---
ver: rpa2
title: Zero-Shot Document Understanding using Pseudo Table of Contents-Guided Retrieval-Augmented
  Generation
arxiv_id: '2507.23217'
source_url: https://arxiv.org/abs/2507.23217
tags:
- document
- retrieval
- page
- content
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DocsRay addresses the challenge of understanding complex multimodal
  documents without requiring training data. The system integrates pseudo Table of
  Contents generation with hierarchical retrieval-augmented generation, leveraging
  multimodal LLMs' native capabilities to process text, images, tables, and charts
  in a unified, training-free manner.
---

# Zero-Shot Document Understanding using Pseudo Table of Contents-Guided Retrieval-Augmented Generation

## Quick Facts
- arXiv ID: 2507.23217
- Source URL: https://arxiv.org/abs/2507.23217
- Authors: Hyeon Seong Jeong; Sangwoo Jo; Byeong Hyun Yoon; Yoonseok Heo; Haedong Jeong; Taehoon Kim
- Reference count: 40
- Primary result: 64.7% accuracy on MMLongBench-Doc benchmark

## Executive Summary
DocsRay addresses the challenge of understanding complex multimodal documents without requiring training data. The system integrates pseudo Table of Contents generation with hierarchical retrieval-augmented generation, leveraging multimodal LLMs' native capabilities to process text, images, tables, and charts in a unified, training-free manner. The approach combines semantic structuring through prompt-based LLM interactions, zero-shot multimodal analysis, and an efficient two-stage retrieval system that reduces computational complexity from O(N) to O(S + k₁·Nₛ). Evaluated on documents averaging 49.4 pages and 20,971 tokens, DocsRay achieves 64.7% accuracy on the MMLongBench-Doc benchmark—a 15 percentage point improvement over previous state-of-the-art—while reducing query latency by 45%. This performance approaches human expert levels while maintaining efficiency and deployment flexibility.

## Method Summary
DocsRay implements a three-component training-free pipeline: (1) pseudo-TOC generation via LLM prompts for boundary detection and title generation, (2) dual embedding with BGE-M3 + Multilingual-E5-Large (concatenated, L2 normalized), and (3) hierarchical coarse-to-fine retrieval. The system processes multimodal documents through multi-column detection, table/image extraction, semantic chunking at 550 tokens with 25 token overlap, and indexing with dual embeddings. Query processing uses coarse search over sections (weighted interpolation of title and content embeddings) followed by fine search within top sections. The architecture uses Gemma-3 models (4B/12B/27B) with temperature=0.7, top-p=0.95, repeat-penalty=1.1.

## Key Results
- Achieves 64.7% accuracy on MMLongBench-Doc benchmark, a 15 percentage point improvement over previous state-of-the-art
- Reduces query latency by 45% (from 3.89s to 2.12s) through hierarchical retrieval
- Approaches human expert performance while maintaining training-free operation
- Handles documents averaging 49.4 pages and 20,971 tokens without performance degradation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Prompt-based pseudo-TOC generation enables semantic document structuring without training data.
- **Mechanism:** Uses two LLM prompts—one for boundary detection between consecutive page excerpts (binary 0/1 response), another for title generation from sampled section content. Detected boundaries create initial segments, which are then merged based on cosine similarity and size constraints.
- **Core assumption:** LLM semantic understanding reliably identifies topic transitions better than formatting cues or fixed-window chunking.
- **Evidence anchors:** "semantic structuring module using prompt-based LLM interactions to generate a hierarchical pseudo-TOC"; "this method relies on semantic coherence rather than layout features, it adapts flexibly across diverse document styles"; BookRAG paper confirms hierarchical structure-aware indexing improves RAG on complex documents (FMR=0.56).
- **Break condition:** Documents where topical boundaries are ambiguous, or where section titles are generic ("Introduction," "Results"), reducing discriminative power during coarse retrieval.

### Mechanism 2
- **Claim:** Dual embedding concatenation (BGE-M3 + Multilingual-E5-Large) improves retrieval accuracy by capturing complementary lexical and semantic signals.
- **Mechanism:** Each chunk is encoded by both models; embeddings are concatenated and L2-normalized. BGE-M3 excels at keyword matching, while E5-Large provides stronger semantic understanding. Concatenation preserves full dimensionality from both (2048-dim vs 1024-dim single models).
- **Core assumption:** Moderately correlated embeddings (not orthogonal, not identical) produce more coherent fused representations.
- **Evidence anchors:** "Figure 3 shows that BGE-M3 shares moderate similarity with Multilingual-E5-Large, satisfying this criterion"; Dual concatenation achieves 62.8% vs 54.0% (BGE-M3 only) and 54.7% (E5-Large only)—8+ point improvement; Mixture-of-RAG paper integrates text and tables with LLMs (FMR=0.50).
- **Break condition:** Memory-constrained deployments where 2x embedding dimension is prohibitive; single embeddings remain viable with ~8 point accuracy tradeoff.

### Mechanism 3
- **Claim:** Hierarchical coarse-to-fine retrieval reduces computational complexity from O(N) to O(S + k₁·Nₛ) while maintaining accuracy.
- **Mechanism:** Stage 1 computes query-section similarity using weighted interpolation of title and content embeddings (β·title + (1-β)·content). Stage 2 searches only within top-k₁ sections. Since S (sections) ≪ N (chunks), search space shrinks dramatically.
- **Core assumption:** Relevant information clusters within semantically coherent sections; coarse search rarely prunes all relevant content.
- **Evidence anchors:** "reduces computational complexity from O(N) to O(S + k₁·Nₛ)"; Query latency reduced from 3.89s to 2.12s (45% improvement) with only 0.7% accuracy drop; Docopilot and related RAG approaches struggle with multi-page retrieval; hierarchical indexing appears in multiple neighbors as a solution pattern.
- **Break condition:** When coarse search misses relevant sections entirely—errors are unrecoverable in fine search. This occurs when section titles are uninformative or boundaries are misidentified.

## Foundational Learning

- **Concept: RAG (Retrieval-Augmented Generation)**
  - Why needed here: DocsRay is fundamentally a RAG system; understanding why naive chunking destroys semantic coherence is essential.
  - Quick check question: Can you explain why fixed-size chunking might split related information across chunk boundaries?

- **Concept: Embedding spaces and cosine similarity**
  - Why needed here: The dual embedding architecture and all retrieval decisions depend on vector similarity calculations.
  - Quick check question: Given two embedding vectors, how would you compute their cosine similarity? Why normalize before concatenation?

- **Concept: Zero-shot vs. fine-tuned inference**
  - Why needed here: DocsRay's core value proposition is training-free operation; understanding prompt-based LLM capabilities is critical.
  - Quick check question: What does "training-free" mean in this context, and what LLM capability does pseudo-TOC generation depend on?

## Architecture Onboarding

- **Component map:** Input documents → Multi-column detection → Table/image extraction → Pseudo-TOC generation (boundary detection → merging → title generation) → Chunking → Dual embedding → Index storage; User query → Query embedding → Coarse search (sections) → Fine search (chunks) → Optional query refinement → LLM answer generation

- **Critical path:** Pseudo-TOC quality → Section representation accuracy → Coarse retrieval effectiveness → Final answer quality. Errors propagate: bad boundaries → irrelevant sections retrieved → correct answer impossible.

- **Design tradeoffs:**
  - Accuracy vs. speed: Pseudo-TOC adds preprocessing cost but reduces query latency 45%
  - Memory vs. accuracy: Dual embeddings (2048-dim) vs single (1024-dim) trades ~8% accuracy for 2x memory
  - β parameter: Higher β emphasizes titles (good for descriptive headings), lower β for generic titles

- **Failure signatures:**
  - Generic section titles ("Introduction") → poor coarse retrieval
  - Multi-image comparison tasks → text-centric approach fails (SlideVQA: 17.1% EM)
  - Missing section in coarse search → unrecoverable error in fine search
  - Small models on multi-page synthesis → Lite model overcounts, loses evidence tracking

- **First 3 experiments:**
  1. Run DocsRay-Lite on a 50-page document with and without pseudo-TOC (flat chunking) to measure accuracy/latency tradeoff
  2. Compare single BGE-M3, single E5-Large, dual-addition, dual-concatenation on 10 queries to reproduce Table 6 results
  3. Vary β (0.1, 0.3, 0.5, 0.7) and k₁ (3, 5, 7) on documents with varying title quality to identify failure boundaries

## Open Questions the Paper Calls Out

- **Question:** Does the DocsRay architecture generalize effectively to non-Latin scripts and morphologically rich languages given the reliance on English-centric prompt engineering?
  - Basis in paper: The conclusion lists "developing a multilingual document QA benchmark" as future work, and Section G.1 notes that evaluation is currently limited to English documents.
  - Why unresolved: While the embeddings (BGE-M3, E5) support multiple languages, the prompt-based pseudo-TOC generation and boundary detection have only been validated on English, leaving their behavior on diverse discourse structures unknown.
  - What evidence would resolve it: Evaluation results on a benchmark spanning at least 20 languages (including non-Latin scripts), specifically measuring the accuracy of boundary detection and retrieval quality compared to English baselines.

- **Question:** Can advanced embedding fusion strategies (e.g., learned gates, cross-attention) outperform simple concatenation in balancing retrieval accuracy against computational overhead?
  - Basis in paper: Section 5 suggests "exploring advanced embedding fusion strategies," and Section G.1 admits the study was limited to concatenation versus addition due to efficiency prioritization.
  - Why unresolved: Preliminary trials with learned layers showed latency increases, leaving the trade-off between the accuracy gains of complex fusion and the speed of simple concatenation unoptimized.
  - What evidence would resolve it: A Pareto frontier analysis comparing fusion mechanisms (Hadamard product, attention-based) on metrics of accuracy, latency, and memory usage on the MMLongBench-Doc dataset.

- **Question:** How can fine-grained, statement-level evidence grounding be integrated into the generation pipeline without disrupting the O(S + k₁·Nₛ) efficiency?
  - Basis in paper: Section 5 identifies "integrating explicit evidence grounding and citation mechanisms" as a key direction, and Section G.1 acknowledges the current system lacks granular attribution.
  - Why unresolved: The system currently provides section-level attribution but cannot map specific claims to exact text spans or bounding boxes, limiting trust in high-stakes domains like finance or medicine.
  - What evidence would resolve it: A modified pipeline that outputs inline citations linked to specific text spans, validated by a user study measuring verification time and error detection rates.

- **Question:** To what extent does the text-centric conversion of visual elements degrade performance on tasks requiring precise quantitative comparisons between multiple images?
  - Basis in paper: Section G.1 lists "Multi-Image Quantitative Comparison" as out of scope, noting that textual descriptions omit visual patterns vital for intuitive comparisons.
  - Why unresolved: By converting images to text (alt-text) for the retrieval index, the system inherently loses the pixel-level spatial relationships required to compare data across multiple charts or diagrams.
  - What evidence would resolve it: A comparative evaluation on a multi-image reasoning benchmark (like SlideVQA), contrasting DocsRay's text-centric performance against vision-native models on quantitative visual queries.

## Limitations

- Pseudo-TOC generation may fail when documents lack clear topical boundaries or use generic section titles, leading to poor semantic structuring
- The dual embedding approach increases memory requirements by 2x, potentially limiting deployment in resource-constrained environments
- Text-centric conversion of visual elements fundamentally limits performance on tasks requiring precise quantitative comparisons between multiple images (17.1% EM on SlideVQA)

## Confidence

- **High Confidence:** The hierarchical retrieval architecture and dual embedding concatenation show consistent quantitative improvements (45% latency reduction, 8+ point accuracy gains) with clear mechanisms explained
- **Medium Confidence:** Pseudo-TOC generation effectiveness across diverse document styles, particularly those with ambiguous boundaries or generic titles, requires more extensive validation
- **Low Confidence:** Generalization to document types with highly specialized layouts or where semantic coherence differs from conventional academic structures has not been thoroughly tested

## Next Checks

1. Evaluate DocsRay-Lite on a diverse corpus of 50 documents spanning academic papers, technical manuals, legal documents, and annual reports to measure accuracy degradation when section titles are generic versus descriptive

2. Systematically compare single vs. dual embedding performance across memory-constrained deployments (e.g., 4GB vs 8GB GPU memory) to quantify the exact accuracy-latency-memory frontier for different use cases

3. Implement detailed logging of coarse retrieval decisions on MMLongBench-Doc to measure the frequency and impact of relevant section pruning, identifying document characteristics that trigger this unrecoverable error mode