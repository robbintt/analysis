---
ver: rpa2
title: Evaluation of LLM-based Strategies for the Extraction of Food Product Information
  from Online Shops
arxiv_id: '2506.21585'
source_url: https://arxiv.org/abs/2506.21585
tags:
- extraction
- information
- product
- pages
- direct
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper evaluates two LLM-based approaches for extracting structured
  food product information from online shops: direct extraction (LLM parses compressed
  HTML directly) and indirect extraction (LLM generates reusable extraction functions).
  Using a dataset of 3,000 product pages from three German online retailers, both
  methods successfully extracted nutrition tables and ingredient lists, with the indirect
  approach achieving 96.48% accuracy (-1.61% vs direct).'
---

# Evaluation of LLM-based Strategies for the Extraction of Food Product Information from Online Shops

## Quick Facts
- arXiv ID: 2506.21585
- Source URL: https://arxiv.org/abs/2506.21585
- Reference count: 1
- Primary result: Indirect extraction achieves 96.48% accuracy with 95.82% fewer LLM calls versus direct extraction

## Executive Summary
This paper evaluates two LLM-based approaches for extracting structured food product information from online shop HTML pages: direct extraction (LLM parses compressed HTML directly) and indirect extraction (LLM generates reusable extraction functions). Using a dataset of 3,000 product pages from three German online retailers, both methods successfully extracted nutrition tables and ingredient lists. The indirect approach achieved 96.48% accuracy while reducing required LLM calls by 95.82%, demonstrating significant cost-effectiveness. The findings show that template-based function reuse provides a scalable solution for large-scale web scraping tasks, though accuracy and variability trade-offs exist.

## Method Summary
The study compares direct and indirect extraction approaches using HTML preprocessing (removing non-essential elements while preserving class/id attributes), schema-constrained structured output via Pydantic models, and a refinement loop for indirect extraction. Direct extraction uses o3-mini with structured prompting to parse compressed HTML into predefined schemas. Indirect extraction generates reusable CSS/XPath-based extraction functions using gpt-4o and o3-mini, with iterative refinement based on similarity evaluation. The system employs a decision function to filter relevant pages and applies registered functions across matching pages, regenerating when existing functions fail.

## Key Results
- Indirect extraction achieved 96.48% accuracy (-1.61% vs direct extraction)
- Reduced required LLM calls by 95.82% compared to direct extraction
- Generated an average of 2.26 functions per shop, refined 3.96 times on average
- HTML_COMPRESSED format retained 97-98% accuracy while significantly reducing tokens

## Why This Works (Mechanism)

### Mechanism 1: Schema-Constrained Structured Output
- Claim: Enforcing Pydantic-based schema constraints via structured prompting improves extraction consistency and enables reliable downstream integration.
- Mechanism: The system uses OpenAI's `response_format` parameter to parse LLM responses directly into typed Pydantic models. Field descriptions from the GS1 Web Vocabulary ontology are embedded in the schema, providing field-level guidance that reduces ambiguity.
- Core assumption: LLMs reliably adhere to schema constraints when schema definitions include both structural types and semantic descriptions.
- Evidence anchors:
  - [abstract]: "explore schema-constrained extraction approaches to retrieve key product attributes"
  - [section 3]: "To parse the extracted information into a predefined Pydantic data model, we make use of the response format parameter provided in the official OpenAI Python package"
  - [corpus]: Limited direct corpus evidence; related work (Dang et al.) notes naive prompting produces non-compliant outputs
- Break condition: When field descriptions are ambiguous or when target attributes lack clear semantic boundaries (e.g., GPC taxonomy attributes embedded in unstructured text).

### Mechanism 2: Template-Based Function Reuse
- Claim: Generating reusable extraction functions for template-based pages reduces LLM calls by ~95% while maintaining comparable accuracy.
- Mechanism: E-commerce product pages share consistent HTML templates within shops. The indirect approach generates CSS/XPath-based extraction functions once per template type, then applies them across all matching pages without additional LLM invocation.
- Core assumption: Product pages follow consistent templates with semantic information accessible via structural selectors (class/id attributes).
- Evidence anchors:
  - [abstract]: "indirect extraction reduced required LLM calls by 95.82%"
  - [section 5]: "its efficiency depends on the number of unique templates rather than the number of individual pages"
  - [corpus]: arxiv:2502.14625 supports template-based multi-record extraction from web pages
- Break condition: When pages have significant structural variations, or when attributes are embedded in unstructured descriptions rather than marked-up HTML elements.

### Mechanism 3: Iterative Refinement with Error Feedback
- Claim: Extraction functions can be improved through guided refinement using similarity-based error signals.
- Mechanism: Generated functions are evaluated against reference objects (from direct extraction). If similarity < 1.0, error types (AdditionalAttributeError, MissingAttributeError, ValueError) are fed back to the LLM for up to 5 refinement iterations. If unsuccessful, up to 3 alternative functions are generated.
- Core assumption: Error categorization provides actionable guidance for LLM code modification.
- Evidence anchors:
  - [section 3.2]: "Guided by error feedback from the similarity evaluation, the function is iteratively refined using o3-mini by adding the error feedback to the prompt, up to five times"
  - [section 4.4]: "an average of 2.26 functions were generated and refined 3.96 times across all shops"
  - [corpus]: arxiv:2501.01237 explores self-refinement strategies for LLM-based attribute extraction
- Break condition: When reference objects contain errors, or when structural fixes require domain knowledge not captured in error messages.

## Foundational Learning

- Concept: **Pydantic Data Models with Optional Fields**
  - Why needed here: The extraction architecture centers on defining structured schemas; the FoodBeverageTobaccoProduct model uses nested QuantitativeValue types with optional fields.
  - Quick check question: Can you define a Pydantic model with a nested optional field that includes a custom description?

- Concept: **HTML Structure and CSS Selectors**
  - Why needed here: Generated extraction functions rely on CSS selectors targeting class/id attributes; preprocessing preserves only these attributes.
  - Quick check question: Given compressed HTML with only class/id attributes retained, can you identify a stable selector for a nutrition table?

- Concept: **LLM Structured Output APIs**
  - Why needed here: Direct extraction uses `response_format` for schema enforcement; indirect extraction embeds JSON schema in prompts.
  - Quick check question: Do you understand how to pass a Pydantic model to OpenAI's API for structured output parsing?

## Architecture Onboarding

- Component map:
  - HTML Preprocessing -> Decision Function -> Direct Extraction Path OR Indirect Extraction Path -> Evaluation Layer
  - HTML_COMPRESSED/TEXT creation -> gpt-4o Boolean classification -> o3-mini parsing/refinement -> Custom similarity evaluation

- Critical path:
  1. Preprocess HTML (compression reduces tokens ~50%+)
  2. Decision function filters pages with relevant content
  3. For indirect: initial direct extraction on first positive page → generate function → evaluate similarity → refine (max 5 iterations) → register function
  4. Apply registered functions to remaining pages; regenerate if all functions fail on positive pages

- Design tradeoffs:
  - **Accuracy vs. Cost**: Direct extraction achieves +1.61% accuracy but requires 1,000 calls per shop; indirect achieves 96.48% with ~44 calls
  - **HTML_COMPRESSED vs. TEXT**: Compressed HTML (97-98% accuracy) retains structural selectors; TEXT (95-96% accuracy) is smaller but selector-incompatible
  - **Function diversity vs. Maintenance**: More functions cover edge cases but add complexity; paper averages 2.26 functions per shop

- Failure signatures:
  - **Hallucinated attributes**: o3-mini infers ingredients from product names when data is absent (e.g., "Braeburn apple" → fabricated ingredient list)
  - **Run-to-run variability**: Non-deterministic function generation causes accuracy fluctuations; paper ran 10 iterations per shop to characterize variance
  - **Unstructured attribute embedding**: GPC taxonomy attributes in prose descriptions (not HTML elements) defeat selector-based functions

- First 3 experiments:
  1. Baseline direct extraction on 20-30 pages using both HTML_COMPRESSED and TEXT; measure accuracy and per-page token costs
  2. Single-shop indirect extraction with logging: track function generation count, refinement iterations, error type distribution
  3. Decision function validation: compare gpt-4o Boolean predictions against ground-truth labels; measure false positive/negative rates

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the indirect extraction approach effectively retrieve attributes embedded in unstructured text, such as GPC taxonomy details, or does this require direct extraction?
- Basis in paper: [explicit] The discussion notes that attributes like GPC bricks are often nested in descriptions rather than explicit HTML, suggesting direct extraction may outperform indirect methods in these cases.
- Why unresolved: The current indirect approach generates functions based on HTML structure (CSS/XPath), which may fail when data is purely textual.
- What evidence would resolve it: A comparative evaluation of both methods on product pages where target attributes are hidden within free-text descriptions.

### Open Question 2
- Question: How do these extraction strategies perform using LLMs outside the OpenAI ecosystem, particularly open-source models?
- Basis in paper: [explicit] The authors state, "In future work, we plan to evaluate our methodology with alternative LLMs beyond the OpenAI ecosystem."
- Why unresolved: The study relied exclusively on OpenAI's o3-mini and gpt-4o models.
- What evidence would resolve it: Benchmarking accuracy and cost results using open-source models (e.g., LLaMA, DeepSeek) on the same dataset.

### Open Question 3
- Question: Can the variability in accuracy observed in the indirect extraction approach be reduced through improved decision procedures?
- Basis in paper: [explicit] The paper observes "variability in accuracy... indicating room for further improvement" and suggests refining attribute-level decision procedures.
- Why unresolved: The non-deterministic nature of LLM function generation and random page selection currently leads to inconsistent results across runs.
- What evidence would resolve it: Demonstrating consistent accuracy scores (lower variance) across multiple runs using a refined generation algorithm.

## Limitations
- The template-based function reuse mechanism requires consistent HTML structure, which may not generalize to shops with more heterogeneous layouts or dynamic content generation.
- The refinement loop's effectiveness depends on error feedback being actionable for the LLM, but complex extraction failures may exceed the model's capabilities.
- The custom similarity function's recursive attribute comparison may not capture semantic equivalence in cases where formatting or unit variations don't affect the underlying information.

## Confidence

- **High Confidence**: The 95.82% LLM call reduction is well-supported by the token-based cost calculations and the observed function reuse patterns across the three German shops.
- **Medium Confidence**: The 96.48% accuracy claim for indirect extraction is reasonable given the controlled dataset, but the ±1.61% gap versus direct extraction and the observed run-to-run variability (2.26 functions generated on average) suggest some instability in real-world applications.
- **Medium Confidence**: The schema-constrained approach using Pydantic models and structured prompting appears effective for the tested attributes, but the assumption that LLMs reliably adhere to complex nested schemas with semantic descriptions needs broader validation.

## Next Checks

1. **Template Variation Test**: Apply the indirect extraction approach to a fourth online shop with significantly different HTML structure (e.g., using more dynamic JavaScript rendering or having fewer semantic class attributes) and measure both accuracy and function generation overhead to assess generalizability.

2. **Error Feedback Robustness Test**: Create a controlled dataset where extraction functions fail due to progressively more complex error types (missing nested attributes, unit conversion requirements, multi-step reasoning) and evaluate whether the refinement loop can successfully handle these cases or reaches diminishing returns.

3. **Scalability Benchmark**: Simulate a large-scale deployment by generating a dataset with 100+ unique templates (representing a shop with high structural diversity) and measure how the function generation/refinement overhead scales compared to the cost savings from reuse, identifying the break-even point where direct extraction becomes more efficient.