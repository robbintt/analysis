---
ver: rpa2
title: Do Internal Layers of LLMs Reveal Patterns for Jailbreak Detection?
arxiv_id: '2510.06594'
source_url: https://arxiv.org/abs/2510.06594
tags:
- jailbreak
- prompts
- layer
- representations
- layers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores whether internal layers of LLMs reveal patterns
  useful for jailbreak detection. The authors propose a tensor-based approach to analyze
  hidden representations of LLMs when processing jailbreak versus benign prompts.
---

# Do Internal Layers of LLMs Reveal Patterns for Jailbreak Detection?

## Quick Facts
- arXiv ID: 2510.06594
- Source URL: https://arxiv.org/abs/2510.06594
- Authors: Sri Durga Sai Sowmya Kadali; Evangelos E. Papalexakis
- Reference count: 6
- Primary result: Tensor decomposition of internal LLM representations achieves F1 scores of 67-95% for jailbreak detection

## Executive Summary
This paper investigates whether internal representations of large language models (LLMs) contain detectable patterns that distinguish jailbreak prompts from benign prompts. The authors propose a tensor-based approach that extracts layer outputs during prompt processing, stacks them into 3D tensors, and applies CP tensor decomposition to obtain latent embeddings. These embeddings are then classified using standard models like Random Forest and SVM. Results show that jailbreak and benign prompts exhibit clear separation in the latent space, with F1 scores ranging from 67% to 95% depending on the model, layer, and component extracted. The method demonstrates that simple tensor decomposition followed by basic classifiers can effectively detect jailbreaks, suggesting that adversarial prompts leave consistent signatures in LLM internal representations.

## Method Summary
The method extracts internal representations from GPT-J (6B) and Mamba-2 (2.8B) models during prompt processing. For each prompt, the authors extract either MHA/Mixer outputs or full layer outputs at early (layers 4-5), middle (14-16), and final (26-27) layers. These 2D representations (sequence length × embedding dimension) are stacked across prompts to form 3D tensors. CP tensor decomposition is applied to obtain three factor matrices, with factor C serving as latent embeddings for classification. Standard classifiers (Random Forest, SVM-RBF, SVM-Linear, Logistic Regression) are trained using 5-fold cross-validation. The approach is motivated by the hypothesis that jailbreak prompts produce distinct representational patterns in hidden layers that can be captured through tensor decomposition.

## Key Results
- GPT-J MHA layer 26 achieved 94.5% F1 with Random Forest, while full layer output achieved 90.4%
- Mamba-2 Mixer layer 32 achieved 94.2% F1 with SVM-RBF, outperforming full Block output
- Attention and mixer outputs consistently outperformed aggregated layer outputs for both models
- Middle-to-final layers (14-27) showed better separation than early layers (4-5)
- t-SNE visualizations confirmed clear clustering between jailbreak and benign prompts in latent space

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Jailbreak and benign prompts produce distinguishable patterns in LLM internal representations that can be extracted via tensor decomposition.
- **Mechanism:** Adversarial prompts designed to bypass safety mechanisms exhibit distinct structural or semantic signatures in hidden layers. When representations from multiple prompts are stacked into tensors and decomposed via CP decomposition, the resulting latent factors capture discriminative patterns that separate the two classes in embedding space.
- **Core assumption:** Jailbreak prompts induce consistent representational shifts across the prompt distribution that differ systematically from benign prompts.
- **Evidence anchors:**
  - [abstract]: "presenting preliminary findings that highlight distinct layer-wise behaviors"
  - [Section 2.3]: "Our approach is motivated by the hypothesis that jailbreak and benign prompts produce distinct representational patterns in the hidden layers"
  - [corpus]: Limited direct evidence; corpus focuses on attack methods rather than internal representation analysis. "SoK: a Comprehensive Causality Analysis Framework" may provide related causality perspectives.
- **Break condition:** If jailbreak prompts vary so widely in structure that no consistent representational signature emerges across different attack types, the approach would fail to generalize.

### Mechanism 2
- **Claim:** Attention and mixer mechanisms encode more discriminative jailbreak features than aggregated layer outputs.
- **Mechanism:** Core sequence-mixing components (Multi-Head Attention in transformers, Mamba Mixer in SSMs) directly process relationships between tokens. Jailbreak prompts likely exhibit anomalous token interaction patterns that these components encode, whereas aggregated layer outputs dilute this signal by combining with residual connections and normalization.
- **Core assumption:** The discriminative information for jailbreak detection is primarily encoded in how tokens attend to or mix with each other, not in downstream feedforward transformations.
- **Evidence anchors:**
  - [Section 3.1]: "For GPT-J, Multi-Head Attention (MHA) representations outperform layer outputs, and for Mamba-2, Mixer representations outperform Block outputs"
  - [Section 3.1]: MHA layer 27 achieved 90.1% F1 with Random Forest vs 90.4% for full layer output, but middle layers show larger gaps (MHA layer 15: 90.0% vs 69.6%)
  - [corpus]: No direct corpus evidence on attention-based detection mechanisms.
- **Break condition:** If future jailbreak techniques specifically optimize to avoid anomalous attention patterns, this mechanism's effectiveness would degrade.

### Mechanism 3
- **Claim:** Tensor decomposition extracts latent structure that simple classifiers can leverage without complex architecture design.
- **Mechanism:** CP decomposition factorizes the 3D prompt tensor into rank-R components where factor matrix C provides compact embeddings (K prompts × R latent dimensions). This dimensionality reduction preserves structural patterns while enabling standard classifiers to achieve strong performance, avoiding the need for specialized deep learning detection architectures.
- **Core assumption:** The relevant discriminative structure can be captured by low-rank tensor factors and preserved through decomposition.
- **Evidence anchors:**
  - [Section 2.3]: "CP decomposition yields three factor matrices... The factor matrix C serves as the embeddings matrix"
  - [Section 3.1]: "we aim to demonstrate that a simple and efficient pipeline consisting of tensor decomposition followed by basic classifiers can achieve effective performance"
  - [Figure 2]: t-SNE visualization shows clear clustering separation
  - [corpus]: "Scalable Defense against In-the-wild Jailbreaking" mentions retrieval-based defenses but not tensor methods.
- **Break condition:** If the optimal rank R is very large or varies significantly across attack types, decomposition may lose critical information.

## Foundational Learning

- **CP Tensor Decomposition (CANDECOMP/PARAFAC)**
  - Why needed here: This is the core dimensionality reduction technique the paper uses to extract latent embeddings from stacked layer representations.
  - Quick check question: Given a 3D tensor of shape (sequence_length, embedding_dim, num_prompts), what does each factor matrix represent after CP decomposition?

- **Multi-Head Attention vs Layer Output Distinction**
  - Why needed here: The paper shows MHA outputs outperform full layer outputs, requiring understanding of what each contains (attention weights + value projections vs. residual connections + layer norm + FFN).
  - Quick check question: In a transformer layer, what is the difference between the output of multi-head attention and the final layer output passed to the next layer?

- **State-Space Models (SSMs) / Mamba Architecture**
  - Why needed here: Half the experiments use Mamba2, which replaces quadratic attention with linear recurrence; understanding mixer vs. block components is essential.
  - Quick check question: What is the functional analogue of multi-head attention in the Mamba architecture?

## Architecture Onboarding

- **Component map:**
  Input Prompts (jailbreak/benign) -> LLM Forward Pass (GPT-J or Mamba2) -> Layer Extraction Hook → Select: early/middle/final layers → Extract: MHA/Mixer OR full layer output -> 2D Matrix per prompt (seq_len × embed_dim) -> Stack across K prompts → 3D Tensor (seq_len × embed_dim × K) -> CP Decomposition (rank R) → Factor matrices A, B, C -> Factor Matrix C (K × R) → Latent embeddings per prompt -> Classifier (RF/SVM/LR) → 5-fold CV evaluation

- **Critical path:**
  1. Hook registration on target layers during forward pass
  2. Tensor construction requires consistent sequence lengths (may need padding/truncation)
  3. CP rank selection (R) controls embedding dimensionality—too low loses information, too high adds noise
  4. Classifier training on factor C embeddings

- **Design tradeoffs:**
  - **Layer selection:** Early layers capture syntax, middle/deep layers capture semantics—paper shows middle-to-final layers perform best but may be model-specific
  - **MHA/Mixer vs. full output:** MHA provides cleaner signal but requires architecture-specific extraction hooks
  - **Rank R:** Higher rank captures more variation but risks overfitting; paper does not specify rank used
  - **Dataset size:** Paper uses only 240-400 prompts due to computational constraints; scaling behavior unknown

- **Failure signatures:**
  - F1 scores near 50% (random baseline): Tensor decomposition not capturing discriminative structure
  - Large variance across folds: Overfitting or insufficient data
  - MHA and layer outputs performing similarly: Hook extraction may be capturing wrong component
  - t-SNE showing no clustering: Wrong layer selected or rank R too low

- **First 3 experiments:**
  1. **Reproduce single model baseline:** Extract GPT-J layer 15 MHA outputs on 100 jailbreak + 100 benign prompts, apply CP decomposition with R=10-50, train Random Forest with 5-fold CV. Target: F1 > 80%.
  2. **Layer sweep ablation:** For same model, compare early (4-5), middle (14-16), and final (26-27) layers using both MHA and full layer outputs. Document which layer/component combination maximizes F1.
  3. **Rank sensitivity analysis:** Fix best-performing layer from experiment 2, vary R from 5 to 100. Plot F1 vs. rank to identify optimal embedding dimensionality and determine if performance degrades gracefully or has sharp transitions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does tensor decomposition-based detection generalize to larger-scale LLMs (e.g., 70B+ parameters) and production-grade models?
- Basis in paper: [explicit] The conclusion states "Further work is needed to extend the methodology to larger datasets, additional architectures, and more sophisticated evaluation settings."
- Why unresolved: Only GPT-J (6B) and Mamba-2 were tested; computational constraints limited dataset size to 240-400 prompts.
- What evidence would resolve it: Evaluation on models like LLaMA-70B or GPT-NeoX with datasets exceeding 10,000 prompts.

### Open Question 2
- Question: Can adaptive attackers craft jailbreak prompts that evade detection by manipulating internal representations?
- Basis in paper: [inferred] The paper claims no defense is fully resistant, but does not test robustness against attacks designed specifically to fool this detector.
- Why unresolved: No adversarial evaluation was conducted; the method assumes jailbreak prompts produce distinguishable patterns, which may not hold under adaptive attack.
- What evidence would resolve it: Evaluation against gradient-based or optimization-driven attacks that explicitly minimize detection scores while maintaining jailbreak effectiveness.

### Open Question 3
- Question: Why do attention outputs (MHA/Mixer) yield better classification than aggregated layer outputs?
- Basis in paper: [inferred] Tables 1-2 show MHA outputs outperform layer outputs for GPT-J; Mixer outputs outperform Block outputs for Mamba-2, but no explanation is provided.
- Why unresolved: The authors note this finding but do not investigate the underlying representational differences.
- What evidence would resolve it: Ablation studies analyzing the semantic content of attention vs. FFN representations, or probing tasks measuring which linguistic features each component encodes.

### Open Question 4
- Question: Does the method generalize across diverse jailbreak strategies (e.g., role-play, translation attacks, prefix injection)?
- Basis in paper: [inferred] Only one dataset was used, and no analysis separates performance by jailbreak type.
- Why unresolved: Different attack strategies may produce distinct internal patterns; lumping them together could mask failure modes.
- What evidence would resolve it: Per-category F1 scores on a taxonomy-annotated jailbreak dataset (e.g., WildTeaming).

## Limitations

- **Limited dataset size:** Experiments used only 240-400 prompts due to computational constraints, raising questions about scalability and real-world applicability
- **No adversarial evaluation:** The method was not tested against attacks specifically designed to evade detection by manipulating internal representations
- **Rank selection unspecified:** The paper does not specify the CP decomposition rank R used, which critically affects embedding quality and classification performance

## Confidence

**High Confidence:** The fundamental observation that MHA and Mixer representations outperform full layer outputs, and that middle-to-final layers show better separation than early layers. These patterns are consistently reported across both GPT-J and Mamba-2 models.

**Medium Confidence:** The specific F1 scores reported (ranging from 67% to 95%) are likely reproducible given the described methodology, but exact values may vary due to unspecified hyperparameters (CP rank, classifier parameters, sequence length handling).

**Low Confidence:** The scalability claims and generalizability to broader jailbreak attack families. The small dataset size and lack of ablation studies on different attack types prevent strong conclusions about real-world applicability.

## Next Checks

1. **Rank Sensitivity Validation:** Systematically sweep CP decomposition rank R from 5 to 100 while keeping all other parameters fixed. Plot both reconstruction error and classification F1 to identify the optimal rank and determine if performance degrades gracefully or shows sharp transitions. This addresses the critical unknown of rank selection.

2. **Dataset Size Scaling Study:** Reproduce the pipeline with progressively larger subsets of the HuggingFace dataset (e.g., 100, 200, 400, 800 prompts). Measure F1 stability and computational cost scaling to assess whether the current small sample size artificially inflates performance metrics or captures the true detection ceiling.

3. **Attack Type Generalization Test:** Stratify the dataset by known jailbreak attack techniques (DAN, Stan, tree-of-thought, etc.) and evaluate whether the tensor decomposition approach maintains consistent F1 scores across attack types. This validates the core assumption that jailbreak prompts produce consistent representational signatures regardless of attack methodology.