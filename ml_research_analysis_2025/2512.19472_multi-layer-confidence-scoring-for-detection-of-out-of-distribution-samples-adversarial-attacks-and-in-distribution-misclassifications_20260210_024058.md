---
ver: rpa2
title: Multi-Layer Confidence Scoring for Detection of Out-of-Distribution Samples,
  Adversarial Attacks, and In-Distribution Misclassifications
arxiv_id: '2512.19472'
source_url: https://arxiv.org/abs/2512.19472
tags:
- samples
- confidence
- detection
- which
- macs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MACS is a post-hoc framework for estimating confidence in pre-trained
  DNNs by analyzing intermediate activations. It processes selected layers through
  dimensionality reduction, clustering, and feature-label association to create classification-maps,
  which are then compared to class-specific proto-maps to produce a confidence score.
---

# Multi-Layer Confidence Scoring for Detection of Out-of-Distribution Samples, Adversarial Attacks, and In-Distribution Misclassifications

## Quick Facts
- arXiv ID: 2512.19472
- Source URL: https://arxiv.org/abs/2512.19472
- Reference count: 40
- MACS is a post-hoc framework for estimating confidence in pre-trained DNNs by analyzing intermediate activations

## Executive Summary
MACS introduces a post-hoc framework that estimates confidence in pre-trained deep neural networks by analyzing intermediate layer activations. The method processes selected layers through dimensionality reduction, clustering, and feature-label association to create classification-maps, which are then compared to class-specific proto-maps to produce a confidence score. Evaluated on VGG16 and ViTB16 for CIFAR-100, MACS demonstrates strong performance across confidence estimation, OOD detection, and adversarial attack detection. The framework is unsupervised, scalable, and maintains low online computational overhead while achieving competitive detection rates.

## Method Summary
MACS operates in two phases: an offline training phase where corevectors are extracted via SVD from selected layer activations, GMM clustering is performed, and proto-maps are built from high-confidence correctly classified training samples; and an online inference phase where classification-maps are computed for new samples and compared to predicted class proto-maps using cosine similarity. The framework analyzes intermediate activations rather than relying solely on softmax outputs, enabling detection of anomalies through deviations in internal activation patterns. Corevectors are low-dimensional projections that preserve decision-relevant information, while GMM clustering reveals activation patterns correlating with semantic labels.

## Key Results
- MACS achieves FPR* values as low as 0.55 and AUCs up to 0.95 across ID misclassification, OOD detection, and adversarial attack scenarios
- The framework consistently ranks among top-performing methods, outperforming or matching supervised alternatives like DMD-a on CIFAR-100 benchmarks
- MACS demonstrates particular effectiveness against high-confidence adversarial attacks where softmax-based methods fail, maintaining low scores for both high-confidence (BIM/PGD) and low-confidence (CW/DF) attack types

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Low-dimensional projections of layer activations preserve decision-relevant information while enabling scalable processing
- Mechanism: SVD decomposition of affine transformation matrix A identifies κ most informative directions; corevectors v = Q'^T[x;1] compress activations via projection onto top singular vectors
- Core assumption: Informative structure for classification decisions resides in low-dimensional subspace of activations
- Evidence anchors: [abstract] "processes selected layers through dimensionality reduction, clustering, and feature-label association"; [Section III-A] SVD-based projection for corevector extraction
- Break condition: If informative features require high-dimensional representations (κ too small), corevectors lose discriminative power

### Mechanism 2
- Claim: Unsupervised clustering of corevectors reveals activation patterns that correlate with semantic labels
- Mechanism: GMM assigns soft cluster memberships m to corevectors; association matrix U links clusters to labels via Pr{τ(v)=l|ς(v)=i}; g = Um estimates label probabilities from layer activations
- Core assumption: Clusters capture Low-Level Features with consistent relationships to High-Level Features (labels)
- Evidence anchors: [Section III-A] "third step consists of associating clustering assignments... with human-understandable HLF represented by the labels"
- Break condition: If clusters don't align with semantic structure, g becomes noisy label estimator

### Mechanism 3
- Claim: Comparing multi-layer activation patterns to class-specific reference patterns detects anomalous decisions
- Mechanism: Classification-maps G collect label-estimation vectors across M layers; proto-maps P_l aggregate G from high-confidence correct training samples per class; confidence score s = cosine_similarity(P_ℓ(X), G)
- Core assumption: Correctly classified high-confidence samples exhibit characteristic activation trajectories; OOD/AA/misclassified samples deviate
- Evidence anchors: [Section V] Table VI shows MACS achieves lowest or second-lowest FPR* across scenarios; [Section V-D] demonstrates score drops for misclassifications
- Break condition: If adversarial or OOD samples produce activation patterns similar to correct predictions, detection fails

## Foundational Learning

- Concept: **Singular Value Decomposition (SVD) for dimensionality reduction**
  - Why needed here: MACS uses SVD to project high-dimensional layer activations onto compact "corevectors" preserving most informative directions
  - Quick check question: Given a 4096-dimensional activation from linear layer, how would you select κ to capture 95% of variance in transformation matrix?

- Concept: **Gaussian Mixture Models (GMM) and soft clustering**
  - Why needed here: GMM provides probabilistic cluster memberships rather than hard assignments, enabling smooth gradients through association step
  - Quick check question: If cluster assignments are too soft (high covariance), what happens to discriminative power of g = Um?

- Concept: **Cosine similarity for high-dimensional comparison**
  - Why needed here: Confidence score uses cosine similarity to compare classification-maps G against proto-maps P, normalizing for magnitude differences
  - Quick check question: Why might cosine similarity be preferable to Euclidean distance when comparing activation patterns across samples with varying magnitudes?

## Architecture Onboarding

- Component map:
Training phase (offline): Layer selector → SVD(A) → Q' matrices; Training data → Corevector extraction → GMM clustering → Membership functions; Corevectors + labels → Association matrix U; High-confidence samples → Proto-maps P_l

Inference phase (online): Input X → Forward pass → Extract activations from M layers; Per layer: v = Q'^T[x;1] → m = GMM_membership(v) → g = Um; Stack g vectors → Classification-map G; Score s = cosine_sim(P_predicted_class, G)

- Critical path:
  1. Hyperparameter selection: κ (corevector dimension) and C (cluster count) tuned to maximize top-3 accuracy of g vs. true labels
  2. Proto-map construction: Only high-confidence (max(z(X)) > δ), correctly classified samples contribute
  3. Layer selection: Trade-off between coverage and overhead; paper uses 13/16 VGG layers, 25 ViTB16 layers

- Design tradeoffs:
  - More layers → richer G but higher overhead; dimensionality reduction dominates cost for conv layers (~90% for VGG16)
  - Higher κ → better approximation but slower; lower κ → information loss
  - Higher C → finer granularity but risk of overfitting to training distribution
  - δ threshold for proto-maps: stricter filtering improves precision but may exclude valid class variants

- Failure signatures:
  - Consistently low scores for correctly classified ID samples → proto-maps too narrow or κ/C mismatch
  - High scores for adversarial samples → attack exploits layers not well-covered by proto-maps
  - Large variance in scores within same class → clustering insufficiently captures class structure
  - DMD-a significantly outperforms MACS → suggests supervised fitting could help

- First 3 experiments:
  1. Baseline sanity check: On CIFAR-100 validation set, plot score distributions for correct vs. incorrect classifications. Confirm MACS pushes misclassified samples toward lower scores with less overlap than MSP
  2. Ablation on layer coverage: Run MACS with (a) only early layers, (b) only late layers, (c) all layers. Measure AUC for OOD detection (SVHN/Places365) to identify which layers contribute most
  3. Attack robustness test: Compare score distributions for high-confidence attacks (BIM/PGD) vs. low-confidence attacks (CW/DF). Verify MACS maintains low scores for both while MSP fails on high-confidence attacks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can MACS framework be effectively generalized to non-vision domains like NLP or time-series analysis?
- Basis in paper: [explicit] Authors list "extending framework to non-vision domains" as future direction in Conclusion
- Why unresolved: Experimental validation restricted to image classification using VGG16/ViTB16 architectures; unclear if SVD and clustering transfer to sequential/sparse data structures
- What evidence would resolve it: Successful application to text/audio datasets using Transformers/RNNs showing comparable AUC and FPR* metrics

### Open Question 2
- Question: Can clustering strategy be made adaptive to data complexity or layer depth rather than relying on fixed hyper-parameters?
- Basis in paper: [explicit] Section VI identifies "exploring adaptive clustering strategies" as future direction; Section III-A notes κ and C are only hyper-parameters
- Why unresolved: Current implementation requires tuning κ and C to maximize top-3 accuracy; static choice may be sub-optimal across different layers or datasets
- What evidence would resolve it: Comparative study showing adaptive mechanism (varying C based on intrinsic dimensionality) outperforms fixed-parameter setup

### Open Question 3
- Question: Can MACS score be utilized as loss function or regularization term during DNN training to improve inherent robustness and calibration?
- Basis in paper: [explicit] Conclusion proposes "integrating MACS into training-time feedback loops to enhance calibration and robustness"
- Why unresolved: MACS currently designed strictly as post-hoc framework analyzing pre-trained models; differentiability of SVD and GMM steps unexplored
- What evidence would resolve it: Training regime including MACS components in optimization objective resulting in higher accuracy or better calibration than cross-entropy only

## Limitations

- Confidence calibration concerns: MACS explicitly avoids high-confidence outputs (scores near 1.0) for correctly classified samples, differing from standard calibration practices and complicating direct comparison with softmax-based methods
- Architectural dependency through layer selection: Method requires selecting which layers to analyze (13/16 VGG16 layers used) without principled guidance, creating non-trivial hyperparameter tuning burden that likely varies by architecture and task
- Potential blind spots against sophisticated attacks: Unsupervised approach relying on distributional assumptions about normal activations may miss attacks that preserve internal activation patterns while corrupting final predictions

## Confidence

- **High confidence**: Core mechanism of using intermediate activations for confidence estimation is well-established (e.g., DkNN, DMD); mathematical formulation using SVD for dimensionality reduction and GMM for clustering follows standard practices
- **Medium confidence**: Performance claims across diverse scenarios supported by comprehensive experiments, though reliance on high-confidence correct samples for proto-map construction means performance could degrade if base model's confidence estimates are poorly calibrated
- **Low confidence**: Claim that MACS consistently ranks among top-performing methods requires context; direct comparisons complicated by different evaluation protocols and dataset variations; unsupervised advantage over supervised alternatives not fully quantified

## Next Checks

1. **Layer sensitivity analysis**: Systematically evaluate MACS performance using different layer subsets (early, middle, late, various combinations) on CIFAR-100; measure impact on OOD detection AUC and AA detection FPR* to identify optimal layer configurations and quantify sensitivity

2. **Calibration curve comparison**: Generate reliability diagrams comparing MACS scores against softmax probabilities for correctly classified CIFAR-100 test samples; quantify gap between expected and actual accuracy at different score thresholds to assess practical calibration limitations

3. **Attack-specific vulnerability testing**: Evaluate MACS against attacks specifically designed to preserve internal activation patterns (e.g., feature-space adversarial examples); compare detection rates against standard pixel-space attacks to identify potential blind spots in unsupervised approach