---
ver: rpa2
title: 'DFCA: Decentralized Federated Clustering Algorithm'
arxiv_id: '2510.15300'
source_url: https://arxiv.org/abs/2510.15300
tags:
- decentralized
- learning
- clients
- federated
- dfca
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DFCA introduces a fully decentralized clustered FL algorithm that
  enables clients to collaboratively train cluster-specific models without central
  coordination. The method uses sequential running-average aggregation to integrate
  neighbor updates as they arrive, providing a communication-efficient alternative
  to batch aggregation while maintaining clustering performance.
---

# DFCA: Decentralized Federated Clustering Algorithm

## Quick Facts
- arXiv ID: 2510.15300
- Source URL: https://arxiv.org/abs/2510.15300
- Reference count: 26
- Introduces fully decentralized clustered FL with sequential running-average aggregation

## Executive Summary
DFCA presents a fully decentralized clustered federated learning algorithm that enables clients to collaboratively train cluster-specific models without central coordination. The method uses sequential running-average aggregation to integrate neighbor updates as they arrive, providing a communication-efficient alternative to batch aggregation while maintaining clustering performance. Experiments on various datasets show DFCA outperforms decentralized baselines and achieves accuracy comparable to centralized IFCA with only ~1% difference in favor of the centralized method.

## Method Summary
DFCA introduces a novel decentralized federated clustering approach where clients can form clusters and train specialized models without requiring a central server. The algorithm employs sequential running-average aggregation, allowing clients to integrate updates from neighbors as they arrive rather than waiting for complete batches. This asynchronous approach enables more efficient communication and better scalability in real-world network conditions. The clustering mechanism operates in a decentralized manner where clients can identify and join appropriate clusters through local interactions with their neighbors in the communication graph.

## Key Results
- DFCA outperforms decentralized baselines (FedSPD, DFedAvgM) on various datasets
- Achieves accuracy comparable to centralized IFCA with only ~1% difference in favor of centralized method
- Demonstrates robustness to low-connectivity settings, maintaining stable performance with only 15% connectivity probability
- Sequential aggregation enables asynchronous updates, improving scalability and suitability for real-world networks

## Why This Works (Mechanism)
DFCA's effectiveness stems from its sequential running-average aggregation mechanism that allows clients to continuously update their models as neighbor information becomes available. This approach reduces communication overhead compared to batch aggregation while maintaining model quality. The decentralized clustering enables clients to specialize their models based on local data characteristics without central coordination. The algorithm's ability to handle asynchronous updates makes it particularly suitable for real-world networks with irregular connectivity patterns, where waiting for complete batches would be inefficient or impractical.

## Foundational Learning
- **Federated Learning**: Distributed machine learning where multiple clients train models collaboratively without sharing raw data; needed for understanding the decentralized learning paradigm and why data privacy matters
- **Clustering in FL**: Grouping clients based on data similarity to train specialized models; needed to understand why one-size-fits-all models may underperform in heterogeneous environments
- **Decentralized vs Centralized FL**: Understanding the trade-offs between peer-to-peer and server-based coordination; needed to appreciate the benefits and challenges of removing central coordination
- **Asynchronous Aggregation**: Techniques for integrating model updates as they arrive rather than in synchronized batches; needed to grasp how DFCA achieves communication efficiency
- **Communication Graph Topology**: Understanding how clients are connected and exchange information in decentralized settings; needed to evaluate the algorithm's robustness to different network structures
- **Running-Average Updates**: Mathematical mechanism for integrating sequential updates while maintaining convergence properties; needed to understand the core algorithmic innovation

## Architecture Onboarding

### Component Map
Client nodes <-> Communication graph -> Local clustering module -> Sequential aggregation module -> Model update module

### Critical Path
1. Client receives neighbor updates from communication graph
2. Local clustering module identifies appropriate cluster membership
3. Sequential aggregation module integrates incoming updates using running average
4. Model update module applies aggregated updates to local model parameters
5. Client shares updated model with neighbors for next iteration

### Design Tradeoffs
- **Sequential vs Batch Aggregation**: Sequential aggregation reduces communication latency but may introduce bias from out-of-order updates; batch aggregation ensures consistency but requires waiting for all neighbors
- **Decentralized vs Centralized Clustering**: Removes single point of failure and improves privacy but may lead to suboptimal cluster formation without global view
- **Connectivity Requirements**: Lower connectivity improves scalability but may slow convergence and affect clustering quality
- **Asynchrony vs Synchronization**: Asynchronous updates improve responsiveness but complicate convergence analysis and may introduce staleness

### Failure Signatures
- **Poor Clustering Performance**: Indicates issues with local clustering module or insufficient neighbor diversity
- **Slow Convergence**: May result from low connectivity, inappropriate learning rates, or inadequate model capacity
- **Model Drift**: Can occur when neighbor updates arrive too asynchronously, causing local models to diverge significantly
- **Communication Bottlenecks**: Despite sequential aggregation, high-frequency updates from busy neighbors can still create congestion

### 3 First Experiments
1. Test basic clustering performance on a simple synthetic dataset with known cluster structure to verify the clustering mechanism works correctly
2. Evaluate convergence speed and accuracy under varying connectivity probabilities (5%, 15%, 30%, 50%) to understand the relationship between network density and performance
3. Compare sequential running-average aggregation against batch aggregation on the same dataset and network topology to quantify communication efficiency gains

## Open Questions the Paper Calls Out
None

## Limitations
- The ~1% performance gap compared to centralized IFCA may depend heavily on dataset characteristics and hyperparameter tuning
- Computational overhead of sequential running-average aggregation compared to simpler decentralized methods remains unclear
- The algorithm's behavior under highly asynchronous conditions with potential model staleness is not fully characterized
- Limited testing across diverse network topologies and real-world conditions with heterogeneous client capabilities

## Confidence
- **High Confidence**: The fundamental algorithmic framework and basic clustering performance claims
- **Medium Confidence**: The specific performance metrics and comparisons with baselines, given potential sensitivity to implementation details
- **Medium Confidence**: The scalability claims, pending more extensive testing across diverse network topologies

## Next Checks
1. Conduct ablation studies comparing sequential running-average aggregation against batch aggregation in terms of both accuracy and communication efficiency across multiple datasets and network conditions.

2. Evaluate convergence behavior and model staleness effects in highly asynchronous scenarios with varying degrees of connectivity sparsity beyond the 15% threshold.

3. Test the algorithm's performance under realistic network conditions with heterogeneous client data distributions and varying computational capabilities to assess practical deployment viability.