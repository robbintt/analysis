---
ver: rpa2
title: Integrating Causal Reasoning into Automated Fact-Checking
arxiv_id: '2512.13286'
source_url: https://arxiv.org/abs/2512.13286
tags:
- evidence
- claim
- events
- reasoning
- causal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a causal reasoning-based framework for automated
  fact-checking that integrates fine-grained event relations (cause, prevent, intend,
  enable) derived from the FARO ontology. The approach combines event relation extraction,
  semantic similarity computation, and rule-based reasoning to detect logical inconsistencies
  between claim and evidence chains.
---

# Integrating Causal Reasoning into Automated Fact-Checking

## Quick Facts
- arXiv ID: 2512.13286
- Source URL: https://arxiv.org/abs/2512.13286
- Reference count: 27
- Primary result: Causal reasoning-based fact-checking framework achieves F1-scores of 0.50 (RSS), 0.27-0.43 (A VeriTeC), and 0.47-0.56 (FEVEROUS)

## Executive Summary
This paper introduces a causal reasoning-based framework for automated fact-checking that integrates fine-grained event relations (cause, prevent, intend, enable) derived from the FARO ontology. The approach combines event relation extraction, semantic similarity computation, and rule-based reasoning to detect logical inconsistencies between claim and evidence chains. Evaluated on two fact-checking datasets, the method achieves F1-scores ranging from 0.27 to 0.56 depending on dataset and evaluation strictness. While not surpassing black-box models in accuracy, the framework offers interpretable causal explanations and potential as a complementary module to enhance explainability in existing systems.

## Method Summary
The framework extracts semantically precise event relations within claims and evidence using REBEL, a sequence-to-sequence model trained on news sentences. It infers cross-context causal relations via commonsense knowledge (ATOMIC) augmented with LLM-generated examples or direct LLM inference. Semantic similarity is computed using SBERT embeddings with a 0.54 threshold, and polarity is determined using DistilBERT. Four rule-based checks (logical alignment, misalignment, causal loops, cherry-picking) determine verdicts. The pipeline is implemented in Python with source code available.

## Key Results
- RSS dataset: F1 = 0.50 (Tolerant), F1 = 0.46 (Strict)
- A VeriTeC: F1 = 0.27-0.29 (Strict), F1 = 0.43-0.44 (Tolerant)
- FEVEROUS: F1 = 0.47 (Tolerant), F1 = 0.56 (Strict)
- Event-relation extraction F1 = 0.82 (combined)
- Cross-context causality: 82.5% accuracy via LLM, 97-99% via ATOMIC

## Why This Works (Mechanism)

### Mechanism 1: Fine-Grained Event Relation Extraction Using FARO Ontology
Extracting semantically precise event relations enables richer causal reasoning than generic "causality" links. The system uses REBEL trained on 2,696 annotated news sentences to extract (Subject, Relation, Object) triplets. FARO ontology provides logical axioms defining how relations interact. Core assumption: Fine-grained causal relations can be reliably extracted and their logical properties hold in real-world scenarios. Break condition: Overly abbreviated events cause downstream failures.

### Mechanism 2: Cross-Context Causality via Commonsense Knowledge or LLMs
Causal relations between claim and evidence events can be inferred using external knowledge. Two strategies: ATOMIC commonsense knowledge graph augmented with LLM-generated prevent/enable examples (achieving F1 0.84-0.99), or Phi-3-Medium LLM with few-shot prompting (82.5% accuracy). Core assumption: Commonsense causal knowledge transfer is valid for fact-checking contexts. Break condition: Domain-specific knowledge requires explicit ontological links.

### Mechanism 3: Rule-Based Logical Inconsistency Detection
Logical alignment or misalignment between claim-evidence causal chains predicts verdict. Four rules implemented: logical alignment (support), misalignment (refute), causal loops (support), cherry-picking (conflicting evidences). Core assumption: Semantic similarity + polarity matching reliably identify event equivalence and opposition. Break condition: Double negation, implicit entailment, and type-based reasoning cause failures.

## Foundational Learning

- Concept: **FARO Ontology Event Relations**
  - Why needed here: Reasoning depends on understanding logical properties (transitivity, disjointness) of cause/prevent/intend/enable
  - Quick check: Given "A prevents B" and "B prevents C," what can you infer about A and C? (Answer: A causes C, via double negation)

- Concept: **Sentence Embeddings and Similarity Thresholds**
  - Why needed here: Event alignment depends on SBERT cosine similarity with empirically set threshold (0.54)
  - Quick check: Why might antonyms have high similarity scores, and how does the system handle this? (Answer: Antonyms share semantic context; system adds polarity detection to distinguish)

- Concept: **Rule-Based vs. Neural Reasoning Tradeoffs**
  - Why needed here: Paper positions this as interpretable but lower-accuracy (F1 0.50) compared to GPT-4o mini (F1 0.67)
  - Quick check: What linguistic structures does the rule-based reasoner fail on? (Answer: Double negation, implicit entailment, type-based mismatches)

## Architecture Onboarding

- Component map: Input text -> Event Relation Extraction (REBEL) -> Cross-Context Causality (ATOMIC/LLM) -> Similarity/Polarity (SBERT/DistilBERT) -> Reasoning Rules -> Verdict
- Critical path: Event Relation Extraction → Cross-Context Causality → Similarity/Polarity → Reasoning Rules → Verdict. Errors in ERE propagate; incomplete events make similarity unreliable.
- Design tradeoffs: Commonsense vs. LLM for cross-context causality (interpretable vs. flexible); Strict vs. Tolerant evaluation (penalizes abstentions vs. excludes them); Rule-based vs. black-box (explanations vs. accuracy).
- Failure signatures: Overly abbreviated events from ERE; double negation failures; type-based mismatches requiring ontological reasoning.
- First 3 experiments:
  1. Reproduce RSS baseline: Run on 86-pair subset, verify F1 ≈0.50, categorize failures by ERE, similarity, or reasoning gaps
  2. Ablate cross-context causality: Compare ATOMIC vs. Phi-3 on FEVEROUS subset, measure F1 difference and latency
  3. Pilot ontology augmentation: Add mapping (5G → non-ionizing radiation) for failure case, rerun, document effort per type hierarchy

## Open Questions the Paper Calls Out

- Can integrating external ontologies effectively resolve type-based mismatches where the system fails to infer subclass relationships? Current pipeline lacks hierarchical knowledge required to connect specific entities to general categories. Evidence: Significant reduction in false negatives on RSS following ontology integration.

- Does incorporating temporal and spatial information into event representations improve similarity and polarity computations? Current ERE outputs are often "overly abbreviated" or lack context. Evidence: Comparative evaluation showing enriched embeddings yield higher precision in similarity module.

- Can symbolic reasoning layers effectively mitigate errors caused by complex linguistic structures such as double negation? Current neural and rule-based components struggle with nested negation and logical equivalences. Evidence: Successful verdict prediction on curated test set of double negation pairs.

## Limitations
- Event relation extraction outputs are overly abbreviated, reducing similarity reliability
- Double negation and ontological type mismatches (e.g., 5G vs. non-ionizing radiation) break rule-based reasoning
- Commonsense knowledge transfer has limited coverage for domain-specific claims

## Confidence
- **High**: F1 scores reported (0.50 RSS, 0.27-0.43 A VeriTeC, 0.47-0.56 FEVEROUS) and method implementation details
- **Medium**: Claims about interpretability and complementary value to black-box systems
- **Low**: Claims about generalizability beyond tested datasets and specific claim types

## Next Checks
1. Reproduce baseline F1 scores on the Reasoner-Specific Subset and categorize failures by error type
2. Compare ATOMIC vs. Phi-3 performance on a subset of FEVEROUS to measure tradeoff between interpretability and flexibility
3. Augment the ontology with domain-specific type mappings (e.g., 5G → non-ionizing radiation) and measure impact on previously failing cases