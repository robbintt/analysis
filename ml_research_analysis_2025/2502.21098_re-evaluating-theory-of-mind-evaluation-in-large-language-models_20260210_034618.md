---
ver: rpa2
title: Re-evaluating Theory of Mind evaluation in large language models
arxiv_id: '2502.21098'
source_url: https://arxiv.org/abs/2502.21098
tags:
- https
- llms
- mind
- theory
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper re-evaluates the current state of Theory of Mind (ToM)
  evaluation in large language models (LLMs). The authors argue that conflicting claims
  about LLMs' ToM abilities stem from a lack of clarity on whether ToM should be defined
  as behavior-matching or computation-matching with humans.
---

# Re-evaluating Theory of Mind evaluation in large language models

## Quick Facts
- arXiv ID: 2502.21098
- Source URL: https://arxiv.org/abs/2502.21098
- Reference count: 40
- Primary result: Conflicting claims about LLM Theory of Mind abilities stem from unclear definitions and flawed evaluation methods

## Executive Summary
This paper argues that conflicting claims about large language models' Theory of Mind (ToM) abilities stem from a fundamental lack of clarity on whether ToM should be defined by behavior-matching or computation-matching with humans. The authors identify two main issues with current evaluations: they overly focus on matching human behavior rather than understanding the underlying computations, and they may not actually measure ToM due to threats like training away, auxiliary task demands, and linguistic artifacts. The paper suggests moving toward computation-centric evaluations grounded in cognitive theory, using frozen and open models, and explicitly describing auxiliary demands in test design.

## Method Summary
This is a position paper analyzing the current state of Theory of Mind evaluation in LLMs rather than presenting original experiments. The authors review existing benchmarks and conflicting claims in the literature, identifying conceptual problems in how ToM is evaluated. They synthesize evidence from studies using Sally-Anne false-belief tasks, unexpected contents/transfer tasks, and adversarial variants, while proposing methodological improvements including using frozen open-weight models to avoid "training away," designing control conditions for auxiliary demands, and comparing to empirically measured human performance rather than assumed ceilings.

## Key Results
- Conflicting claims about LLM ToM abilities arise from conflating behavior-matching with computation-matching definitions
- Current evaluations may not actually measure ToM due to threats like training away and auxiliary task demands
- The relationship between pragmatic communication and ToM represents an underexplored area for future research

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Conflicting claims about LLM ToM arise from conflating behavior-matching with computation-matching.
- Mechanism: Studies claiming "LLMs have ToM" typically measure whether model outputs (M') match human outputs (M) on ToM tasks. Studies claiming "LLMs lack ToM" probe whether models use the same function f to map observations to mental states. These are nested but distinct questions.
- Core assumption: That "true" ToM requires human-like computation, not just human-like outputs.
- Evidence anchors:
  - [abstract]: "lack of clarity on whether ToM should be defined by behavior-matching or computation-matching"
  - [section 2]: Figure 1 distinction between "Same output?" (Q1) and "Same computation?" (Q2); "these claims are not independent, but nested"
  - [corpus]: Neighbor papers (CogToM, XToM) continue focusing on benchmark construction, suggesting the field has not yet converged on computation-matching approaches.
- Break condition: If one explicitly defines ToM as behavior-matching only, the mechanism collapses—no conflict exists.

### Mechanism 2
- Claim: "Training away" creates illusory ToM improvement by memorizing adversarial examples without changing underlying computation.
- Mechanism: Closed-API models receive continual updates. When adversarial ToM test items are input through APIs, models may incorporate them into training (via parameter updates or in-context learning), improving benchmark scores while f remains unchanged.
- Core assumption: That model developers use user interactions (including adversarial test cases) for model improvement.
- Evidence anchors:
  - [abstract]: "threats such as 'training away'"
  - [section 4.2.1]: "training away evaluations" described as updating model on failures "without fundamentally changing the model's underlying computations"
  - [corpus]: Weak direct evidence—corpus neighbors do not address training away explicitly.
- Break condition: If using frozen, open-weight models, this mechanism is eliminated.

### Mechanism 3
- Claim: Adversarial ToM evaluations risk introducing auxiliary task demands that confound failure attribution.
- Mechanism: As adversarial items become more complex (e.g., transparent bags, unusual objects), they require additional capabilities—physical reasoning, longer context tracking, vocabulary knowledge. Failure may reflect these demands rather than ToM deficits.
- Core assumption: That human ToM failures on simplified tasks are minimal, though the paper notes humans also perform worse under some perturbations.
- Evidence anchors:
  - [section 4.2.2]: "As test items become more adversarial, they will inevitably also become more complex...we run the risk of no longer primarily testing ToM abilities"
  - [section 4.2.2]: Notes Strachan et al. (2024) found "human participants also perform worse under these perturbations"
  - [corpus]: Decompose-ToM explicitly addresses task decomposition to reduce auxiliary demands, supporting this mechanism.
- Break condition: If control conditions verify models can handle auxiliary demands independently, ToM-specific failure can be isolated.

## Foundational Learning

- Concept: **Behavior-matching vs. Computation-matching**
  - Why needed here: This is the central distinction driving the paper's argument; without it, one cannot understand why studies conflict.
  - Quick check question: If an LLM correctly predicts where Sally will look for a hidden object, have you proven it "has" ToM? (Answer: Only for behavior-matching; computation-matching requires probing the algorithm.)

- Concept: **Construct Validity**
  - Why needed here: The paper argues ToM evaluations may fail to measure the psychological construct they claim to measure.
  - Quick check question: A model fails an adversarial ToM task because it doesn't understand "transparent." Does this prove it lacks ToM? (Answer: No—construct validity requires isolating the target ability.)

- Concept: **False-Belief Tasks (Sally-Anne paradigm)**
  - Why needed here: The canonical ToM evaluation discussed throughout; understanding what it's designed to test is prerequisite.
  - Quick check question: Why does correctly predicting Sally will look in the wrong location indicate ToM? (Answer: It requires attributing a belief that contradicts reality.)

## Architecture Onboarding

- Component map: A (Input observations) -> f (Computation function) -> M' (Model output) -> M (Human output)
- Critical path:
  1. Define ToM explicitly (behavior or computation)
  2. Select/design evaluation with clear construct validity
  3. Use frozen/open models to avoid training away
  4. Include control conditions for auxiliary demands
  5. Compare to empirically measured human performance (not assumed ceiling)
- Design tradeoffs:
  - Behavior-matching: Easier to implement, but risks "right for wrong reasons"
  - Computation-matching: More diagnostic, but requires interpretability methods and normative cognitive models
  - Adversarial testing: Reveals shallow heuristics, but may introduce confounds
- Failure signatures:
  - Model succeeds on standard items but fails on minimal adversarial modifications (shallow heuristics)
  - Model performance improves across model versions without architecture changes (training away)
  - Model and humans both fail on complex adversarial items (auxiliary demands, not ToM deficit)
- First 3 experiments:
  1. **Frozen model baseline**: Test identical ToM items on an open-weight model version-frozen for 6+ months to establish uncontaminated baseline.
  2. **Control condition design**: For adversarial ToM items, create matched controls testing only the auxiliary demand (e.g., "what does transparent mean?") to isolate failure source.
  3. **Human comparison on adversarial items**: Run the same adversarial ToM items with human participants to establish whether humans are actually at ceiling (the paper suggests they may not be).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do LLMs use the same underlying computations (algorithms) as humans to map observed actions to mental states, or do they merely match human input/output behavior through shallow heuristics?
- Basis in paper: [explicit] The authors argue that a major source of disagreement is the lack of distinction between behavior-matching (matching input/output) and computation-matching (matching the underlying algorithm f).
- Why unresolved: Current evaluations often rely on behavioral benchmarks (like the Sally-Anne task) that can be solved via pattern matching or "lookup tables" without requiring the model to possess a human-like generative world model.
- What evidence would resolve it: Mechanistic interpretability studies that identify internal model features corresponding to cognitive variables like "beliefs" and "goals," or the successful application of cognitive models (e.g., inverse-planning or RSA) to predict model behavior.

### Open Question 2
- Question: Is there a correlation between pragmatic language abilities and Theory of Mind capabilities in LLMs, and does this relationship mirror the one found in human cognition?
- Basis in paper: [explicit] The authors suggest that explicitly studying whether abilities in pragmatic tasks (e.g., irony interpretation) predict abilities in ToM tasks (e.g., false-belief inference) could clarify if these abilities are intertwined in models as they are in humans.
- Why unresolved: ToM and pragmatics have primarily been evaluated using separate tasks and evaluation settings in LLM research, often focusing only on behavior-matching rather than the relationship between the two constructs.
- What evidence would resolve it: A comprehensive analysis correlating performance across distinct pragmatic and ToM benchmarks within the same models, compared against human correlation data.

### Open Question 3
- Question: Can Theory of Mind abilities emerge during a model's pre-training phase, or do they require specific fine-tuning and alignment processes (such as RLHF)?
- Basis in paper: [explicit] The authors ask, "test whether ToM abilities can emerge during a model's pre-training phase, or if ToM reasoning requires some form of fine-tuning/alignment."
- Why unresolved: Past studies have noted performance differences between base and fine-tuned models, but controlled comparisons within model families isolating the specific effect of fine-tuning on ToM reasoning are lacking.
- What evidence would resolve it: Controlled experiments comparing the ToM performance of base models against their instruction-tuned counterparts (within the same model family) on novel, uncontaminated evaluation sets.

### Open Question 4
- Question: Can LLMs exhibit "spontaneous" Theory of Mind, performing mental state inferences without being explicitly prompted or cued?
- Basis in paper: [explicit] The authors highlight the concept of "spontaneous ToM," noting that unlike humans who automatically attribute mental states, LLMs often require explicit prompting strategies (like chain-of-thought) to perform ToM reasoning.
- Why unresolved: It is currently unclear if a general bias toward paying attention to agents or mental states can be induced in models, or if ToM in AI is fundamentally dependent on specific prompt engineering.
- What evidence would resolve it: Demonstrations of models successfully performing ToM tasks in zero-shot settings without specialized prompting, or the development of training objectives that induce a bias toward agency detection.

## Limitations

- The paper presents a conceptual framework rather than empirical findings, lacking direct validation of its claims
- The "training away" threat remains speculative without direct evidence that model developers are using adversarial test items to update their models
- Claims about auxiliary task demands confounding adversarial evaluations are based on theoretical reasoning rather than systematic empirical demonstration

## Confidence

- **High confidence**: The conceptual distinction between behavior-matching and computation-matching is clear and well-articulated. The theoretical framework identifying threats to construct validity (training away, auxiliary demands) is logically coherent.
- **Medium confidence**: The claim that current evaluations conflate behavior and computation is supported by citation patterns but not empirically demonstrated. The assertion that adversarial evaluations introduce confounds is reasonable but untested.
- **Low confidence**: The extent to which training away actually occurs in practice remains unknown without access to model training data and development practices.

## Next Checks

1. **Frozen model validation**: Test identical ToM items on an open-weight model version frozen for 6+ months versus the same model post-updates to empirically demonstrate training away effects.
2. **Controlled adversarial testing**: For each adversarial ToM modification, create matched control conditions testing only the auxiliary demand (e.g., physical reasoning) to isolate whether failures reflect ToM deficits or auxiliary confounds.
3. **Human baseline establishment**: Run the same adversarial ToM items with human participants to empirically determine whether humans are actually at ceiling on these tasks, as the paper claims but does not verify.