---
ver: rpa2
title: Improving Dense Passage Retrieval with Multiple Positive Passages
arxiv_id: '2508.09534'
source_url: https://arxiv.org/abs/2508.09534
tags:
- positive
- passages
- training
- question
- passage
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the impact of incorporating multiple positive
  passages per question during training of the Dense Passage Retrieval (DPR) model.
  While existing DPR variants typically use one positive passage, this work investigates
  whether providing more positive examples improves retrieval performance.
---

# Improving Dense Passage Retrieval with Multiple Positive Passages

## Quick Facts
- arXiv ID: 2508.09534
- Source URL: https://arxiv.org/abs/2508.09534
- Authors: Shuai Chang
- Reference count: 9
- One-line primary result: DPR with multiple positive passages consistently outperforms original DPR, achieving better top-20 and top-100 retrieval accuracy even with smaller batch sizes.

## Executive Summary
This paper investigates whether training Dense Passage Retrieval (DPR) models with multiple positive passages per question can improve retrieval accuracy compared to the standard single-positive approach. The authors reformulate the training objective as a binary classification task using binary cross-entropy (BCE) loss, which naturally accommodates multiple positives by treating each passage independently. Experiments on five QA datasets demonstrate that this approach consistently improves top-k retrieval accuracy, particularly for top-20 and top-100 results. Notably, the method enables training on a single 16GB GPU by using smaller batch sizes without sacrificing performance, making DPR more accessible.

## Method Summary
The method reformulates DPR training as a binary classification task using BCE loss instead of the standard NLL loss. For each question, the model retrieves up to 3 positive passages and 1 hard negative, then applies in-batch negatives during training. The similarity function uses softmax-scaled inner products between [CLS] embeddings from dual BERT encoders. This approach treats each passage independently, allowing the model to learn from multiple positives without competition among them. Training uses batch size 16 on a single 16GB GPU, with Adam optimizer (lr=1e-5), linear warmup, and dropout 0.1 for 40-100 epochs depending on dataset size.

## Key Results
- DPR with multiple positive passages consistently improves top-20 and top-100 retrieval accuracy across all five tested datasets
- The approach achieves better performance even when using significantly smaller batch sizes (B=16) compared to original DPR (B=128)
- Hardware efficiency enables training on a single 16GB GPU instead of requiring 8×32GB GPUs
- Performance gains are most pronounced when questions have sufficient positive passages available

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Increasing the proportion of positive passages during training improves the model's ability to recognize relevant content at inference time.
- **Mechanism:** Standard DPR training creates extreme positive-negative imbalance (roughly 1:2B ratio). By pairing each question with m>1 positive passages and applying in-batch negatives, the ratio improves to approximately 1:B—doubling the positive proportion. This provides stronger positive signal during gradient updates, reducing the risk that the model learns to simply reject most passages rather than identify relevant ones.
- **Core assumption:** The model's retrieval failures partially stem from over-exposure to negatives during training, which erodes positive-class recognition. Assumption: the benefit scales with the availability of genuinely relevant passages per query.
- **Evidence anchors:**
  - [abstract] "equipping each question with multiple positive passages consistently improves retrieval accuracy"
  - [section 2.2] "the ratio of positive to negative passages is approximately 1/B, doubling the proportion compared to the original DPR model"
  - [corpus] Weak direct evidence—neighbor papers do not examine multi-positive training effects.
- **Break condition:** If the dataset has insufficient positive passages per question (e.g., SQuAD with only 49.1% of questions having 3 positives), the mechanism's benefit degrades. Performance may also plateau or degrade if positives are noisy or marginally relevant.

### Mechanism 2
- **Claim:** Reformulating retrieval training as binary classification with BCE loss enables stable optimization with multiple positives per query.
- **Mechanism:** Unlike NLL loss (which normalizes over a single positive against all negatives in a softmax), BCE treats each passage independently—optimizing σ(sim(q,p)) for positives and 1−σ(sim(q,p)) for negatives. This decouples the loss across multiple positive passages, allowing the model to learn from each without competition among positives. Softmax-scaling of the inner product and sigmoid stabilization are applied to maintain training stability.
- **Core assumption:** BCE's independent treatment of positives is compatible with the retrieval objective, and softmax-scaling prevents numerical instability. Assumption: the benefit is primarily from multi-positive exposure, not loss function change per se.
- **Evidence anchors:**
  - [section 2.2] "we discard the NLL loss formulated in Eq. (2) and instead use binary cross-entropy (BCE) loss"
  - [section 4.2] "the difference between DPR-Single and DPR+1 is relatively small" suggesting BCE alone is not the primary driver
  - [corpus] No comparable BCE-vs-NLL ablations found in neighbor papers.
- **Break condition:** If positives are not truly independent (e.g., highly redundant passages), BCE may over-weight near-duplicate signal. If similarity scores are poorly scaled, sigmoid saturation could slow learning.

### Mechanism 3
- **Claim:** Smaller batch sizes can achieve competitive or better accuracy when multi-positive training compensates for reduced in-batch negative diversity.
- **Mechanism:** Original DPR relies on large batches (e.g., B=128) to generate many in-batch negatives. With multiple positives per question, each training step provides more positive signal, partially offsetting reduced negative diversity from smaller batches. This enables training on a single 16GB GPU without significant performance loss.
- **Core assumption:** The positive signal gain from multiple passages meaningfully compensates for reduced negative sampling diversity. Assumption: hardware-constrained scenarios benefit most; larger batches may still offer marginal gains.
- **Evidence anchors:**
  - [abstract] "even when using a significantly smaller batch size, which enables training on a single GPU"
  - [section 4.1] "training the original DPR model with a batch size of 128 requires 8 × 32 GB GPUs"
  - [corpus] No neighbor papers directly validate batch-size vs. multi-positive trade-offs.
- **Break condition:** If the corpus is extremely large or diverse, small-batch in-batch negatives may be insufficiently challenging, limiting generalization. The paper notes effects of larger batches or more positives remain unexplored.

## Foundational Learning

- **Concept:** Dual encoder architecture (bi-encoder)
  - **Why needed here:** DPR uses separate BERT encoders for questions and passages, producing dense vectors compared via inner product. Understanding this separation is essential to grasp how in-batch negatives and multi-positive training operate.
  - **Quick check question:** Given a batch of B question-passage pairs, how many implicit negative pairs does a dual encoder create via in-batch negatives?

- **Concept:** In-batch negatives training
  - **Why needed here:** The efficiency of DPR hinges on treating other questions' passages as negatives. This mechanism determines the positive-negative ratio and explains why batch size strongly influences original DPR performance.
  - **Quick check question:** If batch size is 16 and each question has 3 positives and 1 hard negative, approximately how many negatives does each question see?

- **Concept:** Negative log-likelihood (NLL) vs. binary cross-entropy (BCE) loss
  - **Why needed here:** The paper switches from NLL (softmax over one positive vs. all negatives) to BCE (independent binary labels). Understanding the difference clarifies why BCE accommodates multiple positives naturally.
  - **Quick check question:** For a query with 3 positive passages and 100 negatives, how does NLL vs. BCE differ in how positive passages compete or coexist in the loss?

## Architecture Onboarding

- **Component map:**
  - Question encoder BERT: tokenized query → [CLS] embedding → dense vector q
  - Passage encoder BERT: tokenized passage → [CLS] embedding → dense vector p
  - Similarity function: sim(q,p) = q⊤p (softmax-scaled in this work)
  - Loss layer: BCE with sigmoid, applied independently per (q,p) pair
  - Training batch constructor: for each query, retrieve up to m positives + 1 hard negative; apply in-batch negatives across the batch

- **Critical path:**
  1. Data prep: For each question, pre-identify up to 3 positive passages (using answer span matching as in original DPR) plus 1 hard negative.
  2. Batch assembly: Build batches of size 16; each sample includes question, multiple positives, one hard negative.
  3. Forward pass: Encode all questions and passages through respective BERT encoders.
  4. Similarity computation: Compute softmax-scaled inner products for all (q,p) pairs in batch.
  5. Loss computation: Apply BCE—sum log σ(sim) for positives and log(1−σ(sim)) for negatives.
  6. Backprop: Update both encoders jointly.

- **Design tradeoffs:**
  - More positives per query improves accuracy but requires datasets with sufficient multi-positive annotations.
  - Smaller batches reduce hardware requirements but may limit negative diversity; monitor for underfitting on diverse corpora.
  - BCE decouples positives but may introduce redundancy if positives are near-duplicates—consider deduplication.
  - Hard negative mining is still valuable; do not rely solely on in-batch negatives for challenging contrast.

- **Failure signatures:**
  - Low performance on datasets with few multi-positive questions (e.g., SQuAD at 49.1% with 3 positives)—audit positive passage availability per query before training.
  - Training instability or saturated loss—check softmax scaling and sigmoid output distribution; consider learning rate warmup.
  - Overfitting to seen passages—validate on held-out queries and passages; ensure hard negatives are not trivially discriminable.

- **First 3 experiments:**
  1. Reproduce ablation on TREC with m=1,2,3 positives at batch size 16 to confirm progressive improvement and isolate BCE effect.
  2. Train on NQ or TriviaQA with m=3 at batch size 16; compare top-20 and top-100 accuracy against original DPR baseline to validate hardware-efficient gains.
  3. Audit your own dataset for multi-positive availability; if δ (proportion with 3+ positives) is below ~0.6, measure performance gap and test data augmentation to synthesize additional positives.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What is the optimal trade-off between batch size and the number of positive passages when aiming for maximum retrieval accuracy?
- **Basis in paper:** [explicit] The authors explicitly state in the "Limitations" section that "further investigation is required to understand the trade-off between batch size and the number of positive passages for optimal performance."
- **Why unresolved:** The experiments were restricted to a small batch size of 16 due to hardware constraints (single 16GB GPU), preventing a comprehensive analysis of how larger batch sizes interact with multiple positive passages.
- **What evidence would resolve it:** A grid search experimental result varying batch sizes (e.g., 16, 32, 128) against varying numbers of positive passages (e.g., 1, 3, 5, 10) on a high-memory setup.

### Open Question 2
- **Question:** Does increasing the number of positive passages beyond three yield diminishing returns or continued performance gains?
- **Basis in paper:** [explicit] The paper notes in the "Limitations" section that "The effects of using more positive passages... remain unexplored" because the authors were only able to train with up to three positive passages.
- **Why unresolved:** Hardware limitations restricted the maximum count to three. It is unknown if the upward trend in accuracy observed from 1 to 3 passages plateaus or continues linearly.
- **What evidence would resolve it:** Experiments training the DPR+ model with $m > 3$ positive passages (e.g., 5, 10, 20) to plot the accuracy curve relative to the increase in positive signal.

### Open Question 3
- **Question:** Can the performance drop on the SQuAD dataset be recovered through data augmentation to artificially increase positive passage counts?
- **Basis in paper:** [inferred] In Section 4.1, the authors attribute the poor performance on SQuAD to "data deficiency," noting that only 49.1% of questions had the maximum 3 positive passages (Table 3). They imply this sparsity hinders the model.
- **Why unresolved:** The paper identifies the correlation between low positive passage counts and low accuracy but does not test methods to alleviate this data sparsity.
- **What evidence would resolve it:** An experiment applying techniques like back-translation or paraphrase generation to synthetically increase the positive passage count for SQuAD samples, followed by re-training and evaluation.

## Limitations
- The approach requires datasets with sufficient multi-positive passages per question to realize full benefits
- Hardware constraints limited experiments to batch size 16 and maximum 3 positive passages
- BCE loss may over-weight redundant passages if positives are not sufficiently diverse
- Performance on SQuAD degraded due to data sparsity (only 49.1% of questions had 3 positives)

## Confidence
- **High confidence**: The experimental results showing consistent improvements in top-20 and top-100 retrieval accuracy with multi-positive training are robust and well-documented. The claim that this enables training on a single 16GB GPU is also well-supported.
- **Medium confidence**: The mechanism by which BCE loss enables stable optimization with multiple positives is plausible, but the paper does not directly isolate the effect of BCE versus the number of positives. The claim that hardware-constrained scenarios benefit most is reasonable but not empirically tested.
- **Low confidence**: The paper does not fully address how performance degrades when multi-positive availability is low, nor does it explore the effects of alternative loss functions or batch sizes beyond the reported settings.

## Next Checks
1. **Ablation study on BCE vs. NLL**: Run a controlled experiment on TREC or WebQ, training DPR with m=3 positives using both BCE and NLL losses (with appropriate modifications for NLL), to isolate the contribution of the loss function versus the number of positives.
2. **Dataset availability audit**: For each dataset, calculate the proportion of questions with 3+ positive passages. Then, measure retrieval performance for queries with fewer positives to quantify the impact of multi-positive availability on accuracy.
3. **Batch size and negative diversity analysis**: Train DPR with m=3 positives at batch sizes B=16, 32, and 64. Compare top-20/100 accuracy and in-batch negative diversity (e.g., average negative score distribution) to determine if gains plateau or if larger batches offer additional benefits.