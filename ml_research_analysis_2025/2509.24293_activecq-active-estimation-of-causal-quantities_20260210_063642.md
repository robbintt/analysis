---
ver: rpa2
title: 'ActiveCQ: Active Estimation of Causal Quantities'
arxiv_id: '2509.24293'
source_url: https://arxiv.org/abs/2509.24293
tags:
- treatment
- acquired
- causal
- amse
- cate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of efficiently estimating causal
  quantities (CQs) like CATE, ATE, ATT, and ATEDS, which typically require large datasets
  that can be costly to obtain. The core method idea is a unified Bayesian framework
  that models the regression function with a Gaussian Process and represents the target
  distribution using conditional mean embeddings (CMEs) in a reproducing kernel Hilbert
  space.
---

# ActiveCQ: Active Estimation of Causal Quantities

## Quick Facts
- arXiv ID: 2509.24293
- Source URL: https://arxiv.org/abs/2509.24293
- Reference count: 40
- One-line primary result: Active estimation framework that significantly improves sample efficiency for causal quantities like CATE, ATE, ATT, and ATEDS by minimizing posterior uncertainty in the estimator rather than model uncertainty.

## Executive Summary
This paper addresses the challenge of efficiently estimating causal quantities (CQs) from observational data, where traditional methods require large, costly datasets. The authors propose ActiveCQ, a unified Bayesian framework that leverages Gaussian Processes and Conditional Mean Embeddings (CMEs) to actively select data points that most effectively reduce uncertainty in the causal quantity estimator. By targeting the specific target distribution relevant to the causal query rather than just minimizing model variance, the method demonstrates substantial gains in sample efficiency across various causal quantities, particularly in scenarios with significant distribution shifts.

## Method Summary
The ActiveCQ framework models the regression function with a Gaussian Process and represents the target distribution using conditional mean embeddings (CMEs) in a reproducing kernel Hilbert space. This allows for principled derivation of acquisition strategies based on reducing posterior uncertainty in the CQ estimator. The method instantiates this via information gain and total variance reduction, selecting data points that minimize uncertainty specifically for the causal quantity of interest rather than general model uncertainty. The framework is validated through experiments on synthetic and semi-synthetic datasets, showing significant improvements over traditional baselines in sample efficiency.

## Key Results
- ActiveCQ achieves substantial AMSE reductions compared to traditional baselines across all tested causal quantities
- The CME-based approach is particularly effective for handling high-dimensional data and adapting to learned feature spaces
- The method demonstrates significant advantages in scenarios with distribution shifts between pool and target populations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Aligning the active learning objective with the causal query reduces estimation error more effectively than minimizing model uncertainty alone.
- Mechanism: Instead of selecting data to reduce the variance of the regression function $f$, the system selects data to minimize the posterior variance of the Causal Quantity (CQ) estimator $\hat{\tau}$ itself. This targets the specific "transductive" distribution (e.g., $P(s|z)$) relevant to the query, handling distribution shifts between the pool data and the target population.
- Core assumption: The target distribution for the causal query is estimable from the available covariates.
- Evidence anchors:
  - [abstract]: "Active estimation of Causal Quantities... by minimizing the posterior uncertainty of the target estimator."
  - [section]: Section 5 Key Principle: "Select a subset... in a manner that minimizes the posterior uncertainty of the estimator."
  - [corpus]: Neighbor "Causal-EPIG" also identifies objective mismatch in standard AL for CATE.
- Break condition: If the target distribution exactly matches the pool distribution, the advantage over standard variance reduction vanishes.

### Mechanism 2
- Claim: Conditional Mean Embeddings (CMEs) enable closed-form, analytic computation of the integrals required for causal estimation.
- Mechanism: The framework represents conditional distributions (e.g., $P(s|z)$) as elements in a Reproducing Kernel Hilbert Space (RKHS) using CMEs. This converts the intractable integral defining the CQ into an inner product involving the GP feature map and the CME. This yields an analytic posterior distribution for the estimator, avoiding costly Monte Carlo integration during acquisition.
- Core assumption: The chosen kernels effectively capture the structure of the covariates, treatment, and outcomes.
- Evidence anchors:
  - [section]: Section 4.2 "Conditional Mean Embeddings... allows the CATE estimation... to be treated as a standard prediction problem."
  - [section]: Proposition 1 provides the closed-form mean and covariance for the estimator.
- Break condition: In very high-dimensional settings where kernel methods struggle to capture complex manifold structures.

### Mechanism 3
- Claim: Joint uncertainty quantification from the outcome model and the target distribution improves robustness.
- Mechanism: The Bayesian framework propagates uncertainty from both the GP regression (noise in outcomes) and the CME estimation (uncertainty in the conditional distribution). This ensures the acquisition strategy accounts for the uncertainty in *where* to integrate, not just the uncertainty in the outcome values.
- Core assumption: The regularization parameter $\lambda$ effectively balances the estimation error and complexity of the CME.
- Evidence anchors:
  - [section]: Section 4.2 mentions "regularization of the CME" and noise term.
  - [section]: Section 5.3 provides Theorem 2 bounding the marginal variance based on irreducible uncertainty and information gain.
- Break condition: If the CME estimation uncertainty is ignored, the framework may overestimate confidence in low-data regions of the conditional space.

## Foundational Learning

- **Causal Quantities (CATE, ATE, ATT, DS)**
  - Why needed here: The system is a general estimator; understanding the difference between Conditional Average Treatment Effect (CATE), Average Treatment Effect on the Treated (ATT), and ATE under Distribution Shift (DS) defines the specific target distribution the active learner must chase.
  - Quick check question: Does ATE estimation require estimating $P(s|z)$ like CATE does? (No, ATE marginalizes over the whole population).

- **Reproducing Kernel Hilbert Space (RKHS) & Kernels**
  - Why needed here: The mathematical machinery relies on "kernelizing" everythingâ€”turning distributions into vectors (embeddings) and functions into inner products. You cannot understand the "closed-form" solution without grasping the "kernel trick."
  - Quick check question: What is the benefit of representing a distribution $P(X)$ as a mean embedding $\mu$ in an RKHS? (It allows linear operations on distributions).

- **Gaussian Processes (GP)**
  - Why needed here: GPs provide the Bayesian regression backbone, offering not just a prediction for the outcome $y$, but a full posterior distribution required to calculate the "estimator uncertainty."
  - Quick check question: How does a GP handle the trade-off between fitting observed data and maintaining uncertainty? (Via the covariance kernel and noise hyperparameters).

## Architecture Onboarding

- **Component map:** Pool Dataset $D_P$ (unlabeled) -> Initial Training Set $D_T$ (labeled) -> Gaussian Process (for outcome $y$) and CME Module (for target distributions $P(s|z)$) -> Unified Bayesian Estimator $\hat{\tau}$ (Gaussian) -> Utility Function $U(X_B)$ (Entropy/Variance of $\hat{\tau}$) -> Greedy Batch Selection

- **Critical path:**
  1. Define Query: Specify the target CQ (e.g., CATE at $z=z_0$)
  2. Update Models: Train GP on current $D_T$; compute/update CMEs for the target distribution
  3. Compute Posterior: Derive the Gaussian posterior for the target CQ $\hat{\tau}$ (Prop 1)
  4. Score Pool: Calculate utility $U$ (reduction in estimator variance) for every point in $D_P$
  5. Select Batch: Greedily pick $n_b$ points maximizing marginal utility gain

- **Design tradeoffs:**
  - CME vs. Monte Carlo (MC): CME is analytic and faster but depends on kernel quality; MC (using MDN) is more flexible for complex densities but slower/noisier. (Paper shows CME generally wins).
  - Greedy vs. Top-b: Greedy selection ensures diversity in the batch but is computationally slower ($O(n_b \times |D_P|)$). Top-b is faster but may select redundant points.

- **Failure signatures:**
  - Kernel Mismatch: If RBF kernels fail to capture complex data geometry, both GP and CME fail.
  - No Distribution Shift: If pool == target, standard "Variance Reduction" performs just as well, making the complex framework unnecessary overhead.

- **First 3 experiments:**
  1. CATE Visualization: Run the visualization dataset (2D adjustment variables). Plot the acquired points (Fig 3) to visually confirm the learner is sampling from the *target* density (skewed distribution) rather than the uniform pool.
  2. Comparison against BALD: Run the simulation dataset for CATE. Compare ActiveCQ (CME) against standard BALD/Coresets. Verify significant AMSE reduction, especially in fixed-treatment scenarios (high distribution shift).
  3. Ablation on Batch Selection: Compare "Greedy" vs "Top-b" selection strategies on the simulation data to quantify the trade-off between diversity (Greedy) and speed (Top-b).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the ActiveCQ framework be extended to actively estimate causal quantities in the presence of hidden confounding using instrumental variables or proxy methods?
- Basis in paper: [explicit] The Conclusion states the framework can be extended to "complex causal settings involving hidden confounders or instrumental variables," and Appendix F.1 explicitly lists "Relaxing Identification Assumptions" as a limitation.
- Why unresolved: The current theoretical derivation relies on the backdoor criterion (adjustment via $S, Z$), which is invalid when unmeasured confounders exist, requiring a fundamental change to the identification and estimation strategy.
- What evidence would resolve it: A derivation of new acquisition utility functions based on IV or proxy identification formulas, validated on datasets known to contain hidden confounding.

### Open Question 2
- Question: Does constructing a validation set from the target distribution, rather than the observational distribution, significantly improve the hyperparameter optimization of the GP during the active learning loop?
- Basis in paper: [explicit] Appendix E.2.1 states that the current use of observational data for validation represents a "mismatch" with the goal of minimizing uncertainty over the target distribution, suggesting that an ideal validation set would "mimic the new target distribution."
- Why unresolved: The authors implemented the simpler observational validation approach, noting that constructing a target-specific validation set with limited data could lead to "insufficient data for validation."
- What evidence would resolve it: Ablation studies comparing model performance using standard validation sets against importance-weighted validation sets constructed to reflect the target causal quantity's distribution.

### Open Question 3
- Question: Can the convergence guarantees and uncertainty quantification of the ActiveCQ framework be preserved when replacing the Gaussian Process with a Bayesian Neural Network (BNN) to improve scalability?
- Basis in paper: [explicit] Appendix F.1 suggests replacing the GP with a BNN to address the "cubic complexity" limitation, noting that BNNs offer "superior scalability" while still providing uncertainty quantification.
- Why unresolved: The paper's theoretical convergence proof (Theorem 2) relies on specific GP properties like submodularity and closed-form posteriors, properties which are not guaranteed or are harder to verify for BNNs.
- What evidence would resolve it: Theoretical analysis of the information gain properties of BNNs in this context, alongside empirical results on large-scale datasets comparing ActiveCQ's performance using BNNs versus GPs.

## Limitations
- The framework's advantages are most pronounced under significant distribution shifts between pool and target populations; gains diminish when distributions align
- Computational cost scales with pool size due to greedy batch selection, though top-b provides a faster alternative
- The method relies on kernel quality for both GP and CME components, which may limit performance in very high-dimensional or complex data scenarios

## Confidence
- **High**: The unified Bayesian framework for causal quantity estimation and the superiority of CME-based methods over MDN in standard settings
- **Medium**: The generalization across all four causal quantities given the reliance on kernel methods
- **Low**: The absolute robustness in extreme high-dimensional or highly non-linear cases without deep kernel learning

## Next Checks
1. Test the framework on a real-world high-dimensional dataset (e.g., from healthcare) to assess kernel method limitations
2. Conduct an ablation study isolating the impact of CME uncertainty quantification on acquisition performance
3. Evaluate the computational scalability with increasing pool size to quantify the trade-off between greedy and top-b selection in large-scale applications