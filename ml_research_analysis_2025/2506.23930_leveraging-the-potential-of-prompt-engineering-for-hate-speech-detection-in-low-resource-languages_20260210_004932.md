---
ver: rpa2
title: Leveraging the Potential of Prompt Engineering for Hate Speech Detection in
  Low-Resource Languages
arxiv_id: '2506.23930'
source_url: https://arxiv.org/abs/2506.23930
tags:
- hate
- speech
- prompting
- language
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of hate speech detection in
  low-resource languages, focusing on Bengali. The authors propose a novel metaphor
  prompting technique to circumvent built-in safety mechanisms of large language models
  (LLMs) for improved hate speech classification.
---

# Leveraging the Potential of Prompt Engineering for Hate Speech Detection in Low-Resource Languages

## Quick Facts
- arXiv ID: 2506.23930
- Source URL: https://arxiv.org/abs/2506.23930
- Authors: Ruhina Tabasshum Prome; Tarikul Islam Tamiti; Anomadarshi Barua
- Reference count: 40
- One-line primary result: Metaphor prompting achieves 95.89% F1 on Bengali hate speech detection, surpassing traditional models and other prompting strategies.

## Executive Summary
This paper addresses hate speech detection in low-resource languages, focusing on Bengali, by proposing a novel metaphor prompting technique to bypass LLM safety guardrails. The authors investigate six prompting strategies (zero-shot, refusal suppression, flattering, multi-shot, role prompting, and metaphor prompting) on Llama2-7B, comparing performance with three deep learning models across four datasets. Metaphor prompting achieves the highest F1 scores, particularly for Bengali, by substituting emotionally charged trigger words with neutral metaphor pairs. The study also considers environmental impact factors, suggesting metaphor prompting offers a sustainable approach to hate speech detection in low-resource settings.

## Method Summary
The authors fine-tune Llama2-7B-chat-hf using LoRA for causal language modeling on a multilingual hate speech dataset (Bengali, Hindi, English, German) with 500 samples per language. Inputs in Bengali, Hindi, and German are first translated to English via Google Translate. At inference, metaphor prompts substitute "hate speech" terminology with neutral pairs (e.g., "summer-winter") to bypass safety guardrails. The model generates text which is parsed for keywords indicating classification. Performance is compared against MLP, CNN, and BiGRU models using GloVe, Word2Vec, and FastText embeddings.

## Key Results
- Metaphor prompting achieves 95.89% F1 on Bengali dataset, outperforming all other prompting strategies and traditional models
- Summer-Winter metaphor pair delivers best performance across languages, while red-green shows higher refusal rates
- BiGRU+FastText achieves highest accuracy (94.44%) on English but metaphor prompting performs comparably or better on Bengali, Hindi, and German
- Metaphor prompting demonstrates competitive environmental sustainability compared to fine-tuning approaches

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Substituting emotionally charged trigger words with neutral metaphor pairs circumvents LLM safety guardrails while preserving classification capability.
- **Mechanism**: Llama2-7B is fine-tuned to refuse requests containing terms like "hate speech." The paper shows that replacing "hate/not hate" with metaphor pairs such as "red/green," "rose/thorn," or "summer/winter" avoids triggering these guardrails, allowing the model to process the classification task. The model maps the metaphorical labels to the underlying semantic distinction during inference.
- **Core assumption**: The model can generalize from abstract metaphorical labels to perform binary hate speech detection without explicit category names.
- **Evidence anchors**:
  - [abstract]: "The novel metaphor prompting approach substitutes emotionally charged terms with neutral metaphors to bypass LLM safety mechanisms."
  - [Page 6, Section V.B]: "The more the word 'hate' is used, the less likely the model is to bypass its ethical constraints. This behavior stems from the model's built-in ethical safeguards, which are triggered by the presence of such a sensitive term."
  - [Page 10, Table VIII]: "Summer-Winter" metaphor achieves 95.89% F1 on Bengali, up from 73.36% without metaphor prompting.
- **Break condition**: If metaphors are too culturally specific or abstract, the model may fail to establish the mapping between metaphor and classification target.

### Mechanism 2
- **Claim**: Chain-of-translation enables cross-lingual transfer by converting low-resource language inputs to English before classification.
- **Mechanism**: Bengali, Hindi, and German texts are translated to English via Google Translate before being processed by Llama2-7B. This leverages the model's stronger English comprehension capabilities while introducing translation noise as a tradeoff.
- **Core assumption**: Translation preserves sufficient semantic information for hate speech detection despite potential loss of linguistic nuance.
- **Evidence anchors**:
  - [Page 4, Section IV.C.1]: "The Bengali, Hindi, and German datasets are first translated into English by Google Translator due to Llama-2's inability to comprehend the semantic content of texts in these languages."
  - [Page 4]: "the model exhibits a greater understanding of the English language than of other languages."
  - [corpus]: Limited direct evidence—related work on cross-lingual hate speech detection shows mixed results for translation-based approaches.
- **Break condition**: Culturally specific hate speech terms, code-mixing, or idiomatic expressions may lose meaning or be mistranslated.

### Mechanism 3
- **Claim**: LoRA-based parameter-efficient fine-tuning enables multilingual adaptation without full model retraining.
- **Mechanism**: Low-Rank Adaptation adds trainable low-rank matrices to transformer layers, allowing Llama2-7B to adapt to hate speech classification across Bengali, Hindi, English, and German using only 500 subsampled examples per language.
- **Core assumption**: A unified multilingual fine-tuning approach can generalize across typologically diverse languages with limited data.
- **Evidence anchors**:
  - [Page 5, Section IV.C.3]: "The LoRA (Low-Rank Adaptation) configuration is defined to further customize the model for causal language modeling."
  - [Page 5]: "We subsample 500 entries from the combined dataset and provide the subsampled data to Llama2-7B to fine-tune it for hate speech classification tasks in multilingual settings."
  - [corpus]: Related work on PEFT for low-resource languages supports feasibility, but specific hate speech performance varies.
- **Break condition**: If combined dataset doesn't adequately represent all languages, performance may be uneven (observed: German F1 lower than Bengali).

## Foundational Learning

- **Concept: LLM Safety Guardrails and Refusal Behavior**
  - **Why needed here**: Understanding why direct hate speech classification prompts trigger refusals is prerequisite to designing bypass strategies like metaphor prompting.
  - **Quick check question**: What happens when you prompt Llama2-7B with "Classify this text as hate speech or not hate speech" without any modifications?

- **Concept: Parameter-Efficient Fine-Tuning (LoRA)**
  - **Why needed here**: The architecture uses LoRA to adapt a 7B-parameter model on limited data; understanding rank, alpha, and target modules is essential for replication.
  - **Quick check question**: What are the key hyperparameters in LoRA configuration, and how do they differ from full fine-tuning?

- **Concept: Cross-Lingual Transfer via Translation**
  - **Why needed here**: The chain-of-translation approach assumes semantic preservation; engineers must understand its limitations for deployment.
  - **Quick check question**: Which types of hate speech signals (slurs, coded language, cultural references) are most likely to degrade in machine translation?

## Architecture Onboarding

- **Component map**:
  Input Text → Translation (Google Translate) → LlamaTokenizer → Llama2-7B-chat-hf (LoRA fine-tuned) → Metaphor Prompt Template → Text Generation (temp=0, top_k=10) → Output Parsing (keyword extraction) → Binary Classification (0=NH, 1=HS)

- **Critical path**:
  1. Load Llama2-7B-chat-hf with bfloat16 quantization and device_map="auto"
  2. Configure LoRA adapters for causal language modeling
  3. Fine-tune on combined multilingual dataset (500 samples per language)
  4. At inference: translate input → apply metaphor prompt → generate → parse output

- **Design tradeoffs**:
  - Accuracy vs. environmental cost: Metaphor prompting achieves highest F1 but has higher IF (impact factor) than BiGRU/CNN/MLP baselines
  - Metaphor selection is language-dependent: "summer-winter" best for Bengali (95.89%), "rose-thorn" better for English/Hindi
  - Sample size limited to 500 per language due to computational constraints

- **Failure signatures**:
  - Refusal loop: Model returns "I cannot classify..." if metaphor is insufficiently distinct from trigger terms
  - Class bias: Early experiments showed bias toward "hate speech" class when definition order was not randomized
  - Translation artifacts: Code-mixed or slang-heavy inputs may produce unreliable classifications

- **First 3 experiments**:
  1. Run zero-shot prompt without metaphor on 100 Bengali samples to establish refusal baseline and measure IF
  2. Ablate metaphor pairs ("red-green," "rose-thorn," "summer-winter") on same 100 samples to identify optimal metaphor for target language
  3. Compare metaphor prompting F1 and IF against BiGRU+FastText baseline on full Bengali test set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does metaphor prompting generalize effectively to other large language models beyond Llama2-7B, particularly those with different safety training or architectural designs?
- Basis in paper: [explicit] The study evaluates all prompting strategies exclusively on Llama2-7B and does not test transferability to other LLMs.
- Why unresolved: Safety mechanisms and tokenization strategies differ across LLMs, and metaphor prompting may interact unpredictably with different model architectures.
- What evidence would resolve it: Comparative experiments applying identical metaphor prompting to multiple LLMs (e.g., GPT, Mistral, Claude) on the same multilingual hate speech datasets.

### Open Question 2
- Question: Can a systematic methodology be developed for selecting optimal metaphor pairs, rather than relying on empirical trial-and-error?
- Basis in paper: [inferred] The paper tests four metaphor pairs with varying success ("summer-winter" achieves 95.89% F1 for Bengali vs. "honey-venom" at 91.17%) but offers no theoretical framework for metaphor selection.
- Why unresolved: The performance differences suggest metaphor choice matters significantly, yet the paper provides no principled approach to predicting which metaphors will work best.
- What evidence would resolve it: Controlled ablation studies varying metaphor characteristics (semantic neutrality, emotional distance, cross-cultural recognizability) to identify predictive factors for jailbreaking effectiveness.

### Open Question 3
- Question: How significant is the accuracy degradation from the translation pipeline, and can it be quantified relative to native multilingual processing?
- Basis in paper: [explicit] The authors acknowledge that translation adds noise and loses linguistic features, but do not measure this impact systematically.
- Why unresolved: The chain-of-translation approach may introduce systematic errors in hate speech detection, particularly for culturally-specific slurs or idioms that lack direct equivalents.
- What evidence would resolve it: Ablation experiments comparing translation-based classification against human-translated gold standards, or analysis of specific linguistic features lost during machine translation.

### Open Question 4
- Question: Is metaphor prompting robust to adversarial manipulation once the metaphor-hate speech mapping becomes known to bad actors?
- Basis in paper: [inferred] The paper treats metaphor prompting as a detection tool but does not consider whether adversaries could craft hate speech that avoids triggering the metaphor-based classification.
- Why unresolved: Jailbreaking techniques may be reversible or exploitable if users learn to phrase hate speech in ways that bypass the metaphor-based filtering.
- What evidence would resolve it: Red-team adversarial testing where attackers attempt to craft hate speech that evades metaphor-based detection while remaining comprehensible to human readers.

## Limitations

- Translation Quality and Cross-Lingual Transfer: Reliance on Google Translate introduces potential semantic loss for culturally specific hate speech terms, code-mixing, and idiomatic expressions without validation of translation accuracy.
- Metaphor Prompting Robustness: Optimal metaphor selection is language-dependent and may not generalize; the paper does not test metaphor robustness to linguistic variation, cultural context, or adversarial examples.
- Environmental Impact: Claims of sustainability lack detailed lifecycle analysis; reported Impact Factor values suggest higher computational cost than traditional models.

## Confidence

**High Confidence**:
- Metaphor prompting circumvents LLM safety guardrails by replacing emotionally charged terms with neutral metaphors
- LoRA-based fine-tuning enables multilingual adaptation with limited data
- Metaphor prompting achieves comparable or superior F1 scores to traditional models on Bengali, English, Hindi, and German datasets

**Medium Confidence**:
- Cross-lingual transfer via Google Translate preserves sufficient semantic information for hate speech detection
- Metaphor prompting is more sustainable than full fine-tuning (based on limited IF comparisons)

**Low Confidence**:
- Metaphor prompting generalizes across typologically diverse low-resource languages without language-specific tuning
- Translation-based cross-lingual transfer is robust to cultural and linguistic variation

## Next Checks

1. **Translation Quality Validation**: Manually evaluate the quality of Google Translate outputs for 100 Bengali, Hindi, and German hate speech examples. Measure how translation errors correlate with classification errors to quantify the impact of cross-lingual transfer on model performance.

2. **Metaphor Robustness Testing**: Test metaphor prompting on adversarial examples, including culturally specific slurs, code-mixed texts, and ambiguous hate speech. Compare performance across metaphor pairs ("red-green," "rose-thorn," "summer-winter") to identify the most robust metaphor for each language.

3. **Environmental Impact Lifecycle Analysis**: Conduct a detailed lifecycle analysis of metaphor prompting vs. traditional models (BiGRU, CNN, MLP) across the full training and inference pipeline. Include hardware utilization, energy consumption, and carbon footprint to validate sustainability claims.