---
ver: rpa2
title: 'Skewed Memorization in Large Language Models: Quantification and Decomposition'
arxiv_id: '2502.01187'
source_url: https://arxiv.org/abs/2502.01187
tags:
- memorization
- training
- data
- npre
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the problem of skewed memorization in Large
  Language Models (LLMs), where a small subset of training data contributes disproportionately
  to memorization risk. The core method involves analyzing memorization probabilities
  over sequence lengths and decomposing term-wise probabilities in the LLM generation
  process to understand how data characteristics influence memorization likelihood.
---

# Skewed Memorization in Large Language Models: Quantification and Decomposition

## Quick Facts
- arXiv ID: 2502.01187
- Source URL: https://arxiv.org/abs/2502.01187
- Reference count: 29
- Primary result: This work addresses the problem of skewed memorization in Large Language Models (LLMs), where a small subset of training data contributes disproportionately to memorization risk.

## Executive Summary
This study investigates skewed memorization patterns in large language models, where a small subset of training data contributes disproportionately to memorization risk. The authors develop a method to quantify and decompose memorization probabilities over sequence lengths, revealing how data characteristics influence memorization likelihood. The research demonstrates that memorization increases with training duration and is affected by dataset composition and size, with the skewness of memorization distribution linked to the token generation process.

## Method Summary
The authors introduce a systematic approach to analyze memorization in LLMs by examining the probability of memorizing sequences of varying lengths during training. They decompose term-wise probabilities in the LLM generation process to understand how different data characteristics contribute to memorization risk. The methodology involves tracking memorization patterns across different training durations and dataset compositions, using lexical similarity measures to detect memorized content. The analysis focuses on how the probability of memorization changes as sequences grow longer and how this relates to the underlying token generation mechanisms.

## Key Results
- Memorization risk increases with training duration and is influenced by dataset composition and size
- A small subset of training data contributes disproportionately to overall memorization (skewed distribution)
- The skewness of memorization distribution is linked to the token generation process in LLMs
- Higher embedding similarity correlates with increased memorization probability

## Why This Works (Mechanism)
The mechanism underlying skewed memorization relates to how LLMs process and store information during training. When models encounter training data, they don't uniformly memorize all content; instead, certain sequences become more deeply embedded in the model's parameters. This occurs because the token generation process, which builds sequences incrementally, creates dependencies where earlier tokens influence the likelihood of memorizing subsequent tokens. The decomposition approach reveals that memorization probability increases as sequence length grows, suggesting that longer sequences create stronger memory traces. Additionally, the study finds that data with higher embedding similarity to other training examples is more likely to be memorized, possibly because similar content reinforces memory formation through repeated exposure patterns.

## Foundational Learning
- **Token generation process**: Understanding how LLMs generate sequences token by token is crucial because memorization risk accumulates as sequences grow longer
  - Why needed: The sequential nature of generation creates dependencies that influence memorization probability
  - Quick check: Verify that the probability of memorizing a sequence increases with its length

- **Embedding similarity**: The similarity between embeddings of different training examples affects memorization likelihood
  - Why needed: Higher similarity between embeddings may indicate overlapping or redundant information that reinforces memorization
  - Quick check: Confirm that training examples with higher embedding similarity show increased memorization probability

- **Mutual information**: Measuring the independence between token correctness in sequences helps understand memorization patterns
  - Why needed: Low mutual information suggests tokens are memorized independently, which has implications for how memorization spreads through sequences
  - Quick check: Calculate mutual information between tokens in QA tasks to verify independence assumptions

## Architecture Onboarding
**Component Map**: Training data -> Embedding layer -> Transformer blocks -> Output layer -> Memorization tracking
**Critical Path**: Data ingestion → Embedding computation → Attention mechanism → Probability calculation → Memorization detection
**Design Tradeoffs**: The study balances between comprehensive memorization detection and computational efficiency by focusing on lexical similarity rather than semantic equivalence
**Failure Signatures**: Memorization detection may miss semantically equivalent but lexically different content; embedding collapse may artificially inflate similarity measures
**3 First Experiments**:
1. Track memorization probability across different sequence lengths to verify the accumulation effect
2. Compare memorization patterns between datasets with varying degrees of embedding similarity
3. Measure mutual information between tokens in structured reasoning tasks versus free-form generation

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Does embedding collapse during training contribute significantly to memorization intensity?
- Basis in paper: [explicit] The authors explicitly state in Section 5 (Future Work) that the connection between embedding similarity and memorization remains underexplored regarding whether embedding collapse is a driver of memorization.
- Why unresolved: While the paper links higher embedding similarity to increased memorization, it does not isolate whether the degradation of the embedding space (collapse) is a causal mechanism or merely a correlation.
- What evidence would resolve it: A study tracking the intrinsic dimensionality or singular values of the embedding space during training and correlating these metrics with the skewness of the memorization distribution.

### Open Question 2
- Question: Does memorization sensitivity extend beyond lexical similarity to factual relevance?
- Basis in paper: [explicit] Section 5 asks whether memorization sensitivity extends to "factual relevance" and how embeddings encode structured knowledge, distinguishing it from the lexical similarity measured in this work.
- Why unresolved: The current methodology relies on lexical overlap (prefix continuation) to detect memorization, which fails to capture semantic or factual memorization that uses different wording.
- What evidence would resolve it: Experiments using factual probing datasets to determine if facts associated with high-similarity embeddings are more prone to extraction than those with low similarity, regardless of lexical overlap.

### Open Question 3
- Question: Does the independence of token-wise memorization persist in structured reasoning tasks?
- Basis in paper: [inferred] The Introduction cites Xie et al. (2024) to note that LLMs rely on memorization for logical reasoning (Knights and Knaves). However, Section 3.5.2 finds low Mutual Information (independence) between token correctness in QA tasks.
- Why unresolved: It is unclear if the "independence" finding is an artifact of the specific QA datasets used (Lavita/GPTeacher) or a general rule. Logic puzzles, where later tokens depend heavily on the validity of earlier tokens, might violate this independence.
- What evidence would resolve it: Replicating the Mutual Information analysis (Section 3.5.2) on logical reasoning or code generation benchmarks to test if token dependencies are significantly higher in structured domains.

## Limitations
- The analysis primarily focuses on lexical similarity rather than semantic equivalence, potentially missing non-lexical memorization patterns
- The methodology may not fully account for model-specific behaviors or implementation details that influence memorization
- The study does not address how different model architectures or training objectives might affect memorization patterns

## Confidence
- High confidence: The observation that memorization increases with training duration and correlates with dataset composition and size is well-supported by the experimental evidence and aligns with existing literature on model memorization
- Medium confidence: The link between memorization skewness and the token generation process is plausible but requires further validation across different model architectures and training regimes to confirm its universality
- Medium confidence: The quantification methodology, while rigorous, may not fully account for all factors influencing memorization, such as model capacity, training data diversity, or evaluation procedures

## Next Checks
1. Validate the memorization quantification framework across multiple model architectures (e.g., transformer variants, different attention mechanisms) and training objectives to assess generalizability
2. Conduct ablation studies to isolate the impact of specific dataset characteristics (e.g., token frequency distributions, data diversity) on memorization patterns and their skewness
3. Implement controlled experiments with varying training durations and dataset sizes to establish causal relationships between these factors and memorization risk, while accounting for potential confounding variables