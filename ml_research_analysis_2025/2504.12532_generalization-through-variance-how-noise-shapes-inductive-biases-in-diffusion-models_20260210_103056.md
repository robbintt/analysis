---
ver: rpa2
title: 'Generalization through variance: how noise shapes inductive biases in diffusion
  models'
arxiv_id: '2504.12532'
source_url: https://arxiv.org/abs/2504.12532
tags:
- score
- training
- distribution
- diffusion
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how diffusion models generalize beyond
  their training data. The key insight is that diffusion models do not learn the true
  score function of the training distribution, but instead learn a "noisy" version
  called the proxy score.
---

# Generalization through variance: how noise shapes inductive biases in diffusion models

## Quick Facts
- arXiv ID: 2504.12532
- Source URL: https://arxiv.org/abs/2504.12532
- Authors: John J. Vastola
- Reference count: 40
- Key outcome: Diffusion models generalize through proxy score variance that creates "boundary-smearing" inductive bias, filling gaps between training examples

## Executive Summary
This paper investigates how diffusion models generalize beyond their training data by introducing the concept of the proxy score - a "noisy" version of the true score function that diffusion models actually learn. Through a theoretical framework using path integrals, the author shows that generalization arises not from learning the true data distribution, but from the high variance in score estimators in boundary regions between training examples. This variance introduces randomness that enables diffusion models to interpolate between training samples without simply memorizing them.

The key insight is that the learned distribution depends on both the average score estimator and a noise term proportional to the variance of score estimators across different training sample realizations (the V-kernel). This V-kernel is particularly large in boundary regions, leading to an inductive bias that "fills in gaps" between training examples while preserving the dimensionality of data manifolds. The framework explains why even naive score estimators that directly memorize training data still generalize, and how this mechanism operates across different model architectures from linear models to neural networks in the NTK regime.

## Method Summary
The author develops a theoretical framework using path integrals to characterize the typical distribution learned by diffusion models. The analysis is based on decomposing the learned score function into its average component and variance components across different training sample realizations. By deriving path integral expressions for the typical learned distribution, the framework reveals how the V-kernel (variance component) creates inductive bias in boundary regions. The theoretical predictions are tested through analytical results for linear models and neural networks in the NTK regime, showing how model capacity and time cutoff parameters modulate the extent of generalization.

## Key Results
- Diffusion models learn a "proxy score" rather than the true score function, with high variance in boundary regions enabling generalization
- The V-kernel (variance of score estimators) is particularly large in boundary regions, creating an inductive bias that "fills in gaps" between training examples
- Even naive score estimators that memorize training data still generalize due to proxy score variance
- Generalization behavior depends on model capacity (ratio of features to training samples) and time cutoff parameters
- The framework predicts a "boundary-smearing" inductive bias that preserves data manifold dimensionality while interpolating between examples

## Why This Works (Mechanism)
The mechanism works because diffusion models learn a proxy score function that has high variance in regions between training examples. When sampling from these high-variance regions, the randomness introduced by the variance term causes the model to explore and fill in gaps between training samples. This creates an inductive bias toward generating samples in these boundary regions, effectively "smearing" the boundaries between training examples while preserving their overall structure. The path integral framework captures how this variance-driven exploration leads to a learned distribution that generalizes beyond the training data.

## Foundational Learning

**Path integrals in probability**: Mathematical framework for characterizing distributions through weighted sums over trajectories. Needed to derive the typical learned distribution from the proxy score. Quick check: Can you explain how path integrals relate to the partition function in statistical mechanics?

**Score matching and score functions**: The gradient of log probability density, central to diffusion model training. Needed to understand what diffusion models actually learn. Quick check: What's the difference between the true score function and the proxy score function?

**Neural Tangent Kernel (NTK) regime**: Regime where neural networks behave like linear models during training. Needed to analyze generalization in practical neural architectures. Quick check: How does the NTK regime simplify the analysis of neural network behavior?

**Inductive bias in machine learning**: The tendency of learning algorithms to favor certain solutions over others. Needed to understand how proxy score variance creates generalization behavior. Quick check: How does variance-induced bias differ from traditional architectural inductive biases?

**Feature space density and boundary regions**: The distribution of training data in high-dimensional space. Needed to understand where proxy score variance is largest. Quick check: Why are boundary regions particularly important for understanding generalization?

## Architecture Onboarding

**Component map**: Training data -> Score estimator (with variance) -> Proxy score function -> Path integral over trajectories -> Typical learned distribution

**Critical path**: The proxy score variance in boundary regions -> Random exploration during sampling -> Generation of novel samples between training examples

**Design tradeoffs**: Higher model capacity increases proxy score variance but may lead to overfitting; time cutoff parameters control the balance between exploration and exploitation

**Failure signatures**: 
- Insufficient model capacity: Fails to generate novel samples, simply memorizes training data
- Excessive model capacity: May overfit to noise, losing the boundary-smearing inductive bias
- Poor time cutoff choice: Either insufficient exploration (too short) or excessive noise (too long)

**3 first experiments**:
1. Measure proxy score variance empirically across different training set sizes and correlate with generation diversity
2. Systematically vary training data density in feature space and observe model behavior in sparse regions
3. Compare generalization behavior between linear models and NTK regime networks under identical conditions

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions, but several areas remain for future investigation based on the theoretical framework and limited empirical validation presented.

## Limitations
- Theoretical framework relies on assumptions that may not hold for highly non-linear neural architectures beyond the NTK regime
- Limited empirical validation with theoretical predictions needing more comprehensive testing across different model architectures
- The concept of proxy score variance as the primary mechanism for generalization is not directly observable from training data alone
- The precise quantitative relationship between model capacity and generalization behavior requires more thorough investigation

## Confidence
- **High**: The mathematical framework for analyzing proxy score variance and its relationship to generalization
- **Medium**: The specific mechanism by which variance creates inductive bias in boundary regions
- **Medium**: The extension of results from linear/NTK models to practical diffusion models
- **Low**: The precise quantitative relationship between model capacity and generalization behavior

## Next Checks
1. Test the theoretical predictions on diverse neural network architectures beyond the NTK regime, including modern diffusion model implementations with attention mechanisms and non-linear feature maps

2. Design controlled experiments to measure proxy score variance empirically and correlate it with generalization performance across different training set sizes and dataset distributions

3. Validate the boundary-smearing hypothesis by systematically varying training data density in feature space and measuring the resulting model behavior in sparse regions