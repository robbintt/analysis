---
ver: rpa2
title: 'SABlock: Semantic-Aware KV Cache Eviction with Adaptive Compression Block
  Size'
arxiv_id: '2510.22556'
source_url: https://arxiv.org/abs/2510.22556
tags:
- block
- size
- cache
- semantic
- sablock
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SABlock tackles the memory bottleneck in long-context LLM inference
  by compressing the KV cache through semantic-aware token eviction with adaptive
  block sizes. It segments sequences into semantically coherent spans, refines token
  scores using segment-level importance, and dynamically searches for the optimal
  block size within each segment to balance semantic fidelity and compression efficiency.
---

# SABlock: Semantic-Aware KV Cache Eviction with Adaptive Compression Block Size

## Quick Facts
- arXiv ID: 2510.22556
- Source URL: https://arxiv.org/abs/2510.22556
- Authors: Jinhan Chen; Jianchun Liu; Hongli Xu; Xianjun Gao; Shilong Wang
- Reference count: 40
- Primary result: Achieves 99.9% retrieval accuracy with only 96 KV entries versus 8K for full-cache, reduces peak memory by 46.28% under a 1,024-token budget, and enables up to 9.5× faster decoding on 128K contexts.

## Executive Summary
SABlock addresses the memory bottleneck in long-context large language model (LLM) inference by compressing the KV cache through semantic-aware token eviction with adaptive block sizes. It segments input sequences into semantically coherent spans, refines token scores using segment-level importance, and dynamically searches for the optimal block size within each segment to balance semantic fidelity and compression efficiency. Evaluated on LongBench and Needle-in-a-Haystack, SABlock significantly reduces memory usage while maintaining high retrieval accuracy and enabling faster decoding.

## Method Summary
SABlock tackles the memory bottleneck in long-context LLM inference by compressing the KV cache through semantic-aware token eviction with adaptive block sizes. It segments sequences into semantically coherent spans, refines token scores using segment-level importance, and dynamically searches for the optimal block size within each segment to balance semantic fidelity and compression efficiency. Evaluated on LongBench and Needle-in-a-Haystack, SABlock achieves 99.9% retrieval accuracy with only 96 KV entries versus 8K for full-cache, reduces peak memory usage by 46.28% under a 1,024-token budget, and enables up to 9.5× faster decoding on 128K contexts.

## Key Results
- Achieves 99.9% retrieval accuracy with only 96 KV entries versus 8K for full-cache
- Reduces peak memory usage by 46.28% under a 1,024-token budget
- Enables up to 9.5× faster decoding on 128K contexts

## Why This Works (Mechanism)
SABlock works by combining semantic segmentation of input sequences with adaptive block-size compression. The method first partitions the input into semantically coherent segments, then scores tokens within each segment based on their importance. For each segment, it searches for the optimal block size that preserves semantic meaning while maximizing compression. This approach allows selective retention of critical information while discarding less important tokens, achieving significant memory reduction without sacrificing retrieval accuracy.

## Foundational Learning
- **Semantic segmentation**: Partitioning sequences into meaningful chunks to preserve context; needed because raw token-level eviction can destroy semantic coherence; quick check: verify segments align with natural topic or intent boundaries.
- **Token importance scoring**: Ranking tokens by their contribution to downstream tasks; needed to guide selective eviction; quick check: measure correlation between retained token scores and task performance.
- **Adaptive block-size search**: Dynamically adjusting compression granularity per segment; needed to balance fidelity and efficiency; quick check: compare retrieval accuracy vs. block size curves for each segment.
- **KV cache compression**: Reducing memory footprint of attention keys/values; needed to enable long-context inference; quick check: monitor memory usage and latency during inference.
- **Semantic-aware compression**: Prioritizing tokens by semantic relevance; needed to maintain task performance under compression; quick check: assess impact on retrieval or generation quality.

## Architecture Onboarding
**Component map**: Input sequence → Semantic segmentation → Token scoring → Adaptive block-size search → Compressed KV cache → Decoding

**Critical path**: Input sequence is first segmented into semantic units, tokens are scored for importance, and the optimal block size is searched and applied to each segment, producing a compressed KV cache that is used for efficient decoding.

**Design tradeoffs**: Balances compression ratio and semantic fidelity; adaptive search increases computational overhead but improves accuracy; fixed budgets may require aggressive compression, risking information loss.

**Failure signatures**: Sharp drops in retrieval accuracy when semantic segments are too large or block sizes too small; increased latency if adaptive search is not optimized; memory savings plateau if segments lack semantic coherence.

**Three first experiments**:
1. Measure retrieval accuracy and memory usage on a standard long-context benchmark with varying block sizes.
2. Evaluate the impact of different semantic segmentation granularities on downstream task performance.
3. Benchmark decoding speed and memory usage on a 128K context with the adaptive compression strategy.

## Open Questions the Paper Calls Out
None

## Limitations
- Performance gains are measured primarily on specific benchmarks and with a 1,024-token budget, limiting generalizability.
- Claims of semantic fidelity preservation are based on retrieval accuracy alone, without comprehensive analysis of downstream task performance or generation quality.
- Adaptive block-size search introduces computational overhead not fully quantified in the evaluation.
- Results are not cross-validated on a wider set of models or real-world long-context workloads, and no ablation studies clarify the relative contributions of semantic segmentation versus adaptive block sizing.

## Confidence
- Compression efficiency and memory reduction (46.28% peak memory, 96 KV entries vs 8K full-cache): High confidence, supported by direct experimental comparison.
- Retrieval accuracy preservation (99.9% on Needle-in-a-Haystack): Medium confidence, as accuracy is benchmark-specific and may not transfer to all long-context tasks.
- Speedup in decoding (up to 9.5×): Medium confidence, given limited context and benchmark diversity.

## Next Checks
1. Evaluate SABlock's impact on generation quality and task performance across diverse downstream benchmarks beyond retrieval accuracy.
2. Measure and report the overhead introduced by adaptive block-size search, including latency and energy consumption.
3. Test scalability and robustness by applying SABlock to different model architectures, context lengths, and memory budgets, and compare with recent KV cache compression methods on these variants.