---
ver: rpa2
title: 'Two Intermediate Translations Are Better Than One: Fine-tuning LLMs for Document-level
  Translation Refinement'
arxiv_id: '2504.05614'
source_url: https://arxiv.org/abs/2504.05614
tags:
- translation
- translations
- fine-tuning
- doc2doc
- sent2sent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a document-level translation refinement approach
  that leverages two intermediate translations (sentence-level and document-level)
  to improve translation quality. The method uses an enhanced fine-tuning strategy
  with quality awareness that assigns different weights to sentences based on refinement
  difficulty.
---

# Two Intermediate Translations Are Better Than One: Fine-tuning LLMs for Document-level Translation Refinement

## Quick Facts
- **arXiv ID**: 2504.05614
- **Source URL**: https://arxiv.org/abs/2504.05614
- **Reference count**: 40
- **Primary result**: Dual-hypothesis refinement improves document-level COMET by 2.73 and 1.80 points over single-hypothesis baselines on LLaMA-3-8B-Instruct and Mistral-Nemo-Instruct respectively.

## Executive Summary
This paper proposes a document-level translation refinement approach that leverages two intermediate translations (sentence-level and document-level) to improve translation quality. The method uses an enhanced fine-tuning strategy with quality awareness that assigns different weights to sentences based on refinement difficulty. Experiments across ten translation tasks using LLaMA-3-8B-Instruct and Mistral-Nemo-Instruct demonstrate significant improvements in document-level COMET scores compared to baseline approaches, while also showing better performance on coherence, lexical consistency, and other document-level metrics.

## Method Summary
The method generates two intermediate translations for each source document: a sentence-level translation (Sent2Sent) and a document-level translation (Doc2Doc). These are combined using a two-stage fine-tuning process on LLMs. First, a naïve fine-tuning stage trains the model on all quadruples with equal weights. Second, a quality-aware fine-tuning stage applies dynamic weights based on sentence-level quality scores, emphasizing sentences where both intermediate translations are already high-quality. The approach uses QLoRA for efficient fine-tuning with rank=8, alpha=16, and includes randomization of hypothesis positions to prevent positional bias.

## Key Results
- Document-level COMET improvements of 2.73 and 1.80 points over single-hypothesis baselines on LLaMA-3-8B-Instruct and Mistral-Nemo-Instruct respectively
- Significant gains in document-level coherence (56.21 vs 54.98) and lexical term consistency (59.32 vs 58.66)
- Quality-aware weighting provides additional improvement over naïve fine-tuning alone (88.14 vs 87.76 in validation)
- Ablation studies confirm the importance of both dual hypotheses and quality-aware weighting

## Why This Works (Mechanism)

### Mechanism 1
Combining sentence-level and document-level intermediate translations allows the refinement model to correct complementary error types—trading off between omission/hallucination risks and discourse incoherence. Sent2Sent inputs ensure all source information is present while Doc2Doc inputs better maintain lexical consistency. By conditioning on both simultaneously, the model learns to "copy" the coherent structure of Doc2Doc while "filling in" missing content from Sent2Sent. The core assumption is that the LLM can attend to two distinct input streams and selectively synthesize correct segments from each without introducing new noise.

### Mechanism 2
Weighting training examples higher when their intermediate translations are already of high quality forces the model to learn subtle correction and preservation strategies rather than simple error fixing. The loss function is scaled by $w_i$, which increases as the intermediate COMET score increases. This counter-intuitive definition of "hard" (near-perfect input) forces the model to focus on maintaining quality in good translations rather than just repairing broken ones. The core assumption is that the gradient signal from refining a near-perfect sentence is more valuable for final performance than the signal from rewriting a poor sentence.

### Mechanism 3
Randomizing the position of the two intermediate translations in the prompt prevents the model from learning a positional bias (e.g., always preferring the first hypothesis). The input template placeholders `<hyp1>` and `<hyp2>` are randomly assigned Sent2Sent or Doc2Doc outputs during training. This forces the model to attend to the semantic content of the hypotheses rather than their order. The core assumption is that LLMs default to utilizing shallow positional heuristics when comparing multiple inputs.

## Foundational Learning

- **Translation Metrics (COMET & ALTI+)**: Why needed here: The method relies on COMET to rank "difficulty" and ALTI+ to diagnose under-translation. Without understanding these, you cannot debug the weighting logic or the specific improvements claimed. Quick check question: Why does the paper define "hard" examples as those with *high* COMET scores (closer to the reference)?

- **Document-level vs. Sentence-level Translation**: Why needed here: The entire architecture is built on the trade-off between these two granularities (Context vs. Faithfulness). Quick check question: According to Table 1, which metric detects the "under-translation" issues common in Doc2Doc?

- **Supervised Fine-Tuning (SFT) Loss Weighting**: Why needed here: The core contribution is modifying the standard cross-entropy loss with dynamic weights. You must understand how weights scale the gradient for specific tokens. Quick check question: In Eq. 3, how does the weight $w_i$ affect the backpropagation for a specific sentence $r_i$ compared to a standard average?

## Architecture Onboarding

- **Component map**: Source -> Generator ($M_S$) -> Sent2Sent + Doc2Doc -> Weight Calculator -> Quality-aware weights -> Refiner ($M_T$) -> Refined translation

- **Critical path**:
  1. **Data Gen**: Generate dual translations for the dataset
  2. **Weighting**: Calculate $w$ for every sentence (Eq. 1)
  3. **Stage 1 (Naïve)**: Standard SFT on the quadruples for 1 epoch to establish baseline fluency
  4. **Stage 2 (QA)**: Continue SFT using the weighted loss (Eq. 3) for 1 epoch to focus on preservation
  5. **Inference**: Feed new dual hypotheses into the refined model

- **Design tradeoffs**:
  - Sentence vs. Instance Weighting: The paper weights by *sentence* because quality varies within a document. Instance weighting would wash out this signal
  - Cost: You pay 2x inference cost (generating two hypotheses) for the quality gain
  - Hyperparameters: λ (scale) and ε (threshold) are sensitive; incorrect values can cause the model to ignore the weighting signal

- **Failure signatures**:
  - Catastrophic Forgetting: The model outputs only the first sentence of the document
  - Position Bias: Validation performance is high, but drops significantly if the order of Hyp1/Hyp2 is swapped during inference

- **First 3 experiments**:
  1. **Profile Verification**: Reproduce Table 1 on a small sample to verify that Sent2Sent and Doc2Doc actually exhibit the predicted error profiles for your target language
  2. **Lambda Sensitivity**: Run a sweep on λ (e.g., 1.0 to 5.0) on the validation set to find the optimal weighting slope before committing to full training
  3. **Ablation**: Train a model with fixed positions (Sent2Sent always in Hyp1 slot) and compare against the random-order model to quantify the positional bias magnitude

## Open Questions the Paper Calls Out

- **Cross-domain generalization**: The authors note experiments were "primarily conducted on a news dataset, which may not fully represent LLMs' performance in other specific domains" - effectiveness on technical documentation or literary texts remains untested

- **Multilingual scalability**: The current approach "train[s] one model for one specific translation direction, leading to huge computational cost" - whether a unified multilingual model could achieve comparable performance remains open

- **Alternative optimization strategies**: The authors suggest that further research may "apply pairwise preference-based optimization tuning" as an alternative to the current quality-aware weighted cross-entropy loss

## Limitations

- **Limited language coverage**: Only five language pairs evaluated, all Indo-European or Chinese, leaving typologically diverse languages untested
- **Intermediate translation quality dependence**: The approach assumes the two intermediate translations are of sufficiently different quality to be complementary
- **Prompt sensitivity**: The method relies on carefully crafted prompts for generating Sent2Sent and Doc2Doc translations, with small changes potentially altering the quality distribution

## Confidence

- **High Confidence**: Document-level COMET improvements (2.73 and 1.80 points) are directly measurable from reported experiments and align with proposed mechanism
- **Medium Confidence**: Improvements in Coherence, ALTI+, and LTCR are supported by statistical tests but depend on metric sensitivity
- **Low Confidence**: Quality-aware weighting is essential based on a single ablation without exploring alternative weighting schemes or hyperparameter sensitivity

## Next Checks

1. **Cross-Lingual Robustness Test**: Implement the refinement pipeline for a typologically distant language pair (e.g., English→Japanese) and measure whether the dual-hypothesis approach still produces significant gains

2. **Intermediate Translation Diversity Analysis**: Quantify the semantic and lexical overlap between Sent2Sent and Doc2Doc outputs using metrics like BERTScore and BLEU to determine the minimum diversity threshold below which refinement degrades

3. **Prompt Robustness Evaluation**: Systematically vary the prompt templates for generating intermediate translations and measure the stability of the final refinement model's performance across these variations