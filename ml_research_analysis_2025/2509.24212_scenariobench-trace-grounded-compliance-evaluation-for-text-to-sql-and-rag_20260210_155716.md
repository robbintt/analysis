---
ver: rpa2
title: 'ScenarioBench: Trace-Grounded Compliance Evaluation for Text-to-SQL and RAG'
arxiv_id: '2509.24212'
source_url: https://arxiv.org/abs/2509.24212
tags:
- trace
- retrieval
- latency
- clause
- decision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ScenarioBench introduces a trace-grounded benchmark for evaluating
  Text-to-SQL and retrieval-augmented generation in compliance settings, requiring
  systems to justify decisions using only clause IDs they themselves retrieve. The
  benchmark includes a no-peek gold-standard package with expected decisions, minimal
  witness traces, governing clause sets, and canonical SQL, enabling end-to-end scoring
  of both what a system decides and why.
---

# ScenarioBench: Trace-Ground Compliance Evaluation for Text-to-SQL and RAG

## Quick Facts
- arXiv ID: 2509.24212
- Source URL: https://arxiv.org/abs/2509.24212
- Authors: Zahra Atf; Peter R Lewis
- Reference count: 9
- Primary result: Single budgeted reflection cycle improves trace-completeness from 0.541 to 1.000 with zero hallucination and ~+1 ms latency overhead

## Executive Summary
ScenarioBench introduces a trace-grounded benchmark for evaluating Text-to-SQL and retrieval-augmented generation systems in compliance settings. The framework requires systems to justify decisions using only clause IDs they themselves retrieve, making explanations falsifiable and audit-ready. A no-peek gold-standard package enables end-to-end scoring of both decision accuracy and justification quality without exposing oracle answers during inference.

## Method Summary
The benchmark compiles YAML scenarios into synchronized Prolog facts and SQL Policy_DB materializations, preserving referential integrity across views. Systems retrieve relevant clauses using hybrid BM25/vector search, generate SQL queries via typed intents, and produce grounded traces citing only retrieved clause IDs. Evaluation metrics include decision accuracy, trace completeness/correctness, policy coverage, and hallucination rate, with results aggregated via SDI and SDI-R under per-scenario time budgets.

## Key Results
- Decision accuracy and macro-F1 saturated at 1.000 on synthetic seed suite (N=16)
- Single budgeted reflective step closed trace gaps (trace-completeness and policy-coverage improved from 0.541 to 1.000)
- Zero hallucination achieved at approximately +1 ms latency overhead
- Hybrid retrieval added latency (17.6ms vs 8.2ms) without trace improvement when trace builder was minimal

## Why This Works (Mechanism)

### Mechanism 1: Grounding Invariant
Requiring traces to cite only system-retrieved clause IDs yields falsifiable explanations with zero hallucination. The constraint `all predicted trace IDs ⊆ retrieved@k` binds justifications to verifiable evidence from the Policy_DB, making any unsupported citation detectable as hallucination. Core assumption: the policy canon is complete and consistent for the scenario domain.

### Mechanism 2: Budgeted Reflective Trace Completion
A single constrained reflection cycle can close trace gaps without accessing gold-standard answers, improving trace-completeness from 0.541 to 1.000. Critics analyze the standard log to adjust local knobs (retrieval k, hybrid weights, template choice) within a per-scenario wall-clock budget (B_time), enabling the trace builder to incorporate already-retrieved clauses that were initially omitted. Assumption: the evidence needed for complete traces is already present in the retrieved top-k but not yet incorporated into the trace.

### Mechanism 3: Dual Materialization with Referential Integrity
Synchronized Prolog facts and SQL views preserve referential integrity across reasoning and retrieval, enabling end-to-end evaluation of both "what" and "why." A one-to-one mapping between clause_id, Prolog predicates, and Policy_DB.clauses.id ensures that rules, SQL queries, and traces all reference the same canonical entities; validators enforce consistency. Core assumption: the compilation process from YAML to both materializations is error-free and synchronized.

## Foundational Learning

- Concept: Result-set equivalence for SQL correctness
  - Why needed here: SQL accuracy is judged by whether the sorted multiset of returned clause_ids matches the canonical query, not by string similarity.
  - Quick check question: Given two SQL queries that return the same clause IDs in different orders with different syntax, are they equivalent under ScenarioBench?

- Concept: Top-k retrieval with grounding constraint
  - Why needed here: Traces can only cite clause IDs that appear in the system's own retrieved top-k results; understanding this constraint is essential for debugging trace failures.
  - Quick check question: If a trace cites clause ID "CASL-IDENT-001" but this ID does not appear in the retrieved@10 list, what is the expected hallucination rate impact?

- Concept: Budget-aware trade-offs (SDI vs. SDI-R)
  - Why needed here: SDI measures quality normalized by difficulty; SDI-R penalizes latency overhead, making time-quality trade-offs explicit.
  - Quick check question: If a method improves trace-completeness by 0.2 but adds 50% latency overhead, will SDI-R favor or penalize this change?

## Architecture Onboarding

- Component map:
  YAML Scenarios → ScenarioCompiler → Policy_DB (SQLite/FTS5) + Prolog Facts
                                   ↓
                           Retriever (BM25/Hybrid) → top-k clause IDs
                                   ↓
                           NLQ-to-SQL Layer → parameterized SQL
                                   ↓
                           Rule Engine → Decision + Trace (grounded in retrieved)
                                   ↓
                           Standard Log (JSONL/CSV) → Metrics/SDI/SDI-R

- Critical path:
  1. YAML scenario compilation → synchronized Prolog + SQL materializations
  2. Retrieval surfaces relevant clause IDs into top-k
  3. Rule engine produces decision + trace citing only retrieved IDs
  4. Evaluator compares against gold-standard (no-peek) for accuracy, trace quality, hallucination

- Design tradeoffs:
  - BM25-only vs. Hybrid retrieval: BM25 is faster (8.2ms) but may miss semantic matches; Hybrid adds latency (17.6ms) without trace improvement unless trace builder can exploit richer candidates.
  - Reflection cycles: More cycles may improve trace completeness but increase latency; capped at B_cycles=2.
  - Strict vs. liberal hallucination: Strict penalizes any citation outside gold closure; liberal treats extra consistent citations as neutral.

- Failure signatures:
  - Under-tracing: Trace omits clauses (e.g., IDENT) that appear in retrieved top-k; indicates trace builder limitations.
  - Latency without benefit: Hybrid retrieval increases time without improving TrC/Coverage.
  - Hallucination > 0: Trace cites clause IDs not in retrieved set or gold closure; grounding constraint violated.

- First 3 experiments:
  1. Run the provided demo scenario (CASL-EMAIL-UNSUB-003.yml) with BM25 and Hybrid retrieval; compare TrC, Coverage, and latency to reproduce Table V results.
  2. Enable single-cycle reflection on Hybrid baseline; verify trace-completeness rises from 0.541 to 1.000 with ~+1ms latency as reported in Table III.
  3. Add a rule-only trace critic that appends IDENT with role "applies" when present in retrieved@k but absent from trace; measure impact on TrC and hallucination rate.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does strict clause grounding improve decision reliability over ungrounded baselines in compliance settings?
- Basis in paper: [explicit] The introduction asks "does clause grounding improve decision reliability over ungrounded baselines" as one of three evaluation questions.
- Why unresolved: The paper tests grounded systems but does not compare against an explicitly ungrounded baseline; all experiments enforce the trace⊆retrieved@k invariant.
- What evidence would resolve it: A controlled ablation where the grounding constraint is relaxed, comparing decision accuracy and hallucination rates between grounded and ungrounded configurations on the same scenario suite.

### Open Question 2
- Question: How do retrieval choices (BM25 vs hybrid) shift justification quality as measured by trace completeness, correctness, and unsupported attribution?
- Basis in paper: [explicit] The introduction asks "how do retrieval choices (BM25 and hybrid) shift justification quality as measured by trace completeness/correctness and unsupported attribution."
- Why unresolved: In the seed experiments, BM25 and hybrid achieve identical trace-completeness (0.667) and coverage (0.667) with hybrid adding latency (8.2→17.6 ms) and no explanation gain, suggesting the trace constructor, not retrieval, is the bottleneck.
- What evidence would resolve it: Experiments with an enhanced trace builder (e.g., rule-only critic or re-ranking) that can exploit hybrid's richer top-k to increase TrC and coverage over BM25 under a fixed latency budget.

### Open Question 3
- Question: Can short, constrained reflection consistently improve explanation quality without access to gold-standard packages under per-scenario time budgets?
- Basis in paper: [explicit] The introduction asks "under a per-scenario wall-clock budget B time, can short, constrained reflection deliver consistent gains in explanation quality without access to the gold-standard package."
- Why unresolved: One reflective cycle closed trace gaps (0.541→1.000) on N=16 synthetic scenarios, but external validity to larger, noisier, multi-jurisdictional corpora with human-adjudicated gold labels is untested.
- What evidence would resolve it: Scaling to 50–150 scenarios with bilingual coverage and noisy inputs, reporting SDI-R gains and hallucination rates with and without reflection under fixed B time.

### Open Question 4
- Question: How does the mapping from retrieved clause sets to ordered, role-annotated traces constrain explanation quality when retrieval is already strong?
- Basis in paper: [inferred] The discussion notes "stronger retrieval alone is insufficient; explanation quality is ultimately constrained by how top-k candidates are converted into ordered, role-annotated traces under the grounding policy."
- Why unresolved: Current trace builder emits minimal two-step witnesses even when IDENT appears in retrieved top-k, indicating under-tracing; the gap between retrieval quality and trace completeness remains unquantified across clause families.
- What evidence would resolve it: Ablations with varying trace-construction policies (minimal vs. complete witness) reporting TrC, policy coverage, and latency while holding retrieval fixed.

## Limitations
- The rule engine's trace construction logic and NLQ-to-SQL template library are underspecified, making exact reproduction challenging
- Evaluation relies on synthetic seed suite (N=16) rather than real-world compliance scenarios, limiting generalizability
- Confidence in reported performance metrics is medium due to closed ecosystem and lack of open-source artifacts

## Confidence

- High: Dual materialization preserves referential integrity; grounding constraint eliminates hallucination; no-peek gold standard enables unbiased evaluation
- Medium: Budgeted reflection closes trace gaps from 0.541 to 1.000; hybrid retrieval adds marginal latency without trace benefit unless trace builder improves
- Low: Real-world applicability beyond synthetic CASL-like domains; scalability to larger policy corpora; robustness to retrieval failures

## Next Checks

1. Implement a rule-only trace critic that appends IDENT clauses with role "applies" when present in retrieved@k but missing from the system's trace; verify TrC improves toward 1.0 without increasing hallucination
2. Conduct a head-to-head retrieval ablation: run BM25-only vs. Hybrid (w_vec=0.6, w_bm25=0.4) on a held-out scenario set; measure whether richer retrieval translates to higher TrC when trace builder is held constant
3. Perform a temporal drift test: recompile a scenario's YAML with an additional clause, rebuild both Prolog and Policy_DB, and verify that clause_id ↔ predicate ↔ Policy_DB.clauses.id mapping remains synchronized across materializations