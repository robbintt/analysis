---
ver: rpa2
title: Privacy Amplification in Differentially Private Zeroth-Order Optimization with
  Hidden States
arxiv_id: '2506.00158'
source_url: https://arxiv.org/abs/2506.00158
tags:
- privacy
- bound
- have
- where
- zeroth-order
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of privacy amplification for zeroth-order
  optimization methods under differential privacy (DP). While DP analysis for first-order
  methods like gradient descent has been well-studied, extending privacy amplification
  by iteration (PABI) to zeroth-order methods remains an open challenge.
---

# Privacy Amplification in Differentially Private Zeroth-Order Optimization with Hidden States

## Quick Facts
- **arXiv ID**: 2506.00158
- **Source URL**: https://arxiv.org/abs/2506.00158
- **Authors**: Eli Chien; Wei-Ning Chen; Pan Li
- **Reference count**: 40
- **Primary result**: Convergent DP bounds for zeroth-order optimization via hidden-state analysis and hybrid noise mechanisms

## Executive Summary
This paper addresses the fundamental challenge of extending privacy amplification by iteration (PABI) to zeroth-order optimization (ZOO) methods under differential privacy. While PABI has been well-established for first-order methods like gradient descent, extending it to ZOO has remained an open problem due to the anisotropic nature of directional noise. The authors develop a novel hidden-state privacy analysis for a generalized Noisy-ZOGD algorithm that combines directional and isotropic noise components. This hybrid approach enables tighter privacy bounds that converge as the number of iterations increases, rather than growing linearly.

The key innovation is showing that releasing only the final model iterate (hiding intermediate states) prevents privacy loss from accumulating linearly with training steps. The analysis reveals that using multiple orthonormal update directions not only improves utility but also enhances privacy - a benefit not captured by standard composition-based analysis. These findings provide both theoretical insights and practical guidelines for designing better DP zeroth-order optimization algorithms, particularly for large-scale applications like fine-tuning large language models under memory and privacy constraints.

## Method Summary
The paper develops a generalized Noisy-ZOGD algorithm that extends PABI to ZOO by using hybrid noise (directional + isotropic) and orthonormal update directions. The algorithm estimates gradients via finite differences, adds noise along K orthonormal directions plus isotropic noise, and projects updates onto a bounded domain. The key theoretical innovation is using shifted Rényi divergence to prove that privacy loss saturates as the number of iterations increases when only the final iterate is released. The method specifically targets smooth, strongly convex problems and provides convergent privacy bounds that depend on the update dimension K, showing that higher K values improve both utility and privacy.

## Key Results
- Final iterate satisfies (ε, δ)-DP with bound O(√(∆² log(1/δ)/(n²σ² · min(T, MRn√d/(K∆)))), showing privacy decreases with update dimension K
- Using multiple orthonormal update directions (K > 1) provides both utility and privacy benefits not captured by standard composition analysis
- Hybrid noise mechanism (directional + isotropic) enables the shifted-divergence analysis while maintaining zeroth-order method utility
- Orthonormal directions provide better privacy than i.i.d. random directions due to tighter concentration of random projections

## Why This Works (Mechanism)

### Mechanism 1: Hidden-State Privacy Saturation
Releasing only the final model iterate prevents privacy loss from accumulating linearly with training steps by allowing it to saturate at a constant value. This generalizes PABI using Shifted Rényi Divergence to measure privacy loss, where the contractive nature of optimization updates (due to strong convexity/smoothness) causes divergence between models to shrink as iterates get closer. The privacy "cost" of early iterations is amortized over time through a minimum over τ rather than a sum over T.

### Mechanism 2: Hybrid Noise for Analytical Tractability
A mixture of directional noise and isotropic noise enables the shifted-divergence analysis while maintaining utility benefits. Pure directional noise creates anisotropic noise that breaks the Shift Reduction Lemma, while pure isotropic noise fixes analysis but hurts utility. The hybrid approach adds scalar Gaussian perturbation along update directions plus small isotropic Gaussian component, with the isotropic part satisfying Shift Reduction Lemma conditions while directional part handles privacy-utility trade-off.

### Mechanism 3: Orthonormal Directional Sampling
Drawing update directions from an orthonormal basis reduces the "generalized Lipschitz constant" of the update map, tightening privacy bounds. With orthonormal vectors, random projection terms concentrate tightly around K/d, while i.i.d. vectors have higher variance and result in Lipschitz constant bounded below by 1, preventing the tight "decay" behavior needed for superior privacy amplification.

## Foundational Learning

- **Zeroth-Order Optimization (ZOO)**: Algorithmic substrate that estimates gradients via finite differences to avoid backpropagation memory cost. Quick check: Why does ZOO save memory compared to DP-SGD? (Answer: Requires only forward passes and scalar outputs, avoiding per-sample gradient storage).

- **Shifted Rényi Divergence**: Analytical tool that allows "discounting" divergence by distance between distributions, enabling proof that privacy saturates. Quick check: How does the "shift" parameter z in D^(z)_α relate to distance between model iterates in the proof? (Answer: Allows bounding privacy loss in terms of Wasserstein distance between iterates, which contracts over time).

- **Generalized Lipschitz Continuity**: Property proving the noisy ZOO update map is "generalized Lipschitz" (||φ(x) - φ(y)|| ≤ c₁||x-y|| + c₂), serving as the bridge connecting algorithm dynamics to divergence analysis. Quick check: Why is the update map only "generalized" Lipschitz rather than strictly Lipschitz? (Answer: Zeroth-order gradient estimation error introduces additive bias term c₂).

## Architecture Onboarding

- **Component map**: Direction Generator -> Gradient Estimator -> Hybrid Noise Injector -> State Updater
- **Critical path**: Hybrid Noise Injector and Direction Generator are critical - privacy guarantees only hold if directions are orthonormal AND noise has isotropic component (β > 0). Deviating to i.i.d. directions or pure directional noise breaks theoretical bounds.
- **Design tradeoffs**:
  - Dimension K: Higher K improves privacy (lower ε) and utility (lower gradient variance) but increases compute (2 × K forward passes per step)
  - Noise mix β: Must be >0 for proof validity, but high β hurts utility (noise in irrelevant dimensions)
  - Domain Radius R: Privacy scales with R; smaller bounded domains improve privacy guarantees but may restrict model capacity
- **Failure signatures**:
  - Linear Privacy Growth: Using standard composition accounting instead of hidden-state analysis, or loss not strongly convex
  - Lipschitz Explosion: Using i.i.d. directions (c₁ ≥ 1), causing privacy bound to fail to contract
  - Utility Collapse: If β ≈ 1, utility drops because noise is wasted in orthogonal dimensions
- **First 3 experiments**:
  1. Verify Privacy Saturation: Train on synthetic convex dataset, plot privacy ε vs T to confirm "hockey stick" curve vs linear growth
  2. Ablate Direction Strategy: Compare orthonormal vs i.i.d. Gaussian directions on LLM fine-tuning task, measure utility vs ε
  3. Hybrid Noise Sensitivity: Sweep β ∈ [0.1, 0.9] on large model, plot Privacy-Utility Frontier to find practical sweet spot

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can the Lipschitz assumption on the loss function be relaxed while maintaining convergent privacy guarantees for zeroth-order optimization?
- **Basis in paper**: [explicit] "Relaxing the Lipschitz assumption... mainly affects the proof that the (noiseless) gradient update map ϕ is Lipschitz... We leave a rigorous extension of our analysis to these broader settings as an important direction for future work."
- **Why unresolved**: Current shifted divergence analysis requires Lipschitz continuity to establish generalized Lipschitz property of zeroth-order update map
- **What evidence would resolve it**: Proof showing clipped zeroth-order gradient updates maintain necessary Lipschitz properties without assuming Lipschitz loss

### Open Question 2
- **Question**: Can the smoothness assumption be weakened to Hölder continuity while preserving convergent privacy bounds?
- **Basis in paper**: [explicit] "Chien and Li (2025) demonstrated that the smoothness requirement can be weakened to Hölder continuity... in the context of shifted divergence for first-order methods. We leave a rigorous extension of our analysis to these broader settings as an important direction for future work."
- **Why unresolved**: Smoothness bounds zeroth-order gradient approximation error and establishes generalized Lipschitz properties essential for analysis
- **What evidence would resolve it**: Theoretical framework adapting shifted Rényi divergence to accommodate Hölder continuous gradients

### Open Question 3
- **Question**: Can privacy analysis be extended to shuffled cyclic mini-batch schemes for zeroth-order optimization?
- **Basis in paper**: [explicit] "While it is also possible to extend our results to other mini-batch schemes, such as shuffled cyclic mini-batches... we leave such generalizations for future work."
- **Why unresolved**: Current analysis only covers sampling without replacement; shuffled cyclic schemes may offer different privacy-utility trade-offs
- **What evidence would resolve it**: Convergent DP bounds for Noisy-ZOGD with shuffled cyclic mini-batches

### Open Question 4
- **Question**: Can jointly optimizing K (update dimension) and T (iterations) yield improved privacy-utility dependencies compared to separate analysis?
- **Basis in paper**: [explicit] "It is possible to derive a better dependency with respect to r, d if one jointly optimizes the choice of K, T for all three terms simultaneously. We leave such an effort as an interesting future work."
- **Why unresolved**: Current analysis treats K and T separately, potentially missing optimal trade-offs between gradient convergence, clipping failure, and noise variance terms
- **What evidence would resolve it**: Joint optimization deriving optimal K and T as functions of problem parameters

## Limitations
- Analytical applicability limited to smooth, strongly convex problems over bounded domains
- Hybrid noise trade-off requires extensive tuning; optimal β not fully characterized
- Orthonormal direction constraint proven theoretically but practical magnitude of advantage not empirically validated
- Numerical implementation of constrained optimization in Theorem 3.2 not fully specified

## Confidence
- **High Confidence**: Fundamental mechanisms (PABI, hybrid noise, orthonormal directions) are theoretically sound with rigorous mathematical proofs within stated assumptions
- **Medium Confidence**: Practical utility and privacy gains from orthonormal directions and specific hybrid noise mixing parameter are supported by theory but require empirical validation
- **Low Confidence**: Exact implementation details for numerical optimization in Theorem 3.2 and performance on non-convex, real-world deep learning tasks are not fully specified or validated

## Next Checks
1. **Validate Privacy Saturation**: Implement Noisy-ZOGD on synthetic strongly convex problem (e.g., logistic regression), plot calculated privacy parameter ε from Theorem 3.2 versus iterations T, verify saturation vs linear growth
2. **Compare Direction Strategies**: On standard LLM fine-tuning task, implement Noisy-ZOGD with orthonormal vs i.i.d. Gaussian directions, measure and compare privacy-utility trade-off (accuracy vs ε)
3. **Tune Hybrid Noise Component**: Systematically sweep noise mixing parameter β ∈ [0.1, 0.9] on memory-constrained deep learning task, plot Privacy-Utility Frontier (accuracy vs ε) for each β to identify practical sweet spot