---
ver: rpa2
title: 'Deep Active Learning for Lung Disease Severity Classification from Chest X-rays:
  Learning with Less Data in the Presence of Class Imbalance'
arxiv_id: '2508.21263'
source_url: https://arxiv.org/abs/2508.21263
tags:
- data
- sampling
- learning
- acquisition
- active
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of training deep learning models
  for lung disease severity classification from chest X-rays under severe class imbalance
  and limited labeled data. It proposes a deep active learning framework combining
  Monte Carlo Dropout-based Bayesian Neural Networks with a weighted loss strategy
  to handle imbalanced data.
---

# Deep Active Learning for Lung Disease Severity Classification from Chest X-rays: Learning with Less Data in the Presence of Class Imbalance

## Quick Facts
- arXiv ID: 2508.21263
- Source URL: https://arxiv.org/abs/2508.21263
- Reference count: 40
- Primary result: 93.7% accuracy with 15.4% labeled data using Entropy Sampling for binary classification

## Executive Summary
This study presents a deep active learning framework for lung disease severity classification from chest X-rays that addresses both class imbalance and limited labeled data constraints. The approach combines Monte Carlo Dropout-based Bayesian Neural Networks with weighted loss functions to handle imbalanced data distributions. Seven acquisition functions were systematically evaluated to identify the most efficient methods for reducing labeling requirements while maintaining diagnostic accuracy. The results demonstrate that simpler acquisition methods like Entropy Sampling and Mean STD can achieve baseline performance with significantly less labeled data than traditional supervised learning approaches.

## Method Summary
The method employs a ResNet50 architecture with MC Dropout layers to approximate Bayesian Neural Networks for uncertainty estimation. Training uses weighted cross-entropy loss based on inverse class frequencies to handle the 14%/36%/50% multi-class split. The active learning cycle begins with 25 stratified samples per class, then iteratively acquires 20 new samples per iteration using acquisition functions that operate on MC Dropout predictions. Seven acquisition functions were evaluated: Entropy, Mean STD, BatchBALD, Least Confidence, Margin Sampling, Variation Ratios, and Random. Models are retrained from scratch at each iteration, with early stopping after 3 epochs without improvement.

## Key Results
- Entropy Sampling achieved 93.7% accuracy (AU ROC: 0.91) in binary classification using only 15.4% of training data
- Mean STD achieved 70.3% accuracy (AU ROC: 0.86) in multi-class classification with 23.1% of labeled data
- Weighted loss strategy effectively managed class imbalance without requiring specialized acquisition functions
- Simpler acquisition methods outperformed complex alternatives like BatchBALD while reducing computational cost by ~50%

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Entropy-based uncertainty sampling concentrates labeling effort on decision boundary examples, reducing redundant annotation.
- Mechanism: Acquisition function computes predictive entropy H(y|x) across MC Dropout forward passes; top-k uncertain samples are selected for labeling, iteratively expanding D_train. This preferentially selects samples where the model's predictive distribution is maximally uncertain rather than confident or random.
- Core assumption: Model uncertainty correlates with informativeness—samples near decision boundaries provide more learning signal per label than confident predictions.
- Evidence anchors:
  - [abstract] "Entropy Sampling achieved 93.7% accuracy (AU ROC: 0.91) in binary classification using only 15.4% of training data"
  - [section 3.1] "Entropy Sampling performs the best in the binary setting, achieving baseline accuracy of around 94% with only 15.4% (IQR: [14.17, 27.73]) of the training data outperforming more complex acquisition functions like BatchBALD"
- Break condition: If uncertainty correlates poorly with label value (e.g., inherently ambiguous cases where even experts disagree), entropy sampling may select unhelpful samples.

### Mechanism 2
- Claim: MC Dropout provides computationally tractable Bayesian uncertainty estimates without ensemble overhead.
- Mechanism: Dropout is applied at both training and inference; T stochastic forward passes produce {p̂₁, p̂₂, ..., p̂_T} per sample. The variance across passes estimates epistemic uncertainty, enabling acquisition functions without training separate models.
- Core assumption: Dropout at inference approximates posterior distribution over model weights (Gal & Ghahramani, 2016)—the "Bayesian approximation" holds for the architecture.
- Evidence anchors:
  - [section 2.3] "We utilized a ResNet50 architecture, incorporating Monte Carlo (MC) Dropout layers, which served as an approximation of Bayesian Neural Networks (BNNs)"
- Break condition: If dropout probability is poorly calibrated or architecture lacks sufficient stochastic depth, uncertainty estimates may be unreliable.

### Mechanism 3
- Claim: Weighted loss neutralizes class imbalance without requiring specialized acquisition functions.
- Mechanism: Loss is penalized by inverse class frequency: minority class (normal, 14%) receives higher per-sample gradient contribution. This allows standard acquisition functions to work without explicit class-aware sampling.
- Core assumption: Inverse-frequency weighting sufficiently rebalances gradient contributions; minority class features are learnable with sufficient signal.
- Evidence anchors:
  - [section 2.3] "weighted loss function penalized the model based on the inverse of the class distribution... This approach helped to counterbalance the skewed distribution"
- Break condition: If minority class has inherently lower feature quality (poor signal-to-noise), weighting alone cannot compensate.

## Foundational Learning

- Concept: Active Learning Cycle
  - Why needed here: The entire paper is structured around iterative pool-based sampling; understanding the D_pool → acquisition → labeling → D_train update loop is prerequisite.
  - Quick check question: Can you trace one full iteration: what inputs does the acquisition function receive, and what does it output?

- Concept: Monte Carlo Dropout as Bayesian Approximation
  - Why needed here: All acquisition functions (Entropy, Mean STD, BatchBALD) depend on uncertainty estimates from MC Dropout forward passes.
  - Quick check question: Why must dropout remain enabled at inference time in this framework?

- Concept: Class-Imbalanced Loss Weighting
  - Why needed here: The 14%/36%/50% multi-class split and 14%/86% binary split create gradient bias; understanding why inverse-frequency weighting helps is essential.
  - Quick check question: If normal class is 14% of data, what relative weight should it receive compared to severe (50%)?

## Architecture Onboarding

- Component map:
  - ResNet50 backbone (ImageNet pretrained) → Global pooling → MC Dropout → Linear(512) → BatchNorm → ReLU → Linear(512) → Softmax/Sigmoid
  - Acquisition module: 7 functions operating on MC predictions (Entropy, Mean STD, BatchBALD, Least Confidence, Margin, Variation Ratios, Random)
  - Training loop: Stratified initialization (25/class) → iterative batch acquisition (20 samples/iteration) → retrain from scratch

- Critical path:
  1. Initialize D_train with stratified 25 samples per class
  2. Train BNN with weighted cross-entropy until early stopping (3 epochs without improvement)
  3. Run T MC Dropout forward passes on D_pool to compute acquisition scores
  4. Select top-20 samples by acquisition function, label, add to D_train
  5. Repeat until baseline accuracy achieved or pool exhausted

- Design tradeoffs:
  - Retraining from scratch vs. fine-tuning: Paper retrains from scratch for fair comparison; production may fine-tune for speed
  - Batch size 20: Balance between labeling burden reduction and acquisition diversity
  - 7 acquisition functions evaluated: Entropy and Mean STD best for binary/multi-class respectively; BatchBALD 50% slower (60-70s vs 41-44s acquisition time)

- Failure signatures:
  - Random Sampling matching or beating informed acquisition → implementation bug in uncertainty computation
  - High IQR in convergence points → high seed sensitivity, may need larger initial pool
  - Precision/specificity failing to reach baseline (dashed lines in tables) → minority class underrepresentation in selected samples

- First 3 experiments:
  1. Reproduce binary Entropy Sampling curve: Start with 25 normal + 25 abnormal, plot accuracy vs. % training data; target ~94% accuracy at 15-20% data
  2. Ablate weighted loss: Run same experiment with unweighted cross-entropy; expect slower convergence and higher variance
  3. Compare acquisition time: Measure batch acquisition latency for Entropy vs. BatchBALD on 1000-sample pool; expect ~40s vs ~65s on RTX 2080

## Open Questions the Paper Calls Out

- Question: Would sophisticated weighting methods for radiologist assessments (beyond simple median labeling) improve ground truth accuracy and downstream model performance?
- Question: Can the proposed active learning framework with weighted loss maintain its label efficiency when applied to other medical imaging modalities (e.g., CT, MRI) and non-pulmonary disease domains?
- Question: Do different acquisition functions systematically select images with distinct epistemic versus aleatoric uncertainty profiles, and can this inform radiologist triage of ambiguous cases?
- Question: How robust is the proposed framework across multi-center datasets with different patient populations, imaging protocols, and class imbalance distributions?

## Limitations
- Implementation sensitivity: High variance across runs (IQR in convergence points) suggests method may be sensitive to random seed and initialization
- Generalizability constraints: Performance on different disease types or more extreme imbalance ratios remains unknown
- Active learning assumptions: Framework assumes labeled samples are available on-demand and that uncertainty correlates with informativeness

## Confidence
- High confidence: Weighted loss strategy effectiveness - multiple runs show consistent improvement over unweighted training
- Medium confidence: MC Dropout uncertainty quality - theoretically sound but no direct validation for chest X-rays
- Low confidence: Acquisition function ranking stability - Entropy and Mean STD performed best, but high IQR suggests results may not generalize

## Next Checks
1. Ablation study: Run the binary classification experiment with Entropy Sampling both with and without weighted loss to quantify the contribution of each component to the 94% accuracy at 15.4% data.
2. Uncertainty quality validation: Compare MC Dropout uncertainty estimates against ground-truth expert disagreement by collecting multiple expert labels for high-uncertainty samples and measuring label variance.
3. Extreme imbalance test: Repeat experiments with synthetic imbalance (e.g., 5%/95% binary split) to determine if the weighted loss + standard acquisition approach breaks down under more severe class imbalance.