---
ver: rpa2
title: Evaluating Speech-to-Text Systems with PennSound
arxiv_id: '2504.05702'
source_url: https://arxiv.org/abs/2504.05702
tags:
- https
- speech
- whisper
- systems
- were
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper benchmarks 8 commercial and open-source speech-to-text
  systems on a random 10-hour sample from PennSound, the world's largest online poetry
  reading archive. Reference transcripts were created by trained annotators, and systems
  evaluated included AWS, Azure, Google, IBM, NVIDIA NeMo, Rev.ai, Whisper, and Whisper.cpp.
---

# Evaluating Speech-to-Text Systems with PennSound

## Quick Facts
- arXiv ID: 2504.05702
- Source URL: https://arxiv.org/abs/2504.05702
- Reference count: 0
- All systems achieved word error rates below 15%, with Rev.ai performing best at 9.0% WER

## Executive Summary
This paper benchmarks 8 commercial and open-source speech-to-text systems on a random 10-hour sample from PennSound, the world's largest online poetry reading archive. Reference transcripts were created by trained annotators, and systems evaluated included AWS, Azure, Google, IBM, NVIDIA NeMo, Rev.ai, Whisper, and Whisper.cpp. All systems achieved word error rates below 15%, with Rev.ai performing best at 9.0% WER and Whisper showing strong performance at 9.5% WER (when hallucinations were avoided). AWS achieved the best diarization error rate at 14.5%. The results suggest that current speech-to-text technology is sufficiently accurate for effective information retrieval from audio collections like PennSound.

## Method Summary
The study sampled 100 clips (~10 hours) from PennSound, a diverse poetry audio archive, using a random selection algorithm that prioritized files with ≥5 minutes of speech. Audio was preprocessed to 16kHz mono WAV using SoX normalization. Reference transcripts were created by trained annotators. Eight STT systems were evaluated: five cloud APIs (AWS, Azure, Google, IBM, Rev.ai) and three local systems (NeMo, Whisper, Whisper.cpp). Word Error Rate (WER) was measured using NIST SCTK, while Diarization Error Rate (DER) was measured for cloud services supporting speaker attribution. Whisper required specific parameters to avoid hallucinations.

## Key Results
- All systems achieved WER below 15%, demonstrating sufficient accuracy for information retrieval
- Rev.ai achieved best WER at 9.0%, followed by Whisper at 9.5% (with hallucination mitigation)
- AWS achieved best diarization performance at 14.5% DER
- Whisper hallucinations were mitigable using `--word_timestamps` and `--hallucination_silence_threshold` parameters

## Why This Works (Mechanism)

### Mechanism 1: Neural ASR Robustness Across Recording Conditions
- Claim: Modern speech-to-text systems can achieve usable transcription accuracy (<15% WER) even on heterogeneous audio with varying quality, speakers, and styles.
- Mechanism: Transformer-based acoustic encoders learned from diverse training data generalize to unseen recording conditions; integrated language models provide top-down constraint during decoding.
- Core assumption: The diversity within PennSound (studio readings, field recordings, interviews, Q&A) approximates the difficulty distribution of other untranscribed collections.
- Evidence anchors:
  - [abstract] "PennSound's wide variation in recording conditions and speech styles makes it a good representative for many other untranscribed audio collections."
  - [section] "All systems achieved word error rates below 15%, with Rev.ai performing best at 9.0% WER and Whisper showing strong performance at 9.5% WER."
  - [corpus] Limited direct support; neighbor papers focus on specific domains (clinical, cockpit, dysarthria) rather than generalization mechanisms.
- Break condition: Overlapping speech and unintelligible regions cause correlated WER spikes across all systems (Figure 5 shows this pattern).

### Mechanism 2: Whisper Hallucination via Unconstrained Decoding
- Claim: Whisper's decoder can generate text not grounded in the audio signal, particularly during silence or at file boundaries.
- Mechanism: Sequence-to-sequence architecture with language model in decoder; without explicit constraints, decoder may continue generating plausible-but-ungrounded text when acoustic evidence is weak or absent.
- Core assumption: The hallucination behavior is intrinsic to the decoder's autoregressive generation, not a bug.
- Evidence anchors:
  - [section] "Whisper is known to hallucinate, which means that it adds text in silent regions and after the end of the input, as if the transcribed text were a prompt."
  - [section] Table 3 shows Final Insertion Count (FIC) ranging from 27 (small model) to 1601 (large-v3-turbo) when anti-hallucination options are disabled.
  - [corpus] Cites Koenecke et al. [11] "Careless whisper: Speech-to-text hallucination harms" documenting harmful hallucination content.
- Break condition: Using `--word_timestamps` and `--hallucination_silence_threshold` options suppresses hallucinations in the main trial (FIC dropped to 0).

### Mechanism 3: Speaker Diarization via Embedding Clustering
- Claim: Commercial cloud services can attribute speech segments to individual speakers with ~15% diarization error rate.
- Mechanism: Speaker embedding extraction per segment → clustering in embedding space → assignment of segments to speaker labels; evaluated against reference timestamps and speaker IDs.
- Core assumption: Speaker characteristics are sufficiently distinct in embedding space; cluster count estimation is accurate.
- Evidence anchors:
  - [section] "AWS had the best diarization error rate at 14.5%" with Azure at 15.40% and Rev.ai at 15.49%.
  - [section] Figure 3 shows DER variation across recordings, with all three systems following similar difficulty patterns.
  - [corpus] No direct corpus evidence on diarization architecture; neighbor papers focus on ASR accuracy rather than speaker segmentation.
- Break condition: Correlated DER spikes across systems suggest shared failure modes on difficult recordings (likely overlapping speech).

## Foundational Learning

- **Word Error Rate (WER)**
  - Why needed here: Primary metric for comparing ASR systems; components (substitutions, deletions, insertions) reveal different error patterns.
  - Quick check question: If a system outputs "the cat sat" for reference "the cat sat on mat", what is the WER?

- **Diarization Error Rate (DER)**
  - Why needed here: Essential for multi-speaker audio; measures accuracy of speaker attribution, not just word recognition.
  - Quick check question: What three error types contribute to DER, and how does overlapping speech affect scoring?

- **Hallucination in Sequence Models**
  - Why needed here: Whisper-specific failure mode that can produce harmful or misleading output; mitigable with correct runtime options.
  - Quick check question: Why might an autoregressive decoder generate fluent text without acoustic grounding?

## Architecture Onboarding

- **Component map:**
  Audio → SoX normalization → 16kHz mono WAV → ldc-bpcsad SAD → ASR system → transcript

- **Critical path:**
  1. Normalize audio format and levels
  2. Run ASR system with appropriate parameters
  3. For Whisper: enable `--word_timestamps` and `--hallucination_silence_threshold` (0.1–0.5)
  4. Score against reference transcripts using SCTK

- **Design tradeoffs:**
  - Cloud (AWS, Azure, Google, IBM, Rev.ai) vs. local (NeMo, Whisper, Whisper.cpp): cost, privacy, latency, offline capability
  - Whisper model size: tiny→large-v3 trades memory for accuracy; turbo variant is default but has highest hallucination risk without mitigation
  - WER vs. DER optimization: best WER system (Rev.ai at 9.0%) is not best DER system (AWS at 14.5%)

- **Failure signatures:**
  - Whisper hallucinations: inserted text in silent regions or after audio ends; check Final Insertion Count
  - Overlapping speech: all systems show correlated WER spikes; reference transcripts mark these as unintelligible
  - Non-determinism: Whisper produces slightly different transcripts on identical runs

- **First 3 experiments:**
  1. **Hallucination baseline:** Run Whisper large-v3-turbo with and without anti-hallucination options on a held-out sample; measure FIC and IER difference.
  2. **System comparison on stratified difficulty:** Categorize recordings by SNR/overlap; compare whether Rev.ai's advantage on "harder recordings" (noted in Discussion) replicates.
  3. **Diarization failure analysis:** On high-DER recordings, manually inspect whether errors come from missed speech, false alarm speech, or speaker confusion.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the benchmark performance translate to effective information retrieval when scaling to the full PennSound collection?
- Basis in paper: [explicit] The authors state their "next step is therefore to create automatic transcriptions of the full collection, and to implement a web-based front end for searching."
- Why unresolved: While the study establishes that WERs are sufficient for retrieval theoretically, the actual utility of these transcripts for search and discovery across the entire 6,000-hour archive has not been implemented or tested.
- What evidence would resolve it: A user study or quantitative evaluation of retrieval performance (e.g., precision/recall) using the automatically transcribed full corpus.

### Open Question 2
- Question: How do systems compare when evaluated using time-mediated metrics rather than text-only alignment?
- Basis in paper: [explicit] The paper notes that reference transcripts were converted to single segments to ignore timestamps, but "We will also evaluate time-mediated evaluation of word recognition... This level of precision will be relevant in some situations."
- Why unresolved: The current results rely on NIST SCTK scoring that normalizes spelling and ignores segment boundaries; the accuracy of the systems' timestamp predictions remains unquantified.
- What evidence would resolve it: Re-scoring the hypothesis transcripts against the ground truth using alignment-dependent metrics that penalize words recognized correctly but placed in the wrong time segment.

### Open Question 3
- Question: What is the optimal configuration for Whisper's hallucination suppression parameters to balance accuracy and safety?
- Basis in paper: [explicit] The authors caution that "Whisper is an attractive system, but only if hallucinations are avoided" and note that while `hallucination_silence_threshold` is necessary, "the precise value had no obvious effect."
- Why unresolved: The paper demonstrates that runtime options are critical to prevent hallucinations (e.g., "final insertion count"), but a systematic evaluation of which specific threshold values best minimize hallucinations without degrading word recognition is missing.
- What evidence would resolve it: A grid search over `hallucination_silence_threshold` values, measuring both Word Error Rate (WER) and hallucination frequency (IER/FIC) to identify the optimal setting.

## Limitations

- Diarization performance analysis limited to only three cloud services that provided diarization as a service option
- Reference transcript creation process and specific annotation guidelines are not fully specified, which could affect WER calculations
- Study does not evaluate transcription quality on domain-specific vocabulary or literary features unique to poetry readings

## Confidence

**High Confidence Claims:**
- Modern speech-to-text systems achieve sub-15% WER on heterogeneous poetry audio
- Rev.ai demonstrates best overall accuracy (9.0% WER) among evaluated systems
- Whisper hallucinations are mitigable with specific runtime parameters (--word_timestamps and --hallucination_silence_threshold)
- AWS achieves best diarization performance (14.5% DER) among cloud services

**Medium Confidence Claims:**
- PennSound represents a good proxy for other untranscribed audio collections with similar heterogeneity
- The relative ranking of systems would generalize to other poetry archives
- The observed diarization error patterns (correlated spikes across systems) indicate shared failure modes

**Low Confidence Claims:**
- The exact impact of domain-specific vocabulary on system performance
- Whether hallucination behavior generalizes uniformly across all Whisper model sizes
- The long-term stability of these benchmarks as systems continue to evolve

## Next Checks

1. **Domain-Specific Vocabulary Impact:** Conduct a targeted analysis measuring how poetic terminology, proper names of poets, and literary references affect WER across all systems. Compare performance on recordings with high vs. low literary vocabulary density.

2. **Hallucination Robustness Across Model Sizes:** Systematically test all Whisper model variants (tiny through large-v3-turbo) with and without anti-hallucination parameters on a standardized test set containing varied silence patterns. Quantify the relationship between model capacity and hallucination propensity.

3. **Diarization Failure Mode Analysis:** For recordings showing DER > 25%, conduct detailed manual analysis to categorize failure types (missed speech, false alarms, speaker confusion) and determine whether specific acoustic or linguistic features predict diarization failure across all three cloud diarization systems.