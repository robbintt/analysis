---
ver: rpa2
title: 'TensorSLM: Energy-efficient Embedding Compression of Sub-billion Parameter
  Language Models on Low-end Devices'
arxiv_id: '2506.13514'
source_url: https://arxiv.org/abs/2506.13514
tags:
- embedding
- language
- compression
- tensor
- energy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses two key challenges for Small Language Models
  (SLMs) deployed on low-end devices: adaptability to dynamic deployment environments
  and energy efficiency for extended battery life. The authors propose TensorSLM,
  a training-free approach that compresses token embeddings using Tensor-Train Decomposition
  (TTD).'
---

# TensorSLM: Energy-efficient Embedding Compression of Sub-billion Parameter Language Models on Low-end Devices

## Quick Facts
- **arXiv ID**: 2506.13514
- **Source URL**: https://arxiv.org/abs/2506.13514
- **Reference count**: 40
- **Primary result**: TensorSLM achieves ~2.0× embedding compression with 50% energy reduction while maintaining comparable language task performance on Raspberry Pi 5.

## Executive Summary
This paper addresses the challenge of deploying sub-billion parameter language models on low-end devices by proposing TensorSLM, a training-free embedding compression approach using Tensor-Train Decomposition (TTD). The method converts token embeddings into Matrix Product States (MPS), enabling dynamic vocabulary updates without retraining. Evaluated on Raspberry Pi 5 with GPT-2, CerebrasGPT, and OPT models, TensorSLM demonstrates comparable performance to uncompressed models while reducing single-query energy consumption by half and achieving approximately 2.0× compression in the embedding layer.

## Method Summary
TensorSLM compresses token embeddings by reshaping each embedding vector into an order-N tensor and applying Tensor-Train SVD to decompose it into smaller MPS cores. During inference, embeddings are reconstructed from these cores on-the-fly. The approach enables dynamic vocabulary updates by compressing individual vectors rather than the entire embedding matrix. The method is training-free and specifically designed for sub-billion parameter language models where embedding layers consume a larger proportion of parameters.

## Key Results
- Achieves approximately 2.0× compression of embedding layers while maintaining comparable language modeling performance
- Reduces single-query energy consumption by approximately 50% on Raspberry Pi 5
- Demonstrates particular strength in unprompted and unconstrained reasoning tasks compared to matrix-based SVD approaches
- Shows minimal latency increase (<0.3s per text) despite on-the-fly reconstruction overhead

## Why This Works (Mechanism)

### Mechanism 1: Energy Reduction via Memory-Computation Trade-off
The approach reduces inference energy consumption primarily by decreasing memory access overhead. Memory operations (70-260 pJ/float32 on Raspberry Pi 5) are significantly more energy-expensive than arithmetic operations (addition: 1.0-2.5 pJ/float32). By storing embeddings in compressed MPS format, memory footprint is reduced from O(Vd) to O(VNIr²), trading memory energy cost for smaller computation energy cost during reconstruction.

### Mechanism 2: Preservation of Task Performance via Low-Rank Tensor Representation
Representing token embeddings as low-rank Tensor-Train Decompositions preserves task performance by capturing essential multi-way correlations with fewer parameters. The paper hypothesizes that higher-order tensors are more suitable for small models to model complex patterns than 2D matrices, with performance preserved up to ~2.0× compression ratio.

### Mechanism 3: Adaptability through Vector-Level Compression
Compressing embeddings at individual vector level enables dynamic vocabulary updates on-device without requiring re-decomposition of the entire embedding matrix. This allows edge devices to add new tokens by compressing their embedding vectors locally and register them, or remove tokens by deletion.

## Foundational Learning

- **Concept**: Tensor-Train Decomposition (TTD) / Matrix Product State (MPS)
  - Why needed: Core mathematical tool for compressing embeddings by factorizing high-order tensors into chains of smaller tensors
  - Quick check: Given tensor shape I₁ × I₂ × I₃ and TT-ranks (r₀, r₁, r₂, r₃), what is the total number of parameters in the MPS representation?

- **Concept**: Energy Efficiency in Edge Computing (Memory vs. Computation)
  - Why needed: Paper's primary claim to energy efficiency rests on memory access consuming significantly more power than computation on low-end devices
  - Quick check: On a typical edge processor, which operation is more energy-intensive: reading a 32-bit float from DRAM or multiplying two 32-bit floats?

- **Concept**: Sub-billion Parameter Language Models (SLMs)
  - Why needed: SLMs have different characteristics than LLMs, with embedding layers consuming larger percentage of total parameters
  - Quick check: In a typical SLM architecture, which layer type tends to consume a larger percentage of total parameters compared to much larger LLMs?

## Architecture Onboarding

- **Component map**: Compression (Server/Edge) -> Inference (Edge) -> Vocabulary Update (Edge)
  1. **Compression**: Takes pre-trained token embedding vector x, tensorizes it, runs TT-SVD to produce MPS cores {G^(k)}
  2. **Inference**: Retrieves MPS cores for input tokens, reconstructs full embedding vectors via tensor contraction, feeds to transformer
  3. **Vocabulary Update**: Adds new tokens by running compression phase for their vectors or deletes tokens by removing compressed representations

- **Critical path**: On-demand reconstruction of embedding vector from MPS cores during inference forward pass, a serial process of matrix multiplications

- **Design tradeoffs**:
  - Compression Ratio vs. Performance: Higher compression saves more energy/memory but degrades task performance
  - Compression Ratio vs. Latency: Higher compression often increases reconstruction latency due to more serial matrix operations
  - Vector-level vs. Matrix-level: Vector-level enables adaptability but may miss global cross-token correlations

- **Failure signatures**:
  - Sudden performance drop (PPL > 100) indicates compression is too aggressive
  - Out-of-memory errors if vocabulary size V is not much larger than input length l
  - Unexpectedly high latency on CPU due to serial nature of MPS reconstruction

- **First 3 experiments**:
  1. Measure baseline perplexity and zero-shot scores on target SLM without compression
  2. Implement and benchmark TT-SVD on single embedding vector to measure compression and reconstruction latency
  3. Perform sweep of TT-ranks and tensor orders to plot trade-off curves and identify performance drop threshold

## Open Questions the Paper Calls Out

- Can tensorization be effectively extended to hidden layers (attention and feed-forward) to achieve native tensor compilation throughout the model?
- What theoretical or empirical factors explain why tensor-based compression outperforms matrix-based SVD specifically on unprompted, unconstrained reasoning tasks?
- How can optimal tensor dimensions and TT-ranks be systematically selected without exhaustive search?
- Can specialized CPU tensor operation kernels further reduce the arithmetic overhead of reconstruction during inference?

## Limitations

- Performance degradation occurs beyond 2.0× compression ratio, limiting maximum compression potential
- High-order tensors increase reconstruction latency due to serial matrix operations, potentially problematic for CPU inference
- Optimal tensor dimensions and TT-ranks require empirical tuning due to exponential hyperparameter space

## Confidence

- **Method validity**: High - based on well-established tensor decomposition techniques with clear mathematical foundation
- **Energy measurements**: Medium - relies on estimated energy costs based on per-float32 operations which may not perfectly model real-world consumption
- **Task performance claims**: High - supported by multiple benchmarks across different model types and tasks
- **Adaptability claims**: High - vector-level compression inherently enables the described dynamic update capabilities

## Next Checks

1. Verify the exact tensor shapes used for specific models like GPT-2 or OPT in the main results, as this significantly impacts compression performance
2. Implement and test the energy estimation formula to ensure it accurately predicts real-world energy consumption on target devices
3. Measure actual latency overhead on target hardware to confirm the claimed <0.3s increase per text remains valid for the specific implementation