---
ver: rpa2
title: 'LLM-Crowdsourced: A Benchmark-Free Paradigm for Mutual Evaluation of Large
  Language Models'
arxiv_id: '2507.22359'
source_url: https://arxiv.org/abs/2507.22359
tags:
- evaluation
- question
- llms
- answer
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: League of LLMs (LOL) is a benchmark-free evaluation paradigm that
  organizes multiple LLMs into a self-governed league for multi-round mutual evaluation.
  It mitigates data contamination by dynamically generating questions, enhances transparency
  through open and traceable processes, and reduces subjective bias via decentralized
  multi-LLM evaluation.
---

# LLM-Crowdsourced: A Benchmark-Free Paradigm for Mutual Evaluation of Large Language Models

## Quick Facts
- arXiv ID: 2507.22359
- Source URL: https://arxiv.org/abs/2507.22359
- Reference count: 40
- Primary result: A benchmark-free LLM evaluation framework using mutual peer review to avoid data contamination and reveal capability patterns

## Executive Summary
League of LLMs (LOL) is a novel evaluation paradigm that eliminates the need for fixed benchmarks by organizing multiple LLMs into a self-governed league for multi-round mutual evaluation. Instead of traditional one-way testing, models take turns generating questions, answering them, and peer-evaluating responses. This approach mitigates data contamination through dynamic question generation, enhances transparency through open processes, and reduces subjective bias via decentralized multi-LLM evaluation. Experiments with eight mainstream LLMs in mathematics and programming demonstrate that LOL effectively differentiates model capabilities while maintaining high internal ranking stability.

## Method Summary
LOL operates through a three-phase cycle: (1) a "Questioner" model dynamically generates an original question and reference answer based on abstracted principles, (2) remaining models answer independently, and (3) all models evaluate the answers against the reference. This process repeats for 8 rounds with 5 independent runs per set. For mathematics, LOL uses Borda count ranking to handle diverse solution paths, while programming tasks use absolute 0-100 scoring due to binary correctness. The framework employs strict JSON-based prompts to ensure structured data exchange and prevents self-evaluation through careful prompt engineering.

## Key Results
- Achieves 70.7% top-k consistency in internal rankings across 5 runs
- Successfully differentiates model capabilities in both mathematics and programming
- Identifies "memorization-based answering" behaviors in some models
- Reveals statistically significant homophily bias within the OpenAI family (Î” = 9, p < 0.05)

## Why This Works (Mechanism)

### Mechanism 1: Dynamic Principle-Abstraction for Contamination Avoidance
The system generates questions based on abstracted principles rather than retrieval, forcing models to synthesize novel instances rather than reproduce memorized problems. This breaks the statistical link between test sets and pre-training corpora. Core assumption: LLMs can generate valid, solvable problems from abstract principles without reproducing training data verbatim. Break condition: If generated questions are too derivative of common training distributions, the dynamic advantage collapses.

### Mechanism 2: Decentralized Mutual Evaluation (Cross-Examination)
Aggregating scores from N-1 peer models cancels out individual biases more effectively than single judge models. The "Triangular Consensus" ensures if Model A has systematic bias toward Model B, Models C through N dilute the effect. Core assumption: Error vectors of different LLM families are sufficiently decorrelated. Break condition: If the model pool is dominated by one family, family bias becomes systemic consensus.

### Mechanism 3: Reference-Anchored Relative Ranking (Borda Rule)
For mathematical domains with diverse solution paths, converting absolute scores to relative ranks stabilizes leaderboard positions against scoring scale variance. Instead of asking "Is this a 90/100?", the system asks "Is this better than the other 7 answers?" Core assumption: Evaluators are better at pairwise comparison than absolute scoring. Break condition: If answers are not strictly comparable, forced ranking may introduce artificial noise.

## Foundational Learning

- **Data Contamination**: Why needed: The paper defines its entire value proposition against this problem. Quick check: Why does the paper prohibit the questioner LLM from answering its own question?

- **Homophily Bias**: Why needed: Section 5.2 reveals OpenAI models favor other OpenAI models. Quick check: If you evaluated GPT-4 using only GPT-3.5 and GPT-4o, what specific artifact might appear in the scores?

- **Memorization-Based Answering**: Why needed: This is the failure mode the paper detects (Finding 2). Quick check: How does the "Dirichlet series" example demonstrate this failure mode?

## Architecture Onboarding

- **Component map**: Orchestrator -> Agent Pool -> Context Manager -> Aggregator
- **Critical path**: Prompt Phase (inject Principle/Constraints) -> Isolation Phase (distribute Q+Ref to N-1) -> Evaluation Phase (inject Answers+Ref to N-1) -> Aggregation (convert rankings to points)
- **Design tradeoffs**: Cost vs. Robustness (392 data points per set is expensive); Borda (Math) vs. Absolute (Code) (Math uses relative ranking because paths vary)
- **Failure signatures**: Template Collapse (known benchmark problems), JSON Syntax Errors (malformed output breaks pipeline), Homophily Echo Chambers (model lineage, not capability)
- **First 3 experiments**: 
  1. Sanity Check (Single Round): 3 models on single math question, verify JSON parsing and Borda calculation
  2. Homophily Probe: 3 models from Family A + 1 from Family B, measure score delta given by each family
  3. Memorization Stress Test: Obscure principle question, verify "Hallucinated/Memorized" answers

## Open Questions the Paper Calls Out

- **Open Question 1**: How to adapt LOL for open-ended tasks like dialogue or creative writing without reference answers? The current reference answer mechanism may be less effective for such domains.

- **Open Question 2**: What scheduling or sampling strategies can reduce interaction overhead and API costs while maintaining ranking stability? The current multi-model, multi-round approach incurs significant computational expense.

- **Open Question 3**: How to integrate automated verification to detect correlated errors among LLM evaluators sharing similar training biases? Stronger automated verification and calibration are needed to address family-level bias.

## Limitations
- Requires multiple high-cost API calls due to multi-round mutual evaluation
- Current framework relies on reference answers, limiting applicability to open-ended tasks
- Homophily bias can persist within model families despite aggregation

## Confidence
- **High confidence**: Core mechanism (dynamic generation + mutual evaluation + Borda aggregation) is sound and well-specified
- **Medium confidence**: Empirical findings (Top-k consistency, homophily detection) are plausible but require exact prompt replication
- **Low confidence**: Borda ranking superiority claim lacks strong external validation beyond code correlation

## Next Checks
1. Sanity run with 3 diverse models on a single math question to verify JSON parsing, Borda scoring, and exclusion logic
2. Homophily probe experiment with grouped model families to measure self-favoring bias delta
3. Memorization stress test using an obscure principle to detect "hallucinated" standard answers (Finding 2 validation)