---
ver: rpa2
title: 'A Neural Affinity Framework for Abstract Reasoning: Diagnosing the Compositional
  Gap in Transformer Architectures via Procedural Task Taxonomy'
arxiv_id: '2512.07109'
source_url: https://arxiv.org/abs/2512.07109
tags:
- tasks
- affinity
- accuracy
- task
- architectural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a 9-category taxonomy of the re-arc benchmark,
  validated at 97.5% accuracy via code analysis and 95.24% on raw visual data. The
  taxonomy reveals 35.3% of tasks fall into categories with low neural affinity for
  Transformers.
---

# A Neural Affinity Framework for Abstract Reasoning: Diagnosing the Compositional Gap in Transformer Architectures via Procedural Task Taxonomy

## Quick Facts
- **arXiv ID**: 2512.07109
- **Source URL**: https://arxiv.org/abs/2512.07109
- **Reference count**: 40
- **Primary result**: 69.5% of ARC tasks exhibit a Compositional Gap—high cell-level accuracy (>80%) but low grid-level accuracy (<10%)—revealing Transformers' inability to synthesize global patterns despite local pattern mastery

## Executive Summary
This paper introduces a neural affinity framework that diagnoses why Transformer architectures struggle with abstract reasoning tasks. Through a 9-category procedural taxonomy validated at 97.5% accuracy via code analysis and 95.24% on visual data, the authors identify that 35.3% of tasks fall into categories with low neural affinity for Transformers. Fine-tuning experiments on 302 ARC tasks reveal a pervasive Compositional Gap: while Transformers achieve >80% accuracy at individual cell predictions, they fail to achieve >10% grid-level accuracy, indicating mastery of local patterns without global synthesis. The framework successfully predicts performance on external specialist models, showing 25.8 percentage points lower performance on low-affinity tasks (p<0.001).

## Method Summary
The authors developed a 9-category procedural taxonomy to classify ARC benchmark tasks based on their compositional requirements. This taxonomy was validated through automated code analysis (97.5% agreement) and visual validation on raw data (95.24% agreement). They conducted fine-tuning experiments across 302 ARC tasks to measure both cell-level and grid-level accuracy, defining the Compositional Gap as the phenomenon where models achieve high cell accuracy (>80%) but fail at grid-level synthesis (<10%). The framework was applied to ARC-AGI-2, revealing that 68.6% of failures concentrate in low-affinity categories. External validation was performed using Li et al.'s 400 specialist ViTARC models, confirming the framework's predictive power for identifying challenging task categories.

## Key Results
- 35.3% of tasks fall into categories with low neural affinity for Transformers
- 69.5% of tasks exhibit a Compositional Gap (high cell accuracy >80%, low grid accuracy <10%)
- Low-affinity tasks show 25.8 percentage points lower performance in external validation (p<0.001)
- One task achieved 0% accuracy despite 1M training examples

## Why This Works (Mechanism)
The neural affinity framework works by decomposing abstract reasoning tasks into compositional components that map to specific cognitive operations. Transformers excel at local pattern recognition due to their attention mechanisms but struggle with global synthesis required for complete solution generation. The framework identifies this gap by measuring performance at two granularities: individual cell predictions (where Transformers perform well) versus complete grid-level solutions (where they fail). This dual-level assessment reveals that models can master subcomponents without integrating them into coherent solutions, exposing a fundamental limitation in current Transformer architectures for abstract reasoning.

## Foundational Learning
- **Procedural Task Taxonomy**: A 9-category system classifying ARC tasks by compositional requirements
  - *Why needed*: To systematically identify which task structures challenge Transformer architectures
  - *Quick check*: Can all ARC tasks be classified into exactly one category without overlap?
- **Cell-level vs Grid-level Accuracy**: Dual-granularity performance measurement
  - *Why needed*: To distinguish between local pattern mastery and global synthesis capability
  - *Quick check*: Does cell accuracy >80% consistently predict grid-level failure?
- **Neural Affinity Concept**: Task-specific compatibility between model architecture and problem structure
  - *Why needed*: To predict which tasks will be challenging for specific architectures
  - *Quick check*: Does affinity score correlate with actual performance across different model families?

## Architecture Onboarding
**Component Map**: Procedural Taxonomy Classification -> Cell-level Accuracy Measurement -> Grid-level Accuracy Measurement -> Compositional Gap Detection -> Performance Prediction
**Critical Path**: Taxonomy creation → Validation → Fine-tuning experiments → Gap identification → External validation
**Design Tradeoffs**: The framework sacrifices model complexity for interpretability, focusing on compositional decomposition rather than end-to-end performance metrics
**Failure Signatures**: High cell accuracy (>80%) combined with low grid accuracy (<10%) indicates Compositional Gap
**First Experiments**:
1. Apply taxonomy to new ARC-AGI-2 tasks to test predictive power
2. Test threshold sensitivity by varying cell (70-90%) and grid (5-15%) accuracy cutoffs
3. Compare Compositional Gap detection across different Transformer variants (ViT, Swin, etc.)

## Open Questions the Paper Calls Out
None

## Limitations
- Taxonomic categories may not capture all forms of abstract reasoning complexity
- Compositional Gap thresholds (80% cell, 10% grid) may not generalize across domains
- Framework's predictive power for novel task categories not yet validated

## Confidence
- **High confidence**: Taxonomic validation methodology and statistical significance of affinity-performance correlations
- **Medium confidence**: Universality of 9-category taxonomy across different abstract reasoning domains
- **Medium confidence**: Threshold-based Compositional Gap definition as diagnostic tool
- **Low confidence**: Framework's predictive power for future, unseen task categories

## Next Checks
1. Test framework generalizability on ARC-AGI-2's upcoming tasks to verify predictive accuracy for novel categories
2. Conduct ablation studies removing individual taxonomy categories to measure each category's unique contribution to predictive power
3. Apply the framework to non-visual abstract reasoning tasks (e.g., symbolic reasoning or natural language inference) to assess domain transfer validity