---
ver: rpa2
title: Assessing the Capability of Large Language Models for Domain-Specific Ontology
  Generation
arxiv_id: '2504.17402'
source_url: https://arxiv.org/abs/2504.17402
tags:
- ontology
- llms
- generation
- domains
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explores the use of large language models (LLMs) for
  domain-specific ontology generation, focusing on two state-of-the-art models, DeepSeek
  and o1-preview, equipped with reasoning capabilities. A dataset of 95 competency
  questions (CQs) and corresponding user stories across six distinct domains was curated
  to evaluate performance.
---

# Assessing the Capability of Large Language Models for Domain-Specific Ontology Generation

## Quick Facts
- **arXiv ID:** 2504.17402
- **Source URL:** https://arxiv.org/abs/2504.17402
- **Reference count:** 16
- **Primary result:** Reasoning-enhanced LLMs (DeepSeek and o1-preview) consistently generate domain-specific ontologies from competency questions across diverse domains.

## Executive Summary
This study investigates whether large language models equipped with reasoning capabilities can effectively generate domain-specific ontologies from competency questions (CQs) and user stories. Using a dataset of 95 CQs across six domains, the research demonstrates that reasoning models can generalize ontology generation tasks irrespective of domain. The approach leverages few-shot prompting to guide models in producing OWL ontologies in Turtle format, with consistent accuracy across both simple and complex CQs.

## Method Summary
The study employs reasoning-capable LLMs (DeepSeek and o1-preview) with few-shot prompting to generate ontologies from competency questions and user stories. Using an "Independent Ontology Generation" approach, each CQ is processed separately through a structured prompt containing instructions, a fixed example, and the input text. The generated Turtle syntax ontologies are evaluated based on whether they can answer the original CQ via SPARQL queries, with errors classified as either "not modelled" or "minor issues" (missing exactly one property).

## Key Results
- Both DeepSeek and o1-preview achieved high and consistent accuracy across all six domains tested
- Performance remained stable regardless of CQ difficulty level (Easy/Hard)
- Few-shot prompting significantly outperformed zero-shot approaches in ontology generation accuracy
- The "Events" domain showed slightly lower performance, attributed to shorter, less descriptive input CQs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Reasoning-enhanced LLMs decouple ontology generation capability from specific domain knowledge, allowing generalization across diverse topics.
- **Mechanism:** Models utilize abstract reasoning patterns to map natural language requirements to formal structures without relying solely on domain-specific training data.
- **Core assumption:** Performance consistency derives from reasoning architecture rather than domain-specific token volume in pre-training data.
- **Evidence anchors:** Abstract states "LLMs can generalize ontology generation tasks irrespective of domain" and section 7 confirms reasoning models generalize effectively across datasets.

### Mechanism 2
- **Claim:** Few-shot prompting acts as a rigid structural template, reducing cognitive load on the model regarding syntax.
- **Mechanism:** Providing concrete examples anchors output to provided Turtle syntax and modeling style, constraining the solution space.
- **Core assumption:** Improved performance results from pattern matching the example's structure rather than explicit instructions alone.
- **Evidence anchors:** Section 4.2 notes few-shot prompting "has proven more accurate" compared to trials without examples.

### Mechanism 3
- **Claim:** Input context completeness (length and detail of User Stories/CQs) serves as a critical signal for the model to resolve ambiguity.
- **Mechanism:** Models use descriptive density of input to infer implicit relationships; shorter inputs provide fewer semantic anchors.
- **Core assumption:** Errors in the "Events" domain were caused by input brevity rather than domain complexity.
- **Evidence anchors:** Section 7 attributes errors to shorter CQs and suggests longer, more descriptive inputs could improve performance.

## Foundational Learning

- **Concept: Competency Questions (CQs)**
  - **Why needed here:** CQs are functional requirements for the ontology; study evaluates success based on whether generated ontology can answer these questions via SPARQL.
  - **Quick check question:** Can you distinguish between a natural language question (CQ) and its formal implementation (SPARQL query)?

- **Concept: Turtle Syntax (RDF/OWL)**
  - **Why needed here:** Target output format; LLM must translate natural language into specific triple format (Subject-Predicate-Object).
  - **Quick check question:** Can you read a simple RDF triple in Turtle format (e.g., `:Person a :Class ; :hasName "Alice" .`)?

- **Concept: Independent Ontology Generation**
  - **Why needed here:** Study uses "Independent" method, generating ontology for each CQ in isolation rather than building one large ontology.
  - **Quick check question:** Why might testing CQs in isolation make evaluation easier compared to generating a monolithic ontology for 95 questions at once?

## Architecture Onboarding

- **Component map:** Input (User Story + CQ) -> Prompting Layer (Few-shot template) -> Inference Engine (Reasoning LLM) -> Output (OWL ontology in Turtle) -> Validator (Human expert or SPARQL execution)

- **Critical path:** Mapping from Natural Language (CQ) -> Formal Logic (OWL) depends heavily on the few-shot example. Poor examples will propagate structural errors.

- **Design tradeoffs:**
  - o1-preview vs. DeepSeek: o1-preview may leave "minor issues" (missing one property), while DeepSeek had higher incidence of incomplete modeling in specific domains but fewer minor errors overall
  - Few-shot vs. Zero-shot: Few-shot is strictly better but requires curation of high-quality example

- **Failure signatures:**
  - Short Inputs: Domains with short CQs (e.g., "Events") showed higher error rates
  - Missing Properties: Ontology captures class but fails to define specific ObjectProperty or DataProperty needed to answer CQ

- **First 3 experiments:**
  1. Baseline Replication: Run provided few-shot prompt with o1-preview on single domain (e.g., Music) and check if output is valid Turtle syntax
  2. Input Stress Test: Take successful, descriptive CQ and strip down to 5 words; observe if model fails to model necessary relationships
  3. Model Swap: Run exact same prompt with non-reasoning model (e.g., standard GPT-4) and compare frequency of "minor issues" or structural logic errors

## Open Questions the Paper Calls Out

- **Question 1:** How does performance of reasoning models compare to non-reasoning or smaller LLMs in this specific ontology generation pipeline?
  - **Basis:** Conclusion states "Future research will build on these results to explore additional LLMs" and Limitations notes exclusive use of resource-intensive reasoning models restricts scope
  - **Why unresolved:** Study focused only on DeepSeek and o1-preview to assess reasoning capabilities, leaving performance baseline of standard or smaller models undetermined
  - **What evidence would resolve it:** Comparative evaluation using standard instruct models (e.g., GPT-4o or Llama 3) on same dataset to measure performance gap

- **Question 2:** To what extent does standardization of requirement formats (e.g., CQ length and user story count) impact accuracy of generated ontologies?
  - **Basis:** Discussion attributes errors in "Events" domain to shorter CQs and multiple user stories, concluding "lack of standardization in requirements is also fundamental"
  - **Why unresolved:** Dataset utilized existing requirements with varying structures, making it difficult to isolate whether errors stemmed from model incapacity or inconsistent input formatting
  - **What evidence would resolve it:** Controlled ablation study where CQs and stories are normalized for length and complexity before generation to measure effect on consistency

- **Question 3:** How can influence of pre-training data leakage on ontology generation performance be effectively isolated or mitigated?
  - **Basis:** Limitations highlights "dataset leakage" as risk, noting LLMs may have encountered specific data points during pre-training which could inflate performance metrics
  - **Why unresolved:** Currently difficult to distinguish between model's reasoning capabilities and simple recollection of ontological patterns seen during training
  - **What evidence would resolve it:** Evaluation using domains or competency questions created after models' knowledge cutoff dates, or use of synthetic, novel domains guaranteed absent from training data

## Limitations
- Relies on a fixed few-shot example whose quality and completeness are critical yet not detailed in abstract
- Performance evaluated on curated dataset of 95 CQs; real-world CQs may be noisier and less structured
- Distinction between "reasoning capability" and "pattern matching" not empirically validated

## Confidence
- **High Confidence:** Few-shot prompting significantly improves ontology generation accuracy compared to zero-shot approaches
- **Medium Confidence:** Reasoning models generalize across domains is plausible given consistent performance, but lacks ablation studies to confirm mechanism
- **Low Confidence:** Claim that approach is "scalable and domain-agnostic" for all domains is overstated without testing on truly low-resource or highly specialized domains

## Next Checks
1. **Example Sensitivity Test:** Systematically vary quality of few-shot example (introduce subtle modeling errors) and measure impact on output accuracy to confirm model is pattern-matching example
2. **Domain Transfer Stress Test:** Apply same prompt to domain outside six tested (e.g., aerospace engineering or legal ontologies) and compare performance degradation to quantify true generalization limits
3. **Reasoning vs. Retrieval Ablation:** Compare current reasoning models against non-reasoning model (e.g., GPT-4) using identical few-shot prompts to isolate contribution of reasoning architecture to consistent performance