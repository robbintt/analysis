---
ver: rpa2
title: 'The Two-Stage Decision-Sampling Hypothesis: Understanding the Emergence of
  Self-Reflection in RL-Trained LLMs'
arxiv_id: '2601.01580'
source_url: https://arxiv.org/abs/2601.01580
tags:
- sample
- gradient
- policy
- reward
- decision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The Two-Stage Decision-Sampling Hypothesis provides a mechanistic
  framework explaining why reinforcement learning induces self-reflection capabilities
  in large language models while supervised fine-tuning fails. By decomposing the
  model's policy into sampling and decision components, the analysis reveals that
  surrogate rewards exhibit balanced gradient attribution while KL penalties and SFT
  objectives exhibit unbalanced gradient attribution.
---

# The Two-Stage Decision-Sampling Hypothesis: Understanding the Emergence of Self-Reflection in RL-Trained LLMs

## Quick Facts
- **arXiv ID**: 2601.01580
- **Source URL**: https://arxiv.org/abs/2601.01580
- **Reference count**: 40
- **Key outcome**: RL induces self-reflection by balancing gradient attribution between sampling and decision components, while SFT creates unbalanced gradients that prevent error detection and correction.

## Executive Summary
The Two-Stage Decision-Sampling Hypothesis provides a mechanistic framework explaining why reinforcement learning induces self-reflection capabilities in large language models while supervised fine-tuning fails. By decomposing the model's policy into sampling (generating content) and decision (verifying and correcting) components, the analysis reveals that surrogate rewards exhibit balanced gradient attribution while KL penalties and SFT objectives exhibit unbalanced gradient attribution. This gradient asymmetry explains why RL can develop effective decision-making for error detection and correction, while SFT produces models that echo their first answers without genuine self-correction.

## Method Summary
The study compares reinforcement learning with supervised fine-tuning on multi-digit multiplication tasks, using a two-stage decomposition of policy gradients. SFT is implemented via LLaMA-Factory with standard hyperparameters, while RL uses VeRL with GRPO. The analysis tracks gradient attribution to sampling versus decision components, using calibration parameters p_s (sampling accuracy), p_{d|C} (stop probability given correct), and p_{d|W} (resample probability given incorrect) to quantify self-correction behavior. Empirical validation on arithmetic reasoning tasks confirms RL's superior generalization stems from improved decision quality rather than sampling capabilities.

## Key Results
- RL models maintain discriminative ability to reject incorrect answers (p_{d|W} > 0.4) on out-of-distribution tasks, while SFT collapses to near-zero resampling probability
- The Two-Stage framework accurately predicts accuracy using calibration parameters: p_s × p_{d|C} / (1 − (p_s × (1−p_{d|C}) + (1−p_s) × p_{d|W}))
- Gradient attribution analysis shows balanced signal between sampling and decision components under surrogate rewards, but unbalanced accumulation under token-level KL penalties

## Why This Works (Mechanism)

### Mechanism 1: Balanced Gradient Attribution via Surrogate Rewards
Surrogate rewards distribute gradient signal symmetrically between content generation (π_sample) and verification (π_d) components, enabling coherent joint learning. The advantage A_i is a trajectory-level scalar that multiplies both sampling and decision gradients equally. Both Q-values reduce to the same sufficient statistic Φ(s_k) = γ^Σlen(·)·A_i, meaning a unified value representation serves both policy functions.

### Mechanism 2: Token-Level KL Penalty Creates Length-Weighted Asymmetry
Standard token-level KL penalties impose O(L_k) regularization on π_sample but only O(1) on π_d, systematically constraining content generation while leaving decision-making under-regularized relative to the reward signal. Each sampling action comprises L_k tokens, accumulating KL divergence across all tokens, while decision actions are single tokens.

### Mechanism 3: SFT Mimics KL-Like Objective Without Reward Counterbalance
SFT's implicit reward structure (inversely proportional to π_θ) creates policy-entangled Q-values that prevent clean credit assignment between generation and verification. The implicit Q-values contain 1/π_d terms in Q_sample and 1/π_sample terms in Q_d, creating "miss-meshing" where updates intended for one function interfere with learning the other.

## Foundational Learning

- **Policy Gradient Theorem**: Essential for understanding how gradients flow through policies via Q-values and how gradient decomposition enables attribution analysis.
  - Quick check: Can you explain why ∇_θ log π(a|s) appears in the policy gradient, and what role Q^π(s,a) plays in weighting these gradients?

- **KL Divergence in Regularization**: Critical for understanding the core asymmetry from token-level KL penalties accumulating differently across action types.
  - Quick check: Why would summing KL over L_k tokens for sampling actions vs 1 token for decisions create different effective regularization strengths?

- **Sufficient Statistics in Value Functions**: The paper's definition of balanced attribution hinges on whether a single sufficient statistic Φ can serve both Q_sample and Q_d.
  - Quick check: If two Q-functions require different information about future rewards, why can't a single neural network learn both effectively from a shared objective?

## Architecture Onboarding

- **Component map**: π_sample (implicit reasoning traces and candidate answers) → π_d (revision markers: "I'll go back and check", "Let me recompute", "I made a mistake" → STOP/RESAMPLE behavior) → Advantage A_i (GRPO's group-relative normalization) → KL penalty (token-level log-ratio sum)

- **Critical path**: 1) Identify decision boundaries in outputs using lexical markers → RESAMPLE actions, 2) Estimate p_s from first-attempt accuracy, p_{d|C} from stop rates on correct answers, p_{d|W} from resample rates on incorrect answers, 3) Validate calibration: model accuracy should match (p_s · p_{d|C}) / (1 - (p_s·(1-p_{d|C}) + (1-p_s)·p_{d|W})), 4) Compare RL vs SFT: key diagnostic is whether p_{d|W} remains >0 OOD (RL) or collapses toward zero (SFT)

- **Design tradeoffs**: Trajectory-level vs token-level rewards (trajectory-level enables balanced attribution; token-level reintroduces length asymmetry), KL coefficient strength (higher KL stabilizes training but amplifies asymmetry; paper uses 0.001), reference policy update frequency (frequent updates reduce KL drift but may reintroduce cyclical patterns)

- **Failure signatures**: p_{d|W} → 0 on OOD tasks (indicates decision policy failed to generalize), first-attempt accuracy high but overall accuracy low (sampling works, decision policy broken), high KL gradient magnitude on sampling vs decision (>2:1 ratio) (asymmetry confirmed, may need lower KL coefficient)

- **First 3 experiments**: 1) **Calibration check**: On held-out validation set, compute (p_s, p_{d|C}, p_{d|W}) and verify predicted accuracy matches observed, 2) **OOD generalization test**: Train on in-distribution (e.g., 3×3, 3×4 arithmetic), evaluate on harder OOD (3×7, 3×8), compare RL vs SFT: RL should maintain p_{d|W} > 0.4 while SFT should collapse toward zero, 3) **Gradient magnitude audit**: During training, log |∇_θ J_sample| vs |∇_θ J_d| for KL and reward components separately, confirm reward contributions are O(1):O(1) and KL contributions are O(L):O(1)

## Open Questions the Paper Calls Out

### Open Question 1
Does the Two-Stage Decision-Sampling Hypothesis and the gradient attribution mechanism generalize beyond arithmetic reasoning to complex domains with semantic verification, such as code generation or logical deduction? Empirical validation is conducted on arithmetic reasoning; extension to more complex domains remains future work.

### Open Question 2
How do practical stability mechanisms, such as PPO clipping or TRPO trust regions, quantitatively alter the theoretical predictions of Balanced Gradient Attribution? The theoretical analysis abstracts away practical considerations such as clipping and trust regions.

### Open Question 3
How does the degradation of sampling accuracy over long contexts (length generalization) affect the calibration of the decision policy π_d? The calibration model assumes a simplified two-stage structure that does not fully capture full realistic model behavior such as accuracy deterioration over length generalization.

## Limitations
- The framework's core assumptions about balanced gradient attribution hinge on the surrogate reward accurately reflecting trajectory quality and both policy components meaningfully contributing to outcomes
- The O(L_k) vs O(1) asymmetry in KL penalties assumes token-level divergences are of comparable magnitude, which may not hold for all models or tasks
- The SFT analysis as implicit policy gradient with 1/π_θ rewards relies on prior work (Wu et al., 2025) that may have limitations in different contexts

## Confidence

- **High confidence**: The empirical validation showing RL models maintain discriminative ability to reject incorrect answers while SFT collapses to near-zero on OOD tasks (Figure 3, Section 5.2). The calibration framework and accuracy predictions are well-specified and testable.
- **Medium confidence**: The theoretical proofs of balanced vs unbalanced gradient attribution (Theorems 3.1-3.2). While mathematically sound, these rely on assumptions about advantage quality and token-level divergence magnitudes that may not generalize across all RL algorithms or model architectures.
- **Medium confidence**: The SFT analysis as implicit policy gradient with policy-entangled Q-values. This builds on prior work (Wu et al., 2025) and the empirical evidence from Kang et al. (2025), but the mechanism may be more complex in practice with different training data distributions.

## Next Checks

1. **Gradient magnitude audit during training**: Log |∇_θ J_sample| vs |∇_θ J_d| for KL and reward components separately across multiple training steps. Verify reward contributions remain O(1):O(1) while KL contributions show O(L):O(1) asymmetry.

2. **Advantage quality sensitivity analysis**: Systematically vary the advantage estimation method (e.g., baseline subtraction, normalization techniques) and measure the impact on decision policy generalization.

3. **Cross-task calibration validation**: Apply the two-stage calibration framework to a different task domain (e.g., code generation or commonsense reasoning) with varying levels of difficulty. Test whether the same p_s, p_{d|C}, p_{d|W} decomposition holds and whether RL maintains superior OOD generalization patterns across domains.