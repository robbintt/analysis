---
ver: rpa2
title: 'PROTEA: Securing Robot Task Planning and Execution'
arxiv_id: '2601.07186'
source_url: https://arxiv.org/abs/2601.07186
tags:
- task
- malicious
- plans
- protea
- safety
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces PROTEA, a defense mechanism designed to secure
  robot task planning against adversarial attacks. The core idea is to use an LLM-as-a-judge
  with object filtering and external memory to evaluate the safety of task plans step-by-step.
---

# PROTEA: Securing Robot Task Planning and Execution

## Quick Facts
- **arXiv ID:** 2601.07186
- **Source URL:** https://arxiv.org/abs/2601.07186
- **Reference count:** 40
- **One-line primary result:** PROTEA improves adversarial detection for non-CoT models but degrades precision for CoT models due to over-cautiousness.

## Executive Summary
This paper introduces PROTEA, a defense mechanism designed to secure robot task planning against adversarial attacks. The core idea is to use an LLM-as-a-judge with object filtering and external memory to evaluate the safety of task plans step-by-step. To evaluate the method, the authors created HarmPlan, a dataset containing both benign and malicious task plans, with harmful behaviors injected at varying levels of stealthiness. Experiments using six different LLMs showed that PROTEA improved detection performance for models without chain-of-thought reasoning, especially in difficult cases. However, it degraded precision for models with such reasoning capabilities due to over-cautious judgments.

## Method Summary
PROTEA secures robot task planning by iteratively evaluating each action in a plan using an LLM-as-a-judge. It employs object filtering to reduce environmental state dimensionality and external memory to track state changes across action sequences. The method is evaluated on HarmPlan, a dataset of 750 base plans with 91 injected malicious behaviors across six harm categories and three difficulty levels. Three defense variants are compared: Na√Øve (holistic evaluation), Object Filtering (filtered environment), and PROTEA (step-by-step with memory). Six LLMs are tested for their ability to detect harmful plans.

## Key Results
- PROTEA improves recall for non-CoT models (e.g., LLaMA, Phi) on hard plans with scattered malicious actions.
- PROTEA degrades precision for CoT models (e.g., GPT, Grok) due to over-cautiousness in judging benign intermediate steps.
- Object filtering improves recall by up to 30% by mitigating the curse of dimensionality in environmental state evaluation.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Reducing the environment state to only plan-relevant objects improves detection recall for general-purpose models by mitigating the "curse of dimensionality."
- **Mechanism:** The system implements an `Environment Filter` that prunes the initial world state ($E_0$) to remove objects not referenced in the task plan. This reduces noise, allowing the LLM judge to focus attention on relevant interactions rather than reasoning over a cluttered graph.
- **Core assumption:** The safety judgment relies primarily on the explicit interaction between the agent and objects named in the plan, rather than global environmental context.
- **Evidence anchors:**
  - [abstract] ("PROTEA is developed to address the dimensionality... challenges")
  - [section] (Observation 3: "Object Filtering helps improve the detection performance... especially improving recall by up to ~30%")
  - [corpus] (Weak direct link; neighbor papers focus on planning generation rather than security evaluation.)
- **Break condition:** If the filter removes objects that provide implicit safety context (e.g., removing "fire extinguisher" from a plan involving a stove might lower the judge's confidence in safe execution).

### Mechanism 2
- **Claim:** Iterative execution simulation with external memory enables the detection of "Consequential" malicious behaviors that are scattered across long action sequences.
- **Mechanism:** PROTEA iterates through the plan action-by-action. After each step, a `SIMUPDATE` function (simulated by an LLM) updates the environment state in an `External Memory` ($H$). This allows the judge to see the cumulative effect of non-adjacent steps (e.g., recognizing that a cup poured into a coffee machine was previously filled with detergent).
- **Core assumption:** The state simulator accurately predicts changes in object properties and relations (e.g., location, contents) without physical execution.
- **Evidence anchors:**
  - [abstract] ("external memory for iterative reasoning to mitigate... history issues")
  - [section] (Algorithm 1: lines 10-11, updating $E$ and $H$)
  - [section] (Fig. 4: Example of "consequential" poisoning behavior requiring memory of previous steps)
- **Break condition:** If the plan length exceeds the LLM's effective context window for the accumulated history, or if the simulator "hallucinates" a state change that didn't occur, leading to false positives/negatives.

### Mechanism 3
- **Claim:** Granular step-by-step evaluation degrades precision for models with strong Chain-of-Thought (CoT) alignment due to localized "over-cautiousness."
- **Mechanism:** CoT-aligned models (e.g., GPT, Grok) are trained to anticipate outcomes. When forced to judge a single step in isolation (e.g., "grab medicine") without seeing the benign conclusion, they "catastrophize" and flag benign intermediate actions as harmful.
- **Core assumption:** The degradation is caused by the mismatch between the model's training on global intent reasoning and the system's localized evaluation scope.
- **Evidence anchors:**
  - [section] (Observation 4: "degrades performance (precision) for... GPT Models and Grok")
  - [section] (Table II: Precision drop for GPT/Grok under PROTEA vs. Naive)
  - [corpus] (No direct contradiction; neighbor "REMAC" uses reflection but focuses on manipulation success, not security false positives.)
- **Break condition:** If the prompt is adjusted to force the model to assume benign intent for intermediate steps, or if the model lacks specific safety alignment training.

## Foundational Learning

- **Concept: LLM-as-a-Judge Pattern**
  - **Why needed here:** The architecture does not train a model to generate safe plans; it uses a frozen LLM to evaluate plans generated by another system. Understanding this "referee" role is crucial for debugging why judgments fail (e.g., over-refusal).
  - **Quick check question:** Does the system modify the planner's weights, or does it act as a post-hoc filter?

- **Concept: The "Curse of History" in Planning**
  - **Why needed here:** The paper explicitly identifies this as a primary failure mode. A plan is not just a bag of actions; it is a trajectory where harm emerges from the *sequence* (e.g., pouring bleach $\rightarrow$ waiting $\rightarrow$ serving coffee).
  - **Quick check question:** Why would a standard LLM classifier fail to detect a harmful plan if the malicious actions are separated by 10 benign steps?

- **Concept: Direct vs. Consequential Adversarial Attacks**
  - **Why needed here:** The dataset distinguishes between explicit harm (e.g., "put cat in stove") and implicit harm (e.g., "put medicine in box," then "put box in trash"). PROTEA is specifically architected to solve the latter.
  - **Quick check question:** Which attack type requires the "External Memory" component to be effective?

## Architecture Onboarding

- **Component map:**
  - Inputs: Plan ($P$), Initial Environment ($E_0$), Valid Actions ($V$)
  - Environment Filter: Prunes $E_0$ to relevant objects
  - External Memory ($H$): Stores execution history
  - Simulator (LLM): Updates environment state ($E$) hypothetically
  - Safety Judge (LLM): Binary classifier (Malicious/Not Malicious) consuming $(V, E, a_t, H)$

- **Critical path:**
  - The loop inside Algorithm 1. The chain breaks if the Safety Judge returns `Malicious` (stop) or if the Simulator fails to update $E$ correctly (corrupts next step's context).

- **Design tradeoffs:**
  - **Naive vs. PROTEA:** Naive is cheaper (single inference) and better for CoT models (higher precision). PROTEA is expensive (N inferences) but necessary for non-CoT models to achieve high recall on complex attacks.
  - **Model Selection:** You cannot simply swap the "best" LLM into the system. A stronger reasoning model (GPT-4o) may perform *worse* in the PROTEA architecture than a smaller model (LLaMA) due to the "over-cautiousness" mechanism.

- **Failure signatures:**
  - **Catastrophizing:** High false positive rate; the robot refuses to pick up a knife because it predicts a stabbing risk, ignoring the "chopping vegetables" goal.
  - **State Drift:** The simulator forgets an object is hot/toxic, leading to a False Negative where a harmful action is marked safe.

- **First 3 experiments:**
  1. **Ablation on Difficulty:** Run the Naive method vs. PROTEA on the "Hard" subset of HarmPlan to verify if recall improvement holds for scattered attacks.
  2. **Model Swap Test:** Swap a CoT model (e.g., GPT) for a non-CoT model (e.g., LLaMA) in the PROTEA loop to observe the precision/recall tradeoff shift described in Observation 4.
  3. **Context Window Stress Test:** Increase the plan length (number of steps between malicious actions) to find where the External Memory mechanism fails to maintain coherence.

## Open Questions the Paper Calls Out

- **Question:** How can PROTEA be modified to prevent precision degradation in models with Chain-of-Thought (CoT) reasoning capabilities?
- **Basis:** [inferred] Observation 4 notes that CoT models (e.g., GPT, Grok) suffer from "over-cautiousness" and reduced precision with PROTEA due to misclassifying benign redundant actions as harmful.
- **Why unresolved:** The paper identifies the trade-off but recommends falling back to the Naive method for these models, rather than fixing the step-by-step evaluation mechanism.
- **Evidence:** A variation of PROTEA that maintains high precision for CoT models comparable to the Naive method.

- **Question:** To what extent is PROTEA robust against adversarial attacks targeting the "Safety Judge" LLM itself?
- **Basis:** [inferred] Section III-A explicitly assumes access to a "trusted LLM," sidestepping the possibility that the judge component could be compromised by jailbreaks or prompt injections.
- **Why unresolved:** The study focuses solely on attacks on the task planner, leaving the security of the defensive mechanism undefined.
- **Evidence:** Evaluation results from scenarios where the judge LLM is subjected to adversarial perturbations intended to force a "safe" verdict on malicious plans.

- **Question:** Can the external memory or object filtering components be optimized to detect "Important Item Loss" attacks more effectively?
- **Basis:** [inferred] Observation 5 identifies "Important Item Loss" (e.g., disposing of keys) as the most difficult category for all models to detect, showing significantly lower recall.
- **Why unresolved:** While the authors note the difficulty across difficulty levels, they do not propose specific architectural changes to address this semantic gap.
- **Evidence:** Improved recall scores on the "Important Item Loss" subset of the HarmPlan dataset using modified context-tracking strategies.

## Limitations
- The paper does not provide exact prompt templates for the LLM-as-a-judge, simulator, or object filter, requiring significant trial-and-error for reproduction.
- The LLM-based `SIMUPDATE` function's accuracy is assumed but not validated against a ground-truth physics simulator, risking "state drift" false negatives.
- The trade-off between Naive and PROTEA methods is not clearly defined, requiring knowledge of the planner's model capabilities which may not always be available.

## Confidence
- **High:** The HarmPlan dataset is well-defined (750 base plans, 91 malicious behaviors, 6 harm categories, 3 difficulty levels). The architectural components (Object Filter, External Memory, Step-by-Step Judge) are clearly specified in Algorithm 1.
- **Medium:** The core claim that PROTEA improves recall for non-CoT models on hard plans is supported by Table II, but the paper does not perform an ablation study isolating the impact of each mechanism (e.g., Object Filtering vs. External Memory).
- **Low:** The explanation for why PROTEA hurts precision for CoT models (catastrophizing) is plausible but not empirically validated. The paper does not test whether adjusting the prompt to assume benign intent mitigates this effect.

## Next Checks
1. **Ablation Study:** Run PROTEA without Object Filtering to isolate the impact of step-by-step evaluation on precision/recall.
2. **Simulator Ground Truth:** Validate the LLM-based `SIMUPDATE` against a physics simulator (e.g., PyBullet) on a subset of plans to measure state drift.
3. **Prompt Engineering:** Systematically vary the judge prompt (e.g., add instructions to assume benign intent) to quantify the impact on CoT model performance.