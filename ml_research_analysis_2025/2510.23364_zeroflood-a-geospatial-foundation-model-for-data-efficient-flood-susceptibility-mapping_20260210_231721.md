---
ver: rpa2
title: 'ZeroFlood: A Geospatial Foundation Model for Data-Efficient Flood Susceptibility
  Mapping'
arxiv_id: '2510.23364'
source_url: https://arxiv.org/abs/2510.23364
tags:
- flood
- data
- foundation
- geospatial
- mapping
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces ZeroFlood, a geospatial foundation model framework
  for data-efficient flood susceptibility mapping (FSM). Traditional FSM relies on
  dense geophysical inputs, making it challenging for data-scarce regions.
---

# ZeroFlood: A Geospatial Foundation Model for Data-Efficient Flood Susceptibility Mapping

## Quick Facts
- arXiv ID: 2510.23364
- Source URL: https://arxiv.org/abs/2510.23364
- Reference count: 0
- Key outcome: ZeroFlood achieves 67.21 F1, 75.03 hit rate, and 59.16 true alarm rate using TerraMind-Large with Sentinel-1 input and TiM reasoning for flood susceptibility mapping.

## Executive Summary
ZeroFlood introduces a geospatial foundation model framework that enables data-efficient flood susceptibility mapping (FSM) in data-scarce regions. Traditional FSM requires dense geophysical inputs like DEMs and precipitation data, making it challenging for resource-limited areas. ZeroFlood fine-tunes pre-trained geospatial foundation models (GFMs) like TerraMind and Prithvi using Earth observation data and flood simulation masks, with Thinking-in-Modality (TiM) reasoning to enhance cross-modal understanding. The approach demonstrates that GFMs can effectively infer flood susceptibility using minimal EO inputs, enabling scalable and resource-efficient flood risk assessment.

## Method Summary
ZeroFlood fine-tunes pre-trained geospatial foundation models by freezing their ViT encoders and training a U-Net decoder to generate binary flood susceptibility masks. The model uses Earth observation imagery (Sentinel-1 SAR or Sentinel-2 optical) paired with physics-based flood simulation masks from data-rich regions to learn cross-modal representations. Thinking-in-Modality (TiM) reasoning generates synthetic modality tokens to enrich unimodal inputs during inference. The framework uses focal loss to handle class imbalance and early stopping on validation loss. Training data consists of 186 samples selected from the TerraMesh dataset using quality filtering criteria.

## Key Results
- TerraMind-Large with Sentinel-1 input and TiM achieved the best performance: 67.21 F1, 75.03 hit rate, 59.16 true alarm rate
- Sentinel-1 (SAR) outperformed Sentinel-2 (optical) input, with F1 scores of 65.35 vs 60.51 when using TiM
- S2+LULC auxiliary tokens improved F1 by 2.44 percentage points compared to baseline
- Models showed high recall (hit rate) but lower precision (true alarm rate), indicating overestimation of flood extent

## Why This Works (Mechanism)

### Mechanism 1
Cross-modal representation learning enables flood susceptibility inference from minimal EO inputs by leveraging patterns learned from data-rich regions. The model learns paired associations between EO imagery and physics-based flood simulation masks during fine-tuning, transferring these learned representations to data-scarce regions where only EO data is available.

### Mechanism 2
Thinking-in-Modality (TiM) generates synthetic modality tokens that enrich unimodal inputs, improving robustness under missing data conditions. The mechanism leverages pre-trained multimodal tokenizers to generate "imaginary" tokens for modalities not present at inference time, drawing on cross-modal relationships learned during pre-training.

### Mechanism 3
Freezing pre-trained encoders while training lightweight decoders preserves learned representations while adapting to task-specific outputs. The ViT encoder from pre-trained GFMs remains frozen during fine-tuning, while a U-Net decoder is trained to generate flood susceptibility masks, preventing overfitting on the small training set.

## Foundational Learning

- **Cross-modal / Multimodal Learning**
  - Why needed here: ZeroFlood relies on models pre-trained on multiple EO modalities to enable TiM reasoning and cross-modal transfer
  - Quick check question: Can you explain how contrastive learning or masked autoencoding creates aligned representations across modalities?

- **Transfer Learning with Frozen Encoders**
  - Why needed here: The training strategy freezes pre-trained weights and only updates the decoder, requiring understanding of feature extraction vs. task adaptation
  - Quick check question: What are the trade-offs between full fine-tuning vs. linear probing vs. decoder-only training?

- **Semantic Segmentation Evaluation Metrics**
  - Why needed here: FSM is framed as pixel-wise binary segmentation using F1, Hit Rate, and True Alarm Rate
  - Quick check question: Why might high recall but low precision be acceptable (or problematic) for flood susceptibility mapping?

## Architecture Onboarding

- **Component map:** Sentinel-1/2 imagery -> TiM Tokenizer -> Frozen ViT Encoder (TerraMind/Prithvi) -> U-Net Decoder -> Binary flood susceptibility mask

- **Critical path:** 1) Sample selection via Algorithm 1 filtering TerraMesh grids 2) Modal alignment between EO input and flood simulation masks 3) TiM token generation before encoder forward pass 4) Decoder training with early stopping on validation loss

- **Design tradeoffs:** Sentinel-1 outperforms Sentinel-2 in experiments (F1: 65.35 vs. 60.51 with TiM); SAR penetrates clouds but may have lower spatial resolution. TiM with DEM+LULC increased HR (+6.38 pp) but decreased F1 (-3.31 pp).

- **Failure signatures:** High HR, low TAR indicates over-prediction of flood extent. TiM with DEM+LULC shows unexpected performance degradation. Overfitting risk with only 186 training samples.

- **First 3 experiments:** 1) Baseline replication with TerraMind-Large, Sentinel-1, frozen encoder, U-Net decoder, no TiM (target F1 â‰ˆ 64.77) 2) TiM ablation with S2+LULC auxiliary tokens (target +2.44 pp F1 improvement) 3) Input modality swap to Sentinel-2 with TiM (S1+DEM+LULC) to establish modality sensitivity

## Open Questions the Paper Calls Out

- Can integration of explicit hydrological priors and river-network information into GFMs significantly improve spatial precision and reduce false-alarm rates?
- To what extent does ZeroFlood generalize to unseen geographic regions and historical flood events outside the training domain?
- Can tiled or sliced inference strategies effectively extend ZeroFlood's local-scale predictions to global flood susceptibility assessments?

## Limitations

- The entire model training relies on only 186 samples from Europe, raising questions about generalizability to other regions and flood types
- TiM mechanism validation is limited, with mixed results showing potential for introducing noise rather than consistent signal
- Missing hyperparameter details including learning rate, batch size, optimizer choice, and U-Net architecture specifics make faithful reproduction challenging

## Confidence

- **High confidence**: Core finding that pre-trained geospatial foundation models can achieve data-efficient flood susceptibility mapping using minimal EO inputs is well-supported
- **Medium confidence**: Effectiveness of Thinking-in-Modality reasoning is demonstrated but with mixed results, suggesting context-dependent benefits
- **Low confidence**: Claims about applicability to truly data-scarce regions are not directly tested, as all experiments use data-rich European samples

## Next Checks

1. Cross-regional validation: Test ZeroFlood on TerraMesh samples from non-European regions excluded from training to assess true zero-shot transfer capability
2. TiM ablation study: Systematically evaluate each auxiliary modality individually and in combinations against baseline without TiM, measuring precision-recall tradeoffs
3. Extreme event robustness: Validate model performance on flood simulations with return periods beyond 100 years or pluvial flooding scenarios not represented in training data