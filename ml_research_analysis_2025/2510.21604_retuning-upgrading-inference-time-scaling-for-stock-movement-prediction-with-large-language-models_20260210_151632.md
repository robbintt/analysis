---
ver: rpa2
title: 'RETuning: Upgrading Inference-Time Scaling for Stock Movement Prediction with
  Large Language Models'
arxiv_id: '2510.21604'
source_url: https://arxiv.org/abs/2510.21604
tags:
- deepseek
- stock
- financial
- prediction
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The study addresses the underexplored use of large language models\
  \ for stock movement prediction, a task critical to financial decision-making. The\
  \ authors identify that LLMs tend to follow contextual viewpoints rather than independent\
  \ reasoning, and often fail to weigh adversarial evidence\u2014hindering reliable\
  \ predictions."
---

# RETuning: Upgrading Inference-Time Scaling for Stock Movement Prediction with Large Language Models

## Quick Facts
- arXiv ID: 2510.21604
- Source URL: https://arxiv.org/abs/2510.21604
- Reference count: 40
- Three-class stock movement prediction F1 score: 0.4196 (SFT+GRPO) vs 0.3475 (baseline)

## Executive Summary
This study addresses the underexplored use of large language models for stock movement prediction, a task critical to financial decision-making. The authors identify that LLMs tend to follow contextual viewpoints rather than independent reasoning, and often fail to weigh adversarial evidence—hindering reliable predictions. To address this, they propose RETuning, a cold-start method that trains models to dynamically construct analytical frameworks, collect and score evidence, and reflect before making predictions. RETuning is applied to a newly constructed large-scale dataset covering 2024 for 5,123 A-share stocks, with over 200K samples and long contexts up to 32K tokens. Experiments show RETuning significantly improves prediction accuracy over strong baselines, enabling robust reasoning in the financial domain. The method also generalizes beyond stock prediction, benefiting other financial tasks, and maintains strong performance under inference-time scaling and out-of-distribution conditions.

## Method Summary
The RETuning framework uses a two-stage approach: (1) Cold Start Supervised Fine-Tuning (SFT) that teaches models a structured reasoning schema—understanding task constraints, establishing analytical principles, extracting and scoring evidence from six heterogeneous sources (news, fundamentals, analyst opinions, quantitative reports, macro indicators, similar stocks), reflecting on conflicts, and producing structured predictions; and (2) Group Relative Policy Optimization (GRPO) with curriculum learning that refines predictions using rule-based rewards for format adherence, accuracy, and consistency. The method employs inference-time scaling through majority voting on multiple predictions, and is trained on a novel Fin-2024 dataset with 209K samples covering 5,123 Chinese A-share stocks with 32K token contexts.

## Key Results
- SFT+GRPO achieves 0.4196 F1 score, outperforming baseline 0.3475 (+20.75%) and GRPO alone 0.3377
- Inference-time scaling with majority voting shows monotonic improvements up to n≈8-16 samples
- Strong out-of-distribution generalization to unseen stocks and dates
- Generalizes to other financial tasks beyond stock movement prediction

## Why This Works (Mechanism)

### Mechanism 1: Reflective Evidence Tuning via Structured Reasoning Schema
The cold-start SFT stage trains models to follow a specific analytical pattern: (1) understand task constraints (e.g., ±3% thresholds), (2) establish analytical principles independent of context, (3) extract evidence from heterogeneous sources, (4) group and score evidence for price up or down, (5) reflect and reconcile conflicts, (6) produce structured output. This creates an internal "analytical framework" that resists simply echoing analyst opinions. Core assumption: The model can transfer this structured reasoning pattern to novel contexts without reverting to context-following behavior. Break condition: If input context exceeds 32K tokens and evidence sources are highly contradictory, the framework may collapse into summarization mode without resolving conflicts.

### Mechanism 2: Inference-Time Scaling via Majority Voting on Structured Predictions
At inference, generate n predictions with temperature=0.6, then take majority vote on predicted labels (up/hold/down). This works because SFT+GRPO models learn to produce diverse but calibrated reasoning paths, whereas base LLMs produce near-random guesses that don't benefit from aggregation. Core assumption: The model's predictions are better-than-random and its errors are somewhat independent across samples. Break condition: If model confidence is uniformly low or if errors are highly correlated across samples, majority voting provides no benefit and may amplify systematic errors.

### Mechanism 3: Curriculum Learning for RL via Difficulty-Based Sample Filtering
Filtering training samples by difficulty—discarding easy (≥2/3 predictions correct) and hard (≤1/3 correct), training only on medium-difficulty samples in increasing difficulty order—makes GRPO more effective and stable than training on all data. Core assumption: Difficulty computed by the cold-start model correlates with actual learning value for the RL objective. Break condition: If difficulty distribution is heavily skewed or if medium-difficulty samples still contain high label noise, curriculum filtering may remove too much signal or introduce bias.

## Foundational Learning

- **Chain-of-Thought (CoT) Prompting**:
  - Why needed here: The paper identifies that vanilla LLMs follow context (analyst opinions) rather than reason independently. CoT structures reasoning, but the paper finds CoT alone only helps larger models (32B+). Without SFT, smaller models cannot leverage CoT prompts effectively.
  - Quick check question: Given two model outputs—one that summarizes news sentiment and one that constructs an analytical framework, scores evidence for up/down, and reflects before predicting—can you identify which demonstrates independent reasoning vs. context-following?

- **Reinforcement Learning with Reward Shaping (GRPO variant)**:
  - Why needed here: SFT provides initialization by instilling the reasoning schema, but RL refines predictions using multi-faceted rewards: Format Score (structured output adherence), Accuracy Score (correct direction), Consistency Score (predicted % change matches direction label). Without RL, SFT models plateau at lower performance.
  - Quick check question: Why does GRPO alone fail without SFT initialization? (Answer: The initial policy is too far from the reward-optimal region; random exploration in a sparse-reward financial prediction task rarely discovers high-reward behaviors without a structured starting point.)

- **Majority Voting / Self-Consistency for Inference-Time Scaling**:
  - Why needed here: Enables inference-time compute scaling by aggregating multiple predictions. The paper uses simple label majority voting rather than confidence-weighted aggregation, which is robust when models are calibrated but predictions vary across reasoning paths.
  - Quick check question: What happens if all n predictions are wrong but highly correlated (same reasoning error repeated)? (Answer: Majority voting amplifies the systematic error; this is why diverse reasoning paths from SFT+GRPO are critical.)

## Architecture Onboarding

- **Component map**: Fin-2024 dataset (209K samples, 6 info sources) → SFT data synthesis (188 cold-start samples + 10K general reasoning samples) → DeepSeek-R1-Distill-Qwen-14B/32B → LoRA fine-tuning (rank=32, α=64) on structured reasoning schema → GRPO with curriculum learning → reward shaping (Format + Accuracy + Consistency) → majority voting inference (n∈{1,2,4,8,16,32})

- **Critical path**: Cold-start SFT is non-negotiable (Table 1 shows GRPO alone achieves 0.3377, below baseline 0.3475, while SFT+GRPO achieves 0.4196). Curriculum filtering is critical (discards easy and hard samples, trains only on medium-difficulty ones). Inference-time scaling provides marginal but consistent gains (Figures 7-8 show monotonic improvements up to n≈8-16).

- **Design tradeoffs**: Three-class prediction (up/hold/down with ±3% thresholds) vs binary (more realistic but requires stronger signals); long context (32K tokens) vs efficiency (richer information improves signal but increases compute 4-8x); majority voting vs best-of-N with verifiers (simple majority voting is robust when models are calibrated but lacks learned verification).

- **Failure signatures**: Model defaults to "hold" on label-balanced data (baselines predict mostly "hold"); inference-time scaling regresses at high n (some models dip at n=32); OOD_Date harder than OOD_Stock (temporal distribution shift more challenging than stock distribution shift).

- **First 3 experiments**:
  1. Ablate SFT initialization: Train GRPO from base model (no SFT) on curriculum-filtered data. Expected: Performance drops below baseline, confirming SFT is essential for RL.
  2. Vary curriculum difficulty thresholds: Test definitions of "medium" difficulty (20-80% vs 30-70% vs 40-60% incorrect). Expected: Optimal range exists around 30-70%; too narrow loses training data, too wide includes noisy samples.
  3. Analyze inference-time scaling by ground truth label: Track per-class scaling curves for SFT vs SFT+GRPO. Expected: "Hold" benefits most from scaling (model is uncertain), while "up/down" show lower baseline but scaling helps identify strong signals.

## Open Questions the Paper Calls Out

- **Generalization Across Markets**: Does RETuning generalize to other equity markets (e.g., U.S., European, emerging markets) with different regulatory regimes and liquidity structures? Current experiments limited to Chinese A-share stocks.

- **Lightweight Variants**: Can parameter-efficient fine-tuning variants achieve comparable performance with reduced computational costs? Current implementation requires significant compute (4-32 H100 GPUs).

- **Trading Profitability**: How do RETuning-based predictions translate to profitability in realistic trading simulations with transaction costs, slippage, and position constraints? F1 scores don't capture Sharpe ratio or maximum drawdown.

- **Inference-Time Scaling Limits**: Why does inference-time scaling yield diminishing returns after n≈8-16, and can adaptive or verification-guided sampling extend gains further? Majority voting lacks a learned verifier.

## Limitations

- Generalization across market regimes is unknown—the study tests on Dec 2024 and Jun 2025 data but doesn't evaluate across different market conditions or volatility periods.
- Cold-start sample representativeness is unclear—the 188 synthesized samples form the critical foundation but their diversity across stock sectors and market conditions is not detailed.
- Reward function sensitivity is unaddressed—the weights α, β, γ are not specified, and their impact on performance is unexplored.

## Confidence

**High Confidence**: The core finding that SFT+GRPO significantly outperforms baselines (0.4196 vs 0.3475) is well-supported by controlled experiments. The necessity of SFT initialization before GRPO is demonstrated through ablation. The inference-time scaling improvements up to n≈8-16 are consistent across multiple experiments.

**Medium Confidence**: The generalization benefits to other financial tasks are claimed but only briefly mentioned without detailed experimental validation. The effectiveness of the curriculum learning approach is demonstrated but relies on assumptions about difficulty correlation that lack direct validation.

**Low Confidence**: The method's performance in extreme market conditions (e.g., financial crises, flash crashes) is unknown. The sensitivity to hyperparameter choices beyond the reported settings has not been explored.

## Next Checks

1. **Market Regime Analysis**: Replicate OOD_Date experiments but stratify results by market volatility quartiles or bull/bear periods to reveal whether the method maintains performance across different market regimes.

2. **Reward Function Ablation**: Systematically vary the weights α, β, γ in the GRPO reward function (Format-only, Accuracy-only, Consistency-only, and all combinations) to quantify the relative importance of each reward component.

3. **Extreme Market Stress Test**: Apply RETuning to historical periods of market stress (e.g., 2008 financial crisis, 2020 COVID crash) using appropriate out-of-distribution testing to validate whether the learned reasoning framework maintains robustness when market correlations break down.