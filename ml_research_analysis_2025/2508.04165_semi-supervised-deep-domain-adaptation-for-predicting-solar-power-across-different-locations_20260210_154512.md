---
ver: rpa2
title: Semi-Supervised Deep Domain Adaptation for Predicting Solar Power Across Different
  Locations
arxiv_id: '2508.04165'
source_url: https://arxiv.org/abs/2508.04165
tags:
- domain
- data
- adaptation
- solar
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of accurate solar power prediction
  across geographically diverse locations, where domain shift due to varying weather
  conditions degrades model performance. To overcome this, the authors propose a semi-supervised,
  source-free deep domain adaptation framework using a teacher-student model.
---

# Semi-Supervised Deep Domain Adaptation for Predicting Solar Power Across Different Locations

## Quick Facts
- arXiv ID: 2508.04165
- Source URL: https://arxiv.org/abs/2508.04165
- Reference count: 40
- Key outcome: Up to 11.36% accuracy improvement in solar power prediction across locations using source-free semi-supervised domain adaptation with only 10% labeled target data

## Executive Summary
This paper addresses the challenge of accurate solar power prediction across geographically diverse locations where domain shift due to varying weather conditions degrades model performance. The authors propose a semi-supervised, source-free deep domain adaptation framework using a teacher-student model that trains a CNN on source location data and adapts it to target locations with minimal labeled data. The approach leverages consistency and cross-entropy loss while maintaining storage efficiency by eliminating the need for source data during adaptation. Experimental results on California, Florida, and New York datasets show significant improvements—up to 11.36%, 6.65%, and 4.92% in accuracy, respectively—compared to non-adaptive methods, while reducing annotation costs by 90% when only 10% of target data is labeled.

## Method Summary
The framework employs a two-phase approach: (1) source model training using a CNN classifier on weather features (DNI, DHI, GHI, temperature, wind direction, wind speed) from a source location, achieving ~81% accuracy; (2) source-free adaptation to target locations using a teacher-student architecture. Both models are initialized from source weights, with the student updated via a combined consistency loss (between teacher and student predictions on unlabeled target data) and cross-entropy loss (on labeled target samples), while the teacher is updated via Exponential Moving Average of student weights. This semi-supervised approach enables adaptation with as little as 10% labeled target data, achieving near-supervised performance.

## Key Results
- Source-free adaptation improves accuracy by 11.36%, 6.65%, and 4.92% for CA→NY, CA→FL, and FL→NY respectively compared to non-adaptive baselines
- 90% reduction in annotation costs achieved when using only 10% labeled target data versus 100% labeling
- Limited improvement (<1%) observed in fully unsupervised adaptation (0% labeled data)
- Diminishing returns on annotation ratio above 20% labeled data

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The teacher-student architecture enables stable knowledge transfer from source to target domain by generating consistent pseudo-labels for unlabeled target data.
- **Mechanism:** The student model receives gradients from both supervised (cross-entropy) and unsupervised (consistency) losses, while the teacher model—updated via Exponential Moving Average (EMA)—provides stable target predictions. This creates a self-training loop where the teacher's increasingly refined predictions guide the student without source data access.
- **Core assumption:** The source pre-trained model has learned domain-invariant features that can be refined for the target location with minimal labeled examples.
- **Evidence anchors:**
  - [abstract]: "teacher-student model leverages consistency and cross-entropy loss for semi-supervised learning"
  - [section III-C, eq. 1-4]: Formal specification of L_cons, L_CE, L_total, and EMA update rule
- **Break condition:** If source and target domains share no learnable representations, consistency loss will reinforce incorrect pseudo-labels, leading to negative transfer.

### Mechanism 2
- **Claim:** Source-free adaptation maintains prediction accuracy while eliminating storage requirements for source data during deployment.
- **Mechanism:** By freezing source data access after pre-training and relying solely on the pre-trained weights to initialize both teacher and student models, the method avoids storing potentially large source datasets.
- **Core assumption:** The source model's learned features generalize sufficiently that fine-tuning on target data can adjust for distributional differences without revisiting source samples.
- **Evidence anchors:**
  - [abstract]: "source-free constraint enhances storage efficiency by eliminating the need for source data during adaptation"
  - [section I]: "source-free adaptation eliminates the need for source data during the adaptation step"
- **Break condition:** If the target domain requires learning features absent from source domain, source-free adaptation cannot recover this information.

### Mechanism 3
- **Claim:** Semi-supervised learning with 10-20% labeled target data achieves near-supervised performance by leveraging structure in unlabeled target data.
- **Mechanism:** The dual-loss formulation allows the model to learn from sparse labels via cross-entropy while regularizing using the target domain's underlying structure via consistency loss.
- **Core assumption:** Unlabeled target data contains sufficient structure that consistent predictions across teacher-student models indicate correct learning.
- **Evidence anchors:**
  - [abstract]: "reducing annotation costs by 90% when only 10% of target data is labeled"
  - [Table II]: Quantitative results showing CA→FL adaptation achieves 74.03% accuracy at p=10% vs. 81.56% at p=100%
- **Break condition:** If labeled samples are not representative of target distribution, the model will learn incorrect class boundaries that consistency loss reinforces.

## Foundational Learning

- **Concept: Domain Shift / Covariate Shift**
  - **Why needed here:** The core problem motivating this work. Weather feature distributions differ across locations (California vs. Florida vs. New York), causing models trained on one location to fail on another.
  - **Quick check question:** If you trained a solar model on Arizona data, would you expect it to work unchanged in Seattle? Why or why not?

- **Concept: Semi-Supervised Learning (Consistency Regularization)**
  - **Why needed here:** The mechanism enabling 90% annotation reduction. Consistency regularization enforces that perturbations should not change predictions, allowing learning from unlabeled data.
  - **Quick check question:** Why might enforcing prediction consistency on unlabeled data help the model learn, even without labels?

- **Concept: Exponential Moving Average (EMA) for Model Weights**
  - **Why needed here:** The teacher model update mechanism. EMA provides temporal smoothing of weights, preventing the teacher from changing too rapidly and providing unstable pseudo-labels.
  - **Quick check question:** If α = 0.99, how much does the teacher model change per update relative to the student? What happens if α is too low?

## Architecture Onboarding

- **Component map:** Input (weather features: DNI, DHI, GHI, temp, wind direction, wind speed) → Conv layers → BatchNorm → FC layers → Softmax (5-class output) → Cross-entropy loss

- **Critical path:**
  1. Train source model on CA/FL/NY data to convergence (~81% accuracy)
  2. Initialize teacher and student with source weights
  3. For each adaptation epoch: sample mixed batch from labeled and unlabeled target data, compute losses, update student via backprop, update teacher via EMA
  4. Final model: Use teacher model for inference on target domain

- **Design tradeoffs:**
  - **Number of bins (C):** Paper uses C=5. More bins = finer-grained prediction but harder classification task.
  - **EMA decay (α):** Higher α = more stable teacher but slower adaptation. Paper does not specify value.
  - **Loss weight (λ):** Balances consistency vs. supervised loss. Paper uses λ=1 (implied equal weighting).
  - **Annotation ratio (p):** Trade-off between labeling cost and accuracy. Table II shows diminishing returns above p=20%.

- **Failure signatures:**
  - **Consistency loss diverging:** Teacher and student predictions becoming increasingly different—may indicate learning rate too high or EMA decay too low.
  - **Accuracy plateaus below target:** May indicate insufficient labeled data representativeness or domain shift too extreme for current architecture.
  - **No improvement over non-adaptive baseline:** Check that source model actually transfers and that adaptation is running.

- **First 3 experiments:**
  1. **Baseline validation:** Train source model on CA data, evaluate on NY and FL without adaptation. Replicate Table II "w/o adapt" column (~72-75% accuracy) to confirm domain shift exists.
  2. **Ablation on annotation ratio:** With CA→FL as source→target, run adaptation with p = 0%, 10%, 20%, 50%, 100%. Plot accuracy vs. annotation percentage.
  3. **EMA decay sensitivity:** Fix p=10%, vary α ∈ {0.9, 0.95, 0.99, 0.999}. Monitor both accuracy and training stability.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the framework be modified to achieve significant performance gains under fully unsupervised (p=0%) domain adaptation for solar power prediction?
- **Basis in paper:** The authors state "our proposed approach shows limited improvement in fully unsupervised adaptation" with gains typically less than 1%, indicating this as a known limitation.
- **Why unresolved:** The current consistency loss between teacher-student models appears insufficient to bridge the domain gap without any labeled target samples to anchor the adaptation.
- **What evidence would resolve it:** Demonstrating accuracy improvements greater than 5% over non-adaptive baselines when p=0% across multiple source-target location pairs.

### Open Question 2
- **Question:** Does the framework generalize to geographically proximate locations with subtle weather variations, as opposed to the distinct climate regimes tested in this study?
- **Basis in paper:** The experimental validation uses three U.S. states with substantially different atmospheric regimes, but performance may differ for locations with less pronounced domain shifts.
- **Why unresolved:** The tested locations represent large domain shifts; performance on smaller shifts or different climate types remains unverified.
- **What evidence would resolve it:** Testing on additional geographic regions with varying degrees of weather similarity to source domains, including international locations.

### Open Question 3
- **Question:** How does the choice of bin count (C=5) for formulating solar power prediction as classification affect the trade-off between prediction precision and adaptation performance?
- **Basis in paper:** The paper discretizes continuous power output into 5 bins, which introduces quantization error and may not capture fine-grained power variations needed for grid operations.
- **Why unresolved:** The relationship between classification granularity and domain adaptation effectiveness is unexplored.
- **What evidence would resolve it:** A systematic ablation study varying C (e.g., 3, 5, 10, 20 bins) and reporting both accuracy and regression metrics.

### Open Question 4
- **Question:** Can incorporating multi-source domain adaptation, where knowledge is transferred from multiple geographically diverse source locations simultaneously, improve target domain prediction accuracy over single-source adaptation?
- **Basis in paper:** Each experiment transfers knowledge from only one source location; combining knowledge from multiple sources might provide more robust feature representations.
- **Why unresolved:** Single-source models may capture location-specific biases that do not generalize well; multi-source training could average out such biases.
- **What evidence would resolve it:** Experiments adapting to each target using 2+ source locations simultaneously, compared against single-source baselines.

## Limitations
- Limited improvement in fully unsupervised adaptation (<1% gain when p=0%)
- No comparison to alternative domain adaptation methods beyond simple source-trained baseline
- Missing architectural details (CNN layer specifications, EMA decay value, loss weighting) that prevent full reproduction
- Uncertainty about contribution of semi-supervised learning versus domain adaptation components

## Confidence
- **High Confidence:** The source-free adaptation mechanism and its storage efficiency benefit
- **Medium Confidence:** The overall improvement percentages and 90% annotation reduction claims
- **Low Confidence:** The relative contribution of semi-supervised learning versus domain adaptation, and generalizability to other renewable energy domains

## Next Checks
1. **Architecture reproducibility test:** Implement the CNN with standard configurations (3 Conv layers, 32/64/128 filters, 3x3 kernels, ReLU, 2 FC layers) and verify the source-trained baseline accuracy (~81%) before adaptation.
2. **EMA sensitivity analysis:** Run adaptation experiments with α = 0.95, 0.99, 0.999 and plot accuracy vs. annotation ratio to identify optimal stability-adaptation trade-off.
3. **Ablation on semi-supervised component:** Compare against (a) fully supervised adaptation with all target labels, (b) fully unsupervised adaptation (0% labels), and (c) domain-adversarial neural network baseline to isolate the contribution of consistency regularization.