---
ver: rpa2
title: Non-Uniform Memory Sampling in Experience Replay
arxiv_id: '2502.11305'
source_url: https://arxiv.org/abs/2502.11305
tags:
- sampling
- buffer
- learning
- replay
- uniform
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates non-uniform memory sampling in experience
  replay for continual learning. It challenges the assumption that uniform sampling
  from replay buffers is optimal by conducting experiments where the same memory buffer
  is sampled with different non-uniform probability distributions.
---

# Non-Uniform Memory Sampling in Experience Replay

## Quick Facts
- arXiv ID: 2502.11305
- Source URL: https://arxiv.org/abs/2502.11305
- Reference count: 2
- Primary result: Non-uniform replay distributions significantly outperform uniform sampling across multiple buffer sizes, models, and datasets

## Executive Summary
This paper investigates whether uniform sampling from replay buffers is optimal for continual learning. Through experiments where identical buffer contents are sampled with different non-uniform probability distributions, the authors find that certain non-uniform distributions significantly outperform uniform sampling, with accuracy improvements ranging from 2.33% to 4.68% on CIFAR-10. The results challenge the standard assumption in experience replay methods and suggest that more principled adaptive replay policies could yield further gains in mitigating catastrophic forgetting.

## Method Summary
The paper conducts controlled experiments where the same memory buffer is sampled with different non-uniform probability distributions. Using reservoir sampling to maintain identical buffer contents across all trials, the authors generate 50 random weight vectors (plus one uniform baseline) for each trial. These weights determine sampling probabilities for each buffer position, which remain fixed per sample throughout its lifetime in the buffer. The experiments use online class-incremental learning on CIFAR-10 and Imagenette with five tasks (two classes each), testing buffer sizes of 200, 500, and 1000. Models include a 3-layer CNN for CIFAR-10 and MobileNet-v3-small for Imagenette, trained with SGD and standard ER loss formulation.

## Key Results
- Non-uniform sampling distributions achieved 2.33%-4.68% accuracy improvements on CIFAR-10 and 2.37%-3.54% on Imagenette
- The same buffer contents, when sampled with different distributions, produced significantly different performance outcomes
- Statistical significance confirmed through paired t-tests comparing best non-uniform vs uniform sampling
- Correlation between sampling probability and loss was moderate (ρ = 0.28), suggesting random weights don't simply prioritize high-loss examples

## Why This Works (Mechanism)

### Mechanism 1: Differential Sample Utility
Certain stored samples have disproportionately larger impact on preventing catastrophic forgetting than others, and biased replay can exploit this. Non-uniform sampling distributions upweight higher-utility samples more frequently, better preserving decision boundaries for past tasks. This assumes not all stored samples contribute equally to knowledge consolidation. Evidence shows specific non-uniform distributions significantly outperform uniform sampling across multiple settings.

### Mechanism 2: Implicit Utility Alignment via Random Weights
Random weight distributions occasionally align with higher-utility samples, producing gains even without explicit prioritization. Random fixed-weight assignment creates coverage that, by chance, upweights samples that better consolidate past knowledge. This assumes random assignment provides sufficient variation to discover beneficial distributions. Evidence shows high-performing trials had lower mean per-sample loss (0.742 vs 0.815), though correlation with loss was only moderate.

### Mechanism 3: Isolated Sampling Effect
Sampling distribution affects forgetting independently of buffer composition. By fixing buffer contents via identical reservoir sampling across trials, performance differences are attributable solely to replay frequency distribution. This assumes buffer content effects can be decoupled from sampling effects. Evidence shows identical buffers with different sampling distributions yield different accuracy outcomes.

## Foundational Learning

- Concept: **Experience Replay Buffers**
  - Why needed here: The paper modifies *how* samples are drawn from buffers; understanding standard uniform replay is prerequisite.
  - Quick check question: Explain how reservoir sampling maintains an unbiased representation of streaming data in continual learning.

- Concept: **Catastrophic Forgetting**
  - Why needed here: The entire motivation is mitigating forgetting; you must understand why sequential training degrades past-task performance.
  - Quick check question: What causes a model trained on Task B to lose accuracy on previously learned Task A?

- Concept: **Online Class-Incremental Learning**
  - Why needed here: The experimental protocol uses this specific setting (single-pass, task-IDs unavailable at inference).
  - Quick check question: How does class-incremental evaluation differ from task-incremental evaluation?

## Architecture Onboarding

- Component map:
  - **Memory Buffer M**: Stores past samples; updated via reservoir sampling (sizes tested: 200, 500, 1000)
  - **Sampling Weight Vector**: Length |M|; assigns probability pᵢ to each buffer position; fixed per trial
  - **Replay Loss Term**: L_replay combined with L_new at each training step (λ controls relative weight)
  - **Buffer Index Assignment**: Each sample retains its index from insertion until removal; weights stay fixed per position

- Critical path:
  1. Fix random seed → buffer evolution identical across all trials within a run
  2. Generate weight vector w (random uniform) → normalize to probability distribution p = w/Σw
  3. Sample mini-batch B from M using probabilities p
  4. Compute combined loss L(θ) = L_new + λ·L_replay
  5. Update model; repeat for all tasks
  6. Evaluate final accuracy on all classes

- Design tradeoffs:
  - **Buffer size vs. memory budget**: Larger buffers reduce forgetting but increase storage
  - **Random vs. principled weights**: Random works in experiments, but principled/adaptive schemes could yield larger gains (unexplored)
  - **Computational overhead**: Adaptive weighting would require per-sample metrics (loss, gradient) at each step

- Failure signatures:
  - Uniform sampling baseline underperforms by 2–5% absolute accuracy vs. best non-uniform
  - High-performing trials show lower per-sample replay loss; weighting that upweights high-loss samples may underperform
  - Severe class imbalance in replay frequencies could cause bias toward over-represented classes

- First 3 experiments:
  1. **Reproduce uniform baseline**: Run standard ER with reservoir sampling on CIFAR-10 (buffer=500, 5 tasks), verify accuracy ~41% ± 2%
  2. **Test random non-uniform sampling**: Generate 10 random weight vectors; run trials with identical buffer seeds; expect ≥1 to outperform uniform by 2–4%
  3. **Correlation analysis**: For the best-performing weight vector, compute Spearman correlation between weights and per-sample loss/gradient norms; verify moderate correlation (ρ ≈ 0.3)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can an efficient online mechanism be developed to adaptively update sampling probabilities based on model feedback without requiring computationally expensive multi-step updates?
- Basis in paper: The Discussion section states, "A promising next step is to learn these weights adaptively... without resorting to a multi-step update procedure."
- Why unresolved: Current adaptive methods like Maximally Interfered Retrieval rely on complex two-step updates or extra forward passes, which deviate from standard single-step continual learning efficiency.
- What evidence would resolve it: An algorithm that dynamically adjusts sampling weights in a single pass and consistently outperforms the fixed random distributions identified in this study.

### Open Question 2
- Question: Which specific sample metrics (e.g., loss, uncertainty, gradient norms) should drive non-uniform sampling to yield the most significant mitigation of catastrophic forgetting?
- Basis in paper: The Discussion notes that "determining which features or metrics... should drive non-uniform sampling in an efficient, online manner remains an open challenge."
- Why unresolved: The paper's correlation analysis showed only moderate relationships between performance and metrics like loss or gradient norm, suggesting the optimal signal for prioritization is not yet identified.
- What evidence would resolve it: A study demonstrating a strong, consistent correlation between a specific online metric and replay utility, leading to a principled sampling policy.

### Open Question 3
- Question: How does non-uniform sampling interact with different buffer selection strategies (e.g., reservoir sampling vs. gradient-based selection)?
- Basis in paper: The authors suggest examining "the interplay between buffer selection mechanisms (which examples to store) and replay distributions (how often each example is used)."
- Why unresolved: This study isolated the sampling variable by keeping the buffer content fixed via reservoir sampling across all trials, leaving the interaction with storage policies unexplored.
- What evidence would resolve it: Experiments combining non-uniform sampling with advanced buffer storage methods (like iCaRL or gradient-based selection) demonstrating additive performance gains.

## Limitations

- The paper uses random weight distributions rather than principled adaptive methods, leaving the potential gains from adaptive policies unexplored
- The correlation between sampling probability and loss was only moderate (ρ = 0.28), suggesting the optimal prioritization signal remains unidentified
- The experiments fix buffer contents across trials but don't explore interactions between sampling distributions and different buffer storage mechanisms

## Confidence

- **High confidence**: The experimental methodology (fixed buffer content across trials, paired statistical tests) is sound and the accuracy improvements are real and reproducible
- **Medium confidence**: The claim that random non-uniform sampling significantly outperforms uniform sampling is well-supported, though the generality across different random distributions needs further validation
- **Medium confidence**: The mechanism explanations (differential sample utility, random weight alignment) are plausible but not conclusively proven by the current experiments

## Next Checks

1. **Weight distribution analysis**: Characterize the statistical properties of high-performing weight distributions across multiple runs to identify patterns beyond "random chance"
2. **Adaptive sampling comparison**: Implement a simple adaptive weighting scheme (e.g., based on loss or gradient magnitude) to benchmark against the random distributions and test whether principled approaches yield additional gains
3. **Task boundary effects**: Test whether the sampling benefits persist when tasks are presented in different orders or when task boundaries are unknown to the sampling mechanism