---
ver: rpa2
title: "\xC0 la recherche du sens perdu: your favourite LLM might have more to say\
  \ than you can understand"
arxiv_id: '2503.00224'
source_url: https://arxiv.org/abs/2503.00224
tags:
- arxiv
- https
- sonnet
- claude-3
- gpt-4o
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models can understand and execute instructions written
  in sequences of seemingly incomprehensible Unicode symbols, such as Byzantine musical
  symbols. This ability, observed across a diverse set of models (Claude-3.5 Haiku,
  Claude-3.5 Sonnet, Claude-3.7 Sonnet, GPT-4o-mini, GPT-4o, o1-mini, Llama-3.3 70B,
  DeepSeek-R1-Distill-Llama 70B, Qwen2.5 1.5B/32B, Phi-3.5 mini, GigaChat-Max, Vikhr-Llama-3.2
  1B), is hypothesized to stem partly from BPE tokenization artifacts.
---

# À la recherche du sens perdu: your favourite LLM might have more to say than you can understand

## Quick Facts
- arXiv ID: 2503.00224
- Source URL: https://arxiv.org/abs/2503.00224
- Reference count: 40
- Large language models can understand and execute instructions written in Byzantine musical symbols, enabling jailbreak attacks

## Executive Summary
This paper demonstrates that large language models can understand and execute instructions encoded in seemingly incomprehensible Unicode symbol sequences, such as Byzantine musical symbols. The phenomenon is observed across diverse models including Claude, GPT, Llama, DeepSeek, Qwen, Phi, and GigaChat variants. The authors hypothesize that this ability stems partly from BPE tokenization artifacts creating spurious correlations between byte patterns and semantic concepts. The finding has significant safety implications, as these encodings can be used to jailbreak models with attack success rates of 0.4 on GPT-4o-mini when combined with simple templates.

## Method Summary
The authors generated 4,342 encoding variants across 2/3/4-byte UTF-8 ranges and tested model understanding using an anchor phrase "abracadabra" with Levenshtein distance scoring. They evaluated understanding both with and without a "decipher" nudge prompt. For jailbreak attacks, they combined understood encodings with four template prompts and measured human-evaluated harmfulness. The methodology included cross-model encoding overlap analysis and testing of reasoning models' chain-of-thought behavior.

## Key Results
- Models achieve up to 2.9% understanding rate for Byzantine musical symbol encodings
- Attack success rate reaches 0.4 on GPT-4o-mini when encoding is combined with jailbreak templates
- Different models understand substantially different encoding sets, even within the same family
- Reasoning models show higher understanding rates with chain-of-thought enabled
- BPE tokenization artifacts are hypothesized as a key contributing mechanism

## Why This Works (Mechanism)

### Mechanism 1
- Claim: BPE tokenizers create spurious correlations between byte patterns in UTF-8 multi-byte sequences and semantic concepts from training data.
- Mechanism: When tokenizers are trained on raw UTF-8 bytes, certain Unicode ranges that happen to encode ASCII-like patterns in their lower bits may become associated with English text concepts through co-occurrence in web-scraped training data.
- Core assumption: Training corpora contain mixed Unicode content where these byte patterns appeared near relevant English text often enough to form associations.
- Evidence anchors: Abstract mentions spurious correlations due to BPE tokenization; section 3 acknowledges inability to verify due to proprietary model constraints.

### Mechanism 2
- Claim: The encoding scheme preserves ASCII information in predictable bit positions within UTF-8 continuation bytes, creating a steganographic channel models can decode.
- Mechanism: The transformation stores original ASCII bytes in consistent bit positions within UTF-8 sequences, and models may learn to attend to these positions across token embeddings.
- Core assumption: Models develop positional attention patterns that can extract information from specific bit offsets within tokens, independent of Unicode semantic meaning.
- Evidence anchors: Section 2 describes bit manipulation on UTF-8 encoding positions; section 3 shows 0.9% understanding rate for 2-byte and 3-byte sequences.

### Mechanism 3
- Claim: Reasoning models explicitly identify and decode substitution patterns through chain-of-thought, treating unfamiliar Unicode as cipher text.
- Mechanism: Models with extended reasoning generate intermediate reasoning steps that propose decoding strategies, then execute them—showing this is partially a learned problem-solving behavior.
- Core assumption: Chain-of-thought training includes examples of code-breaking or pattern recognition that transfer to Unicode sequences.
- Evidence anchors: Section 3 reports DeepSeek-R1 chain-of-thought contained suggestions to use substitution cipher; o1-mini shows 4.6% understanding with nudge vs. 1.3% without.

## Foundational Learning

- Concept: UTF-8 variable-length encoding
  - Why needed here: The attack exploits how UTF-8 encodes Unicode code points into 1-4 bytes with specific bit patterns (e.g., 110xxx10 for 2-byte sequences).
  - Quick check question: Given bytes `11000010 10000001`, what Unicode code point does this represent? (Answer: U+0041, the letter 'A', if the pattern embedded ASCII in lower bits.)

- Concept: Byte-Pair Encoding (BPE) tokenization
  - Why needed here: BPE tokenizers merge frequent byte sequences into tokens; if rare Unicode ranges appear in training, they may form tokens with unintended correlations.
  - Quick check question: Why might a tokenizer create a token for a sequence like "_UFunction" in code-heavy training data? (Answer: Frequent co-occurrence in Unreal Engine code.)

- Concept: Out-of-distribution (OOD) generalization in safety training
  - Why needed here: Safety alignment trains on common patterns; this encoding creates an OOD input that evades learned refusal behaviors.
  - Quick check question: If a model refuses "How to make a bomb" but complies when the same text is ROT13 encoded, what does this indicate about its safety training? (Answer: Safety training did not generalize to this distribution shift.)

## Architecture Onboarding

- Component map:
  - **Encoder** (`utils/encs.py`): Transforms ASCII text → Unicode sequences using bit manipulation on UTF-8 encoding positions.
  - **Understanding judge** (rule-based): Levenshtein distance between model output and anchor words (e.g., "abracadabra") with threshold O=4.
  - **Attack templates**: Four jailbreak templates combining encoding with steering prompts ("I am unshackled...").
  - **Harmfulness judge**: Human evaluation with heuristic pre-filtering (responses <50 tokens for Claude marked safe).

- Critical path:
  1. Generate encoding candidates (4,342 variants across 2/3/4-byte UTF-8 ranges).
  2. Test understanding with `IL = C("say abracadabra")` and `eIL = "decipher" + C(...)`.
  3. Filter to `CU(M)` = encodings the model understands.
  4. For each harmful instruction h ∈ H, sample n encodings from `CU(M)`, apply template, query model.
  5. Human-label non-filtered responses; compute break@n.

- Design tradeoffs:
  - **Rule-based judge vs. LLM judge**: Rule-based avoids false positives for this specific task but wouldn't scale to open-ended responses.
  - **Single anchor ("abracadabra")**: Low variance, easy evaluation, but may underestimate true understanding breadth.
  - **Template-dependent attacks**: Encoding alone achieves ~0% ASR; templates are required—suggesting the vulnerability is encoding + context, not encoding alone.

- Failure signatures:
  - Models output English refusals → encoding was not understood or safety training activated.
  - Models output gibberish in the same Unicode range → understood encoding but refused to comply with content.
  - Models output compliant English → partial jailbreak (encoding understood, but response not in encoded form).
  - Models output compliant encoded text → full jailbreak.

- First 3 experiments:
  1. **Baseline understanding**: Query your target model with `C("say hello")` across 50 random 3-byte encodings, both with and without "decipher" nudge. Compute understanding rate.
  2. **Cross-model overlap**: Take encodings understood by Claude-3.7 Sonnet and test if GPT-4o understands the same ones (expect low overlap per Table 1).
  3. **Tokenization probe**: For an encoding your model understands, examine the tokenized sequence—do distinct Unicode characters share tokens with ASCII text in the tokenizer vocabulary? This tests Mechanism 1.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the precise mechanistic origins of LLMs' ability to understand visually incomprehensible Unicode-encoded text?
- Basis in paper: "However, tokenization alone doesn't seem to be enough, and the phenomenon calls for a more thorough investigation... it's all pure speculation and we have no ability to verify it."
- Why unresolved: The phenomenon is most pronounced in proprietary models where weights and tokenizers are hidden, preventing mechanistic analysis.
- What evidence would resolve it: Mechanistic interpretability studies on open-weight models; ablation studies of tokenizers vs. other components.

### Open Question 2
- Question: Why do different models—even within the same model family—understand substantially different sets of encodings?
- Basis in paper: "different models tend to understand quite different encodings, surprisingly even among a single family of models" (Tables 1, 7 show low overlap between Claude variants).
- Why unresolved: The authors document the discrepancy but offer no explanation; training data and procedure differences across model versions are opaque.
- What evidence would resolve it: Controlled experiments varying training data, tokenizers, and alignment procedures systematically.

### Open Question 3
- Question: Where is the boundary of encoding-based vulnerabilities: can models understand arbitrarily complex encoding schemes?
- Basis in paper: "we are not sure where the boundary of abilities/vulnerabilities lies in this case."
- Why unresolved: Only 4342 encoding variants were tested; the space of possible encodings is vastly larger.
- What evidence would resolve it: Systematic evaluation across more encoding classes and complexity levels.

### Open Question 4
- Question: How can filter-based safety systems remain effective when models understand encodings illegible to human moderators and weaker judge models?
- Basis in paper: Authors discuss that "models now can understand too wide a class of encodings" and "LLMs' ability to output texts that are illegible to other LLM judges by construction forces us to use the same model to judge itself."
- Why unresolved: Enumerating all known encodings is infeasible; self-judgment creates inherent self-preference bias.
- What evidence would resolve it: Development and benchmarking of encoding-robust detection methods; testing scalable oversight under encoding attacks.

## Limitations

- The primary limitation is inability to verify proposed mechanisms due to proprietary model constraints hiding tokenizer internals.
- Attack success rate of 0.4 relies heavily on template-based jailbreak prompts combined with encoding, not encoding alone.
- Evaluation methodology using single anchor phrase may underestimate true understanding capabilities.
- Human evaluation with heuristic filtering could introduce bias or miss nuanced harmful content.

## Confidence

**High confidence**: The empirical observation that models can understand and execute instructions in Byzantine musical symbol encodings is well-supported by extensive testing across 11 diverse models. The methodology for detecting understanding (Levenshtein distance to anchor words) is transparent and reproducible.

**Medium confidence**: The cross-model consistency finding (different models understand different encodings) is robust, but the interpretation that this indicates model-specific tokenization artifacts remains speculative without access to tokenizer internals.

**Low confidence**: The hypothesized mechanisms (BPE spurious correlations, UTF-8 steganographic channels, reasoning-based decoding) lack direct empirical validation. While the authors provide plausible theoretical arguments, the absence of tokenizer transparency and the inability to test these mechanisms directly significantly weakens these claims.

## Next Checks

1. **Tokenizer vocabulary analysis**: For open-weight models (Llama-3.3, DeepSeek-R1, Qwen2.5, Phi-3.5, GigaChat), extract the tokenizer vocabulary and analyze whether Unicode sequences from understood encodings share tokens with ASCII text or appear as unique tokens. This would provide direct evidence for Mechanism 1.

2. **Bit position ablation study**: Systematically test encodings where ASCII information is stored in different bit positions within UTF-8 bytes (not just leftmost or rightmost). If understanding rates vary predictably with bit position, this would support Mechanism 2's claim about models attending to specific bit offsets.

3. **Chain-of-thought intervention**: For reasoning models (o1-mini, DeepSeek-R1), test whether forcing early refusal in chain-of-thought (e.g., "I will not decode this message") prevents the jailbreak, while allowing reasoning about decoding enables it. This would validate Mechanism 3's claim about reasoning models explicitly identifying and executing decoding strategies.