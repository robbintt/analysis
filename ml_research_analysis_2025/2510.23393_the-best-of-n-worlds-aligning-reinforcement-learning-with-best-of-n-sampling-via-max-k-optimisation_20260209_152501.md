---
ver: rpa2
title: 'The Best of N Worlds: Aligning Reinforcement Learning with Best-of-N Sampling
  via max@k Optimisation'
arxiv_id: '2510.23393'
source_url: https://arxiv.org/abs/2510.23393
tags:
- pass
- arxiv
- scale
- reward
- rewards
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the degradation of Best-of-N (BoN) sampling\
  \ performance at large N values after RLVR fine-tuning, which stems from decreased\
  \ diversity and increased generation confidence. The authors propose optimizing\
  \ the max@k metric\u2014a continuous generalization of pass@k\u2014using policy\
  \ gradient methods."
---

# The Best of N Worlds: Aligning Reinforcement Learning with Best-of-N Sampling via max@k Optimisation

## Quick Facts
- arXiv ID: 2510.23393
- Source URL: https://arxiv.org/abs/2510.23393
- Reference count: 39
- The authors propose optimizing the max@k metric—a continuous generalization of pass@k—using policy gradient methods to address RLVR fine-tuning degradation of Best-of-N sampling performance at large N values.

## Executive Summary
This paper addresses the degradation of Best-of-N (BoN) sampling performance at large N values after RLVR fine-tuning, which stems from decreased diversity and increased generation confidence. The authors propose optimizing the max@k metric—a continuous generalization of pass@k—using policy gradient methods. They derive unbiased gradient estimates for both on-policy and off-policy scenarios and show that continuous rewards are crucial for successful RLVR. Experiments on coding benchmarks demonstrate that their off-policy approach significantly improves max@k for large k (up to +3.7% on MBPP) while maintaining or slightly improving pass@1, effectively aligning the model with the BoN inference strategy.

## Method Summary
The method optimizes the max@k metric using policy gradient methods with both on-policy and off-policy gradient estimators. The approach uses GRPO (Group Relative Policy Optimization) with a modified reward computation that incorporates combinatorial weights for Best-of-N sampling. The key innovation is deriving unbiased gradient estimates for the max@k objective, which generalizes the pass@k metric to continuous rewards. The implementation includes importance sampling corrections for off-policy optimization and demonstrates that continuous rewards (fraction of tests passed) are essential for effective training.

## Key Results
- Off-policy BoN achieves best max@128 performance across 4/5 datasets, improving from 0.227 to 0.516 on MBPP (+3.7%)
- Binary reward optimization degrades performance significantly (pass@1 drops from 0.211 to 0.092)
- The approach maintains or slightly improves pass@1 while substantially improving max@k for large k values
- On average, the proposed method outperforms existing RLVR approaches by 2.6% on max@k metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Standard RLVR reduces generation entropy, which improves pass@1 but degrades Best-of-N performance at large k values.
- Mechanism: Policy optimization concentrates probability mass on high-reward paths. As the model becomes more confident (entropy drops toward zero), it explores fewer diverse solution strategies. Correct but low-probability solutions from the base model become nearly unreachable, reducing pass@k for large k.
- Core assumption: The entropy-diversity relationship directly causes the pass@k degradation (correlational evidence shown; causal mechanism implied but not isolated).
- Evidence anchors:
  - [abstract]: "the reinforcement learning fine-tuning process may harm the model's exploration ability, as reflected in decreased diversity of generations and a resulting degradation of performance during Best-of-N sampling for large N values"
  - [Section 2.1, Figure 2]: Entropy distributions shift toward zero after GRPO fine-tuning
  - [corpus]: Related work (Cui et al. 2025, Yue et al. 2025) confirms similar trade-offs, suggesting this is a general RLVR phenomenon
- Break condition: If the task has only one valid solution path, entropy collapse would not harm pass@k—diversity penalty only matters when multiple valid approaches exist.

### Mechanism 2
- Claim: The max@k gradient estimate works by reweighting each sample's contribution proportionally to how often it would be selected as the best across all k-subsets.
- Mechanism: For each sample y_i, the objective computes a transformed reward r̃_i = Σⱼ wᵢⱼ · r(yⱼ,x) where weights wᵢⱼ count k-subsets where yⱼ is the maximum and yᵢ is included. This creates a credit assignment that rewards samples for being in the "winning coalition" rather than just being individually correct.
- Core assumption: Monte Carlo sampling over all (n choose k) subsets provides sufficient gradient signal even when k is large relative to n.
- Evidence anchors:
  - [Section 3.2, Eq. 7-9]: Derivation of unbiased on-policy gradient with combinatorial weight calculation
  - [Section 4.5, Table 2]: Off-policy BoN achieves best max@128 across 4/5 datasets
  - [corpus]: Soft Best-of-N Sampling paper (arXiv 2505.03156) provides complementary theoretical grounding for BoN optimization
- Break condition: If rewards are binary and sparse, the gradient variance becomes prohibitive—mechanism requires dense signal to function well.

### Mechanism 3
- Claim: Continuous rewards (fraction of tests passed) provide denser gradient signal than binary rewards, enabling effective max@k optimization.
- Mechanism: Binary rewards create sparse, high-variance gradients where most samples receive zero credit. Continuous rewards assign partial credit based on progress toward correctness, smoothing the loss landscape and allowing the policy to learn from near-misses.
- Core assumption: Test coverage correlates with solution correctness sufficiently that partial progress is meaningful training signal.
- Evidence anchors:
  - [Section 2.1, Table 1]: Binary reward training degrades both pass@1 (0.092 vs 0.257) and pass@128 (0.227 vs 0.516) compared to continuous
  - [abstract]: "we show that optimization of continuous reward is crucial for successful RLVR application"
  - [corpus]: No direct corpus evidence on continuous vs binary rewards; this appears to be a novel contribution of this work
- Break condition: If the test suite is poorly designed (tests don't reflect correctness), continuous rewards provide misleading signal.

## Foundational Learning

- **Concept: Policy Gradient with REINFORCE**
  - Why needed here: The max@k objective requires computing ∇θ E[f(r(y₁,...,yₖ))] where f is non-differentiable. The log-derivative trick (Section 3.2) converts this to an expectation over ∇θ log π(y|x), enabling Monte Carlo estimation.
  - Quick check question: Can you explain why ∇θ π(y|x) = π(y|x) ∇θ log π(y|x) is useful for gradient estimation?

- **Concept: Importance Sampling for Off-Policy Learning**
  - Why needed here: Modern RLVR (GRPO, PPO) reuses samples across multiple gradient steps. The off-policy derivation (Section 3.3) uses probability ratios ρ = πθ/π_old to correct for distribution shift, with a first-order approximation when ρ ≈ 1.
  - Quick check question: Why does the paper assume δᵢ ≈ 0 (probability ratios near 1) to simplify the off-policy gradient?

- **Concept: Pass@k vs Max@k Metrics**
  - Why needed here: Pass@k measures probability of at least one correct solution (binary). Max@k extends this to continuous rewards by taking expected maximum reward. Understanding this distinction clarifies why the paper's objective generalizes prior work.
  - Quick check question: For binary rewards, does pass@k equal max@k? Why or why not?

## Architecture Onboarding

- **Component map**: 
  GRPO Trainer (modified) -> Sampler -> Reward Computer -> BoN Scaler -> Advantage Calculator -> PPO Update

- **Critical path**:
  1. Sample n completions (n ≥ k required; paper uses n=8, evaluates k up to 256)
  2. Compute continuous rewards for all samples
  3. Calculate BoN weights (Listing 2/3 in Appendix C.2/C.3)
  4. Apply weight matrix to rewards to get transformed rewards per sample
  5. Standard GRPO advantage normalization and policy update

- **Design tradeoffs**:
  - Larger n improves weight estimation but reduces batch size (memory constraint)
  - Off-policy correction adds computation O(n²) per prompt but enables sample reuse
  - High temperature (1.0) preserves diversity but may generate more low-quality samples

- **Failure signatures**:
  - Binary rewards → pass@1 and pass@k both collapse (Table 1 pattern)
  - Over-clipping δ values → off-policy correction ineffective, reverts to on-policy behavior
  - Insufficient samples (n < k) → cannot compute max@k weights, falls back to approximation

- **First 3 experiments**:
  1. Reproduce Table 1 comparison: Train identical GRPO setup with binary vs continuous rewards on a small code dataset. Expect binary to underperform significantly.
  2. Ablate the off-policy correction: Compare on-policy BoN (Listing 2) vs off-policy BoN (Listing 3) with different PPO iteration counts. Expect off-policy to improve with more reuse.
  3. Sweep k values during training: Train separate models optimizing max@{1, 4, 16, 64} and evaluate max@k curve. Expect k=1 to optimize pass@1 at expense of large-k performance; expect larger training k to shift the crossover point.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can the max@k optimization framework be effectively extended to mathematical reasoning tasks?
  - Basis in paper: [explicit] The authors list "extending our approach to mathematical reasoning" as a promising direction, noting the challenge that it "requires carefully constructed continuous rewards."
  - Why unresolved: The current study relies on coding tasks where continuous rewards (fraction of passed tests) are naturally available; math domains lack such granular, automated verifiers.
  - What evidence would resolve it: Successful application of the method to math benchmarks using heuristics for partial credit (e.g., equation similarity or step correctness), resulting in improved max@k without performance collapse.

- **Open Question 2**: How does a dynamic training schedule transitioning between max@k and max@1 affect the exploration-exploitation trade-off?
  - Basis in paper: [explicit] The authors suggest "exploring dynamic training schedules that transition from objectives such as max@k to stricter metrics like max@1 or vice versa."
  - Why unresolved: The paper focuses on static optimization of max@k; it does not investigate curriculum strategies that might balance diversity (high k) and accuracy (k=1) over time.
  - What evidence would resolve it: Comparative training runs showing that a dynamic schedule yields superior pass@1 scores compared to static baselines while maintaining competitive max@k performance.

- **Open Question 3**: Can the distribution of a single generation be aligned with the "best of k" distribution to close the performance gap between max@k and max@1?
  - Basis in paper: [explicit] Future work includes "closing the gap between max@k and max@1 performance by aligning the distribution of a single generation with that of the best among k."
  - Why unresolved: Optimizing for max@k currently maintains or slightly improves max@1 but does not explicitly solve the divergence between the single-sample policy and the BoN inference strategy.
  - What evidence would resolve it: A method that minimizes the divergence between the learned policy and the theoretical BoN distribution, resulting in significant simultaneous gains in both metrics.

## Limitations

- The entropy-diversity relationship is shown to be correlative rather than causal—the paper demonstrates that entropy drops after RLVR and diversity drops, but doesn't isolate whether entropy reduction directly causes the performance degradation.
- The off-policy derivation relies on a first-order approximation (δᵢ ≈ 0) that assumes probability ratios remain near 1, but the paper doesn't provide empirical validation of this assumption across training or analyze the approximation error magnitude.
- The binary reward ablation shows dramatic performance degradation, but this comparison conflates reward signal quality with optimization difficulty—binary rewards create sparser gradients that any gradient-based optimizer would struggle with, not specifically the max@k objective.

## Confidence

**High Confidence**: The max@k gradient derivation and unbiased estimation are mathematically sound (Section 3.2). The empirical observation that continuous rewards outperform binary rewards for RLVR is well-supported (Table 1). The off-policy correction implementation follows established importance sampling principles.

**Medium Confidence**: The claim that standard RLVR harms Best-of-N performance at large k values is supported by strong empirical evidence (Table 2, Section 4.5) but relies on comparison to base model performance without controlled ablations of alternative training objectives. The mechanism linking entropy reduction to diversity loss and degraded pass@k is plausible but not rigorously established as causal.

**Low Confidence**: The generalizability of the entropy-diversity trade-off across different model architectures and tasks—the paper focuses on coding tasks where multiple valid solutions may exist, but the mechanism may not apply to tasks with single correct approaches.

## Next Checks

1. **Causal isolation experiment**: Train two models with identical continuous reward objectives but different temperature schedules during sampling (one maintaining base model entropy, one allowing entropy reduction). Compare pass@k curves to isolate whether entropy reduction itself causes the degradation or if it's a correlated phenomenon.

2. **Off-policy approximation error analysis**: During training, log the maximum |δᵢ| values across batches and compute the empirical error in the off-policy gradient estimate (comparing to exact importance-weighted gradients on a subset of data). This would quantify how well the approximation assumption holds in practice.

3. **Binary reward max@k optimization test**: Implement a version of max@k optimization that uses binary rewards but applies different variance reduction techniques (control variates, baseline adjustments). Compare against the continuous reward baseline to determine if the performance gap is specifically about reward informativeness versus optimization difficulty.