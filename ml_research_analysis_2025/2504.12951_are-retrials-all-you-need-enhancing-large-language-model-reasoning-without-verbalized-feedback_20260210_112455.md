---
ver: rpa2
title: Are Retrials All You Need? Enhancing Large Language Model Reasoning Without
  Verbalized Feedback
arxiv_id: '2504.12951'
source_url: https://arxiv.org/abs/2504.12951
tags:
- reasoning
- methods
- language
- arxiv
- feedback
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces "retrials without feedback," a simple yet
  effective mechanism for enhancing reasoning frameworks in large language models
  (LLMs). Unlike conventional iterative refinement methods that rely on verbalized
  self-reflection, this approach allows LLMs to retry problem-solving attempts upon
  identifying incorrect answers without explicit feedback.
---

# Are Retrials All You Need? Enhancing Large Language Model Reasoning Without Verbalized Feedback

## Quick Facts
- **arXiv ID:** 2504.12951
- **Source URL:** https://arxiv.org/abs/2504.12951
- **Reference count:** 5
- **Primary result:** Retrial-based methods without verbalized feedback achieve competitive performance to complex reasoning frameworks at significantly lower computational cost.

## Executive Summary
This paper introduces "retrials without feedback," a simple mechanism for enhancing LLM reasoning by allowing multiple independent attempts at problems when initial answers are incorrect. Unlike iterative refinement methods requiring self-reflection or verbalized feedback, this approach relies solely on deterministic verification to trigger retries. Evaluated across Game of 24, HumanEval, and HotpotQA benchmarks using GPT-4o-mini and LLaMA-3.3-70B, the method demonstrates that simpler approaches like Chain-of-Thought often outperform sophisticated frameworks such as Tree-of-Thoughts or Reflexion in terms of cost efficiency and performance. The study challenges assumptions about the necessity of complex reasoning strategies, highlighting the potential of simpler, cost-effective approaches.

## Method Summary
The retrial mechanism works by generating multiple independent solution attempts for each problem, using deterministic verifiers to detect incorrect answers and trigger additional retries. Unlike conventional iterative refinement, it doesn't require explicit self-reflection or feedback about why previous attempts failed. The method evaluates four prompting strategies (Input-Output, Chain-of-Thought, Tree-of-Thoughts, Reflexion) across multiple temperature settings, with each retry using fresh context. Budget constraints determine the maximum number of attempts, and the approach is limited to tasks with available deterministic verifiers.

## Key Results
- Chain-of-Thought achieved 94% success rate on Game of 24 at fraction of cost required by complex methods
- Higher temperature values improved success rates by increasing solution diversity across retrials
- Simpler prompting strategies (IO, CoT) were significantly more cost-efficient than complex reasoning approaches (ToT, Reflexion) under fixed budget constraints

## Why This Works (Mechanism)

### Mechanism 1: Independence of Retrial Attempts Without Verbalized Feedback
The model generates fresh solution attempts from scratch when incorrect answers are detected, relying on stochastic sampling to produce different reasoning trajectories. The base model's probability distribution contains correct solutions that may surface through repeated sampling, making explicit feedback unnecessary.

### Mechanism 2: Temperature-Mediated Diversity Scaling
Higher sampling temperatures expand the probability distribution, reducing correlation between consecutive failed attempts. This increases effective search coverage of the solution space per unit budget, improving probability of success within fixed constraints.

### Mechanism 3: Per-Attempt Cost Dominance in Budget-Constrained Settings
Under fixed budget constraints, simpler methods achieve better cost-quality tradeoffs because they enable more retrials than computationally expensive frameworks. Complex methods require multiple LLM calls per attempt for evaluation or feedback generation, reducing total attempts possible.

## Foundational Learning

- **Self-Consistency and Sample-Based Reasoning**
  - Why needed: Retrials generate multiple candidates; understanding when to aggregate vs. accept-first-correct is foundational
  - Quick check: How does "retry until correct" differ from "sample N, then vote"—and what verification requirement distinguishes them?

- **Prompting Strategy Cost Structure**
  - Why needed: Paper compares IO/CoT/ToT/Reflexion; each has different per-attempt LLM call counts
  - Quick check: How many forward passes does Tree-of-Thoughts require per problem compared to Chain-of-Thought?

- **Deterministic Verification Oracle**
  - Why needed: The retrial mechanism fundamentally requires knowing when to stop; this constrains applicable task types
  - Quick check: Why does the paper explicitly exclude HotpotQA-style tasks from the retrial mechanism during solving?

## Architecture Onboarding

- **Component map:** [Problem Input] → [Prompting Strategy (IO/CoT/ToT/Reflexion)] → [LLM Inference with Temperature T] → [Deterministic Verifier] → Correct? → Return / Incorrect → [Budget Controller] → Budget remaining? → Retry / No → [Return best/last attempt]

- **Critical path:** Configure prompting strategy and temperature → Generate solution attempt (fresh context each retry) → Verify against deterministic oracle → If correct: terminate; if incorrect and budget remains: increment retry counter and repeat → If budget exhausted: return failure or last attempt

- **Design tradeoffs:** Temperature vs. coherence (higher T → more exploration but risk of incoherent outputs); Strategy complexity vs. retry budget (sophisticated methods reduce retries but increase per-attempt cost); Assumption: Verifier availability determines applicability entirely

- **Failure signatures:** Correlated failures across retries (temperature too low, same error repeated); Budget exhaustion without success (task difficulty exceeds base model capability); Verifier unavailable (cannot determine when to stop)

- **First 3 experiments:** 1) Replicate Game of 24 with CoT at temperatures [0.3, 0.7, 1.0], fixed budget $0.50; plot success rate vs. cost; 2) Compare CoT vs. ToT on HumanEval subset (20 problems), varying budget; identify crossover point where ToT becomes competitive; 3) Measure attempt correlation: Run 10 retries on same problems with temperature 0.5; compute pairwise similarity of failed solutions to verify independence assumption

## Open Questions the Paper Calls Out

### Open Question 1
How can "retrials without feedback" be effectively adapted for tasks that lack a trivial deterministic verifier to validate intermediate answers? The current mechanism depends entirely on external, deterministic checks to trigger retries, which doesn't exist for tasks where ground truth is hidden.

### Open Question 2
Do complex reasoning frameworks (e.g., Reflexion, ToT) eventually outperform simple retrial-based methods if granted significantly higher computational budgets? The current study implemented fixed budget constraints, potentially capping the performance of expensive methods before they could converge.

### Open Question 3
Can the occurrence of retrials be leveraged to optimize the reasoning process itself, thereby reducing the total number of attempts required? The current implementation relies on a "blind" retry mechanism without explicit optimization based on retry count.

## Limitations
- Requires deterministic verifiers, limiting applicability to closed-domain tasks with clear ground truth
- Cost comparisons assume fixed dollar budgets without accounting for latency or practical value considerations
- Temperature effects lack theoretical grounding about optimal settings across problem domains
- Conflation of cost-efficiency with absolute performance in claims about simpler methods

## Confidence

- **High confidence:** Game of 24 results showing CoT superiority at low budgets; basic cost-per-retry calculations
- **Medium confidence:** Temperature scaling effects across different model sizes; generalizability to HotpotQA (oracle-accessible tasks)
- **Low confidence:** Cross-domain applicability to open-ended reasoning tasks; optimal temperature settings as universal heuristics

## Next Checks

1. **Verify independence assumption:** Run 50+ retries on identical Game of 24 problems with temperature 0.7; compute pairwise edit distances and semantic similarity between failed attempts to confirm independent sampling.

2. **Budget crossover validation:** Systematically vary budget from $0.10 to $5.00 on HumanEval subset; plot absolute accuracy curves for CoT, ToT, and Reflexion to identify precise crossover points.

3. **Temperature coherence threshold:** For each prompting strategy, identify temperature at which output coherence degrades while maintaining beneficial diversity, establishing safe operating ranges.