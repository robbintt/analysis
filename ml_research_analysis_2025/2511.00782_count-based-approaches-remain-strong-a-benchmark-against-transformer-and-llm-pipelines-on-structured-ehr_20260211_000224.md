---
ver: rpa2
title: 'Count-Based Approaches Remain Strong: A Benchmark Against Transformer and
  LLM Pipelines on Structured EHR'
arxiv_id: '2511.00782'
source_url: https://arxiv.org/abs/2511.00782
tags:
- medical
- count-based
- patient
- prediction
- table
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study benchmarked three approaches for structured EHR prediction:
  count-based models with LightGBM and TabPFN, a pretrained transformer (CLMBR), and
  a mixture-of-agents (MoA) pipeline that converts tabular histories to clinical summaries
  before classification. Across eight clinical prediction tasks from the EHRSHOT dataset,
  count-based and MoA methods split most wins, with count-based models holding a slight
  overall edge and consistently outperforming CLMBR.'
---

# Count-Based Approaches Remain Strong: A Benchmark Against Transformer and LLM Pipelines on Structured EHR

## Quick Facts
- arXiv ID: 2511.00782
- Source URL: https://arxiv.org/abs/2511.00782
- Reference count: 40
- Primary result: Count-based models with LightGBM/TabPFN achieved competitive or superior performance to transformer and LLM pipelines on structured EHR prediction tasks, with recent events being the strongest predictors.

## Executive Summary
This study benchmarked three approaches for structured EHR prediction: count-based models with LightGBM and TabPFN, a pretrained transformer (CLMBR), and a mixture-of-agents (MoA) pipeline that converts tabular histories to clinical summaries before classification. Across eight clinical prediction tasks from the EHRSHOT dataset, count-based and MoA methods split most wins, with count-based models holding a slight overall edge and consistently outperforming CLMBR. The MoA pipeline generated task-adapted summaries, amplifying relevant ICD-10 chapters depending on the prediction target. Count-based models identified that recent medical events (within one year) were the strongest predictors. While MoA achieved strong performance on some tasks, count-based methods remain a strong, interpretable baseline for structured EHR prediction.

## Method Summary
The study compared three approaches on eight clinical prediction tasks from the EHRSHOT dataset: (1) Count-based: ontology roll-up to ICD-10/ATC sections with two time-binned count vectors (recent/history) classified by LightGBM or TabPFN; (2) CLMBR: pretrained transformer embeddings from raw OMOP concepts classified by LightGBM/TabPFN; (3) MoA: serialize histories as "EVENT at AGE" text, summarize with LLM (Qwen2.5-14B-Instruct/Llama-3-8b) using task-specific JSON prompts, embed with BGE-large-en-v1.5 or ClinicalBERT, and classify with a feed-forward layer. Performance was evaluated using AUROC and AUPR on held-out test sets under earliest-label and latest-label settings.

## Key Results
- Count-based models (LightGBM/TabPFN) and MoA pipeline split most wins across eight clinical prediction tasks, with count-based models holding a slight overall edge
- Count-based models consistently outperformed the CLMBR transformer approach across all tasks
- SHAP analysis revealed recent medical events (within one year) were the strongest predictors across all tasks
- MoA pipeline generated task-adapted summaries that selectively amplified or de-emphasized different ICD-10 chapters based on the prediction target

## Why This Works (Mechanism)

### Mechanism 1: Feature Aggregation via Ontology Roll-up
Count-based models with ontology roll-ups perform competitively by reducing feature sparsity and improving signal-to-noise ratio. Fine-grained medical codes are mapped to higher-level categories (e.g., ICD-10 chapters), then aggregated into counts within two time bins: recent (1 year pre-prediction) and history (prior to 1 year). This two-dimensional feature vector is fed into classifiers. The core assumption is that clinically relevant predictive signal exists at the level of code categories rather than individual fine-grained codes, and recent events carry more predictive weight. When predictive signal is concentrated in rare, specific codes lost during roll-up aggregation, or when temporal ordering is crucial, this mechanism breaks down.

### Mechanism 2: Task-Adaptive Summarization in Mixture-of-Agents
The MoA pipeline generates intermediate clinical summaries that selectively emphasize or de-emphasize medical information based on the prediction target. An LLM is prompted to convert structured "EVENT at AGE" sequences into JSON-formatted summaries containing risk category, positive/negative drivers, and justification. This task-specific prompt causes the LLM to amplify certain ICD-10 chapters while down-weighting others, effectively performing relevance-based feature selection. The core assumption is that LLMs can implicitly learn clinical relevance patterns from task prompts. When the summarization LLM hallucinates clinically irrelevant information, or when critical rare features are omitted due to prompt constraints or LLM context limitations, this mechanism fails.

### Mechanism 3: Recency-Weighted Predictive Signal
Recent clinical events (within one year of prediction time) provide the strongest predictive signal across diverse EHR prediction tasks, reducing the need for modeling long-range temporal dependencies. The count-based feature engineering explicitly separates recent vs. historical time windows. SHAP analysis reveals that top contributors consistently derive from the recent time bin across tasks, suggesting that the most recent clinical state is the primary driver of near-term outcomes. For outcomes with long latency periods or where early-life exposures matter, the one-year window may miss critical signal.

## Foundational Learning

- **OMOP Common Data Model (CDM)**: Understanding how medical concepts (conditions, drugs) are organized into tables with standardized vocabularies is essential for replicating the feature engineering. Quick check: Can you explain how a patient's drug exposure would be represented in OMOP, and what an "ATC code" is?

- **SHAP (SHapley Additive exPlanations) Values**: The paper uses SHAP to interpret LightGBM predictions and identify which features drive model outputs. Understanding SHAP is key to grasping the interpretability argument for count-based models. Quick check: What does a positive SHAP value for a feature like "biliary obstruction" mean for a specific patient's pancreatic cancer risk prediction?

- **AUROC vs. AUPR Metrics**: Performance is reported in both AUROC and AUPR. AUPR is particularly important for imbalanced tasks where AUROC can be misleadingly optimistic. Quick check: Why might a model have a high AUROC but a low AUPR for a rare disease prediction task?

## Architecture Onboarding

- **Component map**: Data Source (EHRSHOT dataset) -> Feature Engineering (Count-based: ontology roll-up → ICD-10/ATC sections → Two time-bin aggregation) -> Classifier (LightGBM or TabPFN) / Feature Engineering (CLMBR: sequential tokenization → pretrained transformer embeddings) -> Classifier (LightGBM/TabPFN) / MoA Summarizer (LLM with task-specific JSON prompt → Structured summary) -> MoA Classifier (Text encoder → [CLS] embedding → Feed-forward layer)

- **Critical path**: Replicate the count-based pipeline first as it's the strongest baseline, simplest to implement, and requires only LightGBM/TabPFN and ontology mapping files. Use the provided training/validation/test splits from EHRSHOT to ensure fair comparison. Run the ablation on roll-up vs. no-rollup to verify performance gains.

- **Design tradeoffs**: Count-based is faster, more interpretable, and requires no LLM inference, while MoA may perform better on tasks requiring semantic reasoning but at higher computational cost and with less interpretability. TabPFN performs well without task-specific hyperparameter tuning but is limited to small- to medium-sized datasets, while LightGBM is more scalable but requires more tuning. Structured JSON prompts consistently outperformed unstructured free-text summaries in ablations.

- **Failure signatures**: Count-based models underperforming suggests checking if ontology roll-up is incorrectly mapping codes or if time bins are misaligned. MoA pipeline performing poorly indicates verifying prompt formatting, checking if the summarizer LLM is truncating long patient histories, or if the classifier is overfitting on short summaries. CLMBR embeddings not helping means ensuring the correct pretrained model weights are loaded and that tokenization matches the training corpus.

- **First 3 experiments**:
  1. Implement the count-based + LightGBM pipeline with ontology roll-up and two time bins on one EHRSHOT task (e.g., Pancreatic Cancer) and compare AUROC/AUPR against reported values.
  2. Remove the "history" time bin and train using only the "recent" bin to quantify performance drop and validate the paper's claim that recent events are the strongest predictors.
  3. Implement the MoA pipeline with structured JSON prompts vs. unstructured free-text prompts for a task where MoA performed well (e.g., Lupus) to measure the AUROC difference.

## Open Questions the Paper Calls Out

- **External validity**: Does the superior performance of count-based models persist when applied to multi-institutional datasets with distinct coding practices and patient populations? The benchmark was conducted exclusively on the EHRSHOT dataset; it is unknown if the "strong baseline" performance generalizes across different healthcare systems or if MoA/Transformers would prove more robust to distribution shifts.

- **Feature expansion**: How does the inclusion of high-granularity data domains (labs, vitals, procedures) alter the relative performance between count-based models and Mixture-of-Agents pipelines? The current study utilized a restricted feature set; LLM-based summarization may theoretically offer greater advantages when processing sparse, high-dimensional, or unstructured notes compared to the simpler tabular data tested.

- **LLM optimization**: Can optimized summarization strategies or more capable foundational LLMs close the performance gap on tasks where MoA currently underperforms count-based baselines? The MoA pipeline showed variable results, suggesting the current summarization step may not be optimally extracting signal for all prediction targets.

## Limitations
- Results are specific to structured EHR data from the OMOP CDM and the EHRSHOT dataset; performance on other EHR schemas or clinical domains remains untested
- The MoA pipeline's performance is contingent on the quality of the LLM summarizer; results may vary with different model versions or prompt engineering
- Count-based models use only two time bins, which may miss nuanced temporal patterns; finer-grained temporal discretization was not explored
- While count-based models offer SHAP-based interpretability, the MoA pipeline's LLM summarizer acts as a "black box," making it difficult to audit why certain features are emphasized or de-emphasized

## Confidence
- **High Confidence**: Count-based models remain competitive baselines; recency-weighted features are consistently important; MoA pipeline adapts summaries to task-specific prompts
- **Medium Confidence**: Count-based models outperform CLMBR across all tasks; MoA achieves strong performance on some tasks but not all; ontology roll-up improves signal-to-noise ratio
- **Low Confidence**: External validity of results beyond EHRSHOT dataset; robustness of MoA pipeline to different LLM versions or prompt formulations

## Next Checks
1. Run the count-based pipeline with three time bins (e.g., 0-1 year, 1-3 years, >3 years) to test whether finer temporal granularity improves performance beyond the current two-bin setup
2. Apply the count-based and MoA pipelines to a different OMOP-based EHR dataset (e.g., from a different hospital system) to assess whether the observed performance gaps persist
3. Manually inspect a sample of MoA-generated summaries for hallucinated or clinically irrelevant content, especially for rare diseases or complex tasks like lupus prediction