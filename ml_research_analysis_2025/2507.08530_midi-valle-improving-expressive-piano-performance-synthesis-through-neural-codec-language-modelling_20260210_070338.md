---
ver: rpa2
title: 'MIDI-VALLE: Improving Expressive Piano Performance Synthesis Through Neural
  Codec Language Modelling'
arxiv_id: '2507.08530'
source_url: https://arxiv.org/abs/2507.08530
tags:
- audio
- midi
- alle
- performance
- synthesis
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces MIDI-VALLE, a neural codec language model
  for expressive piano performance synthesis that overcomes generalization limitations
  of prior methods. By conditioning on reference audio and MIDI prompts, and using
  discrete tokenisation of both MIDI and audio via Octuple MIDI tokenisation and a
  high-fidelity audio codec (Piano-Encodec), the model achieves more consistent alignment
  between symbolic and acoustic representations.
---

# MIDI-VALLE: Improving Expressive Piano Performance Synthesis Through Neural Codec Language Modelling

## Quick Facts
- arXiv ID: 2507.08530
- Source URL: https://arxiv.org/abs/2507.08530
- Reference count: 0
- One-line primary result: Over 75% lower Fréchet Audio Distance than M2A baseline on ATEPP and Maestro datasets

## Executive Summary
MIDI-VALLE introduces a neural codec language model for expressive piano performance synthesis that overcomes generalization limitations of prior methods. The model conditions on reference audio and MIDI prompts, using discrete tokenisation of both MIDI and audio via Octuple MIDI tokenisation and Piano-Encodec, to achieve more consistent alignment between symbolic and acoustic representations. Trained on the large ATEPP dataset, MIDI-VALLE significantly outperforms the state-of-the-art M2A baseline, demonstrating improved synthesis quality and robustness in listening tests, though generalisation to non-classical genres like jazz remains challenging.

## Method Summary
MIDI-VALLE employs a two-stage approach: first, Piano-Encodec fine-tunes a generic Encodec on piano audio to compress 32kHz mono signals into discrete tokens using 4-level RVQ at 50 Hz frame rate. Second, the MIDI-VALLE architecture uses a 12-layer transformer decoder with autoregressive (AR) and non-autoregressive (NAR) components to predict these tokens from Octuple tokenised MIDI prompts. The AR decoder predicts the first codebook (structure/pitch) while the NAR decoder predicts the remaining three codebooks (timbre/fine details), conditioned on a 3-second audio prompt. Training uses ScaledAdam optimizer with learning rate 0.05 and Eden scheduler for approximately 300k steps on 2× A100 GPUs.

## Key Results
- Achieves over 75% lower Fréchet Audio Distance (FAD) than M2A baseline on ATEPP and Maestro datasets
- Demonstrates superior reconstruction quality with lower spectrogram distortion (NRMSE) and chroma distortion (MAE)
- Shows improved synthesis quality and robustness in human listening tests
- Maintains strong performance on classical music while struggling with jazz generalization

## Why This Works (Mechanism)

### Mechanism 1
Unified discrete tokenization improves alignment between symbolic instructions (MIDI) and acoustic outputs by treating performance synthesis as a language translation task. Both MIDI and audio are converted into discrete token sequences via Octuple tokenisation and Piano-Encodec, creating a shared discrete latent space where the model predicts audio tokens conditional on MIDI tokens, minimizing information loss typical of intermediate piano-roll representations.

### Mechanism 2
Prompt-based conditioning enables zero-shot adaptation to diverse acoustic environments by conditioning the NAR decoder on a 3-second audio prompt pair (audio + corresponding MIDI). This provides explicit "acoustic context" (timbre, room reverb, noise floor), allowing the model to reconstruct target audio with matching acoustics without retraining, analogous to speaker prompting in speech synthesis.

### Mechanism 3
Hierarchical decoding (AR + NAR) balances structural coherence with high-fidelity synthesis by separating the task into an autoregressive model predicting the first coarse codebook (structure/pitch) and a non-autoregressive model predicting the remaining 3 codebooks (timbre/fine details). The AR model ensures musical sequence logic while the NAR model efficiently infills high-frequency acoustic details conditioned on the prompt.

## Foundational Learning

- **Residual Vector Quantisation (RVQ)**: Why needed - Understand how Piano-Encodec compresses raw audio into discrete tokens; RVQ is a hierarchy where subsequent codebooks encode the error (residual) of previous ones. Quick check - If codebook 1 captures the primary melody, what does codebook 4 likely represent (timbre, pitch, or room tone)?

- **Inter-Onset Interval (IOI) vs. Duration**: Why needed - Octuple tokenisation captures timing better than piano rolls by distinguishing IOI (time between note starts) from Duration (length of note), critical for understanding articulation handling. Quick check - In a staccato passage, is IOI large or small relative to duration?

- **Autoregressive vs. Non-Autoregressive Decoding**: Why needed - MIDI-VALLE splits work between AR (sequential, slow, accurate structure) and NAR (parallel, fast, texture), explaining the inference pipeline tradeoff. Quick check - Which decoder handles the "which note comes next" problem, and which handles the "what does that note sound like" problem?

## Architecture Onboarding

- **Component map**: MIDI Prompt + Target MIDI (Octuple Tokenized) + Audio Prompt (Piano-Encodec Tokenized) -> AR Decoder (12-layer Transformer) -> Codebook 1 -> NAR Decoder (12-layer Transformer) -> Codebooks 2-4 -> Piano-Encodec Decoder -> Waveform

- **Critical path**: The alignment of the 3-second prompt is critical; misalignment between MIDI and audio prompts causes timing artifacts at the start of generation.

- **Design tradeoffs**: Octuple vs. Piano Roll offers higher fidelity for timing/IOI but requires more complex embedding pooling; Generic vs. Fine-tuned Codec uses Piano-Encodec specifically for piano, sacrificing generalizability for lower spectrogram distortion.

- **Failure signatures**: Genre Drift (high FAD on jazz), Truncation Artifacts (clicks/shifted timing at start), Reverb Leakage (fast notes smeared if prompt has heavy reverb).

- **First 3 experiments**: 1) Prompt Ablation: compare silence vs. real recording prompts; 2) Tokenization Alignment Test: feed deliberately offset prompt pairs; 3) Cross-Dataset Stress Test: train on ATEPP, evaluate on Pijama, inspect chroma distortion for extended chords.

## Open Questions the Paper Calls Out

1. **Genre Generalization**: How can the architecture or training data be adapted to improve generalization to stylistically distinct musical genres like jazz? The model struggles with jazz performances due to training solely on classical music, exhibiting high FAD on jazz datasets.

2. **Model Scaling Impact**: What is the impact of model scaling and codebook depth on the fidelity and expressiveness of generated audio? The study uses fixed configuration without ablating transformer sizes or RVQ depths.

3. **Long-form Synthesis Continuity**: How can segment-wise synthesis be improved to maintain acoustic continuity in long-form performances? Current approach using 3-second prompts results in discontinuities when stitching segments together.

4. **Physical Modeling Comparison**: How does MIDI-VALLE compare to physical modeling synthesis methods in terms of timbral realism and control? Current evaluation only compares against neural baselines, not non-neural DSP-based methods.

## Limitations
- Limited generalization to non-classical genres, particularly jazz, with significantly degraded FAD scores
- Reliance on 3-second prompts may not capture full variability needed for complex acoustic scenarios
- Discrete tokenization introduces quantization artifacts that may limit expressiveness for rapid ornaments
- Exclusion of pedal information from training dataset represents gap in capturing complete piano performance expressiveness

## Confidence

**High Confidence**: Comparative performance metrics showing MIDI-VALLE's superiority over M2A on FAD, spectrogram distortion, and chroma distortion across ATEPP and Maestro datasets.

**Medium Confidence**: Mechanism claims about unified discrete tokenization improving alignment between symbolic and acoustic representations - primarily architectural rather than experimental validation.

**Low Confidence**: Generalization claims to non-classical genres and robustness of prompt-based conditioning across diverse acoustic environments - paper acknowledges poor performance on jazz with limited supporting evidence.

## Next Checks

1. **Prompt Alignment Stress Test**: Conduct controlled experiments with deliberately misaligned 3-second audio prompts (0-200ms offsets) to measure timing drift and note accuracy in generated output.

2. **Cross-Genre Generalization Analysis**: Train on ATEPP and evaluate on Pijama while analyzing token-level differences, specifically tracking chroma distortion for extended chords and comparing token pattern distributions.

3. **Discrete Tokenization Resolution Impact**: Create ablations varying Octuple tokenisation resolution to measure impact on FAD and listening test scores, determining optimal balance between computational efficiency and expressiveness capture.