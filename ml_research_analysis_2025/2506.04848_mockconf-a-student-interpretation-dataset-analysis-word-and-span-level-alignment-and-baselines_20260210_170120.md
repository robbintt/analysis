---
ver: rpa2
title: 'MockConf: A Student Interpretation Dataset: Analysis, Word- and Span-level
  Alignment and Baselines'
arxiv_id: '2506.04848'
source_url: https://arxiv.org/abs/2506.04848
tags:
- alignment
- interpreting
- word
- span
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces MockConf, a new dataset of student simultaneous
  interpreting recordings across five European languages, with human-annotated transcriptions
  and alignments at both span and word levels. It presents InterAlign, a modern web-based
  tool designed for annotating long interpreting transcripts, and evaluates a baseline
  alignment system combining coarse BERT-based alignment with sub-segmentation and
  label classification.
---

# MockConf: A Student Interpretation Dataset: Analysis, Word- and Span-level Alignment and Baselines

## Quick Facts
- **arXiv ID**: 2506.04848
- **Source URL**: https://arxiv.org/abs/2506.04848
- **Reference count**: 40
- **Primary result**: Introduces MockConf dataset of student interpreting recordings with human annotations; achieves 95%+ precision in coarse BERT-based alignment and 60-70% F1 in span segmentation.

## Executive Summary
This paper presents MockConf, a new dataset of student simultaneous interpreting recordings across five European languages (Czech-centered with English, German, Spanish, and French). The dataset includes human-annotated transcriptions and alignments at both span and word levels, capturing the unique challenges of interpreting where paraphrasing, omission, and addition are common. The authors introduce InterAlign, a modern web-based tool for annotating long interpreting transcripts, and evaluate a baseline alignment system combining coarse BERT-based alignment with sub-segmentation and label classification. The system demonstrates strong segmentation performance and moderate span alignment, highlighting the difficulty of automatic alignment in interpreting compared to standard translation alignment tasks.

## Method Summary
The paper proposes a three-step pipeline for automatic alignment of interpreting transcripts: (1) Coarse alignment using BERTAlign with contextual embeddings to identify high-precision span correspondences (max_align=10, top_k=10, window=10), (2) Sub-segmentation via SimAlign's itermax strategy using XLM-R embeddings, splitting spans at aligned punctuation boundaries, and (3) Label classification using a shallow neural network (LaBSE similarity + length ratios) to predict span labels (TRAN, PARA, SUM, GEN, REPL). The system is trained on an 80/20 split of the development set and evaluated on a held-out test set, with metrics including segmentation F1, span alignment F1, word alignment AER, and label classification accuracy.

## Key Results
- BERTAlign achieves very high precision (95.05-97.37%) across all splits, though recall remains moderate (30.87-50.50%)
- BA+sub sub-segmentation improves span segmentation F1 to 65.75 on test set, surpassing annotator 2
- Label classification shows modest improvement (31.91→38.16 F1) using LaBSE similarity and length ratios
- First large-scale analysis of interpreting strategies reveals summarization as dominant divergence type and higher accuracy in relay interpreting

## Why This Works (Mechanism)

### Mechanism 1
Coarse BERT-based alignment provides high-precision initial span segmentation that anchors subsequent fine-grained processing. BERTAlign computes contextual embeddings for source and target text windows, then identifies likely correspondence pairs using similarity scoring with length penalty constraints (max_align=10, top_k=10, window=10). The system prioritizes precision over recall to ensure downstream sub-segmentation operates on valid anchor regions. Core assumption: Interpreting transcripts, despite omissions and paraphrasing, retain enough semantic overlap for embedding-based similarity to detect correspondence. Evidence anchors: [abstract] "Existing parallel corpora... fail to model long-range interactions between speech segments"; [section 4.4 Results] "BA demonstrates a very high precision" (95.05-97.37% across splits); [corpus] Neighbor paper on parallel tokenizers (arXiv:2510.06128) supports embedding-based correspondence detection. Break condition: When source-target semantic divergence exceeds embedding similarity thresholds (e.g., heavy summarization with >40% content reduction), coarse alignment produces false negatives.

### Mechanism 2
Sub-segmentation at punctuation boundaries with itermax word alignment refines coarse spans into interpretable units. After BERTAlign produces n-m span alignments, SimAlign's itermax strategy generates word-level links using XLM-R embeddings. Spans split where punctuation aligns across languages, creating shorter, more granular alignment units. Core assumption: Punctuation in interpreting transcripts marks cognitively meaningful boundaries that correspond across languages. Evidence anchors: [section 4.1 Methodology] "We refine the spans by splitting them at points where two punctuations align in the source and target transcripts"; [section 4.4 Results] "BA+sub even surpasses annotator 2 in inter-annotator comparisons" (F1 65.75 vs 63.75); [corpus] Weak direct corpus support; neighbor papers focus on tokenization/alignment but not specifically punctuation-based segmentation in speech. Break condition: When transcribers insert punctuation inconsistently (common in disfluent speech), sub-segmentation produces misaligned boundaries.

### Mechanism 3
A shallow neural classifier using LaBSE similarity and span length ratios predicts span labels (TRAN, PARA, SUM, GEN, REPL). For each aligned span pair, LaBSE computes cross-lingual sentence embeddings; cosine similarity plus source/target length ratio feeds into a 2-layer MLP (hidden size 100) for 5-way classification. Core assumption: Label categories correlate with semantic similarity and compression ratio—translations score high similarity with ratio ≈1, summarizations score moderately with ratio <1. Evidence anchors: [section 3.2 Analysis] "length ratios for generalization and summarization are lower than one: 0.9 and 0.6 on average, respectively"; [section 4.4 Results] "label classification improves upon the default label prediction, the improvement is modest" (F1 31.91→38.16 on test); [corpus] No direct corpus evidence for LaBSE+length classification; assumption of correlation remains unvalidated externally. Break condition: When paraphrases involve substantial lexical substitution but preserve meaning, low LaBSE similarity triggers misclassification as replacement or summarization.

## Foundational Learning

- **Ear-Voice Span (EVS) / Time Lag**: Why needed here: Simultaneous interpreting operates with "minimal TIME LAG of a few seconds" (cited definition), causing systematic divergences from source that alignment must accommodate. Quick check question: If an interpreter lags 3 seconds behind a speaker speaking 150 words/minute, approximately how many words of context are unavailable when the interpreter begins translating a segment?

- **Word Alignment (Sure vs. Possible Links)**: Why needed here: The dataset distinguishes context-independent "sure" alignments from context-dependent "possible" alignments (section 2.3), affecting how evaluation metrics like AER are computed. Quick check question: Would aligning English "the" to French "le" be a sure link or possible link? Why?

- **Inter-Annotator Agreement (Cohen's Kappa)**: Why needed here: The paper reports moderate segmentation agreement (κ=0.56-0.57) and fair label agreement (κ=0.25-0.41), indicating task subjectivity that baselines must approach as a ceiling. Quick check question: If human annotators achieve κ=0.56 on segmentation, what is the realistic upper bound for automatic system performance?

## Architecture Onboarding

- **Component map**: Raw audio → WhisperX (ASR) → Manual revision → [InterAlign annotation tool] → Span-level alignment (7 labels) → Word-level alignment (sure/possible)
For automatic pipeline: Transcript pair → BERTAlign (coarse) → SimAlign+Punctuation (sub-segment) → LaBSE+Length classifier → Label prediction

- **Critical path**: 1. Coarse alignment quality determines whether valid span pairs exist for downstream processing. 2. Punctuation consistency controls sub-segmentation granularity. 3. Label classifier depends on both alignment correctness and feature quality (similarity + length ratio).

- **Design tradeoffs**: High precision vs. recall in coarse alignment: Paper prioritizes precision (95%+) at cost of recall (~30-50%), accepting missed alignments over incorrect ones. Punctuation-based vs. semantic sub-segmentation: Current approach uses punctuation (fast, interpretable) but paper acknowledges it's "unreliable" for interpreting transcripts. Flat vs. hierarchical annotation: Dataset uses flat span labels; no hierarchical structure capturing nested reformulations.

- **Failure signatures**: Very low span count output: Coarse alignment failed—check for extreme length imbalance or language pair mismatch with BERTAlign training. All spans labeled ADDU: Label classifier not trained or features not computed correctly. Word alignment AER > 70%: Sliding window parameters (128/64) or distance threshold (50 tokens) misconfigured for recording length.

- **First 3 experiments**: 1. **Sanity check on development set**: Run BA only (no sub-segmentation), verify segmentation precision >90%. If lower, inspect BERTAlign parameter configuration. 2. **Inter-annotator baseline comparison**: Evaluate system output against both annotators on the double-annotated recording; target F1 between annotator disagreement bounds (14.85-30.46% exact match). 3. **Ablation of label classifier features**: Compare LaBSE-only vs. LaBSE+length features; paper shows modest improvement from combined features, validate this replicates.

## Open Questions the Paper Calls Out

### Open Question 1
Can automatic alignment performance for simultaneous interpreting be improved by developing methods that do not rely on punctuation as a segmentation signal? Basis in paper: [explicit] "a lot finally remains to be done to improve our automatic processing tools which do not rely on punctuation as it is a very unreliable alignment indicator in interpreting." Why unresolved: Current baseline sub-segmentation depends on punctuation matching, which performs poorly on interpreting transcripts where punctuation is sparse and inconsistent. What evidence would resolve it: A new alignment method that outperforms BA+sub on the MockConf test set by using alternative segmentation cues (e.g., acoustic features, semantic similarity, or prosodic information) rather than punctuation.

### Open Question 2
How can inter-annotator agreement for span-level alignment and labeling be improved in simultaneous interpreting annotation? Basis in paper: [explicit] "The results presented in Table 4 show... Cohen's Kappa scores are 0.56 and 0.57 for the source and target sides, indicating moderate agreement... These results underscore the difficulty of the task, as alignment link presupposes accurate segmentation, which, as we saw, is not guaranteed due to the task ambiguities." Why unresolved: Annotators differ in granularity preferences and in distinguishing paraphrase from translation at the token level. What evidence would resolve it: Revised annotation guidelines or training procedures that yield Cohen's Kappa above 0.7 on a held-out subset of MockConf.

### Open Question 3
How do interpreting strategies (e.g., summarization, paraphrase, omission rates) vary systematically across different source-target language pairs? Basis in paper: [explicit] In Future Work: "to also study how interpreting strategies vary depending on the source and target languages." Why unresolved: The paper's analysis aggregates across language pairs; the dataset contains five languages but systematic cross-linguistic comparison was not conducted. What evidence would resolve it: A comparative analysis showing statistically significant differences in label distributions (TRAN, PARA, SUM, etc.) across the cs→en, cs→de, cs→es, cs→fr, and reverse directions in MockConf.

## Limitations

- **Task inherent difficulty**: The moderate inter-annotator agreement (κ=0.56-0.57 for segmentation, 0.25-0.41 for labels) suggests human-level performance may be the practical ceiling for this dataset, reflecting fundamental task ambiguity rather than methodological shortcomings.

- **Punctuation-based segmentation unreliability**: The paper acknowledges that punctuation-based sub-segmentation is "unreliable" for interpreting data, yet this remains the primary method for refining coarse alignments, potentially limiting downstream performance.

- **Unvalidated feature assumptions**: The label classifier relies on LaBSE similarity and length ratios as proxies for semantic categories, but the paper provides no external validation that these features genuinely capture the intended distinctions.

## Confidence

**High Confidence** (Level 1-2 evidence): The dataset construction methodology and annotation procedures are well-specified; the three-step pipeline architecture is clearly described and reproducible; core quantitative results (precision/recall for BA, F1 scores for segmentation) are stable across multiple evaluations.

**Medium Confidence** (Level 3-4 evidence): Interpretation of analysis findings (e.g., that summarization dominates divergences, relay interpreting shows higher accuracy); claims about the uniqueness of interpreting alignment challenges relative to standard translation alignment; assessment that automatic systems perform at "moderate" levels.

**Low Confidence** (Level 5-6 evidence): The specific mechanism by which LaBSE+length features predict semantic categories; generalizability of findings beyond student interpreting in the five languages studied; claims about the practical utility of the classifier for downstream applications.

## Next Checks

1. **Annotator Consistency Validation**: Recompute inter-annotator agreement metrics using only the double-annotated recording, and test whether the automatic system's performance falls within the human agreement bounds. This would validate whether reported system performance represents meaningful progress or merely approaches the irreducible task difficulty.

2. **Feature Ablation Study**: Systematically evaluate the label classifier with individual features (LaBSE similarity only, length ratio only) and combined features to quantify the marginal contribution of each. This would confirm whether the modest improvements reported are robust and attributable to the claimed mechanisms.

3. **Cross-Domain Generalization Test**: Apply the trained classifier to a held-out subset of recordings from a different language pair or proficiency level than those used in training. This would assess whether the length-based heuristics generalize beyond the specific patterns in the development set.