---
ver: rpa2
title: How Programming Concepts and Neurons Are Shared in Code Language Models
arxiv_id: '2506.01074'
source_url: https://arxiv.org/abs/2506.01074
tags:
- languages
- english
- language
- neurons
- other
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how large language models (LLMs) represent
  multiple programming languages (PLs) and English within their concept space. The
  authors use logit lens to analyze intermediate layer embeddings during few-shot
  translation tasks across 21 PL pairs, revealing that the concept space is closer
  to English and PL keywords, with these tokens appearing in the second half of intermediate
  layers.
---

# How Programming Concepts and Neurons Are Shared in Code Language Models

## Quick Facts
- arXiv ID: 2506.01074
- Source URL: https://arxiv.org/abs/2506.01074
- Reference count: 40
- This paper investigates how LLMs represent multiple programming languages and English within their concept space, revealing English-centric processing with PL-specific neurons concentrated in bottom layers and PL-exclusive neurons in top layers.

## Executive Summary
This study analyzes how large language models represent multiple programming languages and English within their internal concept space. Using logit lens to decode intermediate layer embeddings during few-shot translation tasks across 21 PL pairs, the authors find that the concept space is closer to English and PL keywords, with these tokens appearing prominently in the second half of intermediate layers. They also apply language activation probability entropy (LAPE) to analyze neuron activations across 11 PLs and English, discovering that language-specific neurons cluster in bottom layers while PL-exclusive neurons concentrate in top layers. The findings reveal structural patterns in how LLMs internally represent PLs, with implications for more efficient multilingual code models.

## Method Summary
The authors use three complementary methods to analyze LLM representations of programming languages. Logit lens decodes intermediate layer embeddings during few-shot translation tasks to reveal concept space structure, tracking token probabilities and ranks across all 32 layers. Cross-lingual alignment (MEXA) measures how closely PL representations cluster in embedding space using parallel code snippets. Language activation probability entropy (LAPE) identifies neurons with selective language activation patterns, with deactivation experiments measuring perplexity changes to validate language-specificity. The analysis uses CodeLlama 7B and Llama 3.1 8B models, with data including 581 super-parallel code snippets across 7 PLs and 50k raw files per language for 11 PLs plus English Wikipedia.

## Key Results
- English and PL keywords appear with high probability in the second half of intermediate layers during PL translation, peaking before declining as expected output tokens dominate
- Language-specific neurons concentrate in bottom layers (indices 0-4), while PL-exclusive neurons appear predominantly in top layers (indices 29-31)
- PLs with high cross-lingual alignment (C#, Java) share neurons extensively, making language-specific identification challenging
- The concept space is closer to English than to any individual PL, with PL keywords appearing prominently regardless of input/output PL

## Why This Works (Mechanism)

### Mechanism 1: English-Centric Concept Space with PL Keyword Sharing
- Claim: LLMs route PL processing through an English-adjacent concept space, with PL keywords appearing prominently in intermediate layers.
- Mechanism: Logit lens analysis reveals that during PL translation tasks, English keywords and PL keywords appear with high probability in the second half of intermediate layers (roughly layer 15+), peaking before declining as expected output tokens dominate in final layers.
- Core assumption: The concept space proximity reflects shared semantic representations rather than tokenizer artifacts.
- Evidence anchors:
  - [abstract] "the concept space is closer to English (including PL keywords) and assigns high probabilities to English tokens in the second half of the intermediate layers"
  - [Section 3.1] Figures 2a, 2b show English and PL keyword probabilities rise sharply around layer 15, with PL keywords like C++ and C# appearing prominently regardless of input/output PL
  - [corpus] Limited direct corpus support; related work on "semantic hub hypothesis" (Wu et al.) explores similar multilingual routing but focuses on Python-to-English pairs specifically
- Break condition: If English tokens did NOT appear prominently during PL-to-PL translation, or if PL-specific tokens dominated from early layers without English intermediation, the pivot mechanism would not hold.

### Mechanism 2: Stratified Language-Specific Neuron Distribution
- Claim: Language-specific neurons cluster in bottom layers (indices 0-4), while PL-exclusive neurons concentrate in top layers (indices 29-31).
- Mechanism: LAPE identifies neurons with low entropy across language activation patterns. Deactivating bottom-layer language-specific neurons affects primary language PPL more than other languages. Top-layer exclusive neurons, when present, enable language-specific output generation without cross-language interference.
- Core assumption: FFN neurons store language-specific knowledge in a partitionable manner; GLU-variant transformers allow meaningful neuron-level attribution.
- Evidence anchors:
  - [Section 3.3] "language-specific neurons are more concentrated in the bottom layers, followed by another smaller peak observed around the three quarter point of the layers"
  - [Section 3.3, Figure 6] "neurons exclusive to each PL are predominantly selected from the top layers (indices 29 to 31)"
  - [corpus] Tang et al. (2024) demonstrated LAPE effectiveness for natural languages; this paper extends to PLs with mixed success for highly aligned languages
- Break condition: If deactivating "language-specific" neurons caused uniform PPL increases across all languages, or if neuron distributions were random across layers, stratification would not exist.

### Mechanism 3: Alignment-Based Neuron Sharing Inhibits Language-Specific Identification
- Claim: PLs with high cross-lingual alignment (e.g., C#, Java) share neurons extensively, making language-specific neuron identification infeasible.
- Mechanism: MEXA alignment scores show C# and Java achieve highest alignment across layers. When LAPE-attributed neurons for these languages are deactivated, PPL changes affect both primary and other languages similarly, indicating shared rather than language-specific representations.
- Core assumption: High MEXA alignment correlates with neuron-level sharing; keyword set size influences alignment (C#/C++ have larger keyword sets).
- Evidence anchors:
  - [Section 3.2, Figure 3] "C# achieves the best alignment overall across all layers in both models"
  - [Section 3.3, Figure 5] "for C# and Java (and C++ and HTML in Figure 7) the gap [between primary and other language PPL change] is less pronounced"
  - [corpus] No direct corpus evidence on PL alignment-neuron coupling; natural language work shows similar cross-lingual sharing patterns
- Break condition: If highly aligned PLs showed clean neuron separability with distinct PPL impacts, alignment would not predict neuron sharing.

## Foundational Learning

- **Logit Lens**:
  - Why needed here: Core method for decoding intermediate layer embeddings into token probabilities, revealing the concept space structure.
  - Quick check question: Given hidden state h^(ℓ) at layer ℓ, what operation projects it to logits, and why apply LayerNorm first?

- **LAPE (Language Activation Probability Entropy)**:
  - Why needed here: Quantifies how selectively neurons activate for specific languages; low entropy = language-specific.
  - Quick check question: A neuron activates 80% for Python, 5% for Java, 5% for C++, 5% for Go, 5% for JavaScript—would LAPE classify it as Python-specific under default τ=0.95?

- **Cross-lingual Alignment (MEXA)**:
  - Why needed here: Measures how closely PL representations cluster in embedding space across parallel code snippets.
  - Quick check question: If C# embeddings consistently have highest cosine similarity to parallel snippets in other PLs, what would MEXA score approach?

## Architecture Onboarding

- **Component map**:
  - Input tokens → Embedding → Layers 0-4 (bottom: language-specific neurons, input space) → Layers 5-28 (middle: concept space, English/PL keyword activation) → Layers 29-31 (top: PL-exclusive neurons, output space) → Unembedding → Logits

- **Critical path**:
  1. Prepare parallel PL dataset (7 PLs minimum for super-parallel analysis)
  2. Run few-shot translation with logit lens at each layer
  3. Classify decoded tokens by keyword membership (English vs. each PL)
  4. Compute MEXA alignment for all PL pairs
  5. Run LAPE with τ=0.95, γ=0.01 (or fixed ν≈400 neurons per language)
  6. Deactivate identified neurons; measure PPL changes

- **Design tradeoffs**:
  - Logit lens vs. tuned lens: Logit lens preserves intermediate signals but noisier; tuned lens trained for final prediction may obscure concept space
  - LAPE threshold sensitivity: Higher τ yields fewer but more specific neurons; lower τ captures more but increases cross-language contamination
  - CodeLlama vs. Llama 3.1: CodeLlama has more shared PL neurons (harder neuron identification); Llama 3.1 has cleaner PL-specific neuron separation

- **Failure signatures**:
  - LAPE identifies neurons but deactivation causes similar PPL increase across all languages → neurons are shared, not language-specific (expected for C#, Java, C++, HTML)
  - Keyword probability never rises in intermediate layers → translation task not understood or model architecture fundamentally different
  - Alignment scores < 0.4 for all pairs → parallel data quality issues or model not multilingual

- **First 3 experiments**:
  1. Replicate Figure 2 for a single PL pair (e.g., Java→Python): track expected output probability and keyword ranks across all 32 layers to verify English/PL keyword emergence pattern
  2. Compute MEXA alignment matrix for all 7 PLs at layer 15: confirm C# and Java show highest average alignment
  3. Run LAPE with ν=400, deactivate English-specific neurons, measure PPL change for English vs. average across 11 PLs to verify asymmetry (English deactivation should primarily affect English)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the observed layer-wise distributions of language-specific neurons generalize to non-Llama-based architectures?
- Basis in paper: [explicit] The authors limit analysis to Llama-based models but explicitly state it is "important to explore other architectures for broader validation."
- Why unresolved: The study focuses solely on CodeLlama and Llama 3.1, leaving the structural patterns in encoder-decoder or non-GLU-variant models unknown.
- What evidence would resolve it: Applying the LAPE and logit lens methods to diverse architectures (e.g., CodeT5) and comparing neuron distribution patterns.

### Open Question 2
- Question: Can high-quality, LLM-generated super-parallel datasets successfully extend this analysis beyond the current seven-language limit?
- Basis in paper: [explicit] The analysis is constrained to seven PLs due to the source data; the authors suggest generating "super-parallel data for more languages using more powerful LLMs."
- Why unresolved: Public super-parallel datasets are currently restricted to specific sources (GeeksforGeeks), limiting the breadth of cross-lingual alignment analysis.
- What evidence would resolve it: Generating a synthetic parallel dataset for >10 PLs and validating that cross-lingual alignment metrics (MEXA) remain consistent.

### Open Question 3
- Question: How does the restriction to single-token keywords skew the interpretation of latent embeddings for PLs with complex keywords?
- Basis in paper: [explicit] The authors acknowledge that filtering for single-token keywords excludes constructs that "lack direct English meanings or map to multiple tokens."
- Why unresolved: The logit lens analysis relies on a simplified keyword set, potentially missing semantic nuance or misinterpreting the concept space.
- What evidence would resolve it: Constructing a comprehensive semantic dictionary for PL keywords and re-evaluating the probability distributions in intermediate layers.

## Limitations

- The super-parallel dataset contains only 581 snippets across 7 PLs, which may not capture the full diversity of programming paradigms or edge cases in language-specific constructs
- The LAPE method relies on threshold-based neuron selection (τ=0.95, γ=0.01), which introduces arbitrary cutoffs that could miss language-specific representations or include false positives
- The analysis assumes GLU-variant transformers allow meaningful neuron-level attribution, but this may not hold for other architectures like attention-only transformers

## Confidence

- **High confidence**: The observation that English and PL keywords appear prominently in intermediate layers during PL translation tasks is well-supported by direct logit lens measurements across multiple PL pairs
- **Medium confidence**: The correlation between cross-lingual alignment (MEXA scores) and neuron sharing represents an inference from correlation rather than direct causation
- **Low confidence**: The claim that PLs like C# and Java are "too aligned to identify language-specific neurons" is somewhat overstated—the paper shows reduced but not eliminated separability

## Next Checks

1. **Dataset size sensitivity analysis**: Repeat the LAPE analysis with progressively larger subsets of the 50k raw files (5k, 10k, 25k) to determine the minimum sample size needed for stable neuron identification and whether current conclusions hold at smaller scales

2. **Architecture generalization test**: Apply the same methodology to an attention-only transformer (not GLU-variant) trained on the same multilingual code corpus to verify whether neuron-level attribution patterns persist across architectures or represent GLU-specific artifacts

3. **Cross-task consistency check**: Run the logit lens analysis during non-translation PL tasks (code completion, bug detection) to determine whether English/keyword emergence in intermediate layers is specific to translation or represents a general processing pattern across all code-related tasks