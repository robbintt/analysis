---
ver: rpa2
title: Online Learning and Unlearning
arxiv_id: '2505.08557'
source_url: https://arxiv.org/abs/2505.08557
tags:
- regret
- unlearning
- algorithm
- learning
- online
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the problem of online learning-unlearning,
  where a model is updated sequentially while accommodating unlearning requests. The
  authors propose two algorithms, passive and active OLU, built upon online gradient
  descent.
---

# Online Learning and Unlearning

## Quick Facts
- **arXiv ID:** 2505.08557
- **Source URL:** https://arxiv.org/abs/2505.08557
- **Reference count:** 40
- **Primary result:** Two algorithms (passive and active OLU) achieve regret bounds comparable to standard OGD while providing unlearning guarantees for online convex optimization with interleaved deletion requests

## Executive Summary
This paper introduces online learning-unlearning (OLU), a framework where a model is updated sequentially while accommodating unlearning requests to delete specific data points. The authors propose two algorithms built upon online gradient descent (OGD): passive OLU, which leverages OGD's contractive property and injects noise at deletion times, and active OLU, which uses an offline unlearning algorithm to shift the model toward a solution excluding deleted data. Both achieve regret bounds comparable to standard OGD while providing statistical unlearning guarantees via Rényi divergence constraints.

## Method Summary
The paper presents two algorithms for online learning-unlearning: Passive OLU (Algorithm 1) uses OGD's contractive property to enable unlearning via calibrated noise injection at deletion times, requiring no extra computation. Active OLU (Algorithm 2) performs auxiliary gradient descent steps to move closer to the ERM solution on retained data before adding noise, reducing required perturbation at the cost of O(log τ[i]) overhead per deletion. Both methods provide (α, ε)-online unlearning guarantees while maintaining regret bounds of O(log T) for strongly convex losses and O(√T) for convex losses.

## Key Results
- Passive OLU achieves O(log T) regret for strongly convex losses and O(√T) for convex losses while providing unlearning guarantees
- Active OLU improves regret to O(log T + k(LD² + Ld/με)) for strongly convex losses under Assumption 2
- Both methods demonstrate that unlearning can be achieved without sacrificing the standard OGD regret bounds
- The framework works under standard convexity and smoothness assumptions with γ-contraction property

## Why This Works (Mechanism)

### Mechanism 1: Passive Unlearning via Contractive Decay and Calibrated Noise
When OGD satisfies γ-contraction, the influence of a deleted point decays exponentially with subsequent updates. At deletion time τ[i], injecting Gaussian noise N(0, σ_i²) with scale σ_i ∝ γ^(τ[i]−u[i]) Δ_{u[i]} makes outputs statistically indistinguishable under Rényi divergence. The method requires: (1) Markovian update function g_t, (2) γ-contraction property, (3) bounded sensitivity Δ_t, and (4) appropriate learning rates (η ≤ 2/β for convex, η ≤ 1/(β+μ) for strongly convex).

### Mechanism 2: Active Unlearning via Descent-to-Delete with ERM Anchoring
Active OLU performs I_{1,i} gradient descent steps to move OGD output closer to ERM solution, then I_2 steps on retained set using descent-to-delete. This reduces divergence between trajectories, requiring less noise (σ_i ∝ γ^I_2 rather than γ^(τ[i]−u[i])). Requires: strong convexity, smoothness, Assumption 2 (individual minimizers align with aggregate ERM), and sufficient iterations I_{1,i} ≥ log(1/γ) · μDτ[i]/L.

### Mechanism 3: Interval-Based Unlearning Guarantee with Permanent Protection
The (α, ε)-OLU guarantee ensures deleted points' influence is statistically bounded forever. Each interval [τ[i], τ[i+1]) enforces indistinguishability from retraining without points deleted up to that time, cascading protection to all previously removed points via interval-wise Rényi divergence constraints.

## Foundational Learning

- **Concept: Online Convex Optimization (OCO)**
  - Why needed: The framework builds on OCO as base learning paradigm; understanding regret, oblivious adversary model, and O(log T)/O(√T) targets is essential before adding unlearning constraints
  - Quick check: Can you explain why sublinear regret implies the learner's cumulative loss approaches the best-fixed-hindsight solution, and why this matters when the comparator can change after deletions?

- **Concept: Rényi Differential Privacy**
  - Why needed: Unlearning guarantee uses Rényi divergence D_α rather than (ε, δ)-DP; understanding why Rényi DP composes more cleanly for iterative contractive noise mechanism is crucial
  - Quick check: If an algorithm satisfies (α, ε)-Rényi DP, what is the relationship to (ε', δ)-differential privacy, and why does the paper use α > 1 + ln(1/δ)/ε for conversions?

- **Concept: Contractiveness in Gradient Methods**
  - Why needed: γ-contraction property is core enabler of passive unlearning; understanding why gradient descent on smooth/strongly-convex functions is contractive, how projection preserves this, and why contraction causes influence decay is essential
  - Quick check: For a β-smooth and μ-strongly-convex function with η = 2/(β+μ), what is the contraction coefficient γ, and what happens to γ as the condition number β/μ increases?

## Architecture Onboarding

- **Component map:** Cost Functions f_1:T → Base OGD Learner → Output z_t → Deletion Request → [Passive Path (noise only) / Active Path (GD steps + noise)] → Unlearning Guarantee Check → Future Outputs

- **Critical path:**
  1. Implement base OGD with projection onto convex set K (Equation 2)
  2. Verify contraction coefficient for your loss landscape (compute β, μ; derive γ)
  3. Track deletion timing (u[i], τ[i] pairs) to compute noise scale σ_i
  4. Inject noise at deletion times only (passive) or run auxiliary GD steps (active)
  5. Maintain running regret and verify against theoretical bounds

- **Design tradeoffs:**
  | Choice | Benefit | Cost | When to Use |
  |--------|---------|------|-------------|
  | Passive OLU | Zero extra computation; simple | More noise for early deletions; worse regret for convex (non-strongly-convex) losses | Strongly convex losses; late deletions; compute-constrained |
  | Active OLU | Less noise; better regret independent of deletion indices | O(log τ[i]) extra gradient steps per deletion; requires Assumption 2 | When deletion timing is adversarial; strongly convex losses; can afford compute |
  | Constant learning rate | Uniform O(k^{1.1}√T) regret regardless of deletion schedule | Requires knowing T, k ahead; suboptimal for favorable schedules | Worst-case deletion patterns; known horizon |
  | Adaptive learning rate | Bounds independent of T, U when gradient norms decay | Requires public gradient norms; more complex analysis | When you can observe/estimate gradient magnitudes |

- **Failure signatures:**
  - Regret explodes beyond O(√T) → Check: Is deletion occurring on very early points (small u[i]) in convex setting? Consider constant learning rate or active method
  - Unlearning guarantee violated → Check: Did noise scale σ_i correctly account for contraction? Verify γ and Δ_t computation. Ensure independence of noise samples
  - Active method not converging → Check: Are I_{1,i} and I_2 steps sufficient? Verify Assumption 2 holds for your cost functions
  - Dimension dependence in regret → Check: Are you using DP-based online learning instead of OLU? The d factor in DP methods comes from per-coordinate noise

- **First 3 experiments:**
  1. Baseline OGD regret without deletions: Run standard OGD on synthetic convex/strongly-convex cost functions to establish baseline regret curves O(log T) and O(√T)
  2. Passive OLU with controlled deletion schedule: Inject k deletions at known times, vary u[i] and τ[i]−u[i], measure regret degradation, noise scale prediction, and statistical distance
  3. Active vs. Passive comparison on adversarial deletions: Design deletion schedule with small u[i] to stress-test passive method, compare regret curves, total noise variance, and computational overhead

## Open Questions the Paper Calls Out

- **Question 1:** What is the information-theoretic lower bound on regret for online learning-unlearning as a function of T, and how does it depend on the computational complexity constraints of the unlearning algorithm?
  - Basis: "Although deriving a lower bound on regret in terms of T would be valuable, we defer it to future work, since such a bound requires restricting the computation complexity of the unlearning algorithms."
  - Why unresolved: Proving lower bounds requires restricting the algorithm class; without constraints, FTL-type algorithms can achieve exact unlearning with standard regret
  - What evidence would resolve it: Formal lower bound proof showing tightness with achieved O(log T + k²) regret under specified computational constraints

- **Question 2:** Can active OLU algorithms achieve strong regret guarantees without relying on strong convexity assumptions?
  - Basis: "Several open problems remain including... designing more efficient active OLU algorithms that do not rely on strong convexity"
  - Why unresolved: Descent-to-delete framework and noise calibration in active OLU both depend on strong convexity for convergence guarantees and contraction properties
  - What evidence would resolve it: Active unlearning algorithm with provable regret bounds under only convex or weakly convex loss functions

- **Question 3:** Can online learning algorithms be explicitly designed to be more "unlearning-friendly," achieving better regret-unlearning tradeoffs than adapting existing OGD-based approaches?
  - Basis: "whether more unlearning friendly online learning algorithms can be designed"
  - Why unresolved: Current approaches retrofit OGD with unlearning mechanisms; fundamentally different update rules might better balance regret and unlearning
  - What evidence would resolve it: Novel algorithms whose regret bounds improve upon the G(T,U) dependency term that plagues passive methods in convex settings

## Limitations

- The active method relies on strong convexity assumptions, while passive works for general convex losses
- Requires Assumption 2 (individual function minimizers align with aggregate ERM) for active method convergence
- Exact sensitivity values Δ_t are algorithm inputs but not explicitly specified how to compute from problem parameters
- Assumes oblivious adversary and known or bounded number of deletions k

## Confidence

- **High:** Unlearning guarantee via Rényi divergence (Definition 2, Theorem 1) - follows from established privacy amplification by iteration results
- **Medium:** Regret bounds (Theorems 2-3) - depend on correct sensitivity computation and learning rate tuning
- **Medium:** Assumption 2 validity - crucial for active method but requires empirical validation on specific cost function families

## Next Checks

1. **Sensitivity calibration:** Derive and validate exact computation of Δ_t for standard convex losses (linear, logistic) using problem parameters (L, D) rather than treating as algorithm input
2. **Assumption 2 verification:** Test whether individual function minimizers actually align with aggregate ERM on synthetic strongly-convex losses, or identify cost function families where this breaks
3. **Early deletion stress test:** Systematically evaluate regret degradation when deletions occur on very early points (small u[i]) with long gaps to τ[i], comparing passive vs. active methods under worst-case deletion schedules