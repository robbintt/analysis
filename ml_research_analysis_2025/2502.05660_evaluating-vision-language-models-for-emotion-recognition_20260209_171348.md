---
ver: rpa2
title: Evaluating Vision-Language Models for Emotion Recognition
arxiv_id: '2502.05660'
source_url: https://arxiv.org/abs/2502.05660
tags:
- emotion
- prediction
- llav
- emotions
- class
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents the first comprehensive evaluation of Vision-Language
  Models (VLMs) for evoked emotion recognition from images. The authors create a benchmark
  dataset (EVE) by combining multiple existing emotion datasets and evaluate seven
  popular VLMs including GPT-4o and various LLaVA variants.
---

# Evaluating Vision-Language Models for Emotion Recognition

## Quick Facts
- **arXiv ID**: 2502.05660
- **Source URL**: https://arxiv.org/abs/2502.05660
- **Reference count**: 26
- **Primary result**: VLMs perform poorly at zero-shot emotion recognition, with GPT-4o achieving weighted F1 scores of 0.635-0.503 across datasets while open-source models achieve 0.593-0.401

## Executive Summary
This paper presents the first comprehensive evaluation of Vision-Language Models (VLMs) for evoked emotion recognition from images. The authors create a benchmark dataset (EVE) by combining multiple existing emotion datasets and evaluate seven popular VLMs including GPT-4o and various LLaVA variants. The study reveals that VLMs perform poorly at zero-shot emotion recognition, with significant sensitivity to prompt variations particularly the order of emotion labels and the presence of target labels. Human evaluation shows that many "mistakes" are actually subjective interpretations of closely related emotions, highlighting the inherent ambiguity in emotion recognition tasks.

## Method Summary
The study evaluates seven VLMs (GPT-4o, LLaVA-7B/13B, LLaVA-Next-Vicuna-7B/13B/Mistral-7B, Qwen-VL) on a benchmark dataset (EVE) created by combining EmoSet-Hard, FI-Hard, Abstract, ArtPhoto, and Emotion6 datasets. Hard subsets were created by fine-tuning a ViT model and selecting samples with prediction probability <0.8 or incorrect predictions. Models were evaluated in zero-shot mode with JSON-formatted responses, using weighted F1 scores as the primary metric. The evaluation included 8 prompt variations testing label order, persona adoption, and reasoning mechanisms. Human evaluation was conducted on 500 error samples across three error categories.

## Key Results
- VLMs achieve poor zero-shot performance on evoked emotion recognition, with GPT-4o scoring highest at weighted F1 of 0.635-0.503 across datasets
- Open-source models (LLaVA-Next variants) achieve weighted F1 scores around 0.593-0.401, with LLaVA-Next-Mistral-7B performing best among them
- All models are highly sensitive to prompt variations, particularly label order and presence of target labels in prompts
- When given no labels, all models perform significantly worse, with GPT-4o making the least fine-grained predictions
- Human evaluation reveals that many model "errors" are actually subjective interpretations of closely related emotions

## Why This Works (Mechanism)

### Mechanism 1: Prompt-Anchoring for Emotion Classification
VLMs require explicit emotion label sets in prompts to produce fine-grained emotion predictions; without anchors, outputs become generic and performance degrades substantially. Providing target labels constrains the output vocabulary, reducing ambiguity in the semantic space. When given no labels, models must map visual features to emotion concepts purely from pre-training, leading to less specific predictions.

### Mechanism 2: Label Order Bias in Multimodal Classification
The sequence in which emotion labels are presented systematically shifts model predictions and sentiment bias. VLMs exhibit positional attention bias—labels appearing earlier receive disproportionate probability mass. Grouping positive emotions first amplifies positive sentiment bias; negative-first ordering degrades performance for most open-source models.

### Mechanism 3: Hierarchical Error Decomposition Reflects Task Ambiguity
Model "errors" at fine-grained levels often reflect ambiguous or noisy ground truth rather than model incapacity. Error Category III (correct sentiment, correct arousal, wrong fine-grained class) frequently produces human agreement with model predictions over dataset ground truth, indicating dataset labels lack exclusivity.

## Foundational Learning

- **Evoked vs. Expressed Emotion**: The paper evaluates *evoked* emotion (what a viewer feels) from images, distinct from *expressed* emotion (what a subject displays). Quick check: Given an image of a crying child, is the task to detect "sadness" (expressed) or "empathy/sadness" in the viewer (evoked)?

- **Sentiment-Arousal-Dominance (VAD) Model**: Error categorization (EC I-III) depends on mapping emotions to sentiment and arousal dimensions. Quick check: Are "fear" and "anger" in the same arousal category? (Yes—both high arousal.)

- **Zero-Shot vs. Fine-Tuned Evaluation**: The study isolates *inherent* VLM capabilities by avoiding fine-tuning. Quick check: Why does the paper downsample EmoSet/FI to hard subsets rather than evaluate full datasets? (To focus evaluation on challenging samples where zero-shot limitations are diagnostic.)

## Architecture Onboarding

- **Component map**: EVE Benchmark -> Zero-shot inference -> JSON response parser -> String-match/SBERT similarity -> Weighted F1 metrics -> Robustness perturbations -> Human error analysis

- **Critical path**: Benchmark construction (ViT filtering → candidate selection → stratified subsampling) → Zero-shot inference across 7 VLMs with standardized prompts → Response parsing and metric computation (F1, sentiment bias) → Robustness perturbation experiments → Human evaluation of ~500 error samples

- **Design tradeoffs**: Downsampling vs. full evaluation limits computational cost but may miss easy-sample patterns; single-run evaluation prevents variance estimation but is justified by API costs; expert-only human annotation ensures quality but may introduce domain bias

- **Failure signatures**: Abstract dataset collapse (all models achieve lowest F1 on Abstract, GPT-4o: 0.196, best open-source: 0.27); persona-induced degradation (up to 50% F1 drop for Qwen-VL); multi-step reasoning failures (LLaVA/LLaVA-Next struggle with contextual reasoning format)

- **First 3 experiments**: 1) Replicate simple classification on EmoSet-Hard subset (500 samples) with GPT-4o and LLaVA-Next-Vicuna-7B to validate reported F1 ranges; 2) Ablate label presence: run open-vocabulary prediction, compute fine-grained prediction rate using SBERT threshold, and compare against Table 2 benchmarks; 3) Replicate persona experiment on 100 samples: measure sentiment bias shift for positive/negative personas against neutral baseline

## Open Questions the Paper Calls Out

- **Open Question 1**: Does fine-tuning VLMs on difficult data subsets (e.g., EmoSet-Hard) yield better generalization for evoked emotion recognition than training on full datasets? The study focused exclusively on zero-shot evaluations to establish a baseline, leaving the potential performance gains from targeted instruction tuning unexplored.

- **Open Question 2**: How can VLMs be designed to be robust against the order of emotion labels presented in the prompt? The authors demonstrate that models perform worse when negative emotions are listed first and conclude that "designing models that are robust to such variations is thus an important area for further inquiry."

- **Open Question 3**: Does incorporating emotion distributions or explanatory rationales into training data reduce the noise and subjectivity inherent in single-label emotion datasets? The human evaluation revealed that many model "errors" were actually plausible subjective interpretations, indicating that single-label ground truths are often inadequate.

## Limitations

- The study focuses exclusively on zero-shot performance without exploring fine-tuning or few-shot adaptation, which may significantly underestimate VLM potential
- The downsampling approach for hard subsets, while computationally practical, may not fully represent the difficulty spectrum of original datasets
- The single-run evaluation per model-dataset combination prevents statistical significance assessment
- Expert-only human annotation may introduce domain-specific bias not generalizable to broader populations

## Confidence

- **High confidence**: Prompt-anchoring mechanism (robust across multiple experiments showing consistent F1 degradation without labels), hierarchical error decomposition reflecting task ambiguity (supported by strong human agreement data for EC III)
- **Medium confidence**: Label order bias mechanism (statistically observed but corpus evidence is limited; GPT-4o's immunity suggests implementation-specific rather than fundamental VLM behavior)
- **Medium confidence**: Overall performance rankings (consistent across datasets, but single-run limitation prevents variance assessment)

## Next Checks

1. **Statistical significance validation**: Replicate the core experiments with 5-fold cross-validation or 10 random seeds per model-dataset pair to establish confidence intervals for F1 scores and test whether observed differences between models are statistically significant.

2. **Fine-tuning impact assessment**: Fine-tune one representative VLM (e.g., LLaVA-Next-Mistral-7B) on a subset of EVE training data and re-evaluate zero-shot vs. fine-tuned performance to quantify the gap and determine if poor zero-shot results reflect fundamental limitations or insufficient prompting.

3. **Alternative label order permutations**: Systematically test all 24 permutations of 4 emotion labels rather than binary positive-first/negative-first ordering to determine whether the observed bias follows a general positional attention pattern or specific to the tested orderings.