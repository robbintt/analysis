---
ver: rpa2
title: Teaching Pretrained Language Models to Think Deeper with Retrofitted Recurrence
arxiv_id: '2511.07384'
source_url: https://arxiv.org/abs/2511.07384
tags:
- recurrence
- train
- training
- accuracy
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper shows that pretrained non-recurrent language models
  can be efficiently converted into depth-recurrent models that outperform their static
  counterparts on reasoning tasks while using fewer unique parameters. The method
  involves surgically removing layers from a pretrained model, keeping early layers
  as a prelude, later layers as a recurrent block, and a coda, and then post-training
  with a curriculum that gradually increases recurrence depth.
---

# Teaching Pretrained Language Models to Think Deeper with Retrofitted Recurrence

## Quick Facts
- **arXiv ID:** 2511.07384
- **Source URL:** https://arxiv.org/abs/2511.07384
- **Reference count:** 40
- **Primary result:** Retrofitted recurrence converts pretrained non-recurrent models into depth-recurrent models that outperform static counterparts on reasoning tasks while using fewer unique parameters

## Executive Summary
This paper presents a method to convert pretrained language models into depth-recurrent models that can leverage increased test-time compute for improved reasoning performance. The approach involves surgically removing layers from a pretrained model, keeping early layers as a prelude, later layers as a recurrent block, and a coda, then post-training with a curriculum that gradually increases recurrence depth. The method achieves higher accuracy on mathematical reasoning benchmarks (GSM8K, MATH) while maintaining strong general language modeling performance, with pretrained initialization yielding significant efficiency gains over random initialization.

## Method Summary
The method converts pretrained non-recurrent LLMs into depth-recurrent models through model surgery that creates a three-part architecture: a prelude (early layers), a recurrent block (later layers with linear adapter for state injection), and a coda (final layers). Training uses a two-phase approach with a curriculum that increases recurrence depth according to a 1-sqrt schedule over 75% of training, using Muon optimizer and truncated backpropagation. The approach is evaluated on mathematical reasoning tasks and general language modeling benchmarks, demonstrating test-time compute scaling benefits where performance improves with increased recurrence depth at inference.

## Key Results
- Retrofitted models outperform static counterparts on GSM8K and MATH benchmarks
- Pretrained initialization provides substantial efficiency gains over random initialization
- Models maintain general language modeling performance while improving reasoning
- Test-time compute scaling enables performance improvements with increased recurrence depth

## Why This Works (Mechanism)
The method works by surgically removing layers from a pretrained model to create a three-part architecture (prelude, recurrent block, coda) where the recurrent block can be looped multiple times at inference. The linear adapter allows injection of both the prelude output and previous recurrence state, enabling depth recurrence. Training with a curriculum that gradually increases recurrence depth prevents catastrophic forgetting and allows the model to learn to use recurrence effectively. The approach leverages the fact that transformer layers with residual connections operate in a shared representation space, making the output of one pass a valid input for the next.

## Foundational Learning

- **Residual Connections & Representation Sharing:**
  - Why needed here: The entire method relies on the ability to "loop" a block of layers by feeding its output back as input. This is only possible because transformer layers with residual connections operate in a shared representation space (the residual stream), allowing the output of one pass to be a valid input for a subsequent, identical layer.
  - Quick check question: Can you explain how a residual connection allows a layer's output to be added back into the main data stream, preserving the dimensionality and making it a suitable input for a subsequent, identical layer?

- **Test-Time Compute Scaling:**
  - Why needed here: This is the core motivation. Understanding that model performance can be improved by investing more computation *during inference* (e.g., by generating many tokens or, in this case, by recurring layers) is fundamental to grasping the paper's contribution and its value proposition.
  - Quick check question: How does increasing computation at test-time differ from increasing model size (parameters) as a way to improve performance?

- **Curriculum Learning:**
  - Why needed here: A central technique in the paper is the curriculum for recurrence depth. The idea of starting training on simpler (shallower recurrence) tasks and gradually increasing complexity is a direct application of curriculum learning principles.
  - Quick check question: What is the primary benefit of presenting a model with a curriculum of increasingly difficult examples during training, as opposed to random sampling?

## Architecture Onboarding

- **Component map:** Prelude -> Recurrent Block (with Linear Adapter) -> Coda
- **Critical path:** The critical path for implementation is the **model surgery and weight initialization**. Correctly selecting which layers from the pretrained model to use for the prelude, recurrent block, and coda, and loading their weights is the foundational step. Errors here will cascade. The second critical path is correctly implementing the **recurrence loop**, including the linear adapter and the concatenation of the prelude output with the recurrent block's state.
- **Design tradeoffs:**
  - **Layer Selection vs. FLOPs:** The paper finds selecting early layers for prelude and later layers for the recurrent block works best, but this involves dropping layers. This is a tradeoff between parameter count and computational efficiency.
  - **Recurrence Count:** A higher recurrence count at inference buys more performance but at a linear cost in FLOPs and latency. The training curriculum seeks to make this efficient.
  - **Optimizer:** The paper suggests the Muon optimizer over AdamW for stability and performance in this recurrent setup.
- **Failure signatures:**
  - **Training Instability/Loss Spikes:** May occur, especially with AdamW. Switching to Muon and using a recurrence curriculum are mitigations.
  - **Performance Degradation:** If training on math data causes the model to lose general language modeling abilities (a form of catastrophic forgetting). This is addressed with a "healing" phase on general data before task-specific training.
  - **No Benefit from Recurrence:** If the recurrent model fails to outperform the static baseline, especially at higher recurrence counts, the core premise fails. The paper notes this was an issue with prior work.
- **First 3 experiments:**
  1. **Ablation on Layer Selection:** Train small recurrent models by selecting different subsets of layers from a parent model (e.g., from TinyLlama) to form the (prelude, recurrent, coda) structure and compare their training loss curves. This validates the "model surgery" heuristic.
  2. **Curriculum vs. Fixed Recurrence:** Train two identical recurrent models on the same data for a fixed number of steps. One uses a curriculum that increases recurrence depth; the other uses a high, fixed recurrence depth from the start. Compare their final performance and total FLOPs. This validates the training efficiency claim.
  3. **Inference Recurrence Scaling:** After training a recurrent model, evaluate its performance on a benchmark (e.g., GSM8K) while varying the number of recurrences used at inference (e.g., 1, 2, 4, 8, 16, 32). Plot performance vs. FLOPs to verify the test-time compute scaling benefit.

## Open Questions the Paper Calls Out

- **Open Question 1:** Does retrofitting recurrence maintain its efficiency advantages when applied to models significantly larger than 1 billion parameters?
  - Basis in paper: [explicit] "While our experiments are at the 1B parameter and 50B token scales, more experimentation is required to verify that our method generalizes to much larger model and data scales."
  - Why unresolved: The computational constraints of this study limited the experiments to 1B parameter models (TinyLlama, OLMo, Llama-3.2-1B).
  - What evidence would resolve it: Replicating the retrofitting process on larger foundation models (e.g., 7B or 70B parameters) and demonstrating that the training FLOP efficiency gains persist.

- **Open Question 2:** Can retrofitted models be trained to effectively utilize more recurrence steps at test time than they were trained on?
  - Basis in paper: [explicit] "One unsolved problem is how to most effectively build depth-recurrent models that can recur deeper at test time to solve harder problems than were seen during training."
  - Why unresolved: While models can use test-time compute, the paper notes that solving problems requiring depth extrapolation beyond the training distribution is currently unsolved.
  - What evidence would resolve it: Demonstrating that a model trained with a mean recurrence of $k$ can successfully leverage $k+n$ recurrences to solve tasks of difficulty level $d+n$ without performance degradation.

- **Open Question 3:** Can these models learn to dynamically adjust the number of recurrences based on input difficulty?
  - Basis in paper: [explicit] "A related goal is how to imbue recurrent models with native adaptivity that automatically assigns the right amount of compute (recurrence) to a given problem based on how difficult it is."
  - Why unresolved: The current architecture requires the recurrence depth to be set externally; the model lacks an internal mechanism to "stop" when a solution is found.
  - What evidence would resolve it: Integrating an exit criterion or adaptive mechanism into the recurrent block that correlates the number of internal loops with the complexity of the input prompt.

- **Open Question 4:** Does retrofitting recurrence improve performance in reasoning-intensive domains outside of mathematics?
  - Basis in paper: [explicit] "...future work should extend this to other reasoning-intensive domains."
  - Why unresolved: The experiments and training data were strictly focused on mathematical reasoning (GSM8K, MATH) and general language modeling.
  - What evidence would resolve it: Applying the retrofitting method to datasets involving logical deduction, code generation, or symbolic reasoning and observing similar performance boosts.

## Limitations

- The method requires significant computational resources for training (50B+ tokens) and evaluation across multiple recurrence depths
- The specific layer selection heuristic (early/late layer configuration) may be task-dependent and not generalize across all model families
- The approach depends on transformer architectures with residual connections and may not work with other architectural variations
- The paper doesn't provide systematic analysis of why the particular configuration works best or explore alternative architectures in depth

## Confidence

**High Confidence:** The core architectural insight that pretrained transformers can be converted to recurrent models is technically sound and reproducible. The observation that pretrained initialization outperforms random initialization is well-supported and aligns with transfer learning principles.

**Medium Confidence:** The training methodology (two-phase approach, Muon optimizer preference, curriculum design) is reasonably well-documented and likely to yield similar results when implemented faithfully. The test-time compute scaling benefits are theoretically sound and empirically demonstrated.

**Low Confidence:** The specific layer selection heuristic (early layers for prelude, late layers for recurrent block, middle layers dropped) is presented as effective but may be dataset/task-dependent. The paper doesn't provide systematic analysis of why this particular configuration works best, nor does it explore alternative architectures in depth.

## Next Checks

1. **Architectural Generalization Test:** Implement the recurrent conversion method on a different transformer architecture (e.g., BERT or Mistral) with different layer configurations to verify whether the early/late layer selection heuristic generalizes or is specific to Llama-style models.

2. **Hyperparameter Sensitivity Analysis:** Systematically vary the three unspecified hyperparameters (σ², learning rate schedule, Poisson-Lognormal parameters) across a reasonable range to determine their impact on final performance and identify which have the largest effect on outcomes.

3. **Long-form Task Evaluation:** Evaluate the recurrent models on tasks requiring extended reasoning chains (e.g., long-form question answering or multi-step planning) beyond GSM8K/MATH to determine if the depth-recurrent advantage extends to real-world applications or remains confined to synthetic reasoning benchmarks.