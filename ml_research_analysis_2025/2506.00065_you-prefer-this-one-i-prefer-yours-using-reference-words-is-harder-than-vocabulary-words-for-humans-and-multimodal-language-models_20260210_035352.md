---
ver: rpa2
title: 'You Prefer This One, I Prefer Yours: Using Reference Words is Harder Than
  Vocabulary Words for Humans and Multimodal Language Models'
arxiv_id: '2506.00065'
source_url: https://arxiv.org/abs/2506.00065
tags:
- task
- demonstrative
- possessive
- humans
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates how humans and multimodal language models
  (MLMs) use reference words (possessives and demonstratives) compared to vocabulary
  words. Using a controlled multiple-choice task with three word classes of increasing
  cognitive demand (vocabulary < possessives < demonstratives), the research tests
  seven state-of-the-art MLMs against human participants.
---

# You Prefer This One, I Prefer Yours: Using Reference Words is Harder Than Vocabulary Words for Humans and Multimodal Language Models

## Quick Facts
- **arXiv ID**: 2506.00065
- **Source URL**: https://arxiv.org/abs/2506.00065
- **Reference count**: 23
- **Primary result**: Multimodal language models show human-level performance on vocabulary tasks but struggle with reference words, particularly demonstratives requiring spatial reasoning and perspective-taking.

## Executive Summary
This study compares human and multimodal language model (MMLM) performance on three word classes of increasing cognitive demand: vocabulary words, possessives, and demonstratives. Using a controlled visual task where agents express preferences about objects, the research tests seven state-of-the-art MLMs against human participants. Results reveal a clear difficulty hierarchy: vocabulary words are easiest, possessives require perspective-taking, and demonstratives demand both perspective-taking and continuous spatial reasoning. While MLMs approach human-level accuracy on vocabulary tasks, they show substantial deficits on possessives and even greater difficulties with demonstratives. Error analysis reveals distinct failure patterns: humans err primarily on perspective-taking, while models struggle with spatial reasoning and demonstrative use. Prompt engineering improves possessive performance but has limited effect on demonstratives, suggesting fundamental architectural limitations in handling deictic reference.

## Method Summary
The study employs a controlled multiple-choice task with synthetic visual scenes showing two agents (boy/girl) at a table with objects. Participants and models must select appropriate reference words (vocabulary, possessives, or demonstratives) based on speaker identity, perspective ("I prefer"/"You prefer"), and spatial arrangement. Human participants (N=120 per task) were tested on Prolific, with replication (N=100) and comprehension validation (N=100). Seven MLMs (GPT-4o, Llama-4-Scout, Gemma-3, InternVL 2.5, Ovis 2) were evaluated zero-shot with temperature=0, using GPT-4o as an "LLM-as-a-Judge" for scoring. Intervention methods included in-context learning (4 exemplars) and instruction-guided reasoning prompts. The 768 trials per task were derived from 16 base objects and 6 scene layouts.

## Key Results
- MLMs achieve near-human performance on vocabulary word selection (95-100% accuracy) but show substantial deficits on possessives and demonstratives
- Demonstratives pose the greatest challenge, with MLMs performing at chance levels on some models
- Human errors primarily involve incorrect perspective-taking, while model errors stem from spatial reasoning difficulties and demonstrative use
- Prompt engineering significantly improves model performance on possessive tasks but has limited effect on demonstratives
- Error analysis reveals that personal pronoun selection errors differ between humans and models, suggesting distinct underlying mechanisms

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Vocabulary words are easier than reference words because they rely on stable object-label associations rather than contextual grounding.
- Mechanism: Vocabulary words (e.g., "spoon") have consistent referents across contexts, learned through associative mappings. Reference words (e.g., "mine," "this one") require pragmatic grounding—their meaning shifts based on who is speaking, their spatial relationship to objects, and the social context of the utterance.
- Core assumption: The cognitive load of context-dependent meaning resolution exceeds that of stable lexical retrieval.
- Evidence anchors:
  - [abstract] "Vocabulary words have relatively stable meanings that are learned through associative label-object mappings... Reference words, on the other hand, are part of grammar, and their meaning is grounded in the context of use."
  - [section 1] The introduction explicitly contrasts vocabulary words (stable meanings) with reference words (perspectival, context-dependent).
  - [corpus] Related work on code-switching evaluation (Minimal Pair-Based Evaluation) touches on contextual language use but doesn't address deixis directly; weak relevance.

### Mechanism 2
- Claim: Possessive pronouns require perspective-taking (monitoring whose preference is expressed), which creates systematic performance differences between "I prefer" and "You prefer" conditions.
- Mechanism: Correct possessive use requires identifying the speaker, tracking whose preference is being expressed (via gaze cues), and mapping ownership relationships. This multi-step cognitive operation introduces error opportunities not present in vocabulary tasks.
- Core assumption: Perspective-taking is a separable cognitive operation from lexical retrieval and can be selectively impaired.
- Evidence anchors:
  - [section 4.4] "A binomial Generalized Linear Model (GLM) reveals a significant main effect of Perspective for both humans and models across both Possessive and Demonstrative tasks, with higher accuracy observed in 'I prefer' trials."
  - [section 4.3] "In the Possessive task, humans make relatively few errors, whereas even advanced models occasionally make incorrect personal pronoun choices, suggesting limitations in perspective-taking abilities."
  - [corpus] No directly relevant corpus evidence on perspective-taking in reference word production; this is a gap in prior work.

### Mechanism 3
- Claim: Demonstrative pronouns are hardest because they require both perspective-taking AND spatial reasoning along a continuous proximity gradient.
- Mechanism: Unlike possessives (binary: mine/yours), demonstratives require judging relative distance from the speaker on a continuum. Comprehension data show "this" has a sharper proximity gradient while "that" is vaguer, indicating continuous spatial processing demands.
- Core assumption: Spatial reasoning in language requires embodied or simulated perceptual representations not easily acquired from text-only training.
- Evidence anchors:
  - [section 4.1] "Possessive use relied on a binary object categorization... whereas demonstrative use required distinguishing objects along a continuous spatial dimension."
  - [section 4.1, Figure 2B] Comprehension task shows graded likelihood ratings for demonstratives based on object distance, with "this" showing sharper gradients than "that."
  - [corpus] VR Mover paper (Can You Move These Over There?) addresses spatial language in object manipulation but focuses on instruction following, not demonstrative production; tangentially relevant.

## Foundational Learning

- **Concept: Deictic reference and deixis**
  - Why needed here: The entire paper concerns deictic terms—words whose meaning depends on the context of utterance. Without understanding deixis, the distinction between vocabulary and reference words is opaque.
  - Quick check question: If a speaker says "this cup," what information must the listener access to identify the referent?

- **Concept: Theory of Mind in communication**
  - Why needed here: The paper frames reference word use as requiring social cognition—specifically, representing others' mental states (preferences, perspectives). This connects to broader debates about whether MLMs have genuine ToM or surface-level approximations.
  - Quick check question: How does knowing what another person can see affect how you describe an object's location?

- **Concept: Multimodal grounding in language models**
  - Why needed here: The study evaluates MLMs that process both images and text. Understanding how visual and linguistic representations are aligned in these architectures is essential for interpreting their performance patterns.
  - Quick check question: What types of information might an MLM extract from an image that would help it resolve "mine" versus "yours"?

## Architecture Onboarding

- **Component map:** Visual scenes with two agents, objects, and gaze cues → Three task types (Vocabulary, Possessive, Demonstrative) with 4-option multiple-choice responses → 2×2 factorial crossing Speaker identity × Perspective → Zero-shot prompting with temperature=0 → LLM-as-a-Judge (GPT-4o) for response scoring → Intervention testing with in-context learning and reasoning-through-instruction prompting

- **Critical path:** 1. Stimulus validation: Ensure models can recognize and name objects (sanity check: 94-100% accuracy). 2. Zero-shot evaluation: Test all models on 768 trials per task without examples or hints. 3. Error categorization: Classify errors by type (personal pronoun, possessive/demonstrative, combined). 4. Intervention testing: Apply reasoning-through-instruction and in-context learning to top-performing models. 5. Human-model comparison: Compute accuracy differences and response distribution correlations.

- **Design tradeoffs:** Multiple-choice format enables direct human-model comparison but simplifies naturalistic production. Removing speech bubbles and gaze lines from model inputs (replaced with text prompts) standardizes input but may reduce ecological validity. English-only evaluation limits generalizability; demonstrative systems vary cross-linguistically. Using GPT-4o as judge introduces potential model-specific biases in evaluation.

- **Failure signatures:** Near-chance performance on Demonstratives with adequate Possessive accuracy → spatial reasoning deficit. High personal pronoun errors with correct demonstrative selection → perspective-taking impairment. No improvement from reasoning-through-instruction → lack of compositional understanding. In-context learning fails but reasoning prompts help → inability to generalize from examples without explicit rules.

- **First 3 experiments:**
  1. **Ablate spatial cues**: Manipulate distance granularity (binary near/far vs. 4-point gradient) to test whether continuous spatial reasoning is the bottleneck. Predict: Models show larger performance drops on gradient conditions than humans.
  2. **Cross-perspective training**: Fine-tune models on examples where they must adopt different perspectives systematically. Predict: Performance improves on "You prefer" trials but generalization to novel spatial arrangements remains limited.
  3. **Embodied baseline comparison**: Compare standard MLMs against models trained on embodied interaction data (e.g., robotics, VR). Predict: Embodied models show smaller Demonstrative deficits, supporting the hypothesis that spatial reasoning requires perceptual grounding.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does training or fine-tuning multimodal language models in embodied, interactive environments improve their competence in using demonstrative pronouns?
- Basis in paper: [explicit] The authors explicitly hypothesize in the Discussion that "learning to use demonstratives might require embodied, interactive communication in physical environments," contrasting with how current MLMs learn.
- Why unresolved: Current models failed to reach human-level performance on demonstratives, and improvements in spatial reasoning could not be induced through static prompt engineering or in-context learning.
- What evidence would resolve it: Evaluating demonstrative pronoun accuracy in models trained within simulated physical worlds or interactive agent frameworks compared to statically trained baselines.

### Open Question 2
- Question: Is the observed difficulty hierarchy (vocabulary < possessives < demonstratives) consistent across diverse languages and cultures?
- Basis in paper: [explicit] The Limitations section states that the study evaluated only English materials and that "reference word usage varies across languages and cultures," explicitly calling for future studies to examine these phenomena in diverse contexts.
- Why unresolved: The cognitive demands of spatial reasoning and perspective-taking may manifest differently in languages with different deictic systems (e.g., languages with distance-neutral demonstratives).
- What evidence would resolve it: Replicating the experimental protocol with human participants and multilingual models across languages with varying typological features for reference words.

### Open Question 3
- Question: Can advanced reasoning frameworks or persona-based prompting overcome the specific spatial reasoning deficits preventing correct demonstrative use?
- Basis in paper: [inferred] The authors note in the Limitations that "advanced reasoning frameworks" and "persona-based prompting" remain unexplored, while the Results show that simple instruction-based reasoning failed to improve spatial reasoning for demonstratives.
- Why unresolved: It is unclear if the failure to improve demonstrative use is a fundamental architectural limitation or a failure of the specific prompting strategies (zero-shot and simple instruction) employed.
- What evidence would resolve it: Testing the specific "Demonstrative" task setup using chain-of-thought variants (e.g., Tree of Thoughts) or persona prompts to see if they successfully elicit the necessary spatial distinctions.

## Limitations
- GPT-4o used as both tested model and evaluation judge introduces potential circularity in results
- Multiple-choice format simplifies naturalistic reference word production and may not capture full pragmatic complexity
- English-only evaluation limits generalizability to languages with different deictic systems
- Synthetic visual stimuli may not capture full variability of real-world reference situations

## Confidence

- **High confidence**: Vocabulary word superiority over reference words (both humans and models); zero-shot performance hierarchy (Vocabulary > Possessive > Demonstrative); human performance patterns showing perspective effects.
- **Medium confidence**: Model error patterns (spatial reasoning vs. perspective-taking); effectiveness of prompt engineering interventions; generalizability across different MLM architectures.
- **Low confidence**: Cross-linguistic applicability; ecological validity of multiple-choice format; GPT-4o judge reliability across different error types.

## Next Checks

1. **Cross-linguistic validation**: Test the same paradigm with languages having different demonstrative systems (e.g., Spanish "este/ese/aquel" or Japanese "kore/sore/are") to assess whether the difficulty hierarchy holds cross-linguistically.

2. **Alternative evaluation protocol**: Implement human scoring of a subset of model responses (N=100) to assess whether GPT-4o judge results are consistent with human expert evaluation, particularly for ambiguous cases.

3. **Continuous spatial reasoning test**: Create a separate experiment with fine-grained spatial continua (10 distance gradations instead of 2) to determine whether the spatial reasoning deficit is continuous or categorical in nature.