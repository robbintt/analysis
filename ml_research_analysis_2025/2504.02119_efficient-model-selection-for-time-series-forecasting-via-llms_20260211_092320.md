---
ver: rpa2
title: Efficient Model Selection for Time Series Forecasting via LLMs
arxiv_id: '2504.02119'
source_url: https://arxiv.org/abs/2504.02119
tags:
- forecasting
- time
- selection
- data
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel approach to model selection in time
  series forecasting by leveraging Large Language Models (LLMs) instead of traditional
  meta-learning methods. The proposed method eliminates the need for an explicit performance
  matrix, which is computationally expensive to construct, by utilizing the reasoning
  capabilities of LLMs through zero-shot prompting.
---

# Efficient Model Selection for Time Series Forecasting via LLMs

## Quick Facts
- arXiv ID: 2504.02119
- Source URL: https://arxiv.org/abs/2504.02119
- Reference count: 16
- Primary result: LLM-based model selection achieves up to 89× faster inference than exhaustive evaluation while maintaining competitive forecasting accuracy

## Executive Summary
This paper introduces a novel approach to model selection in time series forecasting that leverages Large Language Models (LLMs) instead of traditional meta-learning methods. The proposed method eliminates the computationally expensive performance matrix by utilizing LLM reasoning capabilities through zero-shot prompting. The approach demonstrates superior performance across 320 datasets compared to random selection, popular methods, and meta-learning baselines, achieving both faster inference times and competitive or better forecasting accuracy.

## Method Summary
The method employs LLMs to perform zero-shot model selection by prompting them to reason about which forecasting model would perform best for given time series data. Rather than constructing an explicit performance matrix as in traditional meta-learning approaches, the LLM directly selects the most appropriate model from a set of candidates (ARIMA, LSTM, TFT, PatchTST) based on the characteristics of the input time series. The framework uses carefully designed prompts that ask the LLM to evaluate dataset features and recommend the optimal forecasting model, effectively replacing the computationally intensive meta-learning pipeline with language model inference.

## Key Results
- Achieved up to 89× faster inference time compared to exhaustive model evaluation
- Outperformed both random selection and popular forecasting methods across 320 datasets
- Surpassed meta-learning baselines in hit@k accuracy and forecasting performance
- Zero-shot prompting eliminated need for explicit performance matrix construction

## Why This Works (Mechanism)
The approach works by leveraging the reasoning capabilities of LLMs to perform model selection tasks that traditionally required computationally expensive meta-learning pipelines. LLMs can understand natural language descriptions of time series characteristics and apply reasoning to select appropriate forecasting models without needing to construct and maintain large performance matrices. The zero-shot prompting approach allows the LLM to directly map dataset characteristics to model recommendations, bypassing the need for extensive training data that meta-learning methods require.

## Foundational Learning

1. **Meta-learning for model selection** - Traditional approach requires constructing performance matrices across multiple datasets and models; needed for understanding why LLM approach is computationally advantageous
2. **Zero-shot prompting** - Technique where LLMs make decisions without task-specific fine-tuning; needed to understand how LLMs can perform model selection without training data
3. **Chain-of-Thought prompting** - Method where LLMs are guided through reasoning steps; needed to evaluate why this approach sometimes degrades performance in the context
4. **Time series forecasting models** - Understanding ARIMA, LSTM, TFT, and PatchTST characteristics; needed to evaluate the scope and limitations of model selection
5. **Computational complexity analysis** - Comparing inference times across different selection methods; needed to validate the 89× speedup claim

## Architecture Onboarding

**Component Map**: Time Series Data -> LLM Prompting -> Model Selection -> Forecasting Performance

**Critical Path**: Input time series → LLM reasoning → Model recommendation → Forecasting execution

**Design Tradeoffs**: Speed vs accuracy (LLM inference vs meta-learning computation), model coverage vs LLM reasoning capacity, prompting complexity vs cost efficiency

**Failure Signatures**: Poor LLM reasoning quality on complex time series, prompt engineering limitations, model recommendation mismatches, computational bottlenecks in LLM inference

**3 First Experiments**:
1. Compare LLM-based selection accuracy against random selection baseline
2. Measure inference time differences between LLM selection and exhaustive model evaluation
3. Test zero-shot prompting performance versus Chain-of-Thought prompting variants

## Open Questions the Paper Calls Out
None

## Limitations
- Narrow scope of evaluated forecasting models (ARIMA, LSTM, TFT, PatchTST) may not generalize to other architectures
- Heavy reliance on specific LLM models (GPT-4o, Llama 3.2, Gemini 2.0) raises questions about robustness across different implementations
- Zero-shot prompting effectiveness may degrade with domain complexity or noisy time series data
- Contradictory findings about Chain-of-Thought prompting performance need systematic investigation

## Confidence
- High confidence: Computational efficiency claims (89× faster inference) and basic performance comparisons are well-supported
- Medium confidence: Superiority claims over meta-learning methods are convincing but limited by specific implementations tested
- Low confidence: Generalizability to other time series domains, forecasting models, and LLM architectures requires further validation

## Next Checks
1. Test the LLM-based selection framework with a broader range of forecasting models including ensemble methods and newer architectures
2. Validate performance consistency across different LLM providers and versions, particularly smaller models for practical deployment
3. Conduct experiments on time series datasets from diverse domains (healthcare, finance, IoT) to assess robustness to varying signal characteristics and noise patterns