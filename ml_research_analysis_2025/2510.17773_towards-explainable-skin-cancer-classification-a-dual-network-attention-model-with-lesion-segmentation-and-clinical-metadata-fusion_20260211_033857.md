---
ver: rpa2
title: 'Towards Explainable Skin Cancer Classification: A Dual-Network Attention Model
  with Lesion Segmentation and Clinical Metadata Fusion'
arxiv_id: '2510.17773'
source_url: https://arxiv.org/abs/2510.17773
tags:
- lesion
- segmentation
- skin
- classification
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study introduces a dual-encoder attention framework for skin\
  \ lesion classification, integrating lesion segmentation and clinical metadata to\
  \ enhance accuracy and interpretability. The approach employs a novel Deep-UNet\
  \ with Dual Attention Gates and Atrous Spatial Pyramid Pooling for precise lesion\
  \ segmentation, followed by two DenseNet201 encoders\u2014one on the original image\
  \ and one on the segmented lesion\u2014whose features are fused via multi-head cross-attention."
---

# Towards Explainable Skin Cancer Classification: A Dual-Network Attention Model with Lesion Segmentation and Clinical Metadata Fusion

## Quick Facts
- arXiv ID: 2510.17773
- Source URL: https://arxiv.org/abs/2510.17773
- Reference count: 25
- Primary result: Dual-encoder attention framework achieves 93.47% accuracy on HAM10000 and 88.56% on ISIC 2019, outperforming baselines.

## Executive Summary
This study introduces a dual-encoder attention framework for skin lesion classification that integrates lesion segmentation and clinical metadata to enhance accuracy and interpretability. The approach employs a novel Deep-UNet with Dual Attention Gates and Atrous Spatial Pyramid Pooling for precise lesion segmentation, followed by two DenseNet201 encoders—one on the original image and one on the segmented lesion—whose features are fused via multi-head cross-attention. Clinical metadata is incorporated through a transformer-based encoder, enabling robust predictions. Evaluated on HAM10000 and ISIC 2019 datasets, the method achieves 93.47% and 88.56% test accuracy, respectively, outperforming baseline models. Grad-CAM visualizations confirm that the model focuses precisely on lesion regions, improving explainability and clinical trust.

## Method Summary
The framework consists of two main stages: lesion segmentation and classification with metadata fusion. First, a Deep-UNet with EfficientNet-B3 encoder, ASPP bottleneck, and Depthwise Attention Gates performs segmentation using BCE + Dice loss. The segmented lesions are then used alongside original images in a dual-DenseNet201 architecture. Multi-head cross-attention fuses features from both encoders, with clinical metadata (age, sex, anatomical site) processed through a transformer-based encoder. The fused representation passes through an additive fusion module and classifier head for final predictions. The model is trained with cross-entropy loss and evaluated on HAM10000 and ISIC 2019 datasets.

## Key Results
- ISIC 2018 segmentation task: mDice 91.17%, mIoU 84.35%
- HAM10000 test accuracy: 93.47% (outperforming baselines)
- ISIC 2019 test accuracy: 88.56% (outperforming baselines)
- Grad-CAM visualizations demonstrate precise focus on lesion regions for interpretability

## Why This Works (Mechanism)
The dual-encoder architecture enables the model to learn both global contextual features from the original image and localized lesion-specific features from the segmented mask. Multi-head cross-attention allows cross-domain feature fusion where the original image provides query and value vectors while the segmented lesion provides key vectors, enabling the model to attend to relevant regions in the original image based on lesion structure. The clinical metadata encoder transforms tabular data into meaningful embeddings that are fused additively with visual features, providing complementary patient-specific information. This multi-modal approach addresses the limitations of single-view models by capturing both visual and clinical context.

## Foundational Learning
- **Atrous Spatial Pyramid Pooling (ASPP)**: Captures multi-scale contextual information through dilated convolutions with different rates. Why needed: Skin lesions exhibit features at multiple scales (pigment networks, globules) requiring multi-scale feature extraction. Quick check: Verify receptive field coverage matches lesion size variation in dataset.
- **Dual Attention Gates (DAG)**: Modulates skip connections between encoder and decoder layers to preserve spatial resolution while maintaining global context. Why needed: Prevents loss of fine-grained segmentation details while incorporating high-level semantic information. Quick check: Compare segmentation quality with and without DAG in validation set.
- **Multi-head Cross-Attention**: Enables cross-modal feature fusion between original and segmented images. Why needed: Original image provides contextual cues while segmented image provides precise lesion boundaries, requiring attention-based fusion. Quick check: Measure attention weight distributions across different lesion types.
- **Clinical Metadata Encoding**: Transforms tabular patient data into continuous embeddings for model integration. Why needed: Demographic and anatomical information provides critical context for accurate classification. Quick check: Perform ablation by removing metadata features and measuring performance drop.
- **Additive Fusion**: Combines visual and metadata embeddings through element-wise addition. Why needed: Preserves distinct feature representations while enabling joint processing. Quick check: Compare with alternative fusion strategies (concatenation, gating).

## Architecture Onboarding

Component map: Raw Images → Deep-UNet → Segmentation Masks → Dual DenseNet201 → Cross-Attention → Metadata Encoder → Additive Fusion → Classifier

Critical path: Image → Segmentation → Dual Encoding → Cross-Attention → Metadata Fusion → Classification

Design tradeoffs: The dual-encoder approach increases computational cost but enables specialized feature learning. Cross-attention provides interpretable attention maps but adds complexity compared to simple concatenation. The segmentation stage introduces an additional training phase but improves classification focus.

Failure signatures: Poor segmentation leads to noisy inputs for classification. Cross-attention failures manifest as attention maps that don't align with lesion regions. Metadata encoding issues appear as poor performance on demographic-specific subsets.

Three first experiments:
1. Train segmentation model alone on ISIC 2018 Task 1 and evaluate Dice/IoU metrics
2. Implement single DenseNet encoder baseline and compare with dual-encoder performance
3. Test cross-attention with different head configurations (1, 4, 8 heads) to find optimal setting

## Open Questions the Paper Calls Out
- How does the proposed framework perform on prospective, external clinical datasets that differ in imaging protocols or patient demographics from the ISIC and HAM10000 benchmarks? The authors state they plan to conduct prospective validation on external, unseen datasets to enhance generalizability, but current validation is limited to HAM10000 and ISIC 2019.
- Does the integration of granular patient history data, such as genetic markers or UV exposure, significantly improve classification performance over the current demographic metadata? The conclusion suggests incorporating wider clinical data, but the current implementation only uses basic metadata (age, sex, lesion site).
- Do the Grad-CAM heatmaps correspond to specific dermoscopic structures (e.g., pigment networks, globules) or merely highlight the general lesion boundary? While the model focuses on lesion areas, Grad-CAM lacks semantic specificity to validate recognition of distinct clinical criteria.

## Limitations
- Multi-head cross-attention configuration lacks specific details about number of heads and key/query dimensions, which could significantly impact performance
- Handling of categorical metadata (sex and anatomical site) is not fully described, leaving uncertainty about embedding strategies
- Exact class balancing approach and augmentation pipeline sequence remain unclear, potentially affecting reproducibility
- Reported improvements over baselines lack ablation studies to isolate contributions of each component

## Confidence
High confidence in reported segmentation accuracy (mDice 91.17%, mIoU 84.35%) and classification performance on HAM10000 (93.47%) and ISIC 2019 (88.56%) given use of established metrics and well-known datasets.
Medium confidence in dual-encoder architecture's effectiveness due to insufficient ablation studies and unclear cross-attention implementation details.
Low confidence in clinical utility given limited discussion of false positive/negative cases and their implications for real-world deployment.

## Next Checks
1. Implement and test multiple cross-attention configurations (varying number of heads and dimensions) to identify optimal settings and establish sensitivity to these hyperparameters.
2. Conduct ablation studies systematically removing components (segmentation stage, cross-attention, metadata fusion) to quantify each element's contribution to overall performance.
3. Evaluate model performance across different demographic subgroups using metadata to assess potential biases and fairness implications for clinical applications.