---
ver: rpa2
title: Training Frozen Feature Pyramid DINOv2 for Eyelid Measurements with Infinite
  Encoding and Orthogonal Regularization
arxiv_id: '2504.00515'
source_url: https://arxiv.org/abs/2504.00515
tags:
- focal
- dinov2
- learning
- page
- national
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study develops a deep learning system for automated eyelid\
  \ measurements (MRD1, MRD2, and Levator Function) using smartphone images. It compares\
  \ three backbone architectures\u2014SE-ResNet, EfficientNet, and DINOv2\u2014across\
  \ frozen and fine-tuned settings."
---

# Training Frozen Feature Pyramid DINOv2 for Eyelid Measurements with Infinite Encoding and Orthogonal Regularization

## Quick Facts
- arXiv ID: 2504.00515
- Source URL: https://arxiv.org/abs/2504.00515
- Authors: Chun-Hung Chen
- Reference count: 0
- Primary result: DINOv2 frozen backbone with MLP regressor achieves MSE of ~0.61 for MRD1, ~0.46 for MRD2, and ~3.41 for Levator Function on smartphone-based eyelid measurements

## Executive Summary
This study develops a deep learning system for automated eyelid measurements (MRD1, MRD2, and Levator Function) using smartphone images. It compares three backbone architectures—SE-ResNet, EfficientNet, and DINOv2—across frozen and fine-tuned settings. DINOv2, leveraging self-supervised learning, achieves the most stable and accurate performance, particularly in frozen configurations suitable for mobile deployment. Lightweight regressors such as MLP and Deep Ensemble deliver high precision with minimal computational overhead. The study integrates focal loss for handling data imbalance, orthogonal regularization for multi-task stability, and binary encoding for improved regression precision. Results show DINOv2 with these enhancements provides consistent, clinically reliable predictions across all tasks, demonstrating the potential of foundation models for scalable, mobile-ready ophthalmic diagnostics.

## Method Summary
The method uses smartphone-captured periocular images (822 eyes from 411 volunteers) to predict three eyelid measurements: MRD1, MRD2, and Levator Function. Three backbone architectures are evaluated: SE-ResNet, EfficientNet, and DINOv2-ViT. Models are trained in both frozen and fine-tuned configurations with optional Feature Pyramid Networks. Regressor heads include MLP, Deep Ensemble, and Transformer variants. Training uses MSE loss with focal loss and orthogonal regularization for stability. Binary encoding reformulates regression as multi-bit classification for improved precision. Models are trained for 20 epochs using Adam (lr=1e-3, batch=4) on RTX 6000 Ada.

## Key Results
- DINOv2 frozen backbone with MLP regressor achieves the best overall performance: MRD1 MSE 0.6087, MRD2 MSE 0.4583, LF MSE 3.4134
- Deep Ensemble regressor improves stability over single MLP, reducing variance by ~10-20%
- Binary encoding stabilizes training, particularly for EfficientNet, preventing loss spikes in MRD2 and LF
- Orthogonal regularization with focal loss (γ=2-4) significantly improves multi-task stability when tasks show high variance

## Why This Works (Mechanism)

### Mechanism 1
DINOv2's frozen backbone produces transferable features for clinical regression without fine-tuning. Self-supervised pretraining on large-scale unlabeled data teaches the model to capture general visual representations that generalize to medical imaging tasks, eliminating the need for domain-specific weight updates. Core assumption: The pretraining distribution contains sufficient visual diversity to transfer to periocular anatomy. Evidence: DINOv2 exhibits remarkably low performance variance between MRD1, MRD2, and LF, indicating that its self-supervised representations generalize effectively across different measurement types. Break condition: Performance degrades significantly on out-of-distribution clinical images (different ethnicities, lighting, devices) not represented in pretraining data.

### Mechanism 2
Binary encoding stabilizes training and improves regression precision by reformulating continuous prediction as multi-bit classification. Discretizing continuous targets into N-bit representations allows each bit to be learned independently, smoothing the optimization landscape and enabling application of classification-specific techniques (focal loss, balanced sampling) to regression. Core assumption: Bit-level predictions can be aggregated without compounding errors across bits. Evidence: Binary encoding notably stabilized training, especially in EfficientNet, preventing loss spikes in MRD2 and LF. Break condition: High bit-depth encodings (>8 bits) may introduce cumulative prediction errors or require exponentially more training data per bit.

### Mechanism 3
Orthogonal regularization (OR) combined with focal loss improves multi-task stability by reducing gradient interference between regression targets. Constraining weight updates to orthogonal subspaces prevents task-specific gradients from overwriting shared representations, while focal loss reweights hard examples. Together they address both optimization geometry and data imbalance. Core assumption: Task gradients would otherwise conflict in shared parameter space. Evidence: When focal loss was combined with orthogonality regularization, model stability improved significantly. Break condition: OR provides no benefit when tasks are highly correlated (MRD1/MRD2) rather than competing, or when gradient magnitudes differ drastically across tasks.

## Foundational Learning

- **Concept: Self-Supervised Learning (SSL) with Knowledge Distillation**
  - Why needed: DINOv2 uses a student-teacher paradigm where the student learns to match the teacher's output on augmented views without labels. Understanding this explains why frozen features work—SSL pretrains representations, not task-specific mappings.
  - Quick check: Can you explain why DINOv2's teacher network is updated via exponential moving average rather than backpropagation?

- **Concept: Feature Pyramid Networks (FPN)**
  - Why needed: The study integrates FPN to capture multi-scale spatial information for eyelid landmark detection. FPN fuses high-level semantic features with low-level spatial detail through top-down pathways and lateral connections.
  - Quick check: Why would a pure ViT benefit more from FPN than a CNN that already has hierarchical feature maps?

- **Concept: Multi-Task Learning with Gradient Conflict**
  - Why needed: MRD1, MRD2, and LF are predicted simultaneously from shared features. Orthogonal regularization addresses a real optimization challenge—conflicting gradient directions across tasks that share parameters.
  - Quick check: What would happen to MRD1 predictions if the model optimized only for LF, which requires integrating up-gaze and down-gaze images?

## Architecture Onboarding

- **Component map:**
  Input Image (smartphone, normalized) -> Backbone (SE-ResNet / EfficientNet / DINOv2 ViT) → [Frozen or Fine-tuned] -> Feature Pyramid Network (optional, multi-scale fusion) -> Embeddings (pooled) -> Regressor Head (MLP / Deep Ensemble / Transformer variants) -> Output: MRD1, MRD2, LF (continuous values or binary-encoded)

- **Critical path:**
  1. Start with DINOv2 ViT-B-Reg frozen + MLP regressor (simplest strong baseline)
  2. Add binary encoding (8-16 bits) if training is unstable
  3. Add orthogonal regularization + focal loss (γ=2-4) only if multi-task variance is high

- **Design tradeoffs:**
| Choice | Pros | Cons |
|--------|------|------|
| Frozen DINOv2 | Fast training, mobile-deployable, no overfitting risk | May underperform on highly specialized anatomy |
| Fine-tuned DINOv2 | Potentially higher accuracy | Requires more data, GPU memory, risk of catastrophic forgetting |
| Binary encoding | Stable gradients, controllable precision | Adds output dimensionality, inference overhead |
| Deep Ensemble regressor | Best stability and uncertainty | 5x inference cost vs. single MLP |

- **Failure signatures:**
  - Loss explosion in LF: Check if EfficientNet-B7 is freezing; switch to DINOv2 or add binary encoding
  - MRD2 negative R²: Model predicting near-mean; check data preprocessing normalization
  - High task variance across MRD1/MRD2/LF: Add orthogonal regularization or use Deep Ensemble head

- **First 3 experiments:**
  1. Baseline validation: Run DINOv2 ViT-B-Reg frozen + MLP on 10% held-out test set; verify MSE ≈ 0.5-0.6 for MRD1 (Table 4 benchmarks)
  2. Regressor ablation: Compare MLP vs. Deep Ensemble on frozen DINOv2; expect Deep Ensemble to reduce variance by ~10-20%
  3. Loss function sweep: Test focal loss (γ=0,2,4,6) with/without OR on EfficientNet-B0; identify which γ value stabilizes training without degrading R²

## Open Questions the Paper Calls Out

### Open Question 1
What is the minimum scale and diversity of domain-specific unlabeled ophthalmic images required for DINOv2 self-supervised pretraining to yield consistent performance gains over general-purpose pretrained features? Basis: Due to GPU memory limitations, domain-specific self-supervised training was not implemented for larger models... we observed no consistent performance gain from continued domain-specific pretraining. This may be attributed to the relatively small and low-diversity corpus of ophthalmic images used for adaptation. Why unresolved: The current corpus was insufficient to improve upon DINOv2's existing features, but the threshold for improvement remains unknown. What evidence would resolve it: Systematic experiments varying dataset size (e.g., 1K, 10K, 100K images) and diversity (multi-center, multi-device) with controlled pretraining runs.

### Open Question 2
Can focal loss be stabilized for eyelid measurement tasks through architectural modifications or hyperparameter schedules, rather than requiring orthogonality regularization as a stabilizer? Basis: Focal loss, while conceptually effective at addressing long-tailed clinical distributions, showed unstable behavior when applied in isolation... training became erratic, with increased variance in loss curves. Why unresolved: The instability mechanism is identified but not fully explained; orthogonality regularization is a workaround, not a fundamental solution. What evidence would resolve it: Ablation studies with focal loss alone using gradient clipping, warmup schedules, or adaptive focusing parameters.

### Open Question 3
What is the optimal bit depth for binary encoding in clinical eyelid measurement regression, balancing precision, computational cost, and calibration reliability? Basis: The paper proposes "infinite encoding" but only experiments with one configuration. Clinical measurements have finite precision requirements, and excessive bits may introduce unnecessary complexity without meaningful accuracy gains. Why unresolved: No systematic exploration of bit depth trade-offs is presented; the "infinite" claim is theoretical. What evidence would resolve it: Experiments comparing 8-bit, 16-bit, 32-bit, and higher encodings across MSE, calibration error, and inference latency.

## Limitations
- Binary encoding mechanism lacks external validation beyond this study, making its generalization to other medical regression tasks uncertain
- Performance on out-of-distribution clinical populations (different ethnicities, devices, or pathological conditions) is unknown
- DINOv2's superiority over domain-specific fine-tuned models on specialized periocular anatomy remains unproven

## Confidence
- **High confidence:** DINOv2 frozen backbone provides stable, accurate performance across all three measurement tasks; Deep Ensemble regressor improves stability over single MLP; orthogonal regularization with focal loss reduces multi-task variance
- **Medium confidence:** Binary encoding improves training stability for EfficientNet; self-supervised pretraining enables effective frozen feature transfer; the three-task multi-task framework is clinically viable
- **Low confidence:** Binary encoding mechanism's theoretical advantages translate to practical gains; frozen DINOv2 outperforms all fine-tuned alternatives on specialized periocular anatomy; orthogonal regularization benefits generalize to other medical multi-task problems

## Next Checks
1. Test DINOv2 frozen performance on an independent dataset with different demographics, lighting conditions, or imaging devices to assess generalization limits
2. Compare binary encoding against standard regression with focal loss on a held-out validation set to isolate its contribution to training stability
3. Evaluate whether task-specific fine-tuning of DINOv2 parameters improves performance beyond frozen features for any individual measurement task