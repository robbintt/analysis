---
ver: rpa2
title: Reasoning by Commented Code for Table Question Answering
arxiv_id: '2602.00543'
source_url: https://arxiv.org/abs/2602.00543
tags:
- reasoning
- table
- arxiv
- question
- code
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a commented, step-by-step code-generation
  framework for TableQA that improves interpretability and numerical accuracy by aligning
  reasoning with executable Pandas operations. Each major dataframe step is paired
  with a concise natural-language comment, beginning with a mandatory PLAN and followed
  by operation-specific steps as needed.
---

# Reasoning by Commented Code for Table Question Answering

## Quick Facts
- arXiv ID: 2602.00543
- Source URL: https://arxiv.org/abs/2602.00543
- Authors: Seho Pyo; Jiheon Seok; Jaejin Lee
- Reference count: 24
- Primary result: 70.9% accuracy on WikiTableQuestions with Qwen2.5-Coder-7B-Instruct, surpassing Repanda baseline (67.6%)

## Executive Summary
This paper introduces a commented, step-by-step code-generation framework for TableQA that improves interpretability and numerical accuracy by aligning reasoning with executable Pandas operations. Each major dataframe step is paired with a concise natural-language comment, beginning with a mandatory # PLAN and followed by operation-specific steps as needed. The model generates a multi-line Python function in a standardized output format, ensuring robust data handling and instruction-following. Evaluated on WikiTableQuestions, the method achieves 70.9% accuracy with Qwen2.5-Coder-7B-Instruct, surpassing the Repanda baseline (67.6%). Integrating the framework with an end-to-end TableQA model via an answer selector further improves accuracy to 84.3%. The approach addresses scalability and numerical reliability issues in existing TableQA methods while maintaining transparency through explicit, executable reasoning traces.

## Method Summary
The method generates a Python function with pandas operations for each reasoning step, accompanied by natural-language comments. The framework enforces a standardized output format with a mandatory # PLAN comment followed by operation-specific steps. This approach ensures each dataframe transformation is explicitly justified through comments, enabling both interpretability and numerical accuracy. The model is evaluated on WikiTableQuestions, and the framework is further integrated with an end-to-end TableQA model using an answer selector that combines code-based reasoning with existing approaches.

## Key Results
- 70.9% accuracy on WikiTableQuestions using Qwen2.5-Coder-7B-Instruct, surpassing Repanda baseline (67.6%)
- End-to-end integration with answer selector achieves 84.3% accuracy
- Code-based reasoning demonstrates improved numerical accuracy and interpretability through explicit pandas operations with natural-language comments

## Why This Works (Mechanism)
The framework works by translating natural language questions into executable pandas code with step-by-step comments, making the reasoning process transparent and verifiable. Each dataframe operation is explicitly documented with a natural-language comment, allowing users to trace the logic from question to answer. The standardized output format with mandatory # PLAN ensures consistent instruction-following, while the pandas operations provide numerical accuracy through direct computation rather than probabilistic reasoning alone.

## Foundational Learning
- **Pandas dataframe operations**: Essential for understanding the executable reasoning steps; quick check: verify familiarity with filtering, grouping, and aggregation functions
- **Code-based reasoning vs. natural language**: Understanding why executable code provides better numerical accuracy than free-form text; quick check: compare accuracy of code vs. NL-only approaches on numerical questions
- **Multi-line function generation**: Critical for structuring complex reasoning chains; quick check: test ability to generate coherent multi-step functions for compound questions
- **Answer selector integration**: How combining code-based and end-to-end approaches improves overall accuracy; quick check: examine error patterns when each approach succeeds or fails

## Architecture Onboarding

**Component map**: Natural language question → Code generator → Pandas operations with comments → Answer selector → Final answer

**Critical path**: Question parsing → Code generation → Execution → Answer selection

**Design tradeoffs**: The framework prioritizes interpretability and numerical accuracy through explicit code over pure natural language reasoning, accepting the computational overhead of code generation and execution. The answer selector balances between code-based and end-to-end approaches to maximize accuracy.

**Failure signatures**: 
- Incorrect code generation leading to wrong dataframe operations
- Answer selector overriding correct code-based answers with incorrect end-to-end predictions (3.23% of cases)
- Poor performance on complex reasoning tasks requiring multiple joins or transformations
- Degradation on noisy or ambiguous natural language queries

**First experiments to run**:
1. Verify code generation accuracy on simple single-step queries
2. Test answer selector behavior when code-based and end-to-end answers differ
3. Measure execution time and numerical accuracy on tables with varying sizes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does code-based reasoning performance scale with table size, and at what table dimensions does the computational overhead outweigh the benefits over end-to-end reasoning?
- Basis in paper: [explicit] The Limitations section states: "Exploring how code-based reasoning interacts with table size and task complexity remains an interesting direction for future work."
- Why unresolved: The paper evaluates only on WikiTQ without systematic variation of table dimensions, making it unclear whether code generation remains practical for very large tables or becomes prohibitively expensive.
- What evidence would resolve it: A controlled experiment varying table rows/columns systematically, measuring both accuracy and latency across different size regimes.

### Open Question 2
- Question: Would incorporating confidence scores or uncertainty estimates into the answer selector reduce the error rate where correct answers are overridden by incorrect predictions?
- Basis in paper: [explicit] Section 4.7 notes: "These errors indicate that answer selection remains a bottleneck and suggest that confidence- or uncertainty-aware selection could further improve robustness."
- Why unresolved: The current selector uses a simple binary choice without accessing model confidence; 3.23% of cases involve one correct answer being incorrectly overridden.
- What evidence would resolve it: Implementing confidence-weighted selection and measuring the reduction in override errors on the WikiTQ test set.

### Open Question 3
- Question: Can the commented code framework be extended to handle free-form text answer generation tasks (e.g., FeTaQA), or is it fundamentally limited to discrete, executable outputs?
- Basis in paper: [inferred] Section 4.1 excludes FeTaQA because "mapping arbitrary text to executable code presents challenges that extend beyond table reasoning," but no solution or workaround is proposed.
- Why unresolved: The framework's reliance on executable pandas operations with list-returning functions may be incompatible with natural language answer generation.
- What evidence would resolve it: A modified framework evaluation on FeTaQA measuring whether hybrid code+generation approaches can bridge the gap.

## Limitations
- Evaluation confined to WikiTableQuestions with relatively small tables, raising scalability concerns for larger industrial datasets
- Performance on noisy or ambiguous natural language queries is not characterized
- Answer selector's own reasoning transparency and robustness are not independently validated
- Assumes clean, structured table inputs and may not generalize to semi-structured or multi-modal data without adaptation

## Confidence
**High confidence** in interpretability improvements and baseline accuracy gains (70.9% vs 67.6%) on WikiTableQuestions.
**Medium confidence** in numerical accuracy claims due to lack of broader benchmark coverage.
**Medium confidence** in end-to-end integration benefits, pending further validation of the selector's robustness and generalizability.

## Next Checks
1. Evaluate