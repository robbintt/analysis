---
ver: rpa2
title: How Does Response Length Affect Long-Form Factuality
arxiv_id: '2505.23295'
source_url: https://arxiv.org/abs/2505.23295
tags:
- factual
- length
- response
- facts
- factuality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether and how response length affects
  the factuality of long-form text generated by large language models (LLMs). Using
  a newly introduced automatic evaluation framework, Bi-level Atomic Fact Evaluation
  (BAFE), which combines Wikipedia and Google Search verification for improved accuracy
  and efficiency, we conduct controlled experiments on biography and long fact description
  tasks.
---

# How Does Response Length Affect Long-Form Factuality

## Quick Facts
- arXiv ID: 2505.23295
- Source URL: https://arxiv.org/abs/2505.23295
- Reference count: 40
- Primary result: Longer responses exhibit significantly lower factual precision due to facts exhaustion

## Executive Summary
This study investigates how response length affects the factuality of long-form text generated by large language models. Using a newly introduced automatic evaluation framework called Bi-level Atomic Fact Evaluation (BAFE), the authors conduct controlled experiments on biography and long fact description tasks. The results show that longer responses exhibit significantly lower factual precision, confirming the existence of length bias. Through empirical analysis examining error propagation, long context, and facts exhaustion hypotheses, the study finds that facts exhaustion—where models progressively exhaust reliable knowledge and substitute with speculative details—is the primary cause of factual degradation.

## Method Summary
The study introduces Bi-level Atomic Fact Evaluation (BAFE), an automatic evaluation framework that decomposes responses into atomic facts and verifies them through a hierarchical retrieval process. BAFE uses Wikipedia pages for high-confidence facts and Google Search as a complementary fallback for unsupported facts. The framework achieves 89.31% agreement with human annotations while being significantly more efficient than existing methods. The main experiments use GPT-4o to generate controlled-length responses (100-500 words) for biography and entity description tasks, which are then evaluated using BAFE to measure factual precision.

## Key Results
- Longer responses show significantly lower factual precision, confirming length bias exists
- Facts exhaustion is identified as the primary cause of factual degradation, not error propagation or long context
- Multi-topic generation consistently achieves 2.25-2.86% higher factual precision than single-topic generation
- Autocorrelation analysis shows weak error propagation (lag-1 coefficient ~0.1), ruling out cascading errors

## Why This Works (Mechanism)

### Mechanism 1: Bi-Level Atomic Fact Verification (BAFE)
Decomposing responses into atomic facts and verifying them through a hierarchical retrieval process improves factuality evaluation accuracy while reducing computational cost. First-level verification against Wikipedia pages handles high-confidence facts efficiently, with only unsupported facts proceeding to second-level Google Search verification for expanded knowledge coverage.

### Mechanism 2: Facts Exhaustion as Primary Cause
When forced to generate extended content on a single topic, models progressively exhaust reliable knowledge and substitute with lower-confidence details, causing factual precision to decline. Models prioritize high-confidence knowledge during early generation, but as length constraints require more content on the same narrow topic, they must access progressively less reliable internal representations.

### Mechanism 3: Weak and Short-Term Error Propagation
Factual errors do not significantly cascade through long-form generation; early errors have minimal impact on subsequent factual precision. Autocorrelation analysis shows only lag-1 correlation (~0.1), indicating errors weakly predict the next token but have no long-range dependency.

## Foundational Learning

- **Atomic Fact Decomposition**: Why needed here: BAFE and all compared methods rely on breaking long responses into minimal verifiable units. Understanding this preprocessing step is essential for implementing or modifying factuality evaluation.
  - Quick check question: Given "Marie Curie won two Nobel Prizes and died in 1934," can you identify the atomic facts?

- **Factual Precision Metric**: Why needed here: The paper's central claim (length bias) is measured via factual precision (% of supported atomic facts). Without this, you cannot replicate the evaluation or interpret the results.
  - Quick check question: If a response has 50 atomic facts and 45 are supported, what is the factual precision?

- **Autocorrelation Analysis for Error Dependencies**: Why needed here: Section 4.1 uses autocorrelation to quantify error propagation. Understanding lag-k coefficients is necessary to interpret why error propagation was ruled out as a primary cause.
  - Quick check question: If autocorrelation coefficients at all lags > 0 are near zero, what does this imply about error propagation?

## Architecture Onboarding

- **Component map**: Response Decomposition Module -> First-Level Verifier (Wikipedia) -> Second-Level Verifier (Google Search) -> Aggregation Layer
- **Critical path**: 1) Input: Long-form response, 2) Decompose → atomic facts list, 3) For each fact: Wikipedia retrieval → LLM judge → if unsupported → self-contained revision → Google Search → LLM judge, 4) Output: Factual precision score
- **Design tradeoffs**: Wikipedia-first vs. Google-only prioritizes efficiency; single vs. multiple search queries uses 1 query per fact (vs SAFE's 5) for diminishing returns; relevance filtering removed assuming modern LLMs follow instructions well
- **Failure signatures**: High false negative rate on specialized knowledge, reference ambiguity in decontextualized facts, inconsistent results across models
- **First 3 experiments**: 1) Validate BAFE against human annotations (target: ≥85% agreement), 2) Confirm length bias on new dataset (plot factual precision vs. word count), 3) Test facts exhaustion hypothesis (compare single-topic vs multi-topic generation)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the observed length bias and facts exhaustion phenomenon generalize to open-source LLMs?
- Basis in paper: Experiments were primarily conducted on GPT-4o due to its instruction-following capabilities, suggesting future work should explore whether findings hold for open-source models
- Why unresolved: The study's experimental design relies on precise response length control via prompting, which may function differently in models with weaker instruction adherence

### Open Question 2
- Question: How can facts exhaustion be effectively mitigated to maintain factual precision in long-form generation?
- Basis in paper: The Conclusion notes that as facts exhaustion is the primary cause of degradation, future work should explore methods to supplement or retrieve deeper factual knowledge
- Why unresolved: This study focused on identifying the cause of the degradation rather than developing or testing mitigation strategies

### Open Question 3
- Question: Can "facts exhaustion" be directly correlated with internal knowledge depletion in the model's hidden states?
- Basis in paper: The Limitations section highlights that due to the black-box nature of LLMs, it is difficult to examine facts exhaustion at the internal knowledge level
- Why unresolved: The paper approaches the problem through empirical output analysis (black-box) rather than mechanistic interpretability (white-box)

## Limitations
- Experimental results primarily based on GPT-4o, raising questions about generalizability to other LLM architectures
- Wikipedia/Google Search verification may systematically underestimate factual precision for entities with limited online presence
- The cognitive mechanism of "facts exhaustion" remains inferred rather than directly measured

## Confidence
- **High confidence**: The existence of length bias and the conclusion that error propagation is weak and short-term
- **Medium confidence**: The facts-exhaustion mechanism as the primary cause of length bias
- **Low confidence**: The generalizability of these findings across all LLM architectures and domains

## Next Checks
1. **Cross-model validation**: Replicate the length-bias experiments across multiple LLM architectures (e.g., Claude, Gemini, Llama) to verify that facts exhaustion is not specific to GPT-4o's knowledge organization
2. **Domain-specific testing**: Apply BAFE and the experimental framework to highly specialized domains (scientific literature, legal documents, or recent events not well-covered by Wikipedia) to assess whether facts exhaustion manifests differently when Wikipedia coverage is limited
3. **Mechanistic interpretability study**: Use activation analysis or probing techniques to directly observe whether model representations change systematically as generation progresses, providing evidence for or against the hypothesized knowledge depletion mechanism