---
ver: rpa2
title: 'ALDEN: Reinforcement Learning for Active Navigation and Evidence Gathering
  in Long Documents'
arxiv_id: '2510.25668'
source_url: https://arxiv.org/abs/2510.25668
tags:
- page
- document
- answer
- pages
- alden
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes ALDEN, a reinforcement learning framework that
  trains vision-language models as interactive agents for navigating long, visually
  rich documents. It introduces a fetch action for direct page access, a cross-level
  reward for fine-grained supervision, and a visual semantic anchoring mechanism to
  stabilize training with large visual inputs.
---

# ALDEN: Reinforcement Learning for Active Navigation and Evidence Gathering in Long Documents

## Quick Facts
- **arXiv ID:** 2510.25668
- **Source URL:** https://arxiv.org/abs/2510.25668
- **Reference count:** 40
- **Primary result:** Achieves 9.14% average improvement in answer accuracy over strong baselines on five long-document benchmarks.

## Executive Summary
ALDEN introduces a reinforcement learning framework that trains vision-language models as interactive agents for navigating long, visually rich documents. The framework combines a novel fetch action for direct page access, a cross-level reward function for dense process supervision, and a visual semantic anchoring mechanism to stabilize training with high-dimensional document images. Trained on a corpus constructed from three open-source datasets, ALDEN achieves state-of-the-art performance on five long-document benchmarks, demonstrating effective multi-turn navigation and evidence gathering capabilities.

## Method Summary
ALDEN adapts multi-turn Proximal Policy Optimization (PPO) to train a vision-language model as an interactive document navigation agent. The policy model, initialized from Qwen2.5-VL-7B-Instruct, generates reasoning traces and structured actions (search, fetch, answer) to navigate multi-page documents. The framework introduces a fetch action for direct page-index access alongside traditional semantic search, implements a cross-level reward function providing both turn-level and token-level supervision, and applies visual semantic anchoring through dual-path KL divergence constraints to stabilize training with numerous visual tokens. The agent operates in a custom environment that returns page images and numbers in response to actions, with training conducted on a corpus of long documents filtered from DUDE, MPDocVQA, and SlideVQA.

## Key Results
- Achieves 9.14% average improvement in answer accuracy over strong baselines on five long-document benchmarks.
- The fetch action improves accuracy from 0.545 to 0.653 and recall from 0.471 to 0.598 on DUDE-sub with page-referenced queries.
- Cross-level reward architecture shows turn-level rewards contribute most to performance gains, while token-level penalties provide marginal improvement.
- Visual semantic anchoring successfully stabilizes training, maintaining higher entropy and bounded visual KL divergence compared to training without this mechanism.

## Why This Works (Mechanism)

### Mechanism 1: Dual-Action Navigation Space
Expanding the action space to include both semantic search and direct page fetch enables more efficient evidence gathering in long documents. The `search` action allows semantic retrieval for open-ended queries, while the `fetch` action enables direct page-index access for explicit references or sequential navigation. This dual-action approach leverages document structure that pure semantic retrieval ignores. Evidence shows fetch improves accuracy from 0.545 to 0.653 on DUDE-sub with page-referenced queries. The approach assumes users query documents in ways that reference both semantic content and structural position, and that ground-truth pages accurately represent optimal evidence.

### Mechanism 2: Cross-Level Reward Shaping
Dense process supervision at both turn-level and token-level improves credit assignment for multi-turn navigation compared to sparse outcome-only rewards. The cross-level reward combines turn-level rewards (format correctness + action outcomes: F1 for answer, NDCG@m for search, proximity for fetch), turn-level GAE to propagate future rewards, and token-level repetition penalty using n-gram Jaccard similarity to discourage redundant queries. This mechanism assumes intermediate steps retrieving ground-truth pages or reducing redundancy contribute causally to final answer correctness. Ablation shows outcome-only yields accuracy 0.483, turn-level improves to 0.509, and full cross-level achieves 0.513.

### Mechanism 3: Visual Semantic Anchoring for Training Stability
Dual-path KL-divergence constraints on visual and textual tokens stabilize RL training with high-dimensional document images. Standard PPO masks visual tokens, but ALDEN applies separate KL penalties: β_gen=0.001 for generated tokens and β_obs=0.01>β_gen for visual observation tokens, constraining hidden-state drift for the larger visual token set while allowing action flexibility. This mechanism assumes visual representations encode semantic grounding that should be preserved during RL fine-tuning. Evidence shows without VSA, reward fluctuates/collapses; with VSA, entropy remains higher and visual KL stays bounded.

## Foundational Learning

- **Concept:** Proximal Policy Optimization (PPO)
  - **Why needed here:** ALDEN adapts single-turn PPO to multi-turn MDP settings; understanding token-level advantage estimation (GAE), clipping, and KL penalties is essential.
  - **Quick check:** How does PPO propagate a scalar turn-level reward to individual token actions, and what role does the KL penalty play?

- **Concept:** Vision-Language Models (VLMs) for Document Understanding
  - **Why needed here:** Policy/value models are VLMs processing page images directly; understanding visual token volume and text integration is critical for the anchoring mechanism.
  - **Quick check:** How does the number of visual tokens scale with document page resolution, and why might this cause gradient instability?

- **Concept:** Retrieval-Augmented Generation (RAG) Paradigm
  - **Why needed here:** ALDEN is an "agentic" evolution of RAG where the VLM actively controls retrieval; understanding standard RAG clarifies the shift.
  - **Quick check:** In standard visual RAG, what are typical retrieval and generation steps, and where does ALDEN introduce agent-driven decision-making?

## Architecture Onboarding

- **Component map:** Query → Policy Model → Parser → Environment → Retrieval Module → Document Images → Reward Function → PPO Update → Policy Model

- **Critical path:**
  1. Query → Policy generates reasoning + action
  2. Parser extracts action and arguments
  3. Environment executes → returns page images/text
  4. Reward function computes turn- and token-level rewards
  5. Turn-level GAE propagates future rewards
  6. Token-level GAE computes advantages
  7. PPO update with dual KL regularization
  8. Repeat until answer or max turns (T=6)

- **Design tradeoffs:**
  - Fetch vs. Search frequency: Fetch efficient for explicit references but cannot discover semantic content; search explores but may miss exact pages
  - Cross-level reward complexity: More components provide denser supervision but risk misalignment; ablation shows turn-level contributes most, token-level adds marginal gains
  - Visual anchoring strength (β_obs vs. β_gen): Strong visual anchoring stabilizes training but may limit adaptation to document-specific features

- **Failure signatures:**
  - Low accuracy on page-referenced queries without fetch suggests action not utilized effectively
  - Repetitive search queries across turns indicate insufficient penalty or agent stuck in loop
  - Training collapse (entropy→0) suggests VSA inadequate; check visual token KL divergence
  - Near-zero retrieval precision indicates weak retriever or poorly formulated search queries

- **First 3 experiments:**
  1. Baseline comparison on DUDE-sub: Evaluate full ALDEN vs. search-only vs. outcome-reward-only (GRPO). Measure Acc, Rec, Pre, F1, #UP.
  2. Visual Semantic Anchoring ablation: Train with/without VSA at batch size 512; monitor reward curve, entropy, and KL divergence.
  3. Retriever generalization test: Train with single-vector retriever, evaluate with multi-vector (ColQwen) to assess strategy transfer.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can curriculum learning be effectively designed to handle A-VRDU tasks of varying difficulty levels?
- **Basis in paper:** The authors state in the conclusion: "Future work could focus on... adopting curriculum learning to handle tasks of varying difficulty."
- **Why unresolved:** The paper trains on a mixed corpus without difficulty-based ordering, leaving unexplored how to structure progressive task complexity.
- **What evidence would resolve it:** Experiments comparing different curriculum strategies (e.g., document length progression, query complexity ordering) against uniform sampling, showing whether difficulty-ordered training improves sample efficiency or final performance.

### Open Question 2
- **Question:** What mechanisms can improve evidence page verification when search returns pages distant from ground-truth evidence?
- **Basis in paper:** The case study section states: "the model still struggles to verify evidence pages and explore sufficiently when the search action returns results far from the ground-truth pages—a challenge we leave for future work."
- **Why unresolved:** ALDEN relies on semantic similarity retrieval; when initial retrieval fails, the agent lacks explicit verification or backtracking mechanisms.
- **What evidence would resolve it:** Ablation studies adding explicit verification tokens or uncertainty-aware exploration actions, measuring improvements in recall when initial retrieval quality is low.

### Open Question 3
- **Question:** How can domain-specific knowledge gaps (e.g., in scientific papers) be addressed within the RL training framework?
- **Basis in paper:** The results section notes "modest performance on scientific-paper datasets (PaperText, PaperTab) suggests domain knowledge remains a limiting factor."
- **Why unresolved:** ALDEN uses a general-purpose VLM backbone; whether domain adaptation requires additional pre-training, retrieval augmentation, or reward shaping is unclear.
- **What evidence would resolve it:** Comparison of training with domain-specific pre-training versus general-purpose models, or analysis of error types on PaperTab/PaperText to determine if failures stem from domain terminology or reasoning.

## Limitations

- **Training Instability:** Visual semantic anchoring addresses empirical instability but lacks rigorous theoretical justification for the specific β values used.
- **Reward Function Complexity:** Cross-level reward combines multiple components whose interaction effects were not adequately explored; token-level penalties provide marginal gains.
- **Action Space Assumptions:** Fetch action assumes reliable document structure that may not hold in real-world documents with inconsistent pagination or missing pages.

## Confidence

**High Confidence:** Dual-action navigation space demonstrably improves performance on page-referenced queries with quantitative evidence. Cross-level reward architecture is well-defined and produces measurable gains over outcome-only baselines.

**Medium Confidence:** Visual semantic anchoring successfully stabilizes training based on entropy and KL divergence metrics, but causal relationship between specific β values and stability is not rigorously established. Framework's generalizability to documents outside training corpus remains unproven.

**Low Confidence:** Claims about framework being "model-agnostic" and applicable to any VLM are not validated as only Qwen2.5-VL-7B-Instruct was tested. Assertion that ALDEN marks a "shift toward autonomous agents" for long-document understanding is aspirational rather than empirically demonstrated.

## Next Checks

1. **Ablation Study on Reward Components:** Systematically disable individual reward components (format penalty, retrieval reward, proximity reward, repetition penalty) to quantify their independent contributions and interaction effects.

2. **Document Structure Robustness Test:** Evaluate ALDEN on documents with artificially corrupted pagination, missing pages, or inconsistent numbering to assess whether fetch action degrades gracefully when document structure assumptions break.

3. **Multi-Document Navigation Extension:** Adapt the framework to handle queries requiring evidence from multiple documents rather than single documents to test generalizability beyond current single-document scope.