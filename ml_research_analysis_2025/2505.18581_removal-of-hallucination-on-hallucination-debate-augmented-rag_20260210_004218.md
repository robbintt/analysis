---
ver: rpa2
title: 'Removal of Hallucination on Hallucination: Debate-Augmented RAG'
arxiv_id: '2505.18581'
source_url: https://arxiv.org/abs/2505.18581
tags:
- retrieval
- debate
- agent
- drag
- response
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses a key limitation of Retrieval-Augmented Generation\
  \ (RAG) systems: even when retrieval is improved, generation can still be misled\
  \ by noisy or incomplete retrieved content, leading to compounded hallucinations\u2014\
  a phenomenon termed \"Hallucination on Hallucination.\" To mitigate this, the authors\
  \ propose Debate-Augoked RAG (DRAG), a training-free framework that integrates Multi-Agent\
  \ Debate (MAD) mechanisms into both the retrieval and generation stages. In the\
  \ retrieval stage, agents (proponents, opponents, judges) iteratively debate and\
  \ refine queries to improve coverage and reduce bias."
---

# Removal of Hallucination on Hallucination: Debate-Augmented RAG
## Quick Facts
- **arXiv ID**: 2505.18581
- **Source URL**: https://arxiv.org/abs/2505.18581
- **Reference count**: 15
- **Primary result**: A training-free framework that reduces compounded hallucinations in RAG systems by integrating Multi-Agent Debate into retrieval and generation stages, improving performance on open-domain QA, multi-hop QA, and commonsense reasoning benchmarks.

## Executive Summary
This paper addresses a critical flaw in Retrieval-Augmented Generation (RAG) systems: even with improved retrieval, generation can still be misled by noisy or incomplete retrieved content, leading to compounded hallucinations termed "Hallucination on Hallucination." To address this, the authors propose Debate-Augmented RAG (DRAG), a training-free framework that introduces Multi-Agent Debate (MAD) mechanisms into both retrieval and generation stages. The approach improves factual consistency and reasoning robustness, especially in multi-hop reasoning tasks.

## Method Summary
DRAG employs a two-stage debate mechanism. In the retrieval stage, agents (proponent, opponent, judge) iteratively debate and refine queries to improve coverage and reduce bias. In the generation stage, agents adopt asymmetric information roles and engage in adversarial debate to strengthen reasoning robustness and factual consistency. The framework is training-free and tested across six benchmarks in three task types: open-domain QA, multi-hop QA, and commonsense reasoning.

## Key Results
- DRAG consistently improves performance across six benchmarks in three task types.
- On 2WikiMultihopQA, DRAG improves exact match by 6 points over the best baseline.
- Demonstrates effectiveness in reducing RAG-induced hallucinations, especially in multi-hop reasoning.

## Why This Works (Mechanism)
DRAG leverages adversarial debate to expose weaknesses in retrieval and generation reasoning. In retrieval, debate forces agents to justify relevance and coverage, leading to more robust query formulation. In generation, asymmetric roles simulate adversarial reasoning, which helps surface and correct potential hallucinations before final output.

## Foundational Learning
- **Multi-Agent Debate (MAD)**: Agents with different roles (proponent, opponent, judge) debate to refine queries or challenge generation reasoning. Needed to expose blind spots in standard RAG pipelines. Quick check: Verify each agent has a clearly defined role and can influence the next agent's input.
- **Hallucination on Hallucination**: Compounding errors when generation is misled by flawed retrieval. Needed to frame the problem DRAG solves. Quick check: Trace error propagation from retrieval to generation in baseline models.
- **Asymmetric Information Roles**: Assigning agents different subsets of retrieved content to force adversarial reasoning. Needed to strengthen robustness in generation. Quick check: Confirm agents do not share identical retrieval contexts.

## Architecture Onboarding
- **Component map**: User Query -> Retrieval Stage (Proponent, Opponent, Judge) -> Refined Query -> Retriever -> Retrieved Passages -> Generation Stage (Asymmetric Agents) -> Final Answer
- **Critical path**: Retrieval debate → query refinement → re-retrieval → generation debate → final output
- **Design tradeoffs**: Training-free vs. potential for suboptimal agent coordination; debate overhead vs. accuracy gains; simplicity vs. need for domain adaptation
- **Failure signatures**: Stalled debate cycles in retrieval; agent consensus without critical review; generation that ignores retrieved evidence
- **First experiments**: (1) Run ablation study removing retrieval-stage debate; (2) Test generation-stage debate with symmetric vs. asymmetric roles; (3) Evaluate performance on a noisy retrieval dataset

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to well-defined QA benchmarks, not real-world deployment scenarios with severe retrieval noise.
- Training-free claim does not address long-term stability or performance degradation over repeated use.
- Asymmetric information roles tested only on structured benchmarks, leaving uncertainty about robustness in noisy or multimodal contexts.

## Confidence
- **High**: Retrieval-stage improvements across multiple benchmarks
- **Medium**: Generation-stage improvements, more pronounced in multi-hop tasks
- **Low**: "Training-free" and generalizability claims due to limited external validation

## Next Checks
1. Test DRAG on noisy or adversarial retrieval datasets (e.g., with injected irrelevant passages) to measure robustness.
2. Conduct ablation studies isolating retrieval-stage vs. generation-stage contributions across more diverse task types.
3. Perform a longitudinal evaluation to assess whether debate-based query refinement degrades or improves over repeated usage cycles.