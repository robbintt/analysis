---
ver: rpa2
title: 'Fact-checking AI-generated news reports: Can LLMs catch their own lies?'
arxiv_id: '2503.18293'
source_url: https://arxiv.org/abs/2503.18293
tags:
- claims
- news
- claim
- llms
- 'false'
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether large language models (LLMs) can
  fact-check their own generated news reports. The authors prompt two LLMs to generate
  92 news stories containing factual errors, then evaluate their ability to detect
  these errors both at the article and atomic claim level.
---

# Fact-checking AI-generated news reports: Can LLMs catch their own lies?

## Quick Facts
- arXiv ID: 2503.18293
- Source URL: https://arxiv.org/abs/2503.18293
- Reference count: 31
- LLMs show mixed performance fact-checking their own generated news, excelling at article-level assessment but struggling with atomic claim verification

## Executive Summary
This study investigates whether large language models (LLMs) can effectively fact-check news articles they generate themselves. The researchers created 92 news stories with intentional factual errors using GPT-4o and GLM-4, then tested their fact-checking capabilities at both article and atomic claim levels. While LLMs performed reasonably well at identifying incorrect content in entire articles, they struggled significantly with individual claims, often providing incorrect assessments or no assessment at all. The study reveals that LLMs are better at fact-checking national/international news and static information compared to local news and dynamic events.

## Method Summary
The researchers employed a multi-stage methodology to evaluate LLM fact-checking capabilities. First, they generated 92 news articles (47 with GPT-4o, 44 with GLM-4) containing factual errors across four dimensions: event, time, location, and participants. These articles were then manually analyzed to extract 1,337 atomic claims, which were decontextualized to include necessary background information. The extracted claims were evaluated using both direct LLM assessment and a Retrieval-Augmented Generation (RAG) approach that incorporated search results from the Google Serper API. Assessments were categorized into five types: Correct Assessment (CA), Correct Assessment/Correct Reasoning (CA/CR), Correct Assessment/Wrong Reasoning (CA/WR), Wrong Assessment (WA), and No Assessment (NA). Human verification through web search served as the gold standard for evaluation.

## Key Results
- LLMs perform well at article-level fact-checking but show significant weaknesses at the atomic claim level
- RAG approaches improve recall of correct assessments but also increase incorrect assessments due to irrelevant search results
- Performance varies significantly by news type: national/international news and static information are assessed more accurately than local news and dynamic events

## Why This Works (Mechanism)
LLMs demonstrate varying effectiveness in fact-checking based on the level of analysis and the nature of the content. At the article level, models can leverage contextual information and overall narrative coherence to identify inconsistencies. However, when evaluating atomic claims, they lose this broader context and must rely on their internal knowledge base or retrieved information, which may be incomplete or irrelevant. The RAG approach helps by providing external verification sources, but its effectiveness is limited by the quality and relevance of search results. The observed performance differences across news types suggest that models have better coverage of widely reported events and static facts compared to local or rapidly evolving stories.

## Foundational Learning
- Atomic claim extraction and decontextualization: Required to break down complex news articles into verifiable statements; Quick check: Each claim should be evaluable independently without requiring article context
- Temperature-based self-consistency: Using multiple runs at different temperatures to determine model confidence; Quick check: Majority vote across 5 runs at temperature 0.0 and 1.0
- RAG implementation for fact-checking: Integrating retrieved search results to augment model knowledge; Quick check: Top-5 search results concatenated with claim for evaluation
- Assessment categorization framework: System for classifying evaluation outcomes (CA, CA/CR, CA/WR, WA, NA); Quick check: Manual verification against web search as gold standard
- Performance benchmarking across news dimensions: Comparing accuracy by news type (national/local) and claim type (state/event); Quick check: Statistical analysis of performance differences between categories

## Architecture Onboarding

**Component Map**
News Generation -> Claim Extraction -> Direct Assessment -> RAG-Enhanced Assessment -> Human Verification

**Critical Path**
The critical path for accurate fact-checking involves: 1) Generating articles with errors, 2) Extracting and decontextualizing claims, 3) Direct LLM assessment, 4) RAG-enhanced assessment, and 5) Human verification. The bottleneck appears at claim extraction and decontextualization, as this manual process is time-intensive and introduces potential subjectivity.

**Design Tradeoffs**
The study balances automation with accuracy by using manual claim extraction despite its labor intensity, prioritizing quality over scalability. The RAG approach trades increased computational overhead and potential noise from irrelevant search results for improved recall of correct assessments. Temperature variation provides confidence scoring but increases computational cost.

**Failure Signatures**
High "No Assessment" rates indicate claims that are too ambiguous, too specific, or fall outside the model's knowledge base. Increased wrong assessments with RAG suggest poor retrieval quality or model confusion from contradictory information. Performance gaps between national and local news reveal coverage limitations in training data.

**First Experiments**
1. Test claim extraction automation using an LLM to identify atomic claims, then compare results against manual extraction
2. Implement relevance scoring for RAG search results to filter out low-quality or irrelevant snippets before feeding to the assessment model
3. Evaluate confidence thresholds by analyzing the relationship between self-consistency scores and actual accuracy across different claim types

## Open Questions the Paper Calls Out
None

## Limitations
- Small sample size (92 articles, 1,337 claims) limits generalizability of findings
- Synthetic errors rather than real-world misinformation may not reflect actual fact-checking challenges
- Manual claim extraction process is labor-intensive and may introduce subjectivity
- Web search as gold standard may not capture nuanced truth in rapidly evolving situations

## Confidence
Medium confidence in core findings due to:
- Constrained sample size and synthetic error generation
- Manual processes introducing potential subjectivity
- Reliance on web search as verification standard
- Performance differences requiring further validation with larger, more diverse datasets

## Next Checks
1. Test the fact-checking pipeline on a larger dataset of real-world AI-generated news articles with naturally occurring errors rather than synthetic ones
2. Implement and evaluate more sophisticated RAG approaches with improved retrieval filtering and relevance scoring to reduce the impact of low-quality search results
3. Conduct a human-in-the-loop study to measure the effectiveness of LLM fact-checking as an assistive tool for human fact-checkers, particularly for local news and dynamic events