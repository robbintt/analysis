---
ver: rpa2
title: 'CogToM: A Comprehensive Theory of Mind Benchmark inspired by Human Cognition
  for Large Language Models'
arxiv_id: '2601.15628'
source_url: https://arxiv.org/abs/2601.15628
tags:
- task
- table
- data
- tasks
- example
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CogToM, a comprehensive Theory of Mind (ToM)
  benchmark for large language models (LLMs) comprising over 8,000 bilingual instances
  across 46 task paradigms, grounded in human cognitive psychology. Unlike existing
  benchmarks that focus narrowly on false belief tasks, CogToM integrates 36 sub-capabilities
  across seven dimensions (emotion, desire, intention, percept, knowledge, belief,
  and non-literal communication), validated by 49 human annotators.
---

# CogToM: A Comprehensive Theory of Mind Benchmark inspired by Human Cognition for Large Language Models

## Quick Facts
- arXiv ID: 2601.15628
- Source URL: https://arxiv.org/abs/2601.15628
- Reference count: 40
- This paper introduces CogToM, a comprehensive Theory of Mind (ToM) benchmark for large language models (LLMs) comprising over 8,000 bilingual instances across 46 task paradigms, grounded in human cognitive psychology.

## Executive Summary
CogToM addresses the limitations of existing ToM benchmarks by providing a comprehensive, cognitively-grounded evaluation framework that spans 36 sub-capabilities across seven cognitive dimensions. The benchmark reveals a striking "developmental inversion" where LLMs excel at late-acquired socio-affective reasoning but struggle with early-developmental perceptual tasks. Through systematic evaluation of 22 models, CogToM demonstrates significant performance heterogeneity and offers a robust instrument for probing the evolving cognitive boundaries of LLMs.

## Method Summary
The CogToM benchmark comprises 8,513 bilingual (Chinese-English) instances across 46 task paradigms, each presented as multiple-choice questions with four options. The evaluation pipeline uses zero-shot inference with temperature=0, requiring models to output answers in the format "[[Option Letter]]". Each question is evaluated across 5 trials with cyclic option rotation to mitigate positional bias. Human inter-annotator agreement rates serve as a proxy for task difficulty and are used jointly with model accuracy to detect cognitive asymmetries.

## Key Results
- Models show inverted developmental patterns, excelling at complex socio-affective reasoning (Emotion, Belief categories) while failing basic perceptual tasks (Percept category)
- Existing ToM benchmarks exhibit ceiling effects (90-100% accuracy), while CogToM's new paradigms show broader, more discriminative performance distributions
- High human inter-annotator agreement on perceptual tasks (100% consensus) contrasts with abysmal model performance (<30%), revealing fundamental architectural limitations

## Why This Works (Mechanism)

### Mechanism 1: Cognitive Asymmetry Detection via Human-Model Agreement Gap
- Claim: CogToM exposes a fundamental asymmetry where LLMs achieve near-ceiling performance on late-acquired socio-affective reasoning while failing on early-developmental perceptual tasks.
- Mechanism: Tasks with near-perfect human inter-annotator agreement (IAR > 90%) but low model accuracy reveal the gap. High IAR indicates unambiguous "common sense" for humans; model failure on these tasks signals divergent cognitive architecture rather than task ambiguity.
- Core assumption: Human consensus on task difficulty correlates with developmental simplicity; divergence from this pattern indicates non-human-like processing.
- Evidence anchors:
  - [Section 4.3.1]: "The red elliptical region in the bottom-right corner reveals a profound cognitive asymmetry... Percept tasks reside at the very bottom. Despite 100% human agreement, model performance is abysmal, below 30%."
  - [Section 4.2.2]: "The Percept category emerges as a critical performance bottleneck for all tested models, with a median accuracy of only approximately 20%."
- Break condition: If model performance correlates positively with human IAR across all task categories (no bottom-right outliers), the asymmetry mechanism fails to hold.

### Mechanism 2: Paradigm Diversity Enables Boundary Detection
- Claim: Existing ToM benchmarks exhibit ceiling effects that mask LLM limitations; expanded paradigm coverage reveals hidden vulnerabilities.
- Mechanism: Traditional false-belief-centered benchmarks cluster in the 90-100% accuracy range (saturation). New tasks spanning perceptual, knowledge-state, and perspective-taking paradigms distribute across 0-60%, providing discriminative granularity.
- Core assumption: Broader cognitive coverage maps to more comprehensive probing of reasoning mechanisms; narrow paradigms test pattern-matching shortcuts.
- Evidence anchors:
  - [Section 4.2.4]: "Existing tasks exhibit a pronounced ceiling effect, with density peaks heavily clustered around the 90%–100% accuracy range... new tasks display a significantly broader and more balanced distribution."
  - [Section 4.2.3]: "Yummy-yummy Task... Mixtral-8x7B's accuracy drops from near 100% in Multiple Desires to approximately 65%."
- Break condition: If new tasks show similar ceiling clustering as existing tasks, paradigm diversity provides no additional discriminative value.

### Mechanism 3: Simulated ToM via Linguistic Pattern Matching
- Claim: High performance on complex socio-affective tasks reflects corpus-derived pattern matching rather than genuine mental state modeling.
- Mechanism: Tasks involving rich linguistic descriptions (Strange Stories, emotion comprehension) map to frequent discourse patterns in training data. Perceptual tasks requiring egocentric-to-allocentric perspective transformation lack corpus analogues.
- Core assumption: Tasks with abundant training-corpus exemplars are solved via statistical regularities; tasks requiring grounded spatial/perceptual reasoning cannot leverage such shortcuts.
- Evidence anchors:
  - [Section 4.3.2]: "This suggests that LLMs likely achieve a form of 'simulated ToM' through linguistic pattern matching and probabilistic prediction derived from massive corpora, rather than through a cognitive developmental process grounded in perception and embodiment."
  - [Section 4.2.3]: "The models' proficiency in tracking a reader's knowledge may stem from stylistic pattern matching or linguistic heuristics learned from explanatory corpora, rather than genuine ToM capabilities."
- Break condition: If models trained on minimal social text but rich perceptual data show reversed patterns (high perceptual, low socio-affective), the corpus-pattern mechanism is insufficient.

## Foundational Learning

- Concept: **Theory of Mind developmental milestones**
  - Why needed here: The paper maps tasks to human developmental sequences (Yummy-yummy at ~18 months → TEC: Hidden Emotions at ~6 years). Understanding this timeline is essential for interpreting the "developmental inversion" finding.
  - Quick check question: Which typically develops first in humans: understanding discrepant desires (Yummy-yummy) or recognizing hidden emotions?

- Concept: **Moravec's Paradox**
  - Why needed here: The paper invokes this principle to explain why high-level reasoning (easy for LLMs) contrasts with low-level perceptual tasks (hard for LLMs). Original formulation: sensorimotor skills require massive computation; abstract reasoning is computationally cheaper.
  - Quick check question: How does the finding that LLMs excel at "late-acquired complex reasoning" relate to Moravec's original claim about evolutionary recency?

- Concept: **Inter-Annotator Agreement Rate (IAR) as difficulty proxy**
  - Why needed here: The joint IAR-model accuracy analysis is central to detecting cognitive asymmetry. Tasks with high human consensus but low model performance indicate non-human-like failure modes.
  - Quick check question: If a task has IAR of 60% and model accuracy of 50%, what can you conclude about the cognitive gap?

## Architecture Onboarding

- Component map:
  - **7 capability dimensions** (ATOMS-derived): Emotion, Desire, Intention, Percept, Knowledge, Belief, Non-literal + Comprehensive
  - **36 sub-capabilities**: Fine-grained mapping (e.g., E1-E9 under Emotion)
  - **46 task paradigms**: Concrete evaluation formats (e.g., False Belief [Location], Scalar Implicature, Spatial Construction)
  - **Data pipeline (6 stages)**: Task design → LLM expansion → Expert supervision → Human annotation → Arbitration → Translation

- Critical path:
  1. Identify which capability dimension your target task probes
  2. Verify the task hasn't been saturated (check Figure 7 distribution)
  3. Cross-reference against human IAR to ensure discriminative validity
  4. Run 5-trial evaluation with cyclic option rotation to mitigate positional bias

- Design tradeoffs:
  - **Multiple-choice vs. generative**: Paper acknowledges loss of ecological validity; multiple-choice ensures objective scoring but may not capture nuanced ToM
  - **Static scenes vs. interactive inference**: Single-turn evaluation cannot replicate recursive mental state updating
  - **Bilingual coverage**: Limited to Chinese-English; cultural/linguistic ToM variation unaddressed

- Failure signatures:
  - **Ceiling clustering**: Task too easy for frontier models (>95% average accuracy)
  - **High IAR, low accuracy**: Model fails "common sense" tasks (Percept category signature)
  - **Positional bias**: Without cyclic rotation, accuracy varies by correct-option position

- First 3 experiments:
  1. Replicate the IAR vs. accuracy scatter (Figure 8) on your target model to identify which quadrant it occupies for each capability dimension.
  2. Isolate the Percept category tasks (Picture Identification, Spatial Construction) and test with/without explicit perspective-taking prompts to probe whether failure is prompting-sensitive.
  3. Compare performance on "Existing" vs. "New" task partitions (Figure 7) to determine if your model shows the same saturation pattern or if new paradigms remain discriminative.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the integration of multi-modal grounding or embodied training resolve the observed "developmental inversion," where LLMs fail early-stage perceptual tasks despite mastering complex late-stage reasoning?
- Basis in paper: [explicit] The paper concludes (Section 4.3.2) that LLMs exhibit a "developmental inversion," failing basic sensory preference tests (e.g., *See-Know Task*) while passing complex emotional reasoning, suggesting a reliance on pattern matching rather than grounded perception.
- Why unresolved: The current evaluation is text-only; the paper does not test whether non-textual sensory inputs can bridge the gap in foundational "Percept" tasks where median accuracy is ~20%.
- What evidence would resolve it: A comparative study evaluating multimodal models (e.g., GPT-4V) or embodied agents on the *Spatial Construction* and *Synesthetic Fallacy* tasks to see if visual grounding improves performance.

### Open Question 2
- Question: To what extent does the multiple-choice evaluation format overestimate LLM Theory of Mind capabilities compared to open-ended, generative interaction?
- Basis in paper: [explicit] The "Limitations" section states that the multiple-choice format "may not fully capture the generative and nuanced nature of ToM in open-ended social interactions."
- Why unresolved: The current methodology verifies the selection of a correct answer (accuracy) but does not validate if the model can generate appropriate social responses autonomously, potentially masking a lack of genuine reasoning.
- What evidence would resolve it: A follow-up evaluation using the same CogToM scenarios but requiring models to generate open-ended responses, scored by human experts for social appropriateness and reasoning depth.

### Open Question 3
- Question: How does LLM performance vary when static scenes are replaced with dynamic, multi-turn interactions that require recursive updating of mental states?
- Basis in paper: [explicit] The "Limitations" section notes that the tasks consist of "static scenes" and that "single-turn evaluations cannot fully replicate" the recursive, dynamic nature of real-world ToM.
- Why unresolved: Real-world ToM involves continuous updates based on new information; it is unknown if models can maintain consistent mental state tracking over extended dialogue without the "reset" provided by discrete questions.
- What evidence would resolve it: Constructing a dynamic variant of CogToM where the scenario evolves over multiple dialogue turns, requiring the model to remember and update previous belief states to succeed.

## Limitations
- Dataset Availability: The CogToM dataset is not publicly available, creating a fundamental barrier to independent validation.
- Model Version Specificity: Claims about GPT-5.1 and Qwen3-Max performance cannot be independently verified without exact API specifications or model weights.
- Cultural Generalization: While bilingual, the benchmark's developmental grounding in Western ToM milestones may not translate across cultural contexts.

## Confidence

- **High Confidence**: The methodological framework combining IAR with model accuracy to detect cognitive asymmetry is sound and well-supported by the presented evidence.
- **Medium Confidence**: The assertion that LLMs achieve "simulated ToM" via pattern matching rather than genuine mental modeling is plausible but relies on indirect evidence.
- **Low Confidence**: Specific performance claims for GPT-5.1 and Qwen3-Max cannot be verified without access to the exact models and dataset.

## Next Checks
1. **Dataset Access Verification**: Contact authors or BrainCog Lab to obtain the CogToM dataset for independent replication. Without this, all performance claims remain unverifiable.
2. **Corpus Analysis for Pattern Matching**: Conduct a corpus analysis to quantify the frequency of linguistic patterns in socio-affective vs. perceptual tasks. Compare with model performance to directly test the "simulated ToM" hypothesis.
3. **Cross-Cultural Validation**: Test the benchmark with models trained on culturally diverse data (e.g., BLOOMZ, Aya) to assess whether the observed cognitive asymmetry holds across different cultural ToM conventions.