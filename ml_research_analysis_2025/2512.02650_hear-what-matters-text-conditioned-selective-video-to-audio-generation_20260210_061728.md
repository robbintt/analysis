---
ver: rpa2
title: Hear What Matters! Text-conditioned Selective Video-to-Audio Generation
arxiv_id: '2512.02650'
source_url: https://arxiv.org/abs/2512.02650
tags:
- video
- audio
- text
- sound
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of generating user-intended sound
  from multi-object videos, a key requirement for precise audio editing in multimedia
  production. The proposed SELVA model introduces a text-conditioned video encoder
  that uses text prompts as explicit selectors to extract prompt-relevant video features,
  achieved through a learnable supplementary token mechanism that refines cross-attention
  and suppresses irrelevant activations.
---

# Hear What Matters! Text-conditioned Selective Video-to-Audio Generation

## Quick Facts
- arXiv ID: 2512.02650
- Source URL: https://arxiv.org/abs/2512.02650
- Reference count: 40
- Primary result: SELVA achieves state-of-the-art performance in text-conditioned selective video-to-audio generation with improved audio quality, semantic alignment, and temporal synchronization.

## Executive Summary
This work introduces SELVA, a framework for generating user-intended sound from multi-object videos using text prompts as explicit selectors. The key innovation is a text-conditioned video encoder that extracts prompt-relevant video features through cross-attention and learnable supplementary tokens, enabling precise sound source selection without requiring mono audio supervision. SELVA is evaluated on the new VGG-MONOAUDIO benchmark and outperforms existing methods in audio quality, semantic alignment, and temporal synchronization, with human perceptual studies confirming its superiority in controllability and coherence.

## Method Summary
SELVA employs a two-stage training approach: first training a text-conditioned video encoder (Synchformer with cross-attention and 5 [SUP] tokens) using teacher-student distillation on auto-mixed video pairs, then fine-tuning an MM-DiT generator with flow matching. The model uses parameter-efficient fine-tuning (14% of parameters) and self-augmentation to learn selective generation without source-separated audio supervision. Text prompts modulate video features to suppress irrelevant activations, while the auto-mixing strategy enables training on mixed-source videos by regressing to teacher encoder outputs from single-source targets.

## Key Results
- Achieves state-of-the-art audio quality (lower FAD/KAD, higher IS) on VGG-MONOAUDIO benchmark
- Superior semantic alignment (higher CLAP/IB scores) compared to text-conditioned baselines
- Best temporal synchronization (lowest DeSync) among evaluated methods
- Human studies confirm SELVA's superiority in controllability and temporal coherence

## Why This Works (Mechanism)

### Mechanism 1
Text prompts act as explicit selectors for sound sources by modulating video encoder attention. A cross-attention block inserted after frozen spatiotemporal attention layers uses text embeddings as keys/values to query video features, producing text-conditioned video representations that suppress irrelevant visual activations. Core assumption: Pretrained video encoders encode entangled features containing both target and distractor sound sources; text guidance can disentangle these.

### Mechanism 2
Learnable [SUP] tokens mitigate attention artifacts and improve text-video alignment. Prepended to text embeddings before cross-attention, [SUP] tokens absorb high-norm artifacts in attention space, allowing the model to strengthen attention to intent-relevant regions. Core assumption: Cross-modal attention in transformers produces spurious correlations that confuse grounding; additional learnable tokens can serve as "registers" to capture these.

### Mechanism 3
Self-augmentation via auto-mixing enables training without source-separated audio supervision. During training, two videos are horizontally concatenated with random ratios; one serves as target, the other as distractor. The student encoder learns to extract target-relevant features by regressing to teacher encoder outputs from the unmixed target video. Core assumption: Mixed-video features contain both target and distractor information; the teacher provides supervision for target-only features.

## Foundational Learning

- **Cross-modal attention mechanisms**
  - Why needed here: Core to how text modulates video features. Understanding Query-Key-Value attention and how different modalities interact in cross-attention blocks is essential.
  - Quick check question: Can you explain how text embeddings would serve as keys/values while video features serve as queries in a cross-attention block?

- **Teacher-student distillation**
  - Why needed here: Stage-1 training uses a frozen teacher encoder (trained on clean single-source videos) to supervise the student encoder's outputs on mixed videos.
  - Quick check question: Why would a teacher model trained on single-source videos be useful for supervising a student model that processes mixed-source videos?

- **Flow matching / diffusion models**
  - Why needed here: The audio generator uses conditional flow matching to synthesize audio latents conditioned on video and text features.
  - Quick check question: How does flow matching differ from standard diffusion, and why might it be preferred for audio generation?

- **Parameter-efficient fine-tuning (PEFT)**
  - Why needed here: SELVA trains only 14% of video encoder parameters (cross-attention + pooling) and 14% of generator parameters (projection + adaLN layers).
  - Quick check question: What are the trade-offs between PEFT and full fine-tuning in terms of compute, memory, and potential performance?

## Architecture Onboarding

- **Component map**: Text encoder → [SUP] tokens prepending → Cross-attention with video features → Conditioned video features v → AdaLN conditioning in MM-DiT → Flow-matching sampling → Audio latent → VAE decode → Waveform

- **Critical path**: Text → [SUP] prepending → Cross-attention with video features → Conditioned video features v → AdaLN conditioning in MM-DiT → Flow-matching sampling → Audio latent → VAE decode → Waveform

- **Design tradeoffs**:
  - Two-stage vs. joint training: Two-stage ensures each module converges before mutual conditioning; joint training shows degraded cross-modal alignment.
  - [SUP] token count: 5 tokens balance semantic (CLAP) and temporal (DeSync) alignment; fewer = underfitting, more = redundancy.
  - PEFT vs. full fine-tuning: PEFT reduces compute (14% params) but may limit performance ceiling.
  - Auto-mixing probability: 0.75 mixing probability with λ > 0.2 for target; higher mixing improves robustness but increases task difficulty.

- **Failure signatures**:
  - Sound substitution: Model generates non-target sounds aligned with text but mismatched to video.
  - Temporal drift: Audio semantically correct but out of sync with video events.
  - Off-screen sounds: Model generates sounds not visible in video.
  - Semantic confusion in intra-class pairs: Model fails to distinguish similar categories.

- **First 3 experiments**:
  1. Ablate [SUP] tokens: Train without [SUP], visualize attention maps, measure DeSync scores. Expect: degraded temporal alignment, noisier attention.
  2. Vary mixing ratio λ: Test extreme ratios (λ=0.1, 0.9) and balanced (λ=0.5). Expect: extreme ratios → easier task but less robust; balanced → harder but better generalization.
  3. Replace cross-attention with concatenation: Instead of cross-attention for text modulation, simply concatenate text embeddings to video features. Expect: poorer selectivity, more off-screen sounds, lower IB/DeSync scores.

## Open Questions the Paper Calls Out

### Open Question 1
Can SELVA achieve fine-grained cross-modal distinction between visually similar actions with distinct sounds (e.g., distinguishing "male singing" from "male burping") or control specific attributes like aggression? Basis: Current text labels are simple noun-verb conjunctions, limiting complex text understanding and attribute controllability. Evidence needed: Evaluation on benchmark containing nuanced auditory events with high visual similarity.

### Open Question 2
Would comprehensive full training of the model weights resolve residual failure cases where the video encoder loses track of target movement changes? Basis: Current parameter-efficient tuning (updating only ~14% of weights) may limit the encoder's ability to consistently track movement changes. Evidence needed: Comparative study measuring temporal alignment between current PEFT version and fully fine-tuned version on complex motion sequences.

### Open Question 3
To what extent does the noisiness of the VGGSound dataset limit selective generation performance, and can a refined auto-mixing scheme with cleaner source data improve results? Basis: "Noisiness of the training data" identified as primary performance bottleneck. Evidence needed: Training on rigorously filtered version of VGGSound or synthetic dataset with guaranteed audio-visual alignment and comparing semantic alignment scores.

## Limitations
- Model requires text prompts to specify target sounds, which may not always be available or intuitive for users
- Auto-mixing training strategy may not fully capture complexity of real-world multi-source audio environments
- VGG-MONOAUDIO benchmark remains relatively small (1,071 mixed pairs), limiting statistical power

## Confidence

- **High confidence**: Audio quality improvements (FAD, KAD, IS scores), temporal synchronization gains (DeSync), and human perceptual superiority
- **Medium confidence**: Semantic alignment improvements (CLAP, IB scores) and superiority over baselines
- **Low confidence**: Claims about [SUP] tokens specifically absorbing attention artifacts

## Next Checks

1. **Temporal generalization test**: Evaluate SELVA on a temporal generalization set where audio-video alignment is systematically varied (e.g., time-shifted audio, frame-sampled videos). This would validate whether the model truly learns temporal grounding rather than memorizing training video durations.

2. **Prompt robustness evaluation**: Systematically test SELVA with increasingly ambiguous or underspecified text prompts (e.g., "sound" vs. "dog barking" vs. "golden retriever barking excitedly"). This would quantify the minimum prompt specificity required for reliable selective generation.

3. **Cross-dataset transfer validation**: Test SELVA on audio-visual datasets outside VGGSound (e.g., Kinetics-Sounds, AudioSet) to assess generalization beyond the training distribution. This would reveal whether the model has learned generalizable selective generation principles or dataset-specific patterns.