---
ver: rpa2
title: 'CreativityPrism: A Holistic Benchmark for Large Language Model Creativity'
arxiv_id: '2510.20091'
source_url: https://arxiv.org/abs/2510.20091
tags:
- creativity
- evaluation
- story
- creative
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces CreativityPrism, a holistic benchmark for
  evaluating large language model (LLM) creativity across nine tasks spanning three
  domains: divergent thinking, creative writing, and logical reasoning. The framework
  uses twenty metrics organized into three creativity dimensions: quality, novelty,
  and diversity.'
---

# CreativityPrism: A Holistic Benchmark for Large Language Model Creativity

## Quick Facts
- **arXiv ID:** 2510.20091
- **Source URL:** https://arxiv.org/abs/2510.20091
- **Reference count:** 40
- **Key outcome:** 17 SOTA LLMs evaluated across 9 tasks showing proprietary models outperform open-source ones by 10-20% across creativity dimensions

## Executive Summary
This paper introduces CreativityPrism, a comprehensive benchmark for evaluating LLM creativity across three domains: divergent thinking, creative writing, and logical reasoning. The framework uses 20 metrics organized into three dimensions: quality, novelty, and diversity. Experiments on 17 state-of-the-art models reveal that while quality and diversity metrics show strong correlations, novelty exhibits much weaker correlation across tasks, confirming that creativity evaluation requires diverse, domain-specific metrics rather than relying on any single measure.

## Method Summary
The benchmark evaluates LLMs on 9 tasks across 3 creativity domains using 20 metrics mapped to Quality, Novelty, and Diversity dimensions. Generation uses vLLM (v0.7.2) or APIs with specific temperature settings (0.75 for writing, 1.0 for divergent/index tasks). Evaluation relies primarily on LLM-as-a-Judge (Qwen2.5-72B) with few-shot prompts, validated through correlation with human annotations or stronger models. Scores are min-max normalized per metric, averaged per task, then averaged per dimension to yield final 0-1 scores. The framework requires specific datasets from cited repositories and careful execution of generation and evaluation scripts.

## Key Results
- Proprietary models (GPT-4.1, Claude-3.5-Sonnet) outperform open-source models (Mistral, Llama, Qwen) by 10-20% across all creativity dimensions
- Quality and diversity metrics show strong correlations (0.6-0.9) while novelty exhibits much weaker correlations (0.1-0.4) across tasks
- Within the novelty dimension, metrics show low correlations with each other, confirming the necessity of including diverse tasks
- The framework successfully distinguishes between convergent and divergent creativity, with different models excelling in different domains

## Why This Works (Mechanism)

### Mechanism 1: Dimensional Decoupling of Creativity
The framework treats creativity as orthogonal dimensions—Quality, Novelty, and Diversity—where performance in one doesn't guarantee performance in others. By evaluating across three domains and aggregating metrics into dimensions, models can't mask deficits in one area with strength in another. The strong correlation between quality and diversity but weak correlation for novelty confirms this dimensional separation.

### Mechanism 2: Proxy-Judge Alignment via Few-Shotting
Scalable evaluation is achieved by validating Qwen2.5-72B as a proxy judge against human annotations or GPT-4.1. Few-shot prompting (20-shot for AUT, 2-shot for TTCW) aligns the proxy's scoring distribution with human experts. The Pearson correlation threshold (>0.5) ensures the proxy judge is reliable enough to replace human evaluators at scale.

### Mechanism 3: Constraint-Based Logical Creativity
In logical reasoning domains, creativity is operationalized as satisfying functional correctness while diverging from standard solution pathways. Tasks like NeoCoder use constraints ("don't use hashmap, while loop") to force models away from memorized solutions. Creativity is measured only when correctness is achieved first, creating a "convergence before divergence" requirement.

## Foundational Learning

- **Concept: Convergent vs. Divergent Creativity**
  - **Why needed:** The benchmark distinguishes between generating many ideas (Divergent Thinking) and finding one novel, correct solution (Logical Reasoning). This distinction explains why a model might score high on AUT but low on NeoCoder.
  - **Quick check:** Does high AUT score predict high Creative Math score? (Likely not, due to domain differences).

- **Concept: Min-Max Normalization & Aggregation**
  - **Why needed:** The framework combines 20 different metrics onto a single 0-1 scale. Without understanding normalization, the final "Overall" score is uninterpretable.
  - **Quick check:** How does the framework prevent a metric with range 0-100 from dominating one with range 1-5? (Min-max normalization to [0,1] before averaging).

- **Concept: LLM-as-a-Judge Reliability**
  - **Why needed:** 6/9 tasks rely on LLM judges, making scores probabilistic proxies dependent on alignment studies. Users must understand these are not absolute truth.
  - **Quick check:** Why use Qwen2.5-72B instead of GPT-4.1 for all evaluations? (Cost/efficiency trade-off, validated by correlation checks).

## Architecture Onboarding

- **Component map:** Task Suite (AUT, DAT, TTCT, TTCW, CS4, Creative Short Story, Creativity Index, NeoCoder, Creative Math) → Generative Engine (Target LLM) → Evaluation Module (Code execution, Feature-based, LLM-Judge) → Score Processor (Normalization → Dimension Averaging → Overall Score)

- **Critical path:** The LLM-Judge prompt engineering is the most fragile component. If prompts for judging "Originality" in TTCT aren't precise, the "Novelty" dimension score becomes noise, violating the core finding that Novelty is distinct.

- **Design tradeoffs:**
  - Breadth vs. Depth: Covers 3 domains but limited to English text, potentially missing multimodal or cultural nuances
  - Automation vs. Validity: LLM-Judges enable scale but introduce model bias (judges may prefer outputs from their own model family)
  - Novelty Definition: H-creative (relative to references) vs. P-creative (self-consistency) varies by task, making cross-task comparison noisy

- **Failure signatures:**
  - Metric Gaming: Models generating random incoherent words to maximize DAT Score while failing Quality
  - Judge Collapse: High correlation between Judge score and perplexity, where judges reward safe grammatical text over risky creative text
  - Novelty-Decoupling: Model achieves 100% correctness in Math but 0% novelty because it memorized training solutions

- **First 3 experiments:**
  1. **Judge Alignment Check:** Run TTCW evaluation prompt with Qwen2.5-72B on 12 ground-truth stories from Appendix E.1. Verify ~0.7 accuracy claimed in Appendix D.
  2. **Metric Correlation Audit:** Generate outputs for Llama3.1-8B and plot correlation matrix of raw metrics. Confirm weak novelty correlation finding from Figure 4.
  3. **Constraint Stress Test:** Run NeoCoder on "State 5" (maximum constraints). Verify if model prioritizes correctness over constraint satisfaction or vice versa.

## Open Questions the Paper Calls Out

- **Open Question 1:** Does fine-tuning on CreativityPrism tasks improve LLM performance across all three creativity dimensions without catastrophic forgetting? (Explicit in Limitations; requires fine-tuning experiments comparing pre/post performance on benchmark and general NLP tasks).

- **Open Question 2:** Can the framework be adapted for multimodal creativity evaluation, and does the correlation structure persist across modalities? (Explicit in Limitations; requires applying benchmark to image/audio generation with correlation analysis).

- **Open Question 3:** How can "high-concept" novelty be automatically evaluated without relying on n-gram overlaps or semantic distance? (Explicit in Limitations; requires developing new protocols that align with human judgment on high-level conceptual novelty).

## Limitations

- The framework relies on LLM judges for 6/9 tasks, introducing potential model bias that cannot be fully eliminated
- Focus on English-language, text-only outputs excludes multimodal creativity and potentially important cultural dimensions of creative expression
- The nine selected tasks may not be representative proxies for all creativity dimensions across all domains, potentially missing important aspects of creativity

## Confidence

- **High Confidence:** Proprietary models outperform open-source models by 10-20% across all creativity dimensions (well-supported by Table 1 and consistent performance patterns)
- **Medium Confidence:** Quality and diversity metrics show strong correlations while novelty exhibits weak correlations (supported by Figure 4 correlation analysis, though weak novelty correlations could reflect measurement difficulty)
- **Low Confidence:** The nine selected tasks are representative proxies for all creativity dimensions across all domains (requires further validation as benchmark may miss important creativity aspects)

## Next Checks

1. **Judge Alignment Verification:** Run the TTCW evaluation prompt with Qwen2.5-72B on the 12 ground-truth stories from Appendix E.1 to verify if your local setup reproduces the ~0.7 accuracy claimed in Appendix D. This validates the core evaluation mechanism.

2. **Novelty Correlation Audit:** Generate outputs for a single model (e.g., Llama3.1-8B) and plot the correlation matrix of the raw metrics. Confirm the weak novelty correlation finding from Figure 4 by checking if novelty metrics correlate poorly with both quality and diversity metrics within the same dimension.

3. **Cross-Model Judge Consistency:** Evaluate the same outputs using both Qwen2.5-72B and GPT-4.1 as judges. Measure the correlation between their scores across novelty metrics specifically, as this will reveal the extent of judge-specific bias in the creativity evaluations.