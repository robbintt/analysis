---
ver: rpa2
title: Basis-Oriented Low-rank Transfer for Few-Shot and Test-Time Adaptation
arxiv_id: '2512.02441'
source_url: https://arxiv.org/abs/2512.02441
tags:
- bolt
- task
- adaptation
- tasks
- atlas
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: BOLT addresses few-shot and test-time adaptation for large pre-trained
  models by constructing orthogonal, task-informed spectral bases from existing fine-tuned
  models and adapting new tasks via learning only diagonal coefficients in this shared
  subspace. This approach avoids costly meta-training and model merging, while constraining
  updates to a low-dimensional, task-informed subspace.
---

# Basis-Oriented Low-rank Transfer for Few-Shot and Test-Time Adaptation

## Quick Facts
- arXiv ID: 2512.02441
- Source URL: https://arxiv.org/abs/2512.02441
- Reference count: 40
- Primary result: Few-shot classification accuracy, strongest in 1-4 shot regimes

## Executive Summary
BOLT addresses few-shot and test-time adaptation for large pre-trained models by constructing orthogonal, task-informed spectral bases from existing fine-tuned models and adapting new tasks via learning only diagonal coefficients in this shared subspace. This approach avoids costly meta-training and model merging, while constraining updates to a low-dimensional, task-informed subspace. On few-shot classification across general and remote-sensing datasets, BOLT achieves the highest average accuracy across multiple backbones and shot levels, with particularly strong gains in the smallest data regimes. In out-of-distribution robustness tests, it delivers the best performance on all four ImageNet variants. In label-free test-time adaptation, BOLT consistently outperforms baselines on general-domain datasets, showing strong transfer without supervised fine-tuning. Overall, BOLT demonstrates that orthogonal, task-informed spectral coordinates provide a simple, scalable, and effective mechanism for parameter-efficient adaptation to unseen tasks.

## Method Summary
BOLT uses a two-phase approach: offline basis construction and online adaptation. In the offline phase, task vectors (fine-tuned model weights minus pre-trained weights) are extracted from source models, decomposed via SVD to capture dominant singular directions, stacked across tasks, and orthogonalized via whitening to create layer-wise spectral bases. These bases serve as a shared subspace for adaptation. In the online phase, new tasks are adapted by learning only diagonal coefficients in this subspace, initialized via pooling and global scaling from source tasks. This enables parameter-efficient adaptation with minimal training data.

## Key Results
- Highest average few-shot classification accuracy across general and remote-sensing datasets, especially in 1-4 shot regimes
- Best out-of-distribution robustness on all four ImageNet variants (A, R, S, V2)
- Consistently outperforms baselines in label-free test-time adaptation on general-domain datasets
- Achieves strong performance with only ~8k learnable parameters (rank ≤ 12 per layer)

## Why This Works (Mechanism)

### Mechanism 1: Task-Informed Orthogonal Subspace Construction
Constraining adaptation to a subspace spanned by prior task directions reduces interference and provides a compact, transferable coordinate system. SVD extracts dominant singular directions from each task vector; stacking and orthogonalizing via whitening yields layer-wise bases that decorrelate update axes across tasks. Core assumption: Task vectors from diverse source tasks span directions useful for unseen target tasks; dominant singular directions capture most transferable structure.

### Mechanism 2: Diagonal-Only Spectral Adaptation
Learning only diagonal coefficients in the shared basis achieves effective adaptation with minimal parameters while controlling update rank. Given frozen bases, the update is Δ = U_orth diag(s) V_orth^T. The optimal diagonal is s = diag(U^T M V), yielding rank(Δ) ≤ r with only r learnable scalars per layer. Core assumption: Off-diagonal terms in the projected coefficient matrix S are negligible or can be discarded without substantial information loss.

### Mechanism 3: Pooled Coefficient Initialization
Averaging diagonal coefficients from source tasks provides a strong, training-free starting point for unseen tasks. Project each source task into shared basis → extract diagonals → pool via averaging → apply global scaling selected by brief validation sweep. Core assumption: Unseen tasks lie near the mean of source task coefficient distributions.

## Foundational Learning

- **Singular Value Decomposition (SVD)**: Core to extracting dominant directions from task vectors; enables low-rank approximation and orthogonal basis construction. Quick check: Given a matrix M, can you explain why keeping top-k singular vectors approximates M while discarding smaller singular values?
- **Task Vectors and Weight-Space Arithmetic**: BOLT represents fine-tuned models as Θ_0 + Δ_i; understanding this decomposition is essential for basis construction. Quick check: What does a task vector Δ_i = Θ_i - Θ_0 represent, and why might averaging task vectors cause interference?
- **Orthogonalization / Whitening**: Transforms correlated singular directions into decorrelated basis; ensures Frobenius norm preservation in diagonal extraction. Quick check: Why does whitening (Z(Z^T Z)^{-1/2}) produce orthonormal columns from a stacked matrix?

## Architecture Onboarding

- **Component map**: Task vector extraction → per-layer SVD → stack singular vectors → orthogonalize via whitening → store {U_orth^(ℓ), V_orth^(ℓ)} → load bases → project/pool source diagonals → global α sweep → initialize s^(ℓ) → optimize s^(ℓ) via AdamW while bases stay frozen
- **Critical path**: Quality and diversity of source fine-tuned models directly determines basis expressiveness; rank r per layer controls capacity vs. overfitting tradeoff (ablation shows saturation at r ≈ 12); global scaling α must be selected before adaptation; wrong α can destabilize early learning
- **Design tradeoffs**: Higher r → more expressive but more parameters and potential overfitting in low-data regimes; more source tasks → better basis coverage but diminishing returns (Figure 6b shows plateau); diagonal-only vs. full coefficient matrix: diagonal sacrifices off-diagonal structure for extreme parameter efficiency
- **Failure signatures**: Accuracy no better than zero-shot: basis may not span target task; check source task diversity; performance degrades with more rank: overfitting in few-shot setting; reduce r; large variance across seeds: initialization scale α may be inappropriate; expand sweep range
- **First 3 experiments**: 1) Basis sanity check: On a held-out source task, verify reconstruction quality ||M - U diag(s) V^T||_F / ||M||_F is low (should be < 0.2 for top-k directions); 2) Rank ablation: Sweep r ∈ {4, 8, 12, 16} on 4-shot setting; confirm performance saturates rather than monotonically increasing; 3) Initialization comparison: Compare pooled+α initialization vs. random s initialization; quantify gap in epoch-1 accuracy to validate training-free benefit

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions.

## Limitations
- Performance depends critically on source task diversity; degrades when target tasks are distributionally distant from source tasks
- Diagonal-only adaptation may systematically underfit tasks requiring significant off-diagonal structure in the shared coordinates
- All experiments use CLIP vision encoders; generalization to other architectures (CNNs, language models) is unverified

## Confidence
- **High**: Task-informed orthogonal subspace construction; diagonal-only spectral adaptation; rank-constrained parameter efficiency
- **Medium**: Pooled coefficient initialization benefit; source task diversity requirements
- **Low**: Generalization bounds for unseen tasks; exact impact of layer selection on performance

## Next Checks
1. Basis generalization test: Hold out one source task, construct basis from remaining sources, measure performance drop on held-out task to quantify basis coverage limits
2. Pool vs. learned initialization: Compare pooled+α initialization against learned initialization from scratch on 1-shot setting; measure early-epoch performance gap to validate training-free benefit
3. Source task diversity ablation: Construct basis from increasingly diverse (but unrelated) source tasks and measure performance on a fixed target; identify threshold where basis becomes detrimental