---
ver: rpa2
title: 'Synthetic Data Blueprint (SDB): A modular framework for the statistical, structural,
  and graph-based evaluation of synthetic tabular data'
arxiv_id: '2512.19718'
source_url: https://arxiv.org/abs/2512.19718
tags:
- distance
- 'null'
- synthetic
- data
- divergence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The Synthetic Data Blueprint (SDB) framework provides a unified,
  modular approach to evaluating synthetic tabular data across statistical, structural,
  and graph-based dimensions. It addresses the fragmentation in current evaluation
  practices by integrating distribution-level metrics (KS, JSD, KLD), dependency measures
  (correlation, mutual information), and topological analyses (embedding similarity,
  graph structure) into a single pipeline.
---

# Synthetic Data Blueprint (SDB): A modular framework for the statistical, structural, and graph-based evaluation of synthetic tabular data

## Quick Facts
- **arXiv ID:** 2512.19718
- **Source URL:** https://arxiv.org/abs/2512.19718
- **Reference count:** 40
- **Primary result:** A unified evaluation framework measuring synthetic tabular data fidelity across statistical, dependency, and topological dimensions

## Executive Summary
The Synthetic Data Blueprint (SDB) addresses the fragmentation in synthetic data evaluation by providing a comprehensive, modular framework that assesses fidelity across multiple dimensions. Through a four-module pipeline—Data Quality Assessment, Statistical and Feature Embedding, Structural and Topological Assessment, and Reporting—SDB evaluates synthetic tabular data using 20 distinct metrics spanning distribution-level (KS, JSD, KLD), dependency (correlation, mutual information), and structural (embedding similarity, graph metrics) measures. Tested across healthcare, socioeconomic/financial, and cybersecurity domains, the framework demonstrates its ability to capture nuanced generator behavior and ensure practical utility in critical applications.

## Method Summary
SDB implements a four-module pipeline for synthetic data evaluation. The Data Quality Assessment (DQA) module automatically detects feature types and validates schema alignment. The Statistical and Feature Embedding (SFE) module computes distribution-level metrics (KS, JSD, KLD) and dependency measures (correlation, mutual information) for mixed data types. The Structural and Topological Assessment (STA) module generates embeddings via PCA/encoding, constructs kNN graphs, and calculates topological metrics including Neighborhood Overlap, Spectral Distance, and Graph Structural Fidelity Score. The framework outputs structured JSON reports with both local (per-feature) and global metrics, supported by automated visualizations. Configuration is managed through YAML files specifying input paths, parameters, and output directories.

## Key Results
- Demonstrated global correlation differences below 0.06 across evaluated domains
- Achieved high neighborhood overlap and near-perfect categorical coverage
- Revealed that comprehensive evaluation must combine multiple metric types to capture generator behavior
- Validated framework across three real-world domains: healthcare, socioeconomic/financial, and cybersecurity

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A unified, multi-layered evaluation pipeline exposes generator pathologies that isolated metrics miss.
- **Mechanism:** The framework sequentially applies distribution-level (marginal), dependency (correlation/MI), and topological (graph/manifold) metrics. By aggregating these into a single JSON report, it correlates surface-level statistical alignment with deep structural fidelity.
- **Core assumption:** High-quality synthetic data requires fidelity across all three axes (distribution, dependency, topology), and a failure in one axis invalidates success in another.
- **Evidence anchors:** [abstract] mentions integrating "distribution-level metrics... dependency measures... and topological analyses into a single pipeline." [section 4] states "no single metric is sufficient for validating synthetic data... fidelity must be assessed along multiple axes."

### Mechanism 2
- **Claim:** Graph-theoretic metrics derived from kNN embeddings capture non-linear structural preservation and "neighborhood" consistency.
- **Mechanism:** The framework projects tabular data into a latent space, constructs k-nearest neighbor graphs for real and synthetic data, and computes overlap (Jaccard) and spectral distance (Laplacian eigenvalues). This detects if the synthetic data maintains the "shape" of the original data manifold.
- **Core assumption:** The geometric arrangement of data points in a reduced embedding space is a proxy for complex multivariate relationships and cluster structures relevant to downstream tasks.
- **Evidence anchors:** [section 2.4.2] defines Neighborhood Overlap and Spectral Distance as methods to "capture subtle distortions such as mode collapse." [abstract] highlights "topological analyses (embedding similarity, graph structure)."

### Mechanism 3
- **Claim:** Automated feature-type detection enables the correct application of mathematically distinct metrics for mixed data types.
- **Mechanism:** The DQA module automatically classifies columns as continuous, binary, or multi-categorical. This preconditioning ensures that appropriate metrics are applied, preventing "apples-to-oranges" comparison errors.
- **Core assumption:** Correct metric application relies on accurate schema inference, and generic application of continuous metrics to categorical data yields meaningless fidelity scores.
- **Evidence anchors:** [section 2.2] DQA focuses on "automated identification of feature... types." [section 2.3.1] details distinct metrics for numerical vs. categorical features.

## Foundational Learning

- **Concept:** **Divergence Metrics (KLD vs. JSD)**
  - **Why needed here:** Understanding the difference between asymmetric (KLD) and symmetric (JSD) divergence is critical for interpreting why a synthetic distribution might look "close" in one direction but not the other.
  - **Quick check question:** If your synthetic data covers only a subset of the real data's modes, which metric (KLD or JSD) would more severely penalize the missing information, and why does symmetry matter here?

- **Concept:** **Graph Laplacians & Spectral Distance**
  - **Why needed here:** The STA module uses the eigenvalues of graph Laplacians to measure global topology. You need to grasp that this measures the "connectivity" and "shape" of the data clusters, not just density.
  - **Quick check question:** If two datasets have identical histograms but one has shuffled cluster memberships, would the Spectral Distance likely catch this when standard correlation metrics might not?

- **Concept:** **Centered Kernel Alignment (CKA)**
  - **Why needed here:** CKA is used to measure similarity between representation matrices (embeddings) independent of rotation or scale. It is a core "Embedding-Based Metric" in Section 2.4.1.
  - **Quick check question:** Why is CKA preferred over simple cosine similarity when comparing two high-dimensional embedding matrices that may have undergone orthogonal transformations (like PCA rotations)?

## Architecture Onboarding

- **Component map:** config.yaml -> DQA (feature detection, schema validation) -> SFE (distribution/dependency metrics) -> STA (embeddings -> kNN graphs -> topological metrics) -> JSON Report + Visualizations
- **Critical path:** The DQA Module is the critical dependency. If feature types are misclassified or schema alignment fails between real/synthetic sets, the downstream metric computation will either crash or produce mathematically invalid results.
- **Design tradeoffs:**
  - **Modularity vs. Complexity:** The framework is highly modular, allowing custom metrics, but this requires the user to manage the pipeline execution and configuration manually.
  - **Global vs. Local:** It balances global metrics (Correlation Matrix Distance) with local metrics (Neighborhood Overlap), potentially creating conflicting signals that require expert interpretation.
- **Failure signatures:**
  - High Distribution Fidelity / Low Structural Fidelity: Synthetic data matches histograms but breaks relationships.
  - High KLD / Low JSD: Indicates specific "tail" events or rare categories are missing.
  - Low Neighborhood Overlap: Indicates "Mode Collapse" or that the generator is hallucinating clusters that don't exist in real data.
- **First 3 experiments:**
  1. **Sanity Check (Identity):** Run SDB comparing the Real dataset against itself. Verify all fidelity metrics are maxed to establish a baseline.
  2. **Naive Baseline (Marginals):** Generate synthetic data by sampling each column independently (shuffling). Run SDB to verify high distribution fidelity but low dependency/topological fidelity.
  3. **Hyperparameter Sensitivity:** Run SDB on a high-quality synthetic set vs. a "noisy" synthetic set (added Gaussian noise). Observe the degradation slope of Spectral Distance vs. Wasserstein Distance.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the SDB architecture be extended to support non-tabular data modalities such as time-series, longitudinal records, and graph-structured data?
- **Basis in paper:** [explicit] The authors state in Section 5 that they plan to "extend SDB's core architecture to support additional data modalities, including time-series data, longitudinal records, graph-structured data, and multimodal tabular-text hybrids."
- **Why unresolved:** The current framework is specifically designed and validated for static tabular datasets, lacking the temporal or topological components necessary for dynamic or graph data.
- **What evidence would resolve it:** A demonstration of the SDB pipeline successfully evaluating synthetic time-series or graph data using adapted or novel fidelity metrics.

### Open Question 2
- **Question:** Can formal privacy risk metrics, such as membership inference attacks or differential privacy bounds, be effectively integrated into the SDB evaluation pipeline?
- **Basis in paper:** [explicit] Section 5 notes that future work aims to "integrate more explicit privacy risk quantification tools, including adversarial membership inference tests... and domain-aware privacy scorecards."
- **Why unresolved:** The current framework relies on distance-based privacy diagnostics which may not capture complex privacy leakage scenarios as effectively as adversarial attacks.
- **What evidence would resolve it:** The successful inclusion of a privacy module that outputs quantitative scores for resistance to membership inference and attribute inference attacks.

### Open Question 3
- **Question:** How does SDB's fidelity assessment perform when applied to synthetic data generated by advanced generative models such as diffusion models or foundation models?
- **Basis in paper:** [explicit] Section 5 highlights the need for "validation of the SDB across a wider spectrum of domains and generative models, such as... diffusion-based tabular models... and foundation-model-driven synthetic data frameworks."
- **Why unresolved:** The framework was demonstrated solely using a Bayesian Gaussian Mixture Model (BGMMOCE) generator; its ability to detect subtle artifacts in data from deep generative models remains unverified.
- **What evidence would resolve it:** A comparative study showing SDB's ability to distinguish fidelity differences in datasets generated by GANs, VAEs, and Diffusion models.

### Open Question 4
- **Question:** To what extent does the choice of representation model bias the Structural and Topological Assessment (STA) module's fidelity scores?
- **Basis in paper:** [inferred] Section 4 notes that "Embedding-based evaluation introduces dependence on representation models, which may vary in quality, particularly for highly categorical datasets."
- **Why unresolved:** The framework uses specific embedding techniques (PCA, TF-IDF) to assess structure, but the sensitivity of the resulting CKA or AWED scores to the choice of these encoders is not quantified.
- **What evidence would resolve it:** An ablation study comparing CKA and Neighborhood Overlap scores across different embedding strategies on the same synthetic datasets.

## Limitations

- The graph-based metrics (NO, SD, GSFS) depend on unspecified k-NN parameters and PCA dimensionality choices that could significantly affect structural fidelity scores.
- The BGMMOCE generator implementation details remain inaccessible, limiting reproducibility of the exact experimental conditions.
- The framework relies heavily on automated feature-type detection without validating this detection against ground truth schemas for ambiguous data types.

## Confidence

- **High Confidence:** The modular pipeline architecture and JSON reporting format are clearly specified and reproducible.
- **Medium Confidence:** Distribution-level metrics (KS, JSD, KLD) and their appropriate application to mixed data types are well-founded.
- **Low Confidence:** Graph-theoretic structural metrics and their sensitivity to hyperparameter choices (k-NN k, PCA components) are not fully specified.

## Next Checks

1. **Baseline Identity Test:** Run SDB comparing real dataset against itself to verify all metrics reach expected maximums (correlation diff=0, NO=1, CC=1.0).
2. **Parameter Sensitivity Analysis:** Systematically vary k-NN k values and PCA component counts to measure impact on NO, SD, and GSFS scores.
3. **Type Detection Validation:** Create test datasets with ambiguous types (integer-encoded categories, mixed numerical/text features) to verify DQA classification accuracy against manual schema.