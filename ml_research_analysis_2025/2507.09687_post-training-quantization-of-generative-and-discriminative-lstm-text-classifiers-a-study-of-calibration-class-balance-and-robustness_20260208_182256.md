---
ver: rpa2
title: 'Post-Training Quantization of Generative and Discriminative LSTM Text Classifiers:
  A Study of Calibration, Class Balance, and Robustness'
arxiv_id: '2507.09687'
source_url: https://arxiv.org/abs/2507.09687
tags:
- quantization
- generative
- calibration
- data
- class
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a systematic comparison of post-training quantization
  (PTQ) for generative and discriminative LSTM text classifiers, highlighting the
  critical role of calibration data class balance. Generative classifiers show strong
  robustness to input noise in full-precision settings but are highly sensitive to
  quantization, especially at low bitwidths and with class-imbalanced calibration
  data.
---

# Post-Training Quantization of Generative and Discriminative LSTM Text Classifiers: A Study of Calibration, Class Balance, and Robustness

## Quick Facts
- arXiv ID: 2507.09687
- Source URL: https://arxiv.org/abs/2507.09687
- Reference count: 40
- Primary result: Class-balanced calibration significantly improves generative LSTM classifier robustness under post-training quantization, while discriminative models remain stable across calibration strategies.

## Executive Summary
This paper systematically compares post-training quantization (PTQ) for generative and discriminative LSTM text classifiers, emphasizing the role of calibration data class balance. Generative classifiers demonstrate strong robustness to input noise in full-precision settings but are highly sensitive to quantization, especially at low bitwidths and with class-imbalanced calibration data. The study finds that class-balanced calibration substantially improves generative classifier performance under PTQ, while discriminative models remain robust across both calibration strategies. Quantized generative classifiers exhibit sharp accuracy drops under noise at lower bitwidths, underscoring a trade-off between compression and robustness. These findings emphasize the need for class-aware calibration in deploying quantized generative models on resource-constrained edge devices.

## Method Summary
The study compares post-training quantization of single-layer LSTM text classifiers for generative and discriminative tasks. Both models use 100 hidden units and 100-dimensional embeddings, trained on AG News and DBPedia datasets. PTQ is applied using Brevitas with symmetric per-tensor quantization, percentile-based activation calibration (99.99th percentile), and Greedy Path-Following Quantization (GPFQ) on the final linear layer. Calibration uses 25% of training data, with class-conditional (balanced) vs. class-unconditional (random) sampling. Noise robustness is evaluated via character substitution at varying levels (ε ∈ [0, 0.1]). The study measures accuracy, accuracy drop vs. FP32 baseline, and KS statistics for weight/activation distribution shifts.

## Key Results
- Generative classifiers show superior full-precision noise robustness but sharp accuracy degradation under low-bit quantization, especially with class-imbalanced calibration.
- Class-balanced calibration significantly improves generative classifier PTQ performance, while discriminative models remain stable across calibration strategies.
- Quantized generative classifiers exhibit sharp accuracy drops under noise at lower bitwidths, highlighting a trade-off between compression and robustness.

## Why This Works (Mechanism)
The study's methodology works because it isolates the impact of calibration data class balance on PTQ performance by controlling for model architecture, dataset, and quantization parameters. By comparing class-conditional vs. class-unconditional calibration strategies, it reveals that generative models are particularly sensitive to activation distribution mismatches during quantization. The use of KS statistics to quantify distribution shifts provides a mechanistic explanation for performance differences, while noise injection tests reveal the practical robustness implications of quantization choices.

## Foundational Learning
- **Post-training quantization (PTQ)**: Method to compress neural networks after training without full retraining; needed because it enables efficient deployment on edge devices with limited resources. Quick check: Verify PTQ reduces model size and maintains acceptable accuracy on validation data.
- **Greedy Path-Following Quantization (GPFQ)**: Optimization technique for quantizing linear layers by finding optimal bitwidth per layer; needed because it improves quantization accuracy compared to uniform bitwidth assignment. Quick check: Compare accuracy with and without GPFQ at low bitwidths.
- **Class-conditional calibration**: Sampling calibration data to maintain class balance; needed because it ensures activation distributions represent all classes equally during quantization. Quick check: Verify calibration data class distribution matches training data.
- **KS statistics**: Non-parametric test measuring distribution similarity; needed because it quantifies activation distribution shifts between calibrated and uncalibrated models. Quick check: Compute KS statistics between weight distributions before and after calibration.
- **Brevitas framework**: PyTorch library for quantization-aware training and PTQ; needed because it provides tools for symmetric quantization and activation calibration. Quick check: Confirm Brevitas implementation matches specified quantization parameters.
- **Character-level noise injection**: Technique to test model robustness by randomly substituting characters; needed because it simulates real-world text corruption and tests model resilience. Quick check: Verify noise injection maintains character distribution statistics.

## Architecture Onboarding

**Component Map:**
Input Text -> Tokenizer -> Embedding Layer -> LSTM Layer -> Linear Layer -> (GPFQ) -> Output

**Critical Path:**
The critical path for PTQ performance is: Input Text → Tokenizer → Embedding → LSTM → Linear Layer, with GPFQ applied to the Linear Layer. The LSTM layer's internal gating and recurrence make it challenging for GPFQ, which is why it's only applied to the final linear layer.

**Design Tradeoffs:**
- Single-layer LSTM vs. deeper architectures: Simpler models are easier to quantize but may sacrifice accuracy.
- Percentile-based vs. entropy-based calibration: Percentile-based (99.99th) is more conservative but may be less adaptive to activation distribution changes.
- Class-conditional vs. class-unconditional calibration: Class-balanced calibration improves generative model robustness but requires additional data processing.

**Failure Signatures:**
- Generative classifier accuracy collapse at 5-bit and below with class-unconditional calibration: Indicates activation distribution mismatch during GPFQ adjustment.
- Sharp accuracy drop under noise at low bitwidths for generative models: Suggests degraded likelihood estimates due to quantization errors.
- High KS values (>0.15) between class-conditional and unconditional activation distributions: Indicates significant activation mismatch causing poor GPFQ adjustment.

**First Experiments:**
1. Implement discriminative and generative LSTM classifiers with 100 hidden units, 100-dim embeddings, batch 32, Adam lr=0.001 until validation convergence.
2. Apply Brevitas PTQ with static weight quantization, percentile-based activation calibration using 25% of training data, GPFQ on final linear layer. Test class-conditional vs. class-unconditional calibration at 8/7/6/5/4/3-bit.
3. Evaluate accuracy, then inject character-level noise (random substitution at ε = 0, 0.002, 0.004, ..., 0.1) and measure robustness. Compute KS statistics between calibrated and pre-calibrated weights/activations.

## Open Questions the Paper Calls Out

**Open Question 1:**
Can GPFQ be effectively extended to recurrent LSTM layers to improve quantization fidelity, given that it currently only applies to the final linear layer?
- Basis in paper: "While Greedy Path-Following Quantization (GPFQ) has been effective for linear layers, its application to recurrent structures such as LSTM remains unexplored due to their internal gating and temporal dependencies."
- Why unresolved: Brevitas does not currently support GPFQ for LSTM layers due to their internal recurrence and gating operations.
- What evidence would resolve it: Development and empirical evaluation of GPFQ-adapted algorithms for LSTM layers, comparing accuracy retention under low bitwidth quantization.

**Open Question 2:**
Do transformer-based text classifiers exhibit similar sensitivity to calibration data class imbalance under PTQ as observed in LSTM-based generative classifiers?
- Basis in paper: "While our study focuses on LSTM-based models, similar investigations are needed for transformer-based text classifier architectures, which are increasingly used in edge NLP tasks."
- Why unresolved: This study only evaluated LSTM architectures; transformer behavior under PTQ with class-imbalanced calibration remains untested.
- What evidence would resolve it: Comparative PTQ experiments on transformer classifiers (e.g., BERT variants) using class-balanced vs. class-imbalanced calibration data.

**Open Question 3:**
Can hybrid calibration techniques that adaptively sample based on layer-wise quantization error profiles improve generative classifier robustness under low-bit quantization?
- Basis in paper: "Future research should also explore hybrid calibration techniques that adaptively sample calibration data based on layer-wise quantization error profiles or activation distribution shifts."
- Why unresolved: Current work used fixed sampling strategies; adaptive approaches could better target sensitive modules but remain unexplored.
- What evidence would resolve it: Development of adaptive calibration algorithms and comparison of activation distribution fidelity and accuracy against fixed sampling methods.

**Open Question 4:**
How do quantized generative and discriminative classifiers perform on actual edge hardware under real-world latency, power, and thermal constraints?
- Basis in paper: "Finally, deploying these quantized models on actual edge devices with hardware-in-the-loop evaluation will provide further insight into their real-world efficiency and stability."
- Why unresolved: All experiments were conducted in simulation; hardware-specific effects like quantized inference latency remain unmeasured.
- What evidence would resolve it: On-device benchmarking measuring inference time, memory footprint, and power consumption across bitwidths and noise conditions.

## Limitations
- The study uses single-layer LSTM architectures, which may not generalize to deeper or more complex text models commonly used in production.
- Training dynamics for the generative model are not fully specified, particularly label embedding dimension and exact convergence criteria.
- Vocabulary size and tokenization specifics are not provided, potentially affecting sequence length distributions and embedding space geometry.

## Confidence
- **High Confidence**: The core finding that class-balanced calibration improves generative classifier PTQ robustness is well-supported by the experimental design and results.
- **Medium Confidence**: The claim that generative models show superior full-precision noise robustness but poor quantized robustness is supported, though the exact mechanism could benefit from deeper analysis.
- **Low Confidence**: The assertion that discriminative models remain robust across calibration strategies lacks exploration of edge cases and may not generalize beyond studied datasets and architecture.

## Next Checks
1. Verify the impact of label embedding dimension (dℓ) on generative model calibration sensitivity by testing configurations where dℓ equals vs. differs from token embedding size (100).
2. Test extreme calibration class imbalance scenarios (e.g., 90/10 vs. 50/50 class ratios) to determine if discriminative model robustness holds under pathological conditions.
3. Implement and compare alternative activation calibration methods (e.g., entropy-based vs. percentile-based) to assess whether the observed generative model sensitivity is specific to the percentile approach used in the study.