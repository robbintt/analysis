---
ver: rpa2
title: 'Towards Multimodal Metaphor Understanding: A Chinese Dataset and Model for
  Metaphor Mapping Identification'
arxiv_id: '2501.02434'
source_url: https://arxiv.org/abs/2501.02434
tags:
- domain
- metaphor
- target
- source
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of understanding multimodal
  metaphors by proposing a Chinese dataset (CM3D) and a Chain-of-Thought Prompting-based
  Metaphor Mapping Identification Model (CPMMIM). CM3D contains 6,108 annotated text-image
  pairs from Chinese advertisements, focusing on identifying specific target and source
  domains in metaphors.
---

# Towards Multimodal Metaphor Understanding: A Chinese Dataset and Model for Metaphor Mapping Identification

## Quick Facts
- arXiv ID: 2501.02434
- Source URL: https://arxiv.org/abs/2501.02434
- Reference count: 40
- Key outcome: Proposes CM3D dataset and CPMMIM model achieving 36.23% target domain accuracy and 28.47% source domain accuracy on Chinese metaphor mapping identification.

## Executive Summary
This paper addresses the challenge of understanding multimodal metaphors by proposing a Chinese dataset (CM3D) and a Chain-of-Thought Prompting-based Metaphor Mapping Identification Model (CPMMIM). CM3D contains 6,108 annotated text-image pairs from Chinese advertisements, focusing on identifying specific target and source domains in metaphors. CPMMIM uses a Bi-Level Optimization framework and Chain-of-Thought prompting to simulate human cognitive processes for metaphor mapping. Experiments show that CPMMIM outperforms existing baselines, demonstrating the effectiveness of the proposed approach in advancing metaphor comprehension in NLP.

## Method Summary
The CPMMIM model employs a two-stage Bi-Level Optimization framework where target domain identification (Lower-Level) informs source domain identification (Upper-Level). The model uses Qwen-VL to generate intermediate reasoning features (purpose and incongruity descriptions) via Chain-of-Thought prompting, which are then concatenated with text and fused with image features through a gated attention mechanism. The MMI module combines BART-large-chinese for text encoding with ViT-L/16 for image encoding, using a single-head attention mechanism with sigmoid-based gating to dynamically weight multimodal contributions before decoding.

## Key Results
- CPMMIM achieves 36.23% accuracy for target domain identification and 28.47% for source domain identification
- Multimodal models outperform unimodal baselines by 12.79 H-E score points for target identification
- Chain-of-Thought prompting provides significant performance gains, confirmed through ablation studies

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Dependency via Bi-Level Optimization (BLO)
The model improves mapping identification by treating the task as a nested hierarchy where source domain identification (Upper-Level) is conditioned on the optimal solution of target domain identification (Lower-Level). This mirrors cognitive theory that the source domain is mapped onto the target domain.

### Mechanism 2: Knowledge Injection via Chain-of-Thought (CoT) Prompting
Injecting intermediate reasoning steps (contextual knowledge) from a large Multimodal LLM into a smaller specialized model allows for better handling of semantic obscurity in metaphors. The system uses Qwen-VL to generate purpose and incongruity descriptions that guide the smaller model's attention.

### Mechanism 3: Gated Multimodal Fusion
Combining visual and textual features through a gating mechanism filters noise and isolates the metaphorical mapping more effectively than unimodal approaches. The Sigmoid-based gate dynamically weighs text features versus attended image features, preventing one modality from dominating erroneously.

## Foundational Learning

- **Concept: Conceptual Metaphor Theory (CMT)**
  - Why needed: Theoretical basis for distinguishing Target (actual subject) from Source (metaphorical vehicle)
  - Quick check: In "Time is money," which is Target and which is Source?

- **Concept: Bi-Level Optimization (BLO)**
  - Why needed: CPMMIM architecture is structured as nested optimization loop
  - Quick check: In BLO, does Upper-Level depend on Lower-Level, or vice versa?

- **Concept: Chain-of-Thought (CoT) Prompting**
  - Why needed: Model generates intermediate context before final prediction
  - Quick check: Why might intermediate reasoning steps improve complex reasoning accuracy?

## Architecture Onboarding

- **Component map:** Image + Text -> LLM Processor (Qwen-VL) -> MMI Encoder (BART+ViT) -> Gated Attention Fusion -> Decoder
- **Critical path:** Stage 1 (Target) → Stage 2 (Source)
  1. Stage 1: LLM generates Purpose → Concat to Text → Train MMI for Target
  2. Stage 2: Construct Prompt using Target → LLM generates Incongruity → Concat to Text → Train MMI for Source
- **Design tradeoffs:** Two LLM passes per sample increases inference latency; model is language-specific (requires backbone swap for English)
- **Failure signatures:** Cultural hallucination (generic descriptions instead of symbolic meanings), domain confusion (swapping Target/Source)
- **First 3 experiments:**
  1. Modality ablation: Test Text-Only, Image-Only vs Multimodal
  2. Prompt dependency: Train MMI on raw data without LLM context
  3. Target vs Source difficulty: Compare accuracy metrics between identification tasks

## Open Questions the Paper Calls Out

- Can CPMMIM generalize to multimodal metaphors outside advertising domain? (Dataset specificity limitation)
- How to improve multimodal models for culturally specific symbols? (Error analysis identified cultural misinterpretations)
- What methodologies can close performance gap between target (36.23%) and source (28.47%) identification? (Explicitly stated as more challenging)

## Limitations

- Data domain specificity: CM3D constructed exclusively from Chinese advertisements, limiting generalizability
- Hierarchical assumption: BLO framework may be overly restrictive for co-dependent metaphors
- Cultural dependency: Poor performance on culturally-specific symbols indicates heavy LLM knowledge reliance

## Confidence

**High Confidence (80-100%):**
- Architecture functions as described and outperforms baselines
- Two-stage optimization framework is implementable
- Dataset construction methodology is reproducible

**Medium Confidence (50-80%):**
- Performance improvements primarily due to CoT prompting
- Model generalizes beyond Chinese advertisements
- Exact match accuracy captures meaningful understanding

**Low Confidence (0-50%):**
- Model develops genuine metaphor comprehension
- Approach transfers to other languages without changes
- Performance gap reflects true cognitive difficulty

## Next Checks

1. Domain transfer experiment: Evaluate CPMMIM on non-advertising metaphor dataset (literary/political discourse)
2. Prompt ablation study: Systematically remove CoT components to isolate reasoning benefits
3. Cultural knowledge audit: Systematic error analysis on culturally-specific symbols comparing Chinese vs non-Chinese annotators