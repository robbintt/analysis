---
ver: rpa2
title: 'EthicAlly: a Prototype for AI-Powered Research Ethics Support for the Social
  Sciences and Humanities'
arxiv_id: '2508.00856'
source_url: https://arxiv.org/abs/2508.00856
tags:
- research
- ethics
- ethical
- ethically
- review
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: EthicAlly is a prototype AI system supporting ethics review for
  social science and humanities research. It uses constitutional AI and a collaborative
  prompt methodology to provide structured ethics assessments incorporating universal
  principles and contextual considerations.
---

# EthicAlly: a Prototype for AI-Powered Research Ethics Support for the Social Sciences and Humanities

## Quick Facts
- arXiv ID: 2508.00856
- Source URL: https://arxiv.org/abs/2508.00856
- Authors: Steph Grohmann
- Reference count: 0
- Primary result: AI system correctly identified target ethical issues in 96% of fictional research proposals

## Executive Summary
EthicAlly is a prototype AI system designed to support ethics review for social science and humanities research by providing structured ethics assessments. The system uses constitutional AI technology (specifically Claude Sonnet 4) combined with a collaborative prompt development methodology to analyze research proposals against universal principles and contextual considerations. Testing on 25 fictional research proposals showed the system correctly identified target ethical issues in nearly all cases, demonstrating sophisticated ethical understanding. The tool aims to help researchers navigate ethics review processes and reduce burden on Research Ethics Committees (RECs) without automating human oversight.

## Method Summary
EthicAlly is built as an API-based system using Claude Sonnet 4 with a structured prompt that incorporates user inputs (discipline, country, proposal text) along with explicit ethics frameworks (Nuremberg Code, Belmont Report, Declaration of Helsinki) and essential/contextual principles. The prompt was co-created with Claude itself, leveraging the model's implicit knowledge of its own processing constraints. The system generates a 1-5 ethics risk score with detailed recommendations and materials assessment. The methodology combines constitutional AI training with comprehensive upfront context scaffolding, distinguishing it from approaches that rely on the model to infer context.

## Key Results
- Correctly identified target ethical issues in 24 out of 25 (96%) fictional research proposals
- System demonstrated sophisticated ethical understanding across diverse research scenarios
- Successfully avoided false positives while maintaining high detection rates
- Robust to attempts to deceive through simple language manipulation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Constitutional AI training provides superior alignment for ethics assessment tasks compared to standard RLHF approaches.
- Mechanism: Claude's constitutional AI explicitly encodes ethical principles (UN Declaration of Human Rights, anti-bias safeguards, non-Western perspectives) into the training objective, creating an inherent disposition toward balanced ethical reasoning rather than relying solely on pattern-matching from generic pre-training data.
- Core assumption: The constitutional training process successfully transfers general ethical dispositions to domain-specific research ethics tasks without requiring additional fine-tuning.
- Evidence anchors: [abstract] "Drawing on constitutional AI technology and a collaborative prompt development methodology"; [section] Page 8-9 describes Claude's constitution containing principles from UN Declaration of Human Rights; [corpus] Weak direct evidence—neighbor papers discuss LLM applications in social science but do not specifically address constitutional AI for ethics.

### Mechanism 2
- Claim: AI-collaborative prompt design produces more effective scaffolding than human-intuitive prompting alone.
- Mechanism: The LLM itself generated the initial prompt structure based on its understanding of what context and formatting it needs for consistent, coherent analysis. This leverages the model's implicit knowledge of its own processing constraints and optimal input structures.
- Core assumption: The model possesses functionally equivalent meta-cognitive capability to anticipate its own failure modes and optimal operating conditions.
- Evidence anchors: [abstract] "collaborative prompt development methodology" is cited as core to the system; [section] Pages 10-12 detail how Claude Sonnet 4 wrote the startup prompt; [corpus] No direct corpus support for this specific mechanism.

### Mechanism 3
- Claim: Single comprehensive prompts with explicit context scaffolding outperform fragmented or inferential prompting strategies.
- Mechanism: EthicAlly provides discipline, geography, regulatory context, and explicit ethics frameworks upfront in one structured prompt, eliminating the need for the model to guess context. This contrasts with approaches that ask the model to role-play without specifying parameters.
- Core assumption: Explicit context reduces variance and improves consistency by constraining the model's interpretation space.
- Evidence anchors: [abstract] "structured ethics assessment incorporating universal principles and contextual considerations"; [section] Page 12-13 contrasts with Sridharan and Sivaramakrishnan's approach; [corpus] "Rethinking LLM Bias Probing" (FMR 0.58) discusses problems with conflicting probe results and lack of principled frameworks.

## Foundational Learning

- Concept: Constitutional AI vs. RLHF
  - Why needed here: Understanding why Claude was selected over other frontier models requires distinguishing between reinforcement learning from human feedback (standard approach) and Anthropic's principle-based constitutional training that explicitly encodes ethical behavior.
  - Quick check question: Can you explain why constitutional AI might produce more consistent ethical reasoning than models trained primarily on preference rankings?

- Concept: Context scaffolding for LLMs
  - Why needed here: The core innovation is providing comprehensive upfront context rather than relying on the model to infer disciplinary norms, regulatory environments, and ethics frameworks.
  - Quick check question: What three types of context does EthicAlly require users to provide before analysis begins?

- Concept: Non-determinism and hallucination in LLMs
  - Why needed here: The paper explicitly flags these as limitations that prevent using AI for formal REC decisions; understanding these failure modes is essential for appropriate system positioning.
  - Quick check question: Why does non-determinism pose specific problems for ethics review documentation and audit trails?

## Architecture Onboarding

- Component map: Web interface (front end) → User input fields (discipline, country, proposal text, supplementary materials) → API call with structured prompt → Claude Sonnet 4 → Structured ethics report (disclaimer, compliance analysis, issues/recommendations, risk score 1-5, materials assessment)

- Critical path: Prompt engineering is the core lever—the system's intelligence lives in the structured prompt that combines user inputs with explicit instructions about ethics frameworks (Nuremberg Code, Belmont Report, Declaration of Helsinki), essential principles (informed consent, beneficence, respect for persons, confidentiality, conflict of interest, social justice), and contextual principles (reflexivity, cultural sensitivity, trauma-informed approaches, political economy considerations).

- Design tradeoffs: Proprietary frontier model (Claude) provides sophisticated reasoning but creates privacy concerns, sustainability risks, and black-box opacity; Open Source future version would trade some capability for transparency and data control. Current API-based architecture is simple but dependent on Anthropic's continued availability and pricing.

- Failure signatures: (1) Over-caution on legitimate sensitive research due to content moderation policies baked into constitutional training; (2) Missed edge cases in novel ethical scenarios not represented in training data; (3) User over-reliance treating AI assessment as equivalent to human REC review; (4) Adversarial gaming where users learn to avoid trigger language.

- First 3 experiments:
  1. Run the same proposal through EthicAlly 10 times to measure variance in risk scores and recommendations—establish non-determinism baseline.
  2. Test with human-generated proposals (not AI-generated) to validate whether the AI-testing-AI methodology inflated performance estimates.
  3. Implement a chat modality and test whether allowing follow-up questions improves user outcomes compared to single-report mode.

## Open Questions the Paper Calls Out

- How does EthicAlly's performance compare to human-only ethics preparation methods across diverse research scenarios?
  - Basis in paper: [explicit] "systematic validation studies are needed, including comparison with human-only preparation methods"
  - Why unresolved: Testing to date used only fictional AI-generated proposals evaluated by the system; no controlled comparison with human ethics assessment has been conducted.
  - What evidence would resolve it: A controlled study comparing EthicAlly-assisted and unassisted researchers on quality of ethics preparation and REC submission outcomes.

- Can EthicAlly reliably parse and apply non-Western ethical frameworks?
  - Basis in paper: [explicit] "its capacity to parse non-Western conceptions of ethics specifically remains to be established"
  - Why unresolved: The system relies on Western-derived frameworks (Nuremberg Code, Belmont Report, Declaration of Helsinki) and was tested primarily in English and German.
  - What evidence would resolve it: Testing with proposals rooted in non-Western ethical traditions and evaluation by experts in those frameworks.

- Can adversarial users successfully "game" EthicAlly by avoiding language that triggers ethical flags?
  - Basis in paper: [explicit] "further testing with more sophisticated adversarial attempts is required"
  - Why unresolved: Preliminary tests showed robustness to simple deception, but sophisticated attempts to manipulate the system remain unexplored.
  - What evidence would resolve it: Red-team testing with researchers explicitly attempting to obscure unethical research through careful wording.

## Limitations
- Testing methodology relies entirely on AI-generated fictional proposals rather than real research submissions, potentially inflating performance estimates
- Lack of specification for API parameters (temperature, max tokens, top_p) makes exact reproduction impossible without empirical tuning
- The condensation process that reduced the prompt for token limitations is not documented, representing a substantial methodological gap

## Confidence
- **High confidence**: The core mechanism of constitutional AI providing ethical reasoning dispositions is well-supported by Anthropic's documented training approach and the explicit inclusion of UN Declaration principles in Claude's constitution.
- **Medium confidence**: The collaborative prompt methodology shows promise but lacks direct validation against human-designed prompts; the claim that AI-generated prompts outperform human intuition requires comparative testing.
- **Low confidence**: The performance metrics on fictional proposals cannot be generalized to real-world applications without testing on actual research submissions from diverse disciplines.

## Next Checks
1. Deploy EthicAlly on 50 actual research proposals submitted to real RECs (with appropriate permissions) and compare AI assessments against human review outcomes to establish real-world accuracy and identify systematic blind spots.
2. Conduct 100 runs of the same 10 proposals over 30 days to quantify non-determinism patterns, measure variance in risk scores, and establish acceptable tolerance thresholds for REC use.
3. Design a randomized controlled trial where 30 researchers use EthicAlly versus 30 who receive standard ethics guidance, measuring time-to-submission, number of revisions, and REC approval rates to quantify actual burden reduction.