---
ver: rpa2
title: 'Tell me why: Visual foundation models as self-explainable classifiers'
arxiv_id: '2502.19577'
source_url: https://arxiv.org/abs/2502.19577
tags:
- prototypes
- prototype
- metrics
- interpretability
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ProtoFM, a novel approach to building self-explainable
  visual classifiers by combining frozen visual foundation models (VFMs) with a prototypical
  architecture. The key innovation is a lightweight head (~1M parameters) trained
  on top of frozen VFMs that achieves both competitive classification accuracy and
  superior interpretability.
---

# Tell me why: Visual foundation models as self-explainable classifiers

## Quick Facts
- arXiv ID: 2502.19577
- Source URL: https://arxiv.org/abs/2502.19577
- Reference count: 26
- Key outcome: ProtoFM achieves state-of-the-art interpretability (mean explainability score of 0.92) while maintaining classification accuracy competitive with non-explainable baselines

## Executive Summary
ProtoFM introduces a novel approach to building self-explainable visual classifiers by combining frozen visual foundation models (VFMs) with a prototypical architecture. The method uses a lightweight head (~1M parameters) trained on top of frozen VFMs, achieving both competitive classification accuracy and superior interpretability. By leveraging patch-level objectives from pre-trained VFMs and introducing specialized consistency losses, ProtoFM addresses common interpretability challenges like spatial misalignment and semantic inconsistency in prototypes. Evaluations on multiple datasets demonstrate that the approach successfully provides faithful explanations that decompose predictions into weighted sums of interpretable concepts.

## Method Summary
ProtoFM builds self-explainable classifiers by freezing a pre-trained Visual Foundation Model (e.g., DINOv2) and training a lightweight projector head with a prototypical architecture. The model uses a student-teacher training scheme where the teacher processes augmented views while the student processes original images. Multiple specialized losses ensure consistent prototype assignments across augmentations while maintaining strong local feature representations. The architecture includes a prototype bank that maps image patches to interpretable concepts, with classification performed through positive-weight linear combinations of prototype activations. This design enables both accurate predictions and faithful explanations that highlight the specific image regions contributing to each decision.

## Key Results
- Achieves state-of-the-art interpretability scores (mean explainability score of 0.92) across multiple datasets
- Maintains classification accuracy competitive with non-explainable baselines and surpassing other prototypical models
- Demonstrates superior performance on fine-grained datasets (CUB, CARS, PETS, FunnyBirds, RSNA)
- Successfully addresses spatial misalignment and semantic inconsistency challenges common in prototypical networks

## Why This Works (Mechanism)

### Mechanism 1: VFM Spatial Feature Leveraging
Leveraging frozen Visual Foundation Models (VFMs) with patch-level objectives appears to resolve spatial misalignment issues common in prototypical networks. The mechanism posits that because VFMs are trained for dense prediction tasks, their patch embeddings inherently contain strong local semantic information, allowing the lightweight projector to map these "high-quality" patches to prototypes with higher spatial fidelity. The core assumption is that the frozen VFM's pre-trained patch embeddings are semantically aligned with the downstream task's fine-grained concepts without further fine-tuning.

### Mechanism 2: Correspondence Distillation for Consistency
Distilling the feature correlation structure of the backbone into the prototype assignment space acts as a regularizer to ensure semantic consistency and prevent model collapse. An alignment loss explicitly enforces that the correlation matrix of the prototype assignments matches the correlation matrix of the backbone features, tying the cluster structure of the prototypes to the dense feature geometry of the VFM. The core assumption is that the geometric structure of the VFM's feature space represents a valid "ground truth" for how concepts should be grouped.

### Mechanism 3: Contrastive Assignment Consistency
A student-teacher framework with contrastive learning enforces stable prototype assignment across augmentations. The model uses an EMA teacher to process an augmented view while the student processes the original, with a contrastive loss combined with an assignment loss forcing the same patches to be assigned to the same prototypes in both views. The core assumption is that semantic concepts are invariant to the augmentations applied, and if a patch changes prototype assignment due to an augmentation, it constitutes a failure of interpretability.

## Foundational Learning

- **Concept: Prototypical Part Networks (ProtoPNet)**
  - Why needed here: ProtoFM modifies the standard prototype pipeline. Understanding that traditional prototypes often struggle with "semantic gaps" (a prototype for a "beak" activating on a "wheel") is essential to see why ProtoFM introduces specific consistency losses.
  - Quick check question: Can you explain why a standard prototype might be inconsistent (e.g., activating on different parts for different images)?

- **Concept: Vision Transformers (ViT) & Patch Embeddings**
  - Why needed here: The mechanism relies on the "frozen VFM" outputting a grid of patch embeddings. Understanding that the image is split into patches (tokens) is essential for visualizing how "local features" are extracted and mapped to prototypes.
  - Quick check question: If the input image is $224\times224$ and the patch size is $14$, what is the dimension of the feature map grid used for prototype comparison?

- **Concept: Stop-Gradient & EMA (Exponential Moving Average)**
  - Why needed here: The student-teacher mechanism relies on the Teacher being an EMA of the Student and gradients being stopped to prevent collapse. Without this concept, the training loop looks like a standard supervised setup rather than a self-supervised alignment task.
  - Quick check question: Why do we update the Teacher weights using an EMA of the Student rather than standard backpropagation?

## Architecture Onboarding

- **Component map:** Frozen VFM (e.g., DINOv2) → Projector → Prototypes (N=300) → Positive-weight Linear Classifier

- **Critical path:** The RoIAlign step between Student and Teacher branches is critical. It aligns the similarity maps (masks) of two differently augmented views so they can be compared via the Assignment Loss. If this alignment fails, consistency training breaks.

- **Design tradeoffs:**
  - Freezing Backbone: Drastically reduces training parameters (~1M) and compute, but risks lower absolute accuracy compared to full fine-tuning if the domain is very distinct from the VFM's training data.
  - Positive Classification Weights: Constraining the final linear layer to positive weights improves interpretability (additive explanation) but may limit the model's ability to express "absence of a feature" as negative evidence.

- **Failure signatures:**
  - Model Collapse: All patches assigned to a single prototype. Usually caused by missing or misconfigured Alignment Loss or excessive Assignment Loss weight.
  - Semantic Drift: Prototypes representing mixed concepts (e.g., "eye" and "wheel" in same prototype). Check Stability metric; likely caused by insufficient contrastive loss weight.
  - Trivial Prototypes: Prototypes activating on background/bias (e.g., text in images). Sparsity loss may be too low.

- **First 3 experiments:**
  1. Baseline Linearity: Train a linear head on top of DINOv2 (frozen) to verify the backbone's baseline performance on the target dataset.
  2. Ablation Alignment: Train ProtoFM without the correspondence distillation loss to observe the model collapse or consistency drop described in Appendix C.
  3. Visualization Check: Run a forward pass on a validation sample and visualize the "Score Sheet" (top-k patches for top prototypes) to manually verify spatial alignment before running full interpretability benchmarks.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can textual descriptions be effectively integrated into ProtoFM to semantically ground the learned prototypes without compromising the current interpretability metrics?
- Basis in paper: The conclusion states, "a natural next step would be incorporating textual descriptions to further clarify the concepts utilized by the model."
- Why unresolved: The current model relies solely on visual similarity to define prototypes, which leaves the semantic meaning of the concept open to user interpretation.
- What evidence would resolve it: A modified architecture capable of generating text labels for prototypes that align with human expert annotations, evaluated on semantic consistency metrics.

### Open Question 2
- Question: Does the constraint of a frozen Visual Foundation Model (VFM) backbone limit the discovery of domain-specific features necessary for high-stakes medical diagnosis?
- Basis in paper: While the model is competitive on general datasets, it shows a performance gap on the RSNA pneumonia dataset (AUROC 86.1) compared to the non-explainable linear probing baseline (88.4).
- Why unresolved: The paper does not determine if this gap results from the prototypical head's simplicity or the frozen backbone's inability to adapt to the subtle, high-frequency features often present in radiology.
- What evidence would resolve it: An ablation study comparing ProtoFM against a version with a fine-tuned backbone on medical imaging datasets to measure the feature adaptability trade-off.

### Open Question 3
- Question: Can the spatial consistency of prototypes be improved to match state-of-the-art levels without sacrificing the model's high stability and correctness?
- Basis in paper: The results show ProtoFM achieves high stability (0.99) and correctness but significantly lower consistency (0.65) compared to PIP-Net (1.0).
- Why unresolved: The authors suggest the "exemplar" approach causes this drop, but it remains unclear if the architecture inherently trades off consistency for the robustness provided by the frozen VFM features.
- What evidence would resolve it: Experimentation with hybrid loss functions that penalize semantic inconsistency across samples, observing if the mean explainability score can be maintained or improved.

## Limitations

- Implementation details critical for exact reproduction are not specified, including projector architecture, augmentation parameters, and temperature settings
- Performance on non-fine-grained classification tasks remains untested, raising questions about generalizability
- Computational efficiency gains may be offset by the student-teacher training framework's computational cost during training
- Prototypes may drift to background features (e.g., text watermarks) without sufficient sparsity regularization

## Confidence

**High Confidence (Level 4-5):**
- The core mechanism of using frozen VFMs with patch-level objectives to resolve spatial misalignment issues
- The effectiveness of the correspondence distillation loss in preventing model collapse and improving consistency
- The overall architecture design and multi-loss training framework
- The interpretability benchmark results and comparison with state-of-the-art methods

**Medium Confidence (Level 3):**
- The specific contribution of each loss term to overall performance (requires full ablation studies)
- The scalability of the approach to very large prototype banks (>300)
- The robustness of prototype consistency across diverse augmentation strategies

## Next Checks

**Check 1: Implementation Verification** - Re-implement the exact alignment loss with adaptive shifting parameters (k_shift=0.1, v_shift=3) and verify it prevents the model collapse described in Appendix C when trained without these components.

**Check 2: Ablation Study** - Conduct a systematic ablation study isolating the contribution of each loss term (Assignment, Alignment, Contrastive, Sparsity) to quantify their individual impact on classification accuracy versus interpretability metrics.

**Check 3: Cross-Domain Generalization** - Test ProtoFM on non-fine-grained classification tasks (e.g., CIFAR-10, ImageNet) and datasets with significant domain shift from the VFM's pre-training data to evaluate robustness beyond the paper's evaluation scope.