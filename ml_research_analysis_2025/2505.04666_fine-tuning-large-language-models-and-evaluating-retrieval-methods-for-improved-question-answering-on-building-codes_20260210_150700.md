---
ver: rpa2
title: Fine-Tuning Large Language Models and Evaluating Retrieval Methods for Improved
  Question Answering on Building Codes
arxiv_id: '2505.04666'
source_url: https://arxiv.org/abs/2505.04666
tags:
- retrieval
- language
- arxiv
- fine-tuning
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated retrieval methods and fine-tuned language
  models for building code question answering. Elasticsearch outperformed other retrievers
  in accuracy, while fine-tuning with LoRA significantly improved model performance,
  with Mistral-Small-24b-Instruct-2501 achieving the highest F1 score (0.688) and
  Llama-3.1-8b showing the largest relative improvement (58.59% F1 gain).
---

# Fine-Tuning Large Language Models and Evaluating Retrieval Methods for Improved Question Answering on Building Codes

## Quick Facts
- arXiv ID: 2505.04666
- Source URL: https://arxiv.org/abs/2505.04666
- Reference count: 40
- Primary result: Fine-tuned Mistral-Small-24b-Instruct-2501 achieved highest F1 (0.688) on building code QA; Llama-3.1-8b showed largest relative improvement (58.59% F1 gain).

## Executive Summary
This study evaluates retrieval methods and fine-tuning strategies for building code question answering using the National Building Code of Canada. The authors find that Elasticsearch (BM25) outperforms dense retrievers for this domain-specific task, and that LoRA-based fine-tuning significantly improves model performance. Mistral-Small-24b-Instruct-2501 achieved the highest F1 score (0.688), while Llama-3.1-8b demonstrated the largest relative improvement (58.59% F1 gain). The research demonstrates that combining effective retrieval with fine-tuned models can optimize question-answering systems for building codes.

## Method Summary
The study employs a RAG pipeline with NBCC documents indexed in Elasticsearch using BM25 ranking. A dataset of 1,436 CQA triplets was generated using InternVL2_5-8B from NBCC PDFs. Llama-3.1-8b and other models were fine-tuned using LoRA adapters on attention and feedforward projections with 4-bit quantization. Models were trained for 10 epochs with batch size 2, learning rate 2e-4, and evaluated using standard metrics including F1, BLEU, ROUGE-1, METEOR, BERTScore, and SMS.

## Key Results
- Elasticsearch outperformed dense retrievers (DPR, S-BERT) in precision and recall for building code retrieval
- LoRA fine-tuning improved model performance across most architectures, with Llama-3.1-8b showing 101.14% improvement in BLEU score
- Mistral-Small-24b-Instruct-2501 achieved highest F1 score of 0.688
- Top-3 or Top-5 context chunks were sufficient; retrieving more than top-5 documents provided diminishing returns

## Why This Works (Mechanism)

### Mechanism 1: Lexical Matching in High-Stakes Domains
Elasticsearch utilizes BM25 ranking with inverted indices and term frequency normalization, allowing precise matching of regulatory terminology where dense embeddings might blur specific numerical distinctions. This works when relevant answers share significant lexical overlap with query terms. Evidence shows ES outperformed other retrievers in accuracy, particularly for technical content requiring high precision. Break condition occurs when queries use conversational language or synonyms not present in source text.

### Mechanism 2: Low-Rank Adaptation (LoRA) for Domain Alignment
LoRA injects trainable rank-decomposition matrices into transformer layers while freezing pre-trained weights, shifting the model's generation distribution toward building code "legalese" while preserving general reasoning capabilities. This works when domain shift requires adjusting attention mechanisms without unlearning general language knowledge. Evidence shows Llama-3.1-8b improved by 101.14% in BLEU score. Break condition occurs with small/noisy datasets causing overfitting to specific phrasing.

### Mechanism 3: Constrained Context Injection
Limiting retrieval to top-3 or top-5 chunks optimizes RAG by maximizing relevant signal while minimizing noise distraction for the LLM. This works when the retriever is accurate enough that correct answers are likely within first 3-5 ranks. Evidence shows diminishing returns after Top-5 for NBCC queries. Break condition occurs when queries require synthesizing information from multiple disparate clauses ranking lower.

## Foundational Learning

- **Concept:** BM25 Ranking Algorithm
  - Why needed here: Essential for understanding why Elasticsearch beat modern neural retrievers on this specific task through Term Frequency (TF) and Inverse Document Frequency (IDF) rather than vector similarity
  - Quick check question: Why would a search for "fire resistance" favor a document mentioning it 5 times over one mentioning it 50 times in a much larger text?

- **Concept:** Parameter-Efficient Fine-Tuning (PEFT/LoRA)
  - Why needed here: To understand how the authors improved Llama-3.1-8b by >58% F1 without retraining billions of parameters through learning deltas added to frozen weights
  - Quick check question: If LoRA freezes the main model weights $W$, how does the model learn new information? (Answer: It learns a delta $\Delta W = B \times A$ added to the forward pass)

- **Concept:** Quantization (4-bit)
  - Why needed here: The paper mentions using 4-bit quantization to fit models on an A6000 GPU, a prerequisite for practical implementation
  - Quick check question: What is the trade-off risk when compressing a 16-bit model to 4-bit for inference on regulatory tasks?

## Architecture Onboarding

- **Component map:** NBCC PDF -> InternVL (Vision Model) -> CQA Triplets -> Elasticsearch Index -> BM25 Scoring -> Top-K Context -> Llama-3.1/Mistral (4-bit quantized) + LoRA Adapters -> Response

- **Critical path:**
  1. Pre-process PDFs using MLLM (InternVL) to create golden dataset
  2. Index raw text into Elasticsearch
  3. Train LoRA adapters on CQA dataset
  4. Query ES -> Pass Top-3 Chunks to Fine-Tuned LLM

- **Design tradeoffs:**
  - Absolute vs. Relative Performance: Mistral-24b had highest absolute score (0.688), but Llama-3.1-8b had highest improvement gain
  - Sparse vs. Dense: Choosing ES (Sparse) over DPR (Dense) trades semantic understanding for exact term matching reliability

- **Failure signatures:**
  - Semantic Overfitting: BERT Recall drop (as seen with Phi-3/Qwen) indicates task-specific overfitting
  - Retrieval Misses: Failure on synonym queries (e.g., "stairs" vs "stairway") confirms sparse retriever limitations

- **First 3 experiments:**
  1. Retriever Baseline: Index NBCC, run 50 queries, compare Top-3 ES vs Semantic results manually
  2. LoRA Ablation: Fine-tune Llama-3.1-8b with ranks 8, 16, 32; check F1 improvement vs overfitting
  3. Context Window Sizing: Feed Top-1 vs Top-5 vs Top-10 context for fixed questions; plot F1 to verify diminishing returns

## Open Questions the Paper Calls Out

- **Question:** How does inclusion of non-textual features (images, tables) affect multimodal model performance vs text-only approaches?
  - Basis: Conclusion suggests future work should include images/tables and multi-modality
  - Why unresolved: Study used text-only LLMs after preprocessing to remove pages not needed for text extraction
  - Evidence needed: Comparative study using multimodal LLM (e.g., LLaVA) fine-tuned on dataset retaining visual structures

- **Question:** Can advanced late-interaction retrieval architectures (Col-Pali) outperform Elasticsearch lexical matching on building code documents?
  - Basis: Conclusion suggests testing advanced retrievers like Col-Pali
  - Why unresolved: Elasticsearch was top performer but relies on inverted indices; newer models might better capture visual layout
  - Evidence needed: Benchmark comparing retrieval accuracy of Col-Pali against Elasticsearch baseline

- **Question:** How does RAG system performance change when evaluated using RAG-specific metrics (faithfulness, context relevance) vs generation-focused metrics used in this study?
  - Basis: Authors note whole RAG setup can be tested using advanced RAG-specific evaluation metrics
  - Why unresolved: Current evaluation focuses on answer similarity but doesn't measure context efficiency or hallucination
  - Evidence needed: Re-evaluation using RAGAS or TruLens to score context precision and faithfulness

- **Question:** What modifications to fine-tuning configuration prevent performance regressions in semantic metrics for Qwen-2.5 and Phi-3?
  - Basis: Results show Qwen-2.5-7b and Phi-3-Mini-4k exhibited minor performance declines in semantic metrics
  - Why unresolved: Uniform LoRA configuration applied to all models, but negative results suggest smaller models need specific tuning
  - Evidence needed: Ablation study varying LoRA hyperparameters specifically for underperforming models

## Limitations
- Small training dataset (991 samples) for fine-tuning may lead to overfitting despite LoRA's parameter-efficient approach
- Optimal LoRA configuration parameters (rank, alpha, dropout) are not fully specified
- Choice of Elasticsearch may not generalize to other building code domains or languages requiring semantic understanding
- Dataset preprocessing steps and exact prompt for InternVL triplet generation not fully detailed

## Confidence

- **High Confidence:** Superiority of Elasticsearch for this specific NBCC dataset (supported by direct performance metrics showing improved precision/recall over DPR and S-BERT)
- **Medium Confidence:** Effectiveness of LoRA fine-tuning for domain adaptation (supported by significant F1 improvements, though with some metric inconsistencies)
- **Medium Confidence:** Recommendation for Top-3/Top-5 context windows (based on observed diminishing returns, dependent on retriever accuracy)

## Next Checks

1. **Retrieval Generalization Test:** Run same retrieval pipeline on conversational question set (synonyms, paraphrased terms) to validate Elasticsearch consistently outperforms dense retrievers across query types

2. **LoRA Configuration Sensitivity:** Systematically vary LoRA rank parameters (8, 16, 32) and measure trade-off between performance gains and overfitting indicators (training vs validation loss gap)

3. **Context Window Stress Test:** Create test cases requiring cross-referencing multiple non-adjacent code sections; measure F1 scores using Top-1, Top-3, and Top-5 contexts to verify diminishing returns claim holds for multi-hop reasoning