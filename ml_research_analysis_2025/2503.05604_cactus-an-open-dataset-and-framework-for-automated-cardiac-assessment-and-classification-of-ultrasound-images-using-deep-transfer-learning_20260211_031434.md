---
ver: rpa2
title: 'CACTUS: An Open Dataset and Framework for Automated Cardiac Assessment and
  Classification of Ultrasound Images Using Deep Transfer Learning'
arxiv_id: '2503.05604'
source_url: https://arxiv.org/abs/2503.05604
tags:
- cardiac
- images
- classification
- grading
- framework
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first publicly available graded dataset
  for Cardiac Assessment and Classification of Ultrasound (CACTUS), containing images
  from scanning a CAE Blue Phantom representing various heart views and quality levels.
  The authors propose a deep learning framework using transfer learning to classify
  cardiac ultrasound images based on heart views and assess their quality.
---

# CACTUS: An Open Dataset and Framework for Automated Cardiac Assessment and Classification of Ultrasound Images Using Deep Transfer Learning

## Quick Facts
- arXiv ID: 2503.05604
- Source URL: https://arxiv.org/abs/2503.05604
- Reference count: 40
- Primary result: 99.43% classification accuracy and 0.3067 grading error on phantom cardiac ultrasound images

## Executive Summary
This paper introduces CACTUS, the first publicly available graded dataset for cardiac ultrasound assessment, comprising images from a CAE Blue Phantom representing six heart views and quality levels. The authors propose a dual-task deep learning framework using transfer learning to classify cardiac views and grade image quality. A shared ResNet18 encoder with two task-specific heads enables efficient inference, achieving up to 99.43% classification accuracy and grading errors as low as 0.3067. The framework demonstrates robustness through fine-tuning on additional cardiac views and receives positive expert feedback in real-time evaluation.

## Method Summary
The framework uses a two-stage transfer learning approach with a ResNet18 backbone. First, a classification head is trained to categorize images into six cardiac views (A4C, PL, SC, PSAV, PSMV, Random). Then, the encoder is frozen and a grading head is trained to predict image quality on a 0-10 scale using MSE loss. The shared architecture reduces computational overhead compared to separate models. Images are preprocessed by cropping machine overlays, resizing to 224x224, and normalizing. The model is trained with SGD (learning rate 0.001, batch size 128) for 30 epochs.

## Key Results
- Achieved 99.43% classification accuracy on test set
- Grading error as low as 0.3067 (MSE) on validation set
- Fine-tuning on new views maintained high performance with minimal additional training
- Real-time evaluation by cardiac experts showed satisfactory classification, grading confidence, responsiveness, and usefulness

## Why This Works (Mechanism)

### Mechanism 1: Transfer Learning from Classification to Grading
Freezing a pre-trained classification encoder and training only a grading head leverages shared visual features, reducing training parameters while maintaining performance. The classification model learns hierarchical features (edges → structures → cardiac views) via ResNet18's convolutional layers. These features are frozen, and a fully-connected regression head is added to predict quality grades (0–10). Only the final layer weights update during grading training.

### Mechanism 2: Shared Encoder Dual-Head Architecture
A single ResNet18 encoder with two task-specific heads reduces inference time and FLOPs compared to separate models. Both classification (softmax over 6 classes) and grading (regression output) heads attach to the same pooled features. During inference, one forward pass generates both outputs.

### Mechanism 3: Expert-Defined Grading Schema as Ground Truth
A 0–10 scale combining completeness (structure visibility) and clarity (noise/speckle levels) provides learnable supervision for regression. Cardiovascular imaging experts manually grade each phantom image. The regression model minimizes MSE between predicted and expert grades.

## Foundational Learning

- **Concept: Transfer Learning in CNNs**
  - Why needed here: Core technique enabling the classification→grading pipeline. Without TL, two full models would require 2× parameters and training time.
  - Quick check question: Explain why freezing early convolutional layers preserves general edge/texture detectors while allowing task-specific adaptation in later layers.

- **Concept: Regression vs. Classification Heads**
  - Why needed here: Grading is a regression task (continuous 0–10), not classification. MSE loss, not cross-entropy, drives training.
  - Quick check question: What activation function should the final layer use for bounded regression to [0,10]? (Answer: Sigmoid scaled ×10 or linear with gradient clipping.)

- **Concept: Phantom Data as Surrogate for Clinical Data**
  - Why needed here: CACTUS uses CAE Blue Phantom, not human patients. Understanding limitations (static valves, no pathological variation) is critical for deployment context.
  - Quick check question: Name two potential domain-shift issues when moving from phantom to real patient echocardiograms. (Answer: Real tissue speckle patterns differ; dynamic valve motion absent in phantom; breathing artifacts; patient variability in anatomy.)

## Architecture Onboarding

- **Component map:**
  Input (224×224×3 normalized US image) → ResNet18 encoder → Feature vector (512-dim) → Classification head (6-class softmax) + Grading head (scalar regression output)

- **Critical path:**
  1. Data preprocessing (crop machine overlays → resize → normalize) is essential; raw images contain irrelevant text/parameters.
  2. Classification training converges by epoch 7–10; grading head training requires 20+ epochs due to finer discrimination.
  3. TL approach preferred over MTL for this task (grading loss 0.1077 vs. 9.916).

- **Design tradeoffs:**
  - ResNet18 vs. deeper (ResNet50): Marginal accuracy gain (99.0% vs. 100%) but higher FLOPs. ResNet18 chosen for efficiency.
  - Phantom vs. real data: Enables open dataset without patient privacy concerns, but limits clinical generalizability.
  - Single-grade regression vs. multi-dimensional quality scoring: Simpler annotation but conflates clarity and completeness.

- **Failure signatures:**
  - High grading loss (>1.0) on validation: Check label noise, inter-rater consistency, or feature-quality correlation.
  - Misclassification of low-quality images as "Random": Model has not learned to separate "poor view" from "no view"—consider explicit low-grade class or thresholding.
  - PSAV confused with Random: Static phantom lacks dynamic valve signature; real data needed.

- **First 3 experiments:**
  1. **Baseline replication**: Train ResNet18 classification on CACTUS (5 views + Random), report accuracy/F1. Target: >99% as reported.
  2. **TL vs. scratch for grading**: Compare grading head trained from (a) frozen classification encoder vs. (b) randomly initialized encoder. Expect lower MSE for (a).
  3. **Cross-view generalization**: Train on A4C/PL/SC/PSAV, fine-tune on PSMV with only 100 samples. Measure accuracy drop vs. full-data training to assess few-shot adaptability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the proposed framework maintain high classification and grading performance when applied to real-time dynamic human heart scans rather than static phantom images?
- Basis in paper: The authors state, "we aim to extend our approach to real-time analysis of dynamic human heart scans, moving beyond static cardiac phantom imaging."
- Why unresolved: The current CACTUS dataset is derived entirely from scanning a static CAE Blue Phantom, which lacks the movement and variability of live human hearts.
- What evidence would resolve it: Validation of the fine-tuned model on a dataset of real human TTE videos showing comparable accuracy and grading error rates.

### Open Question 2
- Question: Can domain adaptation techniques effectively mitigate negative transfer when extending the model to diverse phantoms or human subjects?
- Basis in paper: The conclusion notes, "the risk of negative transfer... remains a crucial issue, particularly when dealing with domain shifts. To mitigate domain shifts, we plan to employ domain adaptation techniques."
- Why unresolved: Transfer learning (TL) performance often degrades when the source domain (static phantom) differs significantly from the target domain (human tissue or different phantom anatomies).
- What evidence would resolve it: A comparative study showing that adversarial training or self-supervised learning reduces the performance gap (domain shift) compared to standard TL methods.

### Open Question 3
- Question: Does the inclusion of dynamic valve structures in training data significantly improve the classification accuracy of the Parasternal Short Axis - Aortic Valve (PSAV) view?
- Basis in paper: The authors note that detecting PSAV was challenging due to the "static nature of the cardiac phantom," suggesting that "acquiring a phantom with dynamic valves would be a valuable approach."
- Why unresolved: The current model misclassifies PSAV as random or other views because the static phantom lacks the distinct movement signatures usually required to identify valve views.
- What evidence would resolve it: A decrease in the misclassification rate of PSAV views in the confusion matrix after training on data acquired from a phantom with moving valve components.

## Limitations
- The dataset relies entirely on phantom data (CAE Blue Phantom) rather than real patient images, which limits clinical generalizability due to absent pathological variation, dynamic valve motion, and patient-specific anatomical diversity.
- The grading rubric combines completeness and clarity into a single 0-10 score, but the paper does not report inter-rater agreement statistics, raising concerns about label noise in the regression target.
- No direct comparison is made with separate single-task models for classification and grading to quantify the actual computational savings of the shared encoder approach.

## Confidence

- **High confidence** in classification accuracy claims (99.43%) and grading loss (0.3067) based on reported validation metrics and clear methodology description.
- **Medium confidence** in the transfer learning mechanism's effectiveness, as the paper shows superior results versus multi-task learning but lacks comparison with separate single-task baselines.
- **Low confidence** in clinical applicability due to the phantom-only dataset and absence of real patient data validation.

## Next Checks
1. Measure inter-expert agreement on the grading rubric to establish label reliability before model training.
2. Train separate ResNet18 models for classification and grading to benchmark against the shared-encoder approach's computational efficiency claims.
3. Test the framework on a small set of real patient echocardiograms to assess domain adaptation requirements and identify specific failure modes not present in phantom data.