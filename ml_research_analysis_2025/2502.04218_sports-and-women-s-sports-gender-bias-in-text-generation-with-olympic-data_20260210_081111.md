---
ver: rpa2
title: 'Sports and Women''s Sports: Gender Bias in Text Generation with Olympic Data'
arxiv_id: '2502.04218'
source_url: https://arxiv.org/abs/2502.04218
tags:
- bias
- gender
- text
- event
- women
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work introduces a framework for quantifying gender bias in\
  \ language models using parallel men's and women's Olympic events. The study defines\
  \ three bias metrics\u2014knowledge-based, explicit-ambiguous, and implicit-ambiguous\u2014\
  to measure how models handle gender in sports-related factual questions."
---

# Sports and Women's Sports: Gender Bias in Text Generation with Olympic Data

## Quick Facts
- arXiv ID: 2502.04218
- Source URL: https://arxiv.org/abs/2502.04218
- Authors: Laura Biester
- Reference count: 25
- Primary result: Models exhibit implicit gender bias in ambiguous prompts, defaulting to male results despite similar knowledge accuracy for both men's and women's events.

## Executive Summary
This work introduces a framework for quantifying gender bias in language models using parallel men's and women's Olympic events. The study defines three bias metrics—knowledge-based, explicit-ambiguous, and implicit-ambiguous—to measure how models handle gender in sports-related factual questions. Using specified prompts (with gender) and underspecified prompts (without gender), the research finds that while models have similar knowledge accuracy for both men's and women's events, they exhibit significant implicit bias when gender is ambiguous, often defaulting to male results. Explicit bias is also observed in most models, favoring men's results when prompted ambiguously. However, one sport (artistic gymnastics) shows a reversal of this trend, favoring women's results, likely due to societal stereotypes and media coverage. The findings highlight the subtle ways bias manifests in LLMs and emphasize the need for careful consideration in model development and deployment.

## Method Summary
The study evaluates six instruction-tuned language models (GPT-4o/4o-mini, Llama3.1-8b/70b, Mistral-Nemo/Large) using Olympic team event data from 1988-2021. Two prompt templates are used: specified (including gender) and underspecified (omitting gender). Human annotators label generated outputs for medal countries and gender mentions, which are mapped to NOC codes. Three bias metrics are computed: knowledge-based (F1 difference for specified prompts), explicit-ambiguous (categorical gender mention scoring), and implicit-ambiguous (F1 comparison to male vs female ground truth). Statistical significance is assessed using permutation tests, binomial tests, and FDR correction.

## Key Results
- Models show no significant knowledge-based bias—similar F1 scores for men's and women's events when gender is specified
- Explicit-ambiguous bias exists in most models, favoring men's results in ambiguous prompts
- Implicit-ambiguous bias is significant, with models defaulting to male results when gender is unspecified
- Artistic gymnastics shows reversed bias (favoring women's results), likely due to cultural stereotypes and media coverage patterns

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs default to male results when gender is unspecified due to training corpus asymmetry.
- Mechanism: The model resolves ambiguity by selecting the more statistically frequent pattern in training data—men's sports coverage dominates media corpora, creating a prior that "sports" ≈ "men's sports" unless explicitly marked otherwise.
- Core assumption: Training data reflects documented media coverage disparities (men receive ~4x more sports coverage per Cooky et al. 2021, cited in paper).
- Evidence anchors:
  - [abstract] "models are consistently biased against women when the gender is ambiguous in the prompt"
  - [section 5] "we believe that the models often default to processing the prompt under the assumption that the user is asking about the men's event"
  - [corpus] Related work on masculine generics bias (arXiv:2502.10577) supports this default-male pattern in LLMs
- Break condition: If a domain has reversed media coverage patterns (e.g., women's gymnastics), the bias should reverse—which it does (artistic gymnastics: -0.32 bias score).

### Mechanism 2
- Claim: Knowledge retrieval accuracy is gender-equitable; bias emerges in generation choices, not knowledge gaps.
- Mechanism: Models encode both men's and women's results with similar fidelity (similar F1 when gender is specified), but generation preferences surface when prompts permit multiple valid completions.
- Core assumption: The F1 metric accurately captures knowledge quality independent of generation preferences.
- Evidence anchors:
  - [section 4.2.1] "knowledge-based bias metric as the difference in average F1 scores among male and female events"
  - [section 5] "lack of statistically significant scores for this metric indicate that LLMs are equally knowledgeable about men's and women's events"
  - [corpus] EuroGEST (arXiv:2506.03867) similarly distinguishes stereotypical reasoning from factual knowledge
- Break condition: If models had true knowledge gaps for women's sports, specified-prompt F1 would differ significantly by gender.

### Mechanism 3
- Claim: Contextual stereotypes modulate bias direction—domains culturally coded as "feminine" show reversed bias.
- Mechanism: Models learn domain-gender associations from training data; when a sport is stereotypically feminine (e.g., artistic gymnastics), the "default" shifts. This suggests bias is not uniform but context-sensitive.
- Core assumption: The gymnastics reversal is driven by media coverage patterns and cultural stereotypes rather than data artifacts.
- Evidence anchors:
  - [section 5, Table 2] "The notable outlier with a score of −.32 is artistic gymnastics; only 18.5% of scores across models and years are positive"
  - [section 5] "gymnastics has been classified among a small number of stereotypically feminine sports... historically been among the sports with a large percentage of television coverage devoted to women"
  - [corpus] Limited direct corpus support for sport-specific reversals; neighboring papers focus on general gender bias
- Break condition: If prompted about mixed-stereotype domains (e.g., swimming with Michael Phelps vs. Simone Biles fame), bias should vary by athlete prominence.

## Foundational Learning

- Concept: Zero-shot evaluation with instruction-tuned models
  - Why needed here: The methodology relies on prompting models without task-specific training; understanding how instruction tuning shapes response behavior is critical.
  - Quick check question: Why might instruction-tuned models behave differently than base models when handling ambiguous queries?

- Concept: F1 score for structured extraction
  - Why needed here: The paper uses F1 (harmonic mean of precision/recall) to measure correctness of medal predictions, penalizing both hallucinations and omissions.
  - Quick check question: If a model outputs 2 correct countries but hallucinates a 3rd, what happens to F1 vs. accuracy?

- Concept: Implicit vs. explicit bias measurement
  - Why needed here: The three-metric framework distinguishes overt favoritism (explicit-ambiguous) from subtler patterns visible only via ground-truth comparison (implicit-ambiguous).
  - Quick check question: Why is implicit bias potentially more harmful in downstream applications than explicit bias?

## Architecture Onboarding

- Component map: Prompt templates (specified/underspecified) → LLM inference (6 models) → Human annotation (medal spans + gender labels) → NOC mapping → Metric computation (3 bias scores + F1)

- Critical path: 1. Generate prompts from Olympic data (338 events, 1988-2021) 2. Run inference with default decoding parameters 3. Annotate generated text for medal countries and explicit gender mentions 4. Map annotations to NOC codes 5. Compute F1 and three bias metrics with permutation/binomial significance tests

- Design tradeoffs: Human annotation (high accuracy, low scalability) vs. automated extraction; Binary gender framework (matches Olympic data structure but excludes non-binary identities); Default decoding parameters (realistic user behavior) vs. tuned parameters (potentially different bias patterns)

- Failure signatures: Ambiguous country references ("Korean" → cannot map to PRK/KOR); Models denying event existence or giving results for wrong events; Doping disqualifications generating temporally unstable "correct" answers

- First 3 experiments: 1. Replicate on a single model (e.g., Llama3.1-8b) with 20 events to validate annotation pipeline before scaling 2. Test whether temperature/sampling parameters affect implicit-ambiguous bias magnitude 3. Extend to a non-sports domain with parallel gendered events (e.g., academic awards, political offices) to test generalizability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the alignment phase (e.g., RLHF) reduce or fail to address explicit gender bias in ambiguous prompts, and why does this bias persist in some models like GPT-4o but not others?
- Basis in paper: [explicit] The authors state: "We hypothesize that the alignment phase of training might lead models away from explicitly stating information about men and not women, but our results indicate that some explicit bias persists."
- Why unresolved: The study measures bias but does not isolate the contribution of alignment procedures or explain differential persistence across models.
- What evidence would resolve it: Controlled experiments comparing pre- and post-alignment model versions, and ablation studies on alignment data composition.

### Open Question 2
- Question: Can parallel event frameworks (like Olympic men's/women's events) be identified in non-athletic domains to measure gender bias in LLMs?
- Basis in paper: [explicit] The authors state: "We welcome future work that identifies other such parallel events that are not related to athletics and can be used to measure bias in LLMs."
- Why unresolved: The paper only explores sports data; generalization to other domains remains untested.
- What evidence would resolve it: Application of the same framework to parallel gendered events in other domains (e.g., academic awards, professional certifications) with comparable analysis.

### Open Question 3
- Question: Does varying decoding parameters (beyond defaults) affect the magnitude or direction of implicit gender bias in generated results?
- Basis in paper: [inferred] The authors acknowledge that their approach "is dependent on decoding parameters" but use defaults "to most closely mimic a realistic user experience," leaving the effect of parameter tuning unexplored.
- Why unresolved: The study deliberately holds decoding parameters constant to reflect typical usage.
- What evidence would resolve it: Systematic evaluation across varying temperature, top-p, and other generation settings to measure bias metric sensitivity.

## Limitations

- Binary gender framework limits generalizability to non-binary athletes and identities
- Annotation process involves subjective interpretation of implicit gender cues
- Findings may be specific to factual knowledge retrieval rather than broader language generation tasks

## Confidence

- High Confidence: Knowledge-based bias results showing no significant gender difference in F1 scores for specified prompts
- Medium Confidence: Explicit-ambiguous bias findings, particularly the reversal observed in artistic gymnastics
- Medium Confidence: Implicit-ambiguous bias results, as these depend on accurate annotation of gender mentions and correct NOC mapping

## Next Checks

1. Test bias patterns on individual Olympic sports (swimming, track) where media coverage may differ from team sports, and where individual athletes have varying levels of fame and gender representation.

2. Evaluate the same prompts with base models (non-instruction-tuned) to determine whether instruction tuning specifically contributes to the observed bias patterns, or if similar patterns emerge in pre-trained models.

3. Extend the framework to parallel gendered events in non-sports domains (academic awards, political positions) to assess whether the bias mechanisms generalize beyond sports contexts or are specific to the cultural framing of sports.