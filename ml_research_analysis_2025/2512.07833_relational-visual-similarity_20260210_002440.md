---
ver: rpa2
title: Relational Visual Similarity
arxiv_id: '2512.07833'
source_url: https://arxiv.org/abs/2512.07833
tags:
- image
- similarity
- relational
- images
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces relational visual similarity, a novel approach
  to image similarity that captures abstract relational structures beyond surface-level
  attributes. Unlike existing methods that focus on perceptual or semantic similarity,
  the proposed method uses anonymized captions to describe the underlying logic of
  images, enabling models to reason about relational patterns.
---

# Relational Visual Similarity

## Quick Facts
- **arXiv ID**: 2512.07833
- **Source URL**: https://arxiv.org/abs/2512.07833
- **Reference count**: 40
- **Key outcome**: relsim achieves GPT-4o score of 6.77 vs 5.91 for CLIP-I and 5.14 for DINO on relational similarity tasks

## Executive Summary
This paper introduces relational visual similarity, a novel approach to image similarity that captures abstract relational structures beyond surface-level attributes. Unlike existing methods that focus on perceptual or semantic similarity, the proposed method uses anonymized captions to describe the underlying logic of images, enabling models to reason about relational patterns. A dataset of 114k images with anonymous captions was curated and used to train a vision-language model (relsim) to measure relational similarity. Evaluations show that relsim outperforms existing similarity metrics, achieving a GPT-4o score of 6.77 compared to 5.91 for CLIP-I and 5.14 for DINO. User studies confirm human preference for relsim's relational understanding.

## Method Summary
The method introduces relational visual similarity by training a vision-language model to understand abstract relations between images. The process involves three main steps: (1) filtering LAION-2B to extract 114k "interesting" images using a fine-tuned Qwen2.5-VL-7B model, (2) manually curating 532 image groups with shared relational logic and generating anonymous captions with placeholders like {Object}, and (3) training relsim using contrastive alignment via InfoNCE loss to align image embeddings with anonymous caption embeddings. The resulting model captures relational patterns that existing methods miss, enabling applications like relational image retrieval and analogical image generation.

## Key Results
- relsim achieves GPT-4o score of 6.77 vs 5.91 for CLIP-I and 5.14 for DINO on relational similarity tasks
- User studies confirm human preference for relsim's relational understanding over existing methods
- relsim successfully transfers conceptual ideas across visually distinct images in analogical generation tasks

## Why This Works (Mechanism)

### Mechanism 1: Anonymous Caption Abstraction
- **Claim**: Replacing object-specific terms with placeholders (e.g., `{subject}`, `{object}`) enables the model to learn relational patterns independent of surface attributes.
- **Mechanism**: The captioning model is trained on manually curated image groups sharing the same relational logic. By observing multiple images with identical underlying structure, the model learns to abstract away concrete details while preserving relational structure.
- **Core assumption**: Humans identify relational patterns more easily when comparing groups of images rather than single images.
- **Evidence anchors**: "captions are anonymized—describing the underlying relational logic of the scene rather than its surface content" (abstract); "Writing an anonymous caption is hard from a single image, but easier with an image group where the pattern is clear" (Section 3.1, Figure 4)
- **Break condition**: If captions leak semantic/attribute information, retrieval reverts to attribute-based matching (CLIP-T/Qwen-T baselines scored 5.33 and 4.86 vs. relsim's 6.77).

### Mechanism 2: VLM Knowledge Integration
- **Claim**: Vision-Language Models provide world knowledge necessary for relational reasoning that pure vision encoders lack.
- **Mechanism**: The LLM component encodes conceptual knowledge (e.g., "transformation over time" as a generalizable pattern) enabling abstraction beyond perceptual features. A learnable query token appended to image input extracts relational features from the LLM's final layer.
- **Core assumption**: Relational similarity requires conceptual understanding beyond visual perception alone.
- **Evidence anchors**: "relational reasoning often requires higher-level semantic knowledge—which can be found nowhere better than in a Large Language Model" (Section 3.2); Tuned CLIP/DINO improve with same training data but still underperform VLM (5.62, 6.02 vs. 6.77)
- **Break condition**: Pure vision encoders, even when fine-tuned on identical data, cannot match VLM performance—knowledge gap persists.

### Mechanism 3: Contrastive Alignment via Anonymous Captions
- **Claim**: Aligning image embeddings with anonymous caption embeddings through InfoNCE loss creates a representation space where relational similarity is encoded.
- **Mechanism**: For batch size B, the loss maximizes similarity between an image and its corresponding anonymous caption while minimizing similarity with other captions. Temperature-scaled dot product s_ij = v_i^T t_j / τ drives alignment. Formally: s_12 = f_V(I1)·f_V(I2) ≈ f_T(A1)·f_T(A2).
- **Core assumption**: Images with similar anonymous captions should occupy nearby positions in embedding space.
- **Evidence anchors**: Formal definition of relational similarity score (Section 3); InfoNCE loss formulation with learnable temperature (Section 3.2)
- **Break condition**: If anonymous captions fail to accurately capture relational logic, alignment propagates errors.

## Foundational Learning

- **Concept: Attribute vs. Relational Similarity**
  - Why needed: Fundamental distinction explaining what relsim captures that LPIPS/CLIP cannot
  - Quick check question: Why is "Earth is like a peach" relational rather than attribute similarity?

- **Concept: Contrastive Learning (InfoNCE)**
  - Why needed: Core training objective enabling the alignment mechanism
  - Quick check question: What does InfoNCE optimize for when aligning image and caption embeddings?

- **Concept: VLM Architecture for Vision Tasks**
  - Why needed: Understanding why VLMs outperform pure vision encoders for relational reasoning
  - Quick check question: What does the LLM component provide that CLIP's vision encoder lacks?

## Architecture Onboarding

- **Component map**: Image Filter (Qwen2.5-VL-7B, LoRA) -> Anonymous Captioning Model (Qwen2.5-VL-7B) -> relsim Feature Extractor (Qwen2.5-VL-7B, LoRA) -> Text Encoder (all-MiniLM-L6-v2, frozen)

- **Critical path**:
  1. Filter LAION-2B for relationally interesting images
  2. Manually curate 532 image groups (2-10 images each) with shared logic
  3. Generate single anonymous caption per group → assign to all group images
  4. Train relsim with InfoNCE loss (15k iterations, batch 64, 8×A100)

- **Design tradeoffs**:
  - Group-based vs. single-image captioning: Groups produce higher-quality abstractions but require manual curation
  - VLM vs. vision-only: VLM brings knowledge but higher compute cost
  - Dataset filtering: 0.7% keep rate ensures quality but may exclude valid relational images

- **Failure signatures**:
  - Caption leakage (object names in captions) → reverts to attribute similarity
  - Single-image captioning → model struggles to identify relevant patterns (Fig. 4)
  - Vision-only backbone → performance gap persists even with identical training data

- **First 3 experiments**:
  1. Reproduce retrieval baseline comparison (LPIPS, DINO, CLIP-I, dreamsim) on 14k test set using GPT-4o evaluation
  2. Ablate VLM vs. CLIP/DINO backbone with identical training data and loss
  3. Compare group-based vs. single-image anonymous caption quality on held-out image groups

## Open Questions the Paper Calls Out

- **Open Question 1**: Can the generation of relational training data be fully automated to scale beyond the current manually curated 532 image groups? The authors state that developing an automated pipeline to expand image groups is "an important direction for future research" because manual curation is imperfect and not scalable.

- **Open Question 2**: How can a system allow users to specify which relational structure to attend to when an image contains multiple valid interpretations? The paper notes that "one image can embody multiple different relational structures," and "Determining how to use text prompts to specify which relational structure a user intends remains an open question."

- **Open Question 3**: How do biases and hallucinations in the VLM-based anonymous captioning process propagate to the final relational similarity metric? The authors acknowledge that "the anonymous captioning model can exhibit biases or hallucinations, which can lead to some incorrect captions."

## Limitations

- Core training dataset (532 manually curated image groups) is not publicly available, making faithful reproduction impossible
- Dataset filtering removes 99.3% of images from LAION-2B, raising questions about scalability and generalization
- Evaluation relies on GPT-4o scoring and human AB testing without established validity for measuring "relational similarity"

## Confidence

**High Confidence**: VLM backbones outperform pure vision encoders for relational reasoning (tuned CLIP/DINO at 5.62/6.02 vs relsim at 6.77); anonymous captions prevent attribute-based retrieval (group-based captions score 6.77 vs single-image 5.33); contrastive alignment mechanism via InfoNCE loss is well-specified.

**Medium Confidence**: Relational similarity captures "abstract relations beyond attributes" - while evaluations show superiority over attribute-based methods, the definition remains somewhat circular; humans prefer relsim's understanding - human AB testing shows preference but lacks detailed methodology.

**Low Confidence**: Scalability claim that this approach can be extended to larger datasets - current dataset (114k) is small relative to LAION-2B; generalization claim beyond curated examples - no out-of-distribution testing or cross-dataset validation.

## Next Checks

1. **Ablation Study Replication**: Recreate the VLM vs. vision-encoder ablation by training relsim and CLIP/DINO backbones with identical training data, loss function, and hyperparameters. Compare retrieval performance on the same 14k test set to verify the 6.77 vs 5.62 performance gap.

2. **Caption Quality Analysis**: Generate anonymous captions using both group-based and single-image approaches on held-out image groups. Compute caption quality metrics (BLEU, ROUGE) and conduct human evaluation to verify that group-based captions better capture relational logic while avoiding attribute leakage.

3. **Cross-Dataset Generalization**: Evaluate relsim on a different dataset (e.g., COCO, Flickr30k) without retraining. Measure retrieval performance and compare against CLIP baselines to test whether relational similarity generalizes beyond the curated LAION-2B subset.