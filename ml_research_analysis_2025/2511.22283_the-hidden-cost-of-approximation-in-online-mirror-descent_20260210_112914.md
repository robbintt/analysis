---
ver: rpa2
title: The Hidden Cost of Approximation in Online Mirror Descent
arxiv_id: '2511.22283'
source_url: https://arxiv.org/abs/2511.22283
tags:
- theorem
- regret
- which
- proof
- trajectory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the impact of approximation errors in Online
  Mirror Descent (OMD), a fundamental algorithm in optimization and machine learning.
  While OMD is widely used, existing analyses typically assume exact updates, limiting
  our understanding of practical performance.
---

# The Hidden Cost of Approximation in Online Mirror Descent

## Quick Facts
- arXiv ID: 2511.22283
- Source URL: https://arxiv.org/abs/2511.22283
- Reference count: 40
- Primary result: Establishes sharp dichotomy between smooth and non-smooth regularizers in OMD's robustness to approximation errors, showing polynomial vs exponential error tolerance requirements

## Executive Summary
This paper analyzes the robustness of Online Mirror Descent (OMD) to approximation errors in its optimization subproblems, a critical gap since practical implementations rarely achieve exact updates. The authors establish a sharp theoretical dichotomy: smooth regularizers require only polynomially small approximation errors (ε = O(1/T²)) to maintain optimal regret, while non-smooth barrier regularizers show surprising variability. For negative entropy regularization, exponentially small errors (ε = Ω(e^(-ηT))) are needed to avoid linear regret, whereas log-barrier and Tsallis regularizers remain robust with only polynomially small errors. Interestingly, this fragility can be overcome in the stochastic setting, where polynomially small errors suffice for negative entropy, though this robustness doesn't extend to general polyhedral subsets of the simplex.

## Method Summary
The authors study ε-approximate OMD where each update wt+1 is an ε-minimizer of the subproblem ϕ_t(w) = η⟨ℓ_t, w⟩ + D_R(w∥w_t) with D_R being Bregman divergence. They analyze regret bounds under various regularizer types (negative entropy, Tsallis entropy, log-barrier) and error magnitudes across adversarial and stochastic loss sequences on the simplex and polyhedral subsets. The analysis reveals that compounding errors and effective smoothness play crucial roles in determining OMD's robustness to approximations, with negative entropy showing particular fragility in adversarial settings but regaining robustness in stochastic settings.

## Key Results
- Smooth regularizers maintain O(√T) regret with polynomially small errors (ε = O(1/T²)), while negative entropy requires exponentially small errors (ε = Ω(e^(-ηT))) to avoid linear regret
- Tsallis entropy (1<ν<2) and log-barrier (ν=2) remain robust with polynomially small errors (ε = O(η⁴(ηTd)^(-ν/(ν-1)))) on the simplex
- Negative entropy regains robustness with polynomially small errors in stochastic settings, but this doesn't extend to general polyhedral subsets
- Compounding errors and effective smoothness are identified as key factors determining OMD's robustness to approximations

## Why This Works (Mechanism)
The paper reveals that OMD's robustness to approximation errors fundamentally depends on the interplay between the regularizer's smoothness properties and the decision domain geometry. For smooth regularizers, the effective smoothness bounds the error propagation, allowing polynomial error tolerance. However, non-smooth barriers exhibit a sharp transition where negative entropy's coordinate-separable structure makes it vulnerable to compounding errors, requiring exponentially small approximations. This fragility is domain-dependent: while stochasticity can mitigate it on the simplex, polyhedral constraints reintroduce vulnerability. The analysis shows that the critical factor is how approximation errors accumulate across iterations and how the regularizer's geometry interacts with the loss sequence structure.

## Foundational Learning
- **OMD Algorithm**: Fundamental optimization method using Bregman divergences; needed for understanding how approximation errors propagate through iterative updates; quick check: verify update wt+1 minimizes η⟨ℓ_t, w⟩ + D_R(w∥w_t)
- **Bregman Divergence**: Measures distance between points under a convex function; essential for characterizing the geometry of the update space; quick check: confirm D_R(w∥w') = R(w) - R(w') - ⟨∇R(w'), w-w'⟩
- **Effective Smoothness**: Modified smoothness parameter over the domain; crucial for bounding regret when updates are approximate; quick check: ensure β bounds the smoothness of ℓ_t within the domain diameter D
- **Tsallis Entropy Regularizer**: Non-separable barrier with parameter ν; provides intermediate smoothness between negative entropy and log-barrier; quick check: verify R(w) = (1/ν)∑w_i^ν for ν∈(1,2)
- **Compounding Errors**: Error propagation across OMD iterations; explains why some regularizers require exponentially small approximations; quick check: track how ε error in step t affects future regret
- **Polyhedral Subsets**: Decision sets defined by linear constraints; domain geometry affects robustness properties; quick check: confirm polytope K = {w∈∆_d : Aw=0} with appropriate matrix A

## Architecture Onboarding

**Component Map**
OMD Update → ε-Approximate Solver → Regret Bound Calculation → Regularizer Analysis

**Critical Path**
Loss sequence generation → OMD update with approximate solver → Regret computation → Robustness verification across regularizer types and domains

**Design Tradeoffs**
- Smooth vs non-smooth regularizers: polynomial vs exponential error tolerance
- Simplex vs polyhedral domains: stochastic robustness availability
- Exact vs approximate solvers: computational efficiency vs theoretical guarantees

**Failure Signatures**
- Linear regret with negative entropy even with small ε indicates iterates getting "stuck" at extreme coordinates
- Worse-than-expected regret for smooth regularizers suggests insufficient domain diameter or incorrect smoothness parameter
- Stochastic setting not showing robustness improvement indicates non-i.i.d. losses or incorrect η scaling

**3 First Experiments**
1. Implement exact OMD update solver as baseline (closed-form for negative entropy over simplex; iterative solver for other settings), then create ε-approximate version that terminates when suboptimality gap ≤ ε
2. Generate adversarial losses with β-smooth regularizer and verify O(√T) regret with ε ≤ D²/2
3. Test negative entropy on simplex with stochastic i.i.d. losses, using η = √(log(d)/T) and ε ≤ δ/(6d²T⁴), verifying O(√T) regret

## Open Questions the Paper Calls Out
**Open Question 1**: Does the robustness of inexact OMD extend to self-concordant barrier regularizers over general convex domains? The authors state in Section 6 that it "would be valuable to characterize the robustness properties of self-concordant barrier regularizers over general convex domains."

**Open Question 2**: Can a unified theory be developed that characterizes the robustness of inexact OMD for general regularizers and geometries? Section 6 lists as a "broader goal... left for future investigation" the development of "a comprehensive theory of inexact OMD for general regularizers and geometries."

**Open Question 3**: Under what precise conditions does the interplay between stochastic losses and domain geometry guarantee polynomial error tolerance? The paper shows negative entropy regains robustness with stochastic losses on the simplex (Theorem 7) but fails on subsets (Theorem 8), suggesting a complex, uncharacterized dependency on the domain structure.

**Open Question 4**: Can OMD variants (such as implicit updates or optimistic methods) mitigate the compounding errors that cause fragility in the standard inexact OMD formulation? Appendix H notes that FTRL handles errors robustly because rounds are independent, whereas OMD suffers from "compounding errors," implying a search for OMD variants that might bridge this stability gap.

## Limitations
- Adversarial loss sequence constructions in Theorem 4 and 14 are only sketched rather than fully specified for general d > 2
- Polytope structure for Theorem 8 subset construction requires specific matrix A and loss distributions beyond what's provided
- Practical implementation of ε-approximate subproblem solvers is not detailed, making it unclear how to measure suboptimality gaps in practice

## Confidence
- **High confidence**: Main theoretical framework (Theorem 1-3) establishing the polynomial vs exponential error dichotomy for smooth vs non-smooth regularizers
- **Medium confidence**: Theorem 6 showing stochastic robustness for negative entropy, as this requires correct stochastic setting implementation
- **Low confidence**: Lower bound constructions (Theorem 4, 14) due to incomplete loss sequence specifications
- **Medium confidence**: Polyhedral subset results (Theorem 8) given the partial specification of polytope structure

## Next Checks
1. **Implement and verify adversarial loss construction**: Complete the full construction of adversarial loss sequences from Theorem 4 for d=2 and verify the linear regret lower bound when ε > e^(-ηT) for negative entropy regularization
2. **Test stochastic robustness empirically**: Implement the stochastic setting with i.i.d. losses and verify that polynomially small errors (ε = O(1/T²)) suffice for O(√T) regret with negative entropy, as claimed in Theorem 6
3. **Validate polyhedral subset behavior**: Construct the polytope from Theorem 8 with m=16log(1/ε) rows and verify that negative entropy shows linear regret while Tsallis/log-barrier maintain polynomial regret with appropriate ε scaling