---
ver: rpa2
title: 'Benchmarking Educational LLMs with Analytics: A Case Study on Gender Bias
  in Feedback'
arxiv_id: '2511.08225'
source_url: https://arxiv.org/abs/2511.08225
tags:
- bias
- gender
- feedback
- llms
- explicit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces an embedding-based benchmarking framework
  to detect gender bias in large language models (LLMs) used for educational feedback.
  Using 600 student essays, the authors constructed counterfactual pairs through gender-swapping
  and explicit author cues, then measured response divergence via cosine and Euclidean
  distances over sentence embeddings.
---

# Benchmarking Educational LLMs with Analytics: A Case Study on Gender Bias in Feedback

## Quick Facts
- **arXiv ID:** 2511.08225
- **Source URL:** https://arxiv.org/abs/2511.08225
- **Reference count:** 40
- **Primary result:** Embedding-based framework detects asymmetric gender bias in LLM feedback: male→female counterfactuals show significant semantic divergence across models, but not female→male.

## Executive Summary
This study introduces a benchmarking framework to detect gender bias in LLM-generated educational feedback using counterfactual evaluation. The authors constructed controlled counterfactual pairs by swapping gender-associated terms in student essays and varying explicit gender cues in prompts. They measured semantic divergence between counterfactual feedback using sentence embeddings and permutation testing. Results revealed significant asymmetric bias: male→female counterfactuals showed consistent semantic shifts across all models, but female→male did not, indicating directional bias. Only GPT and Llama models responded to explicit gender cues. Qualitative analysis showed female-cued feedback contained more dialogic language while male-cued feedback emphasized autonomy support. The findings demonstrate persistent gender biases in LLM feedback and highlight the need for robust fairness auditing in educational applications.

## Method Summary
The study applied counterfactual fairness probing to detect gender bias in LLM feedback. Researchers used 600 student essays from the AES 2.0 corpus, filtering for gendered vocabulary. They constructed counterfactual pairs through implicit cues (lexicon-based swaps of 192 gender-synonymous word pairs) and explicit cues (gendered author background in prompts). Feedback was generated by six LLMs (GPT-5 mini, GPT-4o mini, DeepSeek-R1, DeepSeek-R1-Qwen, Gemini 2.5 Pro, Llama-3-8B) at default temperature. Semantic divergence was quantified using cosine and Euclidean distances on sentence embeddings (text-embedding-3-large, 3072 dimensions). A permutation test (5000 iterations) compared observed distances against a stochastic baseline to establish statistical significance. Qualitative thematic analysis examined linguistic and pedagogical differences in representative samples.

## Key Results
- Asymmetric gender bias detected: male→female counterfactuals showed significant semantic divergence across all models, but female→male did not.
- Only GPT and Llama models responded to explicit gender cues in prompts; other models showed no significant shifts.
- Qualitative analysis revealed linguistic differences (more dialogic language under female cues) and pedagogical bias (more autonomy-supportive feedback under male cues).
- Permutation tests confirmed results exceeded chance variation, with effect sizes ranging from small to medium (Cohen's d).

## Why This Works (Mechanism)

### Mechanism 1: Counterfactual Fairness Probing via Gender Manipulation
Substituting gender-associated terms while holding content constant reveals asymmetric bias patterns in LLM feedback generation. The framework constructs controlled counterfactual pairs along two dimensions: (i) implicit cues via lexicon-based swaps of 192 gender-synonymous word pairs within essays, and (ii) explicit cues via gendered author background in prompts. By comparing LLM responses to these minimally-differentiated inputs, the approach isolates gender as the causal variable driving semantic divergence. Core assumption: Gender-swapped texts maintain semantic equivalence such that observed output differences can be attributed to gender cues rather than content changes. Evidence anchors: Abstract states construction of controlled counterfactuals via lexicon-based swaps and gendered author background; section 3.2.1 details the 192-word lexicon and manual semantic coherence checks. Break condition: If gender word substitutions alter essay semantics or readability significantly, observed differences may confound gender with content quality rather than isolate bias.

### Mechanism 2: Embedding-Based Semantic Distance Quantification
Sentence embeddings capture gender-conditioned semantic drift that surface-level text comparison would miss. Using OpenAI's text-embedding-3-large model, LLM-generated feedback is mapped to a continuous space where geometric distance reflects semantic divergence. Cosine distance captures directional semantic drift, while Euclidean distance captures absolute displacement. Larger distances between counterfactual pairs indicate greater model sensitivity to gender cues. Core assumption: Embedding distances correlate meaningfully with pedagogically consequential differences in feedback—not just lexical variation. Evidence anchors: Abstract mentions measuring response divergence via cosine and Euclidean distances over sentence embeddings; section 3.3.2 explains the distinction between cosine (directional) and Euclidean (absolute) distances. Break condition: If embedding spaces encode spurious associations or fail to capture pedagogically meaningful distinctions, distances may not reflect actual bias harms.

### Mechanism 3: Permutation Testing Against Stochastic Baseline
Non-parametric permutation tests distinguish systematic gender-induced divergence from inherent LLM output variability. A robustness baseline (Group M′) re-prompts 300 original essays without manipulation to establish natural variability. Permutation tests (B=5000 iterations) generate null distributions by randomly shuffling group labels, comparing observed mean distances against this baseline. This controls for both stochastic generation variance and content-specific differences unrelated to gender. Core assumption: The permutation procedure adequately models the null distribution of distances under no-gender-effect conditions despite potential dependencies in the embedding space. Evidence anchors: Section 3.2.1 describes the baseline as ground truth against which gender-manipulated conditions are evaluated; section 3.3.3 details the non-parametric permutation test with 5000 iterations. Break condition: If embedding distances are not exchangeable under the null hypothesis due to structural properties of the embedding space, permutation p-values may be invalid.

## Foundational Learning

- **Concept: Counterfactual Fairness**
  - **Why needed here:** The entire methodology rests on causal inference principles—manipulating only gender while holding all else constant to isolate bias. Without understanding counterfactual consistency, results could be misattributed.
  - **Quick check question:** If an essay about a male engineer is changed to reference a female engineer, and feedback shifts from technical suggestions to encouragement about effort, what confound might undermine the counterfactual interpretation?

- **Concept: Sentence Embeddings and Geometric Semantics**
  - **Why needed here:** Interpreting cosine/Euclidean distances requires understanding that embeddings map semantic similarity to geometric proximity. The asymmetric bias finding (M→F shows divergence, F→M does not) manifests as directional shifts in this space.
  - **Quick check question:** Why might cosine distance detect subtle tone shifts that exact string matching would miss?

- **Concept: Self-Determination Theory (Autonomy Support vs. Control)**
  - **Why needed here:** The qualitative analysis operationalizes pedagogical bias through autonomy-supportive (e.g., "you could explore...") versus controlling (e.g., "you must...") language. Understanding this framework explains why directive feedback to female-cued essays is educationally harmful.
  - **Quick check question:** If feedback to male students emphasizes agency ("you might consider") while feedback to female students emphasizes compliance ("you should fix"), how might this affect intrinsic motivation differently?

## Architecture Onboarding

- **Component map:** Data Preparation Layer (AES 2.0 corpus → lexicon-based counterfactual generation + prompt template construction) -> Model Query Layer (LLM prompts → feedback generation) -> Embedding Layer (text-embedding-3-large → 3072-dim vectors) -> Analysis Layer (distance computation → permutation testing → effect size calculation → t-SNE visualization) -> Interpretation Layer (thematic analysis + textual analysis)
- **Critical path:** Essay selection → counterfactual construction → LLM prompting → embedding generation → distance computation → permutation testing → cluster visualization → qualitative thematic analysis
- **Design tradeoffs:** Lexicon vs. neural gender detection (transparency vs. comprehensiveness); temperature settings (ecological validity vs. variability control); embedding model choice (state-of-art vs. reproducibility)
- **Failure signatures:** Non-significant baseline check would indicate permutation tests cannot distinguish signal from noise; symmetric bias would suggest general sensitivity to gender terms rather than directional bias; embedding clusters completely overlapping would indicate embedding model fails to capture relevant semantic distinctions
- **First 3 experiments:** Replicate on different essay corpus to test generalizability; add intersectional counterfactuals to probe compounding biases; temperature sensitivity analysis to quantify stochasticity effects

## Open Questions the Paper Calls Out
- What specific underlying mechanisms within LLMs drive the observed asymmetric gender bias (significant M→F shift but no F→M shift)?
- To what extent does cultural context or situational framing in prompts modulate the severity of gender bias in educational feedback?
- What specific prompt engineering techniques or post-processing interventions can effectively mitigate the identified pedagogical asymmetries?

## Limitations
- Lexicon-based counterfactual construction may miss implicit gender associations, potentially underestimating bias in non-stereotypical contexts
- Embedding-based metrics assume semantic distance correlates with pedagogically meaningful differences, but validation against human-coded educational quality remains indirect
- Findings may not generalize across educational contexts, as the AES corpus focuses on argumentative essays

## Confidence
- **High Confidence:** Asymmetric bias pattern (M→F divergence significant, F→M not) and finding that only explicit gender cues triggered responses in GPT/Llama models
- **Medium Confidence:** Pedagogical bias interpretations (autonomy-supportive vs. controlling language differences)
- **Low Confidence:** Generalizability of findings across educational contexts

## Next Checks
1. **Cross-Corpus Replication:** Apply the exact methodology to a different educational corpus (e.g., STEM writing or creative writing) to test whether asymmetric gender bias persists across writing types.
2. **Human Validation Study:** Have trained educators code a subset of LLM feedback for autonomy-support indicators and compare their judgments against embedding distances to validate the semantic interpretation.
3. **Intersectional Extension:** Expand the counterfactual design to include intersectional gender-race cues (e.g., names associated with different racial backgrounds) to examine whether the asymmetric bias compounds with other identity dimensions.