---
ver: rpa2
title: 'Ultra-Quantisation: Efficient Embedding Search via 1.58-bit Encodings'
arxiv_id: '2506.00528'
source_url: https://arxiv.org/abs/2506.00528
tags:
- space
- vectors
- distance
- values
- search
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper introduces ultra-quantisation, a method for efficiently\
  \ searching high-dimensional embeddings by mapping them to ternary vectors {\u2212\
  1,0,1} encoded in 1.58 bits per element. The method leverages Equi-Voronoi Polytopes\
  \ (EVPs) in high-dimensional spaces, where vertices serve as proxy representations\
  \ that preserve similarity distances surprisingly well."
---

# Ultra-Quantisation: Efficient Embedding Search via 1.58-bit Encodings

## Quick Facts
- arXiv ID: 2506.00528
- Source URL: https://arxiv.org/abs/2506.00528
- Authors: Richard Connor; Alan Dearle; Ben Claydon
- Reference count: 15
- Primary result: EVP-based quantisation achieves Spearman correlation up to 0.96 with original distances, and search recall improvements up to 158x faster than Euclidean distance

## Executive Summary
Ultra-Quantisation introduces a method for efficiently searching high-dimensional embeddings by mapping them to ternary vectors {−1,0,1} encoded in 1.58 bits per element. The approach leverages Equi-Voronoi Polytopes (EVPs) in high-dimensional spaces, where vertices serve as proxy representations that preserve similarity distances surprisingly well. The method achieves up to 158x speedup over traditional Euclidean distance while maintaining high correlation (Spearman ρ up to 0.96) with original distances.

## Method Summary
The method maps floating-point vectors to ternary vectors by selecting the x largest absolute values (where x ≈ 2d/3), setting them to ±1 based on sign, and zeroing all others. This finds the nearest vertex of a {x,d} EVP. The resulting ternary vectors are encoded as two binary vectors (v⁺ for +1 positions, v⁻ for -1 positions), enabling scalar product computation via bitwise operations and integer addition instead of floating-point multiplication. The negative scalar product of ternary vectors correlates tightly with original Euclidean distances in ℓ2-normalized space, making it suitable for nearest neighbor search.

## Key Results
- Spearman correlation: 0.94 (PubMed-384), 0.96 (Laion-500), 0.83 (GloVe-100)
- Search recall: 30@100 achieves 0.70-0.74 across datasets
- Speedup: 33x-158x faster than Euclidean distance in end-to-end benchmarks
- Accuracy: Outperforms 1-bit and 1.58-bit quantisation methods in both accuracy and speed

## Why This Works (Mechanism)

### Mechanism 1: Equi-Voronoi Polytope Vertex Mapping
Mapping arbitrary floating-point vectors to their nearest EVP vertex preserves distance relationships with high fidelity. For each vector, identify the x largest absolute values, set them to ±1 based on sign, and zero all others. The core assumption is that high-dimensional geometry causes almost all volume of each Voronoi cell to concentrate at maximum distance from the cell center, making the distance from any point to its nearest vertex nearly constant across all points.

### Mechanism 2: Scalar Product as Euclidean Proxy
The negative scalar product of ternary vectors correlates tightly with original Euclidean distances. In ℓ2-normalized space, Euclidean distance = √(2 - 2·scalar_product), making them perfectly correlated. Since all EVP vertices have identical magnitude (√x), the ternary scalar product preserves this relationship.

### Mechanism 3: Bitwise Scalar Product (b2sp)
Ternary scalar products can be computed using only bitwise operations and integer addition, avoiding floating-point multiplication. Encode each ternary vector as two binary vectors (v⁺ for +1 positions, v⁻ for -1 positions). Then: b2sp(v,w) = popcount(v⁺∧w⁺) + popcount(v⁻∧w⁻) - popcount(v⁺∧w⁻) - popcount(v⁻∧w⁺).

## Foundational Learning

- **Voronoi partitioning on hyperspheres**: EVP vertices define equal-volume cells on the hypersphere surface; understanding this explains why proxy distances work. Quick check: Given a set of points on a circle, can you sketch their Voronoi cells?
- **Curse/richness of dimensionality (volume concentration)**: In high dimensions, nearly all volume is "near the surface"—this is why vertex-to-point distances stabilize. Quick check: What fraction of a 250-dimensional unit sphere's volume lies within radius 0.95? (Answer: <10⁻⁶)
- **SIMD bitwise operations (popcount, AND)**: The speed gains come from replacing floating-point multiply-add with bitwise AND and population count. Quick check: On your target CPU, what is the latency of POPCNT vs. FMA?

## Architecture Onboarding

- **Component map**: Raw embeddings → ℓ2-normalize → select top-x absolute values → emit (v⁺, v⁻) bit-pair → b2sp computation → heap-based top-k selection → optional rerank with original vectors
- **Critical path**: 1) Verify input vectors are ℓ2-normalized, 2) Choose x = ⌊2d/3⌋ for {x,d} EVP, 3) Encode: scan for top-x absolute values, emit two bitmasks, 4) Search: b2sp + heap-based top-k, 5) Optional: rerank top-n >> k with original floats
- **Design tradeoffs**: Higher x → more non-zeros → better accuracy but slightly slower popcounts; paper uses x ≈ 2d/3 as sweet spot. Lower dimensions (<100) → weaker geometric concentration → lower Spearman ρ. Rerank overhead: 30@100 gives better recall than 30@30 but requires 3× more exact comparisons.
- **Failure signatures**: Spearman ρ < 0.7: likely non-normalized input or very low dimensions. Recall degrades sharply for 30@30 but not 30@100: expected; increase rerank candidate pool. No speedup observed: check that BitVec is 256-bit aligned and SIMD-enabled.
- **First 3 experiments**:
  1. **Correlation baseline**: Sample 10K vector pairs from your data, compute both true Euclidean and b2sp distances, report Spearman ρ. Target: >0.85 for d ≥ 384.
  2. **Recall@n curve**: For k=30, sweep n∈{30,50,100,200,500}, plot recall. Confirm 30@100 ≥ 0.7 on your data.
  3. **End-to-end latency**: Benchmark 100 exhaustive queries over 1M vectors, compare float32 ℓ2 vs. b2sp. Target: >30× speedup.

## Open Questions the Paper Calls Out

1. **Single-operand translation**: Does comparing a ternary vector against an unquantized floating-point or 8-bit integer vector yield higher accuracy than comparing two ternary vectors? The authors are currently investigating this hybrid approach.

2. **Approximate matrix multiplication**: Can EVP geometry enable accurate, multiplication-free approximate matrix multiplication for deep learning architectures? The paper suggests this might be possible but hasn't validated it for matrix operations.

3. **Connection to BitNet**: Does the Equi-Voronoi Polytope (EVP) geometry provide the missing theoretical explanation for the success of 1.58-bit quantisation in Large Language Models (BitNet)? The authors suspect their theory explains BitNet observations but haven't formally proven this connection.

## Limitations

- **Dimensionality dependence**: The theoretical guarantees rely on high-dimensional concentration effects, but the paper doesn't quantify the exact dimensionality threshold where performance degrades. Spearman correlation drops from 0.96 (d=500) to 0.83 (d=100).
- **Data distribution sensitivity**: While the method assumes isotropic data on hyperspheres, real-world embeddings often have structured correlations. The paper doesn't test how well the approach handles non-uniform distributions.
- **Practical deployment considerations**: The evaluation focuses on exhaustive search rather than typical ANN scenarios with massive databases. The paper doesn't address how EVP quantization performs with indexing structures like IVF or HNSW.

## Confidence

- **High confidence**: The bitwise scalar product computation (b2sp) and its performance advantages are well-established through microbenchmarks and end-to-end timing. The mathematical relationship between scalar products and Euclidean distances in normalized space is correct.
- **Medium confidence**: The Spearman correlation results are robust across multiple datasets, but the exact values depend on preprocessing steps not fully specified. The 158x speedup claim is based on microbenchmarks that may not generalize to all hardware.
- **Low confidence**: The theoretical justification for why distances to EVP vertices are nearly constant across all points relies on high-dimensional concentration arguments that aren't empirically validated across diverse datasets and dimensionalities.

## Next Checks

1. **Dimensionality threshold analysis**: Systematically test EVP quantization across d ∈ {50, 100, 200, 384, 512, 1024} on multiple datasets to identify the dimensionality below which Spearman correlation drops below 0.9.

2. **Distribution sensitivity test**: Apply the method to embeddings with known non-isotropic structure (e.g., word embeddings with semantic clustering, image embeddings with class-specific manifolds) and measure correlation degradation.

3. **Index integration benchmark**: Implement EVP quantization within a standard ANN index (e.g., IVF-Flat or HNSW) and compare recall@100 and latency against float32 baselines at scale (e.g., 10M-100M vectors).