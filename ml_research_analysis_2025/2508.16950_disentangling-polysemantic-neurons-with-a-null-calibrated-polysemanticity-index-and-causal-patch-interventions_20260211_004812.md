---
ver: rpa2
title: Disentangling Polysemantic Neurons with a Null-Calibrated Polysemanticity Index
  and Causal Patch Interventions
arxiv_id: '2508.16950'
source_url: https://arxiv.org/abs/2508.16950
tags:
- neurons
- neuron
- score
- polysemanticity
- clusters
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces the Polysemanticity Index (PSI), a null-calibrated\
  \ metric designed to identify and quantify neurons in neural networks that respond\
  \ to multiple, distinct features (polysemanticity). PSI combines three components\u2014\
  geometric cluster quality, class-label alignment, and open-vocabulary semantic distinctness\u2014\
  to robustly detect neurons whose activations decompose into semantically coherent\
  \ clusters."
---

# Disentangling Polysemantic Neurons with a Null-Calibrated Polysemanticity Index and Causal Patch Interventions

## Quick Facts
- **arXiv ID**: 2508.16950
- **Source URL**: https://arxiv.org/abs/2508.16950
- **Reference count**: 20
- **Primary result**: Introduces PSI, a null-calibrated metric achieving AUROC 0.987 for detecting polysemantic neurons in ResNet-50.

## Executive Summary
This paper introduces the Polysemanticity Index (PSI), a statistically calibrated metric designed to identify neurons in neural networks that encode multiple, semantically distinct features (polysemanticity). PSI combines geometric cluster quality, class-label alignment, and open-vocabulary semantic distinctness, each normalized against randomized baselines. Applied to ResNet-50 on Tiny-ImageNet, PSI achieved near-perfect discrimination (AUROC 0.987) between polysemantic and monosemantic neurons, revealing a depth-wise trend of increasing polysemanticity in later layers. Causal patch-swap interventions confirmed that the discovered feature prototypes are functionally relevant, as aligned patches increased target-neuron activation significantly more than controls.

## Method Summary
PSI operates by extracting top-K activating patches per neuron channel, embedding them via CLIP, and clustering to evaluate three components: geometric separability (S), class-label alignment (Q), and semantic distinctness (D). Each raw score is converted to a z-score relative to a null distribution (generated by random rotations, label permutations, or shuffled text) and passed through a sigmoid. The final PSI is the product $\hat{S} \cdot \hat{Q} \cdot \hat{D}$, which acts as a logical AND filter requiring joint evidence across all three axes. The method is validated through ablation studies and causal patch-swap interventions.

## Key Results
- PSI achieved AUROC of 0.987 for discriminating polysemantic neurons from a null baseline on ResNet-50 layer4.
- Later layers (layer4) exhibited significantly higher polysemanticity than earlier layers (layer3), indicating depth-wise trends.
- Causal patch-swap interventions confirmed functional relevance: aligned patches increased activation by +0.15 on average versus controls.

## Why This Works (Mechanism)

### Mechanism 1: Null-Calibration of Interpretability Metrics
- Claim: Comparing raw clustering scores against randomized baselines isolates meaningful polysemantic structure from statistical noise.
- Mechanism: The pipeline generates three null distributions (rotated embeddings for geometry, permuted labels for class alignment, and shuffled text for semantics). Raw scores are converted to z-scores relative to these nulls, ensuring a high Polysemanticity Index (PSI) only occurs if the neuron's activation clusters are more distinct than chance.
- Core assumption: Assumes that random orthogonal rotations of embeddings and permutations of labels serve as valid proxies for "meaningless" structure.
- Evidence anchors:
  - [Abstract]: "PSI... null-calibrated metric... quantifies when a neuron's top activations decompose into semantically distinct clusters."
  - [Section 3.6]: "We calibrate each score by comparing it against a null distribution... transforms each score into a measure of statistical significance."
  - [Corpus]: Related work like *Disentangling Polysemantic Channels* focuses on decomposition, but this paper uniquely emphasizes statistical calibration to prevent false positives.
- Break condition: If the null distribution parameters (e.g., rotation types) do not accurately simulate noise for specific data modalities, the calibration may yield false confidence.

### Mechanism 2: Multiplicative Consensus (S $\times$ Q $\times$ D)
- Claim: Enforcing agreement between geometric, categorical, and semantic axes filters out neurons that are merely noisy or monosemantic.
- Mechanism: PSI multiplies three calibrated components: Geometric Separability (S - cluster quality), Class-Label Alignment (Q - NMI with labels), and Open-Vocabulary Distinctness (D - CLIP semantic gap). A neuron must score high on *all* three to achieve a high PSI.
- Core assumption: Assumes that true polysemanticity manifests simultaneously as geometrically distinct clusters *and* semantically nameable concepts.
- Evidence anchors:
  - [Section 3.6]: "This multiplicative form acts as a logical AND... rewarding only those neurons that exhibit joint evidence."
  - [Table 1]: Shows that individual components (like $\hat{S}$-only) have different discriminative power than the full PSI product.
  - [Corpus]: *Capturing Polysemanticity with PRISM* uses multi-concept descriptions, but PSI formalizes it into a single quantifiable index.
- Break condition: If a neuron is polysemantic but its concepts do not align with the dataset's class labels (low Q), the metric will suppress the score, potentially missing "superposition" unique to unlabeled features.

### Mechanism 3: Causal Validation via Patch-Swap Interventions
- Claim: Replacing image patches with "prototype" patches from specific clusters causally increases neuron activation, confirming the functional relevance of the discovered features.
- Mechanism: For a target neuron, the authors swap a high-activation patch with an "Aligned" patch (from the same semantic cluster) vs. controls (Random, Shuffled). A significant activation increase only in the Aligned case confirms the cluster represents a causal feature detector.
- Core assumption: Assumes the neuron's activation is locally sensitive to the specific receptive field content and not purely global context.
- Evidence anchors:
  - [Abstract]: "Causal patch-swap interventions confirmed that the discovered feature prototypes are functionally relevant."
  - [Section 5.3]: "Swapping in an Aligned patch... increased the neuron's activation... mean increase of +0.15."
  - [Corpus]: *NeuronScope* and others focus on LLMs; this paper provides a concrete causal mechanism for vision models (CNNs).
- Break condition: If the neuron acts as a complex logical gate (e.g., detecting "not-X") or relies on non-local context, simple patch swaps may fail to elicit the expected activation change.

## Foundational Learning

- **Concept: Polysemantic Neurons & Superposition**
  - Why needed here: You cannot interpret PSI without understanding the problem it solves—that neural networks often pack multiple distinct features into a single neuron to save capacity (superposition).
  - Quick check question: Why might a neuron fire for both "dog faces" and "car wheels," and how does PSI distinguish this from a neuron that just fires on "furry textures"?

- **Concept: Null-Hypothesis Significance Testing**
  - Why needed here: The core innovation of PSI is statistical calibration. You need to understand why raw clustering metrics (like Silhouette score) can be high by chance and why comparing against a null distribution is necessary.
  - Quick check question: If a set of random noise vectors is clustered, why might the geometric score be non-zero, and how does the null calibration fix this?

- **Concept: CLIP Embeddings (Vision-Language Space)**
  - Why needed here: The "D" component of PSI relies on Open-Vocabulary distinctness. You need to grasp that CLIP maps images and text to a shared vector space where cosine similarity reflects semantic relatedness.
  - Quick check question: How does using CLIP allow the PSI pipeline to identify that a neuron's clusters represent "distinct concepts" without pre-defined class labels?

## Architecture Onboarding

- **Component map:**
  - Input -> Miner -> Encoder -> Evaluator -> Calibrator -> Aggregator
  - (Tiny-ImageNet + ResNet-50) -> (Top-K patch extraction) -> (CLIP ViT-B/32) -> (K-Means + Silhouette + NMI + Semantic Gap) -> (Null distribution via rotations/permutations) -> (PSI = $\hat{S} \cdot \hat{Q} \cdot \hat{D}$)

- **Critical path:**
  1. Identifying the Top-K patches (if this is noisy, all downstream calibration fails).
  2. The null-calibration step (converting raw metrics to z-scores is where the "statistical rigor" lives).
  3. The multiplicative fusion (filtering out partial matches).

- **Design tradeoffs:**
  - **K (Number of patches):** Low K is faster but noisier; High K is more stable but computationally expensive. (Paper notes correlation improves with higher K).
  - **Prompt Set for D:** A small prompt set limits semantic discovery; a large set increases compute and noise.
  - **Multiplicative vs. Additive PSI:** Multiplication is stricter (requires all 3 components); Addition might allow neurons with only geometric structure to pass.

- **Failure signatures:**
  - **High S, Low Q/D:** Neuron clusters on low-level features (textures/colors) that don't map to human classes or text concepts.
  - **High Q, Low D:** Neuron splits by class, but concepts are visually similar (e.g., "dog face" vs. "wolf face"), confusing the CLIP distinctness check.
  - **Sensitivity to Encoder:** As noted in Limitations, changing the CLIP text head changes D-scores (Spearman $\rho \approx 0.21$).

- **First 3 experiments:**
  1. **Replicate Null Separation:** Generate the histogram in Figure 2a. Verify that real neurons have a distinctly higher ln(PSI) distribution than the rotated null baseline.
  2. **Ablation Study:** Compute PSI using only $\hat{S}$, only $\hat{Q}$, and $\hat{S} \cdot \hat{Q}$. Compare AUROC against the full PSI ($\hat{S} \cdot \hat{Q} \cdot \hat{D}$) to validate the contribution of the Open-Vocabulary component.
  3. **Causal Patch-Swap Test:** Select a high-PSI neuron and a low-PSI neuron. Perform the "Aligned" vs. "Random" patch swap. Verify that the activation delta is significant for the high-PSI neuron but not the low-PSI one.

## Open Questions the Paper Calls Out
- **Open Question 1:** Can PSI be effectively utilized as a regularizer during training to explicitly control the trade-off between model accuracy and neuron interpretability?
  - Basis in paper: [explicit] The Conclusion states PSI "can be used as a regularizer to study interpretability–performance trade-offs."
  - Why unresolved: The current study only applies PSI as a post-hoc analysis tool on pre-trained models (ResNet-50) rather than integrating it into the training objective.
  - What evidence would resolve it: Training new models with a PSI-based penalty term and measuring the resulting degradation (or maintenance) of accuracy alongside changes in neuron polysemanticity.

- **Open Question 2:** Does the reliance on Top-K activation mining cause PSI to systematically overlook polysemantic structures that manifest primarily at moderate activation levels?
  - Basis in paper: [inferred] The Limitations section notes that the "mining strategy biases the analysis towards high-activation features and may miss polysemanticity that manifests at moderate activation levels."
  - Why unresolved: The method strictly selects the top K patches, potentially ignoring "distributed" features that do not cause peak activation but still represent meaningful distinct concepts.
  - What evidence would resolve it: A comparative analysis varying the activation threshold (e.g., top 10% vs. top 50% of activations) to see if distinct polysemantic clusters appear in the "tail" of the activation distribution.

- **Open Question 3:** How does PSI perform when applied to non-convolutional architectures, such as Transformers or Large Language Models (LLMs)?
  - Basis in paper: [explicit] The Conclusion claims "The approach extends naturally to LLM neurons or transformer heads."
  - Why unresolved: The empirical validation was restricted to a ResNet-50 CNN; the spatial patch-swap interventions and K-means clustering on spatial activation maps may not translate directly to the attention mechanisms or token-based representations in Transformers.
  - What evidence would resolve it: Applying the metric to Multi-Head Attention layers in a Transformer and validating if the geometric (S) and semantic (D) components still correlate with causal interventions on token inputs.

## Limitations
- **Prompt Set Sensitivity**: The semantic component (D) relies on 1602 handcrafted prompts. Changing the prompt set or using a different CLIP model could alter the D scores significantly (Spearman $\rho \approx 0.21$ when swapping text heads).
- **Null Distribution Validity**: The assumption that random rotations of embeddings and permutations of labels generate valid "noisy" baselines may not hold for all feature types.
- **Receptive Field Approximation**: Patch extraction depends on accurate receptive field projection. Errors here propagate to clustering and all downstream metrics.

## Confidence
- **High Confidence**: The null-calibration mechanism works as described. Evidence: AUROC of 0.987 on layer4 clearly separates real from null neurons, and the histogram in Figure 2a shows distinct distributions.
- **Medium Confidence**: The multiplicative consensus rule (S $\times$ Q $\times$ D) effectively filters false positives. Evidence: Ablation studies in Table 1 show PSI outperforms individual components, but edge cases (e.g., polysemantic features with no class labels) may be missed.
- **Low Confidence**: Causal patch-swap interventions definitively prove functional relevance. Evidence: The Aligned vs. Random effect is statistically significant (+0.15 mean), but the interpretation assumes linear, local activation mechanisms, which may not hold for complex neurons.

## Next Checks
1. **Cross-Modal Generalization**: Apply PSI to a transformer-based vision model (e.g., ViT) and compare depth-wise polysemanticity trends. Verify the method still achieves high AUROC.
2. **Prompt Robustness Test**: Vary the prompt set size and content. Measure how D-score correlations change and whether PSI rankings remain stable.
3. **Adversarial Baseline**: Generate adversarial patches designed to fool the clustering algorithm. Test whether PSI still correctly identifies them as low-polysemanticity and whether the null calibration holds.