---
ver: rpa2
title: 'WaterDrum: Watermarking for Data-centric Unlearning Metric'
arxiv_id: '2505.05064'
source_url: https://arxiv.org/abs/2505.05064
tags:
- data
- unlearning
- text
- waterdrum
- owner
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces WaterDrum, the first data-centric unlearning
  metric for large language models (LLMs) that overcomes limitations of existing utility-centric
  metrics. While current metrics rely on model utility and require referencing retrained
  models, WaterDrum leverages robust text watermarking to directly track the influence
  of data.
---

# WaterDrum: Watermarking for Data-centric Unlearning Metric

## Quick Facts
- arXiv ID: 2505.05064
- Source URL: https://arxiv.org/abs/2505.05064
- Reference count: 40
- WaterDrum outperforms existing metrics across all desiderata for LLM unlearning evaluation

## Executive Summary
This paper introduces WaterDrum, the first data-centric unlearning metric for large language models that overcomes limitations of existing utility-centric metrics. While current metrics rely on model utility and require referencing retrained models, WaterDrum leverages robust text watermarking to directly track the influence of data. The metric uses a watermarking framework to embed unique identifiers in data from multiple owners, allowing verification of whether data remains present in LLM outputs after unlearning. Experiments show WaterDrum significantly outperforms existing metrics like ROUGE-L, Truth Ratio, KnowMem, and MIA across all desiderata, enabling effective benchmarking of unlearning algorithms.

## Method Summary
WaterDrum defines a metric using watermarking operators that embed unique identifiers in data from multiple owners. Data owners use the Waterfall framework to generate watermarked datasets with unique keys. An LLM is fine-tuned on this watermarked data, and unlearning algorithms modify the model to remove forget set influence. Verification operators check if watermarks from the forget set appear in model outputs. The paper introduces WaterDrum-Ax (8000 arXiv abstracts, 20 owners) and WaterDrum-TOFU benchmark datasets with varying levels of data similarity, reflecting realistic scenarios where forget and retain sets contain semantically similar content.

## Key Results
- WaterDrum achieves AUROC near 1.0 for separability (D1), distinguishing retain vs. forget set influence even with exact/semantic duplicates
- WaterDrum shows well-calibrated continuous measurement of unlearning extent (D2) with R² values close to 1, outperforming baselines with negative R²
- WaterDrum enables effective benchmarking of unlearning algorithms, revealing their strengths and weaknesses in balancing forget set removal against retain set preservation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: WaterDrum measures data influence by verifying persistence of unique text watermarks in model outputs, rather than relying on model utility.
- Mechanism: Data owners embed unique identifiers into their text data using Waterfall framework. An LLM is fine-tuned on this watermarked data. To evaluate unlearning, a verification operator checks if the specific watermark associated with the forget set appears in generated text.
- Core assumption: The watermarking framework is robust enough to survive fine-tuning and be detectable in outputs.
- Evidence anchors: [abstract] "WaterDrum leverages robust text watermarking to directly track the influence of data." [section 4] "We define a metric M' using the verification operator... M'(φ•(qd), i) := V(φ•(qd), µi)."
- Break condition: If watermark signal degrades below detectability during standard fine-tuning, the metric will falsely indicate unlearning.

### Mechanism 2
- Claim: The metric maintains robustness in presence of semantically similar data by assigning unique keys to different owners.
- Mechanism: Different owners use unique watermark keys, so identical text from different owners carries distinct signals. The verification operator checks only for the specific key of the owner requesting deletion.
- Core assumption: The watermarking framework supports "Overlap Verifiability" (W2), allowing multiple distinct watermarks to coexist without interference.
- Evidence anchors: [section 4] "Unique keys also ensure that similar or even identical data from different owners would have different watermarks, which supports D4." [section 5] "WaterDrum's AUROC remains near 1.0 even when similar data exists" (Table 2).
- Break condition: If the watermarking framework fails to support orthogonal/unique keys (W2), similar data in retain set could trigger verification, falsely indicating forget set remains.

### Mechanism 3
- Claim: The metric provides calibrated, continuous measure of unlearning extent without referencing computationally expensive retrained model.
- Mechanism: Unlike utility-centric metrics requiring "gold standard" retrained model, WaterDrum outputs near-zero for perfect unlearning and scales linearly with proportion of forget set remaining.
- Core assumption: The verification score aggregates linearly with "amount" of data influence, which authors acknowledge is empirical finding.
- Evidence anchors: [section 3] "The aggregate metric (in expectation) should be proportional to the size k of the random subset." [section 5] "Fig. 3 and Table 3 show... WaterDrum is the only well-calibrated metric... achieving R² values close to 1."
- Break condition: If relationship between data volume and watermark signal strength is non-linear or saturates, calibration property fails.

## Foundational Learning

### Machine Unlearning (Exact vs. Approximate)
- Why needed here: Distinguishes between "exact unlearning" (retraining from scratch) and "approximate unlearning" (modifying weights). WaterDrum evaluates the latter without needing the former as reference.
- Quick check question: Can you explain why retraining from scratch is considered the "gold standard" but violates the "Feasibility" (D3) desideratum for evaluation?

### Text Watermarking (Robustness vs. Fidelity)
- Why needed here: Metric relies on Waterfall framework to embed signals that survive LLM training. Must understand trade-off between distorting text (fidelity) and ensuring signal persists (robustness).
- Quick check question: How does the Waterfall framework preserve semantic meaning while embedding a watermark, and why is this critical for the "Fidelity" desideratum (W0)?

### Semantic Text Similarity (STS) and Generalization
- Why needed here: Paper challenges assumption that forget and retain sets are disjoint. Need to understand why LLMs generate similar outputs for similar inputs (generalization), which breaks utility-centric metrics like ROUGE-L.
- Quick check question: Why does high semantic similarity between a forget set and a retain set cause utility-centric metrics to fail (specifically regarding separability)?

## Architecture Onboarding

### Component map:
Data Prep (P1) -> Training -> Unlearning (P2) -> Verification (P3)
- **Data Prep (P1):** Data owners run Waterfall to generate watermarked datasets with unique keys.
- **Training:** Standard LLM fine-tuning on aggregated watermarked data.
- **Unlearning (P2):** Apply unlearning algorithm to produce modified model.
- **Verification (P3):** Run verification operator on model outputs using forget set's keys.

### Critical path:
Watermarking consistency. If watermarking strength is too low during P1, signal may not survive fine-tuning. If too high, it may degrade model utility.

### Design tradeoffs:
- **Waterfall vs. KGW:** Waterfall is CPU-efficient and supports multi-owner detection, while adapted KGW is orders of magnitude slower for verification and requires GPU.
- **Exact vs. Semantic Duplicates:** System must be tested on "exact duplicate" settings to stress-test watermarking uniqueness.

### Failure signatures:
- **False Negative:** Watermark signal too weak to detect even if data remains (check watermarking strength).
- **False Positive:** Semantically similar data from different owner triggers verification operator (check W2/overlap handling).
- **Utility Collapse:** Model outputs become nonsensical because watermarking perturbation was too aggressive.

### First 3 experiments:
1. **Baseline Verification:** Fine-tune LLM on watermarked data. Verify watermark is detectable (AUROC ≈ 1.0) before any unlearning.
2. **Perfect Unlearning Simulation:** Retrain LLM on retain set only. Verify WaterDrum metric drops to ≈ 0 for forget set (calibration check).
3. **Similarity Stress Test:** Inject exact duplicates of forget set into retain set (owned by different entity with different key). Run verification to ensure metric remains near 0 (robustness/D4 check).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different unlearning algorithms systematically compare when evaluated using WaterDrum across varying data similarity levels and multiple data owners?
- Basis in paper: [explicit] "The empirical evaluations in Sec. 5 focus on (i) but include some preliminary results on (ii) in Sec. 5.1. We leave more systematic investigations of (ii) to future work."
- Why unresolved: Paper provides only preliminary benchmarking of unlearning algorithms (GD, KL, TV, SCRUB) with limited configurations. Comprehensive evaluation across full WaterDrum-Ax dataset with all data owner combinations and similarity levels remains unexplored.
- What evidence would resolve it: Systematic study evaluating all major unlearning algorithms using WaterDrum across all 20 data owners in WaterDrum-Ax, under exact duplicate, semantic duplicate, and no duplicate settings, with quantitative trade-off curves.

### Open Question 2
- Question: Can watermarking methods be designed to natively satisfy WaterDrum's desiderata (W1-W6) rather than requiring adaptation from methods designed for single model owners?
- Basis in paper: [inferred] Table 4 shows adapted methods (KGW, Synth-ID, EXP-edit) perform poorly on separability (D1) and calibration (D2), with negative R² values for Synth-ID and EXP-edit. Table 6 indicates these methods were not designed to satisfy key desiderata like W2 (overlap verifiability) and W4 (unique keys).
- Why unresolved: Current text watermarking methods are designed either for model owners (single watermark) or require significant adaptation to support multiple data owners with unique keys.
- What evidence would resolve it: Development and evaluation of novel watermarking method that achieves AUROC > 0.95 on D1 and R² > 0.95 on D2 while satisfying all six watermarking desiderata.

### Open Question 3
- Question: How does WaterDrum's performance scale when number of data owners increases beyond the 20 tested in WaterDrum-Ax, particularly regarding watermark overlap and verification accuracy?
- Basis in paper: [inferred] Waterfall claims support for up to 10^130274 unique watermark keys, but empirical verification was limited to 100,000 keys. Paper only tests with 20 data owners.
- Why unresolved: Real-world deployments may involve hundreds or thousands of data owners. Paper does not characterize how separability (D1) and calibration (D2) degrade as watermark diversity increases.
- What evidence would resolve it: Experiments measuring WaterDrum's AUROC and R² values with 50, 100, 500, and 1000 data owners, along with verification time scaling curves.

## Limitations

- The watermarking framework's detectability in fine-tuned models is empirically validated rather than theoretically guaranteed, with edge cases (extreme learning rates, longer training) untested.
- The calibration property relies on an empirical linear relationship between data volume and watermark signal strength, which may become non-linear under different training conditions.
- The robustness to similar data (D4) claim depends heavily on the Waterfall framework's ability to support unique keys for each owner, with the underlying mechanism for preventing interference not fully detailed.

## Confidence

**High Confidence:** The separability desideratum (D1) results are well-supported—WaterDrum achieves AUROC near 1.0 across all settings including exact and semantic duplicates, with clear superiority over baseline metrics.

**Medium Confidence:** The calibration property (D2) and feasibility (D3) are well-demonstrated, but the calibration relies on empirical rather than theoretical guarantees. The metric's behavior with extreme data proportions or different watermarking strengths remains untested.

**Low Confidence:** The robustness to similar data (D4) claim depends heavily on the Waterfall framework's ability to support unique keys for each owner (W2). While the paper shows good performance, the underlying mechanism for preventing interference between similar watermarked texts is not fully detailed.

## Next Checks

1. **Watermark detectability stress test:** Fine-tune models with varying learning rates, training durations, and architectures to verify watermark signals remain detectable. Document minimum token thresholds and failure conditions.

2. **Calibration boundary testing:** Validate the linear relationship between forget set proportion and metric score across the full range (0-100%), including extreme cases where only 1-5% of data remains. Check for signal saturation or non-linear behavior.

3. **Cross-framework verification:** Implement verification using alternative watermarking methods (e.g., adapted KGW) to confirm results are not Waterfall-specific. Compare detection rates, computational efficiency, and robustness to data similarity.