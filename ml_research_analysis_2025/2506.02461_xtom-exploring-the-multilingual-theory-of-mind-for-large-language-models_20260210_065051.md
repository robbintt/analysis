---
ver: rpa2
title: 'XToM: Exploring the Multilingual Theory of Mind for Large Language Models'
arxiv_id: '2506.02461'
source_url: https://arxiv.org/abs/2506.02461
tags:
- language
- b-instruct
- llama-3
- qwen-2
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces XToM, a high-quality multilingual Theory
  of Mind benchmark designed to evaluate large language models'' ability to reason
  about mental states across five languages. The benchmark consists of three sub-tasks:
  XToMi (theoretical scenarios), XFANToM (interactive dialogues), and XNegotiationToM
  (real-world negotiations).'
---

# XToM: Exploring the Multilingual Theory of Mind for Large Language Models

## Quick Facts
- arXiv ID: 2506.02461
- Source URL: https://arxiv.org/abs/2506.02461
- Reference count: 40
- Key outcome: LLMs show systematic belief-fact performance gaps across languages, revealing separable capabilities in multilingual proficiency vs. ToM reasoning

## Executive Summary
This paper introduces XToM, a multilingual Theory of Mind benchmark designed to evaluate LLMs' ability to reason about mental states across five languages. The benchmark includes three sub-tasks: XToMi (theoretical scenarios), XFANToM (interactive dialogues), and XNegotiationToM (real-world negotiations). Extensive experiments with ten state-of-the-art models reveal that while models excel at multilingual language understanding, they struggle significantly with multilingual ToM reasoning, showing language-dependent performance gaps and lower accuracy on belief questions compared to fact questions.

## Method Summary
The paper constructs XToM by sampling 300 stories each from three existing ToM datasets (ToMi, FANToM, NegotiationToM), then translating them into five languages using GPT-4o with human correction via MQM framework. Ten LLMs are evaluated across three tasks using zero-shot and chain-of-thought prompting. The evaluation measures accuracy on belief vs. fact questions and cross-lingual consistency. Fine-tuning experiments explore task transfer and cross-lingual generalization capabilities.

## Key Results
- Models achieve >80% accuracy on fact questions but <50% on belief questions across all languages
- Cross-lingual consistency is notably weaker for belief reasoning than fact reasoning
- Fine-tuning on one ToM task improves performance on related tasks, suggesting shared underlying representations
- Cross-lingual transfer shows asymmetric patterns depending on source-target language pairs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fact retrieval and belief reasoning appear to be partially separable cognitive capabilities in current LLMs, with multilingual proficiency not automatically conferring multilingual ToM ability.
- Mechanism: When models process fact questions, they perform context retrieval from provided text. For belief questions, they must additionally track mental state representations across time and agents, a higher-order reasoning step that appears to fail more often in non-English languages.
- Core assumption: Performance gaps reflect capability limitations rather than evaluation artifacts; the benchmark tasks validly distinguish language understanding from ToM reasoning.
- Evidence anchors:
  - [abstract] "while models excel in multilingual language understanding, their ToM performance varies across languages"
  - [section 5.1] "Figure 5 presents the performance comparison of belief and fact questions in XFANToM... all models obtain high accuracy in the fact questions... their performance is significantly lower in the multilingual theory of mind reasoning task"
  - [corpus] Related work confirms ToM reasoning remains challenging for LLMs (Decompose-ToM, EnigmaToM)
- Break condition: If fine-tuning on ToM tasks eliminates the belief-fact gap across all languages without improving fact performance, this would suggest shared rather than separable mechanisms.

### Mechanism 2
- Claim: Cross-linguistic performance variation in ToM tasks may stem from both training data distribution (English-dominant corpora) and cultural-linguistic representation misalignment.
- Mechanism: Models may have learned ToM patterns primarily from English training data; when evaluated in other languages, these patterns transfer incompletely. Additionally, cultural context embedded in source scenarios may lose salience when translated.
- Core assumption: Performance differences across languages reflect genuine capability gaps, not translation quality issues (paper validated translations using MQM framework with 95% threshold).
- Evidence anchors:
  - [abstract] "can LLMs exhibit Multilingual Theory of Mind, which is the capacity to reason about mental states across diverse linguistic contexts?"
  - [section 5] "performance discrepancies across different languages in XToM suggest that their apparent reasoning abilities may be surface-level rather than rooted in robust, language-invariant cognition"
  - [section 5, Table 19 case study] "when festival culture discussions from the FANToM English version were translated to other languages, models struggled to maintain performance"
  - [corpus] Limited direct evidence on cultural-linguistic mechanisms in ToM; related work focuses on monolingual evaluation
- Break condition: If multilingual fine-tuning eliminates language-dependent performance gaps while monolingual fine-tuning in each language does not improve cross-lingual consistency, this would support data distribution as primary cause over cultural-linguistic factors.

### Mechanism 3
- Claim: ToM reasoning capabilities in LLMs appear transferable across tasks and potentially improvable through targeted fine-tuning, though cross-lingual transfer is asymmetric.
- Mechanism: Fine-tuning on one ToM task (e.g., XToMi) improves performance on related tasks (e.g., XFANToM), suggesting shared underlying representations. However, cross-lingual transfer from one language to another shows asymmetric patterns depending on source-target language pair.
- Core assumption: Improvements from fine-tuning reflect capability gains rather than memorization; contamination testing suggests XToMi may have training overlap but XFANToM and XNegotiationToM do not.
- Evidence anchors:
  - [section 5.4] "models fine-tuned on both datasets simultaneously (T. & N.) generally achieve the highest performance across languages... diverse theory of mind training datasets create complementary benefits"
  - [section 5.5] "training in French achieves the best performance in English, while training in English yields the best performance in Japanese"
  - [section 5.5] "some languages exhibit improved performance after multilingual fine-tuning"
  - [corpus] Decompose-ToM and similar work explore task decomposition for ToM enhancement; limited corpus evidence on cross-lingual ToM transfer
- Break condition: If task transfer gains disappear when evaluated on held-out scenarios not resembling fine-tuning data, or if cross-lingual transfer is consistently negative, this would suggest surface pattern matching rather than capability transfer.

## Foundational Learning

- Concept: **Theory of Mind (ToM)**
  - Why needed here: Central construct being measured; understanding false belief tasks (first-order, second-order) is necessary to interpret benchmark design and results.
  - Quick check question: Can you explain why a second-order false belief task (e.g., "Alice thinks that Bob thinks that...") is more complex than a first-order task?

- Concept: **Cross-lingual Transfer in LLMs**
  - Why needed here: The paper's core question is whether ToM capabilities transfer across languages; understanding multilingual representation spaces and transfer learning is essential.
  - Quick check question: Why might a model trained primarily on English struggle with ToM reasoning in Japanese even if it performs well on Japanese language tasks?

- Concept: **Benchmark Contamination and Validity**
  - Why needed here: Paper explicitly addresses potential contamination of source datasets (ToMi, FANToM, NegotiationToM); understanding this threat to validity is critical for interpreting results.
  - Quick check question: What methods does the paper use to detect potential data contamination, and what are their limitations?

## Architecture Onboarding

- Component map:
  Source datasets (ToMi, FANToM, NegotiationToM) -> Sampling (300 stories each) -> GPT-4o translation -> Human correction (MQM framework) -> Quality evaluation (LASER, BERTScore, human MQM) -> Model evaluation (10 LLMs × 5 languages × 2 prompting strategies) -> Consistency analysis

- Critical path:
  1. Source data sampling with contamination verification (Tables 4-5)
  2. GPT-4o translation → human correction using MQM framework
  3. Quality evaluation: automatic (LASER, BERTScore) + human (MQM scoring, 95% threshold)
  4. Model evaluation across languages and prompting strategies
  5. Consistency analysis (same answer across languages for semantically equivalent questions)

- Design tradeoffs:
  - **Translation method**: GPT-4o vs DeepL vs Human-only; paper chose GPT-4o + human correction for efficiency-quality balance
  - **Language selection**: 5 languages (EN, ZH, DE, FR, JA) limited by annotator availability
  - **Task coverage**: Three tasks from theoretical to applied, but excluded Faux Pas, OpenToM variants
  - **Contamination handling**: Retained potentially contaminated ToMi for reference despite detected overlaps

- Failure signatures:
  - **Belief-fact gap**: Model scores >80% on fact questions but <50% on belief questions (Figure 5)
  - **Cross-lingual inconsistency**: Different answers to semantically equivalent questions across languages (Figure 6)
  - **Language preference**: Systematic performance variation by language (e.g., GPT-4o: 11% gap between Japanese and Chinese in XFANToM; Figure 10)
  - **ToM reasoning errors**: Correct fact prediction but incorrect belief prediction (Table 20 error analysis)

- First 3 experiments:
  1. **Baseline evaluation**: Run a target model (e.g., 7B parameter model) on all three XToM subtasks across 5 languages with zero-shot prompting. Compute belief accuracy, fact accuracy, and cross-lingual consistency. This establishes model-specific capability profile and identifies weakest areas.
  2. **Contamination probe**: Before trusting results, replicate contamination detection protocol (Tables 4-5) using two methods: (a) prompt model to generate dataset examples, (b) provide first sentence and ask model to complete. Count matches to ground truth. If contamination detected, focus analysis on uncontaminated subtasks (XFANToM, XNegotiationToM).
  3. **Transfer learning pilot**: Fine-tune model on XToMi in English only, then evaluate on XFANToM across all 5 languages. Compare to baseline to assess task transfer and cross-lingual generalization. Use LoRA (rank 8, α=32, lr=5e-5) following paper's hyperparameters.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can multi-language Theory of Mind (ToM) ability be effectively enhanced in large language models?
- Basis in paper: [explicit] Section 5.5 explicitly raises the question, "How to enhance the multi-language ToM ability?" following experiments on multilingual transfer capabilities.
- Why unresolved: While fine-tuning on specific tasks (XToMi, XNegotiationToM) improves performance, the authors found that scaling models alone or simple monolingual transfer does not robustly solve ToM consistency issues across languages.
- What evidence would resolve it: A novel training paradigm or architectural modification that results in consistent, high-accuracy belief reasoning (narrowing the gap between fact and belief accuracy) across all five evaluated languages simultaneously.

### Open Question 2
- Question: What specific factors drive the distinct language preferences observed in LLMs during ToM reasoning?
- Basis in paper: [inferred] Section 5.6 notes that performance varies significantly by language (e.g., GPT-4o performs better in Japanese than Chinese), which "raises essential questions about their underlying linguistic biases."
- Why unresolved: The authors identify the phenomenon but can only speculate that it stems from variations in pre-training corpus, model architecture, or tokenization strategies, without pinpointing the causal mechanism.
- What evidence would resolve it: Ablation studies that systematically vary the linguistic composition of pre-training data and tokenization methods to demonstrate a causal link to specific ToM performance variances.

### Open Question 3
- Question: Why does statistical mastery of language patterns fail to equate to human-like social cognition in multilingual contexts?
- Basis in paper: [inferred] The Conclusion highlights the "fundamental disconnect" where models excel at multilingual language understanding (Fact questions) but fail at ToM reasoning (Belief questions).
- Why unresolved: The paper establishes the existence of this dissonance but does not fully explain why high multilingual proficiency does not naturally confer the ability to infer mental states consistently across diverse linguistic contexts.
- What evidence would resolve it: Mechanistic interpretability analyses showing that ToM reasoning relies on distinct internal representations from general language fluency, and that current training methods fail to align these representations across languages.

## Limitations

- Contamination concerns: ToMi dataset may overlap with model training data, potentially inflating performance on that task while underestimating cross-task transfer capabilities.
- Cultural-linguistic mechanisms: The paper identifies performance variation across languages but cannot definitively attribute it to training data distribution vs. cultural-linguistic misalignment vs. tokenization strategies.
- Translation artifacts: Despite quality validation, the GPT-4o-based translation pipeline may introduce subtle artifacts that affect model performance differently across languages.

## Confidence

- **High confidence**: The empirical finding that LLMs show systematic belief-fact performance gaps across languages is robust, supported by direct measurements in multiple tasks (Figure 5) and consistent across different model families.
- **Medium confidence**: The claim that training data distribution and cultural-linguistic misalignment cause cross-lingual performance variation is plausible but relies heavily on indirect evidence (translation quality scores, case studies) rather than controlled experiments.
- **Low confidence**: The assertion that ToM capabilities transfer across tasks and improve with targeted fine-tuning is tentative, based on limited fine-tuning experiments with unclear train/test splits and potential contamination effects.

## Next Checks

1. **Replication of contamination-free results**: Repeat the core analysis using only XFANToM and XNegotiationToM (the uncontaminated subtasks) to verify that the belief-fact gap and cross-lingual inconsistencies persist without ToMi data influence. This would confirm whether observed effects are genuine capability limitations or artifacts of training data overlap.

2. **Controlled cross-lingual transfer experiment**: Design an experiment where models are fine-tuned on ToM tasks in multiple languages simultaneously (not just English-to-others), then evaluate transfer in both directions. Compare this to monolingual fine-tuning in each language to isolate whether transfer patterns reflect capability sharing or data distribution effects.

3. **Cultural content isolation test**: Create a modified version of XFANToM where culturally specific references are replaced with neutral content while preserving the underlying ToM reasoning structure. Compare model performance on original vs. neutralized versions across languages to quantify the impact of cultural-linguistic factors on ToM reasoning ability.