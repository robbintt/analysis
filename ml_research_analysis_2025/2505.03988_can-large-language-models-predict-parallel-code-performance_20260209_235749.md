---
ver: rpa2
title: Can Large Language Models Predict Parallel Code Performance?
arxiv_id: '2505.03988'
source_url: https://arxiv.org/abs/2505.03988
tags:
- llms
- performance
- code
- roofline
- source
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether Large Language Models (LLMs) can
  predict GPU code performance by classifying kernels as compute-bound (CB) or bandwidth-bound
  (BB) without hardware profiling. The researchers created a balanced dataset of 340
  CUDA and OpenMP GPU kernels from the HeCBench benchmark suite, labeled with ground-truth
  classifications derived from empirical profiling.
---

# Can Large Language Models Predict Parallel Code Performance?

## Quick Facts
- arXiv ID: 2505.03988
- Source URL: https://arxiv.org/abs/2505.03988
- Reference count: 40
- Primary result: LLMs can classify GPU kernels as compute-bound or bandwidth-bound from source code alone with up to 64% accuracy in zero-shot settings.

## Executive Summary
This study investigates whether Large Language Models can predict GPU code performance by classifying kernels as compute-bound or bandwidth-bound without hardware profiling. The researchers created a balanced dataset of 340 CUDA and OpenMP GPU kernels from the HeCBench benchmark suite, labeled with ground-truth classifications derived from empirical profiling. They evaluated LLMs across four scenarios: with profiling data, zero-shot (source code only), few-shot (with examples), and fine-tuned on a small custom dataset. Results showed state-of-the-art LLMs achieved 100% accuracy with explicit profiling data. Reasoning-capable LLMs significantly outperformed standard LLMs in zero- and few-shot settings, reaching up to 64% accuracy on source code alone. Fine-tuning attempts failed due to insufficient training data. The findings suggest LLMs have potential for source-level roofline performance prediction when runtime profiling is infeasible, though better datasets and prompting strategies are needed for practical application.

## Method Summary
The researchers constructed a dataset of 340 CUDA and OpenMP GPU kernels from the HeCBench benchmark suite, balanced across compute-bound and bandwidth-bound classifications determined through empirical profiling. They evaluated multiple state-of-the-art LLMs across four experimental scenarios: (1) with explicit profiling data as context, (2) zero-shot classification from source code alone, (3) few-shot learning with example kernels, and (4) fine-tuning on a small custom dataset. The evaluation measured classification accuracy for the binary task of determining whether each kernel was compute-bound or bandwidth-bound based on roofline analysis principles.

## Key Results
- State-of-the-art LLMs achieved 100% accuracy when provided with explicit profiling data as context
- Reasoning-capable LLMs reached up to 64% accuracy in zero-shot settings using source code alone
- Standard LLMs significantly underperformed reasoning models in both zero- and few-shot scenarios
- Fine-tuning attempts failed completely due to insufficient training data

## Why This Works (Mechanism)
The success of reasoning-capable LLMs in zero-shot settings likely stems from their ability to extract and analyze code patterns that correlate with computational intensity versus memory access characteristics. These models can identify features such as loop structures, memory access patterns, and computational operations that indicate whether a kernel is compute-bound or bandwidth-bound. The 100% accuracy with profiling data suggests LLMs can effectively incorporate explicit performance metrics when available, while the 64% zero-shot accuracy demonstrates their capacity to reason about performance characteristics from source code alone, albeit with significant room for improvement.

## Foundational Learning
- Roofline model: Why needed - Provides the theoretical framework for classifying kernel performance as compute-bound or bandwidth-bound based on arithmetic intensity and achievable performance relative to hardware limits. Quick check - Can you explain how arithmetic intensity determines whether a kernel is compute- or memory-bound?
- CUDA/OpenMP GPU kernels: Why needed - The source code structure and patterns that LLMs must analyze to infer performance characteristics. Quick check - Can you identify key features in GPU code that indicate computational intensity versus memory access patterns?
- Roofline analysis: Why needed - The empirical methodology used to establish ground-truth labels for the dataset by measuring actual kernel performance characteristics. Quick check - Can you describe how hardware counters are used to determine whether a kernel is compute- or bandwidth-bound?

## Architecture Onboarding
Component map: Dataset (340 kernels) -> LLM models (reasoning and standard) -> Classification task (CB/BB) -> Evaluation metrics (accuracy)
Critical path: Kernel source code → LLM input → Model prediction → Classification accuracy
Design tradeoffs: Binary classification simplicity vs. continuous performance spectrum complexity; small curated dataset vs. larger but potentially noisier real-world codebases
Failure signatures: Low accuracy on mixed-bound scenarios; dependence on explicit profiling data for perfect performance; inability to generalize beyond benchmark patterns
First experiments: (1) Test reasoning models on kernels with mixed computational and memory characteristics; (2) Evaluate performance on continuous performance metrics rather than binary classification; (3) Assess cost-benefit tradeoff versus traditional profiling tools

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, though the limitations section implies several areas for future investigation, including the need for larger and more diverse datasets, improved prompting strategies for reasoning models, and exploration of continuous performance prediction beyond binary classification.

## Limitations
- Dataset size of 340 kernels remains relatively small for training and evaluating deep learning models
- Fine-tuning attempts failed entirely due to data scarcity, limiting model adaptation capabilities
- Binary classification task oversimplifies the continuous nature of GPU performance characteristics
- Focus on benchmark kernels may not generalize to diverse real-world codebases
- No evaluation of model robustness across different GPU architectures or programming paradigms

## Confidence
- Confidence in core findings: Medium (methodology is sound but sample size and task complexity are limited)
- Confidence in practical applicability: Low (significant performance gap and data constraints exist)

## Next Checks
(1) Expand the dataset to include diverse real-world GPU codebases and mixed-bound scenarios
(2) Test model performance on continuous performance metrics beyond binary classification
(3) Conduct a cost-benefit analysis comparing LLM-based prediction against traditional profiling tools in realistic deployment scenarios