---
ver: rpa2
title: Multi-Tool Analysis of User Interface & Accessibility in Deployed Web-Based
  Chatbots
arxiv_id: '2506.04659'
source_url: https://arxiv.org/abs/2506.04659
tags:
- accessibility
- chatbots
- chatbot
- user
- insights
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the user interface and accessibility of 106
  deployed web-based chatbots using multiple automated and manual auditing tools.
  The analysis reveals that over 80% of chatbots exhibit critical accessibility issues,
  with 45% suffering from missing semantic structures or ARIA role misuse.
---

# Multi-Tool Analysis of User Interface & Accessibility in Deployed Web-Based Chatbots

## Quick Facts
- arXiv ID: 2506.04659
- Source URL: https://arxiv.org/abs/2506.04659
- Reference count: 32
- Primary result: Over 80% of analyzed chatbots exhibit critical accessibility issues, with 45% suffering from missing semantic structures or ARIA role misuse

## Executive Summary
This study evaluates the user interface and accessibility of 106 deployed web-based chatbots using multiple automated and manual auditing tools. The analysis reveals that over 80% of chatbots exhibit critical accessibility issues, with 45% suffering from missing semantic structures or ARIA role misuse. While accessibility scores correlate strongly between tools (r = 0.861), performance scores do not (r = 0.436), highlighting the need for a multi-tool evaluation approach. Chatbots with more than ten WCAG violations score 24% lower on performance metrics, indicating that accessibility failures negatively impact both usability and technical performance. The findings emphasize the need for integrating accessibility early in design and standardizing best practices to ensure inclusive and robust chatbot interfaces.

## Method Summary
The study employed a hybrid evaluation approach using automated tools (Google Lighthouse, PageSpeed Insights, SiteImprove Accessibility Checker) plus manual audits (Microsoft Accessibility Insights) on 106 web-based chatbots across healthcare, education, and customer service domains. Each chatbot URL was scanned via Chrome DevTools and browser extensions, with results recorded in a structured dataset for comparative and correlation analysis. The methodology focused on measuring Performance, Accessibility, Best Practices, and SEO scores, along with detailed WCAG violation counts.

## Key Results
- Over 80% of chatbots exhibit critical accessibility issues
- 45% of chatbots suffer from missing semantic structures or ARIA role misuse
- Strong correlation in accessibility scores between tools (r = 0.861) but weak correlation in performance scores (r = 0.436)
- Chatbots with more than ten WCAG violations score 24% lower on performance metrics

## Why This Works (Mechanism)

### Mechanism 1: Multi-Tool Correlation Redundancy
Using multiple, complementary automated evaluation tools with strong metric correlations increases confidence in identifying systemic accessibility issues, whereas relying on a single tool creates measurement blind spots. A high correlation between tools for a specific metric suggests they reliably detect the same class of issues, while low correlation indicates different aspects of the system are being captured. This triangulation leverages agreement and investigates disagreement to find the true signal.

### Mechanism 2: Structural Deficits as Universal Performance Degraders
Critical accessibility failures, particularly those related to semantic structure, are correlated with degraded technical performance, suggesting that poor information architecture negatively impacts both inclusive use and system efficiency. Missing semantic structures force browsers and assistive technologies into inefficient rendering or parsing paths, creating computational overhead that negatively impacts performance metrics like LCP or CLS.

### Mechanism 3: Automated-Only Audits Miss User-Facing Barriers
A purely automated accessibility audit is insufficient because it cannot reliably detect dynamic, interactive, or experiential barriers that require human judgment or interaction to confirm. Automated scanners can flag missing attributes but cannot easily determine if custom widgets are operable via keyboard or if focus order makes logical sense to users. Manual audits are required to verify these interactive states prevalent in dynamic chatbot interfaces.

## Foundational Learning

- **Concept: WCAG (Web Content Accessibility Guidelines)**
  - Why needed: The entire evaluation framework is built on WCAG 2.1 compliance
  - Quick check: If a chatbot interface has a rapidly flashing button, which WCAG principle is most relevant?

- **Concept: ARIA (Accessible Rich Internet Applications)**
  - Why needed: The paper identifies "ARIA role misuse" as a critical issue in 45% of chatbots
  - Quick check: When should you use a native HTML `<button>` element instead of adding `role="button"` to a `<div>`?

- **Concept: The Document Object Model (DOM) and Semantic Structure**
  - Why needed: Automated tools analyze the DOM to diagnose "missing semantic structures"
  - Quick check: How does a screen reader use the semantic structure of the DOM to present a chat conversation to a user?

## Architecture Onboarding

- **Component map:** Chat Container -> Message History -> Input Field -> Submit Control
- **Critical path:** Interaction loop for a user with a screen reader: Discover widget container -> Navigate to input field -> Submit text -> Receive announcement of new message
- **Design tradeoffs:**
    1. Custom vs. Native Components: Custom UI requires extensive ARIA work, native provides accessibility for free
    2. Design System vs. Point Solution: Component library scales but requires investment, individual fixes are faster initially
    3. Announcement Verbosity: Polite vs. assertive `aria-live` regions trade information flow vs user annoyance
- **Failure signatures:**
    1. Silent Updates: Visual response without screen reader announcement (missing `aria-live`)
    2. Keyboard Trap: Cannot tab out of chat widget (improper focus management)
    3. Mystery Meat Navigation: Screen reader announces only "Button" (missing `aria-label`)
- **First 3 experiments:**
    1. Baseline Hybrid Audit: Run Lighthouse and manual screen reader test on current chatbot
    2. Semantic Fix Iteration: Fix unlabeled input field and observe score changes
    3. Tool Comparison Test: Run multi-tool suite on competitor chatbots to compare scores

## Open Questions the Paper Calls Out

- **Open Question 1:** Do app-based chatbots exhibit similar levels of accessibility noncompliance as web-based interfaces? The current study focused on web-based interfaces, with plans to extend analysis to app-based chatbots as future work.

- **Open Question 2:** To what extent do automated tool-detected accessibility issues impact actual task success and satisfaction of users with assistive technologies? The study acknowledges automated tools cannot replicate lived user experiences and proposes longitudinal studies with diverse users.

- **Open Question 3:** Does remediation of high-frequency WCAG violations directly cause improvements in performance metrics like TBT and CLS? The study found correlation but cannot determine if fixing accessibility failures actively improves technical performance due to its cross-sectional nature.

## Limitations

- Unknown chatbot URLs prevent exact replication of correlation values
- Assumption that automated tool metrics are valid proxies for accessibility and performance constructs
- Correlation between WCAG violations and performance scores may be confounded by other factors like third-party scripts

## Confidence

- **High confidence:** 45% of chatbots have missing semantic structures or ARIA role misuse (directly observable through DOM analysis)
- **Medium confidence:** Correlation values between tools (r=0.861 and r=0.436) are reliable for tested sample but may vary with different populations
- **Low confidence:** 24% performance degradation from WCAG violations assumes causation from correlation

## Next Checks

1. Replicate correlation analysis: Run multi-tool suite on new sample of 50-100 chatbots and compute pairwise correlations to verify reported r-values
2. Causal relationship test: Fix accessibility issues in 10 high-violation chatbots while keeping other factors constant, then re-run performance tests
3. Manual validation study: Conduct user study with screen reader users testing 5 chatbots with high automated scores but known interactive barriers to determine what percentage of issues automated tools miss