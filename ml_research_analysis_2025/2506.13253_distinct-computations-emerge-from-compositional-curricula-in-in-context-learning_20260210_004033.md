---
ver: rpa2
title: Distinct Computations Emerge From Compositional Curricula in In-Context Learning
arxiv_id: '2506.13253'
source_url: https://arxiv.org/abs/2506.13253
tags:
- task
- compositional
- curriculum
- learning
- in-context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates how in-context curricula\u2014structured\
  \ sequences presenting subtasks before their composition\u2014affect transformer\
  \ models' ability to perform compositional in-context learning. The authors design\
  \ a modular double-exponential task composed of two single-exponential subtasks\
  \ and compare models trained with and without in-context curricula."
---

# Distinct Computations Emerge From Compositional Curricula in In-Context Learning

## Quick Facts
- arXiv ID: 2506.13253
- Source URL: https://arxiv.org/abs/2506.13253
- Reference count: 40
- Key outcome: In-context curricula enable transformers to learn compositional in-context learning, achieving near-perfect zero-shot accuracy on unseen compositional tasks by representing and reusing intermediate computation values.

## Executive Summary
This paper investigates how in-context curricula—structured sequences presenting subtasks before their composition—affect transformer models' ability to perform compositional in-context learning. The authors design a modular double-exponential task composed of two single-exponential subtasks and compare models trained with and without in-context curricula. Models trained with curricula show significantly higher zero-shot accuracy on unseen compositional tasks (near 1.0 accuracy at zero-shot versus random performance without curricula) and greater robustness across compositional task examples. Linear probe analyses reveal that curriculum-trained models represent intermediate computation values from subtasks, enabling compositional computation, while vanilla-trained models do not. The degree of compositional versus standard few-shot learning depends on curriculum design, with longer compositional task blocks showing mixed strategies. These findings demonstrate that data structure in context—specifically compositional curricula—can induce more compositional forms of in-context learning in transformers.

## Method Summary
The authors design a modular double-exponential task where y = b^(a^x) mod P is composed of two single-exponential subtasks. They train transformer models with two different in-context training curricula: (1) a curriculum that presents m examples of each subtask followed by n compositional task examples, and (2) a vanilla approach presenting only compositional task examples. Both curricula use the same total number of exemplars. Models are 8-layer transformers with 128 hidden dimensions and 8 attention heads. Key analyses include zero-shot accuracy evaluation on held-out compositional tasks and linear probe experiments to decode intermediate computation values from hidden states. The authors also run mismatch experiments where task types in the context differ from what the model was trained on.

## Key Results
- Curriculum-trained models achieve near-perfect zero-shot accuracy on unseen compositional tasks, while vanilla-trained models perform at chance level.
- Linear probe analyses show curriculum-trained models represent intermediate computation values (a^x mod P and parameter b) that vanilla-trained models do not.
- The degree of compositional versus standard few-shot learning depends on curriculum design, with longer compositional task blocks leading to mixed strategies.
- Earlier transformer layers (5-6) encode intermediate computation values while later layers produce final outputs, suggesting layer-wise compositional processing.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: In-context curricula enable zero-shot compositional generalization by providing unambiguous subtask information that the model learns to encode and reuse.
- Mechanism: When training sequences present subtask examples (single exponentials) followed by compositional task examples (double exponentials), the model learns to extract and represent intermediate computation values (a^x mod P and parameter b) during the subtask blocks. These representations transfer to earlier layers during the compositional task block. Linear probes show high decodability at the zero-shot position of the compositional task, but not in vanilla models.
- Core assumption: Subtask representations can be reused for composition only when subtask and compositional examples appear in the same context during training.
- Evidence anchors:
  - [abstract] "Linear probe analyses reveal that curriculum-trained models represent intermediate computation values from subtasks, enabling compositional computation, while vanilla-trained models do not."
  - [section 3.2] "The intermediate computation values involving the subtasks (a, b) are highly decodable in compositional task block (shot 20-23), suggesting that the subtask representation inferred from the curriculum is utilized in the compositional task."
  - [corpus] Weak direct evidence; related work on curriculum learning discusses data ordering effects but does not address this specific subtask representation reuse mechanism.
- Break condition: If subtask and compositional task examples never co-occur in context during training, the model fails to represent intermediate values and cannot perform zero-shot compositional inference.

### Mechanism 2
- Claim: Compositional strategy exists on a spectrum modulated by curriculum design (relative lengths of subtask vs. compositional task blocks).
- Mechanism: The curriculum length ratio (m:m:n) affects which task is learned first. With longer compositional blocks (e.g., 4-4-16), the model learns standard few-shot ICL before subtasks, developing zero-shot ability only after subtask learning—resulting in mixed strategies. With shorter compositional blocks (e.g., 10-10-4), compositional learning follows subtask learning, yielding consistent compositional strategy.
- Core assumption: The model prioritizes learning based on information density and correlational structure available in context.
- Evidence anchors:
  - [section 3.4] "With long compositional task sequence (4-4-16), the model is capable of vanilla few-shot learning on compositional task before the subtasks being learned... On the other hand, with shorter compositional task length (10-10-4), the rapid learning of compositional task happens only after the subtask learning."
  - [section 3.3] "This implies that the model can still make some inference without compositional strategy, suggesting the model employs possibly both strategies."
  - [corpus] No direct corpus evidence for spectrum-like strategy modulation in ICL.
- Break condition: If compositional task blocks are too short, the model may fail to learn any strategy; if too long, it may rely primarily on vanilla few-shot learning.

### Mechanism 3
- Claim: Layer-wise processing supports compositional computation, with earlier layers (5-6) encoding intermediate values and later layers producing final output.
- Mechanism: Subtask intermediate values peak in earlier layers during the compositional task block, while task output decoding peaks in the final layer. This suggests functional specialization for subtask integration before output generation.
- Core assumption: Transformer depth supports multi-step compositional computation when trained with appropriate data structure.
- Evidence anchors:
  - [section 3.2] "The highest decoding of the intermediate computation values in the compositional task are not achieved in the last layer but in the earlier layers (layer 5-6). This suggests the layer-wise processing of subtask information."
  - [figure 4 description] "Highest decodability of intermediate values in compositional task block come from not the final layer but earlier layers."
  - [corpus] No direct corpus evidence for layer-wise compositional computation in ICL.
- Break condition: Insufficient depth or curricula that don't incentivize intermediate representation prevent compositional generalization.

## Foundational Learning

- Concept: **In-Context Learning (ICL)**
  - Why needed here: The paper investigates how transformers learn functions from in-context exemplars without weight updates. Understanding ICL as meta-learned few-shot learning is essential.
  - Quick check question: Can you explain why ICL is considered a form of meta-learning rather than standard supervised learning?

- Concept: **Compositional Generalization**
  - Why needed here: The core question is when models can systematically compose learned subtasks to solve novel compositional tasks.
  - Quick check question: What distinguishes compositional generalization from simple interpolation within a single task?

- Concept: **Linear Probing**
  - Why needed here: Primary mechanistic evidence comes from linear probes decoding intermediate values from hidden representations.
  - Quick check question: If a linear probe cannot decode a variable from hidden states, does that mean the information is absent?

## Architecture Onboarding

- Component map:
  - Input sequence: [subtask-a block (m)] → [subtask-b block (m)] → [compositional block (n)]
  - Model: 8-layer transformer with 128 hidden dim, 8 heads, sinusoidal positional embeddings
  - Training: Next-token prediction on all pairs, Adam optimizer, LR 7.5e-4, batch 512, 2e8 sequences
  - Evaluation: Unseen (a, b) pairs, 80/20 train/test split, zero-shot accuracy on compositional task
  - Analysis: Linear probes on hidden states; mismatch experiments; attention pattern visualization

- Critical path:
  1. Implement modular double-exponential task with configurable curriculum
  2. Train models with matched exemplar counts and loss weighting
  3. Evaluate zero-shot accuracy on held-out combinations
  4. Train linear probes to decode intermediate values
  5. Run mismatch experiments to test causal reliance on subtask representations

- Design tradeoffs:
  - Curriculum length (m, n): Shorter n forces compositional strategy; longer n enables mixing
  - Modulus P: Larger P increases difficulty, requiring longer subtask blocks
  - Loss weighting: Must balance subtask vs. compositional task loss for fair comparison

- Failure signatures:
  - Vanilla models: Gradual error reduction but no zero-shot; errors saturate non-zero
  - Curriculum with insufficient m: High errors in early blocks (subtasks not learned)
  - Mismatch experiments: Short-block curriculum models fail completely; long-block models show partial improvement

- First 3 experiments:
  1. Replicate curriculum vs. vanilla for P=59 with (10-10-4), measuring zero-shot accuracy
  2. Train linear probes decoding intermediate values in compositional block
  3. Run mismatch experiments on (10-10-4) and (4-4-16) models to confirm differential subtask reliance

## Open Questions the Paper Calls Out
None

## Limitations
- Experimental scope is narrow, studying only one compositional task (double exponential) with a relatively small transformer architecture.
- Linear probe methodology cannot definitively prove causal necessity of intermediate values for compositional generalization.
- The mechanism by which curricula enable subtask representation reuse is described but not fully explained mechanistically.

## Confidence
**High Confidence:**
- Curriculum training produces near-perfect zero-shot accuracy on compositional tasks while vanilla training does not.
- Curriculum-trained models represent intermediate computation values that vanilla-trained models do not.
- Longer compositional task blocks in curriculum design lead to mixed strategies combining compositional and vanilla ICL approaches.

**Medium Confidence:**
- Subtask representations from curriculum training are causally reused for compositional computation (supported by mismatch experiments but not definitively proven).
- Layer-wise processing supports compositional computation with earlier layers encoding intermediate values.
- The spectrum-like modulation of compositional versus vanilla strategy depends on curriculum design parameters.

**Low Confidence:**
- These findings will generalize to more complex compositional tasks beyond double exponentials.
- The same mechanisms operate in larger transformer architectures.
- Linear probe results definitively prove the presence/absence of information in hidden states.

## Next Checks
1. **Causal Mechanism Validation**: Run targeted ablation experiments where intermediate values are explicitly masked or perturbed in the hidden states of curriculum-trained models during the compositional task block. If compositional accuracy drops significantly while preserving other information, this would provide stronger causal evidence for the reuse hypothesis.

2. **Generalization Across Architectures**: Replicate the core findings (zero-shot accuracy differences, linear probe patterns) using a 12-layer transformer or a model with 256 hidden dimensions. This would test whether the compositional benefits of curricula depend on the specific architecture used.

3. **Complexity Gradient Test**: Design a sequence of compositional tasks with increasing complexity (e.g., triple exponentials, nested functions) and test whether curricula continue to provide benefits. This would establish whether the compositional advantages scale with task complexity or are specific to the double-exponential case.