---
ver: rpa2
title: Understanding Gated Neurons in Transformers from Their Input-Output Functionality
arxiv_id: '2505.17936'
source_url: https://arxiv.org/abs/2505.17936
tags:
- layer
- neurons
- wout
- wgate
- enrichment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes a parameter-based method to classify MLP neurons
  in transformer language models based on their input-output functionality, focusing
  on gated activation functions like SwiGLU and GeGLU. The method examines the cosine
  similarity between the three weight vectors of each neuron (linear input, gate,
  and output) to identify six IO classes: enrichment, conditional enrichment, depletion,
  conditional depletion, proportional change, and orthogonal output.'
---

# Understanding Gated Neurons in Transformers from Their Input-Output Functionality

## Quick Facts
- **arXiv ID**: 2505.17936
- **Source URL**: https://arxiv.org/abs/2505.17936
- **Reference count**: 40
- **Key outcome**: This paper proposes a parameter-based method to classify MLP neurons in transformer language models based on their input-output functionality, focusing on gated activation functions like SwiGLU and GeGLU.

## Executive Summary
This paper introduces a parameter-based method to classify MLP neurons in transformer language models based on their input-output functionality. By examining the geometric relationships between the three weight vectors of each gated neuron (linear input, gate, and output), the method identifies six functional classes: enrichment, conditional enrichment, depletion, conditional depletion, proportional change, and orthogonal output. Applied to 12 models, the study finds distinct layer-wise distributions where conditional enrichment dominates early-middle layers while depletion becomes prevalent in later layers, suggesting a correspondence with stages of inference like feature engineering and residual sharpening.

## Method Summary
The method extracts the three weight vectors (w_in, w_gate, w_out) for each neuron in MLP layers using gated activation functions (SwiGLU/GeGLU). It computes three cosine similarities between these vectors to classify neurons into six IO classes based on directional alignment patterns. The classification uses a threshold τ = 0.5 to distinguish "close to zero" from "clearly non-zero" relationships. Layer-wise distributions are analyzed across 12 models to identify patterns in how different IO classes are distributed throughout the network depth.

## Key Results
- Conditional enrichment neurons dominate early-middle layers across all 12 tested models
- Depletion neurons become prevalent in later layers, potentially implementing residual sharpening
- The method successfully classifies gated neurons using only weight parameters without requiring activation data
- Six distinct IO functionality types emerge from geometric relationships between weight vectors

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Gated neurons can be classified into six IO functionality types based solely on the geometric relationships between their three weight vectors.
- Mechanism: The method computes three cosine similarities—cos(w_in, w_out), |cos(w_gate, w_out)|, and cos(w_gate, w_in)—to determine whether a neuron writes to directions it detects (input manipulation) or orthogonal directions. A threshold τ = 0.5 distinguishes "close to zero" from "clearly non-zero" relationships.
- Core assumption: Weight vector geometry meaningfully reflects neuron functional behavior during inference.
- Evidence anchors: [abstract], [Section 4.2]
- Break condition: If weight vectors don't capture semantic relationships due to superposition or layer normalization distortion.

### Mechanism 2
- Claim: Conditional enrichment neurons dominate early-middle layers across all 12 tested models, suggesting they support feature engineering or representation enrichment.
- Mechanism: These neurons have cos(w_in, w_out) ≈ 1 and cos(w_gate, w_out) ≈ 0 (gate orthogonal to output, input aligned with output). They detect two independent directions via w_gate and w_in, and when both conditions are met, add a similar direction to the residual stream.
- Core assumption: Early-middle layers perform enrichment that builds toward factual recall.
- Evidence anchors: [Section 5.1], [Figure 1]
- Break condition: If the correspondence between enrichment neurons and "feature engineering" stages is not causal.

### Mechanism 3
- Claim: Depletion neurons become prevalent in later layers, potentially implementing residual sharpening by removing unnecessary attributes.
- Mechanism: These neurons have cos(w_in, w_out) ≈ -1, meaning they write the opposite of what they detect. When activated, they reduce the presence of the detected direction in the residual stream.
- Core assumption: Later layers perform "residual sharpening" to remove attributes not directly needed for next-token prediction.
- Evidence anchors: [Section 5.2], [Section 7.5]
- Break condition: If depletion neurons serve different functions (e.g., conflict resolution) rather than sharpening.

## Foundational Learning

- Concept: Gated activation functions (SwiGLU/GeGLU)
  - Why needed here: Unlike ReLU, these compute SwiGLU(x_gate, x_in) = Swish(x_gate) · x_in, giving each neuron two "reading" weight vectors instead of one. This enables more complex IO relationships including conditional activation and double-checking.
  - Quick check question: Can you explain why a SwiGLU neuron can have strongly positive or strongly negative activations depending on context?

- Concept: Residual stream and skip connections
  - Why needed here: The paper's entire theoretical framework treats neurons as units that "read" from and "write" to the residual stream. Enrichment adds to detected directions; depletion removes them.
  - Quick check question: If a depletion neuron detects direction v in the residual stream and activates, what happens to v's presence after the neuron writes?

- Concept: Cosine similarity as a measure of directional alignment
  - Why needed here: The classification method relies entirely on cosine similarities between weight vectors. cos ≈ 1 means aligned, cos ≈ 0 means orthogonal, cos ≈ -1 means opposite.
  - Quick check question: If w_in and w_out have cos = -0.85, what IO class does this suggest, and what functional behavior would you expect?

## Architecture Onboarding

- Component map:
  - Weight extraction: Pull w_in, w_gate, w_out from model MLP layers (gated architectures only)
  - Cosine computation: Calculate three pairwise cosine similarities per neuron
  - Classification logic: Apply τ = 0.5 threshold to assign one of six IO classes (plus atypical variants)
  - Layer-wise aggregation: Bin neurons by layer to identify distributional patterns
  - Visualization: Scatter plots (cos(w_gate, w_out) vs cos(w_in, w_out)) and distribution plots

- Critical path:
  1. Verify model uses gated activation (SwiGLU/GeGLU)—non-gated models require adapted analysis
  2. Extract weights correctly (handle transposes, identify gate vs linear input)
  3. Compute cosines with proper normalization
  4. Apply classification with awareness of "atypical" cases where cos(w_gate, w_in) doesn't match expectations

- Design tradeoffs:
  - Threshold τ = 0.5 is permissive—captures more neurons but may include weaker signals
  - Classification is exhaustive but semantic interpretation requires case-by-case validation
  - Parameter-based analysis is efficient but ignores activation patterns; combine with activation-based methods for full picture

- Failure signatures:
  - All neurons classified as "orthogonal output" → likely wrong weight extraction or wrong activation function assumption
  - No layer-wise pattern → may indicate model architecture differs significantly from tested models
  - Atypical classifications dominate → threshold may be inappropriate for this model

- First 3 experiments:
  1. Replicate Figure 1 (median cos(w_in, w_out) by layer) on a new gated model to verify cross-model consistency
  2. Select a conditional enrichment neuron, project its weight vectors to vocabulary space, and manually inspect top tokens to validate semantic coherence
  3. Ablate a cluster of enrichment neurons in an early-middle layer and measure impact on factual recall tasks to test the hypothesized correspondence

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do conditional enrichment neurons causally facilitate "feature engineering," and do depletion neurons drive "residual sharpening" during inference?
- Basis in paper: [explicit] The authors explicitly state in the Conclusion: "We will do ablation experiments to conclusively show if, as we hypothesized, the conditional enrichment neurons... are responsible for representation enrichment and the depletion neurons... contribute to residual sharpening."
- Why unresolved: The current study is observational, correlating weight configurations with layer depth. While it identifies the distribution of classes, it does not intervene to prove these neurons are necessary for the hypothesized "stages of inference."
- What evidence would resolve it: Targeted ablation studies where specific neuron classes are disabled in their respective layers, followed by an analysis of performance degradation on tasks specifically requiring representation enrichment or output sharpening.

### Open Question 2
- Question: How do the distributions of these IO classes evolve dynamically throughout the model training process?
- Basis in paper: [explicit] The authors list as a direction for future work: "We also plan to investigate the evolution of IO functionalities during model training."
- Why unresolved: The study provides a static analysis of 12 pre-trained models. It does not track when or how neurons differentiate into these specific IO classes (e.g., enrichment vs. depletion) during the optimization steps.
- What evidence would resolve it: A longitudinal analysis of weight vectors at various training checkpoints to observe if classes emerge gradually or abruptly, and if their layer-wise distribution shifts as the model learns.

### Open Question 3
- Question: How do individual neurons interact within and across their IO classes to form complex functional circuits?
- Basis in paper: [explicit] The conclusion notes the intent to "go beyond the analysis of single neurons and address the question of how neurons work together within and across IO classes."
- Why unresolved: The current methodology classifies neurons individually based on isolated weight vectors. It does not map the connectivity or collective behavior of these neurons to determine if they act independently or as coordinated groups.
- What evidence would resolve it: Circuit-level analysis tracking how the output of one IO class feeds into the input of another, determining if specific sequences of IO classes (e.g., enrichment followed by depletion) form standard computational patterns.

## Limitations
- The parameter-based classification cannot distinguish neurons with identical weight geometries but different activation patterns
- The correspondence between IO classes and "stages of inference" lacks rigorous validation beyond distributional patterns
- The method's generalizability is limited to gated activation functions (SwiGLU/GeGLU), not standard MLP architectures

## Confidence
*High Confidence*: The technical methodology for computing cosine similarities between weight vectors is sound and reproducible. The observation that conditional enrichment neurons dominate early-middle layers and depletion neurons appear in later layers is empirically supported across 12 models.

*Medium Confidence*: The functional interpretations of enrichment vs depletion neurons have theoretical grounding but lack rigorous validation. The proposed correspondence with "stages of inference" (feature engineering → residual sharpening) is plausible but not conclusively demonstrated.

*Low Confidence*: The broader implications for model interpretability and the claim that this provides a complete understanding of gated neuron functionality are overstated given the method's limitations and lack of activation-based validation.

## Next Checks
1. Conduct ablation studies on conditional enrichment neurons in early-middle layers to test whether removing them degrades factual recall performance, directly validating the proposed stages-of-inference correspondence.

2. Apply the same classification method to non-gated activation functions (e.g., ReLU, GeLU) and compare the resulting distributions to determine whether the method generalizes beyond gated architectures.

3. Perform activation-based clustering on neurons classified as enrichment/depletion to verify that neurons with similar weight geometries exhibit similar activation patterns during inference on diverse prompts.