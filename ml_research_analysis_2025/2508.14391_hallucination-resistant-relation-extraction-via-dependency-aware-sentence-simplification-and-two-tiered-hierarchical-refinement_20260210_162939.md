---
ver: rpa2
title: Hallucination-Resistant Relation Extraction via Dependency-Aware Sentence Simplification
  and Two-tiered Hierarchical Refinement
arxiv_id: '2508.14391'
source_url: https://arxiv.org/abs/2508.14391
tags:
- relation
- extraction
- depth
- sentence
- entity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of hallucinations in large language
  model (LLM)-based relation extraction, where models frequently predict spurious
  relations for entity pairs that have no true relation. The authors propose DEPTH,
  a two-stage framework that integrates dependency-aware sentence simplification and
  two-tiered hierarchical refinement.
---

# Hallucination-Resistant Relation Extraction via Dependency-Aware Sentence Simplification and Two-tiered Hierarchical Refinement

## Quick Facts
- **arXiv ID:** 2508.14391
- **Source URL:** https://arxiv.org/abs/2508.14391
- **Reference count:** 40
- **Primary result:** Reduces hallucination rates to average 7.9% and achieves 9.3% micro-F1 improvement over LLM baselines

## Executive Summary
DEPTH addresses hallucination in LLM-based relation extraction by combining dependency-aware sentence simplification with two-tiered hierarchical refinement. The framework first simplifies sentences using shortest dependency paths to reduce syntactic noise, then refines predictions using global sentence-level context. A causality-driven reward model mitigates reward hacking during reinforcement learning. Experiments on eight benchmarks show DEPTH significantly reduces hallucinations while maintaining high precision, demonstrating strong generalization across domains and robustness to backbone scaling.

## Method Summary
DEPTH uses a two-stage framework where the Grounding module extracts relations per entity pair using shortest dependency path (SDP)-based sentence simplification, while the Refinement module aggregates and corrects predictions using full sentence context. A causality-driven reward model is trained to mitigate reward hacking by focusing only on reward-relevant prompt/response components. The framework is trained via PPO with OpenRLHF, using Qwen2.5-14B-Instruct as the backbone. Hyperparameters include RM lr=9e-6, actor lr=5e-7, critic lr=9e-6, batch=64.

## Key Results
- Reduces hallucination rates to average 7.9% across eight benchmarks
- Achieves 9.3% micro-F1 improvement over existing LLM-based baselines
- Demonstrates strong generalization across domains and robustness to backbone scaling
- Ablation shows causal reward modeling reduces hallucination rate from 31.4% to 7.9%

## Why This Works (Mechanism)

### Mechanism 1: Dependency-Aware Sentence Simplification
Simplifying sentences around the shortest dependency path (SDP) between entity pairs reduces syntactic noise and improves relation extraction accuracy. A standard dependency parser extracts the SDP connecting two entities, and the sentence is distilled to an SDP-centered context, removing words unrelated to the relational semantics. This minimal context is fed to the LLM for prediction, reducing distraction from irrelevant syntactic structures.

### Mechanism 2: Two-Tiered Hierarchical Refinement
Aggregating local entity-pair predictions and then revising them with sentence-level global context corrects omissions and inconsistencies. The Refinement module takes all candidate relations predicted by the Grounding module for a sentence and prompts the same LLM to perform a three-stage check: omission, contradiction, and misclassification, using the full sentence's dependency structure and global semantic constraints.

### Mechanism 3: Causality-Driven Reward Modeling for RLHF
Training a reward model on only the causally factorized reward-relevant components of prompts and responses mitigates reward hacking and yields a more reliable alignment signal. The prompt and response are split via fixed templates into reward-relevant parts (task definition, simplified sentence, entities, SDP description, and predicted label) and irrelevant parts (examples, explanations), and the reward model is trained only on the relevant components.

## Foundational Learning

- **Concept: Dependency Parsing & Shortest Dependency Path (SDP)**
  - **Why needed here:** The entire Grounding module is built on the premise that syntactic structure, specifically the SDP, provides a distilled signal for relational semantics. Without understanding dependency trees, one cannot implement or debug the simplification process.
  - **Quick check question:** Given the sentence "The cat sat on the mat," can you draw its dependency tree and identify the SDP between "cat" and "mat"?

- **Concept: Reinforcement Learning from Human Feedback (RLHF)**
  - **Why needed here:** DEPTH uses RLHF (PPO algorithm) to fine-tune the LLM. The quality of the reward model is central to the framework's success. Understanding the reward modeling and policy optimization loop is essential for training.
  - **Quick check question:** Explain the role of the reward model in the RLHF process. What is "reward hacking" and why is it a problem?

- **Concept: Causal Factorization in Machine Learning**
  - **Why needed here:** The paper's key innovation for robustness is the causal factorization of prompts. This concept is critical for understanding why their reward model is claimed to be more reliable and how to apply it.
  - **Quick check question:** In the context of a reward model, what does it mean to disentangle reward-relevant from reward-irrelevant factors? Give an example of each for a relation extraction task.

## Architecture Onboarding

- **Component map:** Sentence + Entity Pairs → [Parser] → SDP & Full Tree → [Grounding: Simplify & Predict per pair] → [Aggregate Predictions] → [Refinement: Global Check] → Final Relations. RLHF training is an offline step that produces the aligned LLM used in Grounding and Refinement.

- **Critical path:** Sentence + Entity Pairs → [Parser] → SDP & Full Tree → [Grounding: Simplify & Predict per pair] → [Aggregate Predictions] → [Refinement: Global Check] → Final Relations. RLHF training is an offline step that produces the aligned LLM used in Grounding and Refinement.

- **Design tradeoffs:**
  1. **SDP vs. Full Context:** Simplification via SDP reduces noise but risks losing critical information outside the path. The Refinement module is a partial mitigation.
  2. **Template-Based Factorization:** Uses fixed templates for causal split, which is simple and cost-free but may be brittle if the template does not cover all relevant features.
  3. **Sentence-Level Scope:** The framework assumes entity mentions are given. Extending to document-level or end-to-end extraction (with NER) adds complexity and error propagation.

- **Failure signatures:**
  1. **Parser Errors:** Incorrect SDP leads to poor simplification, causing Grounding failures. The paper claims Refinement can recover these, but only to a point.
  2. **Reward Model Misspecification:** If the factorization template is wrong, the reward model learns a flawed objective, leading to poor PPO fine-tuning.
  3. **Refinement Inconsistency:** If the LLM's global reasoning is unreliable, the Refinement step might "correct" correct predictions or introduce new errors.

- **First 3 experiments:**
  1. **Ablation on Simplification:** Run DEPTH−DP (remove dependency parsing/simplification) on a validation set to quantify the precision/recall drop and increase in hallucinations. This isolates the value of the SDP-based mechanism.
  2. **Reward Model Robustness Check:** Train two reward models—one with causal factorization (CRM), one without (standard RM). Compare their pairwise accuracy on a held-out set and measure downstream PPO performance (e.g., final HR). This tests the anti-hacking claim.
  3. **Refinement Error Analysis:** Manually inspect 50-100 instances where Refinement changed a Grounding prediction. Categorize changes into true corrections and introduced errors. This assesses the reliability of the global consistency check.

## Open Questions the Paper Calls Out
- **Open Question 1:** Can DEPTH be effectively extended to open-set relation extraction where novel relation types emerge, and how should the Refinement module be adapted to cluster NO-RELATION instances for proposing candidate novel relations?
- **Open Question 2:** How can DEPTH be adapted for document-level relation extraction while handling cross-sentence coreference resolution and multi-sentence evidence aggregation?
- **Open Question 3:** How robust is DEPTH to systematic dependency parsing errors, and what is the error tolerance threshold beyond which the two-tiered correction mechanism fails?

## Limitations
- Reliance on parser accuracy for SDP extraction could propagate errors into the Grounding module
- The causal factorization template's completeness is assumed but not empirically validated across diverse relation types
- The ablation study for the Refinement module is incomplete, showing only Grounding vs. full DEPTH without intermediate steps

## Confidence
- **Dependency-Aware Simplification Effectiveness:** High confidence. Clear performance degradation without SDP simplification shown in ablation study.
- **Two-Tiered Hierarchical Refinement Value:** Medium confidence. Mechanism is theoretically sound but empirical isolation is incomplete.
- **Causality-Driven Reward Modeling:** Medium confidence. Ablation shows significant hallucination increase, but lacks comparison to alternative anti-reward-hacking methods.

## Next Checks
1. **Intermediate Ablation Analysis:** Run DEPTH−DP, DEPTH−REF, and DEPTH−CRM on a validation set to isolate component contributions.
2. **Parser Error Robustness Test:** Introduce controlled noise into dependency parsing (5-20% SDP corruption) and measure cascading impact on Grounding accuracy and Refinement recovery rate.
3. **Cross-Schema Generalization:** Evaluate DEPTH on a relation extraction dataset with substantially different relation schema using zero-shot or few-shot transfer.