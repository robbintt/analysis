---
ver: rpa2
title: 'ZSE-Cap: A Zero-Shot Ensemble for Image Retrieval and Prompt-Guided Captioning'
arxiv_id: '2507.20564'
source_url: https://arxiv.org/abs/2507.20564
tags:
- image
- retrieval
- article
- captioning
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ZSE-Cap, a zero-shot system for article-grounded
  image retrieval and captioning in the EVENTA shared task. The method ensembles similarity
  scores from CLIP, SigLIP, and DINOv2 models for retrieval, and uses a carefully
  engineered prompt with the Gemma 3 model for caption generation that links article
  events to visual content.
---

# ZSE-Cap: A Zero-Shot Ensemble for Image Retrieval and Prompt-Guided Captioning

## Quick Facts
- arXiv ID: 2507.20564
- Source URL: https://arxiv.org/abs/2507.20564
- Authors: Duc-Tai Dinh; Duc Anh Khoa Dinh
- Reference count: 8
- Primary result: Zero-shot ensemble retrieval + prompt-guided captioning achieved 4th place on EVENTA private test set (final score 0.42002)

## Executive Summary
This paper presents ZSE-Cap, a zero-shot system for article-grounded image retrieval and captioning in the EVENTA shared task. The approach ensembles similarity scores from CLIP, SigLIP, and DINOv2 models for retrieval, and uses a carefully engineered prompt with the Gemma 3 model for caption generation that links article events to visual content. ZSE-Cap achieved a final score of 0.42002, placing 4th on the private test set. On retrieval, it reached mAP 0.966, R@1 0.955; on captioning, CLIPScore 0.828 and CIDEr 0.133. The approach demonstrates that combining foundation models via ensembling and prompting can effectively solve complex, context-aware multimodal tasks without fine-tuning.

## Method Summary
ZSE-Cap employs a two-stage zero-shot pipeline without fine-tuning on competition data. Stage 1 performs article-grounded image retrieval by computing L2 distances between query image embeddings from CLIP, SigLIP, and DINOv2 models, then fusing these distances using weighted averaging (DINOv2: 0.4545, SigLIP: 0.2727, CLIP: 0.2727) to retrieve the most relevant article. Stage 2 generates captions by feeding the query image, retrieved article text, and an engineered prompt to Gemma-3-27b-it, which synthesizes context-grounded captions connecting article events to visual content. The entire system is inference-only, leveraging pretrained foundation models through careful architectural composition.

## Key Results
- Achieved 4th place on EVENTA private test set with final score 0.42002
- Retrieval performance: mAP 0.966, R@1 0.955
- Captioning performance: CLIPScore 0.828, CIDEr 0.133
- Ensembled retrieval mAP 0.994 vs. CLIP alone 0.981 on public test set
- Captioning CIDEr improved from 0.001 (image-only) to 0.133 (article + prompt)

## Why This Works (Mechanism)

### Mechanism 1: Multi-Model Ensemble for Robust Visual Feature Matching
Combining CLIP, SigLIP, and DINOv2 via weighted L2 distance fusion yields more accurate image-to-image retrieval than any single model. Each encoder produces embeddings in its own feature space, with L2 distances computed independently then combined via learned weights (DINOv2: 0.4545, SigLIP: 0.2727, CLIP: 0.2727). The lowest combined score selects the retrieved image. Different training objectives (contrastive language-image pretraining for CLIP/SigLIP vs. self-distilled self-supervision for DINOv2) capture complementary signals—semantic alignment plus fine-grained visual patterns. Break condition: When the correct image is visually ambiguous (e.g., stock photos reused across articles), visual features alone cannot disambiguate context.

### Mechanism 2: Retrieval-Augmented Prompt Engineering for Contextual Captioning
A carefully structured prompt enables an LLM to synthesize retrieved article context with visual content, producing event-grounded captions. The prompt explicitly instructs multi-step reasoning: (1) contextualize the image via the article, (2) describe visual elements serving the narrative, (3) articulate the connection. The input triplet is (query image, retrieved article text, engineered prompt). Core assumption: The LLM can follow structured reasoning instructions and will not hallucinate details absent from either the image or article. Break condition: If retrieval fails and returns the wrong article, the LLM will generate a plausible but factually mismatched caption.

### Mechanism 3: Zero-Shot Transfer via Foundation Model Composition
Complex multimodal tasks can be addressed without task-specific fine-tuning by composing frozen pretrained models through ensembling and prompting. The pipeline chains inference-only stages: feature extraction → ensemble retrieval → prompt-based generation. No gradients are computed on competition data. Core assumption: Foundation models pretrained at sufficient scale contain transferable representations for the target domain (news imagery and text). Break condition: Domain shift to visual/textual distributions far from pretraining data (e.g., highly specialized medical imagery) may degrade performance.

## Foundational Learning

- **Contrastive Vision-Language Pretraining (CLIP/SigLIP)**
  - Why needed here: Understanding how shared embedding spaces enable zero-shot image-text matching is essential for debugging retrieval failures.
  - Quick check question: Given an image embedding and a text embedding, how would you compute their similarity, and what does a high similarity score imply about their relationship?

- **Self-Supervised Visual Representation Learning (DINOv2)**
  - Why needed here: DINOv2 contributes fine-grained visual features without language supervision; knowing its strengths helps explain why it complements CLIP/SigLIP.
  - Quick check question: What types of visual features might a self-supervised model learn that a language-supervised model might miss?

- **Prompt Engineering for Instruction-Following LLMs**
  - Why needed here: The captioning stage relies on structuring prompts to enforce multi-step reasoning; poor prompts yield generic or hallucinated captions.
  - Quick check question: If an LLM ignores part of a multi-step prompt, what structural changes could increase compliance (e.g., ordering, explicit delimiters, role framing)?

## Architecture Onboarding

- **Component map**: Feature Extractors (CLIP, SigLIP, DINOv2) -> Ensemble Retriever -> Prompt Constructor -> Caption Generator (Gemma 3)
- **Critical path**: Offline: Precompute and store all database embeddings (all three models). Online: Query image → three embeddings → L2 distances → weighted ensemble → retrieve article → construct prompt → Gemma 3 inference → output caption.
- **Design tradeoffs**: Weighted Ensemble vs. RRF: Authors tested both; weighted ensemble performed better on public test set (mAP 0.994 vs. 0.991). Model Size: Gemma-3-27b-it yields higher CLIPScore/CIDEr than 4B but requires more memory; authors ran 27B on a separate high-resource environment. Image-Only vs. Article-Grounded Captioning: Image-only yields near-zero CIDEr; article+prompt is essential but depends on correct retrieval.
- **Failure signatures**: 1. Contextual ambiguity from near-duplicate images: Visually identical images linked to multiple articles produce arbitrary article selection → correct visual match but wrong caption context. 2. Severe visual perturbations: Heavy cropping, compression artifacts, or color shifts distort embeddings → retrieval ranks correct image lower. 3. Prompt non-compliance: LLM may include preamble phrases ("Here is the caption:") despite explicit instructions, requiring post-processing.
- **First 3 experiments**: 1. Ablate the ensemble: Run retrieval using each model individually (CLIP, SigLIP, DINOv2) on a held-out set; compare mAP/R@1 to confirm ensemble gain. 2. Prompt variant test: Simplify the multi-step prompt to a single-sentence instruction; measure CIDEr/CLIPScore degradation to quantify prompt engineering contribution. 3. Retrieval error analysis: Sample 50 retrieval failures; categorize by (a) near-duplicate ambiguity, (b) visual perturbation severity, (c) other. Use findings to prioritize future improvements (e.g., early text signal integration).

## Open Questions the Paper Calls Out

- **Can automated prompt optimization strategies outperform the manually engineered "cognitive orchestrator" prompt in linking article events to visual content?**
  - Basis in paper: The Conclusion states: "Future work could explore automated prompt optimization..."
  - Why unresolved: The current system relies on a single, carefully hand-crafted prompt structure; the authors have not tested whether algorithmic prompt refinement could yield higher CLIPScore or CIDEr metrics.
  - What evidence would resolve it: A comparative study measuring the performance of automatically optimized prompts against the static manual prompt on the EVENTA captioning sub-task.

- **Does incorporating textual signals earlier in the retrieval pipeline effectively resolve the contextual ambiguity caused by near-duplicate images?**
  - Basis in paper: The Error Analysis section identifies "contextual ambiguity from near-duplicate images" as a primary failure mode and suggests "incorporating textual signals earlier in the retrieval process" as a solution.
  - Why unresolved: The current ZSE-Cap pipeline is strictly image-to-image retrieval (visual features only), which fails when multiple articles use identical stock photos, as visual embeddings provide no disambiguating signal.
  - What evidence would resolve it: Implementation of a multimodal retrieval query (using article text alongside the image) demonstrating a reduced error rate for articles containing syndicated or stock imagery.

- **Do dynamic model fusion techniques provide better retrieval robustness than the static weighted averaging of CLIP, SigLIP, and DINOv2 features?**
  - Basis in paper: The Conclusion suggests exploring "more dynamic model fusion techniques" as a direction for future research.
  - Why unresolved: The current system uses fixed weights (determined empirically on the public test set) for the ensemble, which may not be optimal for all query types or visual perturbations.
  - What evidence would resolve it: A comparison of the static weighted ensemble against an adaptive fusion method (e.g., attention-based weighting) on the private test set, specifically analyzing performance on "severely perturbed" images.

## Limitations
- The retrieval stage cannot resolve near-duplicate images linked to multiple articles, requiring textual signal integration for disambiguation
- Captioning quality depends critically on accurate retrieval; wrong article retrieval yields plausible but factually mismatched captions
- Ensemble weights optimized on public test set may not transfer optimally to other domains or datasets

## Confidence
- **High**: Effectiveness of weighted ensemble (supported by public test results)
- **High**: Critical role of prompt engineering (supported by dramatic CIDEr improvements)
- **Medium**: Zero-shot transfer claim (validated only on EVENTA dataset)
- **Low**: Long-term robustness (lack of evaluation on near-duplicate disambiguation and perturbed inputs)

## Next Checks
1. Run ablation study: Compare individual model retrieval (CLIP, SigLIP, DINOv2) vs. ensemble on a held-out set to confirm mAP/R@1 improvements
2. Test prompt variants: Simplify the multi-step prompt to a single instruction and measure degradation in CIDEr/CLIPScore
3. Analyze 50 retrieval failures: Categorize by near-duplicate ambiguity, visual perturbation severity, and other factors to guide future improvements