---
ver: rpa2
title: Hybrid Attribution Priors for Explainable and Robust Model Training
arxiv_id: '2512.14719'
source_url: https://arxiv.org/abs/2512.14719
tags:
- attribution
- priors
- class
- adversarial
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors investigate the limitations of existing attribution
  methods in explanation-guided learning for small language models (SLMs). They find
  that these methods tend to highlight common keywords shared by semantically similar
  classes, which are already difficult to distinguish under standard training, thus
  providing insufficient discriminative cues.
---

# Hybrid Attribution Priors for Explainable and Robust Model Training

## Quick Facts
- arXiv ID: 2512.14719
- Source URL: https://arxiv.org/abs/2512.14719
- Authors: Zhuoran Zhang; Feng Zhang; Shangyuan Li; Yang Shi; Yuanxing Zhang; Wei Chen; Tengjiao Wang; Kam-Fai Wong
- Reference count: 20
- Primary result: CAP Hybrid improves SLM accuracy, interpretability, and adversarial robustness by fusing enriched attribution priors

## Executive Summary
This paper addresses limitations in explanation-guided learning for small language models (SLMs), where existing attribution methods highlight common keywords across similar classes rather than discriminative features. The authors propose Class-Aware Attribution Prior (CAP), which enriches input text with task instructions and label space to generate more salient attribution priors from LLMs. Building on this, they introduce CAP Hybrid, which fuses priors from CAP with LIME and IG to create comprehensive supervisory signals. Experiments across full-data, few-shot, and adversarial settings demonstrate consistent improvements in classification accuracy, interpretability, and robustness compared to baselines.

## Method Summary
The method extracts attribution priors from multiple sources: CAP from LLMs using masked perturbations with task instruction enrichment, LIME from frozen LLMs, and IG from the target SLM. These priors are normalized and fused (typically via mean aggregation) to create comprehensive attribution signals. During training, the SLM's self-attribution (via IG) is aligned with the fused prior through an MSE loss term, optimized jointly with cross-entropy. The approach encourages learning of diverse, decision-relevant features rather than spurious correlations, improving both interpretability and adversarial robustness.

## Key Results
- In 5-shot Banking77, CAP Hybrid achieves 79.09% accuracy with high comprehensiveness (69.12%) and sufficiency (30.66%)
- Full-data Banking77: CAP Hybrid reaches 93.51% accuracy vs. 92.63% baseline; adversarial accuracy 84.97% vs. 70.29%
- Hybrid variants generally outperform individual priors, suggesting complementarity of different attribution strategies
- Improvements are most pronounced in few-shot and adversarial settings

## Why This Works (Mechanism)

### Mechanism 1
Enriching input context with task instructions and label space yields more discriminative attribution priors from LLMs. CAP prompts an LLM with `[Instructions: ...][Given Labels: ...][Text: ...]`, then samples perturbations via random word masking. For each mask `m_i`, it computes the true-label probability `p_i = p(y|T(Inst., x ⊙ m_i))`, transforms it via `z_i = -1/log(p_i)`, and solves a regularized least-squares problem (Eq. 1) to recover per-word attribution scores α via Cholesky factorization.

### Mechanism 2
Fusing multiple attribution sources improves supervisory signal quality by combining complementary strengths—affinity (LIME), salience (CAP), and model-internal sensitivity (IG). Normalize each prior vector (α_lime, α_cap, α_ig), then aggregate via element-wise mean or max (Eq. 5). Mean aggregation generally performed best in ablations.

### Mechanism 3
Aligning the SLM's self-attribution (via IG) with fused external priors during training improves accuracy, interpretability, and adversarial robustness. Compute self-attribution α_self via Integrated Gradients (Eq. 6), normalize both α_self and fused prior a_p, then minimize MSE distance L_a = ||a_p - a_s||^2 (Eq. 7). Optimize jointly with cross-entropy: L = L_ce + βL_a (Eq. 8), with β=1 in experiments.

## Foundational Learning

- **Integrated Gradients (IG)**: Path-based attribution method used to extract self-attribution from the SLM and as one component of the hybrid prior. Quick check: Given a baseline input (e.g., all-zeros embedding), how does IG compute each feature's contribution?
- **Perturbation-based Attribution (LIME/SHAP)**: Local surrogate models that generate explanations by sampling perturbations around an instance. Quick check: How does LIME generate local explanations by sampling perturbations around an instance?
- **Explanation-Guided Learning / Attribution Regularization**: Training paradigm that extends standard supervised learning with attribution alignment term. Quick check: What is the trade-off between classification loss and attribution alignment loss when β is increased?

## Architecture Onboarding

- **Component map**: LLM Prior Extractor (CAP) -> External Priors (LIME, IG) -> Hybrid Fusion -> Target SLM (RoBERTa-Base) -> Training Loop with L_ce + βL_a
- **Critical path**: 1) Pre-compute α_lime, α_cap, α_ig on training set; 2) Fuse into α_hybrid (mean aggregation); 3) Train SLM with combined loss; per batch, compute α_self via IG and align to precomputed α_hybrid
- **Design tradeoffs**: Aggregation (mean vs. max—mean more stable), perturbation count (n=100 used), alignment weight (β=1 used), LLM choice (Llama-3-8B-Instruct used)
- **Failure signatures**: High class overlap in dataset (CAP most beneficial), low-resource (few-shot) settings (prior-guided training more impactful), corrupted priors (if LLM is misaligned)
- **First 3 experiments**: 1) Sanity check prior quality by visualizing α_lime, α_cap, α_ig for confusing class pairs; 2) Ablate fusion size by training with single priors then pairs then full CAP Hybrid; 3) Adversarial robustness test by constructing keyword-addition and keyword-replacement examples

## Open Questions the Paper Calls Out
None

## Limitations
- CAP prior quality depends heavily on LLM's ability to accurately model task-specific class distribution, which is not fully characterized across multiple LLM sizes or domains
- Limited sensitivity analysis of alignment weight β, with no exploration of the trade-off between classification accuracy and attribution alignment
- Evaluation scope is limited to controlled adversarial settings without testing out-of-distribution inputs, domain shifts, or noisy labels
- High correlation between LIME and SHAP priors acknowledged, but systematic testing of correlation structure between all three priors (LIME, IG, CAP) is lacking

## Confidence
- **High confidence**: Mechanism 3 (attribution alignment) and its empirical validation on accuracy and robustness metrics
- **Medium confidence**: Mechanism 1 (CAP prior superiority) relies on LLM capabilities not fully characterized
- **Medium confidence**: Mechanism 2 (hybrid fusion complementarity) demonstrated but depends on attribution correlation not exhaustively tested

## Next Checks
1. **LLM Dependency Stress Test**: Replicate CAP prior extraction using LLMs of varying sizes (e.g., Llama-3-8B vs. Llama-3-70B) and domains (e.g., general vs. domain-specific). Measure how prior quality and downstream SLM performance scale with LLM capacity and domain alignment.
2. **Correlation and Redundancy Analysis**: Compute pairwise Pearson/Spearman correlations between LIME, IG, and CAP priors across all classes in each dataset. Visualize top-10 attributed tokens per class to check for overlap. If correlation is high, test whether pruning redundant priors (e.g., using only CAP+IG) maintains performance.
3. **Hyperparameter Sensitivity Sweep**: Train CAP Hybrid models across a grid of β values (e.g., 0.1, 0.5, 1.0, 2.0) and perturbation counts n (e.g., 50, 100, 200). Report accuracy, comprehensiveness, sufficiency, and adversarial accuracy for each setting to identify stable operating points and potential brittleness.