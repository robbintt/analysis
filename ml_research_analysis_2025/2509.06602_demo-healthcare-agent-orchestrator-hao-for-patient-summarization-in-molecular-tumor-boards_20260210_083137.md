---
ver: rpa2
title: 'Demo: Healthcare Agent Orchestrator (HAO) for Patient Summarization in Molecular
  Tumor Boards'
arxiv_id: '2509.06602'
source_url: https://arxiv.org/abs/2509.06602
tags:
- tbfact
- patient
- facts
- evaluation
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses the labor-intensive and inconsistent preparation
  of patient summaries for Molecular Tumor Boards by introducing the Healthcare Agent
  Orchestrator (HAO), a multi-agent AI system that generates accurate, citation-backed
  summaries through specialized agents coordinated by an orchestrator. To evaluate
  the quality of these summaries, the authors propose TBFact, a claim-level evaluation
  framework that assesses completeness and succinctness by comparing clinical factual
  claims between generated and reference summaries.
---

# Demo: Healthcare Agent Orchestrator (HAO) for Patient Summarization in Molecular Tumor Boards
## Quick Facts
- arXiv ID: 2509.06602
- Source URL: https://arxiv.org/abs/2509.06602
- Reference count: 26
- Primary result: HAO generates citation-backed patient summaries with 94% coverage of high-importance clinical information

## Executive Summary
This study addresses the labor-intensive and inconsistent preparation of patient summaries for Molecular Tumor Boards by introducing the Healthcare Agent Orchestrator (HAO), a multi-agent AI system that generates accurate, citation-backed summaries through specialized agents coordinated by an orchestrator. The system was evaluated using a novel claim-level framework called TBFact that assesses completeness and succinctness by comparing clinical factual claims between generated and reference summaries. On a de-identified dataset of tumor board cases, the Patient History agent achieved 94% coverage of high-importance information and a TBFact recall of 0.84, matching human-level summary quality while enabling traceable, scalable, and institution-controlled deployment.

## Method Summary
The HAO system employs a multi-agent architecture with specialized agents for different clinical domains (Patient History, Laboratory, Imaging, Pathology, Treatment, Clinical Trial) coordinated by an orchestrator. The Patient History agent uses a reasoning module to identify important sections and attributes, a retrieval module to extract supporting text from clinical documents, and a response generation module to create the final summary. The system was evaluated on 20 de-identified tumor board cases using TBFact, a claim-level evaluation framework that extracts and compares clinical factual claims between generated summaries and reference summaries to measure completeness and succinctness. The framework identifies complete and partial entailments to provide granular assessment of summary quality.

## Key Results
- Patient History agent achieved 94% coverage of high-importance clinical information (including partial entailments)
- TBFact recall of 0.84 under strict criteria for clinical factual claims
- System demonstrated human-level summary quality while maintaining traceable citations and institutional data control

## Why This Works (Mechanism)
The system's effectiveness stems from its modular agent architecture that mimics the multi-disciplinary approach of tumor boards. Each agent specializes in a specific clinical domain, allowing for deeper domain expertise and more accurate information extraction. The orchestrator coordinates these agents and integrates their outputs, while the reasoning and retrieval modules ensure that summaries are both comprehensive and citation-backed. The TBFact evaluation framework provides a rigorous, claim-level assessment that goes beyond traditional BLEU or ROUGE metrics to evaluate actual clinical content coverage and accuracy.

## Foundational Learning
- Multi-agent coordination: Needed for parallel processing of diverse clinical domains; quick check: measure inter-agent communication latency and integration accuracy
- Citation-backed summarization: Required for clinical trust and verification; quick check: verify citation accuracy and relevance to claims
- Claim-level evaluation: Essential for granular quality assessment in clinical contexts; quick check: measure inter-annotator agreement on claim identification
- Domain-specific reasoning: Critical for identifying high-importance clinical information; quick check: compare agent performance on domain-specific vs. general tasks
- Traceable information extraction: Necessary for clinical auditability and trust; quick check: verify complete citation trail from summary claims to source documents
- Institution-controlled deployment: Important for data privacy compliance; quick check: validate that no patient data leaves institutional boundaries

## Architecture Onboarding
- Component map: Orchestrator -> [Patient History, Laboratory, Imaging, Pathology, Treatment, Clinical Trial] -> Summary Generation
- Critical path: Clinical document input -> Reasoning module (identify important attributes) -> Retrieval module (extract supporting text) -> Response generation (create summary with citations)
- Design tradeoffs: Specialization vs. integration complexity, citation detail vs. summary readability, comprehensive coverage vs. succinctness
- Failure signatures: Missing critical clinical information, incorrect citations, incomplete domain coverage, inconsistent formatting
- First experiments: 1) Test individual agent performance on domain-specific document types, 2) Evaluate orchestrator's ability to integrate multi-domain summaries, 3) Validate TBFact framework's reliability on synthetic clinical claims

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation based on limited dataset of 20 cases from single institution, raising generalizability concerns
- Full system integration with all six specialized agents not yet evaluated
- Performance heavily dependent on quality and completeness of source clinical documents
- Clinical utility and impact on actual tumor board decision-making not assessed

## Confidence
- 94% coverage metric: Medium (depends on subjective criteria for high-importance information)
- TBFact recall of 0.84: Medium (based on strict criteria that may be overly conservative)
- Human-level comparison: Medium (lacks specificity about reference standard)

## Next Checks
1. Conduct multi-institutional validation study using diverse tumor board cases across different cancer types and clinical settings to assess generalizability
2. Perform user study with actual tumor board participants to evaluate clinical utility and impact on decision-making
3. Implement and evaluate full system with all six specialized agents integrated, measuring inter-agent coordination effectiveness and overall summary quality