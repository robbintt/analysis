---
ver: rpa2
title: 'On Training-Test (Mis)alignment in Unsupervised Combinatorial Optimization:
  Observation, Empirical Exploration, and Analysis'
arxiv_id: '2506.16732'
source_url: https://arxiv.org/abs/2506.16732
tags:
- training
- derandomization
- objective
- optimization
- rounding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper identifies a training-test misalignment issue in unsupervised
  combinatorial optimization (UCO). While training optimizes expected objectives using
  naive random sampling, sophisticated derandomization (e.g., iterative/greedy rounding)
  is used at test time, leading to lower training losses not guaranteeing better post-derandomization
  performance.
---

# On Training-Test (Mis)alignment in Unsupervised Combinatorial Optimization: Observation, Empirical Exploration, and Analysis

## Quick Facts
- arXiv ID: 2506.16732
- Source URL: https://arxiv.org/abs/2506.16732
- Reference count: 5
- The paper identifies a training-test misalignment issue in unsupervised combinatorial optimization (UCO), where training optimizes expected objectives but test-time derandomization yields lower performance.

## Executive Summary
This paper identifies a fundamental training-test misalignment issue in unsupervised combinatorial optimization (UCO). During training, models optimize expected objectives using naive random sampling, but at test time, sophisticated derandomization techniques like iterative or greedy rounding are applied to produce valid discrete solutions. This mismatch means that lower training losses don't necessarily translate to better post-derandomization performance. The authors propose a preliminary solution: incorporating a differentiable soft version of derandomization during training. They validate this approach on toy quadratic problems, showing improved alignment between training and test performance, though on a real facility location problem, soft derandomization introduces training instability, especially with low temperatures. This highlights the trade-off between alignment and training stability in UCO method development.

## Method Summary
The paper addresses training-test misalignment in UCO by proposing soft derandomization during training. The core idea is to replace the discrete arg max operation in iterative and greedy rounding algorithms with a differentiable softmax operation parameterized by temperature τ. This allows gradients to flow through the derandomization process during training, theoretically aligning training objectives with test-time performance. The method is evaluated on two problems: (1) toy quadratic optimization problems where f(D) = Σαᵢⱼdᵢdⱼ with αᵢⱼ sampled from normal distribution, and (2) facility location problems using the experimental setup from prior work. The soft derandomization replaces discrete decisions with softmax probabilities over potential choices, with temperatures tested at τ ∈ {10.0, 1.0, 0.1, 0.01, 0.001}.

## Key Results
- Training-test misalignment is observed: lower training losses don't guarantee better post-derandomization performance
- Soft derandomization improves alignment on toy quadratic problems by incorporating test-time rounding logic into training
- On facility location problems, soft derandomization introduces training instability, especially with low temperatures (τ = 0.01, 0.001), revealing a trade-off between alignment and training ease

## Why This Works (Mechanism)
The mechanism behind training-test misalignment stems from the fundamental difference between how objectives are optimized during training versus evaluation. During training, models optimize expected objectives using continuous relaxations and random sampling, which doesn't capture the discrete constraints and combinatorial structure that derandomization techniques handle at test time. When iterative or greedy rounding is applied at test time, it makes discrete decisions based on marginal gains that weren't part of the training objective. Soft derandomization bridges this gap by making the rounding process differentiable, allowing the model to learn representations that are compatible with the actual decision-making process used at inference. This creates a more coherent optimization pipeline where the training objective better reflects the true goal of producing high-quality discrete solutions.

## Foundational Learning
- **Combinatorial optimization**: Why needed - Core problem domain where discrete decisions must be optimized; Quick check - Can identify NP-hard problems and their applications
- **Unsupervised learning in optimization**: Why needed - Framework where objective functions are given without labeled optimal solutions; Quick check - Understands the difference between supervised and unsupervised combinatorial optimization
- **Differentiable relaxations**: Why needed - Enables gradient-based training for discrete problems; Quick check - Can explain how softmax approximates arg max and the role of temperature
- **Derandomization techniques**: Why needed - Methods to convert randomized solutions into deterministic ones; Quick check - Understands iterative and greedy rounding algorithms
- **Training-test misalignment**: Why needed - Critical issue where training objectives don't match evaluation metrics; Quick check - Can identify scenarios where this phenomenon occurs
- **Softmax temperature control**: Why needed - Balances exploration vs. exploitation in differentiable discrete choices; Quick check - Understands how temperature affects softmax output distribution

## Architecture Onboarding

### Component Map
Training pipeline: Data sampler -> Neural network -> Soft derandomization (softmax) -> Continuous relaxation -> Expected objective
Test pipeline: Trained model -> Hard derandomization (arg max) -> Discrete solution -> Post-derandomization objective

### Critical Path
The critical path for reproducing results involves: (1) implementing the soft derandomization by replacing arg max with softmax, (2) generating synthetic data for toy quadratic problems with specified αᵢⱼ distributions, (3) implementing the iterative and greedy rounding algorithms in their soft and hard versions, and (4) training models while monitoring both training loss and test objective alignment.

### Design Tradeoffs
The primary tradeoff is between alignment quality and training stability. Higher temperatures (τ = 10.0, 1.0) provide more stable training but weaker alignment since the softmax outputs are too smooth and don't capture discrete decision boundaries well. Lower temperatures (τ = 0.01, 0.001) provide better alignment by approximating hard decisions more closely, but can cause training instability through vanishing gradients or exploding activations. The authors don't explore adaptive temperature schedules, which could potentially balance these competing concerns.

### Failure Signatures
Training instability manifests as loss curves that plateau or diverge when using low temperatures (τ ≤ 0.01). This occurs because the softmax becomes nearly one-hot, causing gradient vanishing. Another failure mode is negligible improvement with high temperatures, where the soft derandomization is too weak to capture the discrete structure, resulting in training-test misalignment persisting. Poor alignment is diagnosed by counting "bad" pairs where the ordering of solutions differs between training objectives and post-derandomization objectives.

### First Experiments
1. Reproduce the toy quadratic misalignment experiment by sampling αᵢⱼ ~ N(0,1), generating 100 D̃ ∈ [0,1]⁵⁰, computing both training objectives and post-derandomization objectives, and counting misaligned pairs across all 4950 possible pairs
2. Implement soft iterative rounding by replacing arg max with softmax over Δ_b values using temperature τ, and verify that gradients flow through the rounding process
3. For facility location, clone the referenced repository and run training with soft derandomization at each temperature τ, plotting training loss and test objective curves to observe stability patterns

## Open Questions the Paper Calls Out
None

## Limitations
- The paper doesn't specify the mean and variance of the normal distribution used to sample αᵢⱼ in toy experiments, which could affect reproducibility
- Training instability with low temperatures on real problems suggests the approach may not generalize well without additional stabilization techniques
- The soft derandomization approach is only validated on two specific problems, limiting conclusions about broader applicability

## Confidence
- Training-test misalignment observation: High confidence (clear theoretical reasoning and preliminary empirical evidence)
- Soft derandomization as solution: Medium confidence (logically sound but limited real-world validation)
- Trade-off between alignment and stability: Medium confidence (qualitatively demonstrated but needs more systematic study)

## Next Checks
1. Systematically vary softmax temperature τ and measure both alignment improvement and training stability to identify optimal ranges and quantify the alignment-stability tradeoff
2. Compare soft derandomization against alternative alignment strategies such as directly optimizing post-derandomization objectives or using reinforcement learning approaches
3. Apply the method to additional real-world combinatorial optimization problems beyond facility location to assess generalizability across different problem domains and constraint structures