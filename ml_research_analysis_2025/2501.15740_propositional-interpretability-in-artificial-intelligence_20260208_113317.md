---
ver: rpa2
title: Propositional Interpretability in Artificial Intelligence
arxiv_id: '2501.15740'
source_url: https://arxiv.org/abs/2501.15740
tags:
- propositional
- attitudes
- interpretability
- system
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores propositional interpretability in AI, focusing
  on understanding AI systems through propositional attitudes like belief and desire.
  The author argues that propositional interpretability is crucial for AI safety,
  ethics, and cognitive science.
---

# Propositional Interpretability in Artificial Intelligence

## Quick Facts
- arXiv ID: 2501.15740
- Source URL: https://arxiv.org/abs/2501.15740
- Authors: David J. Chalmers
- Reference count: 4
- Primary result: This paper explores propositional interpretability in AI, focusing on understanding AI systems through propositional attitudes like belief and desire, with thought logging as the central challenge.

## Executive Summary
This paper examines propositional interpretability in AI systems, focusing on the ability to understand and track AI systems through propositional attitudes such as beliefs, desires, and intentions. The author argues that propositional interpretability is crucial for AI safety, ethics, and cognitive science. The central challenge identified is "thought logging" - creating systems that can log an AI's propositional attitudes over time. The paper reviews current methods including causal tracing, probing with classifiers, sparse auto-encoders, and chain-of-thought methods, assessing their strengths and weaknesses for propositional interpretability.

## Method Summary
The paper synthesizes existing interpretability methods through the lens of propositional interpretability without introducing new empirical results. It reviews four existing approaches: causal tracing (identifying where propositions are represented by their causal role), probing with classifiers (using trained classifiers to decode propositional content), sparse auto-encoders (decomposing activations into monosemantic features), and chain-of-thought methods (generating reasoning traces). The paper evaluates each method's capacity to achieve thought logging, identifying SAEs as most promising for feature logging while noting that none fully achieve propositional attitude tracking.

## Key Results
- Sparse auto-encoders show promise for feature logging by decomposing polysemantic activation vectors into monosemantic, interpretable features
- Current methods fall short of true thought logging: causal tracing and probing are supervised and fragile, SAEs primarily yield concepts not propositions, and chain-of-thought methods are often unfaithful
- The paper identifies three open challenges: developing systems for thought logging, adapting SAEs to identify propositions and attitudes, and making chain-of-thought methods faithful reflections of internal reasoning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sparse auto-encoders (SAEs) can decompose polysemantic activation vectors into monosemantic features, enabling conceptual interpretability as a precursor to thought logging.
- Mechanism: An auto-encoder with a sparse bottleneck is trained to reconstruct a model's residual stream activations. This pressure forces individual latent units to specialise on single, human-interpretable features or concepts (e.g., "Golden Gate Bridge," "sycophancy"). By logging which sparse units are active over time, one can achieve "feature logging."
- Core assumption: Interpretable SAE units correspond to causally relevant features used by the model, not merely artifacts of the decomposition. Further, concepts can be bound into propositions.
- Evidence anchors:
  - [abstract] "...sparse auto-encoders show promise for feature logging and sparse features offer monosemantic advantages over previous methods."
  - [section 7.3] "The most powerful auto-encoder used in the study has 34 million units, of which only around 100 units are active at a given time... Just under half of the units seem to be interpretable."
  - [corpus] Evidence from corpus is weak or missing for this specific mechanism.
- Break condition: If SAE units are found to be consistently polysemantic, or if interventions on "feature" units fail to produce consistent downstream behavioral changes, invalidating their causal role.

### Mechanism 2
- Claim: Causal tracing via activation patching can localize the storage of specific factual propositions (e.g., "The Eiffel Tower is in Paris") to specific layers.
- Mechanism: This method corrupts input activations, causing an incorrect output. It then systematically restores "clean" activations from specific layers to identify which restoration is necessary and sufficient to recover the correct output. The critical layer is identified as the location of the "belief."
- Core assumption: The locus of representation can be identified by its causal contribution to the output, and this representation is localised rather than fully distributed.
- Evidence anchors:
  - [section 7.1] "...they typically find that certain activations in a certain middle layer are most important. This tends to suggest that this middle layer is crucial to representing The Eiffel Tower is in Paris."
  - [corpus] Evidence from corpus is weak or missing for this specific mechanism.
- Break condition: If the located representation is fragile to prompt rephrasing (e.g., works for "The Eiffel Tower is in..." but not "Rome has a tower called..."), suggesting it fails to capture a robust conceptual proposition.

### Mechanism 3
- Claim: Probing with classifiers can decode the propositional content of a model's internal state.
- Mechanism: A classifier (probe) is trained on a model's activation vectors to predict the truth value of a specific proposition (e.g., "There is a black tile on e4" in a game state). High probe accuracy suggests the model's activations encode a "world model" of that proposition.
- Core assumption: High classification accuracy indicates that the information is functionally represented by the network, not just that the probe is powerful enough to extract a correlation.
- Evidence anchors:
  - [section 7.2] "Kenneth Li et al (2023) trained a network to play the board game Othello and then used probes to decode the state of the board... This suggests that the state of the board is encoded by activity vectors in the system."
  - [corpus] Evidence from corpus is weak or missing for this specific mechanism.
- Break condition: If causal intervention on the probed location (e.g., changing the "e4" activation) does not alter the model's subsequent moves in a way consistent with the new proposition.

## Foundational Learning

- Concept: Propositional Attitudes
  - Why needed here: This is the central unit of analysis. Distinguishing the *attitude* (belief, desire, goal, supposition) from the *proposition* (the content, e.g., "war has occurred") is critical. Safety failures occur if one confuses a model's *belief* that a dangerous outcome is likely with a *desire* (goal) to cause that outcome.
  - Quick check question: For the proposition "a dangerous outcome occurs," explain the difference between a system having a *belief* vs. a *desire* toward it.

- Concept: Psychosemantics (Information vs. Use)
  - Why needed here: This provides the theoretical basis for attributing meaning. "Information" (upstream correlation) grounds probing methods. "Use" (downstream causal role) grounds causal tracing and intervention. Understanding this distinction is key to evaluating the validity of interpretability evidence.
  - Quick check question: If a unit fires when shown cat images (upstream), that is evidence from which principle? If amplifying that unit makes the model say "meow" (downstream), that is evidence from which principle?

- Concept: Thought Logging
  - Why needed here: This is the paper's primary concrete challenge and the ultimate goal of propositional interpretability. It frames the engineering problem: build a system that outputs a time-series of a target AI's propositional attitudes (e.g., "Goal: I win. Judge (credence 0.8): If I move Qf8, I will win.").
  - Quick check question: What is the key difference between a thought log and a standard log of activations or weights?

## Architecture Onboarding

- Component map: Target AI System -> Interpretability Method -> Psychosemantic Theory -> Thought Logger (Goal)
- Critical path:
  1. **Feature Extraction:** Decompose raw activations into interpretable components (e.g., via SAEs for monosemantic features)
  2. **Propositional Binding:** Determine how extracted features/concepts combine into structured propositions (e.g., using "binding probes")
  3. **Attitude Attribution:** Identify the attitude (belief, goal, credence) associated with each proposition
  4. **Logging:** Output the time-indexed sequence of (Proposition, Attitude) pairs
- Design tradeoffs:
  - **Supervised vs. Unsupervised:** Probes and causal tracing are supervised (test for a pre-specified proposition). SAEs are unsupervised and more open-ended, making them stronger candidates for general thought logging, though they primarily yield concepts, not propositions
  - **Information vs. Use:** Probing is easier (uses correlation/information) but can identify spurious relationships. Causal tracing is harder (requires intervention/use) but provides stronger evidence for a functional representation
- Failure signatures:
  - **Unfaithful Chain-of-Thought:** The model's stated reasons do not causally influence its decision. Signature: intervening on the stated reason does not change the output
  - **Fragile Representation:** A found proposition representation is highly prompt-dependent (e.g., found for "The Eiffel Tower is in..." but not "Paris contains the..."). This indicates a lack of robust, conceptual understanding
  - **Supervised Tunnel Vision:** An interpretability tool that only finds what you explicitly look for, failing to detect novel or unanticipated beliefs/goals
- First 3 experiments:
  1. **SAE Feature Logging:** Train a small SAE on a simple model's residual stream. Log the active sparse features per token and attempt to map features to concepts/propositions. Manually verify if the top active units correspond to interpretable, monosemantic features
  2. **Causal Probe Validation:** On a model with a known world model (e.g., Othello-GPT), train a probe for a proposition. Then perform a causal intervention: alter the activation to flip the proposition's truth value and verify the model's subsequent behavior changes accordingly
  3. **Binding Test:** Use the method from Feng et al. (2024) to identify activation vectors for two concepts (e.g., "Greg" and "Italy") and a "binding subspace." Attempt to construct a composite vector for the proposition "LivesIn(Greg, Italy)" and test if its presence correlates with model behavior

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can we develop a system capable of "thought logging" that tracks an AI's propositional attitudes over time?
- **Basis in paper:** [explicit] The author defines "thought logging" as the "central challenge" of propositional interpretability (p. 2, 9)
- **Why unresolved:** Current methods like causal tracing and probing are supervised, fragile, or non-open-ended, failing to capture a dynamic range of attitudes
- **What evidence would resolve it:** A functional system that outputs a real-time log of an AI's beliefs and goals (e.g., "Goal: I win chess") based solely on internal states

### Open Question 2
- **Question:** Can sparse auto-encoders be adapted to identify propositions and attitudes rather than just individual concepts?
- **Basis in paper:** [explicit] The paper notes SAEs are currently better suited for conceptual interpretability and asks how concepts can "combine into propositional representations" (p. 18)
- **Why unresolved:** SAEs currently isolate monosemantic features (e.g., "Golden Gate Bridge") but lack a mechanism for binding these into propositions or distinguishing attitudes like belief from desire
- **What evidence would resolve it:** SAEs that successfully decode bound structures (e.g., "The bridge is large") and distinct attitude types from network activations

### Open Question 3
- **Question:** Can chain-of-thought methods be modified to provide a faithful reflection of a model's internal reasoning?
- **Basis in paper:** [explicit] The paper highlights that "chains of thought are often unfaithful" and asks if they can be made more accurate (p. 20)
- **Why unresolved:** Models often generate post-hoc rationalizations that do not causally reflect the actual internal decision process
- **What evidence would resolve it:** High correlation between generated chain-of-thought reasoning and independent mechanistic probes (e.g., causal tracing) of the model's activations

## Limitations
- The gap between monosemantic feature logging (achievable with SAEs) and full propositional attitude logging remains unbridged
- No established metrics exist for validating propositional interpretability claims
- The binding problem for combining features into propositions lacks a systematic solution

## Confidence
- High confidence in the conceptual importance of propositional interpretability for AI safety and ethics
- Medium confidence in the identified challenges (feature vs. proposition logging, attitude attribution)
- Low confidence in the completeness of the survey of current methods, as implementation details and success rates are not fully characterized

## Next Checks
1. Implement SAE-based feature logging on a small transformer and evaluate feature monosemanticity using manual verification protocols
2. Test the faithfulness of chain-of-thought outputs by performing causal interventions on stated reasons and measuring behavioral changes
3. Apply binding probes (per Feng et al. 2024) to verify propositional composition from extracted features in a controlled setting