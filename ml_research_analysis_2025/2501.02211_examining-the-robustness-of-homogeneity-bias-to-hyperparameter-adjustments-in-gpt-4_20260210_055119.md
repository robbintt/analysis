---
ver: rpa2
title: Examining the Robustness of Homogeneity Bias to Hyperparameter Adjustments
  in GPT-4
arxiv_id: '2501.02211'
source_url: https://arxiv.org/abs/2501.02211
tags:
- bias
- homogeneity
- temperature
- values
- group
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study examines how homogeneity bias in GPT-4 responds to
  adjustments in two key hyperparameters that control output randomness: sampling
  temperature and top p. The research addresses the problem of whether tuning these
  parameters can mitigate the tendency of AI models to represent certain social groups
  (particularly racial minorities and women) as more homogeneous than others.'
---

# Examining the Robustness of Homogeneity Bias to Hyperparameter Adjustments in GPT-4

## Quick Facts
- arXiv ID: 2501.02211
- Source URL: https://arxiv.org/abs/2501.02211
- Authors: Messi H. J. Lee
- Reference count: 18
- Primary result: Homogeneity bias in GPT-4 persists across most hyperparameter configurations despite tuning adjustments

## Executive Summary
This study investigates whether adjusting LLM hyperparameters (temperature and top_p) can mitigate homogeneity bias - the tendency to represent certain social groups more uniformly than others. Using 60 facial stimuli from the GAN Face Database across four intersectional groups, the research generates 50-word stories for each face under multiple hyperparameter settings. The study finds that while some hyperparameter adjustments can reduce homogeneity bias for specific groups (particularly reducing racial bias), the bias persists across most configurations and affects different social dimensions differently. The results demonstrate that hyperparameter tuning alone cannot serve as a universal solution for addressing bias in AI-generated content.

## Method Summary
The study uses GPT-4o mini to generate stories about individuals from four intersectional groups (Black men, Black women, White men, White women) based on facial stimuli from the GAN Face Database. For each of the 60 faces, 50 stories are generated under each combination of hyperparameter settings, creating 3,000 stories per configuration. Temperature values tested are 0, 0.5, 1.0, 1.5, and 2.0, while top_p values are 0.2, 0.4, 0.6, 0.8, and 1.0. Story similarity is measured using cosine similarity between sentence embeddings (all-mpnet-base-v2), and mixed-effects models analyze how these similarities vary across social groups and hyperparameter settings.

## Key Results
- Homogeneity bias persists across most hyperparameter configurations, with Black Americans and women consistently represented more homogeneously than White Americans and men
- Increasing temperature reduces racial homogeneity bias but affects gender homogeneity bias differently, showing complex non-linear responses
- Extreme hyperparameter values (temperature=2, top_p=0.8) produce unexpected non-linear patterns that cannot be explained by simple adjustments
- Hyperparameter tuning alone cannot serve as a universal solution for addressing homogeneity bias across different social group dimensions

## Why This Works (Mechanism)
The mechanism relies on how LLMs generate text probabilistically - temperature controls randomness in token selection while top_p limits the pool of candidate tokens. These hyperparameters influence the diversity of generated narratives, which can either amplify or reduce the tendency to represent groups in stereotypical or uniform ways. The study measures homogeneity through semantic similarity of generated stories, revealing that even with increased randomness, certain social groups remain more consistently stereotyped in their descriptions.

## Foundational Learning
- **Homogeneity bias**: AI systems representing certain social groups as more uniform than others. Why needed: Central concept being studied. Quick check: Compare similarity scores across different demographic groups.
- **Temperature hyperparameter**: Controls randomness in token selection during generation. Why needed: Key variable tested for bias mitigation. Quick check: Observe increased narrative diversity as temperature increases.
- **Top_p sampling**: Limits token selection to the smallest set whose cumulative probability exceeds p. Why needed: Alternative randomness control tested. Quick check: Compare output diversity at different top_p values.
- **Cosine similarity**: Measures semantic similarity between text embeddings. Why needed: Metric for quantifying homogeneity. Quick check: Calculate similarity scores within and between groups.
- **Mixed-effects models**: Statistical approach accounting for both fixed and random effects. Why needed: Handles the hierarchical structure of the data. Quick check: Verify model convergence and appropriate random effects specification.
- **Vector representations**: Numerical encoding of text for similarity measurement. Why needed: Enables quantitative comparison of narrative content. Quick check: Ensure consistent embedding methodology across all stories.

## Architecture Onboarding
- **Component map**: Facial stimuli -> Story generation (GPT-4o mini) -> Text embedding (all-mpnet-base-v2) -> Similarity calculation -> Statistical modeling
- **Critical path**: The generation and embedding pipeline directly determines the similarity metrics used for bias measurement
- **Design tradeoffs**: Testing hyperparameters separately (as recommended) vs. exploring their interaction effects; using a single model vs. comparing multiple architectures
- **Failure signatures**: Inconsistent bias patterns across hyperparameter settings; unexpected non-linear effects at extreme values; model-specific artifacts in generated content
- **First experiments**: 1) Generate stories at minimum and maximum temperature values to observe baseline bias; 2) Compare racial vs. gender homogeneity patterns at mid-range hyperparameters; 3) Test the effect of top_p=1.0 (no filtering) on narrative diversity

## Open Questions the Paper Calls Out
1. How does simultaneously adjusting both temperature and top_p affect homogeneity bias, compared to adjusting them individually?
2. What are the inflection points in the non-linear relationship between hyperparameters and group representation diversity, and what settings minimize homogeneity bias?
3. Does homogeneity bias persist across different tasks and text formats beyond story generation?
4. What model components and training data characteristics drive homogeneity bias?

## Limitations
- Results may not generalize across different model architectures, versions, or narrative contexts
- The 60-stimuli dataset may not capture full diversity within demographic categories
- Non-linear effects at extreme hyperparameter values may reflect model-specific idiosyncrasies
- Single prompt style limits understanding of how bias manifests across different generation tasks

## Confidence
- High confidence: Finding that hyperparameter tuning cannot universally mitigate homogeneity bias across different social dimensions
- Medium confidence: Non-linear responses at extreme hyperparameter values; persistent bias despite adjustments across most configurations
- Low confidence: Generalization to other models, tasks, or demographic representations beyond the tested scope

## Next Checks
1. Replicate the study using different LLM architectures (e.g., Claude, Llama) to assess model-specific vs. general phenomenon
2. Expand the stimuli set to include more diverse facial representations and test whether bias patterns hold across broader demographic variation
3. Systematically explore intermediate hyperparameter values between tested points to better characterize the non-linear relationships observed at extremes