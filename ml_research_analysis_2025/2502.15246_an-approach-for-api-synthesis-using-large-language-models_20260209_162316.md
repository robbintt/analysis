---
ver: rpa2
title: An approach for API synthesis using large language models
arxiv_id: '2502.15246'
source_url: https://arxiv.org/abs/2502.15246
tags:
- synthesis
- test
- llms
- cases
- apis
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel approach for API synthesis using
  large language models (LLMs). The method leverages LLMs with prompt engineering
  techniques, including assistant context, chain-of-thought prompting, few-shot learning,
  and follow-up queries, to synthesize Java APIs from method signatures and test cases.
---

# An approach for API synthesis using large language models

## Quick Facts
- arXiv ID: 2502.15246
- Source URL: https://arxiv.org/abs/2502.15246
- Reference count: 40
- Key outcome: LLM-based approach achieves 98.5% success rate synthesizing Java APIs from method signatures and test cases, outperforming FrAngel

## Executive Summary
This paper introduces a novel approach for API synthesis using large language models (LLMs). The method leverages LLMs with prompt engineering techniques, including assistant context, chain-of-thought prompting, few-shot learning, and follow-up queries, to synthesize Java APIs from method signatures and test cases. The approach eliminates the need for exhaustive search or manual component specification. Experimental results on 135 real-world programming tasks show that the LLM-based approach achieves a 98.5% success rate in synthesizing correct, test-passing APIs, outperforming the state-of-the-art component-based synthesis tool FrAngel.

## Method Summary
The approach uses GPT-4o with temperature 0.7 to synthesize Java APIs from method signatures and test cases. The prompt engineering strategy includes: (1) assistant role ("Java software engineer"), (2) 8-step chain-of-thought instructions for systematic reasoning, (3) 5 few-shot examples for guidance, and (4) follow-up prompts for iterative refinement when compilation or runtime errors occur (up to 3 attempts). The method leverages the LLM's parametric knowledge of Java libraries rather than requiring explicit component specification, as in traditional synthesis tools like FrAngel.

## Key Results
- 98.5% success rate (133/135 tasks) with follow-up prompts vs 92.6% (125/135) without
- Outperforms state-of-the-art FrAngel component-based synthesis tool
- Synthesized APIs exhibit improved readability with meaningful variable names and comments
- Effective at handling corner cases with minimal test cases

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Decomposing synthesis tasks into an explicit reasoning chain improves the structural validity and correctness of generated code.
- **Mechanism:** The authors utilize an 8-step Chain-of-Thought (CoT) prompt that forces the model to sequentially handle imports, helper classes, class structure, problem analysis, implementation, edge cases, unit tests, and execution validation.
- **Core assumption:** The model possesses sufficient inherent coding knowledge to execute individual reasoning steps correctly when prompted sequentially.
- **Evidence anchors:** Section 3.2 details the 8-step framework; related work supports that prompt structure significantly impacts synthesis performance.

### Mechanism 2
- **Claim:** Iterative refinement using execution feedback closes the gap between syntactically valid code and functionally correct code.
- **Mechanism:** The system executes the generated code against test cases. If execution fails, a "follow-up prompt" feeds the specific error context back to the LLM for repair.
- **Core assumption:** The error signals provided in the follow-up prompt contain sufficient information for the model to identify and localize the bug.
- **Evidence anchors:** Table 2 shows performance improves from ~93% to ~98.5% with follow-up; related work validates the efficacy of LLM-based repair loops.

### Mechanism 3
- **Claim:** Utilizing pre-trained library knowledge allows synthesis without explicit component specification.
- **Mechanism:** Unlike traditional component-based synthesis, this approach leverages the LLM's parametric memory of open-source libraries to "guess" necessary API calls based on method signature and natural language context.
- **Core assumption:** The target libraries and usage patterns were present in the model's training data and can be retrieved accurately.
- **Evidence anchors:** The approach "eliminates the need for exhaustive search or manual component specification"; directs the LLM to use "relevant Java open-source libraries" via prompt context.

## Foundational Learning

- **Concept: Component-based Program Synthesis**
  - **Why needed here:** To understand the baseline (FrAngel/SyPet) against which this LLM approach is compared. Traditional tools search a bounded space of predefined components; this paper argues LLMs replace that search with probabilistic generation.
  - **Quick check question:** Can you explain why traditional synthesis requires a "bounded space" of components and how this paper claims to bypass that constraint?

- **Concept: In-Context Learning (Few-Shot)**
  - **Why needed here:** The architecture relies heavily on providing the LLM with examples (few-shot) to guide the output format and logic, rather than fine-tuning the model weights.
  - **Quick check question:** How does providing input-output examples in the prompt window (context) differ from training a model on a dataset?

- **Concept: Angelic Execution / Oracle-Guided Synthesis**
  - **Why needed here:** The paper references FrAngel's use of "angelic conditions" and general test-driven synthesis. Understanding that test cases act as the "oracle" to verify correctness is central to the evaluation loop.
  - **Quick check question:** In the context of this paper, what acts as the "oracle" to determine if a synthesized API is correct?

## Architecture Onboarding

- **Component map:** Input Interface -> Prompt Constructor -> LLM Engine -> Execution Harness -> Feedback Controller
- **Critical path:** The Prompt Constructor is the most sensitive component. The selection of the 5 few-shot examples and the precise phrasing of the 8-step instruction are the primary determinants of success before any execution occurs.
- **Design tradeoffs:**
  - Token Limits vs. Few-Shot: Using 5 examples improves accuracy but consumes context window
  - Speed vs. Accuracy: The 3-retry limit for follow-up prompts trades potential completeness for latency/cost efficiency
  - Temperature 0.7: Balances deterministic code generation with the creativity needed to solve novel tasks
- **Failure signatures:**
  - Library Hallucination: The LLM implements logic manually instead of calling the correct external API
  - Test Overfitting: The code passes provided tests but misses the broader intent
  - Repetitive Errors: The system fails if the follow-up prompt results in the exact same erroneous code
- **First 3 experiments:**
  1. **Baseline Reproduction:** Implement the prompt template and run it against the 15 "Additional" tasks to verify performance on unseen code.
  2. **Ablation on Feedback:** Run the synthesis loop without the specific error messages in the follow-up prompt to measure the signal value of the error trace.
  3. **Component Constraint:** Force the prompt to only use standard libraries to stress-test the model's ability to implement algorithms from scratch.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can prompting strategies be refined to strictly enforce the use of required external libraries when the LLM defaults to implementing logic from scratch?
- **Basis:** Page 11 details two failure cases where the LLM disregarded instructions to use specific APIs (e.g., `JSONPath.eval()`) and instead attempted to manually implement traversal logic, resulting in test failures.
- **Why unresolved:** The current combination of chain-of-thought and few-shot learning was insufficient to constrain the model to the specified library components in these instances.

### Open Question 2
- **Question:** Do the reported success rates generalize to smaller or open-source language models, or are they dependent on the specific capabilities of GPT-4o?
- **Basis:** Page 9 notes that the experimental setup exclusively utilized the GPT-4o model with a temperature of 0.7.
- **Why unresolved:** The study does not determine if the 98.5% success rate is an artifact of the specific model's scale/intelligence or the efficacy of the prompt engineering approach itself.

### Open Question 3
- **Question:** What is the relationship between the completeness of provided test cases and the semantic correctness (absence of false positives) of the synthesized APIs?
- **Basis:** Page 4 claims the approach handles corner cases with "just a few test cases," but Section 4.4 reveals that manual verification was required to identify correct programs versus those that merely passed the provided tests.
- **Why unresolved:** While the method outperforms FrAngel with fewer tests, the threshold at which "minimal" test cases fail to prevent semantic bugs (false positives) remains undefined.

## Limitations

- The paper does not fully disclose the 5 few-shot examples or exact formulation of the 15 novel benchmark tasks, creating barriers to independent validation
- Evaluation relies on synthetic test cases rather than real-world usage scenarios, potentially missing production complexity
- The paper lacks analysis of computational costs and latency, making practical deployment feasibility assessment difficult

## Confidence

- **High Confidence:** The core mechanism of using chain-of-thought prompting and iterative refinement is well-supported by experimental results and aligns with established LLM capabilities
- **Medium Confidence:** The claim that LLMs eliminate the need for component specification is supported by results but requires caution, as the paper shows some cases where the model hallucinates implementations
- **Low Confidence:** The generalizability claim to "complex programming tasks" is limited by the benchmark's focus on relatively contained API synthesis problems

## Next Checks

1. **Few-shot Sensitivity Analysis:** Systematically vary the number and selection of few-shot examples (0, 1, 3, 5) to quantify their contribution to the 98.5% success rate and determine if this is a brittle hyperparameter.

2. **Cross-model Evaluation:** Test the prompt template on multiple LLM architectures (e.g., Claude, Llama) to assess whether the approach is tied to GPT-4o's specific capabilities or represents a more general methodology.

3. **Real-world Deployment Study:** Apply the synthesis approach to an open-source Java project with actual API gaps or missing functionality, measuring not just test-passing rates but also code integration quality, maintenance overhead, and developer satisfaction.