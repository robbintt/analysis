---
ver: rpa2
title: A Study on Thinking Patterns of Large Reasoning Models in Code Generation
arxiv_id: '2509.13758'
source_url: https://arxiv.org/abs/2509.13758
tags:
- reasoning
- code
- lrms
- generation
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes reasoning behaviors of large reasoning models
  (LRMs) in code generation. The authors construct a taxonomy of 15 reasoning actions
  across four phases by manually annotating 1,150 reasoning traces from five open-source
  LRMs on 230 code generation tasks.
---

# A Study on Thinking Patterns of Large Reasoning Models in Code Generation

## Quick Facts
- arXiv ID: 2509.13758
- Source URL: https://arxiv.org/abs/2509.13758
- Reference count: 40
- One-line primary result: Manual analysis of 1,150 reasoning traces from five LRMs reveals 15 reasoning actions across four phases, with iterative reasoning correlating with better performance and unit test creation strongly predicting functional correctness.

## Executive Summary
This paper presents the first systematic analysis of reasoning behaviors in large reasoning models (LRMs) during code generation tasks. The authors construct a taxonomy of 15 reasoning actions across four phases by manually annotating 1,150 reasoning traces from five open-source LRMs on 230 code generation tasks. They identify distinct reasoning patterns that follow human-like coding workflows, with more complex tasks eliciting additional reasoning actions. The study reveals that Qwen3 models exhibit iterative reasoning while DeepSeek-R1-7B follows a linear approach, and demonstrates that lightweight prompting strategies incorporating context or reasoning guidelines can improve LRM-generated code performance.

## Method Summary
The study analyzes reasoning behaviors of LRMs using the CoderEval Python benchmark (230 tasks) and five open-source LRMs: DeepSeek-R1-7B, Qwen3-1.7B/8B/14B, and QwQ-32B. The researchers extract reasoning traces and apply open coding with two annotators (5+ years programming experience) to construct a taxonomy of 15 reasoning actions across four phases. They validate their taxonomy on a held-out set (85 tasks/425 traces) with Cohen's Kappa = 0.7054. The analysis uses Pass@1 for functional correctness, phi-coefficient correlation to measure relationships between reasoning actions and correctness, and Apriori association rule mining to identify action combinations. Temperature is set to 0.6 for all models.

## Key Results
- Qwen3 models demonstrate iterative reasoning patterns while DeepSeek-R1-7B follows a linear approach, with iterative reasoning correlating with higher functional correctness
- Unit test creation in reasoning traces shows strong positive correlation with functional correctness (φ = 0.162, p < 0.001)
- Lightweight prompting strategies (context insertion, reasoning guidelines) improve Pass@1 scores by 4.9-7.3% on DeepSeek-R1 and 2.6-4.7% on Qwen2.5 models
- More complex coding tasks elicit additional reasoning actions, following human-like coding workflows with phases of clarification, analysis, implementation, and verification

## Why This Works (Mechanism)
The study's methodology works because manual annotation of reasoning traces reveals systematic patterns in how LRMs approach code generation, capturing the cognitive processes that influence final output quality. By focusing on the reasoning phase rather than just final code, the researchers can identify which intermediate steps correlate with success. The temperature setting of 0.6 provides sufficient diversity in reasoning traces while maintaining coherence. The phi-coefficient analysis effectively measures the strength of association between specific reasoning actions and correctness, while association rule mining identifies combinations of actions that commonly occur together in successful generations.

## Foundational Learning
- **Open coding methodology**: Why needed - to systematically categorize qualitative reasoning trace data; Quick check - verify that action definitions are clear and consistently applicable across different traces
- **Phi-coefficient correlation**: Why needed - to measure strength of association between binary variables (reasoning action presence vs. correctness); Quick check - ensure contingency tables are properly constructed with correct expected frequencies
- **Apriori association rule mining**: Why needed - to discover frequent patterns of reasoning actions that co-occur; Quick check - validate support and confidence thresholds are appropriately set for the dataset size
- **Cohen's Kappa for inter-rater agreement**: Why needed - to measure annotation reliability beyond chance agreement; Quick check - calculate kappa values for different action categories to identify problematic distinctions
- **Pass@1 evaluation**: Why needed - to measure functional correctness of generated code against test cases; Quick check - verify test harness correctly executes and evaluates all generated solutions
- **Reasoning trace extraction**: Why needed - to access intermediate thought processes rather than just final output; Quick check - confirm that extracted traces include all relevant thinking steps without filtering

## Architecture Onboarding

**Component Map**: CoderEval tasks -> Prompt formatting -> LRM inference (temperature=0.6) -> Reasoning trace extraction -> Manual annotation -> Taxonomy construction -> Correlation analysis -> Validation

**Critical Path**: Task selection → Prompt generation → Model inference → Trace extraction → Annotation → Taxonomy validation → Performance analysis

**Design Tradeoffs**: Manual annotation provides rich qualitative insights but introduces subjectivity and limits scalability; focusing on Python only enables deep analysis but reduces generalizability; using open-source models enables reproducibility but may not represent state-of-the-art commercial systems.

**Failure Signatures**: 
- Temperature=0 causes timeouts and repetitive reasoning
- Reasoning traces may not align with final output if documentation is filtered
- Ambiguity recognition can trigger reasoning loops on unclear prompts
- Annotation disagreements arise at boundaries between similar action types (SCG vs. CCG)

**Three First Experiments**:
1. Run a single LRM (Qwen3-1.7B) on 5 diverse CoderEval tasks, extract reasoning traces, and manually verify trace structure matches expected phases
2. Apply the taxonomy to annotate 10 reasoning traces from different models to test action definitions' applicability
3. Execute Pass@1 evaluation on 3 tasks to confirm test harness functionality and correctness measurement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent do the identified reasoning patterns (iterative vs. waterfall) generalize across different programming languages beyond Python?
- Basis in paper: [explicit] "Exploring reasoning traces for other programming languages remains an important direction for future work."
- Why unresolved: The study focused only on Python due to imbalanced Java task distribution; no empirical data exists on cross-language reasoning behavior consistency.
- What evidence would resolve it: A replicated study using CoderEval's Java subset or other language benchmarks (e.g., HumanEval-X) with balanced dependency levels, applying the same taxonomy annotation protocol.

### Open Question 2
- Question: Can explicitly training models with iterative-style reasoning traces improve functional correctness compared to waterfall-style traces?
- Basis in paper: [inferred] The paper notes Qwen3 (iterative) outperforms DeepSeek-R1-7B (waterfall), and suggests "generating higher-quality reasoning traces that emulate realistic iterative programming practices for model fine-tuning."
- Why unresolved: The correlation between reasoning style and correctness is observational; causal effects of training data style remain untested.
- What evidence would resolve it: Fine-tuning experiments where models are trained on matched datasets differing only in reasoning trace style (iterative vs. waterfall), with Pass@1 comparisons on held-out code generation tasks.

### Open Question 3
- Question: How can unit test creation in reasoning traces be made more reliable to better predict functional correctness?
- Basis in paper: [explicit] "LRMs generate test cases... However, this behavior is inconsistent... their correctness is unreliable" with examples of flawed expected outputs despite correct reasoning structure.
- Why unresolved: The positive correlation between UTC and correctness exists, but the generated tests themselves contain computation errors and hallucinated expectations.
- What evidence would resolve it: Studies on test verification mechanisms (e.g., execution-based validation of generated tests, formal specification integration) that measure both test accuracy and correlation with final code correctness.

## Limitations
- Manual annotation introduces subjectivity in defining and distinguishing between the 15 reasoning actions, particularly for fine-grained categories like SCG versus CCG
- The study only examines five open-source LRMs, limiting generalizability to other reasoning models and commercial systems
- The temperature setting of 0.6 may influence reasoning patterns differently across model architectures, though this was not systematically explored

## Confidence
- **High Confidence**: The identification of iterative vs. linear reasoning patterns between Qwen3 and DeepSeek-R1 models is supported by clear quantitative differences in action counts and frequencies. The correlation between unit test creation and functional correctness (φ = 0.162, p < 0.001) is statistically significant and robust.
- **Medium Confidence**: The taxonomy construction methodology is sound, but the subjective nature of open coding means some action boundaries may be debatable. The finding that more complex tasks elicit additional reasoning actions is plausible but could benefit from additional validation with different task distributions.
- **Low Confidence**: The effectiveness of lightweight prompting strategies (context insertion, reasoning guidelines) on improving code correctness shows promise but lacks systematic ablation studies. The practical significance of these improvements in real-world coding scenarios remains unclear.

## Next Checks
1. Replicate inter-rater agreement analysis: Have three additional annotators independently label 100 reasoning traces from the validation set to verify the Cohen's Kappa score of 0.7054 and assess whether action definitions are consistently interpretable.
2. Test prompting strategies across model families: Apply the context insertion and reasoning guidelines prompting techniques to at least two additional LRMs (e.g., OpenAI's o1, Google's Gemini Flash Thinking) to determine if improvements in Pass@1 are model-specific or generalizable.
3. Analyze reasoning trace content vs. code output alignment: For a random sample of 50 reasoning traces where unit tests were created, manually verify whether the tests actually correspond to the final generated code and check if they would catch potential bugs, to validate the φ = 0.162 correlation.