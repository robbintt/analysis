---
ver: rpa2
title: Improving Interpretability and Accuracy in Neuro-Symbolic Rule Extraction Using
  Class-Specific Sparse Filters
arxiv_id: '2501.16677'
source_url: https://arxiv.org/abs/2501.16677
tags:
- loss
- filter
- filters
- rule-set
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of information loss when extracting
  interpretable rule-sets from CNNs for image classification. The core method introduces
  a novel sparsity loss function that enforces class-specific filters to converge
  towards binary outputs (0 or 1) during training, rather than relying on post-training
  binarization.
---

# Improving Interpretability and Accuracy in Neuro-Symbolic Rule Extraction Using Class-Specific Sparse Filters

## Quick Facts
- **arXiv ID**: 2501.16677
- **Source URL**: https://arxiv.org/abs/2501.16677
- **Reference count**: 13
- **Primary result**: 9% higher accuracy and 53% smaller rule-set size compared to NeSyFOLD-EBP while maintaining fidelity within 3% of the original CNN

## Executive Summary
This paper addresses the fundamental problem of information loss when extracting interpretable rule-sets from CNNs for image classification. The authors introduce a novel sparsity loss function that enforces class-specific filters to converge towards binary outputs (0 or 1) during training, rather than relying on post-training binarization. This approach is evaluated across five training strategies, with TS3 (random initialization of class-specific filters) achieving the best results: 9% higher accuracy and 53% smaller rule-set size compared to the previous state-of-the-art (NeSyFOLD-EBP), while maintaining fidelity within 3% of the original CNN. The method also scales well to datasets with more classes (10 and 43 classes) and produces interpretable rule-sets that align with visual concepts learned by filters.

## Method Summary
The method trains a CNN with a novel sparsity loss to force filter outputs toward binary values (0/1) during training, then extracts symbolic rules from these binarized outputs. The core innovation is the sparsity loss function that uses sigmoid normalization with threshold subtraction to enable binary separation. The Filter Probability Matrix (P) assigns each class a subset of "responsible" filters, and five training strategies vary how this matrix is initialized. The approach combines this sparsity loss with standard cross-entropy loss, then extracts rules using the FOLD-SE-M algorithm. The TS3 strategy (random initialization) outperforms data-driven initialization and produces more accurate, compact rule-sets while maintaining high fidelity to the original CNN's decisions.

## Key Results
- TS3 strategy achieves 9% higher accuracy than NeSyFOLD-EBP baseline
- Rule-set size reduced by 53% compared to previous state-of-the-art
- Maintains fidelity within 3% of original CNN predictions
- Scales effectively to 10 and 43 class datasets
- Random filter initialization (TS3) outperforms data-driven initialization (TS2)

## Why This Works (Mechanism)
The method works by fundamentally altering how the CNN learns to represent visual concepts during training. By introducing the sparsity loss, filters are forced to specialize for specific classes rather than learning distributed representations for the downstream fully-connected layers. The threshold-subtracted sigmoid ensures binary separation, preventing information leakage that occurs with direct sigmoid outputs. Random initialization of the Filter Probability Matrix forces filters to learn new, specialized roles from scratch, which proves more effective than repurposing pre-trained filters. This end-to-end optimization creates filters that are naturally interpretable and directly compatible with symbolic rule extraction.

## Foundational Learning
- **Concept: Convolutional Feature Maps as Concept Detectors**
  - **Why needed here:** This method fundamentally alters the representation learned by the final convolutional layer, moving from continuous distributed representations to discrete logical predicates
  - **Quick check question:** For a filter in the final convolutional layer of a standard CNN, what does a high L2-norm of its feature map signify? (It signifies that the visual pattern or concept that the filter has learned to detect is strongly present in the input image)

- **Concept: Loss Function Composition & Multi-Objective Optimization**
  - **Why needed here:** The novel sparsity loss is combined with standard cross-entropy loss, requiring understanding of how these competing objectives interact
  - **Quick check question:** If the hyperparameter β (weight for L_sparsity) is set too low, what will likely happen to the filter outputs? (They will remain continuous and un-binarized, leading to high information loss when the rule-set is extracted)

- **Concept: Symbolic Rule Extraction from Boolean Vectors**
  - **Why needed here:** The core output is a rule-set, not just a class score, requiring understanding of how continuous CNN outputs convert to discrete binary inputs for symbolic rule learners
  - **Quick check question:** An image activates filter 5 strongly (L2-norm = 50) and filter 12 weakly (L2-norm = 2). If the threshold for both is 10, what is the resulting binary vector segment? ([filter_5=1, filter_12=0])

## Architecture Onboarding

- **Component map:**
  - Base CNN (VGG16 with modified final conv layer) -> Filter Probability Matrix (P) -> Threshold Tensor -> Sparsity Loss Module (L_sparsity) -> Rule Extraction Pipeline (NeSyFOLD)

- **Critical path:** The performance of the entire system depends on the random initialization of the Filter Probability Matrix (P) in TS3 strategy. This step assigns each filter its "job" of detecting specific class concepts. Poor assignment leads to suboptimal specialization and degraded accuracy and interpretability.

- **Design tradeoffs:**
  - TS2 (Data-driven) vs. TS3 (Random) Initialization: TS2 uses pretrained CNN's activation patterns, TS3 assigns randomly. TS3 outperforms TS2, suggesting re-purposing pretrained filters for novel loss can be worse than forcing them to learn from scratch
  - Cross-Entropy vs. Sparsity (TS4): Training solely on sparsity loss yields smaller, highly interpretable rule-sets and impressive stand-alone accuracy, but the resulting model has no fidelity to original CNN's decision-making process
  - Rule-Set Size vs. Fidelity: TS2/TS3 produce more accurate, faithful models with larger rule-sets; TS4 produces less faithful, highly compact model

- **Failure signatures:**
  - Stuck Sigmoid (TS5-like failure): Without threshold subtraction, sigmoid outputs clamp in [0.5, 1], preventing irrelevant filters from reaching zero and causing information leakage
  - Collapsed Filters: If β is too high, all filters may be forced to near-binary state prematurely, preventing learning of subtle features and resulting in low accuracy
  - Contradictory Targets: If a single filter is assigned as "relevant" for two classes with very different visual features, conflicting gradients prevent specialization and hurt performance for both classes

- **First 3 experiments:**
  1. Baseline Comparison (TS1 vs. TS2 vs. TS3): Replicate core finding on simple 2-3 class subset of Places. Train three models using respective Filter Probability Matrix computation methods. Measure accuracy gap between CNN and extracted rule-set. Goal: Validate TS3 produces smallest accuracy-fidelity gap.
  2. Ablation of Thresholding (TS3 vs. TS5): Train two models on same dataset, one with full threshold-subtracted sigmoid, one with direct sigmoid (no threshold subtraction). Compare NeSy model accuracy. Goal: Confirm critical role of threshold subtraction in enabling binary separation.
  3. Fidelity vs. Compactness (TS3 vs. TS4): Train one model with both losses (TS3) and one with only sparsity loss (TS4). Compare rule-set sizes and fidelity to original CNN. Goal: Characterize tradeoff between explaining existing CNN vs. creating standalone interpretable classifier.

## Open Questions the Paper Calls Out
- **Can the proposed sparsity loss function be effectively adapted for Vision Transformers (ViTs) using sparse autoencoders to extract concepts from attention layers?**
  - The paper suggests it would be intriguing to investigate whether a similar sparsity loss function could be adapted for Vision Transformers, using sparse autoencoders to address the lack of direct concept filters. This remains unresolved because the current method relies on convolutional filters which capture spatial hierarchies, whereas attention layers operate differently.

- **Can integrating the symbolic rule-set directly into the training loop (e.g., via soft decision trees) further reduce information loss compared to the current post-hoc extraction?**
  - The authors identify integrating the symbolic rule-set into the training loop as a promising future direction to mitigate information loss by allowing gradient backpropagation through the rule structure. The current methodology trains the CNN first and extracts rules second, but an end-to-end differentiable approach might optimize filters specifically for the rule-set's logic.

- **Why does random filter initialization (Training Strategy 3) outperform initialization based on pre-trained filter strengths (Training Strategy 2)?**
  - The paper observes that TS3 achieves better performance than TS2 and hypothesizes that ImageNet weights are suboptimal for the sparsity loss, but does not empirically validate the mechanism or test if "sparsity-aware" pre-training could bridge this gap. The underlying dynamic regarding why random assignment facilitates better binary convergence is not theoretically or empirically isolated.

## Limitations
- The superiority of random filter initialization (TS3) over data-driven initialization (TS2) is surprising and may be architecture-dependent
- The paper does not explicitly state whether thresholds for TS3 are recomputed from random filters or inherited from ImageNet weights
- Dependence on FOLD-SE-M implementation details, which are not provided in the paper or supplementary material

## Confidence

- **High Confidence**: Core sparsity loss formulation and integration with cross-entropy training (tested through ablation study TS5); overall improvement in accuracy and rule-set size over baseline; fidelity-interpretability tradeoff characterization (TS4)
- **Medium Confidence**: Specific hyperparameter values (β=5, h₁=0.6, h₂=0.7, Adam LR=5e-6); superiority of TS3 over TS2 (random vs. data-driven initialization); generalization to 10 and 43 class datasets
- **Low Confidence**: Exact mechanism for threshold calculation in TS3 (random initialization case); FOLD-SE-M algorithm implementation details; whether results would replicate on architectures other than VGG16

## Next Checks

1. **Threshold Sensitivity Analysis**: Reproduce the main experiment (TS3 vs baseline) while systematically varying the threshold hyperparameters h₁ and h₂. Document how rule-set size, accuracy, and fidelity change across different threshold values to identify if the reported results are robust to these critical parameters.

2. **Architecture Generalization Test**: Implement the same method on a different backbone architecture (e.g., ResNet-18 or MobileNetV2) using the same Places dataset subset. Compare accuracy-fidelity tradeoffs to determine if the random initialization advantage (TS3) is architecture-specific or generalizable.

3. **Cross-Entropy Ablation**: Train models using only L_sparsity (β=5, α=0) and only L_CE (β=0, α=1) on the same dataset. Measure the resulting rule-set sizes, accuracy, and interpretability to precisely quantify the contribution of each loss component to the final performance.