---
ver: rpa2
title: 'Socratic Students: Teaching Language Models to Learn by Asking Questions'
arxiv_id: '2512.13102'
source_url: https://arxiv.org/abs/2512.13102
tags:
- student
- math
- questions
- question
- turns
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models typically answer questions passively, but
  many real-world tasks require them to ask targeted questions to fill knowledge gaps.
  This work introduces Outcome-Driven Question Optimization Strategy (ODQS), a framework
  that trains language models to ask better questions based on downstream task performance
  rather than human judgments.
---

# Socratic Students: Teaching Language Models to Learn by Asking Questions

## Quick Facts
- arXiv ID: 2512.13102
- Source URL: https://arxiv.org/abs/2512.13102
- Reference count: 40
- Large language models can be trained to ask better questions based on downstream task performance, achieving up to 54.7% improvement in task completion

## Executive Summary
Large language models typically answer questions passively, but many real-world tasks require them to ask targeted questions to fill knowledge gaps. This work introduces Outcome-Driven Question Optimization Strategy (ODQS), a framework that trains language models to ask better questions based on downstream task performance rather than human judgments. The approach achieves significant improvements in both math problem-solving and coding tasks, with students matching static baselines in three fewer turns.

## Method Summary
ODQS is a framework that trains language models to ask better questions based on downstream task performance rather than human judgments. In ODQS, multiple candidate questions are generated at each turn, executed with a teacher model, and scored by how much they improve task completion (measured via Pass@k). The highest-scoring question is then used as a positive example to fine-tune the student via supervised fine-tuning and Direct Preference Optimization (DPO), without human-labeled question quality. The approach is evaluated on GSM8K (math) and HumanEval/OpenCoder (coding) benchmarks, where baseline models start near-zero performance.

## Key Results
- ODQS-trained students achieve up to 54.7% absolute improvement in Pass@5 on math tasks
- Students match static baselines in three fewer turns while achieving 22.9% improvement on coding tasks
- Gains stem from both improved progress per turn and asking more advanced questions earlier in the interaction

## Why This Works (Mechanism)
ODQS works by aligning question quality with actual task outcomes rather than proxy metrics. By generating multiple candidate questions per turn and evaluating them through a teacher model, the framework identifies which questions actually advance task completion. The direct optimization of question quality based on downstream performance (Pass@k) creates a feedback loop that progressively improves the student's questioning strategy. The use of both supervised fine-tuning and DPO allows the model to learn from both explicit positive examples and preference patterns.

## Foundational Learning
- Question generation strategies: Needed to produce diverse candidate questions; Quick check: Can generate at least 5 distinct questions per turn
- Teacher model execution: Required to simulate task completion; Quick check: Teacher provides consistent responses to same questions
- Outcome measurement (Pass@k): Essential for quantifying task progress; Quick check: Pass@k correlates with actual solution quality
- Preference optimization: Necessary for fine-tuning on relative quality; Quick check: DPO improves over SFT alone

## Architecture Onboarding

### Component Map
Teacher Model -> Question Generator -> Execution Engine -> Scorer -> SFT/DPO Module -> Student Model

### Critical Path
1. Student asks question
2. Multiple candidates generated
3. Teacher executes each candidate
4. Outcomes scored via Pass@k
5. Best question selected
6. Student fine-tuned on selected questions

### Design Tradeoffs
- Multiple candidates vs. computational cost: More candidates improve selection quality but increase latency
- Teacher model quality vs. student generalization: High-quality teachers may overfit students to specific reasoning patterns
- Direct outcome optimization vs. human preferences: Task performance may not align with human notions of "good" questions

### Failure Signatures
- Students asking repetitive or circular questions
- Over-reliance on specific question templates
- Degradation in question quality over extended interactions
- Performance plateaus despite continued training

### First Experiments to Run
1. Baseline comparison without ODQS fine-tuning
2. Ablation study with single question per turn
3. Cross-domain transfer test (e.g., math-trained student on coding tasks)

## Open Questions the Paper Calls Out
Major uncertainties remain around the scalability and robustness of the ODQS framework. The experiments rely on synthetic task environments with oracle teacher models, raising questions about how well the approach transfers to real-world scenarios where ground truth is uncertain or delayed. The evaluation focuses on synthetic tasks (GSM8K, HumanEval) rather than authentic interactive domains where question-asking is genuinely necessary, limiting external validity. The computational overhead of generating and evaluating multiple candidate questions per turn is not analyzed, making it unclear whether the performance gains justify the increased cost in practical applications.

## Limitations
- Reliance on synthetic task environments with oracle teacher models
- Focus on synthetic benchmarks rather than authentic interactive domains
- Unanalyzed computational overhead of generating multiple candidate questions per turn

## Confidence
- High confidence: The experimental methodology for ODQS training is clearly described and internally consistent, with reproducible results on the synthetic benchmarks
- Medium confidence: The reported performance improvements (54.7% on math, 22.9% on coding) are well-documented for the specific evaluation setup, but generalization to broader task domains remains uncertain
- Medium confidence: The claim that ODQS-trained models ask more advanced questions earlier is supported by the evidence, though the definition of "advanced" is task-specific and may not translate across domains

## Next Checks
1. Test ODQS on authentic interactive domains where question-asking is genuinely necessary (e.g., customer support, medical diagnosis) rather than synthetic tasks with known solutions
2. Conduct ablation studies removing the teacher model to assess how well ODQS generalizes when ground truth is uncertain or noisy
3. Measure computational overhead and latency impacts of generating multiple candidate questions per turn, comparing the cost-benefit ratio against simpler question-asking approaches