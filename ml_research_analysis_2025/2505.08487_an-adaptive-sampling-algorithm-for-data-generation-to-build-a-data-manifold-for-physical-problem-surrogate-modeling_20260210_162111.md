---
ver: rpa2
title: An adaptive sampling algorithm for data-generation to build a data-manifold
  for physical problem surrogate modeling
arxiv_id: '2505.08487'
source_url: https://arxiv.org/abs/2505.08487
tags:
- data
- points
- manifold
- input
- sampling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents an Adaptive Sampling Algorithm for Data Generation
  (ASADG) designed to build better data manifolds for surrogate modeling of physical
  problems. The method addresses the challenge of generating representative training
  data for computationally expensive physical simulations by iteratively adding new
  data points to the manifold based on simplicial triangulation and a residual-based
  metric threshold.
---

# An adaptive sampling algorithm for data-generation to build a data-manifold for physical problem surrogate modeling

## Quick Facts
- **arXiv ID:** 2505.08487
- **Source URL:** https://arxiv.org/abs/2505.08487
- **Reference count:** 26
- **Primary result:** Adaptive sampling algorithm outperforms LHS and uniform sampling for building data manifolds in surrogate modeling of physical problems

## Executive Summary
This paper introduces an Adaptive Sampling Algorithm for Data Generation (ASADG) that builds better data manifolds for surrogate modeling of physical problems. The method iteratively adds new data points to the manifold based on simplicial triangulation and a residual-based metric threshold, addressing the challenge of generating representative training data for computationally expensive physical simulations. The algorithm efficiently captures complex manifold features with fewer points by focusing on regions of high curvature.

For low-dimensional problems, ASADG achieves 33% better mean normalized relative error (MNRE) in neural network predictions compared to Latin Hypercube Sampling (LHS) with reduced variance. In high-dimensional cases, it significantly outperforms uniform sampling with 16% lower MNRE and tighter error distributions. While rejected points increase computational cost, the resulting data manifolds lead to more accurate surrogate models that better represent the underlying physical problem.

## Method Summary
ASADG builds data manifolds through an iterative adaptive sampling process. The algorithm starts with an initial set of points and performs simplicial triangulation to decompose the parameter space. At each iteration, it evaluates a residual-based metric for candidate points and adds those that exceed a threshold to the manifold. This process continues until convergence criteria are met. The method uses the triangulation structure to identify regions requiring additional sampling based on geometric features like curvature and feature density, allowing it to focus computational resources where they provide the most value for surrogate model accuracy.

## Key Results
- Achieves 33% better MNRE than LHS in low-dimensional problems with reduced prediction variance
- Outperforms uniform sampling by 16% in high-dimensional cases with tighter error distributions
- Efficiently captures complex manifold features with fewer points by focusing on high-curvature regions

## Why This Works (Mechanism)
The algorithm's effectiveness stems from its ability to identify and sample regions of high geometric complexity in the data manifold. By using simplicial triangulation, it can detect areas where the underlying function has high curvature or rapid variation. The residual-based metric threshold ensures that sampling density increases where the surrogate model would otherwise struggle to capture important features. This adaptive approach concentrates computational resources on the most information-rich regions of the parameter space, leading to more efficient data generation compared to uniform or space-filling methods.

## Foundational Learning
- **Simplicial triangulation**: Decomposes the parameter space into simplices to enable geometric analysis and adaptive sampling
  - *Why needed:* Provides the geometric framework to identify regions requiring additional sampling based on manifold structure
  - *Quick check:* Verify that triangulation correctly captures the underlying manifold topology in low dimensions

- **Residual-based metrics**: Quantifies the local approximation error to determine where additional sampling is beneficial
  - *Why needed:* Enables data-driven identification of regions where surrogate model accuracy is insufficient
  - *Quick check:* Confirm that residual metric correlates with actual prediction error on validation points

- **Adaptive thresholding**: Dynamically adjusts sampling criteria based on current manifold coverage and feature density
  - *Why needed:* Prevents oversampling in already-well-represented regions while ensuring adequate coverage of complex features
  - *Quick check:* Test threshold sensitivity by varying the parameter and observing sampling distribution changes

## Architecture Onboarding

**Component Map:**
Initial points -> Simplicial triangulation -> Residual evaluation -> Adaptive threshold -> Candidate selection -> Manifold update

**Critical Path:**
The most computationally intensive step is the simplicial triangulation and residual evaluation loop. This determines where new points should be added and directly impacts the quality of the final data manifold. The adaptive threshold mechanism acts as a control system that balances exploration of new regions against exploitation of known features.

**Design Tradeoffs:**
The algorithm trades computational overhead from rejected point evaluations against improved surrogate model accuracy. The simplicial triangulation approach is more computationally expensive than simple uniform sampling but provides geometric information that enables targeted sampling. The adaptive threshold introduces a hyperparameter that requires tuning but allows the algorithm to adapt to different problem characteristics.

**Failure Signatures:**
- Poor convergence or excessive iterations may indicate inappropriate threshold settings
- Oversampling in certain regions while missing important features suggests issues with the residual metric or triangulation
- High rejection rates increase computational cost without proportional accuracy gains

**3 First Experiments:**
1. Compare ASADG sampling distribution against LHS and uniform sampling on a simple 2D test function with known curvature features
2. Measure convergence behavior by tracking MNRE as a function of sample size for a benchmark physical simulation problem
3. Perform sensitivity analysis on the adaptive threshold parameter to establish tuning guidelines for different problem types

## Open Questions the Paper Calls Out
None

## Limitations
- Effectiveness for problems with sharp discontinuities or highly irregular feature distributions remains uncertain
- Reliance on simplicial triangulation may struggle with extremely high-dimensional spaces due to the curse of dimensionality
- The adaptive thresholding mechanism introduces a hyperparameter requiring careful tuning for different problem types

## Confidence

**High Confidence:**
- Performance advantage over uniform sampling and LHS for low-dimensional problems is well-established through systematic comparison
- Theoretical foundation using simplicial triangulation and residual-based metrics is sound and properly implemented

**Medium Confidence:**
- Claims about computational efficiency gains are reasonable but may not fully account for overhead from rejected point evaluations
- Generalizability to problems with non-smooth features or discontinuities has not been thoroughly validated

**Low Confidence:**
- Scalability claims for high-dimensional problems beyond tested cases are based on limited evidence
- Sensitivity to adaptive threshold parameter and its impact on different problem types requires further investigation

## Next Checks
1. Test ASADG on problems with sharp discontinuities and non-smooth features to evaluate robustness beyond the smooth manifolds presented in the paper
2. Conduct systematic sensitivity analysis on the adaptive threshold parameter across different problem types to establish optimal tuning guidelines
3. Compare computational efficiency including rejected point overhead against other adaptive sampling methods like Bayesian optimization or uncertainty sampling on a standardized benchmark suite