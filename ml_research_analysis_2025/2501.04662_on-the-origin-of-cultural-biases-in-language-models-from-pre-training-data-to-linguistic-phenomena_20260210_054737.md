---
ver: rpa2
title: 'On The Origin of Cultural Biases in Language Models: From Pre-training Data
  to Linguistic Phenomena'
arxiv_id: '2501.04662'
source_url: https://arxiv.org/abs/2501.04662
tags:
- entities
- arabic
- arab
- english
- western
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the origins of cultural biases in language
  models (LMs) towards Western entities when operating in non-Western languages, specifically
  Arabic. The authors introduce CAMeL-2, a parallel Arabic-English benchmark of 58,086
  entities across seven types, and 367 natural contexts.
---

# On The Origin of Cultural Biases in Language Models: From Pre-training Data to Linguistic Phenomena

## Quick Facts
- **arXiv ID:** 2501.04662
- **Source URL:** https://arxiv.org/abs/2501.04662
- **Reference count:** 40
- **Key outcome:** LMs show reduced performance gaps between Arab and Western entities when evaluated in English vs. Arabic, driven by Arabic polysemy, tokenization choices, and cross-script lexical overlap

## Executive Summary
This paper investigates why language models exhibit cultural biases favoring Western entities when operating in non-Western languages, specifically Arabic. The authors introduce CAMeL-2, a parallel Arabic-English benchmark with 58,086 entities across seven types and 367 contexts. Their comprehensive evaluation reveals that LMs struggle with high-frequency Arab entities that exhibit polysemy (multiple word senses), particularly when frequency-based tokenization merges them into single tokens. Cross-script lexical overlap with Farsi, Urdu, Pashto, and Kurdish also degrades performance. The study demonstrates that these linguistic phenomena, rather than representational bias alone, contribute significantly to perceived Western bias in LMs.

## Method Summary
The study introduces CAMeL-2, a parallel Arabic-English benchmark containing 58,086 entities (34,842 Arab, 18,236 Western) across seven types and 367 natural contexts. Three evaluation tasks are employed: Text Infilling using a Cultural Bias Score metric, Extractive QA with specific prompt templates, and NER fine-tuning on ANERCorp (Arabic) and CoNLL-2003 (English) datasets. Entity frequencies are calculated from mC4 corpus using Aho-Corasick string search. Polysemy detection uses Almaany dictionary lookup, while cross-script overlap analysis examines entity frequencies in Farsi, Urdu, Pashto, and Kurdish. Models evaluated include Llama-3.3, Qwen-2.5, ARBERT, and Aya23 across different vocabulary sizes.

## Key Results
- Performance gaps between Arab and Western entities drop from 10-27 points in Arabic to <3 points in English contexts
- High-frequency polysemous Arab entities show 40-60% QA accuracy versus ~90% for non-polysemous entities
- Single-token polysemous entities perform worst, with ARBERT (93k vocab) showing 25% F1 drop versus medium-vocab models
- Performance degrades monotonically with cross-script lexical overlap frequency in Farsi/Urdu/Pashto/Kurdish

## Why This Works (Mechanism)

### Mechanism 1: Arabic Word Polysemy Creates Ambiguous Token Representations
- **Claim:** LMs struggle with Arab entities that hold multiple word senses, causing perceived Western bias when the same token sequence represents both an entity and a common word with different grammatical functions.
- **Core assumption:** Single-token representations for polysemous words reduce available disambiguation signals, and context alone is insufficient for sense resolution in current architectures.
- **Evidence anchors:** Figure 5 shows QA accuracy drops to 40-60% for Arab countries with high polysemy percentages vs. ~90% for Western entities. Figure 4 confirms polysemous entities consistently underperform non-polysemous ones across all model scales.
- **Break condition:** If polysemous entities were tokenized into multiple subword units that separated entity-specific morphemes OR if architectures incorporated explicit sense-disambiguation modules.

### Mechanism 2: Cross-script Lexical Overlap Causes Representation Interference
- **Claim:** Arab entities that frequently appear in other Arabic-script languages (Farsi, Urdu, Pashto, Kurdish) degrade LM performance due to meaning conflicts in shared token space.
- **Core assumption:** Multilingual models share representations across script-compatible languages without adequate language-specific routing, and pre-training does not encode language identity as a contextual signal.
- **Evidence anchors:** Figure 6 shows monotonic performance degradation as entity frequency in Farsi/Urdu/Pashto/Kurdish increases. Locations and food entities are most affected.
- **Break condition:** If pre-training incorporated explicit language markers per document/segment OR if tokenization included language-specific subword vocabularies for Arabic-script languages.

### Mechanism 3: Large Arabic Vocabularies Collapse Context for Polysemous Tokens
- **Claim:** Increasing Arabic vocabulary size harms performance on high-frequency polysemous entities because frequency-based tokenizers merge them into single tokens, removing subword context needed for sense disambiguation.
- **Core assumption:** Subword splits provide disambiguating morphological or positional cues that single tokens lack, and current attention mechanisms don't compensate for this information loss.
- **Evidence anchors:** Figure 7 shows performance is poorest for single-token polysemous entities across all models. Figure 8 shows inverted U-curve: medium vocabulary sizes outperform very large and very small vocabularies.
- **Break condition:** If tokenization algorithms incorporated polysemy-aware merging rules OR if vocabulary construction balanced frequency against ambiguity metrics.

## Foundational Learning

### Concept: Polysemy in Morphologically Rich Languages (Arabic-specific)
- **Why needed here:** Arabic's root-pattern morphology allows words to function as nouns, verbs, and adjectives without morphological markers. Unlike English (capitalization distinguishes "Mark" from "mark"), Arabic has no visual cue separating entities from common words.
- **Quick check question:** The Arabic word "مقلوبة" appears in two contexts: (1) "تحضر جدتي أفضل مقلوبة" (My grandma makes the best Makloube) and (2) describing something as "flipped." What linguistic feature allows this dual use, and why doesn't English "lasagna" have this issue?

### Concept: Frequency-based Subword Tokenization (BPE/WordPiece)
- **Why needed here:** The paper's mechanism depends on understanding how tokenizers work. BPE iteratively merges the most frequent character pairs. A word appearing >1M times gets merged early, becoming a single token.
- **Quick check question:** You have two Arabic entities: "دمشق" (Damascus, ~500k occurrences) and "قرية-صغيرة-في-جنوب-لبنان" (~50 occurrences). How would BPE with 30k vs. 90k vocabulary size tokenize each, and what are the implications for model performance?

### Concept: Script Sharing vs. Language Families in Multilingual Models
- **Why needed here:** Arabic script is used for Arabic, Farsi, Urdu, Pashto, Kurdish—but these belong to different language families (Semitic, Indo-European). They share vocabulary through loanwords but have different meanings.
- **Quick check question:** A multilingual model trained on Arabic and Farsi sees the word "وزنة" (Ouzanne/weight). In Arabic, it's a Moroccan town. In Farsi, it means "weight." Why doesn't this same issue occur between French "Paris" and English "Paris," even though both use Latin script?

## Architecture Onboarding

### Component Map:
CAMeL-2 Dataset -> Entities (58,086 total: 34,842 Arab, 18,236 Western) -> Contexts (367 masked natural contexts) -> Parallel: Arabic ↔ English translations
Evaluation Pipeline -> Task 1: Text-Infilling (CBS metric) -> Task 2: Extractive QA (accuracy) -> Task 3: NER (F1)
Analysis Layers -> Frequency Stratification -> Polysemy Detection -> Tokenization Analysis -> Cross-script Overlap

### Critical Path:
1. Load parallel data: Arabic + English entities/contexts from CAMeL-2 (88.34% have Wikidata mappings; remainder manually transliterated)
2. Run bilingual evaluation: Same entities in Arabic contexts (high polysemy) vs. English contexts (low polysemy)
3. Identify failure modes: Frequency analysis → high-frequency failures, Dictionary lookup → polysemy correlation, Tokenization → single-token vs. multi-token performance gap, Cross-script → overlap with Farsi/Urdu/Pashto/Kurdish frequencies
4. Quantify bias: ΔAcc = Western_Acc - Arab_Acc (should be ~0 for fair model; paper finds 10-27 point gaps in Arabic, <3 points in English)

### Design Tradeoffs:
| Choice | Benefit | Cost | Paper's Finding |
|--------|---------|------|-----------------|
| Large Arabic vocabulary (90k+) | Better rare word coverage | Collapses polysemous high-frequency entities into single tokens | ARBERT (93k) worst on polysemous entities |
| Medium vocabulary (30-60k) | Balances coverage + context preservation | May OOV on rare dialectal words | Llama-3.3 (3.8k Arabic tokens), Aya23 (6.6k) perform best |
| English evaluation proxy | Reduces cultural gaps | Doesn't reflect real Arabic user experience | Gaps drop to <3%, but masks underlying polysemy issues |
| Long implicit contexts | More realistic evaluation | Harder to attribute failures to specific phenomena | Paper uses both for robustness |

### Failure Signatures:
- **Signature 1: High-frequency polysemy collapse** - Trigger: Entity frequency >1M in mC4 AND dictionary shows multiple meanings. Symptom: QA accuracy 40-60% (vs. 90%+ for non-polysemous). Example: "متروحة" (Matrooha location vs. "proposed" adjective).
- **Signature 2: Single-token context starvation** - Trigger: Tokenizer produces 1 token AND token is polysemous. Symptom: Performance degrades with vocabulary size increase. Example: ARBERT (93k vocab) shows 25% F1 drop vs. medium-vocab models.
- **Signature 3: Cross-script meaning conflict** - Trigger: Entity appears in Farsi/Urdu/Pashto/Kurdish with different meaning. Symptom: Performance drops monotonically with cross-script frequency. Example: "وزنة" (Moroccan town / Farsi "weight").
- **Signature 4: Western entity advantage in Arabic** - Trigger: Entity is Western transliteration (e.g., "لازانيا" / Lasagna). Symptom: ~90% accuracy regardless of frequency. Reason: Transliterations are non-polysemous in Arabic.

### First 3 Experiments:
1. **Tokenizer Intervention Study** (isolates Mechanism 3): Take the 100 highest-frequency polysemous Arab entities, force multi-token representation by: (a) adding dummy prefix/suffix, (b) using smaller vocabulary, (c) inserting word boundaries. Measure QA accuracy improvement vs. baseline single-token representation. Expected: 15-25% accuracy gain if tokenization is causal.
2. **Language-Adapter Ablation** (tests Mechanism 2): Train language-specific adapters for Arabic, Farsi, Urdu on same base model. Add explicit language ID token at sequence start. Evaluate overlapping entities with correct vs. incorrect language ID. Expected: Performance recovery when language ID matches entity's source language.
3. **Polysemy-Aware Training** (addresses root cause of Mechanism 1): Fine-tune on synthetic data: "المكان: [entity]" vs. "الوصف: [polysemous_word]". Compare: (a) standard fine-tuning, (b) context-type-tagged fine-tuning. Expected: Tagged training reduces polysemy-induced errors by providing explicit sense cues.

## Open Questions the Paper Calls Out
1. **Morphology-aware Tokenization:** Can morphology-aware or character-level tokenization strategies mitigate the performance degradation observed with frequency-based tokenizers on polysemous Arab entities?
2. **Western Entity Ambiguity:** To what extent does the accidental phonetic overlap between Western entity transliterations and existing Arabic words (e.g., "Ben" vs. "son of") cause recognition failures for Western entities?
3. **Cross-linguistic Generalization:** Do the identified linguistic drivers of bias (polysemy and script overlap) generalize to other non-Western languages that use shared scripts (e.g., Cyrillic or Chinese characters)?

## Limitations
- The exact causal mechanism linking polysemy to accuracy drops remains unclear, as the study shows correlation but not definitive causation
- Cross-script interference analysis doesn't isolate whether degradation stems from genuine meaning conflicts versus tokenization artifacts
- Findings are specific to Arabic-English evaluation and may not generalize to other language pairs or script systems
- The study doesn't experimentally manipulate tokenization to prove its causal role in polysemy-related performance drops

## Confidence
**High Confidence:**
- Arab entities face higher performance gaps than Western entities in Arabic contexts
- These gaps reduce substantially in English evaluation contexts
- Polysemous entities consistently underperform non-polysemous ones
- Single-token polysemous entities perform worse than multi-token equivalents

**Medium Confidence:**
- Frequency-based tokenization merges polysemous entities into single tokens, removing disambiguation cues
- Cross-script lexical overlap causes representation interference
- Larger Arabic vocabularies exacerbate polysemy problems

**Low Confidence:**
- Cultural bias in LMs is primarily driven by linguistic phenomena rather than representational or distributional factors
- The observed gaps would disappear if polysemy and tokenization issues were resolved
- The English evaluation proxy accurately captures the underlying linguistic phenomena

## Next Checks
1. **Experimental Manipulation of Tokenization:** Systematically modify tokenization of the 100 highest-frequency polysemous Arab entities by (a) forcing multi-token representations through character-level prefixes, (b) using different vocabulary sizes, and (c) applying language-specific subword models. Measure whether QA accuracy improves proportionally to token separation.
2. **Cross-Script Language Identification:** Implement explicit language markers in a multilingual model and evaluate whether performance on cross-script overlapping entities improves when correct language ID is provided versus when it's mismatched.
3. **Polysemy-Aware Training with Gloss Supervision:** Fine-tune models on synthetic data where polysemous entities are paired with explicit sense-disambiguation cues (e.g., "متروحة [location]" vs. "متروحة [proposed]"). Compare performance against standard fine-tuning.