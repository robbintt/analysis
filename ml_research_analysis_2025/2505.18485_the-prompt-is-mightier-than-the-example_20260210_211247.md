---
ver: rpa2
title: The Prompt is Mightier than the Example
arxiv_id: '2505.18485'
source_url: https://arxiv.org/abs/2505.18485
tags:
- data
- knowledge
- generation
- synthetic
- examples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces Knowledge-Guided Prompting (KGP) to improve\
  \ synthetic tabular data generation by LLMs, reducing dependence on large numbers\
  \ of in-context examples. KGP injects domain knowledge\u2014symbolic, semantic,\
  \ or statistical\u2014directly into prompts, enabling better data generation with\
  \ fewer examples."
---

# The Prompt is Mightier than the Example

## Quick Facts
- arXiv ID: 2505.18485
- Source URL: https://arxiv.org/abs/2505.18485
- Reference count: 40
- Knowledge-Guided Prompting (KGP) reduces ICL examples by 40%-90% while maintaining or improving synthetic data quality

## Executive Summary
This paper introduces Knowledge-Guided Prompting (KGP) to improve synthetic tabular data generation by LLMs, reducing dependence on large numbers of in-context examples. KGP injects domain knowledge—symbolic, semantic, or statistical—directly into prompts, enabling better data generation with fewer examples. Experiments across mathematical, graphical, and real-world datasets show that KGP reduces the number of ICL examples by 40%-90% while maintaining or improving data quality. For instance, semantic KGP cut ICL examples by 40% on AP Calculus data and by 80%-90% on complex datasets like Bohachevsky, achieving comparable or superior results to traditional ICL-only methods. KGP also improves out-of-distribution generalization and synthetic data quality metrics such as NLL, KL divergence, and MLU. Overall, KGP provides a scalable, knowledge-driven alternative to purely example-based synthetic data generation.

## Method Summary
KGP enhances LLM-based synthetic tabular data generation by integrating domain knowledge into prompts. The method extracts statistical (value ranges), semantic (natural language descriptions), or symbolic (mathematical equations) knowledge from the data or domain, then constructs prompts combining this knowledge with in-context examples. The LLM generates synthetic data per chunk, which are merged for final output. KGP reduces ICL example requirements by 40%-90% while maintaining or improving data quality metrics.

## Key Results
- KGP reduces ICL examples by 40%-90% while maintaining or improving synthetic data quality
- Semantic KGP cut ICL examples by 40% on AP Calculus data and 80%-90% on complex datasets like Bohachevsky
- KGP improves out-of-distribution generalization with 73-98% MSE reduction in unobserved regions
- Statistical and semantic KGP consistently improve results, while symbolic KGP can degrade performance on complex mathematics

## Why This Works (Mechanism)

### Mechanism 1: Knowledge Substitution for ICL Examples
- Claim: Explicit domain knowledge injected into prompts can reduce ICL example requirements by 40%-90% while maintaining or improving synthetic data quality.
- Mechanism: LLMs possess vast pre-trained prior knowledge. KGP activates this knowledge through structured prompting (statistical ranges, semantic descriptions, symbolic constraints), allowing the model to infer distributional properties that would otherwise require many examples to demonstrate.
- Core assumption: LLMs encode sufficient domain-relevant knowledge during pre-training that can be elicited through natural language descriptions rather than exemplars.
- Evidence anchors:
  - [abstract] "KGP injects domain knowledge—symbolic, semantic, or statistical—directly into prompts, enabling better data generation with fewer examples... KGP reduces the number of ICL examples by 40%-90%"
  - [Section 4.2] "Semantic KGP, Symbolic KGP require 40% fewer ICL examples to achieve the same generation quality as a variant without KGP"
  - [corpus] Related work on ICL example selection (TabGen-ICL, KITE) focuses on better example selection rather than knowledge substitution—KGP offers an orthogonal approach.
- Break condition: When domain knowledge is highly specialized or absent from pre-training corpus, KGP provides limited benefit; ICL examples may remain necessary.

### Mechanism 2: Hierarchical Knowledge Effectiveness
- Claim: Strong knowledge (symbolic constraints) and weak knowledge (semantic/statistical) have differential effectiveness depending on data complexity.
- Mechanism: The paper treats knowledge levels as concentric circles—Statistical KGP provides ranges, Semantic KGP adds conceptual descriptions, Symbolic KGP adds exact equations. Each level constrains the generation space progressively.
- Core assumption: LLMs can correctly interpret and apply natural language domain descriptions to numerical data generation tasks.
- Evidence anchors:
  - [Section 3.1] "We classify prior knowledge into strong knowledge (e.g., symbolic constraints, statistical priors) versus weaker knowledge (e.g., monotonicity constraints, dependency relationships)"
  - [Section 4.4/Table 4] "Symbolic KGP, does not always produce beneficial outcomes due to the limited grasp of complex mathematics"
  - [corpus] Corpus lacks comparative studies on knowledge type effectiveness for synthetic data—this paper appears novel in this taxonomy.
- Break condition: Symbolic knowledge can degrade performance when LLMs struggle with complex mathematical reasoning; semantic/statistical knowledge more reliably improves results.

### Mechanism 3: OOD Generalization via Global Constraints
- Claim: KGP enables generation in unobserved distribution regions by providing global knowledge that transcends local ICL example coverage.
- Mechanism: ICL examples provide local knowledge (specific to sampled regions), while KGP provides global constraints (valid across entire distribution). This allows extrapolation beyond observed data regions.
- Core assumption: The injected knowledge accurately characterizes the full target distribution, not just observed regions.
- Evidence anchors:
  - [Section 4.3/Table 3] "The mean squared error (MSE) of the sigmoid function can be significantly reduced by 98%... Bohachevsky function, the absolute value of the error decreased from 1.62 to 0.44"
  - [Section 3] "KGP encodes global knowledge while a data chunk holds local knowledge"
  - [corpus] No direct corpus precedent for OOD generation via knowledge prompting in tabular synthesis.
- Break condition: When injected knowledge is incomplete or incorrect for unobserved regions, KGP may produce confident but erroneous outputs.

## Foundational Learning

- Concept: **In-Context Learning (ICL)**
  - Why needed here: KGP is positioned as an alternative/complement to ICL. Understanding how ICL works (few-shot learning from examples in context window) is essential to appreciate what KGP substitutes.
  - Quick check question: Can you explain why ICL requires representative examples covering the full distribution?

- Concept: **Synthetic Tabular Data Generation**
  - Why needed here: This is the target task. Tabular data has joint distributions across columns that must be preserved—unlike text generation, invalid combinations (e.g., negative age) are detectable failures.
  - Quick check question: What metrics would you use to evaluate if synthetic tabular data matches real data?

- Concept: **Knowledge Encoding Types**
  - Why needed here: KGP's contribution requires understanding how to extract and format domain knowledge into prompts (statistical: ranges; semantic: natural descriptions; symbolic: equations/constraints).
  - Quick check question: Given a dataset, can you identify what statistical vs. semantic vs. symbolic knowledge you could extract?

## Architecture Onboarding

- Component map:
  - Data chunking -> Knowledge extraction (global) -> Per-chunk prompt construction -> LLM generation -> Merge outputs -> Quality evaluation

- Critical path: Data chunking → Knowledge extraction (global, once) → Per-chunk prompt construction → LLM generation → Merge outputs → Quality evaluation
  - Knowledge extraction happens once globally; prompts are constructed per-chunk with same knowledge + different ICL examples.

- Design tradeoffs:
  - More ICL examples (higher token cost) vs. richer KGP (requires domain expertise to extract)
  - Strong knowledge (symbolic) risks LLM math errors; weak knowledge (statistical/semantic) more reliable but less constraining
  - Large chunks capture more distributional context vs. small chunks reduce token costs

- Failure signatures:
  - **High Hausdorff distance with No-KGP**: Indicates ICL examples insufficient for complex distributions (Figure 2f)
  - **Semantic KGP quality degradation**: Occurs when semantic descriptions are misleading (Figure 5b: "bullseye", "slant up" descriptions hurt quality)
  - **Zero DCR**: Indicates data leakage (model copying exact ICL examples)—Table 6 shows this risk exists

- First 3 experiments:
  1. **Baseline comparison**: Generate synthetic data with No-KGP vs. Statistical KGP vs. Semantic KGP at fixed ICL count (e.g., 20, 50 examples). Measure MSE/NLL/KL. Expectation: KGP variants should match or exceed No-KGP quality.
  2. **ICL reduction study**: For a target quality threshold, find minimum ICL examples needed with and without KGP. Calculate reduction percentage. Paper reports 40-90% reduction.
  3. **OOD generalization test**: Provide ICL examples from one region of distribution, test generation in unobserved region with and without KGP. Measure MSE in unobserved region. Expectation: KGP should significantly reduce OOD error (paper shows 73-98% reduction).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can KGP be extended to a multimodal framework that incorporates visual and semantic facets alongside text?
- Basis in paper: [explicit] The conclusion states that future work "will be aimed at developing a multimodal learning framework encompassing KGP with visual and semantic facets."
- Why unresolved: The current implementation and experiments are restricted to text-based prompts and tabular data structures, leaving the interaction with visual modalities unexplored.
- What evidence would resolve it: A new KGP architecture that processes visual inputs (e.g., charts) and demonstrates improved generation fidelity over text-only KGP.

### Open Question 2
- Question: How can KGP systems detect and mitigate "model poisoning" caused by inconsistent semantic knowledge?
- Basis in paper: [explicit] The conclusion warns that "inconsistent semantic KGP... may result in a decrease in generation quality, constituting a form of model poisoning."
- Why unresolved: The paper focuses on the benefits of correct knowledge but identifies the risk of contradictory or malicious prompts as an unsolved vulnerability.
- What evidence would resolve it: Algorithms capable of flagging semantic prompts that statistically contradict the provided ICL examples, preventing quality degradation.

### Open Question 3
- Question: How can Symbolic KGP be optimized to handle complex mathematical constraints where LLMs currently fail?
- Basis in paper: [inferred] Section 4.4 finds that Symbolic KGP "does not always produce beneficial outcomes due to the limited grasp of complex mathematics," despite being the strongest knowledge type.
- Why unresolved: The paper identifies a failure mode where the strongest form of knowledge yields worse results than weaker forms (Semantic/Statistical) on complex functions.
- What evidence would resolve it: A method that allows LLMs to correctly interpret and apply complex symbolic equations, resulting in lower MSE than semantic descriptions.

## Limitations

- Knowledge Quality Dependency: KGP effectiveness fundamentally limited by quality and completeness of extracted knowledge, requiring significant domain expertise
- Generalization to Complex Domains: Experimental validation focuses on mathematical functions rather than real-world tabular data with complex, noisy relationships
- Evaluation Metric Completeness: Quality metrics in isolation may not translate to practical utility for downstream tasks like ML training

## Confidence

- **ICL Reduction (40%-90%)**: High Confidence - Directly measured across multiple datasets and knowledge types with clear quantitative results
- **OOD Generalization Improvement**: High Confidence - 73-98% MSE reduction in unobserved regions demonstrated with clear before/after comparisons
- **KGP vs. ICL Effectiveness**: Medium Confidence - Quality improvements shown but assumes optimal ICL example selection which may not reflect practical usage

## Next Checks

1. **Real-World Domain Validation**: Apply KGP to a non-mathematical tabular dataset (e.g., healthcare insurance claims, financial transactions) with known domain knowledge constraints. Measure whether KGP maintains quality advantages and quantify the knowledge engineering effort required versus ICL example collection.

2. **Downstream Task Performance**: Generate synthetic data using KGP and traditional ICL methods, then train a predictive model on each synthetic dataset and evaluate on real test data. This would validate whether quality metric improvements translate to practical utility.

3. **Knowledge Extraction Automation Study**: Systematically vary the knowledge extraction process (manual vs. automated using LLMs) on the same datasets. Measure the impact on synthetic data quality and the effort/cost tradeoff between knowledge engineering and ICL example collection.