---
ver: rpa2
title: 'Noise is All You Need: Solving Linear Inverse Problems by Noise Combination
  Sampling with Diffusion Models'
arxiv_id: '2510.23633'
source_url: https://arxiv.org/abs/2510.23633
tags:
- noise
- diffusion
- inverse
- process
- optimal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes Noise Combination Sampling (NCS), a novel\
  \ method that addresses the instability of diffusion-based inverse problem solvers\
  \ by approximating the measurement score through an optimal linear combination of\
  \ Gaussian noise vectors. Instead of directly adding the conditional gradient to\
  \ the denoising process\u2014which disrupts the generative trajectory\u2014NCS embeds\
  \ the conditional information into the noise term via a closed-form solution derived\
  \ from the Cauchy-Schwarz inequality."
---

# Noise is All You Need: Solving Linear Inverse Problems by Noise Combination Sampling with Diffusion Models

## Quick Facts
- arXiv ID: 2510.23633
- Source URL: https://arxiv.org/abs/2510.23633
- Reference count: 25
- Key outcome: NCS achieves superior stability and quality in diffusion-based inverse problem solving by embedding conditional guidance into the noise term via closed-form optimal weights

## Executive Summary
This paper introduces Noise Combination Sampling (NCS), a novel method for solving linear inverse problems using diffusion models. Instead of directly adding conditional gradients to the denoising process—which disrupts the generative trajectory—NCS embeds the conditional information into the noise term via a closed-form solution derived from the Cauchy-Schwarz inequality. This approach preserves the consistency of the diffusion process and eliminates the need for step-wise hyperparameter tuning. Experiments on multiple inverse problems (inpainting, super-resolution, deblurring) show that NCS consistently outperforms baseline methods, particularly when the number of diffusion steps is small, achieving superior robustness and stability with negligible computational overhead.

## Method Summary
NCS modifies the standard DDPM sampling process by replacing the random noise term with an optimal linear combination of Gaussian vectors from a fixed codebook. The weights for this combination are computed via a closed-form solution that maximizes the alignment with the conditional measurement score direction while preserving the standard normal distribution of the noise. This allows NCS to incorporate conditional information without disrupting the diffusion model's learned manifold structure. The method requires only a pretrained diffusion model and a fixed noise codebook, making it a zero-shot solution that doesn't require additional training.

## Key Results
- NCS consistently outperforms baseline methods on inpainting, super-resolution, and deblurring tasks
- Superior stability and quality achieved with fewer diffusion steps (e.g., T=20 vs T=1000)
- Computational overhead is negligible due to closed-form weight computation
- NCS accelerates generative image compression methods while maintaining comparable quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Embedding conditional guidance into the noise term preserves the diffusion trajectory's manifold consistency better than direct gradient injection.
- Mechanism: Standard methods add ∇xt logp(y|xt) as an external term, pushing x_t off the learned manifold M_t. NCS replaces the random noise ε_t with a constructed ε*_t that encodes the same conditional direction but remains statistically compatible with the DDPM update: x_{t-1} = μ(x_t, t) + σ_t ε*_t.
- Core assumption: The measurement score direction can be sufficiently approximated within the span of K noise vectors from a fixed codebook.
- Evidence anchors:
  - [abstract] "NCS embeds the conditional information into the noise term via a closed-form solution... preserving the consistency of the diffusion process."
  - [Page 4, Figure 1] Visual comparison showing NCS trajectory staying on manifold while exact approximation methods push off manifold.
  - [corpus] Limited corpus evidence on manifold preservation specifically; related work (MPGD, DAPS) addresses similar stability concerns but via different mechanisms.
- Break condition: If the measurement score has significant components orthogonal to the noise subspace span, approximation quality degrades.

### Mechanism 2
- Claim: The optimal noise combination weights admit a closed-form solution via the Cauchy-Schwarz inequality, eliminating hyperparameter search.
- Mechanism: The optimization max_{||γ||₂=1} ⟨c, E_t γ⟩ is maximized when γ aligns with the projection of c onto the codebook: γ* = c^T E_t / ||c^T E_t||₂. This requires only O(Kd) inner product computations.
- Core assumption: The conditional direction c (approximated measurement score) is available from the base solver (e.g., DPS gradient approximation).
- Evidence anchors:
  - [Page 6, Theorem 2] "The optimal weight vector γ* ∈ R^K that maximizes the inner product ⟨c, E_t γ⟩ subject to ||γ||₂ = 1 is given by: γ* = c^T E_t / ||c^T E_t||₂"
  - [Page 13, Appendix B] Full derivation using Cauchy-Schwarz inequality.
  - [corpus] No direct corpus comparison; most related methods (FlowDPS, DAPS) use iterative or sampling-based guidance without closed-form solutions.
- Break condition: If E_t^T c ≈ 0 (measurement score nearly orthogonal to all codebook vectors), weights become unstable.

### Mechanism 3
- Claim: Under unit-norm weight constraint, the linear combination of independent Gaussian vectors remains standard normal.
- Mechanism: If ε_i ~ N(0, I) independently and ||γ||₂ = 1, then Σ γ_i ε_i ~ N(0, I) by linearity and covariance preservation: Cov(ε*) = Σ γ_i² I = I.
- Core assumption: Weights γ are computed independently of the noise vectors (no adaptive codebook selection based on noise values).
- Evidence anchors:
  - [Page 13, Lemma 1] Formal proof of Gaussianity preservation.
  - [Page 5, Theorem 1] "The synthesized noise vector ε*_t remains standard normal, i.e., ε*_t ~ N(0,I), as a consequence of the unit-norm constraint."
  - [corpus] Assumption: Corpus papers do not explicitly analyze this property; it appears specific to NCS.
- Break condition: If γ depends on ε_i values (e.g., learned selection), the independence assumption fails and the distribution may deviate.

## Foundational Learning

- Concept: **Diffusion model reverse SDE and DDPM discretization**
  - Why needed here: NCS operates within the DDPM update rule; understanding x_{t-1} = μ(x_t, t) + σ_t ε is essential to see where ε*_t substitutes.
  - Quick check question: Can you write the DDPM reverse step update and identify which term NCS modifies?

- Concept: **Bayesian formulation of linear inverse problems**
  - Why needed here: The measurement score ∇xt logp(y|xt) arises from p(x_t|y) ∝ p(y|x_t)p(x_t); approximating this term is the core challenge NCS addresses.
  - Quick check question: Given y = Ax_0 + n, why is ∇xt logp(y|x_t) generally intractable without additional training?

- Concept: **Tweedie's formula for denoising estimation**
  - Why needed here: NCS-DPS and NCS-MPGD rely on ˜x_{0|t} estimated via Tweedie to compute the conditional direction c.
  - Quick check question: How does Tweedie's formula relate the score function to the clean image estimate ˜x_{0|t}?

## Architecture Onboarding

- Component map:
  - Pretrained diffusion model -> Tweedie estimator -> Base solver (DPS/MPGD) -> Noise codebook E_t -> Weight computation γ* -> Noise synthesis ε*_t -> DDPM update

- Critical path:
  1. At each timestep t, compute ˜x_{0|t} via Tweedie using s_θ(x_t, t).
  2. Compute conditional direction c from base solver (e.g., A^T(y - A˜x_{0|t}) for NCS-MPGD).
  3. Project c onto codebook: v = c^T E_t, normalize to get γ*.
  4. Synthesize ε*_t = E_t γ*, apply DDPM