---
ver: rpa2
title: Protecting Bystander Privacy via Selective Hearing in Audio LLMs
arxiv_id: '2512.06380'
source_url: https://arxiv.org/abs/2512.06380
tags:
- bystander
- audio
- speaker
- main
- privacy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces SH-Bench, the first benchmark to evaluate\
  \ selective hearing in audio LLMs\u2014their ability to attend to an intended main\
  \ speaker while refusing to process or reveal information about incidental bystander\
  \ speech. The benchmark includes 3,968 multi-speaker audio mixtures and 77k multiple-choice\
  \ questions covering real and synthetic scenarios."
---

# Protecting Bystander Privacy via Selective Hearing in Audio LLMs

## Quick Facts
- arXiv ID: 2512.06380
- Source URL: https://arxiv.org/abs/2512.06380
- Authors: Xiao Zhan; Guangzhi Sun; Jose Such; Phil Woodland
- Reference count: 18
- Key outcome: State-of-the-art audio LLMs leak bystander privacy; BPFT fine-tuning achieves 47% higher selective-mode bystander accuracy and 16% higher SE without degrading main-speaker comprehension.

## Executive Summary
This paper introduces SH-Bench, the first benchmark for evaluating selective hearing in audio LLMs—their ability to comprehend intended main speaker content while refusing to process or reveal bystander speech. The benchmark includes 3,968 multi-speaker audio mixtures and 77k multiple-choice questions covering real and synthetic scenarios. A novel metric, Selective Efficacy (SE), measures both comprehension and privacy protection. The authors demonstrate significant bystander privacy leakage in state-of-the-art models and propose Bystander Privacy Fine-Tuning (BPFT), a training pipeline that teaches models to refuse bystander-related queries. BPFT achieves substantial improvements in privacy protection while maintaining main-speaker comprehension.

## Method Summary
The approach involves creating multi-speaker audio mixtures from the AMI corpus, generating paired training data with general (answer) and selective (refuse bystander) instructions, and fine-tuning audio LLMs using LoRA on the LLM backbone only. The SH-Bench benchmark evaluates models using 4 accuracies (main/bystander × general/selective modes) and a harmonic mean metric (SE). BPFT trains models to selectively refuse bystander content when given privacy instructions while maintaining comprehension of main speaker content.

## Key Results
- State-of-the-art models show significant bystander privacy leakage under selective mode
- BPFT achieves 47% higher bystander accuracy under selective mode compared to baseline
- BPFT provides 16% higher SE while maintaining main-speaker comprehension
- Real-world evaluation shows BPFT outperforms baseline by 23% SE on real scenarios
- Slight degradation in main speaker accuracy observed post-BPFT (96.0%→93.3%)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A content-conditioned main speaker description enables models to distinguish primary speakers from bystanders.
- **Mechanism:** The natural language description acts as a semantic anchor, correlating with acoustic features and spoken content to identify the target speaker and filter out non-matching speech.
- **Core assumption:** Models can reliably ground textual descriptions in audio for fine-grained speaker discrimination.
- **Evidence anchors:** [abstract] "A model's ability to attend to an intended main speaker while refusing to process or reveal information about incidental bystander speech." [section 6.2.1] "Main speaker description is crucial for identifying the bystanders... provides the essential clue to find the bystander."
- **Break condition:** Fails with ambiguous descriptions, highly similar speakers, or models lacking audio-text grounding.

### Mechanism 2
- **Claim:** An explicit "I don't know" refusal option provides a measurable pathway for privacy-compliant behavior.
- **Mechanism:** The option gives models a semantically appropriate output when they should not have processed bystander speech, avoiding forced hallucination and creating a clean accuracy metric.
- **Core assumption:** A properly constrained model will select "I don't know" rather than guess from unintended information.
- **Evidence anchors:** [section 3.2] "This option is essential since if the model only has access to the main speaker's voice, it should select this option when asked about bystander content." [section 6.2.2] "BPFT achieves higher entropy without 'I don't know' option."
- **Break condition:** Fails if models are over-confident or learn to game the metric.

### Mechanism 3
- **Claim:** Paired instruction fine-tuning creates conditional processing behavior via supervised learning.
- **Mechanism:** BPFT generates training pairs for identical audio questions: (general instruction → answer) and (selective instruction → refuse for bystander content), conditioning the model to inhibit comprehensive speech processing when given privacy instructions.
- **Core assumption:** Models can learn conditional mappings from high-level privacy instructions to low-level filtering without catastrophic forgetting.
- **Evidence anchors:** [abstract] "BPFT... teaches models to refuse bystander-related queries without degrading main-speaker comprehension." [section 4] "Each question in the training set has a pair of instructions."
- **Break condition:** Fails with insufficient training data, overfitting to synthetic scenarios, or catastrophic forgetting.

## Foundational Learning

- **Concept: Speaker Diarization & Source Separation**
  - **Why needed here:** The paper's LLM-centric approach differs from traditional separation pipelines. The paper uses a separation+ASR+LLM pipeline as a baseline.
  - **Quick check question:** How does BPFT's approach differ from the SepFormer→Whisper→GPT-4o pipeline baseline?

- **Concept: Instruction Fine-Tuning vs. In-Context Learning**
  - **Why needed here:** Existing models failed via prompting alone; BPFT requires weight updates via LoRA.
  - **Quick check question:** Why was fine-tuning necessary rather than a more detailed system prompt?

- **Concept: Harmonic Mean for Multi-Objective Metrics**
  - **Why needed here:** Selective Efficacy (SE) uses harmonic mean to penalize imbalanced performance between comprehension and privacy.
  - **Quick check question:** Why use harmonic mean instead of arithmetic average for SE?

## Architecture Onboarding

- **Component map:** Audio LLM Encoder → LLM Backbone (LoRA) → Speaker Description Input → Query Input → BPFT Training Pipeline
- **Critical path:** 1) Generate multi-speaker mixtures with role labels 2) Create paired training data (answer/refuse examples) 3) Apply LoRA fine-tuning to LLM backbone 4) Inference with speaker description + mode-specific prompt
- **Design tradeoffs:** Synthetic training (scalable) vs. real-world generalization (challenging); LoRA efficiency vs. potential capacity limits; explicit "I don't know" option (clear metric) vs. open-ended refusal (flexible deployment)
- **Failure signatures:** Over-refusal (main accuracy drops), privacy leakage (correct bystander answers), comprehension degradation, description dependency
- **First 3 experiments:** 1) BPFT Validation: Apply BPFT, measure SE and refusal rate changes 2) Description Ablation: Test with/without main speaker descriptions 3) Sim-to-Real Gap: Train on synthetic-only, evaluate on real-world audio partition

## Open Questions the Paper Calls Out
- **Open Question 1:** How can selective hearing capabilities be extended to multi-speaker group discussion scenarios where multiple intended speakers interact with an AI system simultaneously? [explicit] The authors state in Limitations: "We only focus on single main speaker scenarios... group discussions (e.g. with AI in an open space) that can be explored in the future."
- **Open Question 2:** Can bystander privacy protection methods be effectively transferred to audio-visual multimodal systems where visual cues may complement or contradict acoustic information? [explicit] The authors note: "The bystander privacy could also be extended to audio-visual scenarios."
- **Open Question 3:** What mechanisms can eliminate the slight degradation in main speaker comprehension that BPFT causes in some models while preserving its privacy benefits? [inferred] Table 2 shows Qwen-2.5-Omni 7B + BPFT drops from 96.0% to 93.3% on general mode main speaker accuracy.
- **Open Question 4:** Why does BPFT show substantially higher selective mode bystander accuracy on synthetic scenarios (99.6%) compared to real scenarios (90.0%)? [inferred] Table 6 reveals this 9.6 percentage point gap between synthetic and real performance.

## Limitations
- Synthetic data dependence may create sim-to-real gaps that aren't fully characterized
- Strong reliance on detailed main speaker descriptions that may not be available in real-world deployments
- BPFT demonstrated only on relatively small models (7B parameters), scalability to larger models unknown

## Confidence

**High Confidence Claims:**
- SH-Bench successfully exposes bystander privacy leakage in state-of-the-art models
- BPFT achieves measurable improvements in selective hearing (47% higher bystander accuracy under selective mode)
- The selective efficacy metric appropriately captures the dual objectives of comprehension and privacy

**Medium Confidence Claims:**
- The paired instruction fine-tuning mechanism is the primary driver of BPFT's success
- Synthetic training data adequately prepares models for real-world scenarios
- Speaker descriptions can be reliably generated and used in deployment

**Low Confidence Claims:**
- BPFT's effectiveness on larger or commercial-scale models
- Performance without speaker descriptions in ambiguous scenarios
- Long-term stability of learned selective hearing behavior

## Next Checks
1. **Real-World Generalization Test:** Train BPFT exclusively on synthetic data, then evaluate on a held-out set of purely real-world multi-speaker recordings. Measure performance degradation and identify specific failure modes.

2. **Description Ablation Study:** Systematically vary the quality and completeness of main speaker descriptions during evaluation. Test scenarios with detailed descriptions, minimal descriptions, no descriptions, and incorrect descriptions to quantify dependency.

3. **Over-Refusal Quantification:** Implement detailed error analysis to distinguish correct refusals, over-refusals, and missed privacy violations. Calculate precision, recall, and F1 scores for the refusal behavior to better understand the privacy-comprehension tradeoff.