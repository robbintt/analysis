---
ver: rpa2
title: Diagnosing and Mitigating System Bias in Self-Rewarding RL
arxiv_id: '2510.08977'
source_url: https://arxiv.org/abs/2510.08977
tags:
- reward
- arxiv
- noise
- bias
- rler
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Reinforcement learning with intrinsic rewards (RLIR) enables large\
  \ language models to self-reward on unlabeled data, but its performance and stability\
  \ lag behind RLVR due to system bias\u2014where the model overestimates its high-confidence\
  \ rollouts, causing biased and unstable reward estimation. We characterize this\
  \ bias using three metrics: noise rate (\u03C1noise), self-feedback bias rate (\u03C1\
  selfbias), and symmetry bias rate (\u03C1symbias)."
---

# Diagnosing and Mitigating System Bias in Self-Rewarding RL

## Quick Facts
- **arXiv ID:** 2510.08977
- **Source URL:** https://arxiv.org/abs/2510.08977
- **Reference count:** 18
- **Primary result:** RLER improves by +13.6% over RLIR baseline and is only 3.6% below RLVR, achieving stable scaling on unlabeled samples.

## Executive Summary
This paper identifies and characterizes "system bias" in self-rewarding reinforcement learning (RLIR), where models overestimate rewards for high-confidence rollouts, leading to biased and unstable training. The authors propose three metrics—noise rate, self-feedback bias rate, and symmetry bias rate—to quantify different aspects of this bias. They then introduce RLER, an ensemble-based method that constructs a unified reward space with adaptive interpolation to mitigate these biases. Extensive experiments demonstrate that RLER significantly outperforms existing RLIR approaches while approaching the performance of RLVR, achieving stable scaling with unlabeled data.

## Method Summary
The authors first characterize system bias in self-rewarding RL by defining three metrics: noise rate (ρ_noise) measuring reward variance, self-feedback bias rate (ρ_selfbias) capturing how the model reinforces its own predictions, and symmetry bias rate (ρ_symbias) measuring reward asymmetry. They observe that ρ_noise and ρ_symbias affect convergence while ρ_selfbias causes instability by amplifying both correct and incorrect updates. To address these issues, they propose RLER (Reward Learning with Ensemble Rewards), which uses an ensemble of diverse models to generate rewards, constructs a unified reward space through adaptive interpolation, and employs selective rollout strategies. This approach effectively optimizes all three bias metrics simultaneously.

## Key Results
- RLER improves by +13.6% over the best RLIR baseline
- RLER performance is only 3.6% below RLVR
- RLER achieves stable scaling on unlabeled samples while effectively optimizing all three bias metrics

## Why This Works (Mechanism)
The ensemble-based approach works by diversifying reward generation across multiple models, reducing the impact of individual model biases. Adaptive reward interpolation allows the system to dynamically balance between different reward signals based on their reliability. Selective rollout strategies prevent over-reliance on high-confidence but potentially biased predictions. This multi-pronged approach addresses the fundamental issue that single models tend to overfit to their own reward estimations.

## Foundational Learning
- **System bias in RLIR:** Why needed - Understanding why self-rewarding models fail to match RLVR performance; Quick check - Compare ρ_noise, ρ_selfbias, and ρ_symbias across different RLIR implementations
- **Ensemble-based reward learning:** Why needed - Mitigating individual model biases through diversity; Quick check - Measure correlation between ensemble member predictions
- **Adaptive reward interpolation:** Why needed - Dynamically weighting reliable vs. unreliable reward signals; Quick check - Track interpolation weights over training
- **Rollout selection strategies:** Why needed - Preventing amplification of biased predictions; Quick check - Compare selected vs. rejected rollout distributions

## Architecture Onboarding

**Component map:** Input data → Ensemble models → Reward space construction → Adaptive interpolation → Rollout selection → Policy update

**Critical path:** The reward space construction and adaptive interpolation components are most critical, as they directly determine how individual model biases are mitigated. The rollout selection mechanism acts as a quality gate, preventing propagation of unreliable rewards.

**Design tradeoffs:** The main tradeoff is between computational overhead (maintaining multiple ensemble models) and bias mitigation effectiveness. The authors chose a moderate ensemble size to balance these concerns.

**Failure signatures:** High ρ_selfbias indicates instability from self-reinforcing incorrect predictions; high ρ_symbias suggests asymmetric reward estimation; persistent high ρ_noise indicates fundamental reward uncertainty.

**Exactly 3 first experiments:**
1. Measure all three bias metrics (ρ_noise, ρ_selfbias, ρ_symbias) on a baseline RLIR implementation to establish baselines
2. Compare reward distributions from individual ensemble members vs. interpolated rewards to verify diversity and interpolation effectiveness
3. Test rollout selection filtering rates to ensure the mechanism is actively removing problematic samples

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- The generalizability of the three bias metrics across diverse task types and reward formulations is not established
- Computational overhead of maintaining ensemble models and associated memory costs are not thoroughly analyzed
- Long-term stability in dynamic or adversarial environments is untested

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Bias characterization using three metrics is valid | Medium |
| RLER improves RLIR performance by +13.6% | Medium |
| RLER approaches RLVR performance within 3.6% | Medium |
| Generalizability across domains | Low |
| Scalability claims | Medium |

## Next Checks

1. **Cross-domain validation:** Apply RLER to non-language domains (e.g., vision-language tasks or sequential decision-making problems) to assess generalizability of bias metrics and mitigation strategies.

2. **Scalability analysis:** Measure wall-clock time, memory usage, and GPU utilization for RLER vs. RLVR/RLIR across varying model sizes (e.g., 1B, 13B, 70B parameters) to quantify practical deployment costs.

3. **Long-term stability test:** Conduct multi-task or continual learning experiments to evaluate whether RLER maintains performance and stability over extended training horizons or in non-stationary reward environments.