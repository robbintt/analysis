---
ver: rpa2
title: What Makes for a Good Saliency Map? Comparing Strategies for Evaluating Saliency
  Maps in Explainable AI (XAI)
arxiv_id: '2504.17023'
source_url: https://arxiv.org/abs/2504.17023
tags:
- saliency
- metrics
- maps
- user
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compared three popular saliency map methods (Grad-CAM,
  Guided Backpropagation, and LIME) for evaluating AI image classifiers across subjective
  measures (trust and satisfaction), objective user abilities (accuracy prediction
  and error detection), and mathematical metrics (fidelity, robustness, complexity,
  and localization). Results showed no differences in trust or satisfaction across
  methods.
---

# What Makes for a Good Saliency Map? Comparing Strategies for Evaluating Saliency Maps in Explainable AI (XAI)

## Quick Facts
- arXiv ID: 2504.17023
- Source URL: https://arxiv.org/abs/2504.17023
- Reference count: 40
- Three popular saliency methods (Grad-CAM, Guided Backpropagation, LIME) were compared for evaluating AI image classifiers

## Executive Summary
This study empirically compared three popular saliency map methods for explaining AI image classifiers across subjective user measures (trust and satisfaction), objective user abilities (accuracy prediction and error detection), and mathematical metrics (fidelity, robustness, complexity, and localization). The results showed that while no differences existed in subjective trust or satisfaction across methods, Grad-CAM significantly outperformed the others in improving user accuracy predictions and classification error detection. However, Guided Backpropagation achieved the most favorable mathematical metric scores. Notably, mathematical metrics were largely uncorrelated with user abilities except for deletion and energy pointing game metrics. These findings demonstrate that mathematical metrics and user-centered evaluations often diverge, highlighting the importance of using both approaches complementarily when assessing explainable AI methods.

## Method Summary
The study employed a between-subjects experimental design with 166 participants randomly assigned to one of three conditions (Grad-CAM, Guided Backpropagation, or LIME). Participants completed two tasks: estimating model confidence for correctly classified images and detecting errors in misclassified images. The saliency maps were generated using a ResNet50 model on ImageNet validation images. Mathematical metrics including fidelity (Insertion, Deletion, Noise), robustness (Causal Metrics, MeanDrop, C^2), complexity (Sparseness, Continuity, Sensitivity), and localization (Pointing Game, Energy Pointing Game) were computed for each method. Subjective measures of trust and satisfaction were collected through questionnaires after task completion.

## Key Results
- No differences in subjective trust or satisfaction across the three saliency map methods
- Grad-CAM significantly outperformed other methods in improving user accuracy predictions and classification error detection
- Guided Backpropagation achieved the most favorable mathematical metric scores
- Most mathematical metrics were uncorrelated with user abilities, except for deletion and energy pointing game metrics

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Optimizing for mathematical fidelity or robustness does not causally increase human task performance; the two are often inversely related or uncorrelated.
- **Mechanism:** Algorithms like Guided Backpropagation optimize for high-frequency detail and low complexity, achieving high mathematical scores. However, humans rely on coarse, semantic object localization for classification tasks. Grad-CAM provides this via coarse heatmaps on convolutional layers, aligning with human cognition despite "worse" math scores.
- **Core assumption:** Human understanding of image classification relies on identifying semantic objects rather than high-frequency pixels that maximize activation.
- **Evidence anchors:** Abstract finding that Grad-CAM outperformed others in user tasks while GBP had better math scores; section 5.1.2 noting complexity metrics are not aligned with human understanding.
- **Break condition:** This mechanism likely breaks when the task requires detecting fine-grained texture defects or adversarial noise rather than object classification.

### Mechanism 2
- **Claim:** Visual integration of the explanation with the original image reduces cognitive load and improves task performance.
- **Mechanism:** Direct overlays reduce the spatial alignment cost required to map the explanation to the content, allowing users to focus cognitive resources on the judgment task itself rather than mental image registration.
- **Core assumption:** The visual presentation format is a significant causal factor in user ability scores, potentially confounding the comparison of underlying algorithms.
- **Evidence anchors:** Section 5.3 noting Grad-CAM displayed the original image in the background, which could partly explain its highest user abilities.
- **Break condition:** If the overlay is too opaque and obscures critical diagnostic features, performance would likely degrade.

### Mechanism 3
- **Claim:** Subjective user trust is insensitive to objective differences in explanation quality.
- **Mechanism:** Users suffer from the "illusion of explanatory depth" and rate satisfaction based on surface-level usability or aesthetics rather than actual utility. Since all three methods provided some form of explanation, subjective trust remained uniformly high regardless of objective error detection rate.
- **Core assumption:** Users cannot accurately calibrate their internal "trust" metric without explicit feedback on their performance.
- **Evidence anchors:** Abstract finding of no differences in trust or satisfaction across methods; section 5.2 noting subjective measures can be influenced by cognitive biases.
- **Break condition:** If users were domain experts performing high-stakes tasks with immediate outcome feedback, trust might correlate more tightly with actual utility.

## Foundational Learning

- **Concept:** **Explanation Fidelity vs. Plausibility**
  - **Why needed here:** The paper demonstrates that high fidelity (math) does not equal high plausibility (human understanding). You must distinguish between an explanation that is technically accurate to the model and one that makes sense to a human.
  - **Quick check question:** "If a saliency map highlights random noise that mathematically influences the model, is it high fidelity or high plausibility?" (Answer: High fidelity, low plausibility).

- **Concept:** **Deletion and Insertion Metrics**
  - **Why needed here:** These were the only mathematical metrics showing some correlation with user ability. Understanding them is critical for building automated evaluation pipelines that approximate human utility.
  - **Quick check question:** "Does the 'Deletion' metric measure how fast confidence drops when removing important pixels, or how fast it rises when adding them?" (Answer: Drops when removing).

- **Concept:** **Between-Subjects Experimental Design**
  - **Why needed here:** The study relies on comparing independent groups to avoid carryover effects where learning one explanation style influences the perception of another.
  - **Quick check question:** "Why did the authors use a between-subjects design instead of showing all three maps to the same user?" (Answer: To prevent learning effects and comparative bias).

## Architecture Onboarding

- **Component map:** Image Class + Model (ResNet50) -> Explainer Module (Grad-CAM | GBP | LIME) -> Visualizer (Overlay | Heatmap | Segments) -> Dual-stream assessment (Math Metrics: Deletion/EPG) + (User Task: Error Detection)

- **Critical path:** The selection of the Generator. If the system goal is debugging (finding model errors), choose Grad-CAM. If the goal is technical benchmarking, choose GBP. Do not assume technical benchmarks predict user success.

- **Design tradeoffs:**
  - Grad-CAM: Best for user task performance (error detection), lower resolution (coarse heatmaps)
  - GBP: Best for mathematical fidelity metrics, "noisier" visualization, lower user utility for classification tasks
  - LIME: Middle ground, but computationally expensive (perturbation-based) and no significant statistical advantage in this study

- **Failure signatures:**
  - High Metric / Low Utility: You achieve 0.9 on Insertion/Mutual Information, but users fail to identify misclassified images
  - Uniform Trust: Users report high satisfaction (4/5) but have a 40% error rate in prediction tasks (indicating a false sense of security)

- **First 3 experiments:**
  1. Baseline Correlation: Calculate Deletion and Energy Pointing Game (EPG) scores for your specific model/dataset and correlate them with a small-scale (n=20) human error-detection task
  2. Visual Ablation: Test Grad-CAM with and without the background image overlay to isolate the "cognitive load" mechanism
  3. Trust Calibration: Run a study where users must bet points on their predictions to see if "trust" scores shift when real stakes are introduced

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does standardizing the visual presentation of saliency maps by uniformly overlaying them on original images eliminate the performance differences in user ability tasks between Grad-CAM, LIME, and Guided Backpropagation?
- Basis in paper: Section 5.4 (Limitations) notes that Grad-CAM displayed the original image in the background while GBP and LIME did not, potentially reducing cognitive load for Grad-CAM users and confounding the results.
- Why unresolved: The study did not control for the visual integration of the saliency map and the original image, leaving a potential confounding variable untested.
- What evidence would resolve it: A follow-up experiment where all three methods are rendered with the background image present (or absent) to isolate the effect of the explanation method from the presentation style.

### Open Question 2
- Question: Do the findings that Grad-CAM improves user abilities better than LIME or GBP generalize to non-ICT populations, such as laypeople or domain experts in high-stakes fields (e.g., medicine)?
- Basis in paper: Section 5.4 (Limitations) and Section 5.3 explicitly state the sample was limited to English-speaking ICT graduates and call for future studies with other stakeholder groups.
- Why unresolved: ICT graduates possess higher technical affinity and prior understanding of AI, which may not represent the cognitive processes or capabilities of general users or domain experts.
- What evidence would resolve it: Replicating the study protocol with distinct cohorts of laypeople and domain-specific professionals (e.g., doctors) to verify if Grad-CAM retains its superiority in user ability tasks.

### Open Question 3
- Question: Can mathematical metrics be systematically designed or weighted to function as reliable proxies for objective user understanding, given the observed divergence between standard metrics and human performance?
- Basis in paper: The Conclusion states "Future research could aim to explore how to match mathematical and user metrics to best meet context-dependent desiderata."
- Why unresolved: The study found that most standard mathematical metrics were either uncorrelated or counterintuitively associated with user abilities.
- What evidence would resolve it: A study that correlates a broader or newly designed set of mathematical metrics with human task performance to identify or construct a metric that statistically predicts user success.

### Open Question 4
- Question: Are the "Deletion" and "Energy Pointing Game" metrics robust proxies for user understanding across different model architectures and datasets, or are their correlations specific to the ResNet50/ImageNet setup used here?
- Basis in paper: Section 4.3 and 5.1.2 note that Deletion and EPG were the only metrics associated with user abilities, but Section 5.4 highlights that only one model/dataset was tested.
- Why unresolved: It is unclear if these specific correlations are fundamental properties of the metrics or artifacts of the specific convolutional neural network architecture used in the experiment.
- What evidence would resolve it: Testing the correlation of Deletion and EPG scores with user ability tasks using different architectures (e.g., Vision Transformers) and datasets.

## Limitations
- The study's between-subjects design cannot control for individual differences in visual perception or prior ML experience that may affect subjective ratings
- Only tested one base model (ResNet50) and one dataset (ImageNet), limiting generalizability to other architectures or domains
- The paper acknowledges that presenting Grad-CAM with the original image overlay may have artificially inflated its user ability scores, but did not conduct an ablation study to quantify this effect

## Confidence

- **High Confidence**: The finding that mathematical metrics and user-centered evaluations diverge significantly
- **Medium Confidence**: The specific ranking of methods for user tasks (Grad-CAM > GBP > LIME for error detection)
- **Medium Confidence**: The claim that subjective trust does not correlate with objective utility

## Next Checks
1. **Model and Dataset Generalization**: Replicate the user study with a different architecture (e.g., Vision Transformer) and a non-standard dataset (e.g., medical X-rays or satellite imagery) to test the robustness of the Grad-CAM user advantage
2. **Visualization Format Ablation**: Conduct a within-subjects study where the same saliency method (e.g., Grad-CAM) is presented in multiple formats (overlay, side-by-side, heatmap-only) to isolate the impact of visual integration on user performance
3. **Expert vs. Novice Calibration**: Compare subjective trust and objective error detection rates between ML expert participants and non-expert participants to determine if the "illusion of explanatory depth" is more pronounced in one group