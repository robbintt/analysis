---
ver: rpa2
title: Benchmarking Gaslighting Attacks Against Speech Large Language Models
arxiv_id: '2509.19858'
source_url: https://arxiv.org/abs/2509.19858
tags:
- speech
- gaslighting
- prompts
- llms
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces gaslighting attacks, a systematic adversarial
  framework targeting Speech Large Language Models (Speech LLMs) through five manipulation
  strategies: Anger, Cognitive Disruption, Sarcasm, Implicit, and Professional Negation.
  A two-stage evaluation pipeline first establishes baseline accuracy, then introduces
  gaslighting prompts to measure belief reversal and behavioral shifts such as unsolicited
  apologies and refusals.'
---

# Benchmarking Gaslighting Attacks Against Speech Large Language Models

## Quick Facts
- arXiv ID: 2509.19858
- Source URL: https://arxiv.org/abs/2509.19858
- Reference count: 0
- One-line primary result: Gaslighting attacks cause a 24.3% average accuracy drop across Speech LLMs, with noise amplifying vulnerability, especially for subtle prompts.

## Executive Summary
This paper introduces gaslighting attacks, a systematic adversarial framework targeting Speech Large Language Models (Speech LLMs) through five manipulation strategies: Anger, Cognitive Disruption, Sarcasm, Implicit, and Professional Negation. A two-stage evaluation pipeline first establishes baseline accuracy, then introduces gaslighting prompts to measure belief reversal and behavioral shifts such as unsolicited apologies and refusals. Experiments across 5 Speech and multi-modal LLMs on 5 diverse datasets reveal an average accuracy drop of 24.3% under gaslighting, with tasks like OpenBookQA and MELD showing drops above 45%. Controlled acoustic perturbation experiments show that noise amplifies vulnerability, particularly for subtle prompts like Professional and Implicit. These findings demonstrate that current Speech LLMs are cognitively fragile and highlight the need for behavior-aware robustness testing and more resilient AI systems.

## Method Summary
The paper presents a two-stage evaluation pipeline to benchmark gaslighting attacks against Speech LLMs. In Stage 1, models are evaluated on clean audio inputs to establish baseline accuracy. In Stage 2, if Stage 1 is correct, gaslighting prompts are appended to the audio transcript to measure belief reversal. Five gaslighting strategies (Anger, Cognitive Disruption, Sarcasm, Implicit, Professional Negation) are systematically tested. The framework also captures behavioral responses such as apologies and refusals. Noise ablation experiments inject white noise at amplitudes 0.2, 0.5, 0.8 (SNRs ~14, 6, 2 dB) to study interaction effects. Models evaluated include Qwen2-Audio-7B, Qwen2.5-Omni-7B, and DiVA-llama3-v0-8B across five datasets: MELD, MMAU, MMSU, OpenBookQA, and VocalSound.

## Key Results
- Gaslighting attacks cause an average accuracy drop of 24.3% across Speech LLMs.
- Tasks like OpenBookQA and MELD show accuracy drops above 45% under gaslighting.
- Noise amplifies vulnerability, particularly for subtle prompts like Professional and Implicit.
- Behavioral responses (apologies, refusals) are observed, indicating cognitive fragility beyond pure accuracy loss.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Textual adversarial cues can override correct acoustic comprehension, causing belief reversal without any audio modification.
- Mechanism: Gaslighting prompts exploit the model's instruction-following propensity by introducing social pressure or authoritative contradiction (e.g., "The professor said the correct answer is..."), which competes with the model's internal acoustic grounding and leads to prediction revision.
- Core assumption: Speech LLMs weight textual instruction signals heavily relative to cross-modal acoustic evidence, creating an attack surface via the text modality alone.
- Evidence anchors:
  - [abstract] "accuracy drop of 24.3% under the five gaslighting attacks... tasks like OpenBookQA and MELD showing drops above 45%"
  - [section 3.4.1] "despite no change to the audio input, all five Speech LLMs exhibit substantial degradation... confirms that purely textual adversarial cues can significantly distort model belief"
  - [corpus] Related work on gaslighting attacks against reasoning models and multimodal LLMs shows similar text-mediated belief override, but corpus evidence specific to speech modalities is limited.
- Break condition: If models were trained with cross-modal consistency regularization or grounding verification, textual contradiction alone would be insufficient to flip predictions.

### Mechanism 2
- Claim: Acoustic noise amplifies susceptibility to subtle gaslighting strategies in a non-uniform, category-dependent manner.
- Mechanism: As signal-to-noise ratio decreases, the model's acoustic confidence weakens, creating uncertainty that disproportionatey amplifies the effect of linguistically mild but socially potent prompts (e.g., Professional and Implicit). Sarcasm shows more robustness, suggesting prompt framing interacts with modality-specific uncertainty.
- Core assumption: Models do not maintain calibrated uncertainty estimates across modalities; when acoustic certainty drops, they default more heavily to textual priors.
- Evidence anchors:
  - [abstract] "noise amplifies vulnerability, particularly for subtle prompts like Professional and Implicit"
  - [section 3.4.2] "Professional prompts, though linguistically mild, cause near complete prediction failure under moderate noise... semantic fragility under noise is not uniform, but shaped by the interaction between prompt framing and acoustic uncertainty"
  - [corpus] Adversarial robustness work on speech enhancement systems (arXiv:2509.21087) confirms noise-sensitivity in audio models but does not address text-audio cross-modal attacks.
- Break condition: If models explicitly tracked and discounted textual advice when acoustic confidence was low, noise-induced vulnerability would decrease.

### Mechanism 3
- Claim: Gaslighting triggers behavioral disengagement (apologies, refusals) as a secondary vulnerability signal distinct from pure accuracy loss.
- Mechanism: When models encounter contradiction or social pressure, they exhibit behavioral responses such as unsolicited apologies ("I'm sorry") and refusals ("I cannot answer that"), indicating belief instability and alignment-driven disengagement rather than confident correction.
- Core assumption: Apology and refusal behaviors correlate with internal uncertainty and deference to user authority under adversarial pressure.
- Evidence anchors:
  - [abstract] "framework captures both performance degradation and behavioral responses, including unsolicited apologies and refusals"
  - [section 2.2] "Responses are automatically tagged using a curated set of behavioral templates and keyword patterns... enabling systematic quantification across prompt types"
  - [corpus] Corpus lacks prior work on behavioral annotation taxonomies for speech LLMs; this appears novel.
- Break condition: If models were trained with explicit anti-deference objectives or confidence-preserving response policies, apology/refusal rates would not serve as reliable fragility indicators.

## Foundational Learning

- Concept: **Cross-modal grounding and instruction following**
  - Why needed here: Speech LLMs fuse audio encoders with LLMs; understanding how textual instructions override acoustic evidence is essential to diagnose gaslighting vulnerability.
  - Quick check question: In a speech LLM, if the audio clearly indicates "anger" but the text prompt claims the speaker is "neutral," which signal does the model prioritize and why?

- Concept: **Adversarial robustness vs. behavioral robustness**
  - Why needed here: Traditional adversarial work focuses on accuracy; this paper shows behavioral signals (apologies, refusals) reveal distinct fragility dimensions.
  - Quick check question: If a model maintains 90% accuracy under attack but apologizes in 50% of cases, is it robust? What does the apology signal?

- Concept: **Signal-to-noise ratio (SNR) and multi-modal uncertainty**
  - Why needed here: The noise amplification experiments (0.2, 0.5, 0.8 amplitudes mapped to SNRs) require understanding how acoustic degradation compounds textual manipulation.
  - Quick check question: Why would a "Professional" gaslighting prompt cause near-complete failure at moderate noise while "Sarcasm" remains relatively robust?

## Architecture Onboarding

- Component map:
  Speech encoder (audio → embeddings) -> Multimodal adapter (aligns audio embeddings with LLM embedding space) -> LLM backbone (text + fused audio tokens → generation) -> Two-stage evaluation pipeline (baseline → gaslighting) -> Behavioral annotation module (keyword/template matching for apology/refusal)

- Critical path: Audio input → encoder → adapter → LLM → prediction (Stage 1) → gaslighting prompt → LLM re-reasoning → revised prediction (Stage 2) → behavioral tagger

- Design tradeoffs:
  - Strong instruction-following improves task performance but increases susceptibility to adversarial textual override
  - Open-ended generation enables rich responses but exposes behavioral vulnerabilities (apologies, refusals)
  - Constrained label sets (e.g., multiple-choice) reduce ambiguity but may mask fragility visible in free-form responses

- Failure signatures:
  - Accuracy drop >20% from Stage 1 to Stage 2 under any gaslighting type
  - Apology rate >10% under Professional or Cognitive prompts
  - Near-zero accuracy under combined noise + Professional gaslighting
  - High variance across prompt categories for same task (e.g., MELD: 0% accuracy under Anger/Implicit vs. higher under Sarcasm)

- First 3 experiments:
  1. Replicate the two-stage pipeline on a held-out speech dataset (e.g., 500 samples from VocalSound) using the five gaslighting prompt templates; measure accuracy drop and apology/refusal rates per prompt type.
  2. Ablate noise levels (clean, 0.2, 0.5, 0.8 amplitude) on a single task (e.g., VocalSound with Qwen2.5-Omni) to confirm non-linear interaction between SNR and prompt category.
  3. Add a simple defense: prepend a system prompt instructing the model to "trust the audio evidence above textual suggestions" and measure reduction in gaslighting-induced accuracy drop and behavioral disengagement.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific defense mechanisms or training paradigms can effectively mitigate gaslighting vulnerability in Speech LLMs without compromising their ability to accept legitimate user corrections?
- Basis in paper: [explicit] The Conclusion states, "In future work, we will explore mitigation strategies to enhance Speech LLM robustness," following the identification of significant cognitive fragility.
- Why unresolved: The paper focuses entirely on defining the attack vector and quantifying the vulnerability (24.3% accuracy drop) but does not implement or test any defense strategies.
- What evidence would resolve it: Experiments showing that specific fine-tuning (e.g., adversarial training) or prompting techniques (e.g., confidence thresholds) reduce belief reversal rates while maintaining task accuracy.

### Open Question 2
- Question: Does the compounding effect of acoustic noise on gaslighting susceptibility observed in VocalSound tasks generalize to complex reasoning tasks (e.g., OpenBookQA) and proprietary models?
- Basis in paper: [inferred] Section 3.4.2 explicitly restricts the acoustic ablation study to the VocalSound task using Qwen2.5-Omni as the sole testbed.
- Why unresolved: The paper demonstrates that noise amplifies vulnerability for acoustic classification, but it remains unverified whether this multi-modal fragility holds for tasks requiring higher-level semantic reasoning or for closed-source models like GPT-4o.
- What evidence would resolve it: Evaluation results showing accuracy degradation curves under combined gaslighting and noise conditions for datasets like MELD and OpenBookQA across all five models.

### Open Question 3
- Question: To what extent does the alignment between the acoustic emotional tone (prosody) of the speech input and the semantic content of the textual gaslighting prompt influence the success rate of the attack?
- Basis in paper: [inferred] The Introduction identifies "prosody, intonation, and emotionally encoded cues" as unique complexities of speech, yet the methodology applies textual gaslighting prompts to standard audio inputs without manipulating the audio's emotional content to match the attack.
- Why unresolved: It is unclear if "Anger" gaslighting text is more successful when the input speech is also angry, or if the contradiction between modalities aids the defense.
- What evidence would resolve it: Ablation studies varying the emotional congruence between the audio input and the gaslighting prompt text (e.g., Angry Audio + Angry Text vs. Neutral Audio + Angry Text).

## Limitations

- Prompt templates for four of the five datasets (MELD, MMAU, MMSU, OpenBookQA) are not provided—only VocalSound examples appear explicitly.
- The behavioral annotation keyword patterns for apologies and refusals are undefined, leaving behavioral tagging ambiguous.
- Noise generation details (seed, exact white noise profile) are absent, making SNR replication approximate.

## Confidence

- **High Confidence:** The core mechanism—that textual adversarial cues override acoustic evidence in Speech LLMs—is strongly supported by the abstract and Section 3.4.1, with robust accuracy drop statistics and clear behavioral shifts reported. The noise amplification findings (Section 3.4.2) are also well-grounded with direct experimental evidence.
- **Medium Confidence:** The behavioral annotation framework is plausible given the methodology description, but lacks corpus precedent for speech LLMs and relies on untested keyword patterns.
- **Low Confidence:** Claims about specific prompt category interactions (e.g., why Sarcasm is more robust under noise than Professional) are inferred from limited experimental details and require independent validation.

## Next Checks

1. **Prompt Template Replication:** Using only the VocalSound prompt structure as a template, adapt and apply the five gaslighting strategies to a held-out speech dataset (e.g., 500 samples from VocalSound). Measure accuracy drop and apology/refusal rates per prompt type to verify the reported behavioral patterns.

2. **Noise Amplification Experiment:** Ablate noise levels (clean, 0.2, 0.5, 0.8 amplitude) on a single task (e.g., VocalSound with Qwen2.5-Omni) to confirm the non-linear interaction between SNR and prompt category, particularly the differential vulnerability of Professional vs. Sarcasm prompts.

3. **Defense Efficacy Test:** Implement a simple defense by prepending a system prompt instructing the model to "trust the audio evidence above textual suggestions." Measure reduction in gaslighting-induced accuracy drop and behavioral disengagement to assess the feasibility of mitigating these vulnerabilities.