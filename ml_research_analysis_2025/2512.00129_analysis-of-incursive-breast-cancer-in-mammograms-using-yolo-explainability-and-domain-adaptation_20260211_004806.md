---
ver: rpa2
title: Analysis of Incursive Breast Cancer in Mammograms Using YOLO, Explainability,
  and Domain Adaptation
arxiv_id: '2512.00129'
source_url: https://arxiv.org/abs/2512.00129
tags:
- detection
- breast
- cancer
- images
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of unreliable breast cancer detection
  in mammography when models encounter Out-of-Distribution (OOD) inputs such as different
  imaging modalities or equipment variations. The authors propose a comprehensive
  approach integrating ResNet50-based OOD filtering with YOLO architectures (YOLOv8,
  YOLOv11, YOLOv12) to improve detection reliability.
---

# Analysis of Incursive Breast Cancer in Mammograms Using YOLO, Explainability, and Domain Adaptation

## Quick Facts
- arXiv ID: 2512.00129
- Source URL: https://arxiv.org/abs/2512.00129
- Authors: Jayan Adhikari; Prativa Joshi; Susish Baral
- Reference count: 40
- Primary result: OOD filtering + YOLO detection achieves 99.77% OOD accuracy and mAP@0.5: 0.947 on invasive breast cancer detection

## Executive Summary
This paper presents a comprehensive pipeline for reliable breast cancer detection in mammograms that addresses the critical challenge of Out-of-Distribution (OOD) inputs. The authors propose a two-stage approach: first filtering non-mammographic inputs using ResNet50-based feature similarity (cosine similarity threshold 0.85), then applying YOLO architectures (v8, v11, v12) for lesion detection. The system achieves 99.77% OOD filtering accuracy while maintaining high detection performance (mAP@0.5: 0.947) and providing interpretability through Grad-CAM visualizations. The framework demonstrates how OOD filtering can enhance system reliability by preventing false alarms on irrelevant imaging modalities while preserving detection accuracy on valid mammographic data.

## Method Summary
The methodology combines OOD detection with object detection in a sequential pipeline. First, ResNet50 extracts 2048-dimensional feature vectors from input images, which are compared against an in-domain gallery using cosine similarity (threshold 0.85). Images below threshold are rejected as OOD. In-domain images proceed to YOLO detection (v8/v11/v12 variants), which uses FPN+PAN architecture for multi-scale lesion localization. The system includes Grad-CAM for interpretability, generating heatmaps that visualize model attention on detected lesions. The framework was trained on the INbreast database (1720 images, 640×640 pixels) with 1686:34 train:test split, validated against OOD test sets (MRI/X-ray images and other disease images).

## Key Results
- OOD filtering achieves 99.77% overall accuracy with 100% accuracy on OOD test sets
- YOLO detection achieves mAP@0.5: 0.947 with precision of 0.931 (class 0) and 0.963 (class 1)
- ResNet50 selected as optimal OOD backbone from 12 evaluated CNN architectures
- Grad-CAM provides interpretability with YOLOv8 achieving highest MGT score (0.86)
- System prevents false alarms on non-mammographic inputs while maintaining high detection accuracy

## Why This Works (Mechanism)

### Mechanism 1: Feature-Similarity Based OOD Filtering
Pre-filtering non-mammographic inputs via cosine similarity to an in-domain gallery prevents downstream detection errors. ResNet50 extracts 2048-dimensional feature vectors from input images. During inference, cosine similarity is computed between test image features and a pre-constructed gallery of mammographic feature vectors. Images with similarity below threshold (SC=0.85) are rejected before reaching the detection model. Core assumption: Mammographic images share a learnable feature distribution that is sufficiently distinct from other imaging modalities under ResNet50 embeddings. Evidence: 99.77% overall accuracy and 100% accuracy on OOD test sets. Break condition: Dramatic mammography equipment or protocol variations may cause valid mammograms to be incorrectly rejected.

### Mechanism 2: Multi-Scale Feature Aggregation in YOLO for Lesion Localization
YOLO's FPN+PAN architecture enables detection of lesions across varying scales while maintaining real-time inference. The Feature Pyramid Network (FPN) progressively downsamples spatial dimensions while expanding channel depth, creating multi-scale feature representations. The Path Aggregation Network (PAN) uses skip connections to merge features across levels, enabling scale-invariant object detection. Core assumption: Breast lesions manifest as localized regions with distinctive texture/edge patterns captured at some level of the feature hierarchy. Evidence: mAP@0.5: 0.947 with precision of 0.931 and 0.963 for different classes. Break condition: Very small lesions (<10mm) may fall below the receptive field resolution at all pyramid levels, leading to missed detections.

### Mechanism 3: Gradient-Based Localization for Clinical Interpretability
Grad-CAM heatmaps align model attention with lesion regions, enabling radiologist verification of AI predictions. Gradients flowing into final convolutional layer are pooled to compute importance weights for each feature map. Weighted combination of feature maps produces a coarse localization map, upsampled and overlaid on input. Core assumption: Gradients with respect to predicted class encode spatial relevance that correlates with clinically meaningful regions. Evidence: YOLOv8 achieves highest MGT score (0.86), with PCC values (0.31-0.39) indicating moderate correlation between heatmaps and ground truth. Break condition: If model relies on spurious correlations (e.g., image artifacts), Grad-CAM will highlight non-lesion regions, reducing clinical trust.

## Foundational Learning

- Concept: Out-of-Distribution Detection
  - Why needed here: The entire pipeline reliability hinges on recognizing when inputs fall outside training distribution. Without this, detection models produce unreliable predictions on non-mammographic data.
  - Quick check question: Given an image feature vector and a gallery of in-domain vectors, how would you determine if the image belongs to the training distribution?

- Concept: Intersection over Union (IoU) and Mean Average Precision (mAP)
  - Why needed here: Detection performance is evaluated via mAP@0.5, which aggregates precision-recall across IoU thresholds. Understanding IoU is prerequisite to interpreting results.
  - Quick check question: If a predicted bounding box covers 80% of a ground truth lesion but also includes 50% non-lesion area, what is the IoU?

- Concept: Transfer Learning with Pre-trained CNN Backbones
  - Why needed here: ResNet50 was selected from 12 architectures as the OOD feature extractor. Understanding why pre-trained weights matter for feature quality is essential.
  - Quick check question: Why might ImageNet-pretrained features transfer to medical imaging, and when might they fail?

## Architecture Onboarding

- Component map:
  Input Layer (640×640 images) -> OOD Module (ResNet50 feature extraction -> Cosine similarity -> Threshold comparison) -> Detection Module (YOLOv8/v11/v12 -> Bounding box prediction + class confidence) -> Explainability Module (Grad-CAM -> Heatmap generation -> PCA visualization) -> Output (Bounding boxes with confidence scores + attention heatmaps)

- Critical path:
  1. Image enters OOD module; if similarity < 0.85, reject immediately
  2. If in-domain, YOLO performs detection
  3. Grad-CAM applied to detection output for interpretability
  4. Results presented to radiologist for final verification

- Design tradeoffs:
  - ResNet50 vs. lighter models: Selected for balanced accuracy (97.06% in-domain) and efficiency (4.1 GFLOPs), but EfficientNet-b0 is 10× faster with slightly lower accuracy
  - Similarity threshold (0.85): Higher threshold increases precision but may reject valid mammograms with distribution shift
  - YOLOv8 vs. v11 vs. v12: YOLOv8 achieved highest MGT (0.86) for explainability; v11 had better PCC (0.39) but lower MGT

- Failure signatures:
  - False OOD rejection: Valid mammograms flagged as OOD → check gallery diversity and threshold calibration
  - High-confidence false positives: Non-lesion regions detected → verify Grad-CAM attention alignment
  - Low recall on small lesions: Sub-10mm masses missed → consider multi-scale training or higher resolution input

- First 3 experiments:
  1. Validate OOD threshold sensitivity: Sweep cosine similarity thresholds (0.70-0.95) on held-out mammograms from different equipment to measure false rejection rate vs. OOD detection accuracy.
  2. Ablate backbone architectures: Compare ResNet50, DenseNet121, and EfficientNet-b0 on the same OOD test sets to verify ResNet50 selection was not dataset-specific.
  3. Measure Grad-CAM clinical alignment: Have radiologists rate heatmap relevance on a sample of true positives to quantify whether MGT/PCC metrics correlate with human judgment.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the proposed OOD-YOLO framework maintain high detection accuracy and filtering reliability when transferred to other medical imaging modalities such as lung CTs, retinal images, or dermatological scans?
- Basis in paper: The "Future Direction" section explicitly states the framework "can be extended... to a wide range of more medical imaging applications" including "lung CT scans... retinal images... or dermatological images."
- Why unresolved: The current study validates the pipeline exclusively on mammograms (INbreast) and rejects non-mammogram modalities as OOD. The architecture's utility as a generalized CAD platform for other distinct anatomical structures remains unproven.
- What evidence would resolve it: Successful implementation and validation of the specific ResNet50+YOLO pipeline on target modalities (e.g., lung nodules in CT scans), demonstrating comparable mAP and OOD rejection rates.

### Open Question 2
- Question: How does the fixed cosine similarity threshold (0.85) impact the rejection rate of valid mammograms acquired from different equipment manufacturers or protocols ("near-OOD" data)?
- Basis in paper: The paper addresses "equipment variations" by rejecting them as OOD, but evaluates the OOD filter only against distinct modalities (MRI/X-ray) rather than "domain-shifted" mammograms (e.g., different vendors).
- Why unresolved: While the filter achieves 100% accuracy on non-mammograms, it is unclear if valid mammograms with slight distribution shifts (different sensors or noise profiles) are incorrectly rejected (false positives in OOD detection), which would limit clinical deployment.
- What evidence would resolve it: A cross-institutional validation study where the model, trained on INbreast, is tested on mammograms from a different dataset (e.g., DDSM or VinDr) to measure the "false rejection" rate of in-domain inputs.

### Open Question 3
- Question: Do the high detection performance metrics (mAP@0.5: 0.947) scale to larger, multi-center datasets given the extremely small validation split used in the study?
- Basis in paper: The methodological section notes a training/testing split of 1686:34 images. A test set of only 34 images introduces high statistical variance, making the reported precision and F1-scores potentially unstable.
- Why unresolved: The reliability of the YOLOv8 detection module is supported by a statistically insignificant test sample size, leaving its true generalization capability uncertain.
- What evidence would resolve it: Re-evaluation of the model on a standardized, larger test set (e.g., >500 images) to confirm if the high mAP and recall metrics persist or if they were inflated by the small sample size.

## Limitations
- Extremely small test set (34 images) may produce unreliable performance estimates and high statistical variance
- OOD filtering robustness across diverse mammography equipment and protocols remains unverified beyond presented datasets
- Grad-CAM's clinical utility lacks direct radiologist validation studies to confirm human assessment alignment

## Confidence
- High: OOD filtering mechanism design and ResNet50 backbone selection process
- Medium: Detection accuracy metrics and explainability visualization quality
- Low: Clinical translation potential without larger-scale validation and radiologist feedback

## Next Checks
1. **OOD threshold calibration**: Systematically sweep cosine similarity thresholds (0.70-0.95) on mammograms from different equipment vendors to quantify false rejection rates while maintaining OOD detection accuracy.
2. **Backbone architecture ablation**: Compare ResNet50 against DenseNet121 and EfficientNet-b0 on the same OOD test sets to verify selection was not dataset-specific.
3. **Clinical alignment validation**: Conduct radiologist evaluation of Grad-CAM heatmaps on true positive detections to determine if MGT/PCC metrics correlate with human assessment of lesion relevance.