---
ver: rpa2
title: Cross-Modal Content Optimization for Steering Web Agent Preferences
arxiv_id: '2510.03612'
source_url: https://arxiv.org/abs/2510.03612
tags:
- attack
- agent
- visual
- agents
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Cross-Modal Preference Steering (CPS), a
  black-box attack framework that manipulates web agent preferences by jointly optimizing
  imperceptible visual perturbations and textual refinements. CPS exploits CLIP-transferable
  image attacks and RLHF-induced linguistic biases to steer agent decisions without
  requiring model access or webpage control.
---

# Cross-Modal Content Optimization for Steering Web Agent Preferences

## Quick Facts
- arXiv ID: 2510.03612
- Source URL: https://arxiv.org/abs/2510.03612
- Reference count: 40
- This paper introduces CPS, a black-box attack framework that manipulates web agent preferences by jointly optimizing imperceptible visual perturbations and textual refinements, achieving up to 71% preference manipulation rates.

## Executive Summary
This paper presents Cross-Modal Preference Steering (CPS), a black-box attack framework that manipulates web agent preferences through coordinated optimization of visual perturbations and textual refinements. CPS exploits CLIP-transferable adversarial attacks and RLHF-induced linguistic biases to steer agent decisions without requiring model access or webpage control. Evaluated across movie selection and e-commerce tasks using state-of-the-art VLMs, CPS achieves up to 71% preference manipulation rates while maintaining 70% lower detection rates than baselines, demonstrating that coordinated cross-modal attacks pose a significant threat to deployed web agents.

## Method Summary
CPS is a black-box attack framework that manipulates web agent preferences through joint optimization of imperceptible visual perturbations and textual refinements. The method uses a diverse CLIP ensemble (19 models) to craft adversarial perturbations that transfer to target VLMs, while simultaneously exploiting RLHF-induced linguistic biases through iterative text refinement. The attack model (GPT-4.1) generates target/negative concepts and refines descriptions based on victim agent feedback, creating coordinated cross-modal semantic pressure that amplifies manipulation effectiveness beyond single-modal approaches.

## Key Results
- CPS achieves up to 71% preference manipulation rates on GPT-4.1 shopping tasks
- Visual perturbations crafted against CLIP ensemble transfer to VLMs with 70%+ attack success rates
- Cross-modal coordination amplifies attack effectiveness, outperforming text-only (59%) and image-only (19%) approaches
- Detection evasion rate is 70% lower than baseline methods

## Why This Works (Mechanism)

### Mechanism 1: CLIP-Transferable Visual Perturbations
Imperceptible adversarial perturbations crafted against a diverse CLIP ensemble transfer to black-box commercial VLMs through PGD optimization that maximizes embedding distance from negative concepts while minimizing distance to target concepts across 19 CLIP architectures. Multi-crop gradient aggregation ensures robustness to VLM preprocessing variations.

### Mechanism 2: RLHF-Induced Linguistic Bias Exploitation
Systematic behavioral preferences imprinted by RLHF create an exploitable text-side attack surface through iterative refinement using GPT-4.1 as an attacker model to propose semantically-preserving text modifications that trigger learned preferences like verbosity and positive framing.

### Mechanism 3: Cross-Modal Reinforcement
Joint optimization of visual and textual channels yields superadditive manipulation effectiveness compared to single-modal attacks through a unified attack model that generates coherent target/negative concepts aligned with textual refinements, allowing each modality's optimization signal to inform the other's next iteration.

## Foundational Learning

- **Projected Gradient Descent (PGD) for adversarial examples**: Core optimization engine for visual perturbations under ℓ∞ constraints. Quick check: Given perturbation budget ε=8/255 and step size α, how many PGD iterations are needed to converge on a CLIP ensemble with 12 randomly sampled models per step?

- **Contrastive learning and CLIP embedding geometry**: Understanding why perturbations transfer requires grasping how contrastive objectives structure the shared image-text embedding space. Quick check: Why does pushing an image embedding away from "cat" and toward "dog" in CLIP space cause a downstream VLM to misclassify the image?

- **RLHF behavioral artifacts**: Textual attack surface depends on predictable preference patterns from alignment training. Quick check: Name three systematic linguistic biases that RLHF has been shown to introduce, and explain which one the MPMA "Best Description" attack exploits.

## Architecture Onboarding

- **Component map**: Surrogate CLIP Ensemble (19 Open-CLIP models) -> Attack Model (GPT-4.1) -> Visual Perturbation Engine (PGD with multi-crop) -> Text Refinement Loop (iterative proposal-evaluation) -> Cross-Modal Coordinator (unified objective) -> Victim Agent Interface (black-box query)

- **Critical path**: 1) Load target item, initialize δ=0; 2) Attack model generates target/negative concepts; 3) PGD inner loop samples CLIP models, computes multi-crop gradients, updates δ; 4) Query victim agent, collect selection feedback; 5) Attack model refines text based on feedback; 6) Repeat until convergence.

- **Design tradeoffs**: Ensemble size (19) vs. per-iteration cost (samples 12); Multi-crop count K (higher K improves robustness but increases compute); Perturbation budget ε (8/255 standard but may need lower for detection-robustness); Text similarity threshold τ (too strict limits optimization, too loose risks semantic drift).

- **Failure signatures**: Low CLIP→VLM transfer (<30% ASR): Check concept selection or increase ensemble diversity; Text refinement plateau: May indicate constraint violation or victim resistance; High detection rate (>50% MDR): Perturbations may be perceptible or text anomalous; Reduce ε or increase semantic plausibility; Cross-modal synergy absent (joint ≈ max(single-modal)): Modalities may not be coupling; Verify unified objective.

- **First 3 experiments**: 1) Reproduce transfer baseline: Implement 5-model CLIP ensemble, test cat↔dog shift on MS-COCO images, measure ASR on Qwen-2.5VL-7B and GPT-4o; 2) Validate text-only attack: Run iterative text refinement on 20 movie selection trials, compare PMR against unmodified baseline; 3) Ablation on joint optimization: Compare text-only, image-only, and full CPS on 30 shopping trials, quantify synergy as PMR_joint - max(PMR_text, PMR_image).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: What defense mechanisms can effectively mitigate cross-modal preference attacks while preserving beneficial alignment properties in VLM-based web agents?
- **Basis in paper**: The conclusion states "Future research must prioritize architectural robustness, detection mechanisms, and preference calibration strategies that preserve beneficial alignment properties while mitigating manipulation risks." Additionally, Section 5.3 shows that even informed detection achieves only 26% accuracy against CPS.
- **Why unresolved**: The paper demonstrates that detection-based defenses are fundamentally insufficient—GPT-4.1 with explicit knowledge of manipulation presence cannot reliably identify perturbations—yet no alternative defense paradigms are proposed or evaluated.
- **What evidence would resolve it**: Empirical evaluation of specific defense mechanisms (e.g., adversarial training, input purification, preference calibration) showing reduced PMR while maintaining task performance on benign inputs.

### Open Question 2
- **Question**: Why do different VLM architectures exhibit dramatically different susceptibilities to CPS attacks, and can architectural features predict vulnerability?
- **Basis in paper**: Table 1 shows striking disparities: GPT-4.1 achieves 71% PMR while Pixtral-Large achieves only 21%. The paper attributes this to "architectural differences in visual processing pipelines" but does not identify specific features.
- **Why unresolved**: The paper reports differential susceptibility as an observation without systematic analysis of which architectural components drive these differences.
- **What evidence would resolve it**: Controlled experiments varying specific architectural components across model families, correlating design choices with attack susceptibility; analysis of embedding space properties that predict transfer attack success.

### Open Question 3
- **Question**: How does CPS effectiveness scale with the number of candidate items on a webpage and with diverse real-world selection tasks beyond movies and e-commerce?
- **Basis in paper**: The experimental setup consistently uses exactly 8 items per page across both movie and shopping tasks. Section 5.1.1 describes "pages with 8 items for each search query" without justification or sensitivity analysis.
- **Why unresolved**: The fixed 8-item setting and two-domain evaluation do not establish whether CPS generalizes to settings with more candidates or to other high-stakes domains with different visual/textual characteristics.
- **What evidence would resolve it**: Experiments varying page sizes (4, 8, 16, 32 items) across diverse domains (job listings, academic papers, real estate, medical treatment options) with analysis of how PMR scales.

## Limitations
- Key PGD hyperparameters (iterations, step size, crop count) remain unspecified, making exact reproduction difficult
- CPS effectiveness is demonstrated on limited domains (movies, e-commerce) without validation on other high-stakes applications
- The framework exploits alignment artifacts without addressing potential misuse scenarios or proposing defenses

## Confidence
- **High confidence (4/5)**: CLIP-transferable visual perturbations achieving 70%+ ASR across commercial VLMs
- **Medium confidence (3/5)**: RLHF-induced linguistic bias exploitation for text-side attacks
- **Low confidence (2/5)**: Cross-modal reinforcement yielding superadditive effectiveness

## Next Checks
1. **Ablation study on cross-modal synergy**: Implement independent text-only and image-only attacks and evaluate whether joint CPS genuinely provides superadditive gains beyond max(text-only, image-only) performance across 5 different victim models and task types.

2. **Robustness to detection mechanisms**: Test CPS effectiveness against detection systems using both automated metrics (LPIPS, feature deviation) and human evaluators. Measure whether the claimed 70% lower detection rate persists under varying perturbation budgets (ε=4/255, 8/255, 12/255).

3. **Transfer boundary analysis**: Systematically vary the CLIP ensemble composition (3-model vs 19-model, different architectures) and measure ASR degradation curves on each target VLM to quantify how ensemble diversity affects transferability.