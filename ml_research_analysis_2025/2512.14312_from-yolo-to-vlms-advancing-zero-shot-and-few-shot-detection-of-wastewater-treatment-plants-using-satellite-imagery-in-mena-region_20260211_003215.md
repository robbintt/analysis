---
ver: rpa2
title: 'From YOLO to VLMs: Advancing Zero-Shot and Few-Shot Detection of Wastewater
  Treatment Plants Using Satellite Imagery in MENA Region'
arxiv_id: '2512.14312'
source_url: https://arxiv.org/abs/2512.14312
tags:
- vlms
- shot
- yolov8
- wwtp
- accessed
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates VLMs against YOLOv8 for detecting wastewater
  treatment plants (WWTPs) in the MENA region using satellite imagery. A dataset of
  1,207 validated WWTP and non-WWTP locations was analyzed with models including Gemma
  3, LLaMA 3.2 Vision, and Gemini variants.
---

# From YOLO to VLMs: Advancing Zero-Shot and Few-Shot Detection of Wastewater Treatment Plants Using Satellite Imagery in MENA Region

## Quick Facts
- arXiv ID: 2512.14312
- Source URL: https://arxiv.org/abs/2512.14312
- Reference count: 0
- VLMs with few-shot prompting achieved F1 scores up to 0.8994, surpassing YOLOv8's 0.7942

## Executive Summary
This study evaluates Vision-Language Models (VLMs) against YOLOv8 for detecting wastewater treatment plants (WWTPs) in the MENA region using satellite imagery. A dataset of 1,207 validated WWTP and non-WWTP locations was analyzed with models including Gemma 3, LLaMA 3.2 Vision, and Gemini variants. In zero-shot mode, VLMs achieved high recall (up to 0.97) but low precision (0.49-0.58) due to excessive false positives. Few-shot prompting dramatically improved precision—for example, Gemma 3's precision rose from 0.4968 to 0.8572—while maintaining or exceeding YOLOv8's recall. Gemma 3 achieved the highest F1 score of 0.8994. VLMs reduced annotation burden from over 83,000 labeled images to just 5-30 exemplars.

## Method Summary
The study used 600m×600m GeoTIFF satellite images (Zoom 18, EPSG:4326) from validated WWTP and non-WWTP locations across Egypt, UAE, and Saudi Arabia. YOLOv8 was trained on 83,566 annotated images with instance segmentation. VLMs (Gemma 3, LLaMA 3.2 Vision, Qwen 2.5 VL, DeepSeek-VL2-tiny, Gemini variants, Pixtral 12B) were evaluated under zero-shot conditions using expert-crafted prompts describing WWTP morphological features, then improved with few-shot exemplars (5-30 images) selected from error-correction analysis. All VLMs output structured JSON with binary classification, component counts, confidence scores, and descriptions.

## Key Results
- Zero-shot VLMs achieved recall up to 0.97 but precision only 0.49-0.58
- Few-shot prompting improved Gemma 3 precision from 0.4968 to 0.8572 while maintaining high recall
- Gemma 3 achieved the highest F1 score of 0.8994
- VLMs reduced annotation burden from 83,566 labeled images to 5-30 exemplars

## Why This Works (Mechanism)

### Mechanism 1
Few-shot prompting calibrates VLM decision boundaries by providing error-correcting exemplars, reducing false positives while preserving recall. In-context learning allows VLMs to adjust classification behavior based on exemplars embedded in the prompt. The paper used "hard positives" (missed WWTPs from zero-shot) and "hard negatives" (false alarm cases like solar panels, industrial silos) to teach boundary discrimination without weight updates.

### Mechanism 2
VLMs achieve high zero-shot recall through semantic interpretation of expert-defined morphological prompts, but lack spatial precision boundaries without exemplars. Zero-shot prompts defined WWTPs as "systems of interconnected units" with specific features (circular tanks, rectangular basins, aeration patterns). VLMs match these textual descriptions against visual features using pre-trained vision-language alignments. High recall emerges because prompts cast a wide semantic net; low precision occurs because the model lacks calibrated thresholds.

### Mechanism 3
Structured JSON output schemas enforce consistent classification behavior and enable automated evaluation at scale. By requiring binary classification, component counts, confidence scores, and descriptions in a fixed schema, prompts constrain model outputs to parsable formats. This reduces hallucination variance and enables systematic comparison across models.

## Foundational Learning

- **Concept: Zero-shot vs. Few-shot Learning**
  - Why needed here: The paper's core comparison hinges on this distinction. Zero-shot uses only prompt instructions; few-shot adds exemplars. Understanding this explains why precision improved dramatically with few-shot.
  - Quick check question: If you add 3 example images to your prompt, is this zero-shot or few-shot? (Answer: Few-shot)

- **Concept: Precision-Recall Tradeoff**
  - Why needed here: Zero-shot VLMs achieved 0.97 recall but 0.49 precision—the paper frames this as "over-detection." Understanding F1-score as the harmonic mean explains why Gemma 3's few-shot F1 of 0.8994 matters more than raw recall.
  - Quick check question: A model flags 100 sites as WWTPs, but only 50 are correct. It missed only 5 real WWTPs. What's the precision and recall? (Answer: Precision = 50/100 = 0.50; Recall depends on total real WWTPs)

- **Concept: In-Context Learning (ICL)**
  - Why needed here: Few-shot improvements occurred without model weight updates—exemplars in prompts modified behavior via ICL. This is the scalability advantage (5-30 exemplars vs. 83,566 labeled images).
  - Quick check question: Does in-context learning require gradient updates? (Answer: No—it operates within the inference context window)

## Architecture Onboarding

- **Component map:** Geo-TIFF satellite images → VLM inference (Hugging Face/ API) → Structured JSON output → Precision/Recall/F1 evaluation

- **Critical path:** 1) Collect and validate ground-truth locations (domain expert required) → 2) Retrieve satellite imagery via Google Earth Engine/Leafmap → 3) Design zero-shot prompt with morphological definitions and edge-case differentiators → 4) Run zero-shot evaluation → identify false positive/negative patterns → 5) Select hard positives/negatives for few-shot exemplars → 6) Run few-shot evaluation → compare against YOLOv8 baseline

- **Design tradeoffs:** Open-source VLMs (LLaMA, Qwen, Gemma) vs. proprietary (Gemini): Open-source offers control and privacy; proprietary may have stronger visual grounding. Zero-shot (no exemplars, fast deployment) vs. few-shot (5-30 exemplars, higher precision): Zero-shot suitable for reconnaissance; few-shot for operational monitoring. Model size vs. inference cost: Gemma 3 27B vs. smaller variants—larger models showed better calibration but higher compute.

- **Failure signatures:** Excessive false positives on industrial facilities, solar panel arrays, agricultural ponds (zero-shot). Missed detections on partially obscured or small-scale WWTPs. JSON parsing failures from instruction-following lapses. Confidence score inflation (many predictions in 0.8-1.0 range) without corresponding accuracy.

- **First 3 experiments:** 1) **Baseline establishment:** Run YOLOv8 on held-out evaluation set (2,414 images) to establish precision/recall benchmark before VLM comparison. 2) **Zero-shot probe:** Test single VLM (recommend Gemma 3 or Gemini 2.5 Pro) with expert prompt on 100-image subset to validate prompt design before full evaluation. 3) **Few-shot calibration:** From zero-shot errors, select 5 hard positives and 5 hard negatives; measure precision improvement on same 100-image subset to validate exemplar selection strategy.

## Open Questions the Paper Calls Out

- **Open Question 1:** Does fine-tuning VLM weights on specific WWTP structural characteristics improve detection accuracy beyond the few-shot in-context learning results? The authors state that "fine-tuning the model weights based on WWTP structural characteristics may further improve accuracy," noting this could push performance higher despite being resource-intensive.

- **Open Question 2:** Can prompt-based model fine-tuning extend these few-shot methodologies effectively to newer or more advanced VLMs? The conclusion suggests "exploring prompt-based model fine-tuning" as a specific future enhancement to the current methodology.

- **Open Question 3:** Does the visual distinctiveness of WWTPs in arid MENA environments limit the transferability of these few-shot models to other climatic regions? The methodology explicitly focused on the MENA region and incorporated "differentiation logic" for local confounders like arid-environment oil/gas facilities and specific agricultural lagoons.

## Limitations

- Geographic generalizability is limited as models were evaluated only on MENA region imagery with specific infrastructure characteristics
- Few-shot exemplar effectiveness depends critically on selection of "hard positives" and "hard negatives" from zero-shot errors
- JSON output schema assumes VLMs can reliably follow structured instructions, which varies by model family and prompt complexity

## Confidence

- **High confidence**: YOLOv8 baseline performance metrics and the fundamental precision-recall tradeoff observed across all models
- **Medium confidence**: VLM zero-shot and few-shot performance gains, particularly the dramatic precision improvement (0.4968 → 0.8572 for Gemma 3)
- **Low confidence**: Generalization claims beyond the MENA region

## Next Checks

1. **Geographic transferability test**: Evaluate Gemma 3 few-shot performance on WWTP imagery from non-MENA regions (e.g., Europe, Southeast Asia) using the same exemplar set. Compare precision/recall degradation to quantify geographic generalization limits.

2. **Temporal consistency check**: Re-evaluate a subset of sites with multi-year imagery to assess VLM robustness to seasonal variations, urban development, and satellite sensor changes. Document precision/recall variance across acquisition dates.

3. **Confounder diversity analysis**: Systematically test model performance on a curated set of potential confounder types (solar farms, industrial silos, agricultural ponds) not present in the original dataset. Measure false positive rates and identify new prompt differentiators needed for robust deployment.