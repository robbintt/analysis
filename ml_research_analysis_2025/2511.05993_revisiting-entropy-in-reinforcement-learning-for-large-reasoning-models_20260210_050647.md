---
ver: rpa2
title: Revisiting Entropy in Reinforcement Learning for Large Reasoning Models
arxiv_id: '2511.05993'
source_url: https://arxiv.org/abs/2511.05993
tags:
- entropy
- training
- grpo
- llms
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates entropy dynamics in large language models
  trained with reinforcement learning from verifiable rewards (RLVR). It identifies
  three key factors affecting entropy: clipping thresholds, off-policy updates, and
  training data diversity.'
---

# Revisiting Entropy in Reinforcement Learning for Large Reasoning Models

## Quick Facts
- **arXiv ID**: 2511.05993
- **Source URL**: https://arxiv.org/abs/2511.05993
- **Reference count**: 40
- **Primary result**: Demonstrates that tokens with positive advantages drive entropy collapse in RLVR, proposing Positive-Advantage Reweighting to regulate entropy while maintaining performance

## Executive Summary
This paper investigates entropy dynamics in large language models trained with reinforcement learning from verifiable rewards (RLVR). The authors identify three key factors affecting entropy collapse: clipping thresholds, off-policy updates, and training data diversity. Through empirical analysis, they demonstrate that tokens with positive advantages are the primary drivers of entropy collapse. To address this, they propose Positive-Advantage Reweighting (PAR), a method that adjusts loss weights for these tokens to maintain entropy while preserving model performance. Experiments on Qwen2.5-Math-7B and Llama-3.1-8B-Instruct show that PAR outperforms baseline methods in controlling entropy without sacrificing competitive performance.

## Method Summary
The paper analyzes entropy dynamics in RLVR training and identifies that tokens with positive advantages (those contributing to reward improvement) are the primary drivers of entropy collapse. The authors propose Positive-Advantage Reweighting (PAR), which modifies the loss function by adjusting weights for positive-advantage tokens using an exponential scaling factor. Specifically, PAR computes the advantage for each token, identifies those with positive advantages, and applies a reweighting scheme that reduces their influence on the loss. This approach aims to maintain policy diversity while still allowing for effective learning from rewards. The method is implemented as a simple modification to existing RLVR pipelines without requiring architectural changes or additional hyperparameters.

## Key Results
- Tokens with positive advantages are identified as the primary drivers of entropy collapse in RLVR
- Positive-Advantage Reweighting effectively regulates entropy while maintaining competitive performance
- Smaller diverse datasets can achieve comparable results to larger ones, challenging the assumption that dataset size is the sole determinant of performance
- Entropy collapse correlates with model miscalibration, suggesting broader implications for RLVR training stability

## Why This Works (Mechanism)
The mechanism behind entropy collapse in RLVR stems from the reinforcement learning objective that encourages exploitation of high-reward actions. When tokens receive positive advantages (indicating they contribute to improved rewards), the policy gradient update pushes the model to increase their probability, leading to deterministic behavior. This creates a feedback loop where successful tokens become increasingly dominant, causing entropy to collapse. PAR works by modulating the influence of these positive-advantage tokens through reweighting, preventing the excessive exploitation that leads to entropy collapse while still allowing the model to learn from successful behaviors.

## Foundational Learning
**Reinforcement Learning from Verifiable Rewards (RLVR)**: A training paradigm where models learn from reward signals that can be automatically computed, commonly used for reasoning tasks. Why needed: RLVR enables scalable training of reasoning models without human feedback. Quick check: Understand the difference between RLVR and RLHF (Reinforcement Learning from Human Feedback).

**Policy Entropy**: A measure of randomness or diversity in the model's action selection. Why needed: Entropy indicates how deterministic or diverse the model's predictions are. Quick check: Higher entropy means more randomness; lower entropy means more deterministic behavior.

**Advantage Function**: The difference between the Q-value of an action and the value of the current state, indicating how much better an action is compared to the average. Why needed: Advantages guide policy updates in RL by identifying which actions contribute to reward improvement. Quick check: Positive advantages indicate actions that improve reward; negative advantages indicate actions that reduce reward.

**Clipping Thresholds**: Mechanisms that limit the magnitude of gradients or probability updates during training. Why needed: Clipping prevents excessively large updates that could destabilize training. Quick check: Understand how clipping affects the trade-off between exploration and exploitation.

**Off-policy Updates**: Training updates that use data generated by a different policy than the current one. Why needed: Off-policy learning enables more efficient use of data but can affect exploration dynamics. Quick check: Recognize how off-policy updates might accelerate entropy collapse compared to on-policy updates.

## Architecture Onboarding
**Component Map**: Tokenizer -> LLM Backbone -> RLVR Head -> Advantage Calculator -> PAR Module -> Loss Function -> Optimizer -> Model Parameters

**Critical Path**: Input sequence → Token generation → Reward calculation → Advantage computation → PAR weighting → Loss calculation → Parameter update

**Design Tradeoffs**: The PAR method trades some exploration for stability, potentially slowing convergence but preventing catastrophic entropy collapse. Alternative approaches might use entropy bonuses or KL divergence constraints, but these require additional hyperparameters and can be harder to tune.

**Failure Signatures**: If PAR is implemented incorrectly, signs include: (1) entropy continues to collapse despite PAR application, (2) performance degradation beyond acceptable levels, (3) training instability or NaN losses, (4) inability to learn from rewards effectively.

**First Experiments**: 
1. Verify that positive-advantage tokens indeed dominate the loss before implementing PAR
2. Test PAR on a small dataset with known entropy dynamics to confirm it maintains diversity
3. Compare PAR against a simple entropy bonus baseline to validate its effectiveness

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Results are based on mathematical reasoning tasks and may not generalize to other domains
- The correlation between entropy and performance needs further investigation to establish causation
- The method's effectiveness across different model architectures and scales remains uncertain

## Confidence
- **High Confidence**: The observation that positive-advantage tokens drive entropy collapse is well-supported by empirical analysis and aligns with established RL theory
- **Medium Confidence**: The effectiveness of PAR in maintaining competitive performance while controlling entropy is demonstrated, but the experimental scope is limited
- **Low Confidence**: Claims about the fundamental importance of entropy regulation for general RLVR training, and the assertion that PAR is a "simple yet effective" solution for all RLVR scenarios, are overstated given the limited experimental scope

## Next Checks
1. **Cross-architecture validation**: Test PAR on diverse model architectures (GPT-NeoX, Mistral, Claude-style models) and scales (from 1B to 70B+ parameters) to verify generalizability of the entropy dynamics and effectiveness of the method.

2. **Domain generalization**: Apply the method to non-mathematical domains (code generation, commonsense reasoning, multilingual tasks) to assess whether the positive-advantage entropy collapse phenomenon and PAR's effectiveness extend beyond mathematical reasoning.

3. **Ablation of PAR components**: Conduct a systematic ablation study isolating the effects of the three PAR components (positive-advantage identification, exponential scaling, and weight adjustment) to determine which aspects are critical for performance and entropy control.