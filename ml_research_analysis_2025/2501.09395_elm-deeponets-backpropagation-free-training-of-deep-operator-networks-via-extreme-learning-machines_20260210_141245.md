---
ver: rpa2
title: 'ELM-DeepONets: Backpropagation-Free Training of Deep Operator Networks via
  Extreme Learning Machines'
arxiv_id: '2501.09395'
source_url: https://arxiv.org/abs/2501.09395
tags:
- network
- learning
- elm-deeponet
- training
- operator
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces ELM-DeepONets, a backpropagation-free training
  framework for Deep Operator Networks (DeepONets) that leverages Extreme Learning
  Machines (ELMs). By reformulating DeepONet training as a least-squares problem with
  fixed neural network parameters and a learnable output weight matrix, the method
  drastically reduces computational complexity while maintaining accuracy.
---

# ELM-DeepONets: Backpropagation-Free Training of Deep Operator Networks via Extreme Learning Machines

## Quick Facts
- **arXiv ID:** 2501.09395
- **Source URL:** https://arxiv.org/abs/2501.09395
- **Reference count:** 26
- **Primary result:** ELM-DeepONets achieve lower relative test errors and significantly shorter training times compared to vanilla DeepONets on benchmark PDE problems.

## Executive Summary
This paper introduces ELM-DeepONets, a backpropagation-free training framework for Deep Operator Networks (DeepONets) that leverages Extreme Learning Machines (ELMs). By reformulating DeepONet training as a least-squares problem with fixed neural network parameters and a learnable output weight matrix, the method drastically reduces computational complexity while maintaining accuracy. Experiments on benchmark problems—including nonlinear ODEs, PDEs, and forward-inverse problems—demonstrate that ELM-DeepONets achieve lower relative test errors and significantly shorter training times compared to vanilla DeepONets. For instance, on an antiderivative problem, ELM-DeepONets achieved a 2.12% relative error with 0.14 seconds of training time, outperforming DeepONets (4.12%–7.19% error, 437–495 seconds). Similar improvements were observed across other test cases. The method offers a scalable and efficient alternative for operator learning in scientific computing.

## Method Summary
ELM-DeepONets reformulate DeepONet training as a least-squares problem by fixing the Branch and Trunk network parameters and only learning the output weight matrix. The training procedure involves: (1) initializing random weights for the Branch and Trunk networks and keeping them fixed, (2) computing feature matrices from input functions and collocation points, and (3) solving for the learnable weight matrix via pseudoinverse operations. This approach eliminates iterative backpropagation, reducing training to a single linear solve. The method is tested on various operator learning tasks including antiderivative computation, nonlinear ODEs, Darcy flow, and reaction-diffusion inverse source problems, consistently showing lower errors and faster training times compared to standard DeepONet training.

## Key Results
- On the antiderivative problem: 2.12% relative error with 0.14 seconds training time vs. DeepONets' 4.12%–7.19% error with 437–495 seconds
- On Darcy flow: 0.56% relative error with 0.43 seconds training time vs. DeepONets' 3.16%–7.89% error with 1222–1501 seconds
- On reaction-diffusion inverse source: 4.65% relative error with 0.20 seconds training time vs. DeepONets' 11.35%–16.84% error with 283–321 seconds

## Why This Works (Mechanism)
The method works by exploiting the linear structure of DeepONet's output layer. By freezing the Branch and Trunk networks and treating the output weight matrix as the only trainable parameter, the problem reduces to solving a linear system. This eliminates the need for gradient-based optimization and enables direct computation of optimal weights through pseudoinverse operations. The random features extracted by the fixed networks serve as a fixed basis, and the least-squares solution finds the optimal combination of these features to approximate the operator.

## Foundational Learning
- **Deep Operator Networks (DeepONets):** Neural network architecture for learning continuous operators between function spaces. Needed to understand the target architecture being modified. Quick check: Verify DeepONet consists of Branch (processes input function) and Trunk (processes evaluation points) subnetworks.
- **Extreme Learning Machines (ELMs):** Training method that fixes hidden layer weights and only learns output weights via least-squares. Needed to understand the core training innovation. Quick check: Confirm ELM reduces training to solving a linear system.
- **Moore-Penrose Pseudoinverse:** Generalized inverse used to solve underdetermined or overdetermined linear systems. Needed for computing the output weight matrix. Quick check: Verify pseudoinverse correctly handles singular or ill-conditioned matrices.
- **Gaussian Random Fields (GRFs):** Stochastic process used to generate input functions for training. Needed to understand data generation. Quick check: Confirm GRF samples have appropriate smoothness and correlation structure.
- **Operator Learning:** Learning mappings between infinite-dimensional spaces. Needed to understand the problem domain. Quick check: Verify the learned operator satisfies relevant continuity and approximation properties.

## Architecture Onboarding
**Component Map:** Input Function → Branch Network (fixed) → B Matrix → W Matrix ← T Matrix ← Trunk Network (fixed) ← Collocation Points → Ground Truth

**Critical Path:** The computation path from input function through Branch network to B matrix, combined with collocation points through Trunk network to T matrix, forms the critical path. The solution depends on the product TGB^T and its pseudoinverse.

**Design Tradeoffs:** The method trades expressive power of trainable hidden layers for computational efficiency. Fixed random features may be less expressive than learned features, but the approach enables direct linear solution. The choice of network width (p1, p2) involves balancing overfitting (too wide) against underfitting (too narrow).

**Failure Signatures:** High relative errors indicate poor random feature extraction (check initialization) or ill-conditioned linear systems (check matrix conditioning). Numerical instability in pseudoinverse computation suggests p1 > N or p2 > M, requiring regularization or dimensionality reduction.

**First Experiments:**
1. **Antiderivative Function:** Start with the simplest 1D case using a single-layer branch network and 3-layer trunk network with p1=1000, p2=100.
2. **Parameter Sensitivity:** Vary p1 and p2 systematically to find optimal width combinations while monitoring conditioning of BB^T and T^T T.
3. **Initialization Robustness:** Test different weight initialization schemes (Xavier, He) and report mean/std across multiple random seeds.

## Open Questions the Paper Calls Out
**Open Question 1:** Why does increasing hyperparameter p1 beyond the number of training samples (N) improve accuracy, despite violating the theoretical condition required for the pseudoinverse to act as a proper right inverse? The authors lack a theoretical explanation for why over-parameterization of the output weight matrix W does not lead to overfitting or numerical instability.

**Open Question 2:** Can the backpropagation-free ELM-DeepONet framework be effectively extended to Physics-Informed DeepONets (PI-DeepONets) to solve inverse problems or PDEs without labeled data? The challenge is that PI-DeepONets typically require differentiating the network output with respect to inputs to enforce PDE residuals, which conflicts with the ELM approach of fixing internal weights.

**Open Question 3:** How does the computational cost of the Moore-Penrose pseudoinverse scale with extremely large training datasets (N) compared to iterative gradient descent methods? While the method is claimed efficient for N=2000, the scalability for Big Data scenarios where N > 10^5 remains unanalyzed.

## Limitations
- Experimental validation focuses primarily on idealized PDE benchmarks, leaving open questions about robustness to noisy data and scaling to higher-dimensional problems.
- The claim of "drastically reduced computational complexity" is supported by wall-clock timing but lacks detailed FLOPs and memory usage comparison with standard DeepONets.
- Initialization of random weights is critical to performance, yet the paper does not specify the distribution or discuss sensitivity to random seeds.

## Confidence
- **High Confidence:** ELM-DeepONets significantly reduce training time compared to vanilla DeepONets (supported by quantitative wall-clock comparisons).
- **Medium Confidence:** ELM-DeepONets achieve lower relative test errors on the presented benchmarks (based on controlled synthetic experiments).
- **Low Confidence:** ELM-DeepONets are broadly applicable and robust across diverse scientific computing tasks (insufficient evidence beyond idealized PDEs).

## Next Checks
1. **Hyperparameter Sensitivity:** Systematically vary p1, p2, and weight initialization distributions; report mean and std across multiple random seeds.
2. **Scaling and Memory Analysis:** Measure FLOPs, GPU/CPU memory usage, and training time for increasing input/output dimensions and network widths.
3. **Real-World Robustness:** Evaluate ELM-DeepONets on noisy, real-world datasets (e.g., fluid dynamics measurements) and compare to baselines in terms of accuracy and generalization.