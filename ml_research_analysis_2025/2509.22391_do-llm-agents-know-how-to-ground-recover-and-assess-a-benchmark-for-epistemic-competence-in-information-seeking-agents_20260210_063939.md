---
ver: rpa2
title: Do LLM Agents Know How to Ground, Recover, and Assess? A Benchmark for Epistemic
  Competence in Information-Seeking Agents
arxiv_id: '2509.22391'
source_url: https://arxiv.org/abs/2509.22391
tags:
- evidence
- reasoning
- agents
- agent
- search
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "SeekBench introduces a process-level evaluation framework for\
  \ LLM search agents, revealing that RL training improves answer accuracy but degrades\
  \ evidence-grounded reasoning. The benchmark identifies three epistemic competencies\u2014\
  groundedness, recovery, and calibration\u2014through expert-annotated traces of\
  \ 28,493 agent interactions."
---

# Do LLM Agents Know How to Ground, Recover, and Assess? A Benchmark for Epistemic Competence in Information-Seeking Agents

## Quick Facts
- arXiv ID: 2509.22391
- Source URL: https://arxiv.org/abs/2509.22391
- Reference count: 40
- Key outcome: SeekBench reveals RL training improves answer accuracy but degrades evidence-grounded reasoning quality

## Executive Summary
SeekBench introduces a process-level evaluation framework for LLM search agents, revealing that RL training improves answer accuracy but degrades evidence-grounded reasoning. The benchmark identifies three epistemic competencies—groundedness, recovery, and calibration—through expert-annotated traces of 28,493 agent interactions. Analysis shows RL agents answer more conservatively (35% vs 63% overconfident answering) but struggle with reasoning quality (RQI=0.08 vs 0.27 for few-shot). Agent synthesis experiments demonstrate that Search-R1 excels at information synthesis (+1.27 F1 improvement) while base models collect higher-quality evidence. These findings expose critical gaps in traditional accuracy-only evaluation, showing that specialized agents can complement each other's strengths when combined strategically.

## Method Summary
SeekBench employs a 3-phase methodology: (1) iterative schema construction with 3 annotators achieving κ>0.8, (2) competency definition based on evidence state E∈{0,1,2} (clarity+sufficiency), and (3) metric operationalization. The framework uses LLM-as-judge (GPT-4.1-mini) with κ=0.731 for scalable annotation, validated against human annotations. Traces are collected from 7 agents across 7 QA benchmarks (190 expert-annotated traces, 1,800+ steps), with data sanitization removing ambiguous and contaminated questions. Three metrics are computed: Reasoning Quality Index (RQI) for groundedness, Evidence Recovery Function (ERF) for recovery speed, and Calibration Error (CE) for evidence-aligned answering.

## Key Results
- RL training improves answer accuracy and calibration but degrades evidence-grounded reasoning quality (RQI drops from 0.27 to 0.18-0.22)
- Few-shot prompting achieves highest reasoning quality (RQI = 0.27) outperforming all RL-trained agents
- REFINE and FOLLOW-UP search strategies enable fastest recovery from low-quality evidence, while REPEAT provides minimal benefit
- Agent synthesis combining Search-R1's synthesis with ASearcher's evidence collection yields +1.27 F1 improvement
- RL agents answer more conservatively (35% vs 63% overconfident answering) but at lower rates with good evidence (8.5% vs 39%)

## Why This Works (Mechanism)

### Mechanism 1: Evidence State as Foundational Primitive for Process Evaluation
The quantified "evidence state" (E ∈ {0,1,2}) encoding clarity and sufficiency of retrieved information enables systematic measurement of three distinct epistemic competencies that accuracy-only metrics conflate. The evidence state E_i,t := C_i,t + Q_i,t (clarity + quality) serves as a conditioning variable for all downstream metrics. Groundedness is assessed by whether reasoning is supported given E; recovery measures transitions from low to high E; calibration evaluates whether answering behavior aligns with E levels.

### Mechanism 2: Competency Decomposition Reveals Hidden Accuracy-Reasoning Trade-offs
Decomposing agent performance into groundedness (RQI), recovery (ERF), and calibration (CE) exposes that RL training improves answer accuracy while degrading evidence-grounded reasoning—a trade-off invisible to accuracy-only evaluation. Separate metrics isolate distinct behaviors: RQI measures whether reasoning steps cite evidence; ERF measures escape rate from low-evidence states; CE measures answer-evidence alignment. RL optimizes for correct final answers but does not optimize (may even penalize) explicit evidence grounding in intermediate steps.

### Mechanism 3: Recovery Strategy Effectiveness Follows Action-Type Hierarchy
REFINE and FOLLOW-UP search strategies yield faster recovery from low-evidence states than REPEAT, with recovery rates quantifiable via survival analysis on trace data. Adaptive query reformulation (REFINE/FOLLOW-UP) introduces new information tokens that increase probability of transitioning E from 0/1 to 2. REPEAT queries provide no new tokens and thus minimal state transition probability.

## Foundational Learning

- **Epistemic Competence** (groundedness, recovery, calibration)
  - Why needed: The paper's core thesis is that accurate answers ≠ competent reasoning. Understanding these three distinct competencies is prerequisite to interpreting SeekBench metrics and the revealed trade-offs.
  - Quick check: Can an agent achieve high accuracy while having low groundedness? If yes, what does this imply about accuracy-only evaluation?

- **Evidence State** (E ∈ {0,1,2})
  - Why needed: This is the central primitive underlying all SeekBench metrics. Without understanding how clarity and sufficiency combine into evidence state, the RQI/ERF/CE metrics are uninterpretable.
  - Quick check: If an agent has E=1 evidence (clear but insufficient), should it answer? What should it do instead?

- **Construct Validity and Latent Construct Inference**
  - Why needed: The paper explicitly draws from psychometrics (Cronbach & Meehl, 1955) to justify inferring unobservable competencies from observable annotations. This methodological framing explains why annotation schema validation matters.
  - Quick check: What observable behaviors would indicate high "groundedness" even though groundedness itself is not directly observable?

## Architecture Onboarding

- **Component map**: Annotation Schema → Evidence State Calculator → Metric Compute Layer → LLM-as-Judge → Agent Synthesis Module
- **Critical path**: 1) Collect agent traces, 2) Annotate each step for functional type + quality attributes, 3) Compute evidence state E per turn, 4) Calculate RQI, ERF, CE metrics, 5) Analyze competency profiles and trade-offs
- **Design tradeoffs**: Human annotation (high validity, κ>0.8) vs. LLM-as-judge (scalable, κ~0.73); Granular step-level analysis vs. trace-level aggregation; Single-metric optimization vs. multi-competency evaluation
- **Failure signatures**: High accuracy + low RQI (ungrounded reasoning), Low answer rate even at E=2 (overcautious calibration), Flat ERF curve (poor search adaptation)
- **First 3 experiments**: 1) Reproduce RQI comparison across Base/Few-shot/RL-trained agents, 2) Test agent synthesis: Feed ASearcher evidence to Search-R1 and measure F1 delta, 3) Ablate search strategy types: Force agents to use only REFINE vs. only REPEAT and measure ERF differences

## Open Questions the Paper Calls Out

### Open Question 1
Can training objectives be designed that simultaneously improve answer accuracy, calibration, and evidence-grounded reasoning quality? The paper demonstrates an inverse relationship where RL training improves accuracy and calibration but degrades RQI, suggesting fundamental tension in current training paradigms.

### Open Question 2
What mechanistic factors cause RL training to degrade evidence-grounded reasoning while improving calibration? The paper documents the correlation but the causal mechanism—whether reward shaping, policy optimization dynamics, or data distribution—remains unidentified.

### Open Question 3
Do SeekBench's epistemic competence metrics predict robustness to distribution shift and adversarial retrieval conditions? High RQI, ERF, and CE scores on benchmark conditions may not transfer to degraded retrieval environments where evidence quality is systematically lower.

### Open Question 4
How generalizable are the observed competency trade-offs across diverse search environments and retrieval architectures? The agents share similar architectures and training paradigms; whether competency profiles generalize to dense retrieval, multi-modal search, or tool-augmented agents is unknown.

## Limitations
- Reliance on LLM-as-judge annotations (κ=0.731) introduces potential systematic bias
- Evidence state annotation schema may not fully capture complex reasoning patterns in specialized domains
- Claim that RL training "degrades" reasoning quality assumes normative baseline where explicit evidence grounding is always desirable

## Confidence

- **High Confidence**: Evidence state framework and competency decomposition are well-supported by experimental results and theoretical grounding
- **Medium Confidence**: Recovery strategy hierarchy (REFINE > FOLLOW-UP > REPEAT) is primarily supported by internal experiments with limited external validation
- **Medium Confidence**: Agent synthesis results (Search-R1 + ASearcher F1 improvement) demonstrate proof of concept but require scaling to more diverse agent combinations

## Next Checks
1. Test inter-annotator reliability on a new domain (e.g., legal or medical) to assess schema generalizability beyond general QA
2. Implement an ablation study where RL training explicitly rewards evidence grounding to test whether the accuracy-reasoning trade-off is inherent or trainable
3. Conduct a large-scale agent synthesis experiment combining three or more agents to evaluate whether complementary strengths scale beyond pairwise combinations