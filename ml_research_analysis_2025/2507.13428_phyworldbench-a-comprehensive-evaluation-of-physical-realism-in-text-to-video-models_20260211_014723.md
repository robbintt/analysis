---
ver: rpa2
title: '"PhyWorldBench": A Comprehensive Evaluation of Physical Realism in Text-to-Video
  Models'
arxiv_id: '2507.13428'
source_url: https://arxiv.org/abs/2507.13428
tags:
- physics
- video
- prompt
- physical
- motion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PhyWorldBench addresses the challenge of evaluating physical realism
  in text-to-video generation models by introducing a comprehensive benchmark spanning
  10 physics categories across fundamental, composite, and anti-physics scenarios.
  The benchmark includes 1,050 curated prompts and introduces a novel "Anti-Physics"
  category to test models' understanding of real-world physics.
---

# "PhyWorldBench": A Comprehensive Evaluation of Physical Realism in Text-to-Video Models

## Quick Facts
- arXiv ID: 2507.13428
- Source URL: https://arxiv.org/abs/2507.13428
- Authors: Jing Gu; Xian Liu; Yu Zeng; Ashwin Nagarajan; Fangrui Zhu; Daniel Hong; Yue Fan; Qianqi Yan; Kaiwen Zhou; Ming-Yu Liu; Xin Eric Wang
- Reference count: 31
- Introduces PhyWorldBench, a comprehensive benchmark for evaluating physical realism in text-to-video generation models

## Executive Summary
PhyWorldBench addresses the critical challenge of evaluating physical realism in text-to-video generation models by introducing a comprehensive benchmark spanning 10 physics categories across fundamental, composite, and anti-physics scenarios. The benchmark includes 1,050 curated prompts and introduces a novel "Anti-Physics" category to test models' understanding of real-world physics. The authors propose a context-aware prompt metric using MLLMs for zero-shot evaluation, achieving 80.3% Semantic Adherence and 75.1% Physical Commonsense on the benchmark. Evaluation of 12 state-of-the-art models revealed significant challenges in simulating complex physical interactions, with Pika 2.0 achieving the best overall performance at 26.2% success rate.

## Method Summary
The authors created PhyWorldBench through a three-stage pipeline: defining 10 physics categories with 50 subcategories, crafting 1,050 prompts across three types (Event, Physics-Enhanced, Detailed Narrative), and establishing evaluation standards with Basic Standards for objects/events and Key Standards for physical phenomena. They developed a Context-Aware Prompt (CAP) evaluator using a two-step chain-of-thought approach with GPT-o1 to assess video outputs against these standards. The benchmark was evaluated on 12 state-of-the-art text-to-video models, generating 12,600 videos total, with human evaluation via Amazon Mechanical Turk for comparison.

## Key Results
- CAP evaluator achieves 80.3% Semantic Adherence and 75.1% Physical Commonsense ROC-AUC using GPT-o1
- Pika 2.0 achieved the best overall performance at 26.2% success rate across all categories
- Identified three systematic failure modes: static-scene rationalization (34% for Pika), erratic visual change avoidance (22.1% success rate), and multi-object complexity breakdown (11.7% vs 18.3% for simpler scenes)
- Models struggle significantly with anti-physics scenarios, often rationalizing prompts into physically correct footage instead of impossible events

## Why This Works (Mechanism)
The benchmark works by providing a structured evaluation framework that isolates physical realism from other aspects of video generation quality. By creating specific physics-enhanced and detailed narrative prompts alongside basic event prompts, the authors can measure how much additional context improves physical accuracy. The two-step evaluation process first ensures semantic adherence before checking physical commonsense, preventing false negatives from simple misunderstanding.

## Foundational Learning
- **Physics Categories**: Why needed - To systematically cover different types of physical phenomena; Quick check - Verify all 10 categories are represented in evaluation results
- **Anti-Physics Evaluation**: Why needed - To test models' ability to distinguish real from impossible physics; Quick check - Check if models generate physically impossible scenarios when requested
- **Context-Aware Evaluation**: Why needed - To provide MLLMs with proper context for assessment; Quick check - Compare CAP results with human evaluation agreement rates

## Architecture Onboarding
- **Component Map**: Prompt Generation -> Video Generation -> Frame Sampling -> CAP Evaluation -> Success Classification
- **Critical Path**: Curated prompts → Text-to-video model → 8 evenly sampled frames → Two-step MLLM evaluation → Binary success determination
- **Design Tradeoffs**: Comprehensive physics coverage vs. evaluation complexity; zero-shot evaluation vs. human annotation accuracy
- **Failure Signatures**: Static scenes in dynamic scenarios, smoothed transitions in abrupt changes, simplified multi-object interactions
- **First Experiments**: 1) Generate videos using basic event prompts vs. physics-enhanced prompts, 2) Compare CAP evaluation with human annotators on same videos, 3) Test model performance on single-object vs. multi-object scenarios

## Open Questions the Paper Calls Out
**Open Question 1**: How can text-to-video models resolve the inherent conflict between cinematic aesthetics and strict physical plausibility? The authors note that high-performing models like Sora and Pika often exaggerate motion dynamics for dramatic effect, prioritizing aesthetics over physical laws.

**Open Question 2**: How can architectures be improved to handle "erratic visual changes" (e.g., shattering, explosions) without defaulting to smoothed animations or static scenes? Current architectures seem biased toward temporal smoothness, interpreting sudden structural disruptions as artifacts to be smoothed over.

**Open Question 3**: How can models learn to comply with "Anti-Physics" prompts without defaulting to the physical priors learned from real-world training data? Models often "rationalize" prompts by generating physically correct footage rather than the requested impossible event.

## Limitations
- Benchmark focuses on 10 physics categories, potentially missing other physical phenomena
- Evaluation relies on GPT-o1, introducing potential biases in physical commonsense interpretation
- Limited human evaluation comparison using Amazon Mechanical Turk rather than physics experts

## Confidence
- **High Confidence**: Benchmark construction methodology and systematic failure mode identification
- **Medium Confidence**: Comparative performance rankings across models
- **Medium Confidence**: Generalizability of failure mode prevalence statistics

## Next Checks
1. **Cross-Evaluator Validation**: Test benchmark consistency using alternative MLLM evaluators (Claude-3.5-Sonnet, Gemini-2.0-Flash) to assess robustness of 80.3% SA and 75.1% PC metrics
2. **Prompt Style Transfer**: Generate and evaluate videos using same physical scenarios but different prompt styles to determine if failure modes are prompt-dependent
3. **Expert Annotation Comparison**: Conduct small-scale evaluation using domain experts to assess alignment with MTurk human evaluation results, particularly for complex multi-object interactions