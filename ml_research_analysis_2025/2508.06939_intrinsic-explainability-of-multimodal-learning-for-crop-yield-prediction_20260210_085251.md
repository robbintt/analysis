---
ver: rpa2
title: Intrinsic Explainability of Multimodal Learning for Crop Yield Prediction
arxiv_id: '2508.06939'
source_url: https://arxiv.org/abs/2508.06939
tags:
- yield
- data
- satellite
- weather
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of explaining multimodal learning
  models for crop yield prediction, which are often complex and lack interpretability
  despite their high performance. The authors leverage the intrinsic explainability
  of Transformer-based models to analyze intermediate representations, temporal attributions,
  and modality importance in yield prediction tasks.
---

# Intrinsic Explainability of Multimodal Learning for Crop Yield Prediction

## Quick Facts
- arXiv ID: 2508.06939
- Source URL: https://arxiv.org/abs/2508.06939
- Reference count: 40
- Primary result: Transformer-based models outperform CNN/LSTM by R² 0.10/0.04 higher; Attention Rollout provides more robust temporal attributions

## Executive Summary
This study tackles the interpretability challenge in multimodal crop yield prediction models by leveraging the intrinsic explainability of Transformer architectures. The authors develop and compare multiple attribution methods to analyze temporal patterns and modality importance, revealing that while Transformer models significantly outperform traditional architectures, different attribution methods provide conflicting estimates of modality importance. The research demonstrates how intrinsic explainability can provide agronomically relevant insights while highlighting the need for further validation of attribution methods.

## Method Summary
The approach uses modality-specific encoders (Transformers for temporal satellite and weather data, MLPs for static soil and DEM data) that output fixed-dimensional vectors, concatenated and passed through a linear regression head for yield prediction. The model employs a learnable regression token and time-index-based positional encoding. Training uses AdamW optimizer with linear warmup and cosine decay scheduling, with early stopping. The study implements and compares two model-specific methods (Attention Rollout and Generic Attention) with a model-agnostic method (Shapley Value Sampling) and proposes a new intrinsic modality attribution method (Weighted Modality Activation).

## Key Results
- Transformer-based model outperforms 1D-CNN and LSTM architectures, achieving R² scores 0.10 and 0.04 higher at subfield and field levels
- Attention Rollout provides more robust temporal attributions than other methods, with lower sensitivity and comparable infidelity scores
- Modality importance analysis reveals conflicting results: SVS attributes 89.5% importance to satellite data while WMA attributes only 29.4%, indicating need for further investigation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transformer-based encoders outperform 1D-CNN and LSTM architectures for temporal crop data.
- Mechan: Self-attention simultaneously attends to all time steps, capturing long-range dependencies and complex seasonal patterns that sequential (LSTM) or local (CNN) processing miss.
- Core assumption: Crop yield depends on non-sequential interactions across the growing season (e.g., early drought effects on late phenology), which require global temporal context.
- Evidence anchors:
  - [abstract] "Transformer-based models outperform other architectures, specifically convolutional and recurrent networks, achieving R² scores that are higher by 0.10 and 0.04 at the subfield and field levels"
  - [section 6.1.1] Table 4 shows Transformer R²=0.52 vs 1D-CNN R²=0.42 at subfield level
  - [corpus] Related work on pixel-level yield prediction (MT-CYP-Net) similarly employs attention mechanisms for temporal modeling

### Mechanism 2
- Claim: Attention Rollout (AR) yields more robust temporal attributions than Generic Attention (GA) or Shapley Value Sampling (SVS).
- Mechan: AR propagates attention weights forward through layers via matrix multiplication, preserving token-level identity; SVS requires costly perturbations that introduce estimation noise; GA's gradients are sensitive to small input changes.
- Core assumption: Attention weights encode meaningful input salience, and their recursive aggregation reflects information flow through the network.
- Evidence anchors:
  - [abstract] "AR is shown to provide more robust and reliable temporal attributions, as confirmed through qualitative and quantitative evaluation"
  - [section 6.3, Figure 16] Sensitivity scores: GA >> SVS ≈ AR; infidelity comparable across all three
  - [corpus] No direct corpus comparison of AR vs GA vs SVS; related work (LEMON) critiques single-modal attribution methods but doesn't evaluate AR specifically

### Mechanism 3
- Claim: Modality attribution via Shapley Value Sampling (SVS) and Weighted Modality Activation (WMA) produce conflicting importance estimates.
- Mechan: SVS perturbs inputs across the full model, capturing feature interactions; WMA relies solely on final-layer weights, ignoring intermediate fusion dynamics. SVS amplifies high-dimensional modalities (satellite: 25 features × T timesteps) via aggregation.
- Core assumption: SVS reflects true marginal contribution; WMA reflects direct linear contribution at output. Both may be valid but measure different things.
- Evidence anchors:
  - [abstract] "SVS estimated the contribution of satellite data at 89.5% on average, whereas the WMA method provided a significantly lower estimate of 29.4%"
  - [section 6.5, Figure 19] Soil: 41.3% (WMA) vs <2% (SVS); Satellite: 29.4% (WMA) vs 89.5% (SVS)
  - [corpus] No corpus papers directly compare SVS-based vs activation-based modality attribution; this is a methodological gap

## Foundational Learning

- **Self-attention mechanism and positional encoding**
  - Why needed here: All temporal encoders use Transformers; understanding Query/Key/Value and sinusoidal position embeddings is prerequisite to interpreting AR/GA.
  - Quick check question: Given attention weights A ∈ R^{T×T}, how would you compute the total attention received by timestep t?

- **Shapley values and feature attribution metrics (sensitivity, infidelity)**
  - Why needed here: SVS baseline is central to the comparison; sensitivity/infidelity scores are the quantitative evaluation criteria.
  - Quick check question: Why does low sensitivity but high infidelity indicate a poor attribution method?

- **Intermediate vs early/late fusion in multimodal learning**
  - Why needed here: The paper uses modality-specific encoders followed by concatenation; understanding fusion tradeoffs clarifies why WMA applies only to this design.
  - Quick check question: If fusion occurred via cross-attention instead of concatenation, would WMA still be directly applicable?

## Architecture Onboarding

- **Component map**: Sentinel-2/ERA5 weather → Transformer encoders → concatenated vectors → linear regression → yield prediction; SoilGrids/DEM → MLP encoders → concatenated vectors → linear regression → yield prediction

- **Critical path**: 1. Preprocess: upsample all modalities to 10m, pad time series to uniform length with -1; 2. Encode: pass each modality through its encoder to get h ∈ R^d; 3. Fuse: concatenate → linear regression → yield prediction; 4. Explain: compute AR/GA (attention weights), SVS (perturbations), WMA (regression weights)

- **Design tradeoffs**: Single-head vs multi-head Transformers: single-head chosen for interpretability (avoids head-averaging), at small cost to accuracy (Table 6); Concatenation fusion: simple and WMA-compatible, but limits cross-modality interaction compared to cross-attention; SVS sampling: robust but expensive (32 pixels per field vs full field for AR)

- **Failure signatures**: Validation/test gap: Table 6 shows R² 0.75–0.77 (validation) vs 0.39–0.52 (test) across configurations → suggests overfitting or distribution shift; LOYO/LOFO performance collapse (Appendix B): negative R² in most years/farms → model does not generalize to unseen regions/years without targeted fine-tuning; Modality attribution conflict (Figure 19): SVS vs WMA disagree on all modalities → indicates neither method is a ground-truth proxy

- **First 3 experiments**: 1. Replicate Transformer vs LSTM vs 1D-CNN comparison on a single crop-year subset to validate performance gap; log training curves and inference time; 2. Implement AR and GA on the trained Transformer; compute sensitivity and infidelity scores on 10 fields to confirm AR's lower sensitivity; 3. Compute WMA and SVS modality scores on the same fields; correlate with linear probe separability (Figure 8) to assess which method aligns better with intermediate representations

## Open Questions the Paper Calls Out

- **Open Question 1**: Which modality attribution method provides the most reliable estimation of feature importance?
  - Basis in paper: [explicit] The Conclusion states that the "conflicting results" between Shapley Value Sampling (SVS) and Weighted Modality Activation (WMA) necessitate a "quantitative evaluation to determine the most reliable approach."
  - Why unresolved: Without ground truth labels for modality importance, the authors could not verify if satellite data (dominant in SVS) or soil properties (dominant in WMA) truly drove the predictions.
  - What evidence would resolve it: A benchmarking framework, potentially using synthetic data with known feature importance, to objectively validate the fidelity of the attribution methods.

- **Open Question 2**: Do model attention weights consistently align with agronomically critical crop growth stages?
  - Basis in paper: [explicit] The Discussion notes that the model sometimes overlooks critical growth stages, raising "questions about potential gaps in the model's reasoning" that require "further experiments across multiple fields."
  - Why unresolved: The study was limited by sparse phenology data, restricting the validation of temporal attributions to a small subset of soybean fields where alignment was inconsistent.
  - What evidence would resolve it: A large-scale correlation analysis using dense phenology metadata to verify if attention peaks statistically correspond to established biological growth stages.

- **Open Question 3**: Can intrinsic explainability insights be used to enforce constraints that improve model performance?
  - Basis in paper: [explicit] The Conclusion suggests that future work should explore "building on the explainability findings to enforce certain rules or constraints during the learning phase."
  - Why unresolved: The current study focused on interpreting existing reasoning patterns but did not attempt to feed these findings back into the training loop to correct identified reasoning gaps.
  - What evidence would resolve it: An experiment where attention mechanisms are regularized based on agronomic rules, resulting in improved predictive accuracy (R²).

## Limitations

- Proprietary ground truth yield data prevents exact numerical reproduction of reported results
- Conflicting Transformer architecture specifications (4 layers vs 1 layer) create ambiguity about the precise model configuration
- Significant disagreement between SVS and WMA modality attribution scores (89.5% vs 29.4% for satellite data) suggests neither method provides definitive ground truth

## Confidence

- **High Confidence**: Transformer outperforms CNN/LSTM architectures (R² scores 0.10 and 0.04 higher); Attention Rollout provides more robust temporal attributions (lower sensitivity scores)
- **Medium Confidence**: WMA provides reasonable modality importance estimates for concatenation-based fusion architectures; spatial misalignment has minimal impact on model performance
- **Low Confidence**: Interpretation of conflicting modality attributions (SVS vs WMA disagreement); generalizability to regions outside the studied farms without fine-tuning

## Next Checks

1. **Architecture Clarification Validation**: Re-implement both the 1-layer and 4-layer Transformer configurations; measure and compare R² performance and attribution stability to determine which configuration was actually used for the explainability analysis

2. **Ground Truth Sensitivity Analysis**: Using a public proxy dataset (e.g., BreizhCrops adapted for regression), systematically vary the spatial aggregation level and compute SVS vs WMA attribution consistency to understand how ground truth granularity affects modality importance estimates

3. **Cross-Attention Fusion Comparison**: Implement a cross-attention-based fusion architecture and compute WMA scores to test whether the method's limitations are specific to concatenation fusion or extend to other fusion strategies