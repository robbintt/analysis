---
ver: rpa2
title: Benchmarking Machine Learning Models for IoT Malware Detection under Data Scarcity
  and Drift
arxiv_id: '2601.18736'
source_url: https://arxiv.org/abs/2601.18736
tags:
- malware
- learning
- data
- performance
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluates four supervised learning models\u2014Random\
  \ Forest, LightGBM, Logistic Regression, and MLP\u2014for IoT malware detection\
  \ using the IoT-23 dataset. Models are assessed across binary and multiclass classification,\
  \ varying training data sizes, and temporal robustness to evolving threats."
---

# Benchmarking Machine Learning Models for IoT Malware Detection under Data Scarcity and Drift

## Quick Facts
- arXiv ID: 2601.18736
- Source URL: https://arxiv.org/abs/2601.18736
- Reference count: 20
- Primary result: Tree-based models (Random Forest, LightGBM) achieve high accuracy and F1 scores even with limited training data, while MLP performance scales with data volume; temporal analysis reveals performance degradation due to evolving malware diversity

## Executive Summary
This study evaluates four supervised learning models—Random Forest, LightGBM, Logistic Regression, and MLP—for IoT malware detection using the IoT-23 dataset. Models are assessed across binary and multiclass classification, varying training data sizes, and temporal robustness to evolving threats. Tree-based models (RF and LGBM) consistently achieve high accuracy and F1 scores, even with limited training data, while MLP improves with more data. Logistic Regression performs adequately in binary tasks but struggles in multiclass settings. Temporal analysis reveals performance degradation over time due to increasing malware diversity, underscoring the need for adaptive learning strategies in dynamic IoT environments.

## Method Summary
The study uses the IoT-23 dataset, processing 25M flows sampled from 325M total flows. Network traffic is converted to flow-level features using tools like CICFlowMeter/Zeek, extracting 21 statistical features. Preprocessing removes UIDs, IPs, and local_orig/local_resp fields, then applies timestamp engineering and label encoding. Mutual information feature selection reduces dimensionality to 12 features, followed by Min-Max scaling to [0,1]. Four models are evaluated: Logistic Regression, Random Forest, LightGBM, and MLP (2 hidden layers: 64→128 neurons with ReLU, 0.2 dropout). Hyperparameter tuning uses RandomizedSearchCV with 50 configurations. Evaluation includes 5-fold cross-validation, training size sensitivity analysis (30–80% ratios), and temporal rolling-window testing using monthly splits.

## Key Results
- Tree-based models (RF and LGBM) achieve F1 scores above 0.998 in binary classification even with 30% training data
- MLP performance scales positively with training data volume, improving from F1=0.7227 to 0.8005 in multiclass classification as training data increases from 30% to 80%
- Temporal analysis shows F1 scores degrading from ~1.0 to <0.5 as malware diversity increases across monthly intervals, with family expansion from 2 types (2018-07) to 5+ types (2019-01 onwards)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Tree-based models maintain high performance under data scarcity due to ensemble variance reduction
- **Mechanism:** RF uses bagging across 100-200 decision trees with randomized feature subsets, reducing overfitting on limited samples. LGBM employs Gradient-based One-Side Sampling (GOSS) and Exclusive Feature Bundling (EFB) to focus learning on informative gradients while compressing features
- **Core assumption:** The informative features identified during training remain discriminative at inference time
- **Evidence anchors:** [abstract] "tree-based models (RF and LGBM) consistently achieve high accuracy and F1 scores, even with limited training data"; [Section IV-C, Table III] RF achieves F1=0.9987 at 30% training data; F1=0.9992 at 80%; [corpus] Related work confirms LightGBM achieving best classification performance on IoT-23
- **Break condition:** Feature distribution shift or novel attack patterns with fundamentally different traffic signatures cause ensemble decision boundaries to become stale

### Mechanism 2
- **Claim:** MLP performance scales positively with training data volume, enabling capture of complex non-linear malware patterns
- **Mechanism:** The 2-hidden-layer architecture (64→128 neurons with ReLU) learns hierarchical feature interactions. Dropout (0.2) regularizes against overfitting. More samples allow gradient descent to better estimate the loss landscape for complex multiclass boundaries
- **Core assumption:** Sufficient labeled data exists to represent the underlying distribution of each malware family
- **Evidence anchors:** [abstract] "MLP improves with more data"; [Section IV-C, Table IV] MLP F1 improves from 0.7227 (30% data) to 0.8005 (80% data) in multiclass; [corpus] Related work on neural approaches shows ANN achieving 0.9708 classification accuracy with sufficient data
- **Break condition:** Class imbalance or insufficient minority-class samples prevent effective gradient-based learning; performance plateaus despite additional majority-class data

### Mechanism 3
- **Claim:** Temporal performance degradation occurs due to concept drift from evolving malware diversity
- **Mechanism:** Models trained on historical traffic (e.g., 2018-05 to 2018-10) encounter previously unseen malware families (e.g., C&C, DDoS, PortScan variants) in later months. Static decision boundaries fail to generalize to new attack signatures and traffic patterns
- **Core assumption:** Malware behavior, device activity, and network configurations shift over time in real deployments
- **Evidence anchors:** [abstract] "performance deteriorates over time as malware diversity increases"; [Section IV-D, Table VII] Malware families expand from 2 types (2018-07) to 5+ types (2019-01 onwards); [corpus] Related work confirms class imbalance and temporal drift as key detection challenges
- **Break condition:** Aggressive periodic retraining or adaptive online learning is implemented; model architecture supports incremental updates

## Foundational Learning

- **Concept: Flow-level feature extraction from network traffic**
  - Why needed here: Raw PCAP files cannot be used directly; tools like CICFlowMeter/Zeek extract statistical features (packet counts, durations, byte rates, protocol types) into tabular format for ML consumption
  - Quick check question: Can you explain why source/destination IP addresses were excluded during preprocessing?

- **Concept: Class imbalance and F1-score evaluation**
  - Why needed here: IoT-23 has severe label imbalance (Benign: 8.78M, DDoS: 5.78M, PortScan: 3.39M, C&C Attack: 8,685). Accuracy masks minority-class failures; F1 balances precision and recall
  - Quick check question: Why did LGBM achieve 97.70% accuracy but only 0.5723 F1 in the 80% multiclass training ratio?

- **Concept: Mutual information feature selection**
  - Why needed here: Reduces dimensionality from 21 to 12 features by quantifying feature-label relevance, lowering computational overhead for resource-constrained IoT deployment
  - Quick check question: What are the top 3 most informative features selected, and why might "hour of day" be relevant?

## Architecture Onboarding

- **Component map:** PCAP Capture → Zeek/CICFlowMeter → CSV Flow Features → Preprocessing: Concat → Clean (drop IPs/UIDs) → Timestamp Engineering → Label Encoding → Feature Selection (Mutual Information) → Min-Max Scaling [0,1] → Model Training (LR/RF/LGBM/MLP) → Hyperparameter Tuning (RandomizedSearchCV, 50 configs) → Evaluation: 5-fold CV | Training Size Analysis | Temporal Rolling-Window

- **Critical path:** Feature selection → Model training → Temporal evaluation (this is where real-world deployment failures surface)

- **Design tradeoffs:** RF vs LGBM: RF offers more stable multiclass F1; LGBM is faster but struggles with minority classes under imbalance. MLP vs Tree-based: MLP scales better with data but requires more compute; tree-based models are deployment-ready with less data. Accuracy vs F1: Use F1 as primary metric for multiclass; accuracy is acceptable for binary detection

- **Failure signatures:** High accuracy, low F1 in multiclass → minority class overprediction (LR pattern). Sudden F1 drop in temporal testing → new malware family not in training distribution. Stable training F1, degrading test F1 → concept drift requiring retraining

- **First 3 experiments:** 1) Replicate binary classification with 5-fold CV using RF; verify F1 > 0.998 on your data subset. 2) Test training size sensitivity (30%, 50%, 70%) for MLP vs RF; plot F1 curves to confirm MLP's data scaling. 3) Implement rolling-window temporal split (train: 2018-05 to 2018-10, test: 2018-12); measure F1 degradation to quantify drift

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can adaptive learning strategies (e.g., online learning, active learning) be optimized to mitigate the temporal performance drift identified in static supervised models?
- **Basis in paper:** [explicit] The Conclusion states, "In future work, we plan to design and evaluate adaptive learning techniques to address temporal drift."
- **Why unresolved:** This study quantifies the degradation (concept drift) over time but does not implement or test mechanisms to update models dynamically as new malware families appear
- **What evidence would resolve it:** A comparative study measuring the F1 score retention of online learning models versus static models when tested on the evolving IoT-23 monthly intervals

### Open Question 2
- **Question:** What are the inference latency and energy costs of the benchmarked models when deployed on actual resource-constrained IoT edge hardware?
- **Basis in paper:** [inferred] The paper emphasizes the need for "lightweight" models for devices with "limited computational resources," but all experiments were conducted using standard Python libraries on hardware constrained only by training memory
- **Why unresolved:** While classification accuracy is benchmarked, the actual computational efficiency (latency, memory footprint) on embedded systems (the target domain) remains unmeasured
- **What evidence would resolve it:** Benchmarking inference time and power consumption for Random Forest and LightGBM on edge devices (e.g., Raspberry Pi or microcontrollers) using the IoT-23 dataset

### Open Question 3
- **Question:** Can hybrid or ensemble architectures resolve the trade-off where linear models over-predict minority classes while deep learning models under-predict them in multiclass settings?
- **Basis in paper:** [inferred] The results show Logistic Regression maintains high recall (0.95) but poor precision in multiclass tasks, whereas MLP shows high precision but lower recall, suggesting complementary failure modes
- **Why unresolved:** The study evaluates models in isolation; it does not explore if combining the high recall of linear models with the precision of tree-based or neural models could improve robustness against class imbalance
- **What evidence would resolve it:** Evaluating the F1 scores of ensemble methods (e.g., stacking or voting classifiers) combining LR and RF/MLP on the imbalanced multiclass dataset

## Limitations
- Temporal generalization bounds remain unquantified—the study shows performance degradation over time but doesn't model the rate of concept drift or establish thresholds for when retraining becomes necessary
- The IoT-23 dataset's sampling methodology (25M from 325M flows) isn't fully specified, creating uncertainty about whether results generalize to the full dataset distribution
- Resource-constrained deployment validation is absent—while models are theoretically suitable for IoT, actual latency, memory, and energy consumption measurements on edge devices are not provided

## Confidence
- **High confidence:** Tree-based model superiority under data scarcity (supported by multiple metrics across training ratios)
- **Medium confidence:** MLP's data-scaling behavior (improvement shown but with wide variance across training ratios)
- **Low confidence:** Temporal robustness conclusions (limited to monthly rolling windows without statistical significance testing)

## Next Checks
1. Implement statistical significance testing on temporal F1 score degradation across multiple rolling-window configurations
2. Validate model performance on the full 325M flow dataset rather than the sampled 25M subset
3. Measure actual resource consumption (CPU, memory, inference latency) when deploying top-performing models on Raspberry Pi or similar IoT hardware