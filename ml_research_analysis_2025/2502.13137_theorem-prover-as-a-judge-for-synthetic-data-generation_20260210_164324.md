---
ver: rpa2
title: Theorem Prover as a Judge for Synthetic Data Generation
arxiv_id: '2502.13137'
source_url: https://arxiv.org/abs/2502.13137
tags:
- total
- theorem
- years
- reasoning
- step
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Theorem Prover as a Judge (TP-as-a-Judge),
  a method that uses Lean theorem prover formalisation to verify intermediate reasoning
  steps in synthetic data generation for mathematical reasoning tasks. The authors
  address the challenge of ensuring validity in LLM-generated reasoning by proposing
  iterative autoformalisation, which refines theorem prover formalisation through
  multiple iterations, increasing Lean execution rates from 60% to 87%.
---

# Theorem Prover as a Judge for Synthetic Data Generation

## Quick Facts
- arXiv ID: 2502.13137
- Source URL: https://arxiv.org/abs/2502.13137
- Reference count: 40
- One-line primary result: Theorem Prover as a Judge (TP-as-a-Judge) achieves 5.56% accuracy gain on Mistral-7B for MultiArith, 6.00% on Llama-2-7B for SVAMP, and 3.55% on Llama-3.1-8B for AQUA through formal verification of intermediate reasoning steps

## Executive Summary
This paper introduces Theorem Prover as a Judge (TP-as-a-Judge), a method that uses Lean theorem prover formalisation to verify intermediate reasoning steps in synthetic data generation for mathematical reasoning tasks. The authors address the challenge of ensuring validity in LLM-generated reasoning by proposing iterative autoformalisation, which refines theorem prover formalisation through multiple iterations, increasing Lean execution rates from 60% to 87%. They replace human annotation with theorem prover feedback in Reinforcement Learning from Theorem Prover Feedback (RLTPF), training models on 3,508 synthetic samples.

## Method Summary
The method employs a multi-stage pipeline: (1) Reverse QA generates synthetic question-answer pairs using gpt-4o; (2) Question formalisation converts natural language math to Lean via CoMAT symbolic conversion, autoformalisation, auto-informalisation, and alignment check; (3) Answer formalisation uses iterative autoformalisation (up to 5 retries with error feedback) to verify reasoning steps with Lean; (4) Training uses SFT on verified samples followed by DPO (RLTPF) on preference pairs. The approach filters out invalid reasoning chains, training models exclusively on logically coherent solutions to improve mathematical reasoning accuracy.

## Key Results
- TP-as-a-Judge achieves 5.56% accuracy gain on Mistral-7B for MultiArith, 6.00% on Llama-2-7B for SVAMP, and 3.55% on Llama-3.1-8B for AQUA
- Iterative autoformalisation increases Lean execution rate from 60% to 87%
- TP-as-a-Judge achieves 0.87 F1 score vs. o1-mini's 0.72, reducing False Positives by 2x
- SFT (Only Verified) + RLTPF achieves 83.75% accuracy, outperforming SFT (All Instances) (82.23%)

## Why This Works (Mechanism)

### Mechanism 1: Iterative Error Correction for Formalisation
- **Claim:** Providing theorem prover error logs back to the LLM significantly increases the success rate of translating natural language reasoning into executable formal logic.
- **Mechanism:** When the Lean prover returns an `error` (e.g., syntax failure or type mismatch), this output is fed into the prompt, allowing the LLM to correct the specific formalisation mistake. This loop runs for up to 5 iterations.
- **Core assumption:** The LLM possesses sufficient capability to interpret formal verification errors and map them to corrections in the formal syntax (Lean).
- **Evidence anchors:** [Abstract]: "increasing the execution rate on the Lean prover from 60% to 87%"; [Section 2.2.3]: "correcting each individual step... prompting the model to revise and reverify... execution rate... from 60% to 87%."

### Mechanism 2: Process-Level Verification via TP-as-a-Judge
- **Claim:** Replacing outcome-based verification (checking the final answer) with process-based verification (checking intermediate steps) reduces false positives in synthetic data generation.
- **Mechanism:** The system decomposes the solution into a sequence of proof steps $\{a'_i(q)\}$. A step is only marked `Verified` if the theorem prover validates the logical transition, not just the final numerical result.
- **Core assumption:** The autoformalisation of intermediate steps accurately captures the semantic intent of the natural language reasoning.
- **Evidence anchors:** [Section 4.2]: Table 4 shows TP-as-a-Judge achieves 0.87 F1 score vs. o1-mini's 0.72, reducing False Positives by 2x; [Section 1]: "LLM-generated CoT reasoning steps may often include errors... even when the final answer appears plausible."

### Mechanism 3: Inductive Bias via Verified-Only SFT
- **Claim:** Training exclusively on samples that pass theorem prover verification provides a stronger learning signal than training on the full noisy dataset.
- **Mechanism:** By filtering out `False` and `Error` samples for the Supervised Fine-Tuning (SFT) phase, the model learns exclusively from logically coherent reasoning chains, preventing the optimization of invalid patterns.
- **Core assumption:** The "Verified" samples are representative of the target distribution and do not introduce a bias toward only simple problems (though the paper notes this risk).
- **Evidence anchors:** [Table 7]: "SFT (Only Verified) + RLTPF" achieves 83.75% accuracy, outperforming "SFT (All Instances)" (82.23%); [Section 4.4]: "SFT with all instances showed degraded performance... likely due to a conflict between SFT [trained on rejected instances] and RLTPF."

## Foundational Learning

- **Concept: Autoformalisation**
  - **Why needed here:** This is the translation layer converting natural language math into Lean code. Without understanding this, you cannot diagnose why verification fails (syntax vs. logic).
  - **Quick check question:** Does a failed Lean execution mean the math is wrong? (Answer: No, it might be a syntax or translation error, which Mechanism 1 addresses).

- **Concept: Direct Preference Optimization (DPO)**
  - **Why needed here:** The paper uses RLTPF (a variant of DPO) to teach the model to prefer verified reasoning over rejected reasoning. You need to distinguish the SFT phase (imitation) from the DPO phase (preference).
  - **Quick check question:** In RLTPF, what pairing of data is required for the DPO phase? (Answer: One `Verified` response and one `False` response for the same question).

- **Concept: Lean Theorem Prover**
  - **Why needed here:** Lean acts as the "Ground Truth" judge. Unlike Python execution which checks runtime, Lean checks logical proofs.
  - **Quick check question:** Why is a theorem prover used instead of a Python interpreter for complex math? (Answer: Python checks if code runs; Lean checks if logical steps are valid deductions).

## Architecture Onboarding

- **Component map:** Generator (LLM) -> Question Formaliser (CoMAT → Autoformalise → Auto-Informalise → Alignment Check) -> Answer Formaliser (Iterative loop with Lean verification) -> Trainer (SFT → DPO)
- **Critical path:** The **Iterative Autoformalisation** loop (Algorithm 1). If this component cannot resolve Lean errors within 5 retries, the sample is lost (discarded or marked error), directly reducing training data yield.
- **Design tradeoffs:** Yield vs. Quality: The system discards questions failing formalisation (approx. 30%, per Fig 4). This ensures high data quality but increases generation costs. Cost vs. Complexity: Longer reasoning sequences (tokens) require more iterations (Table 6), increasing API costs and latency.
- **Failure signatures:** High "Error" Rate: If Lean execution consistently fails, the formalisation prompt or the model's coding capability is insufficient. Low DPO Pairs: If both models (LLM A and LLM B) consistently agree (both Verified or both False), you lack the preference pairs needed for DPO.
- **First 3 experiments:** Unit Test Autoformalisation: Pass 50 GSM8K samples through the Question Formalisation pipeline (s1-s4) to measure the "Alignment Check" pass rate before generating synthetic data. Ablation on Iterations: Compare the `Verified` yield when setting `R=1` (no iteration) vs `R=3` to quantify the value of the iterative loop. SFT vs. RLTPF: Train a small proxy model on the "SFT (All)" dataset vs. "SFT (Only Verified)" to reproduce the performance gap shown in Table 7.

## Open Questions the Paper Calls Out

- **Can the Theorem Prover as a Judge framework be effectively adapted to verify reasoning in non-mathematical domains, such as legal or commonsense reasoning, where formal verification is currently more challenging?**
  - **Basis in paper:** [explicit] The Limitations section states that "theorem prover verification... remains challenging in other domains, making its extension beyond mathematics an open research question."
  - **Why unresolved:** The current study restricts its scope to mathematical reasoning (algebra, counting, probability) because the logic is easily formalisable in Lean; applying this to domains with ambiguous or subjective logic has not been tested.
  - **What evidence would resolve it:** A demonstration of the TP-as-a-Judge framework successfully generating and verifying synthetic data for a non-mathematical benchmark (e.g., logical fallacy detection) with comparable data efficiency.

- **How does the computational cost and iteration count of "iterative autoformalisation" scale when applied to advanced mathematical domains that lack extensive Lean definitions, such as geometry or calculus?**
  - **Basis in paper:** [inferred] The authors note that the current dataset is limited to algebra and probability due to "computational constraints" and the model's ability to solve problems (Limitations), and that "instances requiring more iterations often fail" (Section 4.3).
  - **Why unresolved:** The paper demonstrates success on arithmetic/algebra but excludes geometry and limits MATH dataset complexity; it is unclear if the 5-iteration cap is sufficient for formalizing higher-level concepts without excessive resource consumption.
  - **What evidence would resolve it:** An ablation study applying the iterative autoformalisation pipeline to geometry or calculus problems, reporting the average number of iterations required and the percentage of samples successfully formalised versus discarded.

- **Why does combining Supervised Fine-Tuning on all instances (verified and rejected) with Reinforcement Learning from Theorem Prover Feedback (RLTPF) result in lower accuracy than using only verified instances?**
  - **Basis in paper:** [inferred] Table 7 shows that "SFT (All) + RLTPF" yields 79.60% accuracy, significantly lower than "SFT (Only Verified) + RLTPF" (83.75%), which the authors hypothesize is due to a "conflict" but do not mechanistically explain.
  - **Why unresolved:** The paper establishes the phenomenon but does not investigate if the inclusion of false positives (rejected instances) in the SFT phase creates a gradient conflict that DPO fails to correct, or if the "verified" data distribution is fundamentally different.
  - **What evidence would resolve it:** An analysis of the loss landscapes or gradient updates during the training of "SFT (All)" versus "SFT (Verified)," specifically identifying if the model learns conflicting representations for the rejected samples.

## Limitations

- The iterative autoformalisation mechanism's generalizability remains uncertain beyond mathematical domains, particularly for abstract mathematical domains requiring extensive Lean definitions.
- The evaluation scope is limited to grade-school and high-school level mathematics, with only MATH-500 and AIME 2024 representing more challenging problems.
- The computational efficiency claims lack quantification of the actual overhead, as iterative formalisation requires multiple LLM calls per sample, increasing inference costs.

## Confidence

- **High Confidence:** The mechanism of iterative error correction (Mechanism 1) is well-supported by empirical evidence showing execution rates improving from 60% to 87%. The Lean prover's ability to catch logical errors in intermediate steps is demonstrated through TP-as-a-Judge's superior F1 score (0.87 vs 0.72) and reduced false positives.
- **Medium Confidence:** The claim that process-level verification outperforms outcome-based verification (Mechanism 2) is supported by TP-as-a-Judge's improved performance on synthetic data generation, but the evaluation relies heavily on the synthetic data itself rather than independent verification.
- **Low Confidence:** The effectiveness of training exclusively on verified samples (Mechanism 3) shows mixed results, with the paper acknowledging the risk of bias toward simpler problems and the degradation when mixing rejected samples into SFT training.

## Next Checks

1. **Domain Generalization Test:** Apply the TP-as-a-Judge methodology to undergraduate-level mathematics problems (e.g., real analysis, abstract algebra) to assess whether the 60%→87% execution rate improvement generalizes beyond grade-school arithmetic. Measure both the yield of valid formalisations and the quality of generated reasoning chains.

2. **Cost-Benefit Analysis:** Quantify the computational overhead of iterative autoformalisation by comparing total token consumption and wall-clock time against baseline synthetic data generation methods. Calculate the cost per verified sample and determine the break-even point where the accuracy gains justify the increased resource requirements.

3. **Bias Characterization Study:** Analyze the distribution of problem types and difficulty levels in the final "Verified" dataset versus the original synthetic generation pool. Use statistical tests to determine whether the formalisation and verification process systematically excludes certain mathematical concepts or problem structures, and measure the impact on downstream model performance across different mathematical domains.