---
ver: rpa2
title: A Brief Review for Compression and Transfer Learning Techniques in DeepFake
  Detection
arxiv_id: '2504.21066'
source_url: https://arxiv.org/abs/2504.21066
tags:
- compression
- deepfake
- learning
- transfer
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates compression and transfer learning techniques
  for deepfake detection models on edge devices. The study evaluates pruning, knowledge
  distillation, quantization, fine-tuning, and adapter-based methods across three
  datasets.
---

# A Brief Review for Compression and Transfer Learning Techniques in DeepFake Detection

## Quick Facts
- **arXiv ID:** 2504.21066
- **Source URL:** https://arxiv.org/abs/2504.21066
- **Reference count:** 22
- **Primary result:** Compression and transfer learning can successfully reduce deepfake detection model size while maintaining performance on the same generator, but face domain generalization challenges with different generators.

## Executive Summary
This paper investigates compression and transfer learning techniques for deepfake detection models on edge devices. The study evaluates pruning, knowledge distillation, quantization, fine-tuning, and adapter-based methods across three datasets. Results show that compression methods maintain performance levels up to 90% compression when training and validation data come from the same deepfake model. Knowledge distillation generally outperforms pruning, while adapters enhance performance when positioned appropriately. Transfer learning achieves strong results with consistent data sources but faces domain generalization challenges with different deepfake generators. Quantization is effective but hardware-dependent. The study concludes that compression and transfer learning can be successfully combined for deepfake detection, though domain adaptation remains an open challenge. Future work will focus on addressing domain generalization issues.

## Method Summary
The paper evaluates ten approaches combining compression (pruning, knowledge distillation, quantization) and transfer learning (fine-tuning, adapters) on a VGG-based model (~4.5M parameters). Three datasets are used: SynthBuster (9,000 AI-generated images from 9 models + RAISE authentic images), ForenSynths (720,119 train, 8,000 val, 90,310 test), and Dogs vs Cats (25,000 images) for transfer learning pre-training. All images are resized to 224×224. Compression rates of 60%–90% are evaluated. The methods include baseline training, CPF (pruning + fine-tuning), CKD (knowledge distillation), CQ (quantization float32→int8), and various combinations of transfer learning with compression techniques. Code is available at https://github.com/andreaskarathanasis/Compression-Transfer-of-DeepFake-Models using Python 3, PyTorch, PIL, and scikit-learn on 2× NVIDIA T4 GPUs.

## Key Results
- Compression methods maintain performance up to 90% when training and validation data originate from the same deepfake generator
- Knowledge distillation generally outperforms pruning in preserving detection accuracy
- Adapters improve performance when positioned at the end of convolutional layers, especially for student models
- Transfer learning achieves strong results with consistent data sources but faces domain generalization challenges with different deepfake generators
- Quantization is effective for memory reduction but hardware-dependent for speed benefits

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Knowledge Distillation (KD) generally preserves detection accuracy better than pruning when training and validation data originate from the same generative model, even at high compression rates (up to 90%).
- **Mechanism:** The student model learns to mimic the soft probability distributions of the teacher model, capturing more nuanced feature representations than learning from hard labels alone. This preserves the "knowledge" of the deepfake artifacts specific to the generator present in the training set.
- **Core assumption:** The teacher model has successfully learned discriminative features for the specific deepfake generator used in the training data, and these features are relevant for the validation set.
- **Evidence anchors:**
  - [abstract] "...performance level when the training and validation data originate from the same DeepFake model."
  - [section IV.C] "...when the testing and training datasets come from the same deepfake generator... KD surpasses the pruning approach..."
  - [corpus] Weak direct support; neighbor papers focus on general detection or audio, not this specific comparison.
- **Break condition:** If the deepfake generator used for testing differs significantly from the training generator (domain shift), pruning may outperform KD (except at 90% compression), as KD appears to overfit the teacher's knowledge of the specific source generator.

### Mechanism 2
- **Claim:** Adapter layers enhance transfer learning performance by enabling efficient adaptation to new datasets, provided they are positioned according to model depth.
- **Mechanism:** Adapters introduce lightweight, trainable modules into a frozen pre-trained network. They allow the model to modify feature extraction for the target domain without destabilizing the pre-learned weights of the backbone.
- **Core assumption:** The pre-trained backbone contains general feature extraction capabilities that are useful for the new target task (deepfake detection) but require minor domain-specific adjustments.
- **Evidence anchors:**
  - [section IV.C] "...adapters only improved feature extraction... placing the adapter at the end of the convolutional layers yielded the best results [for student models]."
  - [section III] Methods (9) and (10) describe the architectural insertion of adapters.
  - [corpus] "Bridging the Gap" mentions compression emulation, but specific adapter placement mechanics are unique to this text.
- **Break condition:** Placing adapters near the beginning or middle layers of shallow/student models, or using linear layers in adapters, results in no performance gain (as noted in Section IV.C).

### Mechanism 3
- **Claim:** Quantization effectively compresses models but its practical utility is hardware-dependent, failing to yield benefits on hardware optimized for floating-point operations (e.g., standard GPUs).
- **Mechanism:** Converting weights from float32 to int8 reduces memory footprint and theoretical computational cost. However, if the hardware (like NVIDIA T4 GPUs used in the study) lacks optimized INT8 acceleration kernels or falls back to dequantization, the inference speed does not improve.
- **Core assumption:** The target deployment hardware has dedicated support for low-precision arithmetic to realize the speedup; otherwise, only memory savings are achieved.
- **Evidence anchors:**
  - [section IV.C] "...quantized DeepFake models cannot benefit from accelerators like GPUs... on Android devices... quantization remains valuable."
  - [section IV.B] "We do not provide compression time for the quantization, as converting from float to int is almost negligible."
  - [corpus] "Pay Less Attention to Deceptive Artifacts" discusses compressed deepfakes but not necessarily the hardware constraints of the detector itself.
- **Break condition:** Deployment on hardware without INT8 support results in a model that is harder to accelerate or may run slower due to lack of kernel optimization, negating the compression benefit for latency-sensitive applications.

## Foundational Learning

- **Concept:** **Domain Generalization in Deepfakes**
  - **Why needed here:** The paper highlights a critical failure mode: models compressed or trained on one generator (e.g., ProGAN) fail to detect others (e.g., StarGAN, Diffusion) effectively. Understanding that deepfake artifacts are often generator-specific is key to interpreting the results.
  - **Quick check question:** Does the paper suggest that compression techniques solve the problem of detecting deepfakes from *unseen* generators?

- **Concept:** **Knowledge Distillation (Teacher-Student)**
  - **Why needed here:** This is the primary compression technique evaluated. You must understand that the "Student" is a smaller network trying to replicate the "Teacher" (the original large model) to see why it might preserve accuracy better than simply cutting weights (pruning).
  - **Quick check question:** In the context of this paper, does the student model learn from the raw dataset or the teacher's outputs?

- **Concept:** **Fine-Tuning vs. Feature Extraction (Frozen Weights)**
  - **Why needed here:** The transfer learning approaches distinguish between fine-tuning existing layers and freezing them while training new "Adapter" layers. This distinction determines the computational cost and adaptability of the model.
  - **Quick check question:** When using Adapters (Method 9 & 10), are the original convolutional layers of the backbone updated?

## Architecture Onboarding

- **Component map:** VGG-based model (~4.5M parameters) -> Compression Modules (Pruning masks, Distillation Loss heads, Dynamic Quantization) -> Transfer Modules (Adapter layers inserted post-convolution or mid-network) -> Data Flow (Source Dataset -> Pre-training -> Compression/Transfer Target Dataset -> Adaptation)

- **Critical path:**
  1. Baseline Training: Establish accuracy on the source dataset (e.g., ProGAN)
  2. Compression Application: Apply Pruning (re-training required) or KD (training student from scratch)
  3. Domain Adaptation: Apply Fine-tuning or insert/train Adapters on target dataset
  4. Quantization: Apply INT8 conversion (preferably as a final step for edge deployment)

- **Design tradeoffs:**
  - KD vs. Pruning: Choose KD for maximum accuracy on *known* generators; choose Pruning for better robustness on *unknown* generators (at moderate compression)
  - Adapters vs. Fine-Tuning: Adapters are faster to train (fewer parameters) but require careful architectural placement; Fine-tuning is more flexible but computationally heavier
  - Quantization: Only use if target hardware (e.g., mobile CPU) supports INT8; avoid if deploying on standard server GPUs where float32 is faster

- **Failure signatures:**
  - Generalization Collapse: High accuracy on ProGAN test set (>90%) but random-guess performance on StarGAN or WhichFaceIsReal
  - Quantization Latency: Model size decreases, but inference time increases or stays flat on GPU
  - Adapter Ineffectiveness: Adapters using linear layers show no gain; or adapters placed in middle layers of shallow models fail to converge

- **First 3 experiments:**
  1. In-Domain Compression Baseline: Train on ProGAN, test on ProGAN at 60%, 80%, 90% compression using both Pruning and KD to replicate the "KD outperforms pruning" result
  2. Cross-Generator Generalization Check: Take the models from Experiment 1 and test on a held-out generator (e.g., StarGAN) to verify the specific domain drop and the relative resilience of pruning
  3. Adapter Position Sweep: Insert adapters into the student model at three positions (Start, Middle, End of Conv layers) and train on the target dataset to validate the "End-of-convolution" placement rule for smaller models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can domain generalization be improved when compressed deepfake detection models encounter generators not seen during training?
- Basis in paper: [explicit] The conclusion states: "there is a significant drop in performance when the evaluation dataset is generated by a different DeepFake generator. This highlights a domain generalization challenge combined with compression in DeepFake detection, which we aim to address in our future research."
- Why unresolved: The paper demonstrates the problem exists (performance drops on StarGAN, WhichFaceIsReal when trained on ProGAN), but does not propose or test solutions.
- What evidence would resolve it: Experiments showing methods (e.g., domain adaptation techniques, diverse training data mixtures, or architectural modifications) that maintain accuracy across unseen generators at high compression rates.

### Open Question 2
- Question: What systematic methodology can determine optimal adapter placement across different model architectures and compression levels?
- Basis in paper: [inferred] The paper states adapters' position "significantly affects performance" with different optimal positions for deeper vs. smaller models, but provides no principled approach for determining placement a priori.
- Why unresolved: Findings are empirical and architecture-specific; no theoretical or systematic framework is offered to predict optimal positioning without extensive experimentation.
- What evidence would resolve it: A reproducible framework or heuristic that predicts optimal adapter placement given model architecture characteristics, validated across multiple architectures.

### Open Question 3
- Question: Can combinations of compression techniques (pruning, KD, quantization, adapters) achieve synergistic benefits beyond individual methods?
- Basis in paper: [inferred] The paper evaluates techniques mostly in isolation or simple combinations, noting low-rank factorization "usually requires combining it with other methods" but does not systematically explore multi-technique combinations.
- Why unresolved: Only pairwise combinations (e.g., TL+P+FT, TL+KD+A) were tested; whether combining three or more techniques yields additive or diminishing returns remains unexplored.
- What evidence would resolve it: Systematic ablation studies across all combinations of compression techniques, measuring accuracy, inference time, and memory footprint trade-offs.

## Limitations

- Domain generalization failure when testing across different deepfake generators despite high in-domain performance
- Hardware-specific quantization benefits limit universal applicability, particularly on GPU-optimized systems
- Single baseline architecture (VGG-based) makes it difficult to assess generalizability across model families

## Confidence

**High Confidence:** Compression techniques maintain accuracy up to 90% when training and validation data share the same generator source (well-controlled experimental setup with consistent results).

**Medium Confidence:** Relative performance comparisons between compression methods (KD > Pruning > Quantization for accuracy), though lack of statistical significance testing reduces absolute confidence.

**Low Confidence:** Domain generalization conclusions, as the paper identifies this as a major limitation but provides limited empirical analysis of why cross-generator performance fails and few actionable solutions.

## Next Checks

1. **Cross-Generator Generalization Test:** Systematically evaluate compressed models across all generator pairs (ProGAN→StarGAN, Diffusion→WhichFaceIsReal, etc.) to quantify the exact performance drop and identify which compression method provides the best worst-case robustness.

2. **Hardware-Specific Quantization Validation:** Deploy quantized models on actual edge devices (mobile CPUs with INT8 support) to verify claimed speedup benefits, comparing against GPU inference results to confirm the hardware dependency assertion.

3. **Adapter Placement Optimization Study:** Conduct a systematic ablation study testing adapter placement at every convolutional layer position across different student model depths to validate the claimed "end-of-convolution" optimization rule and determine if it's architecture-dependent.