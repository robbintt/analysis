---
ver: rpa2
title: Guiding Perception-Reasoning Closer to Human in Blind Image Quality Assessment
arxiv_id: '2512.16484'
source_url: https://arxiv.org/abs/2512.16484
tags:
- image
- reasoning
- quality
- human
- subject
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes a reinforcement learning approach to align a
  multimodal large language model's perception-reasoning process with human judgments
  in blind image quality assessment (BIQA). A new dataset capturing human reasoning
  about image quality was collected, and human annotations were used as reward signals
  to guide the model's perception and reasoning.
---

# Guiding Perception-Reasoning Closer to Human in Blind Image Quality Assessment

## Quick Facts
- **arXiv ID:** 2512.16484
- **Source URL:** https://arxiv.org/abs/2512.16484
- **Reference count:** 40
- **Primary result:** Proposed RL approach aligns MLLM perception-reasoning with human judgments in BIQA, achieving state-of-the-art performance with ROUGE-1 score of 0.512 (vs baseline 0.443) on human-annotated samples

## Executive Summary
This work introduces a reinforcement learning approach to align multimodal large language models' perception-reasoning processes with human judgments in blind image quality assessment (BIQA). The authors collected a new dataset capturing human reasoning about image quality and used human annotations as reward signals to guide the model's perception and reasoning. Additionally, a self-consistency reward was introduced to enable the model to infer quality from its own generated captions. The proposed model achieves state-of-the-art performance on multiple BIQA benchmarks while demonstrating strong alignment with human reasoning, as measured by ROUGE-1 scores, indicating substantial coverage of human explanations and advancing toward interpretable, human-aligned BIQA.

## Method Summary
The method employs Qwen2.5-VL-7B-Instruct with LoRA fine-tuning, trained using GRPO with four composite rewards: reasoning alignment (ROUGE-1 against human annotations), self-consistency (cosine-based reward for caption-only quality prediction), prediction accuracy (cosine-based reward for score prediction), and format compliance. The training involves a dual-stage inference process where the model first generates caption-reasoning-rating from an image, then uses only the generated caption to predict quality without visual input. The approach is trained on 7,058 KonIQ images with 1,495 Q-Reasoning annotations providing human perception-reasoning chains, using 482 overlapping samples for direct supervision.

## Key Results
- Achieves state-of-the-art BIQA performance with AVG PLCC/SRCC of 0.836/0.832 across 7 datasets
- Demonstrates strong human reasoning alignment with ROUGE-1 score of 0.512 (vs baseline 0.443) on 1,000+ human-annotated samples
- Maintains robust performance under caption-only evaluation (PLCC/SRCC of 0.871/0.855 on KonIQ) demonstrating internalized reasoning capability
- Shows 0.512 ROUGE-1 score indicates substantial coverage of human explanations in model-generated perception-reasoning chains

## Why This Works (Mechanism)

### Mechanism 1: Human-Guided Perception-Reasoning Alignment via ROUGE-1 Reward
The model generates `<caption>`, `<reasoning>`, and `<rating>` components. ROUGE-1 computes unigram overlap between model outputs and human annotations, incorporated into GRPO optimization to reinforce reasoning trajectories with higher human alignment. This assumes lexical overlap correlates with semantic alignment of reasoning content, though the paper acknowledges this may not always hold—incorrect reasoning can receive rewards if vocabulary overlaps.

### Mechanism 2: Self-Consistency Reward for Internalized Reasoning
Stage-1 produces caption+reasoning+rating from image. Stage-2 feeds only the generated caption back as input; model must predict quality without seeing the image. A cosine-based reward penalizes deviation between predicted and ground-truth scores, with smooth decay controlled by threshold t. This assumes if a model can accurately predict quality from its caption, it has internalized human-like reasoning criteria rather than relying on direct visual shortcuts.

### Mechanism 3: Joint GRPO Optimization with Composite Rewards
Total reward r_total = r_reasoning + r_self-consistency + r_prediction + r_format. GRPO samples N trajectories per image, normalizes rewards within groups, and reinforces high-advantage trajectories while suppressing low-advantage ones via clipped objective and KL regularization. This assumes the four reward components are not fundamentally conflicting, though the paper notes tension where human alignment sometimes reduces score prediction accuracy.

## Foundational Learning

- **Concept: Reinforcement Learning from Human Feedback (RLHF) for Vision-Language Models**
  - Why needed here: The paper uses RL (GRPO) with human annotations as reward signals rather than pure supervised fine-tuning. Understanding policy gradients, advantage functions, and KL constraints is essential.
  - Quick check question: Can you explain why GRPO normalizes rewards within a group of sampled trajectories rather than using absolute reward values?

- **Concept: Blind Image Quality Assessment (BIQA)**
  - Why needed here: The task assumes no reference image; models must predict quality from perceptual and semantic features alone. Understanding MOS, PLCC, and SRCC metrics is prerequisite.
  - Quick check question: What is the difference between PLCC and SRCC, and why might a model optimize for one at the expense of the other?

- **Concept: ROUGE and Text Generation Evaluation**
  - Why needed here: ROUGE-1 is used both as a training reward and evaluation metric for reasoning alignment. Understanding its limitations (lexical overlap without semantic understanding) is critical.
  - Quick check question: Why might ROUGE-1 give a high score to a model that produces factually incorrect reasoning?

## Architecture Onboarding

- **Component map:** Image + Prompt → Caption + Reasoning + Rating → Caption-only → Rating (with rewards: ROUGE-1, cosine-based, binary)
- **Critical path:** 1) Prepare Q-Reasoning dataset (1,495 images with 8-dimension annotations) 2) Warm-up with baseline RL model 3) Implement 4 reward functions and integrate into GRPO 4) Run dual-stage training (image→caption→reasoning→rating, then caption→rating) 5) Evaluate on 7 datasets under both image-only and caption-only conditions
- **Design tradeoffs:** Reasoning alignment vs. score accuracy (ablation shows these objectives can conflict; weighting matters); Training efficiency vs. self-consistency (dual-stage inference ~doubles training time); Prompt template complexity (detailed templates with many fields can prevent valid reward computation)
- **Failure signatures:** ROUGE-1 increases but PLCC/SRCC decreases → over-fitting to human vocabulary without internalizing quality criteria; Caption-only performance collapses → model relying on visual shortcuts rather than reasoning; Template format errors → reward=0 for many samples, preventing effective policy updates
- **First 3 experiments:** 1) Reproduce baseline (Q-Insight-Score) on KonIQ test split; verify PLCC/SRCC and ROUGE-1 metrics match reported values 2) Ablate self-consistency reward alone; measure impact on caption-only evaluation (expect lower PLCC but similar image-only performance per Table 4) 3) Train with reasoning reward only; compare ROUGE-1 (expect ~0.494 vs 0.512 full model) and verify the trade-off with score prediction accuracy noted in Section 5

## Open Questions the Paper Calls Out

- **Open Question 1:** Can learning objectives be designed to reconcile the divergence where optimizing for human-like textual reasoning does not guarantee improved numerical score prediction accuracy? The authors note that "the optimization directions for human-like reasoning and for numerical score prediction are not perfectly aligned," observing that increased human-alignment sometimes caused score accuracy to decline.

- **Open Question 2:** How can the field develop reasoning-aware evaluation metrics that verify semantic correctness rather than just lexical overlap? The authors state that ROUGE-1 fails to capture semantic discrepancies, sometimes rewarding factually incorrect descriptions (e.g., describing a clear image as "blurry") simply because the vocabulary overlaps with human annotations.

- **Open Question 3:** Can the self-consistency reward mechanism be achieved without the significant training overhead caused by the dual-inference design? The authors list the "additional training overhead due to the dual-inference design" as a limitation that "diverges from the efficiency of human learning."

## Limitations
- ROUGE-1 lexical overlap may not reliably indicate semantic reasoning alignment, as incorrect reasoning can receive rewards through vocabulary overlap
- The Q-Reasoning dataset, critical for both training and evaluation, is not publicly available, making full reproducibility uncertain
- The dual-stage training approach significantly increases computational cost (~2x per epoch) without clear quantification of whether self-consistency gains justify this overhead

## Confidence
- **High confidence**: The composite GRPO framework with multiple reward components is technically sound and the ablation results showing performance degradation when removing components are well-documented
- **Medium confidence**: The claim that ROUGE-1 scores (0.512 vs baseline 0.443) indicate substantial human reasoning alignment, given the known limitations of lexical overlap metrics for semantic understanding
- **Low confidence**: The scalability and generalizability of the approach beyond the specific Q-Reasoning dataset and the long-term stability of the reasoning alignment without further fine-tuning

## Next Checks
1. **Semantic reasoning validation**: Implement a human evaluation study comparing model-generated reasoning chains against ground truth annotations using semantic similarity metrics beyond ROUGE-1 (e.g., BERTScore or human ratings) to verify that lexical overlap correlates with actual reasoning quality
2. **Dataset generalization test**: Train the model on a subset of Q-Reasoning data (e.g., 50%) and evaluate on held-out reasoning annotations to measure how well the approach generalizes across different image quality scenarios and annotator preferences
3. **Trade-off optimization experiment**: Systematically vary the weighting between reasoning and prediction rewards in the composite objective to identify Pareto-optimal configurations that balance human alignment with numerical score accuracy, quantifying the exact performance trade-offs observed