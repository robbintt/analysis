---
ver: rpa2
title: 'Towards Comprehensive Argument Analysis in Education: Dataset, Tasks, and
  Method'
arxiv_id: '2505.12028'
source_url: https://arxiv.org/abs/2505.12028
tags:
- argument
- relations
- relation
- discourse
- argumentative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a fine-grained relation annotation scheme for
  argument mining in education, addressing the limitation of current simplistic argument
  relations. The scheme defines 14 relation types across vertical (stance-based, evidence-based,
  discourse-based) and horizontal (coherence, progression, contrast, concession) dimensions,
  providing a more comprehensive representation of complex argument structures in
  real-world scenarios.
---

# Towards Comprehensive Argument Analysis in Education: Dataset, Tasks, and Method

## Quick Facts
- arXiv ID: 2505.12028
- Source URL: https://arxiv.org/abs/2505.12028
- Reference count: 15
- This paper proposes a fine-grained relation annotation scheme for argument mining in education, addressing the limitation of current simplistic argument relations.

## Executive Summary
This paper addresses the limitation of simplistic support/attack relations in educational argument mining by proposing a comprehensive 14-type relation annotation scheme. The scheme captures both vertical dimensions (stance-based, evidence-based, discourse-based) and horizontal dimensions (coherence, progression, contrast, concession) of argumentative structures. The authors annotated a Chinese essay corpus with these relations and conducted experiments on three tasks: argument component detection, relation prediction, and automated essay grading. Results demonstrate that fine-grained annotations improve model performance, particularly in relation prediction and essay grading tasks, with larger language models showing superior capabilities for argument analysis.

## Method Summary
The authors developed the CEAMC corpus with 226 Chinese high school argumentative essays, annotated with a 14-type relation scheme spanning vertical (stance/evidence/discourse-based) and horizontal (coherence/progression/contrast/concession) dimensions. They conducted experiments on three tasks using both PLMs (BERT, RoBERTa) and LLMs (Qwen2-7B, DeepSeek, ChatGLM) with supervised fine-tuning. Task 1 involved sentence-level argument component detection using IOB tagging. Task 2 required multi-label relation prediction between argument pairs within 15-sentence distance. Task 3 was automated essay grading with optional annotation augmentation. Models were trained with AdamW (PLMs) or LoRA (LLMs) on NVIDIA RTX 3090.

## Key Results
- Fine-grained relation annotations improve model performance in relation prediction and essay grading tasks
- Supervised fine-tuning of LLMs substantially outperforms zero-shot and few-shot prompting for argument analysis
- Writing quality affects argument component detection and relation prediction asymmetrically—high-quality essays improve detection but challenge relation prediction

## Why This Works (Mechanism)

### Mechanism 1
Fine-grained relation annotations improve performance on relation prediction and essay grading tasks compared to simplistic support/attack schemas. The 14-type scheme captures argument strategies and patterns that binary relations miss, providing downstream classifiers with structurally meaningful features.

### Mechanism 2
Supervised fine-tuning (SFT) of LLMs substantially outperforms zero-shot and few-shot prompting for argument analysis tasks. Domain-specific SFT aligns model behavior to the task-specific annotation schema, preventing LLMs from applying overly broad relation definitions.

### Mechanism 3
Writing quality affects argument component detection and relation prediction asymmetrically—high-quality essays have clearer component boundaries but employ more diverse and complex argument structures. Low-quality essays use simpler parallel logic, making relations easier to model.

## Foundational Learning

- **Argument Component Taxonomy (Major Claim, Claim, Evidence Types, Elaboration)**: Component detection is the prerequisite task; relations are annotated between typed components. Quick check: In "Life needs a sense of ceremony because ceremony can combat mediocrity," which part is the claim and which is the elaboration?

- **Discourse Relations (RST/PDTB foundations)**: The horizontal dimension draws directly from discourse theory. Understanding these distinctions is essential for annotating and predicting logical flow. Quick check: What distinguishes "Contrast" (comparing two positions) from "Concession" (acknowledging one position before asserting another)?

- **Multi-label Classification with Negative Sampling**: Relation prediction is multi-label, and negative samples vastly outnumber positives (~20K negatives vs. ~4K relations). Proper sampling strategy determines model success. Quick check: Why would setting negative samples too high hurt both LLMs and PLMs in relation prediction?

## Architecture Onboarding

- **Component map**: Sentence segmentation → Argument unit boundary detection → Component classification per unit → Argument pair construction (distance ≤15) → Negative sampling → Relation prediction → Optional annotation injection for grading

- **Critical path**: 1) Sentence segmentation → Argument unit boundary detection (foundation for all tasks) 2) Component classification per unit 3) Argument pair construction (distance ≤15 covers 99% positives) 4) Negative sampling for relation prediction (ratio differs by model class) 5) Optional: Inject annotation features into essay grading input

- **Design tradeoffs**: Negative sampling ratio peaks at 3 negatives/argument for ChatGLM but degrades RoBERTa; annotation integration improves essay grading QWK but adds complexity; larger models (9B) outperform smaller (7B) but limited data may underutilize capacity

- **Failure signatures**: GPT-4 over-predicting "related" pairs (classifying negatives as positives); sharp performance drop when negative sampling ratio is too high for PLMs; LLMs systematically under-scoring essays; 1-shot prompting degrading vs. 0-shot

- **First 3 experiments**: 1) Run RoBERTa on component detection → target F1 ~48-49% to validate setup 2) Test negative sampling ratios {1, 3, 5, 7} for relation prediction on both RoBERTa and 7B LLM 3) Compare essay grading (QWK) with vs. without injected component/relation features

## Open Questions the Paper Calls Out

1. How can fine-grained argumentative information be optimally integrated to unlock the potential of LLMs in automated essay grading? (Basis: ChatGLM scores dropped with annotation addition)

2. To what extent does expanding the dataset size beyond 226 essays improve model performance and generalization for the proposed fine-grained relation scheme? (Basis: Dataset "remains limited in size" and "might limit the performance and generalization")

3. What characteristics define "high-quality samples" for prompt engineering that can stabilize LLM performance in argument mining tasks? (Basis: GPT-4 showed instability—performance dropped in 1-shot vs. 0-shot)

4. Can the proposed annotation scheme, which relies on Chinese discourse frameworks, effectively generalize to argument mining in other languages or educational contexts? (Basis: Scheme grounded in Chinese discourse theory, not tested cross-lingually)

## Limitations

- Dataset size (226 essays) limits generalizability, particularly for LLM experiments where larger models may be under-trained
- Chinese-language focus restricts applicability to other educational contexts without additional validation
- Annotation scheme may still miss certain argument patterns common in real-world essays
- Reliance on sentence-level argument units may fragment complex multi-sentence arguments

## Confidence

- High confidence: Task 1 component detection results and Task 3 essay grading performance metrics are well-supported by empirical evidence
- Medium confidence: Task 2 relation prediction results show some inconsistency in comparative performance between LLMs and PLMs
- Medium confidence: The mechanism explaining asymmetric effects of writing quality is primarily supported by exploratory analysis rather than direct experimental validation

## Next Checks

1. Replicate the negative sampling ablation study across all model types to confirm optimal ratios and investigate why different model classes respond differently to negative sample density

2. Conduct cross-linguistic validation by applying the annotation scheme to argumentative essays in other languages to assess generalizability beyond Chinese

3. Perform ablation studies removing individual relation types (particularly horizontal dimensions) to quantify their specific contribution to downstream task performance