---
ver: rpa2
title: 'Resolving Editing-Unlearning Conflicts: A Knowledge Codebook Framework for
  Large Language Model Updating'
arxiv_id: '2502.00158'
source_url: https://arxiv.org/abs/2502.00158
tags:
- knowledge
- editing
- unlearning
- updating
- uni00000013
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of updating large language models
  (LLMs) with new knowledge while simultaneously removing outdated or unwanted information.
  The core issue lies in the conflict between unlearning and editing tasks, as well
  as ineffective knowledge storage methods that lead to overfitting or underfitting.
---

# Resolving Editing-Unlearning Conflicts: A Knowledge Codebook Framework for Large Language Model Updating

## Quick Facts
- arXiv ID: 2502.00158
- Source URL: https://arxiv.org/abs/2502.00158
- Authors: Binchi Zhang; Zhengzhang Chen; Zaiyi Zheng; Jundong Li; Haifeng Chen
- Reference count: 40
- Primary result: LOKA achieves 0.8044 unlearning truth ratio on TOFU out-profile dataset, outperforming existing methods while preserving remaining knowledge

## Executive Summary
This paper addresses the challenge of updating large language models with new knowledge while simultaneously removing outdated or unwanted information. The core issue lies in the conflict between unlearning and editing tasks, as well as ineffective knowledge storage methods that lead to overfitting or underfitting. To resolve these issues, the authors propose LOKA, a knowledge codebook framework that uses task-specific and multi-task memories to handle conflicts and employs similarity-aware knowledge mapping with a learning-based router for efficient knowledge allocation and retrieval. Extensive experiments on three benchmarks demonstrate LOKA's effectiveness, showing significant improvements in unlearning and editing tasks while preserving remaining knowledge.

## Method Summary
LOKA resolves LLM updating conflicts through a knowledge codebook framework with task-specific and multi-task memories guided by gradient conflict scores. The framework initializes codebooks from target FFN layer weights, clusters knowledge embeddings via Kmeans for similarity-aware allocation, and employs a learning-based router with confidence thresholding to control codebook activation. During training, conflict scores between editing and unlearning gradients determine whether separate memories or a shared memory with MGDA optimization are used. The router fine-tunes a BERT classifier to distinguish updated from retained knowledge, activating the codebook only when confidence exceeds a threshold.

## Key Results
- LOKA achieves 0.8044 unlearning truth ratio on TOFU out-profile dataset, significantly outperforming existing methods
- The framework maintains strong performance in sequential knowledge updating scenarios while preserving remaining knowledge
- Ablation studies show the effectiveness of conflict-aware training and similarity-aware knowledge mapping in resolving editing-unlearning conflicts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Separating task-specific and multi-task memories based on gradient conflict scores resolves editing-unlearning optimization conflicts.
- Mechanism: The framework computes cosine similarity between editing gradients (∇L_e) and unlearning gradients (∇L_u). When negative conflict scores exceed a threshold (indicating heavy conflict), separate memories are trained for each task; otherwise, a shared memory uses Pareto-optimal multi-task learning via MGDA.
- Core assumption: Lipschitz continuity of log P_W(x_k+1|x_0:k) holds for transformer modules (FFNs and attention), enabling the theoretical conflict condition in Theorem 2.3.
- Evidence anchors:
  - [abstract] "LOKA resolves task conflicts by employing task-specific and multi-task memories guided by a conflict score."
  - [section 3.2] Detailed conflict score calculation using "cosine similarity between the task gradients of editing and unlearning"
  - [corpus] Weak direct evidence; neighbor papers discuss knowledge conflicts in RAG and tool-augmented LLMs but not editing-unlearning specifically.
- Break condition: If editing and unlearning datasets have minimal overlap (low d_TV), conflicts may not materialize and the complexity of task-specific memories becomes unnecessary overhead.

### Mechanism 2
- Claim: Similarity-aware knowledge mapping via clustering groups related knowledge pieces, mitigating underfitting seen in dense storage approaches.
- Mechanism: Extract last-token embeddings from the LLM layer preceding the target FFN, cluster using Kmeans, and allocate each cluster to a dedicated memory. This ensures semantically related knowledge updates co-locate, enabling more efficient parameter utilization than either sparse (one-piece-per-memory) or dense (all-in-one) strategies.
- Core assumption: Embedding similarity correlates with knowledge relatedness such that clustered updates benefit from shared parameter updates.
- Evidence anchors:
  - [abstract] "similarity-aware knowledge mapping ensures that related knowledge pieces are clustered and allocated to the same memory"
  - [section 3.1] "We instantiate knowledge mapping with Kmeans... which allows for unsupervised processing and ensures balanced and efficient utilization"
  - [corpus] No direct corpus support for this specific clustering-based allocation mechanism.
- Break condition: If knowledge pieces are semantically dissimilar but functionally related (e.g., different aspects of the same entity requiring coordinated updates), clustering by embedding similarity may fragment logically connected updates.

### Mechanism 3
- Claim: A learning-based router with confidence thresholding selectively activates the codebook, reducing overfitting from unnecessary memory retrieval.
- Mechanism: Fine-tune a BERT classifier on positive samples (X_e ∪ X_u) and negative samples (X_r). During inference, only activate codebook if classifier confidence exceeds a threshold. This prevents sparse-storage overfitting where paraphrased inputs fall outside memory radii.
- Core assumption: The classifier generalizes to distinguish updated-knowledge prompts from unseen retained-knowledge prompts.
- Evidence anchors:
  - [abstract] "A learning-based router controls codebook activation to further improve knowledge utilization"
  - [section 3.1] "we introduce a confidence threshold for the classifier... filter out a larger proportion of irrelevant prompts"
  - [corpus] Weak; neighbor papers address retrieval conflicts but not router-based selective activation.
- Break condition: If updated knowledge semantically overlaps significantly with retained knowledge, the classifier may suffer high false-positive or false-negative rates, degrading either preservation or update efficacy.

## Foundational Learning

- Concept: **Gradient conflict in multi-task learning** (negative cosine similarity between task gradients)
  - Why needed here: The entire conflict-resolution mechanism presupposes understanding why jointly optimizing L_e and L_u can harm both tasks.
  - Quick check question: Given two loss gradients ∇L_1 = [1, 0] and ∇L_2 = [-0.5, 0.8], do they conflict? (Answer: Yes, cosine similarity is negative.)

- Concept: **Pareto optimality and MGDA** (Multiple Gradient Descent Algorithm)
  - Why needed here: Multi-task memories use MGDA to find weights α_e, α_u that minimize combined gradient norm under sum-to-1 constraint.
  - Quick check question: Why does MGDA prefer the direction with smallest gradient norm rather than simple averaging? (Answer: Averaging can follow directions that increase one task's loss.)

- Concept: **Locality-Sensitive Hashing (LSH)** for incremental clustering
  - Why needed here: Sequential updating requires stable knowledge mapping; LSH provides fixed hash-based allocation that doesn't change when new knowledge arrives.
  - Quick check question: How does LSH maintain consistent cluster assignments for existing data when new points are added? (Answer: Hash function is fixed; new points map to existing buckets without reassigning old points.)

## Architecture Onboarding

- Component map:
  - Input -> Router (activate?) -> If yes, extract embedding -> Similarity mapping -> Retrieve memory -> Plug into FFN -> Generate output

- Critical path: Input → Router (activate?) → If yes, extract embedding → Similarity mapping → Retrieve memory → Plug into FFN → Generate output

- Design tradeoffs:
  - **Kmeans vs. LSH mapping**: Kmeans gives better clustering quality but requires full re-clustering for sequential updates; LSH enables incremental updates with slightly worse editing performance (Table 13: edt-rl 0.8148 vs. 0.9435)
  - **Conflict threshold tuning**: Higher threshold → fewer task-specific memories → faster training but potential performance degradation (Figure 6a)
  - **Cluster count**: More clusters benefit editing but can hurt unlearning after ~10 clusters (Figure 6c)

- Failure signatures:
  - High paraphrased-editing ROUGE drop → overfitting from sparse allocation; check if router is activating memories for paraphrased inputs
  - Low unlearning truth ratio with high retained ROUGE → likely using multi-task memory when task-specific is needed; check conflict threshold
  - Sequential update degradation → Kmeans re-clustering shifted prior knowledge assignments; switch to LSH mapping

- First 3 experiments:
  1. **Ablation on conflict threshold**: Run LOKA on TOFU out-profile with conflict thresholds {0.1, 0.3, 0.5, 0.7} and plot unl-tr vs. edt-rl to find optimal balance point.
  2. **Memory count sweep**: Test cluster counts {5, 10, 20, 40} on in-profile updating to identify the crossover where unlearning performance begins degrading.
  3. **Router confidence calibration**: Evaluate false-positive rate (codebook activated for retained data) vs. false-negative rate (codebook not activated for updated data) across confidence thresholds to set task-specific thresholds.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the learning-based router be enhanced to distinguish updated knowledge from remaining knowledge when their data distributions are highly similar?
- Basis in paper: [explicit] Appendix C.1 states that in the ZsRE benchmark, the "similar distribution of the remaining set and updating sets compromises the effectiveness of the learning-based router," leading to suboptimal preservation of remaining knowledge.
- Why unresolved: The current router relies on a BERT classifier fine-tuned on distinct sets, which struggles when the semantic gap between "updated" and "remaining" data is narrow (e.g., short factual answers vs. long text).
- What evidence would resolve it: Demonstration of a routing mechanism (e.g., few-shot prompting or density-based detection) that maintains high F1 scores on remaining knowledge in ZsRE without sacrificing unlearning/editing efficacy.

### Open Question 2
- Question: Can a dynamic mechanism be developed to optimize the trade-off between the utility of Kmeans-based mapping and the efficiency of LSH-based mapping in sequential updating?
- Basis in paper: [explicit] Appendix C.3 concludes that "LOKA-Kmeans" (better utility, slower) and "LOKA-LSH" (faster, lower utility) present a "tradeoff between efficiency and utility" that "can be achieved based on the requirements of the specific... scenario."
- Why unresolved: The framework currently treats these as binary implementation choices based on user needs, rather than offering an automated method to switch or merge them during inference.
- What evidence would resolve it: An adaptive routing strategy that dynamically selects the mapping method based on query complexity or latency constraints, achieving median latency and accuracy metrics between the two static extremes.

### Open Question 3
- Question: How does the fixed size of the knowledge codebook impact performance in lifelong learning scenarios with unlimited sequential updates?
- Basis in paper: [inferred] The paper mentions in Appendix B.4 that the cluster number (codebook size) is fixed to 20. However, Section 3.3 notes that for high-frequency updates, adding new codebooks is resource-intensive, and LSH is used to avoid re-clustering.
- Why unresolved: It is unclear if a static codebook capacity leads to "capacity saturation" or memory interference as the volume of edited/unlearned knowledge grows significantly over time.
- What evidence would resolve it: Experiments measuring editing accuracy and unlearning truth ratios over 100+ sequential update steps to observe if performance degrades as the fixed number of memory slots is forced to store increasingly dense or conflicting information.

## Limitations
- The theoretical foundation relies on assumptions about Lipschitz continuity of transformer modules that are not empirically verified
- Performance depends heavily on hyperparameters (conflict threshold, cluster count, router confidence threshold) that may not generalize across domains
- Kmeans clustering creates sequential updating limitations, requiring either expensive re-clustering or performance trade-offs with LSH
- Memory expansion mechanism is simplistic and may not scale well to very large knowledge bases

## Confidence

- **High confidence**: The overall architecture design and experimental results showing LOKA outperforms baselines on TOFU, PKU-SafeRLHF, and ZsRE benchmarks. The ablation studies on conflict threshold and cluster count are well-executed and demonstrate the importance of these design choices.
- **Medium confidence**: The theoretical analysis of gradient conflicts and the MGDA optimization approach. While the mathematical framework is sound, the assumptions about Lipschitz continuity and the practical effectiveness of the conflict detection mechanism need more rigorous validation.
- **Low confidence**: The router-based activation mechanism and its generalization across domains. The BERT classifier's performance on distinguishing updated from retained knowledge is crucial but not thoroughly validated, particularly for cases where updated and retained knowledge overlap semantically.

## Next Checks

1. **Cross-domain generalization test**: Evaluate LOKA on a completely different domain (e.g., biomedical knowledge updates) to assess whether the conflict threshold and clustering parameters tuned on TOFU/PKU-SafeRLHF transfer effectively.

2. **Router failure analysis**: Systematically measure false positive and false negative rates of the BERT router across different confidence thresholds on held-out data to identify the optimal operating point and understand failure modes.

3. **Memory scaling experiment**: Test LOKA with increasing knowledge base sizes (10x, 100x the current scale) to evaluate memory expansion efficiency and identify practical limits of the codebook framework.