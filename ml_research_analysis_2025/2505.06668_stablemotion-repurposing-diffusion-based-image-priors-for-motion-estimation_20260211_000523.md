---
ver: rpa2
title: 'StableMotion: Repurposing Diffusion-Based Image Priors for Motion Estimation'
arxiv_id: '2505.06668'
source_url: https://arxiv.org/abs/2505.06668
tags:
- diffusion
- image
- ours
- heatmap
- steps
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: StableMotion leverages pre-trained diffusion model priors for motion
  estimation in image rectification tasks. It repurposes Stable Diffusion by adapting
  its VAE for flow refinement and UNet for motion estimation, using conditional and
  perceptual losses during training.
---

# StableMotion: Repurposing Diffusion-Based Image Priors for Motion Estimation

## Quick Facts
- **arXiv ID:** 2505.06668
- **Source URL:** https://arxiv.org/abs/2505.06668
- **Reference count:** 40
- **Primary result:** 25.50 PSNR on RS-Real and 23.06 PSNR on DIR-D for motion estimation

## Executive Summary
StableMotion introduces a novel approach to single-image rectification by repurposing pre-trained diffusion model priors for motion estimation. The method adapts Stable Diffusion 2.0's architecture, specifically its VAE and UNet components, to predict flow fields that warp input images into corrected versions. By explaining the Sampling Steps Disaster phenomenon and introducing an Adaptive Ensemble Strategy, the framework achieves state-of-the-art results while enabling one-step inference that is 200x faster than previous diffusion-based methods.

## Method Summary
StableMotion transforms Stable Diffusion 2.0 into a motion estimator for image rectification tasks. The approach modifies the UNet's first layer to accept concatenated condition latents and noisy flow latents, then trains with a combined loss function including diffusion reconstruction, condition warping, and perceptual components. The method introduces the Sampling Steps Disaster phenomenon, explaining why multi-step inference degrades performance in conditional diffusion models, and proposes an Adaptive Ensemble Strategy to address output inconsistencies. The framework is evaluated on stitched image rectangling and rolling shutter correction, achieving superior performance metrics while maintaining rapid inference speeds.

## Key Results
- Achieves 25.50 PSNR on RS-Real dataset and 23.06 PSNR on DIR-D dataset
- Outperforms previous diffusion-based methods by 200x in inference speed through one-step DDIM sampling
- Demonstrates Sampling Steps Disaster phenomenon where multi-step inference degrades performance in conditional settings

## Why This Works (Mechanism)
The method works by leveraging the rich generative priors learned by diffusion models while adapting them for motion estimation. The key insight is that pre-trained diffusion models contain powerful image priors that can be repurposed through architectural modifications and appropriate loss functions. The UNet's ability to denoise in latent space, combined with perceptual and condition losses, enables accurate flow prediction. The Sampling Steps Disaster phenomenon explains why conditional diffusion models fail with multiple sampling steps due to error accumulation when balancing multiple objectives, making single-step inference both faster and more accurate.

## Foundational Learning
- **Diffusion Model Fundamentals**: Understanding how diffusion models learn to reverse noise through a learned denoising process is essential for adapting them to new tasks. Quick check: Can you explain the forward and reverse processes in diffusion models?
- **Flow Field Normalization**: Normalizing flow predictions is critical for training stability and numerical precision. Quick check: Why is dividing flow by a normalization factor important during training?
- **Perceptual Loss Application**: Using VGG-based perceptual loss helps preserve image structure during warping. Quick check: What specific image features does perceptual loss typically capture?
- **Conditional Diffusion Training**: Training diffusion models with conditional information requires careful loss formulation. Quick check: How does adding condition loss differ from standard diffusion training?
- **VAE Latent Space Utilization**: Repurposing the VAE encoder/decoder for motion fields requires understanding latent space compression effects. Quick check: What are the trade-offs of using VAE latents versus pixel space for flow representation?
- **Sampling Strategies**: Understanding DDIM versus DDPM sampling is crucial for inference efficiency. Quick check: What is the key difference between DDIM and DDPM sampling approaches?

## Architecture Onboarding

**Component Map:** Dataset → DataLoader → VAE Encoder → UNet (Modified) → VAE Decoder → Flow Prediction → Warping → Output Image

**Critical Path:** The core pipeline involves loading conditions and pseudo flows, encoding through the modified VAE, denoising via the adapted UNet in latent space, decoding to obtain flow fields, and finally warping the input image to produce the rectified output.

**Design Tradeoffs:** The framework trades potential flow precision for inference speed by using one-step sampling instead of multi-step approaches. The choice to repurpose Stable Diffusion's VAE, optimized for natural images, may limit flow field fidelity compared to specialized flow architectures. The Adaptive Ensemble Strategy improves output consistency but requires multiple inference passes.

**Failure Signatures:** Training instability occurs when using incorrect flow normalization factors or when attempting multi-step inference (Sampling Steps Disaster). Poor performance results from using image-to-image fine-tuning instead of the image-to-motion formulation. Output artifacts often stem from the VAE's lossy compression of flow fields in latent space.

**First Experiments:**
1. Implement the modified UNet layer with weight replication and verify the 1/(N+1) scaling
2. Test single-step versus multi-step inference to confirm Sampling Steps Disaster
3. Evaluate the impact of different VGG-16 layer selections on perceptual loss performance

## Open Questions the Paper Calls Out

### Open Question 1
Can the Sampling Steps Disaster (SSD) phenomenon be mathematically generalized to other conditional diffusion frameworks and tasks beyond image rectification?
The authors state in the Conclusion that SSD "has the potential for broader applications in other diffusion-related tasks," and the appendix suggests the phenomenon appears in other works like MDM. While the paper provides a theoretical derivation for StableMotion, the empirical validation is limited to image rectangling and rolling shutter correction; it remains unconfirmed if the "error accumulation" theory holds for standard generative tasks or latent space models with different noise schedulers.

### Open Question 2
Can the image-to-motion framework be extended to generic optical flow estimation where motion magnitudes are significantly larger and discontinuities are more frequent than in rectification tasks?
The paper frames StableMotion as a general "motion estimator" and notes the ill-posed nature of single-image tasks, but only validates it on specific rectification flows which often imply global or mesh-based smoothness. The method relies on a repurposed image VAE and perceptual losses which favor natural image structures; it is unclear if these priors can handle the large displacements and sharp object boundaries characteristic of general optical flow without specific flow-oriented architectural changes.

### Open Question 3
Can the intrinsic output inconsistency of the diffusion model be resolved architecturally to remove the need for the multi-pass Adaptive Ensemble Strategy (AES)?
The authors introduce AES to "mitigate inconsistent output" and aggregate results, which inherently requires running inference multiple times (Table 5 shows improvements up to 8 ensembles). While AES improves quality, it linearly increases inference time for each ensemble member, partially offsetting the 200x speedup gained from one-step inference.

### Open Question 4
To what extent does the Stable Diffusion VAE, optimized for natural image reconstruction, limit the precision of the estimated motion fields due to compression artifacts in the latent space?
The method "repurposes" the SD VAE to encode flow fields, but VAEs are generally lossy and prone to blurring high-frequency details; the paper notes that "inaccurate pseudo labels" and distortions require specific loss constraints. The paper attributes remaining distortions mostly to the generative nature or pseudo-labels, but does not analyze if the latent space bottleneck itself creates irrecoverable errors in the motion vectors.

## Limitations
- The exact flow normalization factor γ is unspecified, which is critical for training stability
- The pseudo flow generation pipeline is not detailed, though necessary for training
- The method relies on the VAE's natural image priors, which may limit precision for complex motion patterns
- Multi-pass AES improves quality but partially offsets the speed advantages of one-step inference

## Confidence
- **High Confidence:** Method's core innovation (SSD explanation, AES strategy), PSNR results on RS-Real and DIR-D, inference speedup claims (200x)
- **Medium Confidence:** Training procedure details (loss formulations, hyperparameters), dataset preparation assumptions
- **Low Confidence:** Exact implementation details requiring unspecified parameters (γ value, VGG layer choices)

## Next Checks
1. Verify γ normalization/denormalization values through ablation studies on training stability
2. Reconstruct pseudo flow generation pipeline using standard optical flow methods to validate training data quality
3. Test multiple VGG-16 layer combinations for perceptual loss to identify optimal configuration