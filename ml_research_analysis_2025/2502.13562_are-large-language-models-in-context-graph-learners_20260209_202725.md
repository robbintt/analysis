---
ver: rpa2
title: Are Large Language Models In-Context Graph Learners?
arxiv_id: '2502.13562'
source_url: https://arxiv.org/abs/2502.13562
tags:
- graph
- learning
- llms
- label
- in-context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper establishes a connection between graph neural networks\
  \ (GNNs) and retrieval-augmented generation (RAG), conceptualizing message-passing\
  \ in GNNs as a recursive RAG step. Building on this insight, the authors propose\
  \ three RAG frameworks\u2014QUERY RAG, LABEL RAG, and FEWSHOT RAG\u2014that leverage\
  \ graph structure as inherent context to enhance large language models' (LLMs) in-context\
  \ learning capabilities for graph tasks."
---

# Are Large Language Models In-Context Graph Learners?

## Quick Facts
- arXiv ID: 2502.13562
- Source URL: https://arxiv.org/abs/2502.13562
- Reference count: 27
- Primary result: LLMs with graph-guided RAG frameworks significantly outperform standard in-context learning and match or exceed supervised GNNs on text-attributed graph classification tasks.

## Executive Summary
This paper establishes a theoretical connection between graph neural networks (GNNs) and retrieval-augmented generation (RAG), conceptualizing GNN message-passing as recursive RAG steps. Building on this insight, the authors propose three RAG frameworks—QUERY RAG, LABEL RAG, and FEWSHOT RAG—that leverage graph structure as inherent context to enhance large language models' in-context learning capabilities for graph tasks. These frameworks use local graph neighborhoods to retrieve relevant query, label, or both types of information as context for LLMs. Experiments on eight text-attributed graph datasets show that LLMs augmented with these frameworks significantly outperform standard in-context learning approaches and can match or exceed the performance of supervised GNNs and fine-tuned graph LLMs, particularly when using the FEWSHOT RAG variant.

## Method Summary
The authors propose three RAG frameworks that retrieve context from graph neighborhoods for in-context learning: QUERY RAG retrieves neighbor texts, LABEL RAG retrieves neighbor labels, and FEWSHOT RAG retrieves both text and labels formatted as few-shot examples. The frameworks operate on text-attributed graphs, using 1-hop neighbors as context without fine-tuning the LLMs. The method is evaluated on eight datasets (Cora, Pubmed, Sports-Fitness, Ele-History, Ele-Computers, Books-Children, Books-History, Ogbn-Arxiv-TA) using six different LLMs ranging from 3.8B to 67B parameters. Performance is measured via classification accuracy with 3-10 runs per experiment depending on model type.

## Key Results
- LLMs with graph-guided RAG frameworks significantly outperform standard in-context learning on all eight text-attributed graph datasets
- FEWSHOT RAG with DeepSeek-V3 achieves 90.2% accuracy on Fitness dataset, surpassing supervised GNN baselines (RevGAT: 91.6%)
- Graph-based retrieval consistently outperforms text-based and random retrieval across all three RAG variants
- Optimal performance achieved with ~3 neighbors, with diminishing returns beyond this point due to context length limitations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** GNN message-passing can be conceptualized as a recursive RAG process, where local neighborhood aggregation parallels context retrieval.
- **Mechanism:** Both GNNs and RAG leverage contextual information beyond raw input—GNNs aggregate from structural neighbors via message-passing, while RAG retrieves from an external corpus. The paper shows these share similar formulations: GNN's `UPD(xi, AGG(xj : vj ∈ N(vi)))` parallels RAG's `GEN(qi, AUG(cj : cj ∈ D(qi)))`.
- **Core assumption:** The structural relationships in graphs encode contextual information functionally equivalent to retrieved documents in traditional RAG.
- **Evidence anchors:**
  - [abstract] "learning on graph data can be conceptualized as a retrieval-augmented generation (RAG) process, where specific instances (e.g., nodes or edges) act as queries, and the graph itself serves as the retrieved context"
  - [section 3.2] Eqs. (4) and (5) formally show the parallel between RAG and GNN formulations
  - [corpus] Weak direct validation; related work "CausalRAG" integrates causal graphs into RAG but doesn't validate this specific GNN-RAG equivalence
- **Break condition:** If graph structure lacks homophily (neighbors have dissimilar labels/attributes), the neighborhood-as-context assumption weakens significantly (acknowledged in Limitations section).

### Mechanism 2
- **Claim:** Incorporating label information from graph neighbors (LABEL RAG) provides stronger learning signal than query content alone.
- **Mechanism:** LABEL RAG extends label propagation principles to RAG by embedding neighbor labels directly into prompts: `ri = GEN(qi, AUG({yj or rj : vj ∈ N(vi)}))`. This leverages the homophily assumption that connected nodes share similar labels.
- **Core assumption:** Neighboring nodes' ground-truth labels are accessible during inference and provide reliable contextual guidance.
- **Evidence anchors:**
  - [section 3.3] "LABEL RAG extends the concept of label propagation to RAG learning" with formalization in Eq. (8)
  - [table 1] LABEL RAG consistently shows substantial gains over QUERY RAG (e.g., Cora: 80.6% vs 60.9% with LLaMA)
  - [corpus] No direct corpus validation; related "Leveraging Graph RAG" paper explores graph structure in RAG but not label propagation specifically
- **Break condition:** In heterophilic graphs or when neighbor labels are noisy/unavailable, this mechanism degrades substantially.

### Mechanism 3
- **Claim:** Combining query and label context from neighbors (FEWSHOT RAG) can match or exceed supervised GNNs without fine-tuning.
- **Mechanism:** FEWSHOT RAG constructs in-context examples by retrieving (query, label) tuples from neighbors: `ri = GEN(qi, AUG({(qj, yj) : vj ∈ N(vi)}))`. This mimics few-shot learning but uses graph structure to select demonstrations rather than random or similarity-based selection.
- **Core assumption:** LLMs can effectively parse and utilize structured relational context when formatted as few-shot examples; graph proximity is a better selector than semantic similarity alone.
- **Evidence anchors:**
  - [section 3.3] Eq. (9) formalizes FEWSHOT RAG as combining QUERY and LABEL RAG principles
  - [table 1] FEWSHOT RAG with DeepSeek-V3 achieves 90.2% on Fitness, surpassing supervised GNN baselines (RevGAT: 91.6%)
  - [figure 7] Graph-based retrieval consistently outperforms text-based and random retrieval across all three RAG variants
  - [corpus] Related work "Large Language Models are Good Relational Learners" supports LLMs' capability for relational reasoning but doesn't validate this specific approach
- **Break condition:** Context length limits—excessive neighbors introduce noise and hit token limits; Figure 8 shows performance plateaus or declines beyond ~3 neighbors.

## Foundational Learning

- **Concept: Message-passing in Graph Neural Networks**
  - **Why needed here:** The entire paper's theoretical contribution rests on mapping GNN message-passing (AGGREGATE-UPDATE) to RAG's retrieve-generate pipeline. Without understanding how GNNs iteratively aggregate neighbor information, the core analogy fails.
  - **Quick check question:** Can you explain how a 2-layer GCN updates a node's representation using its 2-hop neighborhood?

- **Concept: In-Context Learning (ICL) Paradigm**
  - **Why needed here:** The proposed frameworks operate entirely within ICL—no weight updates. Understanding how demonstrations in prompts enable task adaptation without training is essential to grasp why this approach works at all.
  - **Quick check question:** What is the difference between zero-shot, one-shot, and few-shot in-context learning, and why might more examples not always improve performance?

- **Concept: Label Propagation and Homophily**
  - **Why needed here:** LABEL RAG directly applies label propagation principles. The assumption that connected nodes share labels (homophily) underpins why neighbor labels provide useful context.
  - **Quick check question:** In a citation network where papers cite opposing viewpoints (heterophily), would label propagation from neighbors help or hurt classification?

## Architecture Onboarding

- **Component map:** Query Node (vi) → Retrieve Neighbors N(vi) → Construct Prompt (Table 3 templates) → LLM → Predicted Label

- **Critical path:**
  1. Load text-attributed graph (node texts + edge list)
  2. For each test node, extract one-hop neighbors
  3. Retrieve neighbor attributes (query text, labels, or both) based on RAG variant
  4. Format into prompt using templates in Table 3
  5. Pass to off-the-shelf LLM (no fine-tuning)
  6. Parse output as predicted class

- **Design tradeoffs:**
  - **QUERY vs LABEL vs FEWSHOT RAG:** LABEL RAG most parameter-efficient (only labels); FEWSHOT RAG highest performance but longest prompts
  - **Graph-based vs text-based retrieval:** Paper shows graph structure outperforms semantic similarity (Figure 7), but requires graph availability
  - **Number of neighbors:** More neighbors = more context but diminishing returns and token limits (Figure 8 shows optimal at ~3)

- **Failure signatures:**
  - Performance drops on heterophilic graphs (homophily assumption violated)
  - Long-context degradation when neighbor count exceeds ~5 (LLM loses relevant signal in noise)
  - Random or text-based retrieval underperforms graph-based retrieval significantly
  - One-shot/few-shot can perform worse than zero-shot if demonstrations are misaligned (Figure 4)

- **First 3 experiments:**
  1. **Baseline reproduction:** Run zero-shot, few-shot, and vanilla RAG on Cora/Pubmed with LLaMA-3.1-8B to confirm the performance gap (Figure 4 patterns)
  2. **Ablation on retrieval mechanism:** Compare graph-based vs text-based vs random neighbor selection for LABEL RAG (replicate Figure 7 finding)
  3. **Neighbor count sweep:** Test K=1,2,3,4,5 neighbors for FEWSHOT RAG to identify optimal context window before saturation (replicate Figure 8)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can graph-guided RAG frameworks be extended to effectively handle heterophilic graphs, where connected nodes tend to have dissimilar attributes or labels?
- **Basis in paper:** [explicit] The authors state in the Limitations section: "our approach assumes homophily in the graph structure, which may not generalize well to heterophilic graphs, where nodes with dissimilar attributes or labels are more likely to be connected."
- **Why unresolved:** The current frameworks rely on the homophily assumption—that neighboring nodes share relevant contextual information. In heterophilic settings, retrieving context from graph neighbors may introduce misleading signals.
- **What evidence would resolve it:** Experiments on established heterophilic graph benchmarks (e.g., Actor, Chameleon, Squirrel) showing whether graph-guided retrieval maintains advantages over standard RAG or requires modified retrieval strategies.

### Open Question 2
- **Question:** How can multi-hop graph context be incorporated into RAG frameworks to enable complex reasoning tasks that require information beyond immediate neighbors?
- **Basis in paper:** [explicit] The authors note: "while GNNs enhance the understanding of a query node by recursively aggregating information from its local neighborhood, RAG typically retrieves only one-hop context for LLMs, which may limit their ability to handle complex multi-hop reasoning tasks."
- **Why unresolved:** The current frameworks retrieve only one-hop neighbors, unlike GNNs which propagate information across multiple hops through iterative message-passing.
- **What evidence would resolve it:** A systematic study varying the number of hops (k=1,2,3...) retrieved, analyzing performance on tasks requiring multi-hop inference, with comparison to k-hop GNN performance.

### Open Question 3
- **Question:** What is the optimal balance between context richness and input complexity when combining query and label information in graph RAG?
- **Basis in paper:** [inferred] The paper observes that "FEWSHOT RAG shows mixed results compared to LABEL RAG" and hypothesizes "the increased input length and complexity, which may hinder LLMs from efficiently processing relevant information." Additionally, ablation studies show performance plateaus or declines beyond 3 retrieved neighbors.
- **Why unresolved:** While combining query and label information should theoretically provide richer context, the trade-off between information gain and LLM processing limitations remains uncharacterized.
- **What evidence would resolve it:** Controlled experiments varying input token length and context composition (query-only, label-only, combined) while measuring both accuracy and attention patterns, potentially with different LLM architectures designed for longer contexts.

## Limitations
- The approach assumes homophily in graph structure, which may not generalize to heterophilic graphs where connected nodes have dissimilar attributes or labels
- The theoretical equivalence between GNN message-passing and RAG lacks empirical validation of whether LLMs actually process retrieved context analogously to GNN aggregation
- Neighbor label availability during inference is assumed but not rigorously addressed for cases where labels are unknown or sparse

## Confidence
- **High confidence:** Performance improvements of RAG frameworks over standard ICL (Table 1 results are consistent across multiple LLMs and datasets)
- **Medium confidence:** The GNN-RAG equivalence mechanism (theoretical formulation is sound, but behavioral equivalence in LLMs remains unproven)
- **Medium confidence:** Superiority of graph-based retrieval over text-based retrieval (Figure 7 shows consistent gains, but ablation studies are limited to specific variants)

## Next Checks
1. Test RAG frameworks on heterophilic graphs (e.g., Amazon co-purchase networks with antagonistic products) to validate robustness beyond homophilic assumptions
2. Conduct ablation studies comparing LLMs with retrieved context to GNNs on identical tasks to empirically validate the claimed functional equivalence
3. Measure and report actual token usage per query to quantify context length efficiency and identify potential optimization opportunities for long-context scenarios