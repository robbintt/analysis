---
ver: rpa2
title: Explore the Use of Time Series Foundation Model for Car-Following Behavior
  Analysis
arxiv_id: '2501.07034'
source_url: https://arxiv.org/abs/2501.07034
tags:
- time
- car-following
- foundation
- series
- chronos
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the application of time series foundation
  models, specifically Chronos, to car-following behavior prediction using the Open
  ACC dataset. Chronos, pre-trained on extensive time series data, offers a promising
  alternative to traditional mathematical models and machine learning approaches by
  requiring minimal re-training and exhibiting strong generalization capabilities.
---

# Explore the Use of Time Series Foundation Model for Car-Following Behavior Analysis

## Quick Facts
- arXiv ID: 2501.07034
- Source URL: https://arxiv.org/abs/2501.07034
- Authors: Luwei Zeng; Runze Yan
- Reference count: 0
- Primary result: Chronos foundation model achieves RMSE 0.60 in zero-shot car-following prediction, improving to 0.53 after fine-tuning

## Executive Summary
This study investigates the application of time series foundation models, specifically Chronos, to car-following behavior prediction using the Open ACC dataset. Chronos, pre-trained on extensive time series data, offers a promising alternative to traditional mathematical models and machine learning approaches by requiring minimal re-training and exhibiting strong generalization capabilities. The research evaluates Chronos in zero-shot and fine-tuned settings, incorporating covariates like vehicle spacing, speed differences, and follower speed. Results show that Chronos without fine-tuning achieves an RMSE of 0.60, comparable to deep learning models like DeepAR and TFT, and outperforms traditional models like IDM and ETS. After fine-tuning, Chronos reduces RMSE to 0.53, a 33.75% improvement over IDM and a 12-37% reduction compared to ETS, DeepAR, WaveNet, and TFT. The findings highlight the potential of foundation models to advance transportation research by offering scalable, adaptable, and highly accurate approaches for predicting and simulating car-following behaviors.

## Method Summary
The study applies Chronos, a time series foundation model based on T5 transformers, to predict car-following acceleration from the Open ACC dataset. The approach uses zero-shot prediction and fine-tuning on acceleration time series, with an optional LightGBM residual correction using covariates (spacing, speed difference, follower speed). The dataset contains 43 trajectories of Hyundai Ioniq vehicles with ACC enabled, recorded at 10 Hz. Performance is evaluated using RMSE across multiple 3-second prediction windows with 6-second context, comparing against traditional models (IDM, ETS) and deep learning approaches (DeepAR, WaveNet, TFT).

## Key Results
- Zero-shot Chronos achieves RMSE 0.60, comparable to DeepAR and TFT
- Fine-tuned Chronos reduces RMSE to 0.53, outperforming IDM (RMSE 0.80) by 33.75%
- Incorporating covariates via LightGBM improves accuracy across all model sizes
- Small (46M) Chronos performs comparably to larger variants on this task

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-training on diverse time series enables zero-shot prediction of vehicle acceleration without domain-specific training.
- Mechanism: Chronos learns general temporal patterns (trends, seasonality, dynamics) from large-scale pre-training data. When applied to car-following trajectories, these learned patterns transfer to predict follower acceleration from historical context alone, bypassing the need for physics-based assumptions about driver behavior.
- Core assumption: Temporal patterns learned from non-transportation time series transfer sufficiently to vehicle trajectory dynamics.
- Evidence anchors:
  - [abstract] "Without fine-tuning, Chronos...achieves similar results to deep learning models such as DeepAR and TFT, with an RMSE of 0.60"
  - [section] "Zero-shot Chronos models are used in the experiment as baseline cases...Chronos models significantly outperform other pretrained models"
  - [corpus] No direct corpus evidence for transfer mechanism; this is a novel application domain per authors.

### Mechanism 2
- Claim: Quantization of continuous time series into discrete tokens allows language model architectures to process trajectory data.
- Mechanism: Chronos scales and quantizes acceleration values into a fixed vocabulary, then trains T5 transformer models using cross-entropy loss on tokenized sequences. This converts regression into a classification-style problem over token buckets.
- Core assumption: Token bucket granularity preserves sufficient resolution for acceleration prediction (typically -5 to +5 m/s²).
- Evidence anchors:
  - [section] "Chronos...scales and quantizes time series values into a fixed vocabulary, then uses cross-entropy loss to train existing transformer-based language models on these tokenized series"
  - [section] "This model is based on the latest Large Language Models (LLMs) from the T5 family, ranging from 20M to 710M parameters"
  - [corpus] Weak corpus evidence; corpus papers focus on classical IDM variants and deep learning, not foundation model tokenization.

### Mechanism 3
- Claim: Incorporating dynamic covariates (spacing, speed difference, follower speed) via residual learning improves prediction accuracy.
- Mechanism: A two-stage ensemble: (1) Chronos predicts acceleration from historical context; (2) LightGBM predicts residuals from covariates. Final prediction = Chronos forecast + residual correction. This addresses Chronos's univariate limitation.
- Core assumption: Residual errors correlate with available covariates; LightGBM can capture these relationships.
- Evidence anchors:
  - [abstract] "After fine-tuning, Chronos reduces the error to an RMSE of 0.53, representing a 33.75% improvement over IDM"
  - [section] "For all three model sizes, the performance with covariates is better than without covariates"
  - [corpus] Corpus papers (e.g., "Modeling Electric Vehicle Car-Following Behavior") confirm covariates like spacing and relative speed are standard inputs for ML-based car-following models.

## Foundational Learning

- **Time Series Tokenization**
  - Why needed here: Understanding how continuous vehicle trajectories become discrete tokens is essential for debugging prediction quality and selecting appropriate vocabulary sizes.
  - Quick check question: Can you explain why a 4096-token vocabulary might cause information loss for acceleration values ranging from -5 to +5 m/s²?

- **Zero-Shot vs. Fine-Tuning Trade-offs**
  - Why needed here: The paper demonstrates both modes; choosing between them depends on data availability, latency constraints, and accuracy requirements.
  - Quick check question: If you had 10 new vehicle trajectories from a different ACC system, would you use zero-shot or fine-tune first?

- **Car-Following State Variables**
  - Why needed here: Covariate selection directly impacts model performance; spacing, speed difference, and follower speed are the paper's chosen inputs.
  - Quick check question: Why might leading vehicle speed be informative but was not included as a covariate in this study?

## Architecture Onboarding

- **Component map:**
  - Historical acceleration data -> Chronos T5 transformer -> Tokenized prediction -> Optional: LightGBM residual correction using [Space_gap, Speed_diff, Speed_FAV] -> Final acceleration forecast

- **Critical path:**
  1. Load pre-trained Chronos weights (HuggingFace)
  2. Prepare trajectory data: segment into context/forecast windows, normalize
  3. For zero-shot: directly inference; for fine-tuning: train on context→forecast pairs using cross-entropy loss
  4. If using covariates: train LightGBM on residuals from training set predictions
  5. Evaluate via multi-window backtesting (RMSE across forecast horizons)

- **Design tradeoffs:**
  - **Model size vs. latency:** Small (46M) performs comparably to large (710M) on this task—paper notes "small size is sufficient"
  - **Zero-shot vs. fine-tuning:** Zero-shot (RMSE 0.60) is deployment-ready; fine-tuning (RMSE 0.53) requires labeled data and compute
  - **With vs. without covariates:** Covariates improve accuracy but require additional sensor data at inference

- **Failure signatures:**
  - Long-horizon drift: Paper notes "as time progresses, some discrepancies...emerge, indicating that long-term forecasting remains challenging"
  - Missing covariates at inference: Residual correction fails or introduces noise
  - Out-of-distribution driving styles: Model trained on one ACC system may not generalize to others (paper only tests Hyundai Ioniq)

- **First 3 experiments:**
  1. **Reproduce zero-shot baseline:** Load chronos-t5-small, predict acceleration on held-out trajectories, compute RMSE—target ~0.60.
  2. **Fine-tune with trajectory data:** Fine-tune on 80% of trajectories, evaluate on 20%—target RMSE reduction to ~0.53.
  3. **Ablate covariates:** Train Chronos+LightGBM with and without spacing/speed_diff inputs—quantify improvement delta from covariate integration.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do time series foundation models perform across different car-following datasets beyond Open ACC?
- Basis in paper: [explicit] The authors explicitly state: "In future work on car-following behavior analysis, we plan to incorporate additional time series foundation models and apply them to other available datasets, such as the Waymo Open dataset and the Vanderbilt ACC dataset."
- Why unresolved: The study only evaluated Chronos on the Casale experiment from Open ACC dataset, leaving performance on other datasets unknown.
- What evidence would resolve it: Comparative performance metrics (RMSE, Std) when applying Chronos and other foundation models to diverse car-following datasets with varying vehicle types, driving conditions, and road environments.

### Open Question 2
- Question: How do time series foundation models compare to a wider range of traditional mathematical car-following models?
- Basis in paper: [explicit] The authors note: "Future research could include additional traditional mathematical models, such as the optimized velocity model, to provide a more comprehensive comparison."
- Why unresolved: Only IDM was used as a traditional mathematical model baseline, limiting understanding of relative performance across model categories.
- What evidence would resolve it: Performance comparison between foundation models and multiple traditional models (optimal velocity model, psycho-physiological models, stimulus-response models) on identical datasets and prediction tasks.

### Open Question 3
- Question: What approaches could improve long-term forecasting accuracy for car-following behavior using foundation models?
- Basis in paper: [inferred] Figure 4 shows "as time progresses, some discrepancies between the forecasted and actual values emerge, indicating that long-term forecasting remains challenging."
- Why unresolved: The study doesn't investigate strategies to improve longer prediction horizons, despite acknowledging this limitation.
- What evidence would resolve it: Systematic experiments with different prediction horizons, model architectures, and input features to identify factors that reduce long-term prediction error.

## Limitations
- Only tested on Hyundai Ioniq ACC data, limiting generalization to other vehicles or manual driving
- Zero-shot performance depends on transferability of pre-training patterns to car-following dynamics
- LightGBM residual correction requires covariate availability at inference time
- Long-horizon forecasting beyond 3 seconds shows significant prediction drift

## Confidence

- **High confidence:** Zero-shot Chronos achieves RMSE 0.60 on Open ACC dataset; fine-tuning improves to 0.53 RMSE; covariate integration via LightGBM improves accuracy.
- **Medium confidence:** Pre-training patterns transfer to car-following prediction; quantization preserves sufficient acceleration resolution; model generalizes beyond the single Hyundai Ioniq platform.
- **Low confidence:** Long-horizon forecasting stability beyond 3 seconds; performance on manual driving data; computational efficiency for real-time deployment.

## Next Checks
1. **Cross-platform validation:** Test Chronos on car-following data from different ACC systems (e.g., Tesla, GM Super Cruise) or manual driving datasets to assess generalization.
2. **Covariate sensitivity analysis:** Systematically evaluate model performance when covariates are missing, noisy, or delayed at inference time.
3. **Long-horizon forecasting:** Extend prediction horizons beyond 3 seconds and quantify drift, comparing Chronos against physics-based models like IDM under extended forecasts.