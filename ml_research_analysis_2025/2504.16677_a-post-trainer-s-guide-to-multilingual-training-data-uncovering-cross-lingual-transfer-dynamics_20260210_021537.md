---
ver: rpa2
title: 'A Post-trainer''s Guide to Multilingual Training Data: Uncovering Cross-lingual
  Transfer Dynamics'
arxiv_id: '2504.16677'
source_url: https://arxiv.org/abs/2504.16677
tags:
- language
- languages
- data
- performance
- instruction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates cross-lingual transfer dynamics in multilingual
  post-training of large language models across three tasks: summarization, instruction
  following, and mathematical reasoning. The study examines two model families (7B
  and 35B parameters) in single-task and multi-task instruction tuning settings, varying
  the amount of multilingual data while keeping English data fixed.'
---

# A Post-trainer's Guide to Multilingual Training Data: Uncovering Cross-lingual Transfer Dynamics

## Quick Facts
- **arXiv ID**: 2504.16677
- **Source URL**: https://arxiv.org/abs/2504.16677
- **Reference count**: 40
- **Primary result**: Cross-lingual transfer efficiency scales with model size, with mathematical reasoning requiring ~13× more multilingual data than simpler linguistic tasks to reach peak performance

## Executive Summary
This study investigates cross-lingual transfer dynamics in multilingual post-training of large language models across summarization, instruction following, and mathematical reasoning tasks. Using Aya 23 (7B, 35B) and Llama 3.1 (8B) models, the research varies multilingual data amounts while keeping English fixed at 10k samples. Results show task-dependent scaling behavior, with mathematical reasoning requiring substantially more multilingual data to plateau than linguistic tasks. Larger models demonstrate more efficient cross-lingual transfer, reducing the performance gap between seen and unseen languages. The study also reveals divergent script preferences by task type, with linguistically-oriented tasks benefiting from non-Latin scripts while reasoning tasks learn more efficiently from Latin-script data.

## Method Summary
The study uses full-parameter fine-tuning on Aya 23 7B/35B and Llama 3.1 8B models with task-specific datasets (XLSum for summarization, ShareGPT/Command-R for instruction following, mCoT-math for mathematical reasoning). English data is fixed at 10k samples (15k for multi-task) while non-English data from Spanish, French, Japanese, and Chinese is varied from 0 to 5,000 samples per language. Training uses batch size 64, constant learning rate 1e-5, and checkpoint selection based on validation loss. Evaluation includes seen languages (es, fr, ja, zh) and unseen languages (ar, ko, pt) across three tasks using RougeL, GPT-4o ratings, and MGSM accuracy metrics.

## Key Results
- Mathematical reasoning requires ~13× more multilingual data than summarization or instruction following to plateau
- Cross-lingual transfer efficiency increases with model scale, reducing seen-unseen language performance gaps
- Linguistically-oriented tasks benefit from non-Latin script inclusion, while reasoning tasks learn more efficiently from Latin-script data
- Task interference in multi-task learning affects mathematical reasoning in smaller models but diminishes at scale

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multilingual performance improvements scale task-dependently, with complex reasoning tasks requiring substantially more multilingual data than linguistic tasks to reach peak performance.
- Mechanism: Tasks with higher reasoning complexity exhibit larger initial performance gaps to English and require ~13× more multilingual data to plateau, whereas simpler linguistic tasks plateau after 200-400 samples per language.
- Core assumption: Task complexity correlates with the amount of multilingual signal needed to align learned capabilities across languages.
- Evidence anchors:
  - [abstract]: "mathematical reasoning requiring significantly more multilingual data than simpler tasks to reach peak performance"
  - [section 4]: "while IF and summarization results plateau after adding as little as 400 non-English samples per language, MR requires roughly 13x more multilingual data"
  - [corpus]: Related work on multilingual knowledge acquisition dynamics supports task-dependent learning curves, though specific mechanisms remain underexplored.
- Break condition: If task complexity does not correlate with initial English–non-English gap, the scaling relationship may not hold.

### Mechanism 2
- Claim: Cross-lingual transfer efficiency improves with model scale, reducing the performance gap between seen and unseen languages.
- Mechanism: Larger models (35B) achieve most performance gains with English-only or minimal multilingual data, showing smaller seen–unseen language gaps. Smaller models (7B) exhibit stronger correlation between seen and unseen language performance, indicating less efficient transfer.
- Core assumption: Scale enables better internal representation alignment across languages without requiring explicit multilingual supervision.
- Evidence anchors:
  - [abstract]: "Cross-lingual transfer becomes more efficient with larger models, reducing the performance gap between seen and unseen languages"
  - [section 4, Table 2]: Spearman correlations between seen/unseen performance decrease with scale (7B: 0.67-0.88; 35B: 0.39-0.51)
  - [corpus]: "Beyond the Rosetta Stone" examines cross-lingual transfer failures in smaller models, suggesting scale-dependent mechanisms.
- Break condition: If model architecture (not just scale) drives transfer efficiency, the relationship may not generalize across model families.

### Mechanism 3
- Claim: Task type determines optimal language script mixtures for post-training—linguistic tasks benefit from non-Latin script inclusion, while reasoning tasks learn more efficiently from Latin-script data.
- Mechanism: Linguistically-oriented tasks require script-diverse data to transfer to lower-performing language groups. Reasoning tasks leverage Latin-script data more efficiently, potentially due to shared symbolic representations in mathematical notation.
- Core assumption: Script similarity provides inductive bias that benefits reasoning transfer, while linguistic surface-form diversity requires explicit exposure.
- Evidence anchors:
  - [abstract]: "linguistically-oriented tasks benefit more from non-Latin script languages, while reasoning tasks learn more efficiently from Latin-script data"
  - [section 6, Figure 8]: Math reasoning with Latin-only mix achieves 37.4% vs 36.6% with non-Latin, despite 10k fewer samples
  - [corpus]: Corpus evidence on script-specific transfer dynamics is limited; most prior work focuses on aggregate multilingual settings.
- Break condition: If script effects are confounded by language-specific pretraining data quality, the mechanism may not hold for languages with different pretraining distributions.

## Foundational Learning

- Concept: **Cross-lingual transfer (CLT)**
  - Why needed here: The paper's central construct—understanding how capabilities learned in one language (typically English) transfer to others without direct supervision.
  - Quick check question: Can you explain why CLT efficiency might vary with model scale and task type?

- Concept: **Instruction tuning data mixtures**
  - Why needed here: The experimental design varies multilingual data proportions while keeping English fixed; understanding mixture effects is essential for interpreting results.
  - Quick check question: How would you design a data mixture experiment to isolate the effect of script diversity on reasoning task performance?

- Concept: **Task interference in multi-task learning**
  - Why needed here: The paper documents interference effects (especially for math reasoning in smaller models) that diminish at scale—critical for understanding when multi-task training helps vs. harms.
  - Quick check question: What evidence would indicate task interference is occurring versus simple capacity saturation?

## Architecture Onboarding

- Component map: Base models (Aya 23 7B/35B, Llama 3.1 8B) → Full-parameter fine-tuning → Task-specific evaluation (IF: LLM-as-judge; Summarization: RougeL; Math: MGSM accuracy). Training languages: en + {es, fr, ja, zh} (seen); evaluation adds {ar, ko, pt} (unseen).

- Critical path: Data preparation (uniform sampling across 4 non-English languages) → Incremental multilingual data addition (0 → 5k samples/language) → Checkpoint selection via validation loss → Per-language evaluation on seen and unseen sets.

- Design tradeoffs:
  - Single-task vs. multi-task: Single-task provides cleaner CLT signal; multi-task reflects realistic training but introduces interference
  - Uniform vs. optimized sampling: Uniform enables controlled comparison but may be suboptimal for specific target languages
  - Full fine-tuning vs. PEFT: Full fine-tuning used here for maximal signal; PEFT may show different dynamics

- Failure signatures:
  - Plateauing performance with additional multilingual data (indicates saturation or data quality issues)
  - Diverging seen–unseen gap (suggests memorization rather than transfer)
  - Performance oscillation in multi-task settings (task interference, especially in smaller models)

- First 3 experiments:
  1. **Baseline scaling curve**: Fix English data at 10k, increment multilingual data (100, 200, 400, 800, 1000, 2000, 5000 per language) for each task individually. Measure seen vs. unseen performance gap.
  2. **Scale comparison**: Repeat experiment 1 on both 7B and 35B models. Quantify correlation between seen and unseen performance to verify scale-dependent CLT efficiency.
  3. **Script mixture ablation**: Train on Latin-only (en, es, fr) vs. non-Latin (en, ja, zh) mixes for each task. Compare performance on Latin-script vs. non-Latin-script evaluation languages to confirm task-dependent script preferences.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can cross-lingual transfer mechanisms be modified or enhanced to bridge the persistent performance gap between seen and unseen languages?
- Basis in paper: [explicit] The authors state: "across all tasks the initial seen—unseen performance gap after English-only training effectively remains constant; despite the addition of multilingual data, the model is not able to close the gap to unseen languages. This highlights possible limits of cross-lingual transfer in LLMs."
- Why unresolved: The study demonstrates the gap exists across tasks and scales but does not investigate interventions to address this fundamental limitation.
- What evidence would resolve it: Experiments with different transfer learning techniques (e.g., adapter modules, representation alignment methods) showing whether the gap can be reduced through architectural or training modifications.

### Open Question 2
- Question: Why do reasoning tasks benefit from Latin-script training data while linguistically-oriented tasks require non-Latin script data for optimal performance?
- Basis in paper: [explicit] Section 6 finds: "while linguistically-oriented tasks (e.g., instruction following, summarization) benefit from data in non-Latin script languages... for reasoning tasks (e.g., mathematical reasoning) the model learns more efficiently by training on Latin script data only."
- Why unresolved: The paper documents this divergent pattern but provides no mechanistic explanation for why script dependency varies by task type.
- What evidence would resolve it: Ablation studies controlling for linguistic distance, pre-training data distribution, and task-specific representations to isolate the causal factors.

### Open Question 3
- Question: What mechanisms enable larger models to overcome task interference that degrades mathematical reasoning performance in smaller models during multi-task learning?
- Basis in paper: [explicit] Section 5.1 notes: "while there is considerable oscillation in terms of mathematical reasoning performance... for the 7B model, the 35B LLM does not suffer from the same issue" and "the math performance of the 7B model degrades by 8% when trained on a multi-task data mix."
- Why unresolved: The emergence of interference resistance at scale is observed but the underlying representational or optimization dynamics remain unexplored.
- What evidence would resolve it: Probing experiments or analysis of gradient interactions across tasks at different model scales to identify when and how interference mitigation emerges.

## Limitations

- The uniform sampling across four non-English languages may not reflect optimal language-specific strategies for multilingual transfer
- The relatively small number of languages (8 total) constrains conclusions about cross-lingual transfer to truly unseen language families
- The use of English data as a fixed baseline creates a strong reference point that may mask more nuanced multilingual dynamics

## Confidence

- **High confidence**: Task-dependent scaling behavior (mathematical reasoning requires substantially more multilingual data than linguistic tasks to plateau) is supported by clear performance curves across multiple model sizes and consistent with established learning theory.
- **Medium confidence**: Scale-dependent cross-lingual transfer efficiency is demonstrated through correlation metrics, but the mechanism remains partially speculative without deeper analysis of learned representations.
- **Medium confidence**: Script-dependent task benefits are observed, but the limited language sample (4 scripts) and potential confounding factors in pretraining distributions reduce certainty about the underlying mechanism.

## Next Checks

1. **Representation analysis**: Conduct similarity measurements between language representations at different training stages to verify whether scale improves cross-lingual alignment through representational overlap rather than memorization.

2. **Language family expansion**: Replicate the experiments with languages from diverse families (e.g., adding Turkish, Hindi, Swahili) to test whether the observed script effects hold across broader typological diversity.

3. **Dynamic sampling validation**: Implement language-specific sampling rates based on initial performance gaps rather than uniform sampling to test whether optimized multilingual mixtures could achieve similar performance with fewer total samples.