---
ver: rpa2
title: 'A Comprehensive Analysis of Large Language Model Outputs: Similarity, Diversity,
  and Bias'
arxiv_id: '2505.09056'
source_url: https://arxiv.org/abs/2505.09056
tags:
- language
- llms
- bias
- text
- similarity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study analyzed approximately 3 million texts generated by
  12 LLMs and human authors to assess output similarity, diversity, and bias. Using
  5,000 prompts across tasks like generation and rewriting, the researchers found
  that LLM outputs are more similar to each other than to human-written texts.
---

# A Comprehensive Analysis of Large Language Model Outputs: Similarity, Diversity, and Bias

## Quick Facts
- arXiv ID: 2505.09056
- Source URL: https://arxiv.org/abs/2505.09056
- Reference count: 40
- Key result: LLM outputs are more similar to each other than to human texts, with GPT-4 showing highest lexical diversity and WizardLM-2-8x22b most uniform outputs

## Executive Summary
This study analyzed approximately 3 million texts generated by 12 LLMs and human authors to assess output similarity, diversity, and bias. Using 5,000 prompts across various tasks, researchers found that LLM outputs are more similar to each other than to human-written texts. The study employed classification models achieving up to 71% accuracy in authorship identification, with GPT-4 being the most distinctive model. Bias analysis revealed that all models showed similar gender bias, but GPT-3.5 and GPT-4 exhibited higher racial bias compared to other models.

## Method Summary
The study used 5,015 prompts from PERSUADE 2.0 and Stanford Alpaca datasets, generating 50 samples per prompt for each of 12 LLMs and human authors (approximately 3 million texts total). Outputs were analyzed using pairwise text comparison metrics (cosine similarity and Levenshtein edit distance), stylometric features (readability, syllable ratios, lexical diversity), and classification models (BERT, DeBERTa-v3, XGBoost-BoW) for authorship attribution. Bias was quantified using Word2Vec embeddings and DirectBias scoring with gender-neutral occupation words.

## Key Results
- LLM outputs are more similar to each other than to human texts, with inner-similarity ranging from 0.54 (WizardLM-2-8x22b) to 0.69 (GPT-4)
- Classification models achieved up to 71% accuracy in identifying authorship, with GPT-4 and GPT-3.5 being hardest to distinguish
- GPT-4 showed the highest lexical diversity (6.1) and lowest self-similarity, while WizardLM-2-8x22b produced the most uniform outputs (lexical diversity 2.2)
- All models showed similar gender bias, but GPT-3.5 and GPT-4 exhibited higher racial bias compared to other models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs produce detectable stylometric fingerprints through consistent vocabulary preferences and linguistic patterns.
- Mechanism: Models develop distinctive word associations and syntactic regularities during training and alignment that persist across prompts. Point-wise Mutual Information (PMI) quantifies how strongly specific words predict model authorship, creating identifiable markers.
- Core assumption: Training corpora and fine-tuning procedures leave measurable linguistic signatures that survive varied prompting contexts.
- Evidence anchors:
  - Classification models achieved up to 71% accuracy in identifying authorship, with GPT-4 and GPT-3.5 being hardest to distinguish.
  - BERT classifier achieved 0.7095 accuracy; models like GPT-4 and GPT-3.5 confused due to architectural similarity.
  - Related work on RLHF effects confirms that alignment procedures alter text detectability in measurable ways.

### Mechanism 2
- Claim: Output diversity varies systematically with model architecture and sampling configuration, not just model size.
- Mechanism: Temperature, top-p, and repetition penalty settings interact with model-specific token distributions. GPT-4's higher lexical diversity (6.1 vs. WizardLM-2-8x22B's 2.2) suggests architectural or training differences in entropy handling, not merely hyperparameter effects alone.
- Core assumption: Observed diversity differences reflect intrinsic model properties rather than solely generation configuration artifacts.
- Evidence anchors:
  - WizardLM-2-8x22b produced the most uniform outputs, while GPT-4 showed the highest lexical diversity.
  - GPT-4: lexical diversity 6.1, unique word ratio 86.4%; WizardLM-2-8x22B: lexical diversity 2.2, unique word ratio 64.3%.
  - LLM output diversity is a systemic limitation requiring guided generation interventions.

### Mechanism 3
- Claim: Bias in LLM outputs correlates with embedding-space associations between demographic terms and neutral concepts.
- Mechanism: Word embeddings trained on model outputs capture co-occurrence patterns; vector arithmetic reveals directional bias subspace. DirectBias scores quantify association strength between gender/race directions and occupation terms.
- Core assumption: Embedding geometry reflects meaningful bias propagation rather than artifact of insufficient data per model.
- Evidence anchors:
  - GPT-3.5 and GPT-4 exhibited higher racial bias, while Gemma-7B and Gemini-pro were most balanced.
  - DirectBias computed via cosine similarity between neutral words and bias direction vector g.
  - Corpus evidence on bias is weak; no direct neighbor papers address bias quantification methodologies.

## Foundational Learning

- Concept: Cosine Similarity & Edit Distance
  - Why needed here: Core metrics for quantifying text similarity; paper uses both lexical (Levenshtein) and semantic (cosine on embeddings) measures.
  - Quick check question: Given two token vectors [1,0,1] and [1,1,0], compute cosine similarity.

- Concept: Word Embeddings (Word2Vec CBOW)
  - Why needed here: Foundation for bias analysis; embeddings encode semantic relationships that reveal stereotypical associations.
  - Quick check question: Explain why "king - man + woman ≈ queen" works in embedding space.

- Concept: PMI (Point-wise Mutual Information)
  - Why needed here: Identifies distinctive vocabulary markers; ranks words by predictive power for model attribution.
  - Quick check question: If word "delve" appears in 80% of GPT-4 outputs but 5% overall, what does high PMI indicate?

## Architecture Onboarding

- Component map: Prompt Set (5,015) → 12 LLMs (50 generations each) → ~3M texts → Similarity Analysis (Cosine, Levenshtein) → Inner/Inter similarity matrices → Stylometric Features (readability, syllable ratios, lexical diversity) → Classification (BERT, DeBERTa-v3, XGBoost-BoW) → Authorship attribution → Bias Analysis (Word2Vec embeddings → DirectBias scoring)

- Critical path: Text generation → Similarity computation → Classification training → Embedding training → Bias quantification. Each stage depends on sufficient output volume per model.

- Design tradeoffs:
  - 50 generations per prompt balances statistical power vs. API cost; may miss long-tail diversity.
  - Word-level edit distance captures surface similarity but misses semantic equivalence; cosine on embeddings addresses this gap.
  - BERT classifiers achieve higher accuracy than DeBERTa variants (0.71 vs. 0.65) but require more compute.

- Failure signatures:
  - High inner-similarity with low inter-similarity → model has strong fingerprint.
  - Low classification accuracy on specific model pairs → architectural similarity (GPT-3.5/GPT-4).
  - DirectBias score near 0 → balanced model (Gemma-7B, Gemini-pro).

- First 3 experiments:
  1. Reproduce inner-similarity analysis on a subset (3 models, 100 prompts) to validate pipeline; expect WizardLM highest, GPT-4 lowest.
  2. Train lightweight classifier (XGBoost-BoW) on sampled data; verify human vs. LLM separability (F1 > 0.99 for human class).
  3. Train Word2Vec embeddings on one model's outputs; compute DirectBias on gender-neutral occupation list; compare to paper's reported scores.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can explainability techniques be integrated into LLM authorship classification to articulate the specific reasons behind model predictions?
- Basis in paper: The authors state: "Future work involves exploring explainability techniques, where the focus extends beyond detecting whether a text is authored by a human or generated by an LLM. The aim is to explore and articulate the reasons behind the model's predictions."
- Why unresolved: Current classification approaches (BERT, DeBERTa, XGBoost) achieve up to 71% accuracy but operate as black boxes without explaining which linguistic features drive predictions.
- What evidence would resolve it: Development of interpretable classification frameworks that identify specific stylometric, lexical, or structural features contributing to authorship attribution decisions.

### Open Question 2
- Question: How do variations in generation hyperparameters (temperature, max tokens, top-p) affect intra-LLM similarity and output diversity across models?
- Basis in paper: The study used different hyperparameters across models (e.g., GPT-4 max tokens=250 vs. 512 for most others; Gemma-7B temperature=1.0 vs. 0.7 for most), potentially confounding observed diversity differences.
- Why unresolved: The experimental design does not control for or analyze the impact of these parameter differences on the reported similarity and diversity metrics.
- What evidence would resolve it: Controlled experiments systematically varying temperature and max tokens within each model while measuring output similarity and lexical diversity.

### Open Question 3
- Question: To what extent do the observed bias patterns generalize across different task domains, prompt types, and languages beyond the 5,000 English-language prompts tested?
- Basis in paper: The bias analysis relied on a specific prompt set from PERSUADE 2.0 and Stanford Alpaca, and the authors note that "individual word pairs do not always behave as expected because a word can have multiple meanings depending on the context."
- Why unresolved: The embedding-based bias detection method captures associations within the specific generated corpus, but domain-specific and multilingual biases remain unexplored.
- What evidence would resolve it: Cross-domain and multilingual bias assessments using the same DirectBias methodology across diverse prompt categories and non-English outputs.

## Limitations

- Data representativeness: The 5,015 prompts from PERSUADE 2.0 and Stanford Alpaca may not capture the full range of real-world use cases, potentially limiting generalizability.
- Embedding-based bias measurement validity: DirectBias scores rely on Word2Vec embeddings trained on model outputs, which may not capture stable semantic relationships due to limited training data per model.
- Hyperparameter standardization: Different hyperparameters across models (e.g., GPT-4 max tokens=250 vs. 512 for others) may confound observed diversity differences.

## Confidence

- **High confidence**: Similarity metrics (cosine, edit distance) and classification accuracy results are well-established methods with clear implementations. The finding that GPT-4 shows highest lexical diversity (6.1 vs. 2.2 for WizardLM) appears robust.
- **Medium confidence**: Bias analysis using DirectBias scores is methodologically sound but depends on embedding quality and choice of gender/race word pairs. The claim about GPT-3.5/GPT-4 showing higher racial bias is plausible but requires careful validation.
- **Low confidence**: The assertion that classification accuracy directly reflects "detectable stylometric fingerprints" may overstate the case, as prompts themselves could carry detectable patterns that classifiers exploit.

## Next Checks

1. **Cross-domain validation**: Test the similarity and bias analysis pipeline on domain-specific prompt sets (medical, legal, creative writing) to assess generalizability beyond the current prompt sources.
2. **Alternative bias metrics**: Implement multiple bias quantification methods (e.g., WEAT, SEAT) to triangulate the DirectBias results and verify whether GPT-3.5/GPT-4 consistently show higher racial bias across different measurement approaches.
3. **Ablation study on hyperparameters**: Systematically vary temperature, top_p, and repetition penalty settings for each model to isolate architectural effects from generation configuration impacts on output diversity and similarity.