---
ver: rpa2
title: 'Do Large Language Models Have a Planning Theory of Mind? Evidence from MindGames:
  a Multi-Step Persuasion Task'
arxiv_id: '2507.16196'
source_url: https://arxiv.org/abs/2507.16196
tags:
- proposal
- will
- target
- information
- states
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a novel multi-step persuasion task called
  MindGames to evaluate large language models' (LLMs) "planning theory of mind" (PToM)
  - their ability to infer and intervene on others' mental states through interactive
  dialogue. Unlike previous spectatorial ToM tasks, this task requires persuaders
  to dynamically gather information about a target's preferences and knowledge through
  dialogue to successfully influence their choices.
---

# Do Large Language Models Have a Planning Theory of Mind? Evidence from MindGames: a Multi-Step Persuasion Task

## Quick Facts
- arXiv ID: 2507.16196
- Source URL: https://arxiv.org/abs/2507.16196
- Reference count: 40
- Humans significantly outperform o1-preview on a novel multi-step persuasion task requiring planning theory of mind

## Executive Summary
This paper introduces MindGames, a novel task designed to evaluate large language models' "planning theory of mind" (PToM) - their ability to infer and intervene on others' mental states through interactive dialogue. Unlike previous spectatorial ToM tasks, MindGames requires persuaders to dynamically gather information about a target's preferences and knowledge through dialogue to successfully influence their choices. Human participants significantly outperformed o1-preview on this task (29% vs 20% success rate; p=0.006), demonstrating humans' superior ability to engage in complex social reasoning and strategic information disclosure. The gap was attributed to humans' more sophisticated causal models of persuasion, as they consistently appealed to targets' mental states while LLMs rarely did so.

## Method Summary
The MindGames task involves a persuader convincing a target agent to choose a specific proposal through dialogue. The task features three proposals with three attributes each, where partial information is revealed strategically to influence the target's choice. Two conditions were tested: HIDDEN (persuader must infer target's preferences through queries) and REVEALED (target's preferences are known). A naively-rational target bot responds to queries with scripted outputs and makes decisions based on utility maximization. The study used chain-of-thought prompting and temperature 0 (except deepseek-r1 at 0.6) with 300-character message limits. Success was measured by whether the target chose the persuader's preferred proposal.

## Key Results
- Humans outperformed o1-preview on the HIDDEN condition (29% vs 20% success rate; p=0.006)
- o1-preview outperformed humans in the REVEALED condition (78% vs 22% success rate)
- Humans consistently appealed to target mental states (~40% of the time) while LLMs rarely did so (<23% of the time)
- LLM success rates remained flat across dialogue turns while human success increased progressively

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Humans outperform LLMs on Planning Theory of Mind tasks because humans possess implicit causal models of other agents that guide information-gathering behavior.
- Mechanism: Humans instinctively query mental states ("What do you know?" and "How do you feel?") before attempting persuasion. This reflects a causal understanding that beliefs and desires generate behavior, enabling strategic intervention.
- Core assumption: The difference in appeal rates reflects fundamentally different representational architectures, not just prompting inadequacies.
- Evidence anchors:
  - "We hypothesize this is because humans have an implicit causal model of other agents (e.g., they know, as our task requires, to ask about people's preferences)."
  - "Humans appeal to all of the mental states of the target about 40% of the time regardless of condition... In contrast, LLMs appeal to the target's mental states no more than 23% of the time."
  - Related work on neural ToM networks suggests generalization from first- to higher-order ToM develops rapidly in humans but remains challenging for neural systems—supporting the representational gap hypothesis.
- Break condition: If LLMs could be prompted to match human appeal rates without architectural changes, the mechanism would shift from representational to prompting-based.

### Mechanism 2
- Claim: LLMs excel at spectatorial (predictive) ToM but fail at planning ToM because their training optimizes for next-token prediction over complete interaction trajectories.
- Mechanism: In the REVEALED condition, o1-preview achieved 78% success versus humans' 22%—suggesting LLMs can enumerate counterfactual outcomes when mental states are given, but cannot autonomously construct the information-gathering plans needed in HIDDEN conditions.
- Core assumption: The REVEALED condition isolates predictive reasoning; the HIDDEN condition requires multi-step planning under uncertainty.
- Evidence anchors:
  - "In contrast, o1-preview outperforms humans in a baseline condition which requires a similar amount of planning but minimal mental state inferences."
  - "In the discrete-game wherein models are only given the choice to appeal to or disclose information on each turn, o1-preview succeeds 80% of the time—comparable to its default performance in the REVEALED condition."
  - Related work on LLM-augmented inverse planning proposes hybrid architectures combining Bayesian inverse planning with LLM hypothesis generation—suggesting pure LLMs lack explicit planning mechanisms.
- Break condition: If chain-of-thought prompting or inference-time compute alone closed the HIDDEN-REVEALED gap, the mechanism would be capacity-based rather than architectural.

### Mechanism 3
- Claim: LLMs fail to strategically manage dialogue turns because they lack explicit models of information value across multi-turn interactions.
- Mechanism: Humans progressively improve success rates across turns (0% → 20% → 29% by turn 8), while LLM success remains flat regardless of turn count. LLMs disclose too much early (2.3 pieces vs. 0.5 for humans on turn 1), entering unrecoverable "sink states" where targets choose suboptimal proposals.
- Core assumption: Flat performance across turns indicates absence of dynamic belief updating, not just poor calibration.
- Evidence anchors:
  - "o1-preview reveals too much information at a much higher rate in the HIDDEN condition (about an eighth of the time) compared to the REVEALED condition (almost never)."
  - "While human participants progressively achieve higher success rates over each dialogue turn... the models we tested succeed at similar rates regardless of whether they send one message to the target or multiple messages."
  - Evidence is indirect; related work on embodied ToM evaluates dynamic multi-agent scenarios but doesn't isolate turn-level planning.
- Break condition: If explicit belief-state tracking modules (e.g., Bayesian updates) failed to improve performance, the mechanism would extend beyond information management to deeper representational issues.

## Foundational Learning

- Concept: **Planning Theory of Mind (PToM) vs. Spectatorial ToM**
  - Why needed here: The paper's central claim depends on distinguishing passive prediction (spectatorial) from active intervention (planning). Without this distinction, the HIDDEN-REVEALED comparison is uninterpretable.
  - Quick check question: Can you explain why revealing mental states improves LLM performance more than human performance?

- Concept: **Causal vs. Associative Mental State Representations**
  - Why needed here: The authors argue humans possess causal models (beliefs + desires → behavior) while LLMs may rely on associative patterns. This underpins why LLMs don't spontaneously query mental states.
  - Quick check question: What behavioral signature would distinguish a causal model from an associative model in this task?

- Concept: **Strategic Information Disclosure Under Partial Observability**
  - Why needed here: Success requires disclosing some but not all information—a classic partial observability problem. Understanding this clarifies why over-disclosure creates sink states.
  - Quick check question: Why does disclosing all available information guarantee failure in this task?

## Architecture Onboarding

- Component map: Task environment -> Target agent -> Persuader interface -> Classification layer
- Critical path:
  1. Generate valid payoff matrices via constraint solver (target must switch preferences under partial disclosure)
  2. Initialize target with hidden information and distinct value function
  3. Run 8-turn dialogue; persuader must gather mental state info (HIDDEN) or skip directly to disclosure (REVEALED)
  4. Classify appeals/disclosures; update target beliefs; compute final choice
- Design tradeoffs:
  - Hard-coded vs. LLM target: Hard-coded target ensures measurement validity but sacrifices ecological validity
  - Constrained vs. open-ended dialogue: Constrained JSON actions (discrete-game variant) improve LLM performance but reduce task generality
  - Working memory demands: 17 pieces of information may depress human performance, potentially masking condition effects
- Failure signatures:
  - Over-disclosure on turn 1: Indicates absence of strategic information management (LLM signature)
  - Zero appeals to mental states: Indicates lack of implicit causal model (LLM signature)
  - Flat success across turns: Indicates no dynamic belief updating (LLM signature)
  - Asking irrelevant questions: Humans sometimes ask "What is your top choice?" without first querying attributes—a heuristic that works less reliably
- First 3 experiments:
  1. **Replicate with scaffolded prompting**: Add explicit hints ("You may want to ask about preferences") to test whether the gap is prompting-vs.-architectural. Predictions: Small improvement in HIDDEN; REVEALED unchanged.
  2. **Vary working memory load**: Reduce attributes from 3 to 2 to test whether human performance converges with LLMs in REVEALED. Predictions: Human REVEALED improves; gap persists in HIDDEN.
  3. **Ablate target rationality**: Replace naively-rational target with LLM-simulated human to test whether human strategies (high appeal rates) are adapted to real agents. Predictions: Human success rate increases; LLM success rate unchanged.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can simpler tasks with lower working memory demands elicit the same divide in Planning Theory of Mind (PToM) capabilities between humans and LLMs?
- Basis in paper: The authors state in the Limitations section that "Future work may discover simpler tasks which elucidate the same divide in PToM."
- Why unresolved: The current task requires tracking up to 17 pieces of information, imposing high cognitive constraints that may deflate performance and mask the true nature of the PToM gap.
- What evidence would resolve it: Designing and administering a variant of the MindGames task with fewer attributes and proposals to compare human and LLM success rates.

### Open Question 2
- Question: Does the ability to succeed in the MindGames task transfer to ecologically valid interactions with human targets?
- Basis in paper: The authors note, "Future work might investigate the relationship of PToM and more ecologically valid interactions like rhetoric."
- Why unresolved: The study utilized a "naively-rational" bot with canned responses rather than a human, leaving the effectiveness of PToM strategies in complex, nuanced human dialogue unconfirmed.
- What evidence would resolve it: Running the persuasion task where the target is a human agent blind to the persuader's identity (human or LLM).

### Open Question 3
- Question: To what extent can specific prompting or scaffolding strategies improve LLM performance on the HIDDEN PToM task?
- Basis in paper: While the authors note that "different prompting or scaffolding... might improve their performance," their results reflect capabilities absent case-specific tuning, leaving the potential for improvement an open question.
- Why unresolved: The study showed default LLMs fail to ask necessary questions, but it is unclear if this is a fundamental lack of causal modeling or a failure of the specific prompting strategy used.
- What evidence would resolve it: Testing LLMs with specialized Chain-of-Thought prompts that explicitly encourage dynamic information gathering before planning.

## Limitations

- The task uses a hard-coded target agent that may not generalize to real human behavior, potentially confounding the performance gap
- High working memory demands (up to 17 pieces of information) may depress human performance, potentially masking condition effects
- The study didn't test whether humans can succeed with LLM targets, leaving unclear whether human strategies are adapted to real agents

## Confidence

- **High confidence**: The fundamental distinction between planning vs spectatorial theory of mind, supported by the reversal of performance rankings between HIDDEN and REVEALED conditions
- **Medium confidence**: The attribution of human superiority to causal mental state models, as this relies on indirect evidence (appeal rates) rather than direct model probing
- **Medium confidence**: The interpretation that over-disclosure creates unrecoverable sink states, though this mechanism is plausible given the payoff structure

## Next Checks

1. Replicate with scaffolded prompting: Add explicit hints about querying mental states to test whether the gap is prompting-vs-architectural
2. A/B test target rationality: Replace the naively-rational bot with an LLM-simulated human to test whether human strategies are adapted to real agents
3. Ablate disclosure freedom: Implement the discrete-game variant with constrained JSON actions to isolate planning capacity from information management