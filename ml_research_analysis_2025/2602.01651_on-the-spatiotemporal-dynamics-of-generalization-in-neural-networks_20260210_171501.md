---
ver: rpa2
title: On the Spatiotemporal Dynamics of Generalization in Neural Networks
arxiv_id: '2602.01651'
source_url: https://arxiv.org/abs/2602.01651
tags:
- generalization
- physical
- arxiv
- sead
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes a fundamental physical framework for understanding\
  \ why neural networks fail at length generalization in algorithmic tasks. The authors\
  \ argue this failure is not a technical limitation but a violation of physical postulates:\
  \ (1) Locality\u2014information propagates at finite speed; (2) Symmetry\u2014computation\
  \ laws are invariant across space and time; (3) Stability\u2014systems converge\
  \ to discrete attractors resisting noise."
---

# On the Spatiotemporal Dynamics of Generalization in Neural Networks

## Quick Facts
- arXiv ID: 2602.01651
- Source URL: https://arxiv.org/abs/2602.01651
- Reference count: 40
- One-line primary result: A physical framework explaining neural network failures at length generalization, solved by a 41-99 parameter neural cellular automaton achieving 100% accuracy on tasks up to L=10^6

## Executive Summary
This paper argues that neural networks fail at length generalization not due to technical limitations but because they violate fundamental physical computation laws: locality (finite information propagation speed), symmetry (invariant computation rules), and stability (discrete attractor convergence). Based on these constraints, the authors derive the SEAD architectureâ€”a neural cellular automaton that achieves perfect length generalization on algorithmic tasks. Experiments show SEAD achieves 100% accuracy on parity and addition tasks from L=16 to L=10^6, and perfect Rule 110 simulation, using only 41-99 parameters while demonstrating input-adaptive computation time.

## Method Summary
The SEAD architecture is a Neural Cellular Automaton that combines a local convolutional update rule with iterative convergence. Training uses "chaos training" where a tiny CNN is optimized to predict single-step state transitions using cross-entropy loss. During inference, the model starts with discrete input, applies the CNN update rule, projects back to discrete symbols via argmax, and repeats until reaching a fixed point. The system operates on a continuous probability manifold during computation but maintains discrete state transitions, enabling it to respect physical computation laws while achieving perfect length generalization.

## Key Results
- 100% accuracy on parity task from L=16 to L=10^6
- 100% accuracy on addition task from L=16 to L=10^6
- Perfect Rule 110 simulation with input-adaptive computation time

## Why This Works (Mechanism)
The failure of standard neural networks at length generalization stems from violating physical computation laws. SEAD succeeds by enforcing locality (local convolutional updates), symmetry (invariant rules across space/time), and stability (convergence to discrete attractors). This design ensures information propagates at finite speed and computation remains consistent across sequence lengths.

## Foundational Learning
1. **Neural Cellular Automata** - Why needed: Provides the computational framework for local, iterative processing. Quick check: Verify state transitions follow local neighborhood rules.
2. **Fixed-point convergence** - Why needed: Ensures stable computation across variable-length inputs. Quick check: Monitor state changes until zero difference.
3. **Chaos training** - Why needed: Enables learning of exact algorithmic rules rather than statistical patterns. Quick check: Verify single-step prediction accuracy reaches 100%.
4. **Spacetime diagrams** - Why needed: Visualizes information propagation and correctness across computation steps. Quick check: Confirm diagonal correctness waves with speed 1.
5. **Physical computation laws** - Why needed: Framework explaining why traditional architectures fail at length generalization. Quick check: Verify locality, symmetry, and stability constraints are respected.
6. **Discrete attractor states** - Why needed: Provides noise-resistant convergence for stable computation. Quick check: Ensure system resists perturbations once converged.

## Architecture Onboarding

**Component Map:** Input Lattice -> Local CNN Update -> Argmax Projection -> Fixed Point Check -> Output

**Critical Path:** The inference loop where local CNN updates iteratively transform the lattice until convergence. This path must maintain exact algorithmic computation while respecting physical constraints.

**Design Tradeoffs:** Minimal parameters (41-99) versus computational depth. The architecture trades parameter efficiency for iterative computation, enabling perfect generalization at the cost of variable inference time.

**Failure Signatures:** Non-convergence (oscillation/limit cycles), information death (signal fading over long sequences), statistical approximation (less than 100% single-step accuracy).

**First Experiments:**
1. Implement the inference loop and verify convergence on short sequences (L=16)
2. Train the CNN on single-step prediction for parity task and verify 100% accuracy
3. Generate spacetime diagrams to confirm correct information propagation

## Open Questions the Paper Calls Out
None

## Limitations
- The framework's applicability to non-algorithmic, real-world tasks remains unproven
- Input encoding details for complex tasks (like addition) are underspecified
- The computational efficiency trade-off (variable inference time) may limit practical deployment

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Physical framework explains length generalization failure | High |
| SEAD achieves 100% accuracy on parity/addition to L=10^6 | High |
| The 41-99 parameter budget is both necessary and sufficient | Medium |
| Input encoding for addition task | Medium |
| Chaos training sampling strategy | Medium |

## Next Checks
1. **Convergence Verification:** Implement the inference loop and verify it converges to fixed points on short sequences (L=16) before attempting length generalization
2. **Spacetime Visualization:** Generate spacetime diagrams for all three tasks to confirm information propagates at the expected speed and maintains signal integrity across long sequences
3. **Parameter Sensitivity Analysis:** Systematically vary the number of CNN parameters to verify the claimed 41-99 parameter budgets are both necessary and sufficient for perfect generalization