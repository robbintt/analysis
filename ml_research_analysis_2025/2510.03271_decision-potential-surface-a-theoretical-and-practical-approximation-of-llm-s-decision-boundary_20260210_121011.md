---
ver: rpa2
title: 'Decision Potential Surface: A Theoretical and Practical Approximation of LLM''s
  Decision Boundary'
arxiv_id: '2510.03271'
source_url: https://arxiv.org/abs/2510.03271
tags:
- decision
- boundary
- logp
- error
- potential
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Decision Potential Surface (DPS) as a novel
  framework for analyzing the decision boundaries of large language models (LLMs).
  Traditional decision boundary analysis becomes computationally infeasible for LLMs
  due to their massive vocabulary sizes and autoregressive nature.
---

# Decision Potential Surface: A Theoretical and Practical Approximation of LLM's Decision Boundary

## Quick Facts
- arXiv ID: 2510.03271
- Source URL: https://arxiv.org/abs/2510.03271
- Authors: Zi Liang; Zhiyao Wu; Haoyang Shang; Yulin Jin; Qingqing Ye; Huadi Zheng; Peizhao Hu; Haibo Hu
- Reference count: 40
- One-line primary result: Introduces Decision Potential Surface (DPS) framework enabling practical visualization and analysis of LLM decision boundaries through efficient K-sampling approximation.

## Executive Summary
This paper addresses the fundamental challenge of analyzing decision boundaries in large language models, which becomes computationally intractable due to massive vocabulary sizes and autoregressive generation. The authors propose the Decision Potential Surface (DPS) framework, where each point encodes confidence in distinguishing different sampling sequences for each input. Crucially, they prove that the zero-height isohypse of DPS corresponds exactly to the LLM's decision boundary. The key innovation is K-DPS, an efficient approximation algorithm requiring only K-finite sequence samplings to approximate the decision boundary with provable error bounds. Extensive experiments validate that errors decrease as O(1/√K), making the method practical for real-world LLM analysis.

## Method Summary
The paper introduces K-DPS, an algorithm that approximates an LLM's decision boundary by sampling K sequences from the model and computing a Decision Potential Function (DPF) for each input. For each input, K sequences are generated using nucleus sampling (p=0.9), and the DPF is calculated as the squared difference between the log-probabilities of the top-2 sequences in the sample. The zero-isohypse of this surface corresponds to the decision boundary. Theoretical analysis establishes upper bounds for absolute error, expected error, and error concentration between K-DPS and the ideal DPS, demonstrating that errors can be traded off with sampling size. The method enables practical visualization through dimensionality reduction (UMAP) and interpolation of the potential values.

## Key Results
- Proves that the zero-isohypse of DPS corresponds exactly to the LLM's decision boundary, with enclosed regions representing decision regions
- Establishes O(1/√K) error scaling between K-DPS and ideal DPS, validated empirically across multiple datasets
- Demonstrates practical applicability with modest sampling sizes (K=10,000) achieving high accuracy in boundary approximation
- Validates theoretical bounds through extensive experiments on Llama3.2-1B across Wikipedia Mini, Tulu-3-SFT-MIX, OpenO1-SFT, and HH-RLHF datasets

## Why This Works (Mechanism)
The method works by leveraging the statistical properties of sampling from the LLM's output distribution. By computing the squared difference between top sequence log-probabilities across multiple samples, the DPF captures the model's confidence in distinguishing different outputs. The zero-isohypse naturally emerges where this confidence is minimal, corresponding to the decision boundary. The O(1/√K) convergence follows from standard sampling theory - as K increases, the sample top-sequences better approximate the true distribution, reducing the estimation error in the DPF.

## Foundational Learning
- **Decision boundary analysis**: Understanding how to characterize where an LLM switches between different output modes - needed because direct computation is intractable for large vocabularies.
- **Sampling theory and concentration bounds**: Mathematical foundations for bounding the error between sampled and true distributions - needed to prove the O(1/√K) convergence rate.
- **Sequence likelihood estimation**: Computing and comparing log-probabilities of autoregressive sequences - needed to construct the Decision Potential Function from sampled sequences.
- **Manifold visualization**: Using UMAP for dimensionality reduction of high-dimensional inputs - needed to create interpretable 2D visualizations of the decision surface.
- **Isohypse extraction**: Identifying zero-height contours in potential surfaces - needed to locate the actual decision boundary from the computed surface.
- **Interpolation methods**: Techniques for estimating potential values across continuous space from discrete samples - needed to create smooth visualizations of the decision surface.

## Architecture Onboarding
- **Component map**: Input prompts → LLM generation (K samples) → Log-probability extraction → DPF calculation (ΦK) → UMAP embedding → Interpolation → Contour visualization
- **Critical path**: The sequence sampling and log-probability computation is the computational bottleneck, as each of K samples requires full autoregressive generation through the LLM.
- **Design tradeoffs**: Higher K improves accuracy (O(1/√K)) but increases computational cost linearly; nucleus sampling (p=0.9) balances exploration and efficiency versus pure top-k sampling.
- **Failure signatures**: Negative potential values in sparse regions indicate interpolation artifacts; slow error convergence suggests insufficient sampling relative to vocabulary diversity.
- **3 first experiments**:
  1. Validate error scaling by plotting |ΦK - Φ∞| vs K on log-log scale for convergence verification.
  2. Compare cubic vs. nearest-neighbor interpolation for handling negative values in sparse regions.
  3. Test different sampling strategies (temperature vs. nucleus) to assess robustness across generation methods.

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can the theoretical error bounds for K-DPS be tightened by replacing the global log-likelihood diameter with a local gap metric?
- Basis in paper: The authors state in Section 4.2 that "a sharper analysis could clearly replace this global spread with a more aggressive local gap" without inflating the bound.
- Why unresolved: The current theorems utilize a worst-case diameter factor R_K(x) which may inflate error estimates, and the authors suggest a tighter constant is mathematically possible.
- What evidence would resolve it: A proof of error bounds using local likelihood gaps (e.g., top-1 vs. top-3 difference) demonstrating tighter convergence constants than the current O(1/√K) rate.

### Open Question 2
- Question: How can geometric properties of the Decision Potential Surface (curvature, isohypse density) be quantitatively linked to specific LLM behaviors like hallucination or memorization?
- Basis in paper: The Introduction lists interpreting behaviors like hallucination as a key motivation, but the experiments focus solely on construction accuracy rather than analyzing the resulting surface geometry.
- Why unresolved: The paper provides the tool (DPS) and proves its validity, but leaves the actual analysis of the surface's shape to explain model failure modes for future work.
- What evidence would resolve it: Empirical correlations between high-curvature regions of the DPS and input prompts that induce hallucinations or adversarial vulnerabilities.

### Open Question 3
- Question: How can the decision potential surface be visualized robustly in high-dimensional input spaces without suffering from interpolation artifacts in sparse regions?
- Basis in paper: Section 5.4 notes that due to limitations in interpolation strategy, the visualized surface may be invalid in sparse regions, showing "significantly below zero" values.
- Why unresolved: Standard interpolation methods (cubic, linear) on dimensionality-reduced embeddings (UMAP) fail to maintain the non-negativity constraint of the Decision Potential Function in areas with low data density.
- What evidence would resolve it: A visualization method or manifold-aware interpolation technique that strictly enforces the theoretical non-negativity of the decision potential function across the entire input domain.

## Limitations
- Error bounds depend on sufficient sampling coverage - if the true top sequence is not included in K samples, approximation error can be significant.
- The O(1/√K) error scaling assumes random sampling from the true distribution, which may not hold for all LLM behaviors.
- UMAP dimensionality reduction introduces its own errors that could distort the apparent decision boundary structure.
- Method's effectiveness across different sampling strategies (top-k, temperature sampling) beyond nucleus sampling remains untested.

## Confidence
- **High Confidence**: The theoretical framework connecting DPS zero-isohypse to decision boundaries is sound and mathematically rigorous. The O(1/√K) error scaling has been validated empirically.
- **Medium Confidence**: The practical implementation details for hidden state extraction and interpolation methods are partially specified. The assumption of temperature=1.0 for nucleus sampling needs verification.
- **Low Confidence**: The method's robustness to different generation lengths and sampling strategies beyond nucleus sampling with p=0.9 is unclear.

## Next Checks
1. **Error Convergence Validation**: Reproduce Figure 2 by plotting absolute error |ΦK - Φ∞| against K on log-log scale to verify the theoretical O(1/√K) scaling holds across different datasets.
2. **Negative Value Handling Test**: Implement nearest-neighbor interpolation as an alternative to cubic interpolation and compare decision boundary visualizations to assess the impact of interpolation artifacts shown in Figure 4.
3. **Hidden State Pooling Experiment**: Test different hidden state extraction methods (final token vs. sequence pooling) to determine which best captures the input representation for UMAP dimensionality reduction.