---
ver: rpa2
title: 'Fast and Furious Symmetric Learning in Zero-Sum Games: Gradient Descent as
  Fictitious Play'
arxiv_id: '2506.13086'
source_url: https://arxiv.org/abs/2506.13086
tags:
- then
- games
- iterates
- regret
- energy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates regret minimization for two non-no-regret
  algorithms in zero-sum games: Fictitious Play and Online Gradient Descent with constant
  stepsizes. While these algorithms can exhibit instability and linear regret in general
  online learning settings, the paper establishes new regret guarantees for both algorithms
  on a class of symmetric zero-sum games that generalize Rock-Paper-Scissors to a
  weighted, n-dimensional regime.'
---

# Fast and Furious Symmetric Learning in Zero-Sum Games: Gradient Descent as Fictitious Play

## Quick Facts
- **arXiv ID:** 2506.13086
- **Source URL:** https://arxiv.org/abs/2506.13086
- **Reference count:** 40
- **Primary result:** Proves O(√T) regret for Fictitious Play and Gradient Descent with constant stepsizes in n-dimensional symmetric zero-sum games (weighted RPS)

## Executive Summary
This paper establishes new regret guarantees for two non-no-regret algorithms in zero-sum games. While Fictitious Play and Online Gradient Descent typically exhibit instability and linear regret in general online learning settings, the authors prove that both algorithms achieve O(√T) regret on a class of symmetric zero-sum games that generalize Rock-Paper-Scissors to weighted, n-dimensional regimes. The key insight is that with symmetric initializations, both algorithms cycle through pure strategies in a fixed deterministic order, and the phase durations grow proportionally to accumulated energy, yielding the √T regret bound. For Gradient Descent, the "fast and furious" behavior emerges when using sufficiently large constant stepsizes, forcing immediate convergence to vertices and effectively mimicking Fictitious Play's stable cycling.

## Method Summary
The paper analyzes two algorithms - Fictitious Play and Online Gradient Descent with constant stepsizes - on n-dimensional weighted Rock-Paper-Scissors games. Fictitious Play selects the pure strategy with highest cumulative payoff, while Gradient Descent uses large stepsizes to force immediate projection onto vertices of the probability simplex. Both algorithms maintain primal strategies (mixed strategies) and dual accumulators (cumulative payoffs). The analysis leverages the geometric structure of the weighted RPS matrix, showing that both algorithms cycle through pure strategies in a fixed order. The phase structure of this cycling, combined with energy accumulation, leads to the O(√T) regret bound. The paper also establishes a connection between the dual space geometry of Fictitious Play and Gradient Descent.

## Key Results
- Proves O(√T) regret for Fictitious Play with any tiebreaking rule on n-dimensional weighted RPS games
- Establishes the first "fast and furious" behavior for Gradient Descent (sublinear regret without time-vanishing stepsizes) in zero-sum games larger than 2x2
- Shows Gradient Descent with sufficiently large constant stepsize achieves O(√T) regret for almost all symmetric initializations
- Introduces a new class of games where Karlin's Fictitious Play conjecture holds
- Demonstrates equivalence between Gradient Descent with large stepsizes and Fictitious Play behavior through cycling analysis

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** In n-dimensional RPS games, FP and GD cycle through pure strategies (vertices) in fixed, deterministic order regardless of tiebreaking rules
- **Mechanism:** The payoff matrix structure causes dual vectors to evolve such that the active coordinate dominates its neighbor until the next neighbor takes over, forcing the primal strategy to rotate cyclically
- **Core assumption:** The game must be an n-dimensional RPS matrix with skew-symmetric cyclic dominance
- **Evidence anchors:**
  - [section 4.1] Lemma 10 proves cycling order for FP under arbitrary tiebreaking
  - [section 13.2] Lemma 28 establishes analogous cycling for GD in large stepsize regime
  - [corpus] Related work discusses similar cyclic structures in zero-sum games via different algorithmic modifications
- **Break condition:** General zero-sum games may not exhibit this fixed cycling order

### Mechanism 2
- **Claim:** Regret is bounded by O(√T) because time spent in each phase grows proportionally to total energy accumulated so far
- **Mechanism:** A phase ends when energy increases. Since strategy cycles and energy gain per phase is constant, duration τ_k ∝ Energy_k. Total time T ≈ Στ_k ≈ Σk ≈ K², where K is number of phases. Since regret ∝ K, we get Regret ∝ √T
- **Core assumption:** Initialization is symmetric (or reaches vertex quickly)
- **Evidence anchors:**
  - [section 4.2] Lemma 13 proves phase lengths proportional to energy
  - [section 5.1] Theorem 17 extends this energy-time logic to GD
- **Break condition:** If strategy does not cycle (gets stuck or chaotic), phase lengths may not grow, breaking √T bound

### Mechanism 3
- **Claim:** Using sufficiently large constant stepsize forces GD to immediately jump to a vertex, making it equivalent to FP
- **Mechanism:** Large stepsize η > 1/γ(x₀) forces projection step in GD to result in basis vector eᵢ, removing interior dynamics that cause instability and aligning GD with stable cycling of FP
- **Core assumption:** Large stepsizes are usually thought to cause instability; here they act as structural constraint
- **Evidence anchors:**
  - [section 5.1] Lemma 16 provides exact threshold for η required to force convergence to vertex in one step
  - [section 1] Introduction contrasts this "fast and furious" behavior with conventional wisdom requiring vanishing stepsizes
- **Break condition:** If η is too small, iterate remains in simplex interior, phase structure breaks, and regret guarantees are lost

## Foundational Learning

- **Concept: Online Convex Optimization (Regret)**
  - **Why needed here:** Paper measures success via "Regret" (difference between cumulative loss and best fixed strategy in hindsight). Understanding O(√T) as gold standard for sublinear regret is essential
  - **Quick check question:** If algorithm has linear regret (Reg(T) ∝ T), does time-averaged strategy converge to Nash Equilibrium?

- **Concept: Fictitious Play vs. Gradient Descent**
  - **Why needed here:** Paper bridges two distinct algorithms. FP is "Follow the Leader" (choose pure strategy with highest cumulative payoff), while GD uses gradient information and regularization. Understanding their standard differences is key to seeing why they converge here
  - **Quick check question:** Does standard Fictitious Play require a learning rate parameter?

- **Concept: Skew-Symmetric Matrices & The Simplex**
  - **Why needed here:** Game defined on probability simplex (Δⁿ) with skew-symmetric payoff matrix (A = -Aᵀ). "Dual" analysis relies on linear algebra in ℝⁿ outside simplex
  - **Quick check question:** In symmetric zero-sum game, if x* is Nash Equilibrium, what is Ax*?

## Architecture Onboarding

- **Component map:** Game Environment -> Learner (Primal) -> Accumulator (Dual) -> Update Logic -> Monitor
- **Critical path:** Correct implementation of Large Stepsize Update (Lemma 16). System fails if stepsize η does not force primal iterate x₁ to vertex immediately. Ensure projection step in GD handles large updates correctly
- **Design tradeoffs:**
  - FP vs. GD: FP simpler (no projection math) but GD smoother for general matrices (though paper argues for "hard" large-step version)
  - Stepsize Selection: Small η provides smoothness but no theoretical guarantee in this framework. Large η provides guarantee but reduces strategy to "pure" cycling (no mixed strategies during loop)
- **Failure signatures:**
  - Interior Stagnation: If η too small, xₜ stays strictly inside simplex. No cycling behavior, regret may grow faster than √T
  - Irregular Cycling: If game is not weighted RPS matrix, cyclic ordering breaks, energy/phase analysis likely fails
- **First 3 experiments:**
  1. **Baseline Cycling (FP):** Implement FP on 3×3 RPS. Initialize at vertex. Plot primal trajectory to confirm e₁ → e₂ → e₃ cycle
  2. **Large Stepsize GD (The "Fast" regime):** Implement GD with η=10. Initialize interior (e.g., x₀=[0.33, 0.33, 0.34]). Verify x₁ is vertex and subsequent iterations cycle identically to FP
  3. **Regret Comparison:** Run both algorithms for T=1000 steps. Plot Reg(T)/√T. Curve should approach constant, confirming O(√T) bound

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Do other instantiations of Follow-the-Regularized-Leader (FTRL), such as Multiplicative Weights Update, obtain O(√T) regret with constant stepsizes in symmetric zero-sum games?
- **Basis in paper:** Explicit: Section 6 states, "Finally, obtaining regret guarantees for other instantiations of FTRL (beyond Gradient Descent) with constant stepsizes remains an important open challenge"
- **Why unresolved:** Paper's analysis relies on geometry of ℓ₂ regularizer (Gradient Descent) and its connection to Fictitious Play; other regularizers (e.g., entropy) do not necessarily result in iterates on simplex boundary, complicating cycling analysis
- **What evidence would resolve it:** Proof of O(√T) regret for MWU on high-dimensional RPS matrices with constant stepsizes, or lower bound proving linear regret in this setting

### Open Question 2
- **Question:** Can Gradient Descent obtain sublinear regret using any constant stepsize, or is large stepsize regime necessary?
- **Basis in paper:** Inferred: Section 14.1 notes that "obtaining O(√T) regret bounds for Gradient Descent using any constant stepsize thus remains open," as boundary invariance property alone is insufficient for main proof technique
- **Why unresolved:** Proof of Theorem 17 requires stepsize η to be sufficiently large to force primal iterate to vertex in one step and ensure regular cycling
- **What evidence would resolve it:** Theorem extending O(√T) regret guarantee to small constant stepsizes, or counter-example showing linear regret for small constant stepsizes in n-dimensional RPS

### Open Question 3
- **Question:** How do different tiebreaking rules impact regret of Fictitious Play in general symmetric zero-sum games beyond RPS class?
- **Basis in paper:** Explicit: Section 6 states, "Better understanding the interplay between tiebreaking and regret for other classes of matrices is left as open"
- **Why unresolved:** Theorem 11 ensures O(√T) regret for RPS matrices under any tiebreaking rule, but this robustness may not hold for general zero-sum matrices (e.g., diagonal matrices) where prior work relied on specific rules
- **What evidence would resolve it:** Regret bounds for Fictitious Play on diagonal or general symmetric matrices under various tiebreaking rules (e.g., adversarial vs. tournament)

## Limitations

- **Structural dependency:** The O(√T) regret guarantees rely specifically on the weighted RPS matrix structure and may not generalize to arbitrary symmetric zero-sum games
- **Initialization requirement:** The results assume symmetric initializations, which may not hold in practice
- **Large stepsize constraint:** For Gradient Descent, the "fast and furious" behavior requires sufficiently large constant stepsizes, which contradicts conventional wisdom about stability

## Confidence

- **Core claims for weighted RPS class:** High - rigorous proofs and clear algorithmic equivalence under large stepsizes
- **Generalization beyond RPS:** Medium - structural assumptions may break down in more complex settings
- **Practical applicability:** Medium - theoretical framework is sound but empirical validation on larger n or diverse game structures would strengthen findings

## Next Checks

1. Verify cycling behavior by implementing FP on 3×3 RPS and plotting the primal trajectory to confirm the e₁ → e₂ → e₃ cycle pattern
2. Test the large stepsize threshold by implementing GD with η=10 on 3×3 RPS and verifying x₁ is a vertex that initiates identical cycling to FP
3. Validate the O(√T) regret bound by running both algorithms for T=1000 steps and confirming Reg(T)/√T approaches a constant value