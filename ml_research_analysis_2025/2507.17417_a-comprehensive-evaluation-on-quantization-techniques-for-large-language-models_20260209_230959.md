---
ver: rpa2
title: A Comprehensive Evaluation on Quantization Techniques for Large Language Models
arxiv_id: '2507.17417'
source_url: https://arxiv.org/abs/2507.17417
tags:
- quantization
- scaling
- rotation
- methods
- error
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a comprehensive evaluation of post-training
  quantization techniques for large language models (LLMs), focusing on W4A4 precision.
  The authors systematically decompose quantization into two key steps: pre-quantization
  transformation (e.g., scaling and rotation) and quantization error mitigation (e.g.,
  GPTQ and low-rank compensation).'
---

# A Comprehensive Evaluation on Quantization Techniques for Large Language Models

## Quick Facts
- **arXiv ID:** 2507.17417
- **Source URL:** https://arxiv.org/abs/2507.17417
- **Reference count:** 40
- **Primary result:** Systematic evaluation of post-training quantization techniques for LLMs at W4A4 precision, revealing optimized rotation/scaling and combined GPTQ+low-rank compensation as top performers.

## Executive Summary
This paper presents a comprehensive evaluation of post-training quantization techniques for large language models (LLMs) at W4A4 precision (4-bit weights, 4-bit activations). The authors systematically decompose quantization into pre-quantization transformation (scaling and rotation) and quantization error mitigation (GPTQ and low-rank compensation). Through extensive experiments across multiple LLM architectures, they demonstrate that optimized rotation and scaling yield the best pre-quantization performance, and that combining low-rank compensation with GPTQ occasionally outperforms using GPTQ alone. The study also extends to FP4 formats (MXFP4 and NVFP4), finding that while FP4 handles long-tail distributions better than INT4, rotation-based pre-quantization methods show limited improvement for these formats.

## Method Summary
The paper proposes a two-step framework for LLM quantization. First, pre-quantization transformation applies optimized rotation (using Riemannian gradient descent) and scaling (SmoothQuant with Î±=0.5) to flatten activation distributions and reduce outlier impact. Second, quantization error mitigation uses GPTQ for weight updates and optionally low-rank compensation (SVD-based) to approximate and subtract quantization error. The evaluation uses LLaMA-3.2 (1B, 3B), LLaMA-3.1 (8B, 70B), Qwen2.5-3B, and Qwen1.5-MoE-A2.7B models, with 128 WikiText-2 samples for GPTQ/low-rank calibration and 1,000 samples for learning rotation/scaling matrices. Key settings include per-token/group-128 asymmetric quantization for activations, per-channel/group-128 symmetric for weights, and linear search for weight clipping optimization.

## Key Results
- Optimized rotation and scaling yield superior pre-quantization performance compared to non-optimized alternatives
- Finer granularity improves quantization accuracy but increases storage overhead for scaling factors
- Asymmetric quantization for activations provides significantly better results than symmetric quantization
- NVFP4 (E4M3 scaling, group size 16) outperforms MXFP4 (E8M0 scaling, group size 32)
- Combining low-rank compensation with GPTQ occasionally outperforms GPTQ alone for perplexity and zero-shot accuracy

## Why This Works (Mechanism)

### Mechanism 1: Outlier Smoothing via Pre-Quantization Transformation
Activation outliers are the primary bottleneck for low-bit quantization. Rotation matrices (e.g., Hadamard) redistribute outlier energy across channels, flattening the distribution, while scaling rebalances magnitude between activations and weights. This preserves the model's functional logic while changing numerical distributions to be more quantization-friendly.

### Mechanism 2: Error Offset via Compensation Branches
Quantization error can be analytically approximated and subtracted. Self-compensation (GPTQ) uses Hessian-based approximations to adjust weights directly, while low-rank compensation creates auxiliary matrices (SVD-based) to reconstruct the residual error during inference. This restores performance without full retraining.

### Mechanism 3: Format-Granularity Alignment (FP4 vs. INT4)
FP4 (E2M1) provides denser sampling near zero and sparser sampling for outliers (logarithmic distribution), whereas INT4 is linear. NVFP4 outperforms MXFP4 by using E4M3 scaling factors (higher precision) and smaller group sizes (16 vs. 32), better aligning with inter-channel variance in LLM weight distributions.

## Foundational Learning

- **Concept: Activation Outliers**
  - Why needed: LLM activations contain extreme values ("outliers") that are 100x larger than typical values, causing standard quantization to fail
  - Quick check: Why does a single outlier ruin the quantization accuracy for an entire channel in per-tensor quantization?

- **Concept: Orthogonal Rotation**
  - Why needed: Mathematical tool used to "mix" channel data, spreading outlier energy so no single channel is unquantizable
  - Quick check: If you rotate input X by matrix O, how must you transform weight W to keep the linear layer output XW mathematically equivalent?

- **Concept: Granularity vs. Overhead**
  - Why needed: The paper highlights that finer grouping (smaller group size) improves accuracy but increases storage for scaling factors
  - Quick check: What is the storage overhead trade-off when moving from group size 128 to group size 16?

## Architecture Onboarding

- **Component map:** Input (FP16/BF16 Weights & Activations) -> Pre-transform (Optimized Rotation + Scaling) -> Quantize (INT4/FP4 with per-group/channel granularity) -> Error Mitigation (GPTQ + Optional Low-Rank branch fusion) -> Output (Quantized model + auxiliary parameters)

- **Critical path:** Optimization of the rotation matrix (via Riemannian gradient descent) is the most compute-intensive step during the calibration phase.

- **Design tradeoffs:**
  - NVFP4 vs. MXFP4: NVFP4 offers better perplexity (uses E4M3 scaling) but is newer hardware-specific; MXFP4 uses E8M0 (power-of-two only) which is coarser
  - GPTQ vs. Low-Rank: GPTQ is slower during calibration but adds no inference overhead. Low-Rank adds ~2-3% memory/compute overhead at inference

- **Failure signatures:**
  - PPL Explosion: High WikiText-2 perplexity (>15.0 for 1B model) usually indicates missing pre-quantization transformation or incorrect scaling factor implementation
  - FP4 + Rotation Degradation: Applying rotation optimized for INT4 to NVFP4/MXFP4 may degrade performance

- **First 3 experiments:**
  1. Baseline Sanity Check: Quantize LLaMA-3.2-1B to W4A4 using RTN only vs. RTN + Optimized Rotation to verify outlier handling
  2. Format Comparison: Evaluate MXFP4 (g32) vs. NVFP4 (g16) on WikiText-2 PPL to validate the scaling factor format hypothesis
  3. Mitigation Ablation: Compare GPTQ alone vs. GPTQ + Low-Rank (rank=32) on zero-shot tasks to measure accuracy recovery vs. memory overhead

## Open Questions the Paper Calls Out

### Open Question 1
Why do rotation-based pre-quantization transformations, which are highly effective for INT4, fail to yield performance gains for FP4 variants (MXFP4 and NVFP4)? The paper identifies this phenomenon but does not propose or validate alternative transformations specifically tailored for the non-uniform distribution of FP4 data formats.

### Open Question 2
What is the optimal scaling factor format and precision that balances storage overhead and accuracy for FP4 quantization? While the paper demonstrates NVFP4's superiority over MXFP4, the search space for scaling factor configurations remains partially unexplored, particularly regarding the trade-off between the extra per-tensor FP32 factor and hardware efficiency.

### Open Question 3
Can low-rank compensation methods be modified to minimize loss error (like GPTQ) rather than weight error to consistently outperform self-compensation techniques? The paper hypothesizes that low-rank compensation underperforms GPTQ because it approximates weight quantization error rather than loss error, but does not evaluate loss-error-optimized low-rank methods.

## Limitations

- Hardware specificity: NVFP4 analysis is constrained by dependence on NVIDIA Hopper architecture, limiting generalizability
- Calibration data dependency: Optimal transformation parameters rely heavily on calibration data quality and representativeness
- Inconsistent effectiveness: Low-rank compensation shows inconsistent performance across different model architectures
- Format-specific applicability: Rotation-based methods provide limited benefit for FP4 formats despite excelling for INT4

## Confidence

**High Confidence:**
- Asymmetric quantization for activations consistently outperforms symmetric quantization
- Finer granularity improves quantization accuracy but incurs higher storage overhead
- GPTQ implementation and baseline effectiveness for W4A4 quantization
- Two-step framework (pre-quantization transformation + error mitigation) is valid

**Medium Confidence:**
- Optimized rotation and scaling yield superior pre-quantization performance
- Low-rank compensation combined with GPTQ occasionally outperforms GPTQ alone
- NVFP4's superiority due to E4M3 scaling and smaller group sizes
- Specific hyperparameter choices for RiemannAdam optimization and SVD rank selection

**Low Confidence:**
- Generalizability of optimal transformation parameters across diverse LLM architectures
- Long-term stability of low-rank compensation branches in production environments
- Scalability to models significantly larger than 70B parameters

## Next Checks

1. **Hardware Portability Validation:** Implement the complete quantization pipeline (including NVFP4-specific optimizations) on alternative hardware architectures (AMD Instinct, Intel Gaudi) to verify performance advantages are not platform-dependent.

2. **Cross-Domain Calibration Test:** Apply optimized rotation and scaling matrices learned from WikiText-2 calibration to models evaluated on completely different domains (medical text, code, multilingual corpora) to measure performance degradation.

3. **Ablation on Low-Rank Parameters:** Systematically vary SVD rank parameter (k) from 8 to 128 in increments of 8, measuring trade-off between accuracy recovery and memory overhead across all tested model architectures.