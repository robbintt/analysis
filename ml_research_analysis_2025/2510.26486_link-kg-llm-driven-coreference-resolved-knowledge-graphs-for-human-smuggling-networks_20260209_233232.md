---
ver: rpa2
title: 'LINK-KG: LLM-Driven Coreference-Resolved Knowledge Graphs for Human Smuggling
  Networks'
arxiv_id: '2510.26486'
source_url: https://arxiv.org/abs/2510.26486
tags:
- resolution
- coreference
- graph
- prompt
- link-kg
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LINK-KG is a modular framework for constructing knowledge graphs
  from legal documents on human smuggling networks. It integrates a three-stage LLM-guided
  coreference resolution pipeline with domain-specific KG extraction.
---

# LINK-KG: LLM-Driven Coreference-Resolved Knowledge Graphs for Human Smuggling Networks

## Quick Facts
- arXiv ID: 2510.26486
- Source URL: https://arxiv.org/abs/2510.26486
- Reference count: 27
- Primary result: Reduces node duplication by 45.21% and noisy nodes by 32.22% in human smuggling network KGs

## Executive Summary
LINK-KG is a modular framework for constructing knowledge graphs from legal documents on human smuggling networks. It integrates a three-stage LLM-guided coreference resolution pipeline with domain-specific KG extraction. Central to the approach is a type-specific Prompt Cache that tracks and resolves references across document chunks, enabling clean and disambiguated narratives for structured knowledge graph construction from both short and long legal texts. Evaluated on U.S. federal and state court documents, LINK-KG reduces average node duplication by 45.21% and noisy nodes by 32.22% compared to baseline methods, resulting in cleaner and more coherent graph structures. These improvements establish LINK-KG as a strong foundation for analyzing complex criminal networks.

## Method Summary
LINK-KG constructs knowledge graphs from legal case documents through a three-stage LLM-based pipeline. First, an NER-LLM extracts entities and descriptions per document chunk. Second, a Mapping-LLM builds a type-specific Prompt Cache mapping aliases to canonical names, optionally refined through a gleaning pass. Third, a Resolve-LLM rewrites chunks using the finalized cache, producing coreference-resolved text. The resolved text is then fed to GraphRAG for entity-relationship extraction and graph construction. The approach addresses loss-in-the-middle LLM attention issues and reduces duplication through structured, type-aware coreference resolution.

## Key Results
- Reduces average node duplication by 45.21% and noisy nodes by 32.22% compared to baseline methods
- Improves graph coherence and reduces redundancy in human smuggling network knowledge graphs
- Demonstrates effectiveness on both short (≤2,500 words) and long (>2,500 words) legal documents

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Type-specific Prompt Cache maintains consistent entity mappings across document chunks, enabling coherent resolution of ambiguous references.
- Mechanism: The cache stores alias-to-canonical mappings separately per entity type (Person, Location, Organization, etc.) with auxiliary descriptions. Each chunk updates the cache incrementally; a "gleaning" pass revises earlier mappings using context from later chunks. This addresses the "loss-in-the-middle" problem where standard LLMs forget mid-document content.
- Core assumption: Entity references in legal text follow type-consistent patterns; aliases within the same type share enough context for disambiguation without cross-type interference.
- Evidence anchors:
  - [abstract] "At the core of our approach is a type-specific Prompt Cache, which consistently tracks and resolves references across document chunks"
  - [Section III-A-2] "We use type-specific prompt caches that store alias-to-canonical mappings separately for each entity type (e.g., person, location), allowing precise disambiguation within types while keeping token usage low"
  - [corpus] LLMLINK (prior work) used type-agnostic caches and struggled with plural mentions and context overflow; LINK-KG addresses this via type separation
- Break condition: If entity types frequently overlap semantically (e.g., an organization name also used as a location metonym), type-specific separation may prevent cross-type disambiguation; the cache would need cross-type reference capability.

### Mechanism 2
- Claim: Structured three-stage pipeline (NER → Mapping → Resolve) reduces attention spread and misclassification in dense legal text.
- Mechanism: Stage 1 extracts proper nouns and noun phrases with descriptions per chunk. Stage 2 builds/updates the prompt cache with alias-to-canonical mappings. Stage 3 performs chunk-wise substitution using the global cache. This separation prevents the LLM from simultaneously handling extraction, resolution, and rewriting—tasks that compete for attention in long documents.
- Core assumption: Legal documents can be chunked without losing essential intra-chunk context for initial entity recognition; inter-chunk dependencies are recoverable via the cache.
- Evidence anchors:
  - [Section III-A] "LINK-KG comprises two core components: (1) A two-stage, prompt-based coreference module... (2) A knowledge graph construction module"
  - [Section III-B] "Sequential Entity Extraction to Mitigate Attention Spread... we enforce a fixed extraction order where entities are extracted by type before relationship extraction"
  - [corpus] CORE-KG (prior work) used single-pass coreference and struggled with long documents due to context window constraints
- Break condition: If critical coreference chains span chunk boundaries with no local disambiguating context, the per-chunk NER stage may produce incomplete entity lists, propagating errors to the cache.

### Mechanism 3
- Claim: Plural-aware and context-sensitive prompting prevents erroneous singular resolutions and reduces speculative entity creation.
- Mechanism: The prompt enforces that plural mentions (e.g., "the agents") resolve only when all referenced individuals are explicitly named; otherwise, they map to null. Shifting references (e.g., "the driver" referring to different people) use surrounding context and auxiliary descriptions to generate disambiguating aliases like "the driver in a truck" vs. "the driver in a patrol car."
- Core assumption: Legal narratives contain sufficient explicit naming for plural resolution; ambiguity is better left unresolved (null) than incorrectly merged.
- Evidence anchors:
  - [Section III-A-2a] "Plural mentions are resolved by linking to multiple canonical names when specific entities are implied or mapped to null if ambiguous"
  - [Section V-A] Case analysis shows correct mapping of "the agents" to named individuals S.P. and A.B., and "the passengers" to M.D.J.G., L.R.C., etc.
  - [corpus] Weak direct evidence; LLMLINK struggled with plural-to-singular errors, but no corpus paper provides comparative plural-handling metrics
- Break condition: If documents heavily use plurals without explicit enumeration (common in redacted or summary documents), the null-mapping strategy will produce incomplete graphs rather than informative clusters.

## Foundational Learning

- Concept: Coreference Resolution
  - Why needed here: The entire LINK-KG architecture is built to resolve when different noun phrases refer to the same entity across a document—fundamental to preventing node duplication in knowledge graphs.
  - Quick check question: Given "Officer Ross detained the driver. The driver later testified," would a coreference system link both "the driver" instances? What if a second driver appears later?

- Concept: Loss-in-the-Middle Phenomenon
  - Why needed here: LINK-KG explicitly addresses this LLM limitation where content in the middle of long documents receives diminished attention, causing fragmented entity linking.
  - Quick check question: Why might an LLM correctly extract entities from the first and last paragraphs of a 50-page document but miss connections in the middle?

- Concept: Prompt Cache / Memory Mechanism
  - Why needed here: The type-specific prompt cache is the central innovation—a persistent memory structure that accumulates entity mappings across chunks.
  - Quick check question: If processing a 10-chunk document, should the cache from chunk 5 be available when processing chunk 8? What tradeoffs does this create?

## Architecture Onboarding

- Component map: NER-LLM -> Mapping-LLM -> Resolve-LLM -> GraphRAG
- Critical path:
  1. Chunk document → 2. NER-LLM per chunk → 3. Mapping-LLM iteratively builds cache → 4. Optional gleaning pass → 5. Resolve-LLM per chunk → 6. Merge resolved chunks → 7. GraphRAG extraction → 8. Post-process (merge duplicates, filter noise)
- Design tradeoffs:
  - Chunk size (300 tokens): Smaller chunks preserve local context but increase cache management overhead; larger chunks risk attention diffusion
  - Gleaning pass: Improves consistency by revisiting early mappings with later context, but doubles Mapping-LLM compute
  - Type-specific vs. unified cache: Type separation reduces token bloat but prevents cross-type disambiguation (e.g., "Apple" as company vs. fruit)
- Failure signatures:
  - High null mappings for plurals: Document may lack explicit enumerations; consider relaxed resolution rules
  - Persistent node duplication: Cache may not be propagating correctly between chunks; verify mapping accumulation
  - Noise rate unchanged: In-prompt filtering rules may not match domain-specific irrelevant entities; review filtering criteria
  - Context overflow errors: Cache growing unbounded for long documents; implement cache pruning or summarization
- First 3 experiments:
  1. Ablation on gleaning pass: Run pipeline with and without the second Mapping-LLM pass on 3 long documents; measure duplication rate difference
  2. Chunk size sensitivity: Test 200, 300, 500 token chunks on same document; track duplication, noise, and runtime
  3. Type-specific vs. unified cache: Compare LINK-KG against a variant with a single shared cache on a document with heavy cross-type references; measure resolution accuracy and token usage

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does LINK-KG's performance scale with documents significantly longer than those tested (e.g., 10,000+ words), given the context overflow challenges noted in prior work?
- Basis in paper: [inferred] The paper mentions LLMLINK struggles with "context overflow from an expanding prompt cache" (Page 2), yet LINK-KG's longest test cases are only ~5,000+ words. The prompt cache grows with each resolved entity, potentially creating similar overflow issues at extreme lengths.
- Why unresolved: No experiments on very long documents; longest cases tested were just above 2,500 words.
- What evidence would resolve it: Experiments on longer legal documents (10,000+ words) showing prompt cache size growth rates and resolution accuracy maintenance.

### Open Question 2
- Question: Does the type-specific Prompt Cache approach transfer effectively to other legal domains (e.g., fraud, drug trafficking) or non-legal narratives?
- Basis in paper: [inferred] The paper states the pipeline is "broadly applicable across domains" (Page 3) but only evaluates human smuggling cases with seven specific entity types.
- Why unresolved: No cross-domain experiments; entity types and prompts are domain-specific.
- What evidence would resolve it: Application of LINK-KG to different legal domains with domain-appropriate entity type definitions, measuring node duplication and noise reduction.

### Open Question 3
- Question: What are the computational costs (latency, token usage) of the three-stage pipeline compared to single-pass approaches?
- Basis in paper: [inferred] The framework uses multiple LLM calls (NER-LLM, Mapping-LLM twice, Resolve-LLM, plus gleaning) per chunk per entity type, but reports no efficiency metrics.
- Why unresolved: No latency, cost, or computational overhead analysis provided in experiments.
- What evidence would resolve it: Runtime measurements and token counts per document across varying lengths; comparison with baseline computational requirements.

### Open Question 4
- Question: How would performance change with formal evaluation against human-annotated ground truth knowledge graphs?
- Basis in paper: [explicit] The paper states "no annotated ground truth exists for structured knowledge graphs in this domain" (Page 6), relying instead on proxy metrics (duplication/noise rates).
- Why unresolved: Lack of annotated legal KG datasets for human smuggling; evaluation relies on structural metrics rather than extraction accuracy.
- What evidence would resolve it: Creation of annotated benchmark with entity and relation labels; comparison of precision, recall, and F1 against human annotations.

## Limitations

- Evaluation corpus is small (16 legal documents), limiting generalizability
- Node-duplication metric uses 75% partial similarity threshold, which may be lenient
- Type-specific cache design effectiveness in cross-type disambiguation scenarios is untested
- No computational efficiency metrics provided for the multi-stage pipeline

## Confidence

- **High**: The modular three-stage pipeline design and its role in reducing attention spread and misclassification.
- **Medium**: The 45.21% reduction in node duplication and 32.22% reduction in noisy nodes, given the limited evaluation corpus and lack of full prompt details.
- **Low**: The robustness of the type-specific Prompt Cache in cross-type ambiguity scenarios and its scalability to much larger or more diverse legal corpora.

## Next Checks

1. **Prompt template reconstruction**: Reconstruct and test the NER, Mapping, and Resolve LLM prompts using the paper's descriptions and repository code; measure impact on coreference resolution accuracy.
2. **Gleaning pass ablation**: Run the pipeline with and without the gleaning pass on three long documents; quantify changes in node duplication and noise rates.
3. **Cross-type ambiguity stress test**: Create a synthetic legal document where entity names are used across types (e.g., "Apple" as company and location); test whether LINK-KG's type-specific cache handles these cases or produces errors.