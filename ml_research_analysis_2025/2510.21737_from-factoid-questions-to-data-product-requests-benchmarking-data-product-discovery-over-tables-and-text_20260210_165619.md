---
ver: rpa2
title: 'From Factoid Questions to Data Product Requests: Benchmarking Data Product
  Discovery over Tables and Text'
arxiv_id: '2510.21737'
source_url: https://arxiv.org/abs/2510.21737
tags:
- data
- product
- tables
- discovery
- table
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Data Product Benchmark is the first large-scale benchmark for data
  product discovery over hybrid table-text corpora. It transforms existing QA datasets
  into realistic analytical requests by clustering related tables and passages, then
  generating professional-level DPRs via multiple LLMs.
---

# From Factoid Questions to Data Product Requests: Benchmarking Data Product Discovery over Tables and Text

## Quick Facts
- arXiv ID: 2510.21737
- Source URL: https://arxiv.org/abs/2510.21737
- Reference count: 40
- First large-scale benchmark for data product discovery over hybrid table-text corpora

## Executive Summary
Data Product Benchmark transforms existing QA datasets into realistic analytical requests by clustering related tables and passages, then generating professional-level Data Product Requests (DPRs) via multiple LLMs. The benchmark is validated through two-phase LLM-as-a-judge evaluation and includes datasets from HybridQA, TAT-QA, and ConvFinQA with 11,749 validated DPR-data product pairs. Baseline experiments show dense and hybrid retrieval methods significantly outperform BM25, with ConvFinQA achieving 0.87 Full Recall@100 while HybridQA remains challenging at 0.11.

## Method Summary
The benchmark constructs DPR discovery tasks by first clustering tables using question embeddings (BERTopic) and refining clusters via schema similarity (k-means on column embeddings). Five LLMs generate candidate DPRs, which undergo two-phase validation: quality/clarity filtering followed by alignment checking against ground-truth components. The resulting dataset supports retrieval evaluation using BM25, dense embeddings (granite-embedding-125m-english), and hybrid approaches with reciprocal rank fusion.

## Key Results
- Dense retrieval (0.62 Recall@100) outperforms BM25 (0.33) on HybridQA text retrieval
- Hybrid approach achieves best Full Recall@100: 0.87 on ConvFinQA, 0.30 on TAT-QA, 0.11 on HybridQA
- Full Recall@100 reveals multi-component assembly difficulty: 0.11 on HybridQA despite 0.62 text Recall@100

## Why This Works (Mechanism)

### Mechanism 1
Clustering tables by question semantics surfaces analytical themes better than schema alone. Concatenate all questions per table → embed with sentence encoder → apply BERTopic topic modeling → tables with similar question intents cluster together regardless of surface schema differences. Core assumption: Questions reflect user intent more directly than table structure. Break condition: If questions are noisy, sparse, or unrepresentative of actual use cases, clustering will produce incoherent topics.

### Mechanism 2
Schema-based refinement bounds cluster size and improves product coherence. Within each topic cluster, embed table schemas (column names) → apply k-means if cluster size > K → enforce maximum tables-per-product constraint. Core assumption: Tables with similar column structures support related analytical workflows. Break condition: If schemas are inconsistent (synonyms, abbreviations) or tables are over-normalized, schema similarity becomes noisy.

### Mechanism 3
Multi-LLM generation with two-phase validation yields higher-quality DPRs than single-model approaches. Five LLMs generate candidate DPRs independently → Phase 1 filters for quality/clarity → Phase 2 validates alignment with ground-truth tables/passages → majority voting prunes irrelevant components. Core assumption: Ensemble reduces model-specific phrasing bias; LLMs can reliably judge quality and alignment. Break condition: If all LLMs share systematic biases (e.g., over-abstracting, hallucinating requirements), validation will not catch errors.

## Foundational Learning

- Concept: **Dense vs. Sparse Retrieval**
  - Why needed here: Baseline experiments show dense embeddings (0.62 Recall@100) outperform BM25 (0.33) on HybridQA text retrieval; understanding when semantic matching beats lexical overlap is critical.
  - Quick check question: On HybridQA tables, BM25 achieves 0.02 Recall@20 while dense achieves 0.36—why might lexical matching fail on Wikipedia-style schemas?

- Concept: **Hybrid Retrieval (Reciprocal Rank Fusion)**
  - Why needed here: Hybrid approach (BM25 + dense) achieves best Full Recall@100 on ConvFinQA (0.87) and TAT-QA (0.30), suggesting complementary signals.
  - Quick check question: If BM25 excels at precise keyword matches and dense captures semantics, when would combining them hurt performance?

- Concept: **Full Recall as Completeness Metric**
  - Why needed here: Unlike standard IR metrics, Full Recall@100 requires retrieving *all* components of a data product—partial matches don't count.
  - Quick check question: HybridQA achieves only 0.11 Full Recall@100 despite 0.62 text Recall@100—what does this gap reveal about multi-component assembly difficulty?

## Architecture Onboarding

- Component map:
  1. Topic Clustering Module: BERTopic on question embeddings → K_raw raw clusters
  2. Schema Refinement Module: k-means on schema embeddings within oversized clusters → bounded products
  3. DPR Generation Module: 5-LLM ensemble (Llama 3.3 70B, DeepSeek V3, GPT-OSS 120B, Qwen 2.5 72B, Mistral 8×22B) → candidate requests
  4. Validation Module: 2-phase LLM-as-judge → quality/clarity filter → alignment check → final DPR-ground-truth pairs
  5. Retrieval Backend: Milvus vector store with multi-vector collection; tables indexed by title + headers only (no cell values)

- Critical path:
  1. Start with table-text QA dataset (HybridQA, TAT-QA, or ConvFinQA)
  2. Topic clustering must complete before refinement
  3. Validation gates all benchmark entries—failed DPRs are discarded
  4. Retrieval experiments use only validated pairs

- Design tradeoffs:
  - Tables indexed without cell values: Prevents memorization leakage but may miss content-level signals
  - K (max tables per product): Not specified in paper; lower K improves coherence but may fragment real products
  - 5-LLM ensemble: Increases robustness but adds cost/latency; paper doesn't ablate ensemble size
  - Dense embedding model choice (granite-embedding-125m-english): Lightweight but may underperform larger models on domain-specific corpora

- Failure signatures:
  - Low Full Recall on HybridQA (0.11): Multi-table, multi-hop products with diverse Wikipedia topics are hardest to retrieve completely
  - BM25 near-zero on tables (0.01–0.03): Schema headers lack lexical overlap with high-level DPRs
  - High variance across datasets: ConvFinQA (0.87 Full Recall) vs. HybridQA (0.11) suggests domain-specific tuning is essential

- First 3 experiments:
  1. Reproduce baseline retrieval: Run BM25, dense, and hybrid on one dataset (start with ConvFinQA—highest scores) to validate pipeline integration with Milvus.
  2. Ablate validation phases: Measure how many DPRs survive Phase 1 vs. Phase 2; identify which LLMs are strictest judges.
  3. Test schema-only vs. question-only clustering: Run topic clustering using only schemas (no questions) and compare cluster coherence; paper assumes questions are better but doesn't ablate this choice.

## Open Questions the Paper Calls Out
- How can evaluation be extended beyond retrieval accuracy to capture coherence, analytical usefulness, and user experience for data product discovery? [explicit] The Limitations section states: "Current evaluation focuses mainly on retrieval accuracy, leaving out other important dimensions such as coherence, analytical usefulness, or user experience."
- What domain-adaptive methods can bridge the performance gap between high-performing financial datasets (ConvFinQA: 0.87 Full Recall@100) and challenging open-domain datasets (HybridQA: 0.11)? [explicit] The paper notes: "Domain-specific variations observed in our experiments suggest that DPBench may require domain-adaptive tuning to achieve optimal performance."
- Does excluding cell values from table representations limit retrieval effectiveness compared to metadata-only approaches? [inferred] The setup states: "tables are represented only by their titles and column headers, with cell values excluded" to prevent memorization, but the tradeoff is unquantified.

## Limitations
- Current evaluation focuses only on retrieval accuracy, not coherence, analytical usefulness, or user experience.
- Performance varies significantly across domains (0.87 Full Recall@100 on ConvFinQA vs. 0.11 on HybridQA), suggesting need for domain-adaptive tuning.
- Tables indexed without cell values may sacrifice signal necessary for distinguishing semantically similar but distinct products.

## Confidence
- **High confidence**: Benchmark's core contribution of transforming QA datasets into DPR discovery tasks is clearly described and validated pairs are publicly available.
- **Medium confidence**: Retrieval baseline claims are reproducible but exact Milvus configuration and fusion parameters could affect results.
- **Low confidence**: LLM generation quality depends on undisclosed judge thresholds and ensemble ablation hasn't been performed.

## Next Checks
1. Replicate schema-only clustering: Run the full pipeline using only column embeddings (no questions) and measure Full Recall@100 on ConvFinQA to test the paper's claim that question-driven clustering is superior.
2. Ablate K parameter: Systematically vary K (max tables per product) from 1 to 10 on TAT-QA and measure how Full Recall@100 changes—this reveals the coherence-fragmentation tradeoff.
3. Validate judge thresholds: Using the public benchmark, randomly sample 100 DPR-ground-truth pairs and manually verify whether LLM-as-judge filtering criteria would have accepted/rejected them as reported.