---
ver: rpa2
title: Cost Minimization for Space-Air-Ground Integrated Multi-Access Edge Computing
  Systems
arxiv_id: '2510.21541'
source_url: https://arxiv.org/abs/2510.21541
tags:
- task
- energy
- computing
- ieee
- offloading
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of optimizing task offloading,
  UAV trajectory, and resource allocation in Space-Air-Ground Integrated Multi-Access
  Edge Computing (SAGIN-MEC) systems for low-altitude economy (LAE) applications.
  The proposed MADDPG-COCG algorithm combines multi-agent deep deterministic policy
  gradient (MADDPG) with a convex optimization and coalitional game (COCG) method
  to handle hybrid discrete-continuous and varying-dimensional decision variables
  in a partially observable environment.
---

# Cost Minimization for Space-Air-Ground Integrated Multi-Access Edge Computing Systems

## Quick Facts
- **arXiv ID**: 2510.21541
- **Source URL**: https://arxiv.org/abs/2510.21541
- **Reference count**: 40
- **Primary result**: Proposed MADDPG-COCG algorithm achieves up to 40.44% reduction in aggregated UD cost, 49.64% reduction in task completion delay, and 50.76% reduction in UD energy consumption in SAGIN-MEC systems.

## Executive Summary
This paper addresses the challenge of optimizing task offloading, UAV trajectory, and resource allocation in Space-Air-Ground Integrated Multi-Access Edge Computing (SAGIN-MEC) systems for low-altitude economy applications. The proposed MADDPG-COCG algorithm combines multi-agent deep deterministic policy gradient (MADDPG) with a convex optimization and coalitional game (COCG) method to handle hybrid discrete-continuous and varying-dimensional decision variables in a partially observable environment. The MADDPG component learns continuous decisions for task offloading ratio and UAV trajectory planning, while the COCG method deterministically handles computing resource allocation via convex optimization and user device (UD) association via a coalitional game. Simulation results demonstrate that the proposed algorithm significantly outperforms benchmark methods, achieving up to 40.44% reduction in aggregated UD cost, 49.64% reduction in task completion delay, and 50.76% reduction in UD energy consumption, while maintaining comparable UAV energy consumption.

## Method Summary
The MADDPG-COCG algorithm decouples the hybrid decision variables in SAGIN-MEC optimization. MADDPG learns continuous actions (offloading ratio and UAV trajectory velocity/direction) through actor-critic training with centralized critic and decentralized actors. The COCG module then analytically solves the discrete and varying-dimensional problems: computing resource allocation using a closed-form convex optimization solution and UD association using a coalitional game approach. This decomposition stabilizes learning by preventing the neural network from struggling with variable input sizes or discrete jumps, while the deterministic solutions guarantee optimal resource usage given current states.

## Key Results
- MADDPG-COCG achieves up to 40.44% reduction in aggregated UD cost compared to baseline methods
- 49.64% reduction in average task completion delay and 50.76% reduction in average UD energy consumption
- Superior convergence stability and scalability compared to conventional MADDPG and MAPPO approaches
- Maintains comparable UAV energy consumption while significantly improving other metrics

## Why This Works (Mechanism)

### Mechanism 1
Decoupling hybrid decision variables (continuous vs. discrete/varying-dimensional) stabilizes learning in multi-agent environments where standard Deep Reinforcement Learning (DRL) fails. The algorithm splits the action space, with MADDPG learning continuous, temporal decisions (UAV trajectory, offloading ratio) via gradient descent, while a deterministic Convex Optimization and Coalitional Game (COCG) module solves the discrete and varying-dimensional decisions (resource allocation, UD association) analytically at each step. This prevents the neural network from struggling with variable input sizes or discrete jumps, reducing the search space complexity for the agents. The core assumption is that the high-temporal-frequency decisions (trajectory) and instantaneous resource allocation are sufficiently separable such that solving one deterministically does not mislead the gradient estimation of the other.

### Mechanism 2
Replacing a neural network policy head for resource allocation with a closed-form convex optimization solution guarantees optimal resource usage given the current state. Given the constraints and the objective function, the resource allocation problem is convex. Instead of training a network to approximate this mapping (which is slow and prone to error), the system analytically derives the optimal frequency using the Lagrangian dual decomposition. This injects "perfect" actions into the environment for the critic to evaluate, reducing variance in training. The core assumption is that the computational load and channel conditions are perfectly known or accurately estimated at the time of optimization to satisfy the convexity requirements.

### Mechanism 3
A coalitional game approach for User Device (UD) association ensures distributed convergence to a stable state (Nash Equilibrium) without centralized coordination overhead. UD association is modeled as a game where UDs form coalitions (groups) around UAVs or satellites. UDs iteratively apply "switch" or "insert" rules based on local utility improvements. This creates a self-organizing association map that resolves the varying-dimensional input problem (number of UDs per UAV changes) without requiring the DRL agent to output a discrete vector of variable length. The core assumption is that the iterative game process converges faster than the environment state changes (mobility/channel), ensuring a stable association exists for the duration of the time slot.

## Foundational Learning

- **Partially Observable Markov Game (POMG)**: The system is defined as a POMG because agents cannot see the global state (e.g., all other agents' locations or queues). Understanding POMG is required to grasp why "Centralized Training, Decentralized Execution" (CTDE) is used. *Quick check*: If a UD agent cannot see the queue length of a neighboring UAV, how does the Critic network help the Actor during training but not execution?

- **Convex Optimization (Lagrangian Multipliers)**: The paper derives a "closed-form solution" for resource allocation. You need to understand why a convex problem allows for a deterministic mathematical formula (via KKT conditions) rather than requiring an iterative search or neural network approximation. *Quick check*: Why is the resource allocation sub-problem considered convex, while the overall UCMOP is NP-hard?

- **Coalitional Game Theory**: Used to solve the discrete association problem. You need to understand "Nash Equilibrium" in the context of coalition formationâ€”specifically, that stability is reached when no single UD can improve its cost by unilaterally changing its association. *Quick check*: In Algorithm 1, what prevents a UD from endlessly switching between two UAVs if the utility difference is marginal?

## Architecture Onboarding

- **Component map**: UDs/MADDPG Actor -> Local observation -> Offloading Ratio Action; UAVs/MADDPG Actor -> Local observation -> Trajectory Velocity/Direction Action; MADDPG Critic -> Global state+actions -> Q-value; COCG Module: Convex Solver (inputs: offload ratio, trajectory; outputs: resource allocation) -> Game Solver (inputs: UAV loc, resource allocation; outputs: UD association); Environment: Simulates mobility, communication, cost; Full Action Vector: [offload ratio, trajectory, resource allocation, UD association] -> Environment.

- **Critical path**: 1) Agents observe local state; 2) MADDPG Actor outputs raw continuous actions; 3) COCG Hook: system runs Convex Solver and Coalitional Game using proposed positions and ratios; 4) Full Action Vector executed in environment; 5) Reward calculated based on full cost; 6) Experience stored; Critic updates Actor.

- **Design tradeoffs**: The COCG module runs every step, introducing computational latency. The paper assumes this is negligible compared to the 1-second timeslot. Actor Scope: trajectory planning is assigned to UAVs and offloading ratios to UDs, requiring distinct network architectures.

- **Failure signatures**: Action Space Drift: COCG module fails to converge or produces NaNs; UAV Clustering: all UAVs converge to map center; Coalition Oscillation: UD association flips between two UAVs every episode.

- **First 3 experiments**: 1) Sanity Check: Implement Convex Optimization module independently and verify it outputs valid resource allocations; 2) Baseline Convergence: Implement MADDPG without COCG module to establish baseline need for COCG; 3) Ablation Study: Run MADDPG-COCG with fixed UAV trajectory to isolate impact of COCG association mechanism.

## Open Questions the Paper Calls Out

### Open Question 1
How does extending the UAV mobility model from 2D fixed-altitude to a 3D variable-altitude model affect the convergence stability and energy optimization accuracy of the MADDPG-COCG algorithm? This is unresolved because introducing a third dimension (altitude) significantly alters channel probability models and propulsion energy constraints, potentially destabilizing the continuous action space learned by the MADDPG. Evidence to resolve: simulation results comparing 2D versus 3D trajectory planning performance, specifically analyzing convergence rates and energy overhead in urban environments with variable altitude constraints.

### Open Question 2
Can the MADDPG-COCG algorithm maintain its performance advantage in dense multi-UAV scenarios when Inter-Cell Interference (ICI) is explicitly modeled in the agent states? This is unresolved because ICI introduces non-stationarity into the environment state and observation spaces, which can degrade the performance of the convex optimization and MADDPG policies if not properly accounted for. Evidence to resolve: performance evaluation of UD cost and task completion delay in a multi-cell topology where interference is calculated based on concurrent spectrum usage by neighboring UAVs.

### Open Question 3
What is the impact of realistic communication latency and signaling overhead on the centralized training phase of the MADDPG component within the hierarchical SAGIN architecture? This is unresolved because in a real SAGIN system, transmitting global state information from highly mobile UDs and UAVs to a central trainer is subject to variable delays and packet loss, which could destabilize the learning update rules. Evidence to resolve: a robustness analysis measuring convergence speed and final policy performance when stochastic delays are applied to the global state information available to the critic networks during the training phase.

## Limitations
- The convexity assumption for resource allocation may not hold in highly dynamic environments, potentially leading to suboptimal or infeasible solutions
- The scalability claims are based on a small-scale simulation (15 UDs, 3 UAVs) and performance in ultra-dense scenarios with 100+ UDs is unknown
- The paper assumes the COCG module runs fast enough to not introduce latency, but this may not hold in ultra-dense networks with thousands of UDs

## Confidence
- **Performance Claims (40.44% cost reduction, etc.)**: Medium confidence - results are impressive but simulation setup is idealized
- **Convergence Stability**: Medium confidence - shows MADDPG-COCG is more stable than MADDPG and MAPPO but doesn't compare against other state-of-the-art multi-agent DRL methods
- **Scalability**: Low confidence - scalability claims are based on small-scale simulation, performance in ultra-dense scenarios is unknown

## Next Checks
1. **Convex Solver Robustness Test**: Implement the closed-form resource allocation and stress-test it with edge-case inputs to verify it always produces feasible allocations
2. **Coalition Convergence Under Mobility**: Simulate high UD mobility scenario and measure coalition changes per episode to ensure coalitional game converges within timeslot
3. **Real-World Channel Sensitivity**: Replace deterministic rain attenuation model with stochastic one and re-train MADDPG-COCG to quantify impact of channel uncertainty on performance gains