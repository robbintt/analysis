---
ver: rpa2
title: 'Breaking to Build: A Threat Model of Prompt-Based Attacks for Securing LLMs'
arxiv_id: '2509.04615'
source_url: https://arxiv.org/abs/2509.04615
tags:
- attacks
- arxiv
- https
- llms
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey systematically categorizes prompt-based attack methodologies
  on LLMs, providing a comprehensive threat model that addresses input manipulation
  (e.g., direct/indirect prompt injection, adversarial crafting), semantic/knowledge-based
  exploits (e.g., CoT misuse, data poisoning), and integration/model-level attacks
  (e.g., Trojan attacks, output exploitation). The authors analyze attack mechanisms
  ranging from simple instruction ignoring to sophisticated automated RL-driven exploits,
  highlighting vulnerabilities in design, training, and system integration.
---

# Breaking to Build: A Threat Model of Prompt-Based Attacks for Securing LLMs

## Quick Facts
- **arXiv ID**: 2509.04615
- **Source URL**: https://arxiv.org/abs/2509.04615
- **Reference count**: 40
- **Primary result**: Systematic categorization of prompt-based attack methodologies on LLMs into a comprehensive threat model

## Executive Summary
This survey provides a comprehensive threat model for prompt-based attacks on Large Language Models (LLMs), systematically categorizing attack methodologies across three domains: input manipulation (direct/indirect injection, adversarial crafting), semantic/knowledge-based exploits (CoT misuse, data poisoning), and integration/model-level attacks (Trojan attacks, output exploitation). The authors analyze attack mechanisms ranging from simple instruction ignoring to sophisticated automated RL-driven exploits, highlighting vulnerabilities in design, training, and system integration. Key findings reveal that attacks targeting model reasoning and internal mechanisms pose greater threats than surface-level defenses can mitigate, motivating the development of inherently secure architectures. The paper concludes by identifying future research directions, including Un-Distillable/Un-Finetunable systems and advanced evaluation frameworks, establishing a foundation for building robust, ethically-aligned LLMs resistant to exploitation.

## Method Summary
The paper is a literature synthesis that systematically categorizes prompt-based attack methodologies on LLMs into a comprehensive threat model. It draws from 40 referenced academic papers and attack frameworks to identify and organize attack mechanisms across three primary categories: input manipulation, semantic/knowledge exploits, and integration/model-level attacks. The survey evaluates these attacks through analysis of their mechanisms, success factors, and impact on LLM security, while proposing future research directions for inherently secure architectures and advanced evaluation frameworks.

## Key Results
- Attackers exploit competing objectives in LLMs (helpfulness vs. harmlessness) to prioritize utility over safety
- External data sources integrated with LLMs (web search, APIs) serve as delivery vectors for malicious prompts
- Internal state manipulation attacks (Trojan activation, bit flipping) create persistent backdoors that bypass input-level defenses
- Automated RL-driven attacks can rapidly discover novel jailbreaks and Trojan triggers, outpacing manual defense methods

## Why This Works (Mechanism)

### Mechanism 1: Competing Objectives Exploitation
- Claim: Adversarial prompts can subvert safety alignment by prioritizing the model's helpfulness objective over its harmlessness objective.
- Mechanism: LLMs are trained with competing directives (be helpful, be harmless). Attackers craft prompts that make the "helpful" instruction more salient, such as forcing a specific response prefix ("Absolutely! Here's...") which commits the model to compliance before evaluating harm.
- Core assumption: The model's objective functions are not strictly ordered, allowing salience manipulation to override safety guardrails.
- Evidence anchors:
  - [section 2.1]: "LLMs are trained with competing objectives: to be helpful and to be harmless. Attackers exploit this by crafting prompts where the instruction to be helpful... takes precedence over safety guardrails."
  - [corpus]: Related surveys confirm this tension between utility and safety alignment persists across model families.
- Break condition: If models implemented strict objective hierarchy (safety-first ordering) or used separate safety classifiers that cannot be overridden by prompt context.

### Mechanism 2: Indirect Prompt Injection via Trusted Data Channels
- Claim: External data sources integrated with LLMs (web search, APIs, databases) can act as delivery vectors for malicious prompts that execute during inference without end-user awareness.
- Mechanism: LLM-integrated applications retrieve data from external sources. When these sources contain embedded prompts (hidden in websites, emails, metadata), the LLM interprets them as instructions rather than data, leading to execution. The model cannot distinguish between "data about instructions" and "instructions to follow."
- Core assumption: LLMs lack reliable context separation between untrusted external content and privileged system instructions.
- Evidence anchors:
  - [section 2.2]: "Indirect attacks poison these external data sources with malicious prompts, which are then consumed by the LLM during its operation... Passive Methods: Malicious prompts are embedded in public content like websites."
  - [section 2.2]: "HOUYI... successfully compromised 36 real-world LLM-integrated applications with high accuracy."
  - [corpus]: "Multi-Stage Prompt Inference Attacks on Enterprise LLM Systems" confirms chaining benign-appearing prompts can gradually extract confidential data from integrated systems.
- Break condition: If systems implement strict sandboxing, data provenance tracking, or separate context windows for external vs. system content with instruction-level access controls.

### Mechanism 3: Trojan Activation via Internal State Manipulation
- Claim: Attacks that modify or exploit internal model states (weights, activation vectors) can create persistent backdoors that bypass input-level defenses entirely.
- Mechanism: Rather than manipulating prompts, these attacks inject malicious vectors into activation layers at inference time (Trojan Steering Vectors) or flip minimal weight bits (Bit Flipping Attacks). When triggered, these modifications steer the model toward attacker-controlled outputs regardless of prompt content.
- Core assumption: Model internals remain accessible or inferable, and defensive monitoring does not extend to activation-level inspection.
- Evidence anchors:
  - [section 4.2]: "Trojan Steering Vectors do not modify weights but instead inject malicious vectors into the model's activation layers at inference to steer its output."
  - [section 4.2]: "Bit Flipping Attacks identify and alter a minimal set of weight bits at test-time to induce malicious behavior, making the modification difficult to detect."
  - [corpus]: Corpus lacks direct replication studies for these specific attack techniques; most remain at proof-of-concept stage.
- Break condition: If models implement integrity verification for weights/activations, use non-differentiable components, or employ formal verification of behavior bounds.

## Foundational Learning

- Concept: **Transformer Attention and Tokenization Transferability**
  - Why needed here: The paper notes that shared architectural foundations (Transformer architectures, tokenizers, training datasets) across major LLMs enable high attack transferability. Understanding how attention mechanisms process prompts helps explain why an attack on one model often works on others.
  - Quick check question: Can you explain why an adversarial suffix crafted for GPT might also affect Llama despite different weights?

- Concept: **Safety Alignment and RLHF Limitations**
  - Why needed here: Multiple attack vectors (competing objectives, cognitive hacking, jailbreak tuning) exploit the fragility of safety training. Understanding how RLHF works—and where it fails—provides context for why adversarial inputs can override learned constraints.
  - Quick check question: Why does training a model to refuse harmful requests not guarantee it will refuse obfuscated or contextually re-framed harmful requests?

- Concept: **Retrieval-Augmented Generation (RAG) Architecture**
  - Why needed here: Indirect prompt injection and data poisoning attacks specifically target RAG systems where external knowledge bases become attack surfaces. Understanding the data flow in RAG is essential for Section 2.2 and 3.3.
  - Quick check question: In a RAG system, where should input validation occur to prevent indirect injection—user query, retrieved documents, or both?

## Architecture Onboarding

- Component map: User Interface Layer -> System Prompt Layer -> Retrieval Layer (if RAG) -> Model Core -> Integration Layer -> Output Processing
- Critical path: 1. Input sanitization (prevent direct injection), 2. Context separation (system vs. user vs. external data), 3. Retrieval validation (prevent indirect injection), 4. Inference monitoring (detect anomalous activations), 5. Output filtering (catch missed exploits)
- Design tradeoffs: Strict input filtering vs. model utility (over-blocking legitimate queries), RAG richness vs. attack surface (more external data = more injection vectors), Model openness vs. security (open weights enable Trojan insertion; closed models resist but lack transparency), Automated defense (RL-based detection) vs. adversarial adaptation (attackers can train against defenses)
- Failure signatures: Direct injection success: Model outputs begin with forced prefixes ("Sure, here's...") or ignore explicit refusals, Indirect injection success: Model behavior changes after retrieving specific documents without user instruction, Trojan activation: Specific trigger phrases cause consistent anomalous outputs across otherwise normal sessions, CoT manipulation: Reasoning chains contain logical leaps or external assumptions not in the prompt
- First 3 experiments: 1. Baseline injection testing: Implement the "Competing Objectives" and "Instruction Ignoring" attacks from Section 2.1 against your model; measure success rate and identify which safety guardrails fail first, 2. RAG poisoning simulation: Embed a benign-appearing but malicious prompt in a test document; verify whether retrieval includes it and whether the model executes it. This tests indirect injection resistance per Section 2.2, 3. Output monitoring pilot: Log all cases where model responses include: (a) forced prefixes, (b) requests for clarification about harmful topics, or (c) reasoning chains with unsupported assumptions. This establishes a detection baseline for Section 5.1 exploits

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: What specific architectural modifications (e.g., non-differentiable components) or training methodologies can render LLMs intrinsically resistant to data poisoning and Trojan attacks?
- **Basis in paper**: [explicit] The authors explicitly call for research into "Architectures for Un-Editable and Un-Finetunable LLMs" to address vulnerabilities in model design and training.
- **Why unresolved**: Current Transformer-based architectures are inherently malleable and differentiable, making them susceptible to gradient-guided poisoning and bit-flipping attacks that compromise integrity.
- **What evidence would resolve it**: Demonstration of a model architecture that maintains performance benchmarks while mathematically proving resistance to gradient-based manipulation or parameter tampering.

### Open Question 2
- **Question**: How can standardized evaluation frameworks effectively benchmark LLM security against automated, Reinforcement Learning-driven attacks?
- **Basis in paper**: [explicit] The Future Work section identifies the need for "Advanced Evaluation Frameworks and Benchmarks" to keep pace with automated exploit generation.
- **Why unresolved**: Manual red teaming is slow and unscalable compared to automated RL frameworks that can rapidly discover novel jailbreaks and Trojan triggers.
- **What evidence would resolve it**: The adoption of a dynamic benchmark suite that successfully validates model robustness against a continuously evolving set of automated, RL-generated adversarial prompts.

### Open Question 3
- **Question**: What formal verification methods or data provenance techniques are required to secure Retrieval-Augmented Generation (RAG) systems against indirect prompt injection?
- **Basis in paper**: [explicit] The paper lists "Secure Integration and Tool Use" as a key direction, specifically citing the need for "data provenance tracking for RAG systems."
- **Why unresolved**: LLMs currently lack the ability to distinguish between trusted instructions and malicious content retrieved from external, untrusted databases.
- **What evidence would resolve it**: A formal verification protocol or sandboxing mechanism that successfully isolates retrieved data from system prompts, preventing indirect injection with provable guarantees.

## Limitations

- The survey relies entirely on reported results from cited papers without empirical validation or success rate data
- Attack techniques effective against older models (2023) may have been patched in current versions, but the survey doesn't distinguish between persistent and time-sensitive vulnerabilities
- The survey identifies defense mechanisms but doesn't empirically test their effectiveness against the described attacks, creating uncertainty about real-world protection levels

## Confidence

**High Confidence (4/5):**
- Basic attack taxonomy (Direct vs. Indirect Injection, Automated vs. Manual) is well-established and consistently reported across the literature
- Model integration vulnerabilities (RAG poisoning, plugin exploitation) represent genuine attack surfaces with documented successful exploits
- Competing objectives exploitation mechanism is theoretically sound and has been demonstrated across multiple model families

**Medium Confidence (3/5):**
- Internal state manipulation attacks (Trojan Steering, Bit Flipping) remain largely theoretical with limited public replication studies
- Automated RL-driven attack effectiveness claims are based on controlled environments that may not generalize to production systems
- Future architecture proposals (Un-Distillable/Un-Finetunable) lack empirical validation or implementation details

**Low Confidence (2/5):**
- Specific success rate claims for individual attacks without model version specification
- Claims about relative threat severity between attack categories without comparative empirical testing
- Proposed evaluation frameworks lack concrete implementation guidelines or benchmark datasets

## Next Checks

1. **Temporal Vulnerability Assessment**: Select five high-impact attacks from the survey (e.g., GCG suffix attacks, competing objectives exploitation) and test them against three model versions from different years (2022, 2023, 2024). Document success rate decay over time to quantify patching effectiveness and identify persistent vulnerabilities.

2. **Cross-Model Transferability Study**: Implement three attacks proven successful on one model family (e.g., GPT) and systematically test them across three different architectures (GPT, Claude, Llama). Measure attack success rates, reasoning chain variations, and safety bypass mechanisms to validate or refute the shared-vulnerability assumption.

3. **Defense Efficacy Validation**: Select two prominent defense mechanisms mentioned in the survey (e.g., input sanitization, output filtering) and implement them in a controlled environment. Run the five attacks from Check 1 against the defended system, measuring both false positive rates and attack bypass rates to quantify real-world protection levels.