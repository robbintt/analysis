---
ver: rpa2
title: 'Beyond Discrete Categories: Multi-Task Valence-Arousal Modeling for Pet Vocalization
  Analysis'
arxiv_id: '2510.12819'
source_url: https://arxiv.org/abs/2510.12819
tags:
- emotion
- arousal
- valence
- classification
- emotional
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces continuous Valence-Arousal (VA) modeling
  for pet vocalization emotion recognition, addressing the limitations of discrete
  classification methods that struggle with boundary ambiguity and cannot express
  intensity variations. The authors propose an automatic VA label generation algorithm
  that derives Arousal from RMS energy and Valence from spectral features augmented
  with emotion-specific priors, enabling large-scale annotation of 42,553 pet vocalization
  samples without subjective self-reports.
---

# Beyond Discrete Categories: Multi-Task Valence-Arousal Modeling for Pet Vocalization Analysis

## Quick Facts
- arXiv ID: 2510.12819
- Source URL: https://arxiv.org/abs/2510.12819
- Authors: Junyao Huang; Rumin Situ
- Reference count: 32
- Primary result: Continuous VA modeling achieves validation VA MAE of 0.1124, resolving 356 territorial/happy confusion cases

## Executive Summary
This paper introduces continuous Valence-Arousal (VA) modeling for pet vocalization emotion recognition, addressing the limitations of discrete classification methods that struggle with boundary ambiguity and cannot express intensity variations. The authors propose an automatic VA label generation algorithm that derives Arousal from RMS energy and Valence from spectral features augmented with emotion-specific priors, enabling large-scale annotation of 42,553 pet vocalization samples without subjective self-reports. A multi-task learning framework jointly trains VA regression with auxiliary tasks (emotion, body size, gender) to enhance feature learning, where size classification forces frequency feature learning and gender classification encourages timbre feature extraction. Their Audio Transformer model achieves validation VA MAE of 0.1124, Valence Pearson correlation r=0.9024, and Arousal r=0.7155, effectively resolving the 356 confusion cases between "territorial" and "happy" categories observed in discrete baselines through natural separation in continuous VA space.

## Method Summary
The approach combines automatic VA label generation with multi-task learning. Arousal labels derive from log-scaled RMS energy (95th percentile) following Weber-Fechner law, while Valence combines spectral centroid, logRMS, and ZCR with emotion-specific biases. The Audio Transformer architecture shares a 6-layer transformer encoder across 5 tasks: VA regression (primary), emotion classification (8 classes), size classification (3 classes), and gender classification (2 classes). The multi-task framework forces learning of frequency features through size classification and timbre features through gender classification, both of which benefit VA estimation. Automatic augmentation (time stretch, pitch shift) expands the dataset, while early stopping prevents overfitting.

## Key Results
- Validation VA MAE of 0.1124, significantly outperforming discrete baselines that show 356 territorial/happy confusion cases
- Valence Pearson correlation r=0.9024 and Arousal r=0.7155, demonstrating strong dimensional emotion representation
- Multi-task learning achieves 49.4% VA MAE improvement over single-task baseline through forced feature learning
- Natural separation of territorial (V≈-0.3) and happy (V≈+0.6) emotions in continuous VA space resolves classification ambiguity

## Why This Works (Mechanism)

### Mechanism 1: Auxiliary Tasks Improve Feature Learning
Auxiliary classification tasks improve VA regression through forced feature learning. Size classification compels the model to learn frequency-related features (large breeds→low frequencies), while gender classification drives timbre feature extraction. These acoustic features are also predictive of VA dimensions, creating knowledge transfer from physical attributes to emotional states. Core assumption: Frequency and timbre features that distinguish size/gender are causally related to emotional valence and arousal, not merely correlated.

### Mechanism 2: Continuous VA Resolves Boundary Ambiguity
Continuous VA representation resolves discrete category boundary ambiguity through dimensional separation. Emotions that share acoustic features but differ in valence (e.g., "territorial" vs. "happy") occupy distinct regions in VA space. Territorial vocalizations cluster at V≈-0.3, while happy vocalizations cluster at V≈+0.6, creating natural separation impossible with hard category boundaries. Core assumption: The dimensional structure of the VA space meaningfully maps to underlying emotional states, not just acoustic artifact.

### Mechanism 3: Automatic Label Generation with Domain Knowledge
Automatic VA label generation from acoustic features with emotion-specific priors produces training-sufficient labels. Arousal labels derive from RMS energy (logarithmic mapping per Weber-Fechner law). Valence labels combine spectral centroid (positive weight), energy (negative weight), and ZCR with emotion-specific biases (fearful: -0.18, excited: +0.14). This injects domain knowledge while maintaining acoustic grounding. Core assumption: RMS energy and spectral features are reliable proxies for internal emotional states; emotion-specific biases accurately encode prior knowledge.

## Foundational Learning

- **Valence-Arousal Circumplex Model**: Why needed here: The entire framework depends on understanding emotions as points in 2D continuous space rather than discrete buckets. Without this, the regression formulation makes no sense. Quick check: Can you explain why "territorial" and "happy" might have similar arousal but different valence, and why this creates classification confusion?

- **Multi-Task Learning with Hard Parameter Sharing**: Why needed here: The model architecture shares a transformer encoder across 5 tasks. Understanding how gradients from auxiliary tasks influence shared representations is critical for debugging training dynamics. Quick check: If size classification loss dominates (high weight), what symptom would you expect in VA regression performance?

- **Logarithmic Perception Scaling (Weber-Fechner Law)**: Why needed here: Arousal label generation uses log-scale RMS mapping. Understanding why linear energy doesn't match perceived loudness prevents naive feature engineering mistakes. Quick check: Why might a 10dB energy increase matter more for quiet vocalizations than loud ones, and how does the log transformation address this?

## Architecture Onboarding

- Component map: Audio (WAV, 3s) → Mel Spectrogram (128×259) → Transformer Encoder (6 layers, 512 dim) → Global Average Pooling → h ∈ R^512 → Valence (Tanh) / Arousal (Sigmoid) / Emotion (Softmax) / Size (Softmax) / Gender (Softmax)

- Critical path: Audio → Mel spectrogram (librosa) → Transformer encoder → pooled features h → Valence/Arousal heads. The auxiliary tasks contribute only during training; inference uses only VA heads.

- Design tradeoffs:
  - **Automatic labels vs. human annotation**: Trades annotation cost/feasibility for potential label noise. Mitigated by AI/expert validation but not eliminated.
  - **Hard sharing vs. soft sharing**: Hard sharing (single encoder) reduces parameters but risks negative transfer if tasks conflict. Ablation shows all auxiliary tasks help, suggesting minimal conflict.
  - **3-second fixed length vs. variable**: Zero-padding/cropping loses temporal context for longer vocalizations but enables batch processing.

- Failure signatures:
  - **Valence-Arousal correlation collapse**: If r < 0.5 for Valence, suspect label generation bugs or emotion bias values
  - **Auxiliary task domination**: If emotion classification > 95% but VA MAE high, reduce auxiliary loss weights
  - **Size-specific overfitting**: Large gap between Large→Small-Medium and Small-Medium→Large cross-validation suggests breed-specific memorization

- First 3 experiments:
  1. **Reproduce baseline ablation**: Train VA-only model (no auxiliary tasks), verify ~0.16 VA MAE. Then add single auxiliary task (gender), expect ~0.08 MAE. Confirms MTL mechanism.
  2. **Label quality stress test**: Randomly corrupt 10% of emotion labels before VA generation. Measure correlation degradation. Establishes sensitivity to label noise.
  3. **Cross-size validation**: Train on large breeds only, test on small breeds. Target <3% MAE gap per Table 5. If gap >10%, investigate frequency feature normalization.

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset inaccessibility: The 42,553-sample LingChongTong dataset is not publicly available, preventing exact reproduction.
- Automatic label quality uncertainty: Only validated via AI/limited expert review; no human VA annotations for ground truth comparison.
- Arousal regression weakness: Performance (r=0.7155) is notably weaker than valence (r=0.9024), suggesting incomplete feature representation for arousal.

## Confidence
- **High Confidence**: Multi-task learning improves feature extraction (supported by ablation showing 49.4% VA MAE improvement; auxiliary tasks demonstrably force learning of frequency and timbre features)
- **Medium Confidence**: Continuous VA space resolves discrete boundary ambiguity (validated on territorial/happy confusion but limited to single pair; needs broader category analysis)
- **Low Confidence**: Automatic label quality (only validated via AI/limited expert review; no human VA annotations for ground truth comparison)

## Next Checks
1. **Cross-categorical validation**: Test whether continuous VA modeling resolves confusion between other emotionally-similar pairs (e.g., playful vs. excited, anxious vs. fearful) to establish general boundary-elimination capability
2. **Label noise sensitivity**: Systematically corrupt 10-30% of emotion labels and measure VA performance degradation to quantify automatic labeling robustness
3. **Dimensionality reduction analysis**: Apply t-SNE or UMAP to pooled features and visualize whether emotions naturally cluster by valence-arousal position, confirming learned representations align with VA space intuition