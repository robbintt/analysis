---
ver: rpa2
title: Canonical Intermediate Representation for LLM-based optimization problem formulation
  and code generation
arxiv_id: '2602.02029'
source_url: https://arxiv.org/abs/2602.02029
tags:
- problem
- time
- rule
- optimization
- constraint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of translating complex natural-language
  optimization problems into executable mathematical models and solver code using
  LLMs. Current direct LLM-based approaches struggle with composite constraints and
  paradigm-sensitive formulations required by complex operational rules.
---

# Canonical Intermediate Representation for LLM-based optimization problem formulation and code generation

## Quick Facts
- arXiv ID: 2602.02029
- Source URL: https://arxiv.org/abs/2602.02029
- Reference count: 40
- Primary result: Introduces CIR schema and R2C framework achieving 47.2% Accuracy Rate on ORCOpt-Bench, approaching GPT-5 performance

## Executive Summary
This paper addresses the challenge of translating complex natural-language optimization problems into executable mathematical models and solver code using LLMs. Current direct LLM-based approaches struggle with composite constraints and paradigm-sensitive formulations required by complex operational rules. To solve this, the authors introduce the Canonical Intermediate Representation (CIR) - a structured schema that LLMs explicitly generate between problem descriptions and optimization models. CIR encodes operational rule semantics through constraint archetypes and candidate modeling paradigms, decoupling rule logic from mathematical instantiation. Building on CIR, they develop the Rule-to-Constraint (R2C) framework, a multi-agent pipeline that parses problem texts, synthesizes CIR implementations via retrieval-augmented generation, and instantiates optimization models. Extensive experiments show R2C achieves state-of-the-art accuracy on the proposed ORCOpt-Bench (47.2% Accuracy Rate) and highly competitive results on established benchmarks, approaching proprietary models like GPT-5. With a reflection mechanism, R2C sets new best-reported results on some benchmarks.

## Method Summary
The paper proposes the Rule-to-Constraint (R2C) framework, a multi-agent pipeline that translates natural language optimization problems into executable Gurobi Python code. The pipeline consists of four specialized agents: Extractor (parses NL to structured JSON), Mapper (retrieves CIR templates via RAG and clusters by paradigm), Formalizer (instantiates mathematical formulation and code), and Checker (validates each handoff). The core innovation is the Canonical Intermediate Representation (CIR), a structured schema encoding constraint archetypes and modeling paradigms that decouples rule semantics from mathematical instantiation. The framework uses a curated CIR knowledge base and includes a reflection mechanism for error recovery. Evaluation is performed on ORCOpt-Bench (47.2% AR) and established benchmarks, showing state-of-the-art performance.

## Key Results
- R2C achieves 47.2% Accuracy Rate on ORCOpt-Bench, setting new state-of-the-art performance
- R2C approaches GPT-5 performance on established benchmarks, with competitive Execution Rates
- CIR knowledge base and staged validation contribute 15.6% absolute AR improvement over raw text approaches
- Reflection mechanism enables new best-reported results on some established benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Introducing an explicit intermediate representation between natural language and mathematical formulation improves translation of complex operational rules into constraints.
- Mechanism: CIR decouples rule semantics from mathematical instantiation by encoding operational intent through constraint archetypes and paradigm candidates. This forces the LLM to first reason about "what the rule means" before "how to implement it mathematically," reducing the composite constraint problem.
- Core assumption: LLMs can reliably generate structured JSON adhering to the CIR schema when given explicit schema definitions and domain knowledge.
- Evidence anchors: [abstract] "CIR encodes the semantics of operational rules through constraint archetypes and candidate modeling paradigms, thereby decoupling rule logic from its mathematical instantiation."; [section 3.2] "A CIR template is not a single constraint but a bundle of semantically equivalent, paradigm-specific formulations."; [corpus] Weak direct corpus support; related work (OPT-Engine) addresses LLM optimization limits but not intermediate representations.
- Break condition: If CIR schema becomes too rigid to capture novel operational rule types outside the predefined knowledge base, the mechanism fails—performance degrades as shown in Supply Chain & Production domain (0.0 AR in Table 2).

### Mechanism 2
- Claim: Retrieval-augmented generation from a curated CIR knowledge base outperforms raw text retrieval for constraint synthesis.
- Mechanism: The Mapper agent retrieves structured CIR templates (not raw literature) and re-parameterizes them for the specific problem context through lifting dimensionalities, bound propagation, or auxiliary variable introduction. This provides canonical modeling patterns rather than requiring the LLM to invent formulations.
- Core assumption: The CIR knowledge base has sufficient coverage of constraint archetypes across domains; uncovered patterns cannot be reliably synthesized.
- Evidence anchors: [abstract] "synthesizes CIR implementations by retrieving domain knowledge"; [section 4.5] Ablation shows removing CIR drops AR from 47.2% to 31.6%, while Vanilla/Self-RAG-CIR achieves 37.9-38.4%—structured CIR retrieval is necessary but insufficient alone; [corpus] No direct corpus evidence comparing structured vs. unstructured RAG for optimization.
- Break condition: If the knowledge base coverage is incomplete for a domain (e.g., novel constraint types not in the curated templates), the system falls back to LLM's internal knowledge which may produce paradigm-incompatible formulations.

### Mechanism 3
- Claim: Multi-agent specialization with staged validation prevents error propagation through the formulation pipeline.
- Mechanism: Four specialized agents (Extractor→Mapper→Formalizer→Checker) operate with explicit handoff validation at each stage. The Checker agent audits completeness (rule→CIR→constraint mapping) before code generation, catching omissions early.
- Core assumption: Each agent's output schema is sufficiently structured that validation can be automated via rule-based checks.
- Evidence anchors: [section 3.3.4] "The Checker Agent performs staged validation to prevent error propagation caused by LLM randomness."; [section 4.5] Disabling validation drops AR from 47.2% to 41.6%; [corpus] No direct corpus evidence on staged validation in optimization pipelines.
- Break condition: If validation criteria miss semantic errors (e.g., syntactically valid but logically incorrect constraint templates), errors propagate to final code undetected.

## Foundational Learning

- Concept: **Mixed-Integer Linear Programming (MILP) modeling paradigms**
  - Why needed here: The paper assumes familiarity with time-indexed vs. continuous-time vs. arc-flow formulations and their computational trade-offs. The Mapper selects paradigms based on problem characteristics.
  - Quick check question: Given a job scheduling problem with 1000 tasks and 100 time slots, which paradigm would minimize variable count—time-indexed or continuous-time with big-M disjunctions?

- Concept: **Retrieval-Augmented Generation (RAG)**
  - Why needed here: The R2C framework uses RAG over a CIR knowledge base. Understanding embedding-based retrieval and template instantiation is essential.
  - Quick check question: How does retrieval from structured CIR templates differ from retrieval from raw documentation text in terms of downstream generation quality?

- Concept: **Constraint archetype decomposition**
  - Why needed here: Complex operational rules (e.g., "each aircraft must undergo maintenance every 100 flight hours") require decomposition into multiple constraints. The Extractor must identify composite rules; the Mapper must map them to multiple CIR implementations.
  - Quick check question: Decompose "no machine processes two jobs simultaneously and each job runs non-preemptively" into constituent constraint intents.

## Architecture Onboarding

- Component map: Extractor Agent → Mapper Agent + RAG retrieval → Formalizer Agent → Checker Agent
- Critical path: Extractor → (validation) → Mapper + RAG retrieval → (validation) → Formalizer → (validation) → executable code. Any failure in validation triggers reflection loop (Appendix F).
- Design tradeoffs:
  - **Soundness vs. completeness**: CIR guarantees soundness (no rule-violating solutions) but not completeness (may exclude valid solutions via over-restrictive constraints)—acknowledged in Section 4.4.
  - **Template rigidity vs. LLM creativity**: Heavily templated CIR improves accuracy but limits novel formulations; relies on knowledge base coverage.
  - **Agent specialization vs. complexity**: Four agents increase robustness but add latency and integration complexity.
- Failure signatures:
  - **AR = 0 for specific domains** (Table 2: Supply Chain & Production = 0.0): Likely indicates missing CIR templates for that domain's constraint patterns.
  - **High ER but low AR** (e.g., Qwen3-32B: ER 58%, AR 18%): Code executes but produces incorrect solutions—suggests semantic extraction or paradigm selection errors, not syntax issues.
  - **IIS (Irreducible Inconsistent Subsystem) in solver output**: Indicates contradictory constraints from Mapper stage—reflection mechanism targets this (Appendix F).
- First 3 experiments:
  1. **Run ORCOpt-Bench baseline**: Execute R2C with DeepSeek-v3.2 on ORCOpt-Bench, compare AR/ER against standard prompting. Verify 47.2% AR is reproducible. Analyze failure cases by domain.
  2. **Ablate CIR knowledge base**: Replace CIR retrieval with raw text RAG (as in "R2C w/o CIR"). Expect ~31.6% AR. Identify which constraint types degrade most.
  3. **Test domain coverage gap**: Run R2C on Supply Chain & Production problems (0.0 AR). Manually inspect whether missing CIR templates or extraction failures cause the breakdown. Add one template and re-run.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the construction and expansion of the CIR knowledge base be automated to broaden its coverage of optimization domains?
- Basis in paper: [explicit] The Conclusion and Section 4.4 state that future work should explore "automating the construction and expansion of the CIR knowledge base" to address the limitation that "performance is contingent on the curated CIR library's domain coverage."
- Why unresolved: The current R2C framework relies on a manually curated, domain-specific library of CIR templates, which limits its scalability to new problem classes.
- What evidence would resolve it: An automated pipeline that extracts CIR archetypes directly from raw optimization literature or code repositories without manual intervention.

### Open Question 2
- Question: How can the framework incorporate constraint refinement mechanisms to ensure model completeness and prevent over-restrictive formulations?
- Basis in paper: [explicit] Section 4.4 explicitly notes that while the CIR provides a soundness guarantee, it "currently does not achieve completeness" and may "instantiate overly restrictive constraints... potentially excluding some valid solutions."
- Why unresolved: The current framework focuses on translating rules faithfully into constraints but lacks a feedback loop to detect and relax constraints that unnecessarily exclude feasible solutions.
- What evidence would resolve it: A "constraint refinement mechanism" that validates the feasibility space of the generated model against the semantic intent of the original problem description.

### Open Question 3
- Question: Can CIR paradigm annotations be enriched with empirical computational profiles to optimize for solver efficiency rather than just semantic correctness?
- Basis in paper: [explicit] Appendix E discusses "efficiency-aware pattern selection" as a "promising direction for future research," suggesting the development of a "prescriptive CIR" that balances fidelity with computational performance.
- Why unresolved: The current "Paradigm Fit Score" relies on conceptual suitability (e.g., time-indexed vs. continuous-time) but ignores computational trade-offs like model size or LP relaxation tightness.
- What evidence would resolve it: An extension of the CIR schema that includes metadata predicting solver performance (e.g., solve time, node count) for different paradigms based on problem scale.

## Limitations
- Performance is contingent on the curated CIR library's domain coverage, with 0.0 AR in Supply Chain & Production indicating coverage gaps
- The framework provides soundness guarantees but not completeness, potentially instantiating overly restrictive constraints that exclude valid solutions
- The comparison to GPT-5 performance uses an unspecified proprietary model, making direct validation difficult

## Confidence

- **High Confidence**: The core mechanism of using structured CIR as an intermediate representation is well-validated through ablation studies showing 15.6% absolute AR improvement over raw text approaches.
- **Medium Confidence**: The RAG-based CIR retrieval advantage over raw text retrieval is supported by ablation but lacks direct comparison to established RAG benchmarks in optimization.
- **Medium Confidence**: The multi-agent staged validation prevents error propagation, with ablation showing 5.6% AR drop when disabled, though the validation criteria's completeness is not independently verified.
- **Low Confidence**: The claim that R2C "approaches" GPT-5 performance is difficult to validate without access to GPT-5 or understanding its configuration.

## Next Checks

1. **Domain Coverage Analysis**: Manually inspect 5–10 problems from the 0.0 AR domain (Supply Chain & Production) to determine whether failures stem from missing CIR templates or extraction errors. Add representative templates and re-evaluate.
2. **Reflection Mechanism Audit**: Run R2C on problems that fail without reflection, capture the IIS outputs, and verify that the reflection mechanism successfully identifies and resolves the inconsistencies.
3. **CIR Template Generalization**: Test R2C on a novel domain not in the knowledge base (e.g., healthcare scheduling) to measure performance degradation and identify whether the system can synthesize novel CIR implementations or fails completely.