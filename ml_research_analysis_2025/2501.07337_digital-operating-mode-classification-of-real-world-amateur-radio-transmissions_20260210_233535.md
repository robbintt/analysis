---
ver: rpa2
title: Digital Operating Mode Classification of Real-World Amateur Radio Transmissions
arxiv_id: '2501.07337'
source_url: https://arxiv.org/abs/2501.07337
tags:
- radio
- classification
- signal
- data
- operating
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents a machine learning approach for classifying
  digital radio operating modes in real-world amateur radio transmissions. The authors
  generated 98 parameterized radio signals from 17 digital operating modes and transmitted
  them on the 70 cm amateur radio band.
---

# Digital Operating Mode Classification of Real-World Amateur Radio Transmissions

## Quick Facts
- **arXiv ID:** 2501.07337
- **Source URL:** https://arxiv.org/abs/2501.07337
- **Reference count:** 34
- **Primary result:** EfficientNetB0 achieved 93.80% accuracy on 17 digital radio operating modes and 85.47% on 98 parameterized signals from real-world amateur radio transmissions.

## Executive Summary
This study presents a machine learning approach for classifying digital radio operating modes in real-world amateur radio transmissions. The authors generated 98 parameterized radio signals from 17 digital operating modes and transmitted them on the 70 cm amateur radio band. These transmissions were recorded using two different SDR receiver architectures. Three lightweight ML models (ResNet-18, EfficientNetB0, and Vision Mamba Tiny) were trained exclusively on spectrograms of non-transmitted signals using an online data augmentation pipeline to simulate radio channel impairments. The best model, EfficientNetB0, achieved 93.80% accuracy across 17 operating modes and 85.47% across all 98 parameterized signals when evaluated on real-world transmissions.

## Method Summary
The approach involves generating synthetic radio signals using Fldigi, converting them to spectrograms, and applying an online augmentation pipeline that simulates channel impairments including noise, frequency shifts, and amplitude variations. Three ImageNet-pretrained CNN models (ResNet-18, EfficientNetB0, Vision Mamba Tiny) are trained on these augmented spectrograms and evaluated on real-world recordings from two SDR receivers (RTL-SDR and HackRF). The augmentation pipeline is critical, as models trained without it show accuracy dropping from ~85% to ~3% on real transmissions.

## Key Results
- EfficientNetB0 achieved 93.80% accuracy for 17 operating modes and 85.47% for 98 parameterized signals on real-world transmissions
- Signal duration of 2 seconds was found more important than FFT bin count for classification accuracy
- Data augmentation simulating channel impairments was essential, with accuracy dropping from ~85% to ~3% without it
- ImageNet pre-training provided an average 26% accuracy boost compared to random initialization

## Why This Works (Mechanism)

### Mechanism 1: Domain Randomization via Channel Impairment Simulation
Training exclusively on synthetic, non-transmitted signals can generalize to real-world radio transmissions if the data augmentation pipeline sufficiently bounds the expected channel impairments. The online augmentation pipeline applies stochastic transformations (Amplify, FreqShift, SimTone, Noise) to clean AF signals, forcing the model to learn robust features invariant to specific distortions caused by path loss, interference, and frequency drift.

### Mechanism 2: Temporal Context Accumulation via Spectrogram Duration
Classification accuracy is more dependent on signal duration (time context) than spectral resolution (FFT bin count) for the evaluated digital modes. Digital operating modes utilize specific symbol rates and error correction sequences, and a duration of at least 2 seconds allows the spectrogram to capture sufficient symbol transitions and synchronization patterns.

### Mechanism 3: Transfer Learning from Natural Images to RF Spectrograms
Initializing weights from ImageNet pre-training provides a significant performance boost (approx. 26%) compared to random initialization. Early convolutional filters learned from ImageNet transfer effectively to spectrograms, where they detect frequency boundaries and amplitude contours.

## Foundational Learning

- **Concept: Spectrogram Generation (STFT)**
  - Why needed here: The entire architecture relies on converting 1D radio signals into 2D images to leverage Computer Vision models. Understanding the trade-off between time resolution and frequency resolution is critical.
  - Quick check question: If you increase the FFT window size to get better frequency resolution, what happens to your time resolution, and how might that affect the visualization of a fast-baud rate signal?

- **Concept: Software Defined Radio (SDR) Architectures**
  - Why needed here: The paper evaluates on two distinct receiver types (Heterodyne vs. Direct Conversion). Understanding why the Direct Conversion receiver suffered from frequency drift while the Heterodyne did not is essential for debugging real-world deployment failures.
  - Quick check question: Why would a "DC spike" or center frequency drift be a more significant issue for a Direct Conversion receiver compared to a Heterodyne receiver in this classification task?

- **Concept: Domain Randomization / Sim-to-Real Gap**
  - Why needed here: The models are never trained on real data, only on "perfect" signals augmented with simulated noise. Understanding that the goal of augmentation is not to perfectly replicate reality, but to randomize the simulation enough so reality looks like just another variation, is the core strategic insight of this paper.
  - Quick check question: If you removed the "FreqShift" augmentation, why would the model specifically fail on the recordings from the Direct Conversion receiver but potentially still work on the Heterodyne receiver?

## Architecture Onboarding

- **Component map:** Fldigi -> Python DSP (Generates 98 OMPs/OMs, Converts to AF, Applies Augmentations) -> EfficientNetB0 (Pre-trained on ImageNet, Input: 3-channel Spectrogram) -> Fully Connected Layer -> Output (98 Classes or 17 Classes) -> Real-world I/Q recordings -> Filter/Demodulate -> Spectrogram -> Model Inference

- **Critical path:** The Augmentation Pipeline. The paper explicitly proves that without specific augmentations (especially Noise and FreqShift), the model collapses from ~85% accuracy to <5%. The specific parameters (e.g., FreqShift âˆˆ [-500, 500]) are calibrated to the receiver drift characteristics.

- **Design tradeoffs:**
  - Latency vs. Accuracy: You must buffer at least 2 seconds of signal before classification is reliable. Sub-second classification is effectively broken (accuracy drops ~5-6%).
  - Resolution vs. Compute: Increasing FFT bins from 64 to 256 yields marginal gains (<2-3%), but increasing duration yields consistent gains. Use lower FFT bins to save memory/compute without major accuracy loss.

- **Failure signatures:**
  - Confusion between "Contestia" and "Olivia": These modes share nearly identical spectral signatures because Contestia is a derivative of Olivia. Expect high misclassification rates here unless higher-order features are engineered.
  - High Frequency Drift: If a receiver drifts >500 Hz (the limit of the FreqShift augmentation), expect accuracy to plummet, as seen in the lower scores for R1 vs R0.

- **First 3 experiments:**
  1. Sanity Check (Augmentation Ablation): Train a model on the generated data without the Noise augmentation. Verify that the accuracy on the real-world test set collapses to single digits (<5%) to validate the "Sim-to-Real" dependency.
  2. Duration Sensitivity Test: Evaluate the best model (EfficientNetB0) on varying window sizes (0.5s, 1.0s, 2.0s, 4.0s) using the R0 dataset. Plot accuracy vs. duration to confirm the "2-second" inflection point for your specific hardware setup.
  3. Receiver Cross-Validation: Train on data augmented strictly for R0 parameters and test on R1, then swap. This diagnoses whether the augmentation pipeline generalizes across different SDR architectures (Heterodyne vs. Direct Conversion).

## Open Questions the Paper Calls Out
None

## Limitations
- Generalization Scope: The study validates on only two SDR architectures within the 70 cm band, potentially missing real-world interference patterns not represented in the augmentation set.
- Operational Latency: The 2-second minimum signal duration requirement may not suit applications requiring sub-second response times, such as fast frequency-hopping systems.
- Mode Ambiguity: The fundamental similarity between Contestia and Olivia modes creates inherent classification ambiguity that cannot be fully resolved through data augmentation or model architecture alone.

## Confidence
- **High Confidence:** The core claim that data augmentation is essential for bridging the sim-to-real gap is strongly supported by ablation studies showing accuracy drops from ~85% to ~3% without augmentation.
- **Medium Confidence:** The assertion that signal duration is more important than FFT bin count for accuracy is supported by empirical results, though the underlying reasons are not fully explored.
- **Medium Confidence:** The model's performance on parameterized signals (98 classes) is lower than on mode classes (17 classes), but the paper does not deeply analyze which specific parameterizations cause the most confusion.

## Next Checks
1. **Augmentation Ablation Test:** Remove the Noise augmentation and retrain the model to verify that real-world accuracy drops to ~3-5%, confirming the critical role of AWGN simulation.
2. **Duration Sensitivity Analysis:** Evaluate the trained model on varying signal durations (0.5s, 1s, 2s, 4s) using the R0 dataset to empirically confirm the 2-second inflection point for classification accuracy.
3. **Receiver Architecture Transfer:** Train models with augmentation parameters calibrated for R0 (Heterodyne) and test on R1 (Direct Conversion), then swap. This will diagnose whether the augmentation pipeline generalizes across different SDR architectures.