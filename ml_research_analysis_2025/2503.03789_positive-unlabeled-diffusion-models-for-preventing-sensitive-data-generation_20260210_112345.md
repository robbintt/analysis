---
ver: rpa2
title: Positive-Unlabeled Diffusion Models for Preventing Sensitive Data Generation
arxiv_id: '2503.03789'
source_url: https://arxiv.org/abs/2503.03789
tags:
- data
- sensitive
- diffusion
- normal
- unlabeled
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of preventing diffusion models
  from generating sensitive data, such as inappropriate or harmful content, when training
  data contains such sensitive examples. The proposed method, called positive-unlabeled
  diffusion models (PU-DM), introduces a binary classification framework into diffusion
  model training and applies positive-unlabeled (PU) learning to this framework.
---

# Positive-Unlabeled Diffusion Models for Preventing Sensitive Data Generation

## Quick Facts
- **arXiv ID:** 2503.03789
- **Source URL:** https://arxiv.org/abs/2503.03789
- **Reference count:** 25
- **One-line primary result:** A novel method for preventing diffusion models from generating sensitive data by adapting positive-unlabeled learning to diffusion training, achieving higher non-sensitive rates without significantly compromising image quality.

## Executive Summary
This paper addresses the challenge of preventing diffusion models from generating sensitive or harmful content when training data contains such examples. The proposed Positive-Unlabeled Diffusion Models (PU-DM) framework introduces a binary classification mechanism into diffusion training, allowing the model to maximize the ELBO for normal data and minimize it for sensitive data using only unlabeled data and a small amount of labeled sensitive data. The method does not require labeled normal data, which is often impractical to obtain. Experiments across multiple datasets demonstrate significant improvements in the non-sensitive rate—the ratio of generated samples classified as normal—compared to unsupervised and supervised baselines, without significantly compromising image quality measured by FID scores.

## Method Summary
The PU-DM method reformulates diffusion model training as a binary classification problem where samples are classified as either normal (y=0) or sensitive (y=1). The ELBO for normal data is maximized while the ELBO for sensitive data is minimized through a bounded binary cross-entropy loss. Under the assumption that the unlabeled data follows a mixture distribution of normal and sensitive data, the normal distribution can be approximated using only unlabeled and sensitive data. The final objective combines a positive risk term (sensitive-as-positive BCE) and a non-negative risk estimator that approximates the negative risk for normal data. A non-negative risk correction with max(0,·) prevents overfitting to finite-sample noise. The method works for both training from scratch and fine-tuning pre-trained models, and can be extended to conditional diffusion models and positive-negative-unlabeled settings.

## Key Results
- PU-DM achieves significantly higher non-sensitive rates (up to 0.98) compared to unsupervised and supervised baselines across MNIST, CIFAR10, STL10, and CelebA datasets
- The method maintains comparable FID scores to baselines, demonstrating that content filtering does not significantly compromise image quality
- PU-DM works effectively for both training from scratch and fine-tuning pre-trained models, with fine-tuning showing slightly better FID preservation
- The approach can be extended to conditional diffusion models and positive-negative-unlabeled settings where some normal data is available

## Why This Works (Mechanism)

### Mechanism 1
Framing diffusion training as binary classification bounds ELBO manipulation, preventing divergence during sensitive-data unlearning. The method models pθ(y|x) using the noise-prediction loss ℓ(x;θ): pθ(y|x) = exp(-ℓ(x;θ)) for y=0 (normal) and 1-exp(-ℓ(x;θ)) for y=1 (sensitive). The resulting BCE loss ℓBCE(x,y;θ) = (1-y)ℓ(x;θ) - y·log(1-exp(-ℓ(x;θ))) minimizes ℓ for normal data and maximizes it for sensitive data through a bounded term, since -log(1-exp(-ℓ)) is bounded above even though ℓ is unbounded. Core assumption: Sensitive and normal data can be distinguished via ELBO magnitudes. Break condition: If normal and sensitive data have highly overlapping distributions with similar reconstruction difficulty, ℓ(x;θ) may not discriminate well.

### Mechanism 2
Rewriting the normal distribution as a linear combination of unlabeled and sensitive distributions enables ELBO optimization without labeled normal data. Under the mixture assumption pU(x) = βpS(x) + (1-β)pN(x), the normal distribution is recovered as (1-β)pN(x) = pU(x) - βpS(x). Substituting into the supervised objective yields LPN(θ) = βEpS[ℓBCE(x,1;θ)] + EpU[ℓBCE(x,0;θ)] - βEpS[ℓBCE(x,0;θ)], which depends only on U and S. Core assumption: The class prior β accurately reflects the true proportion of sensitive data in U. Break condition: If β is severely misestimated or pU is not a simple mixture, the approximation degrades.

### Mechanism 3
A non-negative risk estimator with a max(0,·) correction prevents overfitting to finite-sample noise when estimating the normal-data loss. The term L−U(θ) - βL−S(θ) approximates (1-β)EpN[ℓBCE(x,0;θ)] ≥ 0 theoretically, but sampling noise can make it negative. The max{0, L−U(θ) - βL−S(θ)} clamp prevents the optimizer from exploiting negative values. When negative, gradients instead push the term toward zero. Core assumption: With sufficient data, L−U(θ) - βL−S(θ) converges to a non-negative expectation. Break condition: In high-capacity models, the max-based correction may cause divergence; the paper suggests using |L−U - βL−S| instead in such cases.

## Foundational Learning

- **Concept: Evidence Lower Bound (ELBO) in diffusion models**
  - **Why needed here:** PU-DM reformulates diffusion training as ELBO maximization for normal data and minimization for sensitive data. Without understanding ELBO's role, the loss design is opaque.
  - **Quick check question:** Can you explain why minimizing the noise prediction error ϵ - ϵθ(xt, t) is equivalent to maximizing the ELBO?

- **Concept: Positive-Unlabeled (PU) learning fundamentals**
  - **Why needed here:** The core innovation is adapting PU classification theory to generative models. The class-prior β and risk decomposition are borrowed directly from PU learning.
  - **Quick check question:** Given a dataset with 10% labeled positives and unlabeled data containing both classes, how would you estimate the class prior β?

- **Concept: Binary cross-entropy as a bounded loss for unbounded objectives**
  - **Why needed here:** Direct maximization of ℓ(x;θ) for sensitive data is unstable; BCE provides a bounded alternative via -log(1-exp(-ℓ)).
  - **Quick check question:** Why does -log(1-exp(-ℓ)) remain bounded even as ℓ → ∞, and how does this prevent training instability?

## Architecture Onboarding

- **Component map:**
  - **Noise estimator ϵθ(xt, t):** U-Net backbone (varies by dataset; see Appendix A.1 for configs)
  - **Loss modules:** L+S (sensitive-as-positive BCE), L−U (unlabeled-as-negative BCE), L−S (sensitive-as-negative BCE)
  - **β estimator:** External module or hyperparameter; paper suggests PU estimation methods (Menon et al., 2015)
  - **Training loop:** Standard diffusion training with modified objective LPU(θ) per Algorithm 1

- **Critical path:**
  1. Sample mini-batch from U (unlabeled) and S (sensitive)
  2. Compute L+S, L−U, L−S via Eq. 18 (BCE with appropriate labels)
  3. Check sign of (L−U - βL−S); compute gradient via Eq. 19
  4. Update θ; repeat until convergence

- **Design tradeoffs:**
  - **β sensitivity:** Higher β increases suppression of sensitive data but may degrade FID (Fig. 4). Paper finds β ≈ 0.15-0.2 works well even when true β ≈ 0.09.
  - **max vs. absolute value:** max{0, ·} is standard for PU learning; |·| may be more stable for large models (Stable Diffusion) per Appendix A.6.
  - **From-scratch vs. fine-tuning:** Fine-tuning shows better FID preservation; from-scratch shows slightly higher FID (Tables 2-3 vs. 4-5).

- **Failure signatures:**
  - **High FID with low non-sensitive rate:** β may be too high, over-suppressing diverse normal samples.
  - **Low non-sensitive rate despite PU-DM:** Labeled sensitive data S may not cover all sensitive modes (e.g., digit 9 missing from S; see Appendix A.4).
  - **Training divergence:** For large models, switch from max-based to absolute-value loss.

- **First 3 experiments:**
  1. **MNIST parity test (as in Fig. 1):** Treat even digits as normal, odd as sensitive. Use 10% sensitive ratio in U. Verify non-sensitive rate >0.95 with FID comparable to baseline.
  2. **β ablation on CIFAR10:** Replicate Fig. 4 sweep (β ∈ {0.05, 0.1, 0.15, 0.2, 0.25}) to confirm that non-sensitive rate improves with β while FID has a sweet spot. Use vehicles vs. animals split.
  3. **Fine-tuning pre-trained CelebA model:** Start from `google/ddpm-celebahq-256`, apply PU-DM with male/female split. Measure non-sensitive rate and FID vs. unsupervised fine-tuning baseline.

## Open Questions the Paper Calls Out

- **Open Question 1:** How can the framework be extended to prevent the generation of sensitive data that is semantically similar to, but not exactly represented in, the labeled sensitive dataset? The current method relies on the assumption that labeled sensitive data covers the full distribution of sensitive concepts, which is often impractical. Evidence that would resolve it: A method that successfully suppresses generation of unlabelled sensitive categories using auxiliary information (e.g., text descriptions) or few-shot generalization techniques.

- **Open Question 2:** What is the optimal strategy for determining or adaptively estimating the hyperparameter β (the ratio of sensitive data in the unlabeled set) during training? The paper demonstrates sensitivity to β but relies on manual tuning rather than a principled estimation algorithm integrated into training. Evidence that would resolve it: An analysis showing convergence properties based on β, or an automated mechanism that estimates β from the data distribution without requiring manual validation.

- **Open Question 3:** Can the objective function be modified to consistently improve or maintain Fréchet Inception Distance (FID) in "from-scratch" training scenarios? The current loss balancing effectively filters content but appears to occasionally distort image fidelity when training without pre-trained weights. Evidence that would resolve it: A modification to the loss function that decouples the suppression of sensitive features from the generative capacity for normal features, resulting in equal or better FID scores across all datasets.

## Limitations

- The method requires labeled sensitive data, which may be difficult to obtain comprehensively across all sensitive categories
- The approach relies on accurate class prior estimation (β), though experiments show some robustness to misestimation
- Performance on large-scale models (beyond Stable Diffusion 1.4) has not been extensively validated
- The classifier-based non-sensitive rate metric may not be reliable if the classifier itself is biased or poorly calibrated

## Confidence

- **High:** Theoretical soundness of PU reformulation for diffusion models
- **Medium:** Empirical effectiveness across multiple datasets and model scales
- **Medium:** Generalization to conditional generation and PNU settings

## Next Checks

1. **Large-scale model test:** Apply PU-DM to a 1B+ parameter diffusion model and evaluate whether the max-based loss correction causes divergence
2. **β estimation ablation:** Compare performance using estimated β vs. fixed β=0.1 across datasets with varying sensitive data ratios
3. **Out-of-distribution sensitivity:** Test whether the classifier-based non-sensitive rate metric remains reliable when sensitive data distribution shifts between training and generation