---
ver: rpa2
title: Automatic database description generation for Text-to-SQL
arxiv_id: '2502.20657'
source_url: https://arxiv.org/abs/2502.20657
tags:
- database
- descriptions
- table
- column
- description
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work tackles the cold-start problem in Text-to-SQL by automatically\
  \ generating table and column descriptions when explicit metadata is missing. The\
  \ proposed dual-process approach combines coarse-to-fine (database\u2192table\u2192\
  column) and fine-to-coarse (column\u2192table) reasoning to leverage LLM knowledge\
  \ and contextual alignment."
---

# Automatic database description generation for Text-to-SQL

## Quick Facts
- arXiv ID: 2502.20657
- Source URL: https://arxiv.org/abs/2502.20657
- Authors: Yingqi Gao; Zhiling Luo
- Reference count: 6
- Primary result: Generates table/column descriptions to solve cold-start problem in Text-to-SQL, improving accuracy by 0.98% over no descriptions and achieving 37% of human-level performance

## Executive Summary
This work addresses the cold-start problem in Text-to-SQL systems where database metadata is missing. The authors propose a dual-process approach combining coarse-to-fine and fine-to-coarse reasoning to automatically generate table and column descriptions. The method leverages LLM knowledge to understand database structure, categorize columns, and create concise descriptions that improve downstream SQL generation accuracy. Implemented as an open-source tool supporting multiple database systems, it integrates with M-Schema for NL2SQL tasks.

## Method Summary
The method employs a dual-process approach: first, a coarse-to-fine process that moves from database-level understanding to table and column details, then a fine-to-coarse process that refines table-level understanding from detailed column insights. It categorizes columns into Code, Enum, DateTime, Text, and Measures, with special handling for same-category differentiation. Column descriptions are limited to <20 words and table descriptions to <100 words. The system supports four processing modes (No_comment, Origin, Generation, Merge) and extracts statistical features like count(distinct), min/max/avg values, and string lengths to ground LLM inference.

## Key Results
- Generated descriptions improve execution accuracy by 0.93-0.98% over no descriptions on Bird benchmark
- System achieves approximately 37% of human-level performance in SQL generation accuracy
- Open-source implementation supports SQLite, MySQL, and PostgreSQL databases
- Tool integrates with M-Schema format for downstream NL2SQL consumption

## Why This Works (Mechanism)

### Mechanism 1: Dual-Process Hierarchical Reasoning
The combination of coarse-to-fine and fine-to-coarse reasoning creates a more robust understanding than unidirectional approaches. The coarse-to-fine process establishes domain context and ensures downstream analysis remains aligned with database purpose, while the fine-to-coarse process refines table-level understanding by synthesizing detailed column-level insights upward. This leverages LLM pre-training knowledge to infer table/column semantics from structural patterns and naming conventions.

### Mechanism 2: Cascading Context Propagation
Propagating synthesized context through information hierarchies (db_info → table_info → column_info → column_relation) creates cascading constraints that improve individual element descriptions. Domain recognition at the database level constrains the hypothesis space for table functions, which then bounds column semantics. Same-category comparison adds lateral constraints, with each layer reducing ambiguity for subsequent analysis.

### Mechanism 3: Column Categorization with Same-Category Differentiation
Explicitly categorizing columns (Code, Enum, DateTime, Text, Measures) and forcing comparison within categories reduces schema linking errors between semantically similar fields. Categorization applies type-specific analysis templates, while same-category comparison prompts the LLM to articulate distinctions between similar fields, creating discriminative descriptions that prevent downstream conflation.

## Foundational Learning

- **Text-to-SQL Cold-Start Problem**: Understanding why schema metadata is essential for NL2SQL systems when mapping natural language to database elements. Quick check: Given a database with tables "orders", "customers", "products" but no descriptions, how would an NL2SQL system distinguish between "status" columns that appear in multiple tables with different meanings?

- **Schema Linking**: The task of aligning natural language query terms to specific database elements. Quick check: When a user asks "show me promotional items," which columns should the system consider: "isPromo", "promoType", "promotion_id", or all three? How do descriptions help disambiguate?

- **M-Schema Representation**: The target schema structure format for downstream NL2SQL consumption. Quick check: What metadata fields does M-Schema require beyond table/column names, and which fields does this system populate vs. leave empty?

## Architecture Onboarding

- **Component map**: Database Schema → Database Understanding → db_info → Table Preliminary Understanding → table_info → Column Categorization → column_info + category tag → Same-Category Differentiation → column_relation → Column Description Generation → column descriptions (<20 words) → Table Description Generation → table descriptions (<100 words) → M-Schema Builder → Output for downstream NL2SQL

- **Critical path**: Database understanding → Table preliminary understanding → Column categorization → Same-category differentiation → Column descriptions → Table descriptions → M-Schema output. All steps are sequential dependencies.

- **Design tradeoffs**: Description length constraints (columns <20 words, tables <100 words) balance token costs with semantic completeness. Four processing modes (No_comment/Origin/Generation/Merge) offer flexibility based on trust in existing metadata. Statistical feature extraction adds computational overhead but provides grounding signals for LLM inference.

- **Failure signatures**: Incorrect domain recognition in step 2.1 propagates through all descriptions; DateTime columns stored as int/string may be miscategorized without explicit granularity detection; columns with similar names but different semantics produce conflated descriptions if same-category differentiation fails; descriptions exceeding length limits degrade downstream SQL generation accuracy.

- **First 3 experiments**:
  1. Baseline validation: Run tool on Bird development set in "No_comment" vs. "Generation" mode with Qwen2.5-Coder-14B as SQL generator, comparing execution accuracy to replicate 0.93-0.98% improvement claim.
  2. Ablation on same-category differentiation: Skip step 2.3.2 entirely, measure impact on columns with similar names (e.g., status columns across tables), hypothesis: accuracy drops most for queries involving ambiguous field references.
  3. Mode comparison on partially-annotated schema: Create test database with 50% human annotations, compare "Origin" vs. "Merge" vs. "Generation" modes, assess whether Merge achieves closest to full manual performance.

## Open Questions the Paper Calls Out

- Can the dual-process framework be refined to close the remaining performance gap between generated and manually crafted descriptions? The results section notes that while the method improves accuracy, it only achieves 37% of human-level performance, leaving a significant gap to manual quality. The paper does not analyze the specific types of semantic nuances that humans capture but the LLM-based dual-process misses.

- Does incorporating external evidence improve the quality of the generated database descriptions? The authors explicitly state "The evidence is not utilized in our experiments," despite evidence being a core component of the Bird benchmark for aiding schema understanding. It is unclear if the current performance ceiling is due to the generation method or the intentional exclusion of available evidence.

- How robust is the "Domain Recognition" capability when applied to databases with niche schemas not well-represented in the LLM's pre-training data? The "Database Understanding" phase relies on the LLM's "inherent knowledge" and "Prior Knowledge Acquisition" to identify domains and metrics. If the LLM lacks specific domain knowledge, the initial high-level understanding may be incorrect, potentially propagating errors through the coarse-to-fine and fine-to-coarse processes.

## Limitations

- Evaluation relies on a single benchmark (Bird) with specific database types and narrow accuracy metrics, limiting generalizability
- LLM generation process uses undisclosed prompts and model configurations, creating uncertainty about reproducibility
- 37% human-level performance metric lacks baseline context - it's unclear what constitutes "human-level" descriptions or how they were obtained

## Confidence

- Dual-process mechanism effectiveness: **Medium** - Supported by architectural description but lacks ablation studies comparing coarse-to-fine vs fine-to-coarse in isolation
- Cascading context propagation: **Medium-Low** - Mechanism described but error propagation characteristics untested
- Column categorization benefits: **Medium** - Category taxonomy is detailed, but same-category differentiation impact requires empirical validation
- 0.98% accuracy improvement claim: **Low** - Single benchmark result, unknown LLM generation parameters, no comparison to alternative description generation methods

## Next Checks

1. **Ablation on cascading mechanism**: Create controlled test with synthetic databases where domain recognition is deliberately incorrect, measure how errors propagate through table and column description generation to quantify "catastrophic failure" risk.

2. **Cross-domain generalization test**: Evaluate system on databases from domains underrepresented in LLM pre-training (medical, legal, scientific) versus well-represented domains (e-commerce, social media), compare accuracy degradation patterns to identify domain-specific limitations.

3. **Prompt sensitivity analysis**: Systematically vary temperature, max tokens, and prompt phrasing for the description generation LLM, measure variance in generated descriptions and resulting SQL accuracy to establish stability of the generation process.