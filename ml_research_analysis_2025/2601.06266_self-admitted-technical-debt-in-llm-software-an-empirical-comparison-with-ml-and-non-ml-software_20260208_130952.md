---
ver: rpa2
title: 'Self-Admitted Technical Debt in LLM Software: An Empirical Comparison with
  ML and Non-ML Software'
arxiv_id: '2601.06266'
source_url: https://arxiv.org/abs/2601.06266
tags:
- debt
- satd
- repositories
- systems
- technical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares self-admitted technical debt (SATD) in LLM,
  ML, and non-ML repositories. The authors analyze 477 repositories (159 per category)
  using survival analysis to examine SATD introduction and removal patterns.
---

# Self-Admitted Technical Debt in LLM Software: An Empirical Comparison with ML and Non-ML Software

## Quick Facts
- arXiv ID: 2601.06266
- Source URL: https://arxiv.org/abs/2601.06266
- Reference count: 40
- Primary result: LLM repositories remain debt-free 2.4x longer than ML repositories (492 vs. 204 days median) despite similar accumulation rates (3.95% vs. 4.10%)

## Executive Summary
This study compares self-admitted technical debt (SATD) across 477 repositories (159 each of LLM, ML, and non-ML) to understand how architectural complexity affects debt patterns. LLM systems show similar debt accumulation rates to ML systems despite greater complexity, but remain debt-free significantly longer before debt accumulates rapidly. The research identifies three novel SATD categories unique to LLM development: Model-Stack Workaround Debt (6.3%), Model Dependency Debt (11.2%), and Performance Optimization Debt (4.9%). First SATD introduced into files is rarely removed (<5%) while overall removal rates are higher (49.1%), revealing two distinct debt management modes. The study uses survival analysis to examine temporal patterns and finds SATD concentrates highest in deployment and monitoring stages (30%) and pretraining (23%).

## Method Summary
The study analyzes 477 repositories using a three-phase approach: detection, taxonomy extension, and survival analysis. SATD detection employs Liu et al.'s NLP classifier (F1 0.82) on Python comments extracted via CommentParser, followed by manual filtering due to 44% false positive rate. A card-sorting process with 377 sampled instances extends the SATD taxonomy, identifying three new LLM-specific categories through 85.7% initial inter-rater agreement. Kaplan-Meier survival analysis with log-rank tests examines introduction and removal timing, while Random Forest models predict long-lasting debt with 82% accuracy for LLM vs 96% for ML. The analysis reveals two distinct debt management modes: first-file SATD (<5% removal) versus comment-level debt (≈50% removal).

## Key Results
- LLM repositories remain debt-free 2.4× longer than ML repositories (492 vs. 204 days median) before rapid accumulation
- Three novel SATD categories unique to LLM development: Model-Stack Workaround Debt (6.3%), Model Dependency Debt (11.2%), Performance Optimization Debt (4.9%)
- First SATD introduced into files is rarely removed (<5%) while overall removal rates are higher (49.1%)
- SATD concentrates highest in deployment and monitoring (30%) and pretraining (23%) stages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM repositories remain debt-free 2.4× longer than ML repositories before debt accumulates rapidly.
- Mechanism: LLM projects begin with a prolonged infrastructure-building phase where mature ML practices and standardized frameworks (PyTorch Lightning, Hugging Face) delay initial shortcuts. Once complexity crosses a threshold, debt accumulates rapidly.
- Core assumption: The delayed introduction reflects disciplined early architecture, not simply younger codebases with less history.
- Evidence anchors:
  - [abstract] "LLM repositories remain debt-free 2.4x longer than ML repositories (a median of 492 vs. 204 days)"
  - [section RQ4] "LLM repositories exhibit a two-phase development pattern... reflecting a prolonged infrastructure-building phase before complexity forces shortcuts"
  - [corpus] PromptDebt (arxiv 2509.20497) examines LLM-specific SATD but does not address temporal dynamics, limiting cross-validation
- Break condition: If the 492-day median primarily reflects post-2022 repository age rather than architectural discipline, the mechanism confounds maturity with practice.

### Mechanism 2
- Claim: First SATD introduced into any file becomes structurally permanent (<5% removal rate).
- Mechanism: Initial debt embedded during file creation locks into architectural decisions too costly to revisit. Developers actively clean recent, localized debt (≈50% removal) while accommodating foundational compromises.
- Core assumption: Removal difficulty stems from architectural coupling, not developer negligence or prioritization.
- Evidence anchors:
  - [abstract] "First SATD introduced into files is rarely removed (<5%), while overall removal rates are higher (49.1%)"
  - [section RQ4] "This contrast between comment-level removal (≈50%) and first-file-SATD removal (<5%) reveals two modes of debt management"
  - [corpus] Related SATD papers (arxiv 2510.22249, arxiv 2505.01136) do not examine first-file permanence, limiting external validation
- Break condition: If removal rates increase as repositories mature beyond the study window, permanence may be age-dependent rather than structural.

### Mechanism 3
- Claim: External ecosystem volatility drives 22.4% of LLM debt through three novel categories absent from prior taxonomies.
- Mechanism: LLM systems depend on rapidly evolving external APIs, tokenizers, and frameworks outside developer control. This creates Model-Stack Workaround Debt (compensating for external bugs), Model Dependency Debt (waiting for upstream releases), and Performance Optimization Debt (deferred GPU/kernel tuning).
- Core assumption: These debt sources are qualitatively different from traditional dependency management.
- Evidence anchors:
  - [abstract] "Three new forms of technical debt unique to LLM-based development: Model-Stack Workaround Debt (6.3%), Model Dependency Debt (11.2%), and Performance Optimization Debt (4.9%)"
  - [section RQ2] "Comments such as 'TODO: Remove once accelerate is updated' reveal a dependency-driven debt pattern where resolution depends entirely on external release cycles"
  - [corpus] PromptDebt (arxiv 2509.20497) independently identified prompt-related debt, partially supporting ecosystem-specific debt claims
- Break condition: If traditional software exhibits similar external-dependency patterns at scale, the categorization may not be LLM-unique.

## Foundational Learning

- Concept: **Survival Analysis with Censoring**
  - Why needed here: The paper models SATD introduction and removal timing using Kaplan-Meier estimation, which properly handles unresolved debt (censored observations) at study endpoint.
  - Quick check question: Why would treating unresolved SATD as "removed" bias persistence estimates?

- Concept: **Effect Size vs. Statistical Significance**
  - Why needed here: LLM vs. ML prevalence shows p < 0.005 but Cohen's d = 0.04 (negligible practical difference), illustrating how large samples yield significance for trivial effects.
  - Quick check question: What does d = 0.04 mean for prioritizing LLM-specific debt tooling?

- Concept: **Taxonomy Extension via Card Sorting**
  - Why needed here: 22.4% of LLM SATD required new categories discovered through iterative card-sorting of 377 sampled instances with 85.7% initial inter-rater agreement.
  - Quick check question: Why might OO-focused taxonomies (Bavota & Russo) miss LLM architectural debt?

## Architecture Onboarding

- Component map:
  - CommentParser extracts Python comments -> Liu et al. NLP classifier (0.82 F1) -> temporal metadata for survival analysis
  - Dataset Layer: 477 repos (159 LLM post-2022, 159 ML 2015-2021, 159 non-ML), 5M+ comment events
  - Analysis Layer: Prevalence (density/diffusion), taxonomy (377 sampled + manual classification), survival (Kaplan-Meier), prediction (Random Forest)

- Critical path:
  1. Repository curation via 41 LLM-specific search queries + multi-signal validation (README, topics, dependencies)
  2. Git history extraction (3.7M+ comment events for LLM)
  3. SATD detection -> 44% false positive rate requiring manual filtering
  4. Card-sorting taxonomy extension with consensus resolution

- Design tradeoffs:
  - Cross-sectional snapshots vs. longitudinal: 44× shift in ML introduction time (10 days 2021 -> 441 days 2025) shows age confounds single-snapshot studies
  - Comment-level vs. file-level analysis: 50% vs. <5% removal rates yield opposite conclusions about debt management
  - Statistical power vs. manual burden: 159 repos balances detection with feasible classification

- Failure signatures:
  - Classifier trained on non-LLM code yields 44% false positives on LLM comments
  - Random Forest accuracy drops from 96% (ML) to 82% (LLM), indicating unmeasured factors (prompt quality, API volatility)
  - "Undefined" SATD (TODO-only comments) reaches high proportions, suggesting documentation practices lag system complexity

- First 3 experiments:
  1. Run the replication package on a subset of LLM repos to validate detection pipeline and false positive rate.
  2. Apply extended taxonomy to your own LLM codebase; measure distribution across new categories.
  3. Calculate first-file SATD removal rate in your project history to benchmark against <5% baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LLM practitioners weigh the tradeoffs involved in resolving foundational debt versus accommodating it as permanent architecture?
- Basis in paper: [explicit] The conclusion states that "Qualitative studies with LLM practitioners could clarify how they weigh debt tradeoffs and the challenges of resolving foundational debt."
- Why unresolved: This study relied on quantitative repository mining and analysis of comments, which reveals the presence and persistence of debt but not the human rationale behind retention or removal decisions.
- What evidence would resolve it: Interview data or survey responses from developers detailing the economic and technical constraints that discourage them from removing early, foundational SATD.

### Open Question 2
- Question: To what extent do LLM-specific factors (e.g., prompt quality, API stability, RAG complexity) improve the prediction of long-lasting SATD over standard code metrics?
- Basis in paper: [inferred] The authors note in the Threats to Validity that Random Forest models "may overlook LLM-specific factors such as prompt quality, API stability, or RAG complexity," potentially explaining the lower prediction accuracy for LLM (82%) compared to ML (96%) repos.
- Why unresolved: The study utilized traditional commit-level and file-level features (lines of code, complexity) but did not operationalize or test metrics specific to the unique architecture of LLMs.
- What evidence would resolve it: A comparative study of predictive models that include features extracted from prompt configurations, external API usage, and vector store implementations versus baseline models.

### Open Question 3
- Question: Can automated CI/CD bots effectively flag high-risk pull requests to prevent the introduction of persistent technical debt?
- Basis in paper: [explicit] The authors explicitly suggest "Future work should develop CI/CD bots to flag high-risk pull requests and prevent lasting debt."
- Why unresolved: While the study identifies large commits and specific pipeline stages as predictors of debt, it does not validate the efficacy of automated interventions in real-time development workflows.
- What evidence would resolve it: Empirical results from repositories utilizing such tools, showing a measurable decrease in the introduction or survival time of SATD in flagged pull requests.

## Limitations

- The 44% false positive rate from the NLP classifier introduces significant uncertainty in prevalence estimates, particularly for LLM-specific debt types
- The study confounds chronological age with architectural maturity by comparing post-2022 LLM repos with 2015-2021 ML repos
- Manual card-sorting process for taxonomy extension lacks precise operational definitions for consistent replication

## Confidence

**High Confidence**: LLM vs. ML prevalence difference (3.95% vs. 4.10%) with p < 0.005, though negligible effect size (Cohen's d = 0.04) suggests practical equivalence despite statistical significance. The identification of three novel SATD categories through systematic sampling and consensus resolution shows robust methodology.

**Medium Confidence**: Survival analysis results showing 2.4× longer debt-free periods for LLM repositories, as this depends on accurate censoring assumptions and could be influenced by repository age distributions. The <5% removal rate for first-file SATD is methodologically sound but may not generalize across different development cultures.

**Low Confidence**: The claim that these three debt categories are uniquely LLM-specific, as traditional software systems may exhibit similar patterns under different nomenclature. The 82% Random Forest accuracy for LLM debt prediction is notably lower than ML (96%), suggesting unmeasured confounding variables or model limitations specific to LLM systems.

## Next Checks

1. **Temporal Confound Test**: Re-analyze the data by restricting ML repositories to post-2022 age matches with LLM repositories to isolate architectural effects from age effects on debt accumulation patterns.

2. **Cross-Corpus Validation**: Apply the extended LLM-specific taxonomy to a corpus of traditional enterprise software repositories to determine whether the three new debt categories exist but remain undetected by non-LLM-focused taxonomies.

3. **Classifier Performance Assessment**: Manually validate SATD classifications in a stratified sample of 100 LLM repository comments to independently estimate the false positive rate and assess whether LLM-specific debt patterns systematically bias the detector's performance.