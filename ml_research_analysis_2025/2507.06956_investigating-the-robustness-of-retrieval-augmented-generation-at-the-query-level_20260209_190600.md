---
ver: rpa2
title: Investigating the Robustness of Retrieval-Augmented Generation at the Query
  Level
arxiv_id: '2507.06956'
source_url: https://arxiv.org/abs/2507.06956
tags:
- query
- robustness
- dataset
- performance
- different
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a systematic framework to evaluate the robustness\
  \ of Retrieval-Augmented Generation (RAG) systems to query perturbations. The authors\
  \ generate perturbed queries through LLM prompting (redundancy, formal tone, ambiguity)\
  \ and random typos, then measure performance changes across 12 RAG pipelines (4\
  \ retrievers \xD7 3 LLMs) on three QA datasets."
---

# Investigating the Robustness of Retrieval-Augmented Generation at the Query Level

## Quick Facts
- arXiv ID: 2507.06956
- Source URL: https://arxiv.org/abs/2507.06956
- Reference count: 13
- Key outcome: Systematic framework evaluating RAG robustness to query perturbations (typos, redundancy, formality, ambiguity) across 12 pipelines on three QA datasets

## Executive Summary
This paper introduces a systematic framework to evaluate the robustness of Retrieval-Augmented Generation (RAG) systems to query perturbations. The authors generate perturbed queries through LLM prompting (redundancy, formal tone, ambiguity) and random typos, then measure performance changes across 12 RAG pipelines (4 retrievers × 3 LLMs) on three QA datasets. Key findings include: dense retrievers are more robust to redundancy but sparse methods handle typos better; generator robustness depends heavily on context availability; and pipeline performance is largely dictated by retriever sensitivity. Pearson correlation analysis reveals that different perturbation types affect retriever and generator modules differently, suggesting targeted robustness improvements.

## Method Summary
The framework evaluates RAG robustness through systematic query perturbations: LLM-generated (redundancy, formal tone, ambiguity) and TextAttack-generated typos (10%, 25%). Twelve pipelines are tested: 4 retrievers (BGE-base, Contriever, BM25-Flat, BM25-MF) × 3 generators (Llama-3.1-8B, Mistral-7B, Qwen-2.5-7B) on NQ, HotpotQA, and BioASQ datasets. Performance is measured through Recall@k for retrieval and Match metric (exact span presence) for generation. Three settings are evaluated: closed-book (generator only), oracle (golden documents), and RAG (top-5 retrieved). Pearson correlation analysis identifies which module drives performance degradation under each perturbation type.

## Key Results
- Dense retrievers show superior robustness to semantic perturbations (redundancy, ambiguity) while sparse retrievers resist lexical perturbations (typos)
- Generator robustness strongly depends on context availability, with oracle settings showing significant recovery from query degradation
- Pipeline performance is primarily driven by retriever sensitivity for general domains (NQ) but by generator sensitivity for domain-specific contexts (BioASQ)
- Correlation analysis reveals distinct sensitivity patterns across perturbation types, enabling targeted robustness improvements

## Why This Works (Mechanism)

### Mechanism 1: Differential Retriever Sensitivity to Perturbation Types
Dense and sparse retrievers exhibit inversely correlated robustness patterns—dense methods handle semantic noise (redundancy) better while sparse methods resist lexical noise (typos). Dense retrievers encode queries into continuous semantic embeddings where redundant information integrates smoothly without shifting the representation significantly. Sparse retrievers (BM25) rely on exact token matching, so typos that preserve some token overlap cause less disruption than semantic reformulations that change the token distribution entirely.

### Mechanism 2: Context Availability Mediates Generator Recovery
Generators show dramatically different robustness depending on whether they operate from parametric memory (closed-book) or retrieved context (oracle/RAG). In closed-book settings, query perturbations directly corrupt the prompt the LLM uses to access internal knowledge. When golden context is provided (oracle setting), the LLM can ground generation in external documents, partially compensating for query degradation—especially for typos at 10% level.

### Mechanism 3: Pipeline Performance Dominated by Retriever Sensitivity with Domain Modulation
End-to-end RAG performance correlations reveal retriever-driven degradation for general domains, but generator sensitivity dominates in domain-specific contexts under semantic perturbations. Pearson correlation analysis between module-level and pipeline performance shows that for NQ (general domain), retriever-RAG correlations are consistently high (0.27–0.40). For BioASQ (biomedical), generator-oracle correlations dominate for ambiguity (0.33) and redundancy (0.35), while retriever correlations are near-zero—indicating the generator becomes the bottleneck when domain expertise is required.

## Foundational Learning

- Concept: **Recall@k as a retrieval robustness metric**
  - Why needed here: The paper uses Recall@k to measure how perturbations affect whether relevant documents appear in top-k results—this is the bottleneck for downstream generation.
  - Quick check question: If Recall@5 drops from 70% to 50% under typo perturbation but the generator's oracle performance stays constant, where is the system bottleneck?

- Concept: **Closed-book vs Oracle vs RAG evaluation settings**
  - Why needed here: The paper isolates generator behavior by controlling context availability—closed-book tests parametric knowledge, oracle tests context utilization with perfect retrieval, RAG tests the full pipeline.
  - Quick check question: In which setting would you expect typo perturbations to cause the largest performance drop for a typical LLM?

- Concept: **Pearson correlation for module sensitivity decoupling**
  - Why needed here: The paper uses correlation between module-level and end-to-end performance changes to identify which component drives degradation under each perturbation type.
  - Quick check question: If retriever-RAG correlation is 0.05 and generator-oracle correlation is 0.35 for a perturbation type, which module should you prioritize improving?

## Architecture Onboarding

- Component map: Query input → Perturbation layer (LLM-based for semantic noise, TextAttack for typos) → Retriever (BGE/Contriever for dense, BM25 for sparse) → Returns top-k documents from corpus → Generator (Llama/Mistral/Qwen) → Produces answer from query + retrieved context → Evaluation (Match metric for answer span presence, Recall@k for retrieval)

- Critical path:
  1. Query perturbation (5 variants per sample: redundancy, formal, ambiguity, typo10%, typo25%)
  2. Document retrieval (k=5 for end-to-end, varying k for retrieval-only analysis)
  3. Context concatenation (max 100 words per doc, 4096 total input tokens)
  4. Answer generation (greedy decoding, temp=0)
  5. Span matching against ground truth labels

- Design tradeoffs:
  - Dense vs sparse retriever: Choose dense for semantic robustness, sparse for lexical/typo tolerance
  - k selection: Higher k improves recall but increases irrelevant context risk—Figure 7 shows perturbation-specific optimal k values
  - Evaluation metric: Match is model-free and reproducible but misses semantic equivalence; LLM-as-judge adds cost and variance

- Failure signatures:
  - Retriever-driven failure: Recall@5 drops significantly, RAG performance tracks retriever, oracle performance stable (common in NQ with typos)
  - Generator-driven failure: Recall stable but RAG drops, high generator-oracle correlation (BioASQ with ambiguity/redundancy)
  - Compound failure: Both modules degrade, low correlations overall—requires architectural redesign

- First 3 experiments:
  1. Run the perturbation framework on your corpus with BM25 and BGE retrievers, measuring Recall@5 for each perturbation type to identify which retriever better matches your expected query patterns.
  2. Test your generator in oracle setting (golden documents only) under all perturbations to establish upper-bound robustness—if oracle drops significantly, the generator needs attention regardless of retriever.
  3. Compute Pearson correlations between retriever, generator (closed-book and oracle), and end-to-end performance deltas to identify your pipeline's dominant sensitivity mode before investing in mitigation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the inclusion of reranking modules alter the robustness of RAG pipelines to query perturbations?
- Basis in paper: [explicit] The authors state in the Limitations section that they "leave the analysis of the rerankers to a future study" to keep the pipeline simple.
- Why unresolved: Rerankers rely on retriever performance but add a second stage of filtering; it is unknown if they amplify or dampen retriever sensitivity.
- What evidence would resolve it: Experiments measuring RAG performance with and without rerankers under the defined perturbation types (typos, ambiguity, etc.).

### Open Question 2
- Question: Can fine-tuning on perturbed samples or joint retriever-LLM training mitigate the performance degradation caused by query variations?
- Basis in paper: [explicit] The authors list "mitigation strategies... such as fine-tuning... or including perturbed samples into the end-to-end joint training" as future work.
- Why unresolved: The current study evaluates "off-the-shelf" models without fine-tuning, leaving the potential for training-based robustness improvements unexplored.
- What evidence would resolve it: A comparative study training RAG components on perturbed datasets and measuring performance retention against the baseline established in this paper.

### Open Question 3
- Question: To what extent do query transformation techniques (e.g., disambiguation or expansion) improve robustness, and do they introduce new failure modes?
- Basis in paper: [explicit] The authors note that while transformation methods exist, "their effect within the scope of RAG pipelines when faced with various input variations are not addressed in this study."
- Why unresolved: Transformations are often used to boost performance, but the authors suggest their utility might be dependent on the specific perturbation type (e.g., redundancy vs. typos).
- What evidence would resolve it: An analysis applying specific transformations to perturbed queries and measuring the resulting change in the correlation between module performance and RAG output.

## Limitations

- The framework assumes LLM-generated perturbations preserve semantic intent, which may not hold for severe perturbations like 25% typos
- Pearson correlation analysis assumes linear relationships between module-level and pipeline performance changes, potentially masking complex interactions
- Results are based on BEIR datasets which may not fully represent real-world query patterns and domain-specific language

## Confidence

- High confidence: Differential sensitivity patterns between dense and sparse retrievers to semantic vs lexical perturbations are well-supported by data and align with established retrieval theory
- Medium confidence: Generator robustness findings depend heavily on context availability and domain specificity, though correlation analysis provides supporting evidence
- Low confidence: Generalizability to arbitrary RAG systems requires validation as results may depend on specific retriever implementations and LLM capabilities

## Next Checks

1. **Semantic preservation validation**: Manually evaluate a sample of perturbed queries to verify that semantic meaning is preserved, particularly for the 25% typo condition and LLM-generated semantic perturbations.

2. **Domain generalization study**: Apply the framework to additional domain-specific datasets (e.g., legal, financial) to test whether the observed BioASQ patterns (generator dominance under semantic perturbations) extend to other specialized domains.

3. **Correlation robustness testing**: Recompute module sensitivity correlations using non-parametric measures (Spearman/Kendall) and partial correlation controlling for baseline performance to validate the linear relationship assumptions.