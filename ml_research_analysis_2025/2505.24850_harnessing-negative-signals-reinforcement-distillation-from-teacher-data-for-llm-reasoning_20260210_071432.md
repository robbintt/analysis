---
ver: rpa2
title: 'Harnessing Negative Signals: Reinforcement Distillation from Teacher Data
  for LLM Reasoning'
arxiv_id: '2505.24850'
source_url: https://arxiv.org/abs/2505.24850
tags:
- redi
- training
- step
- performance
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates how to maximize the reasoning performance
  of small language models by effectively utilizing both correct and incorrect reasoning
  traces distilled from advanced teacher models. We find that established preference
  optimization methods, like DPO and SimPO, face a trade-off between stability and
  peak performance due to KL regularization.
---

# Harnessing Negative Signals: Reinforcement Distillation from Teacher Data for LLM Reasoning

## Quick Facts
- arXiv ID: 2505.24850
- Source URL: https://arxiv.org/abs/2505.24850
- Reference count: 40
- Small language model achieves 83.1% on MATH-500 using negative signal distillation

## Executive Summary
This paper addresses the challenge of maximizing reasoning performance in small language models through effective utilization of both correct and incorrect reasoning traces from advanced teacher models. The authors identify that established preference optimization methods like DPO and SimPO face a trade-off between stability and peak performance due to KL regularization. They propose Reinforcement Distillation (REDI), an asymmetrically weighted REINFORCE-style objective that achieves stable training while maximizing performance. Applied in a two-stage training recipe (SFT followed by REDI), their method significantly outperforms SFT alone and optimized baselines, with their Qwen-REDI-1.5B model matching the performance of models trained on much larger proprietary datasets while using only 131k open traces.

## Method Summary
The method uses a two-stage training approach: first, supervised fine-tuning (SFT) on correct reasoning traces to establish foundational patterns, then Reinforcement Distillation (REDI) on preference triplets pairing correct and incorrect traces. REDI applies an asymmetric weighting scheme (α < 1) to negative gradients, preventing training collapse while extracting useful signals from errors. The loss function directly maximizes log-likelihood of positive traces and minimizes negative traces with length normalization. The asymmetric weighting moderates gradient contributions that would otherwise suppress semantically similar positive responses.

## Key Results
- Qwen-REDI-1.5B achieves 83.1% on MATH-500, matching models trained on larger proprietary datasets
- REDI outperforms optimized DPO/SimPO baselines while maintaining stable training
- Exceptional data efficiency demonstrated: 131k open traces achieve parity with models trained on proprietary datasets
- Two-stage training (SFT + REDI) necessary for peak performance

## Why This Works (Mechanism)

### Mechanism 1
Asymmetric weighting of negative gradients stabilizes training while preserving performance gains from learning what to avoid. The REDI objective applies weight α < 1 to penalty term from negative traces, moderating gradient contributions that would otherwise suppress semantically similar positive responses. This prevents training collapse while still extracting signal from errors. The method requires α < 1 to avoid collapse, with α=0.8 being optimal.

### Mechanism 2
The β→0 limit of preference optimization objectives converges to a simpler, regularization-free REINFORCE-style loss that achieves higher peak performance. DPO and SimPO use KL divergence penalties for stability, but as β approaches zero, the sigmoid-weighted preference margin collapses to direct log-likelihood maximization/minimization on positive/negative traces respectively. This regularization-free approach constrains reachable performance less than KL-regularized objectives.

### Mechanism 3
Two-stage training (SFT then REDI) leverages positive traces for foundational reasoning patterns before refining with negative signals. Stage 1 SFT establishes a strong policy that learns format and style of reasoning traces. Stage 2 REDI then refines this policy by learning to avoid specific failure modes captured in negative traces. The student model must first acquire basic reasoning capabilities before it can meaningfully learn from error patterns.

## Foundational Learning

- **Concept: REINFORCE-style policy gradient**
  - Why needed here: REDI objective directly maximizes log-likelihood of positive traces and minimizes negative traces—this IS a policy gradient formulation without a value function.
  - Quick check question: Can you explain why weighting log-probabilities by reward (or trace quality) updates a policy toward better outcomes?

- **Concept: KL divergence regularization**
  - Why needed here: Understanding why DPO/SimPO use KL penalties helps contextualize why REDI intentionally avoids them and what tradeoff this creates.
  - Quick check question: What happens to model behavior if KL constraints are too tight vs. too loose during fine-tuning?

- **Concept: Off-policy gradient instability**
  - Why needed here: The paper attributes training collapse to negative gradients penalizing semantically similar positive responses—understanding off-policy distribution shift is crucial.
  - Quick check question: Why might gradients computed on teacher-generated traces cause different update dynamics than on-policy samples?

## Architecture Onboarding

- **Component map:** Teacher traces (correct + incorrect) → Data curation → DSFT (positive pairs) + DPref (preference triplets) → Stage 1: SFT on DSFT → πSFT checkpoint → Stage 2: REDI on DPref → πREDIFinal → Evaluation (pass@k on reasoning benchmarks)

- **Critical path:** The α hyperparameter tuning is the most sensitive step. Paper recommends starting with α=0.8 and tuning learning rate, not α itself.

- **Design tradeoffs:**
  - Higher α → more negative signal utilization but instability risk
  - Higher learning rate → faster learning but collapse risk
  - More SFT epochs → better baseline but potential overfitting before REDI refinement

- **Failure signatures:**
  - Training collapse: Sharp drop in both chosen AND rejected LogPS with accuracy crash
  - Underfitting negatives: Performance plateaus near SFT baseline (α too low)
  - Gradient explosion: Spiking gradient update sizes despite reasonable LogPS

- **First 3 experiments:**
  1. Reproduce SFT baseline: Train Qwen2.5-Math-1.5B on positive traces for 3-5 epochs; establish MATH-500 baseline (~76-80%).
  2. Symmetric REDI sanity check: Apply REDI with α=1.0 at multiple learning rates; observe collapse dynamics at high LR vs. stable-but-lower performance at low LR.
  3. Asymmetric REDI validation: Train with α=0.8, LR=1e-6; verify that chosen/rejected LogPS decrease gradually without collapse while accuracy improves over SFT baseline.

## Open Questions the Paper Calls Out

- Does the REDI recipe generalize effectively to reasoning traces distilled from diverse teacher models that exhibit different error patterns and failure modes?
- What is the theoretical mechanism that makes the asymmetric down-weighting of negative gradients (α < 1) more effective for stability and peak performance than symmetric weighting with lower learning rates?
- Does the quality or "nearness" of the negative trace (e.g., a minor calculation error vs. a hallucination) impact the efficiency of the REDI objective?

## Limitations

- Claims about REDI's superiority over DPO/SimPO depend on hyperparameter choices and may not persist under exhaustive optimization
- Data efficiency claims demonstrated only on math reasoning using a specific teacher model lineage
- Asymmetric weighting mechanism lacks theoretical grounding explaining why specific α value works optimally
- Method's effectiveness may be specific to OpenR1's reasoning patterns rather than generalizable across teacher architectures

## Confidence

**High Confidence:**
- REDI outperforms SFT baseline on MATH-500 (83.1% vs 76.7%)
- Asymmetric weighting (α < 1) prevents training collapse that occurs with symmetric weighting
- Two-stage training (SFT then REDI) is necessary for achieving peak performance

**Medium Confidence:**
- REDI significantly outperforms optimized DPO/SimPO baselines
- Data efficiency claims relative to proprietary datasets
- Generalization to other reasoning benchmarks

**Low Confidence:**
- Specific mathematical claim about β→0 limit converging to REDI loss
- Claims about REDI being the "simplest" solution

## Next Checks

1. **Hyperparameter Sensitivity Analysis**: Systematically vary α (0.2, 0.4, 0.6, 0.8, 1.0) and learning rates across DPO, SimPO, and REDI to establish whether REDI's performance advantage persists under exhaustive hyperparameter optimization.

2. **Teacher Model Generalization**: Replicate the REDI training pipeline using a different teacher model architecture (e.g., DeepSeek-Coder or CodeLlama) to verify that the method's effectiveness is not specific to OpenR1's reasoning patterns.

3. **Cross-Domain Transfer Validation**: Apply the trained Qwen-REDI-1.5B model to non-math reasoning tasks (e.g., commonsense reasoning benchmarks like HellaSwag) to test whether negative signal distillation provides domain-general reasoning improvements.