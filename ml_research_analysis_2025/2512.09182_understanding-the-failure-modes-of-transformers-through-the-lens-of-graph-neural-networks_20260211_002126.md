---
ver: rpa2
title: Understanding the Failure Modes of Transformers through the Lens of Graph Neural
  Networks
arxiv_id: '2512.09182'
source_url: https://arxiv.org/abs/2512.09182
tags:
- information
- graph
- attention
- over-squashing
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes transformer failure modes through GNN theory,
  revealing that decoder-only transformers suffer from predictable information propagation
  bottlenecks caused by their causal masking structure. The analysis shows that causal
  masking creates a position-biased information flow where early tokens dominate representations
  while later tokens experience over-squashing and representation collapse.
---

# Understanding the Failure Modes of Transformers through the Lens of Graph Neural Networks

## Quick Facts
- arXiv ID: 2512.09182
- Source URL: https://arxiv.org/abs/2512.09182
- Authors: Hunjae Lee
- Reference count: 6
- Primary result: Causal masking in decoder-only transformers creates position-biased information flow that causes predictable bottlenecks—early tokens dominate while later tokens experience over-squashing and representation collapse.

## Executive Summary
This theoretical paper analyzes transformer failure modes through graph neural network (GNN) theory, revealing that decoder-only transformers suffer from predictable information propagation bottlenecks caused by their causal masking structure. The analysis shows that causal masking creates a position-biased information flow where early tokens dominate representations while later tokens experience over-squashing and representation collapse. The study provides theoretical grounding for observed phenomena like attention sinks (where initial tokens absorb disproportionate attention scores) and last-token representation collapse. Existing ad-hoc solutions like pause tokens and differential transformers are reinterpreted as attempts to mitigate information over-mixing and position bias.

## Method Summary
The paper employs theoretical analysis connecting transformer failure modes (attention sinks, last-token representation collapse, U-shaped retrieval performance) to GNN information propagation bottlenecks (over-smoothing, over-squashing, under-reaching). The analysis reinterprets existing solutions (pause tokens, differential transformers, attention sinks) through GNN theory without conducting new empirical experiments. The theoretical framework uses metrics like Dirichlet Energy for over-smoothing, Jacobian bounds for over-squashing, and proxy metrics (effective resistance, commute time, spectral gap) to quantify bottleneck severity.

## Key Results
- Causal masking creates position-biased information flow where early tokens dominate representations while later tokens experience over-squashing and representation collapse
- Attention sinks emerge from the first token's unique structural role as a graph "center node" with maximal out-degree but trivial in-degree
- Last-token representation collapse occurs when the final token carries sole responsibility for next-token prediction despite having maximal receptive field size and minimal processing depth
- Existing solutions like pause tokens and differential transformers are reinterpreted as attempts to mitigate information over-mixing and position bias

## Why This Works (Mechanism)

### Mechanism 1: Causal Masking Induces Position-Biased Information Flow
- Claim: Decoder-only transformers exhibit predictable asymmetric information propagation where early tokens dominate representations while later tokens experience bottlenecking.
- Mechanism: Causal masking creates a directed acyclic graph where token at position i can only attend to positions j ≤ i. This gives early tokens exponentially more "runway" (refinement opportunities across layers) while later tokens receive maximal receptive fields with minimal processing layers.
- Core assumption: Information mixing quality correlates with the number of layers a token's representation survives through before being used for downstream tasks.
- Evidence anchors:
  - [abstract] "causal masking creates a position-biased information flow where early tokens dominate representations while later tokens experience over-squashing and representation collapse"
  - [section 3.2] "tokens toward the beginning of the sequence have many opportunities to mix and refine their representations while tokens toward the end of the sequence have very few opportunities"
  - [corpus] "On the Runway Cascade of Transformers for Language Modeling" (FMR=0.60) directly studies runway dynamics, corroborating the position-bias mechanism.
- Break condition: If early tokens were ablated without performance degradation, the position-dominance claim would weaken. The paper cites work showing forcibly removing attention sinks degrades performance (Gu et al., 2024), supporting but not proving causality.

### Mechanism 2: Attention Sinks as Structural Consequences of Causal Geometry
- Claim: Attention sinks emerge from the first token's unique structural role as a graph "center node," not solely from softmax properties.
- Mechanism: The first token has maximal out-degree (all tokens attend to it) but trivial in-degree (it never attends to others). During training, the model routes stabilizing surplus attention through this always-visible token, creating persistent high attention scores independent of semantic relevance.
- Core assumption: High attention scores on semantically irrelevant tokens indicate structural routing rather than learned semantic importance.
- Evidence anchors:
  - [abstract] "provides theoretical grounding for observed phenomena like attention sinks"
  - [section 3.3] "the first token in each sequence is never actually updated via the attention mechanism... making the first token a natural global anchor"
  - [corpus] Corpus signals attention but lacks direct empirical validation; neighbor papers do not address attention sinks specifically.
- Break condition: If injecting dedicated sink tokens at non-initial positions captured equivalent attention without performance loss, the first-token uniqueness claim would require revision.

### Mechanism 3: Over-squashing in Last-Token Representations
- Claim: The last token in causal transformers experiences the most severe over-squashing precisely when it carries sole responsibility for next-token prediction.
- Mechanism: Last-token receptive field grows to include all sequence tokens, but it has only one attention layer to integrate this information. Distinctive information gets "drowned out" by the accumulated signal from earlier positions, causing representation collapse across sequences differing only in final tokens.
- Core assumption: Representation sharpness degrades as receptive field size increases relative to processing depth available.
- Evidence anchors:
  - [abstract] "last-token representation collapse"
  - [section 3.4] "last token representations of different sequences can become arbitrarily close, losing information sharpness... when two sequences have identical tokens throughout the sequence except for the last position"
  - [corpus] "Two failure modes of deep transformers" mentions rank collapse but does not specifically validate last-token collapse dynamics.
- Break condition: If last-token representations remained discriminative across long sequences in controlled tests, the collapse mechanism would be overstated.

## Foundational Learning

- Concept: Message Passing in Graph Neural Networks
  - Why needed here: The paper reframes transformers as GNNs on fully-connected token graphs; understanding aggregate/update functions (ψ, φ) and adjacency operators is prerequisite to grasping over-smoothing and over-squashing.
  - Quick check question: Can you explain why a GNN layer can be written as h_i^(l+1) = φ(h_i^l, Σ_j A_ij ψ(h_i^l, h_j^l))?

- Concept: Information Propagation Bottlenecks (Over-smoothing, Over-squashing, Under-reaching)
  - Why needed here: These three failure modes are the theoretical vocabulary used to diagnose transformer issues; distinguishing them is essential for applying mitigation strategies appropriately.
  - Quick check question: Given a 4-layer transformer, why is under-reaching not applicable but over-squashing still relevant?

- Concept: Dirichlet Energy and Jacobian Sensitivity Bounds
  - Why needed here: These are the quantitative tools for measuring bottleneck severity; Jacobian bounds connect graph topology (A^r+1) to representational sensitivity.
  - Quick check question: What does a low Jacobian |∂h_i^(r+1)/∂x_s| imply about information flow between nodes i and s?

## Architecture Onboarding

- Component map: Input tokens → Positional encoding → [Layer × N] → Final token representation → Next-token prediction head

- Critical path: The last-token representation at layer N is the single point of failure for generation. It depends on: (1) all earlier token embeddings, (2) accumulated attention patterns through all layers, (3) residual signal preservation. Any bottleneck compounds here.

- Design tradeoffs:
  - Depth vs. over-smoothing: More layers increase mixing but accelerate rank collapse.
  - Width vs. over-squashing: Larger model dimensions can mitigate compression (Di Giovanni bound includes d) but increase overfitting risk.
  - Pause token placement: Pre-pending may help attention sink dynamics; appending addresses runway problems for late tokens. Paper suggests padding both ends may be optimal.

- Failure signatures:
  - Attention sink emergence: Check attention matrices for high scores on initial tokens regardless of semantic content.
  - U-shaped retrieval performance: Performance degradation at sequence middles indicates propagation bottlenecks.
  - Last-token representation collapse: Compare cosine similarity of final-token representations across sequences differing only in last position.

- First 3 experiments:
  1. Ablate first token (replace with learned sink token) and measure attention pattern shifts + generation quality. Expect: redistribution of attention without collapse if sink token captures structural role.
  2. Pad sequences with pause tokens at both ends (not just append). Compare last-token representation sharpness (measured via nearest-neighbor distance in embedding space) against unpadded baselines.
  3. Profile effective resistance or commute time across token positions in a frozen trained model. Hypothesis: earlier tokens show lower effective resistance (more paths to others) than later tokens, correlating with attention dominance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can effective resistance and commute time be properly formulated for causal (directed, bipartite) graphs to quantify over-squashing in decoder-only transformers?
- Basis in paper: [explicit] "The effective resistance formulation in (Black et al., 2023) is confined to non-bipartite graphs, excluding causal graphs as a consequence. In addition, the linear mathematical relationship between effective resistance and commute time further complicates the issue since commute time is not defined for causal graphs."
- Why unresolved: Standard graph-theoretic measures assume undirected graphs with symmetric paths; causal graphs break these assumptions.
- What evidence would resolve it: A rigorous extension of effective resistance bounds to directed acyclic graphs, validated against empirical over-squashing measurements in transformer layers.

### Open Question 2
- Question: Are attention sinks harmful phenomena requiring mitigation, or do they serve a beneficial role in slowing information over-mixing?
- Basis in paper: [explicit] The paper notes "forcibly trying to remove the attention sink phenomenon leads to performance degradation" while also suggesting "attention sinks may be providing a mechanism for LLMs to avoid over-mixing by soaking up large portions of attention scores."
- Why unresolved: Conflicting observations—attention sinks waste computation but removing them hurts performance.
- What evidence would resolve it: Controlled ablations comparing models with/without dedicated sink tokens, measuring both task performance and rank collapse rates across layers.

### Open Question 3
- Question: Would making the differential transformer's subtractive attention map a function of prior rows better address accumulated position bias than the current row-only formulation?
- Basis in paper: [explicit] "Differential transformer may be better positioned to specifically reduce over-squashing if the subtractive attention map becomes a function of the previous rows instead of solely the current row."
- Why unresolved: Current differential attention only de-noises within-row; accumulated noise from position bias propagates through layers unaddressed.
- What evidence would resolve it: Implementation of position-aware differential attention with experiments on long-context retrieval tasks measuring last-token representation fidelity.

## Limitations
- The analysis is primarily theoretical and survey-based, with no original empirical validation presented
- Key mechanisms like attention sink emergence and last-token representation collapse are grounded in existing literature but not directly tested within this paper
- The analysis assumes standard transformer implementations without exploring how architectural variations might alter the causal graph topology and propagation dynamics

## Confidence
- **High confidence**: The position-biased information flow mechanism and its connection to causal masking structure
- **Medium confidence**: The attention sink mechanism as a structural consequence of causal geometry
- **Medium confidence**: The over-squashing characterization for last tokens

## Next Checks
1. **Attention Sink Ablation Study**: Replace the first token with a dedicated learned sink token at various positions and measure attention pattern redistribution and generation quality.
2. **Pause Token Optimization**: Implement pause tokens both prepended and appended to sequences (not just appended as current practice). Measure last-token representation sharpness using nearest-neighbor distances in embedding space across sequence lengths.
3. **Effective Resistance Profiling**: Compute effective resistance or commute time between token positions in a frozen trained decoder-only transformer. Early tokens should show lower effective resistance than later tokens, providing quantitative support for the position-bias mechanism.