---
ver: rpa2
title: 'Introducing TrGLUE and SentiTurca: A Comprehensive Benchmark for Turkish General
  Language Understanding and Sentiment Analysis'
arxiv_id: '2512.22100'
source_url: https://arxiv.org/abs/2512.22100
tags:
- turkish
- dataset
- https
- hate
- sentence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TrGLUE and SentiTurca, the first comprehensive
  benchmarks for Turkish natural language understanding and sentiment analysis. TrGLUE
  covers tasks like sentence classification and inference, using native Turkish corpora
  and a semi-automated labeling pipeline combining LLM and human annotations.
---

# Introducing TrGLUE and SentiTurca: A Comprehensive Benchmark for Turkish General Language Understanding and Sentiment Analysis

## Quick Facts
- arXiv ID: 2512.22100
- Source URL: https://arxiv.org/abs/2512.22100
- Reference count: 40
- Key outcome: First comprehensive benchmarks for Turkish NLU and sentiment analysis

## Executive Summary
This paper introduces TrGLUE and SentiTurca, the first comprehensive benchmarks for Turkish natural language understanding and sentiment analysis. TrGLUE covers tasks like sentence classification and inference using native Turkish corpora with a semi-automated labeling pipeline combining LLM and human annotations. SentiTurca focuses on sentiment analysis, including a large hate speech dataset. Evaluations on BERTurk and LLMs demonstrate that TrGLUE enables robust NLU assessment in Turkish, while sentiment tasks reveal ongoing challenges for LLMs.

## Method Summary
The paper develops two benchmark suites: TrGLUE for general language understanding tasks and SentiTurca for sentiment analysis. TrGLUE utilizes native Turkish corpora and employs a semi-automated labeling pipeline that combines LLM-generated annotations with human verification. SentiTurca includes various sentiment tasks with particular emphasis on hate speech detection. The benchmarks are evaluated using BERTurk and various large language models to establish baseline performance metrics.

## Key Results
- TrGLUE and SentiTurca represent the first comprehensive benchmarks for Turkish NLU and sentiment analysis
- BERTurk and LLMs show varying performance across different tasks, demonstrating benchmark utility
- Sentiment tasks, particularly hate speech detection, present ongoing challenges for current LLMs

## Why This Works (Mechanism)
The benchmarks work by providing standardized evaluation frameworks using native Turkish language resources. The semi-automated labeling pipeline ensures high-quality annotations while maintaining scalability. By covering diverse task types from classification to inference, the benchmarks capture multiple dimensions of language understanding. The inclusion of sentiment analysis with specialized datasets like hate speech detection addresses critical real-world applications.

## Foundational Learning
- **Turkish NLP landscape**: Understanding the current state of Turkish language processing is essential to appreciate the benchmark's significance
  - *Why needed*: Provides context for why these benchmarks are necessary and novel
  - *Quick check*: Review existing Turkish NLP resources and identify gaps

- **Benchmark development methodology**: Knowledge of how to create reliable, standardized evaluation frameworks
  - *Why needed*: Ensures the benchmarks are scientifically valid and reproducible
  - *Quick check*: Verify the methodology follows established benchmark creation principles

- **Semi-automated annotation pipelines**: Understanding the balance between automation and human oversight
  - *Why needed*: Critical for maintaining annotation quality while scaling to large datasets
  - *Quick check*: Examine inter-annotator agreement and consistency metrics

## Architecture Onboarding

**Component Map**: Native Turkish corpora -> Semi-automated labeling pipeline (LLM + human) -> Task-specific datasets -> Benchmark suites (TrGLUE, SentiTurca) -> Model evaluation

**Critical Path**: Corpus collection → Annotation pipeline → Dataset validation → Benchmark compilation → Model evaluation

**Design Tradeoffs**: Automation vs. accuracy in annotation, comprehensiveness vs. manageability of task coverage, native data vs. translated resources

**Failure Signatures**: Inconsistent annotations across tasks, poor model generalization on sentiment tasks, bias in hate speech detection

**First Experiments**: 1) Evaluate baseline models on individual task types, 2) Compare human vs. LLM-only annotations, 3) Test cross-dataset generalization

## Open Questions the Paper Calls Out
None

## Limitations
- The semi-automated labeling pipeline's validation process lacks detailed consistency metrics
- Specific selection criteria for native Turkish corpora are not clearly established
- Limited error analysis prevents understanding of model failure patterns across tasks

## Confidence
- High confidence: TrGLUE and SentiTurca are the first comprehensive Turkish NLU benchmarks
- Medium confidence: Benchmarks enable robust NLU assessment in Turkish
- Low confidence: Claims about LLMs' ongoing challenges without detailed performance breakdowns

## Next Checks
1. Conduct inter-annotator agreement studies to quantify the reliability of the semi-automated labeling pipeline and assess consistency across different annotators and tasks.

2. Perform comprehensive error analysis on model predictions across all TrGLUE and SentiTurca tasks to identify systematic failure patterns and areas requiring improvement.

3. Evaluate the benchmarks on additional Turkish language models beyond BERTurk and LLMs, including smaller fine-tuned models, to establish a broader performance baseline and identify model-specific strengths and weaknesses.