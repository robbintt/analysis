---
ver: rpa2
title: 'MVL-SIB: A Massively Multilingual Vision-Language Benchmark for Cross-Modal
  Topical Matching'
arxiv_id: '2502.12852'
source_url: https://arxiv.org/abs/2502.12852
tags:
- languages
- images
- language
- lvlms
- topic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces MVL-SIB, a massively multilingual vision-language\
  \ benchmark that evaluates cross-modal topical matching across 205 languages\u2014\
  over 100 more than existing VL benchmarks. The authors hand-select images to represent\
  \ seven topics and pair them with professionally translated sentences from SIB-200\
  \ to create parallel VL evaluation data."
---

# MVL-SIB: A Massively Multilingual Vision-Language Benchmark for Cross-Modal Topical Matching

## Quick Facts
- arXiv ID: 2502.12852
- Source URL: https://arxiv.org/abs/2502.12852
- Authors: Fabian David Schmidt; Florian Schneider; Chris Biemann; Goran Glavaš
- Reference count: 40
- Key outcome: Introduces MVL-SIB, a massively multilingual VL benchmark covering 205 languages that exposes severe VL performance gaps in low-resource languages.

## Executive Summary
This paper introduces MVL-SIB, a massively multilingual vision-language benchmark that evaluates cross-modal topical matching across 205 languages—over 100 more than existing VL benchmarks. The authors hand-select images to represent seven topics and pair them with professionally translated sentences from SIB-200 to create parallel VL evaluation data. They benchmark open-weight LVLMs and GPT-4o-mini on both cross-modal and text-only topic matching tasks. Results show that LVLMs struggle with cross-modal topic matching in lower-resource languages, performing near chance level on languages like N'Koo, and that VL support declines disproportionately relative to textual support for these languages. Open-weight LVLMs do not benefit from multiple reference images, indicating limited effectiveness in multi-image tasks. The study also correlates MVL-SIB performance with other multilingual VL benchmarks, confirming its reliability as a comprehensive probe of multilingual VL understanding.

## Method Summary
MVL-SIB is built on SIB-200 (1,004 sentences from Flores DEV/DEVTEST, professionally translated to 205 languages) across 7 topics. Each topic has 10 hand-selected permissibly licensed images. The benchmark evaluates two directions: Images-to-Sentence (I2S) and Sentences-to-Image (S2I), both as 4-way multiple-choice tasks with k reference images per instance. Text-only variants (T2S, S2T) replace images with topic labels for ablation. Evaluation uses accuracy—share of responses starting with correct option letter—with images downsampled to 640×480 and greedy decoding (temperature=0.0). Open-weight LVLMs (Qwen2-VL, InternVL 2.5) and GPT-4o-mini are benchmarked across all 205 languages, with mSigLIP-base serving as a text-image similarity baseline.

## Key Results
- LVLMs show severe performance degradation on cross-modal topic matching in low-resource languages, with N'Koo performance near chance level.
- VL support in LVLMs declines disproportionately relative to textual support for lower-resource languages.
- Open-weight LVLMs do not benefit from representing a topic with more than one image, suggesting limitations in handling multi-image tasks.

## Why This Works (Mechanism)
The benchmark exposes limitations in current multilingual VL models by creating a controlled evaluation environment where cross-modal matching must be performed across a wide range of languages. The use of professionally translated sentences ensures linguistic quality while the hand-selected images provide consistent visual grounding. The 4-way multiple-choice format with varying numbers of reference images (k=1,3,5) allows for systematic probing of model capabilities across different levels of complexity.

## Foundational Learning
- Multilingual text encoding: Needed to process 205 languages; quick check: evaluate text-only task accuracy across languages to establish baseline linguistic support.
- Vision-language alignment: Critical for matching images to text; quick check: compare mSigLIP-base cosine similarity baseline against LVLM performance.
- Multi-image aggregation: Ability to combine information from multiple reference images; quick check: measure accuracy improvement from k=1 to k=5.
- Cross-modal topical matching: Core task of linking visual and textual topic representations; quick check: evaluate both I2S and S2I directions for consistency.

## Architecture Onboarding

Component map: Image Encoder -> Vision Encoder -> Cross-modal Projector -> Language Model -> Output Decoder

Critical path: Image downsampling (640×480) → Image encoding → Text encoding → Cross-modal similarity scoring → Multiple-choice selection

Design tradeoffs: Fixed image size vs. resolution quality; greedy decoding vs. sampling strategies; topic labels vs. full sentences for text-only ablation.

Failure signatures: Near-chance performance on low-resource languages indicates either insufficient language support or cross-modal alignment failures; lack of multi-image benefit suggests architectural limitations in information aggregation.

First experiments:
1. Verify image collection and downsampling procedure by evaluating a subset of languages with independently collected images
2. Compare text-only (T2S/S2T) performance using full sentences versus topic labels to assess ablation validity
3. Test alternative decoding strategies (beam search, higher temperature) to determine if greedy decoding underestimates LVLM potential

## Open Questions the Paper Calls Out
- What training methodologies or architectural modifications are required to enable open-weight LVLMs to effectively aggregate information from multiple reference images?
- Does the use of culturally specific images, rather than topic-generic images, significantly impact cross-modal topic matching performance for low-resource languages?
- Is the disproportionate degradation of vision-language support relative to text-only support in low-resource languages primarily caused by the vision encoder or the cross-modal alignment layers?

## Limitations
- Exact image URLs and specific curated images used are not released, preventing exact reproduction.
- Random negative sampling procedure for distractors is underspecified beyond "from different topics."
- Text-only ablation tasks use topic labels instead of full sentences, creating non-identical comparison conditions.

## Confidence
High confidence: Core finding of severe VL performance degradation in low-resource languages is well-supported and consistent with existing literature.
Medium confidence: Claim about disproportionate VL support decline relative to textual support could benefit from more granular language-pair analysis.
Low confidence: Observation about open-weight LVLMs not benefiting from multiple reference images lacks exploration of underlying mechanisms.

## Next Checks
1. Independently collect 10 permissibly licensed images per topic, downsample to 640×480, and evaluate on a subset of 20-30 languages to confirm reported accuracy patterns.
2. Run text-only T2S/S2T variants with full sentences (not just topic labels) on the same LVLMs to establish the true gap between text-only and cross-modal tasks.
3. Systematically test whether alternative prompt templates or decoding strategies (e.g., beam search, higher temperature) enable open-weight LVLMs to leverage multiple reference images effectively for k=3 and k=5.