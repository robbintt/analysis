---
ver: rpa2
title: Federated Deep Reinforcement Learning for Energy Efficient Multi-Functional
  RIS-Assisted Low-Earth Orbit Networks
arxiv_id: '2501.11079'
source_url: https://arxiv.org/abs/2501.11079
tags:
- mf-ris
- energy
- where
- satellite
- power
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a federated deep reinforcement learning (DRL)
  approach to optimize energy efficiency (EE) in low-Earth orbit (LEO) satellite networks
  equipped with multi-functional reconfigurable intelligent surfaces (MF-RIS). The
  proposed FEMAD scheme combines multi-agent deep deterministic policy gradient (MADDPG)
  with federated learning to dynamically optimize MF-RIS configurations including
  amplification, phase-shifts, energy harvesting ratios, and LEO transmit beamforming.
---

# Federated Deep Reinforcement Learning for Energy Efficient Multi-Functional RIS-Assisted Low-Earth Orbit Networks
## Quick Facts
- arXiv ID: 2501.11079
- Source URL: https://arxiv.org/abs/2501.11079
- Reference count: 19
- The proposed FEMAD scheme achieves up to 10% higher EE compared to centralized DRL and distributed MADDPG baselines.

## Executive Summary
This paper proposes FEMAD, a federated deep reinforcement learning approach to optimize energy efficiency in LEO satellite networks equipped with multi-functional reconfigurable intelligent surfaces (MF-RIS). The scheme combines MADDPG with federated learning to dynamically optimize MF-RIS configurations and LEO transmit beamforming. Results show superior performance compared to centralized and distributed baselines, with optimal performance achieved using 2 LEO satellite groups and 36 active MF-RIS elements.

## Method Summary
The FEMAD framework addresses energy efficiency optimization in LEO-MF-RIS networks through federated MADDPG. The method jointly optimizes MF-RIS amplification, phase-shifts, energy harvesting ratios, and LEO transmit beamforming. Each LEO satellite acts as an agent with its own actor-critic networks, trained using federated averaging of target critic weights. The state space consists of instantaneous channel information, actions include MF-RIS configurations and beamforming vectors, and rewards are based on penalized energy efficiency accounting for rate, power, and battery constraints.

## Key Results
- FEMAD achieves up to 10% higher energy efficiency compared to centralized DRL and distributed MADDPG baselines
- Optimal performance achieved with 2 LEO satellites grouped for federated learning
- Maximum energy efficiency observed with 36 MF-RIS elements activated
- Superior EE compared to scenarios with fixed/no energy harvesting, conventional reflection-only RIS, and deployment without RIS/MF-RIS

## Why This Works (Mechanism)
The approach works by enabling distributed optimization while maintaining global coordination through federated learning. MADDPG handles the continuous action space for MF-RIS configurations and beamforming, while federated averaging of critic weights allows knowledge sharing across LEO satellites without sharing raw data. This architecture balances the need for local adaptation to channel conditions with global coordination for energy efficiency optimization.

## Foundational Learning
- **Rician fading channel model**: Needed to capture both line-of-sight and scattered components in LEO-to-ground links; Quick check: Verify Îº parameter values match typical LEO propagation scenarios.
- **Multi-agent reinforcement learning**: Required for distributed optimization across multiple LEO satellites; Quick check: Confirm each agent's state and action spaces are properly defined.
- **Federated learning aggregation**: Enables knowledge sharing without raw data transmission; Quick check: Validate weight averaging doesn't introduce instability.
- **Nonlinear energy harvesting**: Captures realistic RF-to-DC conversion efficiency; Quick check: Ensure EH model parameters match hardware specifications.
- **Constraint-aware reward shaping**: Maintains feasible solutions while optimizing EE; Quick check: Verify penalty weights effectively enforce constraints.

## Architecture Onboarding
- **Component map**: Users -> LEO satellites (MADDPG agents) -> MF-RIS elements -> Ground stations
- **Critical path**: Channel estimation -> State formation -> Policy execution -> Reward calculation -> Federated aggregation
- **Design tradeoffs**: Centralized vs distributed learning (latency vs privacy), full vs partial model sharing (performance vs communication overhead)
- **Failure signatures**: Training instability (high reward variance), constraint violations (negative penalties), federated divergence (oscillating critic weights)
- **First experiments**: 1) Single LEO satellite baseline without federated learning, 2) Vary group size from 1 to 8 LEOs, 3) Compare EE with different MF-RIS activation patterns

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How does dynamic, orbital-aware clustering of LEO satellites for federated learning aggregation impact energy efficiency compared to static grouping?
- Basis: The paper finds 2 LEO groups optimal but uses fixed groupings despite orbital movement
- Why unresolved: Satellite movement may cause fixed groups to diverge in channel conditions
- Evidence needed: Comparison with dynamic clustering algorithm based on inter-satellite distances

### Open Question 2
- Question: What is the trade-off between model parameters exchanged and convergence speed in communication-efficient federated learning?
- Basis: Paper mentions partial model exchange but doesn't quantify impact
- Why unresolved: Unclear if communication efficiency significantly affects performance
- Evidence needed: Ablation studies varying percentage of shared model weights

### Open Question 3
- Question: How robust is FEMAD to CSI estimation errors and signaling delays in high-mobility LEO scenarios?
- Basis: Perfect CSI assumed but high mobility introduces errors/delays
- Why unresolved: Deep RL policies can be brittle to observation noise
- Evidence needed: Performance evaluation under stochastic noise and time delays

## Limitations
- Critical implementation details missing including network architecture and penalty weights
- Training hyperparameters and convergence criteria not fully specified
- Channel generation methodology for orbital dynamics unclear
- Limited exploration of dynamic clustering strategies for federated learning

## Confidence
- **High Confidence**: Overall system architecture and optimization framework
- **Medium Confidence**: General trend findings (2 LEO groups optimal)
- **Low Confidence**: Exact numerical results and performance comparisons due to unspecified implementation details

## Next Checks
1. Verify actor-critic network architecture independently matches MADDPG requirements for continuous control tasks
2. Test sensitivity of EE performance to penalty weight variations in the reward function
3. Validate federated averaging stability with different LEO grouping strategies beyond the reported optimal of 2 groups