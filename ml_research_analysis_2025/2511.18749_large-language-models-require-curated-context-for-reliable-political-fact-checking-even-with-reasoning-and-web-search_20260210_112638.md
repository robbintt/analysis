---
ver: rpa2
title: Large Language Models Require Curated Context for Reliable Political Fact-Checking
  -- Even with Reasoning and Web Search
arxiv_id: '2511.18749'
source_url: https://arxiv.org/abs/2511.18749
tags:
- search
- fact-checking
- gemini
- politifact
- gpt-4o
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study evaluated 15 large language models on political fact-checking
  using 6,000+ claims from PolitiFact, comparing standard models with reasoning and
  web search variants. Standard models performed poorly, with reasoning offering minimal
  improvements and web search providing only moderate gains.
---

# Large Language Models Require Curated Context for Reliable Political Fact-Checking -- Even with Reasoning and Web Search

## Quick Facts
- arXiv ID: 2511.18749
- Source URL: https://arxiv.org/abs/2511.18749
- Authors: Matthew R. DeVerna; Kai-Cheng Yang; Harry Yaojun Yan; Filippo Menczer
- Reference count: 40
- Primary result: Curated RAG using PolitiFact summaries improves macro F1 by 233% on average across model variants

## Executive Summary
This study evaluates 15 large language models on political fact-checking using 6,000+ claims from PolitiFact, comparing standard models with reasoning and web search variants. Standard models performed poorly, with reasoning offering minimal improvements and web search providing only moderate gains. In contrast, a curated RAG system using PolitiFact summaries improved macro F1 scores by 233% on average. Web search-enabled models frequently cited sources with liberal bias, raising concerns about ideological balance. These findings suggest that providing models with curated, high-quality context is a more effective approach than relying on internal knowledge or general web search for reliable automated fact-checking.

## Method Summary
The researchers built a PolitiFact-specific fact-checking database by crawling 24,611 claims and verdicts (2007–Oct 2024), summarizing each article with GPT-3.5-turbo-0125, and storing sentence embeddings in Chroma DB using all-MiniLM-L6-v2. They evaluated 15 LLMs across three conditions: zero-shot classification, Curated RAG (retrieving k∈{3,6,9} pre-summarized articles), and web search variants. Models were tested on stratified samples of 12,275 claims (standard models) and 6,137 claims (reasoning/search models), measuring macro precision, recall, and F1 across six Truth-O-Meter labels plus "Not Enough Information."

## Key Results
- Standard LLMs achieved only 0.15 macro F1, performing barely better than random chance
- Curated RAG improved macro F1 by 233% on average, with k=3 summaries performing nearly as well as k=9
- Web search enabled modest gains (+0.50 macro F1 for GPT-4o) but Gemini models failed to benefit due to citation differences
- Reasoning models showed minimal improvements (+0.06 on average) and sometimes declined in performance

## Why This Works (Mechanism)

### Mechanism 1
Providing LLMs with curated, claim-specific evidence enables accurate fact-checking classification even for models that perform poorly on their own. A retrieval system (Curated RAG) fetches k pre-summarized fact-checking articles via semantic similarity, supplies them as context in the prompt, and the model maps evidence to veracity labels. The retrieval step is highly accurate (top-3 hit rate: 0.96; median retrieval rank: 1), ensuring relevant evidence is present. Core assumption: Models can reason over evidence when it is available; the bottleneck is information access, not inference capacity.

### Mechanism 2
Reasoning-enhanced models provide minimal accuracy gains for political fact-checking in zero-shot settings. Reasoning models (e.g., o1, DeepSeek-R1) apply additional compute to chain-of-thought inference, but without relevant evidence, extended reasoning does not compensate for missing or incorrect internal knowledge. Core assumption: Fact-checking accuracy is primarily evidence-limited; additional inference steps cannot substitute for ground-truth information.

### Mechanism 3
Web search functionality improves fact-checking only when the search pipeline successfully retrieves and integrates relevant sources. Search-enabled models query the live web, retrieve documents, and synthesize answers. Performance depends on query formulation, source prioritization, and citation integration. GPT models frequently cite PolitiFact directly (44-59% exact-match citations); Gemini models rarely cite URLs at all, explaining their failure to benefit from search. Core assumption: Search utility is implementation-dependent; "search capability" is not uniform across providers.

## Foundational Learning

- **Retrieval-Augmented Generation (RAG)**: Why needed here: The entire improvement mechanism hinges on injecting relevant context into prompts before generation. Quick check question: Can you explain why retrieving k=3 summaries yields nearly the same performance as k=9?

- **Vector Embeddings and Semantic Similarity**: Why needed here: The system uses sentence embeddings (all-MiniLM-L6-v2) and cosine distance to match claims to summaries. Quick check question: What would happen to retrieval accuracy if you used keyword matching instead of embeddings?

- **Multi-class Classification Metrics (Macro F1)**: Why needed here: PolitiFact uses six labels; macro F1 weights each class equally, exposing performance on rare labels like "Pants on Fire." Quick check question: Why might weighted F1 hide poor performance on certain classes?

## Architecture Onboarding

- Component map: PolitiFact crawler -> claim/verdict/article extraction -> summary generation (GPT-3.5) -> faithfulness validation -> Chroma vector database (all-MiniLM-L6-v2 embeddings) -> Query with claim text -> top-k similarity search -> return k summaries -> inject into prompt -> LLM classification -> structured JSON response -> parse label -> compute metrics

- Critical path: Claim input -> embedding query -> retrieve k summaries -> inject into prompt -> LLM classification -> parse response -> compute metrics

- Design tradeoffs:
  - k=3 vs. k=9: Minimal performance difference (SD = 0.002-0.037), so lower k reduces token cost with little accuracy loss
  - Curated RAG vs. open web search: Curated guarantees relevance and source quality; web search offers broader coverage but risks bias and irrelevance
  - "Not Enough Information" label: Enables abstention but complicates evaluation; models rarely use it appropriately

- Failure signatures:
  - Retrieval miss: Matching summary not in top-k (occurs in ~2-4% of claims)
  - Malformed JSON: 0.44% of responses require secondary parsing (Llama 3B highest at 5.5%)
  - NEI misuse: Models rarely abstain when they should; precision/recall on NEI task are uniformly low

- First 3 experiments:
  1. Retrieval ablation: Test performance with k=0 (no context) vs. k=3 vs. k=6 vs. k=9 to confirm context dependency on a held-out claim set
  2. Summary faithfulness audit: Manually verify a random sample of generated summaries against source articles; flag hallucination rate
  3. Cross-domain retrieval test: Query the database with claims NOT in PolitiFact to measure false-positive retrieval and observe how often models incorrectly apply NEI vs. hallucinate labels

## Open Questions the Paper Calls Out

### Open Question 1
How do LLMs perform on fact-checking breaking news claims where curated fact-checks do not yet exist? Basis in paper: Authors state they "do not address the 'breaking news problem'" and that "future work should evaluate how models perform in breaking news contexts where information is incomplete and/or rapidly evolving." Why unresolved: All tested claims had pre-existing fact-checks; live web search was not tested on genuinely novel claims. What evidence would resolve it: Evaluate web-search-enabled models on claims from emerging events within hours of occurrence, before any professional fact-check exists.

### Open Question 2
Is the liberal bias in web-search model citations caused by model bias, search-pipeline design, or structural features of the information ecosystem? Basis in paper: Authors ask whether the left-leaning citation pattern "reflects model or search-pipeline bias, or structural features of the information ecosystem (e.g., a correlation between accuracy and political leaning)." Why unresolved: The study documents the bias pattern but cannot disentangle its root cause from observational data. What evidence would resolve it: Controlled experiments varying search indices, query formulations, and model providers while measuring citation distributions.

### Open Question 3
How can curated fact-checking databases be built and maintained at scale across diverse domains with continuous updates? Basis in paper: Authors note that "building such pipelines at scale... is non-trivial: they must cover diverse domains, update continuously, and manage conflicting or incomplete evidence." Why unresolved: The study used a static PolitiFact archive; practical deployment requires dynamic, multi-source systems. What evidence would resolve it: Prototype systems evaluated on coverage, freshness, and conflict resolution across multiple fact-checking sources over time.

### Open Question 4
How do AI-generated fact-checks affect downstream user beliefs, trust in institutions, and sharing behavior? Basis in paper: Authors note they "do not measure downstream effects on beliefs, trust, or sharing behavior, which are critical for understanding societal impact." Why unresolved: The study evaluates prediction accuracy only, not how outputs influence human decision-making. What evidence would resolve it: User studies measuring belief changes, trust levels, and sharing intentions after exposure to AI fact-checks of varying accuracy.

## Limitations

- Retrieval accuracy depends on PolitiFact's internal cross-linking structure, which may not exist for other fact-checking domains
- Web search utility varies dramatically by provider implementation, with Gemini's grounding mechanism fundamentally different from GPT's search capability
- Summary generation process validated for faithfulness but performed once without iterative refinement, and specific system prompt not provided

## Confidence

- High confidence: Curated RAG provides substantial performance gains over standard models (233% macro F1 improvement)
- Medium confidence: Reasoning models offer minimal benefits for fact-checking
- Medium confidence: Web search utility depends heavily on provider implementation

## Next Checks

1. Cross-domain retrieval validation: Test the PolitiFact-curated RAG system on claims from Snopes or FactCheck.org to determine whether the retrieval architecture generalizes beyond PolitiFact's internal linking structure

2. Provider search mechanism analysis: Systematically compare search outputs from different providers on identical queries to characterize the differences between GPT's search and Gemini's grounding approaches

3. Summary quality audit: Conduct a comprehensive manual review of 100 randomly selected summaries against their source articles to establish the true hallucination rate and identify systematic biases in the summarization process