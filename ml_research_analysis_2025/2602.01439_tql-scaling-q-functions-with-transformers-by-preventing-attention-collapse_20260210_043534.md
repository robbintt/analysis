---
ver: rpa2
title: 'TQL: Scaling Q-Functions with Transformers by Preventing Attention Collapse'
arxiv_id: '2602.01439'
source_url: https://arxiv.org/abs/2602.01439
tags:
- step
- attention
- value
- entropy
- scaling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Transformer Q-Learning (TQL) addresses the problem of scaling value
  functions in reinforcement learning, where naively increasing model size leads to
  training instability and performance degradation. The core issue identified is attention
  collapse - as transformer capacity increases, attention distributions become increasingly
  concentrated on a few tokens rather than utilizing the full input space.
---

# TQL: Scaling Q-Functions with Transformers by Preventing Attention Collapse

## Quick Facts
- **arXiv ID:** 2602.01439
- **Source URL:** https://arxiv.org/abs/2602.01439
- **Reference count:** 40
- **Primary result:** TQL achieves up to 43% improvement in performance when scaling from smallest to largest model sizes, while prior methods suffer from performance degradation

## Executive Summary
TQL addresses the problem of scaling value functions in reinforcement learning, where naively increasing transformer capacity leads to training instability and performance degradation. The core issue identified is attention collapse - as transformer capacity increases, attention distributions become increasingly concentrated on a few tokens rather than utilizing the full input space. TQL prevents this collapse by directly controlling attention entropy through layer-wise temperature parameters that maintain a target entropy level, ensuring attention is distributed across all input tokens. The method also incorporates learnable modality embeddings to help larger models better attend to task-relevant patterns.

## Method Summary
TQL prevents attention collapse in transformer Q-functions by controlling the entropy of attention scores through layer-wise temperature parameters. The method learns adaptive temperatures per layer to maintain entropy near a target value, forcing the model to distribute attention across all input tokens rather than over-specializing. It also uses separate temperatures for the [VALUE] token that aggregates information from the sequence. The approach combines a Q-learning objective with entropy maximization loss and temperature adjustment loss, enabling stable scaling of transformer-based value functions without the performance degradation seen in vanilla approaches.

## Key Results
- TQL achieves up to 43% improvement in performance when scaling from smallest to largest model sizes
- On OGBench benchmark, TQL achieves state-of-the-art performance across 25 challenging tasks
- Outperforms existing offline RL methods including those using flow-matching and transformer-based value functions

## Why This Works (Mechanism)

### Mechanism 1: Attention Entropy Collapse Prevention
- **Claim:** Controlling attention entropy toward a target value prevents the collapse of attention distributions that otherwise occurs when scaling transformer Q-functions.
- **Mechanism:** As model capacity increases, attention scores become increasingly peaked on a few tokens (entropy drops toward zero). TQL adds a loss term maximizing attention entropy while learning adaptive temperature parameters α per layer to maintain entropy near a target Ĥ. This forces the model to distribute attention across all input tokens rather than over-specializing.
- **Core assumption:** Attention collapse is causally responsible for performance degradation, not merely correlated with it.
- **Evidence anchors:**
  - [abstract]: "attention scores collapse as capacity increases. Our key insight is that we can effectively prevent this collapse and stabilize training by controlling the entropy of the attention scores"
  - [Section 5.2]: "attention entropy decreases substantially with model size... This entropy collapse correlates strongly with both the non-smooth value landscapes and diminished task performance"
  - [corpus]: Related work "Critical attention scaling in long-context transformers" documents similar attention collapse phenomena in other domains.

### Mechanism 2: Layer-wise and Token-wise Temperature Adaptation
- **Claim:** Separate learnable temperatures per layer and for the [VALUE] token enable more stable training than a single global temperature.
- **Mechanism:** Different transformer layers learn different attention patterns appropriate for value function learning. The [VALUE] token has distinct entropy requirements since it aggregates information from all other tokens. Per-layer α parameters allow each layer to independently maintain appropriate entropy levels.
- **Core assumption:** Different layers require different entropy targets for optimal value learning.
- **Evidence anchors:**
  - [Section 4.2]: "we use layer-wise temperature parameters αℓ rather than a single global parameter... [VALUE] token can contain different attention entropy patterns, as it is responsible for aggregating information"
  - [Figure 5]: Ablation shows single temperature achieves 41% vs. TQL's 67% success rate.

### Mechanism 3: Target Entropy Guidance vs. Maximum Entropy
- **Claim:** Guiding entropy toward a specific target value outperforms simply maximizing entropy.
- **Mechanism:** Maximum entropy loss without target guidance can destabilize training by pushing entropy too high. TQL's dual-objective (maximize entropy with temperature-adjusted penalty toward target) provides stability while preventing collapse.
- **Core assumption:** There exists an optimal entropy range for value learning—not too collapsed, not too uniform.
- **Evidence anchors:**
  - [Section 4.2]: "By adjusting the entropy of attention scores toward a target value, TQL ensures that the model effectively distributes attention"
  - [Figure 5]: Fixed entropy penalty achieves 38% vs. TQL's 67% on cube-double tasks.

## Foundational Learning

- **Concept: Q-learning with bootstrapped value targets (TD learning)**
  - Why needed here: TQL builds on standard Q-learning objective where Q-values are trained via temporal difference targets using delayed target networks. Understanding bootstrapping is essential to grasp why attention collapse is particularly problematic for value learning.
  - Quick check question: Can you explain why bootstrapped value targets create different optimization dynamics than supervised learning targets?

- **Concept: Attention entropy in transformers**
  - Why needed here: The core contribution is entropy-based regularization. You need to understand how softmax attention produces distributions and what entropy measures (uniformity vs. peakedness).
  - Quick check question: For a 10-token sequence, what is the maximum possible attention entropy and what attention pattern achieves it?

- **Concept: Offline RL distribution shift problem**
  - Why needed here: The paper evaluates on offline RL where policies must be learned from fixed datasets. Understanding why out-of-distribution actions are problematic contextualizes why stable Q-function learning matters.
  - Quick check question: Why does offline RL require special handling compared to online RL with environment interaction?

## Architecture Onboarding

- **Component map:**
  - Input: State s (ns dimensions) + Action a (na dimensions) → each scalar becomes a token
  - Embeddings: Positional + Learnable modality embeddings (es for state, ea for action)
  - [VALUE] token: Prepended learnable token that aggregates sequence information
  - Transformer decoder: L layers of multi-head self-attention + MLP with pre-layer norm
  - Output heads: K ensemble MLP heads on [VALUE] token representation → Q(s,a) ∈ R^K
  - Entropy control: Per-layer αℓ temperatures + separate α[VALUE] for aggregation token

- **Critical path:**
  1. Tokenize state/action → add embeddings → prepend [VALUE] token
  2. Forward through transformer layers, computing attention entropy at each layer
  3. Extract [VALUE] representation → MLP heads → Q-value predictions
  4. Compute losses: L_Q (TD error) + L_attn (entropy maximization) + L_temp (temperature adjustment)
  5. Policy extraction via behavior-constrained flow policy (separate network)

- **Design tradeoffs:**
  - Target entropy Ĥ requires per-environment tuning (paper uses 0.8 × ln(1 + ns + na) as starting point)
  - Larger models require lower learning rates (1e-4 for critic vs. 5e-4 for actor)
  - Deeper networks (4 layers) show same collapse patterns as wider networks—TQL helps both

- **Failure signatures:**
  - Attention maps showing 1-2 tokens receiving >90% of attention weight
  - Highly non-smooth Q-value landscapes with oscillations and discontinuities
  - Success rate dropping with increased model size (e.g., 46% → 6% in Figure 3)

- **First 3 experiments:**
  1. **Replicate attention collapse:** Train vanilla transformer Q-function on a single cube-double task at 0.4M vs. 26M parameters. Verify entropy decreases and performance degrades at larger scale.
  2. **Entropy-entropy ablation:** Compare TQL with full entropy guidance vs. fixed max-entropy penalty vs. no entropy loss on the same task to reproduce the 67% vs. 38% vs. 6% pattern.
  3. **Cross-environment entropy transfer:** Test whether target entropy values tuned on one environment transfer to another with similar ns + na, or whether per-environment tuning is strictly required.

## Open Questions the Paper Calls Out
None

## Limitations
- The empirical evidence for the core mechanism relies heavily on correlation rather than controlled intervention studies
- The target entropy hyperparameter requires per-environment tuning with a heuristic that itself needs empirical calibration
- The offline RL setting limits generality, with online RL performance unexplored

## Confidence
- **High confidence:** The empirical demonstration that transformer Q-functions suffer from attention collapse when scaling, and that TQL prevents this collapse through entropy control
- **Medium confidence:** The causal mechanism linking attention collapse to training instability and performance degradation
- **Medium confidence:** The claim of state-of-the-art performance on OGBench

## Next Checks
1. **Causal intervention study:** Design an experiment where attention entropy is artificially constrained in a vanilla transformer while keeping capacity constant. Measure whether this alone improves performance, or whether TQL's additional components are necessary.
2. **Cross-task entropy transfer validation:** Systematically test whether target entropy values tuned on one environment transfer to others with similar state/action dimensions.
3. **Online RL performance validation:** Evaluate TQL in online reinforcement learning settings with active exploration, comparing against both offline and online baselines.