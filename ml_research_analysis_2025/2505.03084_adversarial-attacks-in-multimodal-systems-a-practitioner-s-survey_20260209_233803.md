---
ver: rpa2
title: 'Adversarial Attacks in Multimodal Systems: A Practitioner''s Survey'
arxiv_id: '2505.03084'
source_url: https://arxiv.org/abs/2505.03084
tags:
- attacks
- attack
- adversarial
- text
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper surveys adversarial attacks targeting multimodal AI
  systems that process text, images, video, and audio. It categorizes attacks by attacker
  knowledge (white-box vs black-box), intention (targeted vs untargeted), and execution
  method (optimization-based, data poisoning/backdoor, membership inference, and model
  inversion).
---

# Adversarial Attacks in Multimodal Systems: A Practitioner's Survey

## Quick Facts
- arXiv ID: 2505.03084
- Source URL: https://arxiv.org/abs/2505.03084
- Reference count: 40
- This paper surveys adversarial attacks targeting multimodal AI systems that process text, images, video, and audio

## Executive Summary
This survey systematically examines adversarial threats against multimodal AI systems that integrate text, image, video, and audio processing. The authors categorize attacks by attacker knowledge (white-box vs black-box), intention (targeted vs untargeted), and execution method (optimization-based, data poisoning/backdoor, membership inference, and model inversion). The paper highlights that while optimization-based attacks are most studied, membership inference and model inversion remain underexplored in multimodal contexts. A key finding is that cross-modal attacks exploit shared embedding spaces to compromise one modality by perturbing another, amplifying the overall threat landscape.

## Method Summary
The authors conducted a comprehensive literature review of 40+ academic papers to create a taxonomy of adversarial attacks against multimodal systems. They mapped existing vulnerabilities across four modalities and identified gaps in current research, particularly for video modality and certain attack types. The survey does not implement attacks end-to-end but synthesizes findings from cited papers to provide a practitioner-focused overview of attack mechanisms, detection challenges, and defense strategies.

## Key Results
- Optimization-based attacks manipulate model outputs by finding minimal input changes that maximize loss divergence
- Cross-modal attacks exploit shared embedding spaces to compromise one modality by perturbing another
- Backdoor attacks embed dormant triggers during training that activate attacker-specified behaviors at inference

## Why This Works (Mechanism)

### Mechanism 1
Optimization-based perturbations manipulate model outputs by finding minimal input changes that maximize loss divergence. Given input x, the attacker computes perturbation δ such that x' = x + δ causes incorrect output f(x'; θ). In targeted attacks, loss J(f(x'; θ), y_t) is minimized toward a specific label y_t; in untargeted attacks, loss J(f(x'; θ), y) is maximized from the true label y. The perturbation is constrained by distance norms (L0, L1, L2, L∞) to remain imperceptible.

### Mechanism 2
Cross-modal attacks exploit shared embedding spaces to compromise one modality by perturbing another. Multimodal models learn joint embeddings that align representations across text, image, audio, and video. An attacker perturbs a source modality (e.g., adding imperceptible noise to an image) to shift the joint embedding, causing the model to produce incorrect outputs in a target modality (e.g., harmful text generation). The attack leverages learned cross-modal correlations.

### Mechanism 3
Backdoor attacks embed dormant triggers during training that activate attacker-specified behaviors at inference. The attacker poisons training data by injecting samples with a trigger pattern (e.g., bright pixels, background noise, syntactic structure) and associates them with target labels. During training, the model learns to correlate the trigger with the target output. At inference, any input containing the trigger produces the attacker's desired output while clean inputs behave normally.

## Foundational Learning

- **White-box vs. Black-box Attack Settings**: Determines what defense strategies are viable. White-box assumes full model access (architecture, weights, gradients); black-box assumes only query access. *Quick check*: If you can query a deployed API but cannot access model weights, which attack setting applies?

- **Joint Embedding Spaces in Multimodal Models**: Cross-modal attacks target the shared representation layer where text, image, and audio are aligned. Understanding this architecture is prerequisite to diagnosing cross-modal vulnerabilities. *Quick check*: In CLIP-style models, where do text and image representations interact?

- **Perturbation Norms and Imperceptibility Constraints**: Optimization attacks balance effectiveness (causing misclassification) against detectability (human perception). L∞ bounds pixel-wise changes; L2 bounds overall distortion. *Quick check*: Why might an L∞-constrained perturbation be more detectable than an L2-constrained one for the same attack success rate?

## Architecture Onboarding

- **Component map**: Input processors (per modality) -> Modality encoders -> Fusion layer -> Task heads -> Output
- **Critical path**: Input → Modality encoder → Fusion layer → Task head → Output. Attacks typically target: (a) raw input before encoding, (b) fusion layer embeddings, or (c) training data pipeline
- **Design tradeoffs**: Open-source model accessibility vs. white-box vulnerability exposure; tight cross-modal alignment (better performance) vs. increased cross-modal attack transferability; defense preprocessing (robustness) vs. inference latency overhead
- **Failure signatures**: Sudden accuracy drops on clean inputs (possible backdoor trigger conflict); inconsistent cross-modal retrieval (embedding space manipulation); high-confidence misclassifications on slightly perturbed inputs (optimization attack success); model revealing training data verbatim (membership inference compromise)
- **First 3 experiments**:
  1. Implement TextAttack or Adversarial Robustness Toolbox against your text modality; measure success rate and perturbation magnitude thresholds
  2. Test cross-modal transfer: Apply image perturbations from section III.C to your multimodal model and measure impact on text outputs
  3. Audit training data pipeline: Scan for potential trigger patterns (repeated pixel regions, anomalous syntax structures) using statistical outlier detection

## Open Questions the Paper Calls Out

**Open Question 1**: Can effective Membership Inference and Model Inversion attacks be developed specifically for the video modality, or do the unique properties of video data prevent the transfer of image-based techniques? The authors note there is a lack of research on these attacks in video, despite the theoretical transferability of image attacks. The high density and temporal dimension of video data may make the computational cost of existing inference methods prohibitive or ineffective.

**Open Question 2**: What unified defense frameworks can effectively mitigate the interconnected risks of cross-modal adversarial attacks in multimodal systems? The paper concludes that current defense literature is fragmented and ad-hoc methods don't offer a holistic view of the multi-modal world. Defenses are currently siloed by modality or attack type, failing to address vulnerabilities that exploit joint embedding spaces.

**Open Question 3**: How can the gap between the rapid evolution of adversarial attack methods and the slower update cycles of open-source defensive tools be bridged to support practitioners? The authors note that tools like Adversarial Robustness Toolbox and TextAttack haven't been updated to keep up with new attacks, making it harder for practitioners to prepare for the threat landscape.

## Limitations

- The survey synthesizes findings from 40+ references without implementing attacks end-to-end, making empirical validation difficult
- Cross-modal attack mechanisms remain theoretical in many cases, with limited quantitative benchmarks demonstrating real-world effectiveness
- Defense strategies and tool recommendations primarily list existing tools without systematic evaluation of their effectiveness against multimodal attacks

## Confidence

- **High**: Optimization-based attack taxonomy and mathematical formulations - directly supported by cited papers with established literature
- **Medium**: Cross-modal attack descriptions - mechanism plausible but empirical validation across modalities would strengthen claims
- **Low**: Defense strategies and tool recommendations - primarily lists existing tools without systematic evaluation

## Next Checks

1. **Empirical Cross-Modal Transfer**: Implement a basic image-to-text attack (e.g., using PGD on image inputs) against a multimodal model like CLIP or LLaVA, then measure the impact on text generation outputs. Document success rate and perturbation magnitude thresholds.

2. **Backdoor Trigger Detection**: Apply statistical outlier detection to training datasets for multimodal models, searching for repeated patterns (bright pixel clusters, anomalous syntactic structures) that could indicate poisoned samples. Validate against known poisoned datasets if available.

3. **Defense Efficacy Benchmarking**: Select two defense tools from the survey (Adversarial Robustness Toolbox and TextAttack) and systematically test their effectiveness against optimization attacks on each modality. Compare clean accuracy retention versus attack success rate reduction.