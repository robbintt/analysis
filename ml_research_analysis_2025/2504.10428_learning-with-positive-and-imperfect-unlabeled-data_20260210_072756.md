---
ver: rpa2
title: Learning with Positive and Imperfect Unlabeled Data
arxiv_id: '2504.10428'
source_url: https://arxiv.org/abs/2504.10428
tags:
- learning
- samples
- algorithm
- theorem
- positive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Positive and Imperfect Unlabeled (PIU)
  learning model, a generalization of Positive and Unlabeled (PU) learning that allows
  for distribution shifts between positive and unlabeled data. The key insight is
  that PIU learning provides a unified framework that underlies several fundamental
  problems in statistics and learning theory, including learning from smooth distributions,
  truncated statistics, and truncation detection.
---

# Learning with Positive and Imperfect Unlabeled Data

## Quick Facts
- **arXiv ID**: 2504.10428
- **Source URL**: https://arxiv.org/abs/2504.10428
- **Reference count**: 40
- **Primary result**: Introduces PIU learning model generalizing PU learning to handle distribution shifts

## Executive Summary
This paper introduces the Positive and Imperfect Unlabeled (PIU) learning model, which generalizes Positive and Unlabeled (PU) learning by allowing for distribution shifts between positive and unlabeled data. The key insight is that PIU learning provides a unified framework that underlies several fundamental problems in statistics and learning theory, including learning from smooth distributions, truncated statistics, and truncation detection. The authors characterize the sample complexity of PIU learning, design computationally efficient algorithms, and demonstrate applications to related problems including truncated estimation and truncation detection.

## Method Summary
The method reduces PIU learning to constrained learning problems rather than standard ERM, using a pessimistic optimization approach that minimizes the volume of hypotheses over unlabeled samples while ensuring coverage of positive samples. The core algorithm is Iterative Constrained Regression, which solves T=1/γ instances of a constrained polynomial regression problem using Linear Programming. The approach requires only L1-approximation of hypotheses with respect to the unlabeled distribution, which is a significantly weaker condition than L2-approximation. The algorithm iteratively refines hypotheses by focusing on survival sets and uses rejection sampling to handle support mismatches.

## Key Results
- Characterizes sample complexity of PIU learning bounded by VC dimension and inversely proportional to smoothness parameters
- Designs computationally efficient algorithms requiring only L1-approximation rather than L2-approximation
- Demonstrates new algorithms for truncated estimation with unknown survival sets
- Provides efficient truncation detection algorithms that work for non-product distributions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Learning is possible under distribution shift if the optimization objective is constrained to be "pessimistic" about the unlabeled data.
- Mechanism: The paper proposes **Pessimistic-ERM**. Unlike standard ERM, which minimizes error over all data, Pessimistic-ERM minimizes the volume of the hypothesis over the unlabeled samples ($|H \cap U|/|U|$) subject to the constraint that it captures almost all positive samples ($|H \cap P|/|P| \ge 1 - \rho$). This prevents the model from simply explaining the positive samples by expanding to cover the full support of the shifted unlabeled distribution $D$.
- Core assumption: The shift is bounded by **Assumption 1 (Generalized Smoothness)**, where the true distribution $D^\star$ is $(\sigma, q)$-smooth with respect to the accessible distribution $D$.
- Break condition: If the distribution shift is unbounded (Assumption 1 fails), Pessimistic-ERM may converge to a trivial solution or fail to generalize to $D^\star$.

### Mechanism 2
- Claim: Computational efficiency relies on approximating the hypothesis with polynomials in $L_1$-norm with respect to the *accessible* distribution, not the *true* one.
- Mechanism: The **Constrained-Regression** subroutine (Algorithm 3) solves a linear program to find a polynomial $p(x)$. Instead of requiring the hypothesis to be approximable with respect to the unknown target distribution $D^\star$, it requires $L_1$-approximability with respect to the known distribution $D$. This decouples the complexity of the hypothesis from the complexity of the distribution shift.
- Core assumption: The hypothesis class $H$ is $\zeta$-approximable by degree-$k$ polynomials in $L_1$-norm with respect to $D$ (Definition 3).
- Break condition: If the hypothesis requires extremely high-degree polynomials to approximate in $L_1$ (e.g., highly discontinuous functions), the computational complexity (poly($d^k$)) becomes intractable.

### Mechanism 3
- Claim: Handling support mismatch (where the unlabeled data contains regions the true distribution ignores) requires an iterative filtering process.
- Mechanism: **Iterative-Pessimistic-ERM** (Algorithm 1 and 2) solves the optimization problem in rounds. In each round $i$, it focuses on a specific region $S_i$ (the "survival set") defined by the intersection of previous hypotheses. It effectively "peels away" layers of the hypothesis space, ensuring that regions of $D$ irrelevant to $D^\star$ are eventually discarded from the optimization.
- Core assumption: **Assumption 2**, ensuring the set of positive samples has non-trivial mass ($\alpha > 0$) under $D^\star$.
- Break condition: If the positive mass is negligible ($\alpha \to 0$), the iterative removal of unlabeled samples may result in empty or statistically insignificant subsets, causing the algorithm to fail.

## Foundational Learning

- **Concept**: VC Dimension (Vapnik-Chervonenkis Dimension)
  - Why needed here: The paper characterizes the sample complexity of PIU learning directly in terms of the VC dimension of the hypothesis class (Theorem 1.1). Understanding this is necessary to interpret the sample bounds $\tilde{O}((\epsilon\sigma)^{-2q} \cdot VC(H))$.
  - Quick check question: If the VC dimension of $H$ is infinite, is PIU learning possible even if Assumption 1 holds?

- **Concept**: Covariate Shift vs. Distribution Shift
  - Why needed here: The paper distinguishes PIU learning from standard "Perfect" PU learning by explicitly modeling $D \neq D^\star$. The core difficulty is that the learner has sample access to $D$ (imperfect) and $P^\star$ (positive), but must learn w.r.t $D^\star$ (true).
  - Quick check question: In the PIU model, do we have sample access to the distribution $D^\star$ against which the error is measured?

- **Concept**: L1-Regression vs. L2-Regression
  - Why needed here: A major contribution is shifting the efficiency requirement from $L_2$-approximability (required by prior work like [LMZ24]) to $L_1$-approximability. This allows the algorithm to handle wider classes of distributions (e.g., non-product distributions).
  - Quick check question: Why is $L_1$-approximability considered a "significantly weaker condition" than $L_2$-approximability in this context?

## Architecture Onboarding

- **Component map**: Positive Samples ($P \sim P^\star$) -> Unlabeled Samples ($U \sim D$) -> Iterative Constrained Regression (Algorithm 2) -> Boosted Constrained Regression (Algorithm 3) -> Aggregator (Intersects hypotheses)
- **Critical path**: 
  1. Initialization with full domain
  2. Rejection sampling to create $D_i$ and $P^\star_i$ based on survival set $S_i$
  3. Solving the constrained LP (Eq 16) to find polynomial $p_i$
  4. Thresholding $p_i$ to get hypothesis $H_i$
  5. Updating $S_{i+1} = S_i \cap H_i$ and repeating
- **Design tradeoffs**:
  - **Improper vs. Proper Learning**: The architecture is an *improper* learner (outputs an intersection of PTFs). Proper learning is proven impossible for PIU (Remark 4.2).
  - **Runtime vs. Generality**: Runtime scales as $poly(d^k)$. While this handles general distributions, the exponential dependence on polynomial degree $k$ (which scales with $1/\epsilon$) makes it computationally heavy for very high precision.
- **Failure signatures**:
  - **Break Condition Trigger**: If `Boosted-Constrained-Regression` returns a hypothesis where $|H \cap P|$ is low, the feasibility check (Step 8 in Alg 2) triggers a break, potentially stopping prematurely if assumptions are violated.
  - **Support Mismatch**: If $supp(D^\star) \not\subseteq supp(D)$, the algorithm cannot recover the true hypothesis because it lacks samples in the missing regions.
- **First 3 experiments**:
  1. **Baseline Validation**: Run `Iterative-Pessimistic-ERM` on synthetic Gaussian data where $D^\star$ is Gaussian and $D$ is a truncated/corrupted version. Verify error scales as $O(\epsilon^{-2q})$.
  2. **Ablation on Smoothness**: Vary the parameter $q$ in Assumption 1 (control the severity of shift) and measure the sample complexity required to reach a fixed error rate.
  3. **Application Test (Truncation)**: Apply the algorithm to the "Truncated Estimation" use case (Section 1.3.3) using a Gaussian target truncated by an arbitrary halfspace $S^\star$. Compare the recovered parameter $\hat{\theta}$ against standard PU baselines.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is there a black-box reduction from efficient agnostic learning to efficient Positive and Imperfect Unlabeled (PIU) learning?
- Basis in paper: [explicit] The authors explicitly ask in Section 1.6 (Open Problem 1) whether such a reduction exists.
- Why unresolved: The algorithm proposed in this paper is a modification of L1-Regression requiring significant new analysis, rather than a black-box reduction that simply utilizes an agnostic learner as a subroutine.
- What evidence would resolve it: A proof demonstrating that PIU learning can be solved computationally using an agnostic learning oracle with only polynomial overhead, or a hardness result showing such a reduction is impossible.

### Open Question 2
- Question: Can computationally efficient algorithms with polynomial dependence on $1/\varepsilon$ be designed for specific concept classes, such as intersections of halfspaces with Gaussian distributions or single halfspaces with arbitrary distributions?
- Basis in paper: [explicit] Open Problem 2 in Section 1.6 asks if poly$(d/\varepsilon)$ time algorithms exist for these specific cases.
- Why unresolved: While the paper provides a general algorithm, it necessarily incurs an exponential dependence on $1/\varepsilon$ due to SQ lower bounds for general problems. The authors suggest specific structured problems might avoid this.
- What evidence would resolve it: New algorithms specifically tailored to intersections of halfspaces or single halfspaces that run in time poly$(d/\varepsilon)$, potentially bypassing the general SQ lower bounds.

### Open Question 3
- Question: What is the optimal sample complexity of PIU learning under the smoothness assumption with $q=1$?
- Basis in paper: [explicit] Open Problem 3 in Section 1.6 highlights the gap between the paper's upper bound and the lower bound derived from realizable learning.
- Why unresolved: Theorem 1.1 provides an upper bound of $\tilde{O}(\text{VC}(H)/\varepsilon^2)$, whereas the lower bound is $\tilde{\Omega}(\text{VC}(H)/\varepsilon)$. It is unknown if the quadratic dependence on $1/\varepsilon$ is inherent.
- What evidence would resolve it: An algorithm achieving $\tilde{O}(\text{VC}(H)/\varepsilon)$ sample complexity for $q=1$, or a proof of a lower bound matching the $1/\varepsilon^2$ rate.

## Limitations
- The framework assumes bounded distribution shift (Assumption 1) and non-negligible positive mass (Assumption 2), which may not hold in all real-world scenarios.
- Computational complexity scales as poly(d^k) where k depends on the desired error tolerance, making the approach potentially intractable for very high precision requirements.
- The algorithm requires careful selection of polynomial degree k and tolerance parameters, which may need tuning for specific applications.

## Confidence

- **High Confidence**: The characterization of sample complexity bounds and the reduction from PIU learning to constrained optimization problems (Mechanism 1). The theoretical framework is well-defined and the proofs appear rigorous.
- **Medium Confidence**: The computational efficiency claims, particularly the shift from L2- to L1-approximability (Mechanism 2). While theoretically justified, practical performance may vary depending on the specific hypothesis class and distribution characteristics.
- **Medium Confidence**: The iterative filtering mechanism's effectiveness in handling support mismatch (Mechanism 3). The approach is sound in principle, but its practical performance depends heavily on the specific structure of the distribution shift.

## Next Checks

1. **Empirical Robustness Testing**: Implement the algorithm on synthetic datasets where Assumption 1 is intentionally violated to different degrees. Measure how performance degrades as the smoothness parameter σ decreases, validating the theoretical bounds on sample complexity.

2. **Computational Scalability Analysis**: Benchmark the algorithm on increasingly high-dimensional problems (d=10, 20, 50) with varying precision requirements (ε values). Document the actual runtime scaling compared to the theoretical poly(d^k) bound, identifying the practical limits of the approach.

3. **Application to Real-World Truncation Detection**: Apply the truncation detection algorithm to a real-world dataset with known truncation (e.g., survey data with selection bias). Compare against standard PU learning baselines and evaluate whether the PIU approach provides measurable improvements in estimation accuracy.