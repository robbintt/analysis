---
ver: rpa2
title: 'LLaVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal Models'
arxiv_id: '2403.15388'
source_url: https://arxiv.org/abs/2403.15388
tags:
- tokens
- visual
- token
- llav
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the high computational cost of Large Multimodal
  Models (LMMs) due to the large number of visual tokens processed by the LLM component.
  The authors propose PruMerge, a method that reduces the number of visual tokens
  by adaptively selecting important ones based on sparsity in attention scores between
  the class token and spatial tokens, and merging similar tokens to enhance representation.
---

# LLaVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal Models

## Quick Facts
- arXiv ID: 2403.15388
- Source URL: https://arxiv.org/abs/2403.15388
- Reference count: 40
- Reduces visual tokens by 14x on average while maintaining performance on LLaVA-1.5

## Executive Summary
LLaVA-PruMerge addresses the high computational cost of Large Multimodal Models (LMMs) by reducing the number of visual tokens processed by the LLM component. The method exploits sparsity in attention patterns between the class token and visual tokens, using adaptive selection via IQR outlier detection and key-similarity merging to preserve important information. PruMerge achieves comparable performance to full-token LLaVA-1.5 across diverse visual question-answering and reasoning tasks while reducing visual tokens by 14x on average.

## Method Summary
PruMerge is a training-free, plug-and-play module that reduces visual tokens in LMMs by adaptively selecting important tokens based on attention sparsity and merging similar tokens to enhance representation. The method extracts attention scores between the class token and spatial tokens in the ViT's penultimate layer, uses IQR outlier detection for adaptive selection, and clusters pruned tokens to retained tokens using key similarity. PruMerge+ extends this by adding spatially uniform sampling to maintain global context. The module can be integrated after the visual encoder and works with LoRA fine-tuning for performance recovery.

## Key Results
- Achieves 14x average token reduction while maintaining performance on LLaVA-1.5
- PruMerge+ improves performance by including additional spatial tokens while maintaining 4x compression
- Reduces computational costs while preserving accuracy on VQAv2, ScienceQA, TextVQA, and other benchmarks

## Why This Works (Mechanism)

### Mechanism 1: Adaptive Selection via Attention Sparsity
The method computes attention scores between the [CLS] token and spatial tokens in the ViT's penultimate layer. Instead of fixed Top-K selection, it applies the Interquartile Range (IQR) method to dynamically determine a threshold. Tokens with attention scores exceeding this threshold are retained. This exploits the sparsity observed in visual encoder attention patterns, where some tokens receive disproportionately high attention from the class token.

### Mechanism 2: Information Recovery via Key-Similarity Merging
Retained tokens act as cluster centers. The algorithm calculates similarity of pruned tokens to these centers using the dot product of their Key vectors from the self-attention layer. Pruned tokens are merged into their nearest center via weighted average, where weights are determined by original attention scores. This preserves local context that would otherwise be lost through simple pruning.

### Mechanism 3: Uniform Spatial Regularization (PruMerge+)
PruMerge+ supplements adaptively selected tokens with a grid of tokens sampled uniformly across the image. This ensures that even "boring" background regions retain representation, preventing the model from hallucinating objects due to missing context. The method combines high-attention focal points with low-attention distributed context for optimal LLM prompting.

## Foundational Learning

- **Concept: Vision Transformer (ViT) Attention Map**
  - Why needed here: The core pruning signal is derived from attention weights between the [CLS] token and patch tokens. Understanding this is necessary to interpret why attention indicates importance.
  - Quick check question: In a standard ViT, which token is typically used as the summary representation for the whole image, and how does PruMerge utilize its interactions?

- **Concept: Interquartile Range (IQR)**
  - Why needed here: Unlike standard Top-K pruning, this paper uses a statistical method to define "important" dynamically. Understanding IQR is necessary to debug why token counts vary between images.
  - Quick check question: If an image has uniformly high attention scores (no outliers), how would an IQR-based selection strategy behave compared to a Top-K strategy?

- **Concept: Quadratic Complexity of Self-Attention**
  - Why needed here: The motivation for this work is the O(NÂ²) cost of the LLM. Understanding this scaling law explains why reducing 576 tokens to ~40 yields significant FLOP savings.
  - Quick check question: Does reducing visual tokens primarily save memory in the ViT encoder or the LLM backbone, and why?

## Architecture Onboarding

- **Component map:** Vision Encoder (ViT) -> [CLS] Attention Extractor -> IQR Scoring & Selection -> Key-Similarity Clustering -> Weighted Merge -> LLM Projector -> LLM

- **Critical path:** The Selection step is the most critical. If the IQR threshold is calculated incorrectly (e.g., picking 0 tokens or all tokens), the subsequent merging logic fails or the LLM context overflows.

- **Design tradeoffs:**
  - PruMerge vs. PruMerge+: PruMerge offers maximum compression (avg 5.5% tokens) but may lose global context. PruMerge+ uses 4x more tokens (25%) but retains spatial awareness and performance parity.
  - Training-free vs. Fine-tuning: The method works training-free, but fine-tuning with LoRA is shown to recover performance drops (e.g., in TextVQA).

- **Failure signatures:**
  - Hallucination increase: If PruMerge prunes background tokens that define relationships between objects, the LLM may hallucinate spatial relations.
  - OCR Collapse: Aggressive merging may blur character strokes; check performance on TextVQA immediately if implementing the merge step.

- **First 3 experiments:**
  1. Visualize the Mask: Run a batch of images through the selection module and plot the retained tokens over the image to verify they align with semantic objects (foreground) vs. background.
  2. Ablate the Merge: Run AITS (selection only) vs. AITS + TS (selection + merge) on VQAv2 to quantify the information gain from the merging mechanism.
  3. Token Count Distribution: Plot a histogram of the number of tokens retained per image in a diverse dataset to verify the "adaptive" nature of the IQR method.

## Open Questions the Paper Calls Out

### Open Question 1
Can specialized temporal token reduction strategies be developed to better exploit the redundancy observed in Video-LLMs, beyond treating video frames as independent images? The authors note that their findings suggest significant redundancy in visual tokens used by video-LLMs and exploring ways to capitalize on this redundancy could shape future research directions.

### Open Question 2
How does PruMerge perform when applied to LMMs utilizing dynamic resolution inputs (e.g., LLaVA-Next) where visual token counts scale significantly higher than the 576 tokens in LLaVA-1.5? The introduction identifies "high-resolution images" as a primary driver for increased computational costs, yet main experiments are restricted to fixed resolution.

### Open Question 3
Is the reliance on the [CLS] token's attention scores a fundamental requirement, or can token importance be determined via alternative proxies for architectures lacking a class token? The methodology explicitly depends on the sparse distribution of attention scores between the class token and visual tokens, creating a dependency on this specific architectural feature.

## Limitations
- Performance drops of 2-3% absolute on some benchmarks despite 14x compression
- Merge mechanism may blur features requiring precise spatial localization (e.g., OCR tasks)
- IQR threshold sensitivity may cause over-pruning on images with uniformly distributed attention scores
- Limited architecture scope with experiments only on LLaVA-1.5 and CLIP-ViT-L/14

## Confidence
- **High Confidence (8/10):** The fundamental mechanism of using attention sparsity for token selection is well-grounded in existing literature on ViT interpretability.
- **Medium Confidence (6/10):** Specific implementation details are underspecified or rely on design choices that could significantly impact results.
- **Low Confidence (4/10):** Claims about real-world applicability and deployment readiness are not substantiated.

## Next Checks
1. **Layer Selection Validation:** Implement the module with hooks on both the penultimate and final transformer layers of the CLIP visual encoder. Run inference on a diverse image set and visualize the selected token masks. Compare the semantic alignment of retained tokens and measure the impact on performance metrics.

2. **Merge Mechanism Ablation Study:** Create three experimental conditions: (a) Selection-only (AITS), (b) Selection + Merge with different Key similarity thresholds (k=5, k=10, k=20), and (c) PruMerge+ baseline. Evaluate all conditions on TextVQA and POPE datasets to quantify the marginal contribution of merging.

3. **Token Count Distribution Analysis:** Process the entire evaluation dataset through the selection module and generate a histogram of retained token counts per image. Calculate the coefficient of variation and identify whether any images fall below a reasonable minimum (e.g., <16 tokens). Implement a minimum token safeguard if variance is high and measure its impact on overall compression ratio and performance.