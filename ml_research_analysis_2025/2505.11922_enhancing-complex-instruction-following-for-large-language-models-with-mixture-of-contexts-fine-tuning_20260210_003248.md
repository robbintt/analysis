---
ver: rpa2
title: Enhancing Complex Instruction Following for Large Language Models with Mixture-of-Contexts
  Fine-tuning
arxiv_id: '2505.11922'
source_url: https://arxiv.org/abs/2505.11922
tags:
- miso
- instruction
- input
- attention
- complex
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of LLMs failing to consistently
  follow complex instructions with multiple constraints. The authors propose MISO
  (Multi-Input Single-Output), an extension to decoder-only transformer-based LLMs
  that transforms sequentially structured input instructions into multiple parallel
  or successive instructions containing subcontexts.
---

# Enhancing Complex Instruction Following for Large Language Models with Mixture-of-Contexts Fine-tuning

## Quick Facts
- arXiv ID: 2505.11922
- Source URL: https://arxiv.org/abs/2505.11922
- Authors: Yuheng Lu; ZiMeng Bai; Caixia Yuan; Huixing Jiang; Xiaojie Wang
- Reference count: 17
- Primary result: MISO achieves 52.1% average accuracy on IFEval compared to 48.1% for standard SFT

## Executive Summary
This paper addresses the challenge of large language models failing to consistently follow complex instructions with multiple constraints. The authors propose MISO (Multi-Input Single-Output), a novel architecture extension that transforms sequentially structured input instructions into multiple parallel or successive instructions containing subcontexts. MISO processes each subcontext independently and fuses their representations in the causal-attention layer during output generation using a weighted sum approach. This innovation avoids the broken generation problem observed in previous multi-input approaches while maintaining the ability to handle complex, constraint-heavy instructions.

The empirical results demonstrate MISO's superiority in complex instruction-following scenarios, with MISO-para achieving 52.1% average accuracy on IFEval compared to 48.1% for standard SFT. The paper also highlights potential training efficiency gains through reduced computational complexity when processing long input sequences, suggesting MISO could offer both performance and efficiency benefits over traditional single-input approaches.

## Method Summary
MISO extends decoder-only transformer-based LLMs by decomposing complex instructions into multiple subcontexts that are processed in parallel or sequentially. The architecture maintains a single output head while allowing multiple input processing streams. Each input sequence is processed independently through the transformer layers, with their representations fused in the causal-attention layer using a weighted sum mechanism. This approach enables the model to capture multiple constraints simultaneously without the generation issues that plague traditional multi-input architectures. The weighted fusion allows the model to dynamically balance contributions from different subcontexts during output generation, creating a more coherent response that satisfies all instruction components.

## Key Results
- MISO-para achieves 52.1% average accuracy on IFEval benchmark compared to 48.1% for standard SFT
- MISO demonstrates superior performance on complex instruction-following tasks with multiple constraints
- The approach shows potential computational efficiency gains for long sequences through parallel subcontext processing

## Why This Works (Mechanism)
MISO works by decomposing complex instructions into manageable subcontexts that can be processed independently, then intelligently fusing their representations during output generation. The key insight is that complex instructions with multiple constraints often contain naturally separable components that can be handled in parallel without losing the overall context. By processing these subcontexts independently through the transformer layers and then combining their representations in the causal-attention layer, MISO avoids the attention bottleneck and memory issues that plague traditional multi-input approaches. The weighted sum fusion mechanism allows the model to dynamically balance the influence of different subcontexts based on their relevance to the current generation step.

## Foundational Learning

**Attention Mechanisms** - Why needed: Core to transformer architecture and MISO's fusion approach. Quick check: Can explain multi-head attention and its role in capturing dependencies.

**Fine-tuning Strategies** - Why needed: MISO introduces new training dynamics with multiple input streams. Quick check: Understands difference between SFT and other fine-tuning approaches.

**Instruction Following Evaluation** - Why needed: IFEval is the primary benchmark used. Quick check: Can describe IFEval's multiple-choice accuracy measurement approach.

**Causal Attention** - Why needed: MISO modifies the standard causal attention mechanism for fusion. Quick check: Can explain how causal attention differs from bidirectional attention.

**Subcontext Decomposition** - Why needed: Core to MISO's input transformation strategy. Quick check: Can identify strategies for decomposing complex instructions into subcontexts.

## Architecture Onboarding

**Component Map**: Input Instruction -> Subcontext Decomposition -> Multiple Parallel/Successive Processing Streams -> Weighted Sum Fusion in Causal Attention -> Single Output Generation

**Critical Path**: The critical path flows from instruction decomposition through parallel processing streams to the weighted sum fusion in the causal attention layer, which directly impacts output quality. This fusion step is where MISO's innovation is most concentrated.

**Design Tradeoffs**: MISO trades the simplicity of single-input architectures for improved constraint handling capability. The multiple input streams increase model complexity but enable better handling of complex instructions. The weighted sum fusion adds hyperparameters but provides flexibility in balancing subcontext contributions.

**Failure Signatures**: Poor subcontext decomposition can lead to fragmented or inconsistent outputs. Improper weighting in the fusion layer can cause certain constraints to be ignored or overemphasized. The approach may struggle with instructions where subcontexts are highly interdependent.

**First Experiments**:
1. Compare MISO's performance on simple vs. complex instructions to validate the complexity threshold where it provides benefits
2. Ablation study varying the number of subcontexts to find optimal decomposition granularity
3. Test different weighting strategies in the fusion layer to determine optimal balance methods

## Open Questions the Paper Calls Out

The paper does not explicitly call out specific open questions in the provided content.

## Limitations

- Evaluation relies heavily on IFEval, which uses multiple-choice questions as a proxy for real-world instruction-following capability
- Modest performance improvements (52.1% vs 48.1%) may not translate to qualitatively different user experiences
- Computational efficiency claims require further validation across diverse instruction structures and real-world use cases

## Confidence

The empirical results are methodologically sound with proper ablation studies and comparisons to SFT baselines, but the limited scope of evaluation datasets and the modest magnitude of improvements suggest the need for broader validation.

- Performance claims: Medium
- Computational efficiency claims: Medium
- Real-world applicability: Low

## Next Checks

1. Evaluate MISO on diverse real-world instruction-following tasks beyond IFEval, including open-ended generation tasks where human evaluation can assess practical utility

2. Conduct systematic ablation studies varying the number of subcontexts and their granularity to determine optimal decomposition strategies across different instruction types

3. Measure actual training and inference time improvements across various sequence lengths and instruction complexities to validate the claimed computational efficiency gains