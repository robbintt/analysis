---
ver: rpa2
title: 'Vendor-Aware Industrial Agents: RAG-Enhanced LLMs for Secure On-Premise PLC
  Code Generation'
arxiv_id: '2511.09122'
source_url: https://arxiv.org/abs/2511.09122
tags:
- code
- local
- mitsubishi
- industrial
- electric
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a vendor-aware industrial coding assistant
  for Mitsubishi Electric's ST implementation using Retrieval-Augmented Generation
  (RAG) to address low-data domain challenges in industrial automation. The system
  employs competitive multi-model orchestration, compiler-in-the-loop validation,
  and directed retrieval over proprietary documentation to generate compilable control
  logic.
---

# Vendor-Aware Industrial Agents: RAG-Enhanced LLMs for Secure On-Premise PLC Code Generation

## Quick Facts
- arXiv ID: 2511.09122
- Source URL: https://arxiv.org/abs/2511.09122
- Reference count: 26
- Primary result: RAG-enhanced fine-tuned local model achieves 86% compilation success for Mitsubishi ST PLC code generation

## Executive Summary
This paper presents a vendor-aware industrial coding assistant for Mitsubishi Electric's ST implementation using Retrieval-Augmented Generation (RAG) to address low-data domain challenges in industrial automation. The system employs competitive multi-model orchestration, compiler-in-the-loop validation, and directed retrieval over proprietary documentation to generate compilable control logic. Evaluation across 100 synthetic queries showed the fine-tuned local model with RAG achieved 86% compilation success rate with 27% requiring repairs, outperforming non-RAG configurations (73% success). Azure GPT-4.1 RAG achieved 73% success, while GPT-5 RAG reached 87%. The study demonstrates that RAG-supported coding assistants can achieve high-quality code generation for proprietary platforms through targeted knowledge base construction and vendor-specific prompt engineering, enabling secure on-premise deployment without extensive fine-tuning or large training datasets.

## Method Summary
The approach combines RAG with compiler-in-the-loop validation for Mitsubishi Electric iQ-R ST PLC code generation. A three-segment knowledge base (function blocks, specifications, auxiliary context) is indexed with metadata filtering and queried via semantic embeddings. Generated code is validated through GX Works3 compilation with bounded iterative repair (max 3 attempts) using compiler diagnostics. Synthetic training data is generated via GPT-4.1 RAG pipeline, filtered through compilation and static analysis (function blocks must be invoked), then used to fine-tune a local DeepSeek-Coder-v2-Lite-Instruct model. The system uses competitive orchestration across Azure GPT-4.1/GPT-5 and local models with vendor-specific hard constraints (reserved-word blacklist, VAR block semantics, POU templates).

## Key Results
- Fine-tuned local model with RAG achieved 86% compilation success vs 73% without RAG
- Azure GPT-4.1 RAG achieved 73% success vs 38% Standard
- GPT-5 RAG reached 87% success with only 14% requiring repairs
- Repair rate: 27% of fine-tuned RAG outputs required ≤3 compiler-guided fixes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Directed retrieval over vendor-specific documentation materially improves compilation success for proprietary ST dialects.
- Mechanism: A three-segment knowledge base (function blocks, specifications, auxiliary context) is indexed with metadata filtering and queried via semantic embeddings. Retrieved context is concatenated with vendor-specific hard constraints and canonical examples before generation.
- Core assumption: Proprietary function block semantics and syntax rules are documented and retrievable; embedding quality supports relevant lookups.
- Evidence anchors:
  - [abstract] Fine-tuned local model with RAG achieved 86% compilation success vs 73% without RAG; Azure GPT-4.1 RAG achieved 73% vs 38% Standard.
  - [section] Page 3 describes the RAG pipeline combining directed knowledge retrieval with competitive multi-model orchestration and compiler validation.
  - [corpus] Weak direct evidence on RAG for PLC; related papers focus on code generation benchmarks and binary analysis rather than retrieval mechanisms.
- Break condition: Documentation is incomplete, outdated, or embedding retrieval fails to surface relevant function block variants for edge-case instructions.

### Mechanism 2
- Claim: Compiler-in-the-loop validation with bounded iterative repair converts non-compiling outputs into valid code at acceptable cost.
- Mechanism: Generated ST code is submitted to a GX Works3 compilation server returning structured diagnostics. On failure, a repair prompt embeds verbatim diagnostics and instructs the model to address specific error categories while preserving valid segments, up to 3 iterations.
- Core assumption: Compiler diagnostics are sufficiently informative for the LLM to localize and fix issues; the model has sufficient reasoning capacity to interpret vendor-specific error messages.
- Evidence anchors:
  - [abstract] Fine-tuned RAG achieved 86% success with 27% requiring repairs; GPT-5 RAG reached 87% with 14% repairs.
  - [section] Page 4 describes diagnostic-guided repair with maximum 3 compilation attempts targeting undeclared variables, reserved-word violations, type mismatches, and disallowed instructions.
  - [corpus] SecureAgentBench and DUALGUAGE address security in code generation but do not evaluate compiler-feedback repair loops for industrial control.
- Break condition: Diagnostics are ambiguous, cascading errors obscure root cause, or the model repeatedly makes the same mistake within the repair budget.

### Mechanism 3
- Claim: Fine-tuning small models on compiler-validated synthetic data enables competitive on-premise performance without large-scale datasets.
- Mechanism: Synthetic task-solution pairs are generated using the RAG-enhanced pipeline, then validated through actual GX Works3 compilation and static analysis (e.g., declared function blocks must be invoked). Only passing samples are retained for fine-tuning.
- Core assumption: Synthetic queries reflect real-world task distribution; compiler-passing code is a meaningful proxy for utility.
- Evidence anchors:
  - [abstract] Fine-tuned local model with RAG achieved 86% success, competitive with GPT-5 RAG at 87%.
  - [section] Page 5 describes synthetic data generation with multiple user personas and automated validation enforcing compilation success and function block invocation.
  - [corpus] No direct corpus evidence on synthetic data quality for PLC fine-tuning; related work focuses on agent benchmarks and optimization.
- Break condition: Synthetic data lacks coverage of edge cases, introduces systematic biases, or fails to align with actual engineering workflows.

## Foundational Learning

- Concept: IEC 61131-3 Structured Text and vendor-specific dialects
  - Why needed here: ST syntax is standardized, but Mitsubishi Electric's implementation has proprietary function blocks, label-based variable management (no inline VAR blocks), and reserved-word constraints that differ from other vendors.
  - Quick check question: Can you explain why code that compiles in CODESYS may fail in GX Works3 due to variable declaration semantics?

- Concept: Retrieval-Augmented Generation (RAG) with semantic embeddings
  - Why needed here: The system relies on directed retrieval over three document segments using text-embedding-3-large; understanding chunking, metadata filtering, and retrieval relevance is essential for debugging low-quality outputs.
  - Quick check question: What would cause a RAG system to retrieve generic IEC 61131-3 examples instead of Mitsubishi-specific function block documentation?

- Concept: Compiler diagnostics interpretation and automated repair
  - Why needed here: The repair loop depends on mapping vendor-specific error messages to corrective actions; understanding diagnostic categories (undeclared variables, reserved words, type mismatches) is critical for prompt design and failure analysis.
  - Quick check question: If GX Works3 returns "identifier expected" at a line using a reserved word, how would you structure a repair prompt to address this?

## Architecture Onboarding

- Component map: Knowledge Base (function blocks, specs, auxiliary context) → Retrieval (3 specialized retrievers, text-embedding-3-large) → Prompt Construction (hard constraints + retrieved context + POU templates + canonical example) → Competitive Model Layer (Azure RAG, Azure Standard, Local RAG) → GX Works3 Compilation Server → Diagnostic-Guided Repair Loop (max 3 iterations) → Validated ST Code

- Critical path: User query → optional query expansion → retrieval → prompt assembly → model inference → compilation → repair loop (if needed) → output. Latency is dominated by retrieval overhead and repair iterations; RAG adds 20-40 seconds per query without repair.

- Design tradeoffs: (1) Cloud vs local: GPT-4.1/5 offers higher quality explanations and structured output but requires external API calls; local model (10GB VRAM, quantized DeepSeek Coder v2 Lite) enables fully offline deployment at cost of shorter context and reduced explanation quality. (2) RAG vs fine-tuning alone: RAG provides +13-35% compilation improvement but adds latency; fine-tuning alone achieves 73% baseline without retrieval overhead. (3) Competitive orchestration: Running 3 models concurrently enables comparison but triples inference cost.

- Failure signatures: (1) Reserved-word violations — model uses IEC keywords not allowed in Mitsubishi ST. (2) Unused function blocks — declared but not invoked, triggering static analysis rejection during training data curation. (3) Empty or incoherent logic — compiles but performs no useful operation (e.g., empty arrays). (4) VAR block rejection — GX Works3 requires external label registration; inline declarations fail conversion.

- First 3 experiments:
  1. Baseline comparison: Run identical 20 queries across Azure Standard, Azure RAG, and Local RAG without repair loop; record compilation success rates and latency to quantify retrieval contribution.
  2. Repair loop effectiveness: Enable diagnostic-guided repair on the same queries for Azure RAG and Local RAG; measure success improvement and average repair iterations needed.
  3. Knowledge base ablation: Temporarily remove the function block segment from retrieval; run 10 function-block-heavy queries to assess impact on compilation success and identify under-documented instruction variants.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does compilation success correlate with functional correctness when generated ST code is executed on physical PLC hardware or simulation environments?
- Basis in paper: [explicit] "We acknowledge that compilation success is a necessary but not sufficient criterion for code quality. Functional correctness would require domain-expert evaluation or execution on physical hardware... Future work should incorporate simulation-based testing or hardware-in-the-loop validation to assess runtime behavior."
- Why unresolved: Evaluation relied solely on compilation success and qualitative expert review; no runtime testing was conducted.
- What evidence would resolve it: Execute generated programs on Mitsubishi Electric iQ-R hardware or approved simulator and measure functional correctness against task specifications.

### Open Question 2
- Question: Can the RAG-enhanced architecture transfer to other vendor platforms (Siemens, Rockwell, CODESYS) without substantial re-engineering of retrieval segmentation and prompt templates?
- Basis in paper: [explicit] "While this work focuses exclusively on Mitsubishi Electric's iQ-R series, the architectural components are conceptually transferable... However, we have not empirically validated cross-vendor generalization."
- Why unresolved: Each vendor has distinct function block semantics, compiler behaviors, and documentation structures; only Mitsubishi Electric was evaluated.
- What evidence would resolve it: Deploy the same architecture on a different vendor platform with equivalent evaluation (100 queries, compilation rates, expert ratings).

### Open Question 3
- Question: What are the real-world productivity impacts and adoption barriers when deploying vendor-specific coding assistants in industrial engineering teams over extended periods?
- Basis in paper: [explicit] "Human evaluation from a single organizational context may limit generalizability; longitudinal deployment studies are needed to assess adoption barriers and real-world productivity impacts."
- Why unresolved: Evaluation used synthetic queries and limited expert feedback; no longitudinal field study was conducted.
- What evidence would resolve it: Longitudinal study measuring engineer productivity, code quality, and adoption friction across multiple industrial sites over months.

## Limitations
- Evaluation relies entirely on synthetic queries rather than real-world engineering tasks, raising questions about external validity
- Knowledge base construction assumes Mitsubishi's documentation is complete and accurately indexed, but proprietary systems often have undocumented edge cases
- The 100-query evaluation set may not capture the full distribution of industrial programming challenges
- Repair loop effectiveness depends on whether compiler diagnostics provide sufficient semantic context for meaningful corrections

## Confidence
- **High confidence**: Compilation success rates and repair rates for the RAG-enhanced configurations (86% success with 27% repairs for fine-tuned local model; 87% with 14% repairs for GPT-5 RAG). These metrics are directly measured and reproducible through the described compilation pipeline.
- **Medium confidence**: The mechanism explanations linking RAG retrieval to compilation success. While the correlation is strong, the paper doesn't provide detailed error analysis showing exactly which retrieved contexts resolved specific compilation failures.
- **Medium confidence**: The claim that this approach enables secure on-premise deployment without extensive fine-tuning. The local model achieves competitive performance, but the 13GB VRAM requirement and 20-40 second latency may limit practical deployment scenarios.

## Next Checks
1. **Real-world task validation**: Deploy the system with actual Mitsubishi Electric iQ-R programmers over 2-4 weeks, collecting authentic programming tasks and measuring compilation success against the synthetic query baseline. This would validate external validity and identify gaps in synthetic data coverage.

2. **Knowledge base coverage analysis**: Systematically map the 100 synthetic queries to knowledge base segments to identify retrieval failures. For queries where RAG provides no benefit or harm, analyze whether documentation gaps exist or if embedding quality needs improvement for specific function block variants.

3. **Diagnostic interpretability study**: Manually trace a sample of failed compilations through the repair loop to determine whether compiler diagnostics contain sufficient semantic information for the model to make meaningful corrections, or whether additional tooling (e.g., static analysis integration) would improve repair effectiveness.