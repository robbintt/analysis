---
ver: rpa2
title: 'OCR-Quality: A Human-Annotated Dataset for OCR Quality Assessment'
arxiv_id: '2510.21774'
source_url: https://arxiv.org/abs/2510.21774
tags:
- quality
- dataset
- text
- image
- assessment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces OCR-Quality, a human-annotated dataset for
  evaluating OCR quality assessment methods. The dataset contains 1,000 PDF pages
  converted to 300 DPI PNG images from diverse sources including academic papers,
  textbooks, e-books, and multilingual documents.
---

# OCR-Quality: A Human-Annotated Dataset for OCR Quality Assessment

## Quick Facts
- arXiv ID: 2510.21774
- Source URL: https://arxiv.org/abs/2510.21774
- Reference count: 18
- Primary result: VLM-as-Judge baseline achieves 36.1-40.0 F1 for OCR quality assessment on 4-level human annotations

## Executive Summary
This paper introduces OCR-Quality, a human-annotated dataset for evaluating OCR quality assessment methods. The dataset contains 1,000 PDF pages converted to 300 DPI PNG images from diverse sources including academic papers, textbooks, e-books, and multilingual documents. Each document was processed using the Qwen2.5-VL-72B Vision-Language Model and manually annotated with quality scores using a 4-level scoring system (1: Excellent, 2: Good, 3: Fair, 4: Poor). Baseline results show that even state-of-the-art VLMs struggle with OCR quality assessment, with F1 scores ranging from 36.1 to 40.0 across different quality levels. The dataset is publicly available and enables development of more reliable OCR quality assessment methods for real-world applications.

## Method Summary
The OCR-Quality dataset was created by collecting 1,000 PDF pages from diverse sources, converting them to 300 DPI PNG images, and processing them with Qwen2.5-VL-72B for OCR extraction. Human annotators evaluated the OCR outputs using a custom Gradio interface, comparing original document images with OCR text to assign quality scores (1-4). The dataset includes detailed source information and represents various document types and languages. Baseline evaluation uses VLM-as-Judge approaches where VLMs score OCR outputs on a 0.0-1.0 scale, and Consensus Entropy methods that aggregate multiple model judgments.

## Key Results
- Dataset contains 1,000 samples with 50.7% scored as "Excellent" (1), 30.5% as "Good" (2), 8.4% as "Fair" (3), and 10.4% as "Poor" (4)
- VLM-as-Judge baselines achieve F1 scores of 36.1-40.0 across quality levels, with score 1 achieving 70.05-72.27 F1 but score 2 dropping to 18.42-23.01 F1
- Consensus Entropy method achieves 48.0 overall F1, outperforming single-model approaches
- Baseline models show strong performance on excellent quality (score 1) but struggle with distinguishing minor errors (score 2)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Human-annotated quality scores on a 4-level scale provide a practical ground truth for training OCR quality assessment systems.
- Mechanism: Annotators view side-by-side comparisons of original document images and OCR outputs, then assign scores (1=Excellent to 4=Poor) based on error severity, structural preservation, and usability for downstream tasks.
- Core assumption: Human judgment captures OCR quality in a manner that generalizes to automated assessment methods.
- Evidence anchors:
  - [abstract]: "manually annotated with quality scores using a 4-level scoring system"
  - [section 2.3]: Detailed scoring criteria with explicit thresholds for each level
  - [corpus]: Related work (E-ARMOR, OCR-Reasoning Benchmark) also relies on human evaluation for OCR assessment, but corpus lacks direct validation of inter-annotator reliability for this specific dataset
- Break condition: If single-annotator scores show high variance or fail to correlate with downstream task performance, the ground truth may be unreliable.

### Mechanism 2
- Claim: VLM-as-Judge approaches can be standardized using a specific evaluation prompt to assess OCR quality.
- Mechanism: A VLM receives the original image, OCR output, and a structured prompt asking it to assign a quality score (0.0-1.0) based on textual match accuracy.
- Core assumption: VLMs can accurately judge text extraction quality by comparing visual content with predicted text.
- Evidence anchors:
  - [section 5.2]: Provides explicit "OCR Quality Assessment Prompt" with 4-level scoring guidelines
  - [section 8]: Baseline results show VLM-as-Judge achieves 36.1-40.0 F1 across quality levels
  - [corpus]: VLMs-in-the-Wild notes disconnect between academic benchmarks and enterprise needs; suggests VLM evaluation may not generalize
- Break condition: If VLMs systematically misjudge certain document types (e.g., mathematical notation, multilingual content), the assessment will be biased.

### Mechanism 3
- Claim: Consensus Entropy (multi-model agreement) provides stronger quality signals than single-model judgment.
- Mechanism: Multiple VLMs process the same document; agreement levels correlate with OCR quality—high agreement suggests reliable extraction, low agreement signals uncertainty.
- Core assumption: Model disagreement indicates genuine ambiguity or difficulty in OCR extraction.
- Evidence anchors:
  - [section 8]: Consensus Entropy achieves 48.0 overall F1 vs. 36.1-40.0 for single VLM judges
  - [section 1]: References prior work on Consensus Entropy for uncertainty quantification
  - [corpus]: Limited direct corpus validation; related work focuses on single-model evaluation
- Break condition: If models systematically agree on incorrect outputs (shared failure modes), consensus will be misleading.

## Foundational Learning

- Concept: **OCR Quality vs. OCR Accuracy**
  - Why needed here: Quality assessment evaluates output usability (structure, completeness), not just character accuracy. The 4-level scoring captures this distinction.
  - Quick check question: Can an OCR output have high character accuracy but low quality score? (Yes—if structure is collapsed or sections are missing.)

- Concept: **VLM-as-Judge Paradigm**
  - Why needed here: The baseline evaluation uses VLMs to score OCR outputs. Understanding prompt design and scoring calibration is essential.
  - Quick check question: Why does the evaluation prompt ask for 0.0-1.0 scores when human annotations use 1-4? (Mapping required for comparison; granularity differs.)

- Concept: **Inter-Annotator Agreement**
  - Why needed here: The dataset acknowledges single-annotator limitation. Without agreement metrics, ground truth reliability is uncertain.
  - Quick check question: What metric would assess annotation reliability? (Cohen's Kappa or Krippendorff's Alpha for categorical scores.)

## Architecture Onboarding

- Component map:
  Document Source → PDF → PNG (300 DPI) → VLM OCR (Qwen2.5-VL-72B) → Human Annotation Interface (Gradio) ← Side-by-side view → Quality Dataset (Parquet) → Evaluation Methods (VLM-as-Judge, Consensus Entropy)

- Critical path:
  1. Data collection ensuring diversity (languages, document types)
  2. Consistent OCR processing with standardized prompt
  3. Annotation with clear scoring criteria
  4. Evaluation protocol (correlation, classification, ranking)

- Design tradeoffs:
  - **Dataset size (1,000 samples)**: Enables rapid experimentation but limits deep model training
  - **Single-VLM OCR outputs**: Controls for OCR variability but doesn't capture model diversity
  - **4-level scoring**: Balances granularity with annotation efficiency; loses fine-grained distinctions
  - **300 DPI resolution**: Preserves detail but increases storage/processing costs

- Failure signatures:
  - VLM-as-Judge scores 2 (Good) poorly (F1: 18.42-23.01) vs. scores 1 (Excellent: 70.05-72.27)—suggests difficulty distinguishing minor errors from perfection
  - No inter-annotator agreement → unknown ground truth noise level
  - Corpus signals (avg FMR=0.42) suggest this is a moderately covered area; limited prior validation

- First 3 experiments:
  1. **Baseline replication**: Run VLM-as-Judge with provided prompt on dataset; verify F1 scores match reported 36.1-40.0 range.
  2. **Inter-annotator analysis**: Duplicate-annotate 100 samples with 2+ annotators; compute agreement metrics to quantify ground truth reliability.
  3. **Error stratification**: Analyze per-document-type performance (academic vs. textbook vs. multilingual) to identify systematic failure modes.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the degree of subjectivity in the 4-level scoring system, and does the absence of inter-annotator agreement data limit the reliability of the ground truth?
- Basis in paper: [explicit] Section 9.1 lists "Single-annotator scores (no inter-annotator agreement analysis)" as a limitation, and Section 9.2 proposes to "Add multi-annotator scores" in future work.
- Why unresolved: The current dataset relies on a single human judgment per document, meaning potential ambiguity between "Good" (2) and "Fair" (3) scores is not quantified.
- What evidence would resolve it: Re-annotating a statistically significant subset (e.g., 20%) of the dataset with multiple annotators to calculate Krippendorff's alpha or Cohen's kappa.

### Open Question 2
- Question: Can quality assessment models trained on this dataset generalize to the specific error distributions of OCR systems other than Qwen2.5-VL-72B?
- Basis in paper: [explicit] Section 9.1 notes the dataset is limited by a "Single VLM," and Section 9.2 suggests future work should "Include outputs from multiple OCR systems."
- Why unresolved: The human scores are based on outputs from a specific model; different OCR engines (e.g., Tesseract, PaddleOCR) produce distinct error patterns (e.g., layout vs. character errors) that the current annotations may not encompass.
- What evidence would resolve it: Evaluating assessment metrics on OCR outputs generated by alternative models (e.g., GPT-4o or traditional OCR) using the existing image set to see if quality predictions correlate with human judgment.

### Open Question 3
- Question: How does the assessment performance degrade when applied to diverse scripts and low-resource languages not represented in the current Chinese and English majority distribution?
- Basis in paper: [explicit] Section 9.2 identifies the need to "Extend language coverage to more diverse scripts" as a future direction.
- Why unresolved: The dataset is primarily composed of Chinese and English sources (Table 1), leaving the effectiveness of derived assessment methods on complex scripts (e.g., Arabic, Hindi) unverified.
- What evidence would resolve it: Benchmarking the baseline VLM-as-Judge approaches on a new hold-out set of documents containing diverse scripts and analyzing the error rates.

### Open Question 4
- Question: Is the 4-level granularity distinguishable for current VLM-as-Judge models, given the significant performance drop in intermediate quality scores?
- Basis in paper: [inferred] Table 3 shows that while VLMs achieve ~72 F1 on "Score 1" (Excellent), they drop to ~18-23 F1 on "Score 2" (Good), suggesting the assessment models fail to distinguish minor errors from perfect text.
- Why unresolved: The scoring criteria define "Score 2" via minor errors like punctuation, which may be too subtle for current VLMs to consistently detect or differentiate from "Score 1."
- What evidence would resolve it: An ablation study comparing the 4-level classification performance against a binary (Acceptable/Unacceptable) classification to determine if the performance drop is due to granularity or model capability.

## Limitations
- Single-annotator scores without inter-annotator agreement analysis
- Limited to outputs from one VLM (Qwen2.5-VL-72B), missing model variability
- Primary focus on Chinese and English documents, lacking diverse script coverage

## Confidence
**High Confidence**: Dataset construction methodology, quality scoring criteria, and dataset structure are well-specified and reproducible.
**Medium Confidence**: Baseline VLM-as-Judge results are reproducible with provided prompt, but exact performance may vary due to unspecified inference parameters.
**Low Confidence**: Consensus Entropy implementation details and inter-annotator reliability metrics are not provided, limiting confidence in the most promising evaluation method and ground truth quality.

## Next Checks
1. **Ground Truth Validation**: Re-annotate 100 randomly selected samples with multiple annotators to compute Cohen's Kappa or Krippendorff's Alpha. This quantifies annotation reliability and identifies whether score variance explains the poor performance on middle-quality levels.

2. **VLM Inference Parameter Sweep**: Systematically test VLM-as-Judge across temperature (0.0, 0.5, 1.0), top_p (0.9, 0.95, 1.0), and max_tokens variations. Document how these parameters affect F1 scores and determine optimal configurations for stable performance.

3. **Document Type Performance Analysis**: Stratify dataset by source (academic, textbook, e-book, multilingual) and compute per-category F1 scores. Identify systematic failure modes (e.g., poor mathematical notation handling) and correlate with human annotation difficulty ratings if available.