---
ver: rpa2
title: A circuit for predicting hierarchical structure in-context in Large Language
  Models
arxiv_id: '2509.21534'
source_url: https://arxiv.org/abs/2509.21534
tags:
- heads
- induction
- tokens
- in-context
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how Large Language Models (LLMs) learn
  to predict structured sequences with hierarchical dependencies in-context, focusing
  on the role of induction heads and a supporting circuit. The authors design synthetic
  token sequences with first-, second-, and third-order hierarchical structures and
  evaluate several LLMs on these tasks.
---

# A circuit for predicting hierarchical structure in-context in Large Language Models

## Quick Facts
- arXiv ID: 2509.21534
- Source URL: https://arxiv.org/abs/2509.21534
- Authors: Tankred Saanum; Can Demircan; Samuel J. Gershman; Eric Schulz
- Reference count: 36
- Primary result: Large Language Models use adaptive induction heads and context matching heads in a circuit to predict structured sequences with hierarchical dependencies in-context.

## Executive Summary
This paper investigates how Large Language Models (LLMs) learn to predict structured sequences with hierarchical dependencies in-context, focusing on the role of induction heads and a supporting circuit. The authors design synthetic token sequences with first-, second-, and third-order hierarchical structures and evaluate several LLMs on these tasks. They discover that adaptive induction heads, which learn what successor tokens to attend to based on context, emerge in later layers and are crucial for accurate prediction. Additionally, they identify context matching heads that encode latent context identities (e.g., whether the current and previous chunks are the same) and show that these heads support the in-context learning ability of induction heads through controlled ablation experiments. The findings are replicated across multiple LLM families, suggesting a general mechanism for hierarchical in-context learning.

## Method Summary
The authors construct synthetic token sequences with hierarchical structure (first-, second-, and third-order dependencies) using random token subsets and permutations. They identify induction heads via standard score matching procedures and evaluate their context sensitivity by measuring whether they attend to successor tokens from matching contexts. Context matching heads are identified through linear probes trained to decode chunk identity from attention head outputs, with heads achieving >85% accuracy selected for ablation studies. The causal role of the circuit is tested through ablation experiments that zero out the outputs of identified context matching heads and measure effects on both model prediction accuracy and induction head behavior.

## Key Results
- Adaptive induction heads emerge in later layers and learn to selectively attend to contextually appropriate successor tokens rather than uniformly attending to all past successors.
- Context matching heads encode latent context identities through look-back attention patterns and achieve >90% accuracy in decoding 2nd-order chunk membership.
- Ablation of context matching heads causes sharp reductions in both model prediction accuracy and induction head context sensitivity, while random head ablation has minimal effect.
- The circuit's effectiveness scales with model size, with 3rd-order decodability improving from 0.5B to 3B parameter models.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Induction heads in later layers can selectively attend to contextually appropriate successor tokens, rather than uniformly attending to all past successors.
- **Mechanism**: Later-layer induction heads receive context information from preceding heads, which modulates their key/query representations to favor successors from matching latent contexts. This enables disambiguation when a token (e.g., "the" or "San") has multiple possible continuations.
- **Core assumption**: The model has already developed basic induction heads during pretraining; the mechanism builds on these rather than creating new ones.
- **Evidence anchors**:
  - [abstract]: "We find adaptive induction heads that support prediction by learning what to attend to in-context."
  - [Section 3]: Induction heads that learned in-context were located in later layers, whereas static heads were in earlier layers.
  - [corpus]: Yin & Steinhardt (2025) find function vector heads also contribute to ICL, suggesting multiple head types may cooperate.
- **Break condition**: If token sequences have no hierarchical structure (pure random or simple repetition), adaptive behavior degrades to standard induction head copying.

### Mechanism 2
- **Claim**: Context matching heads encode latent context identities (e.g., which 2nd-order chunk a token belongs to) by routing information from preceding tokens forward.
- **Mechanism**: These heads implement a look-back attention pattern—either to the immediate predecessor or to N previous tokens. Linear probes can decode chunk identity from their outputs (>90% accuracy for 2nd-order, lower but above chance for 3rd-order). This information propagates to subsequent layers.
- **Core assumption**: The model can build representations of hierarchical structure through iterative token-to-token attention.
- **Evidence anchors**:
  - [Section 4]: "Context matching heads... make tokens attend to either directly preceding tokens, or longer chains of preceding tokens, routing information allowing subsequent induction heads to query into the successor tokens."
  - [Figure 3]: Decodability for 2nd-order chunks peaks earlier; 3rd-order requires more layers and scales with model size.
  - [corpus]: Related work (Akyürek et al., 2024) shows n-gram heads with similar prefix-matching behavior in synthetic language tasks.
- **Break condition**: If context matching heads are ablated, induction heads lose context sensitivity but retain basic successor attention—accuracy to correct context drops sharply.

### Mechanism 3
- **Claim**: The full circuit—context matching heads feeding into adaptive induction heads—is causally necessary for hierarchical in-context learning.
- **Mechanism**: Context matching heads (layers ~13-14 in Qwen2.5-1.5B) build latent context representations; adaptive induction heads in subsequent layers (layer 14+) use these to direct attention appropriately. Ablation of context matching heads reduces both model prediction accuracy and induction head context accuracy.
- **Core assumption**: The circuit operates sequentially within a forward pass; no weight updates are required.
- **Evidence anchors**:
  - [Section 4.2]: Ablating context matching heads causes "consistent and sharp reduction in prediction accuracy" across all models; random head ablation has minimal effect.
  - [Figure 5]: Ablating head 13-4 causes induction head 14-3 to lose context sensitivity while retaining successor attention capability.
  - [corpus]: Limited direct corpus evidence for this specific circuit; related work on induction head circuits (Elhage et al., 2021; Olsson et al., 2022) establishes foundational patterns but not hierarchical extensions.
- **Break condition**: If the task exceeds the model's hierarchical depth capacity (e.g., 4th or 5th-order sequences), the circuit may fail or require substantially larger models.

## Foundational Learning

- **Concept: Induction heads (basic)**
  - Why needed here: The adaptive mechanism builds on standard induction heads. Without understanding that induction heads copy successors of past token occurrences, the hierarchical extension won't make sense.
  - Quick check question: Can you explain why a basic induction head fails when token "a" appears in two different contexts with different successors?

- **Concept: Attention pattern interpretation**
  - Why needed here: Reading attention heatmaps is essential for verifying which tokens heads attend to. The paper relies heavily on visualizing attention to successor tokens vs. context-appropriate successors.
  - Quick check question: Given an attention matrix where row i represents token i attending to previous tokens, how would you identify if a head is looking one token back vs. attending to successors?

- **Concept: Linear probing**
  - Why needed here: The paper uses linear probes to decode latent context identity from head outputs. Understanding this technique is necessary to evaluate the evidence for context matching heads.
  - Quick check question: If a linear probe achieves 90% accuracy on decoding chunk identity from a head's output, what does this imply about the information encoded in that representation?

## Architecture Onboarding

- **Component map**: Early layers (0-10) -> Static induction heads -> Middle layers (10-13) -> Context matching heads -> Later layers (14+) -> Adaptive induction heads -> Output prediction

- **Critical path**:
  1. Input tokens → embedding
  2. Early layers develop basic induction capabilities
  3. Context matching heads (mid-late layers) encode chunk membership via look-back attention
  4. Adaptive induction heads (late layers) query context-matched successors
  5. Output prediction via residual stream aggregation

- **Design tradeoffs**:
  - Model size matters: 3rd-order decodability improves with parameters (0.5B → 3B)
  - Layer depth matters: Hierarchical structure requires sufficient layers to build representations iteratively
  - Vocabulary selection: Synthetic experiments use random token subsets; natural language performance may differ

- **Failure signatures**:
  - If adaptive induction heads attend uniformly to all successors (not context-selective), the context matching circuit is not functioning
  - If 2nd-order decodability is low (<60%), context matching heads haven't developed properly
  - If ablation of non-context heads causes similar accuracy drops, the circuit identification may be incorrect

- **First 3 experiments**:
  1. **Identify induction heads**: Run standard induction head scoring (Olsson et al., 2022) on repeated random token sequences to get candidate heads.
  2. **Test context sensitivity**: Generate 2nd-order sequences (α, β, γ chunks); measure whether each induction head preferentially attends to same-chunk successors.
  3. **Probe for context representation**: Train linear classifiers on attention head outputs to decode chunk identity; identify heads with >85% accuracy as context matching candidates, then ablate and measure downstream effects.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is the proposed circuit of adaptive induction heads and context matching heads sufficient to support in-context learning of abstract relationships, such as those found in the Abstraction and Reasoning Corpus (ARC)?
- Basis in paper: [explicit] The authors state in the Conclusion that "problems from the Abstraction and Reasoning Corpus... require LLMs to learn abstract relationships between tokens in few examples. It is unclear if our proposed circuit... is powerful enough to induce abstract relationships like these."
- Why unresolved: The current study focuses on hierarchical patterns defined by token repetition and copying; it does not test on tasks requiring the inference of abstract rules or transformations that do not rely on repetitive token statistics.
- What evidence would resolve it: Evaluating the performance of the identified circuit on ARC tasks to see if the same context matching mechanism enables the learning of abstract rules, or if distinct circuits are required.

### Open Question 2
- Question: How does explicit verbal reasoning (e.g., Chain-of-Thought) alter the mechanism of in-context learning compared to the non-verbal, pattern-matching circuit identified?
- Basis in paper: [explicit] The Conclusion notes: "all learning analyzed here happened non-verbally, without explicit verbal reasoning. Encouraging in-context learning through reasoning and deliberation may be a second mechanism by which an LLM can change how induction heads allocate attention."
- Why unresolved: The paper isolates a mechanism for direct pattern matching, but does not analyze settings where the model is prompted to "think step-by-step," which might utilize different or supplementary pathways.
- What evidence would resolve it: Performing ablation studies and attention analysis on models while they are engaged in explicit reasoning tasks to determine if the adaptive induction heads remain the primary mechanism or if a separate "reasoning" circuit is engaged.

### Open Question 3
- Question: Does the reliance on "context matching heads" generalize to natural language hierarchies that lack strict token repetition?
- Basis in paper: [inferred] The synthetic data is constructed using "repetitive patterns" and the mechanism is described as "copying previously observed statistical patterns." Natural language often contains hierarchical syntactic structures (e.g., center embedding) where specific token sequences do not repeat within a context.
- Why unresolved: The identified mechanism relies on matching the current context to *past instances* of the same context to route information. It is unclear if this circuit functions when the hierarchical structure is novel or non-repetitive.
- What evidence would resolve it: Testing the causal importance of these context matching heads on linguistic benchmarks requiring structural generalization on novel sequences (e.g., specific syntactic tests) rather than repetitive ones.

## Limitations
- **Synthetic vs. Natural Language Gap**: The study uses artificially constructed token sequences with clear hierarchical structure, which may not fully capture the complexity of natural language hierarchies.
- **Circuit Specificity**: While the paper identifies a circuit involving context matching heads and adaptive induction heads, it's unclear whether this represents the only or primary mechanism for hierarchical in-context learning.
- **Causal Attribution Challenges**: The ablation experiments show correlation between context matching head activity and improved prediction accuracy, but establishing definitive causal mechanisms in neural networks remains challenging.

## Confidence
**High Confidence**: The identification of context matching heads that encode latent context identities through look-back attention patterns. The linear probe results showing >90% accuracy for 2nd-order chunk identity decoding are robust and directly observable from the data.

**Medium Confidence**: The characterization of adaptive induction heads that selectively attend to contextually appropriate successors rather than uniformly to all past occurrences. While the evidence from attention pattern analysis and learning scores is compelling, the distinction between "adaptive" and "static" induction heads relies on layer location patterns that could have alternative explanations.

**Low Confidence**: The generalizability of the identified circuit to natural language tasks and larger model families. The paper's validation is limited to models up to 3B parameters, and the natural language examples provided are minimal.

## Next Checks
1. **Natural Language Replication**: Test the circuit identification methodology on naturally occurring hierarchical structures in real text data (e.g., nested discourse structures, syntactic hierarchies) to validate whether the same mechanisms operate outside synthetic contexts.

2. **Cross-Model Scaling Study**: Evaluate the circuit's presence and effectiveness across a wider range of model sizes (including 10B+ parameter models) and architectures to determine whether the mechanism scales consistently or requires architectural modifications for larger models.

3. **Alternative Circuit Comparison**: Conduct ablation studies that simultaneously disable multiple circuit components (e.g., context matching heads and function vector heads) to determine whether observed effects are truly specific to the identified circuit or represent a broader class of hierarchical learning mechanisms.