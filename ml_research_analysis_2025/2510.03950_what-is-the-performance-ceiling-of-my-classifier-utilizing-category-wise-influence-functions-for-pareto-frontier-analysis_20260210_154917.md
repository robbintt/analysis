---
ver: rpa2
title: What Is The Performance Ceiling of My Classifier? Utilizing Category-Wise Influence
  Functions for Pareto Frontier Analysis
arxiv_id: '2510.03950'
source_url: https://arxiv.org/abs/2510.03950
tags:
- performance
- influence
- training
- samples
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of identifying a classifier's performance
  ceiling by analyzing category-wise tradeoffs rather than relying solely on overall
  accuracy. The authors propose category-wise influence functions, which generate
  an influence vector quantifying each training sample's impact across all classes.
---

# What Is The Performance Ceiling of My Classifier? Utilizing Category-Wise Influence Functions for Pareto Frontier Analysis

## Quick Facts
- **arXiv ID:** 2510.03950
- **Source URL:** https://arxiv.org/abs/2510.03950
- **Reference count:** 15
- **Primary result:** PARETO-LP-GA achieved up to 16.02% accuracy gains on target classes while maintaining minimal degradation on others.

## Executive Summary
This paper tackles the problem of identifying a classifier's performance ceiling by analyzing category-wise tradeoffs rather than relying solely on overall accuracy. The authors propose category-wise influence functions, which generate an influence vector quantifying each training sample's impact across all classes. Using these vectors, they develop a criterion to determine whether further improvements are possible and introduce PARETO-LP-GA, a linear programming-based sample reweighting framework combined with a genetic algorithm to achieve Pareto improvements across target classes while minimizing degradation in others.

## Method Summary
The approach introduces category-wise influence vectors that measure each training sample's impact across all classes. PARETO-LP-GA combines linear programming with genetic algorithms to optimize sample weights for Pareto improvements. The method first computes influence vectors using EKFAC approximations, then searches for threshold configurations via GA that enable LP-based sample reweighting to improve target classes while constraining non-target class degradation.

## Key Results
- Category-wise influence scores successfully predict performance changes with Spearman correlations exceeding 0.8
- PARETO-LP-GA achieved up to 16.02% accuracy gain on target classes in CIFAR-10 experiments
- The method maintained minimal accuracy loss (≤2.90%) on non-target classes while improving targets
- PCA analysis showed influence vectors do not lie on a hyperplane (variance ratio < 0.95), indicating room for improvement

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Category-wise influence vectors reveal whether a classifier has reached its Pareto-optimal performance ceiling across classes.
- Mechanism: For each training sample z, compute a K-dimensional influence vector P(z) where P_k(z) = I_θ(z, S_k) measures the impact on class k if z were removed. Samples with all positive components are jointly beneficial; all negative components are jointly detrimental; mixed signs indicate tradeoff regions.
- Core assumption: Hessian-based influence function approximations remain valid for deep learning non-convex optimization landscapes.
- Evidence anchors:
  - [abstract]: "we propose category-wise influence functions and introduce an influence vector that quantifies the impact of each training sample across all categories"
  - [section 3.3]: "if all values of P(z) > 0, the sample z is beneficial to all categories and if all values of P(z) ≤ 0, the sample z is detrimental to all categories"
  - [corpus]: Limited direct corpus support; related work uses influence for subpopulation shifts but not category-wise Pareto analysis.
- Break condition: If PCA shows first component explains nearly all variance (>0.95), influence vectors lie on a hyperplane and Pareto frontier may be reached.

### Mechanism 2
- Claim: Sample reweighting via linear programming can achieve Pareto improvements that individual sample removal cannot.
- Mechanism: Even when no single sample is jointly beneficial, combining samples via weights w_i can create effectively "new" samples in the joint positive region. Linear programming maximizes Σ_{k∈C_target} Σ_{z_i∈T} w_i P_k(z_i) subject to class-wise performance threshold constraints.
- Core assumption: Linear combinations of influence vectors accurately predict performance changes from weighted training.
- Evidence anchors:
  - [section 3.3]: "when samples are considered collectively as a set, combining certain samples can yield a new sample that falls into the joint positive region"
  - [section 5.2]: "explained variance ratio of the first principal component... consistently exceeds 0.2, indicating that the influence vectors do not fit a hyperplane"
  - [corpus]: No corpus papers specifically address LP-based sample reweighting for Pareto tradeoffs.
- Break condition: If LP becomes infeasible for any threshold configuration, the current model may be at or near its performance ceiling.

### Mechanism 3
- Claim: Genetic algorithm search over class-wise thresholds (α_k) enables optimization without manual threshold tuning.
- Mechanism: The fitness function F(α^g) heavily penalizes target class degradation (−∞) and accumulates non-target class degradation. GA iteratively evolves threshold populations to find configurations that maximize target improvement while minimizing collateral damage.
- Core assumption: The fitness landscape is sufficiently smooth for GA to find good solutions within G iterations.
- Evidence anchors:
  - [section 3.4]: "if performance for C_target decreases at all (i.e., Δ^{e+1}_k ≤ 0) the fitness is set to a large-magnitude negative value"
  - [Table 1]: Direct Improvement achieved +16.02% on class 0 while non-target classes saw ≤2.90% degradation
  - [corpus]: No corpus papers combine GA with influence-based LP; Verification Limits Code LLM Training addresses performance ceilings but via verification bottlenecks, not sample reweighting.
- Break condition: If fitness remains −∞ across all GA iterations, no threshold configuration allows Pareto improvement.

## Foundational Learning

- Concept: **Influence Functions (Koh & Liang 2017)**
  - Why needed here: Category-wise influence vectors build on standard influence function formulation I_θ(z_j, V) = Σ_{z∈V} ∇_θℓ(z; θ̂)^T H^{-1}_θ ∇_θℓ(z_j; θ̂). Without this foundation, the extension to per-class influence is opaque.
  - Quick check question: Can you explain why the Hessian inverse H^{-1}_θ is needed to estimate leave-one-out effects without retraining?

- Concept: **Pareto Frontier in Multi-Objective Optimization**
  - Why needed here: The paper's core question—"has the classifier reached its performance ceiling?"—is formally asking whether the current model is Pareto-optimal across class accuracies. Understanding this is essential for interpreting influence vector distributions.
  - Quick check question: In a 2-class setting, if sample A improves class 1 by 5% but hurts class 2 by 2%, and sample B does the opposite, what combinations might be Pareto-optimal?

- Concept: **Linear Programming Duality and Constraint Satisfaction**
  - Why needed here: PARETO-LP-GA formulates sample reweighting as an LP with class-wise threshold constraints. Understanding feasibility conditions helps diagnose when the performance ceiling is reached.
  - Quick check question: What does it mean if the LP solver returns "infeasible" for all α threshold configurations tried by the GA?

## Architecture Onboarding

- Component map: Influence Computation Module -> Pareto Ceiling Detector -> GA Controller -> LP Solver -> Weighted Retraining
- Critical path: Influence vector computation is the computational bottleneck—requires Hessian approximations via EKFAC for all n training samples across K classes.
- Design tradeoffs:
  - EKFAC vs. exact Hessian: EKFAC is faster but less accurate; paper uses EKFAC for scalability
  - GA population size vs. convergence: Larger populations explore more threshold configurations but increase LP solver calls
  - Target class selection: Paper manually selects based on low accuracy; could automate via per-class monitoring
- Failure signatures:
  - **Influence prediction fails:** Spearman correlation < 0.5 between cumulative influence and actual performance change → influence approximation unreliable
  - **LP always infeasible:** No α configuration yields feasible solution → model likely at Pareto frontier
  - **GA fitness stuck at −∞:** All threshold configurations cause target class degradation → either wrong target selection or fundamental data limitations
- First 3 experiments:
  1. **Synthetic validation:** Replicate Figure 2 on a 2-class linearly separable dataset with label noise. Verify that noisy samples have negative influence on both classes and that removal improves both accuracies.
  2. **Single-class Direct Improvement:** On CIFAR-10 with ResNet-9, identify the worst-performing class after epoch 10, run PARETO-LP-GA with that single target class, and measure accuracy change on target vs. non-target classes.
  3. **Course Correction:** Intentionally train past a "detrimental epoch" where some classes drop, then apply PARETO-LP-GA to recover those classes while measuring collateral damage. Compare recovered epoch to the original detrimental epoch.

## Open Questions the Paper Calls Out

- Question: What are the theoretical guarantees for determining whether a classifier has truly reached its Pareto frontier when no samples exist in joint positive or negative influence regions?
  - Basis in paper: [explicit] The authors state: "This question motivates further investigation into the geometric conditions and theoretical guarantees for achieving the Pareto frontier."
  - Why unresolved: The paper provides a necessary condition (influence vectors near the y=-x line) but does not prove sufficiency or establish formal convergence guarantees.
  - What evidence would resolve it: Formal theoretical analysis proving that specific geometric configurations of influence vectors are both necessary and sufficient for Pareto optimality, validated on diverse datasets.

- Question: How does the accuracy of influence function approximations (via EKFAC) affect the reliability of Pareto frontier estimation in large-scale models?
  - Basis in paper: [inferred] The paper acknowledges "ongoing research that studies their suitability to these models" regarding influence functions for deep non-convex models, while using EKFAC approximations without analyzing approximation error propagation.
  - Why unresolved: The relationship between Hessian approximation quality and the precision of category-wise influence vectors—and consequently Pareto frontier identification—remains unquantified.
  - What evidence would resolve it: Systematic ablation studies comparing exact vs. approximated influence functions on frontier estimation accuracy across model scales.

- Question: How does PARETO-LP-GA scale computationally with increasing dataset size, number of classes, and model parameters?
  - Basis in paper: [inferred] The method combines genetic algorithm search (20 iterations, population 24) with repeated LP solving and model retraining, evaluated only on small-to-medium datasets (CIFAR-10, STL-10) with 10 classes.
  - Why unresolved: Computational complexity analysis is absent, and it is unclear whether the approach remains feasible for datasets with hundreds of classes or millions of samples.
  - What evidence would resolve it: Scaling experiments on larger datasets (e.g., ImageNet, large-scale text corpora) with runtime and memory profiling.

## Limitations

- The approach depends critically on the accuracy of influence function approximations for deep neural networks, which may not be reliable in non-convex optimization landscapes
- The method's computational cost scales poorly with dataset size due to the need to compute influence vectors for all training samples
- The GA-based threshold optimization lacks theoretical guarantees about convergence to global optima and hyperparameter choices appear somewhat arbitrary

## Confidence

**High Confidence:** The empirical results showing category-wise improvements up to 16.02% accuracy gains while maintaining minimal degradation on non-target classes. The Spearman correlation measurements exceeding 0.8 provide strong evidence that influence vectors predict performance changes.

**Medium Confidence:** The theoretical framework connecting influence vectors to Pareto frontiers. While the mathematical formulation is sound, the practical applicability depends heavily on the validity of influence function approximations in deep learning contexts.

**Low Confidence:** The generalizability of the approach across diverse model architectures and the robustness of the GA optimization to hyperparameter choices. The paper lacks ablation studies on GA parameters or systematic evaluation across different neural network families.

## Next Checks

1. **Influence Prediction Validation:** Systematically evaluate Spearman correlation between influence vectors and actual performance changes across different architectures (CNNs, transformers, MLPs) and dataset complexities to establish the reliability bounds of the influence approximation.

2. **Pareto Frontier Detection:** Implement the proposed PCA-based ceiling detection method on multiple training runs with varying dataset sizes to empirically validate whether variance > 0.95 in the first principal component consistently indicates proximity to the performance ceiling.

3. **Computational Scalability Test:** Measure the runtime and memory requirements of computing influence vectors for different dataset sizes (10K, 100K, 1M samples) to quantify the practical scalability limits and identify potential approximation strategies for large-scale applications.