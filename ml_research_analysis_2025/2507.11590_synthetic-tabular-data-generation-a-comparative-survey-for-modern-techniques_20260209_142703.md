---
ver: rpa2
title: 'Synthetic Tabular Data Generation: A Comparative Survey for Modern Techniques'
arxiv_id: '2507.11590'
source_url: https://arxiv.org/abs/2507.11590
tags:
- data
- synthetic
- generation
- privacy
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey provides a comprehensive comparative analysis of synthetic
  tabular data generation techniques, focusing on methods that preserve feature dependencies,
  statistical fidelity, and privacy. It introduces a novel taxonomy based on practical
  generation objectives such as intended downstream applications, privacy guarantees,
  and data utility.
---

# Synthetic Tabular Data Generation: A Comparative Survey for Modern Techniques

## Quick Facts
- **arXiv ID:** 2507.11590
- **Source URL:** https://arxiv.org/abs/2507.11590
- **Reference count:** 40
- **Key outcome:** Comprehensive comparative analysis of synthetic tabular data generation techniques, benchmarking models like CTGAN, FCT-GAN, TABSYN, and privacy-preserving variants across two real-world datasets, CreditRisk and Adult.

## Executive Summary
This survey provides a comprehensive comparative analysis of synthetic tabular data generation techniques, focusing on methods that preserve feature dependencies, statistical fidelity, and privacy. It introduces a novel taxonomy based on practical generation objectives such as intended downstream applications, privacy guarantees, and data utility. The paper benchmarks multiple models—including CTGAN, FCT-GAN, TABSYN, and privacy-preserving variants like DP-CTGAN and PATE-GAN—across two real-world datasets, CreditRisk and Adult. Results show that TABSYN excels in feature dependency preservation with minimal statistical deviation, while privacy-preserving models like PATE-GAN and DP-CTGAN achieve strong privacy guarantees but suffer in utility. Overall, model selection should balance the trade-offs between privacy, fidelity, and downstream task performance depending on dataset complexity and application requirements.

## Method Summary
The paper benchmarks six synthetic tabular data generation models (CTGAN, FCT-GAN, CTAB-GAN, TABSYN, DP-CTGAN, PATE-GAN) against two real datasets (Adult and CreditRisk). Models are trained once per dataset and generate 10 independent synthetic datasets of 10,000 samples each. Evaluation metrics include feature dependency preservation (Pearson correlation difference, uncertainty coefficient difference, correlation ratio difference), statistical similarity (Jensen-Shannon divergence, Wasserstein distance), ML utility (downstream classification performance using RandomForest), and privacy (vulnerability to model inversion attacks). Privacy-preserving models use specific epsilon budgets (PATE-GAN: ε=1.0, DP-CTGAN: ε=2.0). The evaluation pipeline uses libraries like snsynth for DP models and sdv for CTGAN, with TABSYN evaluated separately.

## Key Results
- TABSYN achieves the lowest statistical deviation and best feature dependency preservation among all evaluated models
- Privacy-preserving models (PATE-GAN, DP-CTGAN) provide strong privacy guarantees but suffer significant utility degradation
- CTGAN and FCT-GAN show complete categorical distribution mismatch (JSD = 1.0) on the Adult dataset
- Model selection requires balancing tradeoffs between privacy, fidelity, and downstream task performance based on specific application requirements

## Why This Works (Mechanism)

### Mechanism 1: Distribution Estimation and Sampling
Synthetic tabular data generation works by learning an approximation of the original data distribution P(X), then sampling new records from this learned distribution. Generative models (GANs, VAEs, diffusion models, or statistical methods) estimate P(X) by minimizing divergence between the learned distribution P̂(X) and the true distribution. Sampling from P̂(X) produces synthetic records that retain statistical properties of the original data. This approach assumes the original dataset is sufficiently representative of the true underlying distribution, and the model capacity is adequate to capture relevant feature dependencies.

### Mechanism 2: Conditional Generation for Feature Dependency Preservation
Conditioning generation on specific feature values enables models to preserve complex, non-linear dependencies between mixed-type variables (numerical, categorical). Models like CTGAN use conditional vectors to guide generation based on categorical feature values, while mode-specific normalization transforms numerical features according to their distribution within each category. This prevents the generator from ignoring minority categories and maintains realistic feature correlations. This mechanism assumes feature dependencies in the training data are stable and representative of the population, and conditioning variables are informative of target distributions.

### Mechanism 3: Differential Privacy Noise Injection
Adding calibrated noise to gradients or model outputs limits the influence of any individual training record, providing formal privacy guarantees at the cost of reduced data fidelity. Privacy-preserving models (DP-CTGAN, PATE-GAN) inject Gaussian or Laplacian noise proportional to gradient sensitivity. The privacy budget (ε, δ) bounds cumulative privacy loss via moments accountant tracking. Higher noise (lower ε) increases privacy but distorts learned distributions. This mechanism assumes the sensitivity of queries can be bounded, and privacy budget allocation is appropriately distributed across training iterations.

## Foundational Learning

- **Probability Distribution Estimation**: Understanding maximum likelihood, divergence measures (KL, Jensen-Shannon), and sampling is essential since all synthetic data methods fundamentally approximate P(X). Quick check: Given a dataset with multimodal income distribution, would you use a single Gaussian or a mixture model, and why?

- **Generative Model Architectures (GANs, VAEs, Diffusion)**: The paper benchmarks architectures with different inductive biases; selecting the right one requires understanding adversarial training, latent space structure, and iterative denoising. Quick check: What is mode collapse in GANs, and how does CTGAN's conditional generation mitigate it?

- **Differential Privacy Basics (ε, δ, sensitivity)**: Privacy-preserving models require parameterizing noise injection; misconfiguring privacy budget can render synthetic data either non-private or useless. Quick check: If you set ε = 0.1 vs. ε = 10, which gives stronger privacy, and what is the expected impact on synthetic data quality?

## Architecture Onboarding

- **Component map**: Data preprocessing pipeline -> Generative model core (GAN/VAE/diffusion) -> Conditioning module -> Privacy mechanism -> Evaluation suite

- **Critical path**: 1) Dataset characterization (feature types, distributions, dependencies, imbalance) 2) Model selection aligned with objectives (fidelity vs. privacy vs. downstream utility) 3) Training with appropriate hyperparameters 4) Generation of synthetic samples (typically 10,000+ for stable evaluation) 5) Multi-metric evaluation across at least 10 independent generation runs

- **Design tradeoffs**: Fidelity vs. Privacy (TABSYN achieves lowest statistical deviation but highest privacy vulnerability; PATE-GAN/DP-CTGAN offer strongest privacy but lowest utility), Complexity vs. Scalability (Diffusion models capture intricate dependencies but are computationally intensive; statistical models are faster but may miss non-linear relationships), Conditional Control vs. Generality (Fine-grained conditioning enables targeted generation but requires more training data and tuning)

- **Failure signatures**: Jensen-Shannon Divergence = 1.0 indicates complete categorical distribution mismatch, Privacy AUROC > 0.85 suggests model overfitting to training records, High variance across generation runs indicates unstable training dynamics under differential privacy, Downstream ML utility metrics dropping below 0.5 signals synthetic data has lost predictive signal

- **First 3 experiments**: 1) Baseline fidelity assessment: Train CTGAN on CreditRisk dataset, generate 10 synthetic datasets of 10,000 samples each, compute Pearson correlation difference, JSD, and Wasserstein distance to establish baseline fidelity metrics 2) Privacy-utility tradeoff exploration: Train DP-CTGAN with privacy budgets ε ∈ {0.5, 1.0, 2.0, 5.0}, evaluate synthetic data using model inversion attacks (AUROC) and downstream classification accuracy to quantify tradeoff curve 3) Feature dependency stress test: Apply TABSYN and CTGAN to a dataset with known complex conditional dependencies, measure correlation ratio difference and conditional distribution divergence to assess which architecture better captures multi-way feature interactions

## Open Questions the Paper Calls Out

- **Standardized Evaluation Framework**: How can the field establish a standardized, domain-specific evaluation framework that correlates synthetic data quality metrics with real-world downstream task performance? The authors identify the lack of consensus on comprehensive, domain-specific metrics as a critical limitation, noting that traditional measures like distributional similarity may not reflect utility in high-stakes applications.

- **Decoupling Privacy from Utility Degradation**: Can architectural innovations effectively decouple strong differential privacy guarantees from the degradation of feature dependencies and statistical fidelity in complex tabular datasets? While the paper explicitly discusses the privacy-utility trade-off, the benchmarking results show that privacy-preserving models suffer significant drops in feature dependency preservation compared to non-private models like TABSYN.

- **Model Interpretability for Regulatory Compliance**: What techniques can render the internal generation logic of deep generative models (e.g., GANs, Diffusion) interpretable and auditable to satisfy regulatory requirements? The authors state that the interpretability of generative models remains a barrier to adoption because state-of-the-art models operate as black boxes, limiting trust in regulated industries.

## Limitations

- **Unspecified Hyperparameters**: Exact training configurations (epochs, batch sizes, architectural dimensions) for several benchmarked models remain unspecified, affecting reproducibility
- **Preprocessing Strategy Ambiguity**: The specific strategy for handling missing values in preprocessing is not detailed, which could impact evaluation consistency
- **Attack Methodology Variability**: The exact implementation details of model inversion attacks used for privacy assessment are not specified, potentially affecting privacy vulnerability measurements

## Confidence

- **High confidence**: Taxonomy framework and methodological approach for evaluating synthetic data generation techniques
- **Medium confidence**: Comparative results due to potential hyperparameter variations across implementations
- **Medium confidence**: Privacy guarantees assessment, as attack methodologies may vary in sophistication

## Next Checks

1. **Hyperparameter Sensitivity Analysis**: Reproduce key experiments varying epochs (50-500) and batch sizes (128-1024) to quantify impact on fidelity and privacy metrics across all six benchmarked models

2. **Cross-Attack Privacy Validation**: Apply multiple model inversion attack variants (membership inference, attribute inference) to verify consistency of privacy vulnerability assessments, particularly for TABSYN and CTGAN

3. **Downstream Task Generalization**: Evaluate synthetic data utility beyond binary classification (e.g., multi-class, regression tasks) to assess broader applicability of observed performance tradeoffs