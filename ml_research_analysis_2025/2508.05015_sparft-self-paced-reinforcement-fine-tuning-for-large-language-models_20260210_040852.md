---
ver: rpa2
title: 'SPaRFT: Self-Paced Reinforcement Fine-Tuning for Large Language Models'
arxiv_id: '2508.05015'
source_url: https://arxiv.org/abs/2508.05015
tags:
- training
- examples
- sparft
- difficulty
- cluster
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the challenge of training small language\
  \ models for reasoning tasks, where standard reinforcement learning fine-tuning\
  \ (RFT) methods require extensive data and compute. The proposed SPaRFT framework\
  \ introduces a two-stage approach: first, it clusters training data by semantics\
  \ and difficulty to reduce redundancy while maintaining diversity, then uses a multi-armed\
  \ bandit to dynamically select the most informative examples based on the model\u2019\
  s current performance."
---

# SPaRFT: Self-Paced Reinforcement Fine-Tuning for Large Language Models

## Quick Facts
- arXiv ID: 2508.05015
- Source URL: https://arxiv.org/abs/2508.05015
- Reference count: 40
- Primary result: Achieves 79.5% accuracy on GSM8K using only 100 examples, outperforming standard R1 (77.9%) with 100× fewer training samples

## Executive Summary
SPaRFT introduces a self-paced reinforcement fine-tuning framework designed to address the inefficiency of standard RFT when applied to small language models for reasoning tasks. The approach leverages semantic clustering and difficulty-based sampling to create a dynamic, curriculum-driven training process that adapts to the model's current capabilities. By using a multi-armed bandit to select the most informative examples, SPaRFT achieves state-of-the-art reasoning performance while dramatically reducing the number of training samples required. Experiments demonstrate that tiny models (<1B parameters) trained with SPaRFT can match or exceed the accuracy of larger, standard RFT-trained models on mathematical reasoning benchmarks.

## Method Summary
SPaRFT operates in two stages: first, it clusters training data by semantic similarity and difficulty to reduce redundancy while maintaining diversity; second, it employs a multi-armed bandit algorithm to dynamically select the most informative examples based on the model's current performance. This self-paced curriculum ensures that the model focuses on examples it is ready to learn from, avoiding wasted effort on either too-easy or too-hard samples. The framework is lightweight, adds minimal computational overhead, and is designed specifically for small language models, where standard RFT methods are typically inefficient due to high data and compute requirements.

## Key Results
- SPaRFT achieves 79.5% accuracy on GSM8K using only 100 examples, compared to 77.9% for standard R1 using thousands of samples
- Up to 100× reduction in training samples while maintaining or improving accuracy on mathematical reasoning tasks
- Effective across diverse datasets and model types, including tiny models (<1B parameters)

## Why This Works (Mechanism)
SPaRFT works by aligning the training curriculum with the model's evolving capabilities. The semantic clustering ensures that each training batch covers a diverse set of reasoning patterns, preventing overfitting to narrow problem types. Difficulty-based sampling, guided by the multi-armed bandit, ensures that the model is continually challenged at the right level—neither bored by easy problems nor overwhelmed by hard ones. This dynamic adjustment allows for more efficient learning, as the model spends its computational budget on the most impactful examples. The two-stage process—clustering followed by adaptive sampling—creates a feedback loop that accelerates skill acquisition in reasoning tasks.

## Foundational Learning
- Reinforcement Learning Fine-Tuning (RFT): Needed for adapting language models to maximize reward signals; quick check: ensure reward function is well-defined for reasoning tasks
- Semantic Clustering: Needed to reduce redundancy and maintain diversity in training data; quick check: verify clusters capture distinct reasoning patterns
- Multi-Armed Bandit Algorithms: Needed for dynamic example selection based on model performance; quick check: bandit reward estimates should reflect actual learning progress
- Self-Paced Learning: Needed to adaptively adjust curriculum difficulty; quick check: model performance should improve monotonically as difficulty increases

## Architecture Onboarding

**Component Map**
Data Clustering -> Multi-Armed Bandit -> Example Selection -> Model Training -> Performance Evaluation -> Bandit Reward Update

**Critical Path**
The most time-sensitive sequence is Example Selection -> Model Training -> Performance Evaluation -> Bandit Reward Update, as this loop directly controls the pace and focus of learning.

**Design Tradeoffs**
SPaRFT trades off raw data volume for intelligent sample selection, reducing training costs at the expense of added complexity in curriculum management. The lightweight bandit mechanism adds minimal overhead but requires careful tuning of reward functions and sampling strategies.

**Failure Signatures**
- If clustering is too coarse, the model may overfit to narrow problem types
- If the bandit is too greedy, the model may skip important foundational examples
- If difficulty estimates are inaccurate, the curriculum may be too easy or too hard, stalling progress

**First Experiments**
1. Verify that semantic clusters capture distinct reasoning patterns by sampling and manual inspection
2. Test bandit reward estimation by plotting model performance against selected example difficulties
3. Run ablation studies to isolate the impact of clustering and bandit sampling on final accuracy

## Open Questions the Paper Calls Out
None

## Limitations
- Limited evaluation to tiny models (<1B parameters), raising questions about scalability to larger models
- Narrow focus on mathematical reasoning tasks, with unclear generalizability to other domains
- Lack of thorough exploration of hyperparameter sensitivity and dataset-specific effects

## Confidence
- Model scalability: Low
- Dataset generalizability: Low
- Efficiency claims: Medium
- Experimental robustness: Medium

## Next Checks
1. Evaluate SPaRFT on models in the 7B-70B parameter range to test scalability and robustness
2. Conduct ablation studies to isolate the contributions of clustering, difficulty-based sampling, and the multi-armed bandit to overall performance
3. Test SPaRFT across diverse reasoning domains beyond mathematics (e.g., logical inference, commonsense reasoning) to assess generalizability