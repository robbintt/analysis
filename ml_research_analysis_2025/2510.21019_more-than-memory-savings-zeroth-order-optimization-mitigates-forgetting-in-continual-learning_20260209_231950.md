---
ver: rpa2
title: 'More Than Memory Savings: Zeroth-Order Optimization Mitigates Forgetting in
  Continual Learning'
arxiv_id: '2510.21019'
source_url: https://arxiv.org/abs/2510.21019
tags:
- optimization
- methods
- zo-fc
- learning
- classifier
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates zeroth-order (ZO) optimization for continual
  learning to address the plasticity-stability-efficiency trilemma. Theoretical analysis
  shows that ZO optimization naturally promotes flatter loss landscapes, reducing
  catastrophic forgetting.
---

# More Than Memory Savings: Zeroth-Order Optimization Mitigates Forgetting in Continual Learning

## Quick Facts
- arXiv ID: 2510.21019
- Source URL: https://arxiv.org/abs/2510.21019
- Reference count: 40
- This work proposes ZO-FC, a method achieving state-of-the-art accuracy with up to 6× memory reduction in continual learning

## Executive Summary
This paper investigates zeroth-order (ZO) optimization for continual learning, addressing the fundamental plasticity-stability-efficiency trilemma. Theoretical analysis reveals that ZO optimization naturally promotes flatter loss landscapes, which reduces catastrophic forgetting. However, naive application of ZO to learnable classifiers fails due to noisy gradient estimates impairing plasticity. The authors propose ZO-FC, which applies ZO optimization to a single adapter module while retaining first-order optimization for the classifier, achieving accuracy comparable to state-of-the-art methods while reducing training memory by up to 6×.

## Method Summary
The authors propose ZO-FC, which applies SPSA-based zeroth-order stochastic gradient descent (ZO-SGD) to a LoRA-like adapter module (rank=5) parallel to MLP layers in the first 5 transformer blocks of a frozen ViT backbone, while using first-order SGD with cosine decay for the classifier only. The method is evaluated on CIFAR100, ImageNet-R, and DomainNet with 5/10-class incremental settings. The key insight is that applying ZO optimization only to the adapter module maintains the benefits of flatter minima while avoiding the plasticity degradation that occurs when ZO is applied to the classifier.

## Key Results
- ZO-FC achieves accuracy comparable to state-of-the-art first-order methods (InfLoRA, EASE) while reducing training memory by up to 6×
- The method consistently exhibits low forgetting and better stability across all evaluated benchmarks
- Naive application of ZO optimization to both adapter and classifier fails catastrophically, validating the hybrid approach
- Theoretical analysis shows ZO optimization naturally promotes flatter loss landscapes, reducing catastrophic forgetting

## Why This Works (Mechanism)
The core mechanism relies on the observation that zeroth-order optimization naturally produces flatter loss landscape minima compared to first-order methods. This flatness property reduces catastrophic forgetting in continual learning by creating more stable representations that generalize better across tasks. However, the noisy gradient estimates inherent in ZO optimization impair the plasticity needed for learning new classes, particularly when applied to the classifier. By restricting ZO to the adapter module only, ZO-FC preserves the flatness benefits while maintaining sufficient plasticity through first-order classifier updates.

## Foundational Learning
- **Catastrophic forgetting**: The tendency of neural networks to lose previously learned information when trained on new tasks. Why needed: The paper aims to mitigate this core problem in continual learning.
- **Zeroth-order optimization**: Optimization methods that only require function evaluations, not gradient information. Why needed: Provides memory efficiency and flatter minima properties crucial to the method.
- **Loss landscape flatness**: The curvature of the loss surface around a minimum. Why needed: Flatter minima are associated with better generalization and reduced forgetting.
- **Plasticity-stability trade-off**: The balance between learning new information (plasticity) and preserving old information (stability). Why needed: The fundamental challenge ZO-FC addresses in continual learning.
- **LoRA adapters**: Low-rank adaptation modules that modify pre-trained model behavior. Why needed: The target for ZO optimization in the proposed method.
- **SPSA (Simultaneous Perturbation Stochastic Approximation)**: A specific ZO optimization algorithm using finite-difference gradient estimates. Why needed: The practical implementation of ZO in ZO-FC.

## Architecture Onboarding
- **Component map**: Frozen ViT backbone -> Adapter modules (rank=5 LoRA) -> Classifier head -> Loss function
- **Critical path**: Input -> ViT backbone (frozen) -> Adapter forward pass -> Classifier forward pass -> Loss computation
- **Design tradeoffs**: Memory efficiency vs. accuracy (solved by hybrid FO/ZO approach); Adapter capacity vs. gradient noise (solved by restricting ZO to adapter only)
- **Failure signatures**: Naive ZO on classifier causes accuracy collapse; Insufficient query budget Q leads to noisy updates; Incorrect perturbation magnitude ε destabilizes training
- **3 first experiments**: 1) Implement ZO-SGD on adapter only with Q=4, ε=0.001, compare memory usage to FO baseline; 2) Measure flatness metric (Eq. 8) for ZO vs FO during training; 3) Test classifier-only FO training vs full ZO-FC on CIFAR100 Inc-5

## Open Questions the Paper Calls Out
- **Open Question 1**: Can ZO-FC be effectively validated on harder fine-grained continual learning settings and demonstrated successfully on physical on-device training hardware? The authors state future work aims to validate on harder fine-grained CL settings and demonstrate on-device training, but current experiments use standard benchmarks and simulated resource constraints.
- **Open Question 2**: Can advanced ZO optimization strategies or increased query budgets enable ZO-only methods to fully match the strongest FO baselines without requiring a hybrid FO classifier? The authors note that fully matching strong FO methods may require very large Q or more advanced ZO optimization strategies.
- **Open Question 3**: Can ZO optimization be modified or regularized to successfully train prompt-based CL methods which currently suffer from divergence due to reliance on precise gradients? The paper shows ZO optimization fails on prompt-based methods due to their need for low-variance, directionally accurate gradients.

## Limitations
- The theoretical analysis connecting ZO optimization to flatter minima is primarily analytical rather than empirically validated across diverse architectures
- Performance claims would benefit from testing against a broader range of competing methods and datasets
- Current experiments utilize standard benchmarks rather than harder fine-grained continual learning settings
- The method's effectiveness on actual edge devices with strict hardware limitations remains unverified

## Confidence
- **High confidence**: Memory efficiency claims supported by direct GPU measurements; empirical observation that naive ZO substitution fails
- **Medium confidence**: Accuracy claims compared to state-of-the-art methods need broader validation; forgetting mitigation supported by metrics but could use longer task sequences
- **Medium confidence**: Theoretical analysis relies on assumptions about loss landscape geometry that may not hold universally

## Next Checks
1. Verify the separation of optimization strategies by implementing a variant where both classifier and adapter use FO-SGD, and compare memory usage and accuracy to confirm the 6× reduction is specifically due to ZO on the adapter
2. Test ZO-FC on additional continual learning benchmarks beyond the three used (e.g., CORe50, Split CIFAR-10/100) to assess generalizability across different task distributions and difficulty levels
3. Measure the actual flatness of minima during training using the SAM-based metric (Eq. 8) for both ZO and FO variants to empirically validate the theoretical claim about flatter landscapes promoting stability