---
ver: rpa2
title: 'PERC: a suite of software tools for the curation of cryoEM data with application
  to simulation, modelling and machine learning'
arxiv_id: '2503.13329'
source_url: https://arxiv.org/abs/2503.13329
tags:
- data
- empiarreader
- profet
- which
- empiar
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PERC provides open-source Python tools to streamline cryoEM data
  curation, addressing bottlenecks in accessing experimental and simulated datasets
  for machine learning and algorithm development. profet offers a unified interface
  to download protein structures from PDB or AlphaFold using Uniprot IDs, with caching
  and optional preprocessing like signal peptide removal.
---

# PERC: a suite of software tools for the curation of cryoEM data with application to simulation, modelling and machine learning

## Quick Facts
- arXiv ID: 2503.13329
- Source URL: https://arxiv.org/abs/2503.13329
- Reference count: 39
- Provides open-source Python tools to streamline cryoEM data curation, addressing bottlenecks in accessing experimental and simulated datasets for machine learning and algorithm development

## Executive Summary
PERC delivers three complementary Python tools—profet, EMPIARreader, and CAKED—to address cryoEM data curation bottlenecks. profet provides unified access to protein structures from PDB and AlphaFold via Uniprot IDs with caching. EMPIARreader enables lazy loading of multi-terabyte EMPIAR datasets without local storage through Dask-based streaming. CAKED extends PyTorch primitives to load, preprocess, and augment cryoEM images and metadata, automatically splitting data into training/validation sets. Together, these tools enable seamless workflows from raw data access to ML-ready inputs, accelerating cryoEM research and algorithm development.

## Method Summary
PERC consists of three open-source Python packages designed to streamline cryoEM data curation for machine learning workflows. profet offers a unified API to download protein structures from PDB or AlphaFold using Uniprot IDs, implementing fallback logic and local caching. EMPIARreader uses Dask-based lazy loading to stream large EMPIAR datasets directly into ML-compatible formats without requiring local storage. CAKED extends PyTorch Dataset and DataLoader primitives with cryoEM-specific transforms, preprocessing, and automatic train/validation splitting. The tools are designed to work together but can be used independently, with CAKED supporting custom transformations and memmap support planned for future releases.

## Key Results
- profet enables unified protein structure retrieval from PDB/AlphaFold with intelligent fallback and caching
- EMPIARreader allows ML experimentation on TB-scale EMPIAR datasets without local storage infrastructure
- CAKED integrates cryoEM-specific preprocessing into PyTorch training loops while maintaining standard iterator behavior

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Unified API abstraction reduces friction in accessing heterogeneous structure databases
- Mechanism: A single Fetcher class wraps PDB and AlphaFold APIs, implementing fallback logic (if unavailable in preferred source, try alternative) and local caching to eliminate redundant downloads across sessions
- Core assumption: Users primarily need programmatic read access via Uniprot IDs, not complex cross-database queries
- Evidence anchors:
  - [abstract]: "profet offers a unified interface to download protein structures from PDB or AlphaFold using Uniprot IDs, with caching"
  - [section 2.1]: "If the structure is not available from that source, profet will seek to download it from an alternative database"
  - [corpus]: Weak—neighbor papers focus on DL model architectures, not data access patterns
- Break condition: When workflows require database-specific metadata fields or advanced search filters not exposed in the unified interface

### Mechanism 2
- Claim: Lazy loading enables ML experimentation on multi-terabyte EMPIAR datasets without requiring local storage infrastructure
- Mechanism: Dask-based deferred computation materializes images only when read_partition() is called, streaming data on-demand via HTTP/FTP rather than bulk download
- Core assumption: Training or exploration can proceed with sequential or partition-wise access; random access across full corpus isn't required
- Evidence anchors:
  - [abstract]: "EMPIARreader enables lazy loading of large EMPIAR datasets directly into ML-compatible formats without local storage"
  - [section 2.2]: "The dataset is loaded lazily using Dask, so the images are loaded one at a time when 'read partition' is called"
  - [corpus]: Weak—corpus emphasizes model development, not data loading infrastructure
- Break condition: When training requires high-throughput random sampling across entire datasets, network latency becomes a bottleneck

### Mechanism 3
- Claim: Extending PyTorch Dataset/DataLoader primitives enables cryoEM-specific preprocessing to integrate seamlessly with existing ML training loops
- Mechanism: CAKED subclasses PyTorch primitives, injecting cryoEM-specific transforms (rescaling, normalization, augmentation) and automatic train/validation splitting while maintaining standard iterator interface
- Core assumption: Users are invested in PyTorch ecosystem and expect standard DataLoader behavior
- Evidence anchors:
  - [abstract]: "CAKED integrates with PyTorch to load, preprocess, and augment cryoEM images and metadata"
  - [section 2.3]: "The package extends standard PyTorch Dataset and DataLoader primitives"
  - [corpus]: Strong—multiple neighbor papers (Point transformer, Deep Learning Review) assume PyTorch-based workflows for cryoEM
- Break condition: When teams require TensorFlow/JAX or need custom batching strategies incompatible with PyTorch DataLoader semantics

## Foundational Learning

- Concept: CryoEM data characteristics (low SNR, particle averaging, TB-scale datasets)
  - Why needed here: Motivates why data curation tools are necessary—the signal-to-noise ratio and dataset scale create unique access bottlenecks
  - Quick check question: Why does cryoEM reconstruction require averaging thousands of particle images?

- Concept: Lazy loading and deferred computation (Dask)
  - Why needed here: Core mechanism for EMPIARreader; understanding when data is materialized is critical for debugging performance
  - Quick check question: What happens in memory when you call read_partition(10) on a lazy-loaded dataset?

- Concept: PyTorch Dataset/DataLoader contract
  - Why needed here: CAKED extends these primitives; understanding __len__, __getitem__, and transform composition is prerequisite for customization
  - Quick check question: Which methods must a PyTorch Dataset subclass implement?

## Architecture Onboarding

- Component map:
```
profet.Fetcher → [PDB API / AlphaFold API] → cached .pdb/.cif files
EMPIARreader.EmpiarSource → [EMPIAR FTP/HTTPS] → Dask array (lazy)
EMPIARreader.EmpiarCatalog → [EMPIAR XML metadata] → directory structure
CAKED.DiskDataLoader → [local MRC/Numpy files] → PyTorch DataLoader
```

- Critical path:
1. profet: Fetcher("pdb", save_directory="~/.pdb/") → get_file("4v1w", filetype="cif") → cached file returned
2. EMPIARreader: EmpiarSource(entry, directory="...", filename="*.mrc", regexp=True) → read_partition(n) → numpy array
3. CAKED: DiskDataLoader(classes=[...], transformations=[...]) → load(datapath) → iter(dataset) yields (image, label)

- Design tradeoffs:
  - Lazy loading trades initial latency for memory efficiency; unsuitable for high-throughput random access training
  - Unified API (profet) sacrifices database-specific query capabilities for simplicity
  - PyTorch-only (CAKED) limits audience but gains ecosystem compatibility; future memmap support noted for larger files

- Failure signatures:
  - profet: Uniprot ID not found in any configured database → raises exception
  - EMPIARreader: Network timeout mid-partition → partial read; no resume capability documented
  - CAKED: File naming without class prefix → label extraction fails; assumes {class}_{suffix} convention

- First 3 experiments:
1. Install profet via pip install profet, fetch structure "4v1w" as PDB format, verify second call hits cache (check search_history())
2. Use EMPIARreader CLI: empiarreader search --entry 10934 --select "*" --dir "data" to explore directory structure, then download a single file via glob pattern
3. Clone CAKED, load a small local MRC dataset with class-prefixed naming, iterate through DataLoader and print image tensor shape and label for first 5 samples

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the CAKED architecture be successfully extended to perform on-the-fly data loading directly from online databases without intermediate local storage?
- Basis in paper: [inferred] Page 7 states that the template could be "extended for on-the-fly loading directly from the online databases bypassing the local storage in the future."
- Why unresolved: The current implementation of CAKED methods only considers loading data from local storage sources.
- What evidence would resolve it: Implementation of a streaming dataset class within CAKED that interfaces directly with APIs like EMPIARreader without writing to disk.

### Open Question 2
- Question: How can memory-mapped (memmap) file support be optimized for large cryoEM files within the CAKED pipeline?
- Basis in paper: [inferred] Page 7 notes that "For larger files, support for memmap objects will be added in future."
- Why unresolved: Large files present memory challenges that the current path-based loading mechanism does not fully address for all use cases.
- What evidence would resolve it: A software release including a memmap handler and benchmarks showing memory efficiency improvements for large tomographic datasets.

### Open Question 3
- Question: Does the use of EMPIARreader's lazy loading to access diverse training data result in improved machine learning algorithmic performance compared to static datasets?
- Basis in paper: [inferred] Page 6 states it is "envisioned" that the utility will "allow improved algorithmic performance by allowing fast and lightweight access to diverse training data."
- Why unresolved: The paper asserts the utility of the tool but does not provide experimental benchmarks quantifying performance gains in model accuracy or generalization.
- What evidence would resolve it: A comparative study measuring the accuracy of models trained via EMPIARreader streaming versus fixed local datasets.

## Limitations
- Reliance on external APIs (PDB, AlphaFold, EMPIAR) and network infrastructure creates dependencies outside authors' control
- EMPIARreader's lazy loading trades memory efficiency for sequential access speed, unsuitable for high-throughput random sampling workflows
- CAKED's PyTorch-only implementation limits applicability to teams using alternative ML frameworks like TensorFlow or JAX

## Confidence
- High: Unified API abstraction in profet correctly implements fallback logic between PDB and AlphaFold, with local caching confirmed via search_history() tracking
- Medium: Lazy loading mechanism works as described for sequential access, but performance under realistic multi-TB dataset loads remains unverified without external testing
- Low: PyTorch integration claims for CAKED are mechanically correct, but absence of sample datasets and undefined example variables prevents validation of complete workflow

## Next Checks
1. Test profet's cache hit rate by fetching the same Uniprot ID twice and comparing file timestamps vs. download duration
2. Benchmark EMPIARreader's read_partition() latency on small vs. large EMPIAR entries to quantify the sequential access tradeoff
3. Construct a minimal MRC dataset with class-prefixed naming and validate CAKED's automatic label extraction and DataLoader iteration behavior