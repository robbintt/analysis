---
ver: rpa2
title: 'Gradient Masters at BLP-2025 Task 1: Advancing Low-Resource NLP for Bengali
  using Ensemble-Based Adversarial Training for Hate Speech Detection'
arxiv_id: '2511.18324'
source_url: https://arxiv.org/abs/2511.18324
tags:
- hate
- speech
- bangla
- fgsm
- subtask
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses the challenge of detecting hate speech in low-resource
  Bengali social media text, which suffers from class imbalance, orthographic noise,
  and informal language. The authors propose an ensemble-based fine-tuning approach
  combining Bangla language models with adversarial training (FGSM) and text normalization
  to improve robustness and generalization.
---

# Gradient Masters at BLP-2025 Task 1: Advancing Low-Resource NLP for Bengali using Ensemble-Based Adversarial Training for Hate Speech Detection

## Quick Facts
- **arXiv ID**: 2511.18324
- **Source URL**: https://arxiv.org/abs/2511.18324
- **Reference count**: 9
- **Primary result**: Ensemble-based fine-tuning with FGSM adversarial training achieved 73.23% micro F1 (6th place) for hate-type and 73.28% micro F1 (3rd place) for target group classification on BLP-2025 Task 1

## Executive Summary
This study addresses hate speech detection in low-resource Bengali social media text using an ensemble-based fine-tuning approach. The authors combine Bangla language models with adversarial training (FGSM) and text normalization to improve robustness against class imbalance and orthographic noise. Their method achieved strong performance on both subtasks of the BLP-2025 competition, demonstrating that adversarial training and ensemble methods can effectively handle the challenges of low-resource language processing.

## Method Summary
The approach uses transformer-based models (BanglaBERT, MuRIL-large, XLM-R-large) fine-tuned with FGSM adversarial training on embedding space, combined with rule-based text normalization and 5-fold cross-validation. FGSM perturbs token embeddings during training to simulate realistic input variations, while normalization reduces orthographic variance. The ensemble aggregates predictions across K-fold models. The best configuration used BanglaBERT+FGSM+N for subtask 1A and MuRIL-large+K-fold for subtask 1B.

## Key Results
- Achieved 73.23% micro F1 score for hate-type classification (6th place)
- Achieved 73.28% micro F1 score for target group classification (3rd place)
- Error analysis revealed significant confusion between abusive and political hate categories
- Systematic misclassification of minority target groups (Community, Society) as neutral text

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FGSM adversarial training improves model robustness to noisy social media text by perturbing token embeddings during training.
- Mechanism: FGSM computes the gradient of loss with respect to embeddings and perturbs them in the direction that maximizes loss (Δ = ε·sign(∇J)), then minimizes a combined loss of clean and perturbed examples. This forces the model to learn smoother decision boundaries that resist small input variations.
- Core assumption: Typos, transliteration noise, and informal language in YouTube comments create small perturbations in embedding space that FGSM training can simulate.
- Evidence anchors:
  - [abstract] "adversarial fine-tuning using FGSM, perturbing token embeddings during training which simulates realistic input variations"
  - [section 4.1] "FGSM added 10-15% overhead per epoch due to gradient computations but remained lightweight"
  - [corpus] Neighbor paper by Fahim (BLP-2023) applied adversarial weight perturbation for Bangla sentiment, suggesting adversarial methods have precedent in this language domain.
- Break condition: If input noise primarily manifests at the token/subword level (unknown slang, code-switching) rather than embedding space, FGSM perturbations may not simulate realistic variations.

### Mechanism 2
- Claim: Rule-based text normalization reduces orthographic variance before tokenization, improving downstream model performance.
- Mechanism: Lightweight normalization rules standardize spelling variations and script inconsistencies common in informal Bangla text, reducing vocabulary fragmentation and improving subword token coverage.
- Core assumption: A significant portion of classification errors stems from inconsistent spelling rather than semantic ambiguity.
- Evidence anchors:
  - [abstract] "text normalization to improve robustness and generalization"
  - [section 5.1] "Text normalization (N) further boosted performance, particularly for BanglaBERT+N (74.32% Dev, 71.14% Test)"
  - [corpus] No direct corpus comparison of normalization techniques; evidence is limited to this paper's ablation.
- Break condition: If normalization is too aggressive and collapses semantically distinct spellings (dialect markers, intentional obfuscation), it may remove discriminative signal.

### Mechanism 3
- Claim: K-fold cross-validation with ensemble averaging improves generalization under severe class imbalance.
- Mechanism: Training multiple models on different data splits reduces variance in minority class predictions by exposing each fold to different class distributions, then averaging predictions.
- Core assumption: The minority class examples are sufficiently representative that splitting them across folds still provides meaningful training signal.
- Evidence anchors:
  - [section 5.1] "K-fold cross-validation (KF) improved performance by 2-3% across models, with MuRIL-large+KF reaching 73.61% on Dev"
  - [section 5.2] "MuRIL-large with K-fold cross-validation obtained the best scores of 74.96% on Dev and 73.44% on Test"
  - [corpus] Retriv team (BLP-2025) also used transformer ensembles, corroborating ensemble effectiveness for this shared task.
- Break condition: If minority classes have fewer than K examples or are highly clustered, K-fold may produce folds with zero minority examples or unrepresentative distributions.

## Foundational Learning

- Concept: **FGSM (Fast Gradient Sign Method)**
  - Why needed here: The paper applies FGSM as the core adversarial training mechanism; understanding how gradient-based perturbations work is essential to reproducing and debugging this approach.
  - Quick check question: Can you explain why FGSM perturbs embeddings rather than tokens, and what ε controls?

- Concept: **Class imbalance effects on gradient-based learning**
  - Why needed here: The dataset has extreme imbalance (Sexism: 0.34%, None: 56%); understanding how this affects loss gradients is critical for diagnosing why FGSM may help or hurt minority classes.
  - Quick check question: If the majority class dominates the loss gradient, what happens to FGSM perturbation direction?

- Concept: **Cross-lingual vs. monolingual transformer selection for low-resource languages**
  - Why needed here: The paper compares BanglaBERT (monolingual) against MuRIL and XLM-R (multilingual); understanding the tradeoffs guides model selection.
  - Quick check question: Why might a multilingual model like MuRIL outperform a monolingual model on underrepresented categories?

## Architecture Onboarding

- Component map:
  Input layer (Raw Bengali YouTube comment) → Text normalization → Tokenizer → Encoder (Pre-trained transformer) → Adversarial module (FGSM perturbation) → Classification head (Linear layer with softmax) → Ensemble aggregation

- Critical path:
  1. Data preprocessing (normalization must run before tokenization)
  2. Model initialization from pre-trained weights
  3. FGSM perturbation computation requires forward-backward pass through encoder
  4. K-fold training must preserve stratification for minority classes
  5. Final prediction via averaging softmax probabilities across fold models

- Design tradeoffs:
  - **FGSM vs. PGD/AWP**: FGSM is 50-57% faster but produces weaker perturbations; the authors explicitly traded robustness for computational feasibility on Colab/Kaggle.
  - **BanglaBERT vs. MuRIL-large**: BanglaBERT+FGSM+N won Subtask 1A (72.33%), but MuRIL+KF won Subtask 1B (73.44%); no single model dominated both tasks.
  - **External data augmentation**: Initially helped on Dev but hurt on Test due to domain mismatch; authors reverted to original data only.

- Failure signatures:
  - **Abusive ↔ Political Hate confusion** (229 and 216 misclassifications): Overlapping aggressive vocabulary; consider adding discourse-level features.
  - **Minority → None misclassification** (Community: 266, Society: 234 as None): Model is biased toward majority class; may need resampling or focal loss.
  - **Sexism TPR: 55.17%**: Only 16/29 correct; severe data scarcity (0.34%) may require targeted data collection rather than architectural fixes.

- First 3 experiments:
  1. **Reproduce baseline**: Fine-tune BanglaBERT on Subtask 1A without FGSM or normalization; target ~70.3% Test F1 to match paper baseline.
  2. **Ablate normalization**: Compare BanglaBERT+KF with vs. without normalization to isolate normalization's ~1-2% contribution.
  3. **FGSM hyperparameter sweep**: Test ε ∈ {0.05, 0.1, 0.2} and α ∈ {0.3, 0.5, 0.7} to validate authors' claim that ε=0.1, α=0.5 is optimal; monitor minority class TPR.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would advanced adversarial techniques (PGD, AWP, GAT) significantly improve robustness and minority-class performance over FGSM for Bengali hate speech detection?
- Basis in paper: [explicit] Authors state they "intended to explore more advanced adversarial techniques, such as Geometry-Aware Adversarial Training (GAT) and Adversarial Weight Perturbation (AWP)" but "these were infeasible due to computational constraints."
- Why unresolved: Training larger models already exceeded 12-hour session limits on available platforms (Kaggle, Colab); iterative attacks like PGD increase training time by ~50% compared to FGSM.
- What evidence would resolve it: Experiments comparing FGSM vs. PGD/AWP/GAT on the same model architectures with controlled computational resources, reporting both performance metrics and training overhead.

### Open Question 2
- Question: Can domain-adaptive transfer learning or targeted augmentation strategies overcome the observed domain mismatch between public Bengali hate speech datasets and YouTube-specific corpora?
- Basis in paper: [explicit] External data augmentation from WoNBias and Bangla Hate Speech Detection Dataset "yielded improved accuracy on the development set" but "performed poorly on the test set, likely due to domain mismatch."
- Why unresolved: The authors reverted to original training data but did not investigate techniques to align external data distributions with the YouTube domain.
- What evidence would resolve it: Systematic comparison of domain adaptation methods (e.g., adversarial domain adaptation, curriculum learning, in-domain pretraining) using the same external datasets.

### Open Question 3
- Question: What linguistic features or annotation refinements could reduce the systematic confusion between Abusive and Political Hate categories?
- Basis in paper: [inferred] Error analysis reveals 229 Abusive texts misclassified as Political Hate and 216 Political Hate texts misclassified as Abusive, attributed to "overlapping linguistic features such as aggressive vocabulary and derogatory terms."
- Why unresolved: The paper identifies the problem but does not propose category-specific feature engineering or hierarchical classification to disambiguate these adjacent categories.
- What evidence would resolve it: Feature importance analysis, annotation consistency review, or hierarchical multi-label classification experiments targeting these two categories specifically.

## Limitations

- Severe class imbalance (Sexism: 0.34%, None: 59.66%) fundamentally limits model performance on minority categories regardless of architecture
- Test set domain shift observed when external data augmentation was attempted, suggesting model may not generalize beyond training distribution
- FGSM adversarial training parameters (ε=0.1, α=0.5) were not optimized through systematic grid search

## Confidence

**High Confidence Claims:**
- The ensemble-based fine-tuning approach achieved 73.23% (1A) and 73.28% (1B) micro F1 on test set
- FGSM adversarial training improves robustness to noisy social media text through embedding perturbations
- Text normalization provides measurable performance gains (1-2% improvement)
- K-fold cross-validation with ensemble averaging improves generalization under class imbalance

**Medium Confidence Claims:**
- FGSM perturbation direction effectively simulates realistic input variations in Bengali social media text
- Rule-based normalization reduces orthographic variance without removing discriminative signal
- The specific FGSM parameters (ε=0.1, α=0.5) are optimal for this task
- External data augmentation consistently harms performance due to domain mismatch

**Low Confidence Claims:**
- The exact contribution of each ensemble component (FGSM, normalization, K-fold) to final performance
- Whether more advanced adversarial techniques would outperform FGSM given sufficient compute
- The generalizability of these findings to other low-resource languages or text domains

## Next Checks

1. **Ablation study validation**: Reproduce the exact experimental setup and systematically disable each component (FGSM, normalization, K-fold) to quantify individual contributions. Compare against paper's reported gains.

2. **Minority class performance audit**: Compute per-class F1 scores and confusion matrices for all six hate types and five target groups. Focus on Sexism (0.34%), Community, and Society classes to identify whether architecture or data scarcity is limiting factor.

3. **Adversarial method comparison**: Implement and compare FGSM against PGD and AWP methods using same computational budget. Measure both performance gains and training time overhead to validate authors' computational constraints claim.