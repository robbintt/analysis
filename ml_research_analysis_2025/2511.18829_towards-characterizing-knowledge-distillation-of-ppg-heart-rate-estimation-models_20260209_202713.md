---
ver: rpa2
title: Towards Characterizing Knowledge Distillation of PPG Heart Rate Estimation
  Models
arxiv_id: '2511.18829'
source_url: https://arxiv.org/abs/2511.18829
tags:
- distillation
- student
- performance
- size
- teacher
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work explores knowledge distillation to compress large photoplethysmography\
  \ (PPG) heart rate estimation models for edge deployment on wearables. It evaluates\
  \ four distillation strategies\u2014hard distillation, soft distillation, decoupled\
  \ knowledge distillation (DKD), and feature distillation\u2014across varying teacher\
  \ and student model capacities using a 1D-ResNet backbone."
---

# Towards Characterizing Knowledge Distillation of PPG Heart Rate Estimation Models

## Quick Facts
- arXiv ID: 2511.18829
- Source URL: https://arxiv.org/abs/2511.18829
- Reference count: 20
- Primary result: DKD distillation achieves best performance with 90% inference time and 60% memory reduction

## Executive Summary
This work explores knowledge distillation to compress large photoplethysmography (PPG) heart rate estimation models for edge deployment on wearables. It evaluates four distillation strategies—hard distillation, soft distillation, decoupled knowledge distillation (DKD), and feature distillation—across varying teacher and student model capacities using a 1D-ResNet backbone. DKD outperforms other strategies, achieving the best performance while reducing inference time by up to 90% and memory usage by up to 60% compared to the largest teacher model, with only a modest performance trade-off. Distilled models consistently surpass those trained from scratch. Scaling laws reveal predictable performance improvements with model size, though performance saturates at around 6 residual blocks.

## Method Summary
The study uses a 1D-ResNet backbone to perform heart rate estimation from PPG signals by framing regression as 180-class classification (30-210 BPM). Four distillation strategies are evaluated: hard distillation, soft distillation, decoupled knowledge distillation (DKD), and feature distillation. DKD uses separate weighting for target class (TCKD) and non-target class (NCKD) distillation components. The methodology includes training teacher models of varying sizes (2-12 residual blocks) and distilling them into smaller student models (1-10 blocks). Evaluation uses participant-independent 80/20 splits across three datasets (WildPPG, PPG-DaLiA, GalaxyPPG) with 300 training epochs and cross-entropy loss.

## Key Results
- DKD achieves the best performance among distillation strategies with optimal weighting of 8× for NCKD over TCKD
- Distilled models consistently outperform scratch-trained models of equivalent size
- 1-block student models achieve 90% inference time reduction and 60% memory reduction compared to 12-block teachers
- Performance scales predictably with model size but saturates around 6 residual blocks
- ResNet architectures outperform MLPs for PPG signal processing under distillation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DKD outperforms other strategies by separating target and non-target class distillation with flexible weighting
- Core assumption: Learning incorrect class relationships transfers more useful knowledge than matching target class alone
- Evidence: DKD shows clearest advantage with 8× NCKD weighting, particularly valuable for ordinal classification where classes are semantically ordered (30-210 BPM)

### Mechanism 2
- Claim: Distilled models outperform scratch training through richer supervisory signals
- Core assumption: Teacher probability distributions encode inter-class relationships beyond one-hot labels
- Evidence: Students mimic teacher's internal representation better than learning from sparse ground truth labels alone

### Mechanism 3
- Claim: ResNet architectures scale more efficiently than MLPs under distillation
- Core assumption: PPG task structure aligns with convolutional priors (local temporal patterns)
- Evidence: MLP-based students consistently underperform ResNet-based students at same parameter counts

## Foundational Learning

- Concept: **Knowledge Distillation Basics (Teacher-Student Framework)**
  - Why needed: Understanding what gets transferred and how loss functions differ across strategies
  - Quick check: Can you explain why soft labels contain more information than hard labels for a 3-class problem?

- Concept: **Temperature Scaling in Softmax**
  - Why needed: DKD uses τ=2; understanding how temperature controls probability distribution sharpness
  - Quick check: What happens to teacher output probabilities when temperature increases from 1 to 4?

- Concept: **1D Convolutions for Time-Series Signals**
  - Why needed: The backbone is 1D-ResNet; distinguishing 1D conv from 2D conv and understanding kernel-size effects
  - Quick check: Why might a 1D convolution with kernel size 7 process PPG differently than kernel size 3?

## Architecture Onboarding

- Component map: PPG signal (green channel, 25 Hz) → 8-second windows with 2-second stride → 200 samples/window → 1D-ResNet backbone → Classification layer (180 classes) → DKD loss (TCKD + NCKD + CE) → Student model

- Critical path: 1) Train teacher models across target capacity sweep, 2) Configure DKD hyperparameters (α=1, β=8, τ=2), 3) Train student models with combined DKD + CE loss, 4) Evaluate on held-out participants

- Design tradeoffs: Larger teachers help but may overfit; performance gains diminish beyond ~6 blocks. 1-block model saves 90% inference time but costs ~30% MAE degradation. ResNet students outperform MLP but may be harder to optimize on edge hardware.

- Failure signatures: Distilled student worse than scratch → check teacher quality; Hard distillation saturating early → expected (discrete labels lack ordinal information); Large teacher → degraded student → teacher may have overfit

- First 3 experiments: 1) Train 2-block, 6-block, 10-block ResNet teachers from scratch to verify baselines, 2) Fix 6-block teacher → 2-block student and sweep β from [1, 4, 8, 16] to confirm 8× NCKD weighting, 3) Train on WildPPG, evaluate on PPG-DaLiA to probe dataset transferability

## Open Questions the Paper Calls Out
- Does distilled model performance generalize across datasets when trained and tested on distinct data sources?
- Do self-supervised or foundation model teachers provide better distillation targets than supervised ResNet backbones?
- How do distilled models perform on actual wearable microprocessors regarding latency and power consumption?

## Limitations
- Architecture implementation details (filter counts, kernel sizes) are not fully specified
- Claims about optimal DKD hyperparameters appear tuned to specific conditions without broader validation
- Scaling law predictions (performance saturation at 6 blocks) are based on limited range without testing larger models

## Confidence
- High Confidence: DKD outperforms other distillation strategies; distillation consistently improves over scratch training
- Medium Confidence: 90% inference time reduction and 60% memory reduction claims
- Low Confidence: Optimal DKD hyperparameters being universally effective; scaling law predictions

## Next Checks
1. Implement exact 1D-ResNet configuration and verify parameter counts match reported ranges; compare teacher performance across architectural variants
2. Train best-performing distilled model on WildPPG, then evaluate on PPG-DaLiA and GalaxyPPG without fine-tuning to assess cross-dataset generalization
3. Perform systematic sweeps of DKD hyperparameters (α, β, τ) on held-out validation set to quantify performance variance and robustness