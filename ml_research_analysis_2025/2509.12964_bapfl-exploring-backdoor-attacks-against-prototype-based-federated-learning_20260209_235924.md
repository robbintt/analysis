---
ver: rpa2
title: 'BAPFL: Exploring Backdoor Attacks Against Prototype-based Federated Learning'
arxiv_id: '2509.12964'
source_url: https://arxiv.org/abs/2509.12964
tags:
- bapfl
- prototypes
- prototype
- attack
- benign
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the security of prototype-based federated
  learning (PFL) against backdoor attacks, which remains unexplored despite PFL's
  growing use in addressing data heterogeneity. Existing backdoor attacks fail in
  PFL due to limited influence of poisoned prototypes and data heterogeneity.
---

# BAPFL: Exploring Backdoor Attacks Against Prototype-based Federated Learning

## Quick Facts
- arXiv ID: 2509.12964
- Source URL: https://arxiv.org/abs/2509.12964
- Reference count: 15
- Primary result: Novel backdoor attack method (BAPFL) achieving 35%-75% improvement in attack success rate against prototype-based federated learning

## Executive Summary
This paper addresses the security vulnerabilities of prototype-based federated learning (PFL) systems against backdoor attacks. While PFL has emerged as a promising solution for handling data heterogeneity in federated learning, its security against malicious attacks remains unexplored. The authors identify that traditional backdoor attacks fail in PFL due to limited influence of poisoned prototypes and data heterogeneity issues. To address this gap, they propose BAPFL, a comprehensive backdoor attack framework specifically designed for PFL architectures. BAPFL combines prototype manipulation with trigger optimization to achieve high attack success rates while maintaining the main task performance.

## Method Summary
The authors propose BAPFL, a novel backdoor attack method specifically designed for prototype-based federated learning systems. BAPFL consists of two key components: the Prototype Poisoning Strategy (PPS) and the Trigger Optimization Mechanism (TOM). PPS manipulates global prototypes to push benign prototypes away from trigger prototypes, while TOM learns stealthy, label-specific triggers to align trigger prototypes with target label prototypes. The method addresses the unique challenges of PFL, where traditional backdoor attacks fail due to limited influence of poisoned prototypes and data heterogeneity. The approach is evaluated across multiple datasets (MNIST, FEMNIST, CIFAR-10) and PFL variants, demonstrating significant improvements in attack success rate while maintaining main task accuracy.

## Key Results
- BAPFL achieves 35%-75% improvement in attack success rate compared to traditional backdoor attacks
- Maintains main task accuracy while successfully poisoning PFL systems
- Demonstrates effectiveness across multiple datasets (MNIST, FEMNIST, CIFAR-10) and PFL variants
- Shows adaptability and stealthiness in prototype-based federated learning environments

## Why This Works (Mechanism)
BAPFL works by exploiting the fundamental mechanics of prototype-based federated learning. Traditional backdoor attacks fail in PFL because they cannot effectively manipulate the aggregated prototypes that serve as the central representation of each class. BAPFL overcomes this limitation through two complementary strategies: PPS manipulates the global prototype space by pushing benign prototypes away from trigger prototypes, creating space for the attack to succeed, while TOM optimizes trigger patterns that are both effective at misclassification and stealthy enough to avoid detection. The combination allows the attack to overcome data heterogeneity challenges and achieve consistent success across different PFL implementations.

## Foundational Learning

**Prototype-based Federated Learning**: PFL uses class prototypes as central representations instead of model weights, reducing communication overhead and handling data heterogeneity. Why needed: Understanding PFL mechanics is crucial for grasping why traditional attacks fail and how BAPFL exploits the system. Quick check: Can you explain how prototypes differ from model weights in federated learning?

**Backdoor Attacks**: Traditional backdoor attacks insert triggers into training data to cause misclassification during inference. Why needed: Provides context for why existing attack methods are insufficient for PFL and motivates the need for BAPFL. Quick check: What makes backdoor attacks effective in traditional ML but challenging in PFL?

**Prototype Poisoning Strategy**: PPS manipulates global prototypes to create favorable conditions for backdoor attacks by separating benign and trigger prototypes. Why needed: Core mechanism that enables BAPFL to overcome PFL's natural defenses against backdoor attacks. Quick check: How does PPS differ from simply poisoning individual client models?

**Trigger Optimization Mechanism**: TOM learns label-specific triggers that align trigger prototypes with target label prototypes while maintaining stealth. Why needed: Ensures the attack remains effective while avoiding detection through abnormal trigger patterns. Quick check: What characteristics make a trigger "stealthy" in this context?

## Architecture Onboarding

**Component Map**: Client Data -> PPS -> Global Prototypes -> TOM -> Trigger Prototypes -> Attack Success
**Critical Path**: The attack flows from client data poisoning through prototype manipulation to trigger optimization, with PPS and TOM working in tandem to achieve the final attack goal.
**Design Tradeoffs**: BAPFL balances attack effectiveness against stealth requirements, choosing to optimize trigger patterns that are effective yet subtle enough to avoid detection.
**Failure Signatures**: Attack failure occurs when PPS cannot sufficiently separate prototypes or when TOM fails to learn effective triggers, resulting in low attack success rates.
**First Experiments**: 1) Test PPS effectiveness in isolating trigger prototypes from benign ones, 2) Evaluate TOM's ability to generate label-specific triggers, 3) Measure attack success rate across different PFL variants

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions beyond the general need for further research into defenses against such attacks and exploration of BAPFL's effectiveness in more diverse real-world scenarios.

## Limitations
- Assumes centralized prototype aggregation, which may not hold in all PFL implementations
- Does not extensively explore defenses against BAPFL, leaving security gaps unaddressed
- Limited evaluation of attack effectiveness under realistic network conditions with communication delays

## Confidence
**High**: Effectiveness of Prototype Poisoning Strategy (PPS) and Trigger Optimization Mechanism (TOM) in controlled experimental settings
**Medium**: Stealthiness claims, as evaluation focuses primarily on main task accuracy rather than potential side-channel detections
**Low**: Generalizability to all PFL variants, since experiments cover a limited subset of available PFL approaches

## Next Checks
1. Test BAPFL against active defense mechanisms such as anomaly detection in prototype updates
2. Evaluate the attack's performance under realistic network conditions with communication delays and partial client participation
3. Assess whether the attack remains effective when clients use heterogeneous model architectures or feature extractors