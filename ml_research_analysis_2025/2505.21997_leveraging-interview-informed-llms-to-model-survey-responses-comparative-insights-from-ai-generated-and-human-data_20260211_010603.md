---
ver: rpa2
title: 'Leveraging Interview-Informed LLMs to Model Survey Responses: Comparative
  Insights from AI-Generated and Human Data'
arxiv_id: '2505.21997'
source_url: https://arxiv.org/abs/2505.21997
tags:
- responses
- survey
- llms
- prompt
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examined whether large language models (LLMs), guided
  by personal interviews, could reliably predict human survey responses using the
  Behavioral Regulations in Exercise Questionnaire (BREQ). Interview-informed prompts
  were designed to generate synthetic survey responses across three LLM chatbots (GPT-4,
  Claude 3.7, Gemini 2.0), two temperature settings (0 and 0.5), and four prompt configurations.
---

# Leveraging Interview-Informed LLMs to Model Survey Responses: Comparative Insights from AI-Generated and Human Data

## Quick Facts
- arXiv ID: 2505.21997
- Source URL: https://arxiv.org/abs/2505.21997
- Reference count: 0
- Primary result: Interview-informed prompts improve LLM alignment with human survey responses, but LLMs produce lower response variability than humans

## Executive Summary
This study examined whether large language models, guided by personal interviews, could reliably predict human survey responses using the Behavioral Regulations in Exercise Questionnaire (BREQ). Interview-informed prompts were designed to generate synthetic survey responses across three LLM chatbots (GPT-4, Claude 3.7, Gemini 2.0), two temperature settings (0 and 0.5), and four prompt configurations. Results showed that LLMs captured overall response patterns but produced responses with lower variability than humans. Incorporating interview data improved response diversity for some models (e.g., Claude, GPT), while well-crafted prompts and low-temperature settings enhanced alignment with human responses. Demographic information had less impact than interview content on alignment accuracy. Item-level analysis revealed higher discrepancies for negatively worded questions, suggesting LLMs struggle with emotional nuance. These findings underscore the potential of interview-informed LLMs to bridge qualitative and quantitative methodologies while revealing limitations in response variability, emotional interpretation, and psychometric fidelity.

## Method Summary
The study collected data from 19 after-school program staff who completed the BREQ-15 survey (6-point Likert) and participated in 15-25 minute semi-structured interviews, along with providing demographic information. Three LLMs (GPT-4, Claude 3.7, Gemini 2.0) were tested across 24 conditions: 4 prompt templates × 2 temperature settings. Prompts varied in content (baseline, interview-only, demographics-only, combined). Each LLM generated 15 Likert-scale responses per participant, which were compared to human responses using RMSE and correlation metrics. Analysis was conducted in R with code available via OSF.

## Key Results
- LLMs captured overall response patterns but produced responses with lower variability than humans
- Interview-informed prompts improved alignment for some models (Claude, GPT) compared to demographics-only prompts
- Low-temperature settings (0) enhanced alignment but artificially suppressed response variance
- Negatively worded items showed higher discrepancies, indicating LLM struggles with emotional nuance

## Why This Works (Mechanism)

### Mechanism 1: Context-Conditioned Persona Alignment
If LLMs are provided with rich, unstructured qualitative data (interviews), they condition their response distributions to align more closely with individual human respondents than when provided with demographic data alone. The model uses the semantic content of the interview to construct a high-dimensional "persona" embedding that shifts the prior probability of survey responses from a generic population mean to a context-specific estimate. This mechanism breaks down if the interview content is irrelevant to the survey construct or if the model's context window is exceeded.

### Mechanism 2: Low-Temperature Deterministic Sampling
Constraining the LLM's temperature setting (towards 0) improves the fidelity of synthetic survey responses by reducing stochastic noise in the selection of Likert-scale tokens. Lower temperature settings sharpen the output probability distribution, forcing the model to select the "most likely" response consistent with the persona prompt. This mechanism breaks if the human respondent is inherently inconsistent or random, as a deterministic low-temperature setting will fail to capture their specific variance.

### Mechanism 3: Semantic Polarity Bias
LLMs appear to process survey items based on surface-level semantic polarity rather than deep psychological constructs, leading to specific failures with negatively worded items. The model associates positive words with higher scores and negative words with lower scores or inconsistent mappings, failing to capture the specific "reverse-scored" logic often required in psychometric scales. This mechanism breaks down completely when a valid survey item uses negative phrasing to indicate a positive trait (or vice versa).

## Foundational Learning

- **Concept: Psychometric Validity vs. Item-Level Accuracy**
  - Why needed here: The paper demonstrates that an LLM can match human responses on individual items (accuracy) while failing to reproduce the overall test score structure (validity)
  - Quick check question: If an LLM predicts 14 out of 15 items correctly but miscalculates the aggregate score due to missed reverse-coding logic, has it "passed" the benchmark?

- **Concept: Prompt Abstraction & Persona Injection**
  - Why needed here: The study varies prompts to test information density and persona injection
  - Quick check question: Does adding demographic tokens or interview tokens result in a larger shift in the model's output distribution?

- **Concept: Response Variance & Distributional Alignment**
  - Why needed here: A key finding is that LLMs produce "lower variability" than humans
  - Quick check question: If an LLM generates 1,000 synthetic responses with a Standard Deviation of 0.5 while the human population has a SD of 1.2, can we use this data for population-level inference?

## Architecture Onboarding

- **Component map:** Raw Interview Transcripts + Survey Instrument (BREQ) -> De-identification + Token Counter -> Modular prompt templates (P1-P4) -> Commercial APIs (GPT-4, Claude, Gemini) -> RMSE Calculator + Correlation Matrix

- **Critical path:** Clean interview text to fit token limits (Max ~9,600 tokens observed) -> Select Temperature setting (0 for alignment, 0.5 for diversity) -> Construct Prompt (e.g., P-BR-PI) -> Parse Likert output (1-6 integers) -> Compute RMSE against ground truth

- **Design tradeoffs:**
  - Information Density vs. Noise: Adding full interview transcripts improves alignment but increases token cost and latency, with diminishing returns
  - Stability vs. Diversity: Temperature=0 improves alignment but artificially suppresses response variance

- **Failure signatures:**
  - The "Flat" Distribution: LLM outputs cluster around the mean with insufficient variance
  - The "Negativity" Inversion: High error rates on items containing words like "ashamed" or "failure"
  - The "Psychometric Drift": Item-level accuracy does not scale to test-level accuracy

- **First 3 experiments:**
  1. Baseline Variance Check: Run Prompt 1 (Baseline) N=100 times per model at Temp=0.5 to measure inherent "silicon sampling" variance against the human control group
  2. Ablation on Negative Items: Isolate Items 6, 7, and 11. Test if explicitly instructing the model on "reverse scoring" or "negative emotional interpretation" reduces RMSE
  3. Token Length Correlation: Plot Interview Token Count vs. Person-Level RMSE to verify if "relevance" matters more than "length"

## Open Questions the Paper Calls Out

### Open Question 1
What specific factors—model architecture, training data, or input prompt features—cause LLMs to produce survey responses with lower variability than humans?
- Basis in paper: Page 29 states that the "underlying causes of this limitation remain poorly understood"
- Why unresolved: The study identified the symptom (lower variance) but did not isolate the technical or input-based source
- What evidence would resolve it: Ablation studies or causal analysis linking specific model layers or data sources to output variance metrics

### Open Question 2
Which respondent attributes (e.g., psychographic, behavioral, or contextual variables) should be incorporated into prompts to optimize alignment with human responses?
- Basis in paper: Page 29 notes that identifying which attributes to include is a "central challenge"
- Why unresolved: The current study was limited to demographic and interview data
- What evidence would resolve it: Experimental comparisons of prompts containing isolated attributes against human response benchmarks

### Open Question 3
Can qualitative data analysis techniques be leveraged to identify language features that mitigate bias in LLM-generated outputs?
- Basis in paper: Page 30 explicitly calls for research to explore how "interview data to mitigate bias" using techniques like topic modeling
- Why unresolved: The study observed accuracy variations but did not establish a method for using qualitative features to correct these prediction biases
- What evidence would resolve it: A framework that correlates qualitative features (e.g., topic density) with prediction accuracy in underperforming subgroups

### Open Question 4
How can LLMs be instructed or modified to maintain the intended psychometric structure (e.g., subscale relationships) of multi-scale instruments?
- Basis in paper: Page 28 discusses high test-level RMSE and notes that while LLMs mimic item patterns, they "struggle to grasp the overall psychometric structure"
- Why unresolved: The LLMs generated item-level responses that appeared valid but failed to replicate the weighted structural equations of the instrument
- What evidence would resolve it: Confirmatory Factor Analysis (CFA) demonstrating equivalent factor loadings between LLM-generated and human datasets

## Limitations
- Small sample size (N=19) constrains generalizability and statistical power
- BREQ instrument's specific focus on exercise motivation may not generalize to other survey domains
- Reliance on commercial API endpoints introduces variability across different model versions

## Confidence

- **High Confidence**: Interview-informed prompts improve response alignment compared to demographic-only prompts
- **Medium Confidence**: Low-temperature settings enhance alignment while reducing variability
- **Medium Confidence**: Negatively worded items show higher discrepancies
- **Low Confidence**: Interview-informed LLMs can reliably bridge qualitative and quantitative methodologies

## Next Checks

1. **External Validation**: Test the interview-informed prompting approach on a different survey instrument (e.g., personality assessment) with N≥100 to verify generalizability of alignment improvements

2. **Distribution Fidelity Assessment**: Conduct statistical tests (e.g., Kolmogorov-Smirnov) comparing the full response distributions between human and LLM-generated data across all temperature settings

3. **Negative Item Handling**: Design an ablation study where the model is explicitly instructed on reverse-scoring logic for negative items, then measure whether RMSE reductions persist across multiple survey instruments