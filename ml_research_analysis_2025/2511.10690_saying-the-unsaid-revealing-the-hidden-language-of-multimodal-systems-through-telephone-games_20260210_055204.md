---
ver: rpa2
title: 'Saying the Unsaid: Revealing the Hidden Language of Multimodal Systems Through
  Telephone Games'
arxiv_id: '2511.10690'
source_url: https://arxiv.org/abs/2511.10690
tags:
- concept
- systems
- hidden
- multimodal
- concepts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes a test-time framework for revealing the hidden
  language of multimodal systems by strategically leveraging their preference bias.
  The framework employs multi-round "telephone games," where image-to-text and text-to-image
  transformations progressively shift outputs toward stronger concept connections
  in the system's hidden space.
---

# Saying the Unsaid: Revealing the Hidden Language of Multimodal Systems Through Telephone Games

## Quick Facts
- **arXiv ID**: 2511.10690
- **Source URL**: https://arxiv.org/abs/2511.10690
- **Reference count**: 17
- **Primary result**: Proposes telephone game framework to reveal hidden language of multimodal systems by quantifying concept connection strength through iterative image-to-text and text-to-image transformations

## Executive Summary
This work introduces a test-time interpretability framework that reveals the hidden language of multimodal systems through strategic use of telephone games. The framework leverages systems' inherent preference biases by iteratively transforming images to text and back, causing outputs to progressively shift toward concepts with stronger internal connections. By analyzing co-occurrence frequencies across multiple rounds and repetitions, the approach quantifies concept connection strengths that cannot be captured by traditional semantic or visual similarity metrics. Experiments demonstrate that hidden language patterns are consistent across different multimodal systems and can be used to identify preference biases, assess generalization, and discover stable pathways for fragile concept connections.

## Method Summary
The framework employs multi-round telephone games where an LLM generates a start description from a concept pair, a text-to-image model creates an image, and an image-to-text model describes it. This cycle repeats for n rounds across r repetitions. Co-occurrence frequency F(A,B) measures how often concepts A and B appear together in descriptions. The approach uses a Telescope dataset with 150 concepts forming simple-pattern pairs (2 concepts side-by-side) and complex-pattern pairs (specific fusion strategies). Reasoning-LLMs infer unexpected relationships beyond surface similarities, providing insights into multimodal systems' understanding of the world.

## Key Results
- Semantic and visual similarities fail to capture hidden language, with correlations of only 0.046 and 0.180 respectively
- Different multimodal systems show moderate correlation in hidden languages (0.475-0.506), suggesting convergence toward shared representations
- Bridge concepts can improve stability for fragile concept connections, reducing crash ratios from 0.74 to 0.43 for TV display patterns
- The framework reveals consistent preference biases across systems, enabling identification of stable concept pathways

## Why This Works (Mechanism)

### Mechanism 1
Multi-round telephone games amplify small preference biases into observable concept shifts. Image-to-text compression discards weakly connected concepts while text-to-image reconstruction preferentially synthesizes strongly connected concepts. Iterating this cycle progressively shifts outputs toward stronger internal connections, making hidden preferences observable. Core assumption: preference bias is consistent across rounds and compoundable through iteration.

### Mechanism 2
Co-occurrence frequency quantitatively measures concept connection strength in hidden space. Concepts surviving more rounds together have stronger internal connections. Fragile combinations degrade earlier while stable combinations persist. This captures training frequency effects and generalization capacity. Core assumption: concept drift is driven by hidden connection strength, not random output variation.

### Mechanism 3
Different multimodal systems show correlated hidden languages, suggesting convergence toward shared representations. Systems trained on similar data distributions develop comparable concept connection patterns. This aligns with the Platonic Representation Hypothesis—that scaling drives convergence toward modeling real-world statistical structure. Core assumption: training data similarities produce comparable internal representations across architectures.

## Foundational Learning

- **Concept: Preference Bias in Multimodal Systems**
  - Why needed here: Core assumption that systems prefer certain concept combinations during reconstruction, enabling drift-based probing
  - Quick check question: Why might a system interpret "cow and doughnut" as closer to "pig and doughnut" in feature space?

- **Concept: Test-Time Interpretability (Black-Box Probing)**
  - Why needed here: Framework operates without parameter access—uses only inputs and observable outputs to infer internal structure
  - Quick check question: What can you learn about a model using only its input-output behavior?

- **Concept: Hidden Language vs. Semantic/Visual Similarity**
  - Why needed here: Key finding that traditional similarity metrics (CLIP, ResNet) poorly capture hidden language (correlation ~0.04-0.18)
  - Quick check question: Why might "cow and coke" be strongly connected internally despite low semantic similarity?

## Architecture Onboarding

- **Component map**: LLM (start description) -> Text-to-Image -> Image-to-Text -> LLM (co-occurrence detection) -> Reasoning-LLM (pattern inference)
- **Critical path**: Telephone game loop execution and co-occurrence frequency aggregation across repetitions and rounds
- **Design tradeoffs**: Round number (more rounds improve signal but increase cost linearly); repetition count (more repetitions improve statistics but scale cost); concept pair selection (balanced vs. biased sampling)
- **Failure signatures**: Single-concept crash rate >24% (baseline fragility); co-occurrence frequency = 1.0 (no discriminative signal); high variance across repetitions indicates randomness dominance
- **First 3 experiments**:
  1. Run 5-round telephone game on 10 simple-pattern concept pairs from Telescope to validate drift behavior and confirm framework produces observable concept shifts
  2. Compare co-occurrence frequencies against CLIP semantic similarity to verify low correlation (~0.046), confirming hidden language is distinct from surface similarity
  3. Test bridge concept ("Cartoon Style" for TV display pattern) on 10 concepts to verify improved stability on fragile connections (crash ratio reduction from 0.74 to 0.43 per paper)

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the hidden language framework perform when extended to complex combinations of three or more concepts? The authors state they plan to explore complex combinations of more concepts in future work, leaving the exponential complexity of higher-order interactions unexplored.

- **Open Question 2**: To what extent does the finite number of telephone game rounds bias the estimation of true co-occurrence probability? The current metric relies on a fixed number of rounds (e.g., 5), which acts as an approximation that may not capture the asymptotic behavior of concept stability.

- **Open Question 3**: Which graph algorithms are most effective for identifying stable intermediary pathways between fragile concept connections? While the authors demonstrate the existence of "bridges" (e.g., "Cartoon Style" for TVs), they have not systematically evaluated algorithms for automating the discovery of these paths.

## Limitations

- The exact 150-concept vocabulary list and precise sampling methodology for the 400-pair subset are not provided, preventing exact reproduction
- The framework's dependence on API-based multimodal systems introduces inherent randomness that may obscure or distort the hidden language signal
- Claims about cross-system correlation revealing Platonic Representation Hypothesis convergence are suggestive but not conclusively demonstrated

## Confidence

- **High Confidence**: The telephone game framework works as a test-time interpretability tool for revealing observable concept shifts in multimodal systems
- **Medium Confidence**: Co-occurrence frequency captures meaningful concept connection strength, though the relationship between frequency and actual internal representation similarity requires further validation
- **Low Confidence**: Claims about cross-system correlation revealing Platonic Representation Hypothesis convergence are suggestive but not conclusively demonstrated

## Next Checks

1. **Vocabulary Reconstruction Validation**: Using common nouns from COCO/ImageNet, generate a 150-concept vocabulary and create all pairwise combinations. Run telephone games on this reconstructed dataset to verify whether the observed concept drift patterns (fragile vs. stable connections) replicate without the original Telescope pairs.

2. **Correlation Structure Verification**: Systematically test the correlation between co-occurrence frequency and CLIP semantic similarity across different concept pair categories (high semantic similarity vs. low semantic similarity). Confirm whether the paper's finding of ~0.046 correlation holds across multiple concept pair selections and sampling strategies.

3. **Bridge Concept Generalizability Test**: Beyond the TV display pattern, test bridge concepts on at least 20 different fragile concept pairs from varied semantic categories. Measure crash ratio reduction compared to baseline and verify whether stable pathways can be consistently discovered for arbitrary fragile connections.