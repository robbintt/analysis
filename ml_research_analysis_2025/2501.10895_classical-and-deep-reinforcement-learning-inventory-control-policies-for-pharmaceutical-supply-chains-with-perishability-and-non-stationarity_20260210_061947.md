---
ver: rpa2
title: Classical and Deep Reinforcement Learning Inventory Control Policies for Pharmaceutical
  Supply Chains with Perishability and Non-Stationarity
arxiv_id: '2501.10895'
source_url: https://arxiv.org/abs/2501.10895
tags:
- policy
- inventory
- policies
- cost
- lost
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies inventory control policies for pharmaceutical
  supply chains, addressing perishability, yield uncertainty, and non-stationary demand.
  The authors develop a realistic case study in collaboration with Bristol-Myers Squibb,
  benchmark three policies (order-up-to, projected inventory level, and deep reinforcement
  learning using PPO) against a human-driven baseline, and derive bounds-based procedures
  for optimizing OUT and PIL policy parameters.
---

# Classical and Deep Reinforcement Learning Inventory Control Policies for Pharmaceutical Supply Chains with Perishability and Non-Stationarity

## Quick Facts
- arXiv ID: 2501.10895
- Source URL: https://arxiv.org/abs/2501.10895
- Reference count: 9
- No single inventory control policy consistently outperforms others across pharmaceutical supply chain scenarios

## Executive Summary
This paper addresses the challenge of managing pharmaceutical inventory with perishability, yield uncertainty, and non-stationary demand. The authors develop a realistic case study with Bristol-Myers Squibb and benchmark three policies (order-up-to, projected inventory level, and deep reinforcement learning using PPO) against a human-driven baseline. The study finds that while all three implemented policies achieve lower average costs than the human-driven policy, they exhibit higher cost variability. The Projected Inventory Level policy demonstrates robust and consistent performance, while Order-Up-To struggles under high lost sales costs, and PPO excels in complex scenarios but requires significant computational effort.

## Method Summary
The authors implement a custom OpenAI Gym environment (SCIMAI-Gym) to simulate pharmaceutical supply chains with FIFO inventory management, yield uncertainty (0-10% loss), non-stationary demand following product lifecycles, and perishability (12 timestep lifetime). Three policies are evaluated: (1) Order-Up-To with bounds-based Monte Carlo optimization for safety stock parameters, (2) Projected Inventory Level incorporating expected expired stock and lost sales over lead time, and (3) PPO with enriched feature vectors containing projected inventory levels and demand forecasts. The policies are tested across two scenarios - synthetic 5-year lifecycles and real 20-year BMS data - using 2000 Monte Carlo episodes per evaluation.

## Key Results
- No single policy consistently outperforms others across all experiments
- All three implemented policies achieve lower average costs than the human-driven baseline
- PIL demonstrates robust and consistent performance across scenarios
- OUT policy struggles significantly when lost sales costs are high
- PPO excels in complex and variable scenarios but requires substantial computational resources

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The Projected Inventory Level (PIL) policy manages perishability and lost sales more robustly than standard Order-Up-To (OUT) policies by explicitly accounting for future stock degradation.
- **Mechanism:** Instead of ordering to a simple static level, PIL estimates the expected expired stock and lost sales over the lead time $L$. It then orders the quantity required to raise the *expected* inventory at time $t+L$ to a target level $U_t$ (Eq. 19), effectively netting out predicted waste.
- **Core assumption:** The system can accurately estimate the term $E[\sum (O_j - l_j)]$ (expected expiration minus lost sales) using the demand distribution rather than just mean values.
- **Evidence anchors:**
  - [abstract] "PIL demonstrates robust and consistent performance... while OUT struggles under high lost sales costs."
  - [section 4.3] "We propose incorporating the expected inventory level at timestep t+L into the PIL policy... [using] the true distribution of demand to estimate expired stock more accurately."
  - [corpus] Consistent with literature on structure-informed RL (e.g., arXiv:2507.22040) suggesting utilizing system structure improves performance over naive black-box methods.
- **Break condition:** When yield uncertainty is high or demand is highly non-stationary, causing the estimated expiration term to fluctuate wildly, potentially leading to unstable order quantities.

### Mechanism 2
- **Claim:** Deep Reinforcement Learning (PPO) can effectively manage non-stationary demand if the state representation is enriched with forward-looking features rather than raw inventory vectors.
- **Mechanism:** The authors replace the raw inventory vector input with a feature vector containing *estimated* future inventory levels ($E[x_{t+L}]$) and demand forecasts ($d[t:t+L]$). This reduces the burden on the neural network to learn lifecycle dynamics from scratch.
- **Core assumption:** Deterministic projections of inventory based on forecasts provide a sufficient statistic for the non-stationary lifecycle phase.
- **Evidence anchors:**
  - [section 4.4.1] "Preliminary experiments revealed that [using the inventory vector] results in poor performance... we propose enriching the feature vector by including a forecast of upcoming demands."
  - [corpus] arXiv:2507.22040 supports the view that structure-informed inputs (like forecasts) aid RL in inventory tasks.
- **Break condition:** If the demand forecast error $\xi_t$ is highly correlated or non-Gaussian in ways the deterministic projection ignores, the feature vector may mislead the agent.

### Mechanism 3
- **Claim:** Bounds-based search procedures allow for efficient optimization of classical policy parameters (safety stock) in complex environments where exact solutions are intractable.
- **Mechanism:** The authors derive theoretical lower and upper bounds for the optimal safety stock by simplifying the problem (e.g., ignoring batching/yield uncertainty). They then search within this constrained interval using Monte Carlo simulation.
- **Core assumption:** The optimal parameter for the complex (real) problem lies within the bounds derived from the simplified (theoretical) problem.
- **Evidence anchors:**
  - [section 4.1.2] "...optimal values for both the OUT and PIL policies consistently fall within this interval."
  - [section 4.1.1] "We adopt Monte Carlo simulation to determine the optimal safety stock... [using] bounds-based procedure."
  - [corpus] No direct corpus support for this specific bounds method; anchored primarily in the paper's theoretical derivations (Appendix B).
- **Break condition:** If batch costs $K(q_t)$ are very high, the assumption that they can be "omitted" in the bound derivation (Section 4.1) may fail, potentially shifting the true optimal outside the calculated bounds.

## Foundational Learning

- **Concept: Perishable Inventory Dynamics (FIFO/FEFO)**
  - **Why needed here:** Standard inventory theory often assumes infinite life. Here, items expire after $m$ timesteps, creating a "curse of dimensionality" where the state must track the age of every item.
  - **Quick check question:** Can you explain why a standard base-stock policy is suboptimal when inventory has a fixed expiration date? (Answer: It doesn't account for the age distribution of stock; ordering new stock when old stock is about to expire wastes money).

- **Concept: Non-Stationarity in Time Series**
  - **Why needed here:** Demand follows a product lifecycle (growth, maturity, decline). Policies must adapt safety stock dynamically as variance ($\sigma_t$) and mean ($d_t$) shift over time.
  - **Quick check question:** How does the "worst-case" setting ($\sigma_t \propto \max d_t$) differ from "balanced" noise, and why does it break simple static policies?

- **Concept: Actor-Critic (PPO)**
  - **Why needed here:** The paper uses Proximal Policy Optimization. Understanding the trade-off between the Actor (policy) and Critic (value estimation) and the clipping mechanism is required to debug training instability.
  - **Quick check question:** What is the function of the clipping parameter $\epsilon$ in the PPO objective? (Answer: It prevents destructively large policy updates).

## Architecture Onboarding

- **Component map:** Supply Chain Environment -> Feature Extractor -> Agents (PPO/Actor-Critic Neural Network, OUT/PIL Heuristics)
- **Critical path:**
  1. Define environment parameters ($L, m, \hat{z}$).
  2. **(If OUT/PIL):** Run 2000 Monte Carlo simulations to identify optimal $s$ or $u$ within theoretical bounds.
  3. **(If PPO):** Train agent using projected inventory features; validate against held-out demand trajectories.

- **Design tradeoffs:**
  - **PPO vs. PIL:** PPO excels in "complex/variable" scenarios (high yield uncertainty) but requires high compute. PIL is computationally cheap and robust but may underperform when complex multi-variable interactions exist.
  - **Cost vs. Service Level:** The paper notes automated policies minimize *cost* but have higher *variability* (stockout risk) compared to the human baseline, which prioritizes availability.

- **Failure signatures:**
  - **OUT Explosion:** Total cost spikes significantly when lost sales cost $b$ is high and lifetime $m$ is short (Figure 4/5).
  - **PPO Instability:** Divergence or high variance if the raw inventory vector is used instead of projected features.
  - **PIL Drift:** Sub-optimal performance when $E[O_j - l_j]$ estimation is inaccurate due to non-stationary noise.

- **First 3 experiments:**
  1. **Baseline Validation:** Reproduce the "First Scenario" (Section 5.1) for $m=2, w=2$. Verify that OUT performance degrades as $b$ increases while PIL remains stable.
  2. **Feature Ablation:** Train PPO with *only* the raw inventory vector vs. the proposed projected feature vector (Eq. 41). Confirm the performance drop described in Section 4.4.1.
  3. **Sensitivity Analysis:** In the "Second Scenario" (Real-world data), vary yield uncertainty $\hat{z} \in \{0, 0.1\}$ and observe if PPO outperforms PIL as complexity increases (Table 2).

## Open Questions the Paper Calls Out

- Question: Is the derived upper bound ($s^*_{UB}$) formally provable as a strict cap on the optimal order-up-to level for perishable products under lost sales with positive lead times?
- Basis in paper: [explicit] Section 4.1.2 states that while the upper bound is expected to act as a cap based on non-perishable literature, "providing formal proof of this conjecture in the context of perish

## Limitations

- Policy Parameterization: Bounds-based search relies on simplified theoretical problems that may not fully capture the true optimal solution when batch costs are significant.
- PPO Feature Engineering: Performance depends critically on human-engineered feature vectors rather than pure RL learning, with no exploration of alternative feature representations.
- Demand Forecast Quality: Both PIL and PPO performance hinge on accurate demand forecasts over lead time, but the paper doesn't model forecast error or explore robust policies under forecast uncertainty.

## Confidence

- **High Confidence:** The experimental methodology using 2000 Monte Carlo episodes per policy evaluation is robust. The comparative results showing PIL's consistent performance and OUT's failure under high lost sales costs are well-supported by the data.
- **Medium Confidence:** The PPO performance claims rely heavily on the specific feature engineering approach. While the ablation study shows raw inventory vectors perform poorly, the relative advantage of the proposed features versus other possible representations remains uncertain.
- **Low Confidence:** The generalization of bounds-based optimization across different pharmaceutical products and supply chain structures. The theoretical bounds were derived for the specific case study parameters and may not extend to fundamentally different inventory dynamics.

## Next Checks

1. **Hyperparameter Sensitivity Analysis:** Systematically vary PPO hyperparameters (learning rate, network depth, clipping parameter) to determine whether the reported performance is robust to implementation choices or represents a narrow optimal configuration.

2. **Forecast Error Stress Test:** Implement a demand forecast error model (e.g., Â±20% noise on projected demand) and re-run PIL and PPO policies to quantify their degradation under realistic forecast uncertainty conditions.

3. **Cross-Product Validation:** Apply the same policy framework to a different pharmaceutical product with distinct lifecycle characteristics (faster growth/decline, different batch sizes) to test whether the no-dominant-policy finding holds across product categories.