---
ver: rpa2
title: Improving the Distributional Alignment of LLMs using Supervision
arxiv_id: '2507.00439'
source_url: https://arxiv.org/abs/2507.00439
tags:
- alignment
- distributions
- answer
- llama-3
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of aligning large language models
  (LLMs) with diverse population groups on subjective questions by evaluating their
  ability to generate accurate response distributions. A key limitation in prior work
  is the assumption that group members share homogeneous opinions, which ignores intra-group
  disagreement.
---

# Improving the Distributional Alignment of LLMs using Supervision

## Quick Facts
- arXiv ID: 2507.00439
- Source URL: https://arxiv.org/abs/2507.00439
- Reference count: 40
- Primary result: Supervised regression calibration improves distributional alignment of LLMs with human responses by 16.3% on average across 15 models and 3 datasets

## Executive Summary
This paper addresses the challenge of aligning large language models with diverse population groups on subjective survey questions by evaluating their ability to generate accurate response distributions. The authors identify a key limitation in prior work: the assumption that group members share homogeneous opinions, which ignores intra-group disagreement. They propose using supervised regression to calibrate LLM-generated distributions, making them more consistent with human response distributions. Experiments demonstrate that while sociodemographic prompting alone does not reliably improve alignment, supervised calibration consistently improves distributional alignment by 16.3% on average and reduces variance across models and datasets, suggesting uncalibrated LLM distributions tend to exaggerate inter-group differences.

## Method Summary
The authors evaluate distributional alignment between LLMs and human survey responses across three datasets (WGM, OpinionQA, WVS) using 15 different models. They compare three elicitation methods (verbalized, self-random, paraphrase) with both base and sociodemographic prompts. For calibration, they train supervised regression models (Ridge, Lasso, Random Forest) to map LLM-generated probabilities to human response probabilities, treating each answer choice independently. The calibration is trained on 80% of questions and evaluated on held-out test sets using Wasserstein distance as the alignment metric. They find that verbalized elicitation generally performs best and that as few as 1-10 supervised examples are sufficient for effective calibration.

## Key Results
- Supervised calibration improves distributional alignment by 16.3% on average across datasets and models
- Verbalized elicitation consistently outperforms sampling methods, with verbalized achieving 81.9% alignment compared to 79.7% for self-random and 79.2% for paraphrase
- Calibrated distributions reduce variance across models and datasets, suggesting uncalibrated LLMs exaggerate inter-group differences
- As few as 1-10 supervised examples are sufficient for effective calibration

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Supervised regression can effectively calibrate LLM-generated distributions to align more closely with human response distributions on subjective questions.
- Mechanism: LLM-generated distributions are often directionally correct but systematically uncalibrated, for instance, by exaggerating inter-group differences. Supervised regression learns a transformation (mapping LLM probabilities to ground truth probabilities) to correct these systematic biases. This is done by training a regressor on pairs of LLM-generated probabilities and corresponding human response probabilities for each answer choice.
- Core assumption: The miscalibration is relatively consistent across questions and demographic groups for a given model-elicitation method setting, allowing a single regression model to capture and correct the bias.
- Evidence anchors:
  - [abstract]: "The calibration also reduces variance across models, datasets, and elicitation methods, suggesting that uncalibrated LLM distributions tend to exaggerate inter-group differences."
  - [section 7 (Discussion)]: "We find that LLM-generated distributions (produced in answer to survey questions) are uncalibrated; simple regression allows us to scale distributions to be more aligned with human opinion in aggregate..."
  - [corpus]: Corpus evidence is weak or missing direct support for the specific mechanism of regression-based calibration of opinion distributions.
- Break condition: If the systematic bias in LLM distributions is highly inconsistent or context-dependent (varying significantly question-by-question or group-by-group), a single regression model may fail to improve alignment and could even harm it by over-smoothing.

### Mechanism 2
- Claim: Verbalized probability elicitation is generally more effective for generating aligned distributions than sampling-based methods.
- Mechanism: Directly prompting an LLM to output a probability distribution leverages the model's learned understanding of probability and uncertainty in a single inference step. Sampling-based methods (like self-random or paraphrase) require multiple inferences and aggregation, which can amplify biases or inconsistencies in the model's single-choice predictions, leading to noisier or less representative distributions.
- Core assumption: LLMs have a sufficient internal representation of probability distributions for subjective questions that can be accessed via direct prompting.
- Evidence anchors:
  - [section 4.1]: "For base and SD prompts, LLM distributions are generally most aligned with verbalized elicitation (Table 1), with exceptions (e.g., on the OQA dataset)."
  - [section 9 (Limitations)]: "We found that log probabilities with calibration yielded the most aligned distributions in 10/24 settings considered."
  - [corpus]: Corpus evidence is weak or missing for comparing verbalized elicitation efficiency versus sampling-based methods.
- Break condition: If the model is poorly trained to verbalize probabilities or its internal "probabilistic reasoning" is flawed, the direct output may be miscalibrated or nonsensical, performing worse than aggregated samples.

### Mechanism 3
- Claim: Sociodemographic (SD) prompting alone does not consistently improve distributional alignment with specific human population groups.
- Mechanism: Explicitly instructing a model to adopt a persona (e.g., "Imagine you are {d}") can activate stereotypes and caricatures rather than a nuanced representation of a diverse group's opinions. This can cause the model to exaggerate differences between groups, leading to less accurate distributions compared to a neutral (base) prompt. The model's priors and biases may override or distort the intended demographic steering.
- Core assumption: The primary goal is to model the distribution of a group's opinions, not a stereotypical or extreme version of that group's potential views.
- Evidence anchors:
  - [abstract]: "...sociodemographic prompting alone does not reliably improve alignment..."
  - [section 1 (Introduction)]: "Prompting for a distribution, rather than a representative response, could potentially also make a model less susceptible to caricature and stereotyping... better aligning with the goals of pluralistic alignment."
  - [section 2 (Related Work)]: "Other work warns against risks of misportrayal, othering, and exoticization of identities..."
  - [corpus]: A related paper, PAARS: Persona Aligned Agentic Retail Shoppers (arXiv:2503.24228), also notes that LLMs exhibit biases, including brand and review rating biases, which persona prompting may not fully resolve.
- Break condition: If the SD prompting is exceptionally well-crafted or the model has been fine-tuned on diverse, representative data specifically for this task, it could improve alignment, but this is not the general case as shown by the paper's broad evaluation.

## Foundational Learning

- Concept: Distributional Alignment (vs. Majority Response).
  - Why needed here: The core contribution of the paper is evaluating alignment not by matching a single majority answer, but by comparing the entire probability distribution of LLM outputs to human response distributions. Understanding Wasserstein distance is key.
  - Quick check question: Why is it a "fallacy" to assume a single majority answer per sociodemographic group?

- Concept: Calibration.
  - Why needed here: The paper's primary intervention is calibrating uncalibrated LLM distributions. One must understand that "calibration" here means transforming the output probabilities to be more consistent with ground truth, distinct from other forms of alignment like RLHF.
  - Quick check question: What is the key observation about uncalibrated LLM distributions that makes simple regression effective?

- Concept: Probability Elicitation from LLMs.
  - Why needed here: The paper compares three methods (verbalized, self-random, paraphrase) to get a distribution from a model. Understanding the trade-offs (e.g., single inference vs. multiple inferences) is important.
  - Quick check question: Why might the verbalized method be preferred over self-random sampling?

## Architecture Onboarding

- Component map: Data Source -> LLM Inference Module -> Evaluation Metric -> Calibration Engine
- Critical path: 1. Prompt LLM with a survey question -> 2. Elicit distribution using a chosen method -> 3. Compute initial alignment score. -> 4. Train a regressor on a subset of (LLM distribution, Human distribution) pairs. -> 5. Use the trained regressor to transform held-out LLM distributions. -> 6. Re-normalize and compute final calibrated alignment score.
- Design tradeoffs:
    - **Verbalized vs. Sampling Elicitation**: Verbalized is faster (n=1-3 samples) and often more aligned, but may produce nonsensical outputs if the model is poorly trained. Sampling (n=5+) is more robust but computationally more expensive and may be noisier.
    - **Global vs. SD-Specific Calibration**: The paper uses a single regressor for all groups in a dataset. An alternative would be to train separate calibrators per demographic group, which could improve performance for specific groups but requires more data.
    - **Regression Model Choice**: The paper uses simple regressors (Ridge, Lasso, Random Forest) via scikit-learn, trading off complexity for data efficiency (works with 1-10 examples). More complex models may not work with such low data.
- Failure signatures:
    - **Low Data for Calibration**: With fewer than ~1-10 examples, the regression model may not converge or may produce degenerate transformations.
    - **Over-smoothing**: The component-wise regression ignores correlations between answer choices. This could distort the shape of the distribution, flattening peaks or filling tails incorrectly.
    - **Model/Elicitation Mismatch**: A regressor trained on one model or elicitation method may not transfer well to another, reducing or negating calibration benefits.
- First 3 experiments:
  1. Replicate the main finding: Evaluate verbalized elicitation on a small set of questions with and without calibration using a small regressor (e.g., Ridge regression) trained on 50% of the data.
  2. Ablate data efficiency: Train calibration models with varying numbers of examples (1, 5, 10, 50, 100) and plot the alignment score convergence to confirm the 1-10 example finding.
  3. Test elicitation method impact: Compare alignment scores for verbalized vs. self-random elicitation across a few models (e.g., Claude, Llama, Mistral) before and after calibration to see which benefits most.

## Open Questions the Paper Calls Out

- Question: Does training calibration models specifically for individual sociodemographic (SD) groups yield higher alignment than aggregate calibration models?
  - Basis in paper: [explicit] Discussion section states, "We hypothesize that calibrating responses from each individual SD would likely improve its alignment, though we leave this to future work."
  - Why unresolved: The current study learns a single regression model for all groups within a dataset-setting to enable comparison, potentially smoothing out group-specific nuances.
  - What evidence would resolve it: Experiments comparing alignment scores of per-group regression models against the aggregate models reported in the paper.

- Question: Does constrained optimization that enforces proper probability distributions across all choices simultaneously perform better than predicting individual answer choices independently?
  - Basis in paper: [explicit] Limitations section notes, "Our calibration model predicts each answer choice individually and then normalizes... Future work might further analyze the effects of this individual answer choice prediction on alignment."
  - Why unresolved: The authors tried constrained optimization initially but found independent prediction performed better; however, the risks of over-smoothing or loss of distributional structure in the independent approach remain unquantified.
  - What evidence would resolve it: A comparative analysis of alignment error and distribution divergence (e.g., KL Divergence) between independent regressors and constrained distributional regression models (e.g., Dirichlet regression).

- Question: To what extent do logit-based distribution elicitation methods outperform verbalized elicitation across different model architectures when combined with supervised calibration?
  - Basis in paper: [explicit] Limitations section suggests, "Future work might look more into comparisons between logit-based and verbalized distribution elicitation."
  - Why unresolved: The paper focused on methods applicable to black-box/API models; the smaller scale study in Appendix D.2 showed mixed results (logits won in 10/24 settings).
  - What evidence would resolve it: A full-scale benchmark evaluation (similar to the main experiment) using log probabilities for the open-source models (e.g., Llama, OLMo) compared against the verbalized baselines.

## Limitations

- The paper uses a single global calibration model for all demographic groups within a dataset, which may mask group-specific miscalibrations that per-demographic calibrators could address
- The evaluation focuses on distributional alignment with human responses but does not validate whether calibrated LLM distributions would produce more equitable downstream decisions in real applications
- The component-wise regression approach ignores correlations between answer choices, which could introduce distributional distortions by over-smoothing or incorrectly filling probability mass

## Confidence

- **High Confidence**: The finding that supervised calibration consistently improves distributional alignment by ~16.3% across datasets and models is well-supported by the extensive experimental setup (270 models, 3 datasets, 15 models)
- **Medium Confidence**: The claim that verbalized elicitation generally outperforms sampling methods is supported by results, but with notable exceptions (OQA dataset)
- **Medium Confidence**: The assertion that SD prompting often harms alignment is supported by results showing it does not reliably improve alignment and may exaggerate stereotypes

## Next Checks

1. **Per-Demographic Calibration**: Train separate calibration models for each demographic group (e.g., age, gender, region) rather than a single global model. Compare alignment gains to the baseline global approach to quantify whether group-specific miscalibrations exist and whether they can be better addressed.

2. **Downstream Impact Analysis**: Take a subset of calibrated distributions and simulate a downstream decision task (e.g., survey weighting, policy recommendation). Measure whether calibrated distributions lead to more equitable outcomes compared to uncalibrated or SD-prompted distributions, validating the practical utility of distributional alignment.

3. **Joint Distributional Calibration**: Modify the regression approach to predict full probability vectors rather than independent answer-choice probabilities. Compare alignment scores and distributional shapes (e.g., using KL divergence or correlation) to the current component-wise approach to assess whether ignoring answer correlations introduces meaningful distortions.