---
ver: rpa2
title: Can ChatGPT Perform Image Splicing Detection? A Preliminary Study
arxiv_id: '2506.05358'
source_url: https://arxiv.org/abs/2506.05358
tags:
- image
- detection
- spliced
- images
- splicing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates whether a large multimodal language model,
  specifically GPT-4V, can perform image splicing detection without task-specific
  training. Using a subset of the CASIA v2.0 dataset, the study evaluates three prompting
  strategies: zero-shot, few-shot, and chain-of-thought.'
---

# Can ChatGPT Perform Image Splicing Detection? A Preliminary Study

## Quick Facts
- arXiv ID: 2506.05358
- Source URL: https://arxiv.org/abs/2506.05358
- Reference count: 21
- One-line primary result: GPT-4V achieves >85% detection accuracy in zero-shot image splicing detection using real-world knowledge and semantic reasoning.

## Executive Summary
This paper investigates whether GPT-4V can perform image splicing detection without task-specific training, using the CASIA v2.0 dataset. The study evaluates three prompting strategies: zero-shot, few-shot, and chain-of-thought. Results show that GPT-4V can achieve over 85% detection accuracy in zero-shot mode, with chain-of-thought prompting offering the most balanced performance. The model demonstrates not only the ability to detect visual artifacts but also the capacity to apply real-world contextual knowledge—such as object scale, environmental consistency, and architectural facts—to identify implausible composites. While it does not yet match specialized detectors, its generalizability and interpretability suggest potential as a complementary tool in image forensics.

## Method Summary
The study uses GPT-4V to perform binary classification (Authentic vs. Spliced) on a curated subset of CASIA v2.0 (3,100 Authentic, 600 Spliced images, JPEG only, filtered for Animal, Architecture, Character categories). Three prompting strategies are evaluated: Zero-Shot (direct instruction), Few-Shot (4 examples with category matching), and Chain-of-Thought (4 examples with manual reasoning). Images are base64-encoded and processed via OpenAI API with temperature=0. Detection accuracy is the primary metric, calculated across overall and class-wise performance.

## Key Results
- GPT-4V achieves 85.07% zero-shot accuracy on spliced images, demonstrating strong baseline performance without training.
- Few-shot prompting introduces bias, dropping spliced accuracy to 76.50%, while CoT recovers it to 81.33%.
- Qualitative analysis shows the model uses semantic reasoning (e.g., object scale, habitat facts) to detect manipulations beyond pixel-level artifacts.

## Why This Works (Mechanism)

### Mechanism 1: Semantic Consistency Verification via World Knowledge
- **Claim:** GPT-4V detects manipulations by identifying semantic implausibilities that violate real-world physical or factual constraints, rather than relying solely on pixel-level artifacts.
- **Mechanism:** The model leverages parametric knowledge encoded during pre-training (e.g., object scale ratios, architectural facts, natural habitats) to reason about the visual scene. It flags images where the visual content contradicts this internal knowledge base (e.g., a chick appearing smaller than an ant, or an incorrect clock on a landmark).
- **Core assumption:** The splicing manipulation introduces semantic errors (e.g., incorrect relative size or context) that are discernible to a model with broad encyclopedic knowledge.
- **Evidence anchors:**
  - [abstract] Mentions applying "real-world contextual knowledge—such as object scale, environmental consistency, and architectural facts."
  - [section 4] Details specific examples where the model reasoned about habitat (iguanas vs. salamanders) and architecture (St. Peter's Basilica).
  - [corpus] The "REVEAL" paper (arXiv:2511.23158) supports the efficacy of reasoning-enhanced forensic analysis.
- **Break condition:** Semantically plausible forgeries where the object scale, lighting direction, and context perfectly match the target scene (e.g., moving a car from one street to a similar street).

### Mechanism 2: Low-Level Visual Artifact Recognition
- **Claim:** The model utilizes inherent visual pattern recognition capabilities to identify pixel-level inconsistencies introduced during the splicing process.
- **Mechanism:** Vision-language alignment allows the model to map visual discontinuities—such as edge blending artifacts, lighting mismatches, and texture irregularities—to the concept of "manipulation."
- **Core assumption:** The splicing artifacts are sufficiently visible in the image resolution provided to the model and have not been fully smoothed by post-processing.
- **Evidence anchors:**
  - [abstract] States the model detects "low-level visual artifacts."
  - [section 4] Quotes the model citing "lighting and shadows" and "edges are noticeably sharper" as reasoning factors.
  - [corpus] The "ForensicFormer" paper (arXiv:2601.08873) emphasizes the need for multi-scale analysis including low-level artifacts.
- **Break condition:** Highly sophisticated post-processing (e.g., adversarial smoothing or advanced inpainting) that eliminates visible edge artifacts or lighting inconsistencies.

### Mechanism 3: Bias Mitigation via Chain-of-Thought (CoT) Reasoning
- **Claim:** Chain-of-Thought prompting improves detection balance by counteracting the prediction bias induced by standard few-shot examples.
- **Mechanism:** Standard few-shot examples can anchor the model toward a specific class (e.g., "Authentic"), increasing false negatives. CoT forces the model to generate intermediate reasoning steps, shifting the decision boundary back toward a more balanced evaluation of the specific image features.
- **Core assumption:** The manual reasoning examples provided in the CoT prompt are high-quality and effectively guide the model's attention to relevant forensic cues.
- **Evidence anchors:**
  - [section 3] Table 1 shows Few-Shot (FS) drops spliced accuracy to 76.50%, while CoT recovers it to 81.33%.
  - [section 3] Text explains "CoT effectively mitigates the bias introduced by the FS strategy."
  - [corpus] Corpus context suggests prompting strategies significantly impact model performance (e.g., arXiv:2510.06782 regarding GPT-4V/chart reading).
- **Break condition:** The reasoning chain in the prompt is flawed or the visual evidence is too ambiguous for textual explanation, leading to "hallucinated" reasoning that justifies an incorrect label.

## Foundational Learning

- **Concept: Image Splicing vs. Copy-Move**
  - **Why needed here:** The study isolates "splicing" (external content insertion) from "copy-move" (internal duplication). Understanding this distinction is required to interpret the dataset curation (where copy-move images were excluded) and the specific artifacts (external vs. internal consistency) the model looks for.
  - **Quick check question:** Does the dataset used in the paper include images where a region is duplicated within the same image?

- **Concept: Zero-Shot vs. Few-Shot Prompting**
  - **Why needed here:** The paper evaluates three distinct interfaces to the model. "Zero-shot" tests intrinsic capability, while "Few-shot" tests in-context learning. The paper demonstrates that these are not just performance knobs but fundamentally change the model's behavior (bias toward/against "Authentic").
  - **Quick check question:** Which prompting strategy resulted in the highest false negative rate (classifying spliced images as authentic)?

- **Concept: Multimodal Alignment**
  - **Why needed here:** GPT-4V is not just a vision model; it is a *multimodal* model. The mechanism relies on the "alignment" between visual features (pixels) and textual concepts (e.g., "shadow inconsistency"). This is what allows the model to output human-readable explanations for its classifications.
  - **Quick check question:** How does the model justify its decision to classify an image as "Spliced" in the qualitative analysis?

## Architecture Onboarding

- **Component map:**
  - Input Layer: Base64 Encoder (converts JPEG $\to$ text string)
  - Prompt Assembler: Constructs the payload (System Instruction + Image Data + [Few-Shot Examples | CoT Rationale])
  - Inference Engine: GPT-4.1 API (Temperature=0 for determinism)
  - Parser: Extracts "Authentic" or "Spliced" labels from the text response

- **Critical path:**
  The Prompt Strategy selection is the critical control point.
  1. If **Zero-Shot**: Simple instruction $\to$ High sensitivity (high False Positives)
  2. If **Few-Shot**: Add 4 examples $\to$ High specificities (high False Negatives/bias)
  3. If **CoT**: Add 4 examples + Reasoning text $\to$ Balanced performance

- **Design tradeoffs:**
  - **Latency vs. Accuracy:** CoT requires generating more tokens (reasoning) and larger context windows, increasing API latency and cost compared to Zero-Shot.
  - **Generalization vs. Specialization:** This MLLM approach offers generalization (detecting semantic nonsense) but lags in raw accuracy compared to specialized CNNs trained specifically on noise residuals.
  - **Stability:** Zero-Shot is deterministic but sensitive to phrasing; Few-Shot is highly sensitive to the specific choice of example images.

- **Failure signatures:**
  - **Architecture Blindness:** Consistently low detection rates for "Architecture" splicing (approx. 56-62% in Few-Shot/CoT) suggests the model struggles with structured textures/repetitive patterns.
  - **Label Bias:** If evaluating Few-Shot and seeing >95% "Authentic" accuracy but <80% "Spliced" accuracy, suspect the "Authentic Bias" described in the paper.

- **First 3 experiments:**
  1. **Sanity Check (Zero-Shot):** Run 10 authentic and 10 spliced images using the Zero-Shot prompt to verify the >85% baseline and observe the "over-sensitivity" to artifacts.
  2. **Bias Verification (Few-Shot):** Implement the Few-Shot prompt with category-matched examples. Confirm if the model's false negative rate increases (missing splices) as claimed.
  3. **Semantic Reasoning Test:** Select the 3 qualitative examples from Figure 4 (Chick/Ants, Iguana, Obelisk) and run them without any "splicing" keywords in the prompt (e.g., "Describe this image") to see if the model naturally flags the anomalies as "weird" before being asked to classify them.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent does pre-training data contamination (data leakage) inflate the zero-shot detection performance of GPT-4V on public forensics datasets?
- Basis in paper: [explicit] The authors list "Data Leakage" as a primary limitation, noting that because models like ChatGPT are trained on vast, undisclosed corpora, they cannot verify if the CASIA v2.0 images were seen during pre-training.
- Why unresolved: The closed-source nature of GPT-4 prevents the authors from auditing the training data to ensure the test set was not memorized.
- What evidence would resolve it: Evaluating the model on a dataset of newly created, private image forgeries or synthetic images generated after the model's training cutoff date.

### Open Question 2
- Question: How does image splicing detection performance compare between GPT-4V and other state-of-the-art Multimodal Large Language Models (MLLMs)?
- Basis in paper: [explicit] Section 5 (Limitations) states that the evaluation focuses solely on GPT-4 and does not include comparisons with other strong MLLMs like Gemini, Claude, or open-source vision-language models.
- Why unresolved: The study restricted its scope to a single proprietary model to establish an initial benchmark.
- What evidence would resolve it: A comprehensive benchmark study applying the same Zero-Shot, Few-Shot, and Chain-of-Thought protocols across a diverse set of open- and closed-source MLLMs.

### Open Question 3
- Question: Can advanced prompt engineering strategies mitigate the classification biases (specifically the bias toward "Authentic" predictions) observed in Few-Shot prompting?
- Basis in paper: [explicit] The authors note they relied on "simple, intuitive instructions" and observed that Few-Shot prompting introduced a notable bias that reduced sensitivity to spliced images.
- Why unresolved: The study did not engage in extensive prompt optimization or employ automated prompt tuning techniques.
- What evidence would resolve it: Systematic experimentation with optimized prompt variations or automated prompt engineering agents to determine if the "Authentic" bias can be minimized while maintaining high accuracy.

### Open Question 4
- Question: Can MLLMs extend their reasoning capabilities to accurately localize splicing regions (e.g., via bounding boxes or segmentation) rather than performing only image-level classification?
- Basis in paper: [inferred] The paper evaluates detection solely via accuracy metrics (binary classification), yet the qualitative analysis shows the model can identify specific visual inconsistencies.
- Why unresolved: The current experimental design constrains the output to a binary label ("Authentic" or "Spliced"), leaving the model's localization potential unmeasured.
- What evidence would resolve it: Modifying the prompting strategy to request specific coordinates or masks of the tampered region and evaluating the results using localization metrics like Intersection over Union (IoU).

## Limitations
- The curated CASIA v2.0 subset excludes copy-move forgeries and TIFF formats, potentially inflating detection rates.
- The study does not test adversarial attacks or highly sophisticated forgeries with perfect lighting and scale matching.
- Evaluation is confined to three specific categories from a single dataset, with no testing on external forensic benchmarks or real-world manipulated media.

## Confidence
- **High confidence** in the core finding that GPT-4V can perform splicing detection without task-specific training, supported by the clear quantitative gap between zero-shot (85.07%) and few-shot (76.50%) accuracy for spliced images.
- **Medium confidence** in the CoT prompting benefits, as the paper shows improvement from 76.50% to 81.33% for spliced images, but does not conduct ablation studies to isolate the contribution of reasoning quality versus prompt structure.
- **Low confidence** in generalization claims, as the evaluation is confined to three specific categories from a single dataset, with no testing on external forensic benchmarks or real-world manipulated media.

## Next Checks
1. **Adversarial Testing:** Evaluate the model on semantically plausible forgeries where object scale, lighting, and context perfectly match the target scene to test the claimed "semantic consistency" mechanism.
2. **Dataset Generalization:** Test the exact prompt strategies on the full NIST Nimble Challenge dataset and the RAISE dataset to assess performance on non-curated, real-world forensic cases.
3. **Model Version Comparison:** Compare current GPT-4 Turbo/Omni performance against the claimed GPT-4.1 (2025-04-14) to determine if the specific instruction tuning is critical to the observed accuracy levels.