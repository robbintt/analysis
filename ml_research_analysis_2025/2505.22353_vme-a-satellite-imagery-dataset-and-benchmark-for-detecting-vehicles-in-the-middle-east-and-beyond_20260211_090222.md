---
ver: rpa2
title: 'VME: A Satellite Imagery Dataset and Benchmark for Detecting Vehicles in the
  Middle East and Beyond'
arxiv_id: '2505.22353'
source_url: https://arxiv.org/abs/2505.22353
tags:
- dataset
- images
- detection
- object
- cdsi
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses geographic bias in satellite imagery datasets
  for vehicle detection, particularly the underrepresentation of Middle Eastern regions.
  The authors created the Vehicles in the Middle East (VME) dataset with over 4,000
  high-resolution image tiles from 54 cities across 12 countries, containing more
  than 100,000 vehicle instances.
---

# VME: A Satellite Imagery Dataset and Benchmark for Detecting Vehicles in the Middle East and Beyond

## Quick Facts
- arXiv ID: 2505.22353
- Source URL: https://arxiv.org/abs/2505.22353
- Reference count: 0
- Key outcome: VME dataset improves Middle East vehicle detection accuracy by 56.3% compared to models trained on existing datasets

## Executive Summary
This paper addresses geographic bias in satellite imagery datasets for vehicle detection by introducing the Vehicles in the Middle East (VME) dataset containing over 4,000 high-resolution image tiles from 54 cities across 12 countries with more than 100,000 vehicle instances. The authors demonstrate that existing datasets poorly generalize to Middle Eastern imagery due to different visual contexts (built structures, land cover, urban planning patterns), while models trained on VME achieve significantly better detection accuracy. They also created the Car Detection in Satellite Imagery (CDSI) benchmark by consolidating VME with existing datasets in a standardized format.

## Method Summary
The VME dataset was created by collecting high-resolution satellite imagery from Google Earth and the European Space Agency, covering 54 cities across 12 Middle Eastern countries. Images were tiled into 512×512 patches and annotated with oriented bounding boxes (OBB) and horizontal bounding boxes (HBB) in both YOLO and MS-COCO formats. The CDSI benchmark was developed by standardizing annotations from multiple existing datasets (xView, DOTA-v2.0, VEDAI, DIOR, FAIR1M-2.0) to MS-COCO format, filtering objects by area threshold (<400 pixels for cars), and creating unified training splits. Detection models were trained using the SAHI framework with TOOD and DINO detectors.

## Key Results
- VME dataset improves Middle East vehicle detection accuracy by 56.3% compared to models trained on existing datasets
- CDSI-trained models achieve mAP50 improvements of 19.6% to 84.6% across different models for global car detection
- Models trained on CDSI outperform those trained on individual datasets by 4-6% mAP50 on VME test set
- SAHI framework effectively addresses small object detection challenges for vehicles occupying few pixels

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adding geographically diverse training data improves detection accuracy in underrepresented regions
- Mechanism: Models trained on existing datasets fail to generalize to Middle Eastern imagery because visual context (built structures, land cover, urban planning patterns) differs significantly from Western-centric training data. The VME dataset provides region-specific features that reduce this distribution gap
- Core assumption: The performance gap stems from training data distribution rather than model architecture limitations
- Evidence anchors: [abstract] "models trained on existing datasets perform poorly on Middle Eastern images, while the VME dataset significantly improves detection accuracy in this region"; [section: Background & Summary] "A noticeable contrast is evident in the appearance of built structures and land cover, stemming from unique differences in the natural landscape, climate, economic development, urban planning, and architectural design"

### Mechanism 2
- Claim: Consolidating multiple datasets with standardized annotations improves global detection robustness
- Mechanism: Different datasets use incompatible annotation formats (OBB vs HBB), category taxonomies ("small car" vs "vehicle"), and spatial resolutions. The CDSI pipeline standardizes these to MS-COCO format, filters objects by area threshold (<400 pixels for cars), and creates unified training splits—enabling models to learn from broader feature diversity
- Core assumption: Category mapping across datasets preserves semantic meaning (e.g., "small car" in xView ≈ "car" in VME)
- Evidence anchors: [section: Category Mapping] "we visually inspected these categories to ensure they correspond to the same 'car' object we are targeting"; [section: Data Processing and Filtering] "an area size of less than 400 pixels reports for more than 90% of all car-related object instances across all datasets"

### Mechanism 3
- Claim: Slicing-aided inference improves small object detection in satellite imagery
- Mechanism: Vehicles occupy few pixels (classified as "tiny" objects, area <512 pixels). SAHI slices large images into overlapping patches during both fine-tuning and inference, allowing detectors to process objects at effective scales without downsampling losses
- Core assumption: Patch-based processing doesn't fragment object context beyond recoverability
- Evidence anchors: [section: Detection Benchmarks] "SAHI is developed particularly for small object detection and provides a generalized slicing-aided inference and fine-tuning channel"; [section: VME Dataset] "all of the car instances fall under the small object range (i.e., area (pixels) < 32²) defined in the MS-COCO evaluation, specifically within the first half of the range (i.e., area (pixels) < 512)"

## Foundational Learning

- Concept: **Oriented Bounding Boxes (OBB) vs Horizontal Bounding Boxes (HBB)**
  - Why needed here: VME provides both; OBB captures vehicle orientation useful for traffic flow analysis, but many baseline models require HBB conversion
  - Quick check question: Given OBB corners (x1,y1), (x2,y2), (x3,y3), (x4,y4), how would you derive an HBB?

- Concept: **Geographic/Distribution Shift in Object Detection**
  - Why needed here: Core problem this paper addresses—models trained on Western imagery fail on Middle Eastern scenes due to different visual contexts (desert vs vegetation, architectural styles, road patterns)
  - Quick check question: If your model achieves 85% mAP on xView test data but 15% on VME test data, what does this indicate?

- Concept: **mAP and mAP50 Metrics**
  - Why needed here: Primary evaluation metrics reported; mAP50 uses IoU threshold of 0.5, more lenient for small objects where precise localization is difficult
  - Quick check question: Why might mAP50 be more appropriate than stricter mAP (IoU=0.75) for tiny vehicle detection?

## Architecture Onboarding

- Component map: Raw satellite images -> Tiling (512×512) -> Annotation (OBB/HBB in YOLO/COCO format) -> Format standardization -> Size filtering (<400px for cars) -> Category mapping -> Split generation (5/8 train, 1/8 val, 2/8 test) -> SAHI framework -> TOOD/DINO detectors -> mAP/mAP50 evaluation

- Critical path: 1) Download VME from Zenodo (images + annotations); 2) Download source datasets from original repositories; 3) Run `CDSI_construction_scripts` to convert each to MS-COCO format; 4) Combine using provided split files; 5) Train with SAHI + TOOD/DINO using specified hyperparameters

- Design tradeoffs: Car-only vs car-other training (adding "other" small objects as hard negatives may reduce false positives but complicates multi-class learning); OBB vs HBB (OBB preserves orientation but requires specialized detectors; HBB enables broader model compatibility); Dataset consolidation (larger CDSI improves generalization but may dilute region-specific features)

- Failure signatures: Very low mAP on VME test set despite good performance on other datasets → geographic bias not addressed; High false positive rate on background objects → need "other" category training or better hard negative mining; t-SNE showing VME features isolated from training data → distribution mismatch persists

- First 3 experiments: 1) Baseline reproduction: Train TOOD on VME car-only setup; verify mAP50 ≈ 81% on VME test set; 2) Cross-domain evaluation: Train on xView car-only, evaluate on VME test set; expect mAP50 drop (paper shows ~38%); 3) Ablation on consolidation: Compare CDSI vs CDSI* (without VME) trained models on VME test set; quantify VME contribution (paper shows 4-6% mAP50 improvement)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the high rate of background false positives be minimized in vehicle detection models?
- Basis in paper: [explicit] The error analysis states that "eliminating background false positives... boosts the AP to 0.99," identifying them as the primary area for improvement over localization or class confusion
- Why unresolved: Current state-of-the-art models (like DINO Swin-L) still struggle to differentiate small vehicle signatures from visually similar background clutter
- What evidence would resolve it: Development of a training strategy that significantly reduces background FP rates on the VME test set without compromising recall

### Open Question 2
- Question: How do specific Middle Eastern atmospheric conditions affect detection robustness?
- Basis in paper: [explicit] The Usage Notes mention that "climate conditions in the Middle East, including haze and airborne dust, can affect the clarity of these images," causing blurriness or reflections
- Why unresolved: The paper does not quantify how these specific regional artifacts impact performance compared to other geographic biases like urban design
- What evidence would resolve it: Benchmark results stratified by weather conditions or visibility indices specific to the VME dataset

### Open Question 3
- Question: Does the aggregation of non-car objects into a single "other" class limit fine-grained feature discrimination?
- Basis in paper: [inferred] The CDSI dataset groups all small non-car objects into one "other" category to simplify training, assuming simple hard negatives are sufficient
- Why unresolved: It is possible that explicitly classifying different types of hard negatives (e.g., trash bins vs. AC units) would force the model to learn more robust features for distinguishing cars
- What evidence would resolve it: A comparative study showing performance differences between the current binary setup and a multi-class negative setup

## Limitations

- Geographic coverage excludes significant regions (Central Asia, Africa, Southeast Asia) where similar distribution shifts may occur
- Filtering threshold (<400 pixels) was chosen based on empirical distribution analysis but may exclude relevant small vehicles or include non-vehicle objects
- 12-month imagery timeframe may not capture temporal variations in vehicle patterns across seasons or economic conditions

## Confidence

- High Confidence: Geographic distribution shift as primary driver of performance gaps
- Medium Confidence: CDSI consolidation benefits
- Medium Confidence: Category mapping validity

## Next Checks

1. Perform t-SNE visualization of CDSI-trained model features on VME test set to verify that Middle Eastern imagery no longer forms isolated clusters compared to baseline models

2. Conduct inter-annotator agreement study on a subset of VME images using xView-style annotations to validate that "car" and "small car" categories are semantically consistent across datasets

3. Test model performance on VME images from different months/seasons to assess whether temporal variations in vehicle appearance (weather, lighting, traffic patterns) affect detection accuracy beyond geographic distribution shifts