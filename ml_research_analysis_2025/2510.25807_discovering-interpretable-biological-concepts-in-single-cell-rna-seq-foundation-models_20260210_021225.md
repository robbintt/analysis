---
ver: rpa2
title: Discovering Interpretable Biological Concepts in Single-cell RNA-seq Foundation
  Models
arxiv_id: '2510.25807'
source_url: https://arxiv.org/abs/2510.25807
tags:
- cell
- concepts
- concept
- genes
- gene
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors introduce a framework for interpreting biological concepts
  in single-cell RNA-seq foundation models using sparse autoencoders (SAEs). Their
  method combines counterfactual perturbations with gene attribution to identify genes
  that influence concept activation, moving beyond differential expression analysis.
---

# Discovering Interpretable Biological Concepts in Single-cell RNA-seq Foundation Models

## Quick Facts
- arXiv ID: 2510.25807
- Source URL: https://arxiv.org/abs/2510.25807
- Reference count: 26
- The authors introduce a framework for interpreting biological concepts in single-cell RNA-seq foundation models using sparse autoencoders (SAEs)

## Executive Summary
This paper presents a framework for extracting and interpreting biological concepts from single-cell RNA-seq foundation models using sparse autoencoders. The method goes beyond traditional differential expression analysis by combining counterfactual perturbations with gene attribution to identify genes that influence concept activation. The authors demonstrate that SAE-extracted concepts are more interpretable than individual neurons and can reveal meaningful biological signals while maintaining predictive performance in downstream tasks.

## Method Summary
The framework extracts interpretable biological concepts from foundation models using sparse autoencoders trained to reconstruct model activations. For each concept, counterfactual perturbations systematically modify gene expression levels to observe effects on concept activation, identifying influential genes. Two interpretation approaches are provided: expert-driven analysis through an interactive visualization tool and ontology-driven pathway enrichment using reference databases. The method was applied to scGPT and scVI models on immune cell datasets, demonstrating improved interpretability while preserving downstream task performance.

## Key Results
- SAE-extracted concepts show higher interpretability than individual neurons from foundation models
- Identified genes through counterfactual perturbations align with known biological signals and cell types
- The framework maintains predictive performance in downstream tasks while enabling hypothesis generation
- Expert-driven and ontology-driven interpretation approaches provide complementary insights into biological concepts

## Why This Works (Mechanism)
The method works by leveraging the hierarchical feature learning capabilities of sparse autoencoders to capture abstract biological representations from foundation model activations. By applying counterfactual perturbations, the framework can isolate which genes causally influence concept activation rather than just correlating with it. This approach moves beyond traditional differential expression analysis by considering the model's internal representation space and how small perturbations propagate through the network to affect higher-level concepts.

## Foundational Learning

**Sparse Autoencoders**: Neural networks that learn compressed representations by forcing most activations to be zero, creating sparse, interpretable features. Needed because standard autoencoders produce dense representations that are difficult to interpret biologically. Quick check: Verify sparsity level (typically <10% active units) and reconstruction accuracy (>95%).

**Counterfactual Perturbations**: Systematic modifications to input features to observe causal effects on model outputs. Needed because correlation alone cannot establish which genes actually influence biological concepts. Quick check: Ensure perturbations are small enough to remain biologically plausible but large enough to produce measurable changes.

**Concept Activation Vectors (CAVs)**: Directions in activation space that correspond to specific biological concepts or features. Needed to bridge the gap between abstract model representations and interpretable biological phenomena. Quick check: Validate that CAVs produce consistent activation patterns across similar cell types.

## Architecture Onboarding

**Component Map**: Single-cell data -> Foundation Model (scGPT/scVI) -> Activation Extraction -> Sparse Autoencoder -> Concept Extraction -> Counterfactual Perturbation Analysis -> Gene Attribution -> Interpretation (Expert/Ontology)

**Critical Path**: The core pipeline flows from foundation model inference through SAE training to concept extraction, with counterfactual analysis as the key interpretive step that enables gene attribution.

**Design Tradeoffs**: SAE sparsity vs. reconstruction accuracy (higher sparsity improves interpretability but may lose information), perturbation magnitude vs. biological plausibility (larger perturbations give clearer signals but may be unrealistic), and expert vs. ontology-driven interpretation (flexibility vs. standardization).

**Failure Signatures**: Poor interpretability indicates either inadequate SAE training (insufficient sparsity or poor reconstruction), concepts that don't align with known biology (may indicate model artifacts), or counterfactual analysis that doesn't produce clear gene attributions (may indicate non-linear relationships).

**First Experiments**: 1) Train SAE on foundation model activations and verify sparsity/reconstruction metrics, 2) Apply counterfactual perturbations to a single concept and validate gene attribution against known biology, 3) Compare interpretability of SAE concepts vs. individual neurons using expert annotation.

## Open Questions the Paper Calls Out
None

## Limitations
- Interpretability depends heavily on the quality and completeness of biological annotations/ontologies
- Framework relies on linear or near-linear relationships between gene expression and concept activation
- Study focuses on immune cell datasets, limiting generalizability to other tissue types
- Computational cost of counterfactual perturbations may be prohibitive for large-scale screening

## Confidence

**High Confidence**: Technical implementation of SAEs for feature extraction, counterfactual perturbation methodology, and demonstration that SAE concepts outperform individual neurons in interpretability.

**Medium Confidence**: Claims about facilitating hypothesis generation (supported by case studies but lacking systematic evaluation), and preservation of predictive performance (demonstrated but only on immune cell datasets).

## Next Checks

1. Test the framework on non-immune cell types (neuronal, epithelial, or developmental datasets) to assess generalizability across biological systems.

2. Perform systematic comparison of counterfactual perturbation-based gene attribution against alternative attribution methods (integrated gradients, SHAP) to validate robustness.

3. Conduct user studies with domain experts using the visualization tool to quantify interpretability gains and identify usability bottlenecks.