---
ver: rpa2
title: 'Certain Head, Uncertain Tail: Expert-Sample for Test-Time Scaling in Fine-Grained
  MoE'
arxiv_id: '2602.02443'
source_url: https://arxiv.org/abs/2602.02443
tags:
- expert-sample
- experts
- uncertain
- selection
- sampling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Expert-Sample exploits a structural property of fine-grained MoE\
  \ routing\u2014a small certain head of high-confidence experts followed by a flat\
  \ uncertain tail of low-confidence candidates\u2014to enable test-time scaling without\
  \ the stability-diversity trade-off of temperature sampling. By deterministically\
  \ preserving the certain head while stochastically sampling from the uncertain tail,\
  \ it injects structural diversity into reasoning paths while maintaining generation\
  \ quality."
---

# Certain Head, Uncertain Tail: Expert-Sample for Test-Time Scaling in Fine-Grained MoE

## Quick Facts
- arXiv ID: 2602.02443
- Source URL: https://arxiv.org/abs/2602.02443
- Reference count: 40
- Improves pass@32 accuracy from 85.4% to 91.9% on GPQA-Diamond using Qwen3-30B-A3B-Instruct

## Executive Summary
Expert-Sample introduces a training-free method for test-time scaling in fine-grained Mixture-of-Experts models by exploiting a structural property of routing distributions: a small "certain head" of high-confidence experts followed by a "flat uncertain tail" of low-confidence candidates. By deterministically preserving the certain head while stochastically sampling from the uncertain tail, Expert-Sample injects structural diversity into reasoning paths while maintaining generation quality. Evaluated across four fine-grained MoE models on math, knowledge reasoning, and code tasks, Expert-Sample consistently improves pass@n accuracy over token sampling baselines, achieving up to 6.5% accuracy gains with verification methods.

## Method Summary
Expert-Sample modifies the standard top-k expert selection in fine-grained MoE models by preserving the top k_keep experts deterministically (the "certain head") while sampling the remaining k-k_keep experts from ranks k_keep+1 to r using Gumbel-softmax sampling (the "uncertain tail"). The method then renormalizes using original gating weights to preserve learned expert preferences. This creates diversity at the MoE layer level without destabilizing generation. The approach is training-free and introduces only negligible computational overhead (<1% throughput impact).

## Key Results
- GPQA-Diamond: Qwen3-30B-A3B-Instruct pass@32 improves from 85.4% to 91.9% with Expert-Sample
- GPQA-Diamond: Verification-based accuracy improves from 59.1% to 62.6% with BoN scoring
- AIME-120: Expert-Sample with normal token sampling achieves 79.7% pass@32 accuracy
- AIME 2024/2025: Pass@32 improves by 5.5% and 6.5% respectively over token sampling baselines

## Why This Works (Mechanism)

### Mechanism 1
Fine-grained MoE router score distributions structurally decouple stability (certain head) from diversity (uncertain tail). The softmax-normalized router scores exhibit a sharp drop after the top k_keep experts, creating a natural boundary where high-confidence experts are critical for deterministic accuracy, while low-confidence experts have near-uniform weights suitable for stochastic exploration. This assumes the head-tail pattern generalizes across fine-grained MoE architectures and tasks.

### Mechanism 2
Expert-level stochastic sampling injects structural diversity earlier in computation than token-level sampling, enabling distinct reasoning paths without destabilizing generation. Standard top-k expert selection is deterministic (analogous to greedy decoding). Expert-Sample introduces controlled randomness at expert selection via Gumbel-softmax sampling over ranks k_keep+1 to r, while preserving top-k_keep experts. This creates diversity at the MoE layer level, affecting which experts contribute to each token's hidden state before the final output distribution.

### Mechanism 3
Renormalizing with original gating weights preserves the model's learned expert preferences while exploring alternative expert combinations. After sampling experts from the uncertain tail, Expert-Sample retrieves the original (unsampled) router scores for all selected experts and renormalizes. This ensures that when an expert is selected, its contribution magnitude reflects the router's learned confidence, preventing sampled experts from disproportionately influencing outputs.

## Foundational Learning

- **Fine-Grained Mixture-of-Experts (MoE)**: Expert-Sample operates specifically on fine-grained MoE architectures where each layer has many experts (e.g., 128–512) and multiple experts are activated per token (e.g., k=8). Understanding this architecture is prerequisite to understanding how routing-level sampling differs from standard MoE.
- **Test-Time Scaling**: The paper's goal is to improve performance at inference time by generating multiple candidate solutions. Expert-Sample is a method within this paradigm, contrasting with temperature-based token sampling.
- **Temperature Sampling and Stability-Diversity Trade-off**: The paper positions Expert-Sample as an alternative that avoids the trade-off inherent in token-level temperature sampling. Understanding why high temperature increases diversity but hurts stability is essential.

## Architecture Onboarding

- **Component map**: Token hidden state h -> Router network (h·W_g -> logits -> softmax -> p) -> **[Expert-Sample modification]** -> Selected expert outputs weighted-summed -> MoE output -> Continue through layers -> Final output distribution
- **Critical path**: 1. Token hidden state h enters MoE layer, 2. Router produces scores for all experts -> Expert-Sample selects experts (certain head preserved, uncertain tail sampled), 3. Selected expert outputs weighted-summed -> MoE output, 4. Continue through remaining layers, 5. Final output distribution -> token sampling (separate from Expert-Sample)
- **Design tradeoffs**: k_keep: Lower values → more diversity but potential instability; recommended ⌊k/2⌋+1. Temperature τ: Controls stochasticity in uncertain tail; higher τ → more uniform sampling over candidates; recommended 1.0. Sampling range r: Larger r → more candidate experts but diminishing returns; recommended 4k. Overhead: Negligible (<1% throughput impact) as routing operations are vectorized and computationally minor
- **Failure signatures**: k_keep too small (<k/2): Pass rate on correct problems drops significantly, indicating instability. r too small (<2k): Limited diversity gains as candidate pool is insufficient. τ too low: Diversity gains minimal as sampling becomes near-deterministic. τ too high (with token sampling): Combined with high-temperature token sampling, may still cause instability
- **First 3 experiments**: 1. Router weight distribution analysis: Collect and visualize router scores across tokens/layers for your fine-grained MoE model to verify certain head/uncertain tail pattern exists. 2. Expert reduction impact test: Run greedy decoding with reduced expert count (k/2) to confirm single-run accuracy remains stable while pass@n degrades. 3. Pass@n scaling comparison: Compare Expert-Sample vs. normal/high-temperature token sampling across 2–64 samples on a benchmark to validate scaling efficiency

## Open Questions the Paper Calls Out

Can pre-training or fine-tuning objectives be explicitly modified to accentuate the "certain head" vs. "uncertain tail" separation, thereby enhancing the efficacy of Expert-Sample? The authors hope this work "provides insights for the training side."

Does the "uncertain tail" phenomenon persist sufficiently in coarse-grained MoE architectures (e.g., 8 experts) to enable effective Expert-Sampling? The structural "flat tail" of low-confidence experts may not exist or may be insufficiently deep in models with very few experts.

Would dynamically adjusting $k_{keep}$ based on layer depth or input complexity outperform the static $\lfloor k/2 \rfloor + 1$ configuration? The current implementation uses a global, static hyperparameter for simplicity and efficiency, ignoring potential local variations in the router's confidence distribution.

## Limitations

The core assumption that fine-grained MoE models universally exhibit a sharp certain head followed by a flat uncertain tail is demonstrated only on one model architecture.

The claim that Expert-Sample is "practically parameter-free" is misleading—optimal values likely depend on the specific model's router score distribution shape and task difficulty.

The interaction with high-temperature token sampling is not thoroughly explored, despite the claim that Expert-Sample "avoids the stability-diversity trade-off."

## Confidence

**High Confidence**: The empirical claim that Expert-Sample improves pass@n accuracy on fine-grained MoE models for math, knowledge reasoning, and code tasks (e.g., Qwen3-30B-A3B-Instruct GPQA-Diamond pass@32 from 85.4% to 91.9%).

**Medium Confidence**: The claim that Expert-Sample avoids the stability-diversity trade-off inherent in token sampling. This is logically sound based on the mechanism but relies on the untested assumption that router score distributions always exhibit the required head-tail pattern.

**Low Confidence**: The claim that Expert-Sample is "practically parameter-free" and the recommended hyperparameters work universally across all fine-grained MoE models.

## Next Checks

1. **Distribution Pattern Validation**: Extract router logits from all four MoE models across multiple layers and tokens on GPQA-Diamond. Plot cumulative probability curves for each model to verify the certain head/uncertain tail pattern exists universally or varies significantly.

2. **Hyperparameter Sensitivity Analysis**: Systematically vary k_keep (k/3, k/2, 2k/3, k), τ (0.5, 1.0, 1.5, 2.0), and r (2k, 3k, 4k, 5k) on Qwen3-30B-A3B-Instruct with 32 samples on GPQA-Diamond. Plot pass@32 and BoN accuracy to identify optimal ranges and potential failure modes.

3. **Token Sampling Interaction Test**: Compare Expert-Sample with high-temperature token sampling (T=1.5, T=2.0) and their combination on AIME-120. Measure both pass@32 accuracy and single-sample accuracy to quantify the stability-diversity trade-off Expert-Sample claims to avoid.