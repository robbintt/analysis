---
ver: rpa2
title: Multimodal Deep Learning for Prediction of Progression-Free Survival in Patients
  with Neuroendocrine Tumors Undergoing 177Lu-based Peptide Receptor Radionuclide
  Therapy
arxiv_id: '2511.05169'
source_url: https://arxiv.org/abs/2511.05169
tags:
- patients
- fusion
- laboratory
- prrt
- were
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated multimodal deep learning models for predicting
  progression-free survival (PFS) in patients with neuroendocrine tumors (NETs) undergoing
  177Lu-DOTATOC peptide receptor radionuclide therapy (PRRT). A retrospective analysis
  of 116 patients included clinical, laboratory, and imaging data (SR-PET/CT).
---

# Multimodal Deep Learning for Prediction of Progression-Free Survival in Patients with Neuroendocrine Tumors Undergoing 177Lu-based Peptide Receptor Radionuclide Therapy

## Quick Facts
- arXiv ID: 2511.05169
- Source URL: https://arxiv.org/abs/2511.05169
- Reference count: 39
- Primary result: Multimodal fusion (lab + PET + CT) achieves AUROC 0.72 ± 0.01 for predicting PFS in NET patients

## Executive Summary
This study demonstrates that multimodal deep learning models outperform unimodal approaches for predicting progression-free survival (PFS) in neuroendocrine tumor patients undergoing 177Lu-DOTATOC PRRT. The best-performing model combined laboratory biomarkers, 3D SR-PET, and 3D CT imaging through concatenation-based fusion, achieving AUROC 0.72 ± 0.01. The model's performance significantly exceeded unimodal baselines, with statistical testing confirming that adding iteratively more modalities improves predictive performance (p < 0.01). While the approach shows promise for risk-adapted follow-up strategies, the absence of external validation limits generalizability.

## Method Summary
The study retrospectively analyzed 116 NET patients who underwent 177Lu-DOTATOC PRRT. Seven models were trained to classify low- vs. high-PFS groups: three unimodal (laboratory, SR-PET, or CT only), three bimodal (PET+lab, CT+lab, PET+CT), and one multimodal fusion combining all three. Imaging data were 3D volumes resized to 75×50×50 voxels and harmonized. Laboratory values (AST, ALT, CgA, γ-GT) were extracted within 4 weeks pre-PRRT. Models used 3D-CNNs for imaging and Random Forest for lab-only. The multimodal fusion model used concatenation of flattened image embeddings with lab features, followed by a 3-layer MLP. Training employed Adam optimizer (lr=0.01, weight decay=0.2, dropout=0.1) with BCE loss. Evaluation used repeated 3-fold cross-validation (5 repetitions) with AUROC and AUPRC as metrics.

## Key Results
- Multimodal fusion model (lab + PET + CT) achieved AUROC 0.72 ± 0.01 and AUPRC 0.80 ± 0.01
- Unimodal models performed worse: lab-only AUROC 0.59 ± 0.02, PET-only AUROC 0.42 ± 0.03, CT-only AUROC 0.54 ± 0.01
- Statistical testing confirmed significant improvement with multimodal fusion (p < 0.01)
- Pretrained CT branch initialization further boosted performance by ~0.03 AUROC
- Explainability analysis showed stable models focused on tumor regions while unstable models showed scattered attention

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multimodal fusion improves PFS prediction by capturing complementary disease representations
- Mechanism: Laboratory biomarkers reflect systemic/functional status while SR-PET captures receptor expression and CT provides anatomical detail. Concatenation-based fusion in a shared latent space allows the MLP classifier to learn cross-modal interactions that single modalities cannot express.
- Core assumption: Each modality encodes partially independent prognostic signal; their combination yields synergistic information rather than redundant noise.
- Evidence anchors: [abstract] "A multimodal fusion model laboratory values, SR-PET, and CT—augmented with a pretrained CT branch—achieved the best results (AUROC 0.72 ± 0.01)." [section 3.3] "Statistical testing... suggests that adding iteratively more modalities improves predictive model performance significantly (p < 0.01)."

### Mechanism 2
- Claim: Pretrained CT representations accelerate learning and improve generalization under limited data
- Mechanism: Initializing the CT encoder with weights from a MONAI 3D CT segmentation model provides anatomically meaningful features before fine-tuning. This reduces the effective hypothesis space and mitigates overfitting on n=116 patients.
- Core assumption: Pretrained features transfer to the target task; anatomical structures learned during segmentation remain relevant for survival classification.
- Evidence anchors: [section 2.5] "...the CT branch was initialized with a pretrained MONAI 3D CT segmentation model, that we finetuned rather than being trained from scratch." [section 3.3] "Finally, initializing the CT branch with a pretrained model further boosted the AUROC to 0.72 ± 0.01."

### Mechanism 3
- Claim: Gradient-based explainability reveals whether models learn physiologically plausible patterns
- Mechanism: Saliency maps backpropagate prediction gradients to input voxels. Models with stable training concentrate gradients on tumor regions; unstable models show scattered/noisy attention to irrelevant structures (bladder), indicating failure to learn meaningful representations.
- Core assumption: High gradient magnitude in tumor regions correlates with clinically valid reasoning; noisy gradients indicate overfitting or optimization failure.
- Evidence anchors: [section 3.4] "The PET Fusion model predominantly focuses on relevant tumorous regions, while the PET Only model assigns high importance to the bladder." [section 3.4] "PET Only shows a pronounced shift toward higher gradient values... indicative of exploding gradients."

## Foundational Learning

- Concept: **Multimodal fusion strategies**
  - Why needed here: Understanding concatenation vs. attention-based fusion informs architectural choices when combining imaging and tabular data.
  - Quick check question: Can you explain why early concatenation may suffice for small datasets while attention mechanisms might overfit?

- Concept: **Transfer learning in medical imaging**
  - Why needed here: Pretrained encoders are critical when sample sizes are limited; understanding what transfers (textures, anatomical structures) guides initialization.
  - Quick check question: What properties should the pretraining task share with the target task for effective transfer?

- Concept: **3D CNNs for volumetric medical data**
  - Why needed here: PET/CT scans are 3D volumes; standard 2D architectures discard z-axis context important for lesion characterization.
  - Quick check question: Why might 3D kernels require more data than 2D, and how does this interact with pretraining?

## Architecture Onboarding

- Component map:
  Input (3D PET 75×50×50, 3D CT 75×50×50, 4 lab values) -> Separate 3D-CNN encoders (PET, CT) -> Flatten and concatenate embeddings with lab features -> 3-layer MLP -> Binary classification output

- Critical path:
  1. Preprocess imaging (artifact removal, harmonization, resize to 75×50×50)
  2. Train/fine-tune encoders with cross-entropy loss, Adam optimizer (lr=0.01, weight decay=0.2, dropout=0.1)
  3. Fuse modalities and train MLP classifier
  4. Evaluate via 3-fold cross-validation × 5 repeats (AUROC, AUPRC)

- Design tradeoffs:
  - Simple concatenation fusion vs. attention: Paper uses concatenation for reproducibility; attention may improve but risks overfitting at n=116
  - Pretrained CT only (not PET): CT pretraining readily available; PET pretraining resources are scarcer
  - Excluding censored patients: Simplifies classification but reduces usable data

- Failure signatures:
  - PET Only model (AUROC 0.42) shows exploding gradients, attention to bladder—indicates no learned signal
  - If fusion model AUROC drops below lab-only baseline (0.59), imaging may be adding noise
  - Large variance across CV folds suggests data splitting artifacts or insufficient regularization

- First 3 experiments:
  1. Replicate unimodal baselines (lab-only RF, PET-only 3D-CNN, CT-only 3D-CNN) to confirm reported AUROCs (0.59, 0.42, 0.54) before fusion
  2. Ablate pretraining: Train PET-CT-lab fusion with and without pretrained CT weights; expect ~0.03 AUROC difference per paper
  3. Test fusion robustness: Run 10 repeats of 3-fold CV with different random seeds; if standard error exceeds reported ±0.01–0.03, investigate data leakage or stratification issues

## Open Questions the Paper Calls Out

- Does the multimodal model maintain predictive performance when validated on external, multi-center cohorts?
  - Basis: The authors explicitly identify the "absence of an external validation cohort" as a key limitation
  - Why unresolved: Single-center retrospective analysis with limited sample size (n=116) risks overfitting and restricts generalizability
  - What evidence would resolve it: Replication of AUROC (0.72) and AUPRC (0.80) metrics on independent datasets from different medical institutions

- Can the integration of novel biomarkers (e.g., NETest) or genetic profiles improve the model's predictive accuracy beyond standard laboratory values?
  - Basis: The discussion notes that "future models that integrate such high-specificity biomarkers [NETest]... could further enhance the accuracy and clinical utility"
  - Why unresolved: Current model relies on limited routine laboratory biomarkers and excludes potentially informative data like genomic profiles
  - What evidence would resolve it: Comparative study demonstrating significant performance increase when additional data modalities are fused

- To what extent does heterogeneity of CT acquisition protocols (contrast-enhanced vs. non-contrast) impact the robustness of deep learning features?
  - Basis: Authors list "heterogeneity of CT acquisition protocols" as a specific methodological limitation
  - Why unresolved: Unclear if model relies on features specific to contrast enhancement or generalizes across both acquisition types
  - What evidence would resolve it: Subgroup analysis evaluating performance separately on contrast-enhanced and non-contrast cohorts

## Limitations
- Small sample size (n=116) limits generalizability and increases overfitting risk despite cross-validation
- Lack of external validation means reported performance metrics may not generalize to other centers
- Heterogeneity of CT acquisition protocols (contrast vs. non-contrast) may affect model robustness
- Exact 3D-CNN architecture details not specified, complicating exact reproduction

## Confidence
- **High confidence**: Multimodal fusion outperforms unimodal approaches (AUROC 0.72 vs 0.59 for lab-only, p<0.01)
- **Medium confidence**: Pretrained CT initialization improves performance (0.72 vs 0.69), though exact architecture details are unclear
- **Medium confidence**: Explainability findings showing tumor-focused attention in stable models vs. bladder focus in unstable models, though this analysis is novel with limited corpus support

## Next Checks
1. Replicate unimodal baselines (lab-only RF, PET-only, CT-only) to verify reported AUROCs (0.59, 0.42, 0.54) before attempting fusion
2. Perform ablation study with and without pretrained CT weights to confirm the ~0.03 AUROC improvement
3. Test fusion robustness with 10-fold CV and different random seeds to verify reported standard errors (±0.01-0.03) and rule out data leakage