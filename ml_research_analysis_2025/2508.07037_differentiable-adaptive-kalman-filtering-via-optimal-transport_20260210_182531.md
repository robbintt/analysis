---
ver: rpa2
title: Differentiable Adaptive Kalman Filtering via Optimal Transport
arxiv_id: '2508.07037'
source_url: https://arxiv.org/abs/2508.07037
tags:
- drift
- noise
- noise-statistics
- filtering
- adaptation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes OTAKNet, the first online solution to noise-statistics
  drift within learning-based adaptive Kalman filtering. The method uses optimal transport
  to connect the state estimate and the drift via one-step predictive measurement
  likelihood, enabling fully online adaptation without ground-truth labels or retraining.
---

# Differentiable Adaptive Kalman Filtering via Optimal Transport

## Quick Facts
- **arXiv ID:** 2508.07037
- **Source URL:** https://arxiv.org/abs/2508.07037
- **Reference count:** 6
- **Primary result:** First online solution for noise-statistics drift in learning-based adaptive Kalman filtering using optimal transport, achieving label-free adaptation without retraining

## Executive Summary
This paper introduces OTAKNet, a novel approach to adaptive Kalman filtering that addresses noise-statistics drift in real-time without requiring ground-truth labels or retraining. Unlike existing learning-based methods that rely on offline fine-tuning using batch pointwise matching, OTAKNet establishes a connection between state estimates and drift via one-step predictive measurement likelihood and addresses it using optimal transport. The method leverages OT's geometry-aware cost and stable gradients to enable fully online adaptation, demonstrated on both synthetic Lorenz attractor and real-world NCLT datasets.

## Method Summary
OTAKNet extends a pre-trained neural Kalman filter backbone with an online optimal transport-based adaptation module. At each timestep, the method constructs a source distribution from the filter's prior state estimate and innovation covariance via Monte Carlo sampling, and a target distribution from current observations shifted by past innovations. The Wasserstein-2 distance between these distributions, computed using IPOT, provides a differentiable loss that drives online parameter updates via Adam optimizer. A linear warm-up schedule stabilizes early adaptation when the residual buffer is small. The approach is validated on synthetic Lorenz Attractor data and real-world NCLT robot localization data, showing improved performance under noise-statistics drift compared to classical model-based adaptive filtering and offline learning-based methods.

## Key Results
- OTAKNet achieves superior performance under noise-statistics drift compared to classical adaptive Kalman filtering and offline learning-based filtering
- Ablation study shows MSE degradation without warm-up (-17.48 vs -18.45 dB) and without OT (-17.65 dB) components
- Limited training data robustness demonstrated: OTAKNet maintains performance when trained on only 3 NCLT trajectories versus full 13
- Method works effectively on both synthetic (Lorenz attractor) and real-world (NCLT) datasets

## Why This Works (Mechanism)

### Mechanism 1: One-step predictive measurement likelihood for drift characterization
The one-step predictive measurement likelihood characterizes the impact of noise-statistics drift on state estimation. A source distribution is constructed from the filter's prior state estimate and innovation covariance via Monte Carlo sampling, approximating the predictive measurement distribution under drifted noise statistics. A target distribution is built from current observations shifted by past innovations, encoding temporal drift information. The discrepancy between these distributions reflects the degree of noise-statistics drift. This mechanism relies on the innovation process being second-order stationary and ergodic.

### Mechanism 2: Wasserstein-2 distance for stable gradient computation
The Wasserstein-2 distance provides geometry-aware, stable gradients for detecting and correcting noise-statistics drift. Unlike KL divergence, the Wasserstein distance respects the metric structure of the measurement space and yields non-vanishing gradients even under large distributional shifts. The IPOT algorithm computes the optimal transport plan differentiably, enabling gradient propagation to filter parameters. This mechanism assumes the Polyak–Łojasiewicz condition holds for the OT loss, ensuring convergence of gradient descent.

### Mechanism 3: Label-free online parameter adaptation
Online parameter updates via OT loss achieve label-free adaptation without retraining. At each timestep, the OT loss is backpropagated through neural filter parameters using Adam optimizer. A linear warm-up schedule mitigates early instability when the residual buffer is small. Over successive steps, the source distribution converges toward the target, aligning the filter's internal covariance with the true innovation covariance. This mechanism requires sufficient temporal diversity in the innovation window to approximate drift statistics.

## Foundational Learning

- **Kalman filtering basics (predict-update cycle, innovation, covariance propagation):** Essential for understanding how OTAKNet extends standard Kalman architecture and interprets source distribution construction. Quick check: Can you derive the one-step predictive measurement distribution p(y_t | y_{1:t-1}) for a linear Gaussian system?

- **Optimal transport fundamentals (Wasserstein distance, Sinkhorn/IPOT algorithms):** Core contribution uses OT as differentiable loss; understanding coupling matrix computation and geometry preservation is crucial. Quick check: What is the computational complexity of Sinkhorn iterations, and why does entropic regularization enable differentiability?

- **Differentiable programming and gradient-based online learning:** OTAKNet performs test-time optimization; understanding gradient flow through sampling operations and the PL condition helps diagnose convergence issues. Quick check: Why might single-sample pointwise gradients fail for online adaptation, and how does distributional OT address this?

## Architecture Onboarding

- **Component map:** Neural Kalman Filter backbone -> Source sampler -> Innovation buffer -> Target constructor -> OT solver (IPOT) -> Optimizer (Adam)

- **Critical path:** 1) Run prior prediction to obtain mean and covariance; 2) Sample N source particles; 3) Compute current innovation and update buffer; 4) Construct W target pseudo-measurements; 5) Compute cost matrix and run IPOT for K iterations; 6) Backpropagate OT loss and update parameters; 7) Run Kalman update with adapted parameters

- **Design tradeoffs:** Window size W: larger W improves drift estimation but increases latency and memory (paper uses W=20); Particle count N: more particles improve source approximation but cost O(N²) per IPOT iteration; Learning rate warm-up: stabilizes early steps but delays adaptation

- **Failure signatures:** Early-step instability caused by insufficient residual history (mitigated by warm-up); Slow convergence under rapid drift if noise statistics change faster than W steps; Gradient explosion if cost matrix scale vs. ε mismatch

- **First 3 experiments:** 1) Static drift validation: Train KalmanNet backbone at ν=0dB, test OTAKNet across ν∈{-10,0,10,20,30}dB on Lorenz attractor; 2) Ablation study: Remove warm-up and OT components separately on synthetic data; 3) Limited training robustness: Train on only 3 NCLT trajectories vs. full 13 to validate OT adaptation's compensation ability

## Open Questions the Paper Calls Out

### Open Question 1
**Question:** Can the computational overhead of the OT-based adaptation module be reduced to support high-frequency control applications without sacrificing accuracy gains?
**Basis in paper:** Inferred from Table 4 reporting OTAKNet requires 21.02 seconds for inference on Lorenz Attractor versus 0.53 seconds for standard EKF (40x increase).
**Why unresolved:** Paper validates estimation accuracy but doesn't propose methods to optimize IPOT algorithm or parallelize sampling steps for real-time embedded systems.
**What evidence would resolve it:** Study demonstrating performance on high-frequency data streams (>100Hz) or analysis of latency-accuracy trade-off when reducing particles N or inner iterations L.

### Open Question 2
**Question:** How does the stationarity assumption regarding the innovation process affect stability when noise-statistics drift is abrupt or non-ergodic?
**Basis in paper:** Explicit in Appendix 7.1, Proposition 1 relies on "Assumption (A1): The innovations are second-order stationary and ergodic."
**Why unresolved:** While introduction claims method handles "highly maneuvering scenarios," theoretical convergence guarantees depend on innovation statistics remaining constant over window W, which may not hold during sudden environmental changes.
**What evidence would resolve it:** Theoretical analysis or empirical results evaluating transient response and convergence time when true noise covariance changes discontinuously rather than smoothly.

### Open Question 3
**Question:** Is there a theoretically grounded, adaptive mechanism for selecting sliding window length W and entropic regularization ε?
**Basis in paper:** Inferred from Section 4.3-4.4 where W (e.g., set to 20) and ε are treated as fixed hyperparameters.
**Why unresolved:** Fixed window length W imposes rigid memory on drift estimation; too small yields insufficient target population, too large causes slow reaction to rapid drift.
**What evidence would resolve it:** Ablation study varying W against different drift frequencies, or derivation showing how W could be dynamically adjusted based on variance of transport plan π.

## Limitations
- Key architectural details and hyperparameters (KalmanNet backbone, particle count N, regularization ε, IPOT iterations K) are underspecified, limiting reproducibility
- Method's robustness to rapid drift and non-stationary innovations is untested despite theoretical assumptions requiring second-order stationary and ergodic innovations
- Claims about convergence under PL condition and performance under non-ergodic processes lack empirical validation

## Confidence
- **High:** Core mechanism of using OT loss to align predictive measurement likelihood with observed innovations is theoretically sound and well-supported
- **Medium:** Ablation study results demonstrate individual mechanism contributions, but exact architectural details and hyperparameter choices remain unclear
- **Low:** Claims about convergence under PL condition and robustness to rapid drift are not empirically validated; behavior under non-stationary innovations is unknown

## Next Checks
1. **Convergence verification:** Implement KalmanNet backbone and OT adaptation module with reasonable hyperparameter guesses (N=50-100, ε=0.05-0.1, K=10-20 outer iterations). Measure OT loss trajectories and gradient norms to verify PL condition holds in practice.

2. **Robustness to drift rate:** Test OTAKNet on synthetic data with controlled drift onset times (W/2, W, 2W, 4W steps). Measure MSE degradation and adaptation lag to quantify sensitivity to drift speed relative to residual window W.

3. **Non-stationarity stress test:** Replace ergodic innovation assumption with non-stationary process (abrupt covariance changes, trending innovations). Evaluate whether OT loss converges to meaningful solution or diverges, and assess impact on state estimation accuracy.