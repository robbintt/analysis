---
ver: rpa2
title: A Certified Unlearning Approach without Access to Source Data
arxiv_id: '2506.06486'
source_url: https://arxiv.org/abs/2506.06486
tags:
- unlearning
- data
- dataset
- certified
- surrogate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles certified unlearning without access to the original
  training data, a practical challenge in privacy-sensitive applications. The authors
  propose a method that uses a surrogate dataset to approximate the statistical properties
  of the source data, and scales noise based on the statistical distance between them
  to achieve certified guarantees.
---

# A Certified Unlearning Approach without Access to Source Data

## Quick Facts
- arXiv ID: 2506.06486
- Source URL: https://arxiv.org/abs/2506.06486
- Reference count: 40
- One-line primary result: Achieves certified unlearning without source data by using surrogate datasets and statistical distance-calibrated noise injection, with utility comparable to methods with source access.

## Executive Summary
This paper addresses the practical challenge of certified unlearning when the original training data is unavailable due to privacy constraints. The authors propose a framework that leverages a surrogate dataset to approximate the statistical properties of the source data, enabling unlearning operations without direct access to the original training data. By scaling the noise injection based on the statistical distance between the surrogate and source distributions, the method achieves certified unlearning guarantees while maintaining comparable utility to traditional approaches that require source data access.

## Method Summary
The method estimates the Hessian of the retain set using a surrogate dataset and the Hessian of the forget set, then computes a Newton update to approximate the unlearned model. Noise is calibrated based on the statistical distance between the surrogate and source distributions to achieve certified guarantees. When exact TV distance is unavailable, a heuristic using KL divergence estimation via energy-based models and variational bounds provides an approximation. The final model is obtained by adding calibrated Gaussian noise to the Newton update.

## Key Results
- Achieves certified unlearning with utility comparable to methods requiring source data access
- Validated across CIFAR-10, StanfordDogs, Caltech256, and mixed-linear networks
- Demonstrates consistent performance and robustness across various settings including synthetic and real-world datasets
- Successfully handles forget ratios from 1% to 20% while maintaining certification guarantees

## Why This Works (Mechanism)

### Mechanism 1: Surrogate Hessian Substitution for Second-Order Updates
- Claim: The Hessian computed on surrogate data can approximate the source data Hessian sufficiently for certified unlearning when corrected for distributional shift.
- Mechanism: Estimate retain-set Hessian via ĤDr = (nHDs - mHDu)/(n-m), where HDs is the surrogate Hessian and HDu is the forget-set Hessian.
- Core assumption: Assumption 4.1 holds—the loss is L-Lipschitz, α-strongly convex, β-smooth, and γ-Hessian Lipschitz. Also assumes n1, n2 ≥ mβ/α.
- Evidence anchors:
  - [abstract] "uses a surrogate dataset to approximate the statistical properties of the source data"
  - [Section 4, Methodology] "we estimate the true Hessian HD using the Hessian of the surrogate dataset HDs"
- Break condition: If surrogate distribution ν diverges significantly from source distribution ρ, the Hessian approximation error grows, requiring correspondingly larger noise injection that may degrade utility.

### Mechanism 2: Statistical Distance-Calibrated Noise Injection
- Claim: Certifying unlearning without source data requires noise scaled proportionally to the distributional mismatch between surrogate and source.
- Mechanism: Gaussian noise with variance σ = (∆/ε)√(2ln(1.25/δ)), where ∆ bounds ∥w*r - ŵr∥₂ and explicitly incorporates TV(ρ∥ν).
- Core assumption: The total variation distance TV(ρ∥ν) is known or can be upper-bounded.
- Evidence anchors:
  - [abstract] "scales noise based on the statistical distance between them to achieve certified guarantees"
  - [Section 4.1, Theorem 4.2] Explicit bound: ∆ includes term (2mn₂β·TV(ρ∥ν))/((n₁α-mβ)(n₂α-mβ))
- Break condition: If TV distance is underestimated, certification guarantees weaken. Practical implementations using approximated distances yield "potentially weaker but still meaningful privacy guarantees."

### Mechanism 3: Heuristic KL Divergence Estimation via Energy-Based Models
- Claim: When exact TV distance is unavailable, KL divergence can be approximated using only the trained model and surrogate data through energy-based sampling and variational bounds.
- Mechanism: (1) Decompose KL(ν∥ρ) into conditional and marginal components; (2) Sample from approximated source marginal ρ̂(x) via SGLD using the implicit EBM in classifier w*; (3) Estimate marginal KL using Donsker-Varadhan variational representation.
- Core assumption: The trained model w* implicitly defines an energy-based model over inputs; the classifier's logits approximate conditional distributions sufficiently for KL estimation.
- Evidence anchors:
  - [Section 4.2] "we leverage the implicit energy-based model of the trained model w* to sample from the approximated input marginal distribution"
  - [Section 4.2, Proposition 4.6] Full decomposition formula for practical KL approximation
- Break condition: SGLD sampling quality depends on hyperparameters; Donsker-Varadhan estimation requires training a variational network. Poor convergence yields inaccurate KL estimates, affecting noise calibration.

## Foundational Learning

- Concept: **Second-Order Newton Updates for Influence Approximation**
  - Why needed here: The core unlearning operation uses Hessian-inverse × gradient to approximate the effect of removing data points.
  - Quick check question: Given a loss function at a minimum, what does the Hessian tell you about how parameters would shift if a subset of training data were removed?

- Concept: **Differential Privacy Gaussian Mechanism**
  - Why needed here: Certification relies on proving statistical indistinguishability between unlearned and retrained models.
  - Quick check question: For (ε, δ)-differential privacy with sensitivity ∆, what is the required noise variance and why does larger sensitivity require more noise?

- Concept: **Total Variation and KL Divergence as Distribution Distances**
  - Why needed here: The entire framework hinges on quantifying how "different" the surrogate distribution is from the source.
  - Quick check question: If TV(ρ∥ν) = 0.1 vs. TV(ρ∥ν) = 0.5, which scenario requires more noise injection and why?

## Architecture Onboarding

- Component map: Trained model w* -> Hessian estimator (ĤDr = (nHDs - mHDu)/(n-m)) -> Newton updater (ŵr = w* + (m/(n-m))ĤDr⁻¹∇L(Du, w*)) -> Distance estimator (SGLD sampler -> variational KL estimator -> TV bound) -> Noise calibrator (σ = (Δ/ε)√(2ln(1.25/δ))) -> Output layer (ŵ'r = ŵr + N(0, σ²I))

- Critical path: Hessian estimation accuracy → ∆ bound tightness → noise calibration → certification validity. The weakest link is typically distance estimation without source access.

- Design tradeoffs:
  - Tighter theoretical bounds (smaller ∆) vs. practical implementability (exact TV unknown)
  - Smaller surrogate-source distance (better certification) vs. surrogate availability constraints
  - Strong convexity assumption (enables clean theory) vs. neural network reality

- Failure signatures:
  - **Utility collapse**: Excessive noise from overestimated TV distance—check if σ is orders of magnitude larger than expected
  - **Certification failure**: MIA accuracy significantly above 50% suggests insufficient noise or poor Hessian approximation
  - **KL estimation divergence**: SGLD samples not converging—check energy function gradients and step size

- First 3 experiments:
  1. **Synthetic validation with known TV distance**: Generate source/surrogate from Gaussians with controlled covariance shift (vary ζ parameter). Verify theoretical ∆ bound holds and noise calibration achieves target (ε, δ) certification.
  2. **Real dataset with Dirichlet split**: Use CIFAR-10/StanfordDogs with concentration parameter ξ controlling class distribution mismatch between source and surrogate. Compare exact TV (with data access) vs. heuristic KL estimation accuracy.
  3. **Ablation on forget ratio**: Fix dataset, vary m/n from 1% to 20%. Verify that larger forget sets require proportionally larger noise (per ∆ formula) and confirm utility degradation remains bounded.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can rigorous certified unlearning guarantees be extended to general deep neural networks (DNNs) that inherently violate the strong convexity and Hessian Lipschitz assumptions required by the current theoretical framework?
- Basis in paper: [inferred] The paper notes in Section 5 that "the convexity and smoothness assumptions in Theorem 4.1 may not hold for general neural networks," which limits the applicability of the strict theoretical guarantees to specific architectures like mixed-linear networks.
- Why unresolved: Theoretical proofs for noise calibration rely on Assumption 4.1 (strong convexity, smoothness, Lipschitz Hessian) to bound the model update error, conditions generally not met by standard DNNs.
- What evidence would resolve it: Derivation of certification bounds for non-convex loss landscapes or empirical verification that the proposed noise scaling remains sufficient for certification in standard DNNs without these assumptions.

### Open Question 2
- Question: How can the gap between the heuristic estimation of statistical distance and the exact distance be bridged to prevent the weakening of privacy guarantees during practical implementation?
- Basis in paper: [explicit] The abstract states that "practical implementations typically approximate this distance, resulting in potentially weaker but still meaningful privacy guarantees," and Section 4.2 relies on heuristic approximations using KL divergence and Donsker-Varadhan representation.
- Why unresolved: The theoretical guarantees depend on the exact Total Variation (TV) distance, but the practical method uses an approximate KL divergence derived from energy-based modeling, introducing unquantified uncertainty into the certification budget.
- What evidence would resolve it: A theoretical analysis bounding the error of the heuristic distance estimation or a modified noise calibration mechanism that explicitly accounts for the approximation error to restore strict (ε, δ)-certification.

### Open Question 3
- Question: What are the optimal strategies for constructing or selecting a surrogate dataset in the absence of source data to minimize the required noise variance and maximize model utility?
- Basis in paper: [inferred] The paper assumes the existence of a surrogate dataset Ds and scales noise based on the distance TV(ρ∥ν). However, it does not address how to generate or choose Ds to minimize this distance, which directly impacts utility (Equation 5).
- Why unresolved: While the paper validates the method using existing datasets (e.g., MNIST/USPS, CIFAR subsets), it treats the surrogate dataset as a given input rather than an optimized component of the unlearning pipeline.
- What evidence would resolve it: A comparative study of surrogate generation techniques (e.g., generative models vs. public data selection) demonstrating a reduction in the statistical distance term and improved accuracy-retention trade-offs.

## Limitations

- The framework relies on strong convexity and smoothness assumptions that may not hold for general deep neural networks, limiting theoretical guarantees to specific architectures.
- The Hessian approximation error from surrogate data can become significant when distributional shift is large, potentially requiring excessive noise injection.
- The heuristic KL divergence estimation via energy-based models is sensitive to hyperparameter tuning and may introduce unquantified uncertainty into certification guarantees.

## Confidence

- **High confidence**: The second-order Newton update mechanism for certified unlearning and the basic Gaussian mechanism calibration for differential privacy guarantees. These are well-established techniques with predictable behavior.
- **Medium confidence**: The statistical distance-calibrated noise injection framework and the theoretical bounds on model update error. The bounds are mathematically sound under stated assumptions, but their tightness and practical relevance depend heavily on the distributional shift magnitude.
- **Low confidence**: The heuristic KL divergence estimation via energy-based models. This is a novel contribution with limited validation, and the quality of SGLD sampling and variational approximation can vary significantly across datasets and model architectures.

## Next Checks

1. **Distributional shift sensitivity**: Systematically vary the surrogate-source distance (using ζ parameter in synthetic Gaussians and Dirichlet concentration in real datasets) and measure how theoretical ∆ bounds, actual noise requirements, and utility degradation scale. This validates whether the TV-distance calibration mechanism works as intended.

2. **Hessian approximation quality**: Compare the surrogate-estimated Hessian ĤDr against the true Hessian (when source data is available for validation) across varying forget ratios and distributional shifts. This isolates whether certification failures stem from poor Hessian estimation or distance overestimation.

3. **KL estimation robustness**: Conduct ablation studies on the SGLD hyperparameters (step size, iterations, sample count) and variational network architecture. Compare heuristic KL estimates against ground-truth TV distance (when accessible) to establish error bounds and their impact on noise calibration accuracy.