---
ver: rpa2
title: 'NystagmusNet: Explainable Deep Learning for Photosensitivity Risk Prediction'
arxiv_id: '2512.17943'
source_url: https://arxiv.org/abs/2512.17943
tags:
- risk
- brightness
- movement
- nystagmus
- photosensitivity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of predicting photosensitivity
  risk in nystagmus patients, who experience involuntary eye movements worsened by
  environmental brightness. It introduces NystagmusNet, an AI-driven system that uses
  a dual-branch convolutional neural network (CNN) to combine environmental brightness
  and eye movement variance for risk prediction.
---

# NystagmusNet: Explainable Deep Learning for Photosensitivity Risk Prediction

## Quick Facts
- **arXiv ID:** 2512.17943
- **Source URL:** https://arxiv.org/abs/2512.17943
- **Reference count:** 21
- **Primary result:** 75% validation accuracy on synthetic data for photosensitivity risk prediction

## Executive Summary
NystagmusNet introduces an AI-driven system for predicting photosensitivity risk in nystagmus patients using a dual-branch convolutional neural network that combines environmental brightness and eye movement variance. The system employs synthetic and augmented datasets for training and achieves 75% validation accuracy. Explainability is enhanced through SHAP and GradCAM techniques, providing visual and feature-based interpretations of predictions. A rule-based recommendation engine suggests adaptive visual filters based on risk scores, with future work focusing on real-world validation and reinforcement learning for personalization.

## Method Summary
NystagmusNet uses a dual-branch CNN architecture where Branch 1 processes 128×128 grayscale environmental images through three convolutional blocks to produce 32-dimensional features, while Branch 2 processes 1D eye movement variance through fully connected layers. The two 32-dimensional feature vectors are concatenated and passed through fully connected layers with sigmoid activation to output a continuous risk score [0,1]. The system is trained on synthetic data with brightness (300-1200 lux) and variance (2-10 pixels) values, augmented with Gaussian noise, brightness jittering, and synthetic blurring. Explainability is provided through SHAP values for feature attribution and GradCAM for spatial heatmap visualization of risk zones.

## Key Results
- 75% validation accuracy achieved on synthetic dataset
- SHAP values reveal brightness (Feature 0) contributes more to risk predictions than eye movement variance (Feature 1)
- GradCAM heatmaps successfully highlight spatial risk zones in environmental images
- Rule-based recommendation engine maps risk scores to filter suggestions: Dark Amber (≥0.7), Cool Grey (0.4-0.7), No Filter (<0.4)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A dual-branch CNN architecture can fuse environmental brightness and eye movement variance into a unified photosensitivity risk score.
- Mechanism: Branch 1 processes 128×128 grayscale environmental images through three convolutional blocks (Conv2D → BatchNorm → ReLU → MaxPool), producing a 32-dimensional feature vector. Branch 2 processes 1D eye movement variance through two fully connected layers (64→32 units) with dropout (p=0.3). The fusion module concatenates both 32-dim vectors (64 features total) and passes through two FC layers with sigmoid activation to output a continuous risk score [0,1].
- Core assumption: Environmental brightness and eye movement variance are independently processable modalities whose interaction can be captured through late concatenation rather than early fusion.
- Evidence anchors:
  - [abstract]: "Using a dual-branch convolutional neural network trained on synthetic and augmented datasets, the system estimates a photosensitivity risk score based on environmental brightness and eye movement variance."
  - [section 3.2]: "Branch 1 (Environmental Brightness): Processes 128×128 grayscale environmental images through three convolutional blocks... Branch 2 (Eye Movement Variance): Processes a 1D vector representing eye movement variance through two fully connected layers."
  - [corpus]: Related work (references [13, 14]) discusses dual-branch fusion for multimodal medical imaging; corpus neighbors show multimodal integration in other domains but not specifically for nystagmus.
- Break condition: If early cross-modal interactions dominate the underlying physics (e.g., brightness modulates eye movement in real-time), late concatenation may miss critical temporal dependencies.

### Mechanism 2
- Claim: SHAP and GradCAM provide post-hoc interpretability that maps risk predictions back to clinically meaningful features and spatial regions.
- Mechanism: SHAP values quantify per-feature contribution to individual predictions, revealing whether brightness or eye movement variance drives each risk score. GradCAM computes gradient-weighted activations from the final convolutional layer to generate heatmaps over environmental images, highlighting spatial risk zones (warm colors) versus safe zones (cool colors).
- Core assumption: Post-hoc attributions from a trained model accurately reflect the true decision boundary rather than artifacts of training data distribution.
- Evidence anchors:
  - [abstract]: "Explainability techniques including SHAP and GradCAM are integrated to highlight environmental risk zones, improving clinical trust and model interpretability."
  - [section 3.4]: "SHAP values enable clinicians to understand which input factors most strongly influence risk assessments... GradCAM generates visual heatmaps highlighting regions in environmental images that contribute most to risk predictions."
  - [corpus]: Corpus neighbors reference GradCAM in medical imaging contexts; one paper explicitly addresses interpretable deep learning for fluorescence microscopy with similar XAI goals.
- Break condition: If synthetic training data introduces distributional artifacts, SHAP/GradCAM explanations may reflect synthetic patterns rather than real clinical features.

### Mechanism 3
- Claim: A rule-based recommendation engine can translate continuous risk scores into actionable adaptive filter suggestions.
- Mechanism: The system applies threshold rules to the sigmoid output: risk ≥ 0.7 → Dark Amber (maximum protection); 0.4–0.7 → Cool Grey (moderate); <0.4 → No filter. The engine also supports personalization via user feedback on filter warmth and intensity.
- Core assumption: Photosensitivity management can be discretized into three intervention zones with fixed filter mappings, and individual preferences vary only within filter type/intensity rather than threshold boundaries.
- Evidence anchors:
  - [section 3.5]: "Risk Score ≥ 0.7: Recommend 'Dark Amber' filter... Risk Score 0.4-0.7: Recommend 'Cool Grey' filter... Risk Score < 0.4: No adaptation required."
  - [section 5.1]: "Risk adaptation currently relies on predefined thresholds and fixed filter mappings, lacking the fluidity required for highly personalized, context-sensitive real-world recommendations."
  - [corpus]: No direct corpus evidence for filter-recommendation mechanisms in assistive vision; corpus neighbors focus on risk prediction without intervention engines.
- Break condition: If individual photosensitivity thresholds vary significantly across patients, fixed thresholds will underperform for edge cases, requiring the reinforcement learning personalization proposed in future work.

## Foundational Learning

- Concept: Multimodal fusion architectures
  - Why needed here: Understanding how separately processed modalities (images vs. 1D signals) combine at fusion layers clarifies why the architecture uses late concatenation and what interactions it can/cannot capture.
  - Quick check question: Can you explain why late fusion might fail if brightness and eye movement interact multiplicatively rather than additively?

- Concept: SHAP (Shapley Additive Explanations)
  - Why needed here: SHAP provides feature-level attribution; understanding its additive nature helps interpret why brightness (Feature 0) shows higher mean absolute contribution than eye movement variance (Feature 1) in Figure 6.
  - Quick check question: If SHAP shows brightness contributes more, does that prove brightness causally determines risk, or only that the model learned to weight it more heavily?

- Concept: GradCAM for CNN interpretability
  - Why needed here: GradCAM generates spatial heatmaps from convolutional feature maps; knowing it uses final-layer gradients helps understand why it highlights coarse regions rather than pixel-precise zones.
  - Quick check question: What information would be lost if GradCAM were applied to an earlier convolutional layer versus the final layer?

## Architecture Onboarding

- Component map:
  - Data pipeline: Synthetic data generator (brightness 300–1200 lux, variance 2–10 pixels) → preprocessing (normalization, Gaussian noise σ=0.05, jitter ±10%, synthetic blur)
  - Branch 1: Input (128×128 grayscale) → 3× ConvBlock (Conv2D, BatchNorm, ReLU, MaxPool) → Flatten → Dense(32)
  - Branch 2: Input (1D variance vector) → FC(64, ReLU, dropout 0.3) → FC(32, ReLU, dropout 0.3)
  - Fusion: Concat(64) → FC layers → Sigmoid → Risk score [0,1]
  - Explainability: SHAP (post-fusion feature attribution), GradCAM (spatial heatmaps on Branch 1)
  - Recommendation engine: Rule-based threshold mapper → filter suggestion

- Critical path: Environmental image → Branch 1 convolutions → 32-dim feature vector → Fusion concatenation → Risk score → Threshold rule → Filter recommendation. Breaks in Branch 1 convolution or fusion will propagate directly to incorrect risk scores and inappropriate recommendations.

- Design tradeoffs:
  - Late fusion vs. early fusion: Late fusion preserves modality-specific processing but may miss cross-modal interactions.
  - Synthetic vs. real data: Synthetic enables rapid prototyping but risks distribution shift; paper explicitly notes this limitation.
  - Rule-based vs. learned recommendations: Rules provide interpretability and safety bounds but lack personalization fluidity.

- Failure signatures:
  - Overconfident risk scores near 0.5: May indicate fusion layer failing to discriminate; check class balance and loss curves.
  - GradCAM highlights only image edges/corners: Suggests convolutional features not learning semantic risk zones; inspect intermediate activations.
  - SHAP shows near-zero contributions for both features: Model may be underfitting; verify training convergence and learning rate.
  - Recommendation engine always outputs same filter: Check for risk score distribution collapse or threshold logic errors.

- First 3 experiments:
  1. Ablation study: Train single-branch models (brightness-only, variance-only) and compare validation accuracy to dual-branch to quantify fusion benefit.
  2. Noise sensitivity test: Inject varying Gaussian noise levels (σ = 0.01, 0.05, 0.1) into validation data; measure risk score variance to assess robustness.
  3. Threshold boundary analysis: Plot precision/recall at risk thresholds 0.3, 0.4, 0.5, 0.6, 0.7 to validate whether paper's chosen thresholds are optimal or arbitrary.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the model's performance and generalization capability change when validated against real-world clinical data from nystagmus patients compared to the synthetic dataset?
- Basis in paper: [explicit] The authors explicitly state in the Limitations and Future Directions that the "model is trained primarily on synthetic data" and list "Real Patient Data Collection" as a primary future step.
- Why unresolved: The 75% validation accuracy is achieved solely on synthetic data which may not capture the "nuances of real-world eye movement patterns" or individual variations in photosensitivity.
- What evidence would resolve it: Validation accuracy, sensitivity, and specificity metrics derived from training and testing the model on actual eye-tracking and brightness data collected from ophthalmology clinics.

### Open Question 2
- Question: Can a reinforcement learning agent effectively replace the static rule-based engine to provide personalized, adaptive filter recommendations for individual users?
- Basis in paper: [explicit] The paper identifies the current "Static Recommendation Engine" as a limitation and explicitly proposes "incorporating real-time user feedback using reinforcement learning" in Future Directions.
- Why unresolved: The current system relies on fixed thresholds (e.g., Risk Score >= 0.7), which lacks the fluidity needed for highly personalized, context-sensitive recommendations.
- What evidence would resolve it: A comparative study showing that an RL-based agent improves user comfort scores or reduces eye movement variance over time compared to the static rule-based approach.

### Open Question 3
- Question: What are the trade-offs in model accuracy and inference latency when optimizing the dual-branch CNN architecture for edge deployment on smart glasses?
- Basis in paper: [inferred] The authors mention "Device Integration Complexity" including "power efficiency" as a limitation and "Wearable Integration" as a future direction, implying the current prototype is not optimized for edge hardware.
- Why unresolved: The current architecture (dual-branch with multiple convolutional blocks) may be too computationally heavy for the limited processing power of standard smart glasses.
- What evidence would resolve it: Benchmarking results showing frames-per-second (FPS) and battery consumption on target wearable hardware while maintaining acceptable risk prediction accuracy.

## Limitations

- Model performance based entirely on synthetic data without real-world clinical validation
- Fixed threshold-based recommendation engine lacks personalization for diverse patient populations
- Edge deployment feasibility not demonstrated; current architecture may be too computationally intensive for smart glasses

## Confidence

- **High confidence**: Architectural design and implementation details (dual-branch CNN structure, SHAP/GradCAM integration)
- **Medium confidence**: Synthetic data approach given explicit acknowledgment of this limitation
- **Low confidence**: Recommendation engine thresholds without clinical validation data

## Next Checks

1. Generate a new synthetic dataset using the same specified parameters (brightness 300-1200 lux, variance 2-10 pixels, augmentations) and verify the model achieves similar validation accuracy
2. Conduct ablation studies comparing single-branch vs. dual-branch architectures to quantify the fusion benefit
3. Perform GradCAM sensitivity analysis by testing on synthetic validation images with known brightness patterns to verify heatmaps correctly highlight high-brightness regions