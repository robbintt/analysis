---
ver: rpa2
title: Improving Cardiac Risk Prediction Using Data Generation Techniques
arxiv_id: '2512.20669'
source_url: https://arxiv.org/abs/2512.20669
tags:
- data
- risk
- latent
- cardiac
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of limited and imbalanced clinical
  data in cardiac rehabilitation programs, which hinders accurate cardiac risk prediction.
  The authors propose a Conditional Variational Autoencoder (CVAE) architecture enhanced
  with contrastive learning, sparsity-inducing regularization, and guided latent-space
  interpolation to generate realistic synthetic clinical records.
---

# Improving Cardiac Risk Prediction Using Data Generation Techniques

## Quick Facts
- arXiv ID: 2512.20669
- Source URL: https://arxiv.org/abs/2512.20669
- Authors: Alexandre Cabodevila; Pedro Gamallo-Fernandez; Juan C. Vidal; Manuel Lama
- Reference count: 40
- One-line primary result: Conditional VAE with contrastive learning and guided interpolation improves cardiac risk prediction F1-score up to 0.7153 on imbalanced clinical data.

## Executive Summary
This study addresses the challenge of limited and imbalanced clinical data in cardiac rehabilitation programs, which hinders accurate cardiac risk prediction. The authors propose a Conditional Variational Autoencoder (CVAE) architecture enhanced with contrastive learning, sparsity-inducing regularization, and guided latent-space interpolation to generate realistic synthetic clinical records. The model is designed to produce coherent patient traces conditioned on risk status, preserving semantic validity. Evaluation on a cardiac rehabilitation dataset (811 patients, 91 variables) shows that classifiers trained on data augmented by the proposed SCCVAE model achieve significant improvements in F1-score (up to 0.7153 for the at-risk class), outperforming state-of-the-art generative models like CTGAN, TTVAE, and WGAN-GP. The generated samples also accurately reflect class-specific clinical conditions, demonstrating both high fidelity and clinical coherence.

## Method Summary
The method employs a Conditional Variational Autoencoder (CVAE) with embedding dimension 32, latent dimension 64, and class-conditional generation. The model uses cyclical β annealing (4 cycles, ratio 0.9, β_max=1), InfoNCE contrastive loss (τ=0.5, α=0.1), and L1 sparsity penalty (λ=10⁻³). Total loss combines cross-entropy reconstruction, KL divergence, and contrastive terms. Synthetic samples are generated via SMOTE-like latent space interpolation using nearest neighbors. The cardiac rehabilitation dataset (811 patients, 91 variables) is discretized to categorical format, with risk labels derived from 15% improvement thresholds in ergometry metrics. Generated samples augment training data (2× multiplier) for downstream classifiers (XGBoost, Random Forest).

## Key Results
- SCCVAE achieves F1-risk of 0.7153, outperforming CTGAN (0.4643), TTVAE (0.6081), and WGAN-GP (0.4433) on the at-risk class.
- Class-conditional accuracy reaches 100% for SCCVAE vs. 24.53% for CTGAN, demonstrating superior semantic validity.
- 2× augmentation consistently outperforms 5× augmentation, suggesting excessive synthetic data introduces noise.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Conditional generation preserves class-specific clinical semantics in synthetic patient traces.
- Mechanism: The CVAE conditions both encoder and decoder on risk status (y), forcing the latent distribution q(z|x,y) and reconstruction p(x|z,y) to be risk-class-dependent. This ensures that sampling from p(z|y="at-risk") generates traces consistent with at-risk physiology (e.g., no >15% improvement in VO₂ peak, Watts, METs, or exercise duration).
- Core assumption: The conditioning variable captures the primary source of variation relevant to downstream prediction tasks.
- Evidence anchors:
  - [abstract] "The model is designed to produce coherent patient traces conditioned on risk status, preserving semantic validity."
  - [section 5.2] "The condition represents the class associated with the patient, that is, whether the patient is at risk or has improved during the program."
  - [corpus] Weak direct evidence; corpus papers focus on imaging-based risk assessment, not tabular conditional generation.
- Break condition: If class labels are noisy or if within-class variation exceeds between-class variation, conditional generation may produce inconsistent samples.

### Mechanism 2
- Claim: Contrastive learning structures the latent space to improve class separability and sample diversity.
- Mechanism: InfoNCE loss (L_NCE) pulls same-class latent representations closer while pushing different-class representations apart. Combined with the ELBO, this creates a tradeoff where the encoder learns representations that are both reconstructively faithful and semantically organized, reducing mode collapse in low-data regimes.
- Core assumption: Positive pairs (same-class samples) share meaningful features that should cluster in latent space.
- Evidence anchors:
  - [section 5.4] "This formulation encourages similar latent representations to remain close while separating distinct ones, improving the semantic organization of the latent space."
  - [results] SCCVAE (with contrastive loss) outperforms SCVAE (without contrastive loss) across all classifiers.
  - [corpus] No direct validation; contrastive learning for tabular clinical VAEs is underexplored in corpus.
- Break condition: If τ (temperature) is too low, gradients become unstable; if α (contrastive weight) is too high, reconstruction suffers.

### Mechanism 3
- Claim: Guided latent-space interpolation via SMOTE improves coverage of underrepresented regions without sacrificing fidelity.
- Mechanism: Instead of pure random sampling from N(0,1), latent vectors are generated by interpolating between real encoded representations using Euclidean nearest-neighbor selection. This anchors synthetic samples to observed data manifolds while filling gaps in sparse regions.
- Core assumption: Interpolating between real latent encodings produces clinically plausible intermediate samples.
- Evidence anchors:
  - [section 5.3] "This strategy allows the generation of more realistic and representative synthetic data by preserving local coherence in the latent space."
  - [results] Table 3 shows 100% class-conditional accuracy for SCCVAE vs. 24.53% for CTGAN on at-risk class.
  - [corpus] No validation; SMOTE in latent space is not addressed in corpus papers.
- Break condition: If latent space is poorly structured, interpolation may traverse semantically invalid regions.

## Foundational Learning

- Concept: Variational Autoencoders (VAEs) and the ELBO objective
  - Why needed here: The paper builds on VAE theory (KL divergence, reparameterization trick, β-annealing). Without understanding the reconstruction-regularization tradeoff, the cyclical β schedule and contrastive loss integration won't make sense.
  - Quick check question: Can you explain why minimizing KL divergence to N(0,1) enables generation but can cause posterior collapse?

- Concept: Contrastive learning and InfoNCE loss
  - Why needed here: The α-weighted contrastive term is central to SCCVAE's performance gains. Understanding positive/negative pairs and temperature scaling is essential for tuning.
  - Quick check question: What happens to gradient magnitude if the temperature τ is set too low (e.g., 0.01)?

- Concept: Class imbalance and F1-score evaluation
  - Why needed here: The dataset is imbalanced (most patients improve), making accuracy misleading. The paper uses F1-risk and weighted F1 to evaluate minority-class performance.
  - Quick check question: Why would a model achieving 90% accuracy on this dataset potentially be useless clinically?

## Architecture Onboarding

- Component map:
  - Attribute embeddings → concatenation with condition → linear layers → μ and log(σ²) → reparameterized z
  - L1-sparsified z concatenated with embedded condition → linear layers → per-attribute categorical distributions

- Critical path:
  1. Embed each categorical attribute independently (s_emb=32)
  2. Concatenate attribute embeddings with condition embedding
  3. Sample z via reparameterization from class-conditional distribution
  4. Apply L1 penalty to z before decoder
  5. Reconstruct per-attribute distributions and compute CE loss
  6. Apply cyclical β (4 cycles, ratio 0.9) and contrastive loss (α=0.1, τ=0.5)

- Design tradeoffs:
  - Higher α improves latent structure but risks reconstruction degradation
  - Lower latent dimension (h<64) loses information; higher increases cost without gains
  - 2× augmentation outperforms 5× in many cases (noise introduction risk)

- Failure signatures:
  - Posterior collapse: KLD drops to near-zero early; z ignored by decoder → cyclical β mitigates
  - Class leakage: Generated samples violate class conditions → check conditional accuracy (Table 3)
  - Over-regularization: Reconstruction loss dominates, synthetic samples are near-identical → reduce β_max or α

- First 3 experiments:
  1. Baseline SCCVAE with paper hyperparameters (h=64, s_emb=32, β-cycles=4, α=0.1, τ=0.5) on 2× augmented data; evaluate F1-risk on held-out test set.
  2. Ablation: Remove contrastive loss (SCVAE variant) to quantify its contribution; expect ~0.02-0.06 F1-risk drop.
  3. Ablation: Remove L1 sparsity (CCVAE variant) and compare conditional accuracy; expect slight increase in class leakage.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would employing hierarchical latent spaces or non-Gaussian prior distributions enhance the model's ability to capture complex clinical data distributions compared to the standard Gaussian assumption?
- Basis in paper: [explicit] The conclusion explicitly states that future work should explore "reference distributions more complex than N(0,1)... deeper architectures or hierarchical latent spaces" to enhance generative capacity.
- Why unresolved: The current architecture relies on a standard Gaussian prior and a flat latent space, which may limit the nuance and complexity of the generated cardiac traces.
- What evidence would resolve it: Experiments comparing the standard SCCVAE against variants using hierarchical latent structures or learned priors on the same clinical dataset.

### Open Question 2
- Question: Can the generative model be adapted to capture the full sequential evolution of a cardiac rehabilitation program, rather than aggregating patient data into static tabular records?
- Basis in paper: [explicit] The authors note that using a "more complete and information-rich dataset" including variables for all relevant events would allow "patient traces to form more comprehensive sequential structures."
- Why unresolved: The current study was forced to use a tabular representation (one row per patient) because the available dataset lacked measurements for intermediate clinical events.
- What evidence would resolve it: Applying the model to a dataset with dense, session-by-session measurements and evaluating the temporal fidelity of the generated traces.

### Open Question 3
- Question: What is the optimal ratio of synthetic to real data for augmentation, given that excessive synthetic samples appear to introduce noise?
- Basis in paper: [inferred] The results show that classifiers trained on doubled-size synthetic sets often outperformed those trained on quintupled sets, leading the authors to state that "excessive generation may introduce noise."
- Why unresolved: The study only evaluated fixed multipliers (x2 and x5), leaving the precise "tipping point" where augmentation degrades model generalization unidentified.
- What evidence would resolve it: A parametric study varying the volume of synthetic data to identify the peak in classifier F1-scores before performance declines.

## Limitations

- The study relies on a small, specialized cardiac rehabilitation dataset (811 patients) without external validation.
- The conditioning variable (risk status) is derived from a fixed 15% improvement threshold that may not generalize across clinical settings.
- The ablation experiments are limited, with key design choices not individually validated on multiple datasets.
- Contrastive learning for tabular VAEs lacks precedent in corpus literature.
- The model assumes the condition fully captures class semantics, which may fail if within-class heterogeneity is high.

## Confidence

- Conditional generation preserving semantics: Medium (strong ablation but limited external validation)
- Contrastive loss improving separability: Low (no corpus precedent, limited ablation scope)
- Guided interpolation filling gaps: Low (novel approach, no validation beyond this dataset)

## Next Checks

1. Replicate the ablation study across 3+ diverse clinical tabular datasets to isolate the impact of each SCCVAE component.
2. Test robustness to noisy or mis-specified risk labels by perturbing the 15% threshold and measuring conditional accuracy degradation.
3. Compare against non-generative augmentation methods (e.g., SMOTE, ADASYN) on the same task to quantify whether VAE-based synthesis adds unique value.