---
ver: rpa2
title: 'FedPPA: Progressive Parameter Alignment for Personalized Federated Learning'
arxiv_id: '2510.14698'
source_url: https://arxiv.org/abs/2510.14698
tags:
- client
- fedppa
- clients
- data
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses federated learning in settings where clients
  have heterogeneous model architectures and non-IID data distributions, which degrade
  personalization performance. The authors propose FedPPA, a progressive parameter
  alignment method that aligns common layers between client models and the global
  model to mitigate knowledge inconsistencies while preserving local personalization.
---

# FedPPA: Progressive Parameter Alignment for Personalized Federated Learning

## Quick Facts
- arXiv ID: 2510.14698
- Source URL: https://arxiv.org/abs/2510.14698
- Authors: Maulidi Adi Prasetia; Muhamad Risqi U. Saputra; Guntur Dharma Putra
- Reference count: 26
- Primary result: Achieves up to 98.87% accuracy on CIFAR-10 and 100% on MNIST/F-MNIST in extreme non-IID scenarios with heterogeneous VGG architectures

## Executive Summary
FedPPA addresses the challenge of personalized federated learning with heterogeneous model architectures and non-IID data distributions. The method introduces progressive parameter alignment that aligns common layers between client models and the global model through L2 feature space minimization, rather than direct weight replacement. This approach preserves local personalization while mitigating knowledge inconsistencies during federated aggregation. FedPPA+ extends this with entropy-based weighted averaging to better handle non-IID data distributions.

## Method Summary
FedPPA operates in federated learning rounds where each client trains locally on private data, then participates in aggregation that only considers architecturally-matching common layers. Instead of directly replacing client weights with aggregated parameters, FedPPA extracts intermediate layer outputs (features) from both the current local model and the aggregated model using the client's local data. It then minimizes the L2 distance between these feature representations via gradient descent before updating the client model. This progressive alignment is performed iteratively for each common layer. FedPPA+ adds entropy-based weighting where clients with more diverse label distributions receive higher influence in the global aggregation, based on the assumption that label diversity correlates with generalizable knowledge contribution.

## Key Results
- Personalization accuracy reaches 98.87% on CIFAR-10 and 100% on MNIST/F-MNIST under extreme non-IID conditions (α=0.01)
- Outperforms baseline methods including FedAvg and Max-Common aggregation strategies
- Demonstrates faster convergence compared to existing personalized federated learning approaches
- Maintains strong personalization accuracy while showing improved global model performance with FedPPA+ extension

## Why This Works (Mechanism)

### Mechanism 1: Progressive Parameter Alignment
Aligning aggregated global parameters with client-specific features before overwriting local weights reduces knowledge inconsistency while preserving personalization. Rather than direct weight replacement, FedPPA minimizes L2 distance between feature representations extracted from common layers of both models using local data, performed iteratively for each common layer. This assumes feature representations in common layers encode transferable knowledge that can be aligned without destroying client-specific personalization.

### Mechanism 2: Common Layer Extraction for Heterogeneous Architectures
Restricting aggregation to architecturally-matching layers enables knowledge sharing across heterogeneous models without enforcing uniform architectures. The ExtractMaxCommonLayers function identifies layers that match in both type and sequential order across client models, with only these common layers participating in aggregation. This assumes common layers across different architectures encode functionally similar feature hierarchies that benefit from shared learning.

### Mechanism 3: Entropy-Based Weighted Averaging
Weighting client contributions by label-distribution entropy improves global model robustness under non-IID data distributions. FedPPA+ computes normalized Shannon entropy from each client's label distribution, giving clients with more diverse label distributions higher influence on the global model. This assumes label diversity correlates positively with the generalizable knowledge a client contributes.

## Foundational Learning

- **Concept: Federated Learning Loop (Local Training → Aggregation → Distribution)**
  - Why needed here: FedPPA modifies the standard FL client update step. Understanding where parameter alignment fits in the round structure is prerequisite.
  - Quick check question: At which point in the FL round does FedPPA perform parameter alignment—before local training, during local training, or after receiving the global model?

- **Concept: Base/Head Personalization Split**
  - Why needed here: FedPPA contrasts with traditional PFL that divides models into shared base and personalized head layers. The paper critiques this approach's assumption of uniform architecture.
  - Quick check question: Why does the base/head split fail when clients have heterogeneous computational resources?

- **Concept: Feature-Space Alignment vs. Weight-Space Copying**
  - Why needed here: The core technical contribution uses feature-matching rather than direct weight replacement. Understanding this distinction is essential.
  - Quick check question: What information does L2 feature alignment preserve that direct weight copying might destroy?

## Architecture Onboarding

- **Component map:**
  LocalTraining -> ExtractMaxCommonLayers -> GlobalAggregator -> FeatureExtractor -> ParameterAligner -> EntropyCalculator

- **Critical path:**
  Local training → Extract common layers across clients → Aggregate with (optional) entropy weighting → For each client: extract local + aggregated features → Align via L2 minimization → Update and distribute

- **Design tradeoffs:**
  - Personalization vs. global accuracy: FedPPA optimizes personalization; global performance is secondary (Table IV shows 38-79% global accuracy)
  - Computation vs. alignment quality: Feature extraction and alignment add per-round overhead
  - Architecture diversity vs. aggregation capacity: More heterogeneous architectures reduce common-layer overlap

- **Failure signatures:**
  - Empty or minimal common-layer set: Architectures share insufficient structure
  - Alignment loss diverges: Feature spaces too dissimilar for meaningful alignment
  - Entropy weights collapse to single client: One client dominates aggregation

- **First 3 experiments:**
  1. Replicate Scenario 1 (α=0.5, MNIST, 8 clients with VGG-11/13/16/19) to verify baseline personalization accuracy (~99%)
  2. Run extreme non-IID (α=0.01) on MNIST to validate the reported 100% personalization accuracy claim
  3. Compare FedPPA vs. FedPPA+ on CIFAR-10 global performance (Table IV) to confirm entropy-weighting tradeoffs

## Open Questions the Paper Calls Out

### Open Question 1
**Question:** Does FedPPA maintain convergence stability and computational efficiency when scaled to hundreds or thousands of participating clients?
**Basis in paper:** The authors acknowledge that evaluations were conducted on only eight FL nodes and that "further exploration in scenarios involving hundreds of participating nodes" is required to confirm model convergence.
**Why unresolved:** The current experimental scope is small-scale; communication overhead and alignment complexity may grow non-linearly with a massive increase in client count.
**What evidence would resolve it:** Empirical results demonstrating convergence curves, accuracy retention, and resource consumption metrics in simulations involving >100 heterogeneous clients.

### Open Question 2
**Question:** How does FedPPA perform when applied to non-sequential or structurally complex architectures such as Transformers or ResNets?
**Basis in paper:** The paper notes the method is limited to "VGG model variations" and states that results may differ "when the proposed methods are tested on transformer-based models."
**Why unresolved:** VGG architectures are purely sequential; the progressive alignment strategy may not effectively map to models with skip connections, branching, or attention mechanisms where layer order and dimensionality are less consistent.
**What evidence would resolve it:** Benchmarking results on standard datasets (e.g., CIFAR-100 or ImageNet) using heterogeneous Transformer or ResNet architectures.

### Open Question 3
**Question:** Can the trade-off between local personalization accuracy and global model generalizability be optimized beyond the current FedPPA+ implementation?
**Basis in paper:** The authors state in the conclusion that "future work may be directed toward balancing personalization and global performance."
**Why unresolved:** While FedPPA+ improves global performance on some datasets, Table IV shows it actually degrades global accuracy on MNIST compared to FedPPA, suggesting the trade-off mechanism is not yet robust.
**What evidence would resolve it:** A modified aggregation or alignment strategy that consistently improves both global accuracy and local personalization across all tested datasets without degradation in either metric.

## Limitations
- Architecture scope limitation: Validated only on VGG architectures with identical layer type ordering; cannot handle fundamentally different architecture families
- Non-IID mechanism validation: Entropy weighting assumption (label diversity correlates with knowledge quality) is not empirically validated
- Generalization beyond tested settings: Experiments use only 8 clients and small-scale datasets; performance with larger client pools remains unknown

## Confidence
- **High confidence:** The core mechanism of progressive parameter alignment (L2 feature space minimization) is technically sound and well-implemented
- **Medium confidence:** The entropy-based weighting approach shows promise but lacks rigorous validation
- **Low confidence:** Claims about scalability to diverse real-world heterogeneous architectures are not supported by the experimental evidence

## Next Checks
1. **Mechanism ablation:** Run experiments comparing FedPPA with and without progressive alignment (direct weight replacement) to quantify the specific contribution of L2 feature space minimization versus the aggregation strategy itself
2. **Entropy weighting validation:** Test alternative client weighting schemes (uniform, data-size weighted, gradient-norm weighted) alongside entropy weighting to isolate the benefit of the entropy-based approach
3. **Architecture generalization:** Evaluate FedPPA on heterogeneous architecture pairs beyond VGG (e.g., ResNet vs. MobileNet) to identify the method's breaking points and minimum common-layer requirements for effective knowledge transfer