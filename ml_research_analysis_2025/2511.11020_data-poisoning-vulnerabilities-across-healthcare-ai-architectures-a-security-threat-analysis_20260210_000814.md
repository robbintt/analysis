---
ver: rpa2
title: 'Data Poisoning Vulnerabilities Across Healthcare AI Architectures: A Security
  Threat Analysis'
arxiv_id: '2511.11020'
source_url: https://arxiv.org/abs/2511.11020
tags:
- data
- healthcare
- poisoning
- clinical
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Healthcare AI systems face critical vulnerabilities to data poisoning
  attacks that current defenses and regulatory frameworks cannot adequately address.
  The study analyzed eight attack scenarios across four categories, demonstrating
  that adversaries require only 100-500 poisoned samples to compromise healthcare
  AI systems, achieving over 60% success rates with detection taking 6-12 months or
  sometimes never occurring.
---

# Data Poisoning Vulnerabilities Across Healthcare AI Architectures: A Security Threat Analysis

## Quick Facts
- arXiv ID: 2511.11020
- Source URL: https://arxiv.org/abs/2511.11020
- Reference count: 0
- Primary result: Healthcare AI systems can be compromised with only 100-500 poisoned samples, achieving >60% success rates while remaining undetected for 6-12 months or indefinitely.

## Executive Summary
Healthcare AI systems face critical vulnerabilities to data poisoning attacks that current defenses and regulatory frameworks cannot adequately address. The study analyzed eight attack scenarios across four categories, demonstrating that adversaries require only 100-500 poisoned samples to compromise healthcare AI systems, achieving over 60% success rates with detection taking 6-12 months or sometimes never occurring. Healthcare's distributed infrastructure creates multiple entry points where insiders with routine access can execute attacks with minimal technical skill. Legal protections like HIPAA and GDPR unintentionally shield attackers by restricting the analyses needed for detection. Supply chain vulnerabilities enable a single compromised vendor to poison models across 50-200 institutions simultaneously. The research recommends multi-layered defenses including mandatory adversarial testing, ensemble-based detection, privacy-preserving security mechanisms, and international coordination on AI security standards.

## Method Summary
The study employed an analytical framework combining architecture classification by neural type and clinical domain, threat model construction for insider access scenarios, regulatory framework assessment (FDA, EU AI Act, HIPAA, GDPR), defense mechanism evaluation across five categories, and impact assessment via scenario-based analysis. The research synthesized findings from 41 security studies (2019-2025) from venues like NeurIPS, IEEE S&P, NEJM AI, and Nature Medicine, focusing on attacks using 100-500 poisoned samples. The methodology evaluated attack success rates, detection timeframes, and minimum poisoned samples required for backdoor embedding across different healthcare AI architectures.

## Key Results
- Attackers require only 100-500 poisoned samples to compromise healthcare AI systems regardless of dataset size
- Attack success rates exceed 60% while detection takes 6-12 months or sometimes never occurs
- Privacy regulations like HIPAA and GDPR unintentionally shield attackers by restricting cross-patient analysis needed for detection
- Supply chain compromises can poison models across 50-200 institutions simultaneously

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Data poisoning success depends on absolute sample count, not poisoning rate.
- Mechanism: During gradient-based training with 3-5 epochs, 250 poisoned samples receive 750-1,250 exposures to the backdoor signal. This repeated exposure embeds malicious behavior regardless of clean dataset size, because models update parameters based on gradient accumulation across training iterations, not statistical proportions.
- Core assumption: Attackers can achieve 100-500 poisoned samples through insider access or supply chain compromise.
- Evidence anchors:
  - [abstract] "attackers with access to only 100-500 samples can compromise healthcare AI regardless of dataset size"
  - [Page 12-13] "attack success depends on absolute sample count rather than poisoning rate... 250 poisoned samples provide 750-1,250 exposures"
  - [corpus] Related work on poisoning attacks against LLMs confirms unified benchmarks exist for evaluating attack toxicity (Schwarzschild et al., ICML 2021), though healthcare-specific rate-independent findings are limited to this analysis.
- Break condition: Differential privacy mechanisms with formal ε-guarantees that bound individual sample influence on final parameters.

### Mechanism 2
- Claim: Privacy regulations create a detection paradox that shields attackers.
- Mechanism: HIPAA and GDPR restrict cross-patient pattern analysis without consent or legal cause. Detecting coordinated poisoning (e.g., Medical Scribe Sybil attack) requires analyzing patient visit patterns across individuals—but establishing legal cause requires evidence from the very analysis that is prohibited.
- Core assumption: Attackers exploit legitimate clinical workflows rather than system breaches, making poisoned data appear as protected patient information.
- Evidence anchors:
  - [abstract] "Privacy laws such as HIPAA and GDPR can unintentionally shield attackers by restricting the analyses needed for detection"
  - [Page 8-9] "This creates a catch-22 where detection is legally constrained regardless of technical capability"
  - [corpus] No corpus papers directly address regulatory constraints on poisoning detection—this mechanism appears unique to healthcare domain analysis.
- Break condition: Privacy-preserving anomaly detection (e.g., federated analysis with secure aggregation) that enables pattern detection without exposing individual records.

### Mechanism 3
- Claim: Federated learning amplifies rather than mitigates poisoning risks through distributed trust and attribution obscurity.
- Mechanism: A single malicious institution in a 50-node federated network can submit poisoned model updates that pass Byzantine-robust aggregation. Privacy-preserving protocols prevent inspecting individual institutions' data or raw updates, making source identification nearly impossible. The compromised model distributes to all honest participants.
- Core assumption: Byzantine-robust aggregation methods are insufficient against sophisticated poisoning strategies that calibrate update magnitude to remain within expected distributions.
- Evidence anchors:
  - [Page 7] "federated learning's distributed trust model actually increases the attack surface while making source attribution extremely difficult"
  - [Page 15] "Byzantine-robust aggregation methods... prove inadequate against sophisticated poisoning strategies"
  - [corpus] Corpus confirms Byzantine-robust federated learning with secure aggregation exists as a defense (arxiv:2601.01053), though empirical effectiveness against adaptive attackers remains contested.
- Break condition: Model forensics at parameter level combined with multi-party computation that enables update auditing without data exposure.

## Foundational Learning

- Concept: Gradient accumulation and epoch-based exposure
  - Why needed here: Understanding why small absolute sample counts suffice requires grasping that models see each sample multiple times during training, compounding poisoned signal influence.
  - Quick check question: If a model trains for 4 epochs on 1M images including 300 poisoned samples, how many total exposures does the backdoor signal receive?

- Concept: Byzantine-robust aggregation in federated systems
  - Why needed here: The paper claims these defenses fail against sophisticated attacks; understanding what they attempt (outlier detection, coordinate trimming) clarifies the arms race.
  - Quick check question: Why might a poisoned update that passes statistical similarity tests still embed a backdoor?

- Concept: Backdoor triggers and target misclassification
  - Why needed here: Attacks don't degrade overall accuracy—they associate specific patterns (triggers) with wrong outputs while maintaining normal performance on clean inputs.
  - Quick check question: How does demographic-correlated triggering make detection harder than random triggering?

## Architecture Onboarding

- Component map: Data provenance tracking -> MEDLEY ensemble with disagreement detection -> Byzantine-robust FL protocols -> Differential privacy mechanisms -> Human investigation -> Model rollback
- Critical path: Detection → Human investigation → Model rollback. MEDLEY flags disagreement patterns; trained clinicians investigate whether divergence reflects legitimate complexity or poisoning; if poisoning suspected, incident response triggers forensic analysis and model version rollback.
- Design tradeoffs: Ensemble diversity increases computational cost and deployment complexity but enables detection. Differential privacy degrades model accuracy (ε vs. utility tradeoff). Neurosymbolic constraints reduce flexibility but provide verifiable safety bounds. Human-in-the-loop adds workflow burden but prevents automated evasion.
- Failure signatures:
  - Temporal disagreement spike: Current model version diverges from historical versions on specific demographics
  - Vendor-specific pattern: All institutions using same foundation model show correlated anomalies
  - Gradient concentration: Backdoor features concentrated in specific attention heads or LoRA matrices during parameter inspection
- First 3 experiments:
  1. Implement MEDLEY temporal ensemble for one imaging model: Compare current vs. N-1 vs. N-2 versions on held-out test set stratified by demographics; measure disagreement rates.
  2. Inject controlled poisoned samples (10-50) into a sandbox federated learning simulation; test whether Byzantine-robust aggregation detects or excludes the malicious updates.
  3. Audit current data provenance tracking: Map all collection points (PACS, EHR, labs) and identify which have cryptographic lineage vs. implicit trust.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Are current black-box AI architectures suitable for life-or-death clinical decisions, or does patient safety necessitate a shift toward interpretable, constraint-based systems?
- **Basis in paper:** [explicit] The authors explicitly state they "fundamentally question whether current black-box AI architectures are suitable for life-or-death clinical decisions" in the abstract and conclusion.
- **Why unresolved:** There is a trade-off between the high performance of opaque deep learning models and the need for verifiable safety guarantees in clinical settings.
- **What evidence would resolve it:** Comparative studies demonstrating that interpretable, constraint-based systems can achieve non-inferior clinical accuracy while providing formal proofs of safety constraints.

### Open Question 2
- **Question:** How can defenders detect coordinated "Medical Scribe Sybil" attacks when privacy regulations like HIPAA and GDPR restrict the cross-patient analysis required for identification?
- **Basis in paper:** [explicit] The paper describes a "fundamental legal paradox" where detecting coordinated fake patient visits requires analyzing data across individuals, which is precisely what privacy laws prohibit without prior consent.
- **Why unresolved:** Current legal frameworks create a "catch-22" where establishing probable cause for investigation requires the very analysis that is legally restricted.
- **What evidence would resolve it:** Development of privacy-preserving algorithms (e.g., federated anomaly detection) or new legal frameworks that permit security-focused pattern analysis without exposing individual patient data.

### Open Question 3
- **Question:** Can federated learning architectures be modified to allow for the attribution of poisoning attacks without violating the privacy protocols that prevent inspection of individual institutional updates?
- **Basis in paper:** [explicit] The analysis notes that federated learning "amplifies rather than mitigates these risks by obscuring attack attribution across distributed institutions" and "privacy-preserving protocols prevent inspection."
- **Why unresolved:** The core value proposition of federated learning (keeping local data private) is structurally at odds with the forensic requirement to inspect local model updates for malicious tampering.
- **What evidence would resolve it:** Technical mechanisms, such as zero-knowledge proofs or secure multi-party computation, that can validate the integrity of local updates without revealing the underlying data or raw parameters.

### Open Question 4
- **Question:** How can ensemble-based defenses like MEDLEY remain effective when a supply chain compromise affects the foundation models used by all ensemble members?
- **Basis in paper:** [inferred] While the paper promotes MEDLEY, it admits a critical limitation: "Poisoning that compromises all ensemble members simultaneously (e.g., shared training data across vendors) would evade detection."
- **Why unresolved:** As the industry consolidates around a few dominant commercial foundation models, achieving true independence between ensemble members becomes structurally difficult.
- **What evidence would resolve it:** Empirical evaluation of ensemble disagreement rates in scenarios where diverse models are fine-tuned from a common, compromised foundation model.

## Limitations
- Analysis relies heavily on synthesizing findings from external studies, making conclusions dependent on cited work quality
- Several key references (S5, S30, S62) appear only in supplementary materials without full citation details
- MEDLEY framework implementation specifics remain incompletely specified
- Privacy-preserving detection methods are identified as theoretically possible but lack empirical validation

## Confidence

- High confidence: Absolute sample count dependence for poisoning success - directly supported by referenced empirical studies showing consistent results across multiple attack types
- Medium confidence: Privacy regulations creating detection paradox - logically sound but lacks empirical validation of privacy-preserving detection alternatives
- Medium confidence: Federated learning amplifying poisoning risks - supported by theoretical analysis but limited empirical evidence from actual healthcare federated systems
- Low confidence: Supply chain attack feasibility at scale - claims of 50-200 institution compromise rely on unspecified vendor-specific details

## Next Checks

1. **Implement controlled poisoning experiment**: Reproduce BadNets-style attack on a medical imaging dataset using exactly 250 poisoned samples across varying dataset sizes (10K, 100K, 1M samples) to empirically verify that attack success rate remains constant regardless of total dataset size.

2. **Test Byzantine-robust aggregation limits**: Create a federated learning simulation with 50 participants where one institution submits gradient updates containing backdoor triggers. Systematically vary update magnitude and timing to determine at what point Byzantine-robust methods (coordinate-wise median, Krum) fail to detect the attack.

3. **Audit real-world data provenance capabilities**: Survey 5 healthcare institutions to map their current data lineage tracking capabilities across PACS, EHR, and laboratory systems. Identify which systems have cryptographic provenance vs. implicit trust, and assess the feasibility of implementing mandatory provenance tracking within 12 months.