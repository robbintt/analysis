---
ver: rpa2
title: 'Shaping Sparse Rewards in Reinforcement Learning: A Semi-supervised Approach'
arxiv_id: '2501.19128'
source_url: https://arxiv.org/abs/2501.19128
tags:
- reward
- learning
- ssrs
- shaping
- transitions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of reward shaping in reinforcement
  learning with sparse reward signals, proposing a semi-supervised approach that combines
  reward shaping with semi-supervised learning techniques. The core method, called
  Semi-Supervised Reward Shaping (SSRS), utilizes both non-zero-reward transitions
  and zero-reward transitions through consistency regularization and pseudo-labeling.
---

# Shaping Sparse Rewards in Reinforcement Learning: A Semi-supervised Approach

## Quick Facts
- arXiv ID: 2501.19128
- Source URL: https://arxiv.org/abs/2501.19128
- Reference count: 11
- Outperforms supervised baselines with up to 2× peak scores in sparse-reward settings

## Executive Summary
This paper addresses reward shaping in reinforcement learning with sparse reward signals by proposing a semi-supervised approach called Semi-Supervised Reward Shaping (SSRS). The method leverages both non-zero-reward transitions (as ground truth) and zero-reward transitions (via semi-supervised learning) to learn a reward estimator. The core innovation is applying consistency regularization and pseudo-labeling to the majority of transitions that supervised methods ignore. Results show SSRS outperforms supervised-based methods in Atari and robotic manipulation environments, achieving up to twice the peak scores compared to supervised baselines in sparse-reward settings.

## Method Summary
SSRS combines reward shaping with semi-supervised learning techniques to utilize both non-zero-reward transitions and zero-reward transitions. It introduces entropy augmentation as a data perturbation method suitable for non-image tasks, along with a monotonicity constraint to improve stability. The reward estimator consists of two networks: R_Q (state-action) and R_V (state baseline), both outputting distributions over a set Z of collected non-zero rewards. The method uses consistency regularization between weakly and strongly augmented versions of transitions, pseudo-labeling for high-confidence zero-reward transitions, and a monotonicity constraint ensuring R_Q ≥ R_V.

## Key Results
- SSRS outperforms supervised methods in Atari and robotic manipulation environments
- Achieves up to 2× peak scores compared to supervised baselines in sparse-reward settings
- Entropy augmentation shows 15.8% improvement in best score over other augmentation methods
- Outperforms RCP and SORS baselines across tested environments

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Applying semi-supervised learning to zero-reward transitions improves reward estimation over supervised-only approaches in sparse-reward settings.
- **Mechanism:** The reward estimator is trained via consistency regularization—predictions on weakly and strongly augmented versions of the same transition must agree. This leverages the majority of transitions (zero-reward) that supervised methods ignore, while non-zero transitions provide ground-truth labels. Pseudo-labeling assigns rewards to high-confidence zero-reward transitions.
- **Core assumption:** The trajectory space satisfies SSL's smoothness and clustering assumptions: similar transitions should yield similar rewards, and decision boundaries should lie in low-density regions.
- **Evidence anchors:** [abstract] "employing the Semi-Supervised Learning (SSL) technique combined with a novel data augmentation to learn trajectory space representations from the majority of transitions, i.e., zero-reward transitions"; [section 4.2] "Larger estimated reward values... exhibit clearer diagonal boundaries compared to smaller reward values... aligning with the clustering assumption of SSL"
- **Break condition:** If trajectory clustering is weak (high ambiguity in decision boundaries), pseudo-labels become unreliable and shaped rewards may misguide policy learning.

### Mechanism 2
- **Claim:** Entropy augmentation outperforms traditional image-based augmentations for non-visual RL observations.
- **Mechanism:** Shannon entropy is computed over state submatrices to create domain-agnostic perturbations. Unlike cutout or translation—which may violate smoothness assumptions in non-image spaces—entropy perturbations preserve the unitless, scale-independent structure of trajectory representations.
- **Core assumption:** Traditional augmentations introduce excessive perturbations to non-image data, breaking the smoothness/clustering assumptions required for SSL consistency.
- **Evidence anchors:** [abstract] "The proposed entropy augmentation enhances performance, showcasing a 15.8% increase in best score over other augmentation methods"; [section 3.4] "entropy augmentation better preserves these assumptions in trajectory space"
- **Break condition:** If state entropy is uninformative (e.g., uniformly random observations), augmentation will not induce meaningful variation.

### Mechanism 3
- **Claim:** The monotonicity constraint on the advantage function stabilizes reward estimator training.
- **Mechanism:** A loss term penalizes negative advantage values (R_Q − R_V < 0), enforcing that the state-action estimator R_Q assigns higher rewards to taken actions than the baseline R_V. This compensates for augmentation acting only on states, not actions.
- **Core assumption:** Separating global baseline (R_V) from action-specific advantage (R_Q) improves action discrimination under state-only perturbations.
- **Evidence anchors:** [section 3.3] "This separation makes it easier to assess the relative importance of specific actions, and thus compensates for the insensitivity to actions because the disturbance of data augmentation only takes place on states"; [section 4.1, Table 1] SSRS with monotonicity achieves higher best scores in 3/4 tested environments
- **Break condition:** If valid reward structures require negative advantages for some actions, the constraint may distort learning.

## Foundational Learning

- **Concept: Consistency Regularization in SSL**
  - Why needed: Explains how SSRS extracts signal from unlabeled (zero-reward) transitions.
  - Quick check question: Why should a reward estimator predict the same distribution for a transition and its augmented version?

- **Concept: Pseudo-Labeling Thresholds**
  - Why needed: Determines when high-confidence predictions become training labels.
  - Quick check question: What happens to shaped rewards if the confidence threshold λ is set too low (e.g., 0.5)?

- **Concept: Policy Invariance in Reward Shaping**
  - Why needed: Ensures shaped rewards preserve optimal policies.
  - Quick check question: Under what conditions does modifying the reward function leave the set of optimal policies unchanged?

## Architecture Onboarding

- **Component map:**
  - Experience buffer (30k capacity) → R_Q network (state, action) → R_V network (state) → Data augmentation module → Base RL algorithm (C51/SAC) → Policy updates

- **Critical path:**
  1. Collect transitions; append new non-zero rewards to Z until |Z| = N_z
  2. For zero-reward transitions, compute confidence scores q_t = (R_Q + R_V)/2
  3. Apply weak/strong augmentations; enforce consistency via cross-entropy
  4. Assign pseudo-labels where max(q_t) ≥ λ; shape rewards via argmax
  5. Update policy using shaped rewards; update estimator via L_QV + βL_s + (1−β)L_r

- **Design tradeoffs:**
  - Update probability p_u: higher → more exploitation of shaped rewards; lower → more exploration
  - Confidence threshold λ: optimal range 0.8–0.9; too low adds noisy labels
  - Consistency coefficient β: primarily affects convergence speed, not final performance (Table 2)

- **Failure signatures:**
  - Low best scores + high pseudo-label usage: SSL assumptions violated; check trajectory clustering
  - Training instability: verify monotonicity constraint is active; check advantage loss
  - Augmentation fails to improve scores: switch to entropy augmentation for non-visual observations

- **First 3 experiments:**
  1. **Baseline comparison:** Run SSRS vs RCP vs SORS on 5 sparse-reward Atari games (Seaquest, Venture, Montezuma's Revenge, Kangaroo, Hero); expect 1.5–2× peak score gains in extreme sparsity.
  2. **Augmentation ablation:** Compare entropy vs cutout vs smooth augmentation on Venture; entropy should yield ~15% higher best scores.
  3. **Monotonicity ablation:** Test with/without L_QV on Seaquest/Hero/Montezuma/Venture; constrained version should outperform in most environments.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What is the theoretical relationship between the optimality of the semi-supervised reward estimator and the agent's exploration-exploitation balance?
- **Basis in paper:** [explicit] The authors state in the conclusion: "We believe that a theoretical analysis of the dynamic relationship between the optimality of the estimator and the exploration-exploitation ratio would be a valuable direction for future work."
- **Why unresolved:** The paper empirically observes that the update probability $p_u$ influences the exploration-exploitation trade-off but does not provide a formal derivation or theoretical framework explaining this dynamic.
- **What evidence would resolve it:** A theoretical model defining how the convergence of the reward estimator impacts the policy's optimal exploration strategy, potentially including a derived adaptive schedule for $p_u$.

### Open Question 2
- **Question:** Under what formal conditions do the semi-supervised learning assumptions (smoothness and clustering) hold true in reinforcement learning trajectory spaces?
- **Basis in paper:** [inferred] Section 4.2 notes that the method relies on the "smoothness assumption and clustering assumption" and provides empirical evidence via consensus matrices, but acknowledges that decision boundaries for high rewards can be ambiguous and the assumptions rely on the "continuity of trajectories in the metric space."
- **Why unresolved:** The paper relies on empirical visualization (consensus matrices) to justify the application of SSL but lacks a theoretical guarantee or formal proof that these assumptions hold universally across different RL environments or state representations.
- **What evidence would resolve it:** A theoretical proof outlining the necessary conditions for trajectory clustering or a counter-example showing where the lack of smoothness in the state-space causes the SSL regularization to fail.

### Open Question 3
- **Question:** How does the proposed entropy augmentation technique generalize to high-dimensional image-based inputs or non-probabilistic state representations?
- **Basis in paper:** [inferred] Section 3.4 introduces entropy augmentation specifically for "non-image tasks" utilizing matrix states (RAM), noting it exploits the "unitless nature of entropy" in normalized matrices. The experiments are restricted to RAM and robotic state vectors.
- **Why unresolved:** The method is designed for normalized matrix data where Shannon Entropy is easily calculable; it is unclear if this specific perturbation method is computationally efficient or semantically meaningful for raw pixel inputs or unstructured data.
- **What evidence would resolve it:** Experimental results applying the entropy augmentation method to standard high-dimensional vision-based RL benchmarks (e.g., Atari using pixels) compared against standard image augmentation techniques.

### Open Question 4
- **Question:** Can the update probability $p_u$ be adapted automatically rather than tuned as a static hyperparameter?
- **Basis in paper:** [inferred] The conclusion notes that "tuning of these hyperparameters... can provide valuable insights" and specifically links $p_u$ to the exploration-exploitation balance, yet the experiments use fixed values determined by preliminary tuning.
- **Why unresolved:** Manual tuning of $p_u$ is required for different environments (Atari vs. Robotics), suggesting a static value may be sub-optimal as the agent's need for shaped rewards changes throughout the learning process.
- **What evidence would resolve it:** A comparative study showing that an adaptive mechanism for $p_u$ (e.g., based on estimator loss or policy entropy) outperforms fixed hyperparameter settings across diverse sparse reward environments.

## Limitations
- Neural network architectures for R_Q and R_V are unspecified
- Critical hyperparameters (learning rates, λ, β, p_u, N_z) are not reported
- Entropy augmentation method lacks implementation details

## Confidence
- **High confidence:** The core SSL framework combining consistency regularization with pseudo-labeling for zero-reward transitions is well-motivated and theoretically sound
- **Medium confidence:** The entropy augmentation method shows promising results but lacks rigorous comparison methodology details
- **Low confidence:** The specific implementation details necessary for exact reproduction (network architectures, hyperparameter values) are missing

## Next Checks
1. **Architecture validation:** Implement SSRS with specified entropy augmentation and monotonicity constraint on Seaquest, verifying whether the reported 15.8% improvement over other augmentation methods is reproducible
2. **Hyperparameter sensitivity:** Systematically vary λ (confidence threshold) and β (consistency coefficient) to identify optimal ranges and confirm Table 2 findings
3. **Ablation study:** Compare SSRS performance with/without the monotonicity constraint across all tested environments to validate its contribution to stability and performance