---
ver: rpa2
title: Are We There Yet? A Measurement Study of Efficiency for LLM Applications on
  Mobile Devices
arxiv_id: '2504.00002'
source_url: https://arxiv.org/abs/2504.00002
tags:
- mobile
- memory
- edge
- devices
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper measures the efficiency of deploying LLM applications
  on mobile devices. The authors implemented AutoLife-Lite, a mobile application that
  uses sensor data to infer user location and activity contexts.
---

# Are We There Yet? A Measurement Study of Efficiency for LLM Applications on Mobile Devices

## Quick Facts
- arXiv ID: 2504.00002
- Source URL: https://arxiv.org/abs/2504.00002
- Authors: Xiao Yan; Yi Ding
- Reference count: 40
- Primary result: Only small LLMs (<4B parameters) can run on mobile devices, with latency >30 seconds and limited output quality

## Executive Summary
This paper measures the efficiency of deploying LLM applications on mobile devices through AutoLife-Lite, a sensor-based context inference application. The authors conduct experiments across three deployment settings: mobile-based, edge-based (CPU and GPU), and cloud-based. Key findings reveal that mobile devices are constrained to models under 4B parameters due to memory limitations, with compressed models sometimes failing to generate valid outputs. GPU-based edge deployments significantly outperform CPU-based ones in latency and consistency, while cloud services provide the best performance but raise privacy concerns. The study highlights current limitations and provides insights for future on-device LLM applications.

## Method Summary
The study implements AutoLife-Lite, a mobile application that uses sensor data (Wi-Fi SSID lists, IMU accelerometer, barometer readings) to infer user location and activity contexts. Three deployment settings are evaluated: mobile-based on Google Pixel 8 with MediaPipe/LiteRT and Executorch frameworks; edge-based on AWS P3 instance with Ollama framework (CPU and GPU configurations); and cloud-based via commercial API calls. Models tested include Gemma (2B, 2-2B, 7B), Llama3.2 (1B, 3B), Llama3.1-8B, DeepSeek-R1 (1.5B, 8B), and Qwen (0.5B-7B). The structured JSON prompt enforces uniform output format, and measurements include latency, memory consumption (RSS on mobile, CPU/GPU memory on edge), and qualitative output validity through manual validation.

## Key Results
- Only small-size LLMs (<4B parameters) can run successfully on powerful mobile devices due to memory constraints
- Model compression reduces hardware requirements but may degrade performance, with compressed models sometimes failing to generate valid outputs
- GPU-based edge deployments outperform CPU-based ones in latency and consistency due to specialized parallel memory architecture
- Latency on mobile devices exceeds 30 seconds for meaningful output, while cloud services achieve sub-10 second response times
- Model speed decreases with increasing model size, challenging real-time applications with large models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Mobile device memory capacity creates a hard upper bound on deployable model size, limiting practical applications to models under 4B parameters on 8GB devices.
- Mechanism: LLM inference requires loading model weights into RAM. With OS consuming ~35-40% of 8GB memory, a 3B parameter model occupies ~57% of available memory, leaving insufficient headroom for larger models or concurrent operations.
- Core assumption: Memory scaling is approximately linear with parameter count; future compression techniques may shift this boundary.
- Evidence anchors:
  - [abstract] "Only small-size LLMs (<4B parameters) can run successfully on powerful mobile devices"
  - [section 4.2] "a Llama-3B model would consume 57% memory, with only around 8% free memory"
  - [corpus] "Understanding Large Language Models in Your Pockets" confirms similar constraints on COTS devices
- Break condition: Devices with 16GB+ RAM or more aggressive quantization could enable 7B models (paper acknowledges recent work achieving this).

### Mechanism 2
- Claim: GPU-based edge inference achieves lower and more consistent latency than CPU-based inference due to specialized parallel memory architecture optimized for matrix operations.
- Mechanism: GPUs provide high memory bandwidth and parallel compute units that match LLM inference patterns (batched matrix multiplications), reducing both mean latency and variance compared to sequential CPU execution.
- Core assumption: The observed variance reduction is attributable to GPU architecture rather than other system factors; network latency was excluded from edge measurements.
- Evidence anchors:
  - [abstract] "GPU-based edge deployments outperform CPU-based ones in latency and consistency"
  - [section 4.2] "CPU-based edge server has a much higher average latency and larger variance compared with the GPU-based edge server"
  - [corpus] "Dissecting the Impact of Mobile DVFS Governors" examines related hardware-level efficiency factors
- Break condition: Highly optimized CPU inference engines or NPU offloading may narrow this gap (corpus mentions NPU utilization research).

### Mechanism 3
- Claim: Model compression (quantization, LoRA adaptation) enables deployment on resource-constrained devices but introduces risk of output quality degradation or complete failure.
- Mechanism: Compression reduces precision of weights (e.g., INT8/INT4 quantization), which may cause numerical instability during inference, leading to incoherent outputs or failure to generate valid responses.
- Core assumption: The observed failures are attributable to compression rather than framework-specific bugs or prompt incompatibility.
- Evidence anchors:
  - [abstract] "model compression reduces hardware requirements but may degrade performance"
  - [section 4.2] "Llama3.2-1B and Llama3.2-3B on mobile is an optimized version with techniques like LoRA and quantization... the compressed version fails on mobile"
  - [corpus] "Sustainable LLM Inference for Edge AI" evaluates quantization accuracy tradeoffs explicitly
- Break condition: Newer compression methods or calibration datasets may preserve quality better; results are framework and technique-dependent.

## Foundational Learning

- Concept: **Resident Set Size (RSS) memory measurement**
  - Why needed here: Understanding actual physical memory footprint is critical for determining deployability on constrained devices. The paper uses RSS via `/proc/<pid>/status` rather than virtual memory allocation.
  - Quick check question: Why is RSS preferred over virtual memory size when measuring mobile deployment feasibility?

- Concept: **Token throughput vs. latency tradeoffs**
  - Why needed here: The paper measures both total latency and tokens/second to characterize real-time viability. Larger models show slower token generation rates.
  - Quick check question: If a model generates 5 tokens/second and needs 100 tokens for a response, what is the minimum latency?

- Concept: **Quantization precision and numerical stability**
  - Why needed here: Understanding why compressed models fail requires grasping how reduced precision affects floating-point operations during attention and feedforward passes.
  - Quick check question: What is the difference between FP16, INT8, and INT4 quantization in terms of bits per weight?

## Architecture Onboarding

- Component map:
  - Mobile-based: Android app → MediaPipe/LiteRT or Executorch → On-device model (<4B) → Local inference
  - Edge-based: Mobile app → LAN (Wi-Fi/cellular) → Ollama server (CPU or GPU) → Medium model (up to 8B) → Response returned
  - Cloud-based: Mobile app → WAN (Internet) → Commercial API (OpenAI/Anthropic/Google) → Large model → Response returned

- Critical path:
  1. Sensor data collection (IMU, Wi-Fi, barometer) via Android sensor listeners
  2. Rule-based preprocessing (motion detection, altitude calculation)
  3. Structured prompt construction with sensor context
  4. LLM inference (latency bottleneck varies by deployment: mobile >30s, edge varies, cloud <10s)
  5. JSON response parsing and context extraction

- Design tradeoffs:
  - **Mobile**: Privacy + offline capability vs. limited model quality + high latency
  - **Edge (CPU)**: Moderate model size + local network vs. high latency variance
  - **Edge (GPU)**: Better latency/consistency vs. hardware cost
  - **Cloud**: Best model quality + lowest latency vs. network dependency + privacy concerns

- Failure signatures:
  - **Nothing generated**: Model loads but produces empty output (observed with Gemma-2B on mobile)
  - **Copy-paste/input echo**: Model fails to process, returns prompt fragments (observed with Qwen small models)
  - **OOM crash**: Model size exceeds available RAM (7B+ models on 8GB devices)
  - **High variance latency**: CPU-based inference showing inconsistent response times

- First 3 experiments:
  1. **Baseline memory profiling**: Deploy Gemma2-2B via MediaPipe on target device; measure RSS before, during, and after inference to confirm ~3.4GB footprint and identify memory headroom.
  2. **Latency decomposition**: Run identical prompts on mobile, edge-CPU, edge-GPU, and cloud; record prefill time, token generation rate, and total latency to quantify deployment tradeoffs.
  3. **Compression robustness test**: Compare output quality of Llama3.2-1B original (edge) vs. quantized version (mobile) on structured JSON prompt to identify failure modes specific to compression.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can Vision Language Models (VLMs) be effectively deployed on mobile devices for complex context sensing tasks?
- Basis in paper: [explicit] Section 6 states, "On-device VLM is an interesting problem since many mobile devices are equipped with cameras... we only test the performance of LLMs but not VLMs."
- Why unresolved: The authors were restricted by the availability of frameworks and models capable of running VLMs on-device at the time of the study.
- What evidence would resolve it: A measurement study evaluating latency and memory consumption of mobile-optimized VLMs on standard smartphone hardware.

### Open Question 2
- Question: What is the precise quantitative relationship between model compression ratios (e.g., quantization) and accuracy degradation in sensor-based reasoning tasks?
- Basis in paper: [explicit] Section 4.1 notes, "We did not measure the accuracy... but we have conducted manual validation," while Section 4.2 identifies specific failure cases caused by compression.
- Why unresolved: The study focused on system efficiency (latency/memory) and relied on manual sanity checks rather than rigorous accuracy benchmarking.
- What evidence would resolve it: Comparative benchmarks showing F1 scores or semantic similarity metrics for compressed versus original models running the AutoLife-Lite task.

### Open Question 3
- Question: How do efficiency metrics generalize across heterogeneous mobile hardware and operating systems?
- Basis in paper: [explicit] Section 6 highlights the limitation: "We only measure the system performance on one mobile device... heterogeneity in hardware and operating systems brings significant difficulties."
- Why unresolved: The experimental scope was limited to a single device (Google Pixel 8) and specific edge setups, potentially biasing the results toward high-end hardware capabilities.
- What evidence would resolve it: Deploying the AutoLife-Lite benchmark across a diverse set of devices (e.g., different RAM capacities, chipsets) and standardizing the results.

## Limitations

- Model compression specificity: The paper identifies compressed model failures but lacks detailed specification of quantization techniques and framework optimizations
- Sensor data representativeness: The study uses synthetic sensor data without reporting real-world distributions or external validity testing
- Memory accounting assumptions: Memory measurements rely on RSS values which may not capture fragmentation overhead or system-level caching effects

## Confidence

**High Confidence**:
- Memory constraints on mobile devices create hard upper bounds for model deployment
- GPU-based edge inference outperforms CPU-based inference in both latency and variance
- Cloud deployments achieve lowest latency but highest privacy risk

**Medium Confidence**:
- Model compression degrades output quality in some cases
- Mobile inference latency exceeds 30 seconds for meaningful output
- Larger models show decreased token generation rates

**Low Confidence**:
- Specific failure modes of compressed models on mobile
- Comparative quality of outputs across different model families
- Real-world applicability of sensor-based context inference

## Next Checks

1. **Compression Robustness Validation**: Test identical quantized models (same INT8 calibration, same framework) across multiple mobile devices with varying RAM (4GB, 8GB, 12GB). Compare failure rates and output quality using standardized quality metrics (BLEU, ROUGE) on structured JSON outputs.

2. **Memory Scaling Experiment**: Profile memory usage of 1B, 2B, 3B, and 4B parameter models on mobile devices with identical quantization. Measure not just peak RSS but also memory fragmentation patterns and garbage collection frequency during inference to validate linear scaling assumptions.

3. **Real-World Sensor Data Collection**: Deploy AutoLife-Lite on test devices in diverse environments (urban, rural, indoor, outdoor) for one week. Collect actual sensor streams and compare inference outcomes versus synthetic data benchmarks to quantify performance gaps and failure modes.