---
ver: rpa2
title: 'EYE-DEX: Eye Disease Detection and EXplanation System'
arxiv_id: '2509.24136'
source_url: https://arxiv.org/abs/2509.24136
tags:
- retinal
- fundus
- images
- dataset
- disease
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "EYE-DEX is an automated framework for classifying 10 retinal conditions\
  \ using a large-scale dataset of 21,577 fundus images. The approach benchmarks three\
  \ pre-trained CNNs\u2014VGG16, VGG19, and ResNet50\u2014and achieves a state-of-the-art\
  \ accuracy of 92.36% with fine-tuned VGG16."
---

# EYE-DEX: Eye Disease Detection and EXplanation System

## Quick Facts
- arXiv ID: 2509.24136
- Source URL: https://arxiv.org/abs/2509.24136
- Reference count: 19
- Primary result: Automated classification of 10 retinal conditions with 92.36% accuracy using fine-tuned VGG16

## Executive Summary
EYE-DEX is an automated framework for classifying 10 retinal conditions using a large-scale dataset of 21,577 fundus images. The approach benchmarks three pre-trained CNNs—VGG16, VGG19, and ResNet50—and achieves a state-of-the-art accuracy of 92.36% with fine-tuned VGG16. To improve interpretability, Grad-CAM heatmaps are used to highlight disease-specific regions, increasing clinician trust. The model demonstrates robust performance across classes, including rare conditions, by employing data augmentation and class-balancing techniques such as focal loss and class weighting. This work advances transparent, accurate, and scalable AI-assisted diagnostics for retinal diseases, addressing both performance and interpretability challenges in clinical deployment.

## Method Summary
EYE-DEX employs transfer learning from pre-trained CNNs (VGG16, VGG19, ResNet50) fine-tuned on retinal fundus images. The VGG16 architecture achieved the highest accuracy at 92.36% after unfreezing the last 10 convolutional layers and adding a custom classification head with GlobalAveragePooling2D, BatchNormalization, Dropout, and L2-regularized Dense layers. Class imbalance was addressed using a combination of focal loss and class weighting, while data augmentation (shear, zoom, flips) provided additional training diversity. Grad-CAM heatmaps were generated post-hoc to visualize disease-relevant regions and enhance clinical interpretability.

## Key Results
- VGG16 fine-tuned architecture achieves 92.36% accuracy, outperforming VGG19 and ResNet50 on the 10-class retinal disease classification task.
- Rare classes like Pterygium (119 original images) achieve perfect precision and recall through focal loss and class weighting.
- Grad-CAM heatmaps successfully highlight disease-specific regions, supporting clinician trust in model predictions.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuned VGG16 outperforms deeper architectures on retinal disease classification.
- Mechanism: Transfer learning from ImageNet pre-trained weights, followed by selective fine-tuning of the last 10 convolutional layers. Custom classification head added: GlobalAveragePooling2D → BatchNormalization → Dropout → Dense layers with L2 regularization. The 3×3 filter structure in VGG16 captures subtle pathological features (microaneurysms, drusen, optic disc changes) without overfitting on the 21,577-image dataset.
- Core assumption: ImageNet pre-trained features transfer effectively to fundus imaging despite domain shift.
- Evidence anchors:
  - [abstract] "finetuned VGG16 achieving a state-of-the-art global benchmark test accuracy of 92.36%"
  - [section IV.B] "VGG16 model achieved the highest accuracy at 92.36%... Its streamlined architecture proved exceptionally well-suited for capturing the subtle pathological features"
  - [corpus] Hybrid CNN-Transformer papers (arXiv 2503.21465, 2504.08481) suggest architectural complexity doesn't guarantee superior performance on retinal tasks—consistent with VGG16 outperforming ResNet50 here.
- Break condition: Dataset distribution shift (e.g., pediatric fundus, different imaging devices) may degrade transfer learning effectiveness; external validation required.

### Mechanism 2
- Claim: Combined class weighting and focal loss enables rare-disease detection without synthetic oversampling.
- Mechanism: Class weighting assigns higher importance to underrepresented classes during gradient updates. Focal loss down-weights easy examples, forcing the model to focus on hard, minority-class cases. Data augmentation (30% shear, 30% zoom, vertical flips) expands training diversity without generating synthetic artifacts.
- Core assumption: Real augmented variations generalize better than SMOTE-generated synthetic samples for medical imaging.
- Evidence anchors:
  - [section III.F] "we employed a combination of techniques, including class weighting and focal loss... preferred over synthetic oversampling methods like SMOTE to avoid the potential for generating noisy or unrealistic data points"
  - [section IV.C] "Pterygium (with only 119 original images), achieved perfect scores for both precision and recall"
  - [corpus] arXiv 2506.18414 addresses imbalance in retinal vessel segmentation via class-balancing CNN approaches—supports this as an active research direction, though direct comparison to focal loss requires further evidence.
- Break condition: Extremely rare conditions (<50 samples) may still underperform; consider few-shot learning or external data integration.

### Mechanism 3
- Claim: Grad-CAM heatmaps improve clinical trust by localizing disease-specific regions.
- Mechanism: Grad-CAM computes gradients flowing into the final convolutional layer to produce coarse localization maps highlighting regions most influential to the predicted class. These heatmaps are overlaid on original fundus images for visual verification.
- Core assumption: Assumption: Highlighted regions correspond to clinically meaningful pathology (e.g., optic disc cupping in glaucoma) rather than spurious correlations.
- Evidence anchors:
  - [abstract] "Grad-CAM heatmaps are used to highlight disease-specific regions, increasing clinician trust"
  - [section V] "Grad-CAM is employed to generate heatmaps that highlight the specific regions of the eye fundus images which contribute most significantly to the model's predictions"
  - [corpus] arXiv 2503.02917 (Interpretable Few-Shot Retinal Disease Diagnosis) emphasizes concept-guided interpretability—suggests Grad-CAM is one viable approach among emerging XAI methods.
- Break condition: Grad-CAM provides coarse localization; fine-grained lesion detection may require attention-based or transformer-derived explanations. No quantitative clinician validation reported in this paper.

## Foundational Learning

- **Transfer Learning & Fine-Tuning**
  - Why needed here: Pre-trained CNNs (VGG16, VGG19, ResNet50) provide feature extractors trained on ImageNet. Fine-tuning adapts these to retinal pathology without training from scratch.
  - Quick check question: Can you explain why freezing early layers but unfreezing later layers preserves general features while adapting domain-specific representations?

- **Class Imbalance Handling (Focal Loss, Class Weighting)**
  - Why needed here: The dataset has severe imbalance (Pterygium: 119 images vs. Diabetic Retinopathy: 4,953 images). Without mitigation, the model would bias toward majority classes.
  - Quick check question: How does focal loss differ from standard cross-entropy in its treatment of hard vs. easy examples?

- **Grad-CAM Explainability**
  - Why needed here: Clinical adoption requires transparent reasoning. Grad-CAM provides visual proof that the model attends to disease-relevant anatomy.
  - Quick check question: Grad-CAM uses gradients from which layer to generate the class activation map?

## Architecture Onboarding

- **Component map:**
  Input (224×224 RGB fundus image) → Pre-trained VGG16 (last 10 layers unfrozen) → GlobalAveragePooling2D → BatchNormalization → Dropout → Dense (L2 regularized) → Softmax (10 classes) → Grad-CAM visualization

- **Critical path:**
  1. Data preprocessing: resize → normalize → augment (shear, zoom, flip)
  2. Forward pass through VGG16 backbone + custom head
  3. Loss computation: categorical crossentropy + focal loss weighting + class weights
  4. Backpropagation (Adam optimizer, lr=0.0001)
  5. Post-training: Grad-CAM heatmap generation for interpretability

- **Design tradeoffs:**
  - VGG16 vs. deeper models (VGG19, ResNet50): Simpler architecture generalizes better on this dataset; deeper models risk overfitting without additional data.
  - Augmentation vs. synthetic oversampling: Realistic augmentations preferred to avoid noisy synthetic artifacts.
  - Grad-CAM vs. alternative XAI: Grad-CAM is computationally cheap but provides coarse localization; attention-based methods (see corpus) may offer finer granularity.

- **Failure signatures:**
  - Rare-class collapse: Despite balancing, classes with <100 images (e.g., Pterygium) may show unstable validation metrics—monitor per-class F1.
  - Overfitting on augmented data: If validation accuracy plateaus while training improves, reduce augmentation intensity.
  - Grad-CAM misalignment: Heatmaps highlighting irrelevant regions suggest spurious correlations—inspect visually and validate with domain experts.

- **First 3 experiments:**
  1. **Baseline replication:** Train VGG16 with frozen backbone (no fine-tuning), compare accuracy to reported 92.36% to quantify fine-tuning contribution.
  2. **Ablation on imbalance handling:** Train with (a) class weights only, (b) focal loss only, (c) both—compare per-class F1, especially on Pterygium and Disc Edema.
  3. **External validation test:** Evaluate trained model on an independent dataset (e.g., ODIR or Messidor subset) to assess generalization beyond the Retinal Disease Dataset.

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How does EYE-DEX performance generalize when validated on external, heterogeneous datasets (e.g., ODIR, Messidor) compared to the internal test split?
- Basis in paper: [inferred] Section II.E explicitly warns that "models evaluated on internal datasets... may overestimate real-world performance," and Section VI lists "expanding the dataset diversity" as a future goal.
- Why unresolved: The reported 92.36% accuracy is based solely on a split of the Retinal Disease Dataset, leaving susceptibility to dataset-specific bias untested.
- What evidence would resolve it: Reporting accuracy, F1-scores, and confusion matrices after training on the current dataset and testing on completely distinct public datasets.

### Open Question 2
- Question: Does the Grad-CAM heatmap integration actually improve diagnostic accuracy or confidence levels for ophthalmologists in a clinical user study?
- Basis in paper: [inferred] The abstract claims the system fosters "clinician trust," and Section V states the heatmaps "build trust," but the paper provides no empirical qualitative or quantitative evaluation involving medical experts.
- Why unresolved: While the paper generates heatmaps, it does not verify if these visualizations align with expert reasoning or if they aid rather than distract the clinician.
- What evidence would resolve it: A reader study where clinicians classify images with and without XAI assistance to measure changes in efficiency and accuracy.

### Open Question 3
- Question: Can the framework's diagnostic capability be improved by incorporating multi-modal imaging data, such as Optical Coherence Tomography (OCT)?
- Basis in paper: [explicit] Section VI states: "further research may focus on... incorporating additional imaging modalities."
- Why unresolved: The current study relies exclusively on 2D fundus photography, which may lack the depth information required for nuanced staging of conditions like glaucoma.
- What evidence would resolve it: A comparative study evaluating the performance of a multimodal model against the current fundus-only VGG16 baseline.

### Open Question 4
- Question: Is the reported 100% precision/recall for rare classes (e.g., Pterygium) statistically robust, given the extremely low sample size (N=9) in the test set?
- Basis in paper: [inferred] Table III shows perfect scores for Pterygium, but the dataset description (Table I) and results highlight a severe class imbalance, with the test set containing very few samples for this class.
- Why unresolved: High metrics on a test set with single-digit support (N=9) may result from data leakage or luck in the split rather than true feature learning.
- What evidence would resolve it: Cross-validation results or aggregation of metrics across multiple random seeds to verify stability for minority classes.

## Limitations
- No external validation on independent datasets; reported 92.36% accuracy may not generalize beyond the Retinal Disease Dataset.
- Grad-CAM provides coarse localization; no quantitative clinician validation of interpretability or clinical decision impact.
- Extremely rare conditions (<100 images) remain performance-sensitive despite balancing—Pterygium's perfect metrics may reflect small-sample variance.

## Confidence
- **High**: VGG16 architecture and fine-tuning mechanism (supported by direct experimental comparison to VGG19/ResNet50).
- **Medium**: Class imbalance handling efficacy (focal loss + class weighting improves minority-class F1, but no ablation with synthetic oversampling for direct comparison).
- **Medium**: Grad-CAM interpretability contribution (clinically intuitive, but no user studies or clinician feedback).

## Next Checks
1. External validation on ODIR or Messidor subsets to test generalization.
2. Ablation study comparing focal loss vs. synthetic oversampling (SMOTE) for rare-class performance.
3. Clinician usability study measuring diagnostic accuracy and trust when using Grad-CAM-augmented vs. baseline predictions.