---
ver: rpa2
title: 'GRASP: GRouped Activation Shared Parameterization for Parameter-Efficient
  Fine-Tuning and Robust Inference of Transformers'
arxiv_id: '2512.04296'
source_url: https://arxiv.org/abs/2512.04296
tags:
- parameters
- grasp
- trainable
- peft
- scaling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces GRASP, a parameter-efficient fine-tuning\
  \ method for transformer models that achieves significant parameter reduction by\
  \ grouping D-dimensional token representations into K\u226AD groups and learning\
  \ shared scaling and shifting parameters per group. Building on this, the authors\
  \ propose StochGRASP, which learns Gaussian distributions for weight perturbations\
  \ and includes a noise-aware objective to improve robustness under hardware-level\
  \ noise."
---

# GRASP: GRouped Activation Shared Parameterization for Parameter-Efficient Fine-Tuning and Robust Inference of Transformers

## Quick Facts
- arXiv ID: 2512.04296
- Source URL: https://arxiv.org/abs/2512.04296
- Authors: Malyaban Bal; Abhronil Sengupta
- Reference count: 18
- Primary result: GRASP achieves 75× parameter reduction while matching or exceeding established PEFT methods on GLUE and E2E NLG tasks, with StochGRASP providing significant robustness under hardware noise.

## Executive Summary
This paper introduces GRASP (GRouped Activation Shared Parameterization), a parameter-efficient fine-tuning method for transformer models that achieves significant parameter reduction by grouping D-dimensional token representations into K≪D groups and learning shared scaling and shifting parameters per group. Building on this, the authors propose StochGRASP, which learns Gaussian distributions for weight perturbations and includes a noise-aware objective to improve robustness under hardware-level noise. GRASP matches or exceeds the performance of established methods like LoRA and BitFit on GLUE and E2E NLG tasks while using up to 75× fewer trainable parameters. StochGRASP demonstrates significantly improved robustness under varying levels of simulated hardware noise compared to deterministic variants, making it suitable for energy-efficient and noise-prone edge AI hardware deployments.

## Method Summary
GRASP partitions the D-dimensional hidden representation of selected transformer layers into K groups (K ≪ D) and learns a single scale (γ) and shift (β) parameter per group, applied as x̃ = (x - β_g(i)) / γ_g(i). This reduces trainable parameters from O(n×D) to O(n×K) while preserving task-specific adaptation capability. The grouping is determined by a fixed random permutation applied once at initialization. StochGRASP extends this by modeling perturbations as Gaussian distributions (ΔW ~ N(μ, σ)) and including a noise-aware regularization term that encourages σ to match target hardware noise profiles. During inference, StochGRASP samples from these learned distributions, providing inherent robustness to weight variability.

## Key Results
- GRASP achieves 0.015% trainable parameters (K=128) with 85.9% average GLUE score on RoBERTa-base, matching or exceeding BitFit and (IA)³ baselines
- K=32 configuration provides 0.0049% parameters with only minor performance degradation (84.6% GLUE)
- StochGRASP maintains ≥86% accuracy under σ=0.01 noise vs. 61.5% for deterministic GRASP
- FF2-only configuration achieves 0.003% parameters with competitive performance, demonstrating layer selection flexibility

## Why This Works (Mechanism)

### Mechanism 1
Grouped activation modulation achieves parameter-efficient fine-tuning by sharing scale/shift parameters across dimensions. Partitions the D-dimensional hidden representation into K groups (K ≪ D) and learns a single (γ, β) pair per group, applied as: x̃ = (x - β_g(i)) / γ_g(i). This reduces trainable parameters from O(n×D) to O(n×K) while preserving task-specific adaptation capability. Core assumption: Dimensions within the same group can share modulation parameters without significant expressivity loss. Evidence anchors: [abstract] "partitions the D-dimensional token representations of selected layers into K ≪ D groups and learns a shared scaling and shifting vector for each group" [section III] Equation 1 shows the modulation formula with group mapping g(i) ∈ {1,...,K} [corpus] Related PEFT methods (LoRA, BitFit) similarly reduce parameters through structured constraints; corpus evidence for grouped modulation specifically is weak. Break condition: If K is too small relative to task complexity, underfitting may occur; Table I shows performance drops when K=32 vs K=128.

### Mechanism 2
Aggressive compression (small K) induces multimodal parameter distributions, revealing latent task structure. When parameters are learned independently, distributions are unimodal Gaussian. As K decreases, the constrained parameter budget forces the model to discover distinct latent structures, producing multimodal (mixture-of-Gaussians) distributions. Core assumption: Task-relevant features cluster in ways that can be captured by shared parameters. Evidence anchors: [section III-B] "As K decreases, multiple-modes (mixture of Gaussians) arises in the parameter distribution" [figures 2b-c, 3a] KDE plots show transition from unimodal to multimodal as K decreases from 128 to 8 [corpus] No direct corpus evidence for multimodal emergence in PEFT; this appears novel. Break condition: Assumption: Multimodality reflects genuine task structure rather than optimization artifacts.

### Mechanism 3
Learning Gaussian perturbation distributions instead of deterministic values improves robustness under hardware noise. StochGRASP models weight perturbations as ΔW ~ N(μ, σ), learning distribution parameters rather than fixed values. A noise-aware loss (Eq. 6) regularizes σ toward target hardware noise profiles. During inference, the model internalizes weight variability. Core assumption: Weight perturbations for fine-tuning can be modeled as Gaussian distributions; hardware noise can be approximated by target σ. Evidence anchors: [abstract] "learns Gaussian distributions as perturbations to the pre-trained weights rather than deterministic values" [section III-C.2] Equation 5 defines stochastic formulation; Equation 6 shows noise-aware objective [figure 6] Under σ=0.01 noise, StochGRASP maintains ≥86% accuracy vs. 61.5% for deterministic variant [corpus] Corpus mentions uncertainty-aware transformers (BayesFormer) but for generalization, not hardware robustness. Break condition: If actual hardware noise deviates significantly from Gaussian assumption, robustness gains may not transfer.

## Foundational Learning

- **Parameter-Efficient Fine-Tuning (PEFT) paradigms**
  - Why needed here: GRASP positions itself against LoRA (low-rank matrices), BitFit (bias-only), and (IA)³ (scaling vectors). Understanding these baselines clarifies GRASP's novelty—grouped sharing vs. per-dimension or low-rank approaches.
  - Quick check question: Can you explain why LoRA uses O(D×r) parameters per layer while GRASP uses only O(K)?

- **Transformer hidden representations and projection layers**
  - Why needed here: GRASP modulates input activations to linear projections (Key, Value, FF2). Understanding where D_in and D_out dimensions arise is essential for correct implementation.
  - Quick check question: In RoBERTa-base, what are the hidden dimension and intermediate FFN dimension values?

- **Probabilistic modeling and Gaussian distributions**
  - Why needed here: StochGRASP requires understanding mean/variance parameterization, sampling during inference, and how regularization toward σ_target works.
  - Quick check question: How does sampling from N(μ, σ) during training differ from using deterministic weights?

## Architecture Onboarding

- **Component map:**
  Input activations → GRASP modulation (grouped scaling/shifting) → Frozen linear projection → Output

- **Critical path:**
  1. Initialize γ=1, β=0 for all groups (Section III-A)
  2. Apply random permutation π once at model initialization
  3. Forward pass: modulate activations, compute frozen linear projection
  4. Backward pass: update only (γ, β) or (μ, σ) parameters
  5. For StochGRASP: add regularization λ·Σ(σ - σ_target)² to loss

- **Design tradeoffs:**
  - **K selection:** Smaller K → fewer parameters but potential accuracy drop (Table I: K=128→32 drops SST-2 from 94.3% to 93.8%)
  - **Layer selection:** FF2-only achieves ~0.003% params with competitive results (Table IV); all linear layers achieves best performance at 0.015%
  - **Scaling vs. shifting:** Table V shows shifting alone outperforms scaling alone; both together is best

- **Failure signatures:**
  - Performance collapsing to near-random: likely K too small for task complexity
  - High variance across random seeds: check grouping permutation is fixed per model
  - StochGRASP not improving robustness: verify σ initialization (0.001-0.005 range) and λ scaling (0.01-0.1)

- **First 3 experiments:**
  1. **Baseline replication:** Implement GRASP (K=128) on RoBERTa-base for SST-2; target ~94.3% accuracy with 0.015% trainable parameters. Compare against BitFit and (IA)³ baselines.
  2. **K sensitivity sweep:** Test K ∈ {8, 16, 32, 64, 128} on a single task; plot accuracy vs. parameter count tradeoff curve (reference: Figure 3b).
  3. **StochGRASP noise robustness:** Train deterministic GRASP and StochGRASP on SST-2; evaluate both under injected Gaussian noise (σ ∈ {0.01, 0.02, 0.05}) during inference. Target: StochGRASP should maintain >80% accuracy at noise levels where deterministic drops <65%.

## Open Questions the Paper Calls Out

### Open Question 1
Can StochGRASP achieve improved robustness when trained with hardware-informed noise-variability models from real edge AI chips rather than simulated Gaussian noise? Basis in paper: [explicit] The conclusion states: "Future work can involve moving beyond simulated noise and explore training StochGRASP using hardware-informed noise-variability models, enabling deployment on real edge AI hardware chips." Why unresolved: All robustness experiments use simulated Gaussian noise injection; no real hardware validation has been conducted. What evidence would resolve it: Evaluation of StochGRASP on physical edge AI accelerators (e.g., memristive or FeFET-based devices) with measured device variation profiles.

### Open Question 2
Does learning an optimal grouping function g(i) improve performance over random permutation, and what is the cost-benefit trade-off? Basis in paper: [inferred] Section III-A states the authors "propose an effective strategy to randomly group dimensions" to avoid "computationally expensive pre-processing," but do not compare against learned or task-aware grouping. Why unresolved: Only random grouping is evaluated; no comparison to clustering-based or attention-guided grouping exists. What evidence would resolve it: Ablation comparing random grouping against learned grouping (e.g., via gradient-based optimization or attention similarity clustering) on GLUE/E2E benchmarks.

### Open Question 3
Would modeling perturbations with more expressive distributions (e.g., mixture models, heavy-tailed distributions) yield further robustness gains over the Gaussian assumption? Basis in paper: [inferred] StochGRASP assumes "the learnt perturbation to follow underlying Gaussian Distributions" without exploring alternatives, despite observing multimodal structures in parameter distributions. Why unresolved: The Gaussian choice is motivated but not systematically compared against other parametric families. What evidence would resolve it: Experiments replacing Gaussian distributions with Gaussian Mixture Models or Student-t distributions, measuring both clean accuracy and noise robustness.

### Open Question 4
What is the theoretical mechanism driving the emergence of multimodal parameter distributions as K decreases? Basis in paper: [inferred] The authors note "we hypothesize that the pre-trained weights of each modulated layer are perturbed by K-learnt Gaussians" based on empirical observation, without formal proof. Why unresolved: The multimodal emergence is empirically demonstrated (Figures 2-3) but not theoretically explained. What evidence would resolve it: Theoretical analysis connecting compression ratio (D/K) to modality emergence, or controlled experiments isolating contributing factors (task difficulty, layer depth, initialization).

## Limitations
- Narrow experimental scope: Only GLUE and E2E NLG tasks, single base/large model variants, and simulated rather than measured hardware noise
- Multimodal parameter distribution claim lacks theoretical grounding or corpus evidence for why this structure emerges
- Performance dependence on random grouping permutation not fully characterized

## Confidence
- GRASP parameter efficiency and task performance: **High**
- StochGRASP noise robustness improvements: **High**
- Multimodal parameter distributions reflecting task structure: **Medium**
- Generalization to other tasks, models, and real hardware: **Low**

## Next Checks
1. **Gaussian noise assumption validation:** Measure actual noise profiles from target hardware (e.g., mobile GPU quantization errors, CPU inference precision loss) and test whether StochGRASP trained with Gaussian σ_target still provides robustness benefits when inference noise follows non-Gaussian distributions.

2. **Task generalization benchmark:** Implement GRASP on diverse downstream tasks beyond GLUE/E2E (e.g., SQuAD for QA, CoNLL for NER, LongBench for long-sequence modeling) to test whether the grouped activation sharing approach maintains parameter efficiency and performance across different task types and output spaces.

3. **Scaling analysis with model size:** Evaluate GRASP across the full RoBERTa/DeBERTa/GPT-2 family (base, large, XL, XXL) to determine whether the parameter savings and performance trade-offs scale proportionally with model size, or whether certain mechanisms (like grouping effectiveness) break down at different scales.