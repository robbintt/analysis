---
ver: rpa2
title: 'Still Not There: Can LLMs Outperform Smaller Task-Specific Seq2Seq Models
  on the Poetry-to-Prose Conversion Task?'
arxiv_id: '2511.08145'
source_url: https://arxiv.org/abs/2511.08145
tags:
- sanskrit
- prose
- rules
- llms
- rule
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper compares general-purpose LLMs with smaller task-specific\
  \ Seq2Seq models for Sanskrit poetry-to-prose conversion, a linguistically complex\
  \ task requiring compound segmentation, dependency resolution, and syntactic linearisation.\
  \ Two modelling paradigms were evaluated: (1) LLMs with instruction fine-tuning\
  \ and in-context learning using linguistically grounded prompts based on P\u0101\
  \u1E47inian grammar, and (2) a ByT5-Sanskrit Seq2Seq model fine-tuned on the same\
  \ task."
---

# Still Not There: Can LLMs Outperform Smaller Task-Specific Seq2Seq Models on the Poetry-to-Prose Conversion Task?

## Quick Facts
- arXiv ID: 2511.08145
- Source URL: https://arxiv.org/abs/2511.08145
- Reference count: 40
- Task-specific fine-tuning outperforms LLM instruction tuning on Sanskrit poetry-to-prose conversion

## Executive Summary
This paper compares general-purpose large language models (LLMs) with a smaller, task-specific Seq2Seq model for Sanskrit poetry-to-prose conversion. The task requires complex linguistic operations including compound segmentation, dependency resolution, and syntactic linearisation. The authors evaluate two approaches: LLMs with instruction fine-tuning and linguistically grounded in-context learning prompts based on Pāṇinian grammar, versus a ByT5-Sanskrit model fine-tuned on the same task. Results show that the ByT5-Sanskrit model significantly outperforms all LLM approaches on both datasets, with human evaluation strongly corroborating these findings.

## Method Summary
The study compares a ByT5-Sanskrit model with general-purpose LLMs (GPT-4o, Phi-4-mini, Phi-4) on Sanskrit poetry-to-prose conversion. The ByT5 model is pretrained on Sanskrit segmentation, dependency parsing, lemmatization, and morpho-syntactic tagging before full fine-tuning on 22,860 parallel verse-prose pairs from the Mahābhārata and Rāmāyaṇa epics. LLMs are evaluated using both instruction fine-tuning (with LoRA) and in-context learning with linguistically grounded prompts incorporating Pāṇinian grammar rules. The study employs multiple evaluation metrics including BLEU score, Kendall's Tau for word order, and human expert ratings.

## Key Results
- ByT5-Sanskrit significantly outperforms all LLM approaches on both datasets (BLEU: 38.63 vs 33.12 for Mahābhārata; 39.50 vs 31.95 for Rāmāyaṇa)
- Human evaluation strongly corroborates quantitative findings, with Kendall's Tau showing high correlation with expert judgments
- Cross-domain evaluation demonstrates ByT5-Sanskrit's ability to generalize syntactic principles across epic styles
- Linguistically-grounded prompts improve LLM performance but cannot match task-specific fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
Domain-specific pretraining on linguistically relevant tasks transfers to downstream generation in low-resource languages. ByT5-Sanskrit is pretrained on Sanskrit segmentation, dependency parsing, lemmatization, and morpho-syntactic tagging, implicitly encoding sub-skills required for poetry-to-prose conversion. This pretraining allows joint reasoning during fine-tuning without explicit intermediate supervision. Break condition: If pretraining corpus lacks coverage of target genre's vocabulary or syntactic patterns, transfer degrades (BLEU drops from 38.6 to 20.3 when testing Rāmāyaṇa on Mahābhārata-trained model).

### Mechanism 2
Explicit linguistic rules in prompts improve LLM performance but cannot fully compensate for lack of domain-specific pretraining. Providing Pāṇinian grammar rules and Daṇḍa-anvaya-janaka ordering heuristics guides LLMs toward better outputs. Chain-of-thought formatting further improves results by forcing intermediate reasoning steps. However, rule-following alone does not create implicit morphological and syntactic knowledge that pretraining provides. Break condition: When rules are over-specified or contradictory, performance may degrade slightly.

### Mechanism 3
Byte-level tokenization reduces out-of-vocabulary issues in morphologically rich, low-resource languages. ByT5 operates on raw bytes rather than subword tokens, avoiding tokenizer-induced fragmentation of Sanskrit's complex morphological forms. This is particularly valuable when training data is limited and vocabulary coverage is sparse. Break condition: Byte-level models require more compute per token; efficiency gains may not materialize for very long sequences.

## Foundational Learning

- **Sandhi and Samāsa (Sanskrit phonology and compounding)**: Why needed - The poetry-to-prose task requires resolving sandhi (phonological merging at word boundaries) and segmenting compounds (samāsa). Quick check - Given "ityeṣa" in verse, can you explain why the prose form splits to "iti eṣa"?

- **Encoder-Decoder vs Decoder-Only Architectures**: Why needed - The paper compares Seq2Seq (ByT5, encoder-decoder) against LLMs (decoder-only). Understanding architectural differences explains why bidirectional encoding helps for structured linguistic tasks. Quick check - Why might an encoder-decoder model outperform a decoder-only model on tasks requiring complex input analysis before generation?

- **Pāṇinian Grammar Framework**: Why needed - The prompt design and evaluation rules are grounded in Pāṇinian grammatical categories (kārakas, vibhaktis, karmapravacanīya). Understanding this framework is essential for interpreting the rule-based prompting approach. Quick check - In Daṇḍa-anvaya-janaka ordering, why does the locative case (adhikaraṇa) precede the accusative (karman)?

## Architecture Onboarding

- **Component map**: IAST-transliterated Sanskrit verse -> byte-level encoding (ByT5) or tokenizer (LLMs) -> ByT5-Sanskrit: Pretrained encoder-decoder -> full fine-tuning with "P2P" task prefix OR LLM: General-purpose decoder-only -> PEFT/LoRA instruction fine-tuning OR in-context learning with rule-based prompts -> sacreBLEU (lexical overlap) + Kendall's Tau (word-order correctness) + human scoring

- **Critical path**: 1) Prepare parallel verse-prose pairs (22,860 samples from Rāmāyaṇa and Mahābhārata) 2) Convert to IAST transliteration 3) For ByT5: prepend "P2P" prefix, fine-tune end-to-end 4) For LLMs: design linguistically-grounded prompts with Daṇḍa-anvaya-janaka rules 5) Evaluate with BLEU and Kendall's Tau; validate with human expert ratings

- **Design tradeoffs**: Full fine-tuning vs PEFT (ByT5 uses full fine-tuning, LLMs use LoRA with 4-bit quantization); Explicit rules vs implicit learning (rules help LLMs but add prompt length); BLEU vs Kendall's Tau (BLEU captures lexical correctness; Kendall's Tau better captures structural correctness)

- **Failure signatures**: LLM sandhi errors (GPT-4o hallucinates incorrect morphological forms); Word-order violations (LLMs misplace locatives, ablatives, vocatives despite explicit rules); Cross-domain degradation (both model types drop significantly on out-of-domain data)

- **First 3 experiments**: 1) Reproduce ByT5-Sanskrit baseline: Fine-tune on Rāmāyaṇa, evaluate on held-out test set using BLEU and Kendall's Tau (target: ~39.5 BLEU) 2) Ablate prompting strategy: Compare zero-shot, few-shot, and CoT prompts on Phi-4 without fine-tuning (verify rules + CoT ~2x baseline) 3) Cross-domain generalization test: Train on one epic, test on the other (measure transfer gap for both ByT5 and best LLM)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do these findings generalize to other Sanskrit NLP tasks beyond poetry-to-prose conversion?
- Basis in paper: The conclusion states "Future work should generalize these results to more tasks."
- Why unresolved: The study only evaluated the poetry-to-prose (Anvaya) conversion task, which requires coordinated multi-step reasoning. It remains unknown whether the superiority of task-specific models holds for other Sanskrit NLP tasks with different complexity profiles.
- What evidence would resolve it: Systematic comparison of LLMs vs. task-specific models across diverse Sanskrit NLP tasks using consistent evaluation protocols.

### Open Question 2
- Question: Would using native Devanāgarī script instead of IAST transliteration change the relative performance between LLMs and task-specific models?
- Basis in paper: The Limitations section notes "our use of transliterated text instead of native Devanāgarī script simplifies processing but overlooks script-specific challenges."
- Why unresolved: All experiments used IAST transliteration. Both ByT5-Sanskrit and the LLMs may perform differently when processing native script, potentially due to tokenization differences or script-specific patterns learned during pretraining.
- What evidence would resolve it: Replication of experiments using native Devanāgarī script for both model paradigms, comparing performance gaps against the IAST baseline.

### Open Question 3
- Question: Can the cross-domain generalization gap be reduced through multi-domain training or architectural modifications?
- Basis in paper: Cross-domain evaluation showed substantial BLEU drops (ByT5: 38.6→20.3 when trained on Mahābhārata and tested on Rāmāyaṇa), indicating limited transfer across epic styles despite learning generalizable principles.
- Why unresolved: The paper demonstrates the generalization gap exists but does not explore methods to mitigate it, such as joint training on both corpora or architectural changes to improve domain adaptation.
- What evidence would resolve it: Experiments comparing single-domain vs. multi-domain training, and evaluating whether techniques like domain-adversarial training or mixture-of-experts architectures reduce the cross-domain performance penalty.

## Limitations

- The study only evaluated one specific task (poetry-to-prose conversion) and may not generalize to other Sanskrit NLP tasks
- Dataset size (22,860 parallel verse-prose pairs) is modest compared to standard NLP benchmarks, potentially limiting statistical power
- Expert human judgments were collected for only 50 samples per model per dataset, which may not capture the full distribution of error types
- The paper does not explore full fine-tuning of LLMs, limiting conclusions about whether PEFT methods represent a fundamental limitation

## Confidence

**High Confidence (4-5/5)**: ByT5-Sanskrit model outperforms all LLM approaches on primary evaluation metrics (BLEU scores of 38.63 vs 33.12 for Mahābhārata, 39.50 vs 31.95 for Rāmāyaṇa). Human evaluation strongly validates quantitative results, with Kendall's Tau showing high correlation with expert judgments. Cross-domain generalization patterns are consistently observed.

**Medium Confidence (3/5)**: Attribution of performance differences to domain-specific pretraining versus instruction-following ability requires more extensive ablation studies. The claim that byte-level tokenization is crucial for morphological handling is plausible but not directly tested against subword alternatives.

**Low Confidence (2/5)**: Assertion that findings generalize to other low-resource languages or different linguistic tasks remains speculative. The paper's focus on Sanskrit-specific phenomena means mechanisms identified may not transfer to languages with different morphological or syntactic properties.

## Next Checks

1. **Cross-Lingual Transfer Experiment**: Fine-tune the ByT5-Sanskrit model on poetry-to-prose conversion for another morphologically rich, low-resource language (e.g., Tamil or Tibetan) using the same architectural approach. Compare performance against instruction-tuned LLMs to test whether domain-specific pretraining provides systematic advantages across languages.

2. **Ablation of Pretraining Components**: Create variant ByT5 models that retain only specific pretraining tasks (e.g., segmentation only, or dependency parsing only) and compare their poetry-to-prose performance to the full model. This would isolate which pretraining components are essential for the downstream task.

3. **Full Fine-Tuning of LLMs**: Implement full fine-tuning of the best-performing LLM (Phi-4) on the poetry-to-prose task and compare against the ByT5-Sanskrit model. This would determine whether the performance gap is due to architectural constraints or the choice of parameter-efficient fine-tuning methods.