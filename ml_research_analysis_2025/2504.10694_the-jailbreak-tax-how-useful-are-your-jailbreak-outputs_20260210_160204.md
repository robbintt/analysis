---
ver: rpa2
title: 'The Jailbreak Tax: How Useful are Your Jailbreak Outputs?'
arxiv_id: '2504.10694'
source_url: https://arxiv.org/abs/2504.10694
tags:
- jailbreak
- alignment
- math
- prompt
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Jailbreak attacks bypass LLM safety guardrails but often reduce\
  \ model utility. This paper introduces the jailbreak tax\u2014a measurable drop\
  \ in model performance when jailbreaks succeed."
---

# The Jailbreak Tax: How Useful are Your Jailbreak Outputs?

## Quick Facts
- arXiv ID: 2504.10694
- Source URL: https://arxiv.org/abs/2504.10694
- Authors: Kristina Nikolić; Luze Sun; Jie Zhang; Florian Tramèr
- Reference count: 25
- Primary result: Jailbreak attacks bypass LLM safety guardrails but often reduce model utility by up to 92%, introducing the "jailbreak tax" as a key safety metric.

## Executive Summary
This paper introduces the jailbreak tax—a measurable drop in model performance when jailbreak attacks succeed. The authors construct benchmarks where models refuse to answer benign, easy-to-evaluate questions (math, biology), then test eight jailbreak attacks. Results show jailbreak success rates often inversely correlate with utility: some attacks reduce accuracy by up to 92% (e.g., PAIR on GSM8K). More capable models do not consistently reduce this tax. The jailbreak tax persists across alignment types and task difficulties, highlighting that effective jailbreaks can significantly degrade reasoning. This work introduces the jailbreak tax as a key safety metric and provides new benchmarks for evaluating jailbreak utility.

## Method Summary
The authors measure the jailbreak tax by first aligning models to refuse benign tasks (math and biology questions), then applying jailbreak attacks to bypass these refusals. They evaluate utility by measuring performance on the underlying task after a successful jailbreak. The jailbreak tax is defined as the relative drop in utility when a jailbreak is applied versus when no jailbreak is used. Experiments cover three model sizes (8B, 70B, 405B parameters) and eight attack types, including baselines (system prompt override, fine-tuning) and optimized attacks (GCG, AutoDAN, PAIR, TAP, MultiJail).

## Key Results
- Jailbreak attacks that bypass safety guardrails can reduce model accuracy by up to 92% on benign tasks
- The jailbreak tax remains consistently high across model scales (8B to 405B parameters)
- Attacks that substantially rewrite prompts (PAIR, TAP) degrade reasoning more than those that preserve prompt structure (Many-shot)
- System prompt jailbreak and Many-shot are the only attacks that consistently preserve utility

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Jailbreak methods that substantially rewrite prompts (e.g., PAIR, TAP) degrade reasoning more than methods that add context without altering core semantics (e.g., Many-shot).
- **Mechanism:** Prompt rewriting attacks introduce semantic drift or distractor context that interferes with multi-step reasoning chains, especially for tasks requiring precise arithmetic or logical steps. The model still produces outputs but with flawed intermediate reasoning.
- **Core assumption:** The jailbreak's modification of the prompt structure—not just the bypass of refusal—is the primary cause of utility loss.
- **Evidence anchors:**
  - [abstract] "while all jailbreaks we tested bypass guardrails... this comes at the expense of a drop of up to 92% in accuracy"
  - [section 4, Figure 6] Shows concrete examples where PAIR/TAP succeed in bypass but produce incorrect reasoning steps and wrong final answers
  - [corpus] Limited direct evidence in corpus; related work (SecurityLingua, InfoFlood) focuses on defense/attack success rates, not utility preservation
- **Break condition:** If a rewriting attack is constrained to preserve all numerically/logically relevant information (as attempted with "PAIR (don't modify)"), the tax should decrease—but results show this constraint alone is insufficient.

### Mechanism 2
- **Claim:** The jailbreak tax is largely independent of model capability; scaling from 8B to 405B parameters does not systematically reduce utility loss.
- **Mechanism:** Jailbreaks exploit prompt-level vulnerabilities rather than capability limitations. Larger models may even be more susceptible to certain attacks due to stronger instruction-following tendencies that amplify distractor context.
- **Core assumption:** The tax reflects a mismatch between the jailbreak's perturbation and the model's learned task representations, not a lack of reasoning capacity.
- **Evidence anchors:**
  - [section 4] "we find that the jailbreak tax remains similarly high for most attacks" across 8B, 70B, and 405B models
  - [Figure 9, Appendix B] Shows comparable tax distributions across model scales for both WMDP and GSM8K
  - [corpus] No direct corpus evidence on model scaling effects; neighbor papers focus on attack transferability, not capability-preservation relationships
- **Break condition:** If future jailbreaks target capability restoration explicitly (e.g., by preserving reasoning-relevant tokens), larger models might show lower tax due to greater robustness to minor perturbations.

### Mechanism 3
- **Claim:** The jailbreak tax arises primarily from the attack methodology rather than the alignment process itself.
- **Mechanism:** Since baseline attacks that directly counteract alignment (system prompt jailbreak, finetuning) preserve near-zero tax, the observed utility degradation is not an inherent consequence of bypassing alignment but depends on how the bypass is achieved.
- **Core assumption:** The pseudo-alignment methods used (system prompt, SFT, EvilMath) do not themselves degrade base model capabilities on neutral tasks.
- **Evidence anchors:**
  - [section 4] "the baseline system prompt jailbreak and Many-shot are the only jailbreaks that consistently preserve the utility"
  - [Appendix B, Tables 4-5] Pseudo-aligned models show no significant performance drop on neutral MMLU/MATH subsets
  - [corpus] Related work on fine-tuning vulnerabilities (e.g., "Why LLM Safety Guardrails Collapse After Fine-tuning") suggests alignment can be reversed without capability loss, supporting the separation
- **Break condition:** If alignment itself imposed a significant capability tax (as hypothesized in alignment tax literature), even optimal jailbreaks would show residual utility loss.

## Foundational Learning

- **Concept:** Jailbreak attacks vs. adversarial perturbations
  - **Why needed here:** Jailbreaks are optimized for refusal bypass, not utility preservation; understanding this distinction explains why high success rates can coexist with high tax.
  - **Quick check question:** If a jailbreak achieves 99% success rate on a refusal benchmark, can you infer its utility on the underlying task?

- **Concept:** Conditional evaluation metrics (success rate vs. utility)
  - **Why needed here:** The paper conditions utility on successful jailbreak (Equation 2), separating "did it bypass?" from "was the answer correct?"; conflating these leads to misinterpretation.
  - **Quick check question:** Given JailSucc = 80% and JailUtil = 60%, what fraction of original prompts yield correct answers after jailbreaking?

- **Concept:** Pseudo-alignment for controlled experiments
  - **Why needed here:** The methodology of aligning models to refuse benign tasks (math, biology) creates ground-truth evaluable scenarios; this is the key enabler for measuring tax objectively.
  - **Quick check question:** Why can't we directly measure jailbreak tax on genuinely harmful tasks like bomb-making instructions?

## Architecture Onboarding

- **Component map:**
  - **Alignment module:** Three types tested—system prompt (weakest, highest refusal when no attack), SFT (stronger, 95%+ refusal), EvilMath/UnicornMath (triggers existing safety mechanisms via rewording)
  - **Attack surface:** 8 attack types spanning baselines (finetuning, prompt override), in-context (Many-shot), optimization (GCG, AutoDAN), and LLM-assisted rephrasing (PAIR, TAP, MultiJail)
  - **Evaluation pipeline:** Ground-truth datasets (WMDP MCQ, GSM8K/MATH arithmetic) → alignment → jailbreak application → parse response format → compute JailSucc, JailUtil, JTax

- **Critical path:** 
  1. Select benign evaluable task (e.g., GSM8K math problems)
  2. Apply pseudo-alignment (system prompt fastest; SFT more realistic; EvilMath for closed models)
  3. Apply jailbreak attack (note: attacks cannot modify system prompt portion)
  4. Extract answer, verify format compliance, compare to ground truth
  5. Compute JTax = (BaseUtil - JailUtil) / BaseUtil

- **Design tradeoffs:**
  - System prompt alignment: fastest to implement, highest refusal rates, but less representative of real alignment
  - SFT alignment: more realistic, but requires compute for finetuning; refusal rates slightly lower than strict prompt-based refusal
  - EvilMath: tests real safety mechanisms, but rewording process may introduce noise (controlled via UnicornMath baseline)

- **Failure signatures:**
  - High JailSucc + High JailUtil → attack preserves capabilities (desired outcome for attacker)
  - High JailSucc + Low JailUtil → high jailbreak tax (attack succeeds but degrades output quality)
  - Low JailSucc → attack fails to bypass regardless of utility preservation
  - Incorrect reasoning chains (Figure 6) → signature of semantic interference attacks (PAIR, TAP)

- **First 3 experiments:**
  1. **Establish baseline:** Run unaligned model on GSM8K, record BaseUtil; then apply system prompt alignment without jailbreak, verify refusal rate >95%
  2. **Calibrate low-tax attack:** Apply Many-shot jailbreak (50/100/200 examples) to aligned model, confirm JailSucc and JailUtil remain near BaseUtil
  3. **Measure high-tax attack:** Apply PAIR attack with default settings, expect JailSucc >80% but JailUtil drop >40% on GSM8K; inspect failure cases for reasoning errors per Figure 6 pattern

## Open Questions the Paper Calls Out
None

## Limitations
- The jailbreak tax was measured only on math and biology tasks; generalization to other domains is unknown
- Real-world alignment scenarios are not perfectly captured by the pseudo-alignment methods used
- The analysis treats attacks as black boxes without fully dissecting which prompt features cause the most utility degradation

## Confidence
- **High confidence:** The core empirical finding that jailbreak attacks can significantly degrade utility (high jailbreak tax) is robust, with consistent results across multiple datasets, model scales, and attack types
- **Medium confidence:** The claim that the jailbreak tax is largely independent of model capability is supported by experiments but limited to a small set of models
- **Medium confidence:** The assertion that the jailbreak tax arises from attack methodology rather than alignment itself is plausible but relies on indirect evidence

## Next Checks
1. **Cross-domain validation:** Apply the jailbreak tax methodology to a broader set of tasks (e.g., commonsense reasoning, code generation, multi-hop reasoning) to test whether the tax generalizes beyond math and biology
2. **Mechanism dissection:** Conduct ablation studies on jailbreak prompts to identify which components (semantic drift, distractor context, formatting changes) contribute most to utility degradation, enabling more targeted defenses
3. **Alignment regime sensitivity:** Measure jailbreak tax under different alignment methods (e.g., RLHF with varying reward models, constitutional AI) to determine if the tax varies with alignment process, not just alignment strength