---
ver: rpa2
title: 'Pisets: A Robust Speech Recognition System for Lectures and Interviews'
arxiv_id: '2601.18415'
source_url: https://arxiv.org/abs/2601.18415
tags:
- speech
- whisper
- recognition
- audio
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Pisets, a three-component speech recognition
  system designed to improve accuracy while reducing hallucinations in the Whisper
  model. The system uses Wav2Vec2 for initial speech segmentation, AST for false positive
  filtering, and Whisper for final transcription.
---

# Pisets: A Robust Speech Recognition System for Lectures and Interviews

## Quick Facts
- **arXiv ID**: 2601.18415
- **Source URL**: https://arxiv.org/abs/2601.18415
- **Reference count**: 14
- **Primary result**: Pisets achieves 0.1065 WER vs WhisperX's 0.1683 on lecture/interview domain

## Executive Summary
Pisets is a three-component speech recognition system designed to improve accuracy while reducing hallucinations in the Whisper model. The system combines Wav2Vec2 for initial speech segmentation, AST for false positive filtering, and Whisper for final transcription. The authors introduce uncertainty modeling techniques to highlight unreliable transcriptions. Experimental results demonstrate Pisets outperforms WhisperX with a WER of 0.1065 compared to 0.1683, and achieves a BERT score of 0.9652. The uncertainty estimation methods can identify up to 35% of errors by marking only 5% of words as uncertain.

## Method Summary
Pisets implements a modular approach to speech recognition by combining three specialized components: Wav2Vec2 for speech activity detection and segmentation, AST for filtering out non-speech segments and reducing false positives, and Whisper for high-quality transcription. The system introduces uncertainty modeling techniques that analyze transcription confidence to identify potentially unreliable segments. This architecture specifically targets the lecture and interview domain where background noise and overlapping speech are common challenges.

## Key Results
- Achieves WER of 0.1065 compared to WhisperX's 0.1683 on lecture/interview data
- BERT score of 0.9652 demonstrates high semantic quality
- Uncertainty estimation identifies up to 35% of errors while marking only 5% of words as uncertain
- Successfully reduces hallucinations through the three-component filtering approach

## Why This Works (Mechanism)
The system works by creating a robust pipeline that addresses Whisper's weaknesses in speech segmentation and false positive generation. Wav2Vec2 provides accurate speech activity detection to ensure proper segmentation, AST filters out non-speech segments that could lead to hallucinations, and Whisper handles the actual transcription with its strong language modeling capabilities. The uncertainty modeling component adds a layer of quality control by identifying segments where the model is less confident, allowing users to focus verification efforts on the most problematic areas.

## Foundational Learning

**Speech Activity Detection (Wav2Vec2)**
- Why needed: Accurately identifies speech segments vs non-speech for proper segmentation
- Quick check: Test on varied noise conditions and overlapping speech scenarios

**False Positive Filtering (AST)**
- Why needed: Reduces hallucinations by eliminating non-speech segments before transcription
- Quick check: Measure reduction in hallucination rate vs baseline Whisper

**Uncertainty Modeling**
- Why needed: Identifies unreliable transcriptions for human-in-the-loop workflows
- Quick check: Compare error detection rate vs human annotator uncertainty

## Architecture Onboarding

**Component Map**: Wav2Vec2 -> AST -> Whisper -> Uncertainty Modeling

**Critical Path**: Audio input → Wav2Vec2 segmentation → AST filtering → Whisper transcription → Uncertainty estimation → Final output

**Design Tradeoffs**: The three-component approach adds latency but improves accuracy and reduces hallucinations. The system trades computational efficiency for higher quality transcriptions, making it suitable for applications where accuracy is prioritized over real-time processing.

**Failure Signatures**: System may struggle with highly overlapping speech, extreme background noise, or languages beyond the trained domain. The uncertainty model may not capture all types of errors, particularly semantic misunderstandings that don't affect confidence scores.

**First 3 Experiments**:
1. Test WER improvement on varied acoustic conditions (different noise levels, room acoustics)
2. Evaluate hallucination reduction by comparing false positive rates between Pisets and Whisper
3. Measure uncertainty model effectiveness by correlating uncertainty scores with human error annotations

## Open Questions the Paper Calls Out
None

## Limitations
- Performance evaluation limited to English lectures and interviews only
- Small dataset size (20.9 hours) may not capture full real-world variability
- Computational efficiency and real-time processing capabilities not addressed

## Confidence

**High Confidence**: Technical implementation details and reported WER/BERT score improvements are specific and reproducible

**Medium Confidence**: Uncertainty modeling effectiveness demonstrated but practical utility for end-users underexplored

**Low Confidence**: Claims about robustness across diverse real-world conditions not empirically supported beyond narrow evaluation domain

## Next Checks
1. Evaluate Pisets on multilingual and multidomain datasets to assess cross-lingual robustness
2. Conduct ablation studies to quantify individual contributions of Wav2Vec2 segmentation and AST filtering
3. Test practical utility of uncertainty estimates on downstream tasks like information retrieval or human-in-the-loop correction workflows