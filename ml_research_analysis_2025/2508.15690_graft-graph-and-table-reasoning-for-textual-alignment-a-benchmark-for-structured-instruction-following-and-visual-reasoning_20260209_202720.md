---
ver: rpa2
title: 'GRAFT: GRaPH and Table Reasoning for Textual Alignment -- A Benchmark for
  Structured Instruction Following and Visual Reasoning'
arxiv_id: '2508.15690'
source_url: https://arxiv.org/abs/2508.15690
tags:
- visual
- reasoning
- table
- structured
- chart
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GRAFT is a benchmark for multimodal reasoning over charts and tables,
  addressing the gap in evaluating structured visual understanding. It uses synthetically
  generated visuals and programmatically constructed questions, with answers in JSON/YAML
  format for fine-grained evaluation.
---

# GRAFT: GRaPH and Table Reasoning for Textual Alignment -- A Benchmark for Structured Instruction Following and Visual Reasoning

## Quick Facts
- arXiv ID: 2508.15690
- Source URL: https://arxiv.org/abs/2508.15690
- Reference count: 40
- Key outcome: GRAFT is a benchmark for multimodal reasoning over charts and tables, addressing the gap in evaluating structured visual understanding.

## Executive Summary
GRAFT is a benchmark designed to evaluate multimodal reasoning over charts and tables, addressing the lack of standardized assessment for structured visual understanding. It uses synthetically generated visuals and programmatically constructed questions, with answers in JSON/YAML format for fine-grained evaluation. The benchmark includes a taxonomy of reasoning tasks such as comparison, trend identification, ranking, and aggregation. Evaluation across multiple models shows performance varies by architecture, with larger, instruction-tuned models generally performing better, especially on correctness and visual grounding. GRAFT enables rigorous assessment of fine-grained visual-textual alignment and structured reasoning capabilities.

## Method Summary
GRAFT uses a three-stage pipeline: (1) synthetic visual generation with LLM validation, (2) structured QA construction via VLM saliency extraction and programmatic answer derivation, and (3) jury-of-judges filtering with consensus. Visuals are rendered via Matplotlib/Seaborn from tabular seeds T ∈ R^(m×n) with m ∈ [8,20], n ∈ [3,6]. Questions are answerable solely from the visual. Responses are evaluated by a GPT-4o-based judge on four axes: Correctness, Completeness, Visual Grounding, and Format Fidelity, each scored 1-5. The dataset includes 1,412 Chart-QnA and 1,739 Table-QnA instances, available on Hugging Face.

## Key Results
- Models with compression architectures (e.g., Pixtral) excel at Chart-QnA but degrade on Table-QnA, while fusion architectures (e.g., Qwen) handle both better.
- Performance varies by reasoning type, with some models better at comparison and ranking than aggregation.
- Larger, instruction-tuned models generally perform better on correctness and visual grounding.
- Structured JSON/YAML outputs enable fine-grained, automated evaluation of model responses.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Synthetic visual generation provides a cleaner assessment of reasoning by minimizing noise and controlling variation, isolating logical capabilities from perceptual robustness.
- Mechanism: By programmatically generating charts and tables using Python libraries (Matplotlib/Seaborn) rather than scraping natural images, the pipeline controls for variables like noise levels, sparsity, and trend magnitude. This reduces the confounding factors present in datasets like DocVQA or ChartQA, allowing the benchmark to specifically target analytical reasoning operations (comparison, aggregation) rather than OCR quality or image restoration.
- Core assumption: The synthetic visual artifacts do not introduce a distribution shift that unfairly penalizes models trained primarily on natural images.
- Evidence anchors: [abstract] "...dataset is built around programmatically generated charts and synthetically rendered tables..."; [section 1.1] "...synthetic generation provides a scalable alternative that mitigates these issues—though it may occasionally introduce minor artifacts..."; [corpus] MMTBENCH (arXiv:2505.21771) confirms the difficulty of multimodal tables, supporting the need for controlled environments to isolate reasoning failures.
- Break condition: If a model's failure mode in production is primarily handling corrupted real-world scans (blur, skew) rather than logical inference, this mechanism reduces ecological validity.

### Mechanism 2
- Claim: Requiring structured outputs (JSON/YAML) functions as a dual-constraint mechanism, testing both visual reasoning accuracy and instruction-following fidelity simultaneously.
- Mechanism: The benchmark forces the model to map unstructured visual evidence into a rigid, schema-constrained format. This prevents "fuzzy" natural language answers that might appear correct but lack precision. It exposes a disconnect where models may correctly interpret a visual but fail to serialize that understanding into the specified keys (e.g., `largest_energy_group`) without hallucinations or syntax errors.
- Core assumption: The schema parsing logic is robust enough that syntax errors are truly model failures and not ambiguous prompts.
- Evidence anchors: [abstract] "Responses are formatted in structured outputs such as JSON or YAML, enabling consistent and fine grained evaluation..."; [section 3.3] "Schema and determinism checks. Validate a against S... reject if mismatched."; [corpus] ChartAnchor (arXiv:2512.01017) emphasizes "bidirectional alignment" between visuals and semantics, paralleling GRAFT's schema-binding approach.
- Break condition: If the schema complexity exceeds the model's context window or instruction-following capacity for logic, scores reflect constraint satisfaction rather than visual intelligence.

### Mechanism 3
- Claim: Performance differentials between Chart-QnA and Table-QnA reveal architectural limitations in visual grounding, specifically distinguishing between "shallow" visual feature matching and "deep" relational reasoning.
- Mechanism: The paper observes that models with compression architectures (e.g., Pixtral) excel at Chart-QnA (identifying trends/spikes) but degrade on Table-QnA (relational structures). Conversely, fusion architectures (e.g., Qwen) handle both better. This suggests that compressing visual tokens preserves salient visual features for charts but loses the fine-grained spatial/relational density required for table structures.
- Core assumption: The difference in scores is attributable to the visual encoder/connector architecture rather than just the scale of the language backbone.
- Evidence anchors: [section 5.1] "Pixtral 12B excels on Chart-QnA... but its accuracy degrades on Table-QnA. This divergence suggests that Pixtral’s strengths lie in shallow visual-text alignment... but it lacks the symbolic reasoning depth."; [section 5.2] "Visual grounding remains the main bottleneck. Table-based reasoning in particular exposes weaknesses in capturing implicit relations..."; [corpus] TableMoE (arXiv:2506.21393) supports the difficulty of "symbolic density" in tables, aligning with GRAFT's findings on grounding.
- Break condition: If the table rendering style in GRAFT unintentionally favors specific tokenization strategies, the architectural comparison is biased.

## Foundational Learning

- Concept: **Cross-Modal Grounding vs. Extraction**
  - Why needed here: GRAFT explicitly distinguishes between "Visual Grounding" (does the answer align with the image marks?) and "Correctness" (is the fact true?). A model can hallucinate a correct-sounding fact that isn't in the image.
  - Quick check question: Does the model's cited value (e.g., "54.5%") exist as a visible label in the chart, or is it inferred from external knowledge/hallucination?

- Concept: **Instruction Taxonomy (Reasoning Types)**
  - Why needed here: The benchmark segments tasks into Comparison, Trend, Aggregation, etc. Understanding this taxonomy is required to diagnose *where* a model fails (e.g., it can rank but cannot aggregate).
  - Quick check question: Looking at a failure case, is the error in identifying the highest bar (Ranking) or in summing two bars (Aggregation)?

- Concept: **LLM-as-a-Judge (Jury-of-Judges)**
  - Why needed here: GRAFT relies on a GPT-4o based judge for evaluation. Understanding the judge's rubric (Correctness vs. Format Fidelity) is critical because the "Ground Truth" is validated by this automated jury, not just human annotators.
  - Quick check question: Did the model fail because the answer was wrong, or because the JSON was missing a required key (Format Fidelity)?

## Architecture Onboarding

- Component map: Synthetic Visual Generator -> QA Constructor -> Jury Filter -> Evaluation Pipeline
- Critical path: 1. Seed Data -> Visual Rendering: Ensure tabular seeds match the rendered image (Stage I validation). 2. Visual -> Structured QA: Generate questions that are *answerable solely from the visual* (Stage II constraint). 3. Model Inference: Model receives (Image, Question) and outputs YAML/JSON. 4. Evaluation: Judge compares Model Output vs. Golden Response.
- Design tradeoffs: Trading synthetic vs. natural data for perfect control over reasoning complexity and answerability; Trading generation flexibility for automated, parseable evaluation precision; Trading evaluation cost/latency for robustness against single-judge bias.
- Failure signatures: High Format / Low Correctness: The model follows instructions but hallucinates data (common in smaller models like GPT4o-mini). Chart > Table Performance: Indicates the visual encoder uses compression that destroys relational structure (common in models like Pixtral). Low Visual Grounding: The model answers correctly using prior knowledge rather than the provided image (common in GPT4o).
- First 3 experiments: 1. Baseline Profiling: Run Qwen-2.5-32B-VL and Pixtral-12B on a 100-sample subset of Chart-QnA vs. Table-QnA to replicate the architectural divergence (Fusion vs. Compression). 2. Format Stress Test: Manually modify the required output schema (e.g., add a nested key) to see if Format Fidelity drops faster than Correctness, isolating instruction-following capacity. 3. Grounding Ablation: Evaluate a text-only LLM (blind to the image) with just the question. If scores are non-zero, the benchmark contains data leakage or answerable questions not strictly grounded in the visual.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does high performance on the synthetic GRAFT benchmark correlate with improved performance on real-world, human-annotated chart and table reasoning tasks?
- Basis in paper: [explicit] Section 8 (Limitations) states that the "reliance on synthetic data may reduce real-world generalizability" and lists "incorporating human-annotated benchmarks" as a future improvement.
- Why unresolved: The current study evaluates models exclusively on programmatically generated visuals, leaving the transferability of these reasoning skills to natural, noisy, or scanned documents unverified.
- What evidence would resolve it: A comparative study evaluating models fine-tuned on GRAFT against held-out, human-curated datasets (e.g., ChartQA, DocVQA) to measure the domain gap.

### Open Question 2
- Question: To what degree do LLM-based judges overestimate correctness and visual grounding scores compared to human expert evaluation?
- Basis in paper: [explicit] Section 8 notes that "the use of LLM-based judges can sometimes overestimate answer quality, especially in nuanced cases."
- Why unresolved: The evaluation pipeline relies on automated GPT-4o-based scoring, which introduces a potential systematic bias that remains unquantified relative to human judgment.
- What evidence would resolve it: A correlation analysis between the automated Likert scores and a stratified sample of human expert ratings on the same model outputs.

### Open Question 3
- Question: Do current visual grounding limitations persist in underrepresented complex chart types, such as heatmaps and Sankey diagrams?
- Basis in paper: [explicit] Section 8 identifies that "some complex chart types—such as heatmaps and sankey diagrams—are underrepresented in the current version."
- Why unresolved: The current dataset taxonomy focuses primarily on standard plots (bar, line, pie), leaving the models' ability to parse dense, relational, or matrix-based visual encodings untested.
- What evidence would resolve it: Extending the GRAFT pipeline to generate heatmaps and Sankey diagrams and benchmarking the "Visual Grounding" scores of current state-of-the-art models on these specific types.

## Limitations

- Reliance on synthetic data may reduce real-world generalizability, particularly for tasks requiring handling of scanned documents or noisy visuals.
- The automated jury-of-judges evaluation pipeline depends on the consistency and bias of the LLM judge, and the rubric is not fully open-sourced.
- The lack of transparency in the exact prompts and calibration examples used for the GPT-4o judge introduces uncertainty in reproducibility.

## Confidence

- **High Confidence**: The claim that structured JSON/YAML outputs enable fine-grained, automated evaluation is strongly supported by the paper's methodology and the explicit scoring rubric.
- **Medium Confidence**: The claim that performance differences between Chart-QnA and Table-QnA reveal architectural limitations is plausible but depends on the assumption that the synthetic data distribution is fair to all architectures.
- **Medium Confidence**: The claim that synthetic generation provides a cleaner assessment of reasoning by minimizing noise is supported by the design, but the trade-off in ecological validity is a limitation.

## Next Checks

1. Reproduce the architectural divergence: Run a controlled experiment with Qwen-2.5-32B-VL and Pixtral-12B on a 100-sample subset of Chart-QnA vs. Table-QnA to confirm the observed performance gap and its attribution to fusion vs. compression architectures.
2. Validate the automated judge: Manually review a random sample of 50 model predictions and their GPT-4o judge scores, comparing them against human-annotated scores for Correctness and Visual Grounding to assess judge reliability and potential bias.
3. Test for data leakage: Evaluate a text-only LLM (blind to the image) with just the questions on a 50-sample subset. If scores are non-zero, investigate whether the questions are answerable without visual grounding, indicating a leak in the benchmark design.