---
ver: rpa2
title: Taming Diffusion for Dataset Distillation with High Representativeness
arxiv_id: '2505.18399'
source_url: https://arxiv.org/abs/2505.18399
tags:
- dataset
- distribution
- distillation
- latents
- sampling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper tackles the problem of generating compact, representative\
  \ distilled datasets for deep learning by improving diffusion-based methods. It\
  \ identifies key issues in prior approaches\u2014inaccurate distribution matching\
  \ in the VAE latent space, structural information loss due to random noise injection,\
  \ and lack of consideration for overall distribution alignment\u2014and proposes\
  \ D3HR to address them."
---

# Taming Diffusion for Dataset Distillation with High Representativeness

## Quick Facts
- **arXiv ID:** 2505.18399
- **Source URL:** https://arxiv.org/abs/2505.18399
- **Authors:** Lin Zhao; Yushu Wu; Xinru Jiang; Jianyang Gu; Yanzhi Wang; Xiaolin Xu; Pu Zhao; Xue Lin
- **Reference count:** 37
- **Primary result:** D3HR outperforms state-of-the-art dataset distillation methods, achieving up to 12.5% accuracy improvement on CIFAR-10 with ResNet-18.

## Executive Summary
This paper tackles the problem of generating compact, representative distilled datasets for deep learning by improving diffusion-based methods. It identifies key issues in prior approaches—inaccurate distribution matching in the VAE latent space, structural information loss due to random noise injection, and lack of consideration for overall distribution alignment—and proposes D3HR to address them. The method uses DDIM inversion to map latents from a complex, low-normality VAE latent space to a high-normality Gaussian domain, enabling more accurate distribution matching. It then employs a group sampling scheme to select the most representative subset based on statistical metrics, ensuring both diversity and structural consistency. Experiments across multiple datasets and architectures (CIFAR-10/100, Tiny-ImageNet, ImageNet-1K) show D3HR significantly outperforms state-of-the-art methods, with improvements of up to 12.5% on CIFAR-10 (ResNet-18, 50 IPC) and 27.5% in robustness across different seeds. It also demonstrates superior cross-architecture generalization and reduced storage requirements.

## Method Summary
D3HR addresses three core challenges in diffusion-based dataset distillation: inaccurate distribution matching in VAE latent space, structural information loss from random noise injection, and lack of overall distribution alignment. The method introduces a three-step process: First, DDIM inversion maps latents from the complex, low-normality VAE latent space to a high-normality Gaussian domain, enabling more accurate distribution matching. Second, group sampling selects the most representative subset by sampling multiple subsets and choosing the one minimizing a statistical distance loss (mean, std, skewness). Finally, DDIM sampling decodes the selected latents back to images. The approach is evaluated on CIFAR-10/100, Tiny-ImageNet, and ImageNet-1K using ResNet architectures, showing significant improvements over state-of-the-art methods.

## Key Results
- Achieves up to 12.5% accuracy improvement on CIFAR-10 with ResNet-18 (50 IPC) compared to state-of-the-art methods
- Demonstrates 27.5% improvement in robustness across different random seeds
- Shows superior cross-architecture generalization, maintaining high performance when student models differ from those used in distillation
- Reduces storage requirements while maintaining or improving classification accuracy

## Why This Works (Mechanism)
The effectiveness of D3HR stems from its three key innovations. First, DDIM inversion transforms latents from the VAE's complex, low-normality latent space into a high-normality Gaussian domain, enabling more accurate distribution matching through Gaussian statistics. Second, the group sampling strategy samples multiple subsets (1e5 in experiments) and selects the one minimizing a statistical distance loss, ensuring the distilled dataset captures both diversity and structural consistency. Third, the method explicitly considers overall distribution alignment rather than just class-level statistics, preserving the relationships between different data points. These innovations work together to create a distilled dataset that better represents the original data distribution while maintaining high classification accuracy.

## Foundational Learning
- **Diffusion Models (DiT):** Generative models that denoise latents step-by-step to create images. Why needed: Form the foundation for generating synthetic distilled images. Quick check: Can you explain the forward (noising) and reverse (denoising) processes?
- **Variational Autoencoders (VAE):** Encoder-decoder networks that compress images into latent representations. Why needed: Provide the initial latent space for diffusion models. Quick check: What's the difference between VAE and standard autoencoders?
- **DDIM Inversion:** Technique to map latents from complex spaces to Gaussian domains. Why needed: Enables accurate distribution matching by transforming latents to high-normality space. Quick check: How does DDIM inversion differ from standard DDIM sampling?
- **Statistical Distance Metrics:** Measures like mean, standard deviation, and skewness differences. Why needed: Quantify how well sampled subsets represent the original distribution. Quick check: Can you compute these metrics for a small dataset?
- **Group Sampling:** Strategy to select representative subsets by sampling multiple candidates. Why needed: Ensures diversity and structural consistency in distilled datasets. Quick check: How does this differ from random sampling?

## Architecture Onboarding

**Component Map:**
VAE Encoder -> DiT (Fine-tuned) -> DDIM Inversion -> Group Sampling -> DDIM Sampling -> Distilled Images

**Critical Path:**
The most critical sequence is: VAE Encoding → DiT Fine-tuning → DDIM Inversion → Group Sampling → DDIM Sampling → Student Training. Any failure in this chain will compromise the final distilled dataset quality.

**Design Tradeoffs:**
- DDIM inversion steps (T): Too few (T<31) leads to poor normality and inaccurate distribution matching; too many (T>31) causes structural information loss. The paper finds T=31 optimal for ImageNet-1K.
- Group sampling size (m): Larger m (1e5 in experiments) improves selection quality but increases computational cost (2.6s per class on A6000 GPU).
- Statistical distance weights: The paper uses weights (1, 1, 0.5) for mean, std, and skewness respectively, balancing different aspects of distribution matching.

**Failure Signatures:**
- Poor image quality/diversity: Check DDIM inversion steps T; should be around 31 for optimal results
- High variance in student accuracy: Ensure group sampling is used; random sampling (Base-RS) leads to instability
- Sampling inefficiency: Group sampling with m=1e6 is computationally heavy; requires GPU parallelization

**First Experiments:**
1. Reproduce the 400-epoch DiT fine-tuning with AdamW (learning rate to be determined) on CIFAR-10 and verify latent distribution alignment
2. Compare student model accuracy using random sampling (Base-RS) vs. the proposed group sampling with m=1e5 on CIFAR-10 (10 IPC) to confirm stability improvement
3. Train a student model (e.g., ResNet-50) on distilled images generated for a different architecture (e.g., ResNet-18) to verify cross-architecture generalization

## Open Questions the Paper Calls Out

**Open Question 1:** Can the optimal number of DDIM inversion steps be theoretically determined or adaptively adjusted for different datasets to balance the trade-off between structural information loss and latent normality? The paper relies on empirical grid search to find the "sweet spot" without providing a theoretical bound or metric for optimal inversion depth.

**Open Question 2:** Does the D3HR framework generalize to dense prediction tasks such as object detection or semantic segmentation, where preserving spatial relationships in the distilled dataset is critical? The current distribution matching relies on class-level latent statistics, which may not sufficiently capture the spatial layout required for detection/segmentation tasks.

**Open Question 3:** Can the group sampling strategy be refined to fully close the performance gap with multi-teacher methods without incurring the cost of training multiple teacher models for soft labels? It is unclear if the information gained from multiple soft labels can be encoded directly into the distilled dataset during the sampling phase.

## Limitations
- Missing hyperparameters for DiT fine-tuning stage (optimizer and learning rate unspecified)
- Unspecified DiT architecture configuration (only referenced by citation "Peebles & Xie, 2023")
- Incomplete details on soft label generation process (teacher model training specifics not fully detailed)

## Confidence

**High Confidence:** The core methodological innovations (DDIM inversion for domain mapping, group sampling for representative subset selection) are well-described and validated across multiple datasets and architectures. The ablation studies clearly demonstrate the necessity of each component.

**Medium Confidence:** The quantitative results are impressive, but the exact reproduction depends on matching the fine-tuning procedure and soft label generation. The claim of SOTA performance is supported, but minor implementation differences in the DiT or teacher model could affect absolute numbers.

**Low Confidence:** N/A

## Next Checks

1. Reproduce the 400-epoch DiT fine-tuning with AdamW (learning rate to be determined) on CIFAR-10 and verify latent distribution alignment
2. Compare student model accuracy using random sampling (Base-RS) vs. the proposed group sampling with m=1e5 on CIFAR-10 (10 IPC) to confirm the claimed stability improvement
3. Train a student model (e.g., ResNet-50) on distilled images generated for a different architecture (e.g., ResNet-18) to verify the claimed robustness to model choice