---
ver: rpa2
title: 'When TableQA Meets Noise: A Dual Denoising Framework for Complex Questions
  and Large-scale Tables'
arxiv_id: '2509.17680'
source_url: https://arxiv.org/abs/2509.17680
tags:
- table
- evidence
- question
- enotab
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of table question answering
  (TableQA) under substantial noise from both complex questions and large-scale tables.
  The authors propose EnoTab, a dual denoising framework that enhances two core capabilities:
  relevance filtering for questions and table pruning for tables.'
---

# When TableQA Meets Noise: A Dual Denoising Framework for Complex Questions and Large-scale Tables

## Quick Facts
- arXiv ID: 2509.17680
- Source URL: https://arxiv.org/abs/2509.17680
- Reference count: 36
- Primary result: State-of-the-art TableQA performance on noisy complex questions and large-scale tables, with up to 8.7% improvement on large-scale tasks

## Executive Summary
This paper addresses the challenge of table question answering (TableQA) under substantial noise from both complex questions and large-scale tables. The authors propose EnoTab, a dual denoising framework that enhances two core capabilities: relevance filtering for questions and table pruning for tables. EnoTab decomposes questions into minimal semantic units (evidences), assesses their relevance based on consistency and usability criteria, and constructs an explicit Evidence Tree to guide transparent table pruning with rollback mechanisms to prevent loss of critical data. Experiments on four datasets show that EnoTab achieves state-of-the-art performance, with up to 8.7% improvement on large-scale table tasks, demonstrating its effectiveness in handling complex questions and noisy large-scale tables.

## Method Summary
EnoTab operates in two stages: Evidence-based Question Denoising (EQD) and Evidence Tree-guided Table Denoising (ETD). In EQD, questions are decomposed into evidence triplets (area, condition, action) via LLM using two-stage retrieval (LSH + embedding re-ranking) to sample representative rows. Evidence relevance is assessed through multi-round LLM generation, consistency scoring (n=5 rounds, α=0.8 threshold), and usability checks via grounding toolkit. In ETD, a binary Evidence Tree is constructed with evidences as leaves and AND/OR operators as internal nodes. The tree executes via post-order traversal, with And2Or rollback mechanisms replacing AND with OR when empty tables occur, followed by completeness verification. The final pruned subtable and highlighted question guide LLM answer generation.

## Key Results
- Achieves state-of-the-art performance on WikiTQ (2.3% improvement) and TabFact (1.1% improvement)
- Shows significant gains on large-scale datasets: 8.7% improvement on STQA-L and 6.4% on STQA-N
- Reduces table context size by 87.3% on average while maintaining or improving accuracy
- And2Or rollback mechanism prevents empty table failures and improves robustness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing questions into minimal semantic units ("evidences") and filtering via consistency + usability criteria reduces spurious correlations in complex questions.
- Mechanism: Each evidence is a triplet (area, condition, action) tied to a single column/operation. Multi-round LLM generation produces n evidence sets; an algorithm computes per-evidence consistency scores. A semantic discriminator (e.g., LLaMA-7B) judges condition equivalence across rounds. Evidences below threshold α are discarded. A toolkit P then checks whether each evidence can be grounded in the table (non-empty result), discarding unusable ones.
- Core assumption: Irrelevant semantic units exhibit lower consistency across LLM generations and/or fail table grounding; relevant units are stable and groundable.

### Mechanism 2
- Claim: An explicit Evidence Tree with post-order traversal and And2Or rollback produces reliable sub-tables while preserving answer-critical data.
- Mechanism: Evidence Tree is a binary tree where leaf nodes = evidences (each produces a filtered sub-table) and internal nodes = logical operators (AND/OR). Execution via post-order traversal merges sub-tables stepwise. If an AND node yields an empty table, And2Or sequentially replaces AND with OR on left child, right child, then parent until non-empty. A table verifier (LLM) checks completeness; on failure, rollback to previous node's sub-table (max 2 attempts, else use full table).
- Core assumption: Empty AND results stem from over-constrained intersection rather than true absence of answer; relaxing to OR preserves recall at precision cost, and verifier catches insufficiency.

### Mechanism 3
- Claim: Two-stage retrieval (keyword LSH + embedding re-ranking) selects representative rows that enable accurate evidence generation without processing the full table.
- Mechanism: Extract keywords from Q via lightweight LLM; use Locality-Sensitive Hashing for coarse row filtering; re-rank by weighted combination of semantic similarity (embedding cosine) and lexical similarity (edit distance); select top-k rows as context for evidence/tree generation.
- Core assumption: k representative rows preserve schema/semantic signals needed for evidence extraction; full-table context is unnecessary.

## Foundational Learning

- Concept: Self-consistency in multi-LLM outputs
  - Why needed here: The consistency assessment assumes that relevant evidences appear repeatedly across n rounds of generation; understanding self-consistency helps diagnose when this assumption holds or breaks.
  - Quick check question: Given 5 LLM generations, how do you compute a per-evidence consistency score, and what threshold determines retention?

- Concept: Post-order tree traversal for dependency resolution
  - Why needed here: The Evidence Tree executes leaf operations before internal merges; post-order ensures child sub-tables exist before logical combination.
  - Quick check question: For tree ((E1 OR E2) AND E3), in what order are nodes visited during post-order traversal, and when does rollback trigger?

- Concept: Recall vs. precision tradeoff in table pruning
  - Why needed here: And2Or sacrifices precision (includes more rows) to preserve recall; understanding this tradeoff clarifies why rollback + verification is critical.
  - Quick check question: If a downstream task tolerates noise but cannot recover missing data, should pruning optimize for recall or precision, and how does And2Or align?

## Architecture Onboarding

- Component map:
  - Question Q + Table T → Two-stage Retrieval (produces R) → EQD (produces Er) → ETD (builds tree, executes with rollback, produces T_final) → Answer Generation

- Critical path: Question Q + Table T → Two-stage Retrieval (produces R) → EQD (produces Er) → ETD (builds tree, executes with rollback, produces T_final) → Answer Generation. Each step depends on prior outputs; errors propagate forward.

- Design tradeoffs:
  - Retrieval: k = 10 rows balances efficiency vs. coverage; may miss rare answer rows in ultra-large tables.
  - Consistency: α = 0.8, n = 5 rounds; higher α or n increases precision but raises cost and may over-filter ambiguous-but-relevant evidences.
  - Rollback: At most 2 verifier attempts; trades cost vs. reliability; fallback to full table prevents empty-result crashes but may overwhelm LLM context.
  - And2Or: Preserves recall at precision cost; downstream reasoning must tolerate noisy rows.

- Failure signatures:
  - Empty sub-table after all rollback attempts → verifier fails twice → full table used; indicates over-aggressive filtering or retrieval failure.
  - High evidence count but low accuracy → multi-round consistency may be preserving spurious correlations; consider lowering α or improving usability checks.
  - Verifier always returns True → weak prompt/model; incomplete sub-tables passed; strengthen verification.

- First 3 experiments:
  1. Reproduce STQA-L/N results with fixed GPT-4o-mini; log per-component error rates (retrieval misses, consistency filter false positives/negatives, And2Or invocations) to identify bottlenecks.
  2. Ablate retrieval: compare k = 5, 10, 20, and full-table baselines on WikiTQ; measure accuracy vs. token cost to calibrate retrieval sufficiency.
  3. Stress-test rollback: inject tables where answer rows require 3+ AND conditions; measure And2Or frequency and final accuracy; compare against pure-SQL pruning (Text-to-SQL baseline) on same subset.

## Open Questions the Paper Calls Out

- **Question:** How does EnoTab's performance scale when applied to multi-table TableQA scenarios requiring join operations or cross-table reasoning?
  - **Basis in paper:** [explicit] The authors state in the Limitations section that "EnoTab is currently evaluated in the context of single-table question answering... its performance in multi-table settings remains unclear."
  - **Why unresolved:** The current framework, particularly the Evidence Tree construction and single-table pruning mechanisms, is designed and tested exclusively on single-table inputs.
  - **What evidence would resolve it:** Evaluation results on cross-table benchmarks (e.g., HybridQA) showing how EnoTab handles evidence linking across multiple tables compared to single-table baselines.

- **Question:** How can the evidence grounding mechanism be adapted to reliably handle specialized compound cell formats (e.g., "1-1" representing win-loss records) without discarding critical data?
  - **Basis in paper:** [explicit] The Limitations section notes that the "validation method struggles with certain specialized table formats, such as '1-1', where the numbers represent the count of wins and losses."
  - **Why unresolved:** The current "Action" definitions (string matching, numerical comparison, date evaluation) cannot parse multiple values embedded within a single cell, leading to validation failures or data loss during pruning.
  - **What evidence would resolve it:** An extension of the "Action" toolkit that successfully extracts and reasons over compound cell values without performance degradation on complex datasets.

- **Question:** To what extent does the reliance on $k$ representative rows during evidence generation limit the discovery of rare or outlier evidences necessary for answering specific questions?
  - **Basis in paper:** [inferred] From Sections 2.3 and A.1, evidence generation relies on sampling $k$ representative rows (top-10) via LSH and embedding similarity to provide context to the LLM.
  - **Why unresolved:** If the semantic units required to answer a question (e.g., a specific outlier value or rare entity) do not appear in the top-$k$ retrieved representative rows, the LLM may fail to generate the corresponding evidence, causing reasoning failure.
  - **What evidence would resolve it:** An ablation study comparing evidence generation quality when using full-table context versus the $k$-row sample, specifically analyzing failure rates on questions involving outlier values.

## Limitations
- The framework is currently evaluated only on single-table question answering, with performance on multi-table scenarios remaining unclear
- The validation method struggles with specialized table formats containing compound cell values like "1-1" for win-loss records
- Core LLM prompts and toolkit implementations are not fully specified, limiting exact reproducibility

## Confidence
- High: Core mechanism of evidence decomposition + consistency filtering is novel and internally coherent; performance gains on large-scale tables are statistically significant.
- Medium: Effectiveness of And2Or rollback is plausible but relies on unverifiable prompt details and limited verification attempts.
- Low: External reproducibility is constrained by incomplete prompt specifications and opaque toolkit P implementations.

## Next Checks
1. **Prompt fidelity test**: Reconstruct prompts from examples and evaluate on WikiTQ; compare output triplet quality and tree structure against reported performance.
2. **Component ablation study**: Isolate EQD, ETD, and retrieval modules on STQA-N; measure contribution of each to accuracy and token compression.
3. **Robustness stress test**: Apply EnoTab to tables with 5+ AND conditions; log And2Or frequency, rollback depth, and final accuracy; compare against SQL-based pruning baselines.