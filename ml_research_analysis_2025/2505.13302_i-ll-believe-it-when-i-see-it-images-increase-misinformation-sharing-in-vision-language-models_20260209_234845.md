---
ver: rpa2
title: 'I''ll believe it when I see it: Images increase misinformation sharing in
  Vision-Language Models'
arxiv_id: '2505.13302'
source_url: https://arxiv.org/abs/2505.13302
tags:
- news
- image
- user
- they
- 'false'
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how image presence influences misinformation
  sharing in vision-language models (VLMs), a gap in understanding given humans' susceptibility
  to visual content. The authors introduce a jailbreaking-inspired prompting strategy
  and a new multimodal dataset from PolitiFact to elicit resharing decisions from
  VLMs across personality and demographic profiles.
---

# I'll believe it when I see it: Images increase misinformation sharing in Vision-Language Models

## Quick Facts
- **arXiv ID**: 2505.13302
- **Source URL**: https://arxiv.org/abs/2505.13302
- **Reference count**: 40
- **Key outcome**: Image presence increases VLM resharing of false news by 15.0% and true news by 4.8%, with Claude-3-Haiku showing robustness

## Executive Summary
This paper investigates how image presence influences misinformation sharing in vision-language models (VLMs), a gap in understanding given humans' susceptibility to visual content. The authors introduce a jailbreaking-inspired prompting strategy and a new multimodal dataset from PolitiFact to elicit resharing decisions from VLMs across personality and demographic profiles. Experiments reveal that image presence increases resharing rates by 4.8% for true news and 15.0% for false news. Notably, Claude-3-Haiku is the only model showing robustness to visual misinformation, while Dark Triad traits amplify resharing of false news and Republican-aligned profiles weaken veracity sensitivity. These findings underscore the need for multimodal evaluation frameworks and mitigation strategies in personalized AI systems.

## Method Summary
The authors created a multimodal dataset of 200 news items from PolitiFact with binary true/false labels. They employed a third-person persona-conditioning approach to bypass safety alignment refusals, testing four VLMs (GPT-4o-mini, Claude-3-Haiku, LLaVa-1.6-Mistral-7B, Qwen2-VL-7B-Instruct) across personality (Big Five, Dark Triad) and demographic profiles (age, race, sex, political affiliation). Each model generated 10 completions per item at temperature 0.9, with responses aggregated from Likert scales to binary resharing decisions. Wilcoxon signed-rank tests and mixed-effects models examined the interaction between image presence, veracity, and persona conditioning.

## Key Results
- Image presence increases resharing rates by 4.8% for true news and 15.0% for false news (effect size r = 0.249)
- Claude-3-Haiku shows minimal visual amplification (r = 0.066) and no false news increase
- Dark Triad traits amplify false news resharing; Republican profiles show reduced veracity sensitivity
- LLaVa-1.6 and Qwen2-VL show strongest modality × veracity interactions (β = 0.028 and 0.065, p < .001)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Image presence increases VLM resharing likelihood, with stronger effects for false content.
- Mechanism: Visual inputs appear to amplify sharing behavior, potentially by increasing perceived credibility or reducing critical evaluation—though the paper notes the source (training data vs. alignment) remains unclear.
- Core assumption: VLMs may inherit or develop human-like cognitive biases toward visual content.
- Evidence anchors:
  - [abstract] "image presence increases resharing rates by 4.8% for true news and 15.0% for false news"
  - [Section 4.1] Effect size r = 0.249 aggregated across all VLMs; Wilcoxon tests significant at p < .001
  - [corpus] Weak direct corroboration; neighbor papers focus on detection rather than sharing behavior
- Break condition: Models with stronger reasoning capabilities (e.g., Claude-3-Haiku) show minimal or no amplification effect (r = 0.066, false news actually decreases by 1.17%).

### Mechanism 2
- Claim: Persona conditioning modulates veracity sensitivity, with Dark Triad traits increasing false news sharing and Republican-aligned profiles flattening true/false distinctions.
- Mechanism: Third-person prompting exposes latent persona-dependent behaviors that safety alignment normally suppresses; personas introduce partisan or psychological priors that override baseline veracity discrimination.
- Core assumption: Persona profiles activate training-corpus associations between traits and information-sharing patterns.
- Evidence anchors:
  - [abstract] "Dark Triad traits amplify resharing of false news, whereas Republican-aligned profiles exhibit reduced veracity sensitivity"
  - [Section 4.3] Republican profiles show comparable or higher resharing of false vs. true news; Democratic profiles retain veracity sensitivity
  - [corpus] Not directly tested in neighbor papers
- Break condition: The paper explicitly notes models often refuse to engage under second-person prompting with antisocial personas; third-person framing is required to elicit consistent persona behavior.

### Mechanism 3
- Claim: Model architecture and capability level predict susceptibility to visual misinformation amplification.
- Mechanism: Weaker reasoning models (LLaVa-1.6, Qwen2-VL) show stronger interaction between image presence and veracity; they also exhibit lower instruction-following fidelity (multiple Likert ratings, shallow reasoning).
- Core assumption: Reasoning capability mediates the ability to evaluate visual content critically rather than reflexively.
- Evidence anchors:
  - [Section 4.2] GPT-4o-mini and Qwen2-VL show r = 0.416 and r = 0.419; Claude-3-Haiku shows r = 0.066
  - [Table 1] Significant modality × veracity interaction for LLaVa-1.6 (β = 0.028, p < .001) and Qwen2-VL (β = 0.065, p < .001), but not for GPT-4o-mini or Claude-3-Haiku
  - [corpus] Related work (E2LVLM) suggests evidence-enhanced approaches improve OOC detection, but does not directly address sharing behavior
- Break condition: Claude-3-Haiku is robust to visual amplification but highly sensitive to persona conditioning (r = 0.240 for political affiliation), indicating susceptibility varies by bias type.

## Foundational Learning

- Concept: **Truthiness effect (human psychology)**
  - Why needed here: The paper explicitly grounds its hypothesis in human cognitive research showing images boost perceived credibility; understanding this provides interpretive framing for why VLMs might exhibit analogous behavior.
  - Quick check question: Can you explain why an image might increase perceived truthfulness even when it provides no evidentiary support for the claim?

- Concept: **Persona conditioning via prompting**
  - Why needed here: The study relies on third-person framing to elicit consistent persona-aligned responses; this technique is non-obvious and differs from standard role-prompting approaches.
  - Quick check question: Why might a model refuse to engage with a conspiratorial news item under second-person prompting but comply under third-person framing?

- Concept: **Effect size interpretation (Cohen's r)**
  - Why needed here: The paper reports r values (0.066 to 0.419) rather than raw percentages; understanding what constitutes small/medium/large effects is necessary to evaluate practical significance.
  - Quick check question: An effect size of r = 0.249 is described as "small-to-medium"—what does this imply about the reliability and practical importance of the finding?

## Architecture Onboarding

- Component map: News item (headline + excerpt + source + optional image) -> Persona injection -> Prompt scaffold -> Model families (GPT-4o-mini, Claude-3-Haiku, LLaVa-1.6, Qwen2-VL) -> Output parsing

- Critical path:
  1. Load PolitiFact-derived multimodal dataset (200 items, balanced true/false)
  2. Construct persona-conditioned prompts using third-person template
  3. Run 10 completions per item at τ = 0.9 for variability
  4. Parse Likert ratings, aggregate to binary yes/no
  5. Compare text-only vs. image+text conditions via Wilcoxon signed-rank tests
  6. Fit mixed-effects model for modality × veracity interaction

- Design tradeoffs:
  - Third-person vs. second-person prompting: Third-person elicits persona-consistent responses but bypasses safety alignment; this raises concerns about prompt-sensitivity and behavioral controllability
  - High temperature (τ = 0.9): Promotes response diversity but reduces per-item consistency (Fleiss' κ ranges from ~0.25 to 0.87 across models)
  - Binary aggregation from Likert: Simplifies analysis but discards neutral-response nuance

- Failure signatures:
  - LLaVa-1.6: Returns multiple Likert values per response despite single-value instruction; flat response distribution suggests indecision
  - Qwen2-VL: Strongly affirmative bias (frequent "strongly agree"); shallow reasoning disconnected from content
  - Refusals under second-person: Models reject conspiratorial or controversial content when prompted directly, breaking persona alignment

- First 3 experiments:
  1. Baseline replication: Run the text-only vs. image+text comparison on a held-out subset of PolitiFact items to verify effect sizes before extending to new domains.
  2. Persona ablation: Test whether the third-person framing effect holds for neutral personas (no Dark Triad, no political affiliation) to isolate prompt-structure from persona-content effects.
  3. Blank-image control: Replicate the pseudo text-only condition (text + blank image) across all four models to confirm multimodal pathway activation alone does not drive the effect (GPT-4o-mini showed no difference; verify for others).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does specific image content (e.g., depicting an accuser vs. the accused) modulate resharing decisions compared to simple image presence?
- Basis in paper: [explicit] The authors state in Appendix A.1 that future work requires "a more fine-grained analysis of visual influence" that moves "beyond image presence/absence to examine how different types of images modulate resharing behavior."
- Why unresolved: The current study only evaluates the binary presence or absence of an image, without controlling for the specific narrative content or framing within the visual itself.
- What evidence would resolve it: Experiments using carefully controlled, image-variant datasets where the news text remains constant while the visual context shifts.

### Open Question 2
- Question: Does the visual amplification of misinformation persist for news with intermediate veracity levels (e.g., "mostly true" or "half true")?
- Basis in paper: [explicit] Section A.1 notes the dataset limitation of using only binary labels, stating it "does not capture... ambiguous and nuanced forms of partially false or misleading content common in real-world settings."
- Why unresolved: The current methodology relies exclusively on items labeled "true" or "pants on fire," leaving the behavior for nuanced or misleading content unknown.
- What evidence would resolve it: Extending the dataset to include items rated across the full PolitiFact "Truth-O-Meter" scale and analyzing resharing rates.

### Open Question 3
- Question: To what extent are persona-conditioned behaviors and safety refusals artifacts of prompt phrasing (second vs. third person) rather than robust model traits?
- Basis in paper: [explicit] Appendix A.1 highlights the need for "systematic investigation" into the "broader sensitivity of VLMs to prompt phrasing," specifically comparing second- and third-person formulations.
- Why unresolved: The study utilized a third-person "jailbreaking-inspired" strategy to bypass refusals, but the stability of this behavior across different prompt structures was not tested.
- What evidence would resolve it: A comparative analysis of model alignment and consistency across varied grammatical persons and persona induction strategies.

## Limitations
- Dataset scope limited to 200 binary-labeled PolitiFact items, missing nuanced veracity levels
- High-temperature sampling (τ = 0.9) introduces stochastic variability that may mask systematic biases
- Third-person persona prompting technique's generalizability to real-world usage remains uncertain

## Confidence
- High confidence: Aggregate finding that images increase resharing rates (4.8% true, 15.0% false) with consistent statistical significance across models
- Medium confidence: Persona-specific findings, particularly Republican profile's reduced veracity sensitivity, due to dependency on third-person prompting technique
- Low confidence: Causal mechanism attribution (training data vs. alignment) due to inability to distinguish sources of visual amplification

## Next Checks
1. Replicate findings using first-party PolitiFact data to confirm dataset artifacts don't drive results
2. Test models at multiple temperature settings to isolate stochastic effects from systematic biases
3. Compare third-person prompting outcomes with direct second-person instructions across political personas to validate the prompting mechanism's necessity