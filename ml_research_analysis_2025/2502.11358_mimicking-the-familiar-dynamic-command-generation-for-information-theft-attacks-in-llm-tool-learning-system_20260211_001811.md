---
ver: rpa2
title: 'Mimicking the Familiar: Dynamic Command Generation for Information Theft Attacks
  in LLM Tool-Learning System'
arxiv_id: '2502.11358'
source_url: https://arxiv.org/abs/2502.11358
tags:
- attack
- information
- book
- tool
- command
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents AUTO CMD, a dynamic command generation approach
  for information theft attacks in LLM tool-learning systems. The core method leverages
  learning from open-source systems and reinforcement learning with target system
  examples to infer information utilized by upstream tools and generate more targeted
  commands for information theft.
---

# Mimicking the Familiar: Dynamic Command Generation for Information Theft Attacks in LLM Tool-Learning System

## Quick Facts
- **arXiv ID**: 2502.11358
- **Source URL**: https://arxiv.org/abs/2502.11358
- **Reference count**: 18
- **Primary result**: AUTO CMD achieves +13.2% ASR_Theft over baselines and >80.9% ASR_Theft on three black-box systems

## Executive Summary
This paper presents AUTO CMD, a dynamic command generation approach for information theft attacks in LLM tool-learning systems. The method leverages learning from open-source systems and reinforcement learning with target system examples to infer upstream tool information flows and generate contextually-targeted theft commands. Experiments on three benchmarks with 1,260 samples show AUTO CMD significantly outperforms baselines, achieving over 80.9% ASR_Theft on three black-box systems (LangChain, KwaiAgents, QwenAgent). The authors also design four defense methods that effectively protect systems from these attacks.

## Method Summary
AUTO CMD attacks LLM tool-learning systems through command injection by exploiting information flows between tools. The method constructs an AttackDB from white-box toolchains, capturing victim/attacker tool pairs, attempted commands, results, and guidance about key information relationships. A T5-based Dynamic Command Generator retrieves similar cases from AttackDB and generates commands tailored to the current malicious tool's context. The system uses PPO reinforcement learning with a combined reward signal that balances theft success, stealth, and sentiment neutrality. The approach transfers from open-source benchmarks to black-box systems through online learning adaptation.

## Key Results
- AUTO CMD achieves +13.2% ASR_Theft over baseline methods on three benchmarks (ToolBench, ToolEyes, AutoGen)
- The method achieves over 80.9% ASR_Theft on three black-box systems (LangChain, KwaiAgents, QwenAgent)
- Four defense methods are designed that effectively protect systems from these attacks
- Command injection through tool output achieves higher success rates than prompt injection or RAG poisoning

## Why This Works (Mechanism)

### Mechanism 1: Tool Invocation Pattern Inference
Learning from open-source systems enables inference of upstream tool information flows, allowing generation of contextually-targeted theft commands. AUTO CMD constructs an AttackDB by extracting attack cases from white-box toolchains, capturing tool pairs and their information relationships. The Dynamic Command Generator retrieves similar cases and generates tailored commands based on the current malicious tool's context.

### Mechanism 2: RL Optimization with Combined Reward Signal
Reinforcement learning optimizes command generation by balancing theft success, stealth, and sentiment neutrality. Using PPO, the reward function combines theft reward (stolen information overlap), exposed reward (frontend inference preservation), and sentiment penalty (NLTK polarity scoring). The Reinforce loss bridges rewards to generation probabilities.

### Mechanism 3: Mimicking Familiar Social Engineering Pattern
Commands that reference legitimate-seeming purposes (e.g., "registration") and mimic familiar tool interactions achieve higher stealth and success. The guidance generator extracts key information relationships between tools and suggests command structures that appear functionally justified, embedding theft requests within plausible cross-tool coordination tasks.

## Foundational Learning

- **LLM Tool-Learning System Architecture**: Understanding ⟨Observation, Thought, Action⟩ inference cycles and toolchain formation [T1, T2, ..., Tn] is prerequisite to grasping where command injection occurs (in tool output O) and how it propagates through subsequent inference steps.
  - Quick check: Can you trace how a malicious command in Book_Flight's output affects the next ⟨Observation, Thought, Action⟩ cycle?

- **Reinforcement Learning with Policy Gradients (PPO)**: AUTO CMD uses PPO to optimize command generation; understanding policy networks, reward shaping, and the Reinforce loss L_gen = E[−η log P(C|G,T) · r] is essential for modifying the optimization loop.
  - Quick check: What happens to convergence if you remove the sentiment penalty |S_sent| from the reward? (Answer per Figure 4: faster initial convergence but lower final reward)

- **Command Injection in LLM Systems**: The attack exploits tool output values (O ⊕ C) to inject commands; distinguishing this from prompt injection or RAG poisoning clarifies the threat model.
  - Quick check: How does command injection through tool output differ from prompt injection in user queries?

## Architecture Onboarding

- **Component map**: ToolBench/ToolEyes/AutoGen -> AttackDB construction (GPT-4o generation + manual testing) -> T5 Dynamic Command Generator -> PPO optimization (reward = σ(theft) + σ(exposure) - |S_sent|) -> Command injection -> Black-box targets (LangChain, KwaiAgents, QwenAgent)

- **Critical path**: 1) Prepare AttackDB from white-box examples (GPT-4o generation → manual testing → guidance extraction) 2) Initialize T5 with AttackDB-derived training data 3) Online RL optimization: for each batch, generate command → inject → compute reward → update policy 4) Deploy optimized generator to black-box targets

- **Design tradeoffs**: AttackDB size vs. coverage (252 cases may not cover rare tools), sentiment penalty magnitude (higher slows convergence but improves stealth), online vs. offline optimization (online adapts to new systems but requires continuous inference access)

- **Failure signatures**: High IER (commands detected in frontend; caused by LLM misunderstanding or ineffective commands), Low TSR (information not exfiltrated; caused by rarely-used malicious tools lacking AttackDB coverage), both symptoms: check AttackDB tool pair coverage and target LLM comprehension capacity

- **First 3 experiments**: 1) Reproduce baseline comparison on ToolBench subset (100 samples): Compare FixedCMD vs. AUTO CMD on ASR_Theft to validate +13.2% improvement claim 2) Ablate sentiment penalty: Run AUTO CMD w/o S_sent variant to confirm convergence behavior from Figure 4 3) Test transfer to single black-box system: Train on ToolBench, test on LangChain with 20 registered malicious tools to validate transfer attack feasibility

## Open Questions the Paper Calls Out

### Open Question 1
How can AttackDB coverage be expanded to handle infrequently used tools that currently cause 95% of failed information theft attacks? Current approach relies on historical attack patterns; newly created tools lack co-occurrence data needed for key information extraction.

### Open Question 2
To what extent do stronger LLMs (e.g., GPT-4) increase vulnerability to dynamic command injection attacks compared to smaller models (e.g., Llama-70B)? Section 6.2 notes stronger LLMs may be easier to understand abnormal commands and perform risky operations, but this trade-off was not systematically evaluated.

### Open Question 3
Can defense methods be developed that maintain system usability while achieving higher than ~20% reduction in IER? Current defenses trade off stealthiness detection against false positives; optimal balance unclear.

### Open Question 4
How does AUTO CMD's transferability scale to tool-learning systems with fundamentally different architectures (e.g., retrieval-augmented vs. planning-based)? Only three black-box systems tested; generalization to architectural variants not explored despite claimed generalizability.

## Limitations

- **Data dependency risk**: AttackDB construction relies on GPT-4o to generate and evaluate attack cases, but 95% failure rate for rarely-used malicious tools suggests significant coverage gaps that could limit real-world applicability.
- **Evaluation granularity**: The ASR_Theft metric requires both IER=0 and TSR=1, but evaluation methodology for automatically measuring these sub-metrics in black-box systems isn't fully specified.
- **Transfer learning limits**: While AUTO CMD shows promise transferring from open-source to black-box systems, the paper doesn't explore whether transfer degrades over time or whether target systems develop specific defenses requiring retraining.

## Confidence

- **High confidence**: The core architecture (AttackDB + T5 generator + PPO optimization) is well-specified and the improvement over baselines (+13.2% ASR_Theft) is supported by experimental results across multiple benchmarks.
- **Medium confidence**: The social engineering mechanism ("mimicking the familiar") is intuitively sound but provides limited empirical evidence for why neutral-sentiment commands specifically achieve higher success rates.
- **Low confidence**: The exact implementation details for the GPT-4o guidance generation and the σ(ŷ, y) reward function are underspecified, making exact reproduction challenging.

## Next Checks

1. **Ablation study on AttackDB coverage**: Systematically measure how ASR_Theft varies as a function of AttackDB size and tool-pair diversity. Create controlled subsets of the AttackDB (50%, 75%, 100%) and evaluate performance degradation to quantify the coverage-robustness tradeoff.

2. **Cross-system transfer decay analysis**: Train AUTO CMD on ToolBench, then test on LangChain with increasing time intervals or system updates. Measure how quickly transfer performance degrades and whether periodic AttackDB updates can maintain effectiveness against evolving black-box systems.

3. **Sentiment penalty sensitivity testing**: Beyond the Figure 4 convergence analysis, test whether the sentiment penalty magnitude has optimal ranges for different target LLM sizes. Evaluate whether smaller models like Llama3-70B require different sentiment penalties than larger models like GPT-4o.