---
ver: rpa2
title: 'Language Drift in Multilingual Retrieval-Augmented Generation: Characterization
  and Decoding-Time Mitigation'
arxiv_id: '2511.09984'
source_url: https://arxiv.org/abs/2511.09984
tags:
- language
- target
- multilingual
- across
- drift
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the issue of language drift in multilingual
  Retrieval-Augmented Generation (RAG), where models generate responses in an unintended
  language despite receiving prompts in the target language. The authors show that
  cross-lingual retrieved context often causes language inconsistency, with English
  acting as a dominant attractor, especially under Chain-of-Thought reasoning.
---

# Language Drift in Multilingual Retrieval-Augmented Generation: Characterization and Decoding-Time Mitigation

## Quick Facts
- arXiv ID: 2511.09984
- Source URL: https://arxiv.org/abs/2511.09984
- Authors: Bo Li; Zhenghua Xu; Rui Xie
- Reference count: 23
- Primary result: Proposes Soft Constrained Decoding to mitigate language drift in multilingual RAG, improving both language consistency and content quality across four target languages.

## Executive Summary
This paper addresses the issue of language drift in multilingual Retrieval-Augmented Generation (RAG) systems, where models generate responses in unintended languages despite receiving prompts in the target language. The authors systematically characterize this phenomenon, demonstrating that cross-lingual retrieved contexts often cause language inconsistency, with English acting as a dominant attractor language, especially under Chain-of-Thought reasoning. To address this, they propose Soft Constrained Decoding (SCD), a lightweight decoding-time strategy that softly penalizes non-target-language tokens while preserving reasoning fluency. Experiments on three multilingual datasets and four target languages show consistent improvements in both language consistency and content quality metrics.

## Method Summary
The paper introduces Soft Constrained Decoding (SCD), a decoding-time mitigation strategy for language drift in multilingual RAG systems. SCD works by modifying the token selection process during generation: at each decoding step, it combines the model's original token probabilities with a penalty term for tokens that don't match the target language. The penalty is controlled by a weighting parameter λ, allowing users to balance between language consistency and reasoning fluency. The method is implemented at inference time without requiring additional training or model modifications, making it lightweight and practical. The authors evaluate SCD across multiple multilingual datasets (HotpotQA, MuSiQue, DuReader) and target languages (English, Chinese, Arabic, Russian), demonstrating consistent improvements in both language consistency metrics and content quality measures like ROUGE and BLEU scores.

## Key Results
- Language drift occurs frequently in multilingual RAG, with English acting as a dominant attractor language
- Cross-lingual retrieved contexts significantly contribute to language inconsistency
- SCD consistently improves language consistency (LC) while maintaining or improving content quality (ROUGE/BLEU)
- The method works across multiple target languages including English, Chinese, Arabic, and Russian
- No additional training or model changes are required for SCD implementation

## Why This Works (Mechanism)

The mechanism works because language drift in multilingual RAG systems is driven by the interaction between the model's language preferences and the language of retrieved contexts. When retrieved information is in a different language than the prompt, the model tends to generate responses in the language of the retrieved context, particularly when that language is English. SCD addresses this by introducing a soft constraint during decoding that discourages generation of tokens in non-target languages. By combining the model's original probability distribution with a language-matching penalty term, SCD guides the model toward generating tokens that match the target language while still allowing for fluent and coherent output. The weighting parameter λ provides flexibility to balance between strict language consistency and natural reasoning flow.

## Foundational Learning

Language Drift: The phenomenon where multilingual models generate text in unintended languages during inference, particularly problematic in RAG systems where retrieved contexts may be in different languages than the prompt.
Why needed: Understanding this phenomenon is crucial for building reliable multilingual RAG systems that can consistently produce outputs in the intended language.
Quick check: Verify language drift occurs by testing with prompts in different languages and analyzing the generated output language.

Chain-of-Thought Reasoning: A prompting technique where models are encouraged to show their reasoning process step-by-step before providing final answers.
Why needed: The paper shows that language drift is particularly pronounced under CoT reasoning, making it essential for understanding the full scope of the problem.
Quick check: Observe if language drift patterns change with and without CoT prompting.

Soft Constrained Decoding: A decoding strategy that combines original token probabilities with penalty terms for undesirable behaviors, controlled by a weighting parameter.
Why needed: Provides a lightweight, inference-time solution to language drift without requiring model retraining.
Quick check: Test different λ values to find the optimal balance between language consistency and generation quality.

Cross-lingual Retrieval: The process of retrieving relevant information from documents in languages different from the query language.
Why needed: Critical component of multilingual RAG systems that can introduce language drift when retrieved contexts are in different languages than the prompt.
Quick check: Measure the frequency and impact of cross-lingual retrievals on output language consistency.

Language Consistency Metrics: Evaluation metrics specifically designed to measure whether generated text matches the target language specified in the prompt.
Why needed: Standard content quality metrics don't capture language consistency, requiring specialized evaluation approaches.
Quick check: Implement language detection on generated outputs and compare against target languages.

## Architecture Onboarding

Component Map: Retrieval System -> Cross-lingual Context Fetching -> RAG Model -> Soft Constrained Decoding Module -> Generated Response

Critical Path: User Query → Multilingual Retriever → Retrieved Context (potentially cross-lingual) → RAG Model (with SCD) → Language-Consistent Response

Design Tradeoffs: SCD balances between strict language enforcement (which might reduce reasoning quality) and permissive generation (which allows drift). The λ parameter controls this tradeoff, with higher values enforcing stricter language consistency at potential cost to reasoning fluency.

Failure Signatures: Language drift manifests as outputs in unintended languages, particularly when retrieved contexts are cross-lingual. The severity increases with more complex reasoning tasks and when English is the dominant attractor language in the dataset.

First Experiments:
1. Test SCD with λ = 0.5 on a small multilingual dataset to observe basic language consistency improvements
2. Compare language drift patterns with and without Chain-of-Thought prompting to verify the enhanced drift phenomenon
3. Measure the impact of different λ values on both language consistency metrics and content quality metrics to find the optimal balance point

## Open Questions the Paper Calls Out

The paper does not explicitly call out specific open questions, but several implicit questions arise from the work. These include understanding the mechanisms behind language drift for non-English attractor languages, determining the optimal λ values for different language pairs and reasoning tasks, and exploring how SCD performs with longer chain-of-thought reasoning steps and more complex reasoning tasks.

## Limitations

- The study focuses primarily on English as the dominant attractor language, with less exploration of non-English attractor languages
- Experimental validation covers only four target languages, which may not represent the full diversity of multilingual scenarios
- The effectiveness of SCD may vary depending on specific language pair combinations and retrieved context characteristics
- The paper doesn't extensively explore the impact of different retriever qualities on language drift severity

## Confidence

High confidence in the characterization of language drift phenomenon and its impact on multilingual RAG systems. High confidence in the effectiveness of SCD for the tested language pairs and datasets. Medium confidence in the generalizability of findings to all language combinations and RAG configurations.

## Next Checks

1. Test SCD effectiveness across a broader range of language pairs, including low-resource languages and language pairs where neither language is English, to verify if the method generalizes beyond the current scope.

2. Conduct ablation studies to determine the optimal weighting parameter λ for different language pairs and to understand the trade-offs between language consistency and reasoning fluency in more detail.

3. Evaluate the method's performance with longer chain-of-thought reasoning steps and more complex reasoning tasks to assess scalability and robustness under more demanding conditions.