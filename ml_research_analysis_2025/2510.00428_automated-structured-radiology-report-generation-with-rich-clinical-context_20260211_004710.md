---
ver: rpa2
title: Automated Structured Radiology Report Generation with Rich Clinical Context
arxiv_id: '2510.00428'
source_url: https://arxiv.org/abs/2510.00428
tags:
- clinical
- context
- report
- generation
- prior
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of automated structured radiology
  report generation (SRRG) from chest X-ray images, which has significant potential
  to reduce radiologists' workload. However, existing SRRG systems overlook essential
  clinical contexts, leading to critical problems including temporal hallucinations
  when referencing non-existent clinical contexts.
---

# Automated Structured Radiology Report Generation with Rich Clinical Context

## Quick Facts
- arXiv ID: 2510.00428
- Source URL: https://arxiv.org/abs/2510.00428
- Reference count: 40
- The paper proposes C-SRRG, a framework that incorporates rich clinical context to reduce temporal hallucinations and improve structured radiology report generation from chest X-rays.

## Executive Summary
This paper addresses the challenge of automated structured radiology report generation (SRRG) from chest X-ray images by proposing a contextualized approach that integrates rich clinical context. The authors identify that existing SRRG systems suffer from temporal hallucinations due to the lack of proper clinical context grounding. Their C-SRRG framework incorporates multi-view images, clinical indication, imaging technique, and prior studies with comparisons based on patient histories. Through extensive benchmarking with state-of-the-art multimodal large language models, they demonstrate significant improvements in report generation quality, achieving 80.1% F1-SRR-BERT for findings and 58.0% for impression while substantially reducing temporal hallucinations.

## Method Summary
The C-SRRG framework trains medical MLLMs using next-token prediction on concatenated prompt-response pairs where prompts contain both images and comprehensive clinical context. The authors curate a large-scale dataset by integrating MIMIC-CXR and CheXpert Plus data with multi-view X-ray images, clinical indications, imaging techniques, and prior studies with corresponding comparisons based on patient histories. Models are fine-tuned using LoRA (rank=32, α=64) with specific training parameters (batch size=128, lr=2e-4, 1 epoch, cosine scheduler with 3% warmup). Inference is performed using vLLM with greedy decoding. The framework evaluates both traditional metrics (BLEU, ROUGE-L, BERTScore) and clinical metrics (F1-RadGraph, F1-SRR-BERT, Category Score) while specifically measuring temporal hallucination rates.

## Key Results
- C-SRRG achieves 80.1% F1-SRR-BERT for findings and 58.0% for impression
- Temporal hallucination rates drop from 22.9% to 10.7% (findings) and 43.8% to 25.8% (impression) with clinical context
- All clinical context components contribute incrementally to report quality (except for few cases)
- Larger models (7B parameters) derive greater benefit from clinical context than smaller models (3B)

## Why This Works (Mechanism)

### Mechanism 1: Clinical Context Grounding Reduces Hallucination
Incorporating rich clinical context reduces temporal hallucinations by providing grounded reference material for temporal comparisons. The model learns to condition generation on actual prior study availability, using prompt structure to signal when no comparison should be made. This breaks the spurious correlation between radiology language patterns and temporal statements.

### Mechanism 2: Context Integration Provides Task-Relevant Priors
Each clinical context component contributes incrementally by providing task-relevant priors that constrain the generation space. Multi-view images offer complementary anatomical coverage, indication focuses attention on relevant diagnostic questions, technique informs about imaging limitations, and prior studies enable legitimate temporal reasoning.

### Mechanism 3: Scaling Amplifies Context Benefits
Larger models (7B parameters) derive greater benefit from clinical context than smaller models (3B), suggesting context integration capability scales with model capacity. The richer internal representations in larger models can better encode the conditional logic of when/how to incorporate prior study information.

## Foundational Learning

- **Autoregressive Language Modeling with Multimodal Inputs**: The C-SRRG framework trains via next-token prediction on concatenated prompt-response pairs where prompts contain images + text contexts. Understanding how vision tokens interleave with text tokens is essential. *Quick check*: Can you explain how the loss function handles multimodal input $x$ differently from text-only input?

- **Parameter-Efficient Fine-Tuning (LoRA)**: All experiments use LoRA (rank=32, α=64) rather than full fine-tuning. Understanding low-rank adaptation is necessary for reproducibility and resource planning. *Quick check*: Why might LoRA be preferred over full fine-tuning for medical MLLMs, and what are the tradeoffs?

- **Structured vs. Free-Form Report Generation Evaluation**: The SRRG paradigm uses organ-section headers and structured formats. Metrics like F1-SRR-BERT and Category Score specifically evaluate structured output quality. *Quick check*: How does the Category Score metric differ from traditional BLEU/ROUGE metrics in what it rewards?

## Architecture Onboarding

- **Component map**: Data Curation Pipeline -> Prompt Constructor -> Medical MLLM Backbone -> Inference Engine -> Evaluation Suite
- **Critical path**: 1. Patient-level data splitting (prevent leakage) 2. Longitudinal history extraction (group by subject_id, order by StudyDate/Time) 3. Prompt construction with context availability handling 4. LoRA fine-tuning (1 epoch, LR=2e-4, batch size=128, cosine scheduler) 5. Inference with vLLM (greedy decoding) 6. Temporal hallucination evaluation (33-indicator keyword detection)
- **Design tradeoffs**: Recency-based prior selection may omit important historical context; greedy decoding ensures reproducibility but may sacrifice quality; LoRA limits capacity to learn complex context integration patterns; synthetic annotations may introduce biases
- **Failure signatures**: CheXagent-3B generates single-word outputs instead of structured reports; temporal hallucinations occur without context; BLEU degradation with context for smaller models; image limit exceeded errors
- **First 3 experiments**: 1. Baseline reproduction: Train MedGemma-4B on single-view images without clinical context; verify F1-SRR-BERT ~40-42% on test set 2. Context ablation: Incrementally add multi-view → indication → technique → prior studies; confirm each component adds ~0.5-3% F1-SRR-BERT 3. Hallucination quantification: Train baseline model without context, evaluate on test set, count temporal indicators; verify ~22-28% hallucination rate

## Open Questions the Paper Calls Out

- **Learned Clinical Context Selection**: How would learned clinical context selection policies compare to the current recency-based selection strategy for identifying the most informative prior studies? The current strategy may occasionally omit important historical context that could inform diagnostic reasoning.

- **Retrieval-Augmented Generation**: Can RAG over PACS and EHR systems improve C-SRRG performance by dynamically surfacing relevant historical information? The authors propose that retrieval-augmented generation approaches could dynamically surface the most relevant historical information for each case.

- **Preference Learning Techniques**: Would incorporating preference learning techniques (PPO, DPO) with radiologist feedback enhance clinical appropriateness beyond supervised fine-tuning? The supervised learning paradigm limits the model's ability to learn from comparative feedback.

## Limitations
- Relies on synthetically annotated reports generated by GPT-4, which may introduce subtle biases or hallucinations
- Context window constraints limit the number of prior studies that can be included, potentially omitting relevant clinical history
- Greedy decoding strategy ensures reproducibility but may sacrifice generation quality and diversity

## Confidence

- **High Confidence**: The incremental improvements from adding clinical context components are well-documented and statistically significant across multiple metrics
- **Medium Confidence**: The scaling benefits observed with larger models suggest context integration capability scales with model capacity
- **Medium Confidence**: The assumption that synthetic GPT-4 annotations faithfully represent radiologist report structure without introducing systematic biases

## Next Checks
1. **Hallucination Sensitivity Analysis**: Evaluate the C-SRRG model on a held-out test set with manually verified ground truth reports (not synthetic) to confirm the temporal hallucination reduction is not an artifact of the synthetic annotation process.

2. **Context Window Stress Test**: Systematically vary the number of prior studies included (0, 1, 2, 3+) and measure the performance degradation to quantify the impact of context truncation and identify the optimal prior study limit for different model sizes.

3. **Decoding Strategy Comparison**: Compare greedy decoding results against beam search (width=5) and nucleus sampling (p=0.9) to determine if the reported F1-SRR-BERT scores represent a lower bound and to assess the tradeoff between reproducibility and generation quality.