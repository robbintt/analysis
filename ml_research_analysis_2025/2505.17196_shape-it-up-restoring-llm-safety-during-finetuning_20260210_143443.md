---
ver: rpa2
title: Shape it Up! Restoring LLM Safety during Finetuning
arxiv_id: '2505.17196'
source_url: https://arxiv.org/abs/2505.17196
tags:
- safety
- arxiv
- harmful
- safe
- finetuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces dynamic safety shaping (DSS), a framework
  that uses fine-grained token-level safety signals to mitigate LLM safety risks during
  finetuning. It leverages a guardrail model to compute a Safety Trajectory Assessment
  of Response (STAR) score, which tracks evolving safety risk across partial responses.
---

# Shape it Up! Restoring LLM Safety during Finetuning

## Quick Facts
- arXiv ID: 2505.17196
- Source URL: https://arxiv.org/abs/2505.17196
- Reference count: 40
- Primary result: STAR-DSS improves safety by up to 20.41% over strong baselines during finetuning

## Executive Summary
This paper introduces dynamic safety shaping (DSS), a framework that uses fine-grained token-level safety signals to mitigate LLM safety risks during finetuning. It leverages a guardrail model to compute a Safety Trajectory Assessment of Response (STAR) score, which tracks evolving safety risk across partial responses. The STAR-DSS method dynamically adjusts training weights based on these scores, suppressing harmful content while preserving task capability. Empirical results show STAR-DSS improves safety by up to 20.41% over strong baselines, generalizes across diverse models, guardrails, and attack scenarios, and is robust to context entanglement and poisoning attacks.

## Method Summary
STAR-DSS computes token-level safety scores by querying a guardrail model on partial responses every M tokens, extracting a continuous STAR score via sigmoid of logit differences. During training, the loss is dynamically weighted: high STAR scores prioritize cross-entropy from the finetuning data, while low STAR scores prioritize KL divergence to a reference aligned model. This creates a data-driven schedule that suppresses harmful learning while maintaining task capability. The method includes theoretical analysis bounding harmfulness as reference harm plus KL deviation and guardrail error terms.

## Key Results
- STAR-DSS improves safety by 20.41% over rejection sampling on PureBad datasets
- Method is robust to context entanglement and poisoning attacks
- Generalizes across different guardrail models (Granite Guardian, Llama Guard) and model families
- Smaller chunk sizes (M=1-5) improve safety on prefix attacks without significant capability degradation

## Why This Works (Mechanism)

### Mechanism 1
Guardrail models can be repurposed to evaluate partial responses, not just complete outputs. The STAR score is computed by forwarding partial sequences through a guardrail model and extracting logits for "safe" vs "unsafe" tokens. The sigmoid of the logit difference yields a continuous safety estimate, queried every M tokens. Core assumption: guardrails trained on full-response classification generalize meaningfully to partial completions.

### Mechanism 2
Dynamic loss weighting based on STAR scores suppresses unsafe learning while preserving task capability. For each chunk, the loss interpolates: V_safe × L_CE + (1 - V_safe) × λ_KL × L_KL. High STAR scores prioritize cross-entropy; low scores prioritize KL divergence to the reference model. Core assumption: KL regularization to the reference model prevents harmful behavior from being learned during unsafe segments.

### Mechanism 3
The harmfulness of a STAR-DSS trained model is bounded by the reference model plus interpretable error terms. Theoretical analysis decomposes harmfulness into: (1) reference model harmfulness, (2) KL deviation term √(2ε_KL), and (3) guardrail worst-case false negative rate δ_chunk(M, τ). Smaller chunk size M and better guardrails shrink the bound.

## Foundational Learning

- **Concept: Supervised Finetuning (SFT) Loss**
  - Why needed here: STAR-DSS modifies the standard SFT objective by adding weighted KL terms. Understanding baseline SFT is prerequisite.
  - Quick check question: Can you explain why standard SFT on harmful data degrades safety alignment?

- **Concept: KL Divergence Between Language Models**
  - Why needed here: The method uses KL divergence to the reference model as a regularization mechanism for unsafe segments.
  - Quick check question: What does it mean for two autoregressive policies to have low sequence-level KL divergence?

- **Concept: Guardrail Models and False Negative Rates**
  - Why needed here: STAR scores depend on guardrail reliability; the theoretical bound explicitly includes guardrail error.
  - Quick check question: Why does a high guardrail false negative rate undermine both rejection sampling and STAR-DSS differently?

## Architecture Onboarding

- **Component map:** Data → STAR preprocessing (offline, cacheable) → Training with weighted loss → Finetuned model
- **Critical path:** Data → STAR preprocessing (offline, cacheable) → Training with weighted loss → Finetuned model. The preprocessing step can be pipelined with training.
- **Design tradeoffs:**
  - Chunk size M: Smaller M = finer granularity but more guardrail queries (~4× time increase from M=15 to M=1)
  - λ_KL scaling: Higher values = stronger safety but risk over-regularization; performance peaks around 0.5
  - Guardrail choice: Lower FN rates improve safety; smaller guardrails (e.g., 2B) can outperform larger ones on specific benchmarks
- **Failure signatures:**
  - Safety scores drop when misleading suffixes fool the guardrail into high STAR scores throughout (context entanglement)
  - Capability degrades when λ_KL is set too high (over-regularization prevents task learning)
  - Deep Token fails on prefix attacks because fixed KL schedules don't adapt to late-occurring unsafe content
- **First 3 experiments:**
  1. Reproduce RS baseline: Finetune on PureBad with rejection sampling using Granite Guardian-3.1-2B. Measure safety on AdvBench and capability on MMLU.
  2. Ablate chunk size M: Run STAR-DSS with M ∈ {1, 5, 10, 15} on the response adaptation attack (misleading suffix). Verify safety improves with smaller M.
  3. Test cross-guardrail generalization: Run STAR-DSS with different guardrails (Granite Guardian 2B/8B, Llama Guard 1B/8B) under worst-case setting. Verify robustness to guardrail choice.

## Open Questions the Paper Calls Out

**Open Question 1:** Can STAR-DSS provide effective safety robustness at inference time, beyond its demonstrated finetuning-time protection? The paper notes that harmful prefilling attacks suggest potential inference-time robustness, but systematic evaluation across diverse inference-time attacks is not conducted.

**Open Question 2:** How does STAR-DSS performance scale to frontier-scale models (70B+ parameters) and does the computational overhead remain tractable? Experiments are limited to 1B-8B parameter models, and the STAR computation requires repeated guardrail forward passes per training sample.

**Open Question 3:** Is STAR-DSS robust when the guardrail model itself is adversarially compromised or systematically misaligned with the target LLM's safety policy? The theoretical bound depends on guardrail false negative rate but assumes the guardrail is trustworthy.

**Open Question 4:** What is the theoretically optimal chunk size M that balances safety granularity, computational cost, and protection against adversarial suffix/prefix attacks? Theorem 1 shows the bound includes δ_chunk(M,τ) which shrinks with smaller M, but Table 18 shows mixed results suggesting the relationship is not monotonic.

## Limitations
- Guardrail generalization to partial responses is assumed but not empirically validated across all scenarios
- Theoretical safety bounds may be loose in practice and don't account for complex interaction effects
- Performance may not transfer equally well to models >8B parameters and different task domains

## Confidence
**High Confidence Claims**
- STAR-DSS improves safety by 20.41% over rejection sampling on PureBad datasets
- The method is robust to context entanglement and poisoning attacks
- STAR-DSS generalizes across different guardrail models and model families
- Theoretical analysis correctly decomposes harmfulness into interpretable components

**Medium Confidence Claims**
- STAR-DSS preserves task capability while improving safety
- Smaller chunk sizes (M=1-5) consistently improve safety on prefix attacks
- The method is robust to variations in λ_KL hyperparameter

**Low Confidence Claims**
- Guardrail generalization to partial responses is reliable across all scenarios
- The theoretical safety bound provides tight practical guarantees
- Performance will transfer equally well to models >8B parameters and different task domains

## Next Checks
**Check 1:** Validate guardrail performance on partial responses versus full responses by measuring false positive/negative rate drift as responses are truncated at different chunk sizes.

**Check 2:** Test STAR-DSS on capability benchmarks outside the MMLU/ARC-C domain (GSM8K, HumanEval) to validate generalizability and task preservation claims.

**Check 3:** Systematically compare STAR-DSS performance using different guardrail model sizes (2B vs 8B) on the same finetuning task to test robustness to guardrail choice and identify scaling tradeoffs.