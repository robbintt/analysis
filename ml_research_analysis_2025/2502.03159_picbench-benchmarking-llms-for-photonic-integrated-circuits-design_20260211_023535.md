---
ver: rpa2
title: 'PICBench: Benchmarking LLMs for Photonic Integrated Circuits Design'
arxiv_id: '2502.03159'
source_url: https://arxiv.org/abs/2502.03159
tags:
- design
- syntax
- code
- llms
- error
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PICBench introduces the first comprehensive benchmark for evaluating
  large language models (LLMs) in photonic integrated circuit (PIC) design generation.
  The framework comprises 24 carefully crafted design problems ranging from fundamental
  devices to complex circuit-level designs, all evaluated using an open-source simulator
  (SAX) that compares generated netlists against expert-crafted golden solutions.
---

# PICBench: Benchmarking LLMs for Photonic Integrated Circuits Design

## Quick Facts
- arXiv ID: 2502.03159
- Source URL: https://arxiv.org/abs/2502.03159
- Authors: Yuchao Wu; Xiaofei Yu; Hao Chen; Yang Luo; Yeyu Tong; Yuzhe Ma
- Reference count: 15
- Key outcome: First comprehensive benchmark for evaluating LLMs in PIC design generation using 24 problems and SAX simulator

## Executive Summary
PICBench introduces the first comprehensive benchmark for evaluating large language models in photonic integrated circuit design generation. The framework comprises 24 carefully crafted design problems ranging from fundamental devices to complex circuit-level designs, all evaluated using an open-source simulator (SAX) that compares generated netlists against expert-crafted golden solutions. The evaluation reveals significant performance improvements when incorporating error feedback mechanisms, with models like Claude 3.5 Sonnet showing syntax accuracy increases from 13.33% to 75.83% and functionality improvements from 1.67% to 24.17% across three feedback iterations.

## Method Summary
PICBench evaluates LLMs by generating JSON netlists from natural language descriptions, then validating them using SAX simulator for syntax correctness and frequency-domain comparison for functional correctness. The system uses a three-component prompt: JSON format schema, API documentation for built-in devices, and domain-specific restrictions. An iterative error feedback loop classifies SAX errors into categories and feeds them back to the LLM for regeneration, with up to 3 iterations tested. The benchmark includes 24 problems spanning optical computing, interconnects, switches, and fundamental devices, with golden solutions and pre-computed frequency responses for evaluation.

## Key Results
- Claude 3.5 Sonnet shows syntax accuracy improvements from 13.33% to 75.83% across three feedback iterations
- Domain-specific restrictions dramatically improve LLM output consistency (e.g., Gemini 1.5 Pro syntax from 9.17% to 64.17% with restrictions)
- All models show Pass@5 scores higher than Pass@1, demonstrating benefit of sampling multiple completions
- Syntax correctness reaches high levels (up to 100% for some models with 3 iterations) but functional correctness lags significantly (peaking at 62.5%)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iterative error feedback from simulators enables LLMs to self-correct and dramatically improves code generation accuracy.
- Mechanism: LLM generates netlist → SAX simulator executes → Error message extracted → Error classified into categories (Table II) → Classified error + feedback prompt returned to LLM → LLM revises code. This loop repeats up to 3 iterations.
- Core assumption: LLMs can correctly interpret structured error messages and map them to appropriate code fixes.
- Evidence anchors:
  - [abstract]: "models like Claude 3.5 Sonnet showing syntax accuracy increases from 13.33% to 75.83% and functionality improvements from 1.67% to 24.17% across three feedback iterations"
  - [section IV-B1/Table III]: Shows all models improve with feedback; Claude 3.5 Sonnet Pass@1 syntax: 13.33% → 35.83% (1 iter) → 75.83% (3 iter)
  - [corpus]: Related work on multi-agent PIC design (PhIDO) similarly uses iterative refinement, but this paper provides the first quantitative benchmark comparison.

### Mechanism 2
- Claim: Domain-specific restrictions embedded in system prompts reduce recurring error types by constraining the output space.
- Mechanism: Human experts analyze failure modes across initial runs → Summarize common errors into explicit rules (Table II: 9 failure types) → Rules injected into system prompt under "Restrictions" section → LLM generation constrained to valid patterns.
- Core assumption: Error patterns are systematic and learnable, and LLMs can reliably follow negative constraints.
- Evidence anchors:
  - [section III-D/Table II]: Documents 9 failure types with corresponding restrictions (e.g., "duplicate connections to the same port" → restriction: "Each port can only be connected once")
  - [section IV-B2/Table IV]: With restrictions, Gemini 1.5 Pro syntax improves from 9.17% to 64.17% (Pass@1, no feedback)
  - [corpus]: Weak/missing - Corpus papers don't directly address prompt restriction strategies for PIC design.

### Mechanism 3
- Claim: Frequency-domain simulation comparison provides a sufficient proxy for functional correctness in PIC design evaluation.
- Mechanism: Generate netlist → Run SAX simulation across 1510-1590nm wavelength range → Compare S-parameter frequency response against golden solution → Match indicates functional correctness.
- Core assumption: Frequency response uniquely determines functional equivalence for the tested designs.
- Evidence anchors:
  - [section III-C]: "we simply compare the simulation results between generated code completions and golden reference solutions"
  - [section IV-A]: "We constructed the S-parameters for essential devices... to simulate the frequency-domain response of the specified PIC"
  - [corpus]: MAPS paper discusses multi-fidelity simulation but doesn't validate frequency-domain comparison as a completeness criterion.

## Foundational Learning

- Concept: **Photonic Integrated Circuit (PIC) Netlists**
  - Why needed here: The entire benchmark evaluates LLM ability to generate valid JSON netlists specifying components, connections, and ports. Without understanding netlist structure, you cannot interpret errors or improve prompts.
  - Quick check question: Can you explain the difference between the "instances," "connections," and "ports" sections in a SAX netlist?

- Concept: **S-Parameters and Frequency-Domain Simulation**
  - Why needed here: The evaluation methodology compares frequency responses (S-parameters across 1510-1590nm). Understanding what S-parameters represent is essential to interpret "functional correctness" claims.
  - Quick check question: What does an S-parameter matrix describe, and why is frequency-domain analysis used for PICs rather than time-domain transient analysis?

- Concept: **LLM In-Context Learning and Instruction Following**
  - Why needed here: The paper relies on LLMs' ability to follow complex system prompts with API documentation and restrictions. Understanding the limits of in-context learning helps set realistic expectations.
  - Quick check question: If you provide an LLM with API documentation for 10 components and 5 usage restrictions, what factors determine whether it will correctly apply these constraints to a new problem?

## Architecture Onboarding

- Component map: Natural Language Problem Description -> System Prompt (Format + API Docs + Restrictions) -> LLM -> JSON Netlist -> SAX Simulator -> Syntax Check -> Frequency Response Comparison -> Functional Check -> Error Classifier -> Feedback Prompt -> LLM (Iterative Loop)

- Critical path: Construct system prompt with format schema, API documentation, and accumulated restrictions → LLM generates initial netlist → SAX attempts simulation → if syntax error, classify error and feed back → if syntax passes, compare frequency response to golden solution → if mismatch, feed back functional error prompt → repeat until pass or max iterations reached

- Design tradeoffs:
  - Automation vs. accuracy: Error classification loop requires human inspection initially; fully automating this risks misclassifying novel error types
  - Generality vs. specificity: Restrictions improve performance but may overfit to the 24 benchmark problems; generalization to new architectures is untested
  - Iteration depth vs. cost: 3 iterations improve results significantly but triple API costs and latency

- Failure signatures:
  - Persistent "Wrong Ports" errors: LLM hallucinates port names not in API documentation → add explicit port enumeration to restrictions
  - Multi-pin net connections: LLM applies VLSI netlist conventions (multiple ports per connection) → strengthen "single connection per port" restriction
  - Functional but non-matching frequency response: Netlist is syntactically valid but topology differs from golden solution → this may indicate legitimate alternative designs; current metric flags as failure

- First 3 experiments:
  1. Baseline assessment: Run all 24 problems with each target LLM, 0 feedback iterations, no domain restrictions → establish lower bound and identify most common error types
  2. Ablation on restrictions: Add restrictions incrementally (format-only → +API docs → +all restrictions) to measure contribution of each prompt component to syntax/functionality scores
  3. Feedback iteration scaling: Test 1, 3, 5, and 10 iterations on a subset of hardest problems (e.g., 8×8 optical switches) to determine diminishing returns point and cost/performance tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the PICBench evaluation framework be adapted to assess purely device-level designs, which are currently excluded due to the reliance on netlist connections?
- Basis in paper: [explicit] Section III-B states, "Notably, we do not include any purely device-level design problems in our collection, as these lack connections, and the components section in each netlist inherently addresses device-level designs."
- Why unresolved: The current methodology evaluates designs by simulating netlists (connections of components) against golden solutions; assessing single-component physics or geometry requires a different simulation and validation approach.
- What evidence would resolve it: An extension of the benchmark dataset that includes fundamental device components and a corresponding evaluation metric that does not depend on inter-component connections.

### Open Question 2
- Question: What specific prompt engineering or model architectures are required to close the performance gap between syntactic correctness and functional correctness?
- Basis in paper: [inferred] Tables III and IV show that while syntax scores can reach 100% (e.g., GPT-4 with 3 EFs), functional scores lag significantly (peaking at 62.5%), indicating that correct code structure does not guarantee correct circuit logic.
- Why unresolved: The paper demonstrates that error feedback improves syntax, but functional errors persist, suggesting the LLMs lack a deep understanding of photonic logic or frequency-domain behavior.
- What evidence would resolve it: A study showing a modified feedback loop or model fine-tuning that yields a functional success rate proportional to the syntax success rate (e.g., >80% functionality).

### Open Question 3
- Question: Can the error classification and restriction generation process be fully automated without human intervention while maintaining performance gains?
- Basis in paper: [inferred] Section III-D notes that "errors are iteratively inspected and summarized... by human inspection" to create the restrictions, suggesting the current pipeline is not fully autonomous.
- Why unresolved: Automating the summarization of diverse simulator error messages into generalizable, logical restrictions is a complex natural language understanding task.
- What evidence would resolve it: An ablation study where the "human inspection" step is replaced by an automated summarizer, resulting in comparable syntax and functionality scores.

## Limitations

- Evaluation methodology assumes frequency-domain S-parameter comparison is sufficient for functional correctness verification, though this equivalence is not rigorously validated
- Benchmark's generalizability to unseen architectures remains unknown since all 24 problems were used during prompt engineering and error classification development
- Human-in-the-loop error classification process creates a scalability bottleneck that limits practical deployment

## Confidence

- **High Confidence**: Syntax accuracy improvements from iterative feedback (measured directly by SAX simulation success/failure)
- **High Confidence**: Domain restrictions improve LLM output consistency (demonstrated across multiple model families)
- **Medium Confidence**: Frequency-domain comparison as functional correctness metric (assumes S-parameters uniquely determine circuit behavior)
- **Low Confidence**: Generalization to novel PIC architectures beyond the 24 benchmark problems
- **Low Confidence**: Scalability of human-classified error feedback for production use

## Next Checks

1. Construct two different netlists that produce identical S-parameter responses within measurement tolerance, and two functionally different netlists that produce similar frequency responses to validate whether frequency-domain comparison is a sufficient proxy for functional correctness.

2. Apply the trained system prompts and feedback mechanisms to generate netlists for PIC architectures not present in the original 24 problems (e.g., ring resonator-based filters, mode converters) to measure performance degradation and assess true generalization capability.

3. Implement an automated error classifier trained on the human-labeled examples from the paper, then evaluate its accuracy against the human baseline to address the scalability bottleneck and reveal whether the error taxonomy captures all failure modes.