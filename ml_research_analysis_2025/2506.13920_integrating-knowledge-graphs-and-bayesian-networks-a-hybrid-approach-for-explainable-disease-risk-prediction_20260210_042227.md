---
ver: rpa2
title: 'Integrating Knowledge Graphs and Bayesian Networks: A Hybrid Approach for
  Explainable Disease Risk Prediction'
arxiv_id: '2506.13920'
source_url: https://arxiv.org/abs/2506.13920
tags:
- risk
- data
- knowledge
- health
- domain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of explainable disease risk
  prediction using multimodal electronic health record (EHR) data. The proposed hybrid
  approach integrates knowledge graphs (KGs) built from ontologies with Bayesian networks
  (BNs) to balance generalized medical knowledge with patient-specific context.
---

# Integrating Knowledge Graphs and Bayesian Networks: A Hybrid Approach for Explainable Disease Risk Prediction

## Quick Facts
- arXiv ID: 2506.13920
- Source URL: https://arxiv.org/abs/2506.13920
- Reference count: 40
- Accuracy: 75.95% (hybrid BN on AF prediction)

## Executive Summary
This paper presents a hybrid approach combining knowledge graphs (KGs) built from ontologies with Bayesian networks (BNs) for explainable disease risk prediction. The methodology leverages structured medical domain knowledge from ontologies while incorporating statistical patterns from electronic health record (EHR) data, achieving strong predictive performance with transparent evidence chains. Applied to atrial fibrillation risk prediction using MIMIC-IV data, the hybrid model achieved 75.95% accuracy, 87.00% recall, and 80% AUC while maintaining interpretability through traceable links to scientific evidence.

## Method Summary
The approach constructs a BN structure from ontology-based KGs, then learns parameters from MIMIC-IV EHR data. Patient selection required ≥3 hospital visits with AF timing constraints, yielding 2,242 balanced patients. Five risk factor categories were extracted: anthropometric (BMI, height), comorbidity (6 ICD-coded conditions), demographic (age, race, sex), ECG (PR interval, P-wave duration), and lifestyle (smoking, alcohol). The hybrid BN uses knowledge-driven structure with data-driven parameters—parent node risks calculated from training data prevalence and weights derived from Cramér's V associations. Synthesis nodes aggregate parent contributions using weighted risk algorithms. Three BN variants were compared: knowledge-driven, data-driven, and hybrid.

## Key Results
- Hybrid BN achieved 75.95% accuracy, 87.00% recall, 71.06% precision, 78.23% F1, and 0.80 AUC on AF prediction
- Outperformed purely knowledge-driven approaches while maintaining explainability through evidence chains
- Demonstrated graceful degradation with missing data, suitable for real-world clinical settings
- Provided transparent evidence chains linking predictions to scientific literature via the ontology

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combining knowledge-driven structure with data-driven parameters improves both interpretability and population-specific accuracy compared to either approach alone.
- Mechanism: The ontology defines risk factor classes and their hierarchical relationships, which map directly to BN nodes and edges. CPTs are then learned from EHR data (prevalence, Cramér's V associations) rather than purely from domain knowledge, allowing the model to capture local patterns that differ from epidemiological literature.
- Core assumption: The domain knowledge structure (which factors influence risk and how they group) is valid across populations, but the strength of influence varies by clinical context.
- Evidence anchors:
  - [abstract] "constructing BNs from ontology-based KGs and multimodal EHR data...balances generalised medical knowledge with patient-specific context"
  - [Section III.D.3] "The hybrid and knowledge-driven BNs differ in how they calculate parent node risks and weights. For the hybrid BN, we calculated risks directly from the training data"
  - [corpus] Related work on Bayesian hybrid ML for gallstone risk (arXiv:2506.14561) shows similar hybrid parameterization, but corpus lacks direct replications of this specific KG-to-BN mapping approach.
- Break condition: If the target population's risk factor structure fundamentally differs from the ontology (e.g., novel risk factors not in KG), the structure will be incomplete and predictions systematically biased.

### Mechanism 2
- Claim: BNs provide principled uncertainty quantification that degrades gracefully with missing data.
- Mechanism: BNs compute full posterior distributions over target variables given observed evidence. When some risk factors are unobserved, the BN marginalizes over them using learned priors rather than failing or imputing single values.
- Core assumption: The learned prior distributions from training data are representative of the deployment population.
- Evidence anchors:
  - [abstract] "effectively handles uncertainty, is highly explainable, and achieves good predictive performance"
  - [Section IV.B] "the hybrid BN demonstrates the ability to perform inference with limited evidence, reflecting its suitability to real-world settings where data may be incomplete"
  - [corpus] KAT-GNN paper (arXiv:2511.01249) addresses temporal EHR irregularity but uses neural methods rather than probabilistic marginalization.
- Break condition: If missingness is systematic (e.g., high-risk patients less likely to have complete records) and correlated with outcomes, posterior estimates will be biased.

### Mechanism 3
- Claim: Explicitly modeling evidence sources in the KG enables traceable explanations that clinicians can validate against scientific literature.
- Mechanism: The ontology includes an Evidence class linked to Risk relationships via `is evidenced by`, and Publications via `is published as`. When a risk prediction is made, the BN's contributing factors can be traced to specific risk relationships in the KG, which link to cited publications with metadata (title, authors, date).
- Core assumption: Clinicians will accept explanations that cite scientific literature, and the literature cited is relevant to the specific patient context.
- Evidence anchors:
  - [Section III.C] "Evidence supporting each risk relationship is modelled through an object property, is evidenced by, which links to the Evidence class. The Evidence class connects to a Publication class"
  - [Section IV.D] "our approach provides a transparent evidence chain that connects risk predictions to both domain knowledge (from the BN to the KG to the scientific literature)"
  - [corpus] The explainable disease surveillance paper (arXiv:2501.15969) emphasizes clinical explainability but doesn't formalize evidence linkage through ontologies.
- Break condition: If evidence chains become too long or involve weak/inconclusive publications, clinicians may lose trust in the explanation quality.

## Foundational Learning

- Concept: **Directed Acyclic Graphs (DAGs) and Conditional Probability Tables**
  - Why needed here: BNs require understanding that nodes represent variables with mutually exclusive states, edges represent direct influence, and CPTs quantify dependency strength between parent and child nodes.
  - Quick check question: Given a node with 3 states and 2 parents each with 4 states, how many probability values must be specified in its CPT?

- Concept: **Ontology Design Patterns (Class Hierarchies, Object/Data Properties)**
  - Why needed here: The KG schema defines risk factor taxonomy, relationships, and evidence linkage using OWL constructs that must map cleanly to BN nodes.
  - Quick check question: What is the difference between an object property (relating two classes) and a data property (relating a class to a literal value) in this ontology?

- Concept: **Chi-Square Test and Cramér's V for Association Strength**
  - Why needed here: Feature selection and weight calculation for the BN use chi-square to detect associations and Cramér's V to quantify their strength (weak <0.1, moderate 0.1-0.14, strong 0.15-0.24, very strong >0.24).
  - Quick check question: Why might a risk factor show strong association in epidemiological literature but weak association in a specific ICU dataset like MIMIC-IV?

## Architecture Onboarding

- Component map:
  - Ontology (Protégé/OWL) -> Knowledge Graph (GraphDB) -> BN Structure (PyAgrum) -> Parameter Learning (MIIC + data) -> Evaluation

- Critical path:
  1. Define risk factors from literature → design ontology classes/properties
  2. Preprocess EHR data (feature engineering, categorization) → map to ontology instances
  3. Load KG into GraphDB → validate with sample queries
  4. Generate BN structure from ontology → learn parameters from training data
  5. Evaluate on held-out test set → iterate on thresholds and weights

- Design tradeoffs:
  - **Recall vs Precision**: Hybrid BN prioritizes recall (87%) over precision (71%)—acceptable for screening where false positives trigger follow-up rather than harm
  - **Structure vs Flexibility**: Fixed ontology-driven structure limits discovery of novel risk factor interactions but ensures clinical plausibility
  - **Synthesis nodes**: Aggregating factors (e.g., "Lifestyle factors" from Smoking + Alcohol) improves interpretability but may obscure individual contributions

- Failure signatures:
  - **Low recall (~40%)**: Indicates knowledge-driven weights don't match population—switch to data-derived weights
  - **Incoherent BN structure**: Data-driven learning produced edges contradicting domain knowledge (e.g., age causing sex)—enforce structure constraints
  - **KG query failures**: Ontology class names don't match BN node names—validate naming consistency before deployment

- First 3 experiments:
  1. **Ablation on parameter source**: Train three BNs (knowledge-only weights, data-only weights, hybrid) on same structure; compare recall/precision to quantify value of data-driven adaptation.
  2. **Missing data stress test**: Simulate 10-50% missingness on key risk factors; measure degradation in AUC and calibration to verify graceful degradation claim.
  3. **Evidence chain retrieval**: For 10 predictions, manually verify that KG queries return correct publications; measure latency to ensure clinical workflow compatibility.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the hybrid approach maintain predictive performance and explainability when applied to datasets outside the specific MIMIC-IV critical care context?
- Basis in paper: [explicit] The authors state, "Finally, we will validate the approach across more diverse and generalized datasets in order to establish its broader applicability."
- Why unresolved: The current evaluation relies solely on MIMIC-IV, which the authors note has a "unique critical care setting" where statistical patterns differ significantly from established epidemiological knowledge.
- Evidence: Application of the framework to a general population or outpatient EHR dataset with comparable AUC and explainability metrics.

### Open Question 2
- Question: Can extending the model into a Bayesian Decision Network (BDN) effectively recommend optimal interventions for at-risk patients?
- Basis in paper: [explicit] The authors propose to "extend the BN into a Bayesian decision network through the addition of decision and utility nodes, allowing for the recommendation of optimal interventions."
- Why unresolved: The current model is limited to risk prediction and lacks the structural components (decision/utility nodes) required to model the consequences of specific clinical actions or suggest treatments.
- Evidence: Implementation of the BDN and validation of its recommended interventions against clinical expert guidelines or actual patient outcomes.

### Open Question 3
- Question: Can the implementation of counterfactual reasoning provide meaningful contrastive "why-not" explanations without disrupting the model's explanatory coherence?
- Basis in paper: [explicit] Future work includes the intent to "implement counterfactual reasoning using the BN, enabling contrastive 'why-not' explanations to further enhance explainability."
- Why unresolved: While the current model offers transparent evidence chains, it cannot yet explain why a patient might *not* be in a specific risk category relative to another.
- Evidence: User studies with clinicians evaluating the utility and clarity of counterfactual explanations in the modified system.

## Limitations

- The approach relies on the assumption that domain knowledge structures remain valid across populations, with only parameter values needing adaptation
- Synthesis node thresholds (T1, T2) for converting weighted parent risks to discrete states are unspecified in the paper
- Complete weight ratios for all synthesis node parent contributions are not fully specified, limiting precise reproduction

## Confidence

- **High Confidence**: The hybrid BN's improved recall (87%) over knowledge-driven approaches demonstrates the value of data-driven parameter adaptation. The principled uncertainty handling through marginalization is well-established in BN theory. The ontological evidence linkage provides verifiable traceability from predictions to scientific literature.
- **Medium Confidence**: The claim that the hybrid approach balances interpretability with accuracy compared to purely data-driven alternatives requires more direct comparison, as the paper doesn't report performance metrics for the data-driven variant. The assertion that clinicians will accept evidence chain explanations needs validation beyond technical feasibility.
- **Low Confidence**: The specific numerical thresholds for synthesis node state classification (T1, T2) and complete weight ratios for all synthesis nodes are unspecified, limiting precise reproduction. The generalizability of Cramér's V thresholds across different risk factor types and populations is not demonstrated.

## Next Checks

1. **Ablation Study on Parameter Sources**: Train and evaluate three BN variants (knowledge-only weights, data-only weights, hybrid) on identical structures using the same MIMIC-IV dataset. Compare recall, precision, and AUC to quantify the marginal value of data-driven parameter adaptation versus the baseline knowledge-driven approach.

2. **Cross-Population Validation**: Apply the trained hybrid BN to a different EHR dataset (e.g., eICU or another hospital system) without retraining the structure. Measure performance degradation in accuracy and AUC to test whether the knowledge structure generalizes beyond the training population.

3. **Evidence Chain Clinical Validation**: Conduct a user study with 10 clinicians where each reviews 5 predictions with evidence chains versus 5 predictions without. Measure trust ratings, time to understand predictions, and perceived clinical utility to validate whether ontological evidence linkage provides practical value beyond technical traceability.