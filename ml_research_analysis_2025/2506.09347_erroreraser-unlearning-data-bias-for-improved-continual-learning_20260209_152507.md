---
ver: rpa2
title: 'ErrorEraser: Unlearning Data Bias for Improved Continual Learning'
arxiv_id: '2506.09347'
source_url: https://arxiv.org/abs/2506.09347
tags:
- uni00000013
- data
- learning
- task
- noisy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of data biases in continual learning
  (CL), where noisy labels in training data lead to learning incorrect knowledge that
  accumulates and transfers across tasks, degrading performance. The authors propose
  ErrorEraser, a universal plugin that combines Error Identification and Error Erasure
  modules to identify and forget erroneous knowledge in CL models without requiring
  access to historical data.
---

# ErrorEraser: Unlearning Data Bias for Improved Continual Learning

## Quick Facts
- **arXiv ID:** 2506.09347
- **Source URL:** https://arxiv.org/abs/2506.09347
- **Reference count:** 40
- **Primary result:** ErrorEraser improves continual learning performance across regularization-based, replay-based, and optimization-based methods by identifying and erasing erroneous knowledge from noisy labels.

## Executive Summary
This paper addresses data biases in continual learning where noisy labels cause incorrect knowledge to accumulate and transfer across tasks, degrading performance. The authors propose ErrorEraser, a universal plugin combining Error Identification and Error Erasure modules to identify and forget erroneous knowledge without requiring historical data. The method uses a Normalizing Flow model to learn probability density distributions in feature space to identify potentially biased samples, then selectively shifts decision boundaries of outlier samples to forget only erroneous knowledge. Extensive experiments show ErrorEraser significantly improves accuracy and reduces forgetting rates across multiple benchmark datasets.

## Method Summary
ErrorEraser is a plugin that integrates with existing continual learning methods to handle noisy labels. It operates through two main components: an Error Identification module that uses Normalizing Flows to learn probability density distributions in feature space, identifying low-density samples as potential noise; and an Error Erasure module that remaps decision boundaries of identified outliers to a pseudo-class and prunes them. The method employs incremental feature distribution learning to reduce resource overhead and avoid accessing historical data. During training, samples with probability density below threshold δ are assigned pseudo-labels, the model is fine-tuned to shift their decision space, and the pseudo-class neuron is pruned.

## Key Results
- ErrorEraser significantly improves continual learning performance across regularization-based, replay-based, and optimization-based methods
- The method achieves higher accuracy and lower forgetting rates on benchmark datasets including CIFAR-10, CIFAR-100, MNIST, and WebVision
- ErrorEraser successfully handles 10%, 30%, and 50% asymmetric label noise in synthetic experiments

## Why This Works (Mechanism)

### Mechanism 1: Density-Based Noise Identification
If noisy labels induce feature representations that deviate from the majority distribution of a class, then a probability density estimator can isolate these errors as outliers. The paper uses a Normalizing Flow model to learn the probability density p(x) of data within a compact feature space (h_b). By mapping complex feature distributions to a Gaussian latent space, the model assigns likelihood scores to samples. Low probability density values at the distribution margins are interpreted as indicators of erroneous samples. This assumes correctly labeled samples cluster tightly while noisy samples are spurious and lie in low-density regions.

### Mechanism 2: Decision Boundary Shift via Pseudo-Class Mapping
If a model can selectively remap the decision boundaries of erroneous samples to a "pseudo-class" and then excise that class, the model can "forget" specific errors without retraining on the full dataset. Instead of directly altering gradients for the original class, the method assigns low-probability samples (D_s) a new temporary pseudo-label ŷ. The model adds a corresponding neuron to the output layer and fine-tunes specifically on these samples to shift their decision space to this pseudo-class. Finally, the neuron is pruned. This assumes the parameters updated during fine-tuning are localizable and can be pruned without degrading decision boundaries for remaining high-density samples.

### Mechanism 3: Incremental Distribution Stability
If the distribution learner (NF) updates incrementally using constraints from previous tasks, it can maintain a stable reference for identifying errors in new tasks without accessing historical raw data. To avoid resource overhead and forgetting in the density estimator itself, the paper uses an incremental learning strategy. It constrains the current distribution learning using feature sets (G_h'b) sampled from previous tasks' distributions. This assumes the feature extractor (h_b) remains relatively stable across tasks, or the constraints are sufficient to prevent catastrophic forgetting of feature modes from old tasks.

## Foundational Learning

- **Concept: Normalizing Flows (NF)**
  - **Why needed here:** Standard classifiers output class probabilities but not input density. NF provides the mechanism to calculate exact likelihoods p(x) in the feature space, which is the criteria for identifying "errors" (outliers).
  - **Quick check question:** Can you explain how a bijective (reversible) transformation allows us to calculate the change of variables in a probability distribution?

- **Concept: Catastrophic Forgetting**
  - **Why needed here:** This is the fundamental problem the plugin solves. Understanding that standard CL methods try to retain knowledge while this method adds intentional forgetting is the core thesis.
  - **Quick check question:** Why does updating a neural network on Task B cause it to lose accuracy on Task A?

- **Concept: Pseudo-labeling / Self-training**
  - **Why needed here:** The "Error Erasure" module relies on the ability to treat identified noise as a distinct class (pseudo-class) to manipulate the decision boundary.
  - **Quick check question:** How does assigning a dummy label to data allow us to manipulate the decision boundary of a classifier?

## Architecture Onboarding

- **Component map:**
  - Backbone (h_a,b,c): Main CL model where feature extraction and classification happen
  - Distribution Learner (M): Normalizing Flow model attached to feature layer (h_b) to estimate density
  - Plugin Controller: Logic to select samples based on threshold δ and manage "Expansion → Fine-tune → Prune" cycle

- **Critical path:**
  1. Forward Pass: Input → Backbone Features (h_b) → NF Model (M)
  2. Identification: Calculate p(x) for batch; select samples where p(x) < δ
  3. Modification: Add pseudo-class neuron; fine-tune on selected samples; prune neuron
  4. Update: Train Backbone + NF jointly using combined Loss (L_ce + L_ge + L_cl)

- **Design tradeoffs:**
  - Threshold δ: A lower threshold is conservative (only forgets obvious outliers, high precision) while a higher threshold is aggressive (forgets more, high recall of noise but risks deleting valid data)
  - Overhead: The NF model adds computational cost to training, though the incremental strategy aims to mitigate this compared to re-training

- **Failure signatures:**
  - Attention Drift: Visualization shows the model focusing on background instead of objects. If this persists after applying ErrorEraser, the Error Identification module is likely failing to detect the bias, or the Error Erasure is ineffective
  - High Forgetting Rate (F): If F increases after applying the plugin, the "unlearning" process (pseudo-class mapping) is likely interfering with shared representations of old tasks

- **First 3 experiments:**
  1. Density Validation: Inject known asymmetric noise into a dataset. Train the NF module and plot the histogram of p(x) for clean vs. noisy samples to verify they are separable
  2. Ablation on Threshold (δ): Run ErrorEraser with varying density thresholds to find the "sweet spot" between removing noise and retaining valid data on a validation set
  3. Module Isolation: Test the "Error Erasure" mechanism in isolation: Take a pre-trained model with known noise, manually select the noisy samples (bypassing identification), and test if the pseudo-class shift successfully improves accuracy without dropping performance on other classes

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does ErrorEraser perform in class-incremental and domain-incremental learning scenarios where task identities are unknown or the domain shifts?
- **Basis in paper:** Section 8 states the intention to explore the impact of noisy labels on CL across all three scenarios (task-incremental, class-incremental, and domain-incremental)
- **Why unresolved:** The current method is evaluated exclusively under task-incremental learning settings where the task ID is explicitly known at test time
- **What evidence would resolve it:** Empirical results on class-incremental benchmarks demonstrating that probability density estimation remains robust for error identification

### Open Question 2
- **Question:** Can the method effectively identify and erase errors when tasks contain overlapping classes?
- **Basis in paper:** Section 8 notes the intent to investigate the challenge of overlapping classes between tasks
- **Why unresolved:** The current experimental setup assumes classes are independent and non-overlapping between tasks
- **What evidence would resolve it:** Experiments using datasets where subsequent tasks contain the same classes but different data distributions

### Open Question 3
- **Question:** Is the Error Identification module robust to input-level noise (e.g., blurred or damaged images) rather than just label noise?
- **Basis in paper:** Section 8 mentions the intention to address more realistic data bias situations such as noisy data
- **Why unresolved:** The method currently identifies errors by analyzing probability density in the feature space based on label consistency
- **What evidence would resolve it:** Performance analysis on datasets with synthetic image corruptions combined with label noise

## Limitations

- The paper does not specify the exact CNN architecture or the specific percentile threshold (δ) for error identification, which could affect reproducibility
- While the paper claims to handle systematic biases, it acknowledges that if noisy labels form high-density clusters (e.g., >50% noise), the density-based identification mechanism may fail
- The method is currently evaluated only on task-incremental learning settings, limiting generalizability to more complex continual learning scenarios

## Confidence

- **High Confidence:** The Error Erasure mechanism (pseudo-class mapping and pruning) is technically sound and well-defined
- **Medium Confidence:** The Normalizing Flow-based Error Identification is effective for outlier detection but may struggle with systematic, high-density noise patterns
- **Medium Confidence:** The incremental learning strategy for the density model is a reasonable approach to prevent forgetting, but its effectiveness depends on the stability of the feature extractor

## Next Checks

1. **Noise Density Test:** Inject systematic noise (e.g., 50% of samples from one class labeled as another) to evaluate if the density-based identification fails when noise forms its own cluster
2. **Architecture Sensitivity:** Test ErrorEraser with different CNN depths (e.g., shallow vs. deep) to assess the impact of feature extractor stability on the density model's accuracy
3. **Threshold Sensitivity:** Systematically vary the density threshold δ (e.g., 5th to 30th percentile) and measure the trade-off between noise removal and retention of valid data