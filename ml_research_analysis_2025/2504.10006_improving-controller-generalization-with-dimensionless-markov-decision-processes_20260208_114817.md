---
ver: rpa2
title: Improving Controller Generalization with Dimensionless Markov Decision Processes
arxiv_id: '2504.10006'
source_url: https://arxiv.org/abs/2504.10006
tags:
- learning
- policy
- dimensionless
- context
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the problem of poor generalization in reinforcement\
  \ learning controllers when tested in environments different from their training\
  \ ones. The core method introduces the Dimensionless Markov Decision Process (\u03A0\
  -MDP), which extends Contextual-MDPs by non-dimensionalizing state and action spaces\
  \ using the Buckingham-\u03A0 theorem."
---

# Improving Controller Generalization with Dimensionless Markov Decision Processes

## Quick Facts
- **arXiv ID:** 2504.10006
- **Source URL:** https://arxiv.org/abs/2504.10006
- **Authors:** Valentin Charvet; Sebastian Stein; Roderick Murray-Smith
- **Reference count:** 13
- **Primary result:** Π-MDPs enable zero-shot generalization of RL controllers to physical parameter shifts (e.g., pole length/mass) by non-dimensionalizing state/action spaces.

## Executive Summary
This paper addresses the challenge of poor generalization in reinforcement learning controllers when tested in environments with dynamics different from training. The authors propose Dimensionless Markov Decision Processes (Π-MDPs), which extend Contextual-MDPs by transforming state and action spaces into dimensionless groups using the Buckingham-Π theorem. This transformation induces policy equivariance to context changes, allowing a single controller to succeed across varying physical parameters. The approach is demonstrated on simulated actuated pendulum and cartpole systems, showing significant improvements in controllable area compared to standard controllers.

## Method Summary
The method involves transforming the natural state-action space into dimensionless Π-groups using the Buckingham-Π theorem, where the context (physical parameters like mass, length, gravity) is used to compute these transformations. A probabilistic dynamics model (Gaussian Process) is trained on these dimensionless transitions, and a single policy is optimized to operate in this space. During deployment, the policy outputs a dimensionless action which is then transformed back to physical units using the current context. This creates a controller that is equivariant to changes in the underlying physical dynamics.

## Key Results
- Policies trained on a single environment using the dimensionless approach are robust to shifts in the distribution of the context
- The dimensionless controller achieved significantly larger controllable areas (18) compared to the natural controller
- Zero-shot generalization achieved without requiring additional training or fine-tuning on target environments
- Method successfully handles changes in pole length and mass parameters in simulated pendulum and cartpole systems

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Non-dimensionalizing state and action spaces via the Buckingham-$\Pi$ theorem appears to induce policy equivariance to changes in physical context parameters (e.g., mass, length).
- **Mechanism:** By transforming dimensional inputs into dimensionless $\Pi$-groups, the policy operates on normalized dynamics. When the physical context changes, the dimensionless representation of the state remains statistically similar to the training distribution, allowing the same policy weights to generalize without retraining.
- **Core assumption:** The physical dimensions of the system variables are known, and the context vector is observable to compute the transformation $\Phi_\Pi$.
- **Evidence anchors:** [abstract] "This procedure induces policies that are equivariant with respect to changes in the context of the underlying dynamics." [section 4] Defines $\Pi$-MDP and the transformation $\Phi_\Pi$ mapping natural states to dimensionless ones.
- **Break condition:** Unobservable context or incorrect identification of physical dimensions (units) leading to invalid $\Pi$-groups.

### Mechanism 2
- **Claim:** Training a probabilistic dynamics model (Gaussian Process) in dimensionless space likely reduces model bias when predicting outcomes in perturbed environments.
- **Mechanism:** Standard models learn correlations specific to the training scale. A dimensionless model learns the underlying physical ratios. When deployed on a heavier mass, the dimensionless input remains valid, preventing the compounding errors typical in standard Model-Based RL.
- **Core assumption:** The relationship between dimensionless groups captures the essential dynamics, and the Gaussian Process can approximate these functions effectively.
- **Evidence anchors:** [section 4] "Probabilistic model is able to eliminate most of the bias... we extend the subclass of model-based policy gradient methods with Gaussian Process priors." [section 5] Empirical results show $\Pi$-PILCO maintains success rates where natural controllers fail.
- **Break condition:** High-dimensional systems where data is insufficient to overcome the "curse of dimensionality" in the GP, or where the physics are non-stationary within an episode.

### Mechanism 3
- **Claim:** Explicitly rescaling actions based on observed context ($\Phi^{-1}_\Pi$) allows a single dimensionless policy to generate the correct magnitude of control effort across varying physical scales.
- **Mechanism:** The policy outputs a dimensionless "control intent" (e.g., specific thrust). The inverse transformation $\Phi^{-1}$ scales this back to physical units (e.g., Newtons) using the current context. If the mass doubles, the inverse transformation automatically doubles the output force, maintaining the same acceleration profile.
- **Core assumption:** The context is static during the episode, allowing a fixed scaling factor per rollout.
- **Evidence anchors:** [section 4] Algorithm 1 details the loop: $a_{\Pi,t} = \pi(s_{\Pi,t})$ followed by $a_t = \Phi^{-1}(a_{\Pi,t})$. [section 5] "The Buckingham actions are rescaled to reflect the change in pole length... explaining how zero-shot generalization can be improved."
- **Break condition:** Actuator saturation limits; if the required physical force $a_t$ exceeds hardware limits after rescaling, the policy will fail.

## Foundational Learning

- **Concept:** **Buckingham-$\Pi$ Theorem (Dimensional Analysis)**
  - **Why needed here:** This is the mathematical engine of the paper. Without understanding how to derive dimensionless groups, one cannot define the $\Pi$-MDP.
  - **Quick check question:** Given variables with dimensions of Length ($L$) and Time ($T$), can you construct a dimensionless group using velocity ($L/T$)?

- **Concept:** **Gaussian Processes (GP) for Dynamics Learning**
  - **Why needed here:** The authors extend PILCO, a GP-based algorithm. Understanding how GPs model uncertainty (variance) is crucial, as the method relies on propagating this uncertainty to plan robustly.
  - **Quick check question:** How does a GP's prediction variance change as we query points further from the training data?

- **Concept:** **Contextual Markov Decision Processes (C-MDP)**
  - **Why needed here:** The paper frames generalization as solving a C-MDP where the transition kernel $f|c$ depends on hidden context. Distinguishing this from standard MDPs is necessary to understand the "Generalization Gap" problem statement.
  - **Quick check question:** In a C-MDP, does the context change the transition dynamics, the reward function, or both? (This paper assumes only dynamics).

## Architecture Onboarding

- **Component map:** Raw State $s_t$, Context $c$ -> Encoder ($\Phi_\Pi$) -> Dimensionless State $s_{\Pi,t}$ -> Policy ($\pi_\Pi$) -> Dimensionless Action $a_{\Pi,t}$ -> Decoder ($\Phi^{-1}_\Pi$) -> Physical Action $a_t$ -> World Model (GP)

- **Critical path:** Correctly deriving the dimensionless $\Pi$-groups (the "Encoder"). If this transformation is mathematically wrong (incorrect exponents), the policy will see shifting distributions, breaking the equivariance.

- **Design tradeoffs:**
  - **Physics Priors vs. Flexibility:** Requires domain knowledge (units). You cannot plug this into a black-box visual RL system easily.
  - **GP vs. Neural Nets:** GPs are sample efficient (good for sim-to-real) but scale poorly ($O(n^3)$) with dataset size compared to neural networks used in other generalization methods (e.g., domain randomization).

- **Failure signatures:**
  - **Action Clipping:** If context $c$ scales up mass, the decoder produces large forces. If the simulator/robot clips these, performance drops.
  - **Context Drift:** If the assumption "context is static per episode" is violated, the transformation $\Phi$ will mis-scale states/actions, leading to oscillation or divergence.

- **First 3 experiments:**
  1. **Verify Transformation:** Implement the $\Pi$-group calculation for the Pendulum (Appendix A) and verify that state trajectories overlay when plotting dimensionless time/states across different physical parameters.
  2. **Baseline vs. $\Pi$-PILCO:** Train standard PILCO on a pendulum with $L=1$. Test on $L=0.5$ and $L=1.5$. Then train $\Pi$-PILCO and compare the "Controllable Area" metric.
  3. **Ablation on Context Noise:** Inject noise into the observed context $c$ before the transformation $\Phi$ to test robustness to imperfect system identification.

## Open Questions the Paper Calls Out

- **Question:** Can the context vector be actively inferred online using exploratory control policies when the perturbation variables are unobservable?
  - **Basis in paper:** [explicit] The authors identify the requirement for measuring perturbation variables as a weakness and suggest "identification of the parameters could be achieved with different control policies that aim to actively infer those values," leaving this for future work.
  - **Why unresolved:** The current framework assumes the context $c$ is fully observable and static, acting as a confounding variable rather than a hidden state to be estimated during deployment.
  - **What evidence would resolve it:** A dual-control formulation where the agent balances exploration and control to estimate $c$ from system response, maintaining performance without prior knowledge of the context.

- **Question:** Does the $\Pi$-MDP framework maintain generalization benefits when applied to high-dimensional systems where determining dimensional units is difficult?
  - **Basis in paper:** [explicit] The authors note that "knowing the measurements' dimensions can be prohibitively expensive on high-dimensional systems" and suggest using physical priors or numerical methods to find them.
  - **Why unresolved:** The experiments were restricted to low-dimensional, second-order systems (pendulum and cartpole) with known physical units.
  - **What evidence would resolve it:** Empirical results showing successful zero-shot transfer on complex tasks (e.g., high-DoF manipulation) using numerically discovered dimensionless groups.

- **Question:** Is the performance improvement strictly due to the dimensionless transformation, or does it depend heavily on the Gaussian Process (GP) model's ability to handle uncertainty?
  - **Basis in paper:** [inferred] While the authors claim the framework applies to any Model-Based RL algorithm, the evaluation relies exclusively on $\Pi$-PILCO (which uses GPs). It is unclear if the "conservative" nature of GPs contributes significantly to the reported robustness compared to the transformation alone.
  - **Why unresolved:** There is no ablation study comparing $\Pi$-PILCO against other dimensionless model-based backends (e.g., neural networks) to isolate the contribution of the Buckingham-$\Pi$ transformation from the model prior.
  - **What evidence would resolve it:** A comparative study applying the $\Pi$-MDP transformation to deterministic or deep ensemble models to see if generalization gains persist without the GP's specific uncertainty quantification.

## Limitations
- Requires explicit knowledge of physical dimensions of system variables, creating a practical limitation for systems with complex or poorly understood physics
- Current framework assumes context is fully observable and static per episode, which may not hold in real-world deployment
- Reliance on Gaussian Processes limits scalability to high-dimensional state-action spaces compared to neural network-based approaches

## Confidence
- **High Confidence:** The mathematical framework of Π-MDP and the Buckingham-Π transformation are sound (Sections 3-4). The empirical demonstration of improved generalization metrics (Controllable Area) on simple pendulum/cartpole systems is well-documented.
- **Medium Confidence:** The mechanism by which dimensionless representations induce policy equivariance is theoretically justified but relies on the assumption of static context per episode. The effectiveness of the RBF policy + GP dynamics combination is validated on the tested systems but may not generalize to more complex tasks.
- **Low Confidence:** The claim that this approach is "scalable" for real-world deployment is not directly tested; the paper's experiments are limited to simulated environments. The method's robustness to noise in the context measurement is only briefly mentioned.

## Next Checks
1. **Context Drift Test:** Evaluate Π-PILCO performance in a setting where the context changes slowly during an episode (e.g., a pendulum with decreasing mass). This violates the static context assumption and would reveal failure modes.
2. **High-Dimensional Scaling:** Apply the Π-MDP framework to a more complex system (e.g., a quadruped robot with more state variables) to test the GP's scalability limits and the practicality of deriving dimensionless groups for a larger system.
3. **Noise Sensitivity Analysis:** Systematically inject noise into the context vector $c$ used for the $\Phi$ and $\Phi^{-1}$ transforms. Measure the degradation in performance to quantify the method's sensitivity to system identification errors.