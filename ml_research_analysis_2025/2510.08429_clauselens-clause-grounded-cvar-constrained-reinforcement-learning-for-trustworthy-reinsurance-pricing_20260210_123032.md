---
ver: rpa2
title: 'ClauseLens: Clause-Grounded, CVaR-Constrained Reinforcement Learning for Trustworthy
  Reinsurance Pricing'
arxiv_id: '2510.08429'
source_url: https://arxiv.org/abs/2510.08429
tags:
- clauselens
- legal
- clauses
- policy
- quoting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ClauseLens is a clause-grounded reinforcement learning framework
  that produces transparent, regulation-compliant, and risk-aware reinsurance treaty
  quotes. The core idea is to model quoting as a Risk-Aware Constrained Markov Decision
  Process, integrating retrieved legal clauses into the agent's observations to constrain
  feasible actions and generate clause-grounded natural language justifications.
---

# ClauseLens: Clause-Grounded, CVaR-Constrained Reinforcement Learning for Trustworthy Reinsurance Pricing

## Quick Facts
- arXiv ID: 2510.08429
- Source URL: https://arxiv.org/abs/2510.08429
- Reference count: 40
- Primary result: 51% reduction in solvency violations, 27.9% CVaR tail-risk improvement, 88.2% entailment accuracy for clause-grounded explanations

## Executive Summary
ClauseLens is a reinforcement learning framework for reinsurance treaty pricing that integrates retrieved legal clauses into the agent's decision-making process. The system retrieves relevant regulatory clauses, embeds them into the state representation, and uses them to both constrain actions and generate natural language justifications. Evaluated in a multi-agent treaty simulator, ClauseLens demonstrates significant improvements in regulatory compliance and tail-risk management while maintaining interpretable decision-making.

## Method Summary
ClauseLens models reinsurance quoting as a Risk-Aware Constrained Markov Decision Process where the agent produces quotes (quota share, deductible, attachment points) to maximize tail-risk-adjusted return while satisfying regulatory constraints. The method uses dual-projected PPO with CVaR-weighted advantages and Lagrangian constraint updates. At each decision step, the system retrieves top-k relevant clauses from a legal corpus using dense semantic search, concatenates these embeddings with structured cedent features, and passes the augmented state to both the policy network and justification module. The policy is trained with frozen clause retrieval and frozen T5 justification generation to preserve auditability.

## Key Results
- 51% reduction in solvency violations (9.4%→4.6%) through clause-derived action masking and Lagrangian penalty updates
- 27.9% improvement in tail-risk performance (CVaR_0.10 from -6.8 to -4.9) with minimal return degradation (5.3→5.1)
- 88.2% accuracy in clause-grounded explanations with 87.4% retrieval precision and 91.1% recall

## Why This Works (Mechanism)

### Mechanism 1: Clause-Augmented State Representation
Embedding retrieved legal clauses directly into the agent's observation space enables regulation-aware decision-making. At each decision step, the system retrieves top-k relevant clauses from a legal corpus using dense semantic search (LegalBERT embeddings + FAISS), concatenates these embeddings with structured cedent features, and passes the augmented state to both the policy network and justification module. This creates a state representation that explicitly encodes legal constraints.

### Mechanism 2: CVaR-Weighted Policy Optimization
Optimizing for Conditional Value at Risk (CVaR) at α=0.10 shifts policy learning toward robustness in worst-case scenarios, improving tail-risk performance without catastrophic return degradation. Rather than maximizing expected return, the agent maximizes CVaR_α(R^π)—the expected return in the worst α-quantile of trajectories. During training, advantage estimates are reweighted to emphasize the bottom 10% of episodes by cumulative reward.

### Mechanism 3: Dual-Projected Constraint Enforcement
Combining hard clause-based action masking with soft Lagrangian penalty updates achieves regulatory compliance while preserving policy adaptability. Two complementary constraint mechanisms operate simultaneously: (1) Action masks derived from retrieved clauses preemptively filter out non-compliant actions, creating hard feasibility constraints. (2) Lagrangian dual variables are updated via projected gradient ascent to penalize expected constraint violations that slip through.

## Foundational Learning

- **Concept: Conditional Value at Risk (CVaR)**
  - Why needed here: CVaR quantifies expected loss in the worst α-fraction of outcomes. Unlike Value at Risk (VaR), which only identifies a threshold, CVaR captures the full tail expectation—critical for insurance where catastrophic events matter more than average cases.
  - Quick check question: If a policy has VaR₀.₁₀ = -10 and CVaR₀.₁₀ = -15, what does the difference tell you about the loss distribution's tail shape?

- **Concept: Constrained Markov Decision Processes (CMDPs)**
  - Why needed here: Standard MDPs maximize a single objective, but regulated quoting must satisfy multiple hard constraints (solvency, capital adequacy, jurisdiction-specific limits). CMDPs introduce constraint functions dₖ(s,a) with tolerances εₖ that must be satisfied alongside return optimization.
  - Quick check question: In a CMDP with constraints E[d₁]≤0.05 and E[d₂]≤0.10, if your policy achieves E[d₁]=0.03 and E[d₂]=0.12, is it feasible? What Lagrangian penalty would result?

- **Concept: Proximal Policy Optimization (PPO) with Clipping**
  - Why needed here: PPO provides stable policy gradient updates by clipping the probability ratio between new and old policies, preventing destructively large updates. ClauseLens modifies PPO with CVaR-weighted advantages and dual-variable updates, requiring understanding of the base algorithm.
  - Quick check question: If the PPO clip ratio is 0.2 and the probability ratio r(θ) = π_θ(a|s)/π_θ_old(a|s) = 1.5 with positive advantage, what is the clipped surrogate objective value?

## Architecture Onboarding

**Component map:**
Cedent Request (x) -> Clause Retriever (FAISS + LegalBERT) -> Top-k Clauses {c_i} -> State Constructor -> s = [x; c_1; ...; c_k] -> Quoting Policy π(a|s) and Action Mask M(s) -> Filtered Action a ∈ M -> Treaty Simulator -> Reward r, Violations {d_k} -> CVaR-weighted PPO + Lagrangian Updates and Justification Generator -> Explanation Output

**Critical path:**
1. Clause retrieval correctness (precision/recall of 87.4%/91.1%)—if retrieval fails, all downstream components receive garbage legal context.
2. Action masking logic—must correctly translate clause semantics into action constraints; errors here cause either false violations or over-constrained policies.
3. CVaR estimation stability—requires sufficient samples in the tail (α=0.10 means only 10% of episodes contribute to gradient), sensitive to batch size and episode variance.

**Design tradeoffs:**
- Frozen vs. learnable retriever: ClauseLens freezes LegalBERT/FAISS to preserve interpretability and audit trails, sacrificing potential end-to-end optimization.
- Hard masking vs. pure Lagrangian: Hard masks guarantee zero violations from masked clause categories but may over-constrain; Lagrangian penalties allow soft tradeoffs but can't guarantee strict compliance.
- Explanation module inclusion: Adding justification generation shows no statistically significant return degradation (p>0.1), but adds inference overhead and BLEU/entailment quality requirements.

**Failure signatures:**
- High violation rate despite masking: Indicates clause retrieval is missing relevant provisions, or masks are incorrectly implemented.
- CVaR degradation without return improvement: Suggests the policy is becoming more conservative without learning better risk selection.
- Low entailment accuracy (<80%): Justification generator is producing hallucinated or unsupported claims.
- Training instability after dual variable updates: Lagrangian learning rate may be too aggressive.

**First 3 experiments:**
1. Retrieval ablation: Run ClauseLens-RL with random clause retrieval vs. semantic retrieval to quantify the marginal contribution of relevant legal context.
2. Mask vs. penalty isolation: Compare three conditions—(a) masking only, (b) Lagrangian penalties only, (c) both—to determine the source of violation reduction.
3. Tail-distribution stress test: Evaluate trained policies on out-of-distribution catastrophe scenarios to assess CVaR optimization generalization.

## Open Questions the Paper Calls Out

### Open Question 1
Can retriever–policy co-optimization improve clause relevance and quoting performance while preserving auditability, or does frozen retrieval remain necessary for regulatory traceability? Future work will co-optimize retrieval and policy layers for end-to-end alignment.

### Open Question 2
How does ClauseLens performance degrade when cedents behave strategically—adjusting retention levels, requesting adversarial treaty structures, or responding to the reinsurer's quoting patterns over multiple episodes? Future work will extend the simulator to multi-agent and game-theoretic settings.

### Open Question 3
Does ClauseLens generalize to jurisdictions beyond Solvency II and NAIC RBC (e.g., APRA, MAS, PRA, IAIS) without substantial corpus expansion or architectural modification? Cross-jurisdictional generalization is unstated and requires evaluation.

### Open Question 4
How do generated justifications influence human compliance officer decisions—do they improve oversight efficiency, introduce uncritical reliance, or require calibration to officer expertise levels? Future research will integrate human-in-the-loop oversight.

## Limitations
- Evaluation relies entirely on simulator calibrated to historical hurricane data, leaving open whether performance transfers to real-world portfolios
- Clause ambiguity and overlapping provisions account for 70% of residual violations, suggesting current masking logic may over-constrain
- Frozen retriever and justification module prevent end-to-end optimization that might improve downstream decision quality

## Confidence
- **High**: Core mechanism combining clause-grounded state representations with CVaR optimization (statistically significant violation reduction and tail-risk improvement)
- **Medium**: Explanation module's accuracy (BLEU/ROUGE metrics alone don't guarantee regulatory auditability in edge cases)
- **Low**: Real-world deployment readiness (simulator-only evaluation, lack of external validation)

## Next Checks
1. Evaluate ClauseLens on simulated scenarios with 2× historical severity and novel event types to assess CVaR optimization generalization beyond hurricane-calibrated losses.
2. Have insurance domain experts review 100 randomly sampled clause-grounded explanations to verify regulatory accuracy and identify patterns of misinterpretation or hallucination.
3. Run ClauseLens in a live simulation with dynamically updating clause corpora to measure adaptation speed and identify failure modes when retrieval precision drops below 80%.