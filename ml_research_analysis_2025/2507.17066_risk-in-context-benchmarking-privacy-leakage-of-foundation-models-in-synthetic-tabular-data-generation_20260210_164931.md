---
ver: rpa2
title: 'Risk In Context: Benchmarking Privacy Leakage of Foundation Models in Synthetic
  Tabular Data Generation'
arxiv_id: '2507.17066'
source_url: https://arxiv.org/abs/2507.17066
tags:
- data
- privacy
- synthetic
- tabular
- leakage
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work benchmarks three foundation-model in-context learning
  (ICL) generators (GPT-4o-mini, LLaMA 3.3 70B, TabPFN v2) against four deep-learning
  baselines on 35 real-world tabular datasets. The study finds that foundation models
  consistently exhibit the highest membership-inference privacy leakage, with LLaMA
  3.3 70B achieving up to 54 percentage points higher true-positive rate at 1% false-positive
  rate than the safest baseline.
---

# Risk In Context: Benchmarking Privacy Leakage of Foundation Models in Synthetic Tabular Data Generation

## Quick Facts
- **arXiv ID:** 2507.17066
- **Source URL:** https://arxiv.org/abs/2507.17066
- **Reference count:** 40
- **Primary result:** Foundation models (LLaMA 3.3 70B) exhibit up to 54 percentage points higher membership-inference privacy leakage than deep-learning baselines when generating synthetic tabular data from small samples.

## Executive Summary
This study benchmarks three foundation-model in-context learning (ICL) generators (GPT-4o-mini, LLaMA 3.3 70B, TabPFN v2) against four deep-learning baselines on 35 real-world tabular datasets. The research finds that foundation models consistently exhibit the highest membership-inference privacy leakage, with LLaMA 3.3 70B achieving up to 54 percentage points higher true-positive rate at 1% false-positive rate than the safest baseline. A factorial study reveals that simple prompt-level mitigations—small batch size, low temperature, and inclusion of summary statistics—can reduce worst-case AUC by up to 14 points and rare-class leakage by up to 39 points while preserving over 90% of baseline fidelity. The results provide an actionable blueprint for safer low-data tabular synthesis using foundation models.

## Method Summary
The study evaluates synthetic tabular data generation using the OpenML CTR23 benchmark suite (35 datasets) with subsamples of 32, 64, and 128 rows. Three ICL generators (GPT-4o-mini, LLaMA 3.3 70B via Groq API, TabPFN v2) are compared against four deep-learning baselines (CTGAN, TVAE, TabDiff, SMOTE). Privacy is measured using membership-inference attacks (13 variants via Synth-MIA library) with max AUC as the primary metric, while utility is assessed through downstream classifier ROC AUC and fidelity metrics. A factorial study examines temperature (0.1, 0.5, 1.0), batch size (10 vs 32), and summary statistics inclusion as privacy-mitigation strategies.

## Key Results
- Foundation models show 10-54 percentage points higher membership-inference AUC than deep-learning baselines across all datasets and sample sizes
- LLaMA 3.3 70B exhibits the highest leakage (AUC up to 0.667), while GPT-4o-mini provides the best privacy-utility tradeoff
- Prompt-level mitigations reduce worst-case AUC by up to 14 points and rare-class leakage by up to 39 points while preserving over 90% of baseline fidelity
- Privacy improvements primarily result from suppressing low-frequency events rather than eliminating common pattern copying

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** ICL presents distinct privacy risk compared to trained generators because the model conditions directly on seed rows, leading to verbatim reproduction or strong statistical mimicry of specific individuals.
- **Mechanism:** Unlike GANs or VAEs which learn distributions via parameter updates, ICL requires specific data rows in the prompt window. The transformer attends to these exact tokens during generation, creating a direct path for memorization or exact replication of quasi-identifiers present in few-shot examples.
- **Core assumption:** Attackers can detect the statistical signature of seed rows in synthetic output more easily than extracting weights from trained deep models.
- **Evidence anchors:** Abstract states "ICL repeats seed rows verbatim... introducing a new privacy risk"; section 1 notes "freely reproduce these seed rows token-by-token"; related work "When Tables Leak" confirms string memorization as key attack vector.

### Mechanism 2
- **Claim:** Reducing generation diversity (lower temperature or smaller batch sizes) decreases privacy leakage by suppressing emission of rare or unique values that act as strong membership signals.
- **Mechanism:** Membership Inference Attacks rely on outlier detection—if synthetic data contains rare combinations found in real data, the attacker infers that record was in the training set. Lowering temperature concentrates probability mass on high-frequency values, censoring unique outliers that make MIAs successful.
- **Core assumption:** Primary MIA signal is presence of rare co-occurrences rather than exact replication of common rows.
- **Evidence anchors:** Section 4.4 shows "privacy improvements arise primarily from suppression of low-frequency events"; "visualizing diversity collapse" demonstrates rare bins become underrepresented; "Uniqueness ratio as a predictor of privacy leakage" supports uniqueness as primary predictor.

### Mechanism 3
- **Claim:** Summary statistics in prompts act as "soft regularizer," anchoring the model to population-level trends rather than over-indexing on specific idiosyncrasies of small seed samples.
- **Mechanism:** Providing global means, variances, or categorical distributions gives the model a macro view to balance against the micro view of few-shot rows. This reduces need to slavishly copy specific noise of seed rows to maintain fidelity, thereby reducing leakage signal.
- **Core assumption:** Foundation model can balance instruction following with provided statistical constraints.
- **Evidence anchors:** Section 4.4 states "summary statistics act as a soft regularizer anchoring the model to population-level trends"; removing stats increased leakage to 0.878; "Synthetic Tabular Data Generation: A Comparative Survey" likely discusses summary statistics' role in constraining fidelity.

## Foundational Learning

- **Concept: Membership Inference Attacks (MIA)**
  - **Why needed here:** Primary metric for "privacy leakage" in the paper. You must understand that MIA is essentially a classification game: Can an adversary look at a synthetic row and correctly guess "Yes/No, this was in the training set?"
  - **Quick check question:** If a generator produces synthetic data that is statistically identical to the population but contains no actual rows from the training set, what would the True Positive Rate (TPR) of a perfect MIA attacker be? (Answer: 0%, they can't distinguish because the sets are disjoint and distributionally identical).

- **Concept: In-Context Learning (ICL) vs. Fine-Tuning**
  - **Why needed here:** Paper distinguishes between "data-specific generators" (trained) and "foundation models" (frozen). You need to understand that ICL uses the input window (prompt) as temporary "memory" for the data, which creates the specific leakage mechanism explored here.
  - **Quick check question:** Does ICL modify the model weights? (Answer: No, it modifies the attention patterns during inference based on the prompt).

- **Concept: Quasi-Identifiers**
  - **Why needed here:** Paper highlights that tabular data is risky because a single row (e.g., [Age: 90, Zip: 90210, Disease: Rare]) can identify a person. This is why verbatim repetition in tabular data is more dangerous than in text generation.
  - **Quick check question:** Why is privacy leakage potentially higher for a row with a unique combination of attributes than for a row with common attributes? (Answer: The unique combination acts as a strong "fingerprint" for membership inference).

## Architecture Onboarding

- **Component map:** Generators (3 ICL Agents + 4 Trainable Baselines) -> Prompt Interface (Schema + Summary Statistics + CSV Sample) -> Auditing System (Synth-MIA library implementing 13 attacks) -> Evaluation Loop (Fidelity metrics vs Privacy metrics)

- **Critical path:**
  1. **Subsampling:** Draw $n=32, 64, 128$ rows from real data (simulating low-data)
  2. **Prompting (ICL):** Serialize rows + stats into JSON/CSV prompt
  3. **Generation:** Call API/Model to generate $m$ synthetic rows
  4. **Auditing:** Run 13 MIA attacks on synthetic set $S$ using hold-out reference set $R$
  5. **Scoring:** Report Max AUC across all attackers (worst-case scenario)

- **Design tradeoffs:**
  - **Privacy vs. Utility:** Improving one typically degrades the other. "Safe" setting (low temp, small batch) cuts leakage but destroys correlation fidelity
  - **Convenience vs. Security:** LLaMA 3.3 70B is open-source (good for control) but showed highest leakage risk (0.667 AUC). GPT-4o-mini offers better tradeoff but is a black box
  - **Batch Size:** Large batches provide better context for correlation modeling but increase attack surface (more seeds to leak)

- **Failure signatures:**
  - **High Leakage:** High TPR@FPR=0.01 indicates model emitting rows very close to original seeds
  - **Diversity Collapse:** If correlation similarity drops significantly while marginal shape stays high, model is likely generating "safe" average rows due to low temperature
  - **Parsing Errors:** LLaMA 3.3 70B occasionally failed on token limits or JSON formatting (Appendix A)

- **First 3 experiments:**
  1. **Establish the Baseline:** Run default prompt (batch=32, temp=1.0) on single dataset and compare Max Attack AUC of GPT-4o-mini vs. CTGAN. Verify FM leaks more.
  2. **Stress Test the "Knobs":** Ablate temperature (0.1 vs 1.0) and measure change in "Rare-Class AUC." Confirm leakage drops primarily in rare classes.
  3. **Verify the Regularizer:** Remove summary statistics from prompt and observe if Max Attack AUC increases (leakage worsens) as predicted by factorial study.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Do attribute inference, reconstruction, and linkage attacks pose comparable or greater risks than membership inference for ICL-based tabular synthesis?
- **Basis in paper:** [explicit] Authors state: "We focus solely on membership-inference as the privacy metric; other risks (attribute inference, reconstruction attacks, or linkage to auxiliary data) remain unquantified."
- **Why unresolved:** Benchmark only measured membership inference; other attack vectors were outside scope.
- **What evidence would resolve it:** Extend benchmark to include attribute inference and reconstruction attacks on same 35 datasets and generator configurations.

### Open Question 2
- **Question:** Can formal differential privacy guarantees be integrated with foundation-model ICL synthesis while maintaining competitive fidelity and utility?
- **Basis in paper:** [explicit] Authors call for "integrating formal differential-privacy guarantees" as crucial future work; current mitigation relies only on prompt-level heuristics without formal guarantees.
- **Why unresolved:** DP integration with frozen in-context models is non-trivial and unexplored in this setting.
- **What evidence would resolve it:** Develop DP-aware prompting or sampling mechanisms and measure privacy-utility frontiers against current baseline.

### Open Question 3
- **Question:** What accounts for large disparity in leakage across foundation models—architecture, prompt format, or pre-training corpus?
- **Basis in paper:** [explicit] Authors note that "model architecture, prompt format, or pre-training corpus strongly influences membership exposure" and call for "per-model audits rather than blanket assumptions."
- **Why unresolved:** Benchmark compared models holistically but did not isolate causal factors.
- **What evidence would resolve it:** Controlled ablations holding prompt format constant across models, or varying prompts within single model.

### Open Question 4
- **Question:** Can rare-class privacy be protected without suppressing long-tail statistical signal and diversity?
- **Basis in paper:** [inferred] Factorial study finds privacy gains stem "almost entirely from suppressing rare-value emission," causing "fundamental tension between protecting outliers and preserving their statistical signal."
- **Why unresolved:** Current mitigations trade diversity for privacy; no method disentangles the two.
- **What evidence would resolve it:** Develop synthesis methods that add calibrated noise to rare values while preserving their marginal and joint distributions.

## Limitations

- The study focuses exclusively on membership-inference attacks, leaving other privacy risks (attribute inference, reconstruction, linkage) unexplored
- Results are based on 35 OpenML CTR23 datasets, which may not represent all tabular data domains (e.g., high-dimensional genomics, highly imbalanced fraud detection)
- Privacy metrics assume strong attack capabilities (shadow-box or black-box access), which may overestimate real-world risks

## Confidence

- **High Confidence:** Foundation models (especially LLaMA 3.3 70B) consistently exhibit higher privacy leakage than trained baselines across all datasets and sample sizes
- **Medium Confidence:** Identified mitigation strategies (low temperature, small batch size, summary stats) reliably reduce leakage while preserving utility, though effectiveness may vary by dataset characteristics
- **Low Confidence:** Exact mechanism by which summary statistics reduce leakage (soft regularization vs. other factors) remains somewhat speculative without ablation studies isolating this effect

## Next Checks

1. **Real-World Attack Simulation:** Replicate the study using weaker attack assumptions (e.g., unknown seed rows, noisy synthetic outputs) to assess practical privacy risks
2. **Domain Transfer Test:** Apply the same benchmarking framework to non-CTR23 datasets (e.g., medical imaging metadata, financial transaction logs) to validate generalizability
3. **Longitudinal Monitoring:** Track privacy leakage over time as foundation models are updated or fine-tuned, as leakage patterns may shift with model changes