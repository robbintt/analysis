---
ver: rpa2
title: 'Learning to Refine: An Agentic RL Approach for Iterative SPARQL Query Construction'
arxiv_id: '2511.11770'
source_url: https://arxiv.org/abs/2511.11770
tags:
- agent
- query
- arxiv
- policy
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents an agentic RL approach for iterative SPARQL
  query construction in multi-hop KGQA. The core idea is to train a compact LLM via
  outcome-driven reinforcement learning to iteratively refine SPARQL queries based
  on execution feedback, enabling dynamic error recovery and query adaptation.
---

# Learning to Refine: An Agentic RL Approach for Iterative SPARQL Query Construction

## Quick Facts
- arXiv ID: 2511.11770
- Source URL: https://arxiv.org/abs/2511.11770
- Reference count: 40
- Achieves 49.7% accuracy post-entity-linking with 81.0% executability on LC-QuAD 2.0

## Executive Summary
This paper introduces an agentic reinforcement learning approach for iterative SPARQL query construction in multi-hop knowledge graph question answering. The core innovation is training a compact language model to iteratively refine queries based on execution feedback, rather than relying on one-shot generation. The agent learns to interpret knowledge graph feedback and generate structured queries within a think-act-observe loop, enabling dynamic error recovery and query adaptation. On the LC-QuAD 2.0 dataset, this RL-tuned agent significantly outperforms existing iterative baselines, demonstrating that outcome-driven learning can substantially improve query generation quality.

## Method Summary
The approach employs a think-act-observe loop where a language model agent iteratively refines SPARQL queries based on execution feedback from a knowledge graph. The agent is trained using reinforcement learning with a reward function that considers both executability and correctness. The RL policy is trained on synthetic data generated through bootstrapping, where successful queries are used to create new training examples. The system operates in two modes: a deliberative mode that explicitly reasons about feedback before generating queries, and a direct mode that acts on feedback immediately. The compact LLM is fine-tuned to generate structured SPARQL queries while learning to interpret various types of KG feedback to guide query refinement.

## Key Results
- Achieves 49.7% accuracy post-entity-linking on LC-QuAD 2.0
- 17.5 percentage point improvement over strongest iterative baseline
- 81.0% executability rate on the curated dataset
- Explicit reasoning (deliberative mode) enhances policy precision, though RL alone is primary performance driver

## Why This Works (Mechanism)
The approach works by transforming SPARQL query generation from a static one-shot task into an iterative refinement process guided by execution feedback. By training the agent through reinforcement learning to interpret KG feedback and adapt queries accordingly, the system can recover from errors and handle complex multi-hop reasoning that would be challenging for static approaches. The outcome-driven training enables the agent to learn effective strategies for query refinement that are difficult to capture through supervised learning alone.

## Foundational Learning

**SPARQL Query Construction**: Understanding SPARQL syntax and structure for querying RDF knowledge graphs. *Why needed*: The agent must generate valid, executable queries. *Quick check*: Can the model generate syntactically correct SPARQL for simple triple patterns?

**Reinforcement Learning for Structured Output**: Applying RL to train models that generate structured outputs like code or queries. *Why needed*: Enables iterative refinement based on execution feedback rather than just supervised learning. *Quick check*: Does the reward function properly capture both executability and correctness?

**Knowledge Graph Feedback Interpretation**: Learning to understand and act on various types of feedback from KG query execution (e.g., entity existence, property validity). *Why needed*: The agent needs to interpret feedback to guide query refinement. *Quick check*: Can the agent correctly identify and respond to different feedback types?

## Architecture Onboarding

**Component Map**: User Question -> Entity Linking -> Initial Query Generation -> Execution Engine -> Feedback Processor -> RL Agent -> Refined Query -> Execution Engine (loop)

**Critical Path**: Question -> Entity Linking -> Initial Query -> Execution -> Feedback -> RL Agent Refinement -> Final Query

**Design Tradeoffs**: The paper chooses a compact LLM over larger models for efficiency, balances deliberative vs direct modes for reasoning depth, and uses synthetic data bootstrapping for training scalability. The RL approach trades off immediate performance for long-term adaptability.

**Failure Signatures**: Poor entity linking leading to cascading query failures, inability to recover from certain error types in the feedback loop, and potential overfitting to synthetic training data patterns.

**3 First Experiments**:
1. Test basic SPARQL generation on simple single-hop questions
2. Evaluate entity linking accuracy on the dataset
3. Measure executability of initial queries before RL refinement

## Open Questions the Paper Calls Out

## Limitations
- Performance still limited by entity linking accuracy (measured post-entity-linking)
- May not generalize well to knowledge graphs with significantly different schema
- Computational overhead of iterative refinement compared to one-shot generation

## Confidence
- Accuracy improvement claim: High
- RL methodology validity: High  
- Comparative baseline fairness: Medium
- Generalizability claims: Low

## Next Checks
1. Validate entity linking accuracy independently to confirm post-entity-linking metrics
2. Test the agent on a different knowledge graph schema to assess generalizability
3. Measure computational overhead and latency compared to one-shot approaches