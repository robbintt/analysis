---
ver: rpa2
title: Diffusion-Based Generation and Imputation of Driving Scenarios from Limited
  Vehicle CAN Data
arxiv_id: '2509.12375'
source_url: https://arxiv.org/abs/2509.12375
tags:
- data
- training
- time
- diffusion
- future
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the application of diffusion models for generating
  and imputing realistic synthetic vehicle CAN data, addressing the challenge of training
  on small, long-range time series datasets. The authors propose a hybrid autoregressive/non-autoregressive
  approach that segments laps into overlapping windows and uses diffusion models to
  generate realistic future windows conditioned on past data.
---

# Diffusion-Based Generation and Imputation of Driving Scenarios from Limited Vehicle CAN Data

## Quick Facts
- **arXiv ID**: 2509.12375
- **Source URL**: https://arxiv.org/abs/2509.12375
- **Reference count**: 14
- **Key outcome**: SSSD Mamba architecture outperforms training data in physical correctness while imputing implausible regions in automotive CAN datasets

## Executive Summary
This paper addresses the challenge of generating and imputing realistic synthetic vehicle CAN data using diffusion models, particularly when training on small, long-range time series datasets. The authors propose a hybrid autoregressive/non-autoregressive approach that segments laps into overlapping windows and uses diffusion models to generate realistic future windows conditioned on past data. Their method demonstrates that diffusion models can effectively handle long, limited-sample time series and improve automotive dataset quality by imputing physically implausible regions while maintaining plausible driving behavior.

## Method Summary
The method segments full laps (12,554 points) into smaller windows (1024 points) with a stride, dividing each into past and future halves for training. The model learns to predict the future half conditioned on the past half, treating each window as an independent training sample. During generation, multiple candidates are produced and the one with the best physical score (MSE acceleration) is selected to mitigate error accumulation. Domain-specific improvements include physically correct padding (PCSP) that computes torques for constant speed instead of zero-padding, and sinusoidal track position embeddings to prevent learning implausible stationary states. The core architecture uses SSSD Mamba (S4 layers replaced with Mamba blocks) processing the sequence conditioned on vehicle parameters, elevation profile, and track position.

## Key Results
- SSSD Mamba outperforms training data in physical correctness (MSE acc95) while maintaining plausible driving behavior
- The model successfully imputes physically implausible regions in training data, improving overall data quality
- Without the best-of-16 candidate selection mechanism, models like LongConv diverge after generating a few windows
- Boundary discontinuities remain a challenge despite the improvements

## Why This Works (Mechanism)

### Mechanism 1
Segmenting long time series into overlapping windows enables effective training on limited sample counts by artificially increasing the dataset size. The model learns to predict the future half of a window conditioned on the past half, treating each window as an independent training sample. Local physics and driving behaviors learned in short windows are sufficient to reconstruct global trajectory coherence when chained autoregressively.

### Mechanism 2
Generating multiple candidates and selecting the one with the best physical score mitigates error accumulation during autoregressive rollout. Instead of a single prediction, the diffusion model generates 16 candidate future windows and selects the optimal one using a physics-based metric. This ensures at least one candidate adheres to physical laws (torque-acceleration relationship).

### Mechanism 3
Domain-specific inductive biases stabilize training on heterogeneous vehicle data. The authors replace standard zero-padding with physics-aware padding (computing torques for constant speed) and use sinusoidal embeddings for track position, preventing the model from learning implausible "stationary" states at the start of sequences. These structural hints are necessary since the model cannot infer physics solely from raw CAN data.

## Foundational Learning

- **Concept: Denoising Diffusion Probabilistic Models (DDPMs) for Time Series**
  - Why needed: The core engine of the paper is a diffusion model that learns to reverse a noise process, conditioned on past vehicle states.
  - Quick check: Can you explain how a model predicts "noise" rather than the direct future value, and how removing this noise iteratively generates a trajectory?

- **Concept: State Space Models (SSMs) - S4 & Mamba**
  - Why needed: The paper evaluates LongConv and SSSD (S4 and Mamba variants). Understanding how SSMs handle long-range dependencies efficiently is key to understanding the architecture choice.
  - Quick check: How does the "selection mechanism" in Mamba differ from standard S4 layers in terms of filtering relevant history?

- **Concept: Vehicle Dynamics (Torque vs. Acceleration)**
  - Why needed: The paper's primary evaluation metric is physical correctness (MSE acc), derived from Newton's second law (F=ma).
  - Quick check: If a vehicle is braking, why might the torque sensors fail to capture the full physical picture, rendering the MSE acc metric unreliable in those specific regions?

## Architecture Onboarding

- **Component map**: Raw CAN data -> PCSP Padding -> Windowing (1024 points, stride 512) -> SSSD Mamba Backbone -> Noise Prediction for future half -> Best-of-16 Selection -> Autoregressive Chaining

- **Critical path**:
  1. Apply Physically Correct Same Padding (PCSP) to raw data
  2. Create overlapping windows (stride 512)
  3. Train SSSD Mamba to denoise future window conditioned on past window and track data
  4. Autoregressively sample 16 candidates, select best via physics metric

- **Design tradeoffs**:
  - Naive vs. RePaint Imputation: RePaint harmonizes boundaries better but has slightly worse physical correctness scores than the naive approach
  - Diffusion Steps: Using only 12-16 steps yields the best physics, while more steps (500) do not necessarily improve quality for this specific architecture

- **Failure signatures**:
  - Boundary Discontinuities: Abrupt changes in speed or torque at edges of imputed regions
  - Trajectory Drift: Generating plausible physics but unrealistic driving behavior (e.g., leaving the track)
  - Divergence: Models drift off-track or generate unrealistic speeds without candidate selection

- **First 3 experiments**:
  1. Padding Ablation: Train on zero-padded data vs. PCSP to isolate impact of physical initialization on convergence speed
  2. Architecture Swap: Benchmark SSSD S4 vs. SSSD Mamba on fixed window prediction task to measure inference latency and MSE acc
  3. Imputation Schedules: Implement "Naive" reverse schedule on corrupted validation set and measure boundary discontinuity against ground truth

## Open Questions the Paper Calls Out

### Open Question 1
Can the hybrid diffusion approach maintain high physical correctness and trajectory adherence when applied to diverse, complex tracks and varied vehicle dynamics? The current study is constrained to a dataset where vehicles drive the exact same pre-defined test track, and future work must show how the approach generalizes to more diverse and complex datasets and vehicle types.

### Open Question 2
How can boundary discontinuities in imputed regions be eliminated without compromising the physical correctness of the synthetic data? There is a trade-off between the naive imputation schedule (better physics) and RePaint (better harmonization), and neither method fully resolved the boundary artifacts.

### Open Question 3
Does the synthetic data generated or imputed by diffusion models provide measurable improvements in performance for downstream tasks like motion planning? The evaluation is limited to intrinsic metrics and does not test the data's utility in training a controller or planning algorithm.

## Limitations
- Boundary discontinuities remain a significant challenge despite improvements
- Confidential dataset limits independent verification and adaptation to other domains
- The best-of-16 candidate selection mechanism's effectiveness depends on correlation between physical plausibility and realistic driving behavior

## Confidence

**High Confidence**: Core methodology (diffusion-based autoregressive approach) is technically sound and represents reasonable domain-specific enhancement for automotive data.

**Medium Confidence**: Physical correctness improvements are clearly demonstrated, but trajectory adherence metric is less conclusive and may reflect evaluation methodology differences rather than true quality improvements.

**Low Confidence**: Broader applicability without modifications is questionable due to window-based approach limitations and confidential dataset constraint limiting independent verification.

## Next Checks

1. **Boundary Discontinuity Quantification**: Implement RePaint-style resampling on the same validation set and compare boundary discontinuity metrics (e.g., speed/torque jumps at window boundaries) against the naive approach to quantify the tradeoff between harmonization and physical correctness.

2. **Architecture Ablation with Controlled Data**: Create a synthetic CAN dataset with known ground truth (e.g., sinusoidal speed profiles with added noise) to systematically compare SSSD S4 vs SSSD Mamba performance, isolating architectural effects from dataset-specific factors.

3. **Cross-Dataset Transfer**: Apply the trained model to a different automotive dataset (e.g., public NGSIM data or a different vehicle platform) to assess whether the domain-specific improvements generalize or require retraining for each vehicle type.