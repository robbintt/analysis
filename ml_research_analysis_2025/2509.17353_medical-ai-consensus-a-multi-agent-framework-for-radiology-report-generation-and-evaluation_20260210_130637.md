---
ver: rpa2
title: 'Medical AI Consensus: A Multi-Agent Framework for Radiology Report Generation
  and Evaluation'
arxiv_id: '2509.17353'
source_url: https://arxiv.org/abs/2509.17353
tags:
- agent
- evaluation
- report
- radiology
- clinical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduces a multi-agent reinforcement learning framework
  designed to address the dual challenge of automating radiology report generation
  and establishing rigorous evaluation protocols. The framework integrates large language
  models (LLMs) and large vision models (LVMs) into a modular architecture with ten
  specialized agents that handle image analysis, feature extraction, report generation,
  review, and evaluation.
---

# Medical AI Consensus: A Multi-Agent Framework for Radiology Report Generation and Evaluation

## Quick Facts
- arXiv ID: 2509.17353
- Source URL: https://arxiv.org/abs/2509.17353
- Reference count: 12
- Primary result: Achieved 68.6% accuracy in generating radiology reports without patient metadata using a 10-agent multi-agent framework

## Executive Summary
This paper introduces a multi-agent reinforcement learning framework for automating radiology report generation while establishing rigorous evaluation protocols. The framework integrates large language models (LLMs) and large vision models (LVMs) into a modular architecture with ten specialized agents handling image analysis, feature extraction, report generation, review, and evaluation. A case study using the RHUH-GBM dataset demonstrated the framework's effectiveness, achieving clinically sound reports with an overall accuracy of 68.6%. The approach advances trust in generative AI systems by providing transparent, reproducible, and clinically relevant evaluation.

## Method Summary
The framework employs a modular architecture of ten specialized agents coordinated by an orchestrator: Anatomical Region Detection, Modality Classifier, Modality Interpreters (organ-specific), Clinical Context Processor, Quantitative Segmentation, Diagnostic Classifier, Clinical Report Composer, Quality Assurance Agent, Evaluation Agent/Judge, and Orchestrator. Each agent handles discrete subtasks in the radiology reporting workflow. The system uses GPT-4o as the backbone LLM and employs an LLM-as-judge for automated evaluation across correctness, conciseness, completeness, and image description quality. Evaluation combines agent-level metrics (accuracy, F1, Dice, IoU) with global measures including a composite Report Quality Score and human-in-the-loop review.

## Key Results
- Achieved 68.6% overall accuracy in generating clinically sound radiology reports without patient metadata
- Demonstrated effective multi-agent orchestration with task-specific metric optimization
- Validated LLM-as-judge evaluation methodology with correlation to radiologist assessments
- Established framework for both automated report generation and rigorous evaluation

## Why This Works (Mechanism)

### Mechanism 1: Specialized Agent Decomposition with Task-Specific Metrics
- Claim: Decomposing radiology workflows into specialized agents enables modular optimization and granular evaluation that monolithic models cannot achieve.
- Mechanism: Each of the ten agents handles a discrete subtask (modality classification, feature extraction, segmentation, report composition, etc.). The orchestrator invokes agents conditionally based on intermediate outputs. Agent-level metrics (accuracy, F1, Dice, IoU) provide targeted feedback while global metrics (composite Report Quality Score) assess end-to-end coherence.
- Core assumption: Radiology report generation can be meaningfully decomposed into independent subtasks where local optimization translates to global improvement.
- Evidence anchors: [abstract] "modular architecture composed of ten specialized agents responsible for image analysis, feature extraction, report generation, review, and evaluation"; [section 2] "Each agent is assessed through task-specific metrics, while overall framework performance is evaluated via composite measures of report accuracy, clinical completeness, and human-in-the-loop review"

### Mechanism 2: LLM-as-Judge Evaluation Calibrated to Clinical Ground Truth
- Claim: Automated LLM evaluation can approximate expert radiologist assessment at scale when calibrated against clinician-annotated references.
- Mechanism: An independent Evaluation Agent (Judge) scores reports across four dimensions: correctness, conciseness, completeness, and image description quality. Calibration occurs via correlation with senior radiologist-authored reference reports using Spearman's ρ. The judge can also serve as a reward model in reinforcement learning loops.
- Core assumption: LLM-based evaluators achieve inter-rater reliability comparable to human reviewers for structured clinical evaluation.
- Evidence anchors: [abstract] "LLMs act as evaluators alongside medical radiologist feedback"; [section 2] "leveraging recent evidence that LLM-based evaluators can achieve inter-rater reliability comparable to human reviewers"

### Mechanism 3: Human-in-the-Loop Quality Assurance with Iterative Refinement
- Claim: Multi-stage review combining automated cross-validation with expert feedback improves report reliability and enables continuous system improvement.
- Mechanism: The Quality Assurance Agent cross-validates generated reports against source images and intermediate agent outputs to detect inconsistencies or omissions. It can either consult expert radiologists during review or incorporate radiologist annotations. This feedback integrates into RLHF-style loops for iterative pipeline optimization.
- Core assumption: Structured human feedback, even if sparse, can guide reinforcement learning toward clinically aligned outputs.
- Evidence anchors: [section 2] "The human-in-the-loop design ensures that the quality assurance agent can either consult an expert radiologist...thereby aligning automated reporting with expert clinical judgment"

## Foundational Learning

### Concept: Multi-Agent System (MAS) Orchestration Patterns
- Why needed here: The framework relies on conditional agent invocation—understanding when the orchestrator calls specific agents (e.g., Quantitative Segmentation only when abnormalities are detected) is essential for debugging pipeline flow.
- Quick check question: Given a brain MRI with no detected abnormalities, which agents does the orchestrator skip?

### Concept: Clinical Evaluation Metrics Beyond NLP Overlap
- Why needed here: The paper explicitly critiques traditional metrics like ROUGE for failing to capture clinical accuracy. Understanding CheXpert-based accuracy, factual correctness, and uncertainty representation is required to interpret results.
- Quick check question: Why would a report with high ROUGE score still receive low clinical accuracy rating?

### Concept: Vision-Language Model (VLM) Output Modalities
- Why needed here: The Quantitative Segmentation Agent produces structured outputs (masks, bounding boxes, measurements) that must integrate with text-based report generation.
- Quick check question: How does the Clinical Report Composer incorporate quantitative measurements from the segmentation agent into natural language output?

## Architecture Onboarding

### Component Map:
Input Image → Anatomical Region Detection → Modality Classifier → Modality Interpreter → Clinical Context Processor → Quantitative Segmentation (conditional) → Diagnostic Classifier → Clinical Report Composer → Quality Assurance Agent → Final Report → Evaluation Agent/Judge

### Critical Path:
1. Image preprocessing and modality identification (Anatomical Detection → Modality Classifier)
2. Feature extraction and interpretation (Modality Interpreter + Clinical Context)
3. Conditional quantification (Segmentation, if abnormality detected)
4. Synthesis and output (Diagnostic Classifier → Report Composer → QA → Final Report)

### Design Tradeoffs:
- Model-agnostic flexibility vs. task-specific optimization: Standardized interfaces allow model swapping but may limit fine-tuned performance on individual subtasks
- Evaluation independence vs. RL feedback: The Judge must remain independent for fair benchmarking but also generates reward signals—dual role creates potential conflict of interest
- Latency vs. thoroughness: Ten-agent sequential pipeline maximizes accuracy potential but adds inference overhead unsuitable for real-time deployment without optimization

### Failure Signatures:
- Agent output inconsistency: Orchestrator validation flags when Diagnostic Classifier impression contradicts Modality Interpreter findings
- Metric disconnection: High BLEU/ROUGE with low CheXpert-based accuracy indicates linguistic fluency without clinical correctness
- Metadata inference failure: 68.6% accuracy without patient metadata suggests ~31% of cases require external context; failure mode is incomplete tumor characterization

### First 3 Experiments:
1. Per-agent baseline evaluation: Run each agent independently on RHUH-GBM validation split to establish individual accuracy/F1/Dice baselines; identify weakest agents for targeted improvement
2. End-to-end pipeline benchmark: Generate reports for held-out test set; compute composite Report Quality Score via LLM-as-judge; correlate with radiologist annotations using Spearman's ρ
3. Metadata ablation: Compare report quality with vs. without Clinical Context Processor inputs to quantify the contribution of patient metadata to final accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent does incorporating patient metadata via the Clinical Context Processor improve report accuracy compared to the metadata-free baseline?
- Basis in paper: [explicit] The authors note the evaluation was conducted "without incorporating patient metadata such as tumor size or type," emphasizing the system's ability to infer attributes independently, but they do not quantify the added value of the metadata.
- Why unresolved: The specific performance gain provided by the Clinical Context Processor remains unquantified, leaving the trade-off between inference capability and context injection unclear.
- What evidence would resolve it: An ablation study comparing the Report Quality Score on the RHUH-GBM dataset with the Clinical Context Processor enabled versus disabled.

### Open Question 2
- Question: Does actual reinforcement learning fine-tuning using the Evaluation Agent as a reward model yield significant performance improvements over the baseline?
- Basis in paper: [explicit] The paper states, "We further simulate an RLHF loop, where pipeline adjustments are iteratively re-evaluated, yielding expected gains in factual accuracy," indicating the loop was simulated rather than executed.
- Why unresolved: It is currently undetermined if the proposed evaluation metrics can serve as effective reward signals to optimize the agents via reinforcement learning in practice.
- What evidence would resolve it: Results from a live RLHF training run showing convergence curves and final accuracy metrics compared to the static simulation.

### Open Question 3
- Question: How robust is the multi-agent orchestration when applied to the non-brain MRI modalities described in the framework?
- Basis in paper: [inferred] While the framework proposes specialized "Modality Interpreters" for various organs (e.g., Chest X-ray) and modalities (e.g., CT, US), the experimental validation is restricted to the brain MRI-specific RHUH-GBM dataset.
- Why unresolved: The generalizability of the ten-agent pipeline, particularly the Anatomical Region Detection and Quantitative Segmentation agents, remains unproven outside of neuro-oncology imaging.
- What evidence would resolve it: Benchmark results on a diverse dataset like MIMIC-CXR or CheXpert to demonstrate cross-modality stability.

## Limitations
- Limited to single institutional dataset (RHUH-GBM) focused on brain MRI for glioblastoma patients, restricting external validity
- 68.6% accuracy still indicates significant failure rates requiring human review in clinical deployment
- Framework's generalizability to other imaging modalities and clinical contexts remains unproven

## Confidence
- High Confidence: The modular agent architecture design and evaluation methodology are well-specified and reproducible
- Medium Confidence: The LLM-as-judge evaluation calibration against radiologist assessments is reasonable but lacks extensive validation across diverse clinical scenarios
- Low Confidence: Claims about real-world clinical impact and the balance between automated efficiency and human oversight are speculative

## Next Checks
1. Cross-institutional validation: Test the framework on radiology datasets from multiple institutions and imaging modalities to assess generalizability beyond brain MRI and glioblastoma
2. Human-AI agreement study: Conduct formal inter-rater reliability analysis between the LLM-as-judge and panels of radiologists across diverse clinical cases to validate the evaluation methodology
3. Clinical workflow integration pilot: Implement the framework in a controlled clinical setting to measure actual time savings, error rates, and radiologist acceptance compared to traditional reporting workflows