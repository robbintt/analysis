---
ver: rpa2
title: 'From Quotes to Concepts: Axial Coding of Political Debates with Ensemble LMs'
arxiv_id: '2601.15338'
source_url: https://arxiv.org/abs/2601.15338
tags:
- coding
- axial
- categories
- codes
- coverage
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a method for transforming raw utterances
  in political debates into structured conceptual representations via open and axial
  coding with ensemble LLMs. The approach extends a moderator-based open coding stage
  with two axial coding strategies: clustering embeddings (using DBSCAN/HDBSCAN) and
  direct LLM grouping.'
---

# From Quotes to Concepts: Axial Coding of Political Debates with Ensemble LMs

## Quick Facts
- arXiv ID: 2601.15338
- Source URL: https://arxiv.org/abs/2601.15338
- Authors: Angelina Parfenova; David Graus; Juergen Pfeffer
- Reference count: 39
- Primary result: Introduces ensemble LLM-based method for transforming political debate utterances into structured concepts via open and axial coding, achieving trade-off between coverage and alignment

## Executive Summary
This paper presents a novel method for transforming raw political debate transcripts into structured conceptual representations using ensemble language models. The approach extends moderator-based open coding with two axial coding strategies: density-based clustering (HDBSCAN) and direct LLM grouping (Deepseek-R1). Evaluated on Dutch parliamentary debates, the method demonstrates a fundamental trade-off: clustering maximizes coverage (96%) while LLM grouping produces more interpretable categories with higher alignment to human taxonomies. The authors propose a hybrid pipeline combining both approaches to balance these competing objectives.

## Method Summary
The method operates in two stages: open coding and axial coding. In open coding, multiple fine-tuned LLMs generate candidate labels for utterances, with a moderator LLM arbitrating between candidates and a similarity threshold (τ=0.7) preventing label redundancy. For axial coding, the method offers two paths: (1) embedding-based clustering using HDBSCAN on UMAP-reduced BGE embeddings, or (2) direct LLM grouping where Deepseek-R1 processes batches of code-utterance pairs to generate category mappings without forcing assignments. The approach is evaluated on Dutch parliamentary debates using ROUGE-L and BERTScore against human-generated taxonomies, plus coverage and divergence metrics.

## Key Results
- HDBSCAN with BGE embeddings achieves 96% coverage with moderate external alignment (ROUGE-L 0.111, BERTScore 0.867)
- Deepseek-R1 LLM grouping attains higher fine-grained alignment (ROUGE-L 0.248) but lower coverage (~20%)
- The hybrid pipeline combining clustering and LLM labeling offers the most balanced trade-off
- Moderator-based ensemble open coding produces more consistent and stable code assignments than single-model approaches

## Why This Works (Mechanism)

### Mechanism 1: Moderator-based Code Stabilization
An ensemble of specialized LLMs overseen by a moderator model produces more stable and consistent open codes than a single model by reducing individual annotator bias. Multiple fine-tuned LLMs generate candidate labels independently, with the moderator reviewing candidates against original text to select or synthesize the best fit, followed by cosine similarity checking (τ=0.7) to reuse existing labels and prevent redundancy.

### Mechanism 2: Embedding-based Structural Aggregation
Density-based clustering on embeddings (HDBSCAN) maximizes coverage by identifying dense semantic regions, effectively grouping the majority of utterances while isolating noise. Code-utterance pairs are embedded and dimensionally reduced, with HDBSCAN identifying clusters based on density without pre-defining cluster counts, allowing discovery of categories that align with semantic "grounded theory" concepts.

### Mechanism 3: Generative Semantic Grouping
Direct LLM-based grouping produces more interpretable, fine-grained categories but naturally limits coverage due to an implicit "abstention" behavior where models avoid grouping ambiguous inputs. The LLM processes batches of code-utterance pairs and generates JSON mappings, not forced to assign every item, mirroring human qualitative caution and producing categories with higher semantic fidelity but lower overall coverage.

## Foundational Learning

- **Concept: Open vs. Axial Coding (Grounded Theory)**
  - Why needed here: The paper builds its pipeline on this qualitative distinction; understanding the difference between generating raw labels and organizing them is critical for debugging the system.
  - Quick check question: Can you explain why generating a label "Economy" for a quote (Open Coding) is different from grouping "Economy," "Inflation," and "Jobs" under "Macroeconomics" (Axial Coding)?

- **Concept: Density-based Clustering (HDBSCAN)**
  - Why needed here: The paper identifies HDBSCAN as superior for coverage; understanding how it handles "noise points" is critical to interpreting the 96% coverage metric.
  - Quick check question: How does HDBSCAN determine the number of clusters differently than K-Means, and what happens to data points that don't fit any dense region?

- **Concept: Ensemble LLM Moderation**
  - Why needed here: The quality of final Axial coding depends entirely on Open codes; understanding the moderator's role in arbitrating disputes is key to maintaining system stability.
  - Quick check question: In this architecture, what is the specific role of the moderator model versus the candidate models?

## Architecture Onboarding

- **Component map:** Raw Debate Transcripts -> LoRA-finetuned LLM Ensemble -> Moderator LLM -> Label Refinement (Cosine Sim) -> (Path A: Embedding + UMAP + HDBSCAN -> LLM Labeler) OR (Path B: Batched LLM Grouping -> Category Mapping)
- **Critical path:** The Open Coding Stage (specifically Label Refinement threshold τ=0.7). If open codes are too noisy or generic, neither clustering nor LLM grouping can recover meaningful structure.
- **Design tradeoffs:** Coverage vs. Alignment (choose Path A for ~96% coverage/moderate alignment or Path B for ~20% coverage/high alignment); Stability vs. Novelty (moderator enforces stability but may miss novel nuances).
- **Failure signatures:** High DBSCAN noise rate >90% indicates embedding space too diffuse or eps/min_samples too strict; Generic LLM buckets like "Political Discussion" suggest prompt lacks constraints; Code explosion if τ too strict causes unbounded vocabulary growth.
- **First 3 experiments:**
  1. Unit Test Open Coding: Run Ensemble + Moderator on 50 utterances, verify Label Refinement merges synonyms effectively.
  2. A/B Test Axial Paths: Run HDBSCAN vs. Deepseek-R1 on 500 open codes, compare category counts and "Unassigned" rates.
  3. Hybrid Prototype: Implement suggested hybrid (Cluster first, then LLM-label clusters) on 1k sample to bridge coverage/alignment gap.

## Open Questions the Paper Calls Out
None identified in source material.

## Limitations
- Evaluation relies on single Dutch parliamentary debate dataset, limiting generalizability across political systems and languages
- External alignment metrics compare against human taxonomy but don't capture whether categories reflect underlying semantic structure
- Trade-off between coverage and alignment remains unresolved without definitive guidance on when to prefer one approach
- Open coding threshold (τ=0.7) appears arbitrary without sensitivity analysis showing effects on downstream quality

## Confidence

**High Confidence:** Moderator-based open coding producing more stable codes than single-model approaches; structural distinction between open and axial coding as separate phases.

**Medium Confidence:** HDBSCAN superiority for coverage and Deepseek-R1 for alignment demonstrated empirically but metrics don't fully capture semantic quality; claim that LLM grouping "mirrors human axial coding" through abstention is plausible but not directly validated.

**Low Confidence:** Assertion that hybrid pipeline offers "most balanced trade-off" lacks empirical validation; claim that moderator reduces individual annotator bias assumes moderator itself doesn't introduce systematic bias.

## Next Checks

1. **Cross-dataset Validation:** Apply complete pipeline to political debates from at least two different parliamentary systems (UK Parliament, EU Parliament) to test generalizability; compare category consistency and coverage across datasets.

2. **Human Evaluation Study:** Recruit political science experts to evaluate LLM-generated categories against human-generated categories for semantic fidelity and interpretability; measure inter-rater reliability and compare to reported ROUGE/BERTScore metrics.

3. **Threshold Sensitivity Analysis:** Systematically vary open coding similarity threshold (τ) from 0.5 to 0.9 in increments of 0.1; measure downstream effects on axial coding coverage, category count, and alignment metrics to determine optimal threshold ranges for different use cases.