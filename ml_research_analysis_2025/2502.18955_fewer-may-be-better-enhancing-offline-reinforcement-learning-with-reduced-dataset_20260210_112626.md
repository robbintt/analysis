---
ver: rpa2
title: 'Fewer May Be Better: Enhancing Offline Reinforcement Learning with Reduced
  Dataset'
arxiv_id: '2502.18955'
source_url: https://arxiv.org/abs/2502.18955
tags:
- learning
- offline
- data
- dataset
- selection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of identifying a reduced offline
  dataset that improves offline reinforcement learning (RL) algorithm performance
  while lowering computational costs. The core idea is to formulate dataset selection
  as a gradient approximation optimization problem, proving that the actor-critic
  framework can be transformed into a submodular objective.
---

# Fewer May Be Better: Enhancing Offline Reinforcement Learning with Reduced Dataset

## Quick Facts
- **arXiv ID:** 2502.18955
- **Source URL:** https://arxiv.org/abs/2502.18955
- **Reference count:** 40
- **Primary result:** Reduced dataset selection improves offline RL performance with lower computational costs

## Executive Summary
This paper challenges the conventional wisdom that more data always improves offline reinforcement learning by demonstrating that carefully selected reduced datasets can outperform full datasets. The authors develop ReDOR, a method that transforms dataset selection into a submodular optimization problem solvable via modified Orthogonal Matching Pursuit. By stabilizing learning with fixed target values and trajectory-based selection, ReDOR efficiently identifies high-value data subsets that enhance RL algorithm performance while significantly reducing computational complexity. Empirical results on D4RL benchmarks show ReDOR-selected subsets achieve better or comparable results to full datasets in minutes, even for datasets containing millions of points.

## Method Summary
The core innovation lies in formulating dataset selection as a gradient approximation optimization problem, proving that the actor-critic framework can be transformed into a submodular objective. This enables efficient subset selection using a modified Orthogonal Matching Pursuit (OMP) method. Key modifications include stabilizing learning with fixed target values and trajectory-based selection to handle the dynamic nature of RL. Theoretically, the method provides convergence guarantees and approximation error bounds. The approach is particularly effective for offline RL settings where computational efficiency is crucial and data quality matters more than quantity.

## Key Results
- ReDOR-selected subsets significantly outperform random selection and prioritized loss-based methods
- Achieves better or comparable results to full datasets with much lower computational complexity
- Dataset selection completed in minutes even for datasets with millions of points
- Outperforms supervised learning baselines on D4RL benchmark datasets

## Why This Works (Mechanism)
The mechanism works by leveraging the submodularity of the actor-critic gradient approximation, which allows greedy selection algorithms to achieve near-optimal performance guarantees. The modified OMP approach stabilizes learning through fixed target values, preventing the instability that typically arises from dynamic value estimates during selection. Trajectory-based selection captures the sequential dependencies in RL data, ensuring that selected subsets maintain the temporal structure necessary for effective policy learning. The submodular structure also enables provable approximation bounds, providing theoretical justification for the empirical success.

## Foundational Learning
- **Submodularity**: A diminishing returns property where adding elements to a smaller set yields greater benefit than adding to a larger set. *Why needed*: Enables efficient greedy algorithms with performance guarantees. *Quick check*: Verify diminishing returns property holds for your specific objective function.
- **Orthogonal Matching Pursuit (OMP)**: An iterative greedy algorithm for sparse approximation. *Why needed*: Provides a computationally efficient way to select informative data points. *Quick check*: Ensure residual orthogonality condition is maintained at each iteration.
- **Actor-critic framework**: Combines policy (actor) and value (critic) function learning. *Why needed*: The target for dataset selection optimization. *Quick check*: Verify actor and critic updates are properly synchronized.
- **Gradient approximation**: Estimating gradients from sampled data. *Why needed*: Enables differentiable objectives for subset selection. *Quick check*: Monitor gradient variance across different batch sizes.
- **Trajectory-based selection**: Considering sequences of states rather than individual transitions. *Why needed*: Preserves temporal dependencies crucial for RL. *Quick check*: Verify selected trajectories maintain reasonable state distributions.
- **Fixed target stabilization**: Using constant target values during selection. *Why needed*: Prevents instability from changing value estimates. *Quick check*: Compare results with and without target stabilization.

## Architecture Onboarding

**Component Map:**
ReDOR Core -> Modified OMP -> Trajectory Selector -> Fixed Target Generator -> Performance Monitor

**Critical Path:**
Data Preprocessing -> Submodular Objective Formulation -> Modified OMP Iteration -> Subset Selection -> Policy Training

**Design Tradeoffs:**
- Computational efficiency vs. selection optimality: Greedy approach sacrifices some optimality for practical runtime
- Trajectory vs. transition selection: Trajectories preserve temporal structure but increase complexity
- Fixed vs. dynamic targets: Fixed targets ensure stability but may miss adaptation opportunities

**Failure Signatures:**
- If selection becomes too aggressive: Monitor KL divergence between full and selected dataset distributions
- If performance plateaus early: Check for insufficient exploration in selected subset
- If computational savings diminish: Verify OMP convergence criteria are appropriately set

**First Experiments:**
1. Compare ReDOR selection vs. random selection on a small D4RL dataset
2. Test sensitivity to different fixed target update frequencies
3. Evaluate impact of trajectory length on final performance

## Open Questions the Paper Calls Out
None

## Limitations
- Results heavily dependent on specific D4RL benchmarks and TD3+BC algorithm
- Theoretical guarantees assume regularity conditions that may not hold in all practical scenarios
- Limited exploration of the trade-off between dataset reduction ratio and final performance
- Need validation across diverse RL algorithms beyond actor-critic methods

## Confidence
- **Theoretical Framework:** High - Well-derived submodular formulation and convergence guarantees
- **Empirical Results:** Medium - Strong results but limited to specific algorithms and benchmarks
- **Generalizability:** Low - Unclear how method performs on non-D4RL datasets and different RL settings

## Next Checks
1. Test ReDOR on additional RL algorithms beyond TD3+BC, particularly actor-critic methods with different architectures
2. Validate performance on non-D4RL datasets with different reward structures and state-action distributions
3. Analyze the sensitivity of results to different reduction ratios and dataset sizes to understand the optimal trade-off between efficiency and performance