---
ver: rpa2
title: 'ExplainReduce: Summarising local explanations via proxies'
arxiv_id: '2502.10311'
source_url: https://arxiv.org/abs/2502.10311
tags:
- local
- fidelity
- coverage
- explanations
- reduction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces ExplainReduce, a method for summarizing large
  sets of local explanations from black-box models into a small, interpretable "proxy
  set" of simple models. The core idea is to treat the reduction task as an optimization
  problem balancing coverage, fidelity, and interpretability, solved via efficient
  greedy algorithms with theoretical guarantees.
---

# ExplainReduce: Summarising local explanations via proxies

## Quick Facts
- arXiv ID: 2502.10311
- Source URL: https://arxiv.org/abs/2502.10311
- Reference count: 40
- Key outcome: As few as five proxy models can match or exceed the fidelity of hundreds of original local explanations across diverse datasets and explanation methods.

## Executive Summary
This paper introduces ExplainReduce, a method for summarizing large sets of local explanations from black-box models into a small, interpretable "proxy set" of simple models. The core idea is to treat the reduction task as an optimization problem balancing coverage, fidelity, and interpretability, solved via efficient greedy algorithms with theoretical guarantees. Experiments show that as few as five proxy models can match or exceed the fidelity of hundreds of original local explanations across diverse datasets and explanation methods. The approach outperforms existing aggregation methods in both performance and scalability, while being agnostic to the underlying model and XAI technique.

## Method Summary
ExplainReduce takes a set of m local explanations generated by any XAI method (LIME, SHAP, SLISEMAP, etc.) and reduces them to a small proxy set of k simple models. The method formulates the reduction as a submodular optimization problem with three variants: Max Coverage (maximize items with faithful explanations), Min Loss (minimize total explanation error), and Balanced (trade-off between both). A greedy algorithm selects explanations based on marginal gains, achieving theoretical guarantees. For novel items, nearest-neighbor mapping in feature space assigns them to the most appropriate proxy. The approach is computationally efficient and works with any combination of black-box model and explanation method.

## Key Results
- Five proxy models can match or surpass the fidelity of 500+ original local explanations
- The method outperforms existing aggregation approaches in both performance and scalability
- Coverage and fidelity are robust to hyperparameter choices, with balanced objective (λ=0.5) performing well across datasets
- SLISEMAP explanations produce more stable proxy assignments compared to LIME and SHAP

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Local explanations for similar data items are often redundant and can be replaced by shared proxy models without significant fidelity loss.
- Mechanism: When the underlying closed-box function is smooth, nearby items in feature space yield similar local surrogate models. This means a single proxy model can faithfully approximate the closed-box behavior for multiple items, enabling compression.
- Core assumption: The closed-box model exhibits local smoothness such that similar inputs produce similar decision patterns.
- Evidence anchors:
  - [section] "In practice, the local explanations for similar items will often be similar, provided the underlying closed-box function is smooth in the region... most items could be approximated with almost equal fidelity by using any of the neighbouring local models" (Section 1)
  - [abstract] "This paper shows how a large set of local explanations can be reduced to a small 'proxy set' of simple models"
  - [corpus] Weak direct evidence; neighbor papers focus on XAI evaluation rather than explanation redundancy specifically.
- Break condition: Non-smooth closed-box models with highly localized decision boundaries may violate the redundancy assumption, reducing compression efficiency.

### Mechanism 2
- Claim: Explanation reduction can be formulated as a submodular optimization problem solvable by greedy algorithms with provable worst-case guarantees.
- Mechanism: The reduction task maps to variants of partial set cover (for coverage) and supermodular loss minimization (for fidelity). Greedy selection iteratively picks the explanation maximizing marginal gain, achieving ≥(1-1/e)≈63.2% of optimal performance.
- Core assumption: The coverage function is monotone submodular, and the loss function is supermodular (both proven in Appendix H).
- Evidence anchors:
  - [section] "The greedy algorithm has a lower bound of achieving coverage at least 1−((k−1)/k)k (≥1−1/e≈0.632) times the optimal solution" (Section 4.1.1)
  - [section] "Since U(S) is a linear combination of two monotone submodular functions, U(S) is itself monotone submodular" (Section 4.1.3)
  - [corpus] No corpus evidence on submodular guarantees in XAI contexts.
- Break condition: Non-monotonic coverage or non-supermodular loss structures would invalidate theoretical guarantees.

### Mechanism 3
- Claim: A small proxy set (e.g., k=5) can match or exceed the fidelity of hundreds of original local explanations when evaluated on held-out test data.
- Mechanism: The proxy set generalizes because greedy selection identifies models that capture shared decision patterns across the data manifold, rather than overfitting to individual local explanations. Test fidelity measures true adherence to the closed-box model.
- Core assumption: The initial set of local explanations contains at least one faithful approximation for most items in the dataset.
- Evidence anchors:
  - [section] "As few as k=5 proxies can reach or even surpass the fidelity of a set of 500 local explanations for the selected datasets" (Section 5.2.1)
  - [section] "The loss-minimising greedy algorithms... often matching and sometimes outperforming the full explanation set" (Section 5.2.1)
  - [corpus] Aggregating Local Saliency Maps paper addresses similar aggregation challenges for image classification, supporting the generalization claim.
- Break condition: Highly heterogeneous datasets with many distinct decision regions may require larger proxy sets.

## Foundational Learning

- **Local surrogate explanations (LIME, SHAP, SLISEMAP)**:
  - Why needed here: ExplainReduce operates on pre-existing local explanations; understanding how these are generated (sampling neighborhoods, gradient-based methods) is essential for interpreting proxy model behavior.
  - Quick check question: Can you explain why LIME produces a linear model as its local explanation and what the neighborhood sampling approach entails?

- **Submodular optimization and set cover**:
  - Why needed here: The theoretical guarantees and algorithm design rely on submodularity properties; understanding marginal gain and greedy approximation is critical for debugging poor performance.
  - Quick check question: Why does the greedy algorithm for monotone submodular maximization guarantee a (1-1/e) approximation ratio?

- **Fidelity vs coverage vs interpretability trade-offs**:
  - Why needed here: The three problem formulations (Max Coverage, Min Loss, Balanced) represent different trade-off preferences; practitioners must select the appropriate objective.
  - Quick check question: What does "coverage" measure in this context, and how does it differ from "fidelity"?

## Architecture Onboarding

- **Component map**: Explanation Generator -> Loss Matrix Constructor -> Reduce Algorithm -> Mapping Layer -> Inference (new items)

- **Critical path**:
  1. Generate at least n=500 local explanations (empirically sufficient; Figure 6)
  2. Compute full loss matrix L (dominant O(mn) cost)
  3. Run `reduce` with `Balanced` algorithm, λ=0.5, ε set to 10th-30th percentile of training loss
  4. Validate on held-out test set; expect k=5 proxies to suffice for most datasets

- **Design tradeoffs**:
  - **Max Coverage vs Min Loss vs Balanced**: Max Coverage prioritizes explaining more items (higher coverage) at potential fidelity cost; Min Loss optimizes faithfulness; Balanced (λ=0.5) is empirically robust across both metrics
  - **Proxy set size k**: Diminishing returns beyond k>5 (Figure 5); larger k increases interpretability burden
  - **Initial explanation count n**: 500 explanations typically sufficient; more yields marginal gains (Figure 6)

- **Failure signatures**:
  - Test fidelity >> train fidelity: Overfitting to training explanations; reduce k or increase n
  - Coverage plateaus below target: ε threshold too strict; increase to 30th-50th percentile
  - Proxy models disagree with domain knowledge: Initial XAI method may be unstable; try SLISEMAP (more stable, Figure 7)

- **First 3 experiments**:
  1. **Baseline replication**: On a synthetic dataset with known cluster structure, verify that ExplainReduce recovers ground-truth cluster assignments using k equal to true cluster count (replicate Figure 2/3)
  2. **Ablation on explanation method**: Compare LIME vs SHAP vs SLISEMAP as input generators; expect SLISEMAP to yield lower instability (Figure 7)
  3. **Scalability test**: Measure runtime on loss matrix construction for n ∈ {100, 500, 1000, 5000} items; verify O(mn) scaling and that greedy reduction remains <1s for practical sizes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the sets of applicable local models identified by ExplainReduce be formalized to provide rigorous uncertainty quantification?
- Basis in paper: [explicit] The Discussion section proposes leveraging the method for "uncertainty quantification" to "find, e.g., confidence intervals for explanations."
- Why unresolved: The current work focuses on summarizing explanations into point estimates (proxy models) rather than utilizing the variance among multiple applicable local models as a metric of confidence.
- What evidence would resolve it: A theoretical framework defining confidence intervals based on proxy disagreement, validated by correlating these intervals with prediction errors on test data.

### Open Question 2
- Question: Can the reduction process be combined with manifold learning to enforce spatial continuity in proxy assignments?
- Basis in paper: [explicit] The authors identify a lack of "consideration of the spatial distribution" as a limitation and suggest "combin[ing] the reduction process with manifold learning techniques."
- Why unresolved: The current optimization maximizes coverage and fidelity independently of data topology, meaning similar data points might be assigned to different proxies, hindering visual interpretability.
- What evidence would resolve it: A modified algorithm that optimizes for spatial smoothness alongside fidelity, demonstrating that proxy assignments form coherent regions in a low-dimensional embedding.

### Open Question 3
- Question: What is the optimal strategy for mapping novel, unseen data points to a specific proxy model in the reduced set?
- Basis in paper: [inferred] The Methods section acknowledges the use of a "simple nearest-neighbour approach" to map unseen items, noting it has "obvious limitations" despite working reasonably well in experiments.
- Why unresolved: Relying solely on Euclidean distance in the feature space may fail for complex data structures where feature proximity does not guarantee similar model behavior.
- What evidence would resolve it: Comparative analysis testing learned classifiers or kernel-based methods against the nearest-neighbor baseline for assigning test items to proxy models.

## Limitations
- Performance may degrade on highly non-smooth models or datasets with many distinct decision regions
- Loss matrix computation scales quadratically with the number of explanations and items, limiting scalability to very large datasets
- Method effectiveness depends on quality of initial local explanations - poor explanations lead to poor compression

## Confidence
- **High confidence**: The submodular optimization framework and greedy algorithm guarantees (1-1/e approximation ratio) are well-established and mathematically proven.
- **Medium confidence**: The empirical results showing k=5 proxies matching 500+ original explanations are convincing but require reproduction across diverse domains to validate generalizability.
- **Medium confidence**: The claim that SLISEMAP produces more stable explanations than LIME/SHAP is supported by Figure 7 but needs systematic validation across different dataset types and model architectures.

## Next Checks
1. **Generalization across domains**: Apply ExplainReduce to image classification tasks using SmoothGrad explanations to test whether the 5-proxy sufficiency holds beyond tabular datasets.
2. **Failure mode characterization**: Systematically evaluate on known non-smooth models (e.g., decision trees with deep branching) to identify the practical limits of compression.
3. **Efficiency scaling**: Benchmark loss matrix computation and greedy reduction on datasets with n>10,000 items to identify practical bottlenecks and opportunities for approximation or parallelization.