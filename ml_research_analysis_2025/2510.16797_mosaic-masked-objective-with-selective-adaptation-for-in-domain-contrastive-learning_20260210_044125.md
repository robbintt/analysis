---
ver: rpa2
title: 'MOSAIC: Masked Objective with Selective Adaptation for In-domain Contrastive
  Learning'
arxiv_id: '2510.16797'
source_url: https://arxiv.org/abs/2510.16797
tags:
- contrastive
- domain
- training
- language
- adaptation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MOSAIC, a multi-stage framework for adapting
  large-scale text embedding models to specialized domains by jointly optimizing masked
  language modeling (MLM) and contrastive objectives. The approach addresses the challenge
  of transferring robust semantic discrimination from general-domain models to domain-specific
  contexts while preserving fine-grained token-level learning.
---

# MOSAIC: Masked Objective with Selective Adaptation for In-domain Contrastive Learning

## Quick Facts
- **arXiv ID:** 2510.16797
- **Source URL:** https://arxiv.org/abs/2510.16797
- **Reference count:** 30
- **Key outcome:** Joint MLM-contrastive optimization achieves up to 13.4% NDCG@10 improvement on domain-specific retrieval tasks

## Executive Summary
This paper introduces MOSAIC, a multi-stage framework for adapting large-scale text embedding models to specialized domains by jointly optimizing masked language modeling (MLM) and contrastive objectives. The approach addresses the challenge of transferring robust semantic discrimination from general-domain models to domain-specific contexts while preserving fine-grained token-level learning. By restricting MLM to domain-specific tokens and balancing its contribution with contrastive training, MOSAIC achieves improvements of up to 13.4% in NDCG@10 over strong baselines. Experiments on both high-resource (biomedical) and low-resource (Islamic) domains demonstrate substantial gains, with the method showing particular effectiveness even when in-domain data is scarce.

## Method Summary
MOSAIC extends existing sentence embedding models by first expanding the vocabulary with domain-specific tokens, then applying a three-stage adaptation process. Stage 1 expands the tokenizer vocabulary with domain-specific tokens trained on in-domain corpus. Stage 2 performs joint training of MLM (restricted to domain tokens) and contrastive objectives with balanced loss weights. Stage 3 applies contrastive-only fine-tuning to reinforce sentence-level discrimination. The method specifically addresses gradient imbalance issues that arise when combining MLM with contrastive learning in joint optimization.

## Key Results
- MOSAIC achieves up to 13.4% improvement in NDCG@10 on biomedical retrieval tasks
- Outperforms all baselines on both high-resource (BIOSSES biomedical) and low-resource (Islamic) domains
- Shows consistent gains across all evaluation metrics including semantic similarity, information retrieval, and pairwise semantic comparison
- Domain-restricted MLM (masking only new tokens) significantly outperforms all-token MLM even with 30× lower loss weight

## Why This Works (Mechanism)

### Mechanism 1: Domain-Restricted MLM Balances Joint Training
- **Claim:** Restricting masked language modeling to domain-specific tokens prevents MLM gradients from overwhelming contrastive signals.
- **Mechanism:** Full-vocabulary MLM produces larger gradient magnitudes due to the larger denominator in the softmax (full vocabulary size vs. smaller domain vocabulary). This causes MLM to dominate joint optimization. By masking only domain-specific tokens (new vocabulary), gradient magnitudes become more comparable to contrastive loss gradients, enabling balanced multi-task learning.
- **Core assumption:** Gradient magnitude imbalance is the primary cause of training instability in joint MLM-contrastive optimization, not inherent objective incompatibility.
- **Evidence anchors:**
  - [Page 4, Section 3.2]: "The larger vocabulary size in MLM results in a substantially larger denominator, leading to very low probabilities for the correct token. Consequently, this generates higher loss values and, therefore, larger gradient magnitudes, causing MLM to dominate the training process."
  - [Page 8, Table 2]: Ablation shows all-token MLM (α=0.1) scores 66.092 vs. domain-restricted MLM (α=0.3) scoring 88.116 on BIOSSES, despite domain-restricted using 30× higher α weight.
  - [corpus]: Related work TNCSE addresses anisotropy through tensor norm constraints rather than objective balancing—suggesting alternative mechanisms exist for similar problems.
- **Break condition:** If all-token MLM with very low α (e.g., 0.001) matches or exceeds domain-restricted MLM performance, the gradient-balancing explanation is insufficient.

### Mechanism 2: MLM Provides Localized Supervision for New Token Embeddings
- **Claim:** Pooling operations in sentence embeddings dilute gradient signals to new domain tokens; MLM provides direct token-level supervision.
- **Mechanism:** Sentence embedding models use mean/max pooling over token representations, which averages gradients across all tokens. New domain tokens receive weak, diluted signals through contrastive loss alone. MLM provides direct token-level gradients to embedding lookup table, enabling proper initialization of domain vocabulary.
- **Core assumption:** New domain tokens require direct gradient signals that contrastive loss cannot provide efficiently.
- **Evidence anchors:**
  - [Page 2, Section 1]: "adding new tokens and continuing only with the contrastive objective provides insufficient training signals to update new domain tokens, as the embedding matrix receives diluted signals due to the pooling functions applied to generate text embeddings."
  - [Page 5, Section 3.2]: "In this way, MLM acts as a localized supervision signal that highlights the differences and similarities between pairs, particularly in cases where contrastive loss alone may struggle due to mean pooling."
  - [corpus]: Corpus evidence is weak—neighbors focus on contrastive learning enhancements but don't directly address token embedding initialization mechanisms.
- **Break condition:** If contrastive-only training after vocabulary expansion (no MLM stage) achieves equivalent performance to full MOSAIC pipeline, the token-supervision mechanism is unnecessary.

### Mechanism 3: Staged Training Prevents Representation Distortion
- **Claim:** A three-stage progression (vocab expansion → joint training → contrastive-only) prevents catastrophic degradation of pre-trained capabilities.
- **Mechanism:** Stage 1 (vocab expansion) alone causes performance drop due to embedding mismatch for untrained tokens. Stage 2 (joint MLM+contrastive) recovers performance but may dilute sentence-level discrimination. Stage 3 (contrastive-only) reinforces and re-aligns sentence-level representations before deployment.
- **Core assumption:** Sequential stage ordering matters—applying contrastive first, then joint training degrades performance (which ablation confirms).
- **Evidence anchors:**
  - [Page 7, Section 4.1]: MOSAIC-Bio-Stage1 scores 47.756 avg vs. Stage 2 at 53.853 vs. Stage 3 at 54.638—showing progressive improvement.
  - [Page 8, Table 2]: Reversing stage order (contrastive first, then joint) drops BIOSSES from 88.116 to 70.540 (20% decrease).
  - [corpus]: Domain Adaptation for Japanese Sentence Embeddings paper uses synthetic generation rather than staged training—suggesting multiple valid adaptation strategies exist.
- **Break condition:** If single-stage joint training (no separate Stage 3) matches three-stage performance, the corrective stage mechanism is redundant.

## Foundational Learning

- **Contrastive Learning (InfoNCE formulation):**
  - **Why needed here:** The paper frames both MLM and contrastive objectives as mutual information maximization problems. Understanding InfoNCE loss (log-similarity over negative samples) is essential to follow the gradient imbalance argument.
  - **Quick check question:** Can you explain why increasing the number of negative samples in InfoNCE affects gradient magnitude?

- **Anisotropy in Sentence Embeddings:**
  - **Why needed here:** The paper positions joint MLM-contrastive training as addressing the anisotropy problem (embeddings confined to narrow cone). Understanding why isotropy improves representation quality helps motivate the approach.
  - **Quick check question:** Why would embeddings clustered in a narrow cone hurt semantic discrimination?

- **Subword Tokenization and Vocabulary Expansion:**
  - **Why needed here:** MOSAIC's prerequisite is adding domain-specific tokens to the tokenizer. Understanding BPE/WordPiece tokenization and how new tokens are initialized (mean of subword embeddings) is critical for implementation.
  - **Quick check question:** When adding a new domain token "immunoglobulin," how would you initialize its embedding if the existing tokenizer splits it into ["immuno", "##globulin"]?

## Architecture Onboarding

- **Component map:** Tokenizer (domain-specific vocabulary expansion) -> Embedding Matrix (extended with new tokens) -> Encoder (BERT-style transformer) -> Loss Layer (dual-head contrastive + domain-restricted MLM)
- **Critical path:**
  1. Train domain-specific tokenizer on in-domain corpus
  2. Identify tokens not in base tokenizer, add to vocabulary
  3. Initialize new embeddings as mean of subword components
  4. Stage 2: Joint training with masking rate 0.15, α=0.3, masking restricted to domain tokens only
  5. Stage 3: Contrastive-only training to recover sentence discrimination
  6. Optional: Supervised fine-tuning with hard negatives
- **Design tradeoffs:**
  - **α hyperparameter (MLM weight):** Higher α (0.5) causes MLM dominance and collapse; lower α (0.1) under-trains domain tokens. Paper finds α=0.3 optimal.
  - **Masking rate:** Standard 0.15 works; increasing to 0.3 causes dramatic drop (88.1 → 49.9 in BIOSSES).
  - **Stage ordering:** Must apply joint training before final contrastive-only; reversed order loses 20% performance.
  - **MLM scope:** Domain-token-only masking outperforms all-token masking even with 30× lower α weight.
- **Failure signatures:**
  - **Vocab expansion without training:** Expect ~10-15% performance drop across all metrics (Stage 1 ablation).
  - **All-token MLM with α≥0.1:** Expect substantial degradation; even α=0.001 underperforms domain-restricted masking.
  - **High masking rate (≥0.3):** Expect catastrophic failure with near-random performance.
  - **Reversed stage order:** Expect ~20% drop on semantic similarity tasks.
- **First 3 experiments:**
  1. **Token masking scope ablation:** Implement both all-token and domain-token-only masking with α ∈ {0.1, 0.2, 0.3, 0.4}. Verify that domain-restricted masking consistently outperforms all-token masking across α values. This validates the core mechanism before proceeding.
  2. **Stage progression validation:** Train three variants—(A) Stage 2 only, (B) Stages 2→3 in order, (C) Stages 3→2 reversed. Compare on retrieval + STS benchmarks. Expect B > A > C. This confirms the staged training hypothesis.
  3. **Low-resource domain sanity check:** Replicate Islamic domain experiment with your own low-resource corpus (e.g., legal, financial). Use only ~5-10k pairs. If MOSAIC fails to improve over baselines in extremely low-resource settings, the method may have data quantity thresholds not discussed in the paper.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does MOSAIC's effectiveness vary across domains with different linguistic characteristics, data availability, and cultural contexts beyond biomedical and Islamic texts?
- **Basis in paper:** [explicit] The authors state: "each domain, whether high- or low-resource, can present unique characteristics and challenges that could affect the effectiveness of domain adaptation methods" and "the generalizability of our approach may vary depending on domain-specific linguistic features, data availability, or cultural context."
- **Why unresolved:** Only two domains tested (high-resource biomedical, low-resource Islamic); medium-resource domains and other specialized domains unexplored.
- **What evidence would resolve it:** Systematic evaluation across 5+ diverse domains (legal, financial, technical, low-resource languages) with varying data quantities and linguistic properties.

### Open Question 2
- **Question:** How can domain-adapted models handle queries that fall partially outside the adaptation corpus (e.g., social policy aspects within biomedical domains)?
- **Basis in paper:** [inferred] MOSAIC models underperform on TRECCOVID, where ~20% of queries address social/policy pandemic aspects outside the PubMed training corpus scope.
- **Why unresolved:** Domain adaptation inherently focuses on in-domain data; real-world applications often require handling edge cases spanning multiple sub-domains.
- **What evidence would resolve it:** Ablation studies with hybrid corpora, or architectural modifications (e.g., mixture-of-experts) to handle multi-faceted queries.

### Open Question 3
- **Question:** Can principled, data-driven methods determine the optimal MLM loss weight (α) instead of manual tuning?
- **Basis in paper:** [explicit] Authors note α is "a hyperparameter whose impact is rarely examined in prior literature" and show sensitivity (α=0.5 causes collapse, α=0.3 works best).
- **Why unresolved:** Current approach requires grid search per domain; no theoretical framework predicts optimal α from domain properties.
- **What evidence would resolve it:** Gradient magnitude analysis across domains, or adaptive α scheduling based on vocabulary overlap statistics.

### Open Question 4
- **Question:** What determines the optimal ordering of joint MLM+contrastive training versus contrastive-only stages?
- **Basis in paper:** [explicit] Reversing stages 2 and 3 causes 20% performance drop; authors note "applying the joint objective to an already strong embedding model can disturb its contrastive capability."
- **Why unresolved:** Mechanism for why joint training should precede (not follow) pure contrastive fine-tuning remains unclear.
- **What evidence would resolve it:** Representation geometry analysis at each stage, or controlled experiments with intermediate checkpoints.

## Limitations

- The gradient magnitude imbalance explanation for MLM dominance lacks direct empirical validation through gradient norm measurements during training.
- The necessity of Stage 3 (contrastive-only refinement) is not fully established—single-stage joint training variants were not adequately tested.
- The method's effectiveness on extremely low-resource domains (≤10K pairs) is asserted but not empirically validated beyond the Islamic domain experiment.

## Confidence

- **High Confidence:** The core empirical findings (MOSAIC outperforms baselines on both biomedical and Islamic domains, achieving up to 13.4% NDCG@10 improvement) are well-supported by comprehensive experiments and ablation studies. The three-stage training procedure consistently improves performance when executed in the specified order.
- **Medium Confidence:** The mechanism explanations (gradient balancing, token-level supervision, staged correction) are plausible but not definitively proven. While ablations support these hypotheses, direct measurements of gradient magnitudes, token embedding evolution, and representation geometry during training are absent.
- **Low Confidence:** The paper's claims about MOSAIC's effectiveness on extremely low-resource domains are not adequately validated. The Islamic domain experiment uses ~10K pairs, which may not represent truly low-resource scenarios where only hundreds or thousands of examples exist.

## Next Checks

1. **Gradient Magnitude Validation:** Implement gradient monitoring during joint training to directly measure and compare MLM vs. contrastive gradient norms under all-token vs. domain-restricted masking. This would empirically confirm or refute the stated mechanism of gradient imbalance.

2. **Single-Stage Alternative Testing:** Evaluate a single-stage joint training variant with reduced α (e.g., 0.1) and no Stage 3 refinement. Compare against the full three-stage MOSAIC pipeline on both high-resource (BIOSSES) and low-resource (Islamic) domains to determine if Stage 3 is truly necessary.

3. **Ultra-Low-Resource Scaling Study:** Systematically evaluate MOSAIC on domains with decreasing data availability (100, 1K, 10K pairs) using controlled experiments. Measure the minimum data threshold required for MOSAIC to outperform both base models and simpler fine-tuning approaches, establishing the method's practical applicability limits.