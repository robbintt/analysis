---
ver: rpa2
title: Quantifying Language Disparities in Multilingual Large Language Models
arxiv_id: '2508.17162'
source_url: https://arxiv.org/abs/2508.17162
tags:
- language
- languages
- performance
- evaluation
- framework
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of quantifying language disparities
  in multilingual large language models (LLMs), which are often confounded by fragmented
  evaluations across different datasets, languages, and experimental setups. The authors
  propose a framework that disentangles these confounding factors by estimating a
  latent construct called "performance potential" for each language-task pair using
  a linear mixed-effects model.
---

# Quantifying Language Disparities in Multilingual Large Language Models

## Quick Facts
- arXiv ID: 2508.17162
- Source URL: https://arxiv.org/abs/2508.17162
- Authors: Songbo Hu; Ivan Vulić; Anna Korhonen
- Reference count: 40
- Primary result: Higher overall model performance does not necessarily imply greater fairness across languages

## Executive Summary
This paper addresses the challenge of quantifying language disparities in multilingual large language models (LLMs), which are often confounded by fragmented evaluations across different datasets, languages, and experimental setups. The authors propose a framework that disentangles these confounding factors by estimating a latent construct called "performance potential" for each language-task pair using a linear mixed-effects model. They introduce interpretable metrics: the performance realisation ratio (PRR), which normalises actual performance relative to estimated potential, and the coefficient of variation of PRR (CV-PRR), which measures model-level language disparities. Through a case study of 13 model variants on 11 multilingual datasets, the framework reveals that higher overall model performance does not necessarily imply greater fairness across languages.

## Method Summary
The framework models observed scores as a linear mixed-effects model with language and task as fixed effects and model as a random effect. Performance potential is estimated as the marginal mean over the model population for each language-task pair, creating a "current state of NLP" reference. The PRR metric normalises actual performance by this potential, and CV-PRR measures the dispersion of PRR across languages to quantify model-level disparities. The approach is shown to generalise across benchmarks and tasks, offering a practical tool for fairer multilingual evaluation.

## Key Results
- Fine-tuned models like mT5-Base achieve lower language disparities (CV-PRR) compared to larger LLMs like GPT-4, despite lower overall performance
- The framework provides more reliable estimates for low-resource languages by accounting for task difficulty and uneven dataset coverage
- A model with higher Mean-PRR (e.g., GPT-4 at 1.07) can have greater language disparities than a lower-performing model (mT5-Base at 0.98 Mean-PRR but 0.18 CV-PRR vs 0.23)

## Why This Works (Mechanism)

### Mechanism 1: Linear Mixed-Effects Decomposition
- **Claim:** Disentangling language, task, and model effects via variance decomposition enables fair cross-linguistic comparisons that raw scores cannot provide.
- **Mechanism:** The framework models observed scores as: s_ℓ,t,m = μ + α_ℓ + β_t + u_m + ε_ℓ,t,m, where language (α_ℓ) and task (β_t) are fixed effects, and model (u_m) is a random effect drawn from N(0, σ²_u). This isolates each factor's marginal contribution, allowing "what's achievable for this language-task pair" to be estimated independently of "how good is this specific model."
- **Core assumption:** Effects are additive (no language-task interactions in base model); residuals are homoscedastic and normally distributed; the sampled models are representative of the broader model population.
- **Evidence anchors:**
  - [abstract]: "disentangles these confounding variables"
  - [Section 3, Eq. 1]: Full model specification
  - [corpus]: Tokenization disparities paper (arXiv:2510.12389) confirms systematic cross-lingual computational inequities exist, supporting the need for decomposition, but does not validate this specific statistical approach.
- **Break condition:** When language-task interactions are large (e.g., certain tasks disproportionately harder for specific languages), the additive assumption fails. The paper acknowledges this limitation in Section 5 and proposes an extension (Eq. 6) with γ_ℓ,t interaction terms.

### Mechanism 2: Norm-Referenced Performance Potential
- **Claim:** Estimating a latent "performance potential" from aggregate model behavior provides a principled, data-driven baseline for fairness assessment.
- **Mechanism:** PP_ℓ,t = E_m[s_ℓ,t,m] = μ + α_ℓ + β_t computes marginal means over the model population, creating a "current state of NLP" reference for each language-task pair. Language Potential (LP_ℓ) further aggregates across tasks to rank how well languages are served overall.
- **Core assumption:** The sample of models spans the relevant capability range; higher aggregate performance reflects genuinely better language-task viability rather than benchmark artifacts.
- **Evidence anchors:**
  - [abstract]: "estimating a latent construct called 'performance potential'"
  - [Section 3, Eq. 2-3]: Mathematical definitions of PP and LP
  - [corpus]: Limited external validation. Related work on multilingual benchmarks (MMLU-ProX, arXiv:2503.10497) addresses cross-lingual evaluation but doesn't validate the potential construct.
- **Break condition:** When the model sample is biased (e.g., only English-centric models), potential estimates become unrepresentative. Low-resource languages with sparse coverage are particularly vulnerable—the paper notes Haitian Creole was misranked by baseline methods due to appearing only in an "easy" dataset (XCOPA).

### Mechanism 3: Coefficient of Variation for Scale-Invariant Fairness
- **Claim:** CV-PRR provides a scale-invariant disparity metric enabling comparison across models with different absolute performance levels.
- **Mechanism:** CV(m)_PRR = Std(PRR) / Mean(PRR) normalizes dispersion relative to central tendency. This captures whether a model realizes its potential *consistently* across languages, not just *well* on average. A model with lower Mean-PRR but also lower CV-PRR is "more fair" under equal-opportunity framing.
- **Core assumption:** Fairness means proportional realization of potential across languages (equal opportunity), not equal absolute performance (equal outcome). Also assumes the CV ratio meaningfully captures user-relevant disparities.
- **Evidence anchors:**
  - [abstract]: "higher overall model performance does not necessarily imply greater fairness across languages"
  - [Section 4, Figure 1 & Table 1]: mT5-Base has CV-PRR = 0.18 vs. GPT-4 at 0.23, despite Mean-PRR of 0.98 vs. 1.07
  - [corpus]: Healthcare Q&A disparities paper (arXiv:2510.17476) corroborates cross-lingual performance gaps in high-stakes domains but does not validate CV-PRR specifically.
- **Break condition:** When Mean-PRR approaches zero, CV-PRR becomes unstable. The metric also doesn't identify *which* languages are disadvantaged—only that disparities exist.

## Foundational Learning

- **Concept: Linear Mixed-Effects Models**
  - Why needed here: Core statistical machinery for separating fixed effects (systematic differences we want to measure) from random effects (variation we want to marginalize over).
  - Quick check question: If you treat "model" as a fixed effect instead of random, what changes about the inferences you can make about the broader model population?

- **Concept: Measurement Theory (Construct vs. Operationalization)**
  - Why needed here: The paper explicitly frames "fairness" as a latent construct, with CV-PRR as its operationalization. Understanding this distinction is critical for interpreting what the metrics actually measure.
  - Quick check question: What external evidence would you need to validate that lower CV-PRR actually corresponds to more equitable outcomes for end users?

- **Concept: Coefficient of Variation**
  - Why needed here: Enables comparison of dispersion across models with different mean performance levels—essential when some models are simply more capable overall.
  - Quick check question: Why is comparing standard deviations alone insufficient when evaluating fairness across models like mT5-Base (Mean-PRR ≈ 0.98) vs. GPT-4 (Mean-PRR ≈ 1.07)?

## Architecture Onboarding

- **Component map:** Data Ingestion -> Mixed-Effects Fitting -> Potential Estimation -> PRR Calculator -> Disparity Aggregator
- **Critical path:** Data standardization -> Mixed-effects fitting -> Potential estimation -> PRR computation -> CV-PRR aggregation. The fitting step is the computational bottleneck and assumption-critical stage.
- **Design tradeoffs:**
  - **Additive vs. interaction model:** Paper uses additive (Eq. 1) for simplicity; interaction model (Eq. 6) captures language-task difficulty variations but requires substantially more data per language-task pair
  - **Norm-referenced vs. criterion-referenced:** Current approach uses peer models as reference; could alternatively define potential relative to theoretical maximum or human performance
  - **Assumption:** Linearity assumption ignores potential non-linear interactions between resource availability and performance
- **Failure signatures:**
  - High CV-PRR with high Mean-PRR: Model performs well on average but inconsistently (seen in GPT-4, text-davinci-003)
  - Residual diagnostics fail: Shapiro-Wilk p < 0.001 or Levene test p < 0.001 indicates assumption violations (paper reports both)
  - Rank instability: Language potential rankings shift dramatically when different benchmarks are used (paper reports ρ = 0.87 across MEGA/XTREME-R, suggesting moderate stability)
  - Sparse data warnings: Low-resource languages appearing in <3 datasets produce unstable estimates
- **First 3 experiments:**
  1. **Cross-benchmark validation:** Apply the framework to XTREME-R or M5 benchmark data (the paper reports Spearman ρ = 0.87 for 50 shared languages) to verify language potential rankings generalize.
  2. **Sensitivity to model composition:** Systematically add/remove models from the sample (e.g., exclude all OpenAI models, exclude all fine-tuned models) and observe stability of language potential estimates.
  3. **Interaction model pilot:** Fit the extended model with language-task interactions (Eq. 6) on a subset of languages with dense coverage; compare residual diagnostics and ranking changes to quantify the cost of the additive simplification.

## Open Questions the Paper Calls Out

1. **Question:** Does the inclusion of interaction terms (e.g., language–task or language–model interactions) in the mixed-effects model resolve the observed violations of homoscedasticity and normality?
   - **Basis in paper:** [explicit] The authors state in the Limitations that their simple additive model leads to rejected statistical assumptions and suggest that "employing more sophisticated models... such as those incorporating interaction terms" could mitigate these violations.
   - **Why unresolved:** The current case study uses a strictly additive model due to data constraints, leaving the statistical benefits of interaction terms untested.
   - **What evidence would resolve it:** A re-analysis of the MEGA benchmark using the extended Equation 6, with passing Shapiro-Wilk and Levene tests on the residuals.

2. **Question:** Does a lower Coefficient of Variation of PRR (CV-PRR) correspond to higher perceived utility or fairness for end-users in real-world applications?
   - **Basis in paper:** [explicit] The authors note that demonstrating the "validity" of their framework—proving that lower measured disparities equate to fairer outcomes for users—requires "external evidence" and remains a limitation.
   - **Why unresolved:** The paper establishes the reliability of the metric but relies on the standard NLP assumption that higher scores equal greater utility without empirical user validation.
   - **What evidence would resolve it:** Human evaluation studies correlating CV-PRR scores with user satisfaction or task completion rates across diverse languages.

3. **Question:** Why do smaller, fine-tuned models (e.g., mT5-Base) exhibit lower language disparities (CV-PRR) than larger, advanced models like GPT-4, despite having lower overall performance?
   - **Basis in paper:** [inferred] The paper highlights this counter-intuitive finding in the Results section but does not investigate the underlying mechanisms (e.g., the impact of instruction tuning vs. fine-tuning on variance).
   - **Why unresolved:** The framework quantifies disparities but does not isolate the model architectural or training factors that cause the variance in performance realisation.
   - **What evidence would resolve it:** An ablation study analyzing the variance of PRR across models with controlled differences in parameter size and training regimes.

## Limitations

- The additive fixed effects model assumes no language-task interactions, which may not hold for resource-underserved languages where certain tasks are systematically harder
- The model sample may introduce bias—if predominantly English-centric or fine-tuned on specific datasets, potential estimates become unrepresentative
- Low-resource languages with sparse coverage across datasets remain vulnerable to misranking despite the framework's improvements over raw baselines
- Statistical assumptions of homoscedasticity and normality of residuals were violated in the analysis (Shapiro-Wilk p < 0.001)

## Confidence

**High Confidence:** The mixed-effects framework successfully disentangles confounding factors and produces interpretable metrics (PRR, CV-PRR) that reveal meaningful disparities across model variants. The finding that higher overall performance doesn't guarantee greater fairness is robustly demonstrated across the 13-model sample.

**Medium Confidence:** The potential estimation mechanism provides principled baselines for fairness assessment, but validation is limited to internal consistency checks (benchmark correlation ρ = 0.87) rather than external criteria like user outcomes or human performance baselines.

**Low Confidence:** The additive model's limitations in capturing language-task interactions may systematically understate disparities for certain language-task pairs, particularly in low-resource settings where data sparsity prevents reliable estimation of interaction terms.

## Next Checks

1. **Cross-benchmark validation:** Apply the framework to XTREME-R or M5 data to verify language potential rankings generalize beyond MEGA (current ρ = 0.87 for 50 shared languages).

2. **Model composition sensitivity:** Systematically add/remove models (e.g., exclude all OpenAI models) and observe stability of language potential estimates to assess robustness to sampling bias.

3. **Interaction model pilot:** Fit the extended model with language-task interactions on a subset of languages with dense coverage; compare residual diagnostics and ranking changes to quantify the cost of the additive simplification.