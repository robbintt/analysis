---
ver: rpa2
title: 'SynDaCaTE: A Synthetic Dataset For Evaluating Part-Whole Hierarchical Inference'
arxiv_id: '2506.17558'
source_url: https://arxiv.org/abs/2506.17558
tags:
- which
- syndacate
- parts
- dataset
- part-whole
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces SynDaCaTE, a synthetic dataset for evaluating
  part-whole hierarchical inference in computer vision. The dataset enables precise
  testing of models' ability to infer object representations from images and parts,
  addressing the challenge of evaluating capsule networks' claims about learning part-whole
  hierarchies.
---

# SynDaCaTE: A Synthetic Dataset For Evaluating Part-Whole Hierarchical Inference

## Quick Facts
- arXiv ID: 2506.17558
- Source URL: https://arxiv.org/abs/2506.17558
- Authors: Jake Levi; Mark van der Wilk
- Reference count: 33
- Primary result: Introduced SynDaCaTE synthetic dataset revealing SetTransformers outperform capsule networks on parts-to-whole inference by an order of magnitude

## Executive Summary
SynDaCaTE is a synthetic dataset designed to evaluate part-whole hierarchical inference in computer vision models. The dataset enables precise testing of models' ability to infer object representations from images and parts, addressing the challenge of evaluating capsule networks' claims about learning part-whole hierarchies. The authors demonstrate that a prominent capsule model's bottleneck is image-to-parts inference rather than parts-to-wholes inference, and show that permutation-equivariant SetTransformers are highly effective for parts-to-wholes inference.

## Method Summary
The authors created SynDaCaTE as a synthetic dataset where characters are composed of parts with known ground-truth relationships. The dataset includes single-object classification tasks and multi-character inference tasks where models must predict characters from given parts. This controlled environment allows for precise measurement of models' ability to perform part-whole hierarchical inference, with ground-truth part locations and identities available for evaluation. The dataset was used to test various models including CNNs, capsule networks, and SetTransformers under identical conditions.

## Key Results
- SetTransformers achieve an order of magnitude better performance than other baselines on parts-to-characters inference tasks
- CNNs outperform capsule networks on single-object classification when using pre-trained part representations
- Image-to-parts inference is identified as the bottleneck for capsule networks rather than parts-to-wholes inference

## Why This Works (Mechanism)
The effectiveness of SetTransformers on parts-to-wholes inference tasks stems from their permutation-equivariant architecture, which naturally handles unordered sets of parts without requiring positional encoding. The synthetic nature of SynDaCaTE provides clean, noise-free part representations that allow SetTransformers to focus purely on learning the compositional relationships between parts and wholes, rather than dealing with real-world ambiguities.

## Foundational Learning

**Part-whole hierarchies**: Understanding how objects can be decomposed into constituent parts and how these parts combine to form wholes is fundamental to compositional reasoning in vision. Needed to grasp why evaluating this capability is important for capsule networks and similar architectures. Quick check: Can you explain how a face can be broken down into eyes, nose, mouth, etc., and how these combine to form the whole face?

**Permutation equivariance**: The property that model outputs remain consistent under permutation of inputs is crucial for parts-to-wholes inference where part order is arbitrary. Needed to understand why SetTransformers excel at this task. Quick check: Can you describe why the order of parts (e.g., eyes, nose, mouth) shouldn't affect the identity of a face in a properly designed model?

**Inductive biases**: The architectural assumptions and constraints that guide model learning are critical for understanding performance differences. Needed to appreciate why different architectures perform differently on hierarchical inference. Quick check: Can you list three different inductive biases used in vision models (e.g., translation equivariance, scale invariance, part-whole compositionality)?

## Architecture Onboarding

**Component map**: Image -> CNN/Capsule/Transformer -> Parts representation -> SetTransformer/Capsule routing -> Character classification/prediction

**Critical path**: The most important processing sequence is Image → Parts extraction → Character inference, as this directly tests the part-whole hierarchical reasoning capability that the paper focuses on evaluating.

**Design tradeoffs**: CNNs prioritize spatial hierarchies and local feature extraction, capsule networks emphasize equivariance and part-whole relationships but struggle with part localization, while SetTransformers excel at set-based reasoning but may lack spatial awareness. The tradeoff is between spatial precision and compositional reasoning flexibility.

**Failure signatures**: Poor performance on parts-to-wholes tasks indicates inability to properly reason about compositional relationships. If a model performs well on image-to-parts but poorly on parts-to-wholes, this suggests the bottleneck is in the compositional reasoning stage rather than feature extraction.

**3 first experiments**:
1. Train a CNN on single-object classification with and without pre-trained parts to verify the benefit of part representations
2. Test SetTransformer on parts-to-characters task with varying numbers of parts to assess robustness to part count variations
3. Evaluate capsule network performance on image-to-parts vs parts-to-wholes tasks to confirm the identified bottleneck

## Open Questions the Paper Calls Out
None

## Limitations
- The synthetic nature of SynDaCaTE may not fully capture real-world visual complexity and noise
- SetTransformers' exceptional performance on synthetic data may not directly translate to real-world applications
- Benefits of pre-trained part representations may vary depending on the quality and diversity of pre-training data

## Confidence

**High confidence**: The bottleneck identification in capsule networks (image-to-parts vs parts-to-wholes inference) and the effectiveness of SetTransformers on synthetic parts-to-wholes tasks

**Medium confidence**: The comparison between CNNs and capsule networks on single-object classification, given potential differences between synthetic and real data

**Medium confidence**: The benefits of pre-trained part representations, which may vary depending on the quality and diversity of the pre-training data

## Next Checks
1. Test SetTransformers on real-world datasets with annotated part-whole hierarchies (e.g., Pascal Part, PartImageNet) to assess generalization beyond synthetic data
2. Conduct ablation studies on the synthetic dataset to identify which specific properties (e.g., object symmetry, part visibility) contribute most to SetTransformers' superior performance
3. Evaluate models on a spectrum of synthetic datasets with increasing complexity (e.g., varying occlusion levels, part deformations) to determine performance breakdown points and identify the most critical inductive biases for real-world deployment