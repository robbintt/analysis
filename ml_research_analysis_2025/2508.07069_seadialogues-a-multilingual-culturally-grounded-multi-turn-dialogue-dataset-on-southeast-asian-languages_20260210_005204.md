---
ver: rpa2
title: 'SEADialogues: A Multilingual Culturally Grounded Multi-turn Dialogue Dataset
  on Southeast Asian Languages'
arxiv_id: '2508.07069'
source_url: https://arxiv.org/abs/2508.07069
tags:
- dialogue
- evaluation
- language
- human
- culturally
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SEADialogues, a multilingual dialogue dataset
  grounded in Southeast Asian cultural contexts. The dataset comprises 32,000 dialogues
  across eight languages from six Southeast Asian countries, incorporating persona
  attributes and culturally relevant topics.
---

# SEADialogues: A Multilingual Culturally Grounded Multi-turn Dialogue Dataset on Southeast Asian Languages

## Quick Facts
- **arXiv ID**: 2508.07069
- **Source URL**: https://arxiv.org/abs/2508.07069
- **Reference count**: 40
- **Primary result**: LLM-generated multilingual dialogue dataset grounded in Southeast Asian cultural contexts with 32,000 dialogues across 8 languages

## Executive Summary
SEADialogues introduces a multilingual dialogue dataset grounded in Southeast Asian cultural contexts, featuring 32,000 dialogues across eight languages from six Southeast Asian countries. The dataset incorporates persona attributes and culturally relevant topics, addressing the critical need for culturally aware dialogue systems in underrepresented linguistic regions. The data generation pipeline uses LLM-generated dialogues guided by manually curated templates and culturally grounded entities, followed by comprehensive human and automatic evaluation.

The evaluation reveals that proprietary models (Gemini 1.5 Flash, GPT-4o mini) outperform open-weight models in conversational quality and instruction-following capabilities when tested on this culturally grounded dataset. G-Eval demonstrates good correlation with human annotations for dialogue quality assessment, though the study identifies opportunities for improvement in evaluating culturally nuanced dialogue elements. This work provides valuable resources for advancing culturally aware dialogue systems and benchmarking multilingual models on Southeast Asian languages.

## Method Summary
The SEADialogues dataset was constructed through a multi-stage pipeline that begins with manually curated templates and culturally grounded entities representing Southeast Asian contexts. These templates and entities were used to guide large language models in generating dialogues across eight languages from six Southeast Asian countries. The generation process incorporated persona attributes to create diverse conversational scenarios. Following generation, the dataset underwent both human evaluation and automatic evaluation using G-Eval, a GPT-4-based evaluator. The evaluation assessed multiple dimensions including conversational quality, instruction-following capabilities, and cultural appropriateness. This comprehensive approach ensured the dataset's quality while maintaining cultural grounding across multiple languages and conversational contexts.

## Key Results
- Proprietary models (Gemini 1.5 Flash, GPT-4o mini) outperform open-weight models in conversational quality and instruction-following on SEADialogues
- G-Eval demonstrates good correlation with human annotations for dialogue quality assessment
- The dataset comprises 32,000 dialogues across eight Southeast Asian languages with integrated persona attributes and culturally relevant topics

## Why This Works (Mechanism)
The success of SEADialogues stems from its culturally grounded approach to dialogue generation, where manually curated templates and entities provide strong contextual anchors for LLM-generated conversations. By incorporating persona attributes and region-specific topics, the dataset captures the nuanced communication patterns unique to Southeast Asian cultures. The multi-stage evaluation process, combining human judgment with automated assessment through G-Eval, ensures both cultural authenticity and technical quality. The use of proprietary models for generation and evaluation leverages their superior performance in handling complex instructions and maintaining conversational coherence, while the dataset itself serves as a benchmark for assessing instruction-following capabilities across different model types.

## Foundational Learning
**Culturally Grounded Dialogue Generation**
*Why needed*: Standard dialogue datasets often lack cultural specificity, leading to models that fail in cross-cultural contexts
*Quick check*: Verify that templates incorporate culturally specific entities, idioms, and conversational norms unique to each Southeast Asian region

**Multilingual Dataset Construction**
*Why needed*: Most dialogue datasets focus on high-resource languages, creating performance gaps for underrepresented languages
*Quick check*: Confirm balanced representation across the eight Southeast Asian languages with comparable dialogue volumes

**Persona-Integrated Dialogue Systems**
*Why needed*: Static dialogue systems struggle with context switching and role adaptation across conversations
*Quick check*: Ensure persona attributes are consistently maintained throughout multi-turn dialogues

**Automatic Dialogue Evaluation with G-Eval**
*Why needed*: Human evaluation is expensive and doesn't scale for large datasets
*Quick check*: Validate G-Eval correlation with human judgments across different dialogue quality dimensions

**Instruction-Following Capability Assessment**
*Why needed*: Traditional dialogue metrics don't capture a model's ability to follow complex conversational instructions
*Quick check*: Test instruction adherence across varied complexity levels and cultural contexts

## Architecture Onboarding

**Component Map**
LLM Generator -> Template Engine -> Persona Module -> Cultural Entity Integrator -> Dialogue Synthesizer -> Human Evaluator -> G-Eval System -> Quality Assurance

**Critical Path**
The most critical path is: Template Engine → Persona Module → Cultural Entity Integrator → LLM Generator, as these components directly determine the cultural grounding and quality of generated dialogues. Any failure in template design or cultural entity selection propagates through the entire pipeline.

**Design Tradeoffs**
The dataset uses LLM-generated rather than naturally occurring dialogues, trading authenticity for scalability and control over cultural elements. Proprietary models were chosen for generation and evaluation due to their superior performance, though this limits accessibility for researchers with resource constraints. The multi-stage evaluation balances comprehensive quality assessment with practical feasibility.

**Failure Signatures**
- Generic or culturally inappropriate responses indicate template or entity selection issues
- Inconsistent persona behavior suggests problems in persona module integration
- Low instruction-following scores point to LLM generation limitations
- Poor G-Eval correlation with human judgments indicates evaluation framework misalignment

**3 First Experiments**
1. Cross-validation of dialogue quality: Run G-Eval and human evaluation on the same dialogue samples to quantify correlation and identify systematic differences
2. Model capability comparison: Test both proprietary and open-weight models on the same instruction-following tasks to measure performance gaps
3. Cultural appropriateness assessment: Have native speakers from each represented country evaluate dialogue authenticity and cultural relevance

## Open Questions the Paper Calls Out
None

## Limitations
- All dialogues are LLM-generated rather than naturally occurring, raising authenticity concerns
- Dataset covers only eight languages from six countries, potentially missing important regional variations
- Evaluation relies heavily on automated metrics which may not fully capture cultural dialogue nuances

## Confidence
- **High Confidence**: Dataset construction methodology, pipeline design, template creation, and multi-stage evaluation process are well-documented and reproducible
- **Medium Confidence**: Proprietary models outperforming open-weight models is supported by evaluation metrics, though cultural grounding comparison could be more nuanced
- **Medium Confidence**: G-Eval correlation with human annotations is demonstrated, but correlation could be stronger for nuanced cultural aspects

## Next Checks
1. Conduct comparative study between SEADialogues and naturally occurring dialogue datasets from same cultural contexts to assess authenticity and cultural accuracy
2. Perform cross-cultural validation study where native speakers from each represented Southeast Asian country evaluate cultural appropriateness and naturalness of dialogues
3. Implement longitudinal evaluation framework to assess how well models trained on SEADialogues maintain cultural appropriateness and instruction-following across different conversation contexts and extended interactions