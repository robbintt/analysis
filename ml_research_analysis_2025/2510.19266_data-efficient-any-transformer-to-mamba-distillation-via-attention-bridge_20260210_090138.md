---
ver: rpa2
title: Data Efficient Any Transformer-to-Mamba Distillation via Attention Bridge
arxiv_id: '2510.19266'
source_url: https://arxiv.org/abs/2510.19266
tags:
- attention
- distillation
- training
- student
- alignment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper addresses the challenge of efficiently transferring\
  \ knowledge from pretrained Transformers to state-space models (SSMs) like Mamba,\
  \ which is difficult due to architectural differences and high computational costs.\
  \ The proposed method, Cross-architecture Distillation via Attention Bridge (CAB),\
  \ introduces a lightweight MLP-based attention bridge to align token-level attention\
  \ projections (B and C in Mamba) with the teacher\u2019s key and query representations,\
  \ enabling fine-grained, data-efficient supervision."
---

# Data Efficient Any Transformer-to-Mamba Distillation via Attention Bridge

## Quick Facts
- arXiv ID: 2510.19266
- Source URL: https://arxiv.org/abs/2510.19266
- Reference count: 40
- The proposed CAB method enables efficient Transformer-to-Mamba distillation, achieving up to 16.3% accuracy gains under limited data and lower perplexity in language tasks.

## Executive Summary
This paper addresses the challenge of efficiently transferring knowledge from pretrained Transformers to state-space models (SSMs) like Mamba, which is difficult due to architectural differences and high computational costs. The proposed method, Cross-architecture Distillation via Attention Bridge (CAB), introduces a lightweight MLP-based attention bridge to align token-level attention projections (B and C in Mamba) with the teacher's key and query representations, enabling fine-grained, data-efficient supervision. This approach avoids the quadratic overhead of full attention matrix alignment and supports flexible layer-wise mapping between heterogeneous architectures. Experiments on ImageNet and language modeling tasks show that CAB consistently outperforms standard and cross-architecture distillation baselines, achieving up to 16.3% accuracy gains under limited data (1%–20% of ImageNet) and lower perplexity in language tasks. CAB also accelerates convergence and reduces memory usage by 10× compared to full-matrix methods.

## Method Summary
CAB addresses cross-architecture distillation by introducing learnable MLP-based projection modules (φ_B, φ_C) that map the student's token-dependent projections (B, C) to the teacher's attention space (K, Q). The method includes two key innovations: (1) Initializing the SSM transition matrix A≈0 to force a linear attention mode, simplifying the structural gap between architectures, and (2) Using proportional layer mapping to align student and teacher layers flexibly despite depth differences. The approach combines attention alignment loss with standard KL divergence, optimizing both objectives during training. Vision models use a single-stage training with bidirectional loss, while language models employ a two-stage process (attention alignment → soft KD).

## Key Results
- Up to 16.3% accuracy gains under limited data (1%-20% of ImageNet) compared to standard distillation
- Lower perplexity in language tasks when transferring from DistilGPT2 to Phi-Mamba
- 10× reduction in memory usage compared to full-matrix alignment methods like MOHAWK
- Faster convergence than soft distillation baselines

## Why This Works (Mechanism)

### Mechanism 1: Structural Analogy Projection
- **Claim:** Aligning Mamba's token-dependent projections (B, C) with Transformer keys and queries (K, Q) transfers relational inductive biases without requiring explicit attention matrices.
- **Mechanism:** In Mamba, the recurrent state h_t accumulates history modulated by B, while C extracts output from this state. The paper posits that B and C function as implicit carriers of attention (similar to Keys and Queries). By using lightweight MLPs (φ_B, φ_C) to project these SSM parameters into the Transformer's feature space, the student learns the "retrieval patterns" of the teacher.
- **Core assumption:** The semantic roles of Mamba's input/output projections are functionally isomorphic to the Transformer's Key/Query mechanism under the distillation objective.
- **Evidence anchors:**
  - [abstract] "...aligns the token-dependent projections in Mamba (B, C) with the key and query representations (K, Q) in Transformers..."
  - [section 3.2] "...we introduce learnable MLP-based projection modules that map the student's B and C into the teacher's attention space."
  - [corpus] *"On Structured State-Space Duality"* confirms that a state-space model with a scalar-identity state matrix is equivalent to masked self-attention, validating the theoretical bridge.
- **Break condition:** If the student architecture relies heavily on non-linear state transitions that cannot be approximated by linear attention, the B ↔ K analogy may fail.

### Mechanism 2: Initialization-Induced Linear Attention Mode
- **Claim:** Initializing the SSM transition matrix A ≈ 0 (such that discrete Ā ≈ I) forces the Mamba block into an additive recurrence structurally equivalent to linear attention, facilitating distillation.
- **Mechanism:** Standard SSM initialization (e.g., S4D) captures complex temporal dynamics that diverge from the static weights of Transformer attention. By forcing Ā ≈ I, the recurrence simplifies to h_t ≈ h_{t-1} + input, mirroring the additive nature of linear attention. This reduces the structural gap between teacher and student.
- **Core assumption:** A simplified, identity-like recurrent transition is a better starting point for receiving Transformer knowledge than a complex, memory-heavy initialization.
- **Evidence anchors:**
  - [section 3.1] "When the transition matrix Ā approximates the identity... this recurrence simplifies into a form structurally equivalent to the recursive formulation of linear attention."
  - [section 4.3] "Align B+C w/ default Ā [resulted in] 43.1 [accuracy]; Align B+C w/ Ā ≈ I [resulted in] 49.2."
- **Break condition:** If the downstream task specifically requires the long-range decay characteristics of complex S4-style diagonal matrices, this initialization might degrade the student's theoretical capacity.

### Mechanism 3: Flexible Hierarchical Mapping
- **Claim:** Proportional layer mapping (mapping student layer l to teacher layer ⌊l/L · T⌋) accommodates architectural depth discrepancies more effectively than strict 1-to-1 alignment.
- **Mechanism:** Student and teacher models often differ in depth. Strict alignment forces mismatched semantic levels to interact (e.g., aligning a student's early layer to a teacher's early layer despite different receptive field growth). Proportional mapping ensures that "mid-level" features in the student align with "mid-level" features in the teacher, preserving the semantic hierarchy.
- **Core assumption:** Semantic complexity scales roughly linearly with layer depth across both architectures.
- **Evidence anchors:**
  - [section 3.2] "...we define a general layer mapping function g(l) that aligns each student layer... via proportional indexing."
  - [section 4.2] "...similarity rises sharply [in later layers], showing that our alignment loss effectively guides Vim to mimic Transformer-style attention."
- **Break condition:** If the teacher and student architectures process features at vastly different rates (e.g., one uses residual connections heavily while the other doesn't), proportional mapping may misalign critical functional blocks.

## Foundational Learning

- **Concept:** **Linear Attention & RNN Duality**
  - **Why needed here:** The paper relies on the mathematical equivalence between Mamba (an SSM) and Linear Attention (a Transformer approximation). You cannot understand why B maps to K without grasping that both are mechanisms for storing/retrieving history in a linear recurrent structure.
  - **Quick check question:** If you set the state transition A=I in an RNN, does the hidden state grow unbounded, and how does this relate to the "cumulative sum" nature of linear attention?

- **Concept:** **Knowledge Distillation (Logits vs. Features)**
  - **Why needed here:** The paper critiques "soft distillation" (logits) as insufficient and proposes "Attention Bridge" (feature-level). You need to distinguish between matching output probabilities (what the model decides) and matching internal representations (how the model thinks).
  - **Quick check question:** Why would matching internal attention maps be more beneficial for a student model than just matching the final soft labels, particularly for long-range dependencies?

- **Concept:** **SSM Discretization (Ā, B̄, C̄)**
  - **Why needed here:** The mechanism requires manipulating the continuous parameters (A) to change the discrete behavior (Ā). Understanding how the step size Δ transforms continuous dynamics into discrete steps is crucial for the initialization mechanism.
  - **Quick check question:** In the Mamba architecture, which parameter controls the "memory" decay, and how does initializing A ≈ 0 (forcing Ā ≈ I) effectively turn the model into a "perfect memory" accumulator similar to standard attention?

## Architecture Onboarding

- **Component map:**
  - Teacher: Pretrained Transformer (e.g., DeiT, DistilGPT2) -> Frozen. Extracts K, Q.
  - Student: Mamba/Vim model -> Trainable. Extracts B, C.
  - Bridge: Two lightweight 2-layer MLPs (φ_B, φ_C). Projects Student's B/C dimensions to Teacher's K/Q dimensions.
  - Aligner: Layer mapping function g(l).

- **Critical path:**
  1. Forward pass Teacher and Student.
  2. Hook K, Q from Teacher and B, C from Student at mapped layers g(l).
  3. Project Student projections: K̂ = φ_B(B), Q̂ = φ_C(C).
  4. Compute MSE Loss: L_attn = ||K̂ - K||² + ||Q̂ - Q||².
  5. Combine with standard Distillation Loss (KL divergence).

- **Design tradeoffs:**
  - **Efficiency vs. Granularity:** Unlike MOHAWK (which aligns full N × N matrices), CAB aligns N × d projections. This is 10× more memory efficient but assumes B/C capture sufficient attention info.
  - **Training Speed:** CAB uses a "Bridge" to adapt features rather than forcing the student to mimic raw weights (Weight Reuse) or full matrices (MOHAWK). The paper notes faster convergence than Soft Distillation.

- **Failure signatures:**
  - **Collapse:** Directly injecting attention weights or full matrices into Mamba causes "performance collapse" (Fig 1b). Use the MLP bridge instead.
  - **Over-constraining:** In full-data regimes, running CAB for the entire training duration degrades performance. The paper suggests early stopping the alignment loss (e.g., switch to KL-only after 35-50 epochs).

- **First 3 experiments:**
  1. **Sanity Check (Dimension Alignment):** Run the bridge MLPs on a single batch. Verify that φ_B(B) outputs shapes matching K exactly and that the loss converges to near zero if you overfit on that single batch.
  2. **Ablation (Initialization):** Train a small student (Vim-Tiny) on 10% ImageNet with standard S4D initialization vs. the proposed A ≈ 0 initialization. Confirm the accuracy gap (reported as ~6% in paper).
  3. **Convergence Test:** Compare "Vanilla Training" vs. "CAB" curves on a low-data run (1% ImageNet). Verify that CAB reaches its peak accuracy in roughly half the epochs of the vanilla baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the CAB framework be effectively generalized to other State Space Model (SSM) variants (e.g., S4, RWKV) or hybrid architectures without manual architectural tuning?
- **Basis in paper:** [Explicit] The conclusion states, "our approach currently focuses on Mamba and Transformers, and extending it to other SSM variants or more complex hybrid architectures may require further investigation."
- **Why unresolved:** The method relies on aligning specific projections (B, C) with (K, Q), a mapping derived specifically from Mamba's mathematical formulation, which may not hold for other recurrent mechanisms.
- **What evidence would resolve it:** Empirical results applying the same distillation framework to non-Mamba SSMs or hybrid Transformer-SSM models.

### Open Question 2
- **Question:** What are the underlying dynamics that cause CAB to degrade performance in full-data regimes when applied continuously, and how can this be adaptively mitigated?
- **Basis in paper:** [Explicit] Appendix A.1 notes that enforcing CAB alignment throughout training (e.g., 300 epochs) degrades performance, hypothesizing that "excessive attention supervision can over-constrain the mismatch."
- **Why unresolved:** The paper observes the phenomenon and uses early stopping as a heuristic fix, but does not deeply analyze why the alignment becomes detrimental as the model converges.
- **What evidence would resolve it:** A study on the gradient conflict or representation geometry of the student model during the over-constrained phase compared to the early-stopped phase.

### Open Question 3
- **Question:** Does initializing the transition matrix Ā ≈ I to facilitate distillation permanently limit the student's ability to model long-range dependencies compared to standard Mamba initialization?
- **Basis in paper:** [Inferred] The ablation study (Table 5) shows initializing Ā ≈ I improves distillation performance. However, this initialization simplifies the recurrence to be structurally equivalent to linear attention, potentially sacrificing the unique dynamic properties of SSMs.
- **Why unresolved:** While effective for the distillation task, it is unclear if this specific initialization required for the "bridge" acts as a capacity bottleneck for complex, long-horizon reasoning.
- **What evidence would resolve it:** Evaluating the distilled student on specific long-context retrieval benchmarks against a baseline Mamba trained from scratch with standard initialization.

## Limitations
- CAB's benefits diminish in full-data regimes, requiring early stopping of alignment loss to prevent performance degradation
- The method assumes a roughly linear progression of semantic complexity across layers, which may not hold for all architectures
- Several critical implementation details remain underspecified, including exact MLP bridge dimensions and loss weighting

## Confidence
- **High Confidence:** The core CAB framework (MLP bridges projecting B/C to K/Q via layer mapping) is clearly defined and experimentally validated across both vision and language tasks. The superiority over baselines under limited data is well-supported.
- **Medium Confidence:** The initialization mechanism (A≈0 forcing linear attention mode) is theoretically sound and shown to improve accuracy in ablation studies, but the exact implementation details for A initialization are unclear.
- **Low Confidence:** The precise hyperparameters for MLP bridge dimensions, loss weighting λ, and specific A initialization values are missing, making exact reproduction difficult.

## Next Checks
1. **Initialization Ablation:** Train a small student (Vim-Tiny) on 10% ImageNet with standard S4D initialization vs. the proposed A ≈ 0 initialization. Confirm the reported ~6% accuracy gap and verify that the simplified recurrence truly approximates linear attention.

2. **MLP Bridge Sensitivity:** Vary the hidden dimensions of the φ_B and φ_C MLPs (e.g., same as teacher K/Q dim vs. smaller hidden layers) and measure the impact on convergence speed and final accuracy on a low-data vision task.

3. **Loss Weighting Impact:** Perform a sweep over λ (e.g., 0.1, 1.0, 10.0) balancing L_attn and L_KL during training on a small language model (e.g., DistilGPT2 → Phi-Mamba-123M) to identify the optimal trade-off between alignment fidelity and KD stability.