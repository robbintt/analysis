---
ver: rpa2
title: 'Limitations of Public Chest Radiography Datasets for Artificial Intelligence:
  Label Quality, Domain Shift, Bias and Evaluation Challenges'
arxiv_id: '2509.15107'
source_url: https://arxiv.org/abs/2509.15107
tags:
- datasets
- dataset
- performance
- chest
- chexpert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Public chest radiography datasets, while enabling large-scale
  AI development, suffer from critical limitations that undermine clinical reliability.
  This study systematically evaluates three core challenges: label quality, domain
  shift, and dataset bias.'
---

# Limitations of Public Chest Radiography Datasets for Artificial Intelligence: Label Quality, Domain Shift, Bias and Evaluation Challenges

## Quick Facts
- **arXiv ID:** 2509.15107
- **Source URL:** https://arxiv.org/abs/2509.15107
- **Authors:** Amy Rafferty; Rishi Ramaesh; Ajitha Rajan
- **Reference count:** 40
- **Primary result:** Public chest radiography datasets exhibit critical label quality issues, severe domain shift, and demographic bias that undermine clinical reliability.

## Executive Summary
Public chest radiography datasets, while enabling large-scale AI development, suffer from critical limitations that undermine clinical reliability. This study systematically evaluates three core challenges: label quality, domain shift, and dataset bias. Automated NLP-based label extraction introduces substantial errors, particularly with negation and uncertainty, while expert radiologist review frequently disagrees with assigned labels. Domain shift between datasets causes significant performance degradation, with pronounced drops in AUPRC and F1 scores under cross-dataset evaluation. Dataset bias analysis reveals strong site-specific acquisition signatures and systematic performance disparities across demographic subgroups. A source-classification model achieved near-perfect accuracy in distinguishing datasets, indicating that models may exploit non-clinical cues. Expert audit by two board-certified radiologists found overall label agreement below 60%, with many discrepancies attributable to report-to-label extraction errors. These findings demonstrate that current public datasets, despite their scale, provide an inadequate foundation for reliable clinical deployment without systematic expert validation, fairer evaluation frameworks, and demographically diverse, multi-centre curation.

## Method Summary
This study evaluates label quality, domain shift, and dataset bias across four public chest radiography datasets (MIMIC-CXR, CheXpert, ChestX-ray14, PadChest) using a harmonized 6-label classification task (No Finding, Pneumonia, Pneumothorax, Cardiomegaly, Lung Cancer, Pleural Effusion). Seven CNN architectures (ResNet50/101, ResNeXt101, DenseNet161, ConvNeXt-S/B, EfficientNetV2-S) were trained on PA views with patient-level 80/10/10 splits and one-sided selection undersampling. Models were evaluated using macro-averaged AUROC, AUPRC, and threshold-dependent F1/Sensitivity/Specificity metrics. External validation assessed domain shift, while a DenseNet-161 source-classification model measured dataset bias. Expert review by two radiologists validated label accuracy, and subgroup analyses examined demographic performance disparities.

## Key Results
- Automated NLP label extraction introduces substantial errors, with expert radiologist review finding <60% agreement with assigned labels
- Domain shift causes severe performance degradation in cross-dataset evaluation, with AUPRC and F1 scores collapsing while AUROC remains deceptively high
- Source-classification models achieved ~99% accuracy in distinguishing datasets, revealing site-specific acquisition signatures that models exploit as shortcuts
- Subgroup analyses showed consistently lower F1 scores for underrepresented demographic groups (e.g., Patients <40 in PadChest: F1 0.339 vs. Overall 0.815)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Automated label extraction via NLP introduces systematic noise that models may overfit, creating a divergence between "dataset truth" and clinical reality.
- **Mechanism:** Rule-based labellers (e.g., CheXpert, NegBio) fail to parse negation ("no evidence of pneumonia") and uncertainty correctly, assigning positive labels to negative reports. Deep learning models optimize for these flawed labels, learning annotation errors rather than pathology.
- **Core assumption:** The paper assumes that expert radiologist review (achieving <60% agreement with labels) is a superior proxy for ground truth than the NLP-derived labels.
- **Evidence anchors:**
  - [Abstract] "Automated label extraction... introduces errors, particularly in handling uncertainty and negation."
  - [Section 5(a)] Expert audit showed overall agreement below 60%, with discrepancies attributable to extraction errors.
  - [Corpus] Related work (arXiv:2510.05664) suggests LLM-based extraction with uncertainty adjustment may mitigate this, confirming standard extraction is noisy.
- **Break condition:** If NLP pipelines perfectly captured nuance or if models were robust to label noise (which evidence suggests they are not), this mechanism would fail.

### Mechanism 2
- **Claim:** Deep learning models exploit non-pathological "site-specific signatures" (shortcuts) to achieve high internal performance, causing collapse during external validation.
- **Mechanism:** Datasets contain unique artefacts (text markers, processing fingerprints, ICU devices). Models minimize loss by learning these easy-to-classify features instead of disease patterns. When the deployment site lacks these exact artefacts, the shortcut logic fails.
- **Core assumption:** The near-perfect performance of a source-classification model implies the presence of learnable non-pathological features.
- **Evidence anchors:**
  - [Section 4(a)] A DenseNet-161 classifier distinguished between datasets (MIMIC, CheXpert, etc.) with ~99% accuracy using images alone.
  - [Section 3(e)(ii)] External evaluation shows models retaining high Sensitivity but near-zero Specificity, indicating reliance on dataset-specific cues that do not generalize.
  - [Corpus] Weak direct evidence in corpus for this specific "source classification" causality, though "Counterfactual contrastive learning" (arXiv:2403.09605) addresses similar robustness issues.
- **Break condition:** If datasets were harmonized to remove text overlays, standardize processing, and remove device markers, this shortcut mechanism would likely break.

### Mechanism 3
- **Claim:** Skewed demographic distributions in training data cause performance degradation for underrepresented subgroups (minority age/sex).
- **Mechanism:** Model optimization is dominated by majority groups (e.g., Males 40-65). Features associated with minority groups (e.g., Females, Age <40) are treated as noise or under-weighted, leading to poor feature learning for these subgroups.
- **Core assumption:** Performance differences are driven by data distribution rather than inherent biological difficulty in imaging.
- **Evidence anchors:**
  - [Section 4(b)] Subgroup analysis revealed consistently lower F1 scores for minority groups (e.g., Patients <40 in PadChest: F1 0.339 vs. Overall 0.815).
  - [Section 4(b)] CheXpert models performed better on males (AUROC 0.853) than females (0.798), mirroring the dataset's 65% male skew.
  - [Corpus] Neighbors do not strongly link to this specific demographic mechanism in the provided text.
- **Break condition:** If training used stratified sampling or re-weighting to balance demographic contributions, this degradation would likely diminish.

## Foundational Learning

- **Concept:** **AUPRC vs. AUROC**
  - **Why needed here:** The paper demonstrates that AUROC can remain deceptively high even when a model is failing clinically (low Specificity). AUPRC (Area Under Precision-Recall Curve) is required to see the true impact of domain shift on the positive class.
  - **Quick check question:** If a model has an AUROC of 0.90 but an AUPRC of 0.20, is it ready for deployment on a rare disease? (Answer: No, likely poor precision).

- **Concept:** **Threshold Calibration & Shift**
  - **Why needed here:** Models trained on Dataset A optimize thresholds for Dataset A's prevalence and artefacts. Applying these fixed thresholds to Dataset B results in massive miscalibration (High Sensitivity, near-zero Specificity).
  - **Quick check question:** Why can't you use the same classification threshold for an internal test set and an external hospital dataset? (Answer: Domain shift changes the output probability distributions).

- **Concept:** **Shortcut Learning**
  - **Why needed here:** Understanding that high accuracy doesn't prove the model "knows" the pathology. It may simply recognize a text tag or hospital logo.
  - **Quick check question:** A model detects pneumonia with 99% accuracy but fails when you crop the image. What happened? (Answer: It likely used a shortcut/spurious correlation like a text overlay).

## Architecture Onboarding

- **Component map:** Input PA views -> CNN backbone (DenseNet-161, EfficientNet, ResNet) -> Classifier head -> 6 harmonized labels (No Finding, Pneumonia, Pneumothorax, Cardiomegaly, Lung Cancer, Pleural Effusion)
- **Critical path:** 1. Harmonize labels across datasets (map "Mass" to "Lung Cancer", etc.) 2. Train on Source Dataset (e.g., PadChest) 3. Calibrate Thresholds on Source Validation Set (Optimize F1) 4. Freeze Model & Thresholds 5. Evaluate on Target Dataset (e.g., MIMIC-CXR) to measure degradation
- **Design tradeoffs:** Scale vs. Quality: Large public datasets offer scale but suffer from label noise; smaller datasets are cleaner but insufficient for deep learning. Metric Selection: AUROC is useful for ranking but hides failure modes; F1/AUPRC are better for clinical reliability under imbalance.
- **Failure signatures:** The "High Sens / Low Spec" Collapse: When moving from internal to external data, Sensitivity often stays >0.9 while Specificity drops to <0.1. This indicates the model is over-predicting the positive class because the threshold is miscalibrated for the new domain. Source Leakage: If a classifier can identify which hospital an X-ray came from with >95% accuracy, the dataset contains "source leaks" (artefacts) that will harm generalization.
- **First 3 experiments:** 1. Source Classification Audit: Train a classifier solely to distinguish between datasets (MIMIC vs. CheXpert). High accuracy confirms the presence of non-pathological artefacts. 2. Cross-Dataset Stress Test: Train on PadChest, test on CheXpert. Specifically track the drop in AUPRC and F1 relative to AUROC to quantify "hidden" degradation. 3. Subgroup Slice Analysis: Run the trained model on specific age buckets (<40 vs. >65). If performance diverges significantly, the data is demographically biased.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal strategy for handling uncertainty labels in automated NLP extraction pipelines to maximize clinical reliability without discarding diagnostically ambiguous information?
- Basis in paper: [explicit] Section 2(a) states that "There are currently conflicting methods on how to deal with uncertainties" and that "There is no consensus," while Section 6 lists "explicit handling of uncertainty" as a requirement for improving clinical reliability.
- Why unresolved: Current datasets handle uncertainty inconsistently (e.g., collapsing to binary vs. retaining as a distinct class), and the impact of these differing strategies on model calibration and false-positive rates is not fully characterized.
- What evidence would resolve it: A comparative study evaluating models trained on the same datasets but with different uncertainty handling protocols (e.g., ignored vs. probabilistic vs. separate class), measuring calibration error and F1 scores on external datasets.

### Open Question 2
- Question: Can domain adaptation techniques or targeted data augmentation effectively mitigate the severe performance degradation (AUPRC and F1 drops) observed in cross-dataset evaluations?
- Basis in paper: [inferred] Section 3 demonstrates significant performance collapse during external validation, concluding that "dataset design... is the dominant constraint." However, the study does not test specific algorithmic interventions to counter this shift, noting only that multi-centre curation is currently infeasible.
- Why unresolved: While the paper establishes that domain shift is a critical failure mode, it remains unclear if algorithmic domain adaptation can correct for the "site-specific acquisition signatures" identified in Section 4.
- What evidence would resolve it: Experiments applying state-of-the-art domain adaptation methods to the source–target pairs identified as most problematic in the paper (e.g., PadChest → CheXpert) to see if the external AUPRC drops can be recovered.

### Open Question 3
- Question: Does the integration of radiologist confidence scales and multi-reader consensus into label generation significantly reduce the "expert-label" disagreement observed in automated datasets?
- Basis in paper: [explicit] Section 5 concludes that "Future studies should incorporate confidence scales and larger, multi-reader panels to better characterise label quality," noting that the current binary evaluation was limited by sample size and protocol.
- Why unresolved: The study found expert agreement with labels was below 60%, but it is unknown if this reflects irreducible ambiguity or if a more granular labelling protocol (confidence rather than binary) would align better with the NLP text.
- What evidence would resolve it: Re-annotating the high-disagreement subsets identified in the paper using a multi-reader protocol with confidence intervals, then correlating these refined labels with model prediction entropy.

## Limitations
- The study relies on NLP-derived labels for three of four datasets, introducing inherent noise even with expert validation
- Expert review involved only two radiologists, limiting the robustness of the ground truth assessment
- The paper demonstrates domain shift but does not explore mitigation strategies beyond dataset harmonization

## Confidence
- **High Confidence:** Presence of domain shift and dataset-specific artefacts (Source classification accuracy >99%)
- **Medium Confidence:** NLP label extraction errors as primary source of label noise (expert agreement <60%)
- **Low Confidence:** Demographic bias primarily driven by skewed data distributions rather than inherent imaging difficulty

## Next Checks
1. Manually review 100 randomly sampled reports to quantify relative frequency of negation errors vs. uncertainty errors in NLP extraction pipeline
2. Apply domain adaptation techniques (e.g., domain adversarial training) to cross-dataset models and measure impact on AUPRC/F1 retention
3. Train models with stratified sampling or re-weighting by demographic group to determine if performance disparities can be reduced independently of overall accuracy