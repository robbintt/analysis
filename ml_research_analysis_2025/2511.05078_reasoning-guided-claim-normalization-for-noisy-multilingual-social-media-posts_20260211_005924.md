---
ver: rpa2
title: Reasoning-Guided Claim Normalization for Noisy Multilingual Social Media Posts
arxiv_id: '2511.05078'
source_url: https://arxiv.org/abs/2511.05078
tags:
- claim
- posts
- normalization
- languages
- english
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a reasoning-guided claim normalization system
  for multilingual social media posts using Qwen3-14B fine-tuned with LoRA. The approach
  decomposes posts using 5W1H questions (Who, What, Where, When, Why, How) to extract
  clear, verifiable claims across 20 languages, training exclusively on English data.
---

# Reasoning-Guided Claim Normalization for Noisy Multilingual Social Media Posts

## Quick Facts
- **arXiv ID**: 2511.05078
- **Source URL**: https://arxiv.org/abs/2511.05078
- **Reference count**: 32
- **Primary result**: 5W1H reasoning framework improves METEOR by 41.3% over baseline configurations

## Executive Summary
This paper presents a reasoning-guided claim normalization system for multilingual social media posts using Qwen3-14B fine-tuned with LoRA. The approach decomposes posts using 5W1H questions (Who, What, Where, When, Why, How) to extract clear, verifiable claims across 20 languages, training exclusively on English data. Key innovations include semantic alignment filtering with token-level recall thresholds and retrieval-augmented few-shot learning during inference. The system achieves METEOR scores from 41.16 (English) to 15.21 (Marathi), securing third rank on the English leaderboard and fourth rank for Dutch and Punjabi.

## Method Summary
The system uses Qwen3-14B fine-tuned with LoRA for claim normalization across 20 languages. Posts are decomposed using 5W1H questions (Who, What, Where, When, Why, How) to extract clear, verifiable claims. Semantic alignment filtering with token-level recall thresholds identifies claims matching source posts, while retrieval-augmented few-shot learning is employed during inference. The model is trained exclusively on English data but demonstrates cross-lingual transfer capabilities. Performance is evaluated using METEOR scores, achieving significant improvements over baseline configurations.

## Key Results
- 41.3% METEOR improvement from 5W1H reasoning framework over baseline configurations
- Third rank on English leaderboard and fourth rank for Dutch and Punjabi languages
- METEOR scores range from 41.16 (English) to 15.21 (Marathi), showing varying cross-lingual transfer effectiveness
- Semantic alignment filtering with token-level recall thresholds successfully identifies matching claims across languages

## Why This Works (Mechanism)
The 5W1H reasoning framework provides structured decomposition of social media posts into verifiable claims by systematically addressing key information dimensions. This structured approach helps the model identify and extract the most salient factual content from noisy, informal text. The semantic alignment filtering ensures that generated claims maintain fidelity to the original post content while the retrieval-augmented few-shot learning during inference allows adaptation to specific post characteristics without additional fine-tuning.

## Foundational Learning
- **5W1H Question Framework**: Systematic questioning approach (Who, What, Where, When, Why, How) needed for structured information extraction from unstructured text. Quick check: Apply to sample social media posts to verify coverage of key information elements.
- **Cross-Lingual Transfer**: Ability to apply knowledge from one language (English) to multiple target languages. Quick check: Compare performance across language families to identify transfer patterns.
- **Semantic Alignment Filtering**: Token-level matching technique for verifying claim-post correspondence. Quick check: Test with varying recall thresholds to assess precision-recall trade-offs.
- **Retrieval-Augmented Few-Shot Learning**: Technique for adapting model outputs using retrieved examples without fine-tuning. Quick check: Evaluate performance with different numbers of retrieved examples.

## Architecture Onboarding

**Component Map**: Raw Posts -> 5W1H Decomposition -> Qwen3-14B LoRA Model -> Semantic Alignment Filtering -> Claim Output

**Critical Path**: The core processing pipeline follows Raw Posts → 5W1H Decomposition → Qwen3-14B LoRA → Semantic Alignment Filtering → Final Claims, with retrieval-augmented few-shot learning integrated at the inference stage.

**Design Tradeoffs**: The system trades training efficiency (English-only training) for cross-lingual generalization capabilities, accepting variable performance across language families. Semantic alignment filtering introduces computational overhead but improves claim fidelity at the risk of filtering out valid claims.

**Failure Signatures**: Low METEOR scores for morphologically complex or low-resource languages indicate limitations in zero-shot cross-lingual transfer. Claims that are semantically correct but lack token-level overlap may be incorrectly filtered out.

**3 First Experiments**:
1. Test 5W1H decomposition effectiveness on sample posts across different languages
2. Evaluate semantic alignment filtering performance with varying token-level recall thresholds
3. Measure cross-lingual transfer performance between closely related language pairs

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Would incorporating native-language training data for low-resource languages (e.g., Marathi, Bengali, Tamil) significantly improve METEOR scores compared to English-only training?
- Basis in paper: [explicit] "Future work includes language-specific fine-tuning to accomodate additional low-resource languages"
- Why unresolved: The current zero-shot approach for Marathi (METEOR: 15.21) substantially underperforms English (41.16), but the authors trained exclusively on English data.
- What evidence would resolve it: Fine-tune separate models using available training data in each target language and compare METEOR improvements.

### Open Question 2
- Question: Does the system generalize across different social media platforms beyond the Twitter/Reddit/Facebook sources in the current dataset?
- Basis in paper: [explicit] "Future work includes...testing generalizability across different social media platforms"
- Why unresolved: Posts are sourced from specific platforms with distinct linguistic patterns; transfer to platforms like TikTok or Instagram captions remains untested.
- What evidence would resolve it: Evaluate model performance on posts from platforms not represented in training data.

### Open Question 3
- Question: What is the optimal token-level recall threshold for semantic alignment filtering?
- Basis in paper: [inferred] The 0.4 recall threshold was selected without systematic comparison to alternative values.
- Why unresolved: No ablation comparing different threshold values (e.g., 0.3 vs. 0.4 vs. 0.5) is provided to justify this choice.
- What evidence would resolve it: Grid search over threshold values with validation set METEOR scores.

### Open Question 4
- Question: Which individual 5W1H components contribute most to claim normalization quality?
- Basis in paper: [inferred] Ablation only tested full 5W1H framework as a unit, not individual question types.
- Why unresolved: Unknown whether all six questions are necessary or if subset (e.g., Who, What, Where) suffices.
- What evidence would resolve it: Incremental ablation removing each WH question independently and measuring performance delta.

## Limitations
- Significant cross-lingual transfer gaps with METEOR scores declining sharply for non-Romance/Germanic languages (e.g., 15.21 for Marathi vs 41.16 for English)
- Exclusive use of English training data raises questions about scalability and performance for morphologically complex languages
- Potential cultural or contextual misalignments when applying English-centric 5W1H reasoning patterns to languages with different discourse structures

## Confidence

**High Confidence**: The 41.3% METEOR improvement from 5W1H reasoning framework is well-supported by leaderboard performance metrics and the systematic decomposition approach.

**Medium Confidence**: Cross-lingual transfer effectiveness claims require caution, as performance varies dramatically across language families. The assertion that the approach "demonstrates effective transfer for Romance and Germanic languages" is supported by results but lacks deeper linguistic analysis.

**Low Confidence**: The paper's generalizability to languages beyond the 20 tested, particularly low-resource or structurally divergent languages, remains unproven. Claims about semantic coherence maintenance across diverse linguistic structures are not empirically validated beyond METEOR scoring.

## Next Checks
1. **Linguistic Family Analysis**: Conduct detailed error analysis comparing performance across language families (e.g., isolating Indo-European vs non-Indo-European performance) to identify systematic transfer limitations and potential adaptation strategies.

2. **Token Filtering Impact Assessment**: Implement ablation studies to quantify the trade-off between semantic alignment filtering precision and recall, measuring how different token-level recall thresholds affect overall claim normalization quality.

3. **Zero-Shot Cross-Lingual Transfer**: Evaluate the system's performance on languages not included in the 20-language test set using truly zero-shot transfer, measuring degradation patterns to better understand the approach's practical scalability limits.