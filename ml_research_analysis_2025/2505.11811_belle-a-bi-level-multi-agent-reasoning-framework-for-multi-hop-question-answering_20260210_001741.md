---
ver: rpa2
title: 'BELLE: A Bi-Level Multi-Agent Reasoning Framework for Multi-Hop Question Answering'
arxiv_id: '2505.11811'
source_url: https://arxiv.org/abs/2505.11811
tags:
- multi-hop
- question
- debater
- debate
- type
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents BELLE, a bi-level multi-agent reasoning framework
  for multi-hop question answering that dynamically combines different reasoning methods
  based on question types. The authors analyze four types of multi-hop questions (Inference,
  Comparison, Temporal, Null) and find that different question types benefit from
  different combinations of reasoning operators (Chain-of-Thought, Single-step, Iterative-step,
  Sub-step, Adaptive-step).
---

# BELLE: A Bi-Level Multi-Agent Reasoning Framework for Multi-Hop Question Answering

## Quick Facts
- arXiv ID: 2505.11811
- Source URL: https://arxiv.org/abs/2505.11811
- Reference count: 40
- Primary result: BELLE achieves up to 70.4 F1 on MultiHop-RAG and 75.7 F1 on 2WikiMultiHopQA by dynamically combining reasoning operators based on question types.

## Executive Summary
This paper introduces BELLE, a bi-level multi-agent reasoning framework for multi-hop question answering that dynamically selects optimal combinations of reasoning operators based on question types. The framework analyzes four multi-hop question types (Inference, Comparison, Temporal, Null) and finds that different types benefit from different reasoning operator combinations. BELLE employs a bi-level debate system with opposing debaters, fast and slow memory debaters, and a judge to determine optimal operator combinations. Experiments on four multi-hop QA datasets show BELLE significantly outperforms strong baselines while maintaining superior computational cost-effectiveness.

## Method Summary
BELLE operates through a question-type classifier that categorizes questions into one of four types, followed by a bi-level multi-agent debate system that proposes and refines operator combinations. The framework uses five reasoning operators (Chain-of-Thought, Single-step, Iterative-step, Sub-step, Adaptive-step) selected based on the question type. A bi-level debate involves affirmative and negative debaters proposing combinations, while fast and slow debaters monitor reasonableness and synthesize historical information. The judge consolidates the final plan, which is executed by a multi-hop QA executor using retrieval via BM25 on Wikipedia dumps.

## Key Results
- BELLE achieves 70.4 F1 on MultiHop-RAG and 75.7 F1 on 2WikiMultiHopQA, outperforming strong baselines
- Operator sensitivity analysis shows different question types require different reasoning combinations
- BELLE demonstrates superior cost-effectiveness compared to single models, particularly for complex multi-hop questions
- Bi-level debate with fast/slow debaters improves accuracy over single-level debate approaches

## Why This Works (Mechanism)

### Mechanism 1: Bi-Level Debate for Operator Planning
- Claim: Two-level multi-agent debate produces more robust execution plans than single-level approaches
- Mechanism: First level involves affirmative/negative debaters proposing operator combinations; second level introduces fast debater (evaluates current round) and slow debater (synthesizes all history) to stabilize viewpoints and prevent oscillation
- Core assumption: Combining immediate context with historical consensus reduces bias and improves plan quality
- Evidence anchors: [abstract] "we further leverage fast and slow debaters to monitor whether changes in viewpoints are reasonable"; [Section 4.2.2] slow debater integrates historical information to prevent oscillation

### Mechanism 2: Question-Type-Driven Operator Selection
- Claim: Mapping question types to tailored operator combinations improves performance over uniform approaches
- Mechanism: Classifier categorizes question, then debate selects from operator pool (CoT, Single-step, Iterative-step, Sub-step, Adaptive-step) to form plan aligned with type's needs
- Core assumption: Question types have inherent structural differences making certain reasoning paths more effective
- Evidence anchors: [abstract] "different types of multi-hop questions have varying degrees of sensitivity to different types of methods"; [Section 3, Figure 2] shows performance variation across types

### Mechanism 3: Computational Cost-Effectiveness via Adaptive Planning
- Claim: BELLE achieves higher accuracy per token cost by avoiding unnecessary steps
- Mechanism: Debate-based planning selects minimal sufficient operator sequence, reducing redundant LLM calls
- Core assumption: Accurate upfront planning reduces overall inference steps, offsetting debate overhead
- Evidence anchors: [abstract] "the model consumption of BELLE is higher cost-effectiveness than that of single models"; [Section 5.2.2, Figure 6] shows better F1 with lower token usage

## Foundational Learning

Concept: Multi-Hop Question Answering
- Why needed here: BELLE is designed for questions requiring reasoning across multiple documents/steps
- Quick check question: Can you give an example of a 2-hop question where the answer requires information from two distinct sources?

Concept: Multi-Agent Debate (MAD) Systems
- Why needed here: BELLE's core is a bi-level MAD system
- Quick check question: What is a potential risk of a single-level debate where agents only see the current round's arguments?

Concept: Reasoning Operators in LLMs
- Why needed here: BELLE treats different reasoning methods as interchangeable operators
- Quick check question: When might "Iterative-step" retrieval be preferable over "Single-step" retrieval for a multi-hop question?

## Architecture Onboarding

Component map: Question Type Classifier → Bi-Level MAD (Affirmative/Negative debaters → Fast/Slow debaters → Judge) → Multi-hop QA Executor → Final Answer

Critical path: Question → Classifier → Debate (all agents contribute in sequence) → Judge → Executor → Final Answer

Design tradeoffs:
- **Accuracy vs. Cost**: More debate rounds improve plan quality but increase token usage; default is 2 debaters per level, 2-3 rounds
- **Rigidity vs. Flexibility**: Predefined operator pool ensures reliability but may not cover novel types; meta-prompt allows extension

Failure signatures:
- **Endless Debate**: Judge times out and uses Soft Mode, potentially yielding suboptimal plan
- **Type Misclassification**: Leads to inappropriate operator selection; classifier accuracy is bottleneck
- **Token Overrun**: Combined cost of debate + execution may exceed simple baselines for very simple questions

First 3 experiments:
1. **Reproduce Operator Sensitivity Analysis**: On HotpotQA subset, manually label question types and run single operators vs. combined operators; compare F1 to validate core premise
2. **Ablate the Bi-Level Debate**: Run BELLE without second level (fast/slow debaters) and measure F1 drop and token usage increase
3. **Stress Test the Classifier**: Feed ambiguous or hybrid questions to classifier and observe downstream plan quality; document failure cases

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can BELLE be effectively integrated into real-world applications to maintain efficacy in dynamic and evolving environments?
- Basis in paper: [explicit] Conclusion states future aim to "investigate the integration of BELLE with real-world applications to assess its efficacy in dynamic and evolving environments"
- Why unresolved: Current evaluation only on static benchmark datasets, not reflecting shifting data distributions or noise in production systems
- What evidence would resolve it: Performance benchmarks from live deployment or simulation involving streaming data and changing knowledge base

### Open Question 2
- Question: To what extent can debate rules and strategies be refined to reduce computational overhead without sacrificing reasoning quality?
- Basis in paper: [explicit] Limitations identify reliance on multiple agents interacting iteratively as major issue, suggesting "Refining the debate rules and strategies could potentially reduce overhead"
- Why unresolved: While more cost-effective than other agent-based methods, iterative bi-level debate still incurs significant latency and token costs
- What evidence would resolve it: Study comparing standard BELLE against variants with early-stopping criteria or distilled debate strategies, showing token usage reduction while maintaining F1

### Open Question 3
- Question: How can the framework adapt to handle novel or previously unseen question formats outside the predefined four-type taxonomy?
- Basis in paper: [explicit] Limitations note that although BELLE is robust for known types, it "may struggle with novel or previously unseen question formats"
- Why unresolved: Question Type Classifier relies on fixed labels and in-context examples; out-of-category queries lead to suboptimal planning
- What evidence would resolve it: Experiments on out-of-distribution datasets with reasoning types not present in training/prompts, or implementation of dynamic label space expansion

## Limitations
- Exact prompt templates for each reasoning operator are not fully specified, impacting reproducibility
- 95% classifier accuracy is promising but not independently verified
- Framework's scalability to languages other than English or domains outside Wikipedia is untested
- Marginal benefit of bi-level debate over single-level is not quantified in absolute terms

## Confidence
- **High**: BELLE outperforms strong baselines on all four datasets; bi-level debate improves accuracy; operator selection is sensitive to question type
- **Medium**: Computational cost-effectiveness claims; classifier accuracy; debate convergence in 2-3 rounds
- **Low**: Reproducibility of exact prompt templates; robustness to ambiguous question types; generalization to non-English domains

## Next Checks
1. **Prompt Template Isolation**: Extract and test each operator's prompt independently on held-out multi-hop questions; measure output quality variance
2. **Ablation of Second-Level Debate**: Run BELLE without fast/slow debaters on subset of questions; compare F1, token usage, and plan stability
3. **Classifier Robustness Test**: Construct test set of ambiguous or hybrid question types; measure classifier accuracy and downstream plan quality to identify failure modes