---
ver: rpa2
title: Context informs pragmatic interpretation in vision-language models
arxiv_id: '2511.03908'
source_url: https://arxiv.org/abs/2511.03908
tags:
- humans
- context
- conditions
- were
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates vision-language models' ability to perform
  context-sensitive pragmatic reasoning in iterated reference games. The models are
  tested on human-derived data where players repeatedly describe novel tangram shapes
  for others to identify.
---

# Context informs pragmatic interpretation in vision-language models

## Quick Facts
- **arXiv ID:** 2511.03908
- **Source URL:** https://arxiv.org/abs/2511.03908
- **Reference count:** 39
- **Primary result:** VLMs improve at pragmatic reference resolution with relevant context from same game but struggle with task-specific conventions.

## Executive Summary
This paper evaluates vision-language models' ability to perform context-sensitive pragmatic reasoning in iterated reference games. The models are tested on human-derived data where players repeatedly describe novel tangram shapes for others to identify. Experiments vary the amount, order, and relevance of prior conversational context provided to the models. Without relevant context, models perform above chance but significantly worse than humans. With relevant context from the same game, model performance increases dramatically across trials, approaching human-level accuracy. However, when context comes from different games, performance drops substantially. This demonstrates that models are sensitive to contextual information but rely heavily on task-relevant history.

## Method Summary
The study evaluates instruction-tuned VLMs (Qwen 2.5 VL 32B, Gemma 3 27B, Llama 3.2 11B, Kimi VL A3B) on iterated reference games using human-derived tangram datasets. The task requires identifying target tangrams from human descriptions given varying amounts and types of conversational context (8 conditions: yoked, shuffled, backward, ablated, other-within, other-across, random, no context). Models receive a system prompt, tangram grid image, and chat history context, with accuracy measured as softmax-normalized probability assigned to correct target token (A-L). Performance is compared against human baseline (chance = 0.083).

## Key Results
- VLMs improve dramatically with relevant same-game context, approaching human-level accuracy across trials
- Context from different games yields poor performance (0.3-0.5 accuracy), showing models rely on task-specific history
- Models show text bias in some cases, with Llama 3.2 improving without images due to linguistic priors
- Performance drops to near-chance without visual input, confirming visual grounding is used despite text bias

## Why This Works (Mechanism)

### Mechanism 1: In-Context Convention Learning from Game-Specific History
- **Claim:** VLMs improve at pragmatic reference resolution when provided with relevant conversational history from the same game, but this improvement depends critically on task-specific context rather than general task familiarity.
- **Mechanism:** The model uses in-context examples to learn the specific mapping between descriptions and referents that participants established in that particular game. This involves tracking which expressions map to which tangrams and applying that learned convention to new descriptions.
- **Core assumption:** Models perform pattern matching over context windows to extract description-referent associations, rather than constructing abstract pragmatic reasoning.
- **Evidence anchors:** [abstract] "With relevant context, model performance increased dramatically over trials"; [section p.3-4] "When we varied the relevance of the context by drawing the conversational history from a different game...models had much lower accuracy of 0.3 – 0.5"; [section p.4] "The boost depends on the context and the test trial coming from the same original game. The convention reached by one game is not necessarily predicted by the context from other games."
- **Break condition:** Performance degrades when (1) context comes from different games, (2) target tangram doesn't appear in context (ablated condition), or (3) context length exceeds model's effective retrieval capacity (speculated for Qwen 2.5).

### Mechanism 2: Visual-Linguistic Feature Integration with Text Bias
- **Claim:** VLMs use visual information for reference resolution, but some models show bias toward linguistic priors over visual evidence.
- **Mechanism:** Models combine visual features from tangram images with linguistic descriptions. However, attention mechanisms may overweight textual patterns from training data, leading to suboptimal visual grounding for abstract/novel stimuli.
- **Core assumption:** The visual encoder provides useful features, but the language model component can override visual evidence when textual patterns match training distribution.
- **Evidence anchors:** [section p.12] "All models showed near- or at-chance performance...when no images were shown, suggesting that they were in fact using visual information"; [section p.12-13] "Llama 3.2 performance in fact improved in most cases [without images]...suggesting that some models were primarily using the text information"; [section p.10-11] Error analysis: models struggle with non-literal descriptions ("hand", "leg", "behind") but perform averagely on literal terms.
- **Break condition:** When descriptions involve non-literal mappings (e.g., calling a tangram part a "leg") or abstract spatial relations that don't match training distribution priors.

### Mechanism 3: Few-Shot Adaptation via Corrective Feedback
- **Claim:** Models learn faster from explicit corrections than humans who only receive binary feedback, but this advantage doesn't translate to human-like trial-level calibration.
- **Mechanism:** Models receive the correct target label for all in-context trials, enabling direct association learning. Humans only know they were wrong, not the correct answer, forcing different learning strategies.
- **Core assumption:** Access to ground truth labels in context enables more efficient in-context learning than binary feedback alone.
- **Evidence anchors:** [section p.4] "models received more informative feedback than humans. Models received the correct answers for the in-context examples, while humans only learnt if their selections were correct"; [section p.9] "models were generally able to learn from previous mistakes...whereas naïve humans were at about 0.5 accuracy"; [section p.4] "weak correlations between model and human performance (yoked model r = .10 – .27, human split-half r = .42)".
- **Break condition:** When the number of in-context examples is very small (first few trials), models perform substantially worse than humans, suggesting different few-shot learning strategies.

## Foundational Learning

- **Concept: Iterated Reference Games**
  - **Why needed here:** This is the experimental paradigm. Understanding that players repeatedly describe the same referents, developing conventions over rounds, is essential for interpreting why same-game context matters.
  - **Quick check question:** Why would context from Game A help with Game A trials but not Game B trials if both involve the same 12 tangrams?

- **Concept: Pragmatic Inference**
  - **Why needed here:** The task requires inferring speaker intent beyond literal meaning. Descriptions like "the bunny" for an abstract shape require context to interpret which tangram this conventionally refers to.
  - **Quick check question:** How does "the person one" mean something different in Round 1 vs. Round 6 of the same game?

- **Concept: In-Context Learning**
  - **Why needed here:** Models improve without weight updates, purely through context. The mechanism by which models extract patterns from context windows to improve predictions is central to understanding the results.
  - **Quick check question:** Why might more in-context examples from the wrong game not help (or even hurt) performance?

## Architecture Onboarding

- **Component map:** Input: [System prompt + 12 tangrams image] → Vision Encoder → Text Embedding; Context: [Prior trials as chat history] → Context Buffer → Attention; Test: [Description text] → Log probability extraction over tokens A-L; Output: Softmax-normalized probabilities → Argmax selection

- **Critical path:**
  1. Vision encoder must extract distinguishing features from all 12 tangrams
  2. Attention mechanism must retrieve relevant prior trial associations
  3. Language model must map description to correct tangram label

- **Design tradeoffs:**
  - **Context window length vs. retrieval precision:** Longer contexts (more trials) may exceed effective attention range (suspected for Qwen 2.5 in later rounds)
  - **Vision vs. language bias:** Models like Llama 3.2 may prioritize textual patterns over visual evidence, helping on some tasks but hurting on visually-grounded ones
  - **Feedback informativeness:** Giving models correct answers vs. binary feedback improves learning but reduces human-model comparability

- **Failure signatures:**
  - Performance ~30-50% with wrong-game context (good context structure, wrong content)
  - Performance near chance (~8.3%) without images (no visual grounding)
  - Declining performance in later rounds for some models (context length issues)
  - High error rates on non-literal descriptions (body parts, spatial relations)

- **First 3 experiments:**
  1. **Reproduce the yoked vs. other-across comparison** with your target VLM to establish whether it uses game-specific vs. general reference game context
  2. **Run the text-only vs. image-text comparison** to quantify visual grounding dependency before investing in visual improvements
  3. **Test the ablated condition** (same game, target tangram removed from context) to isolate whether models need direct experience with specific tangram conventions or can infer from related trials

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does equating feedback type between humans and models (providing correct answers vs. only correctness information) eliminate the performance differences in learning from errors?
- **Basis in paper:** [explicit] "Future work can more fully characterise the role of feedback by either allowing naïve humans to know the correct tangram if they answered incorrectly, or by presenting models with only selection correctness feedback."
- **Why unresolved:** Models received correct targets for all previous trials while humans only knew they were correct when they selected correctly themselves, creating a confound in learning comparisons.
- **What evidence would resolve it:** A matched experiment where either humans receive full corrective feedback or models receive only binary correctness feedback.

### Open Question 2
- **Question:** Do findings from abstract tangram stimuli generalize to naturalistic images and real-world referential communication?
- **Basis in paper:** [explicit] "Additionally, we tested a limited set of abstract stimuli as a representative example; generalisability to other (e.g., naturalistic) stimuli sets is left to future work."
- **Why unresolved:** Abstract tangrams are out-of-domain for VLMs and may elicit different reasoning strategies than natural images the models were trained on.
- **What evidence would resolve it:** Replication of the iterated reference game paradigm using naturalistic image datasets.

### Open Question 3
- **Question:** What causal and attentional mechanisms underlie VLMs' use of contextual information in multi-turn reasoning?
- **Basis in paper:** [explicit] "Further work is needed to more comprehensively characterise the role of prompt engineering, causal and attentional factors underlying context use, and humans' own sensitivity to context."
- **Why unresolved:** The study demonstrated sensitivity to context relevance but did not investigate the internal mechanisms driving this sensitivity.
- **What evidence would resolve it:** Attention head analysis, causal intervention studies on context tokens, or probing experiments.

### Open Question 4
- **Question:** Can VLMs generate appropriate referring expressions in iterated reference games with similar adaptation patterns to humans?
- **Basis in paper:** [explicit] "Furthermore, our study focused on interpretation, but generating appropriate descriptions is a yet harder task that is worth exploring."
- **Why unresolved:** The study only tested comprehension (matching descriptions to images), not production (generating descriptions for matchers).
- **What evidence would resolve it:** Experiments where VLMs act as describers across multiple rounds with human or model matchers.

## Limitations
- Models' performance heavily depends on task-specific context rather than general pragmatic competence, suggesting limited transfer to novel domains
- Visual grounding limitations observed in Llama 3.2 indicate architecture-dependent biases that could affect different model families
- Feedback mechanism (providing correct answers in context) creates artificial learning advantage over humans
- Study focuses on abstract tangram shapes, raising questions about generalizability to naturalistic visual reasoning tasks

## Confidence

**High confidence:** The core finding that models perform substantially better with relevant same-game context versus irrelevant or no context is well-supported by experimental results across multiple VLMs and conditions.

**Medium confidence:** Claims about specific learning patterns and calibration differences between models and humans are supported but could benefit from additional experimental controls.

**Low confidence:** The assertion that VLMs learn "conventions" in the same way humans do remains speculative and not definitively proven.

## Next Checks

1. **Generalization test:** Evaluate whether models trained on this task can apply learned pragmatic conventions to completely novel visual domains (e.g., emoji, abstract art) without additional in-context examples, to test whether they've learned abstract pragmatic rules or just task-specific mappings.

2. **Attention mechanism analysis:** Conduct detailed attention weight analysis to determine whether models are actually using visual features for reference resolution or primarily relying on linguistic patterns, particularly for models showing text bias like Llama 3.2.

3. **Feedback mechanism ablation:** Compare model performance when given only binary feedback (correct/incorrect) versus explicit correct answers in context to determine how much of the learning advantage stems from the informative feedback structure rather than genuine pragmatic reasoning capability.