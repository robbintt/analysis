---
ver: rpa2
title: '`Generalization is hallucination'' through the lens of tensor completions'
arxiv_id: '2502.17305'
source_url: https://arxiv.org/abs/2502.17305
tags:
- artifacts
- tensor
- language
- generalization
- hallucinations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces tensor completions and artifacts as a theoretical
  framework for understanding hallucinations and generalizations in language models.
  It shows that both phenomena arise as artifacts in tensor completions, which are
  novel sentences predicted with high probability by the model.
---

# `Generalization is hallucination' through the lens of tensor completions

## Quick Facts
- **arXiv ID:** 2502.17305
- **Source URL:** https://arxiv.org/abs/2502.17305
- **Reference count:** 20
- **Key outcome:** Hallucinations and generalizations in language models are mathematically equivalent artifacts arising from tensor completion under capacity constraints.

## Executive Summary
This paper proposes a theoretical framework where language model hallucinations and generalizations are two manifestations of the same phenomenon: artifacts in tensor completions. The framework shows that standard language modeling objectives only constrain probability distributions on training fibers, leaving most tensor entries unregularized. When models have insufficient capacity (low rank), high-probability completions emerge in these unconstrained regions. These artifacts are mathematically identical whether they match reality (generalizations) or contradict it (hallucinations)—only external verification distinguishes them. Experiments on toy models demonstrate that artifact prevalence increases when models are smaller, supporting the theoretical claims.

## Method Summary
The paper trains simplified attention-only transformers on random triple datasets and enumerates all possible triples to count artifacts—triples with predicted probability ≥ 0.95 that are not in the training set. Figure 2 varies dataset sizes while fixing model capacity, showing artifact count scaling. Figure 3 fixes a dataset and varies model capacity (parameter count), demonstrating inverse relationship between model size and artifacts. The simplified transformer architecture excludes layer norm, biases, and positional encodings to maintain clean tensor representations.

## Key Results
- Artifacts (high-probability novel predictions) are prevalent even in small toy models, with counts 5-20x higher than training samples
- Artifact prevalence increases as model size decreases, supporting the rank constraint hypothesis
- The framework mathematically unifies hallucinations and generalizations as structurally identical phenomena

## Why This Works (Mechanism)

### Mechanism 1: Training Fiber Constraint Creates Unconstrained Tensor Regions
- Claim: Language models only enforce probability distributions on training fibers, leaving most tensor entries unregularized.
- Mechanism: The standard loss function (Eq. 2) minimizes KL divergence only on fibers D^n_t,: for t ∈ D^{n-1}. All other entries in the implicit tensor P^n_θ have no gradient signal forcing them toward zero, so high-probability "artifacts" emerge in unconstrained regions.
- Core assumption: Language model behavior can be meaningfully represented as an implicit n-tensor completion P^n_θ.
- Evidence anchors:
  - [abstract] "both phenomena arise as artifacts in tensor completions, which are novel sentences predicted with high probability by the model"
  - [section 2] "the fibers at t ∈ D^{n-1} are the only parts of D^n that the model 'sees' during training... all other zeros in D^n are not enforced or 'seen' by the model"
  - [corpus] Weak—no corpus papers directly address tensor completion frameworks for LLMs.
- Break condition: If training data covers all n-grams in T^n (infeasible for realistic vocabularies).

### Mechanism 2: Low-Rank Completion Forces Tradeoffs
- Claim: Model capacity constraints (via parameter count) indirectly constrain tensor rank, forcing non-zero entries in untrained regions to maintain fiber consistency.
- Mechanism: A low-rank tensor D' that is consistent with training fibers cannot simultaneously match all zeros in the original tensor D^n. The mathematical structure of low-rank completion introduces non-zero entries (artifacts) as a necessary consequence.
- Core assumption: Parameter count correlates with maximum achievable tensor rank.
- Evidence anchors:
  - [section 2.1] "non-zero entries (i.e. artifacts) tend to emerge when attempting to reduce rank"
  - [section 4, Fig. 3] "As we reduce the number of parameters, which indirectly reduces rank, the number of artifacts increases"
  - [corpus] Weak—related work on intrinsic dimensionality (Aghajanyan et al.) may connect but is not directly tested.
- Break condition: If model has sufficient rank to represent the ground-truth tensor exactly.

### Mechanism 3: Generalization-Hallucination Mathematical Equivalence
- Claim: From the model's perspective, "good" artifacts (generalizations) and "bad" artifacts (hallucinations) are structurally identical; only external verification distinguishes them.
- Mechanism: Both arise as high-probability completions in unconstrained tensor regions. The model has no internal signal differentiating artifacts that match reality from those that contradict it—this determination requires out-of-band knowledge.
- Core assumption: Hallucinations of interest are "confabulations" (novel high-probability predictions), not training data replication errors.
- Evidence anchors:
  - [section 3] "Mathematically, their structures are the same... The only way these artifacts can be precluded is if the dataset contains additional data that contradicts them"
  - [abstract] "framework suggests that hallucinations and generalizations are two sides of the same coin"
  - [corpus] "Banishing LLM hallucinations requires rethinking generalization" (Li et al., arXiv:2406.17642) supports the joint phenomenon claim.
- Break condition: If external verification is embedded in training (e.g., negative examples or "unsupported" tokens).

## Foundational Learning

- **Concept: Tensor rank and decomposition**
  - Why needed here: The entire framework relies on understanding how low-rank approximations introduce structured errors; without this, the artifact-generation mechanism is opaque.
  - Quick check question: Given a 3-way tensor with rank 3, why would a rank-2 completion necessarily differ from the original?

- **Concept: Next-token prediction as fiber matching**
  - Why needed here: Understanding Eq. 1-2 shows precisely why only specific tensor slices receive gradient signals; this is the root cause of artifact emergence.
  - Quick check question: For a corpus of trigrams, which entries of D³ does a language model "see" during training?

- **Concept: Closed-world vs. open-world assumptions in databases**
  - Why needed here: Proposed mitigations reference "local closed world assumptions"—asserting that unobserved statements are false rather than unknown.
  - Quick check question: How would adding an "unsupported" token change what the model learns about out-of-distribution contexts?

## Architecture Onboarding

- **Component map:**
  ```
  Corpus D → n-tensors D^n (sparse, high rank)
       ↓
  Training on fibers D^n_t,: (Eq. 2)
       ↓
  Model parameters θ → implicit tensor P^n_θ (lower rank)
       ↓
  Completion artifacts (high-probability entries not in D^n)
  ```

- **Critical path:** Training data coverage → model capacity (rank constraint) → artifact count → downstream classification as generalization vs. hallucination.

- **Design tradeoffs:**
  - Larger models → higher rank → fewer artifacts, but potential overfitting to training fibers.
  - Explicit negative sampling (unsupported tokens) → reduces hallucinations but may suppress valid generalizations.
  - Assumption: Trade-off is fundamental, not an artifact of current architectures.

- **Failure signatures:**
  - Artifact count >> training sample count (experiments show 5-20x).
  - High confidence (P ≥ 0.95) on out-of-distribution n-grams.
  - Smaller models exhibit more artifacts for fixed dataset.

- **First 3 experiments:**
  1. **Toy replication:** Train attention-only models on random triple datasets; count artifacts (triples with P ≥ 0.95 not in training) to verify scaling trends from Figs. 2-3.
  2. **Parameter sweep:** Fix dataset, vary non-embedding parameters; confirm inverse relationship between model size and artifact count.
  3. **Mitigation prototype:** Augment training with (t, t_u) pairs for random t ∉ D^{n-1} using an "unsupported" token; measure artifact reduction and any generalization degradation on held-out valid artifacts.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the relationships between model size, rank, and artifact prevalence observed in toy models persist when scaled up to large language models (LLMs) and realistic datasets?
- Basis in paper: [explicit] The authors state it is "uncertain if the same trends will persist when scaled up" and call for future work to quantify artifacts in larger models (Section 4, point 1).
- Why unresolved: The paper's experiments rely on simplified toy models with small vocabularies, whereas LLMs involve significantly higher dimensionality and complexity.
- What evidence would resolve it: Empirical measurements of artifact counts in LLMs with varying parameter counts, confirming if smaller models still produce more artifacts.

### Open Question 2
- Question: How can the quantity of tensor artifacts and the effective rank of datasets be tractably approximated for models where exhaustive evaluation is impossible?
- Basis in paper: [explicit] The authors note that theoretical objects like tensor rank "very quickly grow impractical to compute" and suggest future work must find ways to "approximate or estimate" them (Section 4, point 2).
- Why unresolved: Calculating the exact number of artifacts requires evaluating the model on every possible n-gram, which is computationally infeasible for large context windows and vocabularies.
- What evidence would resolve it: The development of algorithms or statistical bounds that accurately estimate tensor rank or artifact density without full enumeration.

### Open Question 3
- Question: Can hallucinations be systematically reduced by augmenting training data with "unsupported" tokens or loss penalties without degrading the model's generalization capabilities?
- Basis in paper: [explicit] The paper proposes mitigating hallucinations via data augmentation but warns that efforts to mitigate hallucinations might impact desirable generalizations (Section 3.4, 3.5, and Section 4, point 3d).
- Why unresolved: While the theory suggests a trade-off, the exact impact of specific mitigation strategies (like "unsupported" tokens) on generalization performance remains untested.
- What evidence would resolve it: Experiments comparing generalization accuracy and hallucination rates in models trained with and without explicit penalties for unsupported predictions.

### Open Question 4
- Question: Does overfitting reduce the number of artifacts, and does this reduction directly cause an increase in generalization error?
- Basis in paper: [explicit] The paper hypothesizes that "overfitting leads to fewer artifacts" and that "generalization error increases when overfitting, due to the lack of artifacts" (Section 4, point 3a & 3b).
- Why unresolved: This counter-intuitive claim is derived from the theoretical framework but lacks substantial empirical validation in the current text.
- What evidence would resolve it: Experiments showing a negative correlation between the degree of overfitting (or model rank) and the number of artifacts, correlated with rising test error.

## Limitations

- **Framework scalability:** The tensor completion framework is elegant but unproven for realistic LLMs; the simplified toy models may not capture full transformer complexity.
- **Verification dependency:** The distinction between hallucinations and generalizations relies on external ground truth verification, making the framework practically under-specified.
- **Experimental scope:** Toy experiments demonstrate the phenomenon but don't establish that the mechanism scales to real language models with meaningful vocabularies.

## Confidence

**High confidence:** The mathematical framework for tensor completions is sound. The claim that training on fibers D^n_t,: leaves most tensor entries unconstrained is mathematically rigorous and represents a valid insight about standard language modeling objectives.

**Medium confidence:** The relationship between model capacity (parameter count) and tensor rank is plausible but not rigorously established. The experiments on toy models support the artifact-generation mechanism, but extrapolation to real models requires additional evidence.

**Low confidence:** The claim that hallucinations and generalizations are "two sides of the same coin" is conceptually interesting but practically under-specified. Without a method to distinguish good from bad artifacts without external knowledge, this equivalence remains formal rather than operational.

## Next Checks

1. **Real model validation:** Train a standard transformer on a realistic corpus (e.g., Wikitext-2) and measure artifact prevalence. Compare the artifact count and distribution to the theoretical predictions. This would establish whether the tensor completion framework captures real LLM behavior.

2. **Mitigation effectiveness:** Implement the proposed mitigation strategies (additional negative examples with "unsupported" tokens or modified loss functions) and measure their impact on both hallucination reduction and generalization preservation. This would test whether the tradeoff is truly fundamental or an artifact of current training approaches.

3. **Rank analysis:** Analyze the effective tensor rank of trained models using tensor decomposition techniques. Correlate rank estimates with model size and artifact counts to validate the proposed mechanism linking parameter count to completion quality.