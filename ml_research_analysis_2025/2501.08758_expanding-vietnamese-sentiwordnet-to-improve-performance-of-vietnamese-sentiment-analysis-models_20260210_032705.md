---
ver: rpa2
title: Expanding Vietnamese SentiWordNet to Improve Performance of Vietnamese Sentiment
  Analysis Models
arxiv_id: '2501.08758'
source_url: https://arxiv.org/abs/2501.08758
tags:
- sentiment
- vietnamese
- sentiwordnet
- phobert
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method to expand the Vietnamese SentiWordNet
  and combines it with the PhoBERT-V2 language model to improve sentiment analysis
  performance on Vietnamese review datasets. The approach extracts sentiment features
  from the extended lexicon and integrates them with deep contextual representations
  from PhoBERT.
---

# Expanding Vietnamese SentiWordNet to Improve Performance of Vietnamese Sentiment Analysis Models

## Quick Facts
- arXiv ID: 2501.08758
- Source URL: https://arxiv.org/abs/2501.08758
- Reference count: 31
- Proposes a method to expand Vietnamese SentiWordNet and integrate it with PhoBERT-V2 for improved sentiment analysis performance

## Executive Summary
This paper introduces CombViSA, a novel approach that expands the Vietnamese sentiment lexicon (ViSentiWordNet) using synonym and antonym relationships, then integrates the expanded dictionary with the PhoBERT-V2 language model through a combined neural architecture. The expanded lexicon (ExtViSentiWordNet) improves coverage from 1,017 to over 6,800 words by propagating sentiment scores from seed words through synonym/antonym relationships. The fusion of contextual embeddings with lexical sentiment features achieves state-of-the-art results on Vietnamese review datasets, outperforming previous models including PhoBERT-V2 alone and multilingual BERT.

## Method Summary
The proposed approach involves two main components: (1) lexicon expansion using synonym/antonym propagation from seed words with high confidence sentiment scores, computing new word scores based on average distances to known positive and negative words, and (2) a combined neural architecture that fuses PhoBERT-V2 contextual embeddings with sentiment vectors extracted from ExtViSentiWordNet. The model handles negation through pattern matching that swaps sentiment polarity for words within negation contexts. The fusion architecture processes PhoBERT outputs through an RCNN layer and combines them with lexical sentiment vectors before final classification through an MLP.

## Key Results
- Achieves F1-scores of 0.95 on VLSP 2016 and 0.94 on AIVIVN 2019 datasets
- Outperforms PhoBERT-V2 alone (F1=0.91 on VLSP 2016) and multilingual BERT
- Extended lexicon improves coverage from 1,017 to over 6,800 words
- Fusion approach particularly effective for long and irregular review texts

## Why This Works (Mechanism)

### Mechanism 1: Lexicon Expansion via Synonym/Antonym Propagation
- **Claim:** Expanding the sentiment lexicon via synonym/antonym propagation improves vocabulary coverage and downstream classification.
- **Mechanism:** Starting from high-confidence positive (P) and negative (N) seed words, synonyms are added to the same polarity set while antonyms are added to the opposite set. New words receive sentiment scores based on average distances to known positive and negative words.
- **Core assumption:** Semantic neighbors share sentiment polarity; antonyms invert polarity. Distance-based scoring approximates human sentiment judgments.
- **Evidence anchors:** [abstract] mentions extending ViSentiWordNet using synonym and antonym relationships; [section 4.1] notes the original lexicon's small size (1,017 words) resulting in insufficient coverage.
- **Break condition:** If synonym/antonym relationships in Vietnamese are noisy or polysemous without disambiguation, propagation may introduce label noise that degrades rather than improves features.

### Mechanism 2: Fusion of Contextual and Lexical Features
- **Claim:** Fusing contextual embeddings (PhoBERT-V2) with lexical sentiment vectors improves classification over either alone.
- **Mechanism:** PhoBERT-V2 produces contextual embeddings processed through RCNN into LMVec, while ExtViSentiWordNet yields PosVec and NegVec processed into SWVec. These are concatenated and fed to an MLP for classification.
- **Core assumption:** PLMs capture context but may lack explicit sentiment signal; lexical features compensate by providing direct polarity cues.
- **Evidence anchors:** [abstract] states the expanded dictionary is integrated with PhoBERT-V2 through a combined neural architecture; [section 5.4, Table 3] shows PhoBERT-V2 + ExtViSentiWordNet achieves F1=0.93 vs PhoBERT-V2 alone at F1=0.91 on VLSP 2016.
- **Break condition:** If lexical features are sparse (many OOV words) or noisy, concatenation may add dimensionality without signal, increasing overfitting risk.

### Mechanism 3: Negation Pattern Handling
- **Claim:** Negation pattern handling improves sentiment feature extraction for Vietnamese.
- **Mechanism:** Algorithm 1 detects negation patterns using predefined words (không, chẳng, etc.). When a word matches a negation pattern, its PosScore/NegScore are swapped before appending to vectors.
- **Core assumption:** Negation words invert the sentiment polarity of adjacent terms; the pattern window captures the relevant scope.
- **Evidence anchors:** [section 4.2] explains that positive words within negation patterns often imply negative meanings; [section 4.2, Algorithm 1] shows explicit reversal logic.
- **Break condition:** If negation scope extends beyond single-word patterns or involves complex syntax (double negation, sarcasm), simple pattern matching will mislabel sentiment.

## Foundational Learning

- **Concept: BERT/RoBERTa architecture and fine-tuning**
  - Why needed here: PhoBERT-V2 is RoBERTa-based; understanding encoder outputs, [CLS] tokens, and fine-tuning vs. feature extraction is essential for reproducing results.
  - Quick check question: What is the difference between using BERT for feature extraction vs. fine-tuning, and which approach does this paper use?

- **Concept: Sentiment lexicons (SentiWordNet-style scoring)**
  - Why needed here: The core contribution extends a sentiment lexicon; you need to understand PosScore/NegScore conventions and how they're combined.
  - Quick check question: Given a word with PosScore=0.8 and NegScore=0.1, how would you classify its sentiment, and what does the sum (or lack thereof) imply?

- **Concept: LSTM/RCNN for sequence modeling**
  - Why needed here: The CombViSA architecture uses RCNN (bidirectional LSTM + conv1d) to process PhoBERT outputs.
  - Quick check question: Why might RCNN be preferred over vanilla LSTM for long review sequences, per the paper's discussion of comment lengths (mean 25–55 words)?

## Architecture Onboarding

- **Component map:** Input → PhoBERT-V2 encoder → RCNN (bi-LSTM + conv1d) → MLP → LMVec; Input → ExtViSentiWordNet lookup + Algorithm 1 (negation handling) → PosVec ⊕ NegVec → MLP → SWVec; LMVec ⊕ SWVec → MLP → Softmax → {positive, negative, [neutral]}

- **Critical path:**
  1. Build/obtain ExtViSentiWordNet (seed extraction → synonym/antonym expansion → distance scoring)
  2. Implement Algorithm 1 for negation-aware PosVec/NegVec extraction
  3. Integrate PhoBERT-V2 (via HuggingFace or VinAI repo) with RCNN head
  4. Fuse pathways and train end-to-end

- **Design tradeoffs:**
  - Lexicon size vs. noise: Aggressive expansion improves coverage but risks propagated errors
  - Vector dimensionality: Fixed 128-word limit for PosVec/NegVec balances memory vs. information loss for long reviews
  - RCNN vs. simpler classifier: Added complexity justified for long sequences (AIVIVN mean length 55.45), but may be overkill for shorter texts

- **Failure signatures:**
  - OOV spikes: Many review words not in ExtViSentiWordNet → SWVec near-zero → fusion degrades to PhoBERT-only
  - Negation errors: Sarcasm or multi-word negation scopes misclassified
  - Class imbalance: VLSP has 3 classes; neutral may be underrepresented
  - Overfitting on small datasets: ~50K samples total; monitor train/val gap

- **First 3 experiments:**
  1. **Ablation on lexicon path:** Train CombViSA with ExtViSentiWordNet vs. original ViSentiWordNet vs. no lexicon. Expect decreasing F1 as lexicon shrinks.
  2. **Negation handling ablation:** Run Algorithm 1 with vs. without negation pattern reversal on a held-out set with known negation examples. Measure F1 delta.
  3. **Architecture simplification:** Replace RCNN with a single linear layer on PhoBERT [CLS] to quantify RCNN's contribution, especially on longer AIVIVN reviews.

## Open Questions the Paper Calls Out
None

## Limitations
- Lexicon expansion methodology lacks external validation and empirical comparison showing propagated sentiment scores accurately reflect human judgments
- Negation handling approach is limited to simple pattern matching and doesn't address complex negation structures, sarcasm, or multi-word negation scopes
- Results may not generalize to Vietnamese review datasets from different domains beyond VLSP 2016 and AIVIVN 2019

## Confidence
**High Confidence:** The fusion architecture combining PhoBERT-V2 with lexical sentiment features is technically sound and demonstrates measurable improvements over PhoBERT-V2 alone in controlled experiments.

**Medium Confidence:** The overall approach of combining contextual embeddings with explicit sentiment lexicons is well-supported by related work, but specific implementation details lack extensive validation.

**Low Confidence:** The lexicon expansion methodology and negation handling approach have minimal external validation; the claim that propagated sentiment scores accurately capture Vietnamese word polarities is primarily theoretical.

## Next Checks
1. **Cross-Domain Evaluation:** Test CombViSA on Vietnamese review datasets from different domains (e.g., product reviews, social media comments) to assess whether the expanded lexicon and architecture generalize beyond the specific VLSP and AIVIVN datasets used in the paper.

2. **Lexicon Ablation with Gold Labels:** Create a gold-standard evaluation set where human annotators label the sentiment accuracy of words in ExtViSentiWordNet. Compare the accuracy of sentiment propagation against this gold standard to quantify how much noise the expansion process introduces.

3. **Negation Scope Analysis:** Conduct a targeted evaluation on Vietnamese sentences with known complex negation patterns (double negation, multi-word negation, sarcasm). Measure whether the current pattern-based approach correctly identifies sentiment polarity in these cases, or if more sophisticated negation detection is needed.