---
ver: rpa2
title: 'TabGRU: An Enhanced Design for Urban Rainfall Intensity Estimation Using Commercial
  Microwave Links'
arxiv_id: '2512.02465'
source_url: https://arxiv.org/abs/2512.02465
tags:
- rainfall
- tabgru
- data
- rain
- estimation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes TabGRU, a hybrid deep learning architecture
  combining Transformer and Bidirectional Gated Recurrent Unit (BiGRU) for rainfall
  intensity estimation using commercial microwave links (CMLs). The model integrates
  a learnable positional encoding and attention pooling mechanism to capture long-term
  dependencies and local sequential features in CML signal data.
---

# TabGRU: An Enhanced Design for Urban Rainfall Intensity Estimation Using Commercial Microwave Links

## Quick Facts
- arXiv ID: 2512.02465
- Source URL: https://arxiv.org/abs/2512.02465
- Reference count: 40
- This paper proposes TabGRU, a hybrid deep learning architecture combining Transformer and Bidirectional Gated Recurrent Unit (BiGRU) for rainfall intensity estimation using commercial microwave links (CMLs).

## Executive Summary
This paper presents TabGRU, a novel hybrid deep learning architecture that combines Transformer and BiGRU layers for rainfall intensity estimation from commercial microwave link (CML) signal data. The model integrates learnable positional encoding and attention pooling to capture both long-term dependencies and local sequential features in CML signals. Evaluated on Gothenburg, Sweden data, TabGRU achieved R² scores of 0.91 at Torp and 0.96 at Barl sites, outperforming deep learning baselines and reducing overestimation errors compared to traditional physics-based models during peak rainfall events.

## Method Summary
TabGRU processes CML Received Signal Level (RSL) data using a hybrid architecture: linear projection → learnable positional encoding → Transformer encoder → BiGRU → attention pooling → fully connected layer. The model uses 30-minute windows of 1-minute resampled RSL data from 12 specific sub-links, along with temporal encodings. Trained on Gothenburg OpenMRG dataset (June-August 2015) with chronological split, the model minimizes regression error using RMSE and MAE metrics while maximizing R² and PCC. Key hyperparameters include 3-layer Transformer (4 heads), 1-layer BiGRU (64 units), window size of 30 steps, learning rate of 5e-4, and dropout of 0.3.

## Key Results
- TabGRU achieved R² scores of 0.91 at Torp site and 0.96 at Barl site
- Significantly reduced overestimation errors compared to physics-based Power-Law models during peak rainfall events
- Outperformed deep learning baselines including LSTM, BiGRU, and pure Transformer models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A hybrid Transformer-BiGRU architecture outperforms standalone attention or recurrent models by capturing both global weather trends and local signal fluctuations.
- **Mechanism:** The Transformer encoder utilizes multi-head self-attention to compute correlations across the entire input window (e.g., 30 minutes), mitigating the vanishing gradient problem common in RNNs for long sequences. The subsequent BiGRU layer processes these features to model fine-grained, short-term dependencies (minute-level fluctuations) that pure Transformers might smooth over.
- **Core assumption:** Rainfall events exhibit features at multiple temporal scales—long-term dependencies (storm systems) and short-term volatility (sudden intensity changes)—which require distinct inductive biases to model accurately.
- **Evidence anchors:** [abstract] "This design synergistically captures both long-term dependencies and local sequential features..."; [section IV.C] "The BiGRU module complements the Transformer's global attention by modeling local temporal dynamics... helping to mitigate bias and capture minute-level fluctuations."

### Mechanism 2
- **Claim:** Learnable Positional Encoding (LPE) improves generalization over fixed sinusoidal encoding by adapting to the specific temporal dynamics of rainfall signals.
- **Mechanism:** Instead of fixed sine/cosine functions, LPE treats positional embeddings as trainable parameters ($P_{learnable}$). This allows the model to optimize the representation of time steps based on the data distribution (e.g., learning that specific phases of a signal drop correspond to specific stages of a rain event).
- **Core assumption:** The temporal position in a rainfall sequence carries semantic meaning specific to the hydrometeorological context that generic trigonometric functions cannot fully capture.
- **Evidence anchors:** [section IV.C] "This module enables the model to dynamically adjust the position encoding according to the characteristics of the actual data during training..."; [section IV.B] "We introduce customizable learnable positional encoding to dynamically adjust the representation... to enhance the flexibility and generalization ability."

### Mechanism 3
- **Claim:** Attention Pooling reduces error by weighting high-information time steps (e.g., signal peaks) more heavily than steady-state or noise segments during feature aggregation.
- **Mechanism:** Rather than taking the final hidden state or a simple average (which dilutes peak signals), an attention mechanism computes a weight $\alpha_t$ for each time step based on its hidden state. The model aggregates features via a weighted sum, prioritizing moments of significant signal change associated with rainfall onset or peaks.
- **Core assumption:** Not all time steps in a rainfall window are equally informative; moments of rapid signal attenuation contain disproportional information about rainfall intensity compared to dry periods.
- **Evidence anchors:** [section IV.C] "Periods of dramatic signal change near the rainfall peak clearly contain more valuable information... attention pooling does not discard critical time segments."; [abstract] "...attention pooling mechanism to improve its dynamic feature extraction..."

## Foundational Learning

- **Concept: Commercial Microwave Links (CMLs) as Opportunistic Sensors**
  - **Why needed here:** Understanding the input data modality is critical. Unlike radar, CMLs measure path-averaged attenuation (signal loss) between cell towers. The raw data is Received Signal Level (RSL), not rain rate.
  - **Quick check question:** Does the model input rainfall depth in mm, or signal strength in dBm?

- **Concept: Wet Antenna Attenuation (WAA)**
  - **Why needed here:** This is the primary noise source the paper aims to overcome. Water on the antenna causes signal loss independent of rain along the path. Physics models struggle to model this non-linear effect; the data-driven approach learns it implicitly.
  - **Quick check question:** Why does a physics-based model tend to overestimate rainfall at the end of a storm, and how does a deep learning model address this differently?

- **Concept: Inductive Bias in Sequence Modeling**
  - **Why needed here:** The paper relies on mixing two biases: Transformers (global, parallelizable) and BiGRU (local, sequential). Knowing why these are combined prevents misapplying the architecture to inappropriate data types.
  - **Quick check question:** Why would a pure Transformer struggle with "minute-level fluctuations" compared to a BiGRU?

## Architecture Onboarding

- **Component map:** Input (RSL + Time Embeddings) → Linear Projection → Learnable Positional Encoding → Transformer Encoder (Global Context) → BiGRU (Local Dynamics) → Attention Pooling (Feature Aggregation) → Fully Connected Layer → Rainfall Intensity Output

- **Critical path:** The sequence of `Transformer -> BiGRU` is the core novelty. If the Transformer fails to capture the global trend, the BiGRU receives a corrupted context. Similarly, if the Attention Pooling layer weights are uniform, the distinction between peak and base rainfall is lost.

- **Design tradeoffs:**
  - **Learnable vs. Fixed Encoding:** Flexibility vs. Risk of Overfitting
  - **Transformer-BiGRU vs. Transformer-Only:** Higher accuracy vs. Higher computational cost and complexity (two sequential deep learning blocks)
  - **Window Size:** Paper uses 30 minutes. Shorter windows reduce latency but may miss long-term storm context required by the Transformer

- **Failure signatures:**
  - **Hysteresis Effect:** The model may underestimate the onset of rain (antenna is dry but path is wet) and overestimate the end (antenna is wet but rain stopped)
  - **Peak Truncation:** If the attention pooling is too aggressive or the loss function is MSE-heavy, the model may smooth out extreme events (peaks), leading to underestimation of flood risks

- **First 3 experiments:**
  1. **Ablation Study (Architecture):** Run the model three times: (1) TabGRU full, (2) TabGRU without BiGRU (Transformer-only), (3) TabGRU without Attention Pooling (use Last-State). Compare RMSE and R² to quantify the contribution of each component.
  2. **Physics Baseline Comparison:** Replicate the comparison against the Power-Law (PL) model specifically during the identified "peak rainfall events" in the test set to verify if the "overestimation" artifact is reproducible in your implementation.
  3. **Window Sensitivity Analysis:** Train the model with varying input window sizes (e.g., 15, 30, 60 minutes) to determine the optimal temporal context length for the Transformer component without introducing excessive latency.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the TabGRU model generalize effectively to diverse geographic and climatic regions outside of the temperate oceanic climate of Gothenburg, Sweden?
- **Basis in paper:** [explicit] The conclusion states that "future work will involve testing on new datasets from different geographical and climatological regions," building on the observation in Section II that deep learning models often struggle to generalize to unseen climatic regions.
- **Why unresolved:** The current validation is restricted to a single dataset from Gothenburg (June–September 2015), limiting the understanding of the model's robustness against different precipitation regimes (e.g., tropical or arid).
- **What evidence would resolve it:** Consistent performance metrics (R², RMSE) when training and testing the model on CML datasets from distinct climates, such as monsoon regions or arid deserts.

### Open Question 2
- **Question:** Can state-aware architectures effectively correct the systematic bias of underestimating rainfall onset and overestimating rainfall cessation caused by wet antenna attenuation (WAA)?
- **Basis in paper:** [explicit] Section VI suggests that "future models should also explore state-aware architectures to address the systematic bias of underestimation at the onset of rainfall and overestimation at the end."
- **Why unresolved:** While TabGRU reduces overestimation compared to physics-based models, the analysis of specific events (e.g., Barl site, Aug 26) shows the model still struggles with the hysteresis effect of water drying on antennas.
- **What evidence would resolve it:** A reduction of lag errors in time-series plots during the drying phase, showing predicted rainfall dropping to zero simultaneously with the end of true precipitation.

### Open Question 3
- **Question:** How can model sensitivity be improved for micro-precipitation (light/drizzle) events where simpler baselines occasionally outperform the complex TabGRU architecture?
- **Basis in paper:** [explicit] Section VI notes that "precise quantification of micro-precipitation (light/drizzle) events remains a challenge" and proposes integrating wet/dry classification modules or focal loss techniques as a solution.
- **Why unresolved:** In low-intensity scenarios (e.g., Barl site, Aug 26), simpler models like BiGRU achieved lower errors, suggesting the current TabGRU architecture may be overfitting or struggling with the low signal-to-noise ratio of drizzle.
- **What evidence would resolve it:** Improved MAE scores specifically in the 0–1 mm/h range, outperforming simpler baselines without degrading accuracy in heavy rainfall.

### Open Question 4
- **Question:** Does training on datasets with a higher frequency of extreme rainfall events mitigate the model's tendency to underestimate peak intensities?
- **Basis in paper:** [explicit] Section VI states "future work will focus on incorporating more diverse datasets containing such events," as the current model tends to underestimate peaks (e.g., predicting 10.20 mm/h against a true 15 mm/h in Section V-C).
- **Why unresolved:** The current dataset likely lacks sufficient samples of extreme attenuation, causing the model to effectively "cap" its predictions during the most intense convective events.
- **What evidence would resolve it:** Validation on datasets containing torrential rain (>20 mm/h) showing predicted peaks that align closely with ground truth gauges.

## Limitations
- Limited dataset size and geographic scope (single urban area, 2-month timeframe) raises generalization concerns
- Architecture complexity may not justify marginal gains over simpler models without cost-benefit analysis
- Loss function specification unclear, potentially leading to systematic underestimation of extreme rainfall events

## Confidence
- **High Confidence:** The general methodology (hybrid Transformer-BiGRU architecture) is sound and aligns with current best practices in time-series modeling
- **Medium Confidence:** The specific architectural choices (learnable positional encoding, attention pooling) are reasonable but require empirical validation
- **Low Confidence:** The claim that TabGRU significantly outperforms physics-based models during peak rainfall events needs more detailed substantiation

## Next Checks
1. **Ablation Study and Baseline Fairness:** Conduct comprehensive ablation study comparing TabGRU to simpler architectures (LSTM-only, Transformer-only) trained with same window size and loss function to quantify marginal benefit of hybrid design
2. **Peak Event Loss Function Sensitivity:** Re-train TabGRU using loss function that explicitly penalizes under-prediction of extreme rainfall events (asymmetric loss, Quantile Loss) and evaluate performance on peak events
3. **Cross-Site Generalization Test:** Apply trained TabGRU model (trained on Gothenburg data) to CML data from different urban area with distinct climate (tropical monsoon region) to assess generalization capability