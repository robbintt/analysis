---
ver: rpa2
title: A Kolmogorov-Arnold Network for Interpretable Cyberattack Detection in AGC
  Systems
arxiv_id: '2509.05259'
source_url: https://arxiv.org/abs/2509.05259
tags:
- detection
- symbolic
- power
- system
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a Kolmogorov-Arnold Network (KAN) approach
  for interpretable cyberattack detection in Automatic Generation Control (AGC) systems.
  The method addresses the vulnerability of AGC systems to False Data Injection Attacks
  (FDIAs) by leveraging KANs' ability to model nonlinear relationships and extract
  symbolic expressions for interpretability.
---

# A Kolmogorov-Arnold Network for Interpretable Cyberattack Detection in AGC Systems

## Quick Facts
- arXiv ID: 2509.05259
- Source URL: https://arxiv.org/abs/2509.05259
- Reference count: 28
- Primary result: 95.97% FDIA detection accuracy with interpretable symbolic expression ξ(x) achieving 95.9% accuracy

## Executive Summary
This paper presents a Kolmogorov-Arnold Network (KAN) approach for interpretable cyberattack detection in Automatic Generation Control (AGC) systems. The method addresses vulnerability to False Data Injection Attacks (FDIAs) by leveraging KANs' ability to model nonlinear relationships and extract symbolic expressions for interpretability. A two-layer KAN model achieves 95.97% accuracy in FDIA detection, with corresponding symbolic expression revealing Area 2's frequency response as most vulnerable to attacks.

## Method Summary
The method trains a two-layer KAN on 20,000 samples from a simulated two-area AGC system, using 18 statistical features extracted from time-series data of tie-line power and frequency measurements. The KAN employs learnable B-spline activation functions and is trained using L-BFGS optimizer for 50 epochs. After training, pruning and fine-tuning reduce the accuracy gap between the model and its symbolic representation. The symbolic expression is extracted using the auto_symbolic method with a library of common mathematical functions.

## Key Results
- KAN model achieves 95.97% accuracy in FDIA detection
- Extracted symbolic expression ξ(x) achieves 95.9% accuracy
- Pruning and fine-tuning reduce accuracy gap from 13.73% to 0.07%
- Area 2's frequency response identified as most vulnerable to attacks

## Why This Works (Mechanism)

### Mechanism 1
B-spline activation functions capture AGC nonlinearities for accurate FDIA detection. The KAN assigns learnable, univariate B-spline functions to network edges rather than fixed activations. These piecewise polynomial functions approximate nonlinear transformations directly from data, mapping input features to outputs. Core assumption: AGC nonlinearities and attack signatures can be represented as sums of univariate, continuous functions.

### Mechanism 2
Symbolification converts learned B-splines into interpretable, human-readable expressions. The auto_symbolic method fits trained B-splines to a predefined library of common mathematical functions, yielding an approximate symbolic formula ξ(x) that maps inputs to outputs transparently. Core assumption: A sufficiently rich function library exists to approximate the learned spline shapes.

### Mechanism 3
Pruning and fine-tuning align model accuracy with symbolic fidelity. Pruning removes low-weight edges, simplifying network structure. Fine-tuning retrains this sparser network, reducing discrepancy between full model performance and its symbolic representation by forcing relationships into simpler, more symbolifiable forms. Core assumption: Important features are encoded in higher-weight edges.

## Foundational Learning

**B-splines as Learnable Activations**
- Why needed: Standard neural networks use fixed activations. This system requires activation functions that can flexibly learn and represent specific, complex nonlinearities inherent in AGC systems.
- Quick check: How does a B-spline's ability to refine its grid points affect its capacity to model a complex nonlinear function?

**Feature Extraction from Time-Series**
- Why needed: Raw time-series data is high-dimensional and noisy. Converting it into structured statistical features provides more stable and informative input for the KAN.
- Quick check: Which six statistical features are extracted from AGC measurements, and what does "kurtosis" capture in this context?

**Symbolic vs. Sub-Symbolic Representation**
- Why needed: The core value proposition is interpretability. Understanding the distinction between internal numerical model and extracted human-readable approximation is critical for evaluating performance-explainability trade-off.
- Quick check: What causes the accuracy gap between a trained KAN model and its extracted symbolic expression ξ(x), and how can this gap be minimized?

## Architecture Onboarding

**Component map:**
- Input Layer: 18 features (6 statistical features × 3 AGC measurements)
- Hidden KAN Layer(s): Contains learnable B-spline activation functions on edges
- Output Layer: Single output for binary classification (attack vs. normal)
- Symbolification Module: auto_symbolic function fitting splines to symbolic library

**Critical path:**
1. Simulate AGC system to generate time-series data
2. Extract statistical features to create dataset
3. Train KAN model using L-BFGS optimizer
4. Apply pruning to remove low-importance edges
5. Fine-tune pruned model to restore performance
6. Execute symbolification to extract interpretable expression ξ(x)
7. Evaluate both model and ξ(x) on test set

**Design tradeoffs:**
- Accuracy vs. Interpretability: Deeper models may achieve higher accuracy but produce less faithful symbolic expressions
- Library Richness: More extensive symbolic library yields more accurate ξ(x) but increases search complexity

**Failure signatures:**
- High Model Accuracy, Low ξ(x) Accuracy: Learned B-splines cannot be well-approximated by current symbolic library
- Low Recall: Model misses attacks, suggesting features or architecture fail to capture attack signatures
- Over-Pruning: Drastic drop in both model and ξ(x) accuracy, indicating critical edges were removed

**First 3 experiments:**
1. Baseline Replication: Train KAN without pruning, measure accuracy gap between model and symbolic expression
2. Pruning Ablation: Systematically vary pruning threshold, record accuracy and expression complexity to find optimal balance
3. Library Analysis: Test different symbolic function libraries to measure impact on ξ(x) fidelity and interpretability

## Open Questions the Paper Calls Out
- Can the framework be extended to classify specific attack types rather than performing only binary detection?
- Is the KAN-based method computationally efficient enough for real-time deployment in operational grids?
- How does the complexity and accuracy of the extracted symbolic expression scale with larger power system models?

## Limitations
- No public dataset or detailed AGC simulation parameters provided for independent validation
- Pruning and fine-tuning presented as post-hoc solution rather than integrated design consideration
- Performance on sophisticated, stealthy attacks beyond basic ramp, pulse, and step attacks remains untested
- Computational efficiency for real-time deployment not addressed

## Confidence
- KAN architecture effectiveness for FDIA detection: High
- Interpretability via symbolic extraction: Medium
- Pruning and fine-tuning as solution to accuracy gaps: High

## Next Checks
1. Request or replicate exact AGC system parameters and attack specifications to enable independent verification
2. Extend evaluation to include more sophisticated, stealthy FDIA scenarios
3. Benchmark computational cost of complete pipeline to determine real-time feasibility