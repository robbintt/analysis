---
ver: rpa2
title: Half-Layered Neural Networks
arxiv_id: '2506.04352'
source_url: https://arxiv.org/abs/2506.04352
tags:
- weights
- layer
- half
- units
- hidden
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Half-Layered Neural Networks

## Quick Facts
- arXiv ID: 2506.04352
- Source URL: https://arxiv.org/abs/2506.04352
- Authors: Ethem Alpaydin
- Reference count: 5
- Primary result: Half-layered networks achieve parameter-efficient classification with random projections and trainable scaling parameters, trading some accuracy for significant reduction in learnable weights.

## Executive Summary
Half-layered neural networks introduce a novel architecture where hidden units use fixed random weights for the input projection, with only two trainable parameters per unit (scale and bias). This design significantly reduces the number of learnable parameters compared to standard fully connected layers while maintaining competitive accuracy. The approach demonstrates that networks can achieve reasonable performance by learning to combine random basis functions rather than optimizing all weight matrices directly.

## Method Summary
The method implements hidden units as a two-stage computation: first, a fixed random projection of inputs using weights drawn from a specified distribution (Normal, Binary, Mexican Hat, or Template schemes), then a trainable affine transformation applied to this projection. Only the scaling factor and bias for each unit are optimized during training, while the random projection weights remain constant. This creates networks where the parameter count is independent of input dimensionality, enabling wide layers with minimal memory overhead.

## Key Results
- Half-layers achieve parameter efficiency: `rnd-256-N` uses ~3K parameters vs `mlp-16` using ~12K for similar MNIST accuracy (~92-94%)
- Binary random weights (B scheme) maintain competitive accuracy while further reducing memory (95.23% vs 95.06% vs Normal weights)
- Networks show reduced overfitting compared to MLPs, with training and test accuracy tracking more closely
- Performance degrades with excessive random units due to over-smoothing, suggesting optimal width exists

## Why This Works (Mechanism)

### Mechanism 1
Constraining hidden unit weights to fixed random values while training only scaling factors and biases acts as a strong regularizer, reducing variance and preventing overfitting. By freezing projection directions, the model capacity is strictly limited regardless of layer width, forcing the network to learn a linear combination of fixed random basis functions rather than fitting arbitrary non-linearities.

### Mechanism 2
Training scale and translation parameters per unit allows the network to normalize the operating range of the activation function, compensating for the variance of random weights. The second stage transforms the uncontrolled magnitude of the random dot product into the sensitive region of the nonlinearity, enabling effective learning despite the fixed first stage.

### Mechanism 3
Decoupling parameter count from input fan-in enables use of very wide layers for non-linear representation without the memory penalty of full weight matrices. Standard dense layers require O(d × H) parameters, while half-layers require only O(H) trainable parameters plus the cost of generating/storing random weights, allowing width to scale independently of parameter memory.

## Foundational Learning

- **Concept: Random Projections / Kernel Methods** - Why needed here: To understand why random weights form a valid basis for learning rather than being mere noise. Quick check question: Why can a fixed, random matrix transform data into a space where it is linearly separable?

- **Concept: Bias-Variance Tradeoff** - Why needed here: To interpret experimental results showing half-layers prevent overfitting (reducing variance) but may lower peak accuracy (introducing bias). Quick check question: If I increase random units indefinitely, am I increasing variance or reducing it?

- **Concept: Backpropagation through Non-Linearities** - Why needed here: To understand how gradients flow through trainable parameters but stop at random weights. Quick check question: In the equation ∂L/∂r_hj = 0, why does the gradient stop at the random weights?

## Architecture Onboarding

- **Component map:** Input → Fixed Random Projection → Scale/Bias → Activation → Output
- **Critical path:**
  1. Initialize random weights using seed (N, B, M, or T variants)
  2. Initialize trainable weights and output weights
  3. Forward pass: Random projection → Scale/Bias → Activation → Output
  4. Backward pass: Compute gradients only for trainable parameters

- **Design tradeoffs:**
  - Accuracy vs. Size: Half-layers generally underperform full MLPs on peak accuracy but match or outperform them when parameter budgets are tight
  - Randomness Type: Normal (N) best general accuracy, Binary (B) best for memory/speed, Mexican Hat (M) and Template (T) domain-specific but performed poorly on FashionMNIST

- **Failure signatures:**
  - Underfitting (High Bias): Training and test accuracy track closely but plateau low. Mitigation: Increase hidden units or check if random initialization suits the data
  - Over-smoothing: Test accuracy degrades when increasing units. Mitigation: Reduce units or increase output layer capacity
  - Gradient Issues: If scaling parameter collapses to zero, unit dies. Mitigation: Check learning rates for scaling parameters

- **First 3 experiments:**
  1. Implement `rnd-256-N` vs `mlp-16` on MNIST to verify parameter-efficiency hypothesis
  2. Compare `rnd-1024-N` (Normal) vs `rnd-1024-B` (Binary) on MNIST to verify binary weights maintain accuracy
  3. Replace dense layer of simple CNN with half-layer to verify integration with convolutional features

## Open Questions the Paper Calls Out

- Can half-layered architectures maintain their parameter efficiency and accuracy when scaled to significantly deeper networks and more complex datasets (e.g., ImageNet, NLP tasks) compared to the simple MNIST benchmarks used?

- What mechanisms can mitigate the "over-smoothing" bias introduced by excessive random units, where accuracy can degrade if the number of half units becomes too large?

- Can the performance gap between template-based initialization and generic random initialization be closed by incorporating data-dependent structural priors into the fixed weights?

## Limitations
- Experiments limited to shallow networks on MNIST and FashionMNIST, requiring validation on larger datasets and deeper architectures
- Performance degradation with excessive random units suggests an optimal width exists but lacks adaptive determination methods
- Four random weight schemes tested, but optimal initialization strategy for different data types remains unclear

## Confidence
- Mechanism 1: Medium - Strong theoretical basis but limited empirical validation for this specific 2-parameter scaling mechanism
- Mechanism 2: Low - No direct corpus evidence for this specific normalization mechanism
- Parameter Efficiency Claims: High - Directly supported by experimental results showing significant parameter reduction

## Next Checks
1. Verify that random weights are truly frozen during training by checking gradient flow to the first-stage weights
2. Confirm that binary weight scheme maintains accuracy advantage while reducing memory requirements
3. Test whether integrating half-layers into deeper architectures preserves the parameter-efficiency benefits observed in shallow networks