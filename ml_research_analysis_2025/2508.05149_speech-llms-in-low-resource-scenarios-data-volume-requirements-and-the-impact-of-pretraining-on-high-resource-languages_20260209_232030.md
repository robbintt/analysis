---
ver: rpa2
title: 'Speech LLMs in Low-Resource Scenarios: Data Volume Requirements and the Impact
  of Pretraining on High-Resource Languages'
arxiv_id: '2508.05149'
source_url: https://arxiv.org/abs/2508.05149
tags:
- data
- speech
- hours
- performance
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the use of Speech LLMs in low-resource
  ASR settings using the SLAM-ASR framework. It finds that achieving performance comparable
  to Whisper-only models requires at least 200 hours of training data, highlighting
  the challenges of data scarcity.
---

# Speech LLMs in Low-Resource Scenarios: Data Volume Requirements and the Impact of Pretraining on High-Resource Languages

## Quick Facts
- arXiv ID: 2508.05149
- Source URL: https://arxiv.org/abs/2508.05149
- Authors: Seraphina Fong; Marco Matassoni; Alessio Brutti
- Reference count: 0
- Achieving Whisper-level performance requires at least 200 hours of training data; pretraining linear projectors on high-resource languages improves low-resource ASR, especially with limited data.

## Executive Summary
This study investigates the use of Speech LLMs in low-resource automatic speech recognition (ASR) settings using the SLAM-ASR framework. The authors systematically examine how data volume and pretraining strategies affect performance when adapting a frozen Whisper encoder and LLM to new languages. Key findings show that at least 200 hours of training data are needed to match Whisper-only models, while pretraining projectors on high-resource languages (e.g., English or Spanish) before fine-tuning on low-resource languages (e.g., Italian or Galician) can reduce WER by 30-40% when limited data (10-15 hours) is available. Multilingual projector pretraining further improves generalization.

## Method Summary
The study uses the SLAM-ASR framework with a frozen Whisper-large-v3-turbo encoder, a trainable linear projector (17.31M parameters), and a frozen LLM (EuroLLM 1.7B or Salamandra 2B). The projector transforms Whisper embeddings into the LLM's embedding space. Training uses AdamW optimizer (lr=1e-4), batch size 4, max 100k steps with early stopping, and beam search decoding. Experiments span data scaling (10-252 hours) on Common Voice Italian, projector pretraining on English/Spanish/Italian, and multilingual projector evaluation on Galician. In-domain (Common Voice) and out-of-domain (Fleurs) test sets are used for evaluation.

## Key Results
- Speech LLMs require at least 200 hours of target-language training data to match Whisper-only ASR performance.
- Pretraining linear projectors on high-resource languages reduces WER by 30-40% when fine-tuning data is limited to 10-15 hours.
- Multilingual projector pretraining (combining English, Spanish, and Italian) further improves low-resource performance and generalization.

## Why This Works (Mechanism)

### Mechanism 1
Pretraining the linear projector on a high-resource language before fine-tuning on a low-resource language reduces WER by 30-40% when fine-tuning data is limited to 10-15 hours. The projector learns general speech-to-text embedding alignment patterns during pretraining that transfer across languages, reducing the alignment burden during low-resource fine-tuning. Acoustic similarity between pretrain and target languages improves transfer (Spanish→Italian outperforms English→Italian on in-domain; English shows stronger out-of-domain gains). The frozen Whisper encoder produces language-agnostic representations that the projector can map to the LLM's embedding space using transferable alignment patterns. [Section 4.2] When fine-tuned on smaller datasets (e.g., CV IT with 10 or 15 hours of training data), the pretrained projectors demonstrate a significant improvement in WER over a model trained from scratch. For example, on CV IT with 10 hours of training data, the WER drops from 14.0% (Scratch) to 9.8% (CV100 EN→) and 8.6% (CV200 ES→). [Section 4.2] Using Spanish data is better than employing a projector trained on English as the acoustic similarity with Italian is higher for the former. Break condition: When fine-tuning data exceeds 100-200 hours, the advantage diminishes (Scratch: 6.4% ≈ LS100 EN→: 6.6% at 200h); transfer provides marginal gains when target language data is sufficient.

### Mechanism 2
Multilingual projector pretraining (combining English, Spanish, and Italian) further improves low-resource performance over monolingual pretraining. Exposure to multiple languages during pretraining creates more robust speech-to-embedding alignments that generalize better, as the projector learns to handle greater phonetic and acoustic diversity. Combining languages during pretraining does not create interference but rather enriches the learned alignment space. [Section 4.3] With only 10 hours of GL finetuning data, a fine-tuned multilingual projector achieves a WER of 13.3% and 19.4% on CV GL and FL GL, respectively, versus 18.6% and 43% when trained solely from Scratch. [Section 4.3] A multilingual projector (MULTI→) can further improve performance and generalization capabilities. Break condition: Assumption: benefits may degrade if pretraining languages are too phonologically distant from the target; corpus lacks evidence on language family constraints.

### Mechanism 3
Speech LLMs require ~200 hours of target-language training data to match Whisper-only ASR performance. The lightweight projector (17.31M parameters) must learn language-specific alignment from scratch; insufficient data yields misaligned embeddings, causing the LLM to generate inaccurate transcripts despite its language knowledge. The encoder and LLM remain frozen, so all language-specific adaptation must occur through the projector's limited capacity. [Section 4.1] The configuration with EuroLLM 1.7B and 200 or 252 hours of training data obtains a WER of 6.4% and 6.1% respectively on CV IT, outperforming Whisper-large-v3-turbo (7.1%). [Section 4.1] At 10h: WER 14.0%; at 100h: WER 7.6%; progressive scaling shown in Table 1. Break condition: Cross-domain generalization remains weaker than Whisper-only even at 200h (FL IT: 13.2% vs Whisper-large-v3-turbo: 5.8%); in-domain match does not imply cross-domain parity.

## Foundational Learning

- **Concept: Speech Encoder–LLM Embedding Alignment via Projector**
  - Why needed here: The entire SLAM-ASR framework depends on the projector transforming Whisper's speech embeddings into the LLM's embedding space; misunderstanding this prevents grasping why pretraining helps.
  - Quick check question: What would happen if the projector output dimension didn't match the LLM's input embedding dimension?

- **Concept: Frozen vs. Trainable Components in Transfer Learning**
  - Why needed here: This paper freezes both encoder and LLM, training only the projector; understanding this constraint clarifies why data volume matters and what LoRA fine-tuning would add.
  - Quick check question: Why might LoRA fine-tuning of the LLM's attention layers (query/value projections) improve out-of-domain performance more than in-domain?

- **Concept: Cross-Lingual Transfer and Acoustic Similarity**
  - Why needed here: The paper shows Spanish→Italian transfer outperforms English→Italian for in-domain data, attributed to acoustic similarity; this guides pretraining language selection.
  - Quick check question: For a low-resource Germanic language, which high-resource language would you pretrain on first—English or Spanish—and why?

## Architecture Onboarding

- **Component map:**
Speech Input (Xs) → Whisper-large-v3-turbo (frozen encoder) → Hs → Downsampling (k=5) → Linear Projector (trainable, 17.31M params): Linear → ReLU → Linear → Es → Concatenate Es + Prompt Embedding Ep (+ Transcript Embedding Et during training) → LLM (frozen: EuroLLM 1.7B or Salamandra 2B) → Predicted Transcript

- **Critical path:** Pretrain projector on high-resource language (100–200h) → Fine-tune on low-resource target (10–15h minimum) → Evaluate on both in-domain (CV) and out-of-domain (Fleurs) test sets.

- **Design tradeoffs:**
  - EuroLLM 1.7B vs. Salamandra 2B: EuroLLM consistently outperforms; Salamandra requires larger context (8192 vs. 4096) and may need more data to close the gap.
  - Monolingual vs. Multilingual pretraining: Multilingual improves generalization but requires coordinating multiple datasets.
  - LoRA fine-tuning: Adds 1.38M parameters; marginal in-domain gains (~0.3–0.5% WER) but notable out-of-domain improvements (~1.2–2.5% WER).

- **Failure signatures:**
  - WER >30% with <20h training from scratch → projector failed to learn meaningful alignment.
  - Large gap between in-domain and out-of-domain WER (e.g., 6.4% vs. 13.2%) → overfitting to training domain; consider LoRA or multilingual pretraining.
  - Salamandra 2B WER >50% at low data → context length mismatch or insufficient training steps.

- **First 3 experiments:**
  1. **Baseline data scaling:** Train projector from scratch on 10, 50, 100, 200h of target language data; plot WER curve to confirm 200h threshold.
  2. **Pretrained projector transfer:** Pretrain on 100h English (LibriSpeech), fine-tune on 10h and 15h target language; compare WER to scratch baseline.
  3. **Multilingual projector test:** Pretrain on combined English + Spanish + related high-resource language, fine-tune on 10h low-resource target; compare to monolingual pretraining.

## Open Questions the Paper Calls Out
The paper identifies several open questions for future research. First, it highlights the need to explore how specific linguistic and acoustic similarities between source and target languages influence the efficacy of cross-lingual projector transfer, particularly between closely related languages. Second, the paper raises the question of whether multilingual projector pretraining benefits stem primarily from increased pretraining data volume or from the diversity of linguistic representations, as the current experiments do not control for total pretraining hours. Third, the persistent performance gap between in-domain and out-of-domain evaluation suggests the need for architectural or training modifications to improve cross-domain generalization. Finally, the paper questions whether high-resource pretraining strategies can effectively bootstrap performance for extremely low-resource languages that are linguistically distant from the pretraining source languages.

## Limitations
- The benefits of pretraining may diminish or reverse for language pairs with very different phonologies, as the assumption of language-agnostic speech representations from Whisper is not empirically validated across diverse language families.
- Data scaling conclusions are based on a single language family (Romance languages) and may not generalize to typologically distinct languages or non-European scripts.
- The evaluation framework focuses on WER as the sole metric, with no analysis of hallucination rates, semantic preservation, or practical deployment considerations like inference latency.

## Confidence
- **High confidence:** Data volume requirements (200h threshold) and the consistent performance advantage of EuroLLM 1.7B over Salamandra 2B are well-supported by systematic experiments.
- **Medium confidence:** The 30-40% WER improvement from pretraining projectors on high-resource languages is demonstrated but relies on specific language pairs (Spanish→Italian) where acoustic similarity is high.
- **Medium confidence:** Multilingual projector pretraining benefits are shown but the optimal composition and pretraining duration for multilingual models remain unclear.

## Next Checks
1. **Cross-family transfer validation:** Replicate the projector pretraining experiments with language pairs from different families (e.g., Spanish→Turkish or English→Vietnamese) to test the acoustic similarity hypothesis and identify breaking points.
2. **Progressive pretraining ablation:** Compare monolingual pretraining, bilingual pretraining, and multilingual pretraining on the same target language to determine if adding languages improves or degrades performance, and identify the optimal pretraining composition.
3. **Out-of-domain robustness test:** Extend evaluation beyond Fleurs to include completely different domains (e.g., conversational speech, noisy environments) to assess whether the 200h threshold holds for practical deployment scenarios.