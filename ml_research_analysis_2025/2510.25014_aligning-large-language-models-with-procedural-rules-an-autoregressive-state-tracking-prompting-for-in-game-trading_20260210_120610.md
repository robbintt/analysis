---
ver: rpa2
title: 'Aligning Large Language Models with Procedural Rules: An Autoregressive State-Tracking
  Prompting for In-Game Trading'
arxiv_id: '2510.25014'
source_url: https://arxiv.org/abs/2510.25014
tags:
- uni00000013
- state
- uni00000014
- price
- trade
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work tackles the problem of maintaining strict procedural\
  \ flows in LLM-driven in-game trading, where the models\u2019 natural language flexibility\
  \ risks skipping critical safeguards like purchase confirmation. To resolve this,\
  \ the paper introduces Autoregressive State-Tracking Prompting (ASTP), a prompting\
  \ strategy that makes dialogue state transitions explicit and verifiable by requiring\
  \ the model to identify and report the previous state label before generating a\
  \ response."
---

# Aligning Large Language Models with Procedural Rules: An Autoregressive State-Tracking Prompting for In-Game Trading

## Quick Facts
- arXiv ID: 2510.25014
- Source URL: https://arxiv.org/abs/2510.25014
- Authors: Minkyung Kim; Junsik Kim; Woongcheol Yang; Sangdon Park; Sohee Bae
- Reference count: 40
- One-line primary result: ASTP achieves >99% procedural compliance and 99.3% price accuracy in LLM-driven in-game trading dialogues

## Executive Summary
This work tackles the problem of maintaining strict procedural flows in LLM-driven in-game trading, where the models' natural language flexibility risks skipping critical safeguards like purchase confirmation. To resolve this, the paper introduces Autoregressive State-Tracking Prompting (ASTP), a prompting strategy that makes dialogue state transitions explicit and verifiable by requiring the model to identify and report the previous state label before generating a response. A state-specific placeholder-based post-processing mechanism is also introduced to ensure accurate price calculations. Evaluation across 300 trading dialogues shows >99% procedural compliance and 99.3% price calculation accuracy. Notably, ASTP with post-processing on smaller models matches larger models' accuracy while reducing response time from 21.2s to 2.4s, demonstrating its practicality for real-time, resource-constrained applications.

## Method Summary
ASTP is a prompting strategy that enforces procedural compliance in LLM-driven dialogues by requiring explicit state tracking. The method has four key elements: (1) state definitions with valid transitions, (2) coupled transition conditions with state definitions, (3) directive to identify previous state first, and (4) directive to output identified previous state in the response. A state-specific placeholder post-processing (PPP) mechanism replaces arithmetic calculations with deterministic post-processing in critical states. The approach was evaluated on 300 trading dialogues with 52 game items and 20 sellable merchant items across two scenarios: specific item requests and purpose-based recommendations.

## Key Results
- >99% State Transition Compliance Rate (STCR) achieved, up from 78.1% in baselines
- 99.3% price calculation accuracy, up from 84.3% without post-processing
- ASTP with PPP enables smaller models to match larger model accuracy while reducing response time from 21.2s to 2.4s
- Single state failure observed in 300 dialogues (0.36%) due to semantic ambiguity

## Why This Works (Mechanism)

### Mechanism 1: Autoregressive State Anchoring
The identification directive creates an explicit reasoning step that forces context retrieval from dialogue history. The output requirement makes this inference verifiable and commits the model to a specific state before determining the next transition. This factorization—separating "what was the previous state?" from "what should I do next?"—reduces the model's tendency to skip procedural safeguards when faced with strong user intent.

### Mechanism 2: Prime-Guide-Enforce Prompt Architecture
The prompt creates a three-stage cognitive workflow: Prime (identify previous state first), Guide (state definitions with coupled transition conditions), Enforce (output requirement creates external verification). This arrangement transforms state-tracking from a suggestion into a mandatory, verifiable task.

### Mechanism 3: State-Specific Placeholder Post-Processing (PPP)
Delegating arithmetic to deterministic post-processing by having the LLM output a placeholder token improves price calculation accuracy. The system instructs the LLM to use `__PRICE__` for total amounts only in the OFFER_SELL state, where external code calculates the correct sum from the item array in the response and replaces the placeholder.

## Foundational Learning

- **Concept: State-Space Formalization**
  - Why needed here: ASTP requires defining explicit state sets (S_base, S_trade) and valid transition constraints. Without this formalization, you cannot write the transition rules or debug compliance.
  - Quick check question: Can you enumerate all possible states in your dialogue flow and draw which transitions are valid/invalid?

- **Concept: Task-Oriented Dialogue (TOD) Paradigm**
  - Why needed here: The paper positions ASTP against TOD approaches. Understanding why TOD's goal-driven optimization misaligns with procedural enforcement helps explain why ASTP's design is necessary.
  - Quick check question: What's the difference between "help the user buy an item" (TOD goal) and "enforce the FINAL_CHECK before COMMIT_SALE" (procedural rule)?

- **Concept: Chain-of-Thought vs. Constrained Reasoning**
  - Why needed here: ZS-CoT comparison shows generic "think step-by-step" achieves lower compliance than ASTP's constrained "identify and output previous state." Understanding this distinction is critical for prompt design.
  - Quick check question: Why would "think step-by-step" still allow a model to skip FINAL_CHECK when a user says "yes, I'll take it"?

## Architecture Onboarding

- **Component map:**
User Utterance → [Prompt Constructor] → System Instructions + Game World Data + Dialogue History + Dialogue Guidelines + Response Format → LLM Inference → JSON Response Parser → State = OFFER_SELL? → Yes → PPP: Replace `__PRICE__` → No → Pass through → Dialogue History Update → NPC Dialogue to Player

- **Critical path:** The state transition FINAL_CHECK → COMMIT_SALE is the most sensitive. The prompt's Element 4 (last_trade_context output) is verified against the previous state. If mismatch, the system should re-inject FINAL_CHECK.

- **Design tradeoffs:**
  - Strictness vs. flexibility: ASTP enforces procedure at the cost of sometimes feeling rigid
  - Model size vs. latency: PPP enables smaller models (2.4s) to match larger model accuracy (21.2s)
  - Placeholder keyword choice: Short keywords (`__PRICE__`) are more reliable than longer variants

- **Failure signatures:**
  1. State ambiguity failure (0.36%): NPC generates a question in OFFER_SELL that semantically resembles FINAL_CHECK confirmation
  2. Multi-intent utterance failure (0.7%): Player modifies cart + negotiates price in one turn
  3. Placeholder parsing failure: Using ambiguous or long placeholder keywords causes malformed JSON output

- **First 3 experiments:**
  1. Ablate one element at a time: Implement Baselines 1-4 as described. Run 50 dialogues per baseline with fixed seeds. Measure STCR specifically for FINAL_CHECK → COMMIT_SALE transition.
  2. Test placeholder robustness: Try `__PRICE__`, `__TOTAL__`, `__AMOUNT__`, and `__PRICE_PLACEHOLDER__` across 100 OFFER_SELL responses. Measure parsing success rate and accuracy.
  3. Stress test with adversarial user intents: Create a test set where players use strong intent signals immediately after OFFER_SELL. Compare ASTP against a ZS-CoT baseline.

## Open Questions the Paper Calls Out

- **Open Question 1:** How does ASTP perform when scaling to state spaces with significantly more states (e.g., 20-50 states) and more complex, non-linear transition rules with branching paths and conditional dependencies?
  - Basis: Conclusion states future work should investigate scalability across larger state spaces
  - Why unresolved: Current evaluation uses only 7 states with simple linear flow
  - Evidence needed: Evaluation on dialogue systems with larger state spaces measuring compliance rates and response latency scaling

- **Open Question 2:** Can ASTP's state-compliance benefits transfer to non-Gemini LLM families (e.g., GPT, Claude, Llama), or is the effectiveness tied to Gemini-specific reasoning capabilities?
  - Basis: All experiments exclusively use Gemini models with no cross-architecture validation
  - Why unresolved: Prompt-based methods often exhibit sensitivity to model architecture
  - Evidence needed: Replication across at least 3 different LLM families with identical prompt structures

- **Open Question 3:** How can ASTP be made robust to semantic ambiguity where responses in one state (e.g., OFFER_SELL) linguistically resemble another state's requirements (e.g., FINAL_CHECK confirmation questions)?
  - Basis: Failure case analysis reveals sole state-compliance failure occurred when OFFER_SELL response included confirmation-like question
  - Why unresolved: Paper identifies this as a "nuanced challenge" but offers no solution
  - Evidence needed: Modified ASTP variant that explicitly disallows confirmation-style questions in non-FINAL_CHECK states

## Limitations

- Experimental setup gaps: Virtual player LLM configuration and dialogue history format not fully specified
- Generalization concerns: PPP effectiveness in other states with complex arithmetic remains untested
- Resource efficiency claims: Trade-off between token savings and computational overhead of external post-processing not fully quantified

## Confidence

**High Confidence (8-10/10):** Core mechanism of explicit state identification and verifiable output reliably enforces procedural compliance. Well-supported by experimental design and results.

**Medium Confidence (5-7/10):** Architectural ordering benefits demonstrated but could benefit from more systematic ablation studies. PPP approach's robustness to various placeholder formats needs further validation.

**Low Confidence (1-4/10):** Virtual player simulation's behavioral consistency and dialogue history format's impact on state identification accuracy remain unclear.

## Next Checks

1. **Virtual Player Simulation Stress Test:** Create adversarial test sets where players use strong intent signals ("I'll take it!", "Yes, buy it now!") immediately after OFFER_SELL. Compare ASTP against a ZS-CoT baseline to validate procedural enforcement under conditions most likely to cause failures.

2. **Multi-State Arithmetic Validation:** Extend PPP to other states involving calculations (e.g., bulk discounts, tax calculations). Run 100 dialogues per state type to identify whether calculation failures cluster in specific contexts.

3. **Prompt Ordering Ablation Study:** Systematically test different prompt element arrangements across 50 dialogues each to quantify how much of ASTP's effectiveness comes from content versus ordering.