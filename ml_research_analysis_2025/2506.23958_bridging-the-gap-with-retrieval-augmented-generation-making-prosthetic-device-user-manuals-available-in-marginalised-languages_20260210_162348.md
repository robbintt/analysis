---
ver: rpa2
title: 'Bridging the Gap with Retrieval-Augmented Generation: Making Prosthetic Device
  User Manuals Available in Marginalised Languages'
arxiv_id: '2506.23958'
source_url: https://arxiv.org/abs/2506.23958
tags:
- languages
- user
- language
- manuals
- prosthetic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This research addresses language barriers in accessing healthcare
  information in Africa by developing an AI-powered framework that translates complex
  prosthetic device user manuals into marginalised languages. The system uses a Retrieval-Augmented
  Generation (RAG) pipeline combined with Natural Language Processing (NLP) models
  to enable users to upload English-language manuals, ask questions in their native
  language, and receive accurate, localised answers in real time.
---

# Bridging the Gap with Retrieval-Augmented Generation: Making Prosthetic Device User Manuals Available in Marginalised Languages

## Quick Facts
- arXiv ID: 2506.23958
- Source URL: https://arxiv.org/abs/2506.23958
- Reference count: 22
- This research develops an AI-powered framework that translates complex prosthetic device user manuals into marginalised languages using RAG and NLP, demonstrated with Nigerian Pidgin.

## Executive Summary
This research addresses language barriers in accessing healthcare information in Africa by developing an AI-powered framework that translates complex prosthetic device user manuals into marginalised languages. The system uses a Retrieval-Augmented Generation (RAG) pipeline combined with Natural Language Processing (NLP) models to enable users to upload English-language manuals, ask questions in their native language, and receive accurate, localised answers in real time. The approach is demonstrated using Nigerian Pidgin as a case study but is designed to be easily extended to other low-resource languages. The open-source framework aims to improve health literacy and global health equity by making critical medical information accessible to underserved populations.

## Method Summary
The framework implements a RAG pipeline where users upload English prosthetic device manuals, which are processed through text extraction and chunking before being embedded into a vector store. When users pose questions in their native language (e.g., Nigerian Pidgin), the system translates the query to English, retrieves semantically similar passages from the RAG index, generates an answer using an LLM, and translates the response back to the target language. The architecture is modular, allowing extension to other marginalised languages by plugging in appropriate translation models, provided they exist. The system prioritizes real-time accessibility over pre-translation of entire documents.

## Key Results
- Enables users to upload English-language medical equipment manuals and receive localised answers in marginalised languages
- Uses cross-lingual question-answering to bypass full document translation, reducing computational cost
- Open-source framework designed for rapid extension to other low-resource languages
- Addresses critical healthcare information access barriers for underserved populations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RAG enables accurate, grounded responses from complex technical documents without requiring model retraining.
- Mechanism: The Retrieval-Augmented Generation pipeline ingests uploaded English manuals, chunking and embedding them into a vector store. When a user queries the system, relevant passages are retrieved based on semantic similarity, then passed to a generative model to construct answers. This grounds responses in source material rather than relying solely on pretrained knowledge.
- Core assumption: The source documents contain the answers users need, and chunking preserves sufficient context for retrieval.
- Evidence anchors:
  - [abstract] "The system integrates a Retrieval-Augmented Generation (RAG) pipeline for processing and semantic understanding of the uploaded manuals."
  - [section] Page 2: "Question Answering (QA) is an NLP task where models are used to extract or generate precise answers to questions... It enables efficient information retrieval, comprehension, and decision-making."
  - [corpus] FHIR-RAG-MEDS (arXiv:2509.07706) demonstrates RAG integration for clinical guidelines, supporting the mechanism's applicability to medical documents.
- Break condition: If chunking loses critical context (e.g., tables, images, cross-references), retrieval quality degrades and answers become incomplete or misleading.

### Mechanism 2
- Claim: Cross-lingual QA bypasses the need for full document translation by translating only queries and answers.
- Mechanism: Users pose questions in their native language (e.g., Pidgin). The system translates the query to English, retrieves relevant content from the RAG index, generates an English answer, and translates it back to the target language. This avoids translating entire manuals, reducing computational cost and enabling real-time interaction.
- Core assumption: Translation models exist for the source-target language pair and preserve meaning in both directions.
- Evidence anchors:
  - [abstract] "Enables users... to upload English-language medical equipment manuals, pose questions in their native language, and receive accurate, localised answers in real time."
  - [section] Page 3: "It is a plug-and-play situation; no additional support is required" — provided translation models are available.
  - [corpus] Corpus evidence for low-resource translation mechanisms is weak; no directly comparable cross-lingual medical QA systems were identified among neighbors.
- Break condition: If translation models are poor quality or unavailable for a language pair, query interpretation and answer fidelity collapse. The authors explicitly note this dependency.

### Mechanism 3
- Claim: Modular architecture enables extension to other low-resource languages without rebuilding the pipeline.
- Mechanism: The framework separates concerns: document processing (language-agnostic RAG), translation (swappable models per language pair), and generation (LLM-based). By standardizing interfaces between components, new languages can be added by plugging in appropriate translation models.
- Core assumption: Translation models for marginalized languages will improve over time through external initiatives (UN, Google, Meta).
- Evidence anchors:
  - [abstract] "Open-source framework has been designed to enable rapid and easy extension to other languages/dialects."
  - [section] Page 3: "The underlying architecture could be deployed with other languages provided translation models are available, and the source user manual is in any of the 20 typical high-resource languages."
  - [corpus] POLYRAG (arXiv:2504.14917) discusses modular RAG integration for medical applications, supporting the pluggable architecture concept.
- Break condition: If translation models exhibit systematic bias or hallucinate medical terminology, the modular design propagates rather than contains errors.

## Foundational Learning

- Concept: **Retrieval-Augmented Generation (RAG)**
  - Why needed here: RAG is the core mechanism for grounding answers in uploaded manuals rather than relying on potentially outdated or incorrect pretrained knowledge.
  - Quick check question: Can you explain why RAG reduces hallucination compared to using a standalone LLM?

- Concept: **Low-resource language challenges in NLP**
  - Why needed here: The paper explicitly notes AI models underperform for low-resource languages due to training data imbalances; understanding this frames the design constraints.
  - Quick check question: What happens to translation quality when parallel corpora are scarce for a language pair?

- Concept: **Question-Answering vs. Text Simplification**
  - Why needed here: The authors abandoned text simplification due to missing evaluation metrics and datasets for Igbo/Yoruba; QA was chosen as an alternative approach.
  - Quick check question: Why might QA be more robust than full document translation for low-resource settings?

## Architecture Onboarding

- Component map:
  - **Document Ingestion**: PDF/manual upload → text extraction → chunking
  - **RAG Index**: Vector embeddings stored in a retrievable index
  - **Query Translation**: Native language → English (using translation model)
  - **Retrieval**: Semantic search over indexed chunks
  - **Generation**: LLM synthesizes answer from retrieved passages
  - **Response Translation**: English → native language output
  - **User Interface**: Question input, answer display

- Critical path:
  1. Translation model availability for target language pair (English ↔ marginalized language)
  2. Quality of document chunking and embedding (affects retrieval precision)
  3. LLM's ability to synthesize accurate answers from retrieved context

- Design tradeoffs:
  - **Plug-and-play flexibility vs. optimized performance**: The framework prioritizes extensibility; domain-specific tuning is deferred.
  - **Real-time translation vs. pre-translated corpus**: Query-time translation reduces upfront work but adds latency and translation error risk per interaction.
  - **Open-source models vs. proprietary APIs**: The authors commit to open-source; this limits model selection but improves accessibility.

- Failure signatures:
  - **Retrieval misses**: User asks about content present in manual, but system returns irrelevant chunks → check embedding quality and chunk size.
  - **Translation drift**: Medical terms mistranslated → validate domain-specific terminology in translation model.
  - **Hallucinated safety information**: System generates instructions not in source → enforce strict grounding constraints; consider expert-in-the-loop validation (noted as future work on Page 3).

- First 3 experiments:
  1. **End-to-end validation with Pidgin**: Upload a prosthetic manual, ask 10-20 domain questions in Pidgin, measure answer accuracy and translation quality against human expert judgments.
  2. **Ablation on chunk size**: Test retrieval precision with varying chunk sizes (e.g., 256 vs. 512 vs. 1024 tokens) to identify optimal granularity for technical manuals.
  3. **Language extension stress test**: Attempt to plug in a different low-resource language (e.g., Igbo or Yoruba) with available translation models; document friction points and failure modes.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does pre-translation text simplification (English to Simple English to Target) or post-translation simplification (English to Target to Simple Target) yield better readability for low-resource languages like Igbo or Yoruba?
- Basis in paper: [explicit] The authors state that due to a lack of metrics, the crucial question "Do you simplify the texts in English and then translate to Igbo or do you translate to Igbo and then simplify the Igbu text?" could not be answered.
- Why unresolved: There is a lack of available dictionaries, datasets, and specific readability evaluation metrics for these marginalised languages.
- What evidence would resolve it: A comparative study utilizing newly developed readability metrics for Igbo/Yoruba to assess the quality of simplified medical texts generated via both pipelines.

### Open Question 2
- Question: How can the RAG pipeline be optimized to accurately retrieve and present non-textual elements, such as images and tables, alongside generated answers?
- Basis in paper: [explicit] The authors identify "optimisation for image and table handling such that results of queries also return associated images and tables" as a specific requirement for further work.
- Why unresolved: Current text-based RAG implementations often discard visual layout information, which is critical for understanding complex device manuals.
- What evidence would resolve it: System benchmarks demonstrating successful multimodal retrieval where user queries return the correct text paired with the specific relevant diagrams or data tables from the manual.

### Open Question 3
- Question: What is the most effective method for integrating an "expert-in-the-loop" to ensure the safety and reliability of generated medical instructions?
- Basis in paper: [explicit] The authors note the need for "ethical, responsible, and regulatory alignment" and suggest that "including an expert-in-the-loop might help enhance the trust and reliability of generated texts."
- Why unresolved: The current framework operates as an automated "plug-and-play" system without inherent mechanisms for human verification or clinical safety guardrails.
- What evidence would resolve it: Usability studies and safety audits comparing the accuracy and user trust of the system with and without professional clinician verification steps in the workflow.

## Limitations
- Performance heavily dependent on translation model quality for low-resource languages, which remains an open challenge
- No empirical validation exists for languages beyond the initial Pidgin case study
- Absence of quantitative evaluation metrics or benchmark datasets makes objective accuracy assessment difficult

## Confidence
- **High**: The core RAG architecture and modular design principles are sound and well-established in the literature
- **Medium**: The translation-based cross-lingual QA approach is feasible but heavily dependent on translation model availability and quality
- **Low**: Claims about accessibility improvements and health equity impact lack empirical validation

## Next Checks
1. Conduct end-to-end accuracy testing with Pidgin: Upload multiple prosthetic manuals, pose domain-specific questions in Pidgin, and measure answer accuracy against expert human judgments across at least 50 test cases.
2. Test translation quality for medical terminology: Evaluate English-Pidgin translation model performance on domain-specific vocabulary using a medical term translation accuracy benchmark.
3. Stress test language extensibility: Attempt to deploy the system for Igbo or Yoruba, documenting translation model availability, performance degradation, and any architectural friction points encountered.