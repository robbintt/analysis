---
ver: rpa2
title: Model-agnostic post-hoc explainability for recommender systems
arxiv_id: '2509.10245'
source_url: https://arxiv.org/abs/2509.10245
tags:
- user
- items
- users
- influential
- most
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Model-agnostic post-hoc explainability for recommender systems

## Quick Facts
- arXiv ID: 2509.10245
- Source URL: https://arxiv.org/abs/2509.10245
- Authors: Irina ArÃ©valo; Jose L Salmeron
- Reference count: 40
- Primary result: A model-agnostic deletion diagnostic method that identifies influential users/items by retraining the recommender without them and measuring performance change

## Executive Summary
This paper proposes a model-agnostic post-hoc explainability method for recommender systems that quantifies the influence of individual users or items on model performance. The approach uses deletion diagnostics: it systematically removes each user or item from the training set, retrains the model, and measures the resulting performance change. Experiments on MovieLens and Amazon Reviews datasets demonstrate the method's ability to identify influential data points and show that removing the least influential users can improve model performance. The framework is validated on both Neural Collaborative Filtering (NCF) and Singular Value Decomposition (SVD) models, demonstrating its model-agnostic nature.

## Method Summary
The method treats recommender systems as black boxes, requiring only the ability to train models and compute evaluation metrics. For each user or item, the approach creates a modified training set without that entity, retrains the same model architecture from scratch, and computes the difference in evaluation metric (MAP) between the original and retrained models. This difference quantifies the entity's influence on overall performance. The method is applied to both NCF and SVD models on MovieLens 100K and Amazon Electronics datasets, using a 75/25 train-test split and various evaluation metrics including MAP, NDCG, and precision@k.

## Key Results
- The method successfully identifies influential users/items across both NCF and SVD models
- Removing the 10 least influential users from MovieLens-NCF dataset improved MAP@K by 18.49%
- Results demonstrate that the least influential users often contribute noise or redundant information
- The approach is validated on two fundamentally different architectures (NCF and SVD), confirming model-agnostic nature

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Removing specific training data points and measuring the resulting performance change quantifies their influence on recommender systems.
- **Mechanism:** The method implements deletion diagnostics via a leave-one-out retraining approach. It systematically retrains the recommender model (NCF or SVD) $n$ times for users and $m$ times for items. Influence is calculated as: $\text{Influence}(-i) = \text{eval} - \text{eval}(-i)$, where a positive value indicates the removed entity positively contributed to the original model's performance.
- **Core assumption:** The evaluation metric (e.g., MAP) accurately reflects the true quality and utility of recommendations. Assumption: The computational cost of full retraining is acceptable for the diagnostic task.
- **Evidence anchors:**
  - [abstract] "The method compares the performance of a model to that of a similar model trained without a specific user or item, allowing us to quantify how that observation influences the recommender, either positively or negatively."
  - [section] "The influence of a user or item i is computed as: Influence(-i) = eval - eval(-i)... Algorithms 1 and 2 detail the computation for users and items, respectively."
  - [corpus] Corpus evidence on perturbation-based diagnostics is limited to one related paper (Towards End-to-End Model-Agnostic Explanations for RAG Systems), which applies a similar concept to a different domain.
- **Break condition:** The evaluation metric is misaligned with actual recommendation goals (e.g., optimizing MAP fails to capture user satisfaction or fairness). Retraining is computationally infeasible for the dataset size.

### Mechanism 2
- **Claim:** This explainability method is model-agnostic because it relies only on a model's input-output behavior, not its internal architecture or parameters.
- **Mechanism:** The framework treats the recommender as a black box. Its only requirements are: (1) the ability to train the model on a given dataset, and (2) the ability to compute a performance metric on its outputs. It was validated on two fundamentally different architectures: the deep, non-linear Neural Collaborative Filtering (NCF) and the linear, classical Singular Value Decomposition (SVD).
- **Core assumption:** The models are fully retrainable, and their performance can be consistently evaluated after each removal. Assumption: The findings from one model type will generalize to others.
- **Evidence anchors:**
  - [abstract] "To demonstrate its model-agnostic nature, the proposal is applied to both Neural Collaborative Filtering (NCF)... and Singular Value Decomposition (SVD)..."
  - [section] "In contrast to many explainability approaches... which typically focus on local, instance-level explanations or rely on surrogate models, the proposed method directly measures the global impact of removing individual data points."
  - [corpus] Evidence for agnostic methods is weak; other related papers focus on specific LLM-based or GNN-based recommendation techniques.
- **Break condition:** The model cannot be efficiently retrained (e.g., it's a large language model with prohibitive training costs). The model's performance is highly stochastic between training runs, making influence scores noisy.

### Mechanism 3
- **Claim:** Identifying low-influence or negatively influential users/items can guide data curation to improve model generalization.
- **Mechanism:** The analysis reveals that "least influential" entities often contribute noise or redundant information. Their removal from the training set can regularize the learned latent space, reduce model variance, and improve performance on unseen data. The paper shows performance gains (e.g., +18.49% in MAP@K on MovieLens-NCF) after removing the 10 least influential users.
- **Core assumption:** The "influence score" is a stable and reliable proxy for data quality and generalizability.
- **Evidence anchors:**
  - [abstract] "Experiments on the MovieLens and Amazon Reviews datasets provide insights into model behavior and highlight the generality of the approach..."
  - [section] "The improvement in performance following the removal of the least influential users suggests that these users contribution is minimal or even detrimental signal during training."
  - [corpus] The "MEGG: Replay via Maximally Extreme GGscore..." paper also focuses on data curation for incremental learning, supporting the broader idea of refining training data.
- **Break condition:** Data is extremely sparse, and removing even low-influence points further degrades the user/item interaction matrix, harming cold-start performance. Influence scores are unstable across different random seeds.

## Foundational Learning

- **Concept: Collaborative Filtering & Matrix Factorization**
  - **Why needed here:** The method is evaluated on SVD (a classic MF technique) and NCF (a neural generalization of it). Understanding the core idea of learning latent user and item vectors from interaction data is essential.
  - **Quick check question:** How does a model predict a user's preference for an item using latent vectors? (Answer: Typically via their dot product).

- **Concept: Model-Agnostic vs. Model-Intrinsic Explainability**
  - **Why needed here:** This paper's contribution is a *post-hoc, model-agnostic* method. Distinguishing this from methods that require access to model internals (e.g., attention weights, gradients) clarifies its design constraints and capabilities.
  - **Quick check question:** Can a model-agnostic explainability technique be applied to any black-box model without modification? (Answer: Yes).

- **Concept: Leave-One-Out Cross-Validation (LOOCV)**
  - **Why needed here:** The deletion diagnostic is a form of LOOCV applied to training data. Understanding the principle of systematically removing a data point to test its impact is central to the paper's methodology.
  - **Quick check question:** What is the primary computational drawback of the leave-one-out approach? (Answer: It requires retraining the model $N$ times, where $N$ is the number of data points).

## Architecture Onboarding

- **Component map:** Data Manager -> Model Training Loop -> Evaluation Module -> Deletion Diagnostic Core

- **Critical path:**
  1.  **Baseline Training:** Train the target recommender model (NCF or SVD) on the full training set and compute its baseline evaluation metrics (e.g., `eval`).
  2.  **Influence Loop (User/Item):** For each user $u$ (or item $i$), create a modified training set $X_{(-u)}$.
  3.  **Retraining & Scoring:** Retrain the *same* model architecture on $X_{(-u)}$. Compute the new metric `eval(-u)`. Calculate and store the influence score $\Delta = \text{eval} - \text{eval}(-u)$.
  4.  **Ranking & Action:** Rank all users/items by their influence scores to identify the most/least influential entities for downstream tasks (debugging, data pruning).

- **Design tradeoffs:**
  - **High Fidelity vs. Computational Cost:** The method provides high-fidelity influence scores by using full retraining, but this is computationally expensive ($O(n \times T_{train})$). Approximations (e.g., influence functions) are faster but may be less accurate.
  - **Global vs. Local Insight:** The method offers global, data-centric influence. It does not provide local, per-recommendation explanations (e.g., "this item was recommended because of feature X").

- **Failure signatures:**
  - **Metric Sensitivity:** Influence scores are tied to the chosen evaluation metric (MAP). A user influential for MAP may not be influential for another metric like MAE.
  - **Instability:** Results can vary if the underlying model training is unstable or highly sensitive to random initialization.
  - **Scalability:** The method becomes prohibitively slow as the number of users/items grows into the millions without parallelization or sampling.

- **First 3 experiments:**
  1.  **Baseline Validation:** Replicate the paper's findings on MovieLens 100K. Train an NCF model, compute baseline MAP, then run the user-deletion loop for the first 10-20 users to verify the influence score calculation.
  2.  **Data Pruning Test:** Identify and remove the top 10 "least influential" users from the training set based on the diagnostic results. Retrain the model on this curated dataset and compare its MAP@K and NDCG against the baseline to quantify the performance gain.
  3.  **Cross-Model Consistency:** Apply the diagnostic to both NCF and a simpler model (like SVD or a basic k-NN). Compare the list of influential users. A high overlap would suggest that influence is a property of the data, while a low overlap would suggest it is an artifact of the model architecture.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can approximation algorithms or parallelization strategies be developed to reduce the computational cost of deletion diagnostics for large-scale, real-time recommender systems?
- Basis in paper: [explicit] The Conclusion identifies "high runtime complexity on large-scale datasets" as a limitation and explicitly lists "approximation algorithms, parallelization strategies... enabling deployment in larger or real-time settings" as necessary future research.
- Why unresolved: The current method requires retraining the model once per user or item ($O(n \times T_{train})$), which is computationally prohibitive for industrial-scale datasets.
- What evidence would resolve it: A modified algorithm achieving statistically similar influence fidelity to the full retraining method but with significantly lower time complexity, validated on datasets exceeding the size of MovieLens 100K.

### Open Question 2
- Question: Does the deletion diagnostics framework effectively identify influential data points in non-collaborative filtering architectures, such as sequential models or autoencoders?
- Basis in paper: [explicit] Section 3.4 states that extension to "other recommender families, including... autoencoders and hybrid systems" is a "promising direction for future research," although experiments were limited to NCF and SVD.
- Why unresolved: While the method is model-agnostic, it has only been validated on static matrix factorization and neural CF paradigms; temporal dynamics in sequential models may alter how influence propagates.
- What evidence would resolve it: Experimental results applying deletion diagnostics to sequential or autoencoder-based recommenders, demonstrating consistent identification of influential users/items.

### Open Question 3
- Question: How does the choice of evaluation metric (e.g., MAP vs. NDCG) alter the identification and ranking of "influential" observations?
- Basis in paper: [inferred] The Conclusion lists "reliance on the chosen evaluation metric to reflect recommendation quality" as a limitation. Additionally, the paper notes that removal effects were more pronounced in MAP because it was the optimization metric.
- Why unresolved: It is unclear if a user or item deemed "influential" by MAP would retain that status if the system were optimized for ranking metrics like NDCG or error metrics like MAE.
- What evidence would resolve it: A sensitivity analysis comparing the overlap and rank correlation of influential users/items identified using MAP versus NDCG or Precision@K on the same model.

## Limitations
- **High computational cost:** Full retraining for every user/item makes the method prohibitively expensive for large-scale datasets
- **Metric dependence:** Influence scores are tied to the chosen evaluation metric (MAP), which may not reflect all recommendation quality aspects
- **Scalability issues:** The method becomes impractical for industrial-scale recommender systems with millions of users/items without optimization

## Confidence
- **High Confidence:** The fundamental mechanism of deletion diagnostics and the model-agnostic design are valid and clearly demonstrated
- **Medium Confidence:** The reported performance improvements from data pruning are plausible but may be dataset-dependent and require further validation
- **Low Confidence:** The scalability claims and the stability of influence scores across different model architectures or random seeds are not thoroughly evaluated

## Next Checks
1. **Replicate on a Subset:** Apply the method to the first 50-100 users of MovieLens 100K to verify the influence score calculation and confirm the reported performance gain from pruning
2. **Test Metric Sensitivity:** Apply the diagnostic using a different evaluation metric (e.g., MAE or Precision@K) and compare the lists of influential users to assess metric dependence
3. **Assess Stability:** Run the user-deletion loop multiple times with different random seeds for NCF training. Calculate the variance in influence scores for the same users to quantify the method's stability