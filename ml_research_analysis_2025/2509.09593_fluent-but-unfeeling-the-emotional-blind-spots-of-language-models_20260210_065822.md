---
ver: rpa2
title: 'Fluent but Unfeeling: The Emotional Blind Spots of Language Models'
arxiv_id: '2509.09593'
source_url: https://arxiv.org/abs/2509.09593
tags:
- emotion
- emotions
- language
- mask
- emotional
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study introduces EXPRESS, a benchmark dataset for fine-grained\
  \ emotion recognition that addresses limitations in existing benchmarks by using\
  \ self-disclosed emotions from Reddit posts with an average length of 259 words.\
  \ A novel evaluation framework decomposes emotions into 10 dimensions using Plutchik\u2019\
  s emotion theory to assess language models beyond predefined emotion categories."
---

# Fluent but Unfeeling: The Emotional Blind Spots of Language Models

## Quick Facts
- **arXiv ID:** 2509.09593
- **Source URL:** https://arxiv.org/abs/2509.09593
- **Reference count:** 40
- **Primary result:** Fine-grained emotion recognition from self-disclosed Reddit posts remains challenging for LLMs, with accuracy improving with model size but degrading under CoT prompting.

## Executive Summary
This study introduces EXPRESS, a benchmark dataset for fine-grained emotion recognition that addresses limitations in existing benchmarks by using self-disclosed emotions from Reddit posts with an average length of 259 words. A novel evaluation framework decomposes emotions into 10 dimensions using Plutchik’s emotion theory to assess language models beyond predefined emotion categories. Systematic testing of 14 models reveals that predicting emotions aligning with human self-disclosures remains challenging, with accuracy metrics (AccL, AccV, F1V) generally low, though performance improves with model size and few-shot prompting. Notably, CoT prompting degraded performance in this subjective task. Qualitative analysis showed that model-generated emotions sometimes matched theoretical definitions better than self-disclosures, but often missed contextual cues. These findings highlight the complexity of emotion alignment and suggest avenues for improving emotionally aware AI systems.

## Method Summary
The study creates EXPRESS by extracting self-disclosed emotions from Reddit posts using regex patterns (e.g., "I feel [emotion]") and masking these emotion words for prediction. The dataset contains 33,679 posts from 6,930 subreddits, with emotions mapped to 10-dimensional vectors based on Plutchik’s theory. Researchers evaluate 14 models (4 MLMs, 3 Seq2Seq, 7 CLMs) across four prompting strategies: zero-shot, few-shot random, few-shot nearest (semantic similarity), and CoT. Performance is measured via Lexical Accuracy (AccL), Vector Accuracy (AccV), and Average Vector F1 Score (F1V), comparing predictions against ground truth emotion labels.

## Key Results
- Larger models consistently outperform smaller ones on emotion recognition, with GPT-4o and Llama-3.1-70B achieving highest accuracy
- Few-shot learning with semantically similar examples improves performance more than random examples
- Chain-of-Thought prompting degrades performance by causing models to rely on theoretical priors rather than specific contextual cues
- Models struggle with nuanced emotions, often predicting lower-intensity versions of ground truth emotions (e.g., "angry" instead of "furious")

## Why This Works (Mechanism)

### Mechanism 1: Semantic Generalization via Parameter Scaling
Increasing model parameter count generally improves the alignment of predicted emotions with human self-disclosures, contingent on the model family. Larger models appear to possess a more nuanced internal representation of emotion lexicons, allowing them to map complex contexts to specific emotion terms more accurately than smaller variants. The paper notes that performance improves "consistently as the number of parameters increases" within families (Flan-T5, Llama, Gemma). This mechanism fails when comparing across architectures (e.g., Masked LMs vs. Causal LMs), where smaller Masked models sometimes outperform larger Causal models.

### Mechanism 2: In-Context Learning via Nearest-Neighbor Priming
Providing few-shot examples selected via semantic similarity ("nearest") improves emotion recognition accuracy more than random examples. Semantically similar examples likely prime the model to attend to specific contextual features or emotional tones present in the target query, effectively narrowing the search space for the masked prediction. The paper shows "few-shot (nearest)" outperforms "few-shot (random)" by providing analogous affective context. This priming may confuse the model if the semantic neighbor is similar in topic but opposite in valence (e.g., a sad story about a dog vs. a happy story about a dog).

### Mechanism 3: Contextual Dilution via Chain-of-Thought (CoT)
Chain-of-Thought prompting degrades performance in this specific emotion recognition task by causing the model to rely on theoretical priors rather than specific contextual cues. CoT encourages the model to "think step by step," which may lead it to over-rely on general definitions of emotions rather than the idiosyncratic, often nuanced way humans self-disclose emotions in text. This finding is specific to subjective emotion alignment; CoT would likely perform better if the task required logical deduction of a cause rather than an affective label.

## Foundational Learning

- **Concept:** Plutchik’s Wheel of Emotions (8 Basic Emotions + Sentiments)
  - **Why needed here:** The paper decomposes 251 fine-grained emotion terms into 10-dimensional vectors (8 basic emotions + positive/negative sentiment) to calculate Vector Accuracy (AccV). Without this, one cannot understand how "furious" and "angry" are treated as similar (sharing the "anger" dimension) despite being lexically different.
  - **Quick check question:** If a model predicts "optimistic" but the ground truth is "hopeful," which basic emotion dimensions likely overlap, and would AccL capture this?

- **Concept:** Masked Language Modeling (MLM) vs. Causal Language Modeling (CLM)
  - **Why needed here:** The study finds that smaller MLMs (e.g., RoBERTa) outperform larger CLMs on this specific "fill-in-the-blank" task. Understanding the bidirectional attention mechanism of MLMs explains why they excel at integrating context from both sides of the masked emotion word.
  - **Quick check question:** Why might a bidirectional model (MLM) have an inherent advantage over a unidirectional model (CLM) when predicting a missing emotion word in the middle of a sentence?

- **Concept:** Ecological Validity in NLP Benchmarks
  - **Why needed here:** The EXPRESS dataset uses "self-disclosed" emotions rather than expert annotations. Understanding this helps interpret the "errors"—sometimes the model predicts a theoretically correct emotion that doesn't match the specific (and possibly idiosyncratic) word the user chose to describe their feeling.
  - **Quick check question:** If a user describes a traumatic event and labels it "tired" (ground truth) while the model predicts "sad" (theoretically sound), is this a model failure or a data ambiguity?

## Architecture Onboarding

- **Component map:** Reddit posts -> Regex masking -> 512-token segmentation -> 14 models with 4 prompting strategies -> NRC EmoLex mapping -> 10-D vector conversion -> AccL/AccV/F1V calculation

- **Critical path:**
  1. **Masking Integrity:** Ensuring the regex correctly identifies self-disclosure (avoiding phrases like "I feel he is sad") is the single point of failure for data quality.
  2. **Vector Mapping:** Converting predicted words to 10-D vectors via EmoLex (or AMT annotations for missing words) is the critical step that enables fine-grained evaluation beyond exact string matching.

- **Design tradeoffs:**
  - **Context Window:** Segmenting posts to 512 tokens ensures fair comparison across models (including older/smaller ones) but risks truncating context. The paper (Table 4) argues the tradeoff is minimal as performance didn't change significantly with full context.
  - **Ground Truth:** Using self-disclosed labels maximizes ecological validity but introduces noise (human labeling variability), whereas expert labels would be cleaner but less representative of natural language use.

- **Failure signatures:**
  - **Intensity Mismatch:** Model predicts "angry" when human wrote "furious" (AccL fails, AccV may pass).
  - **Prior Dominance:** CoT prompting causes the model to output valid reasoning but the wrong emotion due to over-reliance on theoretical definitions rather than user context.
  - **Format Collapse:** Smaller models (e.g., Gemma-2B) failed to follow output instructions under CoT prompting, producing invalid responses.

- **First 3 experiments:**
  1. **Run the Masking Check:** Execute the regex protocol on a sample of 100 posts to verify that "I feel [emotion]" is being captured without capturing third-party attributions (e.g., "I feel he is...").
  2. **Vector Similarity Baseline:** Take the top 3 models (GPT-4o, Llama-3.1-70B, RoBERTa) and run 50 samples through the Zero-shot pipeline to compare Lexical Accuracy (AccL) vs. Vector Accuracy (AccV). Verify that AccV is consistently higher, confirming the "semantic closeness" hypothesis.
  3. **CoT Degradation Test:** Run the "Nearest Few-Shot" setting against the "CoT" setting on a held-out set of 20 nuanced posts. Qualitatively analyze if the CoT responses are ignoring specific adjectives in the text that define the emotion.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Are prediction discrepancies caused by LLMs ignoring context or by utilizing "internal world modeling" that diverges from human contextual cues?
- **Basis in paper:** The Conclusion asks for investigation into whether LLMs are making "errors" (missing clues) or registering information but predicting based on "internal world modeling" specific to the LLM.
- **Why unresolved:** The study identifies misalignment but does not mechanistically determine if the context is being ignored or processed differently.
- **What evidence would resolve it:** Attention visualization or probing studies comparing token weights for context versus semantic priors in failed predictions.

### Open Question 2
- **Question:** How does emotion recognition performance vary when evaluating neurodivergent populations compared to the neurotypical expressions used in EXPRESS?
- **Basis in paper:** The Limitations section states the research focuses on neurotypical expressions and highlights the need for further research to understand emotional expressions in neurodivergent populations.
- **Why unresolved:** Current benchmarks, including EXPRESS, lack specific stratification or annotation for neurodivergent communication styles.
- **What evidence would resolve it:** Evaluation of current SOTA models on a benchmark constructed specifically from neurodivergent self-disclosures.

### Open Question 3
- **Question:** Can fine-tuning LLMs with expert rationales regarding contextual fit improve performance better than training on self-disclosed labels alone?
- **Basis in paper:** The Conclusion suggests future research explore how "expert observations might be used to fine-tune models for improved emotion recognition."
- **Why unresolved:** The qualitative analysis revealed experts sometimes prefer LLM outputs over self-disclosures, suggesting the ground truth labels may be suboptimal training targets for contextual awareness.
- **What evidence would resolve it:** A comparative study fine-tuning models on raw self-disclosures vs. a dataset validated/annotated by emotion experts for contextual consistency.

### Open Question 4
- **Question:** What specific mechanisms cause Chain-of-Thought prompting to degrade performance in subjective emotion recognition tasks?
- **Basis in paper:** The paper notes CoT consistently degraded performance and hypothesizes models may rely on priors rather than context, but does not prove the mechanism.
- **Why unresolved:** The experiments measured output drops but did not analyze the generated reasoning chains to identify where the logic diverged from human intuition.
- **What evidence would resolve it:** A qualitative error analysis of the reasoning steps generated by the model to identify systematic biases or suppression of contextual keywords.

## Limitations
- **Ecological validity tradeoff:** Using self-disclosed emotions as ground truth introduces noise from human variability in emotion labeling
- **Cultural limitation:** The benchmark's reliance on English-language Reddit posts limits generalizability across cultures
- **Context truncation:** Fixed 512-token segmentation may remove relevant context for emotion prediction, though impact appears minimal

## Confidence
- **High Confidence:** Larger models improve emotion recognition; CoT prompting degrades subjective task performance
- **Medium Confidence:** Specific accuracy metrics and their relative rankings across models; MLMs outperforming CLMs
- **Low Confidence:** Generalizability of nearest-neighbor few-shot improvement; precise mechanism of parameter scaling benefits

## Next Checks
1. **Cross-Cultural Validation:** Test EXPRESS on emotion self-disclosures from non-English language forums to assess cultural dependency of observed model performance patterns
2. **Context Window Sensitivity Analysis:** Systematically vary segmentation length (256, 512, 768 tokens) on a subset of posts to quantify truncation impact on emotion prediction accuracy
3. **Alternative Emotion Taxonomies:** Re-run evaluation using different emotion representation frameworks (e.g., James Russell's Circumplex Model) to verify performance pattern robustness