---
ver: rpa2
title: 'Whale: Large-Scale multilingual ASR model with w2v-BERT and E-Branchformer
  with large speech data'
arxiv_id: '2506.01439'
source_url: https://arxiv.org/abs/2506.01439
tags:
- speech
- data
- whale
- owsm
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Whale, a large-scale multilingual automatic
  speech recognition (ASR) model. Whale leverages a 1.87-billion-parameter architecture
  integrating w2v-BERT self-supervised pre-training, an encoder-decoder backbone built
  on E-Branchformer, and joint CTC-attention decoding.
---

# Whale: Large-Scale multilingual ASR model with w2v-BERT and E-Branchformer with large speech data

## Quick Facts
- arXiv ID: 2506.01439
- Source URL: https://arxiv.org/abs/2506.01439
- Authors: Yosuke Kashiwagi; Hayato Futami; Emiru Tsunoo; Satoshi Asakawa
- Reference count: 0
- Primary result: Achieves 2.4% WER on Librispeech test-clean and 3.4% CER on CSJ eval3 using 1.87B-parameter multilingual architecture

## Executive Summary
Whale introduces a large-scale multilingual ASR model that combines w2v-BERT self-supervised pre-training with an E-Branchformer encoder-decoder architecture. The model is trained on 250,000 hours of speech data spanning 144 languages using a curriculum learning approach across seven stages to ensure training stability. Experimental results demonstrate competitive performance against state-of-the-art models like Whisper and OWSM, particularly benefiting from targeted data collection for Japanese and the staged training methodology.

## Method Summary
Whale employs a 1.87-billion-parameter architecture integrating pre-trained w2v-BERT (24-layer, 0.58B parameters) with an E-Branchformer encoder (24 layers) and Transformer decoder (6 layers). The model uses joint CTC-attention decoding (0.3 CTC + 0.7 attention) and self-conditioned CTC at layers 8 and 16. Training follows a seven-stage curriculum learning approach: starting with 8-layer English-only models, progressively scaling to 24 layers, expanding to multilingual data, and finally fine-tuning the frozen w2v-BERT parameters. The model was trained on 128×H100 GPUs for approximately six weeks.

## Key Results
- Achieves 2.4% word error rate on Librispeech test-clean
- Achieves 3.4% character error rate on CSJ eval3
- Demonstrates competitive performance on Common Voice and FLEURS datasets compared to Whisper and OWSM

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Pre-trained w2v-BERT SSL features improve ASR accuracy when fine-tuned jointly with the downstream task.
- **Mechanism:** The 24-layer w2v-BERT extracts contextualized speech representations via masked prediction and contrastive learning, capturing both local acoustic patterns and long-range dependencies. These features are initially frozen, then updated in Stage 7 to align with ASR objectives.
- **Core assumption:** The SSL pre-training quality transfers effectively to multilingual ASR tasks.
- **Evidence anchors:**
  - [abstract] "Whale's architecture integrates w2v-BERT self-supervised model..."
  - [section 2.1] "By incorporating pre-trained w2v-BERT... we are not just using it as a feature extraction, but we are also enhancing its effectiveness by jointly finetuning it with the objective function of ASR."
  - [corpus] Related work (Benchmarking ASR Models for African Languages) confirms w2v-BERT is among effective pre-trained systems for low-resource ASR.
- **Break condition:** If SSL representations are domain-mismatched to target data (e.g., read speech vs. spontaneous), gains diminish.

### Mechanism 2
- **Claim:** E-Branchformer encoder with self-conditioned CTC enables multi-scale feature extraction and iterative refinement.
- **Mechanism:** Parallel branches capture fine-grained local patterns and broader context. Self-conditioned CTC at layers 8 and 16 generates intermediate predictions that condition subsequent layers, providing language-specific cues.
- **Core assumption:** Intermediate CTC predictions carry useful signal for later layers.
- **Evidence anchors:**
  - [abstract] "...an encoder-decoder backbone built on E-Branchformer..."
  - [section 2.2.2] "Self-conditioned CTC conditions the model's predictions on its previous outputs to iteratively refine the intermediate representations."
  - [corpus] OWSM v3.1 paper (cited neighbor) shows E-Branchformer improves over standard Transformer encoders.
- **Break condition:** For very large SSL front-ends, self-conditioning had negligible effect—likely due to limited representation flexibility upstream.

### Mechanism 3
- **Claim:** Curriculum learning stabilizes large-scale multilingual training.
- **Mechanism:** Seven-stage curriculum progressively scales model capacity (8→16→24 layers) and data diversity (English-only → multilingual → all data → SSL fine-tuning), preventing optimization instability.
- **Core assumption:** Gradual complexity increase prevents catastrophic divergence early in training.
- **Evidence anchors:**
  - [abstract] "Training employed a curriculum learning strategy across seven stages to enhance stability and performance."
  - [section 3.3] Details all seven stages, from 8-layer English model to final SSL update.
  - [corpus] Assumption: Related scaling work (OWSM v4, Speech Back-Translation) relies on similar staged or data-scaling strategies, though explicit curriculum comparisons are sparse.
- **Break condition:** If stage transitions are too aggressive or data quality varies sharply, instability may resurface.

## Foundational Learning

- **Concept: Self-supervised speech representations (e.g., w2v-BERT, HuBERT, WavLM)**
  - **Why needed here:** Whale's front-end depends on SSL-extracted features; understanding masking, contrastive objectives, and fine-tuning strategies is essential.
  - **Quick check question:** Can you explain why masked prediction encourages contextual representations in speech?

- **Concept: Joint CTC-Attention decoding**
  - **Why needed here:** Whale combines CTC prefix scores (weight 0.3) with attention decoder scores (0.7) during beam search.
  - **Quick check question:** What are the complementary strengths of CTC (alignment monotonicity) vs. attention (flexible dependency modeling)?

- **Concept: Curriculum learning for large models**
  - **Why needed here:** 7-stage training is central to Whale's stability; understanding when and how to unfreeze parameters matters.
  - **Quick check question:** Why might freezing SSL parameters early and unfreezing later improve final performance?

## Architecture Onboarding

- **Component map:** 16kHz resampling → w2v-BERT (24 Conformer layers, 0.58B params) → E-Branchformer encoder (24 layers, convolution subsampling 100→50 fps) with self-conditioned CTC at layers 8,16 → parallel branches: (a) Transformer decoder (6 layers), (b) CTC branch → joint decoding (0.3 CTC + 0.7 attention) → transcription

- **Critical path:** SSL feature quality (w2v-BERT output fidelity) → Encoder representation (E-Branchformer multi-scale fusion) → Decoder-CTC synergy during beam search

- **Design tradeoffs:**
  - Larger SSL front-end (0.58B params) improves features but limits self-conditioned CTC effectiveness (observed in FLEURS zero-shot adaptation failure).
  - Joint decoding improves accuracy but adds inference complexity (beam search with two score sources).
  - Curriculum learning stabilizes training at the cost of longer total training time (~6 weeks, 16 nodes × 8 H100s).

- **Failure signatures:**
  - High WER on noisy/out-of-domain data (e.g., FLEURS vs. CommonVoice gap) → likely domain mismatch; in-house Japanese data was read speech, not spontaneous.
  - Self-conditioned CTC shows no improvement → SSL encoder may dominate, leaving little room for intermediate refinement.
  - Training instability → curriculum stage transitions may need smoothing or learning rate re-warming.

- **First 3 experiments:**
  1. **Ablate SSL fine-tuning:** Compare Stage 6 (frozen SSL) vs. Stage 7 (unfrozen) on Librispeech test-clean/other to quantify SSL update contribution.
  2. **Self-conditioned CTC layers:** Disable self-conditioning at layers 8 and 16; evaluate impact on CSJ and FLEURS to confirm or refute observed negligible effects.
  3. **Domain robustness test:** Evaluate on FLEURS vs. CommonVoice for high-resource languages (en, ja, zh) to measure domain gap; if large, investigate data augmentation or spontaneous speech augmentation.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Why does the self-conditioned CTC mechanism fail to enable zero-shot language adaptation in the Whale architecture, and is this failure intrinsic to large-scale SSL-based models?
- **Basis in paper:** [explicit] The authors report that their adaptation method "did not have any effect on most languages" and hypothesize that the "huge SSL network exist before the encoder" leaves "no room for adaptation."
- **Why unresolved:** The paper identifies the failure and offers a hypothesis regarding the size of the w2v-BERT frontend, but does not validate if reducing the SSL size or altering the connection point would restore the adaptation capability.
- **What evidence would resolve it:** Ablation studies comparing adaptation performance with frozen versus unfrozen SSL layers, or using a smaller SSL model, to determine if the front-end capacity is indeed the blocking factor.

### Open Question 2
- **Question:** Which specific domain adaptation strategies or data augmentation techniques can effectively mitigate the performance degradation on diverse, noisy datasets like FLEURS?
- **Basis in paper:** [explicit] The authors note that Whale underperforms on FLEURS compared to Whisper, even for Japanese (where Whale has ample data), attributing this to "robustness against the data out of domain" because in-house data is mostly "read speech." The conclusion explicitly lists investigating "data augmentation techniques and domain adaptation strategies" as future work.
- **Why unresolved:** While the authors identify the domain mismatch (read speech vs. noisy/diverse audio), they have not yet tested interventions to close this generalization gap.
- **What evidence would resolve it:** Experiments re-training or fine-tuning Whale with added noise, reverberation, or diverse speaking-style data, followed by re-evaluation on FLEURS.

### Open Question 3
- **Question:** Can the Whale model be effectively compressed or optimized for inference speed without significantly degrading its multilingual recognition capabilities?
- **Basis in paper:** [explicit] The conclusion states that "Future work will explore further optimizations in model compression and inference speed."
- **Why unresolved:** The current model is very large (1.87B parameters) and training required substantial compute (16 nodes of H100s). The feasibility of deploying such a model in resource-constrained environments remains unaddressed.
- **What evidence would resolve it:** Results from applying techniques such as knowledge distillation, quantization, or pruning to the Whale model, reporting the trade-off between WER/CER and inference latency/throughput.

## Limitations

- **Reproducibility constraints:** Critical implementation details including optimizer configuration, learning rate schedules, batch sizes, and gradient clipping parameters for each curriculum stage are not specified.
- **Domain generalization gaps:** The model exhibits notably weaker results on spontaneous conversational speech (FLEURS zero-shot adaptation) compared to read speech benchmarks.
- **Resource intensity:** Training required 128×H100 GPUs for approximately six weeks, creating significant barriers to independent verification.

## Confidence

- **High confidence:** The curriculum learning approach improves training stability for large-scale multilingual models.
- **Medium confidence:** The E-Branchformer architecture with self-conditioned CTC provides multi-scale feature extraction benefits.
- **Low confidence:** The claim that w2v-BERT fine-tuning in Stage 7 provides substantial performance gains.

## Next Checks

1. **SSL fine-tuning impact isolation:** Conduct an ablation study comparing Stage 6 (frozen w2v-BERT) versus Stage 7 (unfrozen w2v-BERT) performance across multiple benchmarks to quantify the contribution of SSL parameter updating.

2. **Self-conditioned CTC verification:** Disable self-conditioned CTC at layers 8 and 16 while keeping all other components constant, then evaluate on CSJ and FLEURS datasets to confirm whether the paper's observation of negligible improvement holds across different speech domains.

3. **Domain robustness assessment:** Perform controlled experiments comparing model performance on FLEURS versus CommonVoice for high-resource languages to measure domain adaptation gaps, then investigate whether data augmentation with spontaneous speech or domain adversarial training can reduce this performance disparity.