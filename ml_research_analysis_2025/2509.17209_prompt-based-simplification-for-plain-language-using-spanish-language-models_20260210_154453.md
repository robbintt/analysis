---
ver: rpa2
title: Prompt-Based Simplification for Plain Language using Spanish Language Models
arxiv_id: '2509.17209'
source_url: https://arxiv.org/abs/2509.17209
tags:
- prompt
- training
- readability
- similarity
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper describes the participation of HULAT-UC3M in CLEARS
  2025 Subtask 1: Adaptation of Text to Plain Language (PL) in Spanish. The team explored
  strategies based on Spanish-trained models, using zero-shot prompt engineering and
  fine-tuning with Low-Rank Adaptation (LoRA).'
---

# Prompt-Based Simplification for Plain Language using Spanish Language Models

## Quick Facts
- arXiv ID: 2509.17209
- Source URL: https://arxiv.org/abs/2509.17209
- Reference count: 22
- Primary result: Ranked 1st in semantic similarity (SIM=0.75) and 4th in readability (FH=69.72) at CLEARS 2025

## Executive Summary
This paper describes HULAT-UC3M's participation in CLEARS 2025 Subtask 1 for Plain Language adaptation in Spanish. The team explored strategies using Spanish-trained models, focusing on zero-shot prompt engineering and LoRA fine-tuning. Their final system combined text normalization, RigoChat-7B-v2 model, and a PL-oriented unified prompt strategy. The approach achieved first place in semantic similarity while ranking fourth in readability, demonstrating the effectiveness of Spanish-centric models and carefully designed prompts for this task.

## Method Summary
The system used RigoChat-7B-v2 in zero-shot mode with a unified prompt strategy (P2) that explicitly instructed the model to retain key vocabulary while simplifying structure. Text preprocessing included normalization for dates, monetary amounts, and numeric elements using custom regex patterns. The approach prioritized semantic preservation over aggressive readability improvements, leading to the selection of zero-shot inference over LoRA fine-tuning due to inconsistencies in the training data. The system was evaluated using cosine similarity for semantic preservation and Fern치ndez-Huerta index for readability.

## Key Results
- Ranked 1st in Semantic Similarity (SIM=0.75) at CLEARS 2025
- Ranked 4th in Readability (FH=69.72) at CLEARS 2025
- Spanish-centric instruction-tuned models outperformed general-purpose multilingual models
- Zero-shot inference with robust instruction-tuned model outperformed LoRA fine-tuning on noisy data

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Spanish-centric instruction-tuned models appear to outperform general-purpose multilingual models for Plain Language adaptation tasks requiring nuanced linguistic restructuring.
- **Mechanism:** RigoChat-7B-v2, trained specifically on Spanish texts and instruction-tuning tasks, exhibits higher adherence to complex rewriting rules compared to models like Mistral or Gemma.
- **Core assumption:** The model's pre-training distribution aligns closely with the syntactic and lexical variations found in Spanish municipal communications.
- **Evidence anchors:** Internal evaluations showed RigoChat consistently outperformed other models in both Semantic Similarity and Readability across all prompt strategies.

### Mechanism 2
- **Claim:** A unified prompt strategy (P2) that explicitly instructs the model to retain key vocabulary while simplifying structure provides a more stable equilibrium between semantic preservation and readability than multi-step or classifier-based pipelines.
- **Mechanism:** Strategy P2 integrates reduction and rewriting instructions into a single prompt, avoiding error propagation risks of multi-step approaches and classification errors from category-based methods.
- **Core assumption:** The LLM can simultaneously process constraints for content retention and structural simplification without requiring decoupled processing stages.
- **Evidence anchors:** The paper reports P2 "reached a stable equilibrium" while P3 showed higher variability and reduced semantic similarity after fine-tuning.

### Mechanism 3
- **Claim:** Zero-shot inference with a robust instruction-tuned model can outperform LoRA fine-tuning when the training data is noisy or inconsistent.
- **Mechanism:** Fine-tuning forces the model to adapt to the specific "noise" in the training set, while zero-shot relies on the model's pre-existing generalized capabilities, avoiding overfitting to inconsistent adaptation styles.
- **Core assumption:** The pre-trained model possesses sufficient baseline capability to follow PL rules without task-specific weight updates.
- **Evidence anchors:** The authors note that conflicting examples in the training data limited the model's ability to learn consistent simplification patterns, leading to the selection of zero-shot over fine-tuning.

## Foundational Learning

- **Concept: Plain Language (PL) vs. Easy-to-Read (ER)**
  - **Why needed here:** The paper distinguishes PL (ISO 24495-1, broad audience) from ER (UNE 153101, strict guidelines for intellectual disabilities). The system targets PL, explaining why it prioritized semantic preservation over aggressive simplification.
  - **Quick check question:** Does the target output require validation by users with intellectual disabilities (ER), or simply clear communication for the general public (PL)?

- **Concept: Semantic Similarity vs. Readability Trade-off**
  - **Why needed here:** The core engineering challenge was balancing the cosine similarity score against the Fern치ndez-Huerta readability index. Improving one often degrades the other.
  - **Quick check question:** If you simplify a complex sentence to improve its readability score by 10 points, what typically happens to the cosine similarity score relative to the original reference?

- **Concept: Low-Rank Adaptation (LoRA)**
  - **Why needed here:** The paper utilized LoRA to fine-tune the LLM efficiently on limited hardware. Understanding this helps explain why they could experiment with fine-tuning but ultimately rejected it due to data quality issues.
  - **Quick check question:** Why would a team choose LoRA over full fine-tuning when working with a 7B parameter model on a single consumer-grade GPU?

## Architecture Onboarding

- **Component map:** Raw Spanish municipal text -> Text Normalization (Regex filters) -> RigoChat-7B-v2 + Prompt P2 (Unified instructions) -> Output Cleaning (Error correction) -> Evaluation (Cosine Similarity + Fern치ndez-Huerta)

- **Critical path:** The Prompt P2 design is the central control lever, dictating the balance between content retention (critical for the 1st place SIM score) and simplification. The specific instruction to "conserve key vocabulary" is the primary semantic anchor.

- **Design tradeoffs:** The system accepted a lower rank in Readability (4th place, FH=69.72) to secure 1st place in Semantic Similarity (SIM=0.75). Zero-shot was chosen over fine-tuning to avoid overfitting to inconsistent training data.

- **Failure signatures:** Over-simplification reducing semantic similarity, hallucination adding non-existent content, format loss from reversed preprocessing normalization.

- **First 3 experiments:**
  1. Baseline Zero-Shot: Run RigoChat-7B-v2 with Prompt P2 on a subset of 50 random texts to establish the FH/SIM baseline without preprocessing.
  2. Prompt Ablation: Compare P2 (Unified) vs. P3 (Category-based) on the "Smallest" subset to quantify the stability penalty introduced by the classifier in P3.
  3. Normalization Impact: A/B test preprocessing logic to verify if manual rules improve FH scores without affecting SIM.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can evaluation frameworks effectively combine automatic metrics with user-centered validation to better capture linguistic clarity and factual consistency in Plain Language adaptation?
- **Basis in paper:** The authors state that current metrics "capture only surface-level features and may penalize meaningful reformulation," suggesting future frameworks should combine metrics with user-centered approaches.
- **Why unresolved:** Current metrics (Cosine Similarity, Fern치ndez-Huerta) rely on surface-level n-gram overlap or syllable counts, failing to reflect actual comprehension or structural improvements required by standards like ISO 24495-1.
- **What evidence would resolve it:** A correlation study showing that high scores on a new hybrid metric correspond to high comprehension scores in human evaluation studies involving the target audience.

### Open Question 2
- **Question:** To what extent does fine-tuning on datasets explicitly aligned with ISO 24495-1:2023 improve model performance compared to training on heterogeneous datasets lacking clear annotation guidelines?
- **Basis in paper:** The paper notes that the lack of a clear annotation methodology in the training data resulted in inconsistencies (content omission/addition) that hindered model generalization.
- **Why unresolved:** The authors could not determine if their model's errors were due to architectural limits or the "noisy" nature of the training data which diverged from normative PL principles.
- **What evidence would resolve it:** A comparative experiment where the same model architecture is trained on the current dataset versus a newly curated, ISO-compliant dataset, measuring the reduction in content hallucination and omission errors.

### Open Question 3
- **Question:** Does actively involving end-users (specifically people with reading difficulties) in the prompt engineering loop result in Plain Language adaptations that achieve higher functional accessibility than automatic metric-optimized prompts?
- **Basis in paper:** While the authors prioritize semantic similarity (SIM) to win the task, they acknowledge that ISO standards require validation by people with intellectual disabilities, implying a gap between automatic metric performance and real-world utility.
- **Why unresolved:** The system prioritized semantic preservation to maximize the contest's similarity metric, potentially at the cost of the deep structural reformulation needed for actual accessibility.
- **What evidence would resolve it:** An A/B test comparing outputs from metric-optimized prompts against outputs refined via human-in-the-loop feedback, assessed by a panel of users with cognitive impairments.

## Limitations

- The model selection heavily depends on the specific Spanish municipal context, and performance may degrade on domain-specific jargon not well-represented in the training data.
- Evaluation metrics (cosine similarity and FH index) capture only surface-level readability and semantic preservation, potentially missing deeper quality issues like factual accuracy or coherence.
- The unified prompt strategy (P2) shows promise but lacks detailed specification of the exact prompt template, making precise reproduction challenging.

## Confidence

- **High confidence:** Spanish-centric instruction-tuned models outperform general-purpose multilingual models for Spanish PL adaptation. Selection of zero-shot over fine-tuning based on data quality concerns.
- **Medium confidence:** Unified prompt strategy P2 provides better stability than multi-step pipelines. Zero-shot outperforms fine-tuning when training data is noisy.
- **Low confidence:** Whether achieved readability improvements translate to actual comprehension gains for target users. Exploration of potential demographic biases in the system's performance.

## Next Checks

1. **Domain Transfer Test:** Evaluate the system on Spanish texts from domains outside municipal communications (e.g., medical, legal, or technical documents) to assess generalization beyond the training distribution.

2. **User Comprehension Validation:** Conduct human evaluation studies with the target audience to verify that improved readability scores (FH) correlate with actual comprehension and information retention, not just surface-level simplification.

3. **Bias and Fairness Audit:** Analyze the system's performance across different demographic groups, text topics, and complexity levels to identify potential biases in simplification quality or content preservation.