---
ver: rpa2
title: 'LLM-Hanabi: Evaluating Multi-Agent Gameplays with Theory-of-Mind and Rationale
  Inference in Imperfect Information Collaboration Game'
arxiv_id: '2510.04980'
source_url: https://arxiv.org/abs/2510.04980
tags:
- game
- hint
- card
- player
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LLM-Hanabi, a benchmark for evaluating multi-agent
  collaboration and Theory-of-Mind (ToM) in Large Language Models using the cooperative
  card game Hanabi. The framework uses automated evaluation to measure both game performance
  and ToM proficiency, capturing rationale inference through structured reasoning
  extraction and post-game scoring.
---

# LLM-Hanabi: Evaluating Multi-Agent Gameplays with Theory-of-Mind and Rationale Inference in Imperfect Information Collaboration Game

## Quick Facts
- arXiv ID: 2510.04980
- Source URL: https://arxiv.org/abs/2510.04980
- Authors: Fangzhou Liang; Tianshi Zheng; Chunkit Chan; Yauwai Yim; Yangqiu Song
- Reference count: 20
- Primary result: Deepseek-R1 achieved highest average game score (30.00) and ToM score (77.68) in 5-player Hanabi benchmark

## Executive Summary
This paper introduces LLM-Hanabi, a benchmark for evaluating multi-agent collaboration and Theory-of-Mind (ToM) in Large Language Models using the cooperative card game Hanabi. The framework uses automated evaluation to measure both game performance and ToM proficiency, capturing rationale inference through structured reasoning extraction and post-game scoring. Experiments with 14 models, including LLMs and Large Reasoning Models (LRMs), show that Deepseek-R1 and GPT-4.1 achieve the highest average game scores (30.00 and 28.56, respectively) and ToM scores (77.68 and 78.14). LRMs outperform LLMs overall, and first-order ToM (interpreting intent) shows stronger correlation with success (r=0.76) than second-order ToM (predicting others' interpretations, r=0.58).

## Method Summary
The LLM-Hanabi benchmark translates Hanabi game states into natural language prompts for LLM agents, requiring them to output structured JSON with action_type, rationale, 1st_order_ToM, and 2nd_order_ToM fields during hint actions. An LLM-as-a-judge evaluates alignment between extracted statements post-game to produce ToM scores on a 0-10 scale. The framework measures correlation between ToM proficiency and game performance across 14 models, comparing LLMs and LRMs in 5-player games with standard rules (50 cards, 8 information tokens, 3 life tokens).

## Key Results
- Deepseek-R1 achieved highest average game score (30.00) and ToM score (77.68)
- LRMs outperformed LLMs overall in both game scores and ToM proficiency
- First-order ToM correlates more strongly with performance (r=0.76) than second-order ToM (r=0.58)
- GPT-4.1 achieved second-highest scores: game (28.56) and ToM (78.14)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: First-order Theory-of-Mind (ToM) capabilities more strongly predict collaborative success than second-order ToM.
- Mechanism: Agents that accurately interpret a partner's intent (first-order ToM) can align their actions with teammate goals. Second-order ToM—predicting how others will interpret your actions—adds complexity without proportional performance gains in this cooperative setting.
- Core assumption: Hanabi's hint-based communication structure generalizes to other collaborative scenarios requiring rationale inference.
- Evidence anchors:
  - [abstract] "first-order ToM (interpreting others' intent) correlates more strongly with performance (r=0.76) than second-order ToM (r=0.58)"
  - [section 4.3] "first-order ToM is more significantly correlated with game success (r=0.76) than second-order ToM (r=0.58)"
  - [corpus] Weak direct validation; neighbor papers discuss ToM evaluation frameworks broadly but do not confirm this first-vs-second-order differential.
- Break condition: Tasks requiring deliberate deception or recursive belief modeling may show different ordering of ToM importance.

### Mechanism 2
- Claim: Structured reasoning extraction combined with LLM-as-judge scoring provides scalable ToM assessment.
- Mechanism: During hint actions, agents generate three structured statements—Rationale, First-Order ToM, and Second-Order ToM. A judge LLM scores alignment between these statements post-game, quantifying inferential accuracy on a 0-10 scale.
- Core assumption: The judge LLM's scoring aligns with human judgments of ToM accuracy.
- Evidence anchors:
  - [abstract] "capturing rationale inference through structured reasoning extraction and post-game scoring"
  - [section 3.3] "an LLM-as-a-judge evaluates the extracted statements to produce ToM scores on a 0-10 scale"
  - [corpus] No direct validation in neighbor papers; LLM-as-judge reliability for ToM scoring remains underexplored.
- Break condition: Judge model bias or low-capacity judge models may systematically mis-score nuanced reasoning.

### Mechanism 3
- Claim: Large Reasoning Models (LRMs) outperform standard LLMs in collaborative ToM tasks.
- Mechanism: LRMs trained with extended chain-of-thought and reinforcement learning exhibit stronger logical inference, which transfers to rationale inference in collaborative settings.
- Core assumption: The performance gap stems from architectural/training differences rather than parameter scale alone.
- Evidence anchors:
  - [abstract] "LRMs outperform LLMs overall"
  - [section 4.3] "LRMs demonstrated markedly superior gameplay, achieving higher average scores than most LLMs"
  - [corpus] Weak external validation; neighbor papers on LLM ToM do not specifically compare LRM vs. LLM performance differentials.
- Break condition: Tasks with minimal reasoning demands may not show LRM advantages; cost-efficiency tradeoffs may favor smaller LLMs.

## Foundational Learning

- Concept: Theory-of-Mind (ToM) in AI agents
  - Why needed here: The entire benchmark evaluates ToM as the cognitive capacity to attribute mental states (beliefs, intentions) to other agents.
  - Quick check question: Can you distinguish first-order ToM (inferring another's belief) from second-order ToM (inferring what another believes about your beliefs)?

- Concept: Imperfect information games
  - Why needed here: Hanabi's core constraint—players cannot see their own cards—creates the uncertainty that necessitates rationale inference.
  - Quick check question: In a 5-player Hanabi game, what information can a player directly observe vs. must infer?

- Concept: LLM-as-judge evaluation
  - Why needed here: The framework relies on a judge LLM to score ToM alignment, introducing potential bias and subjectivity.
  - Quick check question: What are two failure modes when using an LLM to evaluate another LLM's reasoning quality?

## Architecture Onboarding

- Component map: Game state → Agent prompt → Agent decision + ToM statements → Action execution → Hint event triggers extraction → Post-game scoring → Correlation analysis

- Critical path: Game state → Agent prompt → Agent decision + ToM statements → Action execution → Hint event triggers extraction → Post-game scoring → Correlation analysis

- Design tradeoffs:
  - 5-player configuration maximizes interaction complexity but increases game length and token costs
  - LLM-as-judge enables scalable evaluation but introduces judge model dependency and potential bias
  - Structured JSON output enforces consistency but may constrain natural reasoning expression

- Failure signatures:
  - Low first-order ToM scores with high game scores: Agents may succeed through non-ToM strategies (e.g., conservative play)
  - High ToM scores with low game scores: Agents reason correctly but fail action selection
  - Judge model score inflation/deflation: Systematic bias in ToM scoring across all agents

- First 3 experiments:
  1. Replicate the 5-player setup with a single model (e.g., GPT-4.1) playing all positions to isolate ToM reasoning from model heterogeneity
  2. Ablate the structured ToM extraction prompts to test whether forcing explicit ToM statements improves performance or merely enables measurement
  3. Compare judge models (e.g., GPT-4.1 vs. Claude vs. human evaluators on a sample) to quantify LLM-as-judge reliability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can targeted training on first-order ToM tasks yield greater improvements in multi-agent collaboration performance than training focused on higher-order ToM reasoning?
- Basis in paper: [explicit] The authors conclude that "prioritizing first-order ToM is a promising direction for enhancing the collaborative capabilities of future models" and invite future studies to "develop and evaluate methodologies for enhancing these specific inferential skills."
- Why unresolved: The correlation data (r=0.76 for 1st-order vs. r=0.58 for 2nd-order) is observational; no intervention experiments were conducted to test whether prioritizing first-order ToM training causally improves outcomes.
- What evidence would resolve it: Training separate model groups on first-order vs. second-order ToM-augmented datasets and comparing their resulting game scores in LLM-Hanabi or similar benchmarks.

### Open Question 2
- Question: To what extent do LLM-Hanabi findings generalize to other imperfect-information collaborative environments with different communication constraints?
- Basis in paper: [explicit] In Limitations, the authors state "the benchmark is restricted to the Hanabi game environment, which, though ideal for controlled ToM evaluation, may not capture the full diversity of collaborative scenarios encountered in real-world multi-agent systems."
- Why unresolved: Hanabi features sparse, structured hints and purely cooperative dynamics; it remains untested whether the first-order ToM advantage holds in settings with richer communication channels, deception, or mixed cooperative-competitive incentives.
- What evidence would resolve it: Replicating the evaluation framework across diverse environments (e.g., Overcooked, cooperative Minecraft tasks, or negotiation simulators) and comparing correlation patterns between ToM orders and success.

### Open Question 3
- Question: How well does LLM-as-a-judge scoring of ToM statements align with human expert assessments of reasoning quality?
- Basis in paper: [inferred] The Limitations section acknowledges that "the reliance on an LLM-as-a-judge for ToM scoring introduces potential bias and subjectivity, as the assessment quality depends on the judge model's own reasoning abilities," and calls for "human-in-the-loop assessments."
- Why unresolved: No validation was provided comparing automated ToM scores against human ratings; systematic bias (e.g., favoring verbose outputs or certain reasoning styles) could distort conclusions about model capabilities.
- What evidence would resolve it: A human annotation study where experts independently score a sample of extracted ToM statements, followed by correlation analysis between human and LLM-as-a-judge scores.

### Open Question 4
- Question: Do specific architectural features of Large Reasoning Models (e.g., extended chain-of-thought, reinforcement learning from rule-based rewards) causally drive their superior ToM and collaboration performance?
- Basis in paper: [inferred] The paper reports that "LRMs demonstrated markedly superior gameplay" and concludes there is "value of specialized reasoning architectures," but does not isolate which components (training data, scale, reasoning length, reward signals) produce this advantage.
- Why unresolved: LRMs and LLMs differ across multiple dimensions simultaneously; the contribution of any single factor remains confounded.
- What evidence would resolve it: Ablation studies comparing models with controlled variations (e.g., same base model with/without long-CoT fine-tuning or RL rewards) on LLM-Hanabi metrics.

## Limitations
- The LLM-as-judge evaluation mechanism lacks external validation against human judgment, raising questions about the reliability of ToM score measurements.
- Neighbor papers show no citations for this work, suggesting limited peer validation of the methodological approach and findings.
- Hanabi's cooperative, hint-based communication may not generalize to other collaborative scenarios with different communication constraints or mixed incentives.

## Confidence
- High confidence: The observed correlation between first-order ToM and game performance (r=0.76) is well-supported by the experimental data and aligns with game mechanics where accurate intent interpretation drives success.
- Medium confidence: The claim that LRMs outperform LLMs in collaborative ToM tasks, while supported by the results, lacks external validation from the broader literature on LRM vs. LLM performance differentials.
- Low confidence: The mechanism by which structured reasoning extraction enables scalable ToM assessment is not validated, as neighbor papers do not address LLM-as-judge reliability for ToM scoring specifically.

## Next Checks
1. Conduct a human evaluation study comparing LLM-as-judge ToM scores against human ratings on a subset of games to validate scoring reliability and calibrate the 0-10 scale.
2. Test the framework with additional cooperative games (e.g., Codenames, Bridge) to assess whether first-order ToM superiority generalizes beyond Hanabi's hint-based communication structure.
3. Perform an ablation study removing the structured ToM extraction requirements to determine whether forcing explicit rationale statements improves actual reasoning or merely enables measurement.