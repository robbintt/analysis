---
ver: rpa2
title: Compositional Instruction Following with Language Models and Reinforcement
  Learning
arxiv_id: '2501.12539'
source_url: https://arxiv.org/abs/2501.12539
tags:
- tasks
- language
- learning
- agent
- symbol
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of learning to follow compositional
  language instructions in reinforcement learning settings. The key idea is to combine
  compositional value function representations with large language models to enable
  agents to solve complex, language-specified tasks.
---

# Compositional Instruction Following with Language Models and Reinforcement Learning

## Quick Facts
- **arXiv ID:** 2501.12539
- **Source URL:** https://arxiv.org/abs/2501.12539
- **Reference count:** 8
- **Primary result:** Combines compositional value functions with LLMs to achieve 92% success on 162 compositional BabyAI tasks after 600k steps, vs 80% baseline after 21M steps

## Executive Summary
This paper addresses the challenge of learning to follow compositional language instructions in reinforcement learning settings. The key idea is to combine compositional value function representations with large language models to enable agents to solve complex, language-specified tasks. The method, CERLLA, uses pretraining to learn a set of compositional value functions for picking up objects with different attributes, which can be composed using Boolean operators to represent complex tasks. A large language model then maps natural language instructions to these Boolean expressions, leveraging in-context learning and environment feedback to improve its semantic parsing capabilities.

## Method Summary
The CERLLA method consists of two main phases: pretraining a set of compositional World Value Functions (WVFs) that can be combined using Boolean algebra, and using a large language model with in-context learning to map natural language instructions to these compositional expressions. The WVFs are trained to pick up objects with different attributes (colors and shapes), and complex tasks are solved by composing these WVFs through AND, OR, and NOT operations. The LLM generates candidate Boolean expressions which are validated through environment rollouts, with successful parses added to the in-context example pool for future tasks.

## Key Results
- CERLLA achieves 92% success rate on held-out compositional tasks after 600k environment steps
- Outperforms non-compositional baseline (80% success) by a large margin despite requiring only 3% of the training steps (600k vs 21M)
- Successfully generalizes to novel task compositions not seen during pretraining
- GPT-4 performs significantly better than GPT-3.5 for the semantic parsing component

## Why This Works (Mechanism)

### Mechanism 1: Compositional Value Function Algebra
Composing pretrained World Value Functions (WVFs) through Boolean operators creates policies for novel tasks without additional learning. A "task basis" of n WVFs is trained on simple attribute tasks (e.g., pick up red, pick up ball). For a complex task (e.g., "pick up a red ball"), their Q-values are combined algebraically (min for AND, max for OR, custom formula for NOT) rather than training a new policy from scratch.

### Mechanism 2: In-Context Learning with Environment Rollout Feedback
An LLM learns to map natural language to executable Boolean expressions by receiving binary success/failure feedback from environment rollouts, without gradient updates to the LLM. The LLM generates multiple candidate Boolean expressions, each executed as a policy. If success rate exceeds a threshold (e.g., 92%), the expression is added to the in-context example pool.

### Mechanism 3: Decoupling Policy Pretraining from Language Grounding
Decoupling the learning of low-level policies (WVFs) from the learning of language-to-logic mapping (semantic parsing) improves sample efficiency for multi-task learning. By pretraining a universal set of WVFs once (19M steps), the system amortizes the cost of learning control policies, confining the online learning phase to the cheaper problem of finding the correct composition for a given language command (600k steps).

## Foundational Learning

- **Concept: World Value Functions (WVFs)**
  - **Why needed here:** This is the core RL representation. Unlike standard Q-functions that map (state, action) to value, WVFs map (state, goal, action) to value, enabling the same function to evaluate different goals and thus be composed.
  - **Quick check question:** Can you explain the difference between a standard Q-function and a WVF, and why the latter enables logical composition?

- **Concept: In-Context Learning / Few-Shot Prompting**
  - **Why needed here:** The CERLLA agent does not update LLM weights. It learns by accumulating successful (language, expression) pairs in its prompt, relying on the LLM's pre-existing ability to generalize from examples.
  - **Quick check question:** How does changing the examples in an LLM's prompt change its output, without any weight updates?

- **Concept: Semantic Parsing**
  - **Why needed here:** The system's primary challenge is translating a natural language command ("pick up the red key") into a formal, executable representation (e.g., `Symbol_0 & Symbol_7`).
  - **Quick check question:** What is the task of a semantic parser, and why is it necessary for connecting language to a formal system like Boolean logic?

## Architecture Onboarding

- **Component map:**
  Environment (BabyAI) -> WVF Module (Pre-trained) -> LLM Semantic Parser (GPT-4) -> Composition Engine -> Rollout & Validation Loop

- **Critical path:**
  1. Pre-train the 9 basis WVFs to convergence (one-time cost, ~19M steps)
  2. Initialize the LLM with the system prompt and an empty or minimal example pool
  3. For each new task instruction:
      a. Retrieve k relevant examples from the pool using BM25
      b. Prompt the LLM to generate n candidate Boolean expressions
      c. For each candidate: Compose WVFs -> Run rollouts -> Check success rate
      d. If a candidate succeeds, add (instruction, expression) to the example pool
      e. (During inference) Select the first valid or highest-confidence expression

- **Design tradeoffs:**
  - **Fixed Basis vs. Learnable Basis:** The paper uses a fixed set of 9 WVFs, limiting the system to tasks expressible by these 9 attributes
  - **LLM Choice (GPT-4 vs GPT-3.5):** GPT-4 shows far better compositional generalization and faster learning than GPT-3.5
  - **Rollout Cost:** Each candidate validation requires 100 environment rollouts, providing grounded signal but being computationally expensive

- **Failure signatures:**
  - **Wrong object picked up:** Likely a semantic parsing error or flaw in the pretrained WVF for a specific attribute
  - **High variance/low success rate:** Check if the in-context example pool is noisy or if BM25 retrieval is fetching irrelevant examples
  - **Cannot solve a specific type of task (e.g., all negation tasks):** The basis WVFs may be insufficient, or the composition operator for negation may be flawed

- **First 3 experiments:**
  1. **WVF Validation:** Independently test each of the 9 pretrained WVFs. Run 100 episodes on its corresponding base task (e.g., "pick up red"). Verify high success rates (>95%) to isolate the RL component.
  2. **Composition Unit Test:** Manually provide ground-truth Boolean expressions for simple compositional tasks (e.g., "pick up a red ball" -> `red & ball`). Execute the composed policies and verify high success rates. This validates the composition mechanism.
  3. **Parser Bootstrapping Test:** Start with an empty example pool. Present a simple task. Monitor if the LLM can find a working expression via exploration and correctly add it to the pool. Present the same task again and verify it solves it immediately using the new example.

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on a fixed task basis of 9 compositional primitives restricts the method to environments where complex tasks can be expressed as Boolean combinations of specific attributes
- High computational cost of validating each candidate expression (100 environment rollouts) creates a significant barrier to scaling
- Cannot handle tasks requiring temporal reasoning, counting, or other non-Boolean compositional structures

## Confidence
- **High Confidence:** The compositional value function algebra mechanism is well-supported by theoretical framework and empirical results
- **Medium Confidence:** The in-context learning mechanism works as described for GPT-4 but shows inconsistent performance with GPT-3.5
- **Medium Confidence:** The decoupling benefit is demonstrated through the pretraining approach, though amortization gains depend on task distribution

## Next Checks
1. **Robustness to Noise:** Test the system's performance when adding 10-20% of incorrect examples to the in-context pool to measure its resilience to semantic parsing errors
2. **Basis Sufficiency:** Systematically evaluate whether all 162 tasks can be expressed using the 9-attribute basis by attempting to manually ground-truth parse each instruction
3. **Scaling Analysis:** Measure the relationship between the number of base WVFs and the number of environment rollouts required per task, to quantify the computational scaling properties