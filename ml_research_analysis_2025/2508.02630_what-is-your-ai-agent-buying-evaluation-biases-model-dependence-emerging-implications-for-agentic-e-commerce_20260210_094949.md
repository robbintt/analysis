---
ver: rpa2
title: What Is Your AI Agent Buying? Evaluation, Biases, Model Dependence, & Emerging
  Implications for Agentic E-Commerce
arxiv_id: '2508.02630'
source_url: https://arxiv.org/abs/2508.02630
tags:
- product
- position
- agent
- agents
- gemini
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ACES provides a controlled framework to evaluate AI shopping agents
  by randomizing product attributes, positions, and platform cues. It isolates choice
  behavior from navigation errors and tests rationality via one-shot selection.
---

# What Is Your AI Agent Buying? Evaluation, Biases, Model Dependence, & Emerging Implications for Agentic E-Commerce

## Quick Facts
- **arXiv ID:** 2508.02630
- **Source URL:** https://arxiv.org/abs/2508.02630
- **Reference count:** 40
- **Primary result:** AI shopping agents show strong, model-specific position biases and sensitivity to platform signals, with market shares concentrating on a few modal products and shifting dramatically across model versions.

## Executive Summary
This paper introduces ACES, a controlled framework to evaluate AI shopping agents by randomizing product attributes, positions, and platform cues. Experiments reveal that AI agents exhibit strong position biases that vary across providers and persist even in headless API interfaces. The study also demonstrates that sponsored tags reduce selection while platform endorsements increase it, and that sellers can significantly boost market share by optimizing product descriptions through keyword front-loading. These findings indicate that AI-mediated markets are volatile and susceptible to algorithmic manipulation, highlighting the need for continuous auditing and adaptive seller strategies.

## Method Summary
The study employs a "Veni-Vidi-Emi" workflow using LangChain/Selenium agents to interact with a mock e-commerce app (ACES). The framework randomizes product attributes including position in a 2×4 grid, price perturbations, ratings, review counts, and tags (Sponsored, Overall Pick, Scarcity). Agents select products based on a default prompt, and choices are analyzed using conditional logit models to estimate utility coefficients for each attribute. The approach isolates choice behavior from navigation errors through one-shot selection and tests rationality via controlled tasks.

## Key Results
- Strong position biases vary across AI models and persist in headless API interfaces
- Sponsored tags consistently reduce selection probability while platform endorsements increase it
- Seller-side AI agents can boost market share by 7-80 percentage points through keyword front-loading in product titles
- Market shares concentrate on a few modal products and shift dramatically across model versions

## Why This Works (Mechanism)

### Mechanism 1: Position Bias in Selection
- **Claim:** Product position causally influences AI agent selection rates, with direction and magnitude varying by model and version
- **Mechanism:** AI models process listings sequentially; tokens appearing earlier receive disproportionate attention weight, originating in token-order attention patterns rather than vision encoding
- **Core assumption:** Position effects reflect underlying attention allocation during inference
- **Evidence anchors:** GPT-4.1 and GPT-5.1 exhibit opposite position biases; Claude Sonnet 4 shows 5× selection increase moving from bottom-right to top-middle columns
- **Break condition:** If models are retrained with explicit position-invariance objectives or structured prompting successfully overrides positional attention

### Mechanism 2: Platform Signal Sensitivity (Badges/Tags)
- **Claim:** Sponsored tags reduce selection while platform endorsements increase it, holding all other attributes constant
- **Mechanism:** Agents discount advertising signals while treating platform endorsements as credible quality indicators, mirroring human trust patterns
- **Core assumption:** Badge effects reflect learned associations between platform signals and product quality
- **Evidence anchors:** Sponsored tag reduces 10% baseline to 7.9–8.9%; Overall Pick raises same baseline to 19.9–42.6%
- **Break condition:** If platform endorsement signals become saturated or adversarially manipulated at scale

### Mechanism 3: Keyword Front-Loading for Semantic Alignment
- **Claim:** Moving query-relevant keywords to title beginning dramatically increases selection probability against AI agents
- **Mechanism:** LLMs process text sequentially; early tokens exert outsized influence on relevance scoring, reducing semantic friction
- **Core assumption:** Early-token prominence reflects transformer attention patterns and embedding-space proximity scoring
- **Evidence anchors:** Office lamp case study—moving "Office" from title-end to title-front increased SUNMORY market share from 0% to 89.9% with GPT-5.1
- **Break condition:** If models develop query-contextual understanding that overcomes early-token bias

## Foundational Learning

- **Conditional Logit Choice Models**
  - Why needed here: All quantitative results derive from CL specifications estimating how attributes influence selection utility
  - Quick check question: Can you explain why CL models require product fixed effects when estimating position bias?

- **Randomized Controlled Trials for AI Evaluation**
  - Why needed here: Causal claims rest on exogenous randomization of position, badges, and attributes—not observational correlation
  - Quick check question: Why does randomization of "Sponsored" tags allow causal inference about tag effects independent of product quality?

- **VLM vs. Headless/API Interface Tradeoffs**
  - Why needed here: Position effects persist in both interfaces, but mechanism (visual parsing vs. JSON order) differs in implementation
  - Quick check question: If position bias persists in headless mode, what does this suggest about its root cause?

## Architecture Onboarding

- **Component map:** ACES Sandbox (mock app + browser automation + VLM agent) -> Experiment Runner (randomization + batch processing) -> Analysis Pipeline (CL estimation + market share aggregation)
- **Critical path:** Define product categories -> Configure randomization schema -> Run trials -> Fit CL models -> Deploy seller-response simulation
- **Design tradeoffs:** Single-shot selection isolates choice behavior but omits search-funnel dynamics; generic prompt elicits baseline priors vs. personalized prompts that may mask inherent model biases; low reasoning budget reduces latency but may underrepresent extended deliberation
- **Failure signatures:** Model returns invalid JSON or non-parseable choice (discarded from analysis); rate limits causing incomplete trial runs; position bias inverting between model versions without clear documentation
- **First 3 experiments:** 1) Rationality baseline test: Run 50 trials of single-cheapest-item task per model; 2) Position bias calibration: For one category, run 200 trials with positions uniformly permuted; 3) Headless comparison: Replicate position-bias experiment with JSON-only input

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** To what extent are observed agent choice biases driven by pre-training data, post-training alignment, or specific vision encoder architectures?
- **Basis in paper:** The conclusion states a natural future direction is to understand whether biases are driven by pre-training, post-training, or vision encoder
- **Why unresolved:** Current study is phenomenological, measuring what choices agents make but not isolating internal causal mechanisms or training pipeline steps responsible for specific biases
- **What evidence would resolve it:** Ablation studies across model versions with controlled variations in training data and alignment procedures, or mechanistic interpretability techniques

### Open Question 2
- **Question:** What market equilibria emerge when multiple sellers simultaneously deploy algorithmic optimization strategies against AI buyers?
- **Basis in paper:** Paper demonstrates single seller can boost market share via description edits but notes this creates a new strategic dynamic
- **Why unresolved:** Seller response experiments only optimized a single focal product while keeping competitors static; feedback loop of multi-seller optimization was not simulated
- **What evidence would resolve it:** Multi-agent simulations where numerous seller agents iteratively adjust product descriptions in response to competitor changes

### Open Question 3
- **Question:** How do AI agent choice elasticities and biases quantitatively compare to human consumer baselines or digital twin personas?
- **Basis in paper:** The conclusion notes that comparing behavior of individuals to their digital-twin personas could be an important complement for policy questions
- **Why unresolved:** Study deliberately treats AI as autonomous decision-maker rather than human proxy; it does not benchmark agent price sensitivity or position bias against actual human subject data
- **What evidence would resolve it:** Parallel experiments using human participants or validated "silicon samples" in the ACES framework

## Limitations
- Reproducibility of position bias effects across model versions remains unclear due to reported reversals between preview and final releases
- Market share volatility findings depend heavily on prompt design and reasoning budget, which may not generalize to production agent deployments
- Seller-side optimization results are limited to keyword reordering without exploring broader description engineering or adversarial countermeasures

## Confidence

**Major Uncertainties**
- Position bias reproducibility across model versions is unclear due to sensitivity to undocumented architecture changes
- Market share volatility depends on prompt design and reasoning budget that may not reflect production environments
- Seller optimization results are limited to single-product studies without exploring multi-seller dynamics

**Confidence Labels**
- **High Confidence:** Experimental framework design, randomization approach, and basic choice rationality patterns are methodologically sound
- **Medium Confidence:** Position bias magnitude and direction findings are reliable within tested model versions but may not generalize across future updates
- **Medium Confidence:** Platform signal effects show consistent patterns but rely on assumptions about agent trust modeling that could shift
- **Low Confidence:** Market share concentration predictions and seller optimization recommendations require validation in diverse real-world environments

## Next Checks
1. **Cross-version validation:** Replicate position bias experiments across at least three consecutive model versions to quantify stability and identify breaking changes
2. **Real-world deployment test:** Deploy the ACES framework against actual e-commerce sites with live product inventories to validate mock-app findings
3. **Adversarial robustness:** Test seller agent optimization against simulated platform countermeasures (tag detection, keyword stuffing penalties) to assess practical limits