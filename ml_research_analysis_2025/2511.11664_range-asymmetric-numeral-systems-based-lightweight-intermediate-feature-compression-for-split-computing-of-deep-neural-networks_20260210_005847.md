---
ver: rpa2
title: Range Asymmetric Numeral Systems-Based Lightweight Intermediate Feature Compression
  for Split Computing of Deep Neural Networks
arxiv_id: '2511.11664'
source_url: https://arxiv.org/abs/2511.11664
tags:
- compression
- data
- encoding
- rans
- decoding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a lightweight compression framework for split
  computing of deep neural networks using Range Asymmetric Numeral Systems (rANS).
  The framework combines asymmetric integer quantization with sparse tensor representation
  to reduce communication bottlenecks when transmitting intermediate features between
  edge devices and cloud servers.
---

# Range Asymmetric Numeral Systems-Based Lightweight Intermediate Feature Compression for Split Computing of Deep Neural Networks

## Quick Facts
- arXiv ID: 2511.11664
- Source URL: https://arxiv.org/abs/2511.11664
- Authors: Mingyu Sung; Suhwan Im; Vikas Palakonda; Jae-Mo Kang
- Reference count: 40
- Primary result: Achieves up to 7.2× compression ratio while maintaining model accuracy within 0.2% of baselines

## Executive Summary
This paper presents a novel lightweight compression framework for split computing of deep neural networks using Range Asymmetric Numeral Systems (rANS). The framework addresses communication bottlenecks by combining asymmetric integer quantization with sparse tensor representation to compress intermediate features transmitted between edge devices and cloud servers. By leveraging inherent tensor sparsity and distribution-agnostic quantization, the method achieves significant compression ratios without requiring complex probability modeling or network modifications. The approach demonstrates broad applicability across computer vision and natural language processing tasks.

## Method Summary
The framework integrates asymmetric integer quantization with sparse tensor representation for efficient compression of intermediate features in split computing scenarios. It avoids complex probability modeling by exploiting the inherent sparsity patterns in neural network activations. The method employs a novel compression pipeline that combines tensor reshaping optimization with GPU-accelerated rANS encoding/decoding. An approximate theoretical model guides the optimization of tensor reshaping dimensions to maximize compression efficiency. The implementation achieves sub-millisecond encoding/decoding latency while preserving model accuracy within 0.2% of uncompressed baselines.

## Key Results
- Achieves up to 7.2× compression ratio on evaluated neural architectures
- Maintains model accuracy within 0.2% of uncompressed baselines across ResNet, VGG16, MobileNetV2, SwinT, DenseNet121, and EfficientNetB0
- Demonstrates sub-millisecond encoding/decoding latency on GPU implementations
- Validated across computer vision (CIFAR100, ImageNet) and NLP tasks (Llama2 models)

## Why This Works (Mechanism)
The framework exploits the natural sparsity patterns present in neural network activations by combining asymmetric integer quantization with sparse tensor representation. The rANS algorithm efficiently encodes quantized values by modeling the probability distribution of non-zero elements, while tensor reshaping optimization maximizes compression opportunities. The distribution-agnostic approach eliminates the need for complex probability modeling while still achieving high compression ratios. GPU acceleration enables real-time encoding and decoding with minimal latency overhead.

## Foundational Learning

**Asymmetric Numeral Systems (ANS)**: A entropy coding method that combines arithmetic coding efficiency with Huffman coding speed. Needed for efficient lossless compression of quantized tensors. Quick check: Verify that rANS implementation achieves compression ratios close to theoretical entropy limits.

**Sparse Tensor Representation**: Storage format that efficiently represents tensors with many zero elements by storing only non-zero values and their positions. Needed to exploit the inherent sparsity in neural network activations. Quick check: Confirm that sparse representation overhead is negligible compared to storage savings.

**Range Asymmetric Numeral Systems (rANS)**: A variant of ANS optimized for range coding applications with improved computational efficiency. Needed for fast encoding/decoding of compressed tensors. Quick check: Validate that encoding/decoding latency remains sub-millisecond for various tensor sizes.

**Tensor Reshaping Optimization**: Mathematical optimization of tensor dimensions to maximize compression efficiency while preserving information content. Needed to find optimal trade-offs between compression ratio and computational overhead. Quick check: Verify that theoretical model predictions align with empirical compression results.

## Architecture Onboarding

**Component Map**: Input Tensor -> Quantization -> Sparse Representation -> Tensor Reshaping -> rANS Encoding -> Compressed Output

**Critical Path**: Quantization → Sparse Representation → rANS Encoding/Decoding (these steps dominate the computational pipeline)

**Design Tradeoffs**: The framework balances compression ratio against encoding/decoding latency and accuracy preservation. Higher quantization levels increase compression but may reduce accuracy. Tensor reshaping optimization trades off compression gains against computational overhead.

**Failure Signatures**: Reduced compression efficiency when tensors have low inherent sparsity, increased latency when reshaping dimensions are suboptimal, and accuracy degradation when quantization levels are too aggressive.

**First Experiments**:
1. Measure compression ratio and accuracy preservation on a simple ResNet architecture with CIFAR10 dataset
2. Benchmark encoding/decoding latency across different tensor sizes and reshaping dimensions
3. Compare compression efficiency against baseline methods (e.g., naive quantization, standard sparse representations)

## Open Questions the Paper Calls Out

None

## Limitations
- Theoretical model relies on simplifying assumptions about tensor distribution that may not generalize across all architectures
- Performance on highly structured or low-sparsity tensors remains unverified
- Scalability to extreme-scale models beyond tested ImageNet-scale has not been validated

## Confidence
- High confidence in reported compression ratios and latency measurements for evaluated models
- Medium confidence in theoretical model's general applicability across diverse architectures
- Medium confidence in accuracy preservation claims given evaluation on standard benchmarks only
- Low confidence in scalability claims beyond tested model sizes due to lack of extreme-scale validation

## Next Checks
1. Evaluate framework performance on additional neural architectures with varying sparsity patterns and activation functions to verify general applicability
2. Test compression efficiency and accuracy preservation on larger-scale models (beyond tested ImageNet-scale) and different hardware platforms
3. Validate theoretical model predictions across a broader range of tensor dimensions and distributions not covered in initial experiments