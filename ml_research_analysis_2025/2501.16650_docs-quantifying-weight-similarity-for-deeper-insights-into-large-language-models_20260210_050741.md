---
ver: rpa2
title: 'DOCS: Quantifying Weight Similarity for Deeper Insights into Large Language
  Models'
arxiv_id: '2501.16650'
source_url: https://arxiv.org/abs/2501.16650
tags:
- similarity
- matrices
- docs
- orthogonal
- weight
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces the Distribution of Cosine Similarity (DOCS),
  a novel index for quantitatively assessing similarity between weight matrices in
  large language models (LLMs). Unlike existing methods that focus on representational
  similarity, DOCS directly examines weight matrices to uncover structural patterns
  within LLMs.
---

# DOCS: Quantifying Weight Similarity for Deeper Insights into Large Language Models

## Quick Facts
- **arXiv ID:** 2501.16650
- **Source URL:** https://arxiv.org/abs/2501.16650
- **Authors:** Zeping Min; Xinshang Wang
- **Reference count:** 40
- **Key outcome:** Introduces DOCS (Distribution of Cosine Similarity), a novel index for quantitatively assessing similarity between weight matrices in LLMs, overcoming limitations of existing methods on orthogonal matrices.

## Executive Summary
This paper introduces DOCS, a novel method for quantifying weight matrix similarity in large language models. Unlike existing approaches that focus on representation similarity, DOCS directly examines weight matrices using cosine similarity and Gumbel distribution fitting. The method addresses a critical gap: existing measures like CCA, SVCCA, and CKA become constant on orthogonal matrices, which are common in LLMs. DOCS captures meaningful structural patterns, revealing that adjacent layers exhibit high similarity and form clusters, suggesting functional specialization. Experiments demonstrate DOCS outperforms other similarity indices in identifying meaningful structural patterns.

## Method Summary
DOCS computes weight matrix similarity by first calculating cosine similarity between columns of two matrices, then extracting maximum absolute similarities per column. These maxima are fit to Gumbel distributions via maximum likelihood estimation, and the average of the location parameters yields the DOCS index. The method requires transposing certain weight matrices (W_v, W_k, W_q, MLP-UP) so columns correspond to neurons/parameter vectors. Implementation involves computing the cosine similarity matrix, extracting max similarities, fitting Gumbel distributions, and averaging location parameters. The method is scale-invariant and permutation-invariant but not invariant to general invertible linear transformations.

## Key Results
- DOCS reveals high weight similarity among adjacent layers and forms distinct clusters, suggesting depth-wise functional specialization
- DOCS outperforms CCA, SVCCA, and CKA in identifying meaningful structural patterns, evidenced by higher Gini coefficients and more pronounced diagonal structures in similarity heatmaps
- DOCS effectively distinguishes between experts in MoE architectures while showing high similarity between base and instruction-tuned models
- DOCS demonstrates discriminative power on orthogonal matrices where other methods collapse to constants

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DOCS captures meaningful weight similarity by using maximum cosine similarity per column, then modeling the distribution extremity via Gumbel fitting.
- Mechanism: For two matrices X and Y, DOCS computes a cosine similarity matrix C (m×m), then for each column j in X extracts the max |C_{jk}| across all k in Y (and symmetrically for Y). These max-similarity vectors are fit to Gumbel distributions via MLE; the average of the location parameters yields S_{DOCS} ∈ [0,1]. This avoids diluting strong correspondences by averaging over all pairs.
- Core assumption: The maxima of column-wise cosine similarities follow an approximately Gumbel distribution, which the paper validates empirically (Figures 12a–12b in Appendix G).
- Evidence anchors: [abstract] "DOCS uses cosine similarity and a Gumbel distribution to capture meaningful relationships between weights." [section 3] Algorithm 1 and Section 3, Steps 1–4 explicitly describe the pipeline.
- Break condition: If maxima do not concentrate sufficiently (e.g., weight matrices are nearly random with no strong column alignments), the fitted Gumbel location parameter may be unstable or uninformative.

### Mechanism 2
- Claim: DOCS is discriminative on orthogonal matrices, unlike CCA/SVCCA/Linear CKA which collapse to constants on orthogonals.
- Mechanism: Orthogonal matrices satisfy Q^T Q = I. For CCA/SVCCA/Linear CKA, the paper proves (Appendix B) that their index becomes a constant (often 1) for any pair of orthogonal matrices. By operating on distributions of max cosine similarities, DOCS can vary depending on the alignment structure between orthogonal matrices (Theorem 1 constructs X,Y orthogonal where S_{DOCS} = 1/sqrt(m) while ||X-Y||_F = Omega(sqrt(m))).
- Core assumption: LLM weight matrices exhibit sufficient near-orthogonality that the constant behavior of other indices materially harms their discriminative power.
- Evidence anchors: [section 2] Definitions 1–3 and Table 1 categorize methods by behavior on orthogonal matrices. [appendix B] Proofs for CCA/SVCCA/Linear CKA returning constant 1 on orthogonals.
- Break condition: If LLM weight matrices are far from orthogonal in practice, the comparative advantage of DOCS over CKA/SVCCA diminishes; the paper provides some orthogonality evidence (Appendix E.2), but broader empirical characterization would strengthen this.

### Mechanism 3
- Claim: Weight similarity analysis reveals structural patterns (adjacent-layer similarity, layer clusters) not directly inferable from representation similarity due to residual connections.
- Mechanism: Residual connections (y = F(x,W) + x) create input-output correlations across layers, so similar representations do not imply similar weights (Section 1). By directly comparing weight matrices, DOCS finds high similarity among neighboring layers and distinct clusters (e.g., layers 7–12), suggesting depth-wise functional specialization.
- Core assumption: High weight similarity within clusters indicates functional similarity or redundancy; the precise functional implication is not proven, but the correlation is empirically observed.
- Evidence anchors: [section 1] Equation (1) and discussion of residual connections decoupling representation from weight similarity. [section 4.2–4.3] Figures 3–5 show diagonal-heavy heatmaps and block structures.
- Break condition: If model architecture changes (e.g., non-residual, different normalization schemes), the relationship between weight patterns and functional specialization may differ.

## Foundational Learning

- Concept: Orthogonal matrices and their properties (Q^T Q = I, preservation of norms, role in stable training).
  - Why needed here: DOCS’s main theoretical selling point is discriminative behavior on orthogonal matrices; understanding orthogonality is essential to see why prior methods fail.
  - Quick check question: For an n×n orthogonal matrix Q, what is ||Q||_F? (Answer: sqrt(n))

- Concept: Cosine similarity between vectors and its invariances (scale invariance, range [-1,1]).
  - Why needed here: DOCS builds on column-wise cosine similarities; misunderstanding normalization will break interpretation of S_{DOCS} ∈ [0,1].
  - Quick check question: If u and v are orthogonal unit vectors, what is their cosine similarity? (Answer: 0)

- Concept: Extreme value distributions (Gumbel) and location parameter interpretation.
  - Why needed here: DOCS summarizes max-similarity vectors via Gumbel fits; the location parameter captures central tendency of extremes.
  - Quick check question: For a Gumbel distribution, does the location parameter equal the mean? (Answer: No; it is a shift parameter, related but not identical to the mean, which also involves the scale parameter and Euler–Mascheroni constant.)

## Architecture Onboarding

- Component map: Load LLM weights → Transpose specific weight matrices → Compute cosine similarity matrix → Extract max absolute similarities → Fit Gumbel distributions → Average location parameters → DOCS index

- Critical path: Correct transposition (rows = output dim, cols = input dim in most LLM implementations) → accurate cosine matrix → stable Gumbel fit (sufficient columns m for reliable MLE)

- Design tradeoffs:
  - Max vs average: Max emphasizes strongest alignments; average dilutes signal (Appendix G, Figure 13)
  - Gumbel vs simple mean: Distributional fitting is more robust to outliers but adds complexity and requires enough samples
  - Permutation invariance is preserved; isotropic scaling invariance holds, but not general invertible linear transformations (by design, see Appendix D)

- Failure signatures:
  - Very low m (few columns) leading to unstable Gumbel fits
  - Near-constant similarity heatmaps (may indicate strong orthogonality or lack of meaningful structure)
  - Numerical instability in cosine computation due to near-zero norms

- First 3 experiments:
  1. Replicate the MLP-UP heatmap for Llama-3.1-8B-Instruct (Figure 2h) to verify implementation correctness
  2. Compare DOCS vs Linear CKA on orthogonal pairs constructed via Theorem 1 to confirm discriminative behavior
  3. Run DOCS on base vs. instruct variants within a model family to reproduce the >0.7 similarity pattern (Section 4.4)

## Open Questions the Paper Calls Out
None

## Limitations
- The empirical prevalence of true orthogonality in real LLM weight matrices remains uncertain, limiting the practical advantage of DOCS over simpler methods
- The Gumbel fitting step introduces complexity that may be sensitive to sample size, with insufficient validation of DOCS stability across varying matrix dimensions
- Structural interpretations (adjacent-layer similarity, clustering) are empirically observed but mechanistically unexplained, limiting generalization beyond residual architectures

## Confidence

**High confidence**: DOCS implementation (Algorithm 1) is clearly specified and reproducible. The mathematical framework for cosine similarity and Gumbel fitting is sound. The comparative advantage over CCA/SVCCA/CKA on orthogonal matrices is theoretically proven.

**Medium confidence**: The empirical findings about weight similarity patterns (diagonal structures, clustering) are well-documented but require further validation across architectures. The claim that DOCS "uncovers meaningful relationships" is supported but not definitively proven to be more insightful than alternative interpretations.

**Low confidence**: The broader claim that DOCS provides deeper insights than representation similarity measures is partially validated but lacks comprehensive external validation. The relationship between observed weight similarity patterns and functional properties remains correlational rather than causal.

## Next Checks

1. **Orthogonality Prevalence Study**: Systematically measure the degree of orthogonality across weight matrices in multiple LLM families (different architectures, scales, training regimes). Quantify how often weights satisfy ||Q^T Q - I||_F < ε for various ε thresholds. This validates whether the theoretical advantage on orthogonal matrices translates to practical advantage.

2. **DOCS Stability Analysis**: Test DOCS behavior across varying matrix dimensions, particularly for weight matrices with few columns (e.g., attention output projections). Measure sensitivity to Gumbel fitting parameters and establish minimum column requirements for stable estimates. Compare DOCS stability against simpler averaging methods across these regimes.

3. **Cross-Architecture Generalization**: Apply DOCS to non-residual architectures (e.g., vanilla transformers, MLPs without skip connections) and architectures with different normalization schemes. Compare whether the observed diagonal clustering patterns persist or transform, validating whether these patterns are architecture-specific or more general properties of weight evolution during training.