---
ver: rpa2
title: "How to Set $\u03B2_1, \u03B2_2$ in Adam: An Online Learning Perspective"
arxiv_id: '2510.03478'
source_url: https://arxiv.org/abs/2510.03478
tags:
- adam
- learning
- regret
- bound
- psvs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper provides new regret bounds for the Adam optimizer under\
  \ different momentum factor settings (\u03B2\u2081, \u03B2\u2082) in online learning.\
  \ Previous analyses required \u03B2\u2081 = \u221A\u03B2\u2082, but the author generalizes\
  \ this to both \u03B2\u2081 \u2264 \u221A\u03B2\u2082 and \u03B2\u2081 \u2265 \u221A\
  \u03B2\u2082."
---

# How to Set $β_1, β_2$ in Adam: An Online Learning Perspective

## Quick Facts
- arXiv ID: 2510.03478
- Source URL: https://arxiv.org/abs/2510.03478
- Authors: Quan Nguyen
- Reference count: 7
- Primary result: Provides new regret bounds for Adam optimizer under different momentum factor settings (β₁, β₂), showing β₁ = √β₂ is optimal for oblivious adversaries but sub-optimal for non-oblivious ones.

## Executive Summary
This paper analyzes Adam optimizer's momentum factors (β₁, β₂) through the lens of online learning, providing regret bounds that generalize beyond the previously required setting of β₁ = √β₂. The author reformulates Adam as Follow-the-Regularized-Leader (FTRL) with time-varying learning rates, enabling analysis of both β₁ ≤ √β₂ and β₁ ≥ √β₂ regimes. The key insight is that β₁ = √β₂ is optimal for oblivious adversaries but provably sub-optimal for non-oblivious adversaries, demonstrated through a specific construction. The regret bounds are shown to be tight in the worst case, providing a more complete theoretical understanding of Adam's hyperparameter space.

## Method Summary
The paper analyzes Adam by reformulating it as FTRL on discounted losses. The coordinate-wise update ∆ₜ = -ηₜ Σₛ β₁^(t-1-s) gₛ is shown to be equivalent to FTRL with regularizer x²/(2ηₜ) and losses β₁^(-t)gₜx. The analysis requires ηₜ to be non-increasing: when β₁ ≤ √β₂, constant learning rates work; when β₁ ≥ √β₂, exponentially decaying learning rates αₜ = α/p^(t-1) are needed (where p = β₁/√β₂). The regret bounds are derived through standard FTRL regret decomposition, and tightness is proven by constructing specific loss sequences where the regret matches the bound.

## Key Results
- Provides regret bounds for Adam beyond the traditional β₁ = √β₂ constraint, covering both β₁ ≤ √β₂ and β₁ ≥ √β₂ regimes
- Proves that β₁ = √β₂ is optimal for oblivious adversaries but sub-optimal for non-oblivious adversaries
- Shows the derived regret bounds are tight in the worst case through specific constructions
- Establishes that the choice of (β₁, β₂) should depend on whether the adversary is oblivious or non-oblivious

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adam's momentum update can be analyzed as Follow-the-Regularized-Leader (FTRL) on discounted losses, enabling regret bounds beyond β₁ = √β₂.
- Mechanism: By reformulating Adam's coordinate-wise update ∆ₜ = -ηₜ Σₛ β₁^(t-1-s) gₛ as FTRL with regularizer x²/(2ηₜ) and losses β₁^(-t)gₜx, the analysis becomes a standard regret decomposition. The key is ensuring ηₜ is non-increasing: when β₁ ≤ √β₂, constant αₜ works; when β₁ ≥ √β₂, exponentially decaying αₜ = α/p^(t-1) is required (where p = β₁/√β₂).
- Core assumption: The FTRL framework with time-varying learning rates applies to Adam's update rule; clipping to bounded domain X = [-D, D] preserves the analysis.
- Evidence anchors:
  - [abstract]: "Prior works have shown that Adam can be seen as an instance of Follow-the-Regularized-Leader (FTRL)... required setting β₁ = √β₂, which does not cover the more practical cases with β₁ ≠ √β₂"
  - [page 2, Section 1]: "Ahn et al. (2024) considered a special version of Adam with β₁ = √β₂... these results are significant, they only hold for the case β₁ = √β₂ and do not hold for more general settings"
  - [corpus]: Neighbor work "Simple Convergence Proof of Adam From a Sign-like Descent Perspective" also reinterprets Adam structurally but uses sign-descent rather than FTRL framing
- Break condition: If the FTRL-to-Adam equivalence breaks (e.g., bias-correction terms become non-absorbable, or coordinate independence fails), the regret bounds may not hold.

### Mechanism 2
- Claim: The derived regret bounds are tight in worst case—cannot be significantly improved without additional problem structure.
- Mechanism: Theorem 5 constructs a specific loss sequence vₜ = κᵗv₀ (for 0.4 ≤ p ≤ 0.6, κ ≥ 1/p²) where the algorithm never clips, and computes both the actual regret Rₜ(-D) and the bound Bₜ. The regret lower bound matches the upper bound order: both scale as Dv₀κᵀ up to constants.
- Core assumption: Assumption: The adversary can choose loss sequences adversarially within the problem class; the lower bound construction is representative of worst-case behavior.
- Evidence anchors:
  - [abstract]: "Furthermore, we show that our bounds are tight in the worst case"
  - [page 8, Theorem 5]: "On the bounded domain X = [-D, D], there exists a sequence (vₜ)ₜ where vₜ > 0 for all t ∈ [T], such that... Rₜ(u) = Ω(Bₜ,α,p,(vₜ)ₜ(u))"
  - [corpus]: Weak corpus signal—neighbor papers focus on empirical tuning and implicit bias, not tightness of theoretical bounds
- Break condition: If practical loss sequences have additional structure (e.g., bounded variation, smoothness), tighter problem-dependent bounds may exist. Theorem 5 is algorithm-dependent; other algorithms might achieve better worst-case bounds.

### Mechanism 3
- Claim: β₁ = √β₂ is optimal for oblivious adversaries but provably sub-optimal for non-oblivious adversaries.
- Mechanism: For oblivious adversaries (fixed loss sequence independent of β₂), the regret bound is minimized when β₂ = β₁², since β₂^(T-t) ≥ β₁^(2(T-t)) for all t. For non-oblivious adversaries, Theorem 11 constructs two algorithms A (β₁ = p√β₂, p < 1) and A′ (β₁ = √β₂) with loss sequences vₜ = aᵗv and v′ₜ = bᵗv respectively, showing Σ vₜ(∆ₜ - u) < Σ v′ₜ(∆′ₜ - u) when a < b².
- Core assumption: The non-oblivious construction assumes gradient sequences change based on algorithm parameters—a property of real training where wₜ depends on β₂, affecting subsequent gradients.
- Evidence anchors:
  - [abstract]: "We also prove that setting β₁ = √β₂ is optimal for an oblivious adversary, but sub-optimal for a non-oblivous adversary"
  - [page 11-12, Theorem 11]: "Let u = -D. For any T ≥ 2, the sequence of updates from A and A′ satisfy Σₜ vₜ(∆ₜ - u) < Σₜ v′ₜ(∆′ₜ - u)"
  - [corpus]: "In Search of Adam's Secret Sauce" empirically studies Adam on transformers but doesn't address oblivious vs. non-oblivious distinctions
- Break condition: If actual training dynamics don't match the non-oblivious model (e.g., gradients vary weakly with β₂), β₁ = √β₂ may remain empirically optimal. The construction requires specific exponential loss patterns.

## Foundational Learning

- **Follow-the-Regularized-Leader (FTRL)**
  - Why needed here: The entire analysis reframes Adam as FTRL with time-varying learning rates. Understanding FTRL regret decomposition (Rₜ(u) ≤ u²/ηₜ₊₁ + Σ Fₜ(∆ₜ) - Fₜ₊₁(∆ₜ₊₁) + vₜ∆ₜ) is essential to follow the proofs.
  - Quick check question: Can you derive why FTRL with ηₜ non-increasing guarantees bounded regret?

- **β-discounted regret**
  - Why needed here: The paper analyzes Rₜ,β₁(u) = β₁ᵀ Σ β₁^(-t)gₜ(∆ₜ - u), not standard regret. This connects to the online-to-nonconvex framework where discounted regret bounds convergence rates.
  - Quick check question: Why does multiplying by β₁ᵀ and weighting losses by β₁^(-t) make sense for non-stationary optimization?

- **Oblivious vs. Non-oblivious Adversary**
  - Why needed here: The optimality of β₁ = √β₂ depends critically on this distinction. Oblivious adversaries fix loss sequences; non-oblivious adversaries adapt losses to algorithm outputs.
  - Quick check question: In neural network training, is the gradient sequence (gₜ)ₜ oblivious or non-oblivious with respect to β₂? Why?

## Architecture Onboarding

- **Component map**:
  - Algorithm 1 (Adam-as-FTRL): Input β₁, β₂ ∈ (0,1), αₜ, domain X. Computes ηₜ = αₜ(β₁/√β₂)^(t-1)/√(Σ β₂^(-s)g²ₛ), predicts ∆ₜ = argminₓ∈X [x²/(2ηₜ) + Σ β₁^(-s)gₛx], receives gradient gₜ.
  - Algorithm 2 (FTRL notation): Equivalent reformulation with vₜ = β₁^(-t)gₜ, p = β₁/√β₂. Used for theoretical analysis.
  - Clipping operation: clip(x) = x·min(D/|x|, 1) ensures ∆ₜ ∈ [-D, D].

- **Critical path**:
  1. Verify β₁ vs. √β₂ relationship to select appropriate αₜ schedule (constant vs. exponentially decaying)
  2. Track that ηₜ remains non-increasing (violating this breaks the FTRL analysis)
  3. Monitor whether clipping activates—Theorem 5's tightness proof relies on no-clipping regime

- **Design tradeoffs**:
  - **β₁ ≤ √β₂ regime**: Simpler (constant α), but bound has √(β₂/β₁) factor which can be large if β₁ ≪ √β₂
  - **β₁ ≥ √β₂ regime**: Requires decaying αₜ = α/p^(t-1), complicating hyperparameter schedules, but bound doesn't have the √(β₂/β₁) penalty
  - **Tightness vs. generality**: Bounds are worst-case tight; average-case may admit better settings

- **Failure signatures**:
  - If ηₜ increases over time (wrong αₜ schedule for given β₁, β₂), regret analysis fails
  - If gradients grow exponentially faster than the bound assumes, the max|β₁^(-t)gₜ| term dominates
  - If clipping activates frequently, the constructed lower bound may not apply, but the upper bound still holds

- **First 3 experiments**:
  1. **Reproduce Theorem 11's non-oblivious example**: Implement Algorithms A and A′ with p < 1, compare cumulative regret on constructed sequences vₜ = aᵗv, v′ₜ = bᵗv where a < b². Verify that A achieves strictly lower regret.
  2. **Test β₁ = √β₂ vs. β₁ < √β₂ on synthetic oblivious losses**: Generate fixed gradient sequences (e.g., random walk, sinusoidal) and measure regret for different (β₁, β₂) pairs. Confirm β₁ = √β₂ minimizes the theoretical bound's leading term.
  3. **Empirical tightness check**: On the loss sequence from Theorem 5 (vₜ = κᵗv₀ with κ ≥ 1/p²), measure actual regret Rₜ(-D) and compare to bound Bₜ. Compute the ratio Rₜ/Bₜ across time to verify it remains Ω(1) (doesn't decay to zero).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do there exist optimization algorithms that achieve better problem-dependent regret bounds than Adam by accounting for the total variation in the loss sequence?
- Basis in paper: [explicit] Remark 8 states, "This leaves the question of whether there are other algorithms that may achieve better worst-case or problem-dependent regret bounds... We leave this as a future work."
- Why unresolved: The paper proves Adam's bounds are tight in the *worst case*, but suggests performance could be improved in "lazy-training" regimes (where gradients change slowly) if the algorithm explicitly tracked loss variation.
- What evidence would resolve it: A proposed algorithm with a regret bound explicitly dependent on the variation of gradients, along with empirical validation showing superior performance over Adam in low-variation settings.

### Open Question 2
- Question: How can we theoretically characterize the "non-oblivious" nature of adversaries (gradients) in standard non-convex objectives to determine optimal $\beta_1, \beta_2$ settings?
- Basis in paper: [explicit] The Conclusion states: "Future works include characterizing the adversary, i.e. how the sequence of gradients changes according to different values of $\beta_1, \beta_2$, on popular convex and non-convex objectives."
- Why unresolved: The paper demonstrates that $\beta_1 = \sqrt{\beta_2}$ is suboptimal for *constructed* non-oblivious adversaries but lacks a model for how standard neural network training creates such dependencies.
- What evidence would resolve it: A theoretical framework describing the correlation between Adam's iterates and the resulting stochastic gradients in deep learning tasks (e.g., transformer pre-training), leading to a derived optimal hyperparameter schedule.

### Open Question 3
- Question: Can the $\beta_1 \ge \sqrt{\beta_2}$ regime achieve the derived regret bounds using constant learning rates rather than exponentially decaying ones?
- Basis in paper: [inferred] Section 3 explicitly requires an exponentially decaying sequence $\alpha_t$ to ensure non-increasing learning rates when $\beta_1 \ge \sqrt{\beta_2}$, whereas Section 2 ($\beta_1 \le \sqrt{\beta_2}$) allows constant $\alpha$.
- Why unresolved: The requirement for decaying learning rates might be an artifact of the specific analysis technique used to ensure the stability of the FTRL learning rate sequence, rather than a fundamental limitation of the optimizer.
- What evidence would resolve it: A proof extending Theorem 9 to hold for constant learning rates $\alpha_t = \alpha$, or a counter-example showing that constant rates result in unbounded regret when $\beta_1 > \sqrt{\beta_2}$.

## Limitations
- The FTRL framework equivalence between Adam and the proposed analysis relies on careful handling of bias-correction terms and coordinate-wise updates.
- The tightness results depend on specific worst-case constructions that may not reflect typical training dynamics in deep learning.
- The non-oblivious adversary model assumes strong dependencies between algorithm parameters and gradient sequences that may not manifest in all practical settings.

## Confidence
- **High confidence**: Regret bounds for β₁ ≤ √β₂ regime (well-established FTRL analysis)
- **Medium confidence**: Exponentially decaying learning rate analysis for β₁ ≥ √β₂ (requires careful ηₜ monotonicity verification)
- **Medium confidence**: Non-oblivious adversary construction and its implications for β₁ = √β₂ optimality (depends on specific adversarial assumptions)

## Next Checks
1. **Verify ηₜ monotonicity across regimes**: Implement Algorithm 2 and track ηₜ over time for various (β₁, β₂) pairs. Confirm ηₜ is non-increasing when using the correct αₜ schedule (constant for β₁ ≤ √β₂, exponentially decaying for β₁ ≥ √β₂).

2. **Test practical relevance of non-oblivious findings**: Beyond the theoretical construction, empirically compare β₁ = √β₂ vs. β₁ < √β₂ on actual neural network training tasks where gradients are known to depend on previous parameter updates (e.g., language modeling with AdamW).

3. **Characterize when clipping activates**: Determine the gradient growth conditions that trigger clipping in practice. Measure the frequency and magnitude of clipping events across different (β₁, β₂) settings on real datasets to assess whether the no-clipping assumption in Theorem 5's tightness proof is realistic.