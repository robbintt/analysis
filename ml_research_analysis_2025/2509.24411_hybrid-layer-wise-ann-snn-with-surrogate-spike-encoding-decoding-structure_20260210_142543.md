---
ver: rpa2
title: Hybrid Layer-Wise ANN-SNN With Surrogate Spike Encoding-Decoding Structure
arxiv_id: '2509.24411'
source_url: https://arxiv.org/abs/2509.24411
tags:
- neural
- hybrid
- spike
- gradient
- surrogate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a hybrid ANN-SNN framework that integrates
  layer-wise encode-decode SNN blocks within conventional ANN pipelines, addressing
  the challenge of non-differentiability in spike encoding functions. The core method
  involves using surrogate gradients for bit-plane-based spike encoding, enabling
  end-to-end differentiable training across ANN and SNN layers.
---

# Hybrid Layer-Wise ANN-SNN With Surrogate Spike Encoding-Decoding Structure

## Quick Facts
- arXiv ID: 2509.24411
- Source URL: https://arxiv.org/abs/2509.24411
- Reference count: 40
- Primary result: Achieves 79.20% accuracy on CIFAR-100 and 52.30% on ImageNet using hybrid ANN-SNN models with fewer parameters than pure ANNs.

## Executive Summary
This paper introduces a hybrid ANN-SNN framework that integrates layer-wise encode-decode SNN blocks within conventional ANN pipelines. The key innovation is using surrogate gradients to enable differentiable training across the non-differentiable spike encoding interface. By decomposing pixel values into bit-planes and encoding them as spike sequences, the method achieves competitive accuracy with state-of-the-art pure ANN and SNN models while potentially offering efficiency benefits of spiking computation.

## Method Summary
The framework uses bit-plane encoding to convert 8-bit pixel values into spike sequences across 8 timesteps. Surrogate gradients approximate the non-differentiable encoding function during backpropagation, enabling end-to-end training. The architecture features layer-wise hybrid blocks where ANN and SNN paths process inputs in parallel, then fuse results through element-wise addition. Order-aware gradient rescaling prevents training instability from high-variance gradients in lower-order bit-planes. The approach is validated on CIFAR-10/100, Caltech-101/256, and ImageNet datasets.

## Key Results
- HAS-8-ResNet[b32-m2-d4] achieves 79.20% accuracy on CIFAR-100
- SAME model achieves 52.30% accuracy on ImageNet
- Outperforms existing ANN and SNN baselines with fewer parameters
- Gradient rescaling is critical: disabling it causes accuracy to drop from ~79% to ~24%

## Why This Works (Mechanism)

### Mechanism 1: Surrogate Gradient for Bit-Plane Encoding
The framework approximates the non-differentiable bit-plane spike encoding function with smooth periodic functions (Sigmoid-wrapped sine or Fourier series) during backpropagation. This synthetic gradient allows error signals to flow from ANN layers back through the SNN encoding interface. The method relies on the periodicity of bit-plane coding being stably approximated by smooth functions without losing binary structure.

### Mechanism 2: Order-Aware Gradient Rescaling
A scaling factor proportional to bit-plane index attenuates gradients from lower-order bits (which have high variance) while preserving gradients from high-order bits that carry structural information. This prevents training instability caused by variance divergence in lower-order bit-planes.

### Mechanism 3: Layer-Wise Parallel Fusion
Input duplication allows parallel feature extraction through ANN and SNN paths, with results combined via element-wise addition. This treats the SNN path as a residual feature enhancer rather than a replacement, improving signal-to-noise ratio.

## Foundational Learning

- **Surrogate Gradient Descent**: Needed to handle the discrete step function in spike generation. The Heaviside derivative fails backpropagation, replaced by smooth approximations like arctan or sigmoid during backward pass.
- **Bit-Plane Slicing**: Decomposes 8-bit integers into 8 binary planes for temporal spike representation. For pixel intensity 128 (10000000), only timestep t=0 contains a spike.
- **Integrate-and-Fire (IF) Neuron Model**: Used in SNN blocks where membrane potential accumulates until threshold, then resets to zero. Understanding this is critical for debugging spike behavior.

## Architecture Onboarding

- **Component map**: Input Image Tensor → Bit-Plane Encoder → Parallel Fork (ANN Conv | SNN Conv with IF Neurons) → Element-wise Addition → Decoder
- **Critical path**: Surrogate Encoder implementation must correctly implement scaled Fourier/Sine approximation and order-aware rescaling. Incorrect gradients cause complete training failure.
- **Design tradeoffs**: Fixed T=8 timesteps aligns with 8-bit data but limits temporal flexibility. FourierSine offers best stability/accuracy tradeoff. Rate-based decoding is simpler than bit-plane decoding.
- **Failure signatures**: Accuracy collapse to ~20% indicates missing gradient rescaling. Slow convergence suggests incorrect surrogate sharpness parameter. Gradient explosion indicates membrane potential scaling issues.
- **First 3 experiments**:
  1. CIFAR-10 ablation: Train HAS-8-ResNet with/without gradient rescaling to verify performance gap.
  2. Surrogate comparison: Swap FourierSine for SigSine and compare validation loss curves.
  3. Hybrid vs Pure: Compare pure ANN ResNet18 against HAS-8-ResNet on CIFAR-100, monitoring MACs vs accuracy.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions beyond its scope.

## Limitations
- Surrogate gradient quality and gradient rescaling are critical but sensitive to parameter tuning with limited robustness analysis.
- Efficiency claims are relative to hybrid models, not absolute state-of-the-art pure ANNs.
- Long-term stability and generalization across diverse architectures are not demonstrated.

## Confidence
- **High Confidence**: Core surrogate gradient mechanism is well-defined with strong ablation evidence for gradient rescaling necessity.
- **Medium Confidence**: Competitive accuracy and parameter efficiency claims are supported but limited to hybrid model comparisons.
- **Low Confidence**: Surrogate approximation stability and generalization across diverse datasets and architectures remain unexplored.

## Next Checks
1. Systematically vary the FourierSine approximation parameter α to determine optimal and stable range for convergence speed and accuracy.
2. Train models with only highest-order bit-planes to quantify lower-bit contribution and validate gradient rescaling necessity.
3. Apply HAS-8 framework to different backbone (MobileNet/EfficientNet) on CIFAR-10 to test cross-architecture generalization.