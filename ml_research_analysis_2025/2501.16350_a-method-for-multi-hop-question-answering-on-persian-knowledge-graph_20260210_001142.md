---
ver: rpa2
title: A Method for Multi-Hop Question Answering on Persian Knowledge Graph
arxiv_id: '2501.16350'
source_url: https://arxiv.org/abs/2501.16350
tags:
- question
- knowledge
- complex
- questions
- answering
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel method for answering multi-hop complex
  questions in Persian using a knowledge graph. The authors developed a dataset of
  5,600 Persian multi-hop complex questions with their decomposed semantic representations
  and trained Persian language models on this data.
---

# A Method for Multi-Hop Question Answering on Persian Knowledge Graph

## Quick Facts
- arXiv ID: 2501.16350
- Source URL: https://arxiv.org/abs/2501.16350
- Reference count: 40
- Key outcome: 12.57% F1-score improvement over existing methods for Persian multi-hop QA

## Executive Summary
This paper presents a novel method for answering multi-hop complex questions in Persian using a knowledge graph. The authors developed a dataset of 5,600 Persian multi-hop complex questions with their decomposed semantic representations and trained Persian language models on this data. Their approach achieves a 12.57% improvement in F1-score and 12.06% improvement in accuracy compared to the best existing method, demonstrating the effectiveness of their approach for complex Persian question answering over knowledge graphs.

## Method Summary
The method employs a four-component pipeline: (1) Question Decomposition using fine-tuned mT5 to break complex questions into sequential semantic steps, (2) NER using fine-tuned ParsBERT for entity recognition and linking, (3) SPARQL generation using another fine-tuned mT5 model to convert semantic steps into executable queries, and (4) Sequential query execution on the FarsBase knowledge graph. The approach transforms multi-hop questions into semantically equivalent SPARQL queries through this modular architecture, achieving strong performance on the PeCoQ dataset.

## Key Results
- F1-score of 75.55% and Accuracy of 74.81% on PeCoQ dataset
- 12.57% F1-score improvement over existing methods
- Component-level accuracy: Task Decomposition (77.61%), NER (99.16%), SPARQL Generation (82.35%)

## Why This Works (Mechanism)

### Mechanism 1: Semantic Decomposition Reduces Reasoning Complexity
Breaking multi-hop questions into sequential, single-hop semantic representations (MRDCPQs) reduces the error rate of the query generation model by simplifying the mapping task. An mT5 model is fine-tuned to rewrite complex questions into structured intermediate representations, transforming a multi-step reasoning problem into a chain of simpler lookup tasks.

### Mechanism 2: Domain-Specific NER Fine-Tuning for Entity Linking
Fine-tuning ParsBERT specifically on the MRDCPQ dataset significantly improves entity extraction accuracy compared to generic Persian NER models. This specialization allows the model to recognize specific phrasing and entities common in the FarsBase knowledge graph context.

### Mechanism 3: Sequential Query Execution with State Passing
Executing SPARQL queries sequentially and passing intermediate entity results from one query to the next is the primary driver for solving multi-hop constraints. The system generates a query for the first hop, executes it to find an intermediate entity, and then uses that entity as the subject for the second hop's query.

## Foundational Learning

### Concept: Semantic Parsing in KGQA
Why needed here: This paper adopts a Semantic Parsing approach (converting text to logic/SPARQL) rather than Information Retrieval. Understanding this distinction is required to grasp why the system needs a query generator rather than just a vector store.
Quick check question: Does the system retrieve answer candidates based on embedding similarity or by executing structured database queries?

### Concept: Multi-hop Reasoning
Why needed here: The core challenge is "multi-hop"—finding a fact (A → B) to use as a bridge to find the answer (B → C).
Quick check question: In the query "Who is the wife of the director of X?", what is the "bridge" entity that connects the starting point X to the final answer?

### Concept: Encoder-Decoder Architectures (mT5)
Why needed here: The paper utilizes mT5, a text-to-text transformer, for both decomposition and query generation. You must understand that this model takes text as input and generates text (or code) as output.
Quick check question: Why is a generative model (Decoder) suitable for creating SPARQL queries compared to a classification model (Encoder-only)?

## Architecture Onboarding

### Component map:
Input (Persian Question) → Decomposer (mT5) → NER & Linker (ParsBERT) → SPARQL Generator (mT5) → Executor

### Critical path:
The Decomposer → NER link is the most fragile. If the decomposition grammar does not match what the NER expects, or if the NER fails to link the entity to FarsBase, the SPARQL generator receives null/invalid inputs.

### Design tradeoffs:
Pipeline vs. End-to-End: The authors chose a modular pipeline allowing independent debugging and dataset creation for each step but suffers from error propagation. Specialization vs. Generalization: The NER is aggressively fine-tuned on PeCoQ/MRDCPQ data (99% accuracy), likely at the cost of generalization to other Persian text domains.

### Failure signatures:
- Syntax Errors in SPARQL: The generator generates structurally invalid code
- Empty Entity Links: The NER detects "Tehran" but fails to find the specific FarsBase URI due to string mismatch
- Logic Drift: The decomposition reverses the relation order (e.g., asking for the "director of the wife" instead of "wife of the director")

### First 3 experiments:
1. Unit Test Decomposer: Feed the test set into the mT5 decomposer and measure Task Decomposition Accuracy (TDA) manually to verify the 77.61% claim
2. NER Stress Test: Input questions with entities that exist in FarsBase but are absent from the MRDCPQ training set to test the generalization of the Fine-Tuned ParsBERT
3. Pipeline Ablation: Manually replace the generated SPARQL queries with Gold Standard queries for the final step. If accuracy spikes, the query generator is the bottleneck

## Open Questions the Paper Calls Out
- Exploring multimodal learning by integrating textual and visual data to improve handling of complex multimedia queries
- Incorporating advanced language models (larger Persian LLMs) to enhance accuracy without compromising efficiency
- Developing approaches to handle parallel or non-linear reasoning paths beyond strictly sequential multi-hop chains

## Limitations
- Heavy dependency on the MRDCPQ dataset which is not publicly available, limiting independent validation
- Strong specialization for FarsBase knowledge graph and Persian language raises concerns about generalizability
- Sequential query execution pipeline may struggle with questions requiring parallel or non-linear reasoning paths

## Confidence
- High confidence: Component-level metrics (Task Decomposition: 77.61%, NER: 99.16%, SPARQL Generation: 82.35%) are internally consistent
- Medium confidence: End-to-end performance metrics (F1: 75.55%, Accuracy: 74.81%) are credible but lack independent verification
- Low confidence: 12.57% F1-score improvement claim cannot be independently verified without baseline implementations

## Next Checks
1. Request access to MRDCPQ dataset or reconstruct it using the described annotation methodology to verify decomposition quality
2. Conduct ablation studies by replacing individual components with baseline alternatives to isolate contribution of each module
3. Test the system on Persian questions containing entities not present in MRDCPQ training data or on a different knowledge graph (e.g., DBpedia) to evaluate generalization