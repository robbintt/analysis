---
ver: rpa2
title: 'AngularFuse: A Closer Look at Angle-based Perception for Spatial-Sensitive
  Multi-Modality Image Fusion'
arxiv_id: '2510.12260'
source_url: https://arxiv.org/abs/2510.12260
tags:
- image
- fusion
- gradient
- loss
- visible
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AngularFuse addresses the limitations of existing unsupervised
  visible-infrared image fusion methods, which struggle with detail loss and uneven
  brightness due to suboptimal loss functions. The core idea is to introduce an angle-aware
  perceptual loss that jointly constrains both gradient magnitude and direction, alongside
  a complementary masking strategy and fine-grained reference image synthesis.
---

# AngularFuse: A Closer Look at Angle-based Perception for Spatial-Sensitive Multi-Modality Image Fusion

## Quick Facts
- arXiv ID: 2510.12260
- Source URL: https://arxiv.org/abs/2510.12260
- Authors: Xiaopeng Liu; Yupei Lin; Sen Zhang; Xiao Wang; Yukai Shi; Liang Lin
- Reference count: 40
- AngularFuse introduces an angle-aware perceptual loss for unsupervised visible-infrared image fusion, achieving state-of-the-art performance on three datasets.

## Executive Summary
AngularFuse addresses the limitations of existing unsupervised visible-infrared image fusion methods, which struggle with detail loss and uneven brightness due to suboptimal loss functions. The core idea is to introduce an angle-aware perceptual loss that jointly constrains both gradient magnitude and direction, alongside a complementary masking strategy and fine-grained reference image synthesis. The complementary mask forces the network to learn cross-modal information, while the fine-grained reference combines Laplacian edge enhancement with histogram equalization to produce more detailed and balanced supervision. Comprehensive experiments on MSRS, RoadScene, and M3FD datasets show that AngularFuse outperforms existing methods with clear margins: 5% higher in EN, 6% in SD, 7% in SF, and 3% in AG on MSRS; similar improvements on other datasets. Visual results confirm sharper, more detailed fusion, especially in challenging scenes.

## Method Summary
AngularFuse is an unsupervised visible-infrared image fusion method that addresses detail loss and brightness imbalance through three key innovations: a Fine-Grained Reference Image Synthesis module, an Angle-aware Perceptual Loss, and a Complementary Mask (ComMask) data augmentation strategy. The Fine-Grained Reference combines Laplacian edge enhancement (75% weight) with histogram equalization (25% weight) to create detailed supervision. The Angle-aware Loss jointly constrains gradient magnitude and direction using cosine similarity between fused and reference gradients. ComMask randomly masks patches in complementary ways across the two input modalities to force the network to learn cross-modal information. The method is trained on 128x128 patches using Adam optimizer with cosine annealing for 70 epochs.

## Key Results
- AngularFuse achieves 5% higher Entropy (EN), 6% higher Standard Deviation (SD), 7% higher Spatial Frequency (SF), and 3% higher Average Gradient (AG) on the MSRS dataset compared to state-of-the-art methods
- On RoadScene dataset: 4% higher EN, 6% higher SD, 8% higher SF, and 4% higher AG than existing methods
- On M3FD dataset: 3% higher EN, 5% higher SD, 6% higher SF, and 2% higher AG, demonstrating consistent improvements across diverse datasets
- Visual results show sharper, more detailed fusion with better brightness balance, particularly in challenging scenes with extreme illumination differences

## Why This Works (Mechanism)
AngularFuse works by addressing the fundamental limitations of existing fusion methods through three complementary mechanisms. The angle-aware loss ensures that not only the strength but also the direction of gradients are preserved, preventing structural distortions that occur when only magnitude is considered. The complementary masking strategy forces the network to learn information from both modalities by making each input partially informative on its own. The fine-grained reference synthesis combines edge enhancement and histogram equalization to create supervision that preserves both structural details and global contrast, avoiding the detail loss that occurs with single-reference approaches.

## Foundational Learning
- **Angle-aware Perceptual Loss**: Why needed - Traditional fusion methods lose structural details by only matching gradient magnitudes, not directions. Quick check - Verify that cosine similarity between reference and fused gradients is properly implemented with epsilon to prevent division by zero.
- **Complementary Masking (ComMask)**: Why needed - Forces the network to learn cross-modal information rather than relying on one modality being fully informative. Quick check - Ensure random patches are correctly masked with complementary binary matrices across IR and visible inputs.
- **Fine-Grained Reference Synthesis**: Why needed - Single-reference approaches either lose details (intensity-only) or amplify noise (histogram-only). Quick check - Visualize the reference image to confirm it shows enhanced edges without excessive noise.
- **Gradient-based Loss Functions**: Why needed - Gradients capture edge and texture information critical for fusion quality assessment. Quick check - Verify Sobel operators correctly compute gradient magnitude and direction.
- **Unsupervised Training Strategy**: Why needed - Eliminates the need for paired ground truth fused images, which are expensive to obtain. Quick check - Confirm the reference image is constructed from the input modalities themselves, not external data.
- **Multi-scale Fusion Architecture**: Why needed - Different spatial scales contain different types of information (global structure vs. local details). Quick check - Verify the U-Net downsampling and upsampling stages correctly preserve spatial information.

## Architecture Onboarding
- **Component Map**: Input Image Pair → ComMask Augmentation → U-Net with Restormer-CNN Blocks → Fusion Output → Fine-Grained Reference Synthesis → AngularFuse Loss (Intensity + Gradient)
- **Critical Path**: The angle-aware loss calculation is the most critical component, as it directly addresses the core problem of structural preservation during fusion.
- **Design Tradeoffs**: The fixed 75/25 weighting between edge enhancement and histogram equalization in the reference synthesis provides stability but may not be optimal for all noise conditions. The Restormer-CNN architecture offers superior performance but adds implementation complexity compared to standard residual blocks.
- **Failure Signatures**: Exploding or NaN losses typically indicate division by zero in cosine similarity calculations, especially in flat image regions. Color imbalance or artifacts suggest incorrect weighting in the reference synthesis or aggressive histogram equalization.
- **First Experiments**:
  1. Implement and validate the Fine-Grained Reference and Angle-aware Loss modules independently using Sobel operators, ensuring gradient selection and cosine similarity calculations work correctly.
  2. Build and test the ComMask data loader to verify complementary masking is applied correctly to random patches in the input modalities.
  3. Train the U-Net architecture with the composite loss function on 128x128 crops for a small number of epochs to confirm the overall pipeline works before full training.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several important questions emerge from the limitations section regarding downstream task performance, adaptive weighting mechanisms, and generalization to other fusion domains.

## Limitations
- The exact architecture of the "Restormer-CNN" block is unspecified, creating ambiguity in reproducing the exact network structure and potentially affecting results.
- The size of the random patch k in the ComMask module is unspecified, though its impact is likely moderate given the randomness of the approach.
- Limited ablation studies on the specific contribution of each loss component (e.g., angle vs. magnitude) to overall performance make it difficult to quantify the relative importance of each innovation.

## Confidence
- **High Confidence**: The core algorithmic contributions (Angle-aware loss, ComMask, Fine-grained reference synthesis) are clearly specified and well-motivated with strong theoretical grounding.
- **Medium Confidence**: Performance claims are supported by extensive quantitative and qualitative results across three datasets, but minor variations in implementation (e.g., Restormer block) may affect exact numbers.
- **Low Confidence**: None identified.

## Next Checks
1. **Gradient Calculation Robustness**: Add a small epsilon (1e-8) to the denominator in cosine similarity to prevent NaN losses in flat regions, which is critical for stable training.
2. **Reference Image Validation**: Visualize I_ref during training to ensure it appears as a sharp, balanced fusion of the inputs, not noise or artifacts, confirming the reference synthesis works correctly.
3. **Ablation on Patch Size**: Test different sizes of the random patch k in ComMask to determine its impact on fusion quality and identify optimal values for different scenarios.