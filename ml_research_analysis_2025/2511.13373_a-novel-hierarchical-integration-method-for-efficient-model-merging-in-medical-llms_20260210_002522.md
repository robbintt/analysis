---
ver: rpa2
title: A Novel Hierarchical Integration Method for Efficient Model Merging in Medical
  LLMs
arxiv_id: '2511.13373'
source_url: https://arxiv.org/abs/2511.13373
tags:
- medical
- merging
- base
- task
- averaging
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a systematic comparison of six model merging
  techniques for combining medical LLMs derived from a common base model. The study
  evaluates simple averaging methods (Task Arithmetic, Linear Averaging) against complex
  approaches (DARE-TIES, DELLA, Breadcrumbs, and a novel Hierarchical Cosine-OT-LERP
  method) across five medical benchmarks.
---

# A Novel Hierarchical Integration Method for Efficient Model Merging in Medical LLMs

## Quick Facts
- **arXiv ID**: 2511.13373
- **Source URL**: https://arxiv.org/abs/2511.13373
- **Reference count**: 32
- **Primary result**: Task Arithmetic achieves 45.80% accuracy on MedQA, outperforming complex pruning-based approaches for compatible medical LLMs

## Executive Summary
This paper systematically compares six model merging techniques for combining medical LLMs derived from a common base model. The study evaluates simple averaging methods against complex approaches across five medical benchmarks, finding that architecturally compatible models benefit significantly from simple averaging. Task Arithmetic achieves state-of-the-art performance of 45.80% on MedQA while being 10-100x faster than pruning-based methods. The Hierarchical Cosine-OT-LERP method integrates task-vector similarity with selective attention head alignment but does not surpass simple averaging for compatible models.

## Method Summary
The study merges two medical LLMs (BioMistral and TachyHealth) derived from Mistral-7B-Instruct-v0.1 using six parameter-space merging techniques without additional training. Methods include Linear Average (α=0.5), Task Arithmetic (α=0.5), DARE-TIES (density=0.6), DELLA (density=0.6, ε=0.05), Breadcrumbs (density=0.9, γ=0.01), and a novel Hierarchical Cosine-OT-LERP method. Evaluation uses Language Model Evaluation Harness with 5-shot prompting on five benchmarks (MedQA, PubMedQA, MMLU Prof. Med., MedMCQA, HellaSwag) in bfloat16 precision on NVIDIA A100 40GB GPUs.

## Key Results
- Task Arithmetic achieves 45.80% accuracy on MedQA, outperforming all complex methods including DARE-TIES (36.45%) by +9.35 percentage points
- Simple averaging methods (Linear Average, Task Arithmetic) outperform pruning-based approaches for compatible medical LLMs
- Hierarchical Cosine-OT-LERP provides competitive performance but does not surpass simple averaging for compatible models
- Merging models from the same base with minimal parameter conflict enables efficient knowledge consolidation without complex conflict resolution

## Why This Works (Mechanism)

### Mechanism 1: Task Vector Arithmetic for Compatible Models
- **Claim**: When merging architecturally identical models fine-tuned from a common base, simple task vector addition outperforms complex conflict-resolution methods
- **Mechanism**: Task vectors (∆ = θ_finetuned - θ_base) capture domain-specific adaptations. When parent models share a base and occupy proximate loss basins, their task vectors are largely non-conflicting, allowing direct linear combination: θ_merged = θ_base + α∆_A + (1-α)∆_B
- **Core assumption**: Parent models reside in the same or adjacent loss basins with minimal parameter interference
- **Evidence anchors**: [abstract] Task Arithmetic achieving 45.80% accuracy on MedQA; [Section V.A] Both BioMistral and TachyHealth are fine-tuned versions of the same Mistral-7B-Instruct-v0.1 base; [corpus] Related work on checkpoint merging (arXiv:2504.18580) shows similar benefits for PEFT adapter merging

### Mechanism 2: Implicit Regularization via Weight Averaging
- **Claim**: Simple averaging provides implicit regularization that mitigates overfitting artifacts from individual training runs
- **Mechanism**: Averaging parameters from models trained on related domains smooths idiosyncratic noise while reinforcing shared beneficial adaptations. This produces a model that generalizes better than either parent alone
- **Core assumption**: Parent models exhibit complementary strengths and their overfitting patterns are uncorrelated
- **Evidence anchors**: [Section V.A] Simple averaging performs implicit regularization by combining beneficial adaptations while mitigating overfitting artifacts; [Section IV.C] Task Arithmetic improved MedQA accuracy by +2.83 points over the unmerged Mistral baseline

### Mechanism 3: Pruning-Based Methods Degrade High-Quality Fine-Tunes
- **Claim**: Aggressive parameter pruning (as in DARE-TIES with density=0.6) destroys subtle but critical medical knowledge
- **Mechanism**: In high-quality medical fine-tunes, small-magnitude delta parameters represent nuanced domain adjustments, not noise. Pruning 40% of these deltas removes valuable information
- **Core assumption**: Magnitude-based pruning thresholds appropriate for general NLP tasks are suboptimal for specialized medical models
- **Evidence anchors**: [Section IV.B] Task Arithmetic's MedQA score represents an improvement of +9.35 percentage points over Uniform DARE-TIES (36.45%); [Section IV.A] Preliminary experiments with higher density values (0.8-0.95) showed improved performance

## Foundational Learning

- **Concept: Task Vectors (Model Arithmetic)**
  - **Why needed here**: All merging methods except simple Linear Averaging operate on task vectors (θ_finetuned - θ_base). Understanding this abstraction is essential for interpreting why Task Arithmetic works
  - **Quick check question**: Given base model θ_base and two fine-tuned models θ_A and θ_B, can you write the formula for computing the merged model using Task Arithmetic?

- **Concept: Permutation Variance in Neural Networks**
  - **Why needed here**: The Hierarchical method specifically addresses permutation variance in attention heads via Optimal Transport alignment. Understanding this helps explain why the method was designed and when alignment matters
  - **Quick check question**: Why can't two functionally identical neural networks always be merged by direct weight averaging?

- **Concept: Loss Basin Geometry**
  - **Why needed here**: The paper's central finding—that simple averaging works for compatible models—relies on the assumption that fine-tuned models from the same base occupy proximate loss basins
  - **Quick check question**: What property of two models' positions in the loss landscape would suggest that simple parameter averaging might succeed?

## Architecture Onboarding

- **Component map**: Input: Two full-checkpoint medical LLMs (BioMistral, TachyHealth) + base model (Mistral-7B-Instruct-v0.1) → Merging layer: Six algorithms → Evaluation layer: Language Model Evaluation Harness with 5-shot prompting on five benchmarks → Output: Accuracy scores ± standard error

- **Critical path**: 1. Verify architectural compatibility (identical layer shapes, full checkpoints not LoRA adapters) 2. Compute task vectors for each parent relative to base 3. Apply merging algorithm with specified hyperparameters 4. Evaluate using standardized harness with consistent precision (bfloat16) and shot count (5-shot)

- **Design tradeoffs**:
  - **Simple methods (O(P))**: 10-100x faster, better performance for compatible models, no hyperparameter tuning needed
  - **Pruning methods (O(P log P))**: Higher compute, sensitive to density hyperparameter, may destroy valuable parameters
  - **Hierarchical method (O(L·H³ + P))**: Highest compute due to Hungarian algorithm for OT alignment, competitive but not superior for compatible models

- **Failure signatures**:
  - DARE-TIES with density=0.6 produces ~9 point MedQA degradation vs. Task Arithmetic
  - Attempting to merge PEFT/LoRA adapters with full-checkpoint methods causes configuration errors
  - Merging models from different architecture families creates incompatible weight matrices

- **First 3 experiments**:
  1. **Baseline comparison**: Run Linear Averaging (α=0.5) and Task Arithmetic on your compatible models. If these match or exceed parent performance, complex methods are likely unnecessary
  2. **Density sensitivity sweep**: If using DARE-TIES or DELLA, test density values [0.6, 0.8, 0.9, 0.95, 0.99] on a held-out validation set. Default 0.6 is likely too aggressive
  3. **Compatibility verification**: Before merging, confirm both parents are full checkpoints with identical architecture by comparing layer shapes and parameter counts. Attempting to merge incompatible formats will silently fail or produce degraded outputs

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Does the superiority of simple averaging methods generalize across different LLM architectures (e.g., Llama 2/3, Qwen) and scales (13B, 70B parameters)?
- **Basis in paper**: [explicit] The authors state in Section VI.B: "Scalability and generalization efforts should replicate findings across different architectures (Llama 2/3, Qwen) and scales (13B, 70B parameters)."
- **Why unresolved**: The study only evaluated Mistral-7B fine-tuned models; different architectures may have different parameter interference patterns or loss basin structures
- **What evidence would resolve it**: Systematic comparison of merging techniques across multiple architectures and parameter scales using the same evaluation protocol

### Open Question 2
- **Question**: What is the optimal density hyperparameter for pruning-based methods (DARE-TIES, DELLA) when merging high-quality, compatible medical fine-tunes?
- **Basis in paper**: [explicit] Section IV.A notes: "The significant underperformance of DARE-TIES suggests these defaults may be suboptimal for highly compatible medical models. Preliminary experiments with higher density values (0.8-0.95) showed improved performance."
- **Why unresolved**: Only default density (0.6) was systematically evaluated; higher density values showed promise but were not fully tested
- **What evidence would resolve it**: Controlled hyperparameter sweep testing densities from 0.6 to 1.0 on compatible medical models with performance metrics

### Open Question 3
- **Question**: How can safety objectives be explicitly incorporated into model merging to ensure merged medical LLMs maintain alignment with clinical safety requirements?
- **Basis in paper**: [explicit] Section VI.B calls for "safety-aware merging strategies [that] must incorporate safety objectives through alignment-aware loss functions."
- **Why unresolved**: Current merging methods operate purely on parameter arithmetic without any safety constraints or clinical alignment preservation mechanisms
- **What evidence would resolve it**: Development and evaluation of merging methods that include safety constraints, validated on adversarial medical scenarios and harmful query rejection rates

### Open Question 4
- **Question**: Do advanced merging techniques become necessary when combining models from truly disjoint medical specialties (e.g., radiology and genomics) or domains (medicine and finance)?
- **Basis in paper**: [explicit] Section VI.A states: "Merging truly disjoint domains (e.g., medicine and finance) would likely introduce significant parameter conflicts, potentially favoring advanced interference mitigation techniques." Section V.B also notes: "Future gains could be achieved by merging models from truly disjoint medical specialties...where advanced alignment strategies may become essential."
- **Why unresolved**: Both parent models in this study operated within the general medical domain, introducing minimal parameter conflict; the boundary where advanced methods become beneficial remains unknown
- **What evidence would resolve it**: Comparative study merging models fine-tuned on distinct specialties/domains, measuring when complex methods begin to outperform simple averaging

## Limitations
- Generalization to architecturally diverse models is limited, as results are derived from architecturally identical models fine-tuned from the same base
- Benchmark coverage is restricted to five medical benchmarks, potentially missing specialized performance characteristics in underrepresented medical subspecialties
- Pruning-based methods exhibit high sensitivity to density parameters that were not exhaustively explored across the full range

## Confidence
- **High confidence**: Task Arithmetic outperforms complex methods for architecturally compatible medical LLMs (45.80% vs 36.45% on MedQA), simple averaging provides implicit regularization benefits
- **Medium confidence**: Pruning-based methods (DARE-TIES, DELLA) degrade performance due to aggressive parameter removal in high-quality fine-tunes; Hierarchical method's design is sound but suboptimal for compatible models
- **Low confidence**: Hierarchical Cosine-OT-LERP would outperform simpler methods for incompatible architectures, optimal density values for pruning methods across different medical domains

## Next Checks
1. **Cross-architecture compatibility test**: Apply Task Arithmetic to merge BioMistral with a medical LLM from a different architecture family (e.g., Llama-2-based model) to validate whether simple averaging still outperforms complex methods when architectural compatibility is reduced
2. **Density hyperparameter sweep**: Systematically test DARE-TIES and DELLA across density values [0.6, 0.7, 0.8, 0.85, 0.9, 0.95, 0.99] on both MedQA and a held-out validation set to identify optimal pruning thresholds that preserve medical knowledge
3. **Extended benchmark validation**: Evaluate merged models on additional medical domain-specific benchmarks (e.g., clinical text understanding, medical dialogue, radiology report generation) to assess performance beyond multiple-choice question answering tasks