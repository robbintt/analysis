---
ver: rpa2
title: Enhancing Burmese News Classification with Kolmogorov-Arnold Network Head Fine-tuning
arxiv_id: '2511.21081'
source_url: https://arxiv.org/abs/2511.21081
tags:
- classification
- embeddings
- fourierkan
- fasterkan
- icientkan
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares different Kolmogorov-Arnold Network (KAN) variants
  as classification heads for Burmese news sentence classification, using static embeddings
  (TF-IDF, random, fastText) and contextual embeddings (mBERT, Distil-mBERT). The
  authors evaluate FourierKAN, EfficientKAN, and FasterKAN against a baseline MLP,
  training only the classification head while keeping the backbone frozen.
---

# Enhancing Burmese News Classification with Kolmogorov-Arnold Network Head Fine-tuning

## Quick Facts
- arXiv ID: 2511.21081
- Source URL: https://arxiv.org/abs/2511.21081
- Reference count: 29
- KAN-based classification heads achieve competitive F1-scores (0.917-0.928) compared to MLPs for Burmese news sentence classification.

## Executive Summary
This paper investigates Kolmogorov-Arnold Networks (KANs) as lightweight classification heads for Burmese news sentence classification, comparing FourierKAN, EfficientKAN, and FasterKAN variants against a standard MLP baseline. The study evaluates performance across five embedding types (TF-IDF, random, fastText, mBERT, Distil-mBERT) with frozen backbones and trainable classification heads. Results show EfficientKAN with fastText embeddings achieves the highest F1-score of 0.928, while FasterKAN provides the best speed-accuracy tradeoff. The findings demonstrate that KAN-based heads are expressive yet parameter-efficient alternatives to conventional MLPs, particularly effective for low-resource language classification tasks.

## Method Summary
The paper addresses Burmese news sentence classification (6 classes: Sports, Politics, Technology, Business, Entertainment, Environment) using a frozen-backbone, trainable-head approach. The dataset contains 7,315 sentences from VOA, BBC, and RFA sources, split 80/20 for training/testing. Five embedding types are evaluated: static embeddings (TF-IDF, random, fastText) and contextual embeddings (mBERT, Distil-mBERT). Three KAN variants are implemented as classification heads: FourierKAN (Fourier basis functions, grid=8), EfficientKAN (spline basis functions), and FasterKAN (grid-based with RSWAF activation). The baseline is a standard MLP. All models train only the classification head while keeping the backbone frozen, using AdamW optimizer (2e-5 for backbone, 2e-4 for head), cosine annealing scheduler, cross-entropy loss, and early stopping with patience=3. Training runs for 15 epochs with batch size 32 for static embeddings and 5 epochs with batch size 8 for transformers.

## Key Results
- EfficientKAN with fastText embeddings achieves the highest F1-score of 0.928, outperforming the MLP baseline (0.912).
- FasterKAN provides the best trade-off between speed and accuracy, matching MLP performance (0.917 F1) with mBERT embeddings.
- FourierKAN performs poorly on sparse TF-IDF features (0.538 F1) but achieves 0.791 F1 with EfficientKAN, suggesting basis function choice significantly impacts performance.
- All KAN variants demonstrate superior parameter efficiency compared to MLPs while maintaining competitive accuracy.

## Why This Works (Mechanism)
KAN-based classification heads leverage learnable activation functions through basis expansions (Fourier, spline, or RSWAF), providing more expressive power per parameter than fixed-activation MLPs. By freezing the backbone and training only the head, the approach focuses adaptation on the classification layer while preserving the learned semantic representations. The spline-based EfficientKAN and grid-based FasterKAN variants are particularly effective at modeling the non-linear decision boundaries in Burmese news classification, achieving competitive accuracy with significantly fewer parameters than MLPs.

## Foundational Learning
- **Kolmogorov-Arnold Networks (KANs)**: Neural networks that replace fixed activation functions with learnable univariate functions defined by basis expansions. Needed to understand the core innovation; quick check: verify KAN uses learnable basis functions rather than fixed activations like ReLU.
- **Basis Functions in KANs**: Mathematical functions (Fourier, spline, RSWAF) used to construct learnable activation functions. Needed to understand performance differences between variants; quick check: confirm FourierKAN uses Fourier basis, EfficientKAN uses spline basis, FasterKAN uses RSWAF.
- **Backbone Freezing**: Training strategy where the feature extraction layers are kept fixed while only the classification head is updated. Needed to understand the experimental setup; quick check: verify backbone weights are not updated during training.
- **Cross-Entropy Loss**: Standard loss function for multi-class classification that measures the difference between predicted and true class distributions. Needed for correct implementation; quick check: ensure loss is computed between softmax outputs and one-hot labels.
- **Weighted F1-Score**: Evaluation metric that accounts for class imbalance by weighting precision and recall according to class frequencies. Needed to understand the primary performance metric; quick check: verify F1 is computed with class weights matching the test set distribution.

## Architecture Onboarding

Component Map: Input Embeddings -> Frozen Backbone -> Classification Head (MLP/KAN) -> Output Probabilities

Critical Path: Embedding extraction (backbone) → Classification head training → F1-score evaluation

Design Tradeoffs:
- KAN variants provide better parameter efficiency but may require more complex implementation compared to MLPs
- FourierKAN shows poor performance on sparse features but competitive results on dense embeddings
- Backbone freezing reduces training time but may limit adaptation to downstream task

Failure Signatures:
- FourierKAN severely underperforms on sparse TF-IDF embeddings (F1 gap >0.15 vs MLP) - expected behavior
- Burmese Unicode misordering causes tokenization errors - verify normalization before training
- Overfitting with small heads - monitor train/val gap, increase dropout or reduce head capacity

First Experiments:
1. Train EfficientKAN with fastText embeddings on the 80% training split, evaluate on 20% test set
2. Compare parameter counts and FLOPs between EfficientKAN and MLP baselines
3. Test FasterKAN with mBERT embeddings to verify the 0.917 F1 performance claim

## Open Questions the Paper Calls Out
- How do KAN-based classification heads perform on other low-resource languages and diverse NLP tasks beyond Burmese news classification?
- Can the development of novel basis functions improve the stability and performance of KAN heads, particularly for sparse data?
- Does unfreezing the transformer backbone during full fine-tuning negate the efficiency advantages of KAN heads compared to MLPs?
- What is the root cause of FourierKAN's significant performance degradation on sparse TF-IDF embeddings compared to other variants?

## Limitations
- Dataset not publicly available, creating a fundamental barrier to independent validation
- Critical architectural details missing, particularly classification head layer dimensions and KAN-specific parameters
- Limited to Burmese news classification only, with generalization to other languages and tasks unproven
- Validation strategy gaps, including unspecified validation split ratio and sampling methodology

## Confidence
- **High Confidence**: KAN-based classification heads achieve competitive F1-scores (0.917-0.928) compared to MLP baselines, with clear parameter efficiency advantages. The comparative ranking (EfficientKAN > FasterKAN > FourierKAN) appears reliable.
- **Medium Confidence**: Specific performance claims (e.g., 0.928 F1 with EfficientKAN+fastText) depend heavily on dataset-specific factors and missing hyperparameters, making precise reproduction uncertain.
- **Low Confidence**: Claims about generalization to other low-resource languages and specific recommendations for production deployment are not empirically supported in this work.

## Next Checks
1. Obtain or reconstruct the Burmese news classification dataset with 7,315 sentences and 6-class distribution, verifying syllable-based Unicode normalization and myWord tokenization
2. Implement classification heads with confirmed layer dimensions and KAN-specific parameters, comparing parameter counts and FLOPs against MLP baselines
3. Train complete pipeline (frozen backbone + KAN head) following specified hyperparameters, measuring whether EfficientKAN with fastText achieves F1-scores within ±0.02 of reported 0.928 on held-out test set