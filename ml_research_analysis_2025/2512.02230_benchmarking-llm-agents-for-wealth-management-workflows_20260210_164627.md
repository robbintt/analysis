---
ver: rpa2
title: Benchmarking LLM Agents for Wealth-Management Workflows
arxiv_id: '2512.02230'
source_url: https://arxiv.org/abs/2512.02230
tags:
- tasks
- task
- agent
- owncloud
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This dissertation introduces a finance-focused agent evaluation
  benchmark within TheAgentCompany framework. It constructs 12 paired wealth-management
  tasks spanning retrieval, analysis, and synthesis/communication, with explicit acceptance
  criteria and deterministic graders.
---

# Benchmarking LLM Agents for Wealth-Management Workflows

## Quick Facts
- **arXiv ID:** 2512.02230
- **Source URL:** https://arxiv.org/abs/2512.02230
- **Reference count:** 0
- **One-line primary result:** LLM agents resolve 0/12 legacy TAC tasks but 2/12 new high-autonomy tasks, with low-autonomy prompts raising checkpoint accuracy from 49% to 69%.

## Executive Summary
This dissertation introduces a finance-focused agent evaluation benchmark within TheAgentCompany framework. It constructs 12 paired wealth-management tasks spanning retrieval, analysis, and synthesis/communication, with explicit acceptance criteria and deterministic graders. The study seeds realistic domain data across EspoCRM, OwnCloud, Rocket.Chat, and Plane, and introduces high- vs. low-autonomy variants to test the effect of prompt granularity. Experiments with GPT-4o show agents resolve 0/12 legacy TAC tasks but 2/12 new high-autonomy tasks, raising checkpoint accuracy from 15% to 49%. Low-autonomy prompts improve accuracy further (49% → 69%) with near-identical step-counts and lower cost. Qualitative analysis reveals most failures stem from authentication, WebDAV uploads, and messaging, rather than computation. The suite demonstrates that improved task design and deterministic grading meaningfully raise measured agent competence, while cross-system synthesis remains the primary bottleneck.

## Method Summary
The benchmark uses the OpenHands agent runtime within TheAgentCompany framework, evaluating GPT-4o on 12 paired wealth-management tasks (high vs. low autonomy). The environment integrates EspoCRM (client records), OwnCloud (file storage via WebDAV), Rocket.Chat (messaging), and Plane (issue tracking). Tasks span retrieval, analysis, and synthesis/communication across these systems. Evaluation uses deterministic graders that check file existence, schema conformance, and numeric correctness with normalization (currency symbol stripping, row order irrelevance). 28 synthetic artefacts (CSV, TXT, PDF) are seeded into the environment. The study measures % checkpoints passed, task resolution rate, mean step-count, and API cost.

## Key Results
- Agents resolve 0/12 legacy TAC tasks but 2/12 new high-autonomy tasks
- Checkpoint accuracy improves from 15% to 49% with task redesign
- Low-autonomy prompts raise accuracy from 49% to 69% with near-identical step-counts and lower cost
- Most failures stem from authentication, WebDAV uploads, and messaging rather than computation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Granular checkpointed evaluation reveals agent competence that coarse all-or-nothing grading obscures.
- Mechanism: Decomposing tasks into 3–5 smaller checkpoints localizes failures (access vs. computation vs. delivery) and credits partial progress, improving measured accuracy without changing task difficulty.
- Core assumption: Failures are localized to specific sub-steps rather than uniformly distributed; evaluators can reliably detect each sub-step's outcome.
- Evidence anchors: [abstract] "explicit acceptance criteria and deterministic graders"; [section 3.5.4] "finer granularity (3–5 checks per task with small point weights, allowing more effective analysis of what sub-steps of the task the agent may have struggled with)"

### Mechanism 2
- Claim: Reducing autonomy via schema/path specificity constrains exploration without inflating search depth.
- Mechanism: Low-autonomy prompts explicitly name sources, paths, schemas, and delivery targets, reducing tool-wandering and fabrication while preserving the underlying reasoning task.
- Core assumption: The agent's core limitation is navigation/provenance, not reasoning; constraining the search space does not remove the substantive challenge.
- Evidence anchors: [abstract] "Low-autonomy prompts improve accuracy further (49% → 69%) with near-identical step-counts and lower cost"; [section 4.2.1] "low-autonomy prompts resolve more tasks (4/12 vs. 2/12) and raise mean checkpoints passed from 49% to 69%, while adding only 1.5 steps per run on average"

### Mechanism 3
- Claim: Deterministic grading with normalization (not byte-exact matching) reduces false negatives while maintaining rigor.
- Mechanism: Evaluators normalize CSVs (strip currency symbols, ignore row order), use tolerances for floats, and enforce schema/column requirements—avoiding brittle failures while rejecting incorrect outputs.
- Core assumption: Benign formatting differences (header case, row order) are orthogonal to correctness; the ground truth can be programmatically recomputed.
- Evidence anchors: [section 3.5.5] "normalise numeric inputs (e.g., strip currency symbols, commas, whitespace) before recomputation; compare value columns against a recomputed ground truth, treating row order as irrelevant"; [section 5.1] "strict but tolerant evaluators (schema/format exactness with numeric/CSV normalisation) avoid both false positives and brittle byte-match failures"

## Foundational Learning

- Concept: **Checkpoint-based agent evaluation**
  - Why needed here: The benchmark decomposes multi-step workflows into verifiable sub-tasks, enabling partial credit and failure localization.
  - Quick check question: Can you explain why a task that passes 3/5 checkpoints provides more diagnostic information than a binary pass/fail?

- Concept: **Autonomy spectrum in agent prompts**
  - Why needed here: High-autonomy (brief) vs. low-autonomy (schema-explicit) prompts test different capabilities—exploration vs. reliable execution under constraints.
  - Quick check question: What type of failure (access, computation, or delivery) would be most affected by switching from high to low autonomy?

- Concept: **WebDAV and CRM integration patterns**
  - Why needed here: The environment uses OwnCloud (WebDAV) and EspoCRM as primary data stores; understanding their APIs is prerequisite to debugging agent failures.
  - Quick check question: Why might an agent successfully compute a result but still fail a checkpoint that requires uploading to OwnCloud?

## Architecture Onboarding

- Component map: OpenHands agent runtime orchestrates OwnCloud (file storage via WebDAV), EspoCRM (client records), Rocket.Chat (messaging), and Plane (issue tracking). Sotopia provides simulated colleagues. Evaluators run post-hoc, querying service state via APIs—not agent trajectory alone.

- Critical path: Task prompt → agent retrieves inputs (OwnCloud/EspoCRM) → performs computation → delivers output (file upload or message) → evaluator checks each checkpoint against service state and recomputed ground truth.

- Design tradeoffs: Local file mirrors improve robustness to auth failures but reduce ecological validity; deterministic graders ensure reproducibility but require explicit schema specification; LLM-as-judge is limited to yes/no chat predicates to avoid subjectivity.

- Failure signatures:
  - E1 (access/auth): 404/405 errors, login pages—agent queries wrong endpoints.
  - E2 (delivery): Computation correct but PUT/WebDAV fails or uploads to wrong path.
  - E3 (cross-tool): Rocket.Chat DMs not sent or Plane issues not updated despite correct logic.
  - E4 (logic): Band-capping or double-count errors when data is accessible.

- First 3 experiments:
  1. Run one Level 1 task (e.g., Net Worth Snapshot) under both high and low autonomy; observe where checkpoints fail and compare step counts.
  2. Inspect the evaluator script for a Level 2 task (e.g., Capital Gains) to understand normalization logic and tolerance thresholds.
  3. Intentionally trigger E1 by removing a local file mirror; compare checkpoint scores with and without local access to isolate the access vs. computation contribution.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How robust are LLM agents to realistic data perturbations (header case variations, row reordering, alternate but equivalent schemas, or misplaced files) in wealth-management tasks?
- Basis in paper: [explicit] Section 5.2 proposes "counterfactual robustness" testing with perturbations including "header case, benign row reorders, tiny numeric noise, alternate but equivalent schemas, and not in the specified location" to improve realism.
- Why unresolved: The current benchmark provides data in canonical formats and locations; no perturbation experiments were conducted.
- What evidence would resolve it: Run the same task suite with systematically perturbed inputs and measure accuracy degradation patterns.

### Open Question 2
- Question: Can multi-agent architectures (e.g., planner–executor with handoffs) improve end-to-end completion rates on cross-system synthesis tasks?
- Basis in paper: [explicit] Section 5.2 identifies "multi-agent collaboration" as a future direction, specifically "planner–executor with handoffs in Plane/Rocket.Chat."
- Why unresolved: All experiments used a single agent configuration; no multi-agent coordination was tested.
- What evidence would resolve it: Compare single-agent vs. multi-agent performance on Level 3 tasks requiring cross-tool coordination.

### Open Question 3
- Question: To what extent do the observed failure patterns (authentication, WebDAV uploads, messaging) generalize across different LLM backends beyond GPT-4o?
- Basis in paper: [inferred] Section 5.1 acknowledges limitation of "primary evaluation on a single agent configuration" and notes failures cluster around access and delivery.
- Why unresolved: Only GPT-4o was evaluated; alternative models may handle tool integration differently.
- What evidence would resolve it: Replicate experiments with Claude, Gemini, and open-source models; compare failure mode distributions.

### Open Question 4
- Question: Does the high- vs. low-autonomy accuracy gap persist when authentication and tool-access barriers are removed?
- Basis in paper: [inferred] Section 4.2.1 shows low autonomy improves Level 2 tasks but not Level 3, where "external constraints (authentication, cross-tool delivery) rather than schema inference limit performance."
- Why unresolved: The interaction between prompt granularity and tool-access friction remains confounded.
- What evidence would resolve it: Run autonomy comparison with guaranteed service authentication to isolate pure reasoning effects.

## Limitations

- The evaluation environment's data seeding process is underspecified, with only 28 synthetic artefacts described but no explicit scripts for generating or loading them.
- The OpenHands agent configuration and exact prompt templates for all 24 tasks (12 high-autonomy + 12 low-autonomy variants) are not provided, making exact reproduction difficult.
- The deterministic graders' source code and NPC colleague definitions are referenced but not included.

## Confidence

- **High confidence**: Task design (3-level progression from retrieval to synthesis) and measured improvements (0→2 tasks resolved, 15%→69% checkpoint accuracy) are well-supported by the experimental results section.
- **Medium confidence**: The claim that low-autonomy prompts reduce cost and step-count while improving accuracy is supported by specific metrics but relies on a single LLM model (GPT-4o) without ablation studies across different models.
- **Medium confidence**: The assertion that most failures stem from authentication, WebDAV uploads, and messaging rather than computation is based on qualitative analysis but lacks systematic error categorization across all runs.
- **Low confidence**: The generalizability of the benchmark to other financial domains or non-financial domains is not demonstrated or discussed.

## Next Checks

1. **Reproduce the authentication failure pattern**: Run a single Level 1 task (e.g., Net Worth Snapshot) with high autonomy, deliberately remove a local file mirror, and verify that checkpoint scores drop due to access failures rather than computational errors.
2. **Test normalization robustness**: Implement the evaluator's CSV normalization logic (strip currency symbols, ignore row order) and verify that it correctly accepts a reformatted but semantically equivalent answer while rejecting a minor numeric error.
3. **Compare autonomy variants systematically**: Execute all 12 task pairs with both autonomy levels using the same environment and LLM, then verify that low-autonomy variants consistently achieve higher checkpoint accuracy with similar or fewer steps.