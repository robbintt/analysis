---
ver: rpa2
title: 'MoTE: Mixture of Ternary Experts for Memory-efficient Large Multimodal Models'
arxiv_id: '2506.14435'
source_url: https://arxiv.org/abs/2506.14435
tags:
- expert
- mote
- experts
- image
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MoTE introduces a scalable and memory-efficient approach for training
  multimodal Mixture-of-Ternary-Experts models by leveraging ternary quantization
  of routed experts while preserving a full-precision shared expert. This design addresses
  the memory inefficiency of large multimodal MoE models, which pose deployment challenges
  on edge devices.
---

# MoTE: Mixture of Ternary Experts for Memory-efficient Large Multimodal Models

## Quick Facts
- arXiv ID: 2506.14435
- Source URL: https://arxiv.org/abs/2506.14435
- Reference count: 40
- Primary result: MoTE achieves comparable performance to full-precision MoE-LLaVA on image understanding tasks while using ternary-quantized routed experts.

## Executive Summary
MoTE introduces a scalable and memory-efficient approach for training multimodal Mixture-of-Ternary-Experts models by leveraging ternary quantization of routed experts while preserving a full-precision shared expert. This design addresses the memory inefficiency of large multimodal MoE models, which pose deployment challenges on edge devices. Extensive experiments show that MoTE achieves comparable performance to full-precision baseline MoE-LLaVA on image understanding tasks when up-cycled from models larger than 1.5 billion parameters. Notably, when combined with post-training quantization, MoTE outperforms MoE-LLaVA by 4.3% average accuracy on end tasks under the same expert memory budget of 3.4GB, demonstrating its effectiveness for memory-constrained devices.

## Method Summary
MoTE trains memory-efficient multimodal models through a three-stage process. First, it trains a connector MLP to align vision and language modalities. Second, it fine-tunes the LLM and connector using a mixture of datasets. Third, it performs up-cycling by replacing FFN layers with MoE layers containing one shared BF16 expert (frozen, initialized from pre-trained FFN) and four routed ternary experts (trainable). The ternary weights are quantized to {-1, 0, 1} using absmean quantization, while activations use INT8 with per-token absmax quantization. The model uses Top-1 gating for routed experts plus the always-active shared expert, maintaining computational parity with Top-2 baseline MoEs.

## Key Results
- MoTE achieves comparable performance to full-precision MoE-LLaVA on image understanding tasks when up-cycled from models larger than 1.5 billion parameters
- When combined with post-training quantization, MoTE outperforms MoE-LLaVA by 4.3% average accuracy on end tasks under the same expert memory budget of 3.4GB
- The performance gap between MoTE and full-precision baselines closes as model scale increases, with significant degradation observed below 1.5B parameters

## Why This Works (Mechanism)

### Mechanism 1
The architecture preserves pre-trained knowledge by maintaining a full-precision "shared expert" alongside low-precision routed experts. The paper hypothesizes that the FFN encodes substantial factual knowledge that is disrupted by ternary quantization, so retaining the original FFN as a full-precision, always-active shared expert anchors the model to the dense baseline's capabilities while ternary experts provide additional capacity.

### Mechanism 2
Under strict memory constraints, training a larger number of low-precision (ternary) experts yields superior performance compared to training fewer high-precision experts. By constraining routed experts to ternary values (approximately 1.58 bits), MoTE decouples parameter count from memory usage, allowing the model to scale expert count significantly within a fixed memory budget.

### Mechanism 3
Directly training experts in ternary precision (Quantization-Aware Training) is more effective than initializing full-precision and fine-tuning down. The paper finds that initializing full-precision experts and then fine-tuning them to ternary yields worse results than training in ternary from the start, suggesting the optimization landscape for ternary weights is distinct.

## Foundational Learning

- **Sparse Up-cycling (MoE)**: Understanding how MoE models replace dense FFNs with router+experts layers to scale parameters without increasing FLOPs is essential. Quick check: How does MoTE modify the standard "replace FFN with routed experts" approach to handle quantization noise?

- **Quantization-Aware Training (QAT) & STE**: MoTE learns ternary weights during training using the Straight-Through Estimator to handle non-differentiable rounding operations. Quick check: In MoTE, are the ternary weights stored in high precision during training and rounded on-the-fly, or are they permanently stored as {-1, 0, 1}?

- **Scaling Laws for Precision**: Low-bit representations generally require higher model width/dimensions to maintain expressivity. Quick check: Why might a small model (0.5B) suffer more from ternary constraints than a large model (3B), even with the same architecture?

## Architecture Onboarding

- **Component map**: Input (SigLIP Vision Encoder + Qwen2.5 LLM Backbone) → MoTE Block (MSA - Frozen + Shared Expert (BF16, Frozen) + Router + 4 Ternary Experts (INT8 activations)) → Output (Logits)

- **Critical path**: Stage I & II train connector and dense LLM normally. Stage III (up-cycling): Duplicate FFN weights 4 times to initialize ternary experts, freeze original FFN as shared expert, train only router and ternary experts using QAT with Top-1 gating.

- **Design tradeoffs**: MoTE uses Top-1 (routed) + 1 (shared) gating to match FLOPs with Top-2 baselines. The shared expert must remain in BF16 for performance; quantizing it to ternary causes >7% accuracy drop.

- **Failure signatures**: Catastrophic forgetting occurs if shared expert/attention layers are unfrozen during Stage III. Expert collapse happens with improper load balancing loss tuning. Underfitting is expected below 1B scale due to insufficient ternary representational capacity.

- **First 3 experiments**: 1) Ablate shared expert precision by re-running 1.5B up-cycling with INT8 or ternary quantization. 2) Verify FLOP equivalence by profiling MoTE vs. MoE-LLaVA inference. 3) Scale check by training 0.5B and 3B MoTE models to verify performance gap closure at higher parameter counts.

## Open Questions the Paper Calls Out

- **Open Question 1**: The paper does not provide a theoretical explanation for why ternary up-cycling can match the performance of its full-precision counterpart, leaving this investigation as future work.

- **Open Question 2**: The experiments consistently use a fixed, small number of routed experts (4), but it's unclear if the performance benefits hold when significantly increasing the number of experts (e.g., to 16 or 64).

- **Open Question 3**: While the paper shows that quantizing the shared expert to ternary causes significant accuracy drops, it does not propose a method to successfully quantize it, limiting the theoretical maximum memory reduction.

## Limitations

- MoTE's effectiveness is strongly dependent on model scale, showing significant performance degradation below 1.5B parameters
- The necessity of a full-precision shared expert limits the theoretical maximum memory reduction
- The paper lacks theoretical justification for the observed scaling threshold and training dynamics

## Confidence

**High Confidence**: The core claim that MoTE achieves comparable performance to full-precision MoE-LLaVA on image understanding tasks at larger scales (>1.5B parameters) is well-supported by experimental results across multiple benchmarks.

**Medium Confidence**: The claim that MoTE outperforms MoE-LLaVA by 4.3% average accuracy when combined with post-training quantization is supported by results, but generalizability to other model architectures requires further validation.

**Low Confidence**: The claim about the superiority of direct ternary training over fine-tuning from full-precision lacks sufficient theoretical grounding and ablation studies to support the mechanism hypothesis.

## Next Checks

1. **Initialization Ablation Study**: Systematically compare FFN initialization against random initialization, zero initialization, and alternative pre-trained weights for the ternary experts to quantify the true contribution of the initialization strategy.

2. **Shared Expert Precision Sensitivity**: Conduct a comprehensive study varying the shared expert precision (BF16, FP16, INT8) and its training status (frozen vs. trainable) to establish the exact precision threshold where performance degradation begins.

3. **Scaling Law Verification**: Train MoTE models at 0.5B, 1.5B, and 3B scales using identical hyperparameters and datasets to empirically verify the claimed scaling trend where performance gaps close at higher parameter counts.