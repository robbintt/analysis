---
ver: rpa2
title: On the Contribution of Lexical Features to Speech Emotion Recognition
arxiv_id: '2509.05634'
source_url: https://arxiv.org/abs/2509.05634
tags:
- speech
- emotion
- recognition
- lexical
- acoustic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the contribution of lexical features to
  speech emotion recognition, challenging the assumption that paralinguistic cues
  are the primary drivers of emotion detection. The authors extract lexical content
  from speech using automatic speech recognition and analyze its performance compared
  to acoustic features alone.
---

# On the Contribution of Lexical Features to Speech Emotion Recognition

## Quick Facts
- arXiv ID: 2509.05634
- Source URL: https://arxiv.org/abs/2509.05634
- Reference count: 27
- Primary result: Lexical features achieve 51.5% weighted F1-score on MELD, outperforming acoustic-only at 49.3%

## Executive Summary
This paper challenges the assumption that paralinguistic cues are primary drivers of speech emotion recognition by demonstrating that lexical content extracted via ASR can match or exceed acoustic-only performance. The authors systematically compare self-supervised speech and text models, conduct layer-wise analysis of transformer encoders, and evaluate the impact of audio denoising. On the MELD dataset, the lexical-based approach achieved a weighted F1-score of 51.5%, outperforming the acoustic-only pipeline which reached 49.3%. The study found that intermediate transformer layers captured more emotionally relevant information than final layers, and that denoising audio degraded performance across both modalities, suggesting paralinguistic nuances are crucial for emotion recognition.

## Method Summary
The study uses frozen self-supervised learning models as feature extractors: wav2vec2-xls-r-2b for acoustic features and DeBERTa for lexical features derived from Whisper-large-v3 ASR transcripts. A simple 3-hidden-layer MLP classifier with average pooling over time/tokens is trained for 500 epochs using Adam optimizer (lr=3e-5). The authors conduct systematic layer-wise analysis to identify optimal transformer layers (Layer 26 for wav2vec2-xls-r-2b, Layer 19 for DeBERTa) and evaluate the effect of audio denoising using DEMUCS. The MELD dataset with 7 emotion classes (anger, disgust, fear, joy, neutral, sadness, surprise) serves as the evaluation benchmark.

## Key Results
- Lexical-based approach achieves 51.5% weighted F1-score on MELD test set vs 49.3% for acoustic-only pipeline
- Intermediate transformer layers capture more emotionally relevant information than final layers
- Audio denoising with DEMUCS degrades performance across both modalities
- Using manual transcripts instead of ASR output yields 60.9% WF1 (9.4-point improvement)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Lexical content extracted via ASR encodes emotionally relevant semantic information that can match or exceed acoustic-only emotion recognition performance.
- **Mechanism:** Self-supervised text encoders map transcribed speech to embeddings where emotional semantics are linearly separable; a simple MLP suffices because SSL representations already cluster by emotional content.
- **Core assumption:** ASR transcription quality is sufficiently high to preserve emotionally salient words and phrasing.
- **Evidence anchors:**
  - [abstract]: "lexical-based approach obtains a weighted F1-score (WF1) of 51.5%, compared to 49.3% for an acoustic-only pipeline"
  - [section IV.B]: "when using the manual transcriptions, the WF1 score is 60.9%"
  - [corpus]: Related work (arXiv:2510.10444) investigates lexical vs. acoustic cue reliance in audio LLMs, suggesting ongoing debate about modality contributions; direct corpus support for this specific ASR→NLP mechanism is limited.
- **Break condition:** If ASR word error rate exceeds ~15–20% on emotional vocabulary, or if target emotions rely primarily on tone rather than word choice (e.g., sarcasm), this mechanism degrades.

### Mechanism 2
- **Claim:** Intermediate transformer layers capture more emotionally relevant information than final layers for both speech and text SSL models.
- **Mechanism:** Early-to-middle layers retain phonetic/prosodic detail (speech) or lexical semantics (text) before subsequent layers specialize toward pretraining objectives that may dilute task-specific signal.
- **Core assumption:** Emotion-relevant features form a linearly separable subspace accessible without task-specific fine-tuning.
- **Evidence anchors:**
  - [abstract]: "intermediate layers of transformer models captured more emotionally relevant information than final layers"
  - [section IV.A]: "mid-level representations of the wav2vec2-xls-r-2b model achieve the best performance, yielding a 48.9% WF1"
  - [corpus]: Related work on discrete speech tokens (arXiv:2601.17085) notes paralinguistic information loss during quantization, aligning with layer-wise feature dilution observations.
- **Break condition:** If downstream classifiers require highly abstracted representations, or if pretraining objectives change substantially, optimal layer depth may shift.

### Mechanism 3
- **Claim:** Aggressive audio denoising removes paralinguistic cues (e.g., breathiness, tremor, micro-prosody) that are informative for emotion recognition.
- **Mechanism:** Denoising models like DEMUCS, trained for speech enhancement or music separation, suppress non-lexical vocal artifacts that correlate with affective states.
- **Core assumption:** Noise and paralinguistic vocalizations share spectral/temporal characteristics that denoisers cannot disentangle.
- **Evidence anchors:**
  - [abstract]: "denoising audio degraded performance across both modalities"
  - [section IV.A]: "aggressive denoising... appears to remove not only noise but also subtle non-verbal vocalizations that are crucial for emotion recognition"
  - [corpus]: arXiv:2510.25577 examines voice quality variation in speech foundation models, noting that paralinguistic features are often underrepresented; direct corpus validation of denoising-specific degradation is weak.
- **Break condition:** If denoising is trained with emotion-preserving objectives, or if input audio is studio-quality with minimal background noise, this negative effect may not manifest.

## Foundational Learning

- **Concept: Self-supervised representations (frozen feature extraction)**
  - **Why needed here:** The entire methodology depends on using pretrained wav2vec2, BERT, and DeBERTa as fixed feature extractors rather than fine-tuning them.
  - **Quick check question:** Can you explain why average-pooling frozen embeddings might underperform compared to fine-tuning on a small emotion dataset?

- **Concept: Layer-wise feature selection**
  - **Why needed here:** Performance varies by transformer layer; practitioners must know to probe multiple layers rather than defaulting to the final hidden state.
  - **Quick check question:** Given a 24-layer DeBERTa, how would you efficiently identify the best layer for a new SER dataset without training 24 separate classifiers?

- **Concept: ASR error propagation in downstream tasks**
  - **Why needed here:** The 9.4-point gap between ASR transcripts (51.5%) and manual transcripts (60.9%) highlights ASR quality as a bottleneck.
  - **Quick check question:** If your ASR mis-transcribes "I hate this" as "I ate this," what emotional signal is lost, and how might you detect such errors systematically?

## Architecture Onboarding

- **Component map:**
  Audio Input
      ├─► [Acoustic Path] → wav2vec2-xls-r-2b (frozen) → Layer N selection → Avg Pool → MLP → Emotion
      └─► [Lexical Path] → Whisper-large-v3 (ASR) → DeBERTa (frozen) → Layer N selection → Avg Pool → MLP → Emotion

- **Critical path:** Layer selection is the highest-leverage hyperparameter; Table I and Table II show 5–10 point WF1 variance across layers. Use the development set to sweep layers before committing to test evaluation.

- **Design tradeoffs:**
  - Acoustic path: Higher parameter count, captures prosody, sensitive to noise, no ASR dependency.
  - Lexical path: Simpler classifier, depends on ASR quality, ignores tone, more robust to audio artifacts.
  - Assumption: The paper uses frozen SSL models; fine-tuning could narrow or reverse the observed gap but increases computational cost and overfitting risk on small datasets.

- **Failure signatures:**
  - Final-layer features underperforming mid-layers by >5 WF1 points.
  - Denoised audio consistently degrading results across models.
  - ASR transcripts with high word error rate on emotional keywords causing lexical path collapse.

- **First 3 experiments:**
  1. Reproduce the layer-wise sweep on MELD dev set for both wav2vec2-xls-r-2b and DeBERTa; confirm peak layers align with reported values (Layer 26 and Layer 19).
  2. Compare ASR-generated vs. manual transcripts using the same DeBERTa + MLP setup to quantify ASR error impact on your deployment data.
  3. Apply DEMUCS denoising to a held-out noisy subset; verify the degradation pattern before committing to preprocessing pipelines in production.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can speech enhancement models be specifically tailored to preserve paralinguistic cues while removing noise, thereby improving rather than degrading SER performance?
- Basis in paper: [inferred] The study found that applying the DEMUCS denoising network consistently degraded performance across both acoustic and lexical modalities, likely because it removed subtle non-verbal vocalizations crucial for emotion detection.
- Why unresolved: Standard enhancement models like DEMUCS prioritize speech intelligibility (for ASR) or music separation, often flattening the prosodic nuances required for emotion recognition.
- What evidence would resolve it: Developing a noise suppression model trained explicitly to maximize emotion classification metrics rather than signal-to-noise ratio, resulting in higher Weighted F1-scores on the MELD dataset.

### Open Question 2
- Question: To what extent can specialized ASR error correction mechanisms bridge the performance gap between automatic and manual transcriptions in lexical-based SER?
- Basis in paper: [explicit] The authors note that using manual transcriptions yielded 60.9% WF1 compared to 51.5% for ASR-derived text, explicitly identifying "ASR error correction" as a necessary area for future research.
- Why unresolved: Standard ASR models may misinterpret emotionally charged prosody as lexical errors or fail to capture disfluencies that signal specific emotional states.
- What evidence would resolve it: Integrating an ASR post-processing module that corrects transcripts using emotional context, demonstrating a significant reduction in the performance gap with the manual transcription baseline.

### Open Question 3
- Question: How does a multimodal architecture with trainable modality-specific weights quantitatively determine the relative importance of acoustic versus lexical features?
- Basis in paper: [explicit] The conclusion states a future direction is creating "a multi-modal architecture with trainable modality-specific weights, allowing the model to learn how much to rely on acoustic versus lexical features."
- Why unresolved: The current study evaluates modalities in isolation (unimodal), leaving the optimal fusion strategy and the dynamic contribution of each modality unexplored.
- What evidence would resolve it: A fused model architecture that outputs attention weights or gating values, showing a dynamic shift in reliance between text and audio depending on the utterance content.

## Limitations
- 9.4-point performance gap between ASR and manual transcripts suggests sensitivity to transcription quality
- wav2vec2-xls-r-2b model (2 billion parameters) creates significant computational barriers for practical deployment
- MELD dataset focuses on acted dialogue emotions, which may not reflect natural emotional expression patterns

## Confidence
**High Confidence**: The core finding that intermediate transformer layers outperform final layers for emotion-relevant features (48.9% WF1 at Layer 26 for wav2vec2-xls-r-2b) is well-supported by the systematic layer-wise analysis presented in Table I and Table II.

**Medium Confidence**: The claim about denoising degrading performance is supported by experimental results, but the mechanism remains incompletely explained.

**Low Confidence**: The broader implication that speech emotion recognition should be treated primarily as an NLP task requires more validation across diverse datasets and emotional categories.

## Next Checks
1. Cross-dataset validation: Replicate the lexical vs acoustic comparison on at least two additional emotion recognition datasets (e.g., IEMOCAP, CREMA-D) to test generalizability beyond MELD's acted dialogue context.

2. ASR robustness analysis: Systematically measure WF1 performance degradation across different ASR word error rates (10%, 20%, 30%) using synthetic errors injected into ground-truth transcripts to quantify the practical limits of the lexical approach.

3. Layer transferability test: Evaluate whether the optimal layer selections (Layer 26 for wav2vec2-xls-r-2b, Layer 19 for DeBERTa) transfer to other SSL models or if they're specific to the model architectures used in this study.