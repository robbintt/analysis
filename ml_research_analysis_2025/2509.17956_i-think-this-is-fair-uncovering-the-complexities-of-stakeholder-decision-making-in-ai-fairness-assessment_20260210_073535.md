---
ver: rpa2
title: '"I think this is fair'''': Uncovering the Complexities of Stakeholder Decision-Making
  in AI Fairness Assessment'
arxiv_id: '2509.17956'
source_url: https://arxiv.org/abs/2509.17956
tags:
- fairness
- features
- participants
- metrics
- metric
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how non-AI expert stakeholders assess fairness
  in AI systems by involving them in full fairness assessment tasks typically reserved
  for AI experts. Through a qualitative study with 30 participants, the research explores
  how stakeholders select and prioritize features, choose fairness metrics, and set
  fairness thresholds in a credit rating scenario.
---

# "I think this is fair'': Uncovering the Complexities of Stakeholder Decision-Making in AI Fairness Assessment

## Quick Facts
- **arXiv ID**: 2509.17956
- **Source URL**: https://arxiv.org/abs/2509.17956
- **Reference count**: 40
- **Primary result**: Non-AI expert stakeholders select broader features, tailor metrics, and set stricter thresholds than typical AI expert practices when assessing AI fairness.

## Executive Summary
This paper investigates how non-AI expert stakeholders assess fairness in AI systems by involving them in full fairness assessment tasks typically reserved for AI experts. Through a qualitative study with 30 participants, the research explores how stakeholders select and prioritize features, choose fairness metrics, and set fairness thresholds in a credit rating scenario. The study reveals that stakeholders' fairness decisions are more complex than typical AI expert practices: they consider features beyond legally protected ones, tailor metrics for specific contexts, set diverse yet stricter fairness thresholds, and prefer designing customized fairness definitions. The findings highlight the importance of incorporating stakeholders' nuanced fairness judgments into AI governance and provide actionable design implications for tools that enable meaningful stakeholder participation in fairness assessment.

## Method Summary
The study used a qualitative user study with 30 non-AI expert participants to assess fairness in a credit rating scenario. Participants used the German Credit Dataset (1000 instances, 20 features) with a pre-trained Logistic Regression model (accuracy: 0.76). The study protocol involved three tasks: feature selection and ranking, feature-metric mapping with threshold setting, and re-ranking based on results. Eight fairness metrics were offered, including group fairness metrics (Demographic Parity, Equal Opportunity, Equalized Odds, Predictive Equality, Outcome Test, Conditional Statistical Parity) and individual fairness metrics (Counterfactual Fairness, Consistency). Data analysis combined descriptive statistics of participant choices with thematic analysis of think-aloud transcripts.

## Key Results
- Stakeholders selected 19 out of 20 available features at least once, far exceeding typical expert selections of only legally protected features
- Participants frequently switched metrics for different features (average of 2.2 metrics per participant) rather than applying uniform metrics
- Fairness thresholds set by stakeholders averaged 6-7% (e.g., 6.00% for Counterfactual Fairness|Age), stricter than common benchmarks like the 80% rule or 10% difference
- 20 participants preferred designing their own custom fairness definitions rather than using existing metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Stakeholders identify fairness-relevant features based on contextual relevance rather than legal protection status.
- Mechanism: When non-expert stakeholders assess AI fairness, they draw on lived experience and domain understanding to evaluate whether a feature (e.g., 'Telephone,' 'Purpose') could introduce bias or unfairness in a specific context, leading them to select and prioritize a broader set of features than typical expert-driven legal/technical checklists.
- Core assumption: Stakeholders possess contextual knowledge about how features may affect individuals in ways that are not captured by formal protected-class definitions.
- Evidence anchors:
  - [abstract] "...they considered features far beyond legally protected features..."
  - [section] "In the credit rating scenario, AI experts often select only three features... In contrast, 19 of the 20 available features were selected at least once by participants..."
  - [corpus] Related work on participatory AI frameworks (e.g., "Beyond Predictions: A Participatory Framework for Multi-Stakeholder Decision-Making") supports the value of stakeholder contextual knowledge in system design.
- Break condition: If stakeholders lack domain familiarity or if the context is highly technical and removed from everyday experience, their feature selections may become arbitrary or fail to align with actual fairness risks.

### Mechanism 2
- Claim: Stakeholders tailor fairness metrics to specific features rather than applying uniform metrics across all features.
- Mechanism: Given the opportunity to explore multiple fairness metrics with plain-language explanations and visualizations, stakeholders engage in contextual reasoning, matching metric properties (e.g., focus on individual vs. group, handling of qualified/unqualified groups) to the perceived fairness concerns associated with each feature.
- Core assumption: Stakeholders can understand and differentiate among multiple fairness metrics when presented in accessible, context-rich formats.
- Evidence anchors:
  - [abstract] "...tailored metrics for specific contexts..."
  - [section] "We found that 20 participants often switched metrics for different features... For example, 10 participants used 3 different metrics, 8 used 2 metrics..."
  - [corpus] The corpus includes work on metric preferences (e.g., "Towards Multi-Stakeholder Evaluation of ML Models"), but direct evidence for feature-specific metric pairing is weak; this pattern is primarily documented in the present paper.
- Break condition: If metric explanations are overly technical or if stakeholders are rushed, they may default to a single metric or a superficially "familiar" one, reducing the contextual tailoring effect.

### Mechanism 3
- Claim: Stakeholders set stricter fairness thresholds than typical legal or technical standards.
- Mechanism: After selecting feature-metric pairs, stakeholders set thresholds by reflecting on their tolerance for unfairness in that specific context, often aspiring to high fairness initially and then allowing small margins for practical feasibility. This leads to thresholds that are numerically lower (stricter) than common benchmarks like the 80% rule or 10% difference.
- Core assumption: Stakeholders' threshold settings are driven by value judgments about acceptable harm and are not merely random or uniformly lenient.
- Evidence anchors:
  - [abstract] "...set diverse yet stricter fairness thresholds..."
  - [section] "For example, thresholds were set at on average 6.00% for (Counterfactual Fairness|Age) and 4.75% for (Counterfactual Fairness|Telephone) while this is even lower for some feature-metric pairings, such as 3.20% for (Equalized Odds|Foreign Worker)..."
  - [corpus] No direct corpus evidence on strict thresholds; this is a novel finding in this paper.
- Break condition: If stakeholders perceive the system as purely experimental or if they lack personal stakes in the outcomes, they may set arbitrarily high thresholds (lenient) or default to suggested values, breaking the strictness pattern.

## Foundational Learning

- Concept: **Feature Contextualization**
  - Why needed here: Stakeholders select features based on perceived relevance to fairness in a specific scenario. Engineers must understand that "important features" are not fixed and should support user-driven exploration and prioritization.
  - Quick check question: Can you list at least three features (beyond protected attributes) that a layperson might consider fairness-relevant in a loan approval context, and why?

- Concept: **Metric Flexibility & Pairing**
  - Why needed here: Stakeholders assign different metrics to different features. Systems must support not just a catalog of metrics, but the ability to map and re-map metrics to features dynamically.
  - Quick check question: Why might a stakeholder choose "Counterfactual Fairness" for an age-related feature and "Equalized Odds" for a foreign-worker feature in the same credit rating system?

- Concept: **Threshold Calibration**
  - Why needed here: Fairness thresholds are not one-size-fits-all. Stakeholders set them based on context-specific tolerance for unfairness. Systems need interfaces for granular, per-feature-metric threshold input.
  - Quick check question: If the legal standard allows up to a 20% disparity, what factors might lead a stakeholder to set a threshold of 5% instead?

## Architecture Onboarding

- Component map:
  - **Feature Selection Module**: Displays all available features with contextual info (distributions, importance, causal graphs). Allows multi-select, drag-to-rank, and deselection.
  - **Metric Exploration & Pairing Interface**: Presents group and individual fairness metrics with plain-language descriptions, strengths/limitations, and visualizations of metric results on current data. Supports dropdown selection per feature and a "Custom Fairness" option with free-text/structured input.
  - **Threshold Configuration**: Per feature-metric pair, a slider or input field (0–100%) for setting maximum allowable unfairness. Visual feedback shows current system performance relative to the threshold.
  - **Review & Re-Ranking**: A summary view of all feature-metric-threshold triples, with drag-to-reprioritize based on trade-off reflection.
  - **Results Dashboard**: Visualizes fairness assessments across all user-defined pairs, highlighting violations of thresholds.

- Critical path:
  1. User explores features and selects/ranks those they deem fairness-relevant.
  2. For each selected feature, user explores available metrics, views explanations and results, and chooses one (or defines a custom metric).
  3. User sets a fairness threshold for each feature-metric pair.
  4. System computes and displays fairness results; user may re-rank pairs based on outcomes and trade-offs.

- Design tradeoffs:
  - **Richness vs. Overwhelm**: Providing extensive feature context and metric explanations increases decision quality but risks cognitive overload. Use progressive disclosure and clear visual hierarchy.
  - **Customizability vs. Consistency**: Allowing custom metrics and per-feature thresholds aligns with stakeholder values but complicates aggregation and comparison across users. Decide early on how to synthesize diverse inputs.
  - **Simplicity vs. Accuracy**: Simplifying metric explanations may boost usability but can lead to misinterpretation. Pair lay descriptions with optional technical details and interactive examples.

- Failure signatures:
  - Users select only legally protected features and apply the same metric/threshold to all, mimicking expert defaults (indicates insufficient scaffolding for broader reasoning).
  - Users skip metric exploration and use only the first or default metric (indicates poor discoverability or overwhelming interface).
  - Users set thresholds at extreme values (0% or 100%) without adjustment (indicates possible misunderstanding of the scale or lack of engagement).
  - Users abandon the task before completing all steps (indicates time-on-task issues or unclear value proposition).

- First 3 experiments:
  1. **Feature Selection Scope**: Test whether providing contextual information (causal graphs, distributions) increases the diversity of features selected compared to a baseline with only feature names. Measure selection breadth and ranking rationale.
  2. **Metric Pairing Patterns**: Evaluate whether a guided metric-explanation interface leads to more feature-specific metric choices than a simple list-of-metrics interface. Analyze consistency vs. tailoring in user choices.
  3. **Threshold Sensitivity**: Explore how different framings of threshold input (e.g., "tolerance for unfairness" vs. "required fairness level") affect the strictness and variability of user-set thresholds. Compare to standard benchmarks.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can stakeholder-defined "custom fairness metrics"—which combine existing metrics or define new rules—be operationalized within technical AI mitigation frameworks?
- Basis in paper: [inferred] The authors note that while stakeholders preferred custom metrics, the current prototype provided "limited insight into how such metrics could be operationalized in practice" (Section 5.1).
- Why unresolved: Translating open-ended, composite stakeholder definitions into the formal mathematical constraints required for model training or auditing is technically complex and lacks standard methodologies.
- What evidence would resolve it: A functional software pipeline that translates custom user inputs into actionable fairness constraints, validated by showing the resulting model aligns with the stakeholder's intent.

### Open Question 2
- Question: How can fairness assessment tools be designed to support large-scale, independent stakeholder participation without the need for expert facilitation?
- Basis in paper: [inferred] The authors identify that real-world deployment requires systems accessible online for simultaneous use, noting that this area "remains underexplored" (Section 5.4).
- Why unresolved: The current study relied on intensive, in-person facilitation to manage cognitive load and explain metrics; removing this support risks overwhelming users or resulting in superficial assessments.
- What evidence would resolve it: Usability studies of a self-guided, web-based tool demonstrating that diverse users can successfully complete complex fairness assessments (feature selection to threshold setting) without human assistance.

### Open Question 3
- Question: How can we aggregate diverse, conflicting stakeholder fairness preferences into a unified AI strategy without reinforcing existing societal biases?
- Basis in paper: [explicit] The authors explicitly state that "How to guard against unfairness arising from these societal biases is still an open research question" when discussing the aggregation of stakeholder preferences (Section 5.5).
- Why unresolved: Simplifying complex preferences via majority voting or consensus may simply replicate systemic societal prejudices, yet filtering these biases without overriding stakeholder autonomy remains a governance challenge.
- What evidence would resolve it: A framework or algorithm that aggregates collective stakeholder input while successfully filtering known societal biases, validated through comparative fairness audits.

## Limitations
- The qualitative nature and small sample size (30 participants) limit generalizability of findings
- Specific fairness metric implementations and plain-language explanations are not fully detailed, making precise replication challenging
- The single context (credit rating) may not represent other AI fairness assessment scenarios

## Confidence

- **High Confidence**: The core finding that stakeholders select features beyond legally protected ones and set stricter thresholds is well-supported by the data (e.g., 19/20 features selected, average thresholds below common benchmarks). The thematic analysis of think-aloud transcripts provides strong qualitative backing.
- **Medium Confidence**: The claim that stakeholders tailor metrics to specific features is supported by observed switching behavior (e.g., 20 participants used an average of 2.2 metrics), but the reasoning behind these pairings is inferred from transcripts and may be influenced by the study interface design.
- **Low Confidence**: The assertion that stakeholders prefer designing customized fairness definitions is based on a small number of custom entries and is not extensively explored in the analysis.

## Next Checks

1. **Replicate Feature Selection Patterns**: Re-run the study protocol with a new cohort in a different domain (e.g., hiring) to test if non-protected features are consistently prioritized over legal defaults.

2. **Validate Metric-Tailoring Effect**: Conduct an A/B test comparing a "guided metric explanation" interface against a "simple metric list" to confirm if richer explanations lead to more feature-specific metric pairings.

3. **Threshold Calibration Experiment**: Test different framings of threshold input (e.g., "tolerance for unfairness" vs. "required fairness level") to determine their impact on the strictness and variability of stakeholder-set thresholds.