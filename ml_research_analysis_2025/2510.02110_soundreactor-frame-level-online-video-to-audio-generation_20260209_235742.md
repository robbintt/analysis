---
ver: rpa2
title: 'SoundReactor: Frame-level Online Video-to-Audio Generation'
arxiv_id: '2510.02110'
source_url: https://arxiv.org/abs/2510.02110
tags:
- audio
- generation
- diffusion
- video
- inproc
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the task of frame-level online video-to-audio
  (V2A) generation, where a model generates audio from video in an autoregressive
  manner without access to future video frames. This is motivated by applications
  in live content creation and generative world models that require real-time audio
  generation.
---

# SoundReactor: Frame-level Online Video-to-Audio Generation

## Quick Facts
- arXiv ID: 2510.02110
- Source URL: https://arxiv.org/abs/2510.02110
- Reference count: 40
- First tailored framework for frame-level online video-to-audio generation achieving 26.3ms latency with NFE=1 on 30FPS 480p videos

## Executive Summary
SoundReactor introduces the task of frame-level online video-to-audio (V2A) generation, where audio is generated autoregressively from video in real-time without access to future frames. The framework achieves high-quality, semantically and temporally aligned, full-band stereo audio generation under strict causal constraints. Key innovations include using DINOv2 grid features with temporal differences for vision conditioning, continuous audio latents via VAE for autoregressive modeling, and diffusion pre-training followed by consistency fine-tuning for low per-frame latency.

## Method Summary
SoundReactor employs a two-stage training pipeline: Stage 1 trains a diffusion model with EDM2-style denoising score matching, while Stage 2 applies Easy Consistency Tuning (ECT) to enable single-step generation. The model uses DINOv2 grid features augmented with temporal differences as visual conditioning, compressed 48kHz stereo audio through a VAE to 64-dimensional latents at 30Hz, and a causal transformer-decoder with diffusion head for autoregressive prediction. The entire network is trained end-to-end with classifier-free guidance, achieving low latency (26.3ms with NFE=1) while maintaining high audio quality on gameplay videos.

## Key Results
- Achieves 26.3ms per-frame waveform-level latency (NFE=1) and 31.5ms (NFE=4) on 30FPS 480p videos using single H100 GPU
- Outperforms offline autoregressive V2A models on objective metrics (FAD, MMD, KL PaSST, IB-Score) and human evaluations
- Demonstrates strong temporal alignment (low DeSync scores) and semantic consistency with input video frames
- Validates effectiveness of continuous latents over discrete tokenization for autoregressive modeling at similar compression rates

## Why This Works (Mechanism)

### Mechanism 1: Causal Grid Features with Temporal Differencing
Frame-level visual conditioning provides sufficient temporal cues for audio-visual synchronization without future-frame access. DINOv2 grid features are extracted per-frame, augmented with channel-wise temporal differences (frame_i - frame_{i-1}), then aggregated to a single token via shallow transformer aggregator. Adjacent video frames at 30FPS have high semantic similarity (0.99±0.007 cosine similarity), so frame differencing isolates motion/temporal information. Removing frame subtraction consistently degrades performance across CEV levels.

### Mechanism 2: Continuous Latents Enable Single-Token AR Prediction
Continuous audio latents simplify autoregressive modeling while preserving reconstruction quality better than discrete tokenization at comparable compression rates. VAE compresses 48kHz stereo audio to 64-dim latents at 30Hz (1600× temporal downsampling). Each time frame becomes one continuous latent, enabling single-token AR prediction instead of multi-code prediction required by RVQ. Reconstruction quality is typically higher at the same temporal downsampling rate, but requires larger diffusion head (≥70M for cx=64 latents).

### Mechanism 3: Consistency Fine-tuning Preserves Quality with Few-Step Sampling
Initializing from pretrained diffusion model and progressively annealing Δt→0 yields high-quality 1-4 step generation. Stage 1 trains with EDM2 DSM loss. Stage 2 applies ECT, minimizing distance between G(xt, t) and G(xr, r) where t>r, gradually shrinking gap. The mapping function m(r|t, Iters) controls annealing schedule. ECT(NFE=4) achieves 31.5ms latency vs 26.3ms for NFE=1, both under real-time threshold (33.3ms for 30FPS). Dropout rate critically affects NFE=1 quality (CF-0.2 succeeds, CF-0.1 fails).

## Foundational Learning

- **Autoregressive Modeling over Continuous Latents (MAR paradigm):**
  - Why needed here: Standard AR uses discrete tokens; this framework predicts continuous distributions via diffusion head per token.
  - Quick check: Can you explain why MAR decouples tokenization from AR modeling, and how the diffusion head parameterizes p(x_i | z_i)?

- **Consistency Models and Easy Consistency Tuning:**
  - Why needed here: Core technique enabling low-latency inference; need to understand how it differs from distillation and progressive distillation.
  - Quick check: In ECT, what happens to the gap Δt=t-r during training, and why does shared noise matter?

- **Causal Attention and Streaming Inference:**
  - Why needed here: Fundamental to online generation—understanding causal masking, KV-cache, and what breaks causality (e.g., bidirectional attention in vision encoders).
  - Quick check: Why is Synchformer's video encoder not compatible with frame-level online V2A despite being an "AR" model?

## Architecture Onboarding

- **Component map:**
  Video Frame → DINOv2 (21M) → Grid Features → [Diff with prev] → PCA(384→59) → Aggregator Transformer → Vision Token v_i (512-dim)
  
  Audio Waveform → VAE Encoder (157M) → Audio Latent x_i (64-dim @ 30Hz)
  
  [v_1, x_1, v_2, x_2, ...] → Causal Decoder Transformer (18L, 1024d, 250M) → z_i = Concat(z_{2i-1}, z_{2i}) → Diffusion Head (8 blocks, 1280ch, 70M) → x_i_pred
  
  Audio Latents → VAE Decoder → Waveform

- **Critical path:**
  1. Vision encoding must be single-frame (no temporal attention across frames)
  2. Audio VAE encoder can be non-causal for training; decoder must be causal for streaming
  3. CFG applied at transformer output (z_2i), not inside diffusion head
  4. NTK-aware interpolation for extending beyond training context window

- **Design tradeoffs:**
  - Grid features vs CLS tokens: Grid provides spatial info + temporal via differencing; CLS lacks temporal cues
  - Continuous vs discrete latents: Better reconstruction, simpler AR; but requires larger diffusion head (70M needed)
  - CF vs IN mapping in ECT: CF-0.2 enables NFE=1; IN-0.2 fails at NFE=1
  - Fine-tune entire network vs head-only: Entire network better; head-only still viable for efficiency

- **Failure signatures:**
  - NFE=1 produces noise: Check dropout rate in ECT (needs 0.2, not 0.1)
  - Poor temporal sync: Check if vision encoder uses non-causal attention (like Synchformer)
  - Context window overflow artifacts: Use NTK scaling, not PI (PI slows periodic sounds)
  - 10M diffusion head fails silently: Need ≥70M for cx=64 latents

- **First 3 experiments:**
  1. **Validate causal pipeline:** Replace DINOv2 with Synchformer on same data—should see quality improvement but latency/c violation (Synchformer processes chunks with future frames)
  2. **ECT ablation:** Train Stage 2 with CF-0.1 vs CF-0.2, compare NFE=1 quality—confirm dropout sensitivity
  3. **Context window stress test:** Generate 16-second sequences with PI, NTK, and SWA; measure FAD/MMD on first vs second 8 seconds—verify NTK preserves quality on latter half

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can frame-level online V2A models maintain quality and synchronization over minute- to hour-scale generation horizons beyond the tested 16-second clips?
- Basis in paper: "Given that interactive multimodal applications might require minute- to hour-scale generation, we leave exploring longer-horizon generation as future work."
- Why unresolved: Current evaluation only extends to twice the training context window (16 seconds); NTK-aware interpolation showed promise but longer sequences remain untested.
- What evidence would resolve it: Benchmarks on 60+ second continuous generation showing consistent FAD, MMD, and temporal alignment metrics without drift.

### Open Question 2
- Question: How can causal VAE decoders achieve reconstruction fidelity comparable to non-causal counterparts without increasing decoding overhead?
- Basis in paper: "Accordingly, improving the fidelity of the causal VAE while keeping the decoding overhead small remains an important direction for future work."
- Why unresolved: Causal VAE decoder showed degraded reconstruction quality (rMMD_OpenL3: 62.7 vs 60.4 non-causal), which propagates to final generation quality.
- What evidence would resolve it: Novel causal decoder architectures achieving ≤5% reconstruction quality gap while maintaining <33ms per-frame latency.

### Open Question 3
- Question: Can larger vision encoders (e.g., CLIP-H with 840M parameters) or audio-visual pre-trained encoders be integrated while preserving end-to-end causality and low latency?
- Basis in paper: "However, designing effective architectures that incorporate these components while preserving the end-to-end causality and low frame-level latency, which are crucial for the online V2A task, is a non-trivial challenge. Therefore, we leave this as future work."
- Why unresolved: Current 21M parameter DINOv2 variant limits semantic richness; larger encoders may violate causal or latency constraints.
- What evidence would resolve it: Systematic evaluation of encoder scaling laws with explicit latency budgets and causal masking constraints.

## Limitations

- Task novelty vs practical significance: While frame-level online V2A is technically novel, actual use cases beyond live streaming remain unclear, and 26-31ms latency may be insufficient for professional live production workflows
- Architecture complexity: Two-stage training pipeline (diffusion + consistency) adds significant engineering complexity; simpler causal AR architectures might achieve comparable results with less computational overhead
- Generalization beyond gameplay: Results demonstrated only on gameplay videos; performance on other video domains (sports, concerts, vlogs) may vary significantly due to different audio-visual correlation patterns

## Confidence

- **High**: Technical implementation details, latency measurements, objective metrics (FAD, MMD, DeSync) on benchmark dataset
- **Medium**: Quality of generated audio as assessed by human evaluation, practical utility in real-world applications
- **Low**: Claims about applicability to live streaming scenarios without user studies, performance on video domains beyond gameplay

## Next Checks

1. **Cross-domain generalization test**: Evaluate SoundReactor on non-gameplay video datasets (sports, music videos, nature documentaries) and compare objective metrics to baseline models
2. **User experience validation**: Conduct user studies with content creators to assess the practical utility of 26-31ms latency in real production workflows
3. **Ablation study on architectural complexity**: Compare SoundReactor against a simplified single-stage causal AR baseline trained only with consistency objectives to quantify the value of the two-stage pipeline