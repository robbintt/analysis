---
ver: rpa2
title: Control Disturbance Rejection in Neural ODEs
arxiv_id: '2509.18034'
source_url: https://arxiv.org/abs/2509.18034
tags:
- control
- robust
- have
- algorithm
- points
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes an iterative training algorithm for Neural
  ODEs that provides models resilient to control (parameter) disturbances. The method
  builds on Tuning without Forgetting and introduces training points sequentially,
  updating parameters on new data within the space of parameters that do not decrease
  performance on previously learned training points.
---

# Control Disturbance Rejection in Neural ODEs

## Quick Facts
- arXiv ID: 2509.18034
- Source URL: https://arxiv.org/abs/2509.18034
- Authors: Erkan Bayram; Mohamed-Ali Belabbas; Tamer Başar
- Reference count: 14
- One-line primary result: Robust Neural ODE maintains classification accuracy around 0.96 up to disturbance magnitude 0.1, while standard Neural ODE degrades rapidly beyond 0.07.

## Executive Summary
This paper introduces a robust training algorithm for Neural ODEs that provides resilience against control (parameter) disturbances. Building on the Tuning without Forgetting framework, the method sequentially introduces training points and updates parameters within the space that preserves performance on previously learned data. Inspired by flat minima, the approach solves a minimax problem over an infinite-dimensional control space to find parameters that minimize worst-case loss under bounded perturbations. Through simulations on a binary classification task, the robust formulation effectively learns new data while maintaining accuracy under controlled disturbances.

## Method Summary
The method trains Neural ODEs via iterative sequential learning with kernel projection. Starting with initial parameters u₀, the algorithm introduces data points one at a time. For each new point (xⱼ, yⱼ), it solves a minimax problem: finding the worst-case disturbance εⱼ*(u) that maximizes loss under ||ε||∞ ≤ ρ, then updating parameters via projected gradient descent onto the kernel of previously learned endpoint mappings. The kernel projection ensures updates don't degrade performance on earlier data points. The algorithm uses a finite-dimensional approximation of the infinite-dimensional disturbance space via compact operator theory, making computation tractable. Key hyperparameters include regularization coefficient λ₁=0.2 and disturbance bound ρ=0.1.

## Key Results
- Robust nODE maintains classification accuracy ~0.96 up to disturbance magnitude 0.1
- Standard nODE degrades rapidly beyond disturbance magnitude 0.07
- Sequential training with kernel projection prevents catastrophic forgetting
- Method effectively learns new data points while preserving prior knowledge

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Projecting gradient updates onto the kernel of previously learned endpoint mappings preserves prior knowledge while enabling sequential learning.
- Mechanism: The algorithm maintains invariance of endpoint mappings R(φ(u, xᵢ)) = yᵢ for all previously seen points by constraining parameter updates δu to lie in ker(u, Xⱼ) = span{δu ∈ V | R(L(u,xᵢ)(δu)) = 0 ∀ xᵢ ∈ Xⱼ}. This is implemented via kernel projection: δuₖ := proj_ker(uₖ,Xⱼ) DδuJⱼ₊₁(uₖ + ε*ᵢ(uₖ)), which ensures the update does not alter previously learned mappings.
- Core assumption: The intersection ⋂ᵢ U(xᵢ, yᵢ) forms a finite-codimension Banach submanifold (requires assumptions A.1-A.3: bracket-generating vector fields, linearized controllability property, and transversal intersection of control sets).
- Evidence anchors:
  - [abstract]: "introduces training points sequentially, and updates the parameters on new data within the space of parameters that do not decrease performance on the previously learned training points"
  - [section 3]: "we propose to select δuₖ such that (11) and (13) hold... δuₖ := proj_ker(uₖ,Xⱼ) DδuJⱼ₊₁(uₖ + ε*ᵢ(uₖ))"
  - [corpus]: Related work "Geometric Foundations of Tuning without Forgetting in Neural ODEs" (arXiv:2509.03474) provides theoretical foundations for this kernel projection approach.
- Break condition: Fails if the control sets U(xᵢ, yᵢ) do not intersect transversally (A.3 violated), or if bracket-generating condition (A.1) fails, making the kernel empty or the submanifold structure invalid.

### Mechanism 2
- Claim: A minimax formulation over infinite-dimensional control space induces flat minima that provide robustness against bounded parameter perturbations.
- Mechanism: The algorithm solves min_u max_{||ε||∞≤ρ} ∑ᵢ ||R(φ(u+ε, xᵢ)) - yᵢ||₂² by first finding closed-form worst-case disturbance ε*ᵢ(u) via calculus of variations, then minimizing over u. The gradient is evaluated at uₖ + ε*ᵢ(uₖ) (perturbed parameters) while the forward pass uses uₖ, creating a mismatch that encourages flat regions where small parameter changes minimally affect loss.
- Core assumption: The regularization coefficient λ₁ is sufficiently large to ensure the second variation δ²J̃ᵤᵢ|ε is uniformly negative definite, guaranteeing the inner maximization has a unique solution.
- Evidence anchors:
  - [abstract]: "inspired by the concept of flat minima, we solve a minimax problem for a non-convex non-concave functional over an infinite-dimensional control space"
  - [section 1]: "This mismatch between where the cost functional and the gradient are evaluated is a key ingredient for the search of a flat minima"
  - [corpus]: Corpus evidence for this specific mechanism in Neural ODEs is weak; flat minima literature (SAM) focuses on finite-dimensional networks.
- Break condition: Fails if λ₁ is too small (second-order optimality condition violated), or if the disturbance set Vρ is too large relative to the local curvature of the loss landscape.

### Mechanism 3
- Claim: Reduction from infinite-dimensional disturbance space to finite-dimensional compact subspace preserves worst-case disturbance analysis while enabling tractable computation.
- Mechanism: Lemma 1 establishes that for the operator L(u,xᵢ)(·) mapping control variations to endpoint variations, there exists a finite-dimensional compact subspace V*ρ ⊂ Vρ such that L(u,xᵢ)(V*ρ) = L(u,xᵢ)(Vρ). The operator has finite rank k ≤ n (state dimension), allowing construction of V*ρ from k basis functions. This reduces the search space for worst-case ε without loss of optimality for the first-order approximation.
- Core assumption: The Jacobians ∂f(x,u)/∂u and ∂f(x,u)/∂x are uniformly bounded along trajectories, ensuring L(u,xᵢ) is a bounded operator.
- Evidence anchors:
  - [section 3]: "there exists a finite-dimensional compact subspace of Vρ, denoted by V*ρ, such that L(u,xᵢ)(V*ρ) = L(u,xᵢ)(Vρ)"
  - [section 4]: "the image L(u,xᵢ)(Vρ) is a subset of Rⁿ, and hence its dimension, say k, satisfies k ≤ n"
  - [corpus]: No directly comparable mechanisms found in corpus; this appears novel to this work.
- Break condition: Fails if the dynamics f(x,u) have unbounded Jacobians, or if the state dimension n is insufficient to capture the disturbance effects on endpoint mappings.

## Foundational Learning

- Concept: **Calculus of Variations and Functional Derivatives**
  - Why needed here: The paper formulates optimization over infinite-dimensional function spaces (control functions u: [0,1] → Rᵖ). First- and second-order variations δJ|ε(η) and δ²J|ε(η) are used to derive optimality conditions for the inner maximization. Understanding functional derivatives is essential to follow why ε*ᵢ(u) in equation (9) is optimal.
  - Quick check question: Given a functional J[u] = ∫₀¹ L(t, u(t), u̇(t))dt, can you derive the Euler-Lagrange equation and explain when the second variation determines if a critical point is a maximum or minimum?

- Concept: **Banach and Hilbert Spaces with Compact Operators**
  - Why needed here: The control space V = L∞([0,1], Rᵖ) is a Banach space. The algorithm projects onto infinite-dimensional subspaces (kernels) that are Banach submanifolds. Understanding why compactness matters (Lemma 1 relies on Heine-Borel in finite dimensions, and compact operators have finite-rank approximations) is crucial for understanding the tractability claims.
  - Quick check question: Why does L∞ lack the nice properties of Hilbert spaces, and how does the paper work around this by constructing finite-dimensional subspaces?

- Concept: **Neural ODEs as Controlled Dynamical Systems**
  - Why needed here: The paper views neural networks through the lens ẋ = f(x, u), where u is the control (parameters). The endpoint mapping φ(u, xᵢ) and its linearization via the state transition matrix Φ(u,xᵢ)(t, τ) are fundamental. The q-folded method and controllability assumptions (A.1, A.2) come from geometric control theory.
  - Quick check question: Given ẋ = f(x, u) with initial condition x(0) = xᵢ, can you derive the first-order sensitivity of the endpoint x(1) with respect to a perturbation δu(t)?

## Architecture Onboarding

- Component map:
  - **nODE dynamics**: ẋ = f(x, u) with f smooth, state x ∈ Rⁿ, control u ∈ L∞([0,1], Rᵖ)
  - **Embedding map**: E: Rⁿ → Rⁿ̄ augments input dimension (identity in paper's example)
  - **Readout map**: R: Rⁿ̄ → Rⁿᵒ (linear, 1-Lipschitz, full row rank Jacobian)
  - **Endpoint mapping**: R(φ(u, E(xᵢ))) maps input xᵢ to output under control u
  - **Disturbance set**: Vρ = {v ∈ L∞ : ||v||∞ ≤ ρ}, bounded perturbations to u
  - **Kernel operator**: L(u,xᵢ)(v) = ∫₀ᵗ Φ(u,xᵢ)(t, τ) ∂f/∂u v(τ)dτ, maps control variation to endpoint variation
  - **Gram matrix Kᵤᵢ**: Kᵤᵢ(t, τ) = mᵤᵢ(t)ᵀ mᵤᵢ(τ) where mᵤᵢ(t) = R(Φ(u,xᵢ)(1, t) ∂f/∂u)

- Critical path:
  1. **Initialization**: Set u₀, discretize time into N steps, initialize loss history L
  2. **Outer loop (j = 1 to q)**: Introduce data point (xⱼ, yⱼ)
  3. **Inner loop (repeat until convergence)**:
     - Compute Lᵢ = R(L(uₖ,xᵢ)(·)) for all i ≤ j via Algorithm 1 from [9]
     - Compute Gram matrix Kᵤᵢ = Lᵢᵀ Lᵢ
     - Solve for worst-case disturbance: εⱼ(u) = ρ·γ/||γ||∞ where γ = (LⱼᵀLⱼ - λ₁I)⁻¹ DδuJⱼ(u)
     - Project gradient onto kernel: δuₖ = proj_N(L) DδuJⱼ(uₖ + εⱼ(uₖ))
     - Update: uₖ₊₁ = uₖ - αₖ δuₖ
  4. **Test time**: Evaluate R(φ(u + ε, x)) under disturbance ||ε||∞ ≤ ρ

- Design tradeoffs:
  - **ρ (disturbance bound)**: Larger ρ → more robust but potentially lower clean accuracy. Paper uses ρ = 0.1, achieves 0.96 accuracy up to ||ε||∞ = 0.1.
  - **λ₁ (regularization)**: Must be "large enough" for second-order condition; too large → trivial ε* = 0; too small → no unique maximizer. Paper uses λ₁ = 0.2.
  - **Discretization resolution (N)**: 100 layers (Δt = 0.01) used in experiments. Finer discretization → better approximation of continuous ODE but higher computational cost.
  - **Sequential vs batch training**: Sequential enables kernel projection for continual learning but requires careful ordering; batch training simpler but lacks robustness guarantees.

- Failure signatures:
  - **Catastrophic forgetting**: If kernel projection fails (Lᵢ has no null space, or intersection of kernels is empty), updating for new point (xⱼ₊₁, yⱼ₊₁) will corrupt R(φ(u, xᵢ)) ≈ yᵢ for i ≤ j. Signature: training loss on previous points increases after outer loop iteration.
  - **Non-convergence of inner loop**: If assumptions A.1-A.3 fail, the kernel N(L) may not exist or be too small to allow progress. Signature: inner loop oscillates or DδuJⱼ(uₖ + εⱼ(uₖ)) always orthogonal to N(L).
  - **Trivial disturbance**: If λ₁ too large or ρ too small, εⱼ(u) ≈ 0, reducing to standard gradient descent without robustness benefits. Signature: test accuracy drops sharply for ||ε||∞ > 0.
  - **Numerical instability**: Solving (LⱼᵀLⱼ - λ₁I)⁻¹ requires λ₁ not equal to eigenvalues of LⱼᵀLⱼ. Signature: NaN or Inf in γ computation.

- First 3 experiments:
  1. **Replicate binary classification with varying ρ**: Train robust nODE with ρ ∈ {0.05, 0.1, 0.15, 0.2} on the disk classification task. Plot accuracy vs ||ε||∞ to verify that robustness extends up to the chosen ρ. Compare to Figure 1(a)(b) to confirm the tradeoff between robustness and clean accuracy.
  2. **Ablation on kernel projection**: Train two models: (a) with kernel projection as specified, (b) with standard gradient descent (no projection). Measure catastrophic forgetting by tracking per-sample loss on earlier points as new points are added. This tests whether Mechanism 1 is necessary for continual learning.
  3. **Sensitivity to λ₁**: Sweep λ₁ ∈ {0.1, 0.2, 0.5, 1.0} with fixed ρ = 0.1. Monitor (a) whether inner loop finds non-trivial ε* (||ε*||∞ should approach ρ), (b) whether second variation is negative (compute eigenvalue spectrum of discretized Hessian), and (c) final test accuracy under disturbance. This validates the "sufficiently large λ₁" assumption in Mechanism 2.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the reliance on a first-order approximation of the endpoint map limit the method's effectiveness for large disturbances or highly non-linear dynamics?
- **Basis in paper:** [inferred] The derivation of the optimal disturbance $\epsilon^*_i$ (Eq. 9) and the cost functional $\tilde{J}$ (Eq. 10) depends on the linearization $R(\phi(u+\epsilon, x_i)) \approx R(\phi(u, x_i)) + R(L(u,x_i)(\epsilon))$ (Eq. 6).
- **Why unresolved:** The paper does not bound the approximation error. If the true higher-order dynamics dominate, the calculated worst-case disturbance may be suboptimal, degrading robustness.
- **What evidence would resolve it:** Theoretical error bounds on the approximation relative to $\rho$ or empirical tests on systems with known high non-linearity.

### Open Question 2
- **Question:** How does the algorithm perform when the geometric assumptions of transversality (A.3) or linearized controllability (A.2) are violated?
- **Basis in paper:** [inferred] Theorem 1 establishes that the space of admissible controls is a Banach submanifold strictly under Assumptions A.1, A.2, and A.3.
- **Why unresolved:** In high-dimensional or randomly initialized networks, verifying transversality is difficult. If these topological conditions fail, the kernel projection method lacks theoretical guarantees.
- **What evidence would resolve it:** Empirical analysis of the algorithm's convergence behavior on systems engineered to fail the transversality or controllability conditions.

### Open Question 3
- **Question:** Can the computational complexity of the kernel projection method scale to modern deep learning tasks and high-dimensional data?
- **Basis in paper:** [inferred] The numerical method involves computing and inverting operators related to the state transition matrix $\Phi$ and Gram matrix $K$, demonstrated only on a simple 2D-to-5D classification task.
- **Why unresolved:** The paper does not address the memory or compute overhead of calculating $L(u_{k}, x_i)$ or the inverse in Eq. 9 for networks with millions of parameters.
- **What evidence would resolve it:** Complexity analysis and benchmarks on standard datasets (e.g., CIFAR/ImageNet) comparing runtime against standard or SAM-trained Neural ODEs.

## Limitations

- The robustness claims are demonstrated only on a single synthetic classification task with a specific ODE architecture (tanh residual network). Generalization to other tasks, network structures, and real-world datasets remains untested.
- Several critical implementation details are underspecified, including training set size, learning rate schedules, and convergence criteria for the inner loop. The kernel projection and matrix inversion steps require careful numerical handling.
- The theoretical foundations rely on assumptions (bracket-generating vector fields, linearized controllability, transversal intersections) that may not hold for arbitrary neural ODE architectures.

## Confidence

- **High**: The geometric control theory framework and kernel projection mechanism for preventing catastrophic forgetting are well-founded and theoretically sound.
- **Medium**: The minimax formulation for inducing flat minima and the reduction to finite-dimensional subspaces (Lemma 1) are mathematically correct but their practical impact on robustness requires more empirical validation.
- **Low**: Claims about scalability to complex tasks and real-world applications are not supported by the current experimental evidence, which is limited to a binary classification toy problem.

## Next Checks

1. **Ablation study on kernel projection**: Train two models—one with kernel projection and one with standard gradient descent—on the disk classification task. Track the loss on previously learned points as new points are added to quantify catastrophic forgetting and validate Mechanism 1.

2. **Robustness under varying disturbance sets**: Systematically vary ρ (disturbance bound) and λ₁ (regularization) to map out the robustness-accuracy tradeoff. Verify that ε*ᵢ(u) approaches the boundary ||ε||∞ = ρ when λ₁ is sufficiently large, and test whether second-order optimality conditions hold.

3. **Extension to multi-class classification**: Apply the robust training algorithm to a multi-class problem (e.g., MNIST digit classification) using a neural ODE architecture. Measure classification accuracy under bounded parameter perturbations and compare to a standard nODE baseline to assess generalization beyond the binary case.