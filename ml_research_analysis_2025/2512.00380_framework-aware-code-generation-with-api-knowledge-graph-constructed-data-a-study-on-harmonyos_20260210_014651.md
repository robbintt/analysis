---
ver: rpa2
title: 'Framework-Aware Code Generation with API Knowledge Graph-Constructed Data:
  A Study on HarmonyOS'
arxiv_id: '2512.00380'
source_url: https://arxiv.org/abs/2512.00380
tags:
- code
- data
- generation
- knowledge
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of poor code generation performance
  in low-resource software frameworks, exemplified by HarmonyOS, where large language
  models lack sufficient API-specific knowledge during pre-training. The authors propose
  APIKG4SYN, a knowledge-graph-driven framework that synthesizes API-oriented training
  data by constructing an API knowledge graph from documentation and leveraging Monte
  Carlo Tree Search to generate diverse, realistic question-code pairs.
---

# Framework-Aware Code Generation with API Knowledge Graph-Constructed Data: A Study on HarmonyOS

## Quick Facts
- arXiv ID: 2512.00380
- Source URL: https://arxiv.org/abs/2512.00380
- Reference count: 40
- Pass@1 accuracy achieved: 25.00% on HarmonyOS benchmark

## Executive Summary
This paper addresses the challenge of code generation for low-resource software frameworks, using HarmonyOS as a case study. The authors propose APIKG4SYN, a framework that constructs API knowledge graphs from documentation and uses Monte Carlo Tree Search to synthesize API-oriented training data. By fine-tuning Qwen with this synthetic data, the model achieves significantly improved performance on HarmonyOS code generation tasks, demonstrating the potential of targeted data synthesis for specialized frameworks.

## Method Summary
The APIKG4SYN framework constructs an API knowledge graph from HarmonyOS documentation, then uses uncertainty estimation and Monte Carlo Tree Search to identify informative API combinations. It generates question-code pairs through a two-stage process using DeepSeek-V3, creating both single-API and multi-API scenarios. The resulting dataset is used to fine-tune Qwen2.5-Coder-7B via LLaMA-Factory with LoRA, achieving improved performance on the OHBen benchmark without requiring executable code examples during training.

## Key Results
- Fine-tuned Qwen2.5-Coder-7B achieves 25.00% pass@1 accuracy on HarmonyOS benchmark
- Outperforms baseline GPT-4o model (17.59% pass@1) by 42%
- Multi-API data integration essential, with removal reducing pass@1 to 10.19%
- Synthetic data generation enables effective fine-tuning without real executable examples

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Targeting API combinations where the model exhibits high uncertainty likely yields more informative training data than random sampling.
- **Mechanism:** The framework computes an Uncertainty Estimation (UE) score for API nodes based on the negative log probability of predicting child nodes given a parent. Monte Carlo Tree Search (MCTS) then uses these scores as rewards to navigate the knowledge graph, identifying paths containing APIs the model is least familiar with.
- **Core assumption:** The model's internal probability distribution accurately reflects its functional knowledge gaps regarding specific API interactions.
- **Evidence anchors:**
  - [abstract] "deriving... uncertainty estimation (UE)-driven Monte Carlo Tree Search... to identify unfamiliar or highly correlated API nodes."
  - [section 2.3] "The negative logarithm of this probability yields the uncertainty estimation... MCTS is executed with the UE value serving as the reward function."
  - [corpus] While neighbor papers discuss RAG for API docs, evidence for *uncertainty-driven* synthesis specifically is not present in the provided corpus.
- **Break condition:** If the model's low probability scores are caused by tokenization artifacts rather than semantic ignorance, the search will optimize for syntax noise rather than knowledge gaps.

### Mechanism 2
- **Claim:** Decoupling question generation from code generation via a Knowledge Graph (KG) reduces syntax hallucinations in low-resource languages.
- **Mechanism:** The system separates the creation of the problem (question) from the solution (code). It retrieves precise, fine-grained API metadata from the KG—parameters, return types, and constraints—and injects this into the code generation prompt as "ground truth," forcing the LLM to adhere to the actual framework specification rather than its pre-trained (potentially incorrect) priors.
- **Core assumption:** The API documentation parsed into the KG is complete, up-to-date, and syntactically correct.
- **Evidence anchors:**
  - [abstract] "exploit API knowledge graphs for the construction of API-oriented question-code pairs... specifically tailored for low-resource frameworks."
  - [section 2.4] "LLMs address low-resource framework-related questions by leveraging... fine-grained API information retrieved from the API knowledge graph."
  - [corpus] "When LLMs Meet API Documentation" supports the general mechanism that retrieval augmentation aids code generation, aligning with the usage of KG here.
- **Break condition:** If the KG extraction parser fails to capture complex constraints (e.g., asynchronous callbacks or threading requirements), the generated code will compile but fail at runtime.

### Mechanism 3
- **Claim:** Integrating multi-API scenarios with single-API data is necessary to bridge the gap between syntactic knowledge and functional problem solving.
- **Mechanism:** Single-API data teaches the model how to call a specific method (syntax), while multi-API data (derived from MCTS paths) teaches the model how to combine distinct functionalities to solve a goal (composition). The paper argues this complements the feature space, covering "regions underrepresented by single-API data."
- **Core assumption:** The complexity of software tasks scales primarily through the composition of independent API calls rather than deep usage of a single API.
- **Evidence anchors:**
  - [abstract] "integrates both single-API and multi-API knowledge... enabling the creation of a diverse and informative dataset."
  - [section 3.4] "Removing multi-API data reduces the pass@1 score from 25.00% to 10.19%... Multi-API data can... compensate for the knowledge deficiencies inherent in single-API data."
  - [corpus] Corpus papers do not explicitly compare single vs. multi-API synthesis ratios.
- **Break condition:** If the multi-API combinations are semantically incoherent (e.g., combining audio APIs with cryptography APIs in a way that doesn't reflect real usage), the model may overfit to synthetic artifacts.

## Foundational Learning

- **Concept:** Monte Carlo Tree Search (MCTS)
  - **Why needed here:** To efficiently navigate the massive combinatorial space of API interactions without exhaustive search.
  - **Quick check question:** How does the reward function in this paper differ from standard MCTS used in games (e.g., Chess)?

- **Concept:** Uncertainty Estimation (Shannon Entropy)
  - **Why needed here:** To quantify what the model *doesn't* know, allowing the system to prioritize generating data for "hard" examples.
  - **Quick check question:** Why is $-\log_2 P(v|u, \rho)$ used instead of raw probability $P$?

- **Concept:** Supervised Fine-Tuning (SFT) vs. RAG
  - **Why needed here:** To distinguish between teaching a model permanent knowledge (SFT) versus giving it temporary context (RAG); this paper focuses on SFT for low-resource environments where RAG context might be insufficient.
  - **Quick check question:** Why might fine-tuning a 7B model be preferred over just providing a 671B model with documentation via RAG in this specific context?

## Architecture Onboarding

- **Component map:** Ingestion: SDK Documentation → Rule-based Parser → JSON Metadata → KG Construction: JSON → Graph (Nodes: Classes/Methods; Edges: Hierarchy/Membership) → Strategy Engine: LLM calculates Uncertainty Estimation (UE) → MCTS finds high-uncertainty paths → Selects API Seeds (Single & Multi) → Synthesis: Prompt Generator (API Info + Guidelines) → LLM (Question Gen) → LLM (Code Gen) → Post-Processing: De-duplication (Levenshtein distance) → Standardization (LLaMA-Factory format)

- **Critical path:** The reliability of the entire system hinges on the **Parser → KG** step. If the rule-based extraction (Figure 3) misses critical metadata (e.g., `@throws` exceptions), the Code Generation step will produce code that looks correct but fails validation.

- **Design tradeoffs:**
  - **Synthetic vs. Real Data:** Trading the authenticity of human-written code for the availability of unlimited synthetic pairs.
  - **MCTS vs. Random Walk:** Trading computational cost (MCTS simulations) for data diversity and targeted difficulty.
  - **7B Model Focus:** Trading the raw power of large models (GPT-4o) for the efficiency and deployability of fine-tuned smaller models.

- **Failure signatures:**
  - **Low Variance in Questions:** MCTS gets stuck in a local optimum, repeatedly sampling the same high-uncertainty APIs (e.g., a specific complex class) and generating duplicate questions.
  - **Syntax Hallucination:** Generated code uses `any` types or disallowed syntax, indicating the KG metadata injection in the prompt is not being followed or is missing type constraints.
  - **Levenshtein False Positives:** De-duplication step removes valid distinct questions that happen to share similar phrasing.

- **First 3 experiments:**
  1. **Validation of the KG:** Manually inspect 50 random nodes in the KG against the source SDK documentation to verify extraction accuracy (Parameters, Return Types, Descriptions).
  2. **MCTS Sanitize Check:** Run the UE-MCTS module and visualize the selected paths. Do they represent meaningful API combinations (e.g., `ArrayList` + `Iterator`) or random noise?
  3. **Overfit Check:** Train the model on the synthetic data and check if pass@1 improves on the *benchmark* but fails on a manually held-out "real-world" HarmonyOS task.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the APIKG4SYN framework effectively generalize to other low-resource software frameworks or proprietary libraries outside of HarmonyOS?
- **Basis in paper:** [explicit] Section 4 (Threats to Validity) states, "Our current experiments and data generation were conducted solely within low-resource scenarios on HarmonyOS... we plan to extend our deployment to a wider range of low-resource environments in the future."
- **Why unresolved:** The unique characteristics of the HarmonyOS API structure and ArkTS language may have specific advantages that are not present in other low-resource environments.
- **What evidence would resolve it:** Successful replication of the study's performance improvements on a distinct low-resource framework (e.g., a specialized DSL or private library).

### Open Question 2
- **Question:** To what extent does the lack of execution-based verification in the synthetic training data impact the reliability of the fine-tuned model?
- **Basis in paper:** [inferred] Section 2.5 describes generating code solutions via an LLM (DeepSeek-V3) without explicit unit testing of these training samples, whereas the evaluation benchmark relies on unit tests (Section 3.1).
- **Why unresolved:** The paper does not measure how much "noise" (non-executable code) exists in the synthesized training set or if filtering this noise would improve Pass@1 scores.
- **What evidence would resolve it:** An ablation study comparing the current model against a version fine-tuned exclusively on execution-verified synthetic samples.

### Open Question 3
- **Question:** How does the Uncertainty Estimation (UE)-driven MCTS strategy perform regarding computational efficiency when applied to frameworks with significantly larger API knowledge graphs?
- **Basis in paper:** [inferred] The method relies on MCTS to navigate the API graph; while effective for HarmonyOS, the paper does not analyze the scalability or latency of this search process on "high-resource" or more complex legacy frameworks.
- **Why unresolved:** MCTS is computationally expensive, and it is unclear if the "high sum uncertainty" discovery method remains tractable as the API graph grows exponentially larger.
- **What evidence would resolve it:** Performance benchmarks detailing the time and memory costs of the data synthesis pipeline when applied to a large-scale framework (e.g., the full Android SDK).

## Limitations
- **Synthetic data reliability:** Lack of execution-based verification means training data may contain non-functional code that could negatively impact model performance.
- **Framework specificity:** Results are demonstrated only on HarmonyOS, raising questions about generalizability to other low-resource frameworks with different API structures.
- **Scalability concerns:** The computational efficiency of uncertainty-driven MCTS for much larger API ecosystems has not been evaluated.

## Confidence

**High Confidence:** The core architectural approach of using API knowledge graphs for code synthesis is well-grounded in established principles of knowledge retrieval and fine-tuning.

**Medium Confidence:** The quantitative improvements demonstrated on the OHBen benchmark, while promising, require validation on additional low-resource frameworks beyond HarmonyOS.

**Low Confidence:** The scalability of the uncertainty estimation mechanism to significantly larger API ecosystems (e.g., Android with 10x more APIs) has not been demonstrated.

## Next Checks

1. **Cross-Framework Generalization:** Apply APIKG4SYN to another low-resource framework (e.g., Flutter or Tizen) and measure whether the 25.00% pass@1 performance level transfers or degrades significantly.

2. **Human Evaluation of Synthetic Data Quality:** Conduct blind evaluation where developers rate the realism and usefulness of generated questions and code, comparing APIKG4SYN output against both real code examples and baseline synthetic approaches.

3. **Long-Tailed API Coverage Analysis:** Analyze the distribution of API usage in the generated dataset to determine whether MCTS exploration successfully discovers and trains on rare but important API combinations, or whether it remains biased toward frequently occurring patterns.