---
ver: rpa2
title: 'Decoding the Multimodal Maze: A Systematic Review on the Adoption of Explainability
  in Multimodal Attention-based Models'
arxiv_id: '2508.04427'
source_url: https://arxiv.org/abs/2508.04427
tags:
- attention
- used
- multimodal
- explainability
- studies
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This systematic review synthesizes recent research on explainability
  in multimodal attention-based models, focusing on architectures, explanation algorithms,
  and evaluation methods. The review covers 55 studies published between 2020-2024,
  revealing a predominance of vision-language and language-only tasks.
---

# Decoding the Multimodal Maze: A Systematic Review on the Adoption of Explainability in Multimodal Attention-based Models

## Quick Facts
- **arXiv ID:** 2508.04427
- **Source URL:** https://arxiv.org/abs/2508.04427
- **Reference count:** 40
- **Primary result:** Reviews 55 studies showing attention-based models dominate multimodal explainability but often fail to capture full inter-modal interactions, with inconsistent evaluation methods

## Executive Summary
This systematic review synthesizes recent research on explainability in multimodal attention-based models, focusing on architectures, explanation algorithms, and evaluation methods. The review covers 55 studies published between 2020-2024, revealing a predominance of vision-language and language-only tasks. While attention-based models are widely used for explanation, they often fall short in capturing full multimodal interactions. Evaluation methods remain inconsistent and lack systematic approaches, especially for cross-modal dependencies.

The review identifies critical gaps including the need for better algorithms to model grouped interactions, standardized evaluation metrics for cross-modal dependencies, and cognition-aware fusion strategies. Recommendations include streamlining architectural choices, developing more sophisticated explanation algorithms, and adopting rigorous evaluation frameworks. The findings highlight the need for standardized reporting and deeper, modality-specific evaluation to improve the transparency and trustworthiness of multimodal AI systems.

## Method Summary
The review conducted a systematic literature search across Web of Science and Scopus using specific Boolean queries, filtering papers published between January 1, 2020 and January 31, 2024. The selection process used PRISMA guidelines with title/abstract screening followed by full-text review. An LLaMA 3.1 model assisted in screening 65 ambiguous papers using predefined assessment criteria. Snowball sampling from Chefer et al. [39] as seed study added 15% top-cited forward and 30% backward references. The final corpus of 55 papers was analyzed to classify architectures (Early, Hierarchical, Cross-Attention), explanation algorithms, and evaluation metrics.

## Key Results
- **Architecture distribution:** Early fusion (34.6%), Hierarchical (32.7%), Cross-Attention (32.7%) dominate multimodal model architectures
- **Explanation method prevalence:** Attention-based methods comprise 65% of all explanation algorithms, with self-attention alone at 34.6%
- **Evaluation inconsistency:** Only 40.6% of objective evaluations measure faithfulness; human-centered qualitative metrics dominate at 57.3% of evaluation instances

## Why This Works (Mechanism)

### Mechanism 1: Cross-Attention for Inter-Modal Dependency Capture
- **Claim:** Cross-attention layers enable bidirectional modeling of relationships between token representations from distinct modalities
- **Mechanism:** Queries from one modality attend to keys and values from another, producing attention scores that weight how much each source token influences target token representations. The softmax-normalized scores create a probability distribution over cross-modal token pairs
- **Core assumption:** Token-level attention weights meaningfully reflect information flow between modalities (debated in literature per citations [100, 120, 121])
- **Evidence anchors:** [abstract] states attention-based models "often fall short in capturing the full spectrum of interactions between modalities"; [section] Equation 3 defines cross-attention as `CA(Q_T, K_S, V_S) = softmax(Q_T K_S^T / √d_k) V_S`; [corpus] weak direct corpus support—neighbor papers focus on unimodal LLM explainability

### Mechanism 2: Attention-Composite Relevance Propagation
- **Claim:** Combining attention weights with gradient-based attributions produces more faithful explanations than raw attention alone
- **Mechanism:** Transformer attribution (Chefer et al.) computes relevance scores via layer-wise relevance propagation (LRP), then multiplies by attention gradients via Hadamard product: `M̃(k) = E_h(∇M(k) ⊙ R(k))_+`. This is accumulated across layers to produce final relevance maps that capture both attention flow and output sensitivity
- **Core assumption:** Gradient information corrects for attention's inability to reflect localized, discriminative features
- **Evidence anchors:** [section] Equations 10-15 describe relevance update rules that differ for self-attention vs. cross-attention layers; [section] Figure 10 shows visual comparison demonstrating transformer attribution produces sharper object localization than raw attention or Grad-CAM alone

### Mechanism 3: Perturbation-Based Faithfulness Evaluation
- **Claim:** Masking input features by explanation saliency rank reveals whether explanations reflect true model decision dependencies
- **Mechanism:** Positive perturbation masks features from most to least important; a sharp accuracy drop indicates faithful explanations. Negative perturbation masks from least to most; stable performance confirms robustness. AUC is computed across masking levels (10%-90%)
- **Core assumption:** Model predictions should degrade proportionally to the importance of masked features
- **Evidence anchors:** [section] Chefer et al. [39] and Huang et al. [42] applied this to each modality in multimodal models using AUC as primary metric; [section] "Positive perturbation can reflect the sensitivity side of faithfulness, whereas negative perturbation represents the robustness side"

## Foundational Learning

- **Concept:** Self-attention vs. Cross-attention distinction
  - **Why needed here:** Self-attention captures intra-modal token relationships; cross-attention captures inter-modal relationships. The paper shows 34.6% of explanation methods focus on self-attention but fail to capture cross-modal interactions
  - **Quick check question:** Given a vision-language model, which attention type would you inspect to understand why a specific image region influenced the text output?

- **Concept:** Fusion timing (early vs. hierarchical vs. cross-attention)
  - **Why needed here:** Fusion strategy determines where and how modalities interact, directly constraining which explanation methods are applicable. Early concatenation enables dense token interactions but obscures modality-specific contributions
  - **Quick check question:** If you need to attribute a prediction separately to visual vs. textual inputs, which fusion strategy would complicate this analysis most?

- **Concept:** Faithfulness vs. Plausibility in evaluation
  - **Why needed here:** The paper reveals 40.6% of objective evaluation instances measure only faithfulness, yet human-centered qualitative metrics dominate (57.3% of evaluation instances). These criteria can diverge—explanations may be plausible but unfaithful
  - **Quick check question:** An explanation aligns with expert domain knowledge but fails positive perturbation tests. Which criterion does it satisfy?

## Architecture Onboarding

- **Component map:** Input encoders (modality-specific, e.g., ViT for images, BERT for text) -> Fusion module (pre-encoder, mid-network, or late) -> Attention layers (self-attention for intra-modal, cross-attention for inter-modal) -> Decoder/generator or classification head

- **Critical path:** Select fusion architecture based on explainability requirements -> Choose explanation algorithm compatible with architecture (model-agnostic methods work universally, attention-based methods require attention layers, composite methods need gradient access) -> Define evaluation criteria before implementation (faithfulness, localization, or human-centered)

- **Design tradeoffs:**
  - Early fusion (concatenation): Dense interactions, simpler architecture, but harder to disentangle modality contributions
  - Cross-attention variants: Explicit inter-modal modeling, but increases complexity and explanation method constraints
  - Self-explaining models (NLE generation): High-level accessibility, but explanations themselves are black-box outputs

- **Failure signatures:**
  - Explanations highlight only one modality despite multimodal input -> likely unidirectional cross-attention or early fusion without modality-specific analysis
  - Attention maps are diffuse and non-localized -> raw attention without gradient attribution; use attention-composite methods
  - Perturbation tests show no accuracy change -> modalities may be redundant or explanation method is unfaithful

- **First 3 experiments:**
  1. **Baseline attention visualization:** Extract attention weights from final cross-attention layer, visualize as heatmaps over input modalities; document whether cross-modal token pairs are captured
  2. **Perturbation faithfulness test:** Implement positive/negative perturbation on each modality independently using AUC metric; compare explanation methods (raw attention vs. transformer attribution)
  3. **Architecture ablation:** Compare early concatenation vs. cross-attention on the same task using identical explanation algorithm; measure localization accuracy (IoU with ground-truth regions)

## Open Questions the Paper Calls Out

- **Open Question 1:** How can XAI algorithms be designed to effectively capture and quantify "grouped interactions" involving multiple tokens across modalities, rather than limiting analysis to intra-modal or pairwise inter-modal interactions?
  - **Basis:** Section 10.2 states current attention-based methods can only capture token interactions within modalities or between token pairs, failing to model "grouped interactions involving multiple tokens and modalities"
  - **Why unresolved:** Existing post-hoc methods generally rely on pairwise attention weights or gradients, which cannot natively represent the joint contribution of a cluster of visual features combined with a phrase of text
  - **What evidence would resolve it:** Development of algorithms that can decompose a model's output into contributions from defined groups of cross-modal features, validated against human-annotated grouping benchmarks

- **Open Question 2:** What objective evaluation metrics can be developed to specifically measure the quality of explanations regarding cross-modal dependencies?
  - **Basis:** Section 10.5 highlights that "none address multimodal interaction metrics specifically" and calls for "metrics that quantify the effectiveness of explanations in capturing cross-modal dependencies"
  - **Why unresolved:** Current evaluation relies heavily on unimodal faithfulness metrics (like perturbation) or subjective qualitative analysis, lacking a standardized quantitative measure for how well an explanation represents the integration of two distinct data sources
  - **What evidence would resolve it:** A benchmark suite containing "ground-truth" cross-modal dependencies (e.g., specific text-image pairs required for a prediction) against which explanation algorithms can be scored for precision and recall

- **Open Question 3:** How can fusion strategies be explicitly designed to incorporate cognition-aware weighting to reflect the unequal contributions of modalities in decision-making?
  - **Basis:** Section 10.3 recommends developing "cognition- and domain-aware fusion strategies" because existing algorithms often assume equal contribution from all input modalities, contradicting principles of human cognition
  - **Why unresolved:** Most current architectures use static fusion mechanisms (like summation or concatenation) that treat all modalities uniformly, failing to model the context-dependent dominance of one modality over another
  - **What evidence would resolve it:** Fusion architectures that dynamically adjust modality weights based on input context, demonstrating improved performance on tasks where modality importance varies, accompanied by weight visualizations that align with human attention patterns

## Limitations
- **Methodological scope:** Database coverage limited to Web of Science and Scopus may exclude relevant work from non-indexed venues or pre-print servers
- **Attention method bias:** Systematic review focuses on attention-based explanations despite literature questioning their faithfulness as standalone explanation methods
- **Corpus representativeness:** 55-paper corpus represents only a fraction of the rapidly evolving multimodal explainability landscape

## Confidence
- **Cross-attention mechanism:** Medium confidence due to limited direct corpus evidence
- **Perturbation evaluation:** High confidence with clear methodological specification
- **Attention-composite propagation:** Medium confidence with promising visualizations but limited validation
- **SLR methodology:** High confidence in systematic approach but limited by database coverage and temporal constraints

## Next Checks
1. Replicate the LLaMA 3.1 screening process on the 65 ambiguous papers using the exact prompt template to verify the 72.3% agreement rate and resulting corpus composition
2. Conduct a small-scale perturbation analysis on a representative multimodal model to validate the claimed relationship between attention patterns and faithfulness metrics
3. Perform forward snowball sampling from the 55-paper corpus to assess whether the systematic search missed significant recent developments in multimodal explainability