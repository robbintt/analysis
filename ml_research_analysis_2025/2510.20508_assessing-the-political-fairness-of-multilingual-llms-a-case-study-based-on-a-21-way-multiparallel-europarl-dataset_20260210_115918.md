---
ver: rpa2
title: 'Assessing the Political Fairness of Multilingual LLMs: A Case Study based
  on a 21-way Multiparallel EuroParl Dataset'
arxiv_id: '2510.20508'
source_url: https://arxiv.org/abs/2510.20508
tags:
- language
- political
- languages
- translation
- source
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study introduces a new multilingual dataset derived from
  European Parliament speeches, annotated with political affiliations, to assess political
  biases in large language models (LLMs) through translation quality. By translating
  speeches from 21 languages into one another and analyzing differences in translation
  accuracy across political parties, the research reveals systematic disparities:
  mainstream parties from the left, center, and right are consistently translated
  with higher quality than outsider parties.'
---

# Assessing the Political Fairness of Multilingual LLMs: A Case Study based on a 21-way Multiparallel EuroParl Dataset

## Quick Facts
- **arXiv ID**: 2510.20508
- **Source URL**: https://arxiv.org/abs/2510.20508
- **Reference count**: 0
- **One-line primary result**: Mainstream EU political parties receive systematically higher translation quality than outsider parties across 420 language pairs in multilingual LLMs.

## Executive Summary
This study introduces a novel framework for assessing political bias in multilingual language models by measuring translation quality disparities across political party affiliations. Using a 21-way multiparallel corpus of European Parliament speeches, the research demonstrates that mainstream political parties (EPP, S&D, ALDE) consistently receive higher translation quality than outsider parties (NGL, NA) across multiple language pairs and evaluation metrics. The findings suggest that political content influences LLM translation performance in ways that raise concerns about fairness in democratic contexts, particularly given the increasing use of machine translation in political communication.

## Method Summary
The study employs a 21-way multiparallel EuroParl dataset containing speeches from 8 EU political parties across 21 languages. Translation quality is assessed by having LLMs (Qwen3-8B, Llama-3.1-8B) translate 23,386 aligned sentences across 420 language pairs. Performance is measured using sBLEU and COMET metrics, which are then aggregated using a novel Borda count method to enable cross-language-pair comparison. Statistical significance is tested using Kruskal-Wallis tests to determine whether political party affiliation significantly explains variance in translation performance.

## Key Results
- Mainstream political parties (EPP, S&D, ALDE) consistently achieve higher translation quality scores than outsider parties (NGL, NA) across 420 language pairs
- The performance gap between parties is statistically significant according to Kruskal-Wallis tests for most language pairs
- Results are robust across different models (Qwen3-8B, Llama-3.1-8B) and evaluation metrics (sBLEU, COMET)
- Borda count aggregation successfully normalizes for metric calibration issues across different language pairs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The study establishes a correlation between political party affiliation and LLM translation quality, using translation performance as a proxy for political fairness.
- Mechanism: The method leverages a 21-way multiparallel dataset to isolate the variable of political affiliation. By translating speeches from 21 languages into 420 distinct language pairs and scoring each translation, the authors can compare the average quality for each political party. The use of Borda count aggregation allows for ranking parties across metrics (sBLEU, COMET) that are not directly calibrated across language pairs.
- Core assumption: The content (topic, style, vocabulary) of speeches from different political parties varies in a way that is systematically captured by the underlying training data distribution of the LLMs, leading to performance differences not solely attributable to random noise or generic language difficulty.
- Evidence anchors:
  - [abstract]: "systematically compare the translation quality... observing systematic differences with majority parties... being better translated than outsider parties."
  - [section 5.3]: "For most language pairs, the party variable significantly explains the variance of performance according to a Kruskal-Wallis test."
- Break condition: If the variance in translation quality were found to be fully explained by other confounding variables like speaker nationality, specific topic, or speech complexity, not tied to party, the mechanism would be broken.

### Mechanism 2
- Claim: Mainstream political parties (EPP, S&D, ALDE) are consistently ranked higher in translation quality than outsider parties (NGL, NA).
- Mechanism: This is attributed to potential biases in the LLM's pre-training or post-training data. Parties with larger representation in public discourse (e.g., major EU parties) likely have more of their language patterns, rhetoric, and topics represented in the model's training corpus. This leads to better internal representations for generating fluent and accurate translations for these groups.
- Core assumption: The pre-training data for the models (Qwen, Llama) contains a significantly higher volume of text from or about mainstream political discourse compared to text from fringe or outsider parties.
- Evidence anchors:
  - [abstract]: "...systematic differences with majority parties from left, center, and right being better translated than outsider parties."
  - [section 1]: "...biases have so far mostly been assessed through simulated surveys... We propose an alternative framing of political biases, relying on principles of fairness in multilingual translation."
- Break condition: If an analysis of the model's training data showed proportional representation of outsider party discourse, this mechanism would not hold, suggesting the bias arises from the model's architecture or learning dynamics instead.

### Mechanism 3
- Claim: A novel Borda-based aggregation method enables the comparison of translation fairness across different language pairs and metrics.
- Mechanism: Since standard translation metrics like COMET are not well-calibrated across different language pairs (e.g., a score of 85 for English-to-French may represent different quality than 85 for Finnish-to-Estonian), a direct average is unreliable. Borda count converts raw scores into relative rankings within each language pair (0 for worst party, 7 for best). Summing these ranks across all 420 language pairs provides a robust, aggregate fairness score that normalizes for language-pair-specific difficulty.
- Core assumption: The ordinal ranking of parties within a single language pair is a reliable signal of relative fairness, even if the absolute metric scores are not comparable across pairs.
- Evidence anchors:
  - [section 4.1]: "COMET scores are not well calibrated across language pairs... sBLEU, while also not well calibrated across target languages... still enables to compare multiple source languages translating into the same target language."
  - [section 5.3]: "We therefore introduce a novel way to aggregate translation scores... based on Borda counts."
- Break condition: If the relative performance gap between parties varied wildly in magnitude across language pairs in a way that Borda count (which ignores magnitude) could not capture, this aggregation method might be insufficient for nuanced analysis.

## Foundational Learning

- Concept: **Borda Count Aggregation**
  - Why needed here: This is the core statistical method used to solve the problem of cross-language-pair metric comparability. Understanding it is essential to interpret the study's fairness scores.
  - Quick check question: How does converting raw BLEU scores to a Borda rank (e.g., 0-7) solve the problem that BLEU scores are not comparable between an English-to-French pair and a Finnish-to-Estonian pair?

- Concept: **Metric Calibration in MT**
  - Why needed here: The paper's central methodological justification is that metrics like COMET are not "calibrated" across language pairs. This concept is key to understanding why a novel aggregation method was required.
  - Quick check question: Why does a COMET score of 90 for a high-resource language pair not necessarily mean the translation is of the same absolute quality as a 90 for a low-resource language pair?

- Concept: **Group Fairness in NLP**
  - Why needed here: The paper frames its investigation within the context of "group fairness," treating political affiliation as a protected attribute. This ethical and theoretical framework underpins the entire study.
  - Quick check question: How does this study's definition of fairness (equal translation quality for all political parties) differ from a definition of fairness based on equal demographic parity in a classification task?

## Architecture Onboarding

- Component map: 21-EuroParl Dataset -> Translation Pipeline -> Evaluation Layer -> Aggregation Layer
- Critical path: The validity of the entire analysis rests on the sentence alignment quality of the 21-EuroParl dataset. If the parallel sentences are misaligned, all subsequent translation scores and fairness inferences would be compromised.
- Design tradeoffs:
  - Alignment Strictness vs. Data Volume: The authors used a high alignment score threshold (0.8 LaBSE similarity) and enforced 1-to-1 alignments, which reduced the dataset by ~76%. This tradeoff favors data quality over quantity, which is crucial for a fairness study.
  - Metric Choice: The authors use both sBLEU (good for comparing source languages into the same target) and COMET (a neural metric), finding them correlated but not perfectly aligned. Relying on only one could miss biases detected by the other.
- Failure signatures:
  - A high percentage of identical translations for different parties would indicate a lazy model.
  - The study reports that only ~0.2% of translations had a perfect sBLEU score of 100, suggesting low data contamination.
  - A break in consistency between Borda rankings derived from sBLEU vs. COMET would signal the results are metric-dependent and not robust.
- First 3 experiments:
  1. Replicate Core Ranking: For a single model (e.g., Qwen3-8B) and a single target language (e.g., English), calculate the average sBLEU score for each of the 8 EU parties. Manually confirm that mainstream parties (S&D, EPP, ALDE) score higher than outsider parties (NGL, NA).
  2. Validate Metric Correlation: Select 10 diverse language pairs and plot sBLEU scores against COMET scores. Calculate the Pearson correlation to confirm the paper's reported strong correlation (ρ > 0.9).
  3. Test Alignment Sensitivity: Rerun the analysis for a single language pair using a lower alignment threshold (e.g., 0.6 instead of 0.8) and observe if the relative ranking of parties changes significantly. This tests the robustness of the core dataset.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the observed translation disparities arise from the distinct linguistic styles or specific vocabularies used by outsider parties, rather than a direct bias against their political ideology?
- Basis in paper: [explicit] The authors state that further analyses are needed to understand the origin of these differences, suggesting that "differences in topic or style across EU parties" could be a "complementary explanation" to data bias.
- Why unresolved: The study measures correlation between party affiliation and translation quality but does not isolate the causal mechanism (e.g., syntactic complexity vs. semantic bias) driving the performance gap.
- What evidence would resolve it: A controlled analysis comparing translations while normalizing for stylistic complexity, or an experiment using "style-transferred" texts where content is preserved but rhetorical style is altered to match majority parties.

### Open Question 2
- Question: Do human annotators validate the systematic performance gaps detected by automatic metrics like sBLEU and COMET between mainstream and outsider parties?
- Basis in paper: [explicit] The authors explicitly note in the Limitations section that due to the scale of experiments, they "had to rely on automatic metrics" and that "results should be confirmed using human judgments at a smaller scale."
- Why unresolved: Automatic metrics are known to be poorly calibrated across languages and may not capture nuanced semantic errors or pragmatic failures in political discourse.
- What evidence would resolve it: A human evaluation study (e.g., using Multidimensional Quality Metrics) specifically targeting the error types in the outlier parties to see if they align with the automated score drops.

### Open Question 3
- Question: To what extent does the "translationese" effect—where the original spoken language differs from the text's language—influence the perceived political fairness scores?
- Basis in paper: [explicit] The authors identify "the effect of the original language... including the well-known translationese effect" as a "related question" that remains "challenging" because parties are not equally represented in all original languages.
- Why unresolved: The dataset identifies original languages, but the current fairness analysis aggregates over them, potentially confounding "party difficulty" with "source-language interference."
- What evidence would resolve it: A stratified analysis of translation quality that controls for the original spoken language to determine if NGL or NA parties are disadvantaged simply because their speeches originate more frequently from specific source languages.

## Limitations
- The study relies on automatic metrics rather than human evaluation, which may not capture nuanced translation quality differences in political discourse
- The mechanism linking political content to translation performance depends on untested assumptions about the training data composition of evaluated models
- Potential confounding variables (topic distribution, speech complexity, linguistic characteristics) that correlate with party affiliation are not fully controlled for in the analysis

## Confidence
- **High confidence**: The methodological framework (21-way multiparallel dataset, Borda count aggregation for cross-pair comparison) is sound and technically well-executed
- **Medium confidence**: The finding that mainstream parties consistently outperform outsider parties across multiple models and metrics is robust within the dataset, but the interpretation as "political bias" remains speculative
- **Low confidence**: The claim that this bias reflects systematic underrepresentation of outsider parties in model training data is not directly supported by evidence

## Next Checks
1. **Confounding Variable Analysis**: Replicate the analysis while controlling for potential confounders including topic distribution (using topic modeling on the source speeches), speaker nationality, speech length, and linguistic complexity metrics
2. **Training Data Audit**: For the specific models evaluated (Qwen3-8B and Llama-3.1-8B), conduct an analysis of their training data to quantify the representation of mainstream versus outsider political discourse across the 21 languages
3. **Cross-Dataset Validation**: Apply the same analytical framework to an independent multiparallel political corpus (such as UN parliamentary records or national parliamentary transcripts) to test whether the pattern replicates across different political contexts and time periods