---
ver: rpa2
title: 'MixQuant: Pushing the Limits of Block Rotations in Post-Training Quantization'
arxiv_id: '2601.22347'
source_url: https://arxiv.org/abs/2601.22347
tags:
- block
- rotations
- hadamard
- mixquant
- activation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents the first systematic, non-asymptotic analysis\
  \ of outlier suppression for block Hadamard rotations in post-training quantization.\
  \ The analysis reveals that outlier suppression is fundamentally limited by the\
  \ geometry of the input vector, specifically when pre-rotation \u21131 norm mass\
  \ is unevenly distributed across blocks."
---

# MixQuant: Pushing the Limits of Block Rotations in Post-Training Quantization

## Quick Facts
- **arXiv ID**: 2601.22347
- **Source URL**: https://arxiv.org/abs/2601.22347
- **Reference count**: 40
- **Primary result**: Introduces the first non-asymptotic analysis of outlier suppression for block Hadamard rotations in PTQ, showing that rotation effectiveness is fundamentally limited by ℓ1 norm mass concentration across blocks.

## Executive Summary
This paper presents MixQuant, a block rotation-aware PTQ framework that addresses the fundamental limits of outlier suppression in block Hadamard rotations. Through a non-asymptotic analysis, the authors show that rotation effectiveness is governed by the geometric distribution of activation mass across blocks. Based on these insights, MixQuant redistributes activation mass via permutations before rotation using a greedy mass diffusion algorithm. The framework achieves up to 90% recovery of full-vector rotation perplexity when quantizing Llama3 1B to INT4 with block size 16, compared to 46% without permutations.

## Method Summary
MixQuant applies block Hadamard rotations at the down-projection of FFN layers in transformers, with permutations pre-merged into surrounding weights to eliminate inference overhead. The framework uses MassDiff to calibrate permutations by equalizing expected blockwise ℓ1 norms across calibration data, then applies block rotations (random Hadamard or learned Cayley) followed by rounding (Qronos or RTN). The method is implemented in PyTorch + Brevitas and tested on Llama3 and Qwen3 models with various bit-widths.

## Key Results
- Recovers up to 90% of full-vector rotation perplexity (INT4, Llama3 1B, b=16) versus 46% without permutations
- MixQuant* (QuaRot + Qronos) consistently outperforms MR-GPTQ and BRQ across all block sizes
- Permutation merging eliminates inference overhead while maintaining accuracy gains
- Performance gap narrows for MXFP4 format due to group-wise scaling

## Why This Works (Mechanism)

### Mechanism 1: Block Hadamard Rotation Bounds via Local Geometry
- Claim: Block Hadamard rotations suppress outliers when pre-rotation ℓ1 norm mass is evenly distributed across blocks, with worst-case outlier bounded by the block with maximum ℓ1 norm.
- Mechanism: Proposition 3.3 establishes that ∥X̃R∥∞ ≤ max_j δ_j √b ∥X_j∥∞, where δ_j = ∥X_j∥₁/(b∥X_j∥∞) captures per-block mass concentration.
- Core assumption: Hadamard matrix columns are normalized with ∥R_i∥∞ = 1/√b; activation geometry is fixed across inference.
- Evidence anchors: [abstract] "post-rotation outliers are deterministically minimized when the pre-rotation ℓ1 norm mass is evenly distributed across blocks"; [section 3.2, Proposition 3.3] Derives the deterministic bound showing block-wise mass concentration controls outlier suppression.
- Break condition: When mass is highly concentrated in few blocks, reducing block size b increases worst-case outliers with high probability (Proposition 3.5).

### Mechanism 2: Greedy Mass Diffusion for Block Equalization
- Claim: A greedy coordinate assignment algorithm minimizes the maximum per-block ℓ1 norm over calibration data, improving rotation effectiveness.
- Mechanism: MassDiff (Algorithm 1) sorts coordinate indices by descending average magnitude, then greedily assigns each to the block minimizing the resulting average maximum per-block ℓ1 norm across calibration samples.
- Core assumption: Calibration data (e.g., 128 sequences of 2048 tokens) is representative of test-time activation distributions.
- Evidence anchors: [abstract] "greedy mass diffusion algorithm to calibrate permutations by equalizing expected blockwise ℓ1 norms"; [section 4, Algorithm 1] Pseudocode shows the greedy assignment procedure.
- Break condition: If calibration distribution diverges significantly from test distribution, the learned permutation may not transfer.

### Mechanism 3: Permutation-Equivariant Region Merging
- Claim: Permutations can be absorbed into surrounding weight matrices at permutation-equivariant subgraphs, eliminating inference overhead.
- Mechanism: Elementwise activations (Swish, ReLU) are permutation-equivariant along feature dimension. Permutations can be commuted through such regions and merged as W̃₁ = W₁P and W̃₂ = P^T W₂.
- Core assumption: Subgraph contains only elementwise operations and linear layers; no non-equivariant ops (e.g., softmax across features) exist in the region.
- Evidence anchors: [abstract] "permutations are merged into model weights by identifying permutation-equivariant regions in transformer architectures"; [section 4, Definition 4.1, Remark 4.2, Figure 5] Formal definition and FFN merging example.
- Break condition: LayerNorm or BatchNorm in the subgraph breaks equivariance; permutation cannot be commuted through.

## Foundational Learning

- **ℓ1 norm mass concentration (δ)**
  - Why needed: Core geometric quantity determining rotation effectiveness; δ = ∥X∥₁/(d∥X∥∞) ∈ [d⁻¹, 1].
  - Quick check question: Why does δ close to 1 indicate uniform magnitude while δ close to d⁻¹ indicates outlier dominance?

- **Hadamard matrix properties**
  - Why needed: Understanding ∥R_i∥∞ = 1/√k and orthogonal structure enables the theoretical bounds.
  - Quick check question: For a normalized Hadamard matrix R ∈ ℝ^{d×d}, why is the rotation complexity O(d log d) when d is a power of 2?

- **Permutation equivariance**
  - Why needed: Identifying where permutations can be absorbed into weights without changing computation.
  - Quick check question: Is a softmax operation across the feature dimension permutation-equivariant?

## Architecture Onboarding

- **Component map:** MassDiff permutation P → block rotation R̃ → (optional) learned rotations R₁, R₂ → RTN / GPTQ / Qronos rounding → merged weights W̃₁, W̃₂

- **Critical path:**
  1. Identify permutation-equivariant regions (e.g., FFN: up-proj → Swish → gate → down-proj)
  2. Collect calibration activations, run MassDiff to generate P
  3. Update weights: W̃₁ = W₁P, W̃₂ = P^T W₂
  4. Apply merged rotations at R₁, R₂; apply online block rotation at R̃₃
  5. Run rounding algorithm with quantized activations

- **Design tradeoffs:**
  - Block size b: smaller → faster rotation (O(d log b) vs O(d log d)) but worse outlier suppression without permutation
  - Permutation method: MassDiff outperforms random, absmax, zigzag (Table 5)
  - Rotation type: Random Hadamard (MixQuant*) vs learned Cayley (MixQuant†) trades calibration cost for accuracy

- **Failure signatures:**
  - Perplexity degrades sharply at small b without permutation (Table 2: 35.8 → 16.1 for Llama3-1B at b=16)
  - Minimal gains on MX formats (group-wise scaling already mitigates outliers)
  - Calibration overfitting if perplexity improves but zero-shot accuracy drops

- **First 3 experiments:**
  1. Replicate Table 2: Sweep block sizes (16–2048) with/without MassDiff on Llama3-1B INT4 to verify theoretical predictions.
  2. Replicate Table 5: Compare permutation methods (random, absmax, zigzag, MassDiff) on WikiText2 perplexity and zero-shot accuracy.
  3. Replicate Table 3 cross-format analysis: Test INT4, FP4, MXFP4 to identify regimes where permutation-based equalization provides maximal benefit.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can MixQuant's permutation-based equalization be effectively extended to alternative rotation structures, such as Givens matrices, or to explicitly suppress weight outliers?
- **Basis in paper:** [explicit] The authors state, "Our theoretical analysis does not explicitly account for weight outlier suppression or alternative rotation structures... We leave such extensions for future work."
- **Why unresolved:** The current theoretical framework and MassDiff algorithm are specialized for block Hadamard rotations and activation geometry.
- **What evidence would resolve it:** Deriving theoretical bounds for Givens matrices with MassDiff and empirically validating the method on architectures like ButterflyQuant.

### Open Question 2
- **Question:** How do rotation merging strategies, pipeline compositions (e.g., MixQuant* vs. MixQuant†), and rounding algorithms systematically interact to affect quantization accuracy?
- **Basis in paper:** [explicit] The authors note "non-trivial interactions" in ablations and "leave a more systematic exploration of these interactions to future work."
- **Why unresolved:** Ablation results show contradictory trends, such as RTN outperforming Qronos with learned rotations but underperforming with Hadamard rotations.
- **What evidence would resolve it:** A joint ablation study isolating the contribution of each component across different model sizes and bit-widths.

### Open Question 3
- **Question:** Is equalizing blockwise ℓ1 norms the optimal objective for formats that inherently mitigate outliers via group-wise scaling, such as MXFP4?
- **Basis in paper:** [inferred] The authors note in Limitations that the "performance gap between MixQuant and MR-style baselines narrows" for MXFP4, suggesting the worst-case analysis may be loose in this regime.
- **Why unresolved:** The current theory relies on worst-case outlier bounds which may not predict performance accurately when group-wise scaling reduces sensitivity to outliers.
- **What evidence would resolve it:** A study correlating the MassDiff objective function against accuracy degradation specifically for MX-formatted models.

## Limitations

- The theoretical bound is only tight when ℓ1 mass is concentrated in a single block, but the greedy MassDiff algorithm targets average ℓ1 equalization rather than worst-case minimization
- Permutation-equivariant region merging assumes no layer normalization or batch normalization in the subgraph, which is common in practical FFN implementations
- The claim of "zero overhead" assumes fast block Hadamard implementations for non-power-of-2 dimensions across all hardware platforms

## Confidence

- **High confidence**: The deterministic bound in Proposition 3.3 and its connection to worst-case outlier suppression (Mechanism 1). The experimental improvements over baseline methods are reproducible and significant.
- **Medium confidence**: The greedy MassDiff algorithm's effectiveness in practice versus theoretical worst-case bounds, and the assumption that calibration data is representative of test-time distributions.
- **Low confidence**: The claim that permutations can always be merged without overhead, particularly in architectures with normalization layers, and the generalization of MixQuant's benefits across all quantization formats and block sizes.

## Next Checks

1. **Theoretical gap analysis**: Implement MassDiff to minimize worst-case (maximum) per-block ℓ1 norm rather than average, then compare perplexity improvements against the greedy average-minimizing version to test if the theory-practice gap is significant.

2. **Normalization layer stress test**: Evaluate MixQuant on FFN architectures that include LayerNorm in the subgraph to quantify performance degradation when permutation-equivariant region merging assumptions are violated.

3. **Block size scaling validation**: Sweep block sizes from 8 to 4096 on a diverse set of activation distributions (uniform, power-law, bimodal) to verify Proposition 3.5's prediction that smaller blocks worsen outlier suppression when mass is highly concentrated.