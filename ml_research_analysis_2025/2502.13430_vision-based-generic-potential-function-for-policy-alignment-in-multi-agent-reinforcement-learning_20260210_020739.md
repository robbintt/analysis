---
ver: rpa2
title: Vision-Based Generic Potential Function for Policy Alignment in Multi-Agent
  Reinforcement Learning
arxiv_id: '2502.13430'
source_url: https://arxiv.org/abs/2502.13430
tags:
- uni0000000f
- uni00000054
- potential
- reward
- uni00000011
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of aligning multi-agent reinforcement
  learning policies with human common sense in complex, long-horizon tasks. The authors
  propose a hierarchical vision-based reward shaping method called V-GEPF that uses
  a visual-language model (VLM) as a generic potential function to guide policy alignment
  with human understanding.
---

# Vision-Based Generic Potential Function for Policy Alignment in Multi-Agent Reinforcement Learning

## Quick Facts
- **arXiv ID:** 2502.13430
- **Source URL:** https://arxiv.org/abs/2502.13430
- **Reference count:** 40
- **Primary result:** Vision-based reward shaping using VLM potential functions achieves higher win rates and better policy alignment with human common sense in Google Research Football 11v11.

## Executive Summary
This paper addresses the challenge of aligning multi-agent reinforcement learning policies with human common sense in complex, long-horizon tasks. The authors propose a hierarchical vision-based reward shaping method called V-GEPF that uses a visual-language model (VLM) as a generic potential function to guide policy alignment with human understanding. At the top layer, a vLLM-based adaptive skill selection module dynamically selects appropriate potential functions from a pre-designed pool based on instructions, video replays, and training records. The method is theoretically proven to preserve the optimal policy. Experiments in Google Research Football show that V-GEPF achieves higher win rates compared to state-of-the-art methods while effectively aligning policies with human common sense in both easy and hard 11 vs 11 scenarios.

## Method Summary
V-GEPF implements a hierarchical reward shaping framework that uses a visual-language model (VLM) as a generic potential function for policy alignment. The bottom layer computes a dense potential signal by comparing rendered environment states to text instructions using CLIP embeddings. The top layer employs a visual Large Language Model (vLLM) that periodically reviews training statistics and video replays to dynamically select the most appropriate potential function from a pre-designed pool. The shaped reward is computed as the difference in discounted potentials between consecutive states, ensuring theoretical policy invariance. The system is implemented on MAPPO and tested in Google Research Football 11v11 scenarios.

## Key Results
- V-GEPF achieves higher win rates compared to state-of-the-art methods in Google Research Football 11v11 (both easy and hard scenarios)
- The method effectively aligns policies with human common sense while maintaining optimal policy invariance
- vLLM-based adaptive skill selection dynamically addresses policy deficiencies by selecting appropriate potential functions
- Radar charts demonstrate improved policy style metrics (passes, shots, dribbles) aligned with human expectations

## Why This Works (Mechanism)

### Mechanism 1: Cross-Modal Semantic Alignment as Potential
A Visual-Language Model (VLM) functions as a dense potential function by mapping environment states and human instructions to a shared latent space. The system visualizes the environment state into an image, then a pre-trained CLIP model encodes this image and a text instruction into vectors. The cosine similarity between these vectors serves as the potential, allowing semantic "common sense" to guide exploration. This mechanism relies on the VLM's pre-training distribution containing sufficient knowledge to distinguish "good" states from "bad" states in the specific domain based on visual similarity to a text prompt.

### Mechanism 2: Policy Invariance via Potential-Based Shaping
The specific formulation of the reward shaping $F(s,s') = \gamma\phi(s') - \phi(s)$ ensures that the agent is guided toward intermediate goals without altering the final optimal policy. By structuring the shaping reward as the difference in potential between time steps (discounted by $\gamma$), the sum of the shaping rewards over an episode collapses to boundary terms. This guarantees that the optimal policy for the shaped environment is identical to that of the original environment.

### Mechanism 3: Hierarchical Adaptive Curriculum
A visual Large Language Model (vLLM) acts as a meta-controller to dynamically select the most relevant "skill" (instruction) from a pool, preventing the policy from stalling in long-horizon tasks. The vLLM reviews training statistics and video replays periodically to diagnose weaknesses and selects a new instruction/potential function from a pre-designed pool to address the specific deficiency.

## Foundational Learning

- **Concept: Potential-Based Reward Shaping (PBRS)**
  - **Why needed here:** The core contribution relies on PBRS to guarantee that adding a VLM-based dense reward doesn't invalidate the optimal policy.
  - **Quick check question:** If a potential function $\phi(s)$ is added as a reward shaping term, does the optimal policy change if $\phi(s)$ is defined as $F(s, s') = \gamma \phi(s') - \phi(s)$? (Answer: No).

- **Concept: Contrastive Language-Image Pre-training (CLIP)**
  - **Why needed here:** The mechanism hinges on using CLIP to compute the "value" of a state.
  - **Quick check question:** Does the model predict the exact text description of the image, or does it maximize the cosine similarity between the image embedding and the text embedding? (Answer: Similarity).

- **Concept: Centralized Training, Decentralized Execution (CTDE)**
  - **Why needed here:** The method is implemented on MAPPO, a CTDE algorithm.
  - **Quick check question:** Does the VLM potential function compute a reward for the individual agent or the global state? (Answer: Global state, utilized as a shared shaping reward).

## Architecture Onboarding

- **Component map:** Visualizer -> CLIP Encoder -> Potential Calculator -> Reward Shaper -> MAPPO Agent -> vLLM Controller (periodic)
- **Critical path:** State Visualization is the single most brittle component. If the visualization is missing key elements (like player IDs or ball trajectory), the VLM cannot calculate meaningful potential.
- **Design tradeoffs:** Visual Fidelity vs. Inference Speed (running VLM every step is computationally expensive). Adaptability vs. Stability (updating vLLM skills too frequently could destabilize value function approximation).
- **Failure signatures:** Reward Hacking (agents cluster in center to satisfy CLIP), Semantic Drift (VLM gives high similarity for unintended visual artifacts), Stationary Failure (vLLM repeatedly selects same skill despite low win rates).
- **First 3 experiments:** 1) Visualizer Validation: Train agent using only VLM reward with simple instruction like "blue players move right." 2) Coefficient Sweep: Run V-GEPF with different $\rho$ values (0.1, 0.5, 1.0) to confirm optimal balance. 3) Fixed vs. Adaptive Ablation: Compare win rates with fixed instruction vs. adaptive vLLM selection.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but the methodology raises several important considerations about the scalability and generalizability of vision-based potential functions in multi-agent reinforcement learning.

## Limitations
- Visual rendering precision is underspecified, creating potential for semantic misalignment between the VLM's potential and actual game state
- vLLM reasoning robustness is not independently validated through ablation studies
- Generalization outside football is uncertain due to reliance on visual-linguistic priors that may not transfer to other domains

## Confidence
- **High confidence:** Policy invariance proof and MAPPO baseline performance are theoretically sound and empirically validated
- **Medium confidence:** Adaptive skill selection shows promise but lacks rigorous ablation on vLLM reasoning quality
- **Low confidence:** Assumption that CLIP embeddings capture "common sense" football tactics is plausible but untested in controlled settings

## Next Checks
1. **Visualizer Validation:** Run an agent using only the VLM reward (œÅ=1.0, r_env=0) with a simple instruction like "blue players move right." Verify the agent actually moves right, isolating the VLM's causal influence from environmental rewards.
2. **Instruction Pool Ablation:** Compare V-GEPF's performance when using a fixed instruction (e.g., "Attack") versus the adaptive vLLM selection to quantify the contribution of the hierarchical curriculum.
3. **CLIP Embedding Analysis:** Plot win rate versus average potential reward during training. If potential rises while win rate stays flat or drops, the visual-linguistic mapping is flawed and the agent is reward hacking.