---
ver: rpa2
title: Retrieval and Augmentation of Domain Knowledge for Text-to-SQL Semantic Parsing
arxiv_id: '2510.02394'
source_url: https://arxiv.org/abs/2510.02394
tags:
- domain
- query
- retrieval
- refers
- molecule
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of varying accuracy in large language
  models (LLMs) for translating natural language (NL) queries into SQL across different
  databases. The authors propose a systematic framework for associating structured
  domain statements at the database level, as opposed to the query level, to improve
  semantic parsing.
---

# Retrieval and Augmentation of Domain Knowledge for Text-to-SQL Semantic Parsing

## Quick Facts
- arXiv ID: 2510.02394
- Source URL: https://arxiv.org/abs/2510.02394
- Reference count: 40
- Primary result: Structured domain statements with sub-string retrieval achieve 47.5% execution accuracy for IN-set queries, outperforming query-specific oracle hints (42.1%) and unstructured NL hints.

## Executive Summary
This paper addresses the problem of varying accuracy in large language models (LLMs) for translating natural language (NL) queries into SQL across different databases. The authors propose a systematic framework for associating structured domain statements at the database level, as opposed to the query level, to improve semantic parsing. Their method involves soliciting domain knowledge from experts in natural language, translating it into a structured format with clear mappings between NL expressions and SQL snippets, and retrieving relevant statements using a novel sub-string level match approach. Evaluation on eleven realistic database schemas across five open-source and proprietary LLMs demonstrates that their method achieves significantly higher accuracy than existing approaches, including those using query-specific textual domain statements or other retrieval techniques.

## Method Summary
The framework solicits natural language domain statements from experts, structures them into NL-to-SQL mappings, and stores them at the database level. During inference, queries are decomposed into sub-strings and matched against stored domain statements using embedding similarity. The top-K retrieved statements are provided to an LLM along with the schema and query to generate SQL. The method uses a sub-string level retrieval (SbR) approach that decomposes queries into contiguous phrases and matches them against domain statement embeddings, achieving higher precision than whole-query retrieval methods.

## Key Results
- Structured domain statements with explicit NL-to-SQL mappings improve execution accuracy by 5-10% over unstructured NL hints for IN-set queries
- Sub-string level retrieval outperforms whole-query retrieval methods with Evidence F1 of 0.39 vs 0.33-0.37 for baselines
- Database-level knowledge accumulation provides better practical coverage than query-specific oracle hints, with 47.5% vs 42.1% execution accuracy for IN-set queries using GPT-3.5-Turbo

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structured domain statements with explicit NL-to-SQL mappings improve retrieval precision and downstream SQL generation compared to unstructured NL hints.
- Mechanism: Converting free-form NL statements into a templatized format `d.x refers to d.q` creates a semantic "link" where `d.x` serves as a retrieval-friendly text anchor and `d.q` provides executable SQL snippets. This decoupling allows partial query matching while preserving SQL correctness signals.
- Core assumption: Domain experts can provide accurate NL statements, and LLMs can reliably translate them to structured form with optional DBA correction.
- Evidence anchors: [abstract] "translating it into a structured format with clear mappings between NL expressions and SQL snippets"; [Section III-B] "d is of the form 'd.x refers to d.q' where d.x denotes a NL domain expression, and d.q denotes a SQL snippet"

### Mechanism 2
- Claim: Sub-string level retrieval (SbR) outperforms whole-query retrieval because domain statements often relate to query fragments, not entire queries.
- Mechanism: Queries are decomposed into contiguous sub-strings within length bounds `[|d.x|-n, |d.x|+n]`. Each sub-string embedding is compared against pre-indexed `d.x` embeddings. The score `Score(t, d) = max SIM(d.x, sub-string)` selects the best matching DS per fragment. Top-K DSs are aggregated for context augmentation.
- Core assumption: Relevant domain knowledge maps to contiguous query phrases; the threshold `n` bounds sub-string explosion while preserving recall.
- Evidence anchors: [abstract] "retrieving relevant statements using a novel sub-string level match approach"; [Section III-C] "A standard method of retrieval augmentation is to match the query embedding t to the embeddings of each stored d. This approach is inadequate for our case, as a d may be relevant only to a part of a query"

### Mechanism 3
- Claim: Database-level knowledge accumulation with retrieval-time selection is more practical and accurate than query-specific oracle hints.
- Mechanism: Domain statements are solicited incrementally as experts identify expressions requiring clarification, stored at DB-level, and retrieved on-demand. This shifts from unrealistic per-query annotation to reusable, query-agnostic knowledge bases. Retrieval provides ~5-10% execution accuracy gains over oracle QS for IN-set queries.
- Core assumption: Enterprise DBs have stable schemas and recurring domain vocabulary; the DS repository grows over time to cover common expressions.
- Evidence anchors: [abstract] "associating structured domain statements at the database level, as opposed to the query level"; [Section I] "Soliciting DSs from domain experts for each new query posed by the end-user on the DB in real-time is unrealistic"

## Foundational Learning

- Concept: Embedding-based semantic similarity (cosine similarity over BERT/MPNet embeddings)
  - Why needed here: Core to retrieving DSs by matching query sub-strings to `d.x` embeddings; enables fuzzy semantic matching beyond lexical overlap.
  - Quick check question: Can you explain why cosine similarity is invariant to vector magnitude and suitable for comparing embeddings of different-length text spans?

- Concept: Prompt engineering with in-context examples (few-shot ICL)
  - Why needed here: Used to translate NL domain statements to structured format via LLM few-shot prompting; also for final SQL generation with retrieved DSs in context.
  - Quick check question: Given a 4-shot prompt for DS structuring, how would you diagnose if the LLM is overfitting to exemplar patterns vs. generalizing to new domain statements?

- Concept: Execution accuracy vs. syntactic accuracy for SQL evaluation
  - Why needed here: The paper uses execution accuracy as the primary metric because semantically equivalent SQL may have different syntax; this is critical for fair evaluation.
  - Quick check question: Why might execution accuracy be misleading if the database has limited or skewed data? How would you complement it with additional metrics?

## Architecture Onboarding

- Component map: DS Solicitation Layer -> DS Structuring Pipeline -> Embedding Index -> Retrieval Engine (SbR) -> SQL Generation Module

- Critical path: Domain expert identifies ambiguous NL expression → submits d_NL → Structuring pipeline generates d → DBA validates/corrects → Embedding index updated for d.x → At query time: SbR retrieves top-K DSs → LLM generates SQL → Execute SQL, return results, optionally log failures for future DS expansion

- Design tradeoffs:
  - **K value (retrieval count)**: Lower K (4) for IN-set precision; higher K (10) for OUT-set recall tradeoff
  - **DBA validation vs. automation**: Fully automated structuring risks error propagation; DBA correction adds latency but improves reliability
  - **Sub-string length bounds**: Tighter bounds reduce computation but may miss longer expressions; looser bounds increase cost and noise

- Failure signatures:
  - **Retrieval misses**: Query contains domain expression not in DS repository → OUT-NO set performance drops (5.1% below QS for GPT-3.5-Turbo)
  - **Incorrect structuring**: LLM-generated d contains wrong SQL snippet → downstream SQL errors; DBA review mitigates but doesn't eliminate
  - **Context overload**: Providing all DSs instead of top-K reduces accuracy (All-DS-Str: 36.6% vs SbR-Str: 42.6% for consistent K=10) due to LLM distraction

- First 3 experiments:
  1. **Baseline replication**: Implement SbR retrieval on BirdSQL Dev split; compare execution accuracy against No-DS, All-DS-NL, and oracle QS baselines using GPT-3.5-Turbo. Target: replicate ~47.5% IN-set accuracy.
  2. **Ablation on structuring**: Compare SbR-NL (retrieving unstructured NL DSs) vs. SbR-Str vs. SbR-L-Str (LLM-structured, no DBA) to quantify structuring and correction contributions. Expect: SbR-Str > SbR-L-Str > SbR-NL.
  3. **Sub-string vs. whole-query retrieval**: Benchmark SbR against BM25, BE, MS-M, STSb, and sBSR baselines on Evidence F1 and execution accuracy. Target: confirm SbR Evidence F1 ~0.39 vs. baselines 0.33-0.37.

## Open Questions the Paper Calls Out

- **Question**: Can the framework automatically identify unresolved domain-specific expressions in user queries and route them to experts?
- **Basis in paper**: [explicit] The authors state in the Conclusion, "In future, for new NL user queries we plan to automatically identify unresolved domain-specific expressions and route to domain experts for disambiguation."
- **Question**: How can the pipeline accommodate real-time updates to domain repositories in frequently changing enterprise databases?
- **Basis in paper**: [explicit] The Conclusion notes the plan to "extend our pipeline to accommodate real-time updates of the domain repository" to handle DBs that change often.
- **Question**: Is the reliance on labeled NL-SQL pairs for tuning the sub-string retrieval threshold a bottleneck for zero-shot domains?
- **Basis in paper**: [inferred] Section III-C states, "For a new DB, we assume the availability of a few NL-SQL pairs to tune the threshold $n$."
- **Question**: Can the DS Structuring phase be fully automated to close the performance gap between LLM-generated and DBA-screened statements?
- **Basis in paper**: [inferred] Section IV-E (RQ4) shows that DBA-screened structured DSs (Str) outperform LLM-generated ones (L-Str), particularly for the OUT-Set (36.6% vs 32.4% for GPT-3.5).

## Limitations

- Knowledge structuring quality: The effectiveness heavily depends on the accuracy of LLM-generated structured domain statements, with no systematic evaluation of error propagation to SQL generation.
- Retrieval specificity tradeoff: The sub-string level retrieval may retrieve DSs that are semantically similar but not contextually appropriate for complete queries.
- Database schema diversity: The evaluation uses 11 BirdSQL databases, but the diversity of schema complexity and domain vocabulary is not characterized.

## Confidence

- **High confidence**: Structured domain statement format improves retrieval precision; Database-level knowledge accumulation is more practical; Execution accuracy is more appropriate metric
- **Medium confidence**: Sub-string retrieval consistently outperforms whole-query retrieval; 77.67% lexical overlap is representative; Parameter choices (K=4/10) are optimal

## Next Checks

1. **Ablation study on knowledge structuring**: Implement the pipeline without DBA correction and measure the degradation in execution accuracy. Compare LLM-only structuring (SbR-L-Str) against LLM-with-DBA structuring (SbR-Str) across multiple databases.

2. **Retrieval precision analysis**: For a subset of queries where SbR retrieves incorrect DSs, manually analyze whether the retrieved statements are semantically related but contextually inappropriate. Calculate precision@k for retrieved DSs and correlate with SQL generation errors.

3. **Schema diversity stress test**: Evaluate the method on databases with varying structural characteristics (e.g., highly normalized vs. denormalized schemas, different relationship patterns). Measure execution accuracy variance across schema types to identify structural patterns that may challenge the retrieval mechanism.