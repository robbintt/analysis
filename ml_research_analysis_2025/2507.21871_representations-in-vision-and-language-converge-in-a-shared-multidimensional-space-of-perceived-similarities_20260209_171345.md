---
ver: rpa2
title: Representations in vision and language converge in a shared, multidimensional
  space of perceived similarities
arxiv_id: '2507.21871'
source_url: https://arxiv.org/abs/2507.21871
tags:
- visual
- rdms
- brain
- representational
- similarity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates whether visual and linguistic similarity\
  \ judgements converge in a shared representational space by having participants\
  \ arrange 100 natural scene images and corresponding captions according to perceived\
  \ similarity. Using representational similarity analysis, the study found strong\
  \ convergence between visual and linguistic similarity judgements (Spearman \u2374\
  \ = 0.781) and that both predicted similar brain response patterns in high-level\
  \ visual areas during natural scene viewing."
---

# Representations in vision and language converge in a shared, multidimensional space of perceived similarities

## Quick Facts
- arXiv ID: 2507.21871
- Source URL: https://arxiv.org/abs/2507.21871
- Reference count: 0
- Strong convergence found between visual and linguistic similarity judgments (Spearman ⍴ = 0.781)

## Executive Summary
This study investigates whether visual and linguistic similarity judgments converge in a shared representational space. Using the Multiple Arrangements task, participants arranged 100 natural scene images and corresponding captions by perceived similarity. The research found strong convergence between visual and linguistic similarity judgments and demonstrated that both predicted similar brain response patterns in high-level visual areas during natural scene viewing. Computational models trained to predict large language model embeddings outperformed category-trained and AlexNet controls in explaining the behavioral similarity structure, suggesting humans encode knowledge using a multidimensional, embedding-like structure.

## Method Summary
The study employed the Multiple Arrangements (MA) task where participants arranged 100 natural scene images or corresponding sentence captions by perceived similarity in a circular arena. Behavioral representational dissimilarity matrices (RDMs) were generated from 63 participants across three sessions. Searchlight analysis with 6-voxel radius was performed on 7T fMRI data from 8 NSD participants to extract brain RDMs. Cross-validated non-negative least squares (NNLS) regression weighted individual participants' RDMs to predict neural or model representations. Two RCNN variants were trained on MS COCO (excluding NSD stimuli) to predict either MPNet embeddings or category labels, compared against AlexNet baseline.

## Key Results
- Strong convergence between visual and linguistic similarity judgments (Spearman ⍴ = 0.781 fixed-effects)
- Both visual and linguistic similarity judgments predicted similar brain response patterns in high-level visual areas during natural scene viewing
- LLM-trained RCNNs outperformed category-trained and AlexNet controls in explaining behavioral similarity structure

## Why This Works (Mechanism)

### Mechanism 1: Second-Order Isomorphism in Cross-Modal Representations
The mind recognizes relational structure between concepts independent of first-order visual experience. Similarity judgments from text alone can predict brain responses to corresponding images even without prior exposure, suggesting conceptual representations preserve relational structure of objects observed in the environment rather than being arbitrary products of modality-specific input.

### Mechanism 2: LLM Embeddings as Approximations of Human Conceptual Structure
LLM embeddings trained on large-scale linguistic corpora capture statistical world-knowledge that parallels how humans encode conceptual relationships. Visual neural networks trained to predict LLM embeddings produce representations that better match human similarity judgments than category-based training objectives because category labels collapse rich semantic structure into orthogonal bins.

### Mechanism 3: High-Level Visual Cortex as a Domain-General Relational Encoder
Mid- and high-level visual areas encode abstract relational structure that can be activated by both visual and linguistic inputs. The visual system transforms sensory inputs into modality-agnostic representations reflecting stable real-world relational patterns, allowing linguistic inputs to reactivate this structure through mental reconstruction and enabling cross-modal prediction.

## Foundational Learning

- **Representational Similarity Analysis (RSA)**: Core methodological framework for comparing representational geometries across modalities without requiring direct feature correspondence. *Quick check*: Can you explain why RSA uses representational dissimilarity matrices (RDMs) rather than raw activations to compare different neural systems?

- **Multiple Arrangements (MA) Task**: Behavioral paradigm for recovering high-dimensional representational geometry from pairwise similarity judgments on complex naturalistic stimuli. *Quick check*: How does the weighted-averaging approach with iterative "lift-the-weakest" stimulus selection correct for placement errors and capture multidimensional similarity relations?

- **Cross-Validated Non-Negative Least Squares (NNLS) Regression**: Method for creating behavior-predicted RDMs that optimally weight individual participants' similarity judgments to predict neural or model representations while controlling for overfitting. *Quick check*: Why is the non-negativity constraint important when weighting participants' RDMs to predict brain activity?

## Architecture Onboarding

- **Component map**: Input Layer (100 images + captions) -> Behavioral Layer (MA task producing visual/linguistic RDMs) -> Model Layer (RCNNs + AlexNet + MPNet) -> Neural Validation Layer (7T fMRI searchlight RDMs) -> Comparison Layer (RSA correlations)

- **Critical path**: 1) Collect behavioral similarity judgments via MA task -> generate individual participant RDMs 2) Average RDMs across participants or apply NNLS weighting with 10-fold cross-validation 3) Extract model representations and compute searchlight brain RDMs 4) Correlate behavior-predicted RDMs with observed brain/model RDMs via Pearson correlation 5) Test significance with FDR-corrected statistical testing

- **Design tradeoffs**: Fixed-effects RSA shows strong group-level convergence but may overestimate generalizability; Session 1 data controls for cross-modal familiarity but reduces statistical power; NNLS fitting optimizes prediction accuracy but requires control analyses to rule out inflation artifacts

- **Failure signatures**: Low correlation between visual and linguistic behavioral RDMs; category-trained RCNNs matching or outperforming LLM-trained RCNNs; linguistic similarity judgments predicting only classical language areas, not visual cortex; cross-modal convergence emerging only after participants experience both modalities

- **First 3 experiments**: 1) Replicate with new image-caption pairs outside MS COCO to test generalization 2) Test LLM-trained RCNN predictions on congenitally blind participants during spoken scene description 3) Perform dimension-wise ablation on MPNet embeddings to identify semantic features driving convergence

## Open Questions the Paper Calls Out

- Does reading sentence captions recruit the same neural populations in high-level visual cortex as viewing natural scenes? The current study only modeled brain responses evoked by viewing images using linguistic similarity judgments, but did not measure brain activity while participants performed the linguistic task.

- Does the shared relational similarity structure exist for non-visual domains such as sound or motion perception? The current experiment restricted stimuli to natural scene images and their corresponding text captions, leaving other sensory modalities unexplored.

- What distinct representational dimensions do visual and linguistic inputs uniquely contribute to the shared multidimensional space? While the study demonstrates convergence, it does not disentangle the specific features unique to the visual modality (e.g., perceptual details) versus the linguistic modality (e.g., semantic abstractions) within the embedding structure.

## Limitations

- Fixed-effects RSA approach may overestimate the generalizability of cross-modal convergence; random-effects analysis showed substantially weaker alignment (⍴ = 0.161)
- LLM-trained RCNN advantage could partially reflect training data differences rather than embedding objective per se
- Limited evidence for whether the shared representational space reflects learned associations versus inherent conceptual structure

## Confidence

- **High**: Visual and linguistic similarity judgments converge onto a shared representational geometry (⍴ = 0.781 fixed-effects, robust across sessions)
- **Medium**: LLM embeddings better capture human conceptual structure than category labels (model comparison significant but alternative explanations possible)
- **Low**: Visual cortex represents domain-general relational structure activated by linguistic inputs (evidence correlational; causal direction not established)

## Next Checks

1. Replicate cross-modal convergence using stimulus sets outside MS COCO to test generalizability across domains
2. Test whether LLM-trained RCNN representations predict neural responses in congenitally blind participants during spoken scene description
3. Perform ablation analysis on MPNet embeddings to identify which semantic dimensions (e.g., animacy, spatial relations) drive convergence with human similarity judgments