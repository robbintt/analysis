---
ver: rpa2
title: 'MGM-Omni: Scaling Omni LLMs to Personalized Long-Horizon Speech'
arxiv_id: '2509.25131'
source_url: https://arxiv.org/abs/2509.25131
tags:
- speech
- audio
- arxiv
- generation
- mgm-omni
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MGM-Omni introduces a unified Omni LLM that supports long-form
  multimodal understanding and robust long-duration speech generation with personalized
  voices. Its dual-track architecture separates multimodal reasoning (MLLM) from real-time
  speech synthesis (SpeechLM), enabling efficient cross-modal interaction within an
  end-to-end framework.
---

# MGM-Omni: Scaling Omni LLMs to Personalized Long-Horizon Speech

## Quick Facts
- arXiv ID: 2509.25131
- Source URL: https://arxiv.org/abs/2509.25131
- Authors: Chengyao Wang; Zhisheng Zhong; Bohao Peng; Senqiao Yang; Yuqi Liu; Haokun Gui; Bin Xia; Jingyao Li; Bei Yu; Jiaya Jia
- Reference count: 19
- Primary result: Unified Omni LLM achieving 1.5% WER (English) and 1.8% CER (Chinese) on understanding, with 2.28% WER (English) and 1.28% CER (Chinese) on long-form speech generation, plus 3x faster inference.

## Executive Summary
MGM-Omni introduces a unified Omni LLM capable of long-form multimodal understanding and robust long-duration speech generation with personalized voices. Its dual-track architecture separates multimodal reasoning (MLLM) from real-time speech synthesis (SpeechLM), enabling efficient cross-modal interaction within an end-to-end framework. For understanding, it employs a dual audio encoder that fuses acoustic and semantic cues, yielding robust long-form audio perception. For generation, Chunk-Based Parallel Decoding bridges the token-rate gap between text and speech, enabling efficient, low-latency synthesis, while conditioning SpeechLM on reference audio to support zero-shot voice cloning with consistent timbre.

## Method Summary
MGM-Omni employs a dual-track architecture: an MLLM (based on Qwen2.5-VL) for multimodal understanding and a SpeechLM (Qwen3 with 6-block TTS-Adapter) for speech generation. The MLLM uses a dual audio encoder (Qwen2-Audio + Belle-Whisper-large-v3) with information mining via cross-attention to fuse acoustic and semantic cues for robust long-form audio perception. The SpeechLM addresses the text-to-speech token-rate mismatch using Chunk-Based Parallel Decoding (parallel size=4) and an extended vocabulary (|V|=|V_text|+4|V_speech|). Training occurs in two stages for each track: MLLM first undergoes audio-to-text pre-training, then unified omni-modal training; SpeechLM pre-trains with a frozen LLM and TTS-Adapter, then jointly optimizes both. The system is trained on ~400k hours of audio data and evaluated on benchmarks including LibriSpeech, CommonVoice, AISHELL, AIR-Bench, and proposed Long-TTS-Eval.

## Key Results
- Achieves 1.5% WER (English) and 1.8% CER (Chinese) on audio understanding benchmarks, outperforming existing models.
- For long-form speech generation, achieves 2.28% WER (English) and 1.28% CER (Chinese) with 3x faster inference than baselines.
- Demonstrates superior timbre consistency and context-aware speech synthesis with zero-shot voice cloning from reference audio.

## Why This Works (Mechanism)
MGM-Omni's effectiveness stems from its dual-track architecture that separates multimodal reasoning from real-time speech synthesis, allowing each component to be optimized for its specific task while maintaining end-to-end integration. The dual audio encoder with information mining captures both acoustic and semantic features, providing robust long-form audio perception. Chunk-Based Parallel Decoding bridges the fundamental token-rate mismatch between text and speech, enabling efficient, low-latency synthesis without sacrificing quality. The conditioning of SpeechLM on reference audio enables zero-shot voice cloning with consistent timbre across long sequences, addressing a key limitation of previous systems.

## Foundational Learning
- **Dual Audio Encoding**: Combining acoustic (Qwen2-Audio) and semantic (Belle-Whisper-large-v3) encoders captures complementary information for robust audio understanding. *Why needed*: Single encoders often miss either fine-grained acoustic details or high-level semantic meaning. *Quick check*: Compare performance using only one encoder vs. the dual setup on audio understanding benchmarks.
- **Information Mining via Cross-Attention**: Cross-attention between dual encoders enables information fusion and knowledge transfer. *Why needed*: Simple concatenation would lose interaction patterns between acoustic and semantic features. *Quick check*: Measure information gain by comparing cross-attention outputs to concatenated inputs.
- **Chunk-Based Parallel Decoding**: Processing speech generation in chunks bridges the text-speech token-rate gap while maintaining streaming capability. *Why needed*: Direct autoregressive generation is too slow for real-time applications due to the token-rate mismatch. *Quick check*: Measure inference latency with vs. without chunk-based decoding on fixed-length inputs.
- **Two-Stage Training**: Separate pre-training and post-training phases allow specialized optimization before unified fine-tuning. *Why needed*: Joint training from scratch often fails to converge properly for complex multi-task models. *Quick check*: Compare final performance using two-stage vs. one-stage training procedures.
- **Reference Audio Conditioning**: Conditioning SpeechLM on reference audio enables zero-shot voice cloning. *Why needed*: Traditional TTS requires extensive speaker-specific training data. *Quick check*: Measure speaker similarity (SIM) with vs. without reference audio conditioning on the same content.

## Architecture Onboarding

**Component Map**: Raw Audio -> Dual Audio Encoder (Qwen2-Audio + Belle-Whisper) -> Information Mining (Cross-Attention) -> MLLM (Qwen2.5-VL) -> Text Output; Text Input + Reference Audio -> SpeechLM (Qwen3 + TTS-Adapter) -> Chunk-Based Parallel Decoding -> Flow-Matching + HiFi-GAN -> Speech Output

**Critical Path**: Understanding path: Raw audio → dual encoder → information mining → MLLM → text output. Generation path: Text + reference audio → SpeechLM → chunk-based decoding → flow-matching → vocoder → speech output.

**Design Tradeoffs**: The dual-track architecture trades some end-to-end optimization potential for specialized efficiency and performance in each domain. Chunk-based decoding sacrifices some global coherence for real-time capability and efficiency.

**Failure Signatures**: Token-rate mismatch causing misaligned prosody manifests as robotic or unnatural speech patterns. Timbre degradation over long sequences appears as voice quality degradation or identity drift. Audio understanding failures show up as mistranscriptions or missed semantic content in long-form audio.

**Three First Experiments**:
1. Validate dual audio encoder performance on short audio clips (1-2 minutes) comparing WER/CER against single-encoder baselines.
2. Test chunk-based parallel decoding efficiency and quality on 5-minute speech generation with fixed reference audio.
3. Evaluate speaker similarity preservation over increasing sequence lengths (1min, 5min, 10min) using the same reference audio.

## Open Questions the Paper Calls Out
None

## Limitations
- Key hyperparameters (learning rates, batch sizes, epochs) and detailed TTS-Adapter architecture remain underspecified, creating reproduction challenges.
- The information mining module via cross-attention and unified batching implementation details are not fully disclosed.
- Performance on very long-form generation (>10 minutes) and real-world streaming efficiency are claimed but not exhaustively validated.
- Results rely on GPT-4 evaluation for AIR-Bench and self-defined benchmarks (Long-TTS-Eval), introducing potential variability.

## Confidence

**High Confidence**: Core architectural innovation (dual-track MLLM + SpeechLM, dual audio encoders, Chunk-Based Parallel Decoding) and reported benchmark results on standard datasets (LibriSpeech, CommonVoice, AISHELL) are well-specified and reproducible in principle.

**Medium Confidence**: Zero-shot voice cloning capability with timbre consistency over long sequences and streaming inference performance claims are supported by results but depend on exact hyperparameter tuning not fully provided.

**Low Confidence**: Specific operational details of the information mining module, exact TTS-Adapter design, and precise unified batching implementation introduce uncertainty in faithful reproduction.

## Next Checks

1. **Hyperparameter and Implementation Audit**: Request and verify the complete set of training hyperparameters (learning rates, batch sizes, epochs, optimizer configurations) for all stages, the exact architecture of the TTS-Adapter (number of layers, hidden dimensions, attention mechanisms), and the detailed preprocessing pipeline for long audio inputs.

2. **Open-Source Benchmark Validation**: Replicate the core results (WER/CER on LibriSpeech, CommonVoice, AISHELL; speaker similarity on Seed-TTS-Eval) using the released model weights and code on a held-out test set not used in the original training or validation, ensuring no data leakage.

3. **Long-Form Generation Stress Test**: Conduct a systematic evaluation of the model's performance on generating and maintaining timbre consistency over sequences exceeding 10 minutes, measuring WER/CER degradation, speaker similarity drop-off, and real-time factor (RTF) under continuous streaming conditions.