---
ver: rpa2
title: Enhancing Long Document Long Form Summarisation with Self-Planning
arxiv_id: '2512.17179'
source_url: https://arxiv.org/abs/2512.17179
tags:
- summary
- summaries
- sentences
- highlights
- linguistics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of generating accurate and faithful
  long-form summaries from lengthy documents. The authors propose a highlight-guided
  generation framework that first identifies key sentences from the input document
  and then uses these highlights to guide summary generation.
---

# Enhancing Long Document Long Form Summarisation with Self-Planning

## Quick Facts
- arXiv ID: 2512.17179
- Source URL: https://arxiv.org/abs/2512.17179
- Authors: Xiaotang Du; Rohit Saxena; Laura Perez-Beltrachini; Pasquale Minervini; Ivan Titov
- Reference count: 40
- Primary result: Two-stage highlight-guided generation improves factual consistency by up to 35% on long documents while maintaining relevance

## Executive Summary
This paper addresses the challenge of generating accurate and faithful long-form summaries from lengthy documents. The authors propose a highlight-guided generation framework that first identifies key sentences from the input document and then uses these highlights to guide summary generation. Two approaches are explored: an end-to-end method that generates highlights and summary in one pass, and a two-stage pipeline that separates these steps. Experiments on GovReport and QMSum datasets show that the two-stage pipeline with generative highlights significantly improves factual consistency (up to 35% gains in SummaC scores) while maintaining relevance, outperforming both direct prompting and extractive methods like LexRank.

## Method Summary
The proposed highlight-guided generation (HiGen) framework treats summarization as a two-stage process: D→H→S (document to highlights to summary) rather than direct D→S. In Stage 1, an LLM extracts k highlights (key sentences) from the document using a carefully designed prompt. In Stage 2, a fresh LLM generates the summary conditioned on both the original document and the extracted highlights, with explicit instruction to focus only on the highlights. The paper compares this two-stage pipeline against direct prompting, LexRank (graph-based extractive method), and ContextCite (perturbation-based attribution method). The framework is evaluated on GovReport and QMSum datasets using ROUGE-L for relevance, BERTScore-F1 for semantic similarity, SummaC for factual consistency, and FactScore for atomic fact verification.

## Key Results
- Two-stage HiGen pipeline outperforms direct prompting on long, information-dense documents (GovReport)
- Generative highlights produce significantly better factual consistency (up to 35% improvement in SummaC scores) compared to extractive methods
- End-to-end approach shows comparable performance to direct prompting on QMSum but degrades on GovReport, highlighting the importance of explicit content planning for complex documents
- ContextCite attribution-based highlights result in longer, less focused summaries compared to generative highlights

## Why This Works (Mechanism)

### Mechanism 1
Separating content selection from generation improves factual consistency by reducing cognitive load. The two-stage pipeline forces explicit content planning before generation, allowing the model to focus on synthesizing information rather than simultaneously selecting and synthesizing. This separation is particularly beneficial for complex documents with dense information where the model might otherwise attend to irrelevant portions during generation.

### Mechanism 2
Generative highlights outperform attribution-based extraction by synthesizing contextual information rather than only extracting verbatim sentences. When the LLM generates highlights, it can rephrase scattered information into coherent statements while preserving relationships, converting dialogue utterances into concise propositions. Attribution methods like ContextCite rank sentences by importance scores but cannot reorganize information or handle speaker-utterance relationships effectively.

### Mechanism 3
Conditioning generation on explicit highlights provides traceability anchors that reduce hallucination. By instructing the model to generate summaries "only focusing on the extracted sentences," the highlights serve as verifiable content boundaries. Each claim in the summary can theoretically be traced to a specific highlight, constraining the model's generation space and reducing the likelihood of generating unsupported content.

## Foundational Learning

- **Content Planning in Summarization**: Why needed here: The core innovation is treating summarization as D→H→S rather than D→S, requiring understanding of intermediate representations. Quick check: Can you explain why entity chains, keyphrases, and sentence highlights represent different granularities of content plans?

- **Factual Consistency Metrics (SummaC, FactScore)**: Why needed here: The paper's main claim rests on factual consistency improvements; understanding these metrics is essential for interpreting results. Quick check: How does SummaC's sentence-level entailment approach differ from FactScore's atomic fact verification?

- **Attribution Methods in LLMs**: Why needed here: The paper compares generative highlights against perturbation-based attribution (ContextCite), requiring understanding of both approaches. Quick check: Why would occlusion-based attribution struggle with dialogue documents compared to generative extraction?

## Architecture Onboarding

- **Component map**: Document D → Stage 1 (Highlight Extraction) → Highlights H → Stage 2 (Summary Generation) → Summary S
- **Critical path**: Highlight quality determines downstream summary quality; poor highlights lead to incomplete summaries. Prompt design for extraction significantly affects highlight informativeness. Two-stage separation requires managing context window carefully.
- **Design tradeoffs**: End-to-end vs. Two-stage (faster vs. better quality on dense documents); Generative vs. Attribution-based highlights (coherent vs. comprehensive); Number of highlights (coverage vs. guidance dilution).
- **Failure signatures**: Positional bias in generative highlights from underlying LLM; verbose summaries with ContextCite; end-to-end degradation on GovReport.
- **First 3 experiments**: 1) Baseline comparison of direct prompting, LexRank→generate, and HiGen two-stage on 50-document sample; 2) Highlight quality ablation with k∈{15, 30, 50}; 3) Prompt sensitivity test with 2-3 extraction prompt variants.

## Open Questions the Paper Calls Out

- **Extension to multi-document summarization**: The framework could be extended to long narratives or multiple documents by integrating it with hierarchical or iterative frameworks, but this remains untested.
- **Positional bias impact**: The generative highlighting approach can exhibit positional bias in content coverage inherent to the underlying LLM, but the extent and impact of this bias is not quantified.
- **Prompt sensitivity**: Variations in prompt instructions may lead to different extracted sentences, but the paper does not perform systematic analysis of prompt sensitivity.

## Limitations

- Limited to single-document summarization where document length fits within context window
- Evaluation restricted to GovReport and QMSum datasets, limiting generalizability
- Manual optimization of extraction prompts without systematic sensitivity analysis

## Confidence

- **High confidence**: Factual consistency improvements (35% gains in SummaC scores) - directly supported by quantitative metrics across both datasets
- **Medium confidence**: Two-stage pipeline superiority on dense documents - supported by experimental results but based on only two datasets
- **Low confidence**: Generative highlights' contextual coherence advantage - lacks systematic comparison across diverse document types

## Next Checks

1. **Cross-domain replication**: Apply the two-stage HiGen pipeline to a third long-document domain (e.g., scientific papers or legal documents) with 50-100 samples, measuring both ROUGE-L and SummaC to verify generalization beyond GovReport and QMSum.

2. **Prompt sensitivity study**: Systematically vary the extraction prompt structure (structured vs. free-form output) and number of requested highlights (k∈{15, 30, 50}) to quantify robustness and identify optimal configuration for different document lengths.

3. **Hallucination attribution analysis**: Select 10 summaries from each method (direct prompting, LexRank, ContextCite, HiGen) and trace every factual claim back to its source in the original document to quantify actual hallucination rates and verify that highlights genuinely improve traceability.