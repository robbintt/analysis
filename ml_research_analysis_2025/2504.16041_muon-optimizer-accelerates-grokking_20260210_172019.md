---
ver: rpa2
title: Muon Optimizer Accelerates Grokking
arxiv_id: '2504.16041'
source_url: https://arxiv.org/abs/2504.16041
tags:
- grokking
- muon
- arxiv
- optimizer
- softmax
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether optimizer choice affects grokking
  onset in Transformer models on modular arithmetic tasks. Muon optimizer was compared
  to AdamW across seven tasks, with softmax variants (softmax, stablemax, sparsemax)
  as a secondary factor.
---

# Muon Optimizer Accelerates Grokking

## Quick Facts
- arXiv ID: 2504.16041
- Source URL: https://arxiv.org/abs/2504.16041
- Reference count: 14
- Optimizer Muon reduces grokking onset by ~33% compared to AdamW (mean epoch: 102.89 vs 153.09, p = 6.33e-08)

## Executive Summary
This study demonstrates that the Muon optimizer significantly accelerates the grokking phenomenon in Transformer models on modular arithmetic tasks. Across seven tasks (modular addition, multiplication, division, exponentiation, gcd, and parity), Muon reduced the mean epoch to reach 95% validation accuracy by approximately 50 epochs compared to AdamW. The acceleration is attributed to Muon's orthogonalized gradient updates and spectral norm constraints, which help models escape memorization basins and maintain stable training dynamics. Results show that optimizer update geometry critically influences the transition from memorization to generalization.

## Method Summary
The study compares Muon optimizer against AdamW on seven modular arithmetic tasks using small Transformer models with identity embeddings, RoPE positional encoding, RMSNorm, and SiLU activation. Datasets include modular operations mod 97 (9409 samples each) and 10-bit parity (1024 samples). Train/validation splits vary by task (50-80%). Three softmax variants (standard, stablemax, sparsemax) are tested as a secondary factor. Muon incorporates orthogonalized gradient updates and spectral norm constraints, while AdamW uses standard β1=0.9, β2=0.98 settings. Grokking epoch is defined as the first epoch where validation accuracy ≥ 95% after training accuracy nears 100%. Experiments run on Nvidia H100 GPUs with multiple random seeds per condition.

## Key Results
- Muon reduced mean grokking epoch from 153.09 to 102.89 (t = 5.0175, p = 6.33e-08)
- Effect is consistent across all seven tasks and three softmax variants
- Optimizer choice had larger effect size than softmax variant selection
- Muon produced tighter distribution of grokking epochs compared to AdamW

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Orthogonalized gradient updates encourage broader exploration of the loss landscape, helping models escape memorization basins.
- **Mechanism:** Muon applies orthogonalization to gradient updates, preventing gradient directions from collapsing onto narrow memorization paths. This forces the optimizer to seek more structured, generalizable solutions.
- **Core assumption:** Memorization in grokking is associated with narrow, non-orthogonal gradient directions.
- **Evidence anchors:** Abstract mentions spectral norm constraints and second-order information; Figure 1 explicitly states Muon "Uses orthogonalized gradient updates" which "Helps escape memorization and discover real patterns."

### Mechanism 2
- **Claim:** Spectral norm constraints prevent "runaway weights" and maintain stability during the high-variance memorization phase.
- **Mechanism:** By constraining the spectral norm of weight matrices, Muon limits the capacity of the network to scale weights excessively to fit noise. This implicit regularization acts similarly to weight decay but is structural rather than element-wise.
- **Core assumption:** The delay in grokking is partly caused by unstable weight growth during the initial overfitting phase.
- **Evidence anchors:** Abstract attributes acceleration to "spectral norm constraints"; Figure 1 lists "Applies spectral norm constraints" to "Keep training stable and avoid softmax collapse."

### Mechanism 3
- **Claim:** Update geometry aligned with layer shape (second-order information) ensures all layers learn at a synchronized pace.
- **Mechanism:** Muon approximates second-order information (curvature) to scale updates relative to the matrix shape of the layer. This prevents specific layers from lagging or racing ahead, ensuring the network moves coherently toward the generalization circuit.
- **Core assumption:** Grokking requires a coordinated shift in multiple layers, which standard AdamW struggles to time correctly.
- **Evidence anchors:** Figure 1 states "Matches update size to layer shape" and "ensures all layers learn at the right pace"; paper notes Muon "incorporates... approximations of second-order information."

## Foundational Learning

- **Concept: Grokking**
  - **Why needed here:** This is the target phenomenon—a phase transition from memorization to generalization. Understanding that it is a *delayed* event is crucial for interpreting the results.
  - **Quick check question:** If a model reaches 100% training accuracy but 50% validation accuracy, has it "grokked"? (Answer: No, grokking requires the subsequent jump to high validation accuracy).

- **Concept: Spectral Norm**
  - **Why needed here:** Muon distinguishes itself from AdamW primarily via spectral norm constraints. One must understand this refers to the largest singular value of a weight matrix (a measure of "magnitude" or "sensitivity" of a layer).
  - **Quick check question:** Does constraining spectral norm increase or decrease a model's sensitivity to input perturbations? (Answer: Decrease).

- **Concept: Modular Arithmetic**
  - **Why needed here:** The tasks possess a rigid algebraic structure. The optimizer must navigate a finite input space to find a generalizable rule rather than a lookup table.
  - **Quick check question:** Why is modular arithmetic preferred for studying grokking over image classification? (Answer: It has a definitive, crisp generalization boundary and small data regime).

## Architecture Onboarding

- **Component map:** Discrete integers -> Identity Embeddings -> Transformer Blocks (Multi-head Attention + FFN) -> RMSNorm -> SiLU/Softmax variants -> Output
- **Critical path:** The interaction between the Optimizer's Spectral Constraint and the Softmax Numerical Stability appears to be the critical interaction. If the optimizer allows weights to explode (AdamW), the softmax mechanism may destabilize (softmax collapse), delaying the circuit formation required for generalization.
- **Design tradeoffs:** Trading the stability and familiarity of AdamW for faster grokking with Muon. Muon may require more compute per step due to spectral decomposition (orthogonalization).
- **Failure signatures:** Stagnant Memorization (train ≈ 0, val ≈ random), Softmax Collapse (divergence/NaNs), Unstable Grokking (high variance in grokking epoch).
- **First 3 experiments:**
  1. Baseline Replication: Run modular addition (mod 97) task using exact hyperparameters to verify ~50-epoch reduction in grokking onset.
  2. Ablation on Spectral Norm: Implement "Muon-lite" that removes spectral norm constraint but keeps momentum settings to isolate if speedup comes from constraint or orthogonalization.
  3. Softmax Stability Stress Test: Train with standard Softmax + AdamW vs. Stablemax + AdamW to quantify how much baseline grokking delay is due to numerical instability vs. optimizer geometry.

## Open Questions the Paper Calls Out

- **Question:** Does the Muon optimizer accelerate grokking in larger-scale Transformer architectures?
  - **Basis:** Conclusion explicitly recommends "investigating the generalizability of these findings to larger model architectures."
  - **Why unresolved:** Experiments were restricted to small algorithmic datasets and standard Transformer setups.
  - **What evidence would resolve it:** Replicating experimental protocol on models with significantly higher parameter counts (e.g., 100M+ parameters).

- **Question:** Is Muon-induced acceleration observed in diverse task domains beyond modular arithmetic?
  - **Basis:** Conclusion suggests investigating "diverse task domains."
  - **Why unresolved:** Study focused primarily on modular arithmetic tasks (GCD, division, etc.) and a parity task.
  - **What evidence would resolve it:** Testing optimizer on non-arithmetic tasks known to exhibit grokking, such as NLP or symbolic reasoning benchmarks.

- **Question:** How does Muon interact with other regularization techniques besides weight decay?
  - **Basis:** Conclusion calls for research on "the interplay between Muon and other regularization techniques."
  - **Why unresolved:** Current setup isolated optimizer differences by keeping weight decay equivalent but did not test interactions with dropout or data augmentation.
  - **What evidence would resolve it:** A factorial design study combining Muon with varying strengths of auxiliary regularization methods.

## Limitations

- Model architecture and optimizer hyperparameters require external code repository consultation for complete reproduction
- No ablation studies to isolate which component of Muon (orthogonalization, spectral constraints, or second-order information) drives the acceleration
- Generalization performance beyond 95% threshold is not reported, leaving questions about final solution quality
- Absence of negative results or failure conditions where Muon does not accelerate grokking

## Confidence

- **High Confidence**: Statistical significance of acceleration effect (t = 5.0175, p = 6.33e-08) and broad consistency across seven tasks
- **Medium Confidence**: Attribution of acceleration specifically to Muon's unique features rather than other implementation details
- **Low Confidence**: Claims about superiority of Muon for final generalization performance beyond the threshold metric

## Next Checks

1. **Ablation Study on Spectral Norm Constraints**: Create and test a "Muon-lite" variant that removes the spectral norm constraint while maintaining momentum and orthogonalization components to isolate the primary driver of acceleration.

2. **Final Generalization Performance Analysis**: Extend evaluation beyond 95% threshold to measure final validation accuracy and generalization gap (train vs validation) for both optimizers across all tasks to determine if Muon's advantage extends beyond phase transition acceleration.

3. **Stability and Failure Mode Investigation**: Systematically test Muon's performance across a range of learning rates and model capacities to identify conditions where optimizer fails or produces unstable training, establishing boundaries of effectiveness.