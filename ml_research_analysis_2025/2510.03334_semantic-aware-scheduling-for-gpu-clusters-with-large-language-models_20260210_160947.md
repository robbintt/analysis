---
ver: rpa2
title: Semantic-Aware Scheduling for GPU Clusters with Large Language Models
arxiv_id: '2510.03334'
source_url: https://arxiv.org/abs/2510.03334
tags:
- uni00000014
- jobs
- uni00000012
- scheduling
- schedmate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SchedMate introduces a new paradigm of semantic-aware scheduling
  that addresses the critical semantic gap in deep learning schedulers. Traditional
  schedulers operate on limited metadata, leading to high profiling overhead, unreliable
  duration estimation, inadequate failure handling, and poor observability.
---

# Semantic-Aware Scheduling for GPU Clusters with Large Language Models

## Quick Facts
- arXiv ID: 2510.03334
- Source URL: https://arxiv.org/abs/2510.03334
- Reference count: 40
- Primary result: LLM-powered semantic-aware scheduling reduces average job completion times by up to 1.91x

## Executive Summary
SchedMate introduces a new paradigm of semantic-aware scheduling that addresses the critical semantic gap in deep learning schedulers. Traditional schedulers operate on limited metadata, leading to high profiling overhead, unreliable duration estimation, inadequate failure handling, and poor observability. SchedMate bridges this gap by extracting deep insights from overlooked unstructured data sources: source code, runtime logs, and historical jobs. The system employs three LLM-powered components: a Scheduling Advisor that analyzes source code to find similar historical jobs for accurate workload prediction, a Metric Tracker that non-intrusively extracts performance metrics from logs for real-time observability, and a Failure Handler that performs automated root cause analysis on logs to identify and recover from infrastructure failures.

## Method Summary
SchedMate uses LLM agents to extract semantic insights from unstructured data sources to enhance deep learning scheduling. The Scheduling Advisor employs a ReAct agent to extract structured workload metadata from source code, then uses embedding-based similarity search to find historically similar jobs for duration prediction. The Metric Tracker uses a two-stage approach: embedding-based log classification to filter progress lines, followed by LLM extraction of performance metrics. The Failure Handler uses binary search guided by semantic log classification to locate root-cause errors, then LLM classification to determine if automated infrastructure recovery is appropriate. The system is implemented on a Ray-based scheduler with vLLM serving, Redis for vector storage, and integrates with SLURM clusters.

## Key Results
- Reduces average job completion times by up to 1.91x compared to state-of-the-art schedulers
- Achieves 85.7% accuracy with relative errors less than 100% for duration prediction
- Metric Tracker handles noisy log data with 84.3% success rate in metric extraction
- Failure Handler achieves 68.2% F1-score in identifying infrastructure failures

## Why This Works (Mechanism)

### Mechanism 1: Semantic Similarity-Based Duration Estimation
- **Claim**: Retrieving semantically similar historical jobs from source code metadata enables more accurate duration prediction than profiling or metadata-only ML models.
- **Mechanism**: An LLM agent extracts structured workload metadata (model architecture, dataset, training config) from source code using filesystem tools. This metadata is embedded and compared against historical job vectors via cosine similarity. The top-k similar jobs' average duration becomes the prediction.
- **Core assumption**: Jobs with similar code-level workload characteristics exhibit similar runtime distributions, and sufficient historical jobs exist for retrieval.
- **Evidence anchors**: [abstract] "85.7% accuracy with relative errors less than 100% for duration prediction"; [Section 5.4, Figure 12] SchedMate achieves relative errors <100% for 85.7% of estimations vs. Lucid at 27.7%.

### Mechanism 2: Non-Intrusive Runtime Observability via Two-Stage Log Parsing
- **Claim**: Embedding-based log classification followed by targeted LLM extraction enables real-time metric extraction from noisy logs with acceptable latency.
- **Mechanism**: Log Classifier computes cosine similarity between line embeddings and category vectors (progress, error, info, warn). Only progress-flagged lines proceed to LLM-based Metric Extractor, which parses step_time and loss values. Processing in reverse chronological order with early termination bounds latency.
- **Core assumption**: Jobs log training progress in parseable formats; embedding models generalize across logging styles.
- **Evidence anchors**: [abstract] "Metric Tracker successfully handles noisy log data with 84.3% success rate"; [Section 5.4, Figure 13] Metric Tracker achieves 84.3% success rate vs. 59.2% for pure LLM; 8.7s vs. 9.8s latency per log.

### Mechanism 3: Binary Search-Guided Failure Localization for Automated Recovery
- **Claim**: Binary search using semantic log classification efficiently locates root-cause errors in large logs, enabling accurate LLM-based failure classification and automated infrastructure failure recovery.
- **Mechanism**: Failure Locator uses binary search on log chunks, guided by Log Classifier's error detection, to find the first error message. A 500-line context window around this point goes to LLM for classification (infrastructure vs. application). Infrastructure failures trigger predefined recovery (isolate node, restart from checkpoint).
- **Core assumption**: First error triggers cascading errors creating semi-sorted log structure; infrastructure failures are distinguishable and recoverable via restart.
- **Evidence anchors**: [abstract] "Failure Handler achieves 68.2% F1-score in identifying infrastructure failures"; [Section 5.4, Table 4] Failure Handler (7B model) achieves 68.2 F1, outperforming RCACopilot (43.0 F1).

## Foundational Learning

- **Concept: Retrieval-Augmented Workload Prediction**
  - **Why needed here**: The Scheduling Advisor relies on embedding-based similarity search between job metadata vectors. Understanding approximate nearest neighbor search, embedding models (e.g., BGE-m3), and vector databases is essential.
  - **Quick check question**: Can you explain why cosine similarity on embedded metadata outperforms exact string matching for finding similar training jobs?

- **Concept: ReAct-Style LLM Agents with Tool Use**
  - **Why needed here**: The Metadata Extractor uses a ReAct agent with file_tree_tool and file_read_tool to navigate codebases. Understanding tool-augmented LLMs and iterative reasoning loops is critical.
  - **Quick check question**: How does a ReAct agent decide when to stop exploring files and output structured metadata?

- **Concept: Semantic Log Classification via Embedding Similarity**
  - **Why needed here**: The Metric Tracker and Failure Handler both depend on classifying log lines by semantic type using pre-computed category embeddings. This is not keyword matching but dense vector comparison.
  - **Quick check question**: Why might an embedding-based log classifier generalize better across different logging frameworks than regex-based parsers?

## Architecture Onboarding

- **Component map**: Scheduling Advisor: Metadata Extractor (LLM agent) → Similar Job Retriever (embedding search via Redis) → Workload Estimator; Metric Tracker: Log Classifier (embedding filter) → Metric Extractor (LLM parser) → Scheduler feedback loop; Failure Handler: Failure Locator (binary search) → Failure Classifier (LLM) → Recovery Actions (node isolation, job restart)
- **Critical path**: Job submission → Metadata Extractor (source code analysis) → Similar Job Retriever → Duration estimate → Scheduler decision → Runtime: Metric Tracker monitors logs → If interference detected → Packing cancellation → On failure: Failure Handler locates root cause → If infrastructure failure → Automated recovery
- **Design tradeoffs**: LLM model size vs. latency/accuracy (7B models offer good balance; 32B slightly better but slower); embedding filter threshold vs. recall (aggressive filtering reduces LLM load but may miss relevant log lines); profiling bypass vs. cold-start accuracy (no profiling saves overhead but relies entirely on historical similarity); automated recovery vs. false positive risk (misclassified application failures could trigger unnecessary restarts)
- **Failure signatures**: No similar jobs found (k=0) → falls back to default profiling or baseline estimator; Metric extraction fails → scheduler continues with default behavior, no packing cancellation; Failure misclassification → application bugs may trigger incorrect recovery attempts; infrastructure failures may go unhandled; LLM API timeout → queue jobs, serve stale predictions, or degrade to metadata-only scheduling
- **First 3 experiments**: 1) Reproduce duration estimation accuracy: Run Scheduling Advisor on Mars trace subset; compare relative error distribution against Lucid's GA²M model (target: 85.7% predictions with <100% error); 2) Validate Metric Tracker on noisy logs: Inject synthetic noise into progress logs; measure success rate and latency vs. pure LLM baseline (target: >80% success rate, <10s latency); 3) Test Failure Handler on labeled failures: Sample 100 failed jobs with known root causes; compute F1-score for infrastructure vs. application classification (target: ~68% F1)

## Open Questions the Paper Calls Out

- Can semantic-aware scheduling principles from SchedMate be effectively adapted to other domains such as big data analytics and serverless computing?
- How can SchedMate improve scheduling decisions for novel workloads with no semantically similar historical jobs in the database?
- Can fine-tuned smaller LLMs (e.g., <1B parameters) achieve comparable performance to larger models for SchedMate's extraction and classification tasks?
- How does SchedMate's latency and throughput scale when deployed on clusters with 10,000+ GPUs handling thousands of concurrent job submissions?

## Limitations

- Effectiveness critically depends on availability of sufficiently similar historical jobs for novel architectures
- Semantic extraction assumes well-structured codebases and may fail on minimalist or obfuscated implementations
- Failure recovery mechanism limited to infrastructure-level issues, leaving application-level bugs requiring manual intervention
- Performance on heterogeneous GPU clusters or with mixed workload types remains unverified

## Confidence

- **High Confidence**: The fundamental architecture combining semantic extraction with LLM-based analysis is sound, as demonstrated by measurable improvements across all three components
- **Medium Confidence**: The 1.91x JCT improvement is based on trace-based simulations that may not capture all real-world dynamics
- **Low Confidence**: The long-term scalability of the embedding-based approach as the historical database grows, and the system's behavior under extreme failure conditions or with adversarial log patterns

## Next Checks

1. **Cold-start scenario testing**: Evaluate Scheduling Advisor performance when no sufficiently similar historical jobs exist, measuring fallback accuracy against baseline profiling methods
2. **Cross-framework log compatibility**: Test Metric Tracker on logs from diverse deep learning frameworks (PyTorch Lightning, TensorFlow, custom frameworks) to validate embedding-based generalization claims
3. **Automated recovery precision**: Conduct systematic analysis of false positive and false negative rates in failure classification to quantify risk of inappropriate automated interventions