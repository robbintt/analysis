---
ver: rpa2
title: Uncertainty in Machine Learning
arxiv_id: '2510.06007'
source_url: https://arxiv.org/abs/2510.06007
tags:
- uncertainty
- data
- aleatoric
- epistemic
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This chapter provides a comprehensive framework for understanding
  and quantifying uncertainty in machine learning models. It distinguishes between
  epistemic (model) and aleatoric (data) uncertainty, explaining how each type affects
  predictions differently.
---

# Uncertainty in Machine Learning

## Quick Facts
- arXiv ID: 2510.06007
- Source URL: https://arxiv.org/abs/2510.06007
- Authors: Hans Weytjens; Wouter Verbeke
- Reference count: 0
- Provides comprehensive framework for quantifying epistemic and aleatoric uncertainty in ML models

## Executive Summary
This chapter presents a comprehensive framework for understanding and quantifying uncertainty in machine learning models. It distinguishes between epistemic (model) and aleatoric (data) uncertainty, explaining how each type affects predictions differently. The work introduces practical methods for quantifying uncertainty across various model types including linear regression, random forests, neural networks, and conformal prediction, with emphasis on real-world business applications.

## Method Summary
The chapter presents four main techniques for uncertainty quantification: (1) OLS confidence intervals using t-distribution for linear regression; (2) Random forests with entropy-based uncertainty decomposition (total entropy Hh, aleatoric entropy Ch, epistemic entropy Ih = Hh-Ch); (3) Bayesian neural networks using dropout at inference time to approximate posterior distributions through T=50 stochastic forward passes; (4) Conformal prediction that provides finite-sample coverage guarantees through quantile-based thresholding on held-out calibration data. The methods are demonstrated on datasets including Iris (classification), simulated sinusoidal data (regression), and Covertype (conformal prediction).

## Key Results
- Dropout at inference time approximates Bayesian posterior distributions, enabling epistemic uncertainty quantification in neural networks
- Entropy decomposition separates aleatoric from epistemic uncertainty in ensemble classifiers via mutual information
- Conformal prediction provides model-agnostic finite-sample coverage guarantees without distributional assumptions
- Accuracy-rejection curves enable practical evaluation of uncertainty estimates for business decision-making

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dropout at inference time approximates Bayesian posterior distributions in neural networks, enabling epistemic uncertainty quantification.
- Mechanism: During inference with dropout enabled, each forward pass uses a randomly thinned network. The variance across T stochastic passes approximates the integral over the posterior weight distribution. Point estimates come from averaging outputs; epistemic uncertainty from their sample variance.
- Core assumption: The trained model's weights, regularized with L2, approximate a Bayesian posterior; calibration data and test data are exchangeable.
- Evidence anchors:
  - [section 1.6.1] Gal and Ghahramani [2016] showed mathematically how dropout, in combination with an additional L2 regularizer, can be used to approximate BNNs.
  - [section 1.6.1] Equations 1.2 and 1.3 define the point estimate as the mean of T forward passes and the variance decomposition.
  - [corpus] Weak direct corpus evidence on MC dropout; neighboring papers focus on surrogate modeling and entropic approaches, not dropout-based BNNs.
- Break condition: If calibration and test distributions drift (non-exchangeable), variance estimates no longer reflect true epistemic uncertainty.

### Mechanism 2
- Claim: In ensemble classifiers, entropy decomposition separates aleatoric from epistemic uncertainty via mutual information.
- Mechanism: Total uncertainty = Shannon entropy of the mean predicted probability vector. Aleatoric uncertainty = mean entropy of individual model predictions. Epistemic uncertainty = mutual information (total - aleatoric), capturing inter-model disagreement.
- Core assumption: The ensemble spans the hypothesis space; individual models are calibrated enough that their probability outputs are meaningful.
- Evidence anchors:
  - [section 1.5.2] The total uncertainty for the prediction of observation xh can be defined as the (Shannon) entropy Hh... Mutual information... corresponds to the epistemic uncertainty.
  - [section 1.5.2] Table 1.4 provides intuition: ensembles with unanimous certain predictions have zero epistemic uncertainty; split votes yield high epistemic uncertainty.
  - [corpus] "Extending the Entropic Potential of Events for Uncertainty Quantification" discusses entropy-based UQ, supporting the general approach.
- Break condition: If ensemble members are highly correlated or underfit, epistemic estimates become unreliable (low disagreement even when uncertainty is high).

### Mechanism 3
- Claim: Conformal prediction provides finite-sample coverage guarantees for prediction sets without distributional assumptions.
- Mechanism: Hold out a calibration set, compute conformal scores (e.g., 1 - predicted probability for true class), set threshold at the k-th quantile where k = ceil((n+1)(1-α)). Prediction sets include all classes with probability ≥ 1 - threshold.
- Core assumption: Calibration and test data are exchangeable (typically satisfied by i.i.d. splits).
- Evidence anchors:
  - [section 1.7.1] It works with all data distributions, without any assumptions... The finite-sample validity constitutes a major advantage of CF.
  - [section 1.7.2] Example achieves 80.35% coverage for 80% target on covertype dataset.
  - [corpus] Weak corpus evidence; conformal prediction not directly addressed in neighbor papers.
- Break condition: If exchangeability fails (e.g., temporal drift between calibration and deployment), coverage guarantees degrade.

## Foundational Learning

- Concept: Bayesian inference and posterior distributions
  - Why needed here: Bayesian neural networks weight uncertainty as distributions rather than point estimates; understanding p(ω|X,y) is prerequisite for MC dropout intuition.
  - Quick check question: Can you explain why marginalizing over a posterior gives a predictive distribution rather than a single prediction?

- Concept: Shannon entropy and mutual information
  - Why needed here: Random forest uncertainty quantification relies on entropy decomposition to separate aleatoric from epistemic components.
  - Quick check question: If two classifiers predict (0.5, 0.5) each, is the total uncertainty high, and is it aleatoric or epistemic?

- Concept: Quantiles and calibration sets
  - Why needed here: Conformal prediction requires selecting the (1-α) quantile from conformal scores on a held-out calibration set.
  - Quick check question: Why does conformal prediction require a separate calibration set rather than using training data directly?

## Architecture Onboarding

- Component map: Data split (Train/Calibration/Test) -> Model (RF, NN, linear) -> Uncertainty head (entropy calculator, T stochastic passes, conformal score calculator) -> Decision layer (accuracy-rejection curve -> threshold -> accept/reject/human-escalate)

- Critical path:
  1. Train model with uncertainty capability (dropout in NN, ensemble for RF, or standard model for CP)
  2. Compute uncertainty estimates on test set
  3. Build accuracy-rejection curve to correlate uncertainty with error
  4. Set threshold based on target accuracy or coverage level
  5. Route predictions above threshold to human review

- Design tradeoffs:
  - MC dropout: Low overhead but assumes L2 + dropout ≈ BNN; may underestimate uncertainty on out-of-distribution data
  - Entropy-based (RF): Requires ensemble; fast inference but assumes probability outputs are calibrated
  - Conformal: Model-agnostic with guarantees, but requires held-out calibration data and exchangeability; prediction sets can be large for ambiguous cases

- Failure signatures:
  - High epistemic uncertainty on in-distribution data → model underfitting or insufficient ensemble diversity
  - Aleatoric uncertainty not decreasing with more features → missing explanatory variables or label noise
  - Conformal coverage significantly below target → calibration/test distribution mismatch
  - Accuracy-rejection curve flat → uncertainty estimates uncorrelated with error (calibration problem)

- First 3 experiments:
  1. Implement MC dropout inference on a regression NN with T=50 passes; plot epistemic vs aleatoric uncertainty across domain; verify epistemic rises outside training range.
  2. Train random forest on classification task; compute Hh, Ch, Ih for test set; build accuracy-rejection curves and compare which uncertainty type better predicts errors.
  3. Apply conformal prediction to a pretrained classifier using 20% calibration split; verify coverage matches 1-α on held-out test; measure average prediction set size.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is the strict disentanglement of epistemic and aleatoric uncertainty achievable in practice, or is their distinction fundamentally dependent on available knowledge?
- Basis in paper: [explicit] The paper states that "aleatoric and epistemic uncertainty are not fully distinguishable, but rather context-dependent" (Page 6).
- Why unresolved: The authors illustrate that uncertainty types can shift (e.g., dice rolls appearing aleatoric but being epistemic with perfect measurement), suggesting the boundary is fluid rather than absolute.
- What evidence would resolve it: A theoretical proof or empirical framework that can robustly separate the two components regardless of the observer's context or missing variables.

### Open Question 2
- Question: How can models be engineered to reliably estimate aleatoric uncertainty in regions characterized by high epistemic uncertainty (data sparsity)?
- Basis in paper: [inferred] The authors note that in regions lacking training observations, "there is no way for the model to evaluate the aleatoric uncertainty" (Page 17).
- Why unresolved: The paper shows that while Bayesian Neural Networks successfully flag epistemic uncertainty in sparse regions, their ability to characterize data noise (aleatoric) collapses without local training samples.
- What evidence would resolve it: An architecture or loss function that decouples noise estimation from data density, validated by stable aleatoric predictions during extrapolation tasks.

### Open Question 3
- Question: Can rigorous methods be developed to evaluate uncertainty estimates directly, bypassing the reliance on indirect proxies like accuracy-rejection curves?
- Basis in paper: [inferred] The text notes that "there is no ground truth for uncertainty, uncertainty estimates cannot be evaluated directly" (Page 13).
- Why unresolved: Current evaluation relies on correlation (e.g., high uncertainty linking to low accuracy) rather than measuring the correctness of the uncertainty magnitude itself.
- What evidence would resolve it: A standardized benchmark dataset with known, injected noise distributions allowing for a direct statistical comparison between predicted uncertainty and true variance.

## Limitations

- Dropout-as-BNN approximation breaks down with improper regularization or highly non-linear posteriors
- Entropy-based uncertainty assumes well-calibrated probability outputs, which is rarely true for modern classifiers
- Conformal prediction requires held-out calibration data, reducing training data and potentially suffering from distribution drift

## Confidence

- Epistemic/Aleatoric Decomposition: High
- MC Dropout as Bayesian Approximation: Medium  
- Entropy-Based Uncertainty in Random Forests: High
- Conformal Prediction Coverage Guarantees: Medium (theoretical) / Low (empirical)

## Next Checks

1. **Distributional Robustness Test**: Evaluate MC dropout uncertainty estimates on out-of-distribution samples (e.g., corrupted CIFAR-10 or shifted covariate distributions) to verify epistemic uncertainty genuinely increases when model is uncertain.

2. **Calibration Verification**: Perform reliability diagram analysis on random forest probability outputs to confirm entropy-based uncertainty is meaningful; recalibrate if necessary using temperature scaling or isotonic regression.

3. **Coverage Validation**: Implement conformal prediction on a temporal dataset with train/validation/test split by time; measure actual coverage on test set and compare to theoretical 1-α to assess exchangeability assumption violation.