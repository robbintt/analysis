---
ver: rpa2
title: 'PluriHop: Exhaustive, Recall-Sensitive QA over Distractor-Rich Corpora'
arxiv_id: '2510.14377'
source_url: https://arxiv.org/abs/2510.14377
tags:
- questions
- answer
- question
- document
- documents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work addresses a class of questions in question answering
  (QA) requiring aggregation of data across all documents in a knowledge base without
  a clear stopping point for retrieval. These "pluri-hop" questions are formalized
  by three criteria: recall sensitivity (omitting even a single relevant passage leads
  to an incorrect answer), exhaustiveness (it is impossible to infer from the retrieved
  context whether the evidence set is complete), and exactness (there is only one
  best answer to the question).'
---

# PluriHop: Exhaustive, Recall-Sensitive QA over Distractor-Rich Corpora

## Quick Facts
- arXiv ID: 2510.14377
- Source URL: https://arxiv.org/abs/2510.14377
- Authors: Mykolas Sveistrys; Richard Kunert
- Reference count: 11
- Primary result: PluriHopRAG improves pluri-hop QA F1 by 18-52% over baselines

## Executive Summary
This paper introduces pluri-hop QA, a class of questions requiring exhaustive aggregation across all documents in a knowledge base without clear stopping points for retrieval. Missing even one relevant passage leads to incorrect answers. To study this setting, the authors create PluriHopWIND, a multilingual dataset of 48 pluri-hop questions over 191 wind industry reports. Current approaches, including traditional RAG, graph-based, and multimodal methods, struggle with this task, achieving at most 40% F1. The authors propose PluriHopRAG, which decomposes queries into document-level subquestions and uses cross-encoder filtering to discard irrelevant documents before LLM reasoning, achieving 18-52% relative F1 improvements.

## Method Summary
PluriHopRAG addresses pluri-hop QA by first decomposing queries into document-level subquestions using a fine-tuned GPT-4o model. It then employs a two-stage retrieval: KNN on document summaries followed by KNN on chunks within each document. A cross-encoder filter (Cohere Rerank 3.5 with threshold τ=0.1) discards irrelevant documents before costly LLM reasoning. Documents are chunked at 500 characters with 100-character overlap, and embeddings use text-embedding-3-large. The method achieves significant F1 improvements by ensuring exhaustive retrieval and early filtering, contrasting with traditional top-k approaches.

## Key Results
- PluriHopWIND dataset is 8-40% more repetitive than other common datasets
- Current approaches reach at most 40% statement-wise F1 on pluri-hop questions
- PluriHopRAG achieves relative F1 improvements of 18-52% depending on base LLM
- Cross-encoder filtering discards ~50% of irrelevant documents while missing <10% of relevant ones

## Why This Works (Mechanism)
PluriHopRAG works by ensuring exhaustive retrieval through query decomposition and two-stage KNN retrieval, then filtering irrelevant documents early using a cross-encoder. This approach addresses the core challenge of pluri-hop QA: the impossibility of knowing whether the evidence set is complete. By decomposing queries into document-level subquestions, it ensures each document is considered individually. The cross-encoder filter with threshold τ=0.1 provides an effective trade-off between recall and computational efficiency, discarding irrelevant documents before expensive LLM reasoning while maintaining high recall of relevant documents.

## Foundational Learning
- **Pluri-hop QA**: Questions requiring aggregation across all documents without clear stopping point; needed to formalize the recall-sensitive QA challenge in repetitive document corpora.
- **Statement-wise evaluation**: Semantic-level comparison via GPT-4o rather than token-level; needed because pluri-hop answers are summaries requiring semantic matching.
- **Query decomposition**: Breaking complex queries into document-level subquestions; needed to ensure exhaustive retrieval across all documents.
- **Cross-encoder filtering**: Using rerankers to filter documents before LLM reasoning; needed to reduce computational cost while maintaining recall.
- **Two-stage retrieval**: Hierarchical retrieval from summaries to chunks; needed to efficiently navigate large document collections.
- **Threshold optimization**: Selecting τ=0.1 for cross-encoder filtering; needed to balance recall vs computational efficiency.

## Architecture Onboarding

**Component map**: Query -> Query Decomposer -> Document-level Subquestions -> Summary KNN -> Chunk KNN -> Cross-encoder Filter -> LLM Reasoning -> Answer

**Critical path**: Query decomposition → Two-stage retrieval → Cross-encoder filtering → LLM reasoning

**Design tradeoffs**: Exhaustive retrieval vs computational cost (solved by early filtering); semantic matching vs token-level precision (solved by statement-wise evaluation)

**Failure signatures**: Cross-encoder threshold too aggressive (valid docs filtered out); query decomposition misaligned with document structure; two-stage retrieval missing relevant documents

**First experiments**:
1. Evaluate threshold sensitivity by varying τ from 0.05 to 0.3 and plotting recall/precision curves
2. Compare two-stage retrieval (summary→chunk) against direct chunk-level retrieval to quantify hierarchical benefit
3. Apply PluriHopRAG to a different repetitive document corpus (e.g., financial reports) to test domain generalization

## Open Questions the Paper Calls Out
- How does PluriHopRAG's performance scale to corpora with thousands or tens of thousands of documents?
- Can pluri-hop QA methods generalize to domains with different document structures (healthcare, financial, legal)?
- What is the optimal cross-encoder filtering threshold for balancing recall versus LLM token efficiency?
- How does multilingual retrieval affect pluri-hop QA performance when documents span multiple languages?

## Limitations
- PluriHopWIND dataset unavailable for independent verification of results
- Fine-tuning data generation procedure for query decomposer not fully specified
- Performance claims depend on specific dataset and evaluation setup
- Cross-encoder filtering approach may face computational bottlenecks at much larger scales

## Confidence
- Core contribution identification: High
- Empirical results: Medium
- Practical applicability: Medium

## Next Checks
1. Systematically vary the cross-encoder threshold τ and plot recall/precision curves to confirm τ=0.1 is near-optimal
2. Compare PluriHopRAG's two-stage retrieval against direct chunk-level retrieval to quantify hierarchical benefit
3. Apply PluriHopRAG to a different repetitive document corpus (e.g., monthly financial reports) to evaluate relative F1 gains and domain generalization