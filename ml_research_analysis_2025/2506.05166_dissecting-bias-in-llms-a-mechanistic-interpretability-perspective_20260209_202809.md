---
ver: rpa2
title: 'Dissecting Bias in LLMs: A Mechanistic Interpretability Perspective'
arxiv_id: '2506.05166'
source_url: https://arxiv.org/abs/2506.05166
tags:
- bias
- gpt-2
- edges
- gender
- demographic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how demographic and gender biases are structurally
  embedded within the architectures of GPT-2 Small, GPT-2 Large, and LLaMA-2 using
  mechanistic interpretability. The authors employ Edge Attribution Patching (EAP)
  to identify edges responsible for biased behavior and analyze their localization,
  stability, and generalizability across different conditions.
---

# Dissecting Bias in LLMs: A Mechanistic Interpretability Perspective

## Quick Facts
- **arXiv ID:** 2506.05166
- **Source URL:** https://arxiv.org/abs/2506.05166
- **Authors:** Bhavik Chandna; Zubair Bashir; Procheta Sen
- **Reference count:** 11
- **Primary result:** Edge Attribution Patching identifies localized bias circuits in GPT-2 and LLaMA-2 models, revealing high instability across grammatical structures and functional overlap with general language capabilities.

## Executive Summary
This paper applies mechanistic interpretability techniques to investigate how demographic and gender biases are structurally embedded within transformer architectures. Using Edge Attribution Patching (EAP), the authors identify specific edges responsible for biased behavior and analyze their localization, stability, and generalizability. The work reveals that bias computations are highly localized to small subsets of layers but exhibit instability across grammatical structures and types of bias. Critically, removing bias-related edges reduces biased outputs but also negatively impacts performance on unrelated NLP tasks, indicating functional overlap between bias circuits and general language understanding capabilities.

## Method Summary
The study employs Edge Attribution Patching (EAP) to measure causal importance of individual edges for bias behavior. Using datasets with demographic (224 nationalities) and gender (320 professions) templates, the method computes attribution scores through two forward passes and one backward pass. Top-ranked edges are then ablated by patching corrupted activations into clean inputs, measuring the resulting drop in bias metrics and performance on downstream tasks like CoNLL-2003 (NER) and CoLA (linguistic acceptability). The approach leverages TransformerLens framework for modular access to model activations.

## Key Results
- Bias-related computations are highly localized, concentrated in small subsets of layers (e.g., layers 2-6 for GPT-2 Small)
- Identified bias components are unstable across grammatical structures, showing minimal overlap between DSS1 and DSS2 variants
- Removing bias-related edges reduces biased outputs but also negatively impacts performance on unrelated NLP tasks like NER and linguistic acceptability
- 40% of top-ranked edges can reduce bias metrics by >90% in GPT-2 Small and LLaMA-2 models

## Why This Works (Mechanism)

### Mechanism 1
**Claim:** Edge Attribution Patching approximates causal importance of specific edges for bias behavior with minimal compute.
**Mechanism:** EAP measures the metric difference L(x_clean|do(E=e_corr)) - L(x_clean) by applying corrupted activations to individual edges while keeping others clean. Edges that significantly reduce the bias metric when corrupted are identified as important. This requires only two forward passes and one backward pass rather than exhaustive ablation.
**Core assumption:** The corruption strategy (C1 or C2) produces semantically meaningful counterfactuals that isolate bias-relevant information without introducing confounds.
**Evidence anchors:**
- [abstract] "we explore different metrics to identify the internal edges responsible for biased behavior"
- [Section 3] "Attribution patching is an approach that is used to compute the value of Equation 1 with only two forward passes and one backward pass through the network"
- [corpus] No direct corpus validation of EAP efficiency claims; related work on Sparse Feature Circuits (Marks et al.) provides alternative causal intervention methods
**Break condition:** If corrupted samples don't preserve token length or introduce semantic drift beyond bias-related aspects, attribution scores become unreliable.

### Mechanism 2
**Claim:** Demographic and gender biases localize to small, specific edge subsets rather than distributing uniformly across the network.
**Mechanism:** Bias-related computations concentrate in identifiable layer ranges (e.g., layers 2-6 for GPT-2 Small, layers 0-11 and 30-31 for LLaMA-2). Ablating 40% of top-ranked edges reduces bias metrics by >90% for GPT-2 Small and LLaMA-2 (60% for GPT-2 Large), indicating functional specialization.
**Core assumption:** Layer-wise edge distribution patterns are consistent across bias types within the same model architecture.
**Evidence anchors:**
- [abstract] "bias-related computations are highly localized, often concentrated in a small subset of layers"
- [Section 5, Figure 2] "for all the models only a few layers contribute to the important edges for bias"
- [Section 5, Figure 3] "within 40% of the top edges for GPT-2 Small and Llama-2 models, the metric value drops by more than 90%"
- [corpus] Related work on cultural bias (arXiv:2508.08879) supports mechanistic localization but doesn't replicate EAP specifically
**Break condition:** If bias were uniformly distributed, ablating edge subsets would produce proportional—not superlinear—metric reductions.

### Mechanism 3
**Claim:** Bias-related edges exhibit functional entanglement with general language understanding tasks, creating a performance trade-off when edges are corrupted.
**Mechanism:** Corrupting bias-associated edges (via corrupted logit substitution) reduces biased outputs but simultaneously degrades performance on NER (CoNLL-2003) and linguistic acceptability (CoLA). This indicates bias circuits share computational infrastructure with broader linguistic capabilities.
**Core assumption:** Performance degradation on unrelated tasks reflects genuine component sharing rather than collateral interference from corruption methodology.
**Evidence anchors:**
- [abstract] "removing these components not only reduces biased outputs but also affects other NLP tasks... because of the sharing of important components"
- [Section 5, Table 3] Shows CoLA and CoNLL-2003 performance decreases across all models after edge corruption
- [Section 5] "This differential impact across tasks hints at a hierarchical organization of linguistic knowledge"
- [corpus] Kaneko et al. (2023, cited in paper) demonstrated similar debiasing-performance trade-offs
**Break condition:** If bias and language tasks used disjoint circuits, edge corruption would show task-specific effects without cross-task degradation.

## Foundational Learning

- **Concept: Computational Graphs and Edges in Transformers**
  - **Why needed here:** The paper analyzes "edges" (connections between computational nodes like attention heads, MLPs, and layer outputs) rather than just neurons or attention heads. Understanding how information flows between these components is prerequisite for interpreting EAP results.
  - **Quick check question:** Can you sketch the information flow from input tokens through attention heads and MLP layers to logits, identifying what an "edge" represents in this graph?

- **Concept: Causal Intervention via Activation Patching**
  - **Why needed here:** EAP is fundamentally a causal intervention method—corrupted activations are "patched" into specific edges to measure their contribution. This differs from correlational attribution methods.
  - **Quick check question:** How does activation patching differ from gradient-based attribution methods like Integrated Gradients? What does it mean to "do(E=e_corr)" in causal terms?

- **Concept: Bias Metrics for Text Generation (L1 vs L2)**
  - **Why needed here:** The paper compares two bias quantification approaches: L1 (difference between positive/male and negative/female token probabilities) and L2 (unidirectional probability sum). Understanding these metrics is essential for interpreting ablation results.
  - **Quick check question:** Given model outputs "The woman worked as a nurse" vs "The woman worked as an engineer," how would L1 and L2 metrics differ in capturing gender bias?

## Architecture Onboarding

- **Component map:**
  - **Nodes:** Input, logits, MLP layers (m{i}), attention heads (a{i}.h{j}), and sub-components (query/key/value)
  - **Edges:** Connections between nodes (e.g., m11→logits, input→m0, a0.h5⟨k⟩→next layer)
  - **Circuit:** Subgraph of high-attribution edges forming the bias pathway
  - **Hooked-Transformer:** TransformerLens framework providing modular access to activations for patching

- **Critical path:**
  1. Define clean/corrupted sample pairs using STR (Symmetric Token Replacement)
  2. Compute bias metric (L2 recommended based on Table 2) for clean inputs
  3. Run EAP to score all edges (2 forward, 1 backward pass)
  4. Rank edges by attribution scores
  5. Ablate top-K edges (K varies by model: 400 for GPT-2 Small, 1000 for GPT-2 Large, 3000 for LLaMA-2)
  6. Evaluate bias reduction and task-specific performance changes

- **Design tradeoffs:**
  - **Metric choice (L1 vs L2):** L1 captures directional bias skew; L2 provides more stable localization (smaller change differences in Table 2)
  - **Corruption strategy (C1 vs C2):** C1 (out-of-distribution tokens like "abc") vs C2 (neutral replacements like "Emirati" or "broadcaster")—paper uses C2 for stability
  - **Edge vs node analysis:** Edges capture information flow; nodes capture computation points. Paper focuses on edges for finer granularity
  - **Ablation threshold:** Higher K removes more bias but increases collateral damage to other tasks

- **Failure signatures:**
  - **Low metric change after ablation:** Indicates either wrong edges identified or corruption strategy ineffective
  - **Instability across grammatical structures (DSS1 vs DSS2):** Demographic bias edges show minimal overlap across syntactic variations, suggesting context-dependency
  - **High cross-task degradation:** Corrupting edges that overlap heavily with general language circuits (e.g., late-layer MLP→logits edges)

- **First 3 experiments:**
  1. **Replicate EAP scoring on a single bias type** (e.g., GSS1 male-biased dataset) with GPT-2 Small using L2 metric and C2 corruption. Verify top edges match Table 6 (input→m0, input→a0.h5⟨k⟩, input→a0.h5⟨q⟩) and check metric drops >50% when top 10% edges are ablated.
  2. **Test cross-structure stability:** Run EAP on DSS1 vs DSS2 for demographic bias. Compute overlap percentage of top-100 edges. Expect low overlap (as per Figure 4), confirming instability finding.
  3. **Measure functional overlap:** Corrupt top-400 bias edges in GPT-2 Small and evaluate on CoLA. Compare degradation percentage to Table 3 (expect ~0.66-22.6% drop depending on bias type). This validates the entanglement claim before attempting custom debiasing interventions.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can bias-related edges be selectively modified or isolated in a way that reduces biased outputs without degrading performance on unrelated NLP tasks?
- **Basis in paper:** [explicit] The conclusion states "findings underscore a fundamental trade-off between debiasing and general task performance, and point to the need for more selective interventions that can isolate bias-related functionality while preserving broader model competence."
- **Why unresolved:** The paper demonstrates edge corruption reduces bias but also harms CoLA and CoNLL-2003 performance, showing functional overlap between bias circuits and general language capabilities.
- **What evidence would resolve it:** Demonstrating a debiasing technique that maintains or improves bias metrics while preserving baseline performance across a diverse suite of NLP benchmarks.

### Open Question 2
- **Question:** Do bias-related circuits identified for demographic and gender bias generalize to other bias types (e.g., age, religious, political, socioeconomic)?
- **Basis in paper:** [explicit] The limitations section states "for other biases, the circuits obtained from this work may not be applied" and notes that different bias types have largely disjoint circuits.
- **Why unresolved:** The study only examined demographic and gender bias; findings show minimal overlap between even these two categories, suggesting other biases may have entirely distinct circuit structures.
- **What evidence would resolve it:** Applying EAP and circuit analysis to additional bias categories and comparing the identified edges across all bias types.

### Open Question 3
- **Question:** What mechanisms underlie the instability of bias-related edges across grammatical variations, and can more robust circuits be identified?
- **Basis in paper:** [explicit] The paper finds "edges identified for biased behavior don't remain consistent under perturbations to both the model and input text space," with demographic bias showing "minimal overlap" between DSS1 and DSS2 structures.
- **Why unresolved:** The instability suggests bias may be encoded through multiple parallel pathways or that current attribution methods capture surface-level patterns rather than core bias mechanisms.
- **What evidence would resolve it:** Identifying higher-level circuit patterns or shared computational primitives that remain stable across grammatical and syntactic variations.

### Open Question 4
- **Question:** Why do bias-related edges exhibit differential impacts across downstream tasks (e.g., NER vs. linguistic acceptability)?
- **Basis in paper:** [inferred] Table 3 shows non-uniform performance degradation—CoLA drops 22.6% while NER drops only 0.01% in some configurations—suggesting hierarchical organization of linguistic knowledge.
- **Why unresolved:** The paper hypothesizes "hierarchical organization of linguistic knowledge" but does not systematically characterize which capabilities are more entangled with bias circuits.
- **What evidence would resolve it:** Probing a broader range of linguistic tasks and analyzing which model components are shared between bias and each task type.

## Limitations

- The edge-level attribution approach may miss higher-order interactions between bias components that emerge at the circuit or behavior level.
- Findings are limited to GPT-2 Small/Large and LLaMA-2 architectures, restricting architectural generalizability.
- The STR-based corruption strategy may not capture all bias-relevant phenomena, particularly context-dependent biases.

## Confidence

- **High confidence:** The localization finding (Mechanism 2) is well-supported by multiple ablation experiments showing >90% metric reduction with 40% edge removal. The computational efficiency of EAP is also well-demonstrated.
- **Medium confidence:** The instability findings (Mechanism 3) are convincing within tested conditions but may not generalize to all bias types or contexts. The performance trade-off evidence is correlational rather than establishing causal functional overlap.
- **Low confidence:** Claims about the broader implications for debiasing are speculative given the study's limited scope to three model architectures and specific bias types.

## Next Checks

1. Replicate the EAP approach on a fourth model architecture (e.g., OPT or BLOOM) to test architectural generalizability of the localization and instability findings.
2. Test whether the instability pattern holds across additional bias types (e.g., racial, religious) and syntactic variations beyond the DSS1/DSS2 comparison.
3. Conduct a controlled ablation study where bias edges are selectively restored while monitoring cross-task performance to establish causality in the functional overlap claim.