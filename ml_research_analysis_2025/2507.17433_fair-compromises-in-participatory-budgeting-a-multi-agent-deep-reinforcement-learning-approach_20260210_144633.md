---
ver: rpa2
title: 'Fair Compromises in Participatory Budgeting: a Multi-Agent Deep Reinforcement
  Learning Approach'
arxiv_id: '2507.17433'
source_url: https://arxiv.org/abs/2507.17433
tags:
- learning
- projects
- reinforcement
- voters
- voting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper uses multi-agent deep reinforcement learning to improve
  decision-making in participatory budgeting. Voters often face "choice overload"
  when selecting from many projects, and their votes may not lead to fair or satisfying
  outcomes.
---

# Fair Compromises in Participatory Budgeting: a Multi-Agent Deep Reinforcement Learning Approach

## Quick Facts
- **arXiv ID:** 2507.17433
- **Source URL:** https://arxiv.org/abs/2507.17433
- **Reference count:** 31
- **Primary result:** Multi-agent deep reinforcement learning with branching neural networks improves fairness and voter satisfaction in participatory budgeting by shifting votes toward lower-cost projects

## Executive Summary
This paper addresses the challenge of "choice overload" in participatory budgeting by applying multi-agent deep reinforcement learning to find voting strategies that produce fairer collective outcomes. The authors develop a branching neural network architecture that decomposes the complex combinatorial action space of cumulative voting into manageable parallel decisions, enabling agents to learn effective voting policies. Tested on two real-world participatory budgeting elections, the trained agents achieved higher fairness metrics than actual voters by systematically favoring lower-cost projects, demonstrating that fair compromises can emerge from decentralized optimization when guided by appropriate reward functions and aggregation rules.

## Method Summary
The method employs a branching Deep Q-Network architecture where each agent's decision to allocate tokens across projects is decomposed into T parallel action heads (one per token), with shared representation layers processing project costs and impact areas. Agents are trained independently using experience replay and epsilon-greedy exploration over 400 episodes. The environment aggregates all agent ballots using the Method of Equal Shares and utilitarian greedy algorithms to determine winning projects, with rewards calculated based on the logarithmic cost of supported projects and alignment with voter impact area preferences. Two real-world elections provide the test data: Stadtidee Aarau 2023 (1703 voters, 33 projects, 10 tokens each) and Budget Participatif Toulouse 2019 (1494 voters, 30 projects, 7 tokens each).

## Key Results
- Trained agents achieved higher fairness metrics than actual voters in both tested elections
- Agents shifted vote allocations toward lower-cost projects, improving Gini coefficient and egalitarian welfare
- More voters had preferred projects selected in winning sets compared to real election outcomes
- The branching architecture successfully handled the large action space (reducing from ~1.5×10^9 to 330 actions for Aarau)

## Why This Works (Mechanism)

### Mechanism 1: Action Space Decomposition via Branching Architecture
The branching DQN architecture resolves combinatorial explosion by splitting the decision into T parallel "action dimensions," where each branch selects one project per token. This reduces the action space complexity from exponential to linear relative to tokens, enabling tractable learning in high-dimensional spaces.

### Mechanism 2: Reward-Shaping via Cost-Logarithmic Utility
The reward function includes a logarithmic cost term that incentivizes agents to favor lower-cost projects, which correlates with improved fairness metrics by creating a "return on investment" preference for supporting cheaper projects.

### Mechanism 3: Decentralized Optimization of Fairness
Self-interested agents optimizing for individual reward can achieve emergent collective fairness when constrained by aggregation methods like Equal Shares, which inherently limit the dominance of expensive projects and force strategic compromises.

## Foundational Learning

- **Deep Q-Learning (DQN) & Experience Replay**: Essential for understanding how neural networks approximate Q-functions and stabilize learning in complex voting environments. *Quick check:* Can you explain why storing transitions in a buffer and sampling randomly helps prevent catastrophic forgetting?

- **Action Branching Architectures**: Critical for grasping how multi-dimensional actions are decomposed into independent sub-actions to make combinatorial problems tractable. *Quick check:* If an agent has 5 tokens to assign, does a branching architecture output 5 separate actions or 1 combined vector?

- **Welfare Economics (Egalitarian vs. Utilitarian)**: Necessary for interpreting fairness metrics and understanding the trade-off between maximizing total utility versus ensuring the worst-off voter gets something. *Quick check:* If the Gini coefficient decreases but Utilitarian welfare stays the same, what does that imply about project distribution?

## Architecture Onboarding

- **Component map**: Input Layer (Project costs, Impact areas, Voter preferences) → Shared Representation (Fully connected layers) → Branching Layers (T parallel heads) → Action Selection (Argmax per branch) → Environment (Ballot Aggregation) → Reward Module (Calculates scalar reward)

- **Critical path**: Initialize N agents → Forward pass through branching network → Aggregate ballots to determine winners → Calculate rewards → Store transitions in replay buffer → Sample mini-batch → Update Q-networks via backprop

- **Design tradeoffs**: Scalability vs. Coordination (decentralized learning scales but may miss complex coordination); Exploration vs. Convergence (multi-agent non-stationarity requires careful epsilon decay tuning)

- **Failure signatures**: Collapse to zero-reward (insufficient exploration); Simulacrum bias (modeled preferences don't reflect reality); Constraint violation (invalid ballots from branching architecture)

- **First 3 experiments**: 1) Run environment with random agents to establish baseline; 2) Ablation study removing log(cost) term from reward; 3) Stress test with synthetic data beyond 2000 voters

## Open Questions the Paper Calls Out

### Open Question 1
Can inverse reinforcement learning derive more behaviorally accurate decision-making models for voting agents? The paper suggests future work using IRL to improve agent accuracy beyond the current simplified "issue voting" framework.

### Open Question 2
How robust is the finding that prioritizing lower-cost projects improves fairness across diverse election contexts? The current results are limited to two specific datasets and may not generalize universally.

### Open Question 3
How do specific impact areas (issues) influence the trade-offs voters make compared to project cost? While results showed a shift to lower-cost projects, the specific trade-offs regarding impact areas were not deeply analyzed.

## Limitations

- The model relies on simplified voter preference representations that may not capture full complexity of real-world decision-making
- The logarithmic cost function's validity as a proxy for voter satisfaction remains unverified against actual human preferences
- The 400-episode training horizon may not guarantee convergence in larger or more complex participatory budgeting scenarios

## Confidence

**High Confidence:** The branching DQN architecture effectively handles combinatorial action spaces, and empirical improvements in fairness metrics are robust and reproducible.

**Medium Confidence:** The mechanism by which agents learn to favor lower-cost projects is well-documented, but its alignment with actual voter satisfaction requires further validation.

**Low Confidence:** Generalizability to different participatory budgeting contexts and performance under strategic voting or incomplete information remain untested.

## Next Checks

1. **Ablation Study on Reward Function:** Retrain agents with alternative reward formulations (linear cost, binary win/loss) to determine whether the logarithmic cost function is essential to achieving fairness improvements.

2. **Human Preference Validation:** Conduct user studies where participants vote using trained agent strategies versus natural voting patterns, measuring actual satisfaction rather than model-predicted satisfaction.

3. **Stress Test on Scale:** Apply the branching DQN to synthetic scenarios with 5000+ voters and 100+ projects to evaluate architectural advantages at larger scales and convergence behavior.