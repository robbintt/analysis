---
ver: rpa2
title: Donate or Create? Comparing Data Collection Strategies for Emotion-labeled
  Multimodal Social Media Posts
arxiv_id: '2505.24427'
source_url: https://arxiv.org/abs/2505.24427
tags:
- emotion
- data
- posts
- text
- donation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper compares three methods for collecting multimodal social
  media posts labeled by their authors for emotion: study-created (CREATION), genuine
  donated (DONATION), and genuine recent (RECENT). It finds that CREATION posts are
  longer, rely more on text than images for emotion expression, and focus on more
  prototypical emotional events compared to genuine posts.'
---

# Donate or Create? Comparing Data Collection Strategies for Emotion-labeled Multimodal Social Media Posts

## Quick Facts
- arXiv ID: 2505.24427
- Source URL: https://arxiv.org/abs/2505.24427
- Reference count: 36
- One-line primary result: Study-created posts differ systematically from genuine posts in length, text-image reliance, and prototypicality; realistic model performance requires genuine test data.

## Executive Summary
This paper compares three strategies for collecting emotion-labeled multimodal social media posts: study-created (CREATION), genuine donated (DONATION), and genuine recent (RECENT). The authors find that CREATION posts are longer, rely more on text than images for emotion expression, and focus on more prototypical emotional events compared to genuine posts. Participants willing to donate posts versus create them differ demographically, with CREATION attracting older, less student-heavy, and more European participants. Models trained on CREATION generalize well to genuine data, but realistic effectiveness estimates require DONATION test data, as zero-shot and DONATION test performance is lower than CREATION test performance.

## Method Summary
The authors collected 2,507 multimodal posts across three strategies: CREATION (prompt emotion → recall event → select Flickr image), DONATION (prompt emotion → find genuine post), and RECENT (submit 5 recent posts → annotate emotions). Each post received emotion labels with intensity ratings, text-image relationship ratings (5 dimensions), 15 appraisal dimensions, and event/emotion duration annotations. They trained unimodal models (RoBERTa-base for text, ViT-base for images) and multimodal models (CLIP-base embeddings + concatenation + classifier) on 800 posts per strategy, with 150 each for dev/test. Performance was evaluated using macro F1 score across 5 runs, comparing model performance when training and testing on different collection strategies.

## Key Results
- CREATION posts are longer and rely more on text and less on images for emotion expression compared to DONATION and RECENT posts.
- CREATION participants select more emotionally prototypical events and rate them as more intense than DONATION participants.
- Demographic samples differ significantly: CREATION attracts older, less student-heavy, and more European participants than DONATION.
- Models trained on CREATION generalize well to genuine data, but DONATION test performance (realistic estimates) is lower than CREATION test performance.

## Why This Works (Mechanism)

### Mechanism 1: Prototypical Event Selection in Study-Created Data
When participants are asked to recall events matching specific emotion labels, memory accessibility biases them toward events that strongly exemplify that emotion. This bypasses the ambiguous, mixed-emotion events typical of genuine social media posting. Assumption: The difference in event prototypicality translates to observable differences in post characteristics (length, text-image reliance) and model performance. Evidence: CREATION events are rated 0.34 points more intense than DONATION events (p < 0.001). Break condition: If genuine social media posts were similarly dominated by single, clear emotions, the performance gap would disappear.

### Mechanism 2: Privacy Concerns Create Demographic Selection Bias
DONATION and RECENT require participants to share actual social media content, creating higher barriers for privacy-conscious individuals. This produces a different demographic composition (younger, more students, fewer European) than CREATION. Assumption: Demographic composition affects post characteristics and model generalization to target populations. Evidence: Decline rates are considerably higher in DONATION and RECENT than in CREATION (χ2 test, p < 0.001). Break condition: If demographic differences did not affect post characteristics or model performance, controlling for demographics would eliminate the performance gap.

### Mechanism 3: Text-Image Interdependence Differs by Collection Method
In CREATION, participants select images from a database (Flickr) that approximate what they would use, rather than actual event images. This produces posts where images are less necessary for text understanding and convey less emotion independently. Assumption: The degree of text-image integration in training data affects how multimodal models learn cross-modal dependencies. Evidence: CREATION participants describe their post images as less necessary to understand the text and as conveying emotion less than DONATION and RECENT posts. Break condition: If the text-image relationship in CREATION matched genuine posts, multimodal models would perform equivalently on both test sets.

## Foundational Learning

- **Appraisal Theory in Emotion Modeling**: The paper uses appraisal dimensions (pleasantness, goal relevance, control, etc.) to characterize events underlying posts. Understanding how events are cognitively appraised to produce emotions is essential for interpreting differences between collection methods. Quick check: Can you explain why an event rated high on "unpleasantness" and "others' control" might produce anger rather than sadness?

- **Selection Bias in Data Collection**: The paper demonstrates that different collection methods produce different participant samples. Understanding selection bias is crucial for interpreting whether model performance differences stem from data characteristics or sample composition. Quick check: If CREATION attracts older participants and genuine social media users skew younger, what are two ways this could affect model generalization?

- **Early vs. Late Multimodal Fusion**: The paper uses CLIP with early fusion (embedding concatenation). Understanding how text and image modalities are combined helps interpret why performance differs across test sets with varying text-image interdependence. Quick check: Why might early fusion be more sensitive to the text-image relationship than late fusion (averaging unimodal predictions)?

## Architecture Onboarding

- **Component map**: Data Collection Layer (CREATION → prompt emotion → recall event → select Flickr image; DONATION → prompt emotion → find genuine post; RECENT → submit 5 recent posts → annotate emotions) → Annotation Layer (emotion labels, intensity, text-image ratings, appraisal dimensions, duration) → Model Layer (RoBERTa-base, ViT-base, CLIP-base with concatenation) → Evaluation Layer (cross-strategy testing with macro F1)

- **Critical path**: Choose collection strategy based on privacy constraints and emotion balance needs → Collect posts with emotion labels and intensity ratings → Annotate text-image relationships and event appraisals (optional, for analysis) → Train models on 800 posts per strategy; reserve 150 each for dev/test (25 per emotion) → Test on genuine data (DONATION) for realistic performance estimates

- **Design tradeoffs**: CREATION: Lower privacy risk, balanced emotions, easier recruitment—but less realistic data, different demographics, more text-reliant. DONATION: Realistic data—but privacy concerns, label bias risk from target prompting, lower participation. RECENT: Natural emotion distribution—but highly imbalanced (79% joy), requires multi-label handling

- **Failure signatures**: CREATION test F1 (~0.60) vs. DONATION test F1 (~0.40) shows optimistic estimates. Models struggle on posts where images are required to understand text (-1.02 log-odds for minicpm-v). Demographic blind spots: worse performance on posts by authors with advanced degrees or non-European backgrounds

- **First 3 experiments**: 1) Train identical CLIP models on CREATION vs. DONATION (800 posts each). Test on both. Confirm CREATION test scores are artificially inflated (ΔF1 ≈ 0.20). 2) Stratify test data by "image needed to understand text" rating. Measure performance degradation as interdependence increases. Identify if specific emotion categories are more affected. 3) Split DONATION test data by participant education and gender. Quantify performance gaps. Determine if training on CREATION (older, more European) systematically underperforms for specific subgroups.

## Open Questions the Paper Calls Out

- How do specific social media platform norms (e.g., Instagram vs. X/Twitter) interact with the observed differences between study-created (CREATION) and genuine (DONATION) posts? (The authors did not control for platform and consider platform effects important for future work.)

- How do specific post features (e.g., image necessity for text understanding) and author demographics influence emotion classification accuracy in larger, more statistically powered datasets? (The regression analysis was limited by test set size; the authors identify this as an important avenue for future research.)

- Does requiring participants to select a primary emotion in case of intensity ties improve the utility of the RECENT collection strategy for single-label modeling? (The authors did not ask participants to break ties in RECENT, preventing its use in primary experiments.)

- Do the findings regarding model generalization (CREATION data trains models that perform well on genuine data) hold when the dataset is scaled up significantly? (The current corpus size may not capture full variance; the authors plan to expand the dataset.)

## Limitations
- Selection bias unmeasured: The paper demonstrates demographic differences but does not measure how these differences affect model performance for specific subgroups.
- Generalizability to other contexts: Results are based on English social media posts from UK/Ireland participants; findings may not apply to other platforms, languages, or cultural contexts.
- Zero-shot limitations acknowledged but not quantified: The paper notes zero-shot models struggle but does not quantify how performance varies across collection strategies.

## Confidence
- **High confidence**: CREATION posts differ systematically from genuine posts in length, text-image reliance, and prototypicality. Demographic selection effects are clearly demonstrated with statistical significance.
- **Medium confidence**: Models trained on CREATION generalize to genuine data but with performance drops. The exact magnitude may depend on specific data splits and hyperparameters.
- **Low confidence**: The mechanism that CREATION's "less realistic" nature specifically causes performance degradation. Alternative explanations (demographic differences, platform-specific features) are not ruled out.

## Next Checks
1. **Subgroup performance analysis**: Stratify DONATION test performance by participant education level, gender, and age. Compare against CREATION-trained model performance on these subgroups to quantify demographic blind spots.

2. **Platform transfer experiment**: Train models on CREATION and DONATION data, then test on a held-out sample of posts from different social media platforms (e.g., Instagram vs. Twitter-style posts if available). This tests whether text-image interdependence differences affect cross-platform generalization.

3. **Synthetic data intervention**: Create a hybrid dataset by taking CREATION posts and replacing their Flickr images with actual event photos from participants (where available). Retrain models and test on DONATION to isolate the image selection mechanism from other CREATION differences.