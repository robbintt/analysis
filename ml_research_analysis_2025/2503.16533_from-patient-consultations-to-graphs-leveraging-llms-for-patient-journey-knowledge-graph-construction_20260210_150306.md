---
ver: rpa2
title: 'From Patient Consultations to Graphs: Leveraging LLMs for Patient Journey
  Knowledge Graph Construction'
arxiv_id: '2503.16533'
source_url: https://arxiv.org/abs/2503.16533
tags:
- patient
- data
- healthcare
- clinical
- medical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study developed a method to construct Patient Journey Knowledge
  Graphs (PJKGs) from patient consultations using Large Language Models (LLMs). The
  approach integrates structured patient data with unstructured clinical conversations,
  extracting entities and relationships through ontology-driven prompt engineering.
---

# From Patient Consultations to Graphs: Leveraging LLMs for Patient Journey Knowledge Graph Construction

## Quick Facts
- arXiv ID: 2503.16533
- Source URL: https://arxiv.org/abs/2503.16533
- Reference count: 20
- Primary result: LLMs can construct Patient Journey Knowledge Graphs from clinical conversations with high structural compliance (F1=0.73 semantic accuracy, 3131.50 queries/sec throughput)

## Executive Summary
This study presents a method for constructing Patient Journey Knowledge Graphs (PJKGs) from patient consultations using Large Language Models. The approach integrates structured intake forms with unstructured clinical conversations, extracting entities and relationships through ontology-driven prompt engineering. Four LLMs were evaluated for structural compliance, computational efficiency, and semantic accuracy, with Anthropic achieving the highest semantic accuracy (F1=0.73) while Llama 3.1 demonstrated superior computational performance.

## Method Summary
The method uses ontology-driven prompt engineering to guide LLMs in extracting entities and relationships from patient consultations, producing JSON outputs that are validated syntactically, semantically, and temporally before being integrated into Neo4j graphs. The pipeline processes both structured intake forms and unstructured conversation transcripts, constructing patient profile nodes and encounter subgraphs that are linked through temporal and causal relationships.

## Key Results
- All four LLMs achieved perfect structural compliance (ICR/IPR=1.0) on ontology-aligned JSON extraction
- Anthropic achieved highest semantic accuracy (F1=0.73) for entity extraction while OpenAI showed lowest (F1=0.59)
- Llama 3.1 demonstrated best computational performance with 2.58ms latency and 3131.50 queries/sec throughput
- Relationship extraction remained challenging across all models, with F1 scores ranging from 0.18 to 0.73

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Ontology-driven prompting constrains LLM outputs to valid schema structures, enabling reliable JSON generation
- Mechanism: Pre-defined ontology provides explicit entity categories and relationship constraints that reduce hallucination by bounding the output space
- Core assumption: LLMs can reliably follow explicit structural instructions when constraints are well-defined
- Evidence anchors: [abstract] "extracting entities and relationships through ontology-driven prompt engineering"
- Break condition: If ontology schema drifts or context window overflows with complex nested structures

### Mechanism 2
- Claim: Multi-stage validation filters invalid extractions before graph insertion
- Mechanism: Three-stage validation (syntactic, semantic, temporal) catches errors before Neo4j insertion
- Core assumption: Most LLM errors can be caught by deterministic schema checks
- Evidence anchors: [section III-C-5] "The validation process ensures data integrity through multiple checks"
- Break condition: If LLM outputs are syntactically valid but semantically incorrect, validation may not catch it

### Mechanism 3
- Claim: Separating patient profile nodes from encounter subgraphs enables incremental journey construction
- Mechanism: Static patient data creates foundation node while each encounter generates append-only subgraph
- Core assumption: Patient profiles are relatively stable while encounters are additive and temporally ordered
- Evidence anchors: [section III-C-1] "The foundation of each PJKG begins with creating a comprehensive patient profile node"
- Break condition: If patient demographics or history change significantly, profile update logic must handle mutations

## Foundational Learning

- Concept: **Knowledge Graph Ontology Design**
  - Why needed here: The entire extraction pipeline depends on a well-defined ontology with classes, properties, and relationships
  - Quick check question: Given a new medical specialty (e.g., pediatrics), can you identify what new subclasses might be needed beyond Diagnosis, Symptom, and Medication?

- Concept: **Neo4j Cypher and Graph Patterns**
  - Why needed here: The PJKG is stored in Neo4j. Computational metrics assume you can write efficient queries
  - Quick check question: Write a Cypher query to find all encounters where a patient was prescribed metoprolol within 30 days of a hypertension diagnosis

- Concept: **LLM Structured Output Strategies**
  - Why needed here: The paper uses prompt engineering to enforce JSON output
  - Quick check question: If an LLM returns `"diagnosis": "hypertension"` instead of `"diagnosis": {"name": "hypertension", "ICD-10": "I10"}`, which validation layer catches this?

## Architecture Onboarding

- Component map: Ontology Layer -> Data Input Layer -> LLM Extraction Layer -> Validation Layer -> Storage Layer
- Critical path: 1. Define/validate ontology schema 2. Construct prompt template 3. Send transcript + prompt to LLM 4. Validate JSON output 5. Insert valid subgraph into Neo4j 6. Link encounter node to patient
- Design tradeoffs: Anthropic vs. Llama 3.1 (accuracy vs. speed), general-purpose vs. fine-tuned models (adaptability vs. accuracy), JSON vs. RDF/OWL (simplicity vs. expressiveness)
- Failure signatures: Syntactic failures (malformed JSON), semantic failures (non-ontology entities), temporal failures (encounters out of order), relationship extraction failures (low F1 scores)
- First 3 experiments: 1. Schema compliance test across all four LLMs 2. Relationship extraction drill-down with manual annotation 3. Latency scaling test with 10, 50, 100 encounters

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can specialized medical context understanding and improved entity-relationship modeling significantly enhance LLM performance on clinical relationship extraction?
- Basis in paper: [explicit] "Future work should focus on enhancing relationship extraction performance through specialized medical context understanding"
- Why unresolved: Current models achieved F1-scores as low as 0.18, suggesting fundamental challenges in contextual and temporal reasoning
- What evidence would resolve it: Comparative evaluation of medical-domain fine-tuned LLMs against general-purpose models

### Open Question 2
- Question: How does integrating structured EHR data with unstructured patient-provider conversations affect PJKG completeness and accuracy?
- Basis in paper: [explicit] "The current implementation effectively captures conversational data, it lacks the integration of EHRs"
- Why unresolved: EHR systems contain lab results, imaging, and longitudinal records that could fill gaps in conversational data
- What evidence would resolve it: Comparative study constructing PJKGs with and without EHR integration

### Open Question 3
- Question: Does PJKG performance generalize to larger patient populations and broader medical conditions beyond cardiology and oncology?
- Basis in paper: [explicit] "Enhancing the scalability and diversity of datasets will be crucial to accommodate larger patient populations"
- Why unresolved: The dataset comprised only 6 patients across two specialties with 30 total encounters
- What evidence would resolve it: Evaluation on expanded datasets (100+ patients across 5+ specialties)

### Open Question 4
- Question: Does PJKG extraction accuracy translate to improved clinical decision-making, care coordination, or outcome prediction?
- Basis in paper: [inferred] Claims PJKGs "support improved care coordination and outcome prediction" but no clinical utility validation reported
- Why unresolved: High semantic accuracy does not guarantee actionable insights for clinicians
- What evidence would resolve it: Pilot studies measuring clinician satisfaction and patient outcome metrics

## Limitations

- Small dataset (6 patients, 30 encounters) limits generalizability to diverse clinical scenarios
- Relationship extraction performance remains weak across all models with F1 scores below 0.3
- Paper does not address error propagation when invalid extractions bypass validation
- Computational metrics measured in isolation without accounting for concurrent query loads

## Confidence

- **High confidence**: Structural compliance claims (ICR/IPR=1.0) and computational performance metrics due to deterministic validation and measurable timing
- **Medium confidence**: Entity extraction accuracy (F1=0.73 for Anthropic) given ground truth annotation by a single medical professional
- **Low confidence**: Relationship extraction performance and real-world deployment scalability, particularly for temporal and causal relationship inference

## Next Checks

1. Scale validation: Test the extraction pipeline on 100+ patients across multiple specialties to evaluate whether entity F1 scores remain stable and whether relationship extraction improves with model exposure to diverse clinical narratives

2. Error analysis: Conduct systematic failure mode analysis by having two independent medical annotators label 50 encounters, comparing disagreement patterns with LLM extraction failures to identify systematic biases

3. Production readiness: Deploy the Neo4j-based PJKG system in a clinical environment with concurrent user queries, measuring actual query response times, memory usage, and failure rates under realistic workloads