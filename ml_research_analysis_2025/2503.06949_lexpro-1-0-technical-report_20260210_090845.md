---
ver: rpa2
title: LexPro-1.0 Technical Report
arxiv_id: '2503.06949'
source_url: https://arxiv.org/abs/2503.06949
tags:
- legal
- number
- injuries
- minor
- confession
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces LexPro-1.0, a Chinese legal domain-specific
  large language model addressing two core issues: insufficient legal expertise in
  existing models and limited access to high-quality, comprehensive legal data. The
  approach involves curating millions of legal documents from 31 provinces in China,
  covering over 20 crime types, and employing a multi-stage training strategy: supervised
  fine-tuning (SFT) on legal knowledge, reinforcement learning (RL) for enhanced reasoning
  and interpretability, and retrieval-augmented generation (RAG) for efficient, accurate
  legal element extraction.'
---

# LexPro-1.0 Technical Report

## Quick Facts
- arXiv ID: 2503.06949
- Source URL: https://arxiv.org/abs/2503.06949
- Authors: Haotian Chen; Yanyu Xu; Boyan Wang; Chaoyue Zhao; Xiaoyu Han; Fang Wang; Lizhen Cui; Yonghui Xu
- Reference count: 6
- Primary result: Chinese legal domain-specific LLM with 30%+ ROUGE score improvements and 40%+ legal element extraction accuracy gains

## Executive Summary
LexPro-1.0 addresses critical gaps in legal domain-specific language models by developing a comprehensive Chinese legal LLM trained on millions of documents from 31 provinces covering over 20 crime types. The model employs a multi-stage training strategy combining supervised fine-tuning on legal knowledge, reinforcement learning for enhanced reasoning and interpretability, and retrieval-augmented generation for efficient legal element extraction. Available in 14B, 32B, and 70B configurations based on DeepSeek-R1-Distilled, LexPro-1.0 demonstrates significant performance improvements over baseline models while addressing the dual challenges of insufficient legal expertise and limited access to high-quality legal data.

## Method Summary
The LexPro-1.0 development follows a three-stage approach: first, supervised fine-tuning (SFT) on curated legal documents to establish domain expertise; second, reinforcement learning (RL) to enhance reasoning capabilities and output interpretability; third, retrieval-augmented generation (RAG) for dynamic legal element extraction. The training corpus spans 31 Chinese provinces and encompasses over 20 crime types, providing comprehensive coverage. The model architecture builds upon DeepSeek-R1-Distilled as the foundation, with each training stage designed to address specific limitations in existing legal LLMs. The RL component focuses on improving output readability and consistency, while RAG integration enables efficient retrieval of relevant legal elements during inference.

## Key Results
- SFT improved ROUGE scores by over 30% compared to baseline models
- Legal element extraction accuracy increased by more than 40%
- RL enhancement significantly improved output readability and consistency
- RAG integration improved efficiency by dynamically retrieving relevant legal elements

## Why This Works (Mechanism)
The multi-stage training approach effectively addresses the fundamental limitations of general-purpose LLMs in legal domains. SFT provides the foundational legal knowledge necessary for accurate domain-specific responses, while RL optimizes for interpretability and reasoning quality that are crucial in legal contexts. RAG enables the model to access up-to-date legal information without requiring retraining, addressing the dynamic nature of legal systems. The comprehensive training data from 31 provinces ensures broad coverage of Chinese legal scenarios, while the three model sizes (14B, 32B, 70B) provide flexibility for different deployment scenarios.

## Foundational Learning
- Supervised Fine-Tuning (SFT): Why needed - Establishes domain-specific knowledge base; Quick check - Verify training data covers required legal domains
- Reinforcement Learning (RL): Why needed - Optimizes for reasoning quality and interpretability; Quick check - Test reward function alignment with legal reasoning
- Retrieval-Augmented Generation (RAG): Why needed - Enables access to current legal information; Quick check - Validate knowledge base indexing and retrieval accuracy
- Legal Document Curation: Why needed - Provides high-quality training data; Quick check - Assess document diversity and quality across regions
- Multi-stage Training Pipeline: Why needed - Sequential optimization of different capabilities; Quick check - Measure contribution of each stage to final performance

## Architecture Onboarding
- Component map: Data Curation -> SFT -> RL -> RAG Integration -> Inference
- Critical path: Legal document processing → SFT training → RL fine-tuning → RAG implementation → Model deployment
- Design tradeoffs: Larger model sizes provide better performance but require more computational resources; comprehensive training data improves accuracy but increases training time
- Failure signatures: Poor legal element extraction indicates RAG issues; inconsistent reasoning suggests RL optimization problems; domain knowledge gaps point to insufficient SFT training
- First experiments:
  1. Baseline performance evaluation on legal element extraction tasks
  2. A/B testing of SFT vs. non-SFT performance on legal reasoning tasks
  3. Efficiency benchmarking of RAG retrieval times across different query types

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Insufficient detail on RLHF implementation, particularly reward function design and human preference data collection
- Limited transparency in knowledge base construction, indexing, and maintenance for RAG component
- Evaluation methodology lacks full disclosure of test datasets, protocols, and baseline model specifications

## Confidence
- High confidence in technical approach and methodology description
- Medium confidence in reported performance improvements
- Low confidence in real-world applicability claims

## Next Checks
1. Conduct ablation studies to quantify individual contributions of SFT, RL, and RAG components
2. Perform cross-jurisdictional testing to assess generalization beyond Chinese legal context
3. Implement comprehensive efficiency benchmarking across all model variants under realistic deployment conditions