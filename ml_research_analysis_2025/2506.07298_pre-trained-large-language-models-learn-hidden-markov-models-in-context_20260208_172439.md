---
ver: rpa2
title: Pre-trained Large Language Models Learn Hidden Markov Models In-context
arxiv_id: '2506.07298'
source_url: https://arxiv.org/abs/2506.07298
tags:
- entropy
- states
- state
- observations
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether pre-trained large language models
  (LLMs) can learn Hidden Markov Models (HMMs) through in-context learning (ICL) on
  sequential data. The authors conduct systematic experiments on synthetic HMMs with
  varying parameters and demonstrate that LLMs achieve prediction accuracy approaching
  the theoretical optimum (Viterbi algorithm with ground-truth parameters) across
  diverse configurations.
---

# Pre-trained Large Language Models Learn Hidden Markov Models In-context

## Quick Facts
- **arXiv ID**: 2506.07298
- **Source URL**: https://arxiv.org/abs/2506.07298
- **Reference count**: 40
- **Primary result**: Pre-trained LLMs achieve near-optimal prediction accuracy on synthetic HMMs via in-context learning, with performance scaling predictably based on entropy and mixing rate.

## Executive Summary
This paper demonstrates that pre-trained large language models can learn Hidden Markov Models through in-context learning without parameter updates. The authors show that LLMs achieve predictive accuracy approaching the theoretical optimum across diverse HMM configurations, with convergence behavior governed by entropy and mixing rate. The study establishes ICL as a powerful approach for uncovering hidden structure in sequential data, with implications for scientific applications where model structure is unknown.

## Method Summary
The authors generate synthetic HMM sequences with controlled parameters and evaluate LLM in-context learning performance by measuring next-observation prediction accuracy and Hellinger distance against ground truth. They compare ICL against traditional methods like Baum-Welch and LSTM, and test various LLM families (Qwen, Llama) and tokenization schemes (ABC, random) across model sizes (0.5B-8B). The framework is validated on real-world mouse decision-making data.

## Key Results
- LLMs achieve prediction accuracy approaching Viterbi algorithm (with ground-truth parameters) across diverse HMM configurations
- Convergence speed scales with HMM entropy and mixing rate following predictable patterns
- ICL demonstrates superior stability compared to Baum-Welch, showing monotonic improvement with context length
- On real mouse decision-making tasks, ICL outperforms expert-designed GLM-HMM models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Pre-trained LLMs perform implicit belief-state updates over hidden HMM states through attention mechanisms, converging to near-optimal predictions given sufficient context.
- **Mechanism**: The LLM receives observation sequences o₁:t as tokenized context and outputs next-observation probabilities. Through ICL, the model implicitly estimates hidden state posteriors and applies transition dynamics without explicit parameter updates—behaving as an improper learner that bypasses explicit matrix estimation.
- **Core assumption**: Transformers trained on sequential text develop generalizable pattern-completion circuits that transfer to structured stochastic sequences outside their training distribution.
- **Evidence anchors**: [abstract] "LLMs achieve predictive accuracy approaching the theoretical optimum... via in-context learning"; [Section 2.3] "ICL with pre-trained LLM achieves this near-optimal prediction accuracy across diverse HMM parameter configurations"
- **Break condition**: Convergence fails when entropy approaches maximum (H(A) → log M or H(B) → log L) or when mixing is extremely slow (λ₂ → 1).

### Mechanism 2
- **Claim**: ICL performance scales predictably with HMM properties—specifically entropy and mixing rate—following patterns analogous to spectral learning algorithms.
- **Mechanism**: The paper conjectures that LLM ICL approximates spectral learning by estimating observation operators Ao from in-context samples. Theoretical analysis shows prediction error scales with 1/(1-λ₂) and depends on observability conditions tied to entropy.
- **Core assumption**: The implicit computations in transformer attention parallel the matrix operations in spectral methods, though this is a conjecture rather than a proven mechanism.
- **Evidence anchors**: [Section 3.1] "The entropies of both the transition matrix A and the emission matrix B are positively correlated with the number of steps required for LLM convergence"; [Section 3.3] "A key idea in spectral learning literature is to compute the probability of observation sequences in terms of observation operators"
- **Break condition**: Spectral learning requires rank conditions on A and B that may not hold; ICL appears more robust to these violations but the theoretical explanation remains incomplete.

### Mechanism 3
- **Claim**: Monotonic performance improvement with context length distinguishes LLM ICL from traditional methods like Baum-Welch, which exhibits non-convex optimization instability.
- **Mechanism**: Unlike Baum-Welch (EM-based parameter estimation with local optima), ICL directly predicts next observations through forward inference without iterative parameter fitting. The model leverages accumulated context to refine implicit beliefs rather than alternating between E-step and M-step updates.
- **Core assumption**: Pre-training on diverse sequential patterns provides a sufficiently rich prior that enables stable online inference without optimization instability.
- **Evidence anchors**: [Section 3.2] "Baum-Welch... suffers from nonconvex optimization. Its global convergence is not guaranteed... LLMs via ICL demonstrate clearly superior behavior—achieving faster, more stable convergence"; [Appendix D.4] "LLM performance almost always improves monotonically with longer context length—a property notably absent in other learning baselines"
- **Break condition**: Very small models (e.g., Qwen2.5-0.5B) show degraded performance, suggesting a minimum capacity threshold for stable ICL.

## Foundational Learning

- **Concept: Hidden Markov Model structure (λ = π, A, B)**
  - **Why needed here**: The entire experimental framework depends on understanding how hidden states Xt generate observations Ot through transition matrix A and emission matrix B.
  - **Quick check question**: Can you explain why HMM observations are non-Markovian when emission entropy H(B) > 0?

- **Concept: Mixing rate and stationary distributions**
  - **Why needed here**: Section 3.1 shows mixing rate λ₂ directly affects convergence speed—slower mixing (λ₂ → 1) delays learning regardless of model quality.
  - **Quick check question**: Given λ₂ = 0.95, approximately how many steps until the hidden state distribution is close to stationary?

- **Concept: Viterbi algorithm as theoretical optimum**
  - **Why needed here**: All ICL performance is benchmarked against Viterbi with ground-truth parameters; understanding this baseline is essential for interpreting results.
  - **Quick check question**: Why does Viterbi represent an "unfair" comparison, and what does ICL matching it imply?

## Architecture Onboarding

- **Component map**: Observation sequence [o₁, o₂, ..., oₜ] → Tokenizer (ABC/123/random) → LLM backbone (Qwen/Llama families tested, 0.5B–8B parameters) → Probability distribution over next observation P(oₜ₊₁ | o₁:ₜ) → Evaluation: Accuracy (argmax match) + Hellinger distance (distributional match)

- **Critical path**: 
  1. Generate HMM sequences with controlled parameters (Section 2.2 optimization procedure for constructing A)
  2. Tokenize observations consistently—ABC encoding (letters) performed best in ablations
  3. Format as prompt without task description; LLM predicts next token directly
  4. Extract logits for observation vocabulary; compute accuracy and Hellinger vs. ground truth

- **Design tradeoffs**: 
  - Tokenization: ABC faster convergence with high A entropy; suffers with repetitive low-entropy sequences due to pre-training n-gram filtering
  - Model size: 7B+ models consistently strong; 0.5B–1.5B show degraded performance especially with fast mixing
  - Context length: Longer is monotonically better for ICL but increases compute; 512–1024 tokens often sufficient for convergence

- **Failure signatures**:
  - Accuracy plateaus well below Viterbi → likely high entropy or slow mixing; check H(A), H(B), λ₂ values
  - Non-monotonic accuracy curves → likely Baum-Welch or LSTM baseline, not LLM ICL
  - Very poor initial accuracy with rapid recovery → check for repetitive n-grams in early sequence (tokenization artifact)

- **First 3 experiments**:
  1. **Sanity check**: Replicate Figure 3 (middle) with M=8, L=8, moderate entropy (H(A)=1.5, H(B)=1), λ₂=0.75; verify monotonic convergence to Viterbi level by context length 512
  2. **Entropy ablation**: Hold M=L=8, λ₂=0.75 constant; vary H(A) from 0.5 to 3.0; plot convergence context length vs. entropy to replicate Figure 4 scaling
  3. **Real-data probe**: Test on IBL mouse dataset with full information ("stimulus choice reward"); confirm ICL exceeds GLM-HMM (82.2%) at context >1000 trials, matching Figure 6

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can LLM in-context learning be extended to continuous state-space HMMs or models with real-valued observations (e.g., neural recordings)?
- **Basis in paper**: [explicit] Discussion section states: "adapting LLMs to handle continuous state-space models or direct real-valued inputs remains an open question."
- **Why unresolved**: Current work only addresses discrete token sequences; tokenization strategies for continuous observations are non-trivial.
- **What evidence would resolve it**: Demonstrating ICL on HMMs with continuous observation spaces (e.g., Gaussian emissions) or showing a principled tokenization method that preserves continuous structure.

### Open Question 2
- **Question**: Can transition and emission matrices be extracted in interpretable form from the internal representations of LLMs performing HMM ICL?
- **Basis in paper**: [explicit] Discussion section: "extracting explicit and interpretable parameters—such as transition or emission probabilities—from the model's internal representations is nontrivial."
- **Why unresolved**: LLMs are black-box; the mechanism by which they encode HMM structure is unknown.
- **What evidence would resolve it**: Probing methods or mechanistic interpretability techniques that reliably recover HMM parameters from LLM activations.

### Open Question 3
- **Question**: Why does ICL handle spectral learning's rank conditions and conditioning issues more gracefully, and what does this reveal about LLM learning mechanisms?
- **Basis in paper**: [inferred] Section 3.3 notes ICL "seems to handle such issues more gracefully, pointing to an intriguing gap in our statistical understanding for learning HMMs."
- **Why unresolved**: Theoretical conjecture is offered but not proven; precise mechanism remains unclear.
- **What evidence would resolve it**: Systematic comparison of ICL and spectral learning on HMMs violating rank conditions, combined with attention mechanism analysis.

### Open Question 4
- **Question**: How does pretraining data filtering (e.g., removal of repeated n-grams) affect ICL convergence on low-entropy HMMs?
- **Basis in paper**: [inferred] Appendix E.2 shows ABC tokenization has poor initial performance on low-entropy sequences, attributed to pretraining filtering of repetitive patterns.
- **Why unresolved**: Interaction between pretraining characteristics and ICL performance across entropy profiles is not systematically characterized.
- **What evidence would resolve it**: Controlled pretraining experiments with varying n-gram filtering levels, evaluated on synthetic HMMs.

## Limitations

- Theoretical mechanism linking LLM attention to spectral learning operations remains a conjecture without direct mechanistic validation
- Real-world application uses limited dataset (50 trials per session) and single alternative model comparison
- Performance shows sensitivity to tokenization choices that may interact with pre-training biases

## Confidence

- **High confidence**: LLMs achieve near-optimal prediction accuracy on synthetic HMMs across diverse parameter configurations; monotonic improvement with context length distinguishes ICL from EM-based methods
- **Medium confidence**: The relationship between HMM entropy/mixing rate and convergence speed follows predictable scaling laws; ICL outperforms expert-designed models on real scientific data
- **Low confidence**: The specific mechanism by which attention operations implement spectral learning computations; generalizability of findings to continuous-state models or other structured domains

## Next Checks

1. **Mechanistic validation**: Use probing techniques or attention visualization to directly observe whether LLM representations during ICL approximate belief-state posteriors or operator estimates predicted by the spectral learning hypothesis.

2. **Cross-domain generalization**: Test ICL on continuous-state linear dynamical systems and compare against established methods like Kalman filtering to assess whether the observed capabilities extend beyond discrete HMMs.

3. **Scaling law characterization**: Systematically measure the relationship between model size, context length, and convergence speed across a broader range of HMM complexities to establish whether current trends hold at scales beyond 8B parameters or context lengths exceeding 2048 tokens.