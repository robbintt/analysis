---
ver: rpa2
title: Simulating Viva Voce Examinations to Evaluate Clinical Reasoning in Large Language
  Models
arxiv_id: '2510.10278'
source_url: https://arxiv.org/abs/2510.10278
tags:
- clinical
- information
- diagnosis
- diagnostic
- confidence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces VivaBench, a benchmark for evaluating sequential
  clinical reasoning in large language models (LLMs) through simulated viva voce medical
  examinations. The dataset comprises 1,762 physician-curated clinical vignettes requiring
  agents to actively gather information across multiple turns to reach a diagnosis.
---

# Simulating Viva Voce Examinations to Evaluate Clinical Reasoning in Large Language Models

## Quick Facts
- arXiv ID: 2510.10278
- Source URL: https://arxiv.org/abs/2510.10278
- Reference count: 40
- Primary result: Top-1 accuracy dropped from 69% with full information to 35% in interactive clinical diagnosis scenarios

## Executive Summary
This paper introduces VivaBench, a benchmark for evaluating sequential clinical reasoning in large language models through simulated viva voce medical examinations. The dataset comprises 1,762 physician-curated clinical vignettes requiring agents to actively gather information across multiple turns to reach a diagnosis. Evaluation of six state-of-the-art LLMs revealed significant performance degradation when models had to navigate diagnostic uncertainty compared to single-turn diagnosis tasks, with accuracy falling from 69% to 35%. Analysis identified four failure modes mirroring clinical errors: fixation on initial hypotheses, excessive investigation ordering, premature diagnostic closure, and missing critical conditions.

## Method Summary
VivaBench uses a multi-turn clinical diagnosis framework where agents query for information (history, physical exam, imaging, labs) before reaching a diagnosis. The dataset contains 990 human-reviewed clinical vignettes from PubMed case reports, structured with History (H), Physical Examination (P), Imaging (I), Laboratory (L), and Diagnosis (D) components. The evaluation framework uses an LLM-based mapper (gpt-4.1, temp=0) for query-to-key matching and parser (gpt-4o-mini) for natural language responses, enforcing turn limits (10 history, 5 examination, 3 imaging, 3 lab requests, 20 global max). Performance is measured through top-k accuracy, confidence-weighted accuracy score (Sconf), information-seeking precision/recall, and diagnostic adaptation metrics between provisional and final stages.

## Key Results
- Top-1 accuracy dropped from 69% with full information to 35% in interactive clinical diagnosis scenarios
- Models showed systematic failure modes: anchoring bias (348 cases), premature diagnostic closure (291), inadequate investigations (90)
- All models tended to increase confidence in diagnoses they maintained between stages, regardless of correctness
- Information-seeking precision/recall varied by mapper type: MLLM achieved 0.88 precision/recall in investigation queries versus 0.79 for deterministic mapper

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Performance degradation from full-information to interactive settings reveals a gap between knowledge retrieval and sequential information-gathering capabilities.
- **Mechanism:** The benchmark constrains information access, forcing models to actively query rather than passively receive. This tests whether models can formulate targeted queries that maximize diagnostic information gain per turn.
- **Core assumption:** Medical knowledge encoding in LLMs is separable from strategic information-seeking behavior.
- **Evidence anchors:**
  - [abstract] "Top-1 accuracy dropped from 69% with full information to 35% in interactive scenarios"
  - [section 4.1] "most models at least doubling in accuracy performance" between final diagnosis and full information conditions
  - [corpus] MedKGI paper similarly identifies that LLMs "struggle to emulate the iterative, diagnostic hypothesis-driven reasoning" in clinical scenarios
- **Break condition:** If models were simply retrieving memorized cases, full-information and interactive performance would correlate more strongly. The gap suggests genuine sequential reasoning limitations.

### Mechanism 2
- **Claim:** The four failure modes (anchoring bias, excessive investigation, premature closure, missing critical conditions) map onto well-characterized human clinical cognitive errors.
- **Mechanism:** By structuring evaluation around recognized clinical failure patterns, the benchmark provides actionable failure signals rather than aggregate accuracy alone. The correlation analysis between diagnostic adaptation behaviors and outcomes quantifies how models update beliefs.
- **Core assumption:** LLM failure modes are analogous to human cognitive biases rather than random errors.
- **Evidence anchors:**
  - [section 4.2] Quantitative analysis found "inappropriate hypothesis generation (348 cases), premature diagnostic closure (291), inadequate investigations (90)"
  - [section 4.1] "Diagnosis removal and diagnosis maintenance both showed significant correlation to all 3 outcome metrics"
  - [corpus] DiagnosisArena paper also benchmarks diagnostic reasoning, suggesting convergent validation of this evaluation paradigm
- **Break condition:** If failure modes were randomly distributed rather than clustering into these patterns, the taxonomy would lack diagnostic utility.

### Mechanism 3
- **Claim:** Confidence calibration degrades independently of accuracy, with models increasing confidence in maintained diagnoses regardless of correctness.
- **Mechanism:** The benchmark tracks confidence-weighted accuracy (Sconf) and confidence shifts across stages. Models that recalibrate confidence meaningfully when receiving new information show better diagnostic adaptation.
- **Core assumption:** Confidence miscalibration indicates metacognitive limitations rather than output distribution artifacts.
- **Evidence anchors:**
  - [section 4.1] "All models tended to increase confidence in diagnoses they maintained between stages, regardless of whether those diagnoses were correct"
  - [section 4.1] "This suggests that models' growing certainty often developed independently of diagnostic correctness, indicating a form of confirmation bias"
  - [corpus] Weak direct corpus evidence on confidence calibration in medical LLMs; this represents an underexplored area
- **Break condition:** If confidence were well-calibrated, Sconf would strongly correlate with accuracy, and confidence shifts would systematically favor correct diagnoses.

## Foundational Learning

- **Concept: Partially Observable Markov Decision Processes (POMDPs)**
  - Why needed here: Clinical diagnosis is explicitly framed as a POMDP where physicians iteratively update belief states. Understanding belief-state updates, value of information, and exploration-exploitation tradeoffs is essential.
  - Quick check question: Can you explain why ordering a test with 100% sensitivity but 50% specificity might have different information value depending on current diagnostic uncertainty?

- **Concept: Hypothetico-Deductive Clinical Reasoning**
  - Why needed here: The benchmark evaluates whether models follow this standard clinical pattern: generate early hypotheses → gather discriminating evidence → refine or reject hypotheses. Deviations (premature closure, anchoring) map to specific failure modes.
  - Quick check question: Given symptoms A, B, C with differential diagnoses X (80% prior) and Y (20% prior), what question would maximally discriminate between them?

- **Concept: Information Retrieval Precision vs. Recall in Clinical Context**
  - Why needed here: The benchmark explicitly measures both (targeted inquiry vs. thoroughness). High precision/low recall indicates selective but incomplete gathering; the inverse suggests undifferentiated data collection.
  - Quick check question: If a model asks 10 questions but only 3 are relevant to the correct diagnosis, what does this suggest about its diagnostic strategy?

## Architecture Onboarding

- **Component map:** Clinical Case (C) -> Mapper Module (M) -> Parser Module (P) -> Examiner (E)
- **Critical path:** Agent query → Mapper extracts entities and matches to KC → Parser formats response → Agent updates hypothesis → Repeat until diagnosis. The Mapper's accuracy directly gates evaluation validity.
- **Design tradeoffs:**
  - Deterministic vs. LLM-based Mapper: Deterministic ensures reproducibility but struggles with complex queries; LLM-based achieves higher precision/recall (Table 6) but requires validation for consistency
  - Turn limits (20 global, category-specific) prevent infinite loops but may constrain complex cases
  - Negative result handling: Explicitly returning "not available" prevents hallucination but requires complete case schemas
- **Failure signatures:**
  - Models order tests irrelevant to their stated hypotheses → information-seeking inefficiency
  - Confidence increases on maintained incorrect diagnoses → confirmation bias
  - Early diagnosis submitted without adequate investigation → premature closure
  - Testing for rare conditions while missing common presentations → inappropriate hypothesis generation
- **First 3 experiments:**
  1. **Mapper validation:** Run both MD and MLLM variants on held-out queries with human-annotated ground truth. Target: MLLM should achieve >0.85 precision/recall (per Table 6).
  2. **Failure mode frequency analysis:** Classify a sample of failed cases by failure mode. Verify that reasoning failures (hypothesis generation, closure) outnumber knowledge failures (investigation ordering).
  3. **Confidence-accuracy calibration check:** For each model, compute the correlation between confidence shifts and accuracy improvements. Target: positive correlation indicates calibrated updating; near-zero suggests miscalibration.

## Open Questions the Paper Calls Out
- **Question:** How does the diagnostic performance of SOTA LLMs on VivaBench compare to a statistically significant cohort of human clinicians?
- **Basis in paper:** [explicit] Appendix C states, "A larger scale human baseline would be one valuable direction for future research," noting the current baseline was limited to only 4 clinicians on 14 cases.
- **Why unresolved:** The small human sample size limits the statistical power to define a robust performance ceiling or quantify the true human-AI performance gap.
- **What evidence would resolve it:** A comprehensive study evaluating a larger cohort of board-certified physicians on the full VivaBench dataset.

- **Question:** Can specific training or prompting strategies mitigate the identified failure modes, such as anchoring bias and excessive investigation ordering?
- **Basis in paper:** [inferred] The Discussion identifies these failure modes as "fundamental limitations" in current models, implying a need for methods to overcome them.
- **Why unresolved:** The paper evaluates off-the-shelf models but does not test specific interventions designed to correct these systematic reasoning errors.
- **What evidence would resolve it:** Experiments comparing baseline models against those fine-tuned or prompted with explicit debiasing strategies on VivaBench metrics.

- **Question:** What is the variance in diagnostic accuracy over multiple evaluation runs, given the stochastic nature of long-horizon reasoning?
- **Basis in paper:** [explicit] The Limitations section notes that "we conducted only a single evaluation run for each model, which may not account for the stochastic nature of LLM outputs."
- **Why unresolved:** Single runs at temperature 0 may not capture the variance in reasoning trajectories that can diverge significantly over the multi-step interaction limit.
- **What evidence would resolve it:** Reporting performance variance and failure mode frequency across multiple evaluation runs with different random seeds.

## Limitations
- Structured vs. Free-text Evaluation: The framework evaluates structured information retrieval rather than free-text clinical reasoning, potentially artificially constraining models' diagnostic capabilities.
- Ground Truth Granularity: ICD-10 mapping for evaluation may not capture clinically meaningful distinctions between related diagnoses, potentially inflating accuracy metrics.
- Reproducibility Constraints: Evaluation pipeline depends on specific LLM configurations (gpt-4.1 with temperature=0) that may not be accessible to all researchers.

## Confidence
- **High Confidence:** The performance degradation gap (69% → 35% accuracy) between full-information and interactive settings is well-documented and statistically significant.
- **Medium Confidence:** The confidence miscalibration finding is compelling but requires further validation; correlation between diagnostic adaptation behaviors and outcomes is statistically significant.
- **Low Confidence:** Information-seeking precision/recall metrics for different mapper variants are reported but not independently validated; clinical relevance of 0.88 vs 0.79 precision/recall is not fully contextualized.

## Next Checks
1. **Free-text Extension Validation:** Modify the evaluation framework to allow free-text information gathering while maintaining the structured ground truth. Compare performance between structured and free-text conditions to quantify the artificial constraint effect on diagnostic accuracy.
2. **Confidence Calibration Benchmark:** Implement proper calibration analysis by binning model confidence scores and comparing predicted vs. observed accuracy rates. Test whether models systematically over- or under-estimate their diagnostic certainty across different clinical scenarios.
3. **Temporal Reasoning Stress Test:** Design cases that specifically require temporal reasoning (disease progression, treatment response) and evaluate whether the four failure modes manifest differently in time-sensitive diagnostic scenarios compared to static presentations.