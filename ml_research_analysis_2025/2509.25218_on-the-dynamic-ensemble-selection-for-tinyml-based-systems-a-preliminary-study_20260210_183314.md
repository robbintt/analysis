---
ver: rpa2
title: On The Dynamic Ensemble Selection for TinyML-based Systems -- a Preliminary
  Study
arxiv_id: '2509.25218'
source_url: https://arxiv.org/abs/2509.25218
tags:
- classifiers
- selection
- ensemble
- dynamic
- tinyml
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explores Dynamic Ensemble Selection (DES) for TinyML
  systems, addressing the challenge of balancing inference time and classification
  quality under resource constraints. A DES-Clustering approach was examined for multi-class
  computer vision tasks, allowing adjustment of classification accuracy to affect
  latency and energy consumption.
---

# On The Dynamic Ensemble Selection for TinyML-based Systems -- a Preliminary Study

## Quick Facts
- arXiv ID: 2509.25218
- Source URL: https://arxiv.org/abs/2509.25218
- Reference count: 17
- Primary result: DES-Clustering enables runtime accuracy-latency tuning via the J parameter on resource-constrained devices, improving classification accuracy at the cost of increased inference time.

## Executive Summary
This study explores Dynamic Ensemble Selection (DES) for TinyML systems, addressing the challenge of balancing inference time and classification quality under resource constraints. A DES-Clustering approach was examined for multi-class computer vision tasks, allowing adjustment of classification accuracy to affect latency and energy consumption. The TinyDES-Clustering library was implemented for embedded systems. Results showed that larger pools of classifiers for dynamic selection improved classification accuracy but increased average inference time. Statistical analysis confirmed that DES-Clustering with larger J values outperformed methods with smaller pools, demonstrating a trade-off between accuracy and energy efficiency.

## Method Summary
The method employs DES-Clustering, which uses K-means clustering to partition the feature space into regions of competence. For each cluster, a pool of classifiers is pre-trained and ranked by accuracy. At inference, a test sample is assigned to its nearest cluster, and the J most diverse classifiers from the top-N accurate ones are selected for evaluation. The J parameter directly controls the trade-off between accuracy and inference time. Experiments used Random Forest classifiers (25 estimators, max_depth=10 and 20 estimators, max_depth=5) and datasets MNIST, Fashion-MNIST, and EMNIST. The TinyDES-Clustering library converts trained models to C code for deployment on the NUCLEO-L476RG microcontroller.

## Key Results
- Larger pools of classifiers for dynamic selection improve classification accuracy, leading to increased average inference time.
- DES-Clustering with larger J values statistically significantly outperforms methods with smaller pools (α = 0.05).
- Clustering-based DES is lighter at inference than neighborhood-based methods due to reduced storage requirements.

## Why This Works (Mechanism)

### Mechanism 1
DES-Clustering enables runtime accuracy-latency tuning via the J parameter on resource-constrained devices. K-means partitions the feature space into clusters offline; each cluster stores a ranked pool of classifiers. At inference, the test sample's cluster is identified, and J diverse classifiers from the top-N accurate ones are selected. Larger J increases ensemble confidence but requires more evaluations. The core assumption is that cluster assignments made during training generalize to test samples, and diversity among selected classifiers meaningfully improves ensemble robustness.

### Mechanism 2
Clustering-based DES is lighter at inference than neighborhood-based methods because only N centroids (not a full validation set) are stored. Neighborhood methods require ROM for a validation set and online kNN distances. DES-Clustering replaces online neighbor search with centroid lookup, trading heavier offline training for leaner inference memory/compute. The core assumption is that storing centroids and associated classifier pools fits within TinyML ROM constraints and centroid lookup is sufficiently discriminative for competence estimation.

### Mechanism 3
Larger classifier pools available for dynamic selection yield statistically better accuracy, but increase average inference time. A richer pool increases the probability that competent classifiers exist for each local region. DES-Clustering selects the J best among them, so larger pools improve selection quality at the cost of managing more metadata and potentially larger N. The core assumption is that base classifiers are diverse and weak enough that ensemble size meaningfully impacts accuracy without prohibitive overhead.

## Foundational Learning

- **Dynamic Ensemble Selection (DES)**: DES chooses classifiers per-sample at inference rather than fixing an ensemble offline, enabling accuracy-resource trade-offs. Quick check: Given a test sample, how does DES decide which classifiers to use versus a static ensemble?
- **TinyML resource constraints**: The mechanism explicitly targets microcontrollers with limited flash/SRAM and energy budgets. Quick check: Why is storing a validation set for kNN problematic on a device with 128 kB SRAM?
- **Accuracy-latency-energy trade-off**: The J parameter directly controls this trade-off; understanding it is critical for system design. Quick check: If you halve J, what is the expected direction of change for accuracy and for energy per inference?

## Architecture Onboarding

- **Component map**: Dataset -> preprocessing -> K-means clustering -> classifier pool training (Random Forests) -> per-cluster classifier ranking -> export centroids and pools. Online: Input -> feature extraction -> centroid lookup -> select J classifiers -> J inferences -> majority/aggregated vote -> output.
- **Critical path**: 1. Cluster assignment must be correct and fast (centroid distance computation). 2. J classifier inferences dominate latency; optimize base models for target hardware. 3. Aggregation must be deterministic and low-overhead.
- **Design tradeoffs**: Higher J → better accuracy, higher latency/energy. Higher N (clusters) → finer competence regions, more ROM, slightly higher lookup cost. Deeper/complex base classifiers → higher per-classifier latency; shallower models may reduce J impact.
- **Failure signatures**: Accuracy drops after deployment: likely distribution shift invalidating cluster competence. Latency spikes: J too large or base models too complex for target MCU. ROM overflow: pool size plus centroids exceeds flash; reduce pool or N.
- **First 3 experiments**: 1. Baseline calibration: Run DES-Clustering on the target MCU with J in {5, 10, 15, 20}, measure accuracy and average inference time on a held-out test set; plot the Pareto frontier. 2. Memory profiling: Measure flash and SRAM usage for each J/N configuration; ensure headroom for application code and buffers. 3. Energy estimation: Using a power profiler or MCU energy measurement tool, quantify energy per inference across J values; correlate with latency to validate assumed proportionality.

## Open Questions the Paper Calls Out

- **Can a feedback loop utilizing battery level monitoring and solar energy harvesting be effectively implemented to dynamically adjust the J parameter in real-time?**: The authors state in the conclusion that future work involves "incorporating battery level dependency and integrating energy harvesting via solar panels" to enable the system to "dynamically adjust the value of the J in real time." The current study evaluates static configurations of J but does not implement the logic or hardware integration required for autonomous, runtime adaptation based on power state.

- **Does the TinyDES-Clustering method generalize to non-computer vision use cases or different model architectures?**: The paper explicitly mentions that "Further work may also include exploring additional use cases." The experiments were restricted to three flattened image datasets using Random Forest classifiers, leaving the library's performance on time-series, audio, or tabular data unverified.

- **Does the observed linear increase in inference time translate linearly to energy consumption on the embedded hardware?**: The authors infer energy consumption impacts from inference time metrics ("consumption of more energy"), but report only time (ms) and not actual power draw (e.g., mJ). While time and energy are often correlated, the specific power profile of the STM32L476RG microcontroller during dynamic selection may reveal non-linear energy costs due to overhead or peak currents not captured by timing alone.

## Limitations

- The study's conclusions are limited by the small set of datasets (MNIST, Fashion-MNIST, EMNIST) and the specific choice of base classifiers (two Random Forest variants).
- The J parameter's impact on the accuracy-latency-energy trade-off needs validation across diverse computer vision tasks and classifier types.
- While the TinyDES-Clustering library enables deployment on embedded systems, the paper does not provide extensive real-world testing or long-term performance monitoring under varying operational conditions.

## Confidence

- **High confidence**: The trade-off between classification accuracy and inference time/energy is well-supported by the experimental results and statistical analysis.
- **Medium confidence**: The effectiveness of DES-Clustering for TinyML systems is supported, but the generalizability to other tasks and datasets needs further validation.
- **Medium confidence**: The TinyDES-Clustering library enables deployment on embedded systems, but its performance and usability in real-world scenarios require more extensive testing.

## Next Checks

1. **Generalization across datasets**: Test DES-Clustering on a broader range of computer vision datasets, including more complex and diverse tasks, to validate its effectiveness beyond MNIST-like datasets.
2. **Real-world deployment testing**: Deploy the TinyDES-Clustering library on target embedded systems and monitor its performance under varying operational conditions, including changes in input data distribution and resource availability.
3. **Impact of base classifier diversity**: Experiment with different types and configurations of base classifiers to understand how their diversity affects the overall performance of DES-Clustering and the accuracy-latency-energy trade-off.