---
ver: rpa2
title: Do Language Model Agents Align with Humans in Rating Visualizations? An Empirical
  Study
arxiv_id: '2505.06702'
source_url: https://arxiv.org/abs/2505.06702
tags:
- agent
- ratings
- visualization
- human
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the alignment between language model-based
  agents and human participants in rating visualizations, addressing a gap in understanding
  how well AI agents can simulate human feedback in empirical visualization studies.
  The authors conducted three studies, replicating existing human-subject experiments
  on visualization tasks and comparing agent ratings to human results.
---

# Do Language Model Agents Align with Humans in Rating Visualizations? An Empirical Study

## Quick Facts
- **arXiv ID:** 2505.06702
- **Source URL:** https://arxiv.org/abs/2505.06702
- **Reference count:** 40
- **Primary result:** LLM agents align more closely with human ratings on visualization tasks when expert hypotheses are highly confident, but cannot capture diverse user profiles or complex findings

## Executive Summary
This study investigates whether language model-based agents can effectively simulate human subjective ratings in empirical visualization studies. Through three experimental studies replicating published human-subject experiments, the authors systematically evaluate agent alignment with human responses across six different visualization tasks. The research reveals that agent ratings show meaningful alignment with human conclusions when expert hypotheses are highly confident, but struggle to capture diverse user profiles or replicate complex findings. While agent-based evaluations show promise for rapid prototyping and parameter tuning in experimental design, they cannot fully replace human studies and require careful consideration of their limitations.

## Method Summary
The study used GPT-4V as the primary agent, implementing a three-step prompting process: eliciting internal visualization knowledge, outlining task completion steps, and assigning ratings. The authors replicated six published human-subject studies from the Open Science Framework, converting between-subjects designs to within-subjects batches (3-8 images per session) to address agent consistency issues. Each experiment was repeated 5-20 times, with statistical analysis performed using bootstrap (10,000 replicates). Expert confidence was assessed via 5 visualization experts rating hypotheses on a 3-point Likert scale, and alignment was measured by comparing agent conclusions against original human conclusions rather than absolute ratings.

## Key Results
- Agent ratings align significantly with human responses when expert hypotheses are highly confident
- Agents struggle to capture diverse user profiles and simulate individual differences
- RAG-enhanced evaluation shows promise but suffers from unreliable web search quality
- Agent alignment is conditional rather than universal across visualization tasks

## Why This Works (Mechanism)

### Mechanism 1
Agents function as aggregators of historical training data rather than simulators of individual cognition. They approximate the "average" human response. When a visualization task relies on common design conventions (where experts have high confidence), the agent's training data contains sufficient examples to mimic the aggregate human response. Alignment degrades when tasks require nuanced perception or diverse user profiles.

### Mechanism 2
Agents prioritize textual-semantic reasoning over visual-perceptual processing, leading to "anti-human" biases when text and visuals conflict. The model processes visual inputs by effectively converting them into semantic concepts to reason about them. If textual cues contradict visual patterns, the agent often defaults to textual logic, whereas humans prioritize visual perception.

### Mechanism 3
External knowledge injection (RAG) can correct reasoning errors but risks introducing bias if retrieved context is misleading. Agents use a multi-step process and injecting specific external knowledge at the "step planning" phase steers the subsequent reasoning. However, if the retrieval is generic or irrelevant, it disrupts the chain-of-thought.

## Foundational Learning

- **Concept: Within-Subjects vs. Between-Subjects Experimental Design**
  - **Why needed here:** Agents cannot maintain a consistent "standard" across independent sessions. The architecture forces a transition from between-subjects (human standard) to within-subjects (all conditions in one prompt) to enforce local consistency.
  - **Quick check question:** If you present an agent with one visualization per prompt to rate "ease of use," will the ratings be comparable across 100 prompts? (Answer: No, the paper suggests they lack a global baseline).

- **Concept: "Ground Truth" vs. "Aggregate Alignment"**
  - **Why needed here:** The study measures success not by matching exact human scores but by matching the *conclusions*. Understanding this distinction is vital for interpreting agent evaluation results.
  - **Quick check question:** If humans rate a chart 85/100 and the agent rates it 65/100, but both agree it is better than a chart rated 40/100, is the agent "aligned"? (Answer: Yes, per the paper's methodology).

- **Concept: Hallucination in Visual Reasoning**
  - **Why needed here:** Agents often hallucinate steps or reasons (e.g., claiming to see "grayscale" when looking at color, or ignoring color entirely). Engineers must monitor the "reasoning" output, not just the rating.
  - **Quick check question:** An agent rates two images differently but gives the exact same textual explanation for both. What does this imply about its visual attention? (Answer: It is likely ignoring the visual differences and relying on the shared prompt context).

## Architecture Onboarding

- **Component map:** GPT-4V -> Input Preprocessor -> Prompt Engineer -> Agent Core -> Knowledge Injector (Optional)
- **Critical path:**
  1. Select target human study (must have open-source data)
  2. Verify expert confidence (if low, expect low agent alignment)
  3. Convert experimental design to within-subjects batches
  4. Execute multi-turn prompt (Visuals + Text -> Steps -> Ratings)
  5. Analyze alignment based on *trends/conclusions*, not absolute values
- **Design tradeoffs:**
  - Batching multiple images improves comparative accuracy but hits token limits (max ~9 images)
  - RAG improves performance on niche charts but risks "data pollution" from web search noise
- **Failure signatures:**
  - Stereotyping: Agent outputs generic "textbook" answers regardless of visual input
  - Text reliance: High ratings for charts with clear labels even if visual encoding is misleading
  - Diversity Collapse: Running same prompt 30 times yields near-identical ratings
- **First 3 experiments:**
  1. **Sanity Check (Time Series):** Replicate Study 1 using Line vs. Heatmap comparison to verify agent preferences
  2. **Alignment Stress Test (Uncertainty):** Run Imputation for Uncertainty experiment with text-only vs text+image conditions
  3. **Knowledge Injection (Fit Estimation):** Attempt to replicate "decentering" finding using manual knowledge injection

## Open Questions the Paper Calls Out

### Open Question 1
How can language model agents be effectively modified to simulate diverse user profiles and capture individual differences in visualization evaluation? The study found that GPT-4V acts as an "aggregation of historically trained knowledge" that mimics an average user, but current methods cannot accurately model the variance found in human subject pools.

### Open Question 2
Can agent-based simulation methodologies be extended to evaluate interactive visualization systems and complex visual analytics workflows? The current scope is "limited to rate basic 2D visualizations without interaction," leaving the evaluation of interactive systems as a gap.

### Open Question 3
How can automatic knowledge injection (such as RAG) be implemented to improve alignment without introducing the biases or reliability issues observed in Study III? While knowledge injection helped, web-retrieval introduced potential biases and suffered from semantic mismatches with visualization design knowledge.

## Limitations
- Agent alignment heavily depends on expert confidence levels, which may not generalize to novel visualization designs
- Within-subjects design modification was necessary due to agent consistency issues but may introduce artificial constraints
- RAG-enhanced evaluation shows promise but suffers from unreliable web search quality and potential data pollution
- Analysis focuses on conclusion-level alignment rather than absolute rating accuracy

## Confidence
- **High Confidence:** Agents align better with human ratings when expert hypotheses are highly confident
- **Medium Confidence:** Agents struggle with diverse user profiles and low-level perceptual details
- **Medium Confidence:** Text-semantic reasoning often overrides visual-perceptual processing in agents

## Next Checks
1. **Perceptual Detail Test:** Conduct a controlled experiment isolating low-level visual features (color encoding, texture patterns) with text removed to quantify the exact threshold where agent performance degrades compared to humans.
2. **Expert Confidence Calibration:** Systematically vary expert confidence ratings across a spectrum of visualization tasks and measure corresponding agent alignment to establish a predictive model of when agents can be trusted.
3. **Demographic Simulation Test:** Design visualization tasks requiring distinct user profiles (novice vs expert, student vs professional) and measure whether agents can simulate divergent responses rather than producing a single "average" rating.