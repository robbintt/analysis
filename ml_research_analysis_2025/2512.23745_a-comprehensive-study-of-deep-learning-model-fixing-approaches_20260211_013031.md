---
ver: rpa2
title: A Comprehensive Study of Deep Learning Model Fixing Approaches
arxiv_id: '2512.23745'
source_url: https://arxiv.org/abs/2512.23745
tags:
- approaches
- fixing
- data
- these
- model-level
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a large-scale empirical study of 16 state-of-the-art
  deep learning model fixing approaches. The study evaluates these approaches across
  multiple dimensions, including fixing effectiveness, robustness, fairness, backward
  compatibility, and efficiency, using a diverse set of datasets, model architectures,
  and application domains.
---

# A Comprehensive Study of Deep Learning Model Fixing Approaches

## Quick Facts
- **arXiv ID:** 2512.23745
- **Source URL:** https://arxiv.org/abs/2512.23745
- **Reference count:** 40
- **Primary result:** No single approach can achieve superior fixing performance while maintaining accuracy, robustness, fairness, and backward compatibility.

## Executive Summary
This paper presents a large-scale empirical study of 16 state-of-the-art deep learning model fixing approaches. The study evaluates these approaches across multiple dimensions, including fixing effectiveness, robustness, fairness, backward compatibility, and efficiency, using a diverse set of datasets, model architectures, and application domains. The experimental setup is standardized to ensure fair and comprehensive evaluation. Key findings reveal that model-level approaches generally outperform others in accuracy improvement and error correction, but no single approach can simultaneously maximize all desired properties. The study highlights the need for more effective approaches for large-scale models and provides actionable recommendations for practitioners and insights for researchers.

## Method Summary
The study evaluates 16 fixing approaches across three categories (model-level, layer-level, neuron-level) using five benchmark datasets (MNIST, UTKFace, CIFAR10, CIFAR10S, ImageNet) with associated model architectures (LeNet5, FaceNet, VGG16, ResNet18, DenseNet121). Metrics include Accuracy, Repair Rate (RR), Attack Success Rate (ASR) for robustness, Average Absolute Odds Difference (AAOD) for fairness, and Negative Flip Rate (NFR) for backward compatibility. The evaluation platform standardizes data splits (train/valid/repair/test) and library conversions (TensorFlow to PyTorch via ONNX). Approaches are adapted to new datasets via grid search, and fixing is performed using the Repair set with model selection via the Validation set and final evaluation on the Test set.

## Key Results
- Model-level approaches outperform layer- and neuron-level approaches in accuracy improvement and error correction across datasets.
- No single approach can achieve superior fixing performance while improving accuracy and maintaining all other properties (robustness, fairness, backward compatibility).
- Current fixing approaches are more effective for small-scale models, with performance degrading significantly on large and complex models.
- Layer-level approaches are the most time-efficient, followed by neuron-level and model-level approaches.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Model-level approaches generally outperform layer- and neuron-level approaches in correctness improvement.
- **Mechanism:** Model-level approaches optimize all parameters globally (e.g., via retraining or finetuning on augmented/selected data), which helps preserve inter-neuron synergy and better correct systematic data distribution issues. Layer-level approaches focus on structural fixes with conservative weight changes, while neuron-level approaches make highly localized adjustments that risk disrupting learned representations.
- **Core assumption:** Global parameter optimization more effectively addresses the root causes of mispredictions than constrained, localized modifications.
- **Evidence anchors:**
  - [abstract] "Model-level approaches demonstrate superior fixing effectiveness compared to others."
  - [section 6.2] "Model-level approaches consistently outperform others across datasets... 87% (13/15) of top-3 performance results fall into model-level approaches."
  - [corpus] No direct corpus evidence; related work discusses explainability/robustness broadly, not this specific fixing comparison.
- **Break condition:** For large-scale models where full retraining is impractical or causes unacceptable regression, the advantage of model-level approaches may diminish.

### Mechanism 2
- **Claim:** Fixing effectiveness degrades as model complexity increases.
- **Mechanism:** Current fixing strategies (especially weight-adjustment and search-based methods) struggle with the larger parameter spaces and more complex inter-dependencies in large models (e.g., DenseNet121, ResNet18). Methods like Apricot (AP) achieve high repair rates on small models (e.g., LeNet5 on MNIST) but fail or corrupt accuracy on complex models.
- **Core assumption:** The difficulty of searching or optimizing in high-dimensional weight space grows faster than the capacity of current fixing algorithms.
- **Evidence anchors:**
  - [abstract] "Current fixing approaches are more effective for small-scale models, and there is a need for more effective approaches for large and complex models."
  - [section 6.2] "Take AP... on MNIST... RR of over 70%... On CIFAR10S... drops to merely 2.62%... On ImageNet... accuracy plummets by more than 10%."
  - [corpus] No direct corpus support for this specific scaling claim.
- **Break condition:** If new scalable fixing methods (e.g., efficient finetuning or modular repair for LLMs) are developed, this degradation may be mitigated.

### Mechanism 3
- **Claim:** No single approach simultaneously maximizes fixing performance, accuracy, robustness, fairness, and backward compatibility.
- **Mechanism:** Aggressive fixes (e.g., retraining with objective function changes) improve repair rate but introduce regression faults or hurt robustness/fairness. Conservative fixes (e.g., verification-guided or spectrum-based methods) preserve other properties but have low repair rates. This tradeoff is inherent in how much the model parameters are altered.
- **Core assumption:** Multi-property preservation requires explicitly balancing modification extent across all properties, which current single-objective fixing methods do not do.
- **Evidence anchors:**
  - [abstract] "No single approach can achieve superior fixing performance while improving accuracy and maintaining all other properties."
  - [section 6.3] "Table 4 shows that no approach falls into group a across all metrics... approaches with higher RR usually have worse NFR."
  - [corpus] Probabilistic robustness and explainability works emphasize tradeoffs, but not this exact multi-property fixing tradeoff.
- **Break condition:** If multi-objective or post-processing mitigation strategies (e.g., ensemble, uncertainty alignment) become standard in fixing pipelines, a single approach might achieve better balance.

## Foundational Learning

- **Concept:** Tradeoff between repair rate (RR) and side effects (NFR, ASR, AAOD).
  - **Why needed here:** Understanding that improving correctness often harms other properties is essential for selecting or designing fixing approaches that match industrial priorities.
  - **Quick check question:** Can you explain why a method with high repair rate might still be unacceptable for a safety-critical deployment?

- **Concept:** Granularities of fixing: model-level, layer-level, neuron-level.
  - **Why needed here:** The paper classifies and compares approaches at these three granularities; knowing their assumptions and typical performance patterns helps in choosing the right approach for a given bug and model.
  - **Quick check question:** Given a misbehavior suspected to be due to a poorly learned layer in a CNN, which granularity would you first consider and why?

- **Concept:** Data leakage and evaluation regularization in model fixing.
  - **Why needed here:** The paper highlights that many prior works used test data for fixing or evaluation, causing over-optimistic results. A standardized evaluation setup (repair set, validation set for model selection, test set for final eval) is critical for reliable measurement.
  - **Quick check question:** Why is using part of the test set for repair data considered problematic, and how does the paper's setup prevent this?

## Architecture Onboarding

- **Component map:** Five datasets (MNIST, UTKFace, CIFAR10, CIFAR10S, ImageNet) -> Three categories of approaches (model-level, layer-level, neuron-level) -> Five architectures (LeNet5, FaceNet, VGG16, ResNet18, DenseNet121) -> Five metrics (Accuracy, RR, ASR, AAOD, NFR)

- **Critical path:** 1) Choose appropriate fixing granularity based on bug hypothesis and model scale (e.g., model-level for data issues, neuron-level for limited data access). 2) Apply fixing approach with the standardized setup (repair set for fixing, validation set for early stopping/model selection). 3) Evaluate on all relevant metrics (Accuracy, RR, ASR, AAOD, NFR) to understand tradeoffs. 4) If side effects are unacceptable, apply post-processing mitigation (e.g., ensemble voting, uncertainty alignment).

- **Design tradeoffs:** Higher repair effectiveness vs. lower regression faults; global optimization vs. scalability for large models; verification-guided soundness vs. runtime overhead; data availability vs. approach applicability.

- **Failure signatures:** 1) CORRUPTED: accuracy drops >10% (e.g., AP on ImageNet). 2) FAILURE: runtime errors or OOM (e.g., 3M on ImageNet). 3) UNADAPTABLE: missing structural requirements or external resources (e.g., RA requires 3+ consecutive FC layers). 4) High NFR: many regression faults introduced (e.g., ET on ImageNet).

- **First 3 experiments:**
  1. Baseline applicability check: Attempt to run each candidate approach on your target model/dataset using the paper's standardized setup; record which ones succeed, fail, or corrupt.
  2. Effectiveness vs. side-effect profiling: For applicable approaches, measure Accuracy, RR, ASR, AAOD, and NFR; compare to baseline model to quantify tradeoffs.
  3. Post-processing mitigation test: For approaches with high RR but unacceptable side effects, apply ensemble or uncertainty-alignment post-processing (as in Figure 1) and re-evaluate all metrics.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can existing deep learning (DL) model fixing approaches be effectively scaled or redesigned to support large-scale architectures, such as Large Language Models (LLMs) and autonomous agents?
- **Basis in paper:** [explicit] The paper concludes (Finding 4) that "Current fixing approaches suit small-scale models better" and states in Section 7.2 that "There is an urgent need for scalable fixing approaches for large-scale architectures like LLMs and Agents."
- **Why unresolved:** The study demonstrates that as model complexity increases (e.g., moving from LeNet to DenseNet), the performance of current fixing methods degrades significantly or fails completely due to computational costs and complexity.
- **What evidence would resolve it:** The development of a fixing approach that maintains high Repair Rate (RR) and accuracy improvement on LLM benchmarks without inducing excessive computational overhead or regression faults.

### Open Question 2
- **Question:** What are the theoretical or causal mechanisms linking specific model fixing operations to the degradation of secondary properties (robustness, fairness, backward compatibility)?
- **Basis in paper:** [explicit] The authors state in Section 7.2 that "Future research should clarify how side effects occur during fixing to enable proposing more mitigation strategies."
- **Why unresolved:** While the study observes that radical modifications (like retraining) often harm backward compatibility, the precise internal mechanics of *why* fixing a specific fault disrupts robustness or fairness remain under-explored.
- **What evidence would resolve it:** A formal analysis or method that can predict the specific impact on robustness/fairness metrics based on the proposed weight changes before the fix is applied.

### Open Question 3
- **Question:** How can model-level, layer-level, and neuron-level strategies be optimally combined to maximize repair effectiveness while minimizing side effects?
- **Basis in paper:** [explicit] Section 7.2 notes that "HybridRepair and Apricot repair largely disjoint sets of samples" and "Combining their outputs... may enhance overall repair performance," but concludes that "how to combine such strategies effectively while preserving their respective strengths remains an open question."
- **Why unresolved:** The paper finds a trade-off where model-level approaches are most effective but most disruptive, while neuron-level approaches are safer but less effective; no hybrid method currently exists to leverage the advantages of both.
- **What evidence would resolve it:** A hybrid framework that dynamically selects the repair granularity (model, layer, or neuron) based on the specific fault type, resulting in superior performance on the standardized benchmarks proposed in the paper.

## Limitations

- The study is limited to image classification tasks and specific model architectures (CNNs), leaving the behavior of these approaches in other domains (e.g., text, video) unknown.
- The scalability analysis for large models is based on a limited sample (five datasets and architectures), and the specific hyperparameters used to adapt each approach are not fully specified.
- The claim about multi-property tradeoffs is well-supported by experimental data, but lacks broader corpus validation.

## Confidence

- **High confidence** in the finding that model-level approaches outperform others on small-to-medium models.
- **Medium confidence** in the scalability limitation claim due to limited model complexity sampling.
- **Low confidence** in claims about corpus support for specific findings due to limited external references.

## Next Checks

1. Reproduce the core results (accuracy, repair rate, side effects) for at least two model-level and two layer-level approaches on MNIST and CIFAR10 to verify the stated performance hierarchy.
2. Test the failure modes (CORRUPTED, FAILURE, UNADAPTABLE) on a held-out small model (e.g., LeNet5) to confirm the diagnostic framework.
3. Apply post-processing mitigation (ensemble or uncertainty alignment) to a high-RR but high-NFR approach and re-evaluate all metrics to assess practical feasibility.