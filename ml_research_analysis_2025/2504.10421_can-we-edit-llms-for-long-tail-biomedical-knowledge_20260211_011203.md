---
ver: rpa2
title: Can We Edit LLMs for Long-Tail Biomedical Knowledge?
arxiv_id: '2504.10421'
source_url: https://arxiv.org/abs/2504.10421
tags:
- knowledge
- biomedical
- editing
- llms
- long-tail
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates knowledge editing for long-tail biomedical
  knowledge, revealing that while editing improves LLM performance on rare biomedical
  facts, it remains less effective than for popular knowledge due to the high prevalence
  of one-to-many knowledge triples. Pre-edit LLMs show declining accuracy with decreasing
  co-occurrence frequency, especially for one-to-many triples.
---

# Can We Edit LLMs for Long-Tail Biomedical Knowledge?

## Quick Facts
- **arXiv ID**: 2504.10421
- **Source URL**: https://arxiv.org/abs/2504.10421
- **Authors**: Xinhao Yi; Jake Lever; Kevin Bryson; Zaiqiao Meng
- **Reference count**: 40
- **Primary result**: Editing improves LLM performance on rare biomedical facts but remains less effective than for popular knowledge due to one-to-many relationships.

## Executive Summary
This study investigates knowledge editing for long-tail biomedical knowledge, revealing that while editing improves LLM performance on rare biomedical facts, it remains less effective than for popular knowledge due to the high prevalence of one-to-many knowledge triples. Pre-edit LLMs show declining accuracy with decreasing co-occurrence frequency, especially for one-to-many triples. Post-edit, all methods boost accuracy but gaps persist; ROME achieves over 98% reliability yet generalization still suffers for low-frequency knowledge. Analysis shows 90% of long-tail triples are one-to-many, limiting effective learning. Effective handling of one-to-many relationships is critical for bridging performance gaps.

## Method Summary
The study constructs the CliKT dataset from SNOMED CT and PubMed, classifying biomedical knowledge triples by document-level co-occurrence frequency to identify long-tail knowledge (frequency < 10). Five editing methods (ROME, MEMIT, MEND, IKE, FT) are applied to four LLMs (BioMedLM, BioGPT, Llama2, GPT-J). Knowledge is represented as triples (subject, relation, object) and converted to questions using templates. Performance is evaluated on reliability (edit accuracy), generalization (rephrased queries), and locality (unrelated fact preservation), with results stratified by knowledge frequency and one-to-many versus one-to-one classification.

## Key Results
- LLM accuracy on biomedical facts declines significantly as subject-object co-occurrence frequency decreases
- Post-edit accuracy improves across all methods but gaps between popular and long-tail knowledge persist
- ROME achieves over 98% reliability but generalization remains lower for long-tail knowledge
- 90% of long-tail triples are one-to-many relationships, limiting effective learning
- One-to-many knowledge shows larger performance gaps than one-to-one knowledge across all frequency bands

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LLM accuracy on biomedical facts declines as subject-object co-occurrence frequency in the pre-training corpus decreases.
- **Mechanism:** Pre-training learns factual associations through repeated exposure. When subject and object co-occur frequently (e.g., "Type 1 Diabetes" and "Insulin" in 1000+ documents), gradient updates consolidate these associations into model weights. Rare co-occurrences (e.g., "Evans Syndrome" in ~23 papers) provide insufficient signal for reliable learning.
- **Core assumption:** Document-level co-occurrence frequency approximates pre-training data exposure and learning opportunity.
- **Evidence anchors:**
  - [abstract] "real-world biomedical data often exhibit a long-tailed distribution, with a small amount of popular knowledge and a large amount of long-tail knowledge that appears rarely or only once"
  - [section 4.2] Figure 3 shows accuracy drops 16-23% relative when comparing long-tail (<10 co-occurrences) vs popular knowledge (≥1000 co-occurrences) across BioMedLM, Llama2, GPT-J, and BioGPT
  - [corpus] Limited direct corpus support; related work (Mallen et al. 2023, Kandpal et al. 2023) shows similar frequency-accuracy correlations in general domains
- **Break condition:** If co-occurrence doesn't capture actual pre-training exposure (e.g., entity appears frequently but not in knowledge-relevant contexts), the proxy fails.

### Mechanism 2
- **Claim:** One-to-many knowledge triples (one subject-relation pair mapping to multiple valid objects) disproportionately limit both pre-training acquisition and post-edit generalization for long-tail biomedical knowledge.
- **Mechanism:** In one-to-many cases, the model must learn multiple valid associations for the same query. During pre-training, rare co-occurrences compound this difficulty because each object-object pair receives fewer gradient updates. During editing, methods designed for single-object targets struggle to preserve or generalize across multiple correct answers.
- **Core assumption:** Current editing methods implicitly optimize for one-to-one mappings; evaluation metrics treat non-target valid objects as errors.
- **Evidence anchors:**
  - [abstract] "Analysis shows 90% of long-tail triples are one-to-many, limiting effective learning"
  - [section 4.4.1] Figure 5: One-to-one knowledge accuracy is relatively stable across frequency bands; one-to-many accuracy drops sharply. BioGPT shows 115.56% higher accuracy on one-to-one vs one-to-many
  - [section 4.4.2] Figure 6: Post-edit accuracy gap between one-to-one and one-to-many narrows but persists (42.19% → 16.43% gap after ROME editing on long-tail)
  - [corpus] Related work on multi-object editing (Same-Subject Editing) suggests this is a known but unresolved challenge
- **Break condition:** If editing methods explicitly handle one-to-many scenarios (e.g., by preserving existing valid associations while adding new ones), this limitation would diminish.

### Mechanism 3
- **Claim:** Locate-then-edit methods (ROME, MEMIT) achieve high reliability on edited facts but show reduced generalization and locality compared to in-context or meta-learning approaches.
- **Mechanism:** ROME and MEMIT use causal mediation analysis to identify specific MLP layers where factual associations are stored, then directly update weights. This creates precise local changes (high reliability) but can disrupt distributed representations needed for paraphrased queries (lower generalization) and may bleed into semantically proximate facts (lower locality).
- **Core assumption:** Factual knowledge is stored in specific MLP layers as key-value memories, and targeted weight updates don't cascade unpredictably.
- **Evidence anchors:**
  - [abstract] "ROME achieves over 98% reliability yet generalization still suffers for low-frequency knowledge"
  - [section 4.3, Table 2] ROME reliability: 98.02-98.66% across all frequency groups; Generalization: 68.42-72.54% (drops as frequency decreases); Locality: 83.70-84.62% (lower than MEMIT/MEND/IKE's 96-98%)
  - [section 4.3] "ROME achieves the best reliability and generalisation, it may slightly affect unrelated knowledge"
  - [corpus] EvoEdit and EtCon papers propose improved locate-then-edit variants, suggesting ongoing refinement of this mechanism
- **Break condition:** If factual storage is more distributed than assumed, or if weight updates interact non-linearly with other layers, targeted editing becomes unreliable.

## Foundational Learning

- **Concept: Knowledge triples (subject, relation, object)**
  - **Why needed here:** All experiments represent biomedical knowledge as structured triples (e.g., ⟨Diabetes, treated_by, Insulin⟩). Understanding this representation is essential for following the probing and editing methodology.
  - **Quick check question:** Given the triple ⟨Hypertension, associated_with, Kidney Disease⟩, what are the subject, relation, and object?

- **Concept: Co-occurrence frequency as knowledge popularity proxy**
  - **Why needed here:** The paper's central claim rests on using document-level co-occurrence (how often subject and object appear together in PubMed) as a proxy for knowledge frequency. This drives the long-tail classification and explains performance gaps.
  - **Quick check question:** If a subject-object pair appears in 5 documents vs. 5000 documents, which is classified as long-tail knowledge under the paper's threshold (α = 10)?

- **Concept: Knowledge probing vs. knowledge editing**
  - **Why needed here:** The paper uses probing to assess what LLMs know pre/post-edit, then evaluates editing methods on reliability, generalization, and locality. These are distinct operations with different goals.
  - **Quick check question:** What does "reliability" measure in editing evaluation, and how does it differ from "generalization"?

## Architecture Onboarding

- **Component map:**
  1. **CliKT Dataset Construction Pipeline** (Appendix A): SNOMED CT → Entity Linking (SapBERT) → PubMed co-occurrence counting → Triple classification → Question generation
  2. **Knowledge Probing Module**: Template-based question generation → LLM forward pass → Object extraction → Accuracy evaluation
  3. **Editing Methods**: ROME/MEMIT (locate-then-edit on MLP layers), MEND (meta-learning with gradient transformation), IKE (in-context learning), FT (single-layer fine-tuning)
  4. **Evaluation Framework**: Reliability (exact edit recall), Generalization (paraphrased queries), Locality (unrelated fact preservation)

- **Critical path:**
  1. Extract and classify triples by co-occurrence frequency
  2. Run pre-edit probing to establish baseline accuracy across frequency groups
  3. Apply editing method (e.g., ROME) to inject target knowledge
  4. Evaluate post-edit probing accuracy, reliability, generalization, and locality
  5. Subdivide by one-to-one vs. one-to-many to diagnose performance gaps

- **Design tradeoffs:**
  - **ROME vs. MEMIT:** ROME edits single layers with high precision; MEMIT extends to multiple layers for batch editing but trades some reliability
  - **Locate-then-edit vs. In-context (IKE):** Weight updates (ROME/MEMIT/MEND/FT) persist across sessions; IKE requires demonstration context at inference time but preserves original weights
  - **Document-level vs. sentence-level co-occurrence:** Document-level is computationally tractable but may overcount spurious co-occurrences (acknowledged limitation in paper)

- **Failure signatures:**
  - **High reliability, low generalization:** Model memorizes edited fact verbatim but fails on paraphrased queries (common for one-to-many long-tail knowledge)
  - **Locality violations:** Editing one fact inadvertently changes predictions for unrelated facts (ROME shows 83-84% locality vs. 96-98% for MEMIT/IKE)
  - **Persistent long-tail gap:** Post-edit accuracy on long-tail knowledge remains 16% lower than popular knowledge even with best methods

- **First 3 experiments:**
  1. **Baseline probing across frequency bins:** Run zero-shot QA on CliKT test set, grouped by co-occurrence (<10, 10-100, 100-1000, ≥1000). Expected: accuracy increases with frequency. Confirms pre-training exposure hypothesis.
  2. **ROME editing on long-tail subset:** Apply ROME to all long-tail triples (<10 co-occurrence). Measure reliability, generalization, locality. Expected: reliability >95%, generalization ~68%, locality ~84%. Identifies generalization as primary bottleneck.
  3. **One-to-one vs. one-to-many stratification:** Subdivide long-tail results by knowledge type. Expected: one-to-one shows minimal frequency sensitivity; one-to-many drives the performance gap. Validates one-to-many prevalence as root cause.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can knowledge editing methods be specifically tailored to handle one-to-many relationships prevalent in long-tail biomedical knowledge?
- Basis in paper: [explicit] The authors state that "90% of long-tail triples are one-to-many" and recommend "development of advanced editing techniques specifically tailored to long-tail knowledge" that "prioritise strategies for effectively handling the intricacies of one-to-many knowledge scenarios."
- Why unresolved: Current methods (ROME, MEMIT, MEND, IKE, FT) still show performance gaps between one-to-one and one-to-many knowledge after editing.
- What evidence would resolve it: Development of modified editing algorithms that achieve comparable generalization performance between one-to-one and one-to-many knowledge triples.

### Open Question 2
- Question: Would sentence-level co-occurrence analysis improve long-tail knowledge extraction and editing compared to document-level analysis?
- Basis in paper: [explicit] The authors acknowledge their approach "lacks refinement at the sentence level" and suggest "future work could enhance the long-tail knowledge extraction pipeline by investigating co-occurrence on the sentence-level."
- Why unresolved: Document-level co-occurrence may miss finer patterns where sentence-level context provides essential nuances.
- What evidence would resolve it: Comparative experiments using sentence-level versus document-level co-occurrence thresholds for knowledge extraction, measuring resulting editing performance.

### Open Question 3
- Question: Do the findings about long-tail biomedical knowledge editing generalize to other specialized domains?
- Basis in paper: [explicit] The authors state their "future research should focus on extracting long-tail knowledge from a broader range of domains to further validate the generalisability of our findings."
- Why unresolved: This study focused exclusively on biomedical knowledge from PubMed and SNOMED CT.
- What evidence would resolve it: Replication of the experimental framework across diverse domains (e.g., legal, financial, technical documentation) with similar long-tail distributions.

## Limitations
- One-to-many vs. one-to-one classification methodology is not fully detailed, raising questions about classification accuracy
- Document-level co-occurrence frequency may overcount spurious associations but impact is not quantified
- The specific hyperparameters for editing methods (layers targeted, training parameters) are not specified

## Confidence

- **High confidence**: The observation that pre-training accuracy declines with decreasing co-occurrence frequency is well-supported by Figure 3 across four different LLMs.
- **Medium confidence**: The claim that one-to-many triples disproportionately limit editing effectiveness is supported by relative performance gaps but relies on the one-to-many classification methodology.
- **Medium confidence**: The locate-then-edit mechanism showing high reliability but lower generalization is directly supported by Table 2, though the causal explanation of distributed representations is inferred rather than experimentally validated.

## Next Checks

1. **Replicate the one-to-many vs. one-to-one classification pipeline** on a subset of triples to verify the 90% prevalence claim and ensure consistent classification criteria.
2. **Test an editing method designed for multi-object knowledge** (such as Same-Subject Editing) on the long-tail dataset to directly assess whether explicit one-to-many handling reduces the observed performance gaps.
3. **Conduct ablation studies varying co-occurrence thresholds** (α = 5, 10, 15) to determine if the performance-frequency relationship is robust to threshold selection or driven by specific boundary effects.