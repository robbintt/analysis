---
ver: rpa2
title: Probing for Arithmetic Errors in Language Models
arxiv_id: '2507.12379'
source_url: https://arxiv.org/abs/2507.12379
tags:
- probes
- accuracy
- circular
- arithmetic
- error
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether arithmetic errors in language models
  can be detected from internal activations. It proposes lightweight probes that decode
  numerical information from hidden states during arithmetic reasoning.
---

# Probing for Arithmetic Errors in Language Models

## Quick Facts
- **arXiv ID**: 2507.12379
- **Source URL**: https://arxiv.org/abs/2507.12379
- **Reference count**: 40
- **Primary result**: Lightweight probes decode numerical information from LLM hidden states during arithmetic reasoning, achieving >90% accuracy in detecting both model predictions and ground-truth answers

## Executive Summary
This paper investigates whether arithmetic errors in language models can be detected from internal activations. It proposes lightweight probes that decode numerical information from hidden states during arithmetic reasoning. Experiments on 3-digit addition show that circular, logistic, and MLP probes can recover both the model's predicted output and the ground-truth answer with over 90% accuracy, particularly in deeper layers. These probes also detect model correctness with over 90% accuracy. Extending to chain-of-thought reasoning on GSM8K, the same probes generalize well, maintaining 80-90% accuracy. Finally, the probes guide selective re-prompting of erroneous steps, correcting up to 11.8% of flagged errors without affecting correct outputs. The findings suggest that internal representations of arithmetic are robust and reusable across contexts, offering a viable path for lightweight model self-correction.

## Method Summary
The method involves training lightweight probes on residual stream activations extracted at the equals sign token during 3-digit addition. Four probe types are tested: circular (2 weight vectors), logistic (10 weight vectors), MLP (512 hidden units), and linear (which fails). Probes are trained for 10,000 epochs using Adam/AdamW optimization on a dataset of 800 synthetic addition pairs, balanced by correctness and output digit. The probes predict the hundreds digit of either the model's output or ground truth. Error detection is performed by comparing the two probe outputs, and selective re-prompting is applied to flagged errors using softer or stronger messages.

## Key Results
- Circular, logistic, and MLP probes achieve >90% accuracy in recovering both model predictions and ground-truth answers from deep layers
- Error detection accuracy exceeds 90% using the same probes
- Probes trained on pure arithmetic generalize to GSM8K chain-of-thought with 80-90% accuracy
- Selective re-prompting corrects up to 11.8% of flagged errors without affecting correct outputs

## Why This Works (Mechanism)
The method works because arithmetic reasoning in LLMs creates structured, geometric representations in the residual stream that can be decoded by appropriate probes. The circular probe succeeds because digits naturally cluster in circular patterns in PCA space, reflecting the modular arithmetic properties. Deep layers (20-25) contain the most informative representations, particularly at the equals sign token position. The probe geometry matches the underlying representation structure, enabling high-accuracy decoding. Error detection works by comparing two separate probe outputs (predicted vs. ground truth), leveraging the fact that correct answers produce similar representations while errors diverge.

## Foundational Learning

- **Concept: Residual Stream Probing**
  - Why needed here: All probe implementations operate on residual stream activations (sum of all component outputs) at specific token positions. Understanding this architecture is essential for knowing where to extract signals.
  - Quick check question: At which token position does the paper extract activations for probing, and why?

- **Concept: Circular Representation Geometry**
  - Why needed here: The circular probe's effectiveness hinges on the observation that digits cluster in circular patterns in PCA space. Without this insight, you might default to linear probing which the paper shows fails.
  - Quick check question: Why does the circular probe outperform the linear probe despite having fewer parameters?

- **Concept: Error Detection as Comparison**
  - Why needed here: The practical application combines two probe signals—predicted output vs. ground truth—to detect errors. This comparison-based approach is what enables selective re-prompting.
  - Quick check question: What are the two strategies for training error detectors, and which achieves better FP preservation?

## Architecture Onboarding

- **Component map:**
  - Input Layer: Tokenized arithmetic query → residual stream
  - Early Layers (0-15): Encode operand digits (recoverable with >95% accuracy at layer 5)
  - Middle Layers (15-20): Transition zone where representations become structured
  - Deep Layers (20-25): Geometric encoding of results (circular digit clustering emerges)
  - Probe Types: Circular (2 weight vectors), Linear (fails), Logistic (10 weight vectors), MLP (512 hidden units)
  - Error Detection: Compare model-prediction probe vs. ground-truth probe outputs

- **Critical path:** Extract activations at the equals sign (=) token → train probe on residual stream at layer 20-25 → apply to new queries → flag disagreements between predicted-output and ground-truth probes

- **Design tradeoffs:**
  - Circular probe: Simplest (2 parameters), matches observed geometry, slightly lower accuracy on CoT
  - MLP probe: Most accurate (>92%), but less interpretable and more parameters
  - Separate vs. Joint training: Separate probes allow post-hoc application; joint training may capture interaction effects
  - Re-prompting style: Softer messages ("suspicious") correct more errors; stronger messages preserve more correct outputs

- **Failure signatures:**
  - Linear probe consistently fails (<30% accuracy across all layers)—this indicates you're using the wrong geometry
  - Early-layer probing fails for ground-truth prediction—wait until layer 20+
  - High false positive rate on re-prompting (>5% degradation of correct outputs)—reduce prompt strength

- **First 3 experiments:**
  1. **Replicate representation analysis:** Run PCA on residual stream activations at equals sign for 3-digit additions, verify circular digit clustering emerges by layer 25
  2. **Probe comparison:** Train all four probe types on model prediction task, confirm circular/MLP/logistic reach >90% while linear stays below 30%
  3. **Cross-setting generalization test:** Train error detector on pure arithmetic (800 samples), evaluate on held-out GSM8K CoT data, target 80-85% accuracy

## Open Questions the Paper Calls Out
None

## Limitations
- Narrow task scope: Limited to 3-digit addition in a single model family (Gemma 2 2B IT)
- Reduced performance on chain-of-thought: 80-85% accuracy on GSM8K vs. >90% on pure arithmetic
- Limited practical impact: Only 11.8% error correction rate with selective re-prompting
- No analysis of scalability: Doesn't address how performance scales with larger models or different architectures

## Confidence
- **High Confidence**: The core finding that residual stream activations at the equals sign encode both predicted and ground-truth arithmetic outputs with >90% accuracy using non-linear probes
- **Medium Confidence**: The generalizability claim that probes trained on pure arithmetic transfer to GSM8K CoT reasoning with 80-90% accuracy
- **Low Confidence**: The practical utility of the re-prompting intervention with only 11.8% error correction rate

## Next Checks
1. **Probe Geometry Verification**: Replicate the PCA analysis on residual stream activations at the equals sign token to confirm circular digit clustering emerges specifically in layers 20-25. Test whether this circular pattern holds when varying operand ranges (e.g., 2-digit vs 3-digit) or model scales.

2. **Cross-Model Transfer Test**: Train probes on Gemma 2 2B IT and evaluate directly on a different architecture (e.g., LLaMA 2 or Mistral). Measure accuracy drop to quantify probe architecture dependence and determine whether circular representations are universal or model-specific.

3. **Error Detection Robustness**: Generate adversarial arithmetic queries where the model makes systematic errors (e.g., consistent carry-over mistakes). Test whether probes trained on clean data maintain >85% detection accuracy or require fine-tuning on error patterns, revealing sensitivity to error type distribution.