---
ver: rpa2
title: 'FundaQ-8: A Clinically-Inspired Scoring Framework for Automated Fundus Image
  Quality Assessment'
arxiv_id: '2506.20303'
source_url: https://arxiv.org/abs/2506.20303
tags:
- quality
- fundus
- image
- images
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces FundaQ-8, an expert-validated framework for
  automated fundus image quality assessment (FIQA) that systematically evaluates eight
  critical parameters including field coverage, anatomical visibility, illumination,
  and artifacts. Using FundaQ-8 as a structured scoring reference, a ResNet18-based
  regression model was developed to predict continuous quality scores (0-1) and trained
  on 1,800 fundus images from clinical and Kaggle datasets.
---

# FundaQ-8: A Clinically-Inspired Scoring Framework for Automated Fundus Image Quality Assessment

## Quick Facts
- **arXiv ID:** 2506.20303
- **Source URL:** https://arxiv.org/abs/2506.20303
- **Reference count:** 18
- **Primary result:** ResNet18-based regression model achieves MAE 0.0992 and R² 0.7734 on 1,800 fundus images using expert-validated 8-parameter scoring framework

## Executive Summary
This study introduces FundaQ-8, an expert-validated framework for automated fundus image quality assessment (FIQA) that systematically evaluates eight critical parameters including field coverage, anatomical visibility, illumination, and artifacts. Using FundaQ-8 as a structured scoring reference, a ResNet18-based regression model was developed to predict continuous quality scores (0-1) and trained on 1,800 fundus images from clinical and Kaggle datasets. The model achieved strong performance with a Mean Absolute Error of 0.0992 and R² score of 0.7734, demonstrating reliable alignment with expert assessments. Validation against the EyeQ dataset and integration into diabetic retinopathy grading workflows showed that FundaQ-8 enhances diagnostic robustness by filtering low-quality images, thereby improving detection accuracy and reducing unnecessary re-screening. FundaQ-8 offers a scalable, clinically relevant solution to improve retinal imaging quality assessment and optimize screening efficiency.

## Method Summary
The method involves developing an automated fundus image quality assessment system using the FundaQ-8 framework, which scores eight clinical parameters (resolution, field of view, color fidelity, artifacts, vessels, macula, optic disc, optic cup) on a 0-2 scale, normalized to a continuous 0-1 quality score. A ResNet18 model with ImageNet pre-trained weights was modified with a single-neuron regression head to predict these scores. The model was trained on 1,800 images (70/15/15 train/val/test split) using MSE loss and Adam optimizer (lr=1e-4), achieving strong correlation with expert assessments. The framework was validated against EyeQ dataset and integrated into diabetic retinopathy grading workflows to demonstrate improved diagnostic performance through quality-based image filtering.

## Key Results
- ResNet18 regression model achieved MAE of 0.0992 and R² of 0.7734 on held-out test set
- Strong Spearman correlation of -0.752 with EyeQ dataset categories, confirming generalizability
- Diabetic retinopathy detection accuracy improved from 0.5464 to 0.7357 when filtering out low-quality ("Bad") images
- Framework successfully identifies and filters images requiring re-screening, reducing unnecessary diagnoses

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A fine-grained, multi-attribute scoring framework reduces the subjectivity of quality assessment compared to coarse categorical labels.
- Mechanism: By decomposing image quality into eight clinically-validated parameters and scoring each on a defined scale, the framework operationalizes expert intuition into a structured protocol. This structured data provides a richer, more objective training signal than binary or ternary labels.
- Core assumption: Clinical experts can reliably and consistently distinguish between the defined quality levels for each attribute.
- Evidence anchors:
  - [abstract] "FundaQ-8, an expert-validated framework... systematically assesses fundus image quality based on eight critical parameters."
  - [section] TABLE I shows the "FundaQ-8 Scoring Framework" with specific 0-2 criteria for eight attributes.
- Break condition: If inter-rater reliability among experts using the FundaQ-8 framework were low, it would indicate the attributes are still too subjective for consistent application.

### Mechanism 2
- Claim: Using a regression model on a continuous quality score provides higher precision than a classification model for quality assessment.
- Mechanism: A regression model (ResNet18 with single-neuron output) predicts a continuous 0-1 score, preserving ordinal relationships and magnitude of quality differences that are lost in discrete categorization.
- Core assumption: The quality of a fundus image exists on a continuous spectrum and the normalized sum of 8 scores is a valid proxy for this continuous quality.
- Evidence anchors:
  - [abstract] "...predict continuous quality scores (0-1)... demonstrating reliable alignment with expert assessments."
  - [section] "Unlike traditional categorical grading, FundaQ-8 assigns a continuous 0–1 quality score... enabling precise and objective assessment."
- Break condition: If the error distribution showed systematic bias at certain quality ranges or if only a simple "usable/unusable" decision was needed, classification might be more efficient.

### Mechanism 3
- Claim: Filtering low-quality images downstream improves the performance and reliability of automated diabetic retinopathy grading.
- Mechanism: The FIQA model acts as a gatekeeper, predicting quality scores and setting thresholds (e.g., <0.4) to flag low-quality images for re-take rather than diagnosis, reducing noise in DR model training/inference.
- Core assumption: DR grading model errors are strongly correlated with image quality, and FundaQ-8 scores accurately capture degradation factors causing DR model failures.
- Evidence anchors:
  - [abstract] "Incorporating FundaQ-8... enhances diagnostic robustness by filtering low-quality images..."
  - [section] Table V shows DR detection accuracy improves from 0.5464 on "Bad" quality images to 0.7357 on "Good" quality images.
- Break condition: If the FIQA model rejects many images the DR model could diagnose correctly, or passes "high quality" images still confusing for the DR model, clinical utility would decrease.

## Foundational Learning

- **Transfer Learning for Medical Imaging**
  - Why needed here: The dataset of 1,800 images is too small to train a deep CNN from scratch without severe overfitting. Pre-trained weights from ImageNet allow the model to leverage general visual features learned from millions of natural images.
  - Quick check question: Why is the final fully connected layer of the pre-trained ResNet18 replaced in this study?

- **Regression vs. Classification**
  - Why needed here: The problem is framed as predicting a continuous quality score (0-1), not a discrete class. Understanding the difference in output layer and loss function is critical.
  - Quick check question: What loss function is optimized for this task, and why is a single output neuron sufficient?

- **Spearman vs. Pearson Correlation**
  - Why needed here: When validating against the EyeQ dataset, the paper uses Spearman correlation because EyeQ categories are ordinal, not continuous or necessarily linearly related to the model's predicted score.
  - Quick check question: Why is Spearman correlation used to compare the predicted continuous scores with the EyeQ dataset's categorical labels?

## Architecture Onboarding

- **Component map:** 512x512 RGB fundus image -> ResNet18 feature extractor -> Single-neuron regression head -> Quality score [0,1]
- **Critical path:**
  1. Apply black region cropping, 1:1 aspect ratio correction, resize to 512x512 pixels
  2. Load torchvision.models.resnet18(pretrained=True)
  3. Replace model.fc with nn.Linear(in_features, 1) for regression
  4. Forward pass -> Compute MSE Loss -> Backpropagation with Adam optimizer
- **Design tradeoffs:**
  - ResNet18 vs. Deeper Models: Chosen for computational efficiency at potential cost of some feature extraction power
  - Custom Scoring vs. Public Labels: Tailored high-quality signal but limits dataset size
  - Regression vs. Classification: Chosen for granular control but requires setting thresholds for downstream use
- **Failure signatures:**
  - Systematic under/over-prediction: Indicates bias in training data or model calibration
  - Poor performance on out-of-distribution images: Model fails on images from different cameras or with rare artifacts
  - Collapsed output: Model predicts same score for all images, indicating failure to learn meaningful features
- **First 3 experiments:**
  1. Reproduce baseline: Train ResNet18 on 1,800-image dataset with specified hyperparameters and verify R² (~0.77) and MAE (~0.099) on held-out test set
  2. Threshold sensitivity analysis: Experiment with different thresholds for "Good/Medium/Bad" categories and measure impact on downstream DR detection accuracy
  3. Cross-dataset validation: Evaluate trained model directly on EyeQ dataset subset without retraining and compute Spearman correlation

## Open Questions the Paper Calls Out

- **Domain Adaptation for Hardware Variation:** The framework may overfit to specific imaging characteristics from Topcon NW8F, reducing adaptability across different clinical settings and camera systems.
- **Rare Pathologies and Extreme Degradations:** The dataset lacks rare pathologies and extreme degradations from factors like cataracts and device artifacts, limiting model robustness to these edge cases.
- **Optimal Quality Threshold Determination:** The study uses arbitrary thresholds (<0.4 for "Bad", >0.8 for "Good") without systematic analysis to determine clinically optimal cutoffs for maximizing screening efficiency.

## Limitations

- **Small dataset size:** Performance metrics derived from only 1,800 images with proprietary expert annotations limits independent validation
- **Lack of inter-rater reliability data:** No reported Cohen's kappa or ICC for the eight FundaQ-8 parameters among expert graders
- **Limited generalizability:** Model's robustness to diverse real-world conditions (different cameras, ethnic variations, rare pathologies) not thoroughly evaluated

## Confidence

- **High Confidence:** ResNet18 regression model architecture and training procedure are clearly specified and reproducible, with strong performance metrics on held-out test set
- **Medium Confidence:** Improvement in diabetic retinopathy detection accuracy when filtering images is convincing but based on single downstream model and specific threshold
- **Low Confidence:** Generalizability to other retinal imaging modalities and performance on underrepresented populations are speculative claims not supported by presented experiments

## Next Checks

1. Calculate and report Cohen's kappa or ICC for the eight FundaQ-8 parameters across multiple expert graders to quantify framework consistency
2. Conduct systematic analysis to determine optimal quality score threshold for filtering images, measuring impact on downstream DR grading accuracy, sensitivity, and specificity
3. Evaluate trained FundaQ-8 model on completely independent publicly available fundus image dataset to assess robustness to domain shift and data distribution differences