---
ver: rpa2
title: 'ReBoot: Encrypted Training of Deep Neural Networks with CKKS Bootstrapping'
arxiv_id: '2506.19693'
source_url: https://arxiv.org/abs/2506.19693
tags:
- encrypted
- reboot
- training
- ckks
- bootstrapping
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ReBoot enables fully encrypted, non-interactive training of deep
  neural networks using CKKS bootstrapping. It introduces an HE-compliant MLP architecture
  with local error signals and a packing strategy that exploits CKKS SIMD capabilities,
  minimizing multiplicative depth and noise accumulation.
---

# ReBoot: Encrypted Training of Deep Neural Networks with CKKS Bootstrapping

## Quick Facts
- arXiv ID: 2506.19693
- Source URL: https://arxiv.org/abs/2506.19693
- Reference count: 40
- First solution enabling end-to-end encrypted training of real-valued DNNs under CKKS

## Executive Summary
ReBoot introduces a novel framework for fully encrypted training of deep neural networks using CKKS bootstrapping. The key innovation is a local-loss block architecture that confines gradient propagation within individual network segments, dramatically reducing multiplicative depth and noise accumulation. Combined with an alternating row/column encrypted layer strategy and polynomial activation approximation, ReBoot achieves accuracy matching plaintext training while reducing latency by up to 8.83× compared to existing encrypted methods.

## Method Summary
ReBoot implements encrypted training through a novel MLP architecture with local-loss blocks that segment the network into independent training units. Each block contains fully connected layers with alternating row-encrypted and column-encrypted weight formats, eliminating costly data repacking operations. The framework uses a second-degree polynomial approximation (EncryptedPolyReLU) for activation functions, requiring only one multiplication per operation. Training proceeds through independent forward and backward passes within each block, followed by encrypted weight updates using Nesterov-accelerated gradient methods. Bootstrapping is applied after each iteration to refresh ciphertext noise.

## Key Results
- Achieves accuracy matching FP32 plaintext training across multiple image and tabular benchmarks
- Improves test accuracy by up to 6.83% over existing encrypted DNN methods
- Reduces training latency by up to 8.83× compared to baseline encrypted training approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Local-loss blocks confine gradient propagation, reducing multiplicative depth compared to standard backpropagation.
- Mechanism: Each layer is paired with a local classifier; gradients propagate only within individual blocks rather than through the entire network. This limits noise accumulation per training iteration, reducing bootstrapping frequency.
- Core assumption: Local error signals provide sufficient learning signal without end-to-end gradient flow.
- Evidence anchors:
  - [section 4.2.3] "ReBoot encrypted architecture segments the MLP into multiple encrypted local-loss blocks... confine gradient propagation within each encrypted local-loss block, thereby eliminating the need to backpropagate gradients through the entire NN."
  - [section 4.4] Table 1 shows ReBoot backward pass depth is constant (~4-5) vs BP which scales linearly with H
  - [corpus] No direct corpus comparison on local-loss for HE; mechanism is novel to this work
- Break condition: If tasks require long-range feature dependencies that local losses cannot capture, accuracy may degrade significantly vs end-to-end BP.

### Mechanism 2
- Claim: Alternating Row-Encrypted (RE-FC) and Column-Encrypted (CE-FC) layers eliminates costly repacking operations between layers.
- Mechanism: RE-FC outputs repeated-format activations; CE-FC expects repeated-format inputs. CE-FC outputs expanded-format; next RE-FC expects expanded-format. This alignment avoids homomorphic rotations and multiplications for data reordering.
- Core assumption: Network depth is even or can be structured to alternate formats throughout.
- Evidence anchors:
  - [section 4.2.1] "This alternation between output and input formats enables the composition of consecutive encrypted layers without requiring explicit repacking"
  - [section 4.1] Packing schemes defined for vectors (repeated/expanded) and matrices (row-wise/column-wise)
  - [corpus] Related work (TFHE-based) lacks SIMD; sequential operations result in O(r×c) complexity—ReBoot achieves logarithmic scaling via SIMD
- Break condition: If layer dimensions don't fit power-of-2 constraints for r and c, zero-padding overhead or multiple ciphertexts may negate efficiency gains.

### Mechanism 3
- Claim: EncryptedPolyReLU (second-degree polynomial approximation) minimizes multiplicative depth while providing sufficient non-linearity.
- Mechanism: Uses f(z) = z² + z instead of ReLU, requiring only 1 multiplication. Derivative for backward pass is 1 + 2z, also single multiplication with plaintext-ciphertext operation.
- Core assumption: Polynomial approximation of ReLU preserves learning dynamics for MLPs.
- Evidence anchors:
  - [section 4.2.2] "EncryptedPolyReLU has a multiplicative depth of 1, contributed by the element-wise multiplication"
  - [section 5.3] ReBoot achieves accuracy comparable to FP32 BP with ReLU across datasets (Table 4)
  - [corpus] PrivSpike and FastFHE use polynomial approximations for activations in encrypted inference; accuracy preserved
- Break condition: For architectures requiring precise ReLU behavior (e.g., sparsity-inducing), approximation errors may accumulate over deep networks.

## Foundational Learning

- Concept: **CKKS Scheme and Multiplicative Depth**
  - Why needed here: Understanding that each multiplication consumes a "level" in the modulus chain; when exhausted, bootstrapping is required. ReBoot's design explicitly minimizes depth per iteration.
  - Quick check question: Given a RE-Block with forward depth 2 and backward depth 4, how many bootstraps are needed if scheme level is 8 and bootstrapping itself has depth 16?

- Concept: **SIMD Batching and Summation Primitives**
  - Why needed here: ReBoot's RE-Matmul and CE-Matmul rely on homomorphic summation across rows/columns using rotations. Without understanding these primitives, the packing strategy will be opaque.
  - Quick check question: How many rotation operations are needed to sum across rows for an r×c matrix?

- Concept: **Local Loss Functions**
  - Why needed here: ReBoot uses RSS loss per block rather than global cross-entropy. Understanding why this enables independent block training is essential.
  - Quick check question: Why does RSS gradient not increase multiplicative depth compared to cross-entropy with softmax?

## Architecture Onboarding

- Component map:
  - Packing Layer -> RE-FC/CE-FC Layers -> EncryptedPolyReLU -> Local Classifier -> Weight Update -> Bootstrapping
- Critical path: Forward pass → Loss gradient → Backward pass → Weight update → Bootstrapping. Each block executes independently; total depth per block is ~6-8 depending on RE/CE type.
- Design tradeoffs:
  - Wider layers: More SIMD parallelism, but requires larger N (polynomial degree), which increases latency super-linearly
  - Deeper networks: Moderate latency increase (adds sequential operations), but local losses may reduce accuracy for complex tasks
  - Bootstrapping frequency: Fewer iterations between bootstraps means fresher ciphertexts but higher overhead (bootstrapping accounts for up to 86% of iteration time per related work)
- Failure signatures:
  - Precision collapse: If scaling factor Δ is too small or bootstrapping depth is miscalculated, weights lose precision logarithmically—check weight bit-precision curves
  - Format mismatch: If RE-Block feeds another RE-Block without repacking, activations will be misaligned—verify alternating block types
  - Memory exhaustion: Peak memory scales with batch size and N; eMLP-3 reaches 213 GB—monitor ciphertext counts
- First 3 experiments:
  1. **Precision validation**: Train eMLP-1 on MNIST in both plaintext and encrypted modes; plot training accuracy overlap (Figure 2a) to verify CKKS approximation errors are bounded.
  2. **Depth scaling test**: Compare ReBoot vs BP multiplicative depth by varying H (1-5 hidden layers); confirm Δτ gap widens as H increases per Table 1 formula.
  3. **Latency profiling**: Measure single-iteration time for eMLP-1 with N∈{2^15, 2^16, 2^17}; confirm latency more than doubles when N doubles, but stays constant when layer width increases within slot limits (Figure 3).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can ReBoot be adapted to support the encrypted training of Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs)?
- Basis in paper: [explicit] The conclusion states future work will focus on extending ReBoot to these complex architectures.
- Why unresolved: The current framework, including the packing strategy and local-loss blocks, is specifically optimized for Multi-Layer Perceptrons (MLPs).
- What evidence would resolve it: A functional implementation training encrypted CNNs or RNNs on standard benchmarks (e.g., CIFAR-10) with competitive accuracy.

### Open Question 2
- Question: Can distributed training effectively parallelize the independent local-loss blocks to reduce training latency?
- Basis in paper: [explicit] The authors identify the exploration of distributed training across separate machines as a necessary step for scalability.
- Why unresolved: While blocks are independent, the communication overhead and synchronization feasibility of distributing encrypted states in a cloud setting remain unquantified.
- What evidence would resolve it: Empirical latency measurements showing linear or near-linear speedup as local blocks are distributed across multiple nodes.

### Open Question 3
- Question: Can the framework handle high-dimensional input data without aggressive downscaling?
- Basis in paper: [inferred] Table 2 shows inputs were downscaled (e.g., MNIST 14x14), and memory usage for deeper networks (eMLP-3) reached 213 GB.
- Why unresolved: The efficiency relies on fitting data into ciphertext slots ($r \times c = N/2$); high dimensions may exhaust memory or require computationally prohibitive polynomial degrees.
- What evidence would resolve it: Successful training benchmarks on datasets with native high-resolution inputs (e.g., >784 pixels) without resizing.

## Limitations

- Scaling to larger models: Framework validated only on MLPs with up to 3 hidden layers; local-loss design may struggle with deeper architectures requiring long-range dependencies
- Memory constraints: Peak memory usage grows rapidly with batch size and N (up to 213 GB for eMLP-3), limiting deployment to GPU-accelerated environments
- Security parameter tradeoffs: Reported latency and memory figures assume standard CKKS parameters; reducing N to improve speed would weaken security guarantees

## Confidence

- **High confidence**: Local-loss blocks reduce multiplicative depth and bootstrapping frequency, well-supported by theoretical analysis and empirical depth measurements; accuracy preservation validated across multiple datasets
- **Medium confidence**: Latency improvement claims (up to 8.83×) based on controlled experiments with fixed architectures; real-world performance may vary with different data distributions and hardware
- **Medium confidence**: Claim of being "first solution" for end-to-end encrypted training of real-valued DNNs is plausible given literature review but may be temporary in rapidly evolving FHE landscape

## Next Checks

1. **Deeper network evaluation**: Test ReBoot on 5-10 hidden layer MLPs to quantify accuracy degradation from local losses and validate whether multiplicative depth savings persist at scale
2. **Memory optimization analysis**: Profile memory usage across different N values and batch sizes to identify bottlenecks and evaluate whether memory efficiency scales linearly with model complexity
3. **Security parameter sweep**: Systematically vary CKKS parameters (N, scaling factor) to quantify the security-latency tradeoff curve and establish minimum parameter requirements for practical deployment