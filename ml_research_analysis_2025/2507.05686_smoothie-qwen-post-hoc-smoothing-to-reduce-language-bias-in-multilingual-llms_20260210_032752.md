---
ver: rpa2
title: 'Smoothie-Qwen: Post-Hoc Smoothing to Reduce Language Bias in Multilingual
  LLMs'
arxiv_id: '2507.05686'
source_url: https://arxiv.org/abs/2507.05686
tags:
- language
- tokens
- chinese
- smoothie-qwen
- suppression
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses language confusion in multilingual large\
  \ language models (LLMs), where models generate responses in a dominant language\
  \ despite prompts in other languages. The authors propose Smoothie-Qwen, a lightweight\
  \ post-hoc method that mitigates this bias by directly modifying the model\u2019\
  s output layer weights."
---

# Smoothie-Qwen: Post-Hoc Smoothing to Reduce Language Bias in Multilingual LLMs

## Quick Facts
- **arXiv ID**: 2507.05686
- **Source URL**: https://arxiv.org/abs/2507.05686
- **Reference count**: 2
- **Primary result**: Reduces unintended Chinese generation by over 95% while preserving task accuracy

## Executive Summary
This paper addresses language confusion in multilingual large language models, where models generate responses in a dominant language despite prompts in other languages. The authors propose Smoothie-Qwen, a lightweight post-hoc method that mitigates this bias by directly modifying the model's output layer weights. The method identifies target tokens (e.g., Chinese characters) via Unicode and subword analysis, assigns probabilistic risk scores, and applies a non-linear smoothing function to downscale their generation probabilities. Applied to Qwen2.5-Coder-14B-Instruct, Smoothie-Qwen reduced unintended Chinese output by over 95% while preserving task accuracy on multilingual benchmarks like KMMLU. Qualitative results showed the model intelligently substituted risky outputs (e.g., pinyin instead of Chinese characters) rather than rigidly blocking them. The method is training-free, efficient, and adaptable, offering a practical solution for enhancing the reliability of multilingual LLMs.

## Method Summary
Smoothie-Qwen is a post-hoc smoothing technique that modifies the output layer weights of multilingual LLMs to reduce language bias. The method identifies target tokens (e.g., Chinese characters) using Unicode and subword analysis, assigns probabilistic risk scores, and applies a non-linear smoothing function to downscale their generation probabilities. This approach avoids retraining and is applied directly to the model's output layer, making it efficient and adaptable. The technique was tested on Qwen2.5-Coder-14B-Instruct, demonstrating over 95% reduction in unintended Chinese generation while maintaining task performance on multilingual benchmarks like KMMLU.

## Key Results
- Reduced unintended Chinese generation by over 95% in multilingual LLM outputs
- Preserved task accuracy on multilingual benchmarks like KMMLU
- Demonstrated intelligent substitutions (e.g., pinyin instead of Chinese characters) rather than rigid blocking

## Why This Works (Mechanism)
Smoothie-Qwen works by directly modifying the output layer weights of multilingual LLMs to reduce language bias. The method identifies target tokens (e.g., Chinese characters) using Unicode and subword analysis, assigns probabilistic risk scores, and applies a non-linear smoothing function to downscale their generation probabilities. This approach avoids retraining and is applied directly to the model's output layer, making it efficient and adaptable. The technique was tested on Qwen2.5-Coder-14B-Instruct, demonstrating over 95% reduction in unintended Chinese generation while maintaining task performance on multilingual benchmarks like KMMLU.

## Foundational Learning

**Unicode Token Identification**: Identifying target tokens via Unicode ranges is crucial for distinguishing language-specific characters. Quick check: Verify Unicode ranges for target languages match expected character sets.

**Subword Analysis**: Understanding subword tokenization helps in accurately identifying and handling language-specific tokens. Quick check: Ensure subword tokenization aligns with Unicode-identified tokens.

**Probabilistic Risk Scoring**: Assigning risk scores to tokens allows for nuanced handling of language bias. Quick check: Validate risk scores reflect actual language confusion patterns.

## Architecture Onboarding

**Component Map**: Input Layer -> Unicode/Subword Analysis -> Risk Score Assignment -> Non-linear Smoothing -> Output Layer Modification

**Critical Path**: The critical path involves identifying target tokens, assigning risk scores, and applying smoothing to modify output probabilities, ensuring efficient bias reduction without retraining.

**Design Tradeoffs**: The method prioritizes efficiency and adaptability by avoiding retraining, but may face challenges in generalizing across diverse linguistic contexts and script systems.

**Failure Signatures**: Over-generalization of token risk scoring and lack of contextual disambiguation are potential failure points, as the method relies on static Unicode and subword-based identification.

**First Experiments**:
1. Evaluate performance across multiple language pairs with varying script systems.
2. Conduct user studies on semantic coherence and task completion.
3. Test robustness against adversarial prompts triggering language switching.

## Open Questions the Paper Calls Out
The study leaves open questions about the method's generalizability across different language pairs and script systems, as the evaluation primarily focuses on Chinese language interference. Additionally, the long-term impact on semantic coherence and user experience in multilingual settings remains underexplored.

## Limitations
- Potential for over-generalization of token risk scoring across diverse linguistic contexts
- Limited evaluation scope focusing primarily on Chinese language interference
- Lack of systematic evaluation of semantic coherence and user experience impacts

## Confidence

**High confidence**: The method's effectiveness in reducing unintended Chinese generation and maintaining task accuracy on KMMLU benchmarks.

**Medium confidence**: The generalizability of the approach across different language pairs and script systems, given limited evaluation scope.

**Medium confidence**: The qualitative assessment of intelligent substitutions, as these observations are based on limited examples without systematic evaluation.

## Next Checks
1. Evaluate Smoothie-Qwen's performance across multiple language pairs with varying script systems (e.g., Japanese, Arabic, Cyrillic) to assess cross-linguistic generalizability.
2. Conduct user studies measuring the impact of probabilistic substitutions on semantic coherence and task completion in real-world multilingual applications.
3. Test the method's robustness against adversarial prompts designed to trigger language switching, to evaluate security and reliability in production environments.