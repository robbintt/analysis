---
ver: rpa2
title: 'SortBench: Benchmarking LLMs based on their ability to sort lists'
arxiv_id: '2504.08312'
source_url: https://arxiv.org/abs/2504.08312
tags:
- list
- sorting
- score
- size
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SortBench, a new benchmark for evaluating
  large language models' (LLMs) ability to sort lists. The benchmark includes tasks
  of varying difficulty and length to assess models' sorting accuracy, faithfulness
  to input data, and ability to produce valid Python list outputs.
---

# SortBench: Benchmarking LLMs based on their ability to sort lists

## Quick Facts
- arXiv ID: 2504.08312
- Source URL: https://arxiv.org/abs/2504.08312
- Reference count: 40
- Primary result: SortBench benchmark reveals LLMs struggle with faithfulness to input data when sorting longer lists, with test-time reasoning models often overthinking problems

## Executive Summary
This paper introduces SortBench, a novel benchmark for evaluating large language models' (LLMs) ability to sort lists of varying difficulty and length. The benchmark assesses models' sorting accuracy, faithfulness to input data, and ability to produce valid Python list outputs. The author evaluates seven state-of-the-art LLMs, including models with test-time reasoning capabilities. The primary finding is that while OpenAI's o3-mini performs best overall, all models struggle with faithfulness to input data for longer lists, often dropping or adding items. Test-time reasoning models like o3-mini and DeepSeek-r1 tend to overthink problems, leading to performance degradation. Even without reasoning capabilities, models like GPT-4o perform comparably well. The results highlight that sorting, a seemingly simple task, remains challenging for LLMs, particularly with longer sequences and complex data types.

## Method Summary
The SortBench benchmark evaluates LLMs on their ability to sort lists of varying lengths and complexity. The benchmark includes tasks with different data types (numbers, strings, tuples) and varying levels of difficulty. Models are assessed on sorting accuracy, faithfulness to input data (ensuring no items are dropped or added), and valid Python list output generation. Seven state-of-the-art LLMs are evaluated, including models with test-time reasoning capabilities like o3-mini and DeepSeek-r1. The evaluation measures both accuracy and faithfulness, with particular attention to how models handle longer lists where performance degradation is observed.

## Key Results
- OpenAI's o3-mini achieves the best overall performance on the SortBench benchmark
- All tested models struggle with faithfulness to input data, particularly for longer lists where items are frequently dropped or added
- Test-time reasoning models like o3-mini and DeepSeek-r1 tend to overthink problems, leading to performance degradation despite their reasoning capabilities
- GPT-4o performs comparably well to reasoning models without having reasoning capabilities

## Why This Works (Mechanism)
The benchmark works by systematically testing LLMs on sorting tasks of increasing complexity, revealing fundamental limitations in how these models handle sequence manipulation and data integrity. The evaluation exposes that while models can perform basic sorting operations, they struggle with maintaining faithfulness to the original input, particularly as list length increases. This suggests that LLMs may be using pattern-based approaches to sorting rather than truly understanding the sorting process and maintaining data integrity throughout.

## Foundational Learning
- **Sorting algorithms**: Understanding basic sorting mechanisms (why needed: to evaluate model outputs against correct sorting; quick check: can identify correct sorted order)
- **Python list syntax**: Familiarity with Python list structures and output formats (why needed: benchmark requires valid Python list outputs; quick check: can validate output format)
- **Sequence faithfulness**: Concept of maintaining all input elements in output (why needed: benchmark specifically measures this; quick check: can count items in input vs output)
- **Test-time reasoning**: Understanding how models with reasoning capabilities process tasks differently (why needed: benchmark compares reasoning vs non-reasoning models; quick check: can identify reasoning patterns in outputs)
- **Data type handling**: How models process different data types (numbers, strings, tuples) (why needed: benchmark includes mixed data types; quick check: can verify correct type preservation)
- **Benchmark evaluation metrics**: Understanding accuracy and faithfulness metrics (why needed: to interpret benchmark results; quick check: can calculate accuracy rates)

## Architecture Onboarding

**Component map**: Input list -> LLM processing -> Sorting operation -> Output list -> Faithfulness check -> Accuracy validation

**Critical path**: Input generation -> Model inference -> Output parsing -> Faithfulness verification -> Accuracy calculation

**Design tradeoffs**: The benchmark prioritizes faithfulness over pure accuracy, revealing that models may sacrifice data integrity for sorting correctness. This tradeoff highlights the tension between optimization for different evaluation metrics.

**Failure signatures**: Common failure modes include systematic dropping of certain item types, addition of non-existent items, and incorrect sorting of complex data types. Models with reasoning capabilities show overthinking patterns where they introduce errors through excessive reasoning steps.

**3 first experiments**:
1. Test models on progressively longer lists to identify the threshold where faithfulness degradation begins
2. Compare model performance on simple vs. complex data types to identify type-specific weaknesses
3. Evaluate the impact of different output format requirements (JSON, CSV) on model performance

## Open Questions the Paper Calls Out
None

## Limitations
- The benchmark's focus on Python list outputs may not reflect real-world sorting scenarios with different output formats
- The relatively small sample of seven tested models limits generalizability across the broader LLM landscape
- The specific focus on list-sorting tasks may not capture broader sequence manipulation capabilities of LLMs
- The exact causes of performance degradation in reasoning models remain unclear

## Confidence
- Benchmark methodology: High
- Model comparison validity: Medium
- Generalizability to other tasks: Low
- Explanation of reasoning model degradation: Low

## Next Checks
1. Test the benchmark across a broader range of model architectures, including both larger and smaller models than those evaluated, to determine if the observed patterns hold across the model spectrum
2. Investigate the specific failure modes in faithfulness by analyzing model outputs to distinguish between systematic patterns (e.g., always dropping certain types of items) versus random errors
3. Evaluate model performance on sorting tasks with alternative output formats (e.g., JSON, CSV) to assess whether the Python list constraint significantly impacts results