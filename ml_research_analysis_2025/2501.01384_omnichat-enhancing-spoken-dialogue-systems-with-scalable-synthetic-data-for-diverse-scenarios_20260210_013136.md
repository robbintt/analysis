---
ver: rpa2
title: 'OmniChat: Enhancing Spoken Dialogue Systems with Scalable Synthetic Data for
  Diverse Scenarios'
arxiv_id: '2501.01384'
source_url: https://arxiv.org/abs/2501.01384
tags:
- dialogue
- spoken
- data
- audio
- emotion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces OmniChat, a multi-turn spoken dialogue system
  that leverages scalable synthetic data to handle diverse real-world conversation
  scenarios including emotional expressions, audio events, and background music. The
  authors created ShareChatX, the first large-scale synthetic spoken dialogue dataset
  spanning these complex scenarios, by using large language models to generate dialogue
  scripts and controllable text-to-speech to synthesize speech.
---

# OmniChat: Enhancing Spoken Dialogue Systems with Scalable Synthetic Data for Diverse Scenarios

## Quick Facts
- **arXiv ID**: 2501.01384
- **Source URL**: https://arxiv.org/abs/2501.01384
- **Reference count**: 7
- **Primary result**: OmniChat achieves state-of-the-art performance on DailyTalk (METEOR: 14.24, BERTScore: 86.99) using synthetic data and Mix-Former fusion.

## Executive Summary
OmniChat is a multi-turn spoken dialogue system that leverages scalable synthetic data to handle diverse real-world conversation scenarios including emotional expressions, audio events, and background music. The authors created ShareChatX, the first large-scale synthetic spoken dialogue dataset spanning these complex scenarios, by using large language models to generate dialogue scripts and controllable text-to-speech to synthesize speech. OmniChat employs a heterogeneous feature fusion module called Mix-Former that optimally integrates speech content, emotion, and audio features for different dialogue contexts. Through extensive experimentation, they found that synthetic data significantly improves performance on real-world dialogue datasets, with OmniChat achieving state-of-the-art results on DailyTalk and superior emotion prediction.

## Method Summary
OmniChat is a multi-turn spoken dialogue generation model that uses heterogeneous feature fusion to handle diverse conversation scenarios. The system processes raw audio through three frozen expert encoders (Whisper for content, emotion2vec for affect, BEATs for audio events/music), fuses their outputs using Mix-Former with learned weighting, and feeds the result to a frozen Llama-3.1-8B-Instruct backbone with LoRA adapters. The model is pre-trained on synthetic ShareChatX data and fine-tuned on real DailyTalk data using an optimal 20% synthetic-to-real sampling ratio. The architecture enables direct speech-to-dialogue modeling that surpasses traditional ASR-based approaches when trained on sufficient synthetic data (20K+ samples).

## Key Results
- Direct speech models outperform ASR-based approaches when trained on sufficient synthetic data (20K+ samples)
- Optimal performance achieved with 20% synthetic-to-real data sampling ratio
- OmniChat achieves state-of-the-art results on DailyTalk (METEOR: 14.24, BERTScore: 86.99) and superior emotion prediction (F1e: 75.46)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Direct speech-to-dialogue models can surpass ASR-based systems when trained on sufficient synthetic data (20K+ samples).
- Mechanism: At scale, the model learns a continuous speech-to-semantics mapping that captures prosodic and emotional information lost in ASR transcription. Text becomes redundant because speech contains strictly more signal than its transcription.
- Core assumption: Synthetic speech from controllable TTS provides sufficient acoustic diversity to generalize to real speech distributions.
- Evidence anchors: Abstract states direct models surpass ASR-based at 20K+; Figure 3 shows direct speech model surpassing ASR-based between 10K-20K scale with BLEU improving from 6.06 at 20K to 6.17 at 80K while ASR-based plateaus.

### Mechanism 2
- Claim: Weighted heterogeneous feature fusion (Mix-Former) improves dialogue quality by dynamically adjusting the feature contributions per context.
- Mechanism: Three frozen expert encoders produce aligned frame-level features. Window-level Q-Formers compress each modality, and learned sigmoid-weighted concatenation allows the model to suppress irrelevant features (e.g., BEATs features in emotion-centric dialogues).
- Core assumption: Optimal feature weighting varies by scenario and can be learned from data rather than hard-coded.
- Evidence anchors: Abstract describes Mix-Former optimally integrating features for different contexts; Table 5 shows Mix-Former combining all three features achieved best METEOR (15.8) and BERTScore (87.8) versus naive concatenation.

### Mechanism 3
- Claim: A 20% synthetic-to-real sampling ratio optimally balances domain coverage from synthetic data with distributional alignment to real conversations.
- Mechanism: Synthetic data expands lexical diversity and scenario coverage while real data anchors the model to natural speech patterns. The 20% ratio provides enough synthetic signal for semantic coherence without overwhelming the real distribution.
- Core assumption: Synthetic dialogues differ from real conversations in ways that help at low ratios but hurt at high ratios.
- Evidence anchors: Abstract states 20% sampling ratio yields optimal performance; Table 4 shows α=0.2 achieved best sentence-level metrics and +4.41 F1e improvement while α>0.2 caused ROUGE-L to drop by 0.91.

## Foundational Learning

- **Q-Former (Query-based Feature Alignment)**
  - Why needed here: Bridges frozen audio encoders to frozen LLM by learning compact, language-aligned representations from audio features via trainable queries.
  - Quick check question: Can you explain why Q-Former uses a fixed number of queries rather than variable-length output?

- **LoRA (Low-Rank Adaptation)**
  - Why needed here: Enables efficient fine-tuning of large LLM by updating only low-rank decomposition of attention weight deltas, keeping base model frozen.
  - Quick check question: Which weight matrices does LoRA typically modify in self-attention layers?

- **Multi-turn Dialogue Loss (Causal Language Modeling)**
  - Why needed here: Trains model to predict each token conditioned on all prior dialogue history (audio features + previous turns), ensuring temporal coherence across conversation.
  - Quick check question: How does the loss function ensure the model learns from multi-turn context rather than single-turn pairs?

## Architecture Onboarding

- **Component map**: Raw audio (16kHz) → Whisper (speech content) + emotion2vec (affect) + BEATs (audio events/music) → Mix-Former (Q-Former + learned weights) → Llama-3.1-8B-Instruct + LoRA → Response tokens + style tokens → TTS

- **Critical path**:
  1. Audio input → expert encoders → frame-aligned features
  2. Features → Mix-Former → fused representation Z
  3. Z + dialogue history → LLM → response text + style
  4. Response + style → controllable TTS → synthesized speech

- **Design tradeoffs**:
  - Frozen vs. fine-tuned encoders: Freezing reduces compute but limits domain adaptation; LoRA on LLM only is a middle ground.
  - Window size L=17: Smaller windows capture finer temporal dynamics but increase compute; 0.33s was empirically sufficient.
  - Direct speech vs. ASR-based: Direct is simpler at inference but requires 20K+ training samples to surpass ASR pipeline.

- **Failure signatures**:
  - Low emotion F1 but high content metrics: Mix-Former may be underweighting emotion features; check weight distributions.
  - Good on ShareChatX, poor on DailyTalk: Synthetic-real gap; increase real data ratio or improve synthetic diversity.
  - ASR-based model outperforming direct model: Insufficient training data (<20K); scale up synthetic data or add real data.

- **First 3 experiments**:
  1. Ablation on expert features: Train with only Whisper, only emotion2vec, only BEATs, and all combinations with/without Mix-Former to validate fusion mechanism (replicate Table 5).
  2. Scaling curve on subset: Train on 5K, 10K, 20K, 40K synthetic samples to identify where direct speech model surpasses ASR-based baseline (replicate Figure 3 pattern).
  3. Synthetic-real ratio sweep: With fixed 80K synthetic data, vary real data sampling ratio (α=0.0, 0.1, 0.2, 0.3, 1.0) on DailyTalk validation split to confirm 20% optimum (replicate Table 4).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the utility of synthetic data be effectively translated to end-to-end spoken dialogue systems that generate audio waveforms directly, rather than relying on cascaded TTS?
- Basis in paper: The Limitations section states the current system relies on a controllable TTS model and that "In the future, we will explore the application of synthetic data in developing end-to-end spoken dialogue systems."
- Why unresolved: The current OmniChat architecture decouples the understanding and generation stages, leaving the efficacy of synthetic data for direct audio-to-audio generation untested.
- What evidence would resolve it: A study training a unified audio-to-audio model on ShareChatX and comparing its latency, naturalness, and error accumulation against the current cascaded approach.

### Open Question 2
- Question: How can synthetic data pipelines be refined to proactively mitigate "accidental guidance" risks in voice dialogue systems?
- Basis in paper: The Ethical Discussion notes that while synthetic data helps consistency, the authors "plan to explore how to further reduce the risk of accidental guidance in voice dialogue systems in the future."
- Why unresolved: The paper establishes that synthetic data allows for better ethical control than public data, but specific methodologies for detecting and filtering subtle, accidental negative guidance within the generation process are not detailed.
- What evidence would resolve it: The proposal of a specific filtering or verification mechanism for synthetic dialogue scripts, followed by red-team testing showing reduced rates of accidental harmful guidance compared to the baseline synthetic approach.

### Open Question 3
- Question: Is the empirically determined 20% synthetic-to-real data sampling ratio robust for highly specialized, low-resource domains (e.g., medical emergencies) that differ significantly from general chitchat?
- Basis in paper: The authors identify the 20% ratio as optimal for the DailyTalk dataset (general scenarios), while the Introduction highlights that collecting data for specific scenarios like emergencies is uniquely challenging.
- Why unresolved: The optimal balance is derived from general conversation data; the transferability of this specific ratio to specialized domains where real data is scarce and synthetic data approximations may be less accurate remains unverified.
- What evidence would resolve it: Experimentation with the sampling ratio using a domain-specific subset of ShareChatX against a specialized real-world benchmark, observing if the 20% peak holds or shifts.

## Limitations
- The synthetic data generation pipeline (LLM prompts, TTS parameters) is underspecified, limiting reproducibility
- Claims about generalizability to other domains are extrapolation beyond tested DailyTalk and ShareChatX
- The 20% synthetic-real ratio is specifically tuned to ShareChatX quality and DailyTalk scale; optimal ratios may differ for other datasets

## Confidence

- **High confidence**: Direct speech models surpass ASR-based approaches at 20K+ synthetic samples; Mix-Former improves heterogeneous feature fusion over naive concatenation; 20% synthetic-real ratio is optimal for DailyTalk.
- **Medium confidence**: Claims about cross-scenario generalization (emotion → audio → music); scalability to other dialogue domains; synthetic data quality sufficient for real-world deployment.
- **Low confidence**: Exact reproducibility of synthetic dataset generation; hyperparameter sensitivity; performance on datasets outside DailyTalk/ShareChatX.

## Next Checks

1. **Ablation on expert features**: Train with only Whisper, only emotion2vec, only BEATs, and all combinations with/without Mix-Former to validate fusion mechanism (replicate Table 5).
2. **Scaling curve on subset**: Train on 5K, 10K, 20K, 40K synthetic samples to identify where direct speech model surpasses ASR-based baseline (replicate Figure 3 pattern).
3. **Synthetic-real ratio sweep**: With fixed 80K synthetic data, vary real data sampling ratio (α=0.0, 0.1, 0.2, 0.3, 1.0) on DailyTalk validation split to confirm 20% optimum (replicate Table 4).