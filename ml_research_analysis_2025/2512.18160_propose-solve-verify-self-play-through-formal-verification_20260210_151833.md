---
ver: rpa2
title: 'Propose, Solve, Verify: Self-Play Through Formal Verification'
arxiv_id: '2512.18160'
source_url: https://arxiv.org/abs/2512.18160
tags:
- pass
- verification
- self-play
- formal
- code
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces PROPOSE, SOLVE, VERIFY (PSV), a self-play
  framework for code generation that uses formal verification to provide reliable
  correctness signals. The method alternates between generating problem specifications,
  training on verified solutions, and adapting difficulty based on solver performance.
---

# Propose, Solve, Verify: Self-Play Through Formal Verification

## Quick Facts
- arXiv ID: 2512.18160
- Source URL: https://arxiv.org/abs/2512.18160
- Reference count: 40
- PSV-Verus achieves up to 9.6× improvement in pass@1 rates over inference-only and expert-iteration baselines across three benchmarks.

## Executive Summary
This paper introduces PROPOSE, SOLVE, VERIFY (PSV), a self-play framework for code generation that leverages formal verification to provide reliable correctness signals. The method alternates between generating problem specifications, training on verified solutions, and adapting difficulty based on solver performance. PSV-Verus, trained with this approach, significantly outperforms baseline methods across three code generation benchmarks. The work demonstrates that formal verification can effectively guide self-play training loops, enabling models to generate and solve increasingly challenging problems while maintaining correctness guarantees.

## Method Summary
PSV implements a self-play loop where a proposer generates code specifications, a solver attempts to implement them, and a formal verifier checks correctness. The system alternates between proposing new problems (with difficulty-aware prompting), solving them via sampling, verifying solutions with Verus (sound but incomplete), and fine-tuning the solver only on verified solutions using rejection fine-tuning. Difficulty is estimated via pass rates and used to condition proposer generation, creating an adaptive curriculum. The framework maintains a growing dataset of verified problems and solutions, with each iteration generating B new questions and training for multiple epochs on the accumulated verified solutions.

## Key Results
- PSV-Verus achieves 9.6× improvement in pass@1 rates over inference-only and expert-iteration baselines across three benchmarks
- Performance scales monotonically with the number of generated questions (1k to 32k) and training iterations
- Formal verification is critical: removing it causes 51.5% pass@1 drop on Dafny2Verus and 55.2% on MBPP
- PSV achieves 45.3% uniqueness vs 29.3% with fixed prompt, yielding 2.48× more unique questions

## Why This Works (Mechanism)

### Mechanism 1: Sound Verification Prevents Error Propagation
Formal verification provides a binary, mathematically guaranteed correctness signal that prevents incorrect solutions from contaminating the training loop. Unlike unit tests which can pass for incorrect programs, Verus verifier is sound—if it accepts a program, that program is guaranteed to satisfy its specification for all inputs. This creates clean separation between verified (trainable) and unverified (discarded) solutions. Core assumption: verifier's trusted computing base is correct; specifications accurately capture intended behavior.

### Mechanism 2: Difficulty-Aware Proposal Creates Adaptive Curriculum
Estimating problem difficulty via solver pass rates and conditioning the proposer on difficulty labels enables controlled curriculum expansion at the solver's frontier. Pass rates categorize problems (EASY ≥0.8, MEDIUM 0.2–0.8, HARD <0.2, IMPOSSIBLE =0). The proposer receives few-shot examples with difficulty labels and is prompted to generate problems at target levels. This keeps problems challenging but tractable. Core assumption: pass rate is meaningful proxy for difficulty; in-context learning can modulate output difficulty.

### Mechanism 3: Diversity Through Dynamic Context Sampling Expands Coverage
Continuously refreshing the proposer's in-context examples from the growing dataset increases problem diversity, yielding more unique solvable training data. At each iteration, k_prop problems are sampled from D_t rather than fixed seed examples. This prevents mode collapse and exposes proposer to evolving problem distribution. Core assumption: broader problem coverage translates to better generalization; proposer can extrapolate from diverse examples.

## Foundational Learning

- Concept: **Formal Verification Soundness vs. Completeness**
  - Why needed here: Understanding that Verus is sound (verified ⇒ correct) but incomplete (correct ⇒ may not verify) explains why some valid solutions are discarded and why proof annotations are required.
  - Quick check question: Given a sorting function that is correct but fails Verus verification without loop invariants, is this a soundness or completeness failure?

- Concept: **Rejection Fine-Tuning (RFT) as Offline RL**
  - Why needed here: PSV trains only on verified solutions (positive-only signal). Understanding RFT helps explain why advantage-based RL (e.g., GRPO) was avoided—incompleteness would incorrectly punish correct-but-unverified solutions.
  - Quick check question: Why does RFT avoid the risk of penalizing correct solutions that fail verification?

- Concept: **Difficulty Estimation via Pass Rate**
  - Why needed here: The entire curriculum depends on pass rate as a difficulty proxy. Understanding its limitations (partial control, overlapping distributions) sets realistic expectations for proposer behavior.
  - Quick check question: If a problem has pass rate 0.15 with 10 samples, what difficulty category is it assigned, and what confidence do you have in that estimate?

## Architecture Onboarding

- Component map:
  Seed Corpus (X_0) → Proposer (P_φ) → Spec Verifier → Solver (S_θ) → Verifier (Verus) → Data Pool (D_t) → Trainer (RFT) → Difficulty Estimator

- Critical path:
  1. Solver samples → Verifier checks → Verified solutions collected → RFT updates solver → Pass rates computed → Proposer generates new specs (informed by difficulty) → Specs verified for validity → New specs added to pool → Repeat
  2. Bottleneck: Verification is compute-intensive (parallelizable across problems); spec validation filters ~52% invalid specs, saving downstream inference cost.

- Design tradeoffs:
  - **Verifier choice**: Verus provides soundness but is incomplete; alternative verifiers have different tradeoffs
  - **k_trn (samples per spec)**: Higher values increase chance of finding verifiable solution but multiply inference cost; paper uses k_trn=10
  - **B (questions per iteration)**: Scales with parallel compute; paper shows monotonic improvement from 1k to 32k
  - **Iterations vs. budget tradeoff**: For fixed total questions, more iterations outperform single large batch
  - **Difficulty thresholds (τ_E, τ_M)**: Paper uses 0.8/0.2; sensitivity not fully ablated

- Failure signatures:
  - **Spec validation failure**: 52.3% of generated specs are syntactically invalid; these are filtered before solving
  - **Verification timeout**: SMT solver may not terminate; paper does not specify timeout handling
  - **Curriculum collapse**: If proposer generates only EASY problems, solver plateaus; difficulty-aware prompting mitigates but does not guarantee
  - **Transfer failure**: Limited scaling on HumanEval transfer suggests domain mismatch

- First 3 experiments:
  1. **Verify loop integrity on small seed**: Run 1 iteration with B=100 on Dafny2Verus subset. Confirm: (a) spec validation filters invalid specs, (b) verifier accepts/rejects solutions correctly, (c) RFT trains only on verified solutions. Log pass rates by difficulty category.
  2. **Ablate verification signal**: Replace Verus with unit-test-based verification on same seed. Compare pass@1 after 3 iterations. Expect significant degradation per paper's ablation (>50% relative drop).
  3. **Scale question budget**: Run PSV with B ∈ {1k, 4k, 16k} questions for 3 iterations on MBPP-Verified. Plot pass@1,5,10 vs. budget. Expect monotonic scaling as in Figure 5.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can on-policy reinforcement learning algorithms outperform rejection fine-tuning (RFT) in formally verified self-play despite verifier incompleteness?
- Basis: The authors state, "Future work on on-policy RL algorithms for formally verified self-play may investigate more performant RL algorithms in this setting."
- Why unresolved: RFT was chosen because the Verus verifier is sound but incomplete; advantage-weighted algorithms might incorrectly punish correct solutions that the verifier fails to verify.
- What evidence would resolve it: A study comparing the performance and stability of algorithms like GRPO against RFT within the PSV framework.

### Open Question 2
- Question: How effectively does the PSV framework generalize to other formal verification languages or problems with sound verifiers?
- Basis: The authors note, "While we consider formally verified code generation here, in principle our methods apply to any problem with a sound verifier... Applying PSV to other such problems is left for future work."
- Why unresolved: The experiments were restricted to the Verus language for verified Rust code.
- What evidence would resolve it: Successful application and evaluation of the PSV loop on other formal systems, such as Dafny or theorem provers like Lean.

### Open Question 3
- Question: Can more precise mechanisms for controlling question difficulty improve the efficiency of the self-play curriculum?
- Basis: In the ablation discussion, the authors note that "difficulty-aware prompting provides partial but incomplete control over generated question difficulty" and suggest this is an area "future work may find fruitful to improve upon."
- Why unresolved: The current method relies on in-context learning with pass rates, which results in substantial overlap between difficulty distributions.
- What evidence would resolve it: A modified proposer architecture that generates problems with tighter adherence to target difficulty levels, resulting in faster solver convergence.

### Open Question 4
- Question: What are the asymptotic performance limits of PSV relative to the number of generated questions and training iterations?
- Basis: The conclusion identifies the current work as a "first step in understanding the limits of scaling for self-play reasoning training."
- Why unresolved: The paper demonstrates positive scaling trends but does not establish the saturation point where additional questions or iterations yield diminishing returns.
- What evidence would resolve it: Extended training runs at significantly larger compute scales to identify performance plateaus.

## Limitations

- The paper does not quantify how many correct solutions are rejected due to verifier incompleteness, which could substantially limit the training signal
- The reliance on pass rate as a difficulty proxy provides only "partial but incomplete control" over generated difficulty levels, yet curriculum sensitivity to difficulty thresholds is not thoroughly explored
- Transfer results to HumanEval are limited and show reduced effectiveness, raising questions about domain generalization

## Confidence

- **High Confidence**: The core experimental results showing PSV-Verus outperforming baselines (up to 9.6× improvement in pass@1) are well-supported by the ablation studies and scaling experiments presented in the paper
- **Medium Confidence**: The claimed importance of difficulty-aware proposal and diversity mechanisms is supported by ablation studies, but the partial effectiveness of difficulty control suggests the curriculum adaptation may not be as robust as implied
- **Low Confidence**: The paper's transfer results to HumanEval are limited and show reduced effectiveness, raising questions about domain generalization

## Next Checks

1. **Incompleteness Impact Analysis**: Measure the fraction of correct-but-unverified solutions by randomly sampling rejected solutions and manually verifying their correctness. This would quantify the practical completeness ceiling and its impact on training efficiency.

2. **Difficulty Threshold Sensitivity**: Run ablation studies varying τ_E and τ_M (e.g., 0.7/0.3 and 0.9/0.1) to determine whether the chosen thresholds are optimal or whether the curriculum is sensitive to these hyperparameters.

3. **Solver-Proposer Coupling Analysis**: Investigate whether using separate models for proposer (P_φ) and solver (S_θ) versus a single shared model affects performance, particularly given the 52.3% spec validation failure rate which suggests the proposer may be generating problematic specifications.