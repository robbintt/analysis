---
ver: rpa2
title: 'RoseRAG: Robust Retrieval-augmented Generation with Small-scale LLMs via Margin-aware
  Preference Optimization'
arxiv_id: '2502.10993'
source_url: https://arxiv.org/abs/2502.10993
tags:
- preference
- knowledge
- prompt
- language
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RoseRAG enhances small-scale language models (SLMs) in retrieval-augmented
  generation by introducing margin-aware preference optimization to mitigate the impact
  of noisy retrieved documents. The framework uses multi-turn prompting with rejection
  sampling to generate high-quality rationales, then applies contrastive preference
  selection to maximize the margin between preferred and non-preferred responses.
---

# RoseRAG: Robust Retrieval-augmented Generation with Small-scale LLMs via Margin-aware Preference Optimization

## Quick Facts
- arXiv ID: 2502.10993
- Source URL: https://arxiv.org/abs/2502.10993
- Reference count: 40
- One-line primary result: RoseRAG significantly improves small-scale language models' performance in retrieval-augmented generation by using margin-aware preference optimization with multi-turn prompting and rejection sampling.

## Executive Summary
RoseRAG addresses the challenge of noisy retrieval in RAG systems when using small-scale language models (SLMs). The framework introduces a margin-aware preference optimization approach that generates high-quality rationales through multi-turn prompting with rejection sampling, then applies contrastive preference selection to maximize the likelihood gap between preferred and non-preferred outputs. By focusing on the most challenging chosen and rejected response pairs, RoseRAG enhances SLM robustness to noisy retrieved documents while maintaining or improving exact match and F1 scores across multiple open-domain question answering benchmarks.

## Method Summary
RoseRAG implements a three-stage pipeline: (1) preference data generation using multi-turn prompting with ground-truth answers and rejection sampling to filter rationales, (2) contrastive selection that identifies the most challenging chosen response (lowest confidence correct) and rejected response (highest confidence incorrect), and (3) ORPO optimization that maximizes the likelihood margin between preferred and non-preferred responses. The method uses LoRA fine-tuning with rank 16 and trains with ORPO loss where β < 0.5, focusing on small-scale models like Qwen2.5-1.5B, Llama-3.2-1B, and Gemma-2-2B across HotPotQA, 2WikiMultiHopQA, and StrategyQA benchmarks.

## Key Results
- RoseRAG significantly outperforms state-of-the-art baselines across all tested SLM backbones
- Improvements in exact match and F1 scores are consistent across three open-domain QA benchmarks
- The method demonstrates robustness to increasing numbers of retrieved documents (K values)
- Performance gains are maintained across different preference optimization approaches

## Why This Works (Mechanism)

### Mechanism 1: Margin-aware Preference Optimization via ORPO Loss
Maximizing the likelihood margin between preferred and non-preferred responses improves SLM robustness to noisy retrieval by forcing the model to push apart correct reasoning from incorrect or noise-influenced reasoning in probability space. The ORPO loss integrates a preference alignment term directly into SFT using an odds ratio penalty, creating explicit separation between correct and incorrect outputs.

### Mechanism 2: Contrastive Hard Pair Selection
Selecting the most challenging chosen/rejected pairs (lowest-confidence correct, highest-confidence incorrect) amplifies learning signal by targeting decision boundary regions where gradient updates are most informative. This hard negative mining approach focuses on uncertain-correct and confident-incorrect responses that represent the model's blind spots.

### Mechanism 3: Ground-Truth-Guided Rationale Generation with Rejection Sampling
Injecting ground-truth answers during rationale generation, then filtering via rejection sampling, produces high-quality preference data without requiring teacher LLMs. This approach constrains the reasoning space enough to produce valid rationales while filtering out spurious post-hoc justifications through answer verification.

## Foundational Learning

- **Concept: Retrieval-Augmented Generation (RAG) pipeline**
  - Why needed here: RoseRAG modifies the post-retrieval generation phase; understanding baseline retrieve-then-generate is prerequisite.
  - Quick check question: Given a query and top-K retrieved documents, can you trace how they're concatenated into the prompt and conditioned on during generation?

- **Concept: Preference optimization (DPO/ORPO family)**
  - Why needed here: RoseRAG uses ORPO loss; without understanding odds ratios and reference-free preference learning, the margin mechanism is opaque.
  - Quick check question: Explain why ORPO doesn't need a separate reference model compared to DPO.

- **Concept: Contrastive learning and hard negative mining**
  - Why needed here: The selection strategy is a form of hard negative mining applied to text generation.
  - Quick check question: Why would selecting the highest-likelihood negative be more informative than a random negative?

## Architecture Onboarding

- **Component map:**
  Query → Retriever (ColBERT v2) → Top-K Docs → Multi-turn prompt (query + docs + ground-truth) → Rejection sampling → Chosen candidates/Rejected candidates → Contrastive selection (argmin chosen, argmax rejected) → ORPO Optimization (SFT + odds ratio loss)

- **Critical path:** Rejection sampling correctness → selection quality → ORPO margin signal. If sampling fails, everything downstream degrades.

- **Design tradeoffs:**
  - Ground-truth injection: Enables self-supervised rationale generation but requires labeled training data
  - Selection aggressiveness: Harder pairs give stronger signal but may overfit to training distribution
  - β tuning: Higher values emphasize preference separation; too high neglects positive learning

- **Failure signatures:**
  - EM/F1 plateaus or drops after K=5 documents (indicates noise sensitivity not addressed)
  - Performance gap between selected and non-selected variants narrows (selection not finding informative pairs)
  - Training loss decreases but validation EM flatlines (overfitting to preference pairs)

- **First 3 experiments:**
  1. Ablate rejection sampling: Run RoseRAG without filtering on HotPotQA subset; expect ~20% EM drop per Figure 4
  2. Vary selection strategy: Compare min/max selection vs random selection vs no selection (Table 4 provides expected baselines)
  3. Scale retrieved documents K: Test K∈{1,2,5,8,10} to verify robustness claim (should maintain performance unlike vanilla RAG)

## Open Questions the Paper Calls Out

### Open Question 1
Can the preference data generation mechanism be adapted for tasks lacking explicit ground-truth labels, such as open-ended generation or summarization? The current methodology is strictly supervised, limiting its application to datasets where gold-standard exact answers are available. Evidence would require modification of the rejection sampling step to use self-consistency or reward-model scoring instead of exact match, tested on a generative benchmark.

### Open Question 2
Does RoseRAG's margin-aware optimization provide performance gains for Large Language Models (LLMs), or is the robustness effect specific to the limited capacity of SLMs? The paper focuses exclusively on "Small-scale LLMs (SLMs)" and contrasts them with LLMs that "struggle to capture... knowledge." Evidence would require experimental results applying RoseRAG to larger backbones to compare relative improvements against standard RAG.

### Open Question 3
To what extent do the theoretical guarantees of the contrastive selection strategy hold when the distribution of model outputs deviates from the exponential family assumptions used in the analysis? The theorem relies on exponential distribution assumptions that may not hold for real-world language model distributions. Evidence would require empirical analysis of actual output distributions compared to theoretical exponential assumptions.

## Limitations

- Reliance on ground-truth labels during preference data generation restricts applicability to datasets with annotated answers and prevents fully unsupervised deployment
- Method assumes retrieval quality is reasonably high initially—robustness improvements build upon existing retrieval performance rather than compensating for fundamentally broken retrieval systems
- Doesn't address scalability to very large K values (>10 retrieved documents) or evaluate on truly open-domain settings without ground truth during training

## Confidence

- **High confidence:** Experimental results showing consistent improvements across multiple benchmarks and SLM backbones, with ablation studies providing strong evidence for claimed mechanisms
- **Medium confidence:** Generalization claim that RoseRAG "is compatible with various preference optimization approaches" is supported by ORPO implementation but not systematically tested with alternatives
- **Medium confidence:** Robustness to increasing K documents is demonstrated up to K=10, but performance beyond this point and with deliberately injected noise remains untested

## Next Checks

1. **Scalability test:** Evaluate RoseRAG on K=20 and K=50 retrieved documents to determine whether robustness gains extend to scenarios with high retrieval noise density, and measure computational overhead at larger K values.

2. **Ground-truth independence:** Implement a version of RoseRAG that uses synthetic ground truths or test on datasets where ground truth can be generated through alternative means to assess real-world applicability without annotated answers.

3. **Retrieval quality sensitivity:** Systematically vary the quality of the initial retrieval (using corrupted documents, irrelevant passages, or weaker retrievers) to determine whether RoseRAG can recover from fundamentally poor retrieval or if it requires a minimum retrieval quality threshold to function effectively.