---
ver: rpa2
title: Evaluating SAE interpretability without explanations
arxiv_id: '2507.08473'
source_url: https://arxiv.org/abs/2507.08473
tags:
- intruder
- interpretability
- latent
- examples
- latents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors introduce two new methods for evaluating the interpretability
  of sparse autoencoders (SAEs) without generating natural language explanations.
  The first method, intruder detection, presents evaluators with five sentences (four
  activating a given latent, one not) and measures how accurately they can identify
  the non-activating example.
---

# Evaluating SAE interpretability without explanations

## Quick Facts
- arXiv ID: 2507.08473
- Source URL: https://arxiv.org/abs/2507.08473
- Authors: Gonçalo Paulo; Nora Belrose
- Reference count: 6
- Primary result: Human participants achieved 65% accuracy on intruder detection task across 56 latents, with 78% accuracy for highest activation deciles

## Executive Summary
This paper introduces two novel methods for evaluating sparse autoencoder (SAE) interpretability without requiring natural language explanations. The first method, intruder detection, presents evaluators with five sentences (four activating a given latent, one not) and measures how accurately they can identify the non-activating example. The second method, example embedding scoring, uses small embedding models to cluster activating versus non-activating examples in vector space. Both methods show promise as alternatives to explanation-based SAE evaluation, with intruder detection achieving strong human performance and embedding scoring offering faster computation despite lower correlation with human judgments.

## Method Summary
The authors evaluate SAE interpretability through two approaches that avoid generating natural language explanations. Intruder detection presents evaluators with five sentences (four from the same activation decile as the latent, one from a different decile) and measures accuracy in identifying the non-activating example. Example embedding scoring computes cosine similarity differences between positive and negative queries and their respective example sets, using AUROC to measure separability. Both methods are tested on SAEs trained on MLP outputs at layers 9, 15, 21, and 27 of SmolLM2 135M with TopK activation (k=32), using activations collected over a text corpus.

## Key Results
- Human participants achieved 65% accuracy on intruder detection across 56 latents, with 78% accuracy for highest activation deciles
- LLM evaluators (Claude Sonnet 3.5) achieved 84% correlation with human judgments on intruder detection
- Example embedding scoring showed 48% correlation with human judgments, with highest decile AUROCs between 0.64-0.70
- Interpretability varies significantly across activation deciles, with higher deciles being more interpretable

## Why This Works (Mechanism)

### Mechanism 1: Intruder Detection as Interpretability Proxy
- Claim: If a latent encodes a coherent concept, evaluators can identify non-activating examples without explicit explanations.
- Mechanism: Sample 4 activating examples + 1 non-activating intruder from same decile. Evaluators detect patterns across activating examples and identify the outlier. Accuracy serves as interpretability score.
- Core assumption: Interpretable latents exhibit consistent patterns that humans/LLMs can recognize implicitly.
- Evidence anchors:
  - [abstract] "Human participants achieved 65% accuracy on the intruder task across 56 latents, with 78% accuracy for the highest activation deciles."
  - [section 4.1] "This supports the claim that explicit natural language interpretations are not needed to decide whether a given example should be active or not."
  - [corpus] Related work (Chang et al. 2009) validates intruder detection for topic models; corpus evidence supports transferability to SAEs.
- Break condition: If evaluators achieve ~20% accuracy (random guessing), the latent is uninterpretable via this method.

### Mechanism 2: Decile-Dependent Interpretability Gradient
- Claim: Higher activation deciles are systematically more interpretable than lower deciles.
- Mechanism: Activation strength correlates with concept clarity—stronger activations represent clearer instances of the latent's concept. Examples from top deciles show more distinct patterns.
- Core assumption: SAE latents encode scalar features where activation magnitude indicates concept presence/intensity.
- Evidence anchors:
  - [abstract] "Interpretability varies significantly across activation deciles, with higher deciles being more interpretable."
  - [section 4.1, Figure 3] "Examples from the highest activating decile can have up to 20% higher accuracy than examples from the smallest activations."
  - [corpus] Weak corpus evidence on decile-specific interpretability; this appears to be a novel finding.
- Break condition: If accuracy is uniform across deciles, the latent may be binary/monosemantic rather than scalar.

### Mechanism 3: Embedding Clustering as Lightweight Proxy
- Claim: Activating examples from interpretable latents cluster more cohesively in embedding space than non-activating examples.
- Mechanism: Compute cosine similarity between positive/negative queries and their respective class sets (Δ⁺, Δ⁻). Use AUROC to measure separability. Smaller embedding models (22M params) enable fast evaluation.
- Core assumption: Off-the-shelf embedding models capture semantic patterns relevant to latent activation without fine-tuning.
- Evidence anchors:
  - [abstract] "Example embedding scoring produced lower but still meaningful correlations with human judgments (48% correlation)."
  - [section 4.2] "The highest decile is a notable exception, yielding AUROCs between 0.64 and 0.70."
  - [corpus] Corpus shows limited validation of embedding-based interpretability scoring; this is an emerging approach.
- Break condition: AUROC near 0.5 indicates no clustering signal—embedding model cannot distinguish classes.

## Foundational Learning

- Concept: **Sparse Autoencoders (SAEs)**
  - Why needed here: Understanding that SAEs decompose dense activations into sparse, interpretable latents via overcomplete basis with sparsity constraints.
  - Quick check question: Can you explain why sparsity helps address polysemanticity in neural representations?

- Concept: **TopK Activation Function**
  - Why needed here: The paper uses TopK (k=32) to enforce sparsity directly rather than L1 penalties. Understanding this clarifies how latents are selected.
  - Quick check question: How does TopK differ from ReLU-based SAEs in terms of sparsity enforcement?

- Concept: **Decile Stratification**
  - Why needed here: The paper's key finding depends on analyzing interpretability across activation strength bins. You must understand why sampling from specific deciles matters.
  - Quick check question: Why might examples from decile 1 vs decile 10 show different interpretability patterns?

## Architecture Onboarding

- Component map:
  - Base model (SmolLM2 135M) → SAE encoder (layers 9, 15, 21, 27) → latent activations → decile sampling → evaluation pipeline (intruder detection OR embedding scoring) → correlation analysis with human baseline

- Critical path:
  1. Collect activations over corpus (10B tokens for training, subset for evaluation)
  2. Sample 4 activating + 1 non-activating example per latent per decile
  3. Run intruder detection via LLM or human evaluators
  4. Compute accuracy per latent, aggregate across deciles
  5. Validate LLM evaluators against human baseline (target: >0.80 correlation)

- Design tradeoffs:
  - Intruder detection accuracy vs. computational cost: Claude Sonnet 3.5 achieves 84% correlation with humans but requires large model; example embedding scoring is 10-100x faster but only 48% correlation.
  - Decile granularity vs. statistical power: More deciles = finer analysis but fewer examples per bin increases variance.
  - Highlighting strategy: Paper highlights activating tokens with << >> but embedding models may not attend to this notation effectively.

- Failure signatures:
  - Accuracy ≈20% (random): Latent is uninterpretable or task is poorly designed
  - Large human-LLM gap: LLM may be finding spurious patterns humans cannot verify
  - Low inter-human agreement (<0.6 correlation): Subjective task, unclear latent concept
  - Decile accuracy inversion: Lower deciles more interpretable than higher—suggests feature absorption or splitting pathologies

- First 3 experiments:
  1. Replicate intruder detection on 10 latents with human evaluators to establish baseline for your specific SAE/model combination.
  2. Compare 3 LLM evaluators (e.g., Claude, GPT-4, Llama 70B) on same latent subset to measure inter-LLM agreement before scaling.
  3. Test example embedding scoring with different embedding models (all-MiniLM-L6-v2 vs. larger models) to quantify speed/accuracy tradeoff for your use case.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What causes the asymmetry in intruder decile detection, where identifying a high-activating intruder among low-activating examples is significantly harder than the reverse?
- Basis: [explicit] Section 4.1.1 notes that the accuracy matrix for detecting intruder deciles is nonsymmetric, contrasting with the expectation that scalar features would exhibit symmetric distinguishability.
- Why unresolved: The authors rule out the hypothesis that features are simply binary and monosemantic, but the specific mechanism driving the difficulty of spotting high-activation intruders in low-activation contexts remains unidentified.
- What evidence would resolve it: A detailed ablation study analyzing whether low-activation contexts contain overlapping features that mask the intruder, or if the SAE represents feature intensity via distinct, asymmetric mechanisms.

### Open Question 2
- Question: Can example embedding scoring be modified to better attend to highlighted activating tokens rather than the surrounding context?
- Basis: [explicit] Section 4.2 identifies the method's "main weakness" as its low sensitivity to activating tokens, as the off-the-shelf embedding model (all-MiniLM-L6-v2) does not interpret the `<< >>` highlighting syntax.
- Why unresolved: The current implementation relies on standard sentence embeddings which conflate the highlighted token with the full sentence context, limiting the resolution of the metric.
- What evidence would resolve it: Fine-tuning an embedding model specifically on the highlighting syntax or using attention-based pooling to weight highlighted tokens more heavily during embedding calculation.

### Open Question 3
- Question: Do interpretable latents exist that humans can distinguish via intruder detection but which resist description via succinct natural language explanations?
- Basis: [explicit] The Introduction states the authors "question this philosophical assumption" that interpretability requires a meaning "succinctly expressible in words."
- Why unresolved: While the paper establishes intruder detection as a valid metric, it does not provide a direct comparison proving the existence of latents that score high on intruder tasks but fail explanation-based simulation tasks.
- What evidence would resolve it: A direct correlation analysis comparing the scores of the proposed intruder detection method against traditional explanation-based simulation scores across a wide range of latents.

## Limitations

- Intruder detection achieves only 65% average accuracy across latents, suggesting many SAE latents remain poorly interpretable even with these new approaches
- Example embedding scoring shows even weaker performance with 48% correlation to human judgments, raising questions about its reliability as a standalone evaluation tool
- Methods were tested primarily on SmolLM2 135M with specific SAE architectures (TopK activation, layers 9, 15, 21, 27), limiting generalizability to other model families

## Confidence

- **High confidence**: The decile-dependent interpretability finding (higher deciles more interpretable) is well-supported with clear statistical patterns across multiple latents and human evaluators.
- **Medium confidence**: The overall viability of intruder detection as an interpretability proxy, given reasonable human performance (65%+) and strong LLM-human correlation (84%), though the absolute accuracy remains modest.
- **Low confidence**: The example embedding scoring method's utility, given its lower correlation (48%) with human judgments and lack of clear interpretability gains over random chance in many cases.

## Next Checks

1. **Cross-model validation**: Apply both methods to SAEs from different base models (e.g., GPT-2, Llama) and architectures (e.g., ReLU-based SAEs) to test generalizability of the findings. Measure whether the 84% LLM-human correlation holds across different SAE families.

2. **Temporal stability test**: Collect intruder detection judgments for the same latents at different time points (e.g., one week apart) to measure inter-rater reliability and test whether the 65% average accuracy represents stable, repeatable judgments or noisy measurements.

3. **Ablation on highlighting strategy**: Systematically remove the << and >> token highlighting from intruder detection examples to test whether embedding models are actually attending to these markers versus learning genuine semantic patterns, and measure the impact on AUROC scores.