---
ver: rpa2
title: 'Moon: A Modality Conversion-based Efficient Multivariate Time Series Anomaly
  Detection'
arxiv_id: '2510.01970'
source_url: https://arxiv.org/abs/2510.01970
tags:
- data
- anomaly
- time
- detection
- series
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Moon, a supervised modality conversion-based
  multivariate time series anomaly detection framework. It addresses challenges in
  existing methods including reliance on error thresholds in unsupervised methods,
  underuse of anomaly labels in semi-supervised methods, and high computational costs
  in supervised methods.
---

# Moon: A Modality Conversion-based Efficient Multivariate Time Series Anomaly Detection

## Quick Facts
- **arXiv ID**: 2510.01970
- **Source URL**: https://arxiv.org/abs/2510.01970
- **Reference count**: 40
- **Primary result**: Moon outperforms six SOTA methods by up to 93% in efficiency, 4% in accuracy, and 10.8% in interpretation performance

## Executive Summary
This paper proposes Moon, a supervised multivariate time series anomaly detection framework that converts numeric time series into image representations using Multivariate Markov Transition Fields (MV-MTF), then fuses these with raw numerical features using a Multimodal-CNN with parameter sharing and multimodal attention. The framework addresses challenges in existing methods including reliance on error thresholds in unsupervised methods, underuse of anomaly labels in semi-supervised methods, and high computational costs in supervised methods. Extensive experiments on six real-world datasets demonstrate superior performance in accuracy, efficiency, and interpretability compared to state-of-the-art approaches.

## Method Summary
Moon converts multivariate time series into image representations via MV-MTF, which encodes transition probabilities across variables and timestamps into 2D matrices. These images are fused with raw numerical features using a Multimodal-CNN featuring parallel 1D-CNNs (raw data) and 2D-CNNs (image data) with cross-modal attention. The framework uses quantile binning with entropy-based Q selection, consecutive-timestamp transitions only (O(n) complexity), and combines Kernel and Gradient SHAP for interpretability with weighted contributions. Training uses Adam optimizer with lr=0.001, and key hyperparameters include α=0.9 for MV-MTF and ω=0.6 for interpretability weighting.

## Key Results
- Outperforms six SOTA methods by up to 93% in training efficiency (9.25s vs 34862s for Extended MTF)
- Achieves 4% higher accuracy and 10.8% better interpretation performance than baselines
- Demonstrates strong F1 scores across SMAP, MSL, SWaT, WADI, PSM, and SMD datasets
- Provides interpretable anomaly reports using SHAP-based methods with Hit@P% and NDCG metrics

## Why This Works (Mechanism)

### Mechanism 1: Distribution Separability via Modality Conversion
Converting raw MTS into image representations using MV-MTF appears to increase distinguishability between normal and abnormal data distributions by encoding structural dynamics into texture patterns. This transforms temporal transitions into visual features, reportedly enlarging KL divergence between distributions. The core assumption is that anomalies manifest as distinct structural transitions in joint variable-time space captured as visual textures rather than point-wise numeric deviations.

### Mechanism 2: Complementary Feature Fusion (Numeric + Image)
Fusing raw numeric data with MV-MTF image data via Multimodal-CNN compensates for fine-grained detail loss in image conversion. The architecture uses parallel 1D-CNNs (raw data) and 2D-CNNs (image data) with cross-attention mechanism. Numeric branch retains precise values while image branch captures global structural dependencies. The assumption is that features from both modalities are complementary and weighted attention can successfully align them.

### Mechanism 3: Computational Efficiency via Markov Simplification
Achieves high efficiency (O(n) complexity) by simplifying Markov Transition Field calculation to consecutive timestamps only. Standard MTF requires O(n²) for all time pair transitions, but Moon restricts to t and t-1, reducing matrix calculation to sparse operation. The assumption is that temporal dependency decays rapidly, making long-range transitions negligible for anomaly detection.

## Foundational Learning

- **Concept: Markov Transition Fields (MTF)**
  - Why needed here: Core encoding technique transforming 1D time series into 2D probability matrices representing state transitions
  - Quick check question: Can you explain how a univariate time series is mapped to a Q x Q transition matrix before being expanded into the field M?

- **Concept: Cross-Modal Attention**
  - Why needed here: Multimodal-CNN relies on this to fuse numeric and image features; understanding Q/K/V operations is necessary to debug fusion layer
  - Quick check question: How does the Attention_num->img calculation allow numeric features to influence selection of image features?

- **Concept: SHAP (Shapley Additive Explanations)**
  - Why needed here: Framework's interpretability rests on SHAP values; need to distinguish Kernel Explainer (numeric) vs Gradient Explainer (image)
  - Quick check question: Why does paper combine s_K (Kernel SHAP) and s_G (Gradient SHAP) rather than using just one?

## Architecture Onboarding

- **Component map**: Input Layer -> Modality Conversion (MV-MTF) -> Feature Extraction (Dual CNN-Blocks) -> Fusion Module (Cross-Modal Attention + GDFN) -> Output (Classifier + SHAP Explainer)

- **Critical path**: MV-MTF transformation (Eq. 8) is bottleneck; if binning strategy or α weight is misconfigured, resulting image lacks structural texture causing 2D-CNN to learn noise

- **Design tradeoffs**:
  - Efficiency vs. Completeness: Simplifies MTF to adjacent time steps (O(n)), sacrificing long-range temporal context for speed
  - Interpretability Weighting (ω): Balances raw data vs transition structure contribution; high ω prioritizes raw values, low ω prioritizes structural transitions

- **Failure signatures**:
  - High False Positives: Likely caused by α parameter being too low, causing normal fluctuations in other variables to pollute target variable's transition matrix
  - Low Interpretability Hit Rate: Suggests SHAP weighting ω is misaligned with anomaly type (explaining structural anomaly using raw value SHAP scores)

- **First 3 experiments**:
  1. Reproduce Table I (MV-MTF Efficiency): Verify O(n) scaling claim on data subset to ensure Eq. 8 implementation is correct
  2. Ablation on Attention: Disable Cross-Modal Attention (force simple concatenation) to quantify specific contribution to F1 score vs Table IV results
  3. Parameter Sensitivity (α): Run tests in Figure 8 to find optimal transition weight α for new dataset, as this varies by data characteristics

## Open Questions the Paper Calls Out

- **Open Question 1**: Can Moon framework be extended to perform other downstream time series tasks beyond anomaly detection, such as forecasting or imputation?
  - Basis: Conclusion states "extend our anomaly detection model to handle other downstream tasks"
  - Why unresolved: Current modules designed for binary classification; MV-MTF utility for generative/continuous prediction not investigated
  - Evidence needed: Application of MV-MTF and Multimodal-CNN to forecasting benchmarks

- **Open Question 2**: Does optimization of MV-MTF to O(n) complexity by restricting transitions to consecutive timestamps result in loss of long-range temporal dependency information?
  - Basis: Section 4.A describes simplification to consecutive timestamps only
  - Why unresolved: Paper asserts essential local information preserved but doesn't quantify degradation for anomalies with long-range periodicity
  - Evidence needed: Ablation study comparing simplified O(n) vs full O(n²) on datasets with long-term cyclical anomalies

- **Open Question 3**: How robust is SHAP-based anomaly explainer across datasets with different structures given evaluation restricted to SMD dataset?
  - Basis: Section 5.G states evaluation "exclusively on SMD dataset, as other datasets lack interpretation labels"
  - Why unresolved: Interpretability performance unverified for industrial or spatial datasets used in detection experiments
  - Evidence needed: Ground truth labels for SWaT or SMAP datasets to evaluate explainer performance

## Limitations

- Architectural details underspecified: CNN depth, kernel size schedules, parameter sharing configurations, and cross-modal attention dimensions not clearly defined
- Dataset preprocessing variations: Specific preprocessing steps, window sizes, normalization methods, and exact train/test splits beyond sample counts not detailed
- Hyperparameter sensitivity not fully explored: Limited ablation studies on α and ω sensitivity; optimal values may be dataset-dependent

## Confidence

- **High confidence**: Core methodology (MV-MTF technique, multimodal fusion approach, framework design) clearly articulated with sufficient mathematical formulation; efficiency claims supported by explicit equations and comparative timing
- **Medium confidence**: Performance claims show Moon outperforming baselines across datasets, but lack of detailed specifications limits full verification
- **Low confidence**: Interpretability metrics implementation details for combining Kernel and Gradient SHAP values, and computation of hit rates/NDCG scores, not fully specified

## Next Checks

1. **Reproduce MV-MTF efficiency**: Implement O(n) optimization from Equation 8 and verify dramatic runtime improvements shown in Table I - this is most critical component affecting both performance and efficiency

2. **Ablation on cross-modal attention**: Systematically disable attention mechanism and compare against Table IV results to quantify specific contribution to F1 scores - isolates fusion benefit from other architectural components

3. **Parameter sensitivity analysis**: Conduct controlled experiments varying α (transition weight) and ω (interpretability weight) across plausible ranges to establish robustness and identify optimal settings for different dataset characteristics