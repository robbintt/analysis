---
ver: rpa2
title: 'TableDART: Dynamic Adaptive Multi-Modal Routing for Table Understanding'
arxiv_id: '2509.14671'
source_url: https://arxiv.org/abs/2509.14671
tags:
- table
- fusion
- tabledart
- path
- routing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of effective table understanding
  by proposing TableDART, a dynamic adaptive multi-modal routing framework. The key
  innovation is a lightweight 2.59M-parameter MLP gating network that intelligently
  selects between Text-only, Image-only, or Fusion processing paths for each table-query
  pair, based on complexity and efficiency considerations.
---

# TableDART: Dynamic Adaptive Multi-Modal Routing for Table Understanding

## Quick Facts
- arXiv ID: 2509.14671
- Source URL: https://arxiv.org/abs/2509.14671
- Authors: Xiaobo Xing; Wei Yuan; Tong Chen; Quoc Viet Hung Nguyen; Xiangliang Zhang; Hongzhi Yin
- Reference count: 40
- One-line primary result: 4.02% better average accuracy than strongest baseline while providing significant efficiency gains through dynamic routing

## Executive Summary
TableDART introduces a dynamic adaptive multi-modal routing framework that intelligently selects between Text-only, Image-only, or Fusion processing paths for each table-query pair. The key innovation is a lightweight 2.59M-parameter MLP gating network that routes queries based on complexity and efficiency considerations, reusing frozen pre-trained single-modality models rather than fine-tuning expensive multimodal LLMs. Extensive experiments on seven benchmarks demonstrate state-of-the-art performance among open-source models while providing significant efficiency gains through intelligent path selection.

## Method Summary
TableDART employs a lightweight MLP gating network that dynamically routes table-query pairs to optimal processing paths by analyzing concatenated embeddings from three frozen expert models: TableGPT2-7B (text), Ovis2-8B (image), and MiniLM-L6-v2 (query). The framework introduces resource-aware training that balances task performance with computational cost through KL-divergence loss and expected cost regularization. When Fusion is selected, a frozen LLM agent mediates cross-modal synthesis by acting as an Arbitrator or Rescuer. The approach achieves 4.02% better average accuracy than baselines while maintaining efficiency through intelligent path selection.

## Key Results
- Achieves 4.02% better average accuracy than strongest baseline among open-source models
- Demonstrates 58.7% of test instances can be solved by both unimodal paths, validating routing efficiency
- Shows 24.0% complementarity rate between unimodal paths, confirming fusion is often unnecessary
- Achieves 14.0% synergy success rate on hard cases through Fusion agent rescue mechanism

## Why This Works (Mechanism)

### Mechanism 1
A lightweight MLP can learn to route table-query pairs to the optimal processing path without fine-tuning large backbone models. The gating network concatenates three embeddings (query via MiniLM-L6-v2, table-as-text via TableGPT2-7B pooled embeddings, table-as-image via Ovis2-8B visual tokens) into a 10,112-dim vector, then applies a 2-layer MLP with 256 hidden units and 3 output logits. During inference, argmax selects Text-only, Image-only, or Fusion. Core assumption: Not every table-query pair benefits from multimodal fusion; structural cues favor image paths while fine-grained semantics favor text paths. Evidence: 58.7% of test instances solved by both unimodal paths, confirming fusion is often unnecessary.

### Mechanism 2
Resource-aware regularization during training improves generalization by preventing over-reliance on the expensive Fusion path. The total loss is L_task + λL_resource. L_task uses KL divergence between the softmax-normalized path accuracy scores and the gating network's predicted distribution. L_resource computes the expected cost via softmax(z/τ_g)^T c, where c = [0.73, 0.81, 0.96] is empirically measured for Text/Image/Fusion paths. Core assumption: A purely performance-driven routing policy (λ=0) will overfit to training artifacts by selecting Fusion even when unnecessary. Evidence: λ=0.15 achieves highest average performance (69.32%) vs. λ=0.00 (69.27%), demonstrating regularization benefit.

### Mechanism 3
A frozen LLM agent can mediate cross-modal synthesis by acting as an Arbitrator (selecting the better output) or Rescuer (synthesizing from partial evidence). When Fusion is selected, both Text and Image models generate outputs with explanations. The Fusion agent (Gemini 2.0 Flash) receives the question, full markdown table, and both outputs. It analyzes conflicts, validates reasoning paths against the source table, and either selects the correct answer or combines valid fragments into a new answer. Core assumption: Single-modality models can fail in complementary ways—one may hallucinate while the other misses structural cues. Evidence: 14.0% synergy success rate on hard cases where both unimodal paths fail.

## Foundational Learning

- **Concept: Mixture of Experts (MoE) routing**
  - Why needed here: The gating network is conceptually similar to MoE routing, where a learned selector dispatches inputs to specialized experts. Understanding softmax routing, load balancing, and expert capacity helps diagnose why some paths are underutilized.
  - Quick check question: Can you explain why a hard argmax routing decision is differentiable during training, and why the paper uses a soft KL-divergence target instead of reinforcement learning?

- **Concept: Multi-modal representation alignment**
  - Why needed here: TableDART concatenates embeddings from different encoders (text, vision, query) without cross-modal pretraining. Understanding the challenges of embedding space misalignment helps anticipate when the gating network may fail to capture cross-modal signals.
  - Quick check question: What would happen if the text and image embeddings were on vastly different scales before concatenation? How does the MLP address (or not address) this?

- **Concept: Resource-constrained optimization**
  - Why needed here: The λ hyperparameter explicitly trades off accuracy against inference cost. This is a form of constrained optimization where the resource loss acts as a Lagrangian penalty.
  - Quick check question: If you wanted to enforce a strict latency budget (e.g., max 2.0s per query), how would you modify the training objective or inference policy?

## Architecture Onboarding

- **Component map:**
  Multimodal Encoding (parallel) -> Gating Network (2-layer MLP) -> Dynamic Inference Pathways (Text-only, Image-only, or Fusion) -> LLM Fusion Agent (Gemini 2.0 Flash for synthesis)

- **Critical path:**
  1. Encoding phase: All three encoders run in parallel; latency bottlenecked by Ovis2-8B
  2. Gating: Concatenate embeddings → MLP forward pass (~1ms)
  3. Execution: If unimodal selected, run only that decoder; if Fusion, run both decoders in parallel + API call (~0.3s overhead)
  4. Synthesis (Fusion only): LLM agent analyzes both outputs and generates final answer

- **Design tradeoffs:**
  - Parallel vs. sequential encoding: Encoding all modalities upfront enables instant routing but incurs cost even for unimodal paths (7.15%/7.63% of backbone params activated for embeddings)
  - Frozen experts vs. fine-tuned MLLM: Freezing experts enables plug-and-play but limits joint multimodal representation learning
  - Hard routing vs. soft routing: Inference uses hard argmax, but training uses soft KL targets. This avoids non-differentiability but creates train-test mismatch

- **Failure signatures:**
  - Over-reliance on Fusion (low λ): High latency, marginal accuracy gains; check routing distribution—if Fusion >50% on simple datasets, λ may be too low
  - Under-utilization of Image path: If the gating network rarely selects Image-only, visual features may be poorly aligned with text embeddings
  - Fusion agent hallucination: If the agent synthesizes answers not grounded in either output, prompt constraints may be too loose

- **First 3 experiments:**
  1. Routing distribution analysis on a new benchmark: Run TableDART on a held-out dataset (e.g., FinQA) and visualize path selection percentages
  2. Ablation of embedding pooling strategies: Replace attention-pooled text embeddings with mean pooling and measure gating network accuracy
  3. Sensitivity of λ to latency budget: Vary λ from 0 to 1.0 on a latency-constrained test set (target <2.0s) and plot accuracy vs. latency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does TableDART's routing policy generalize when applied to different combinations of single-modality expert models beyond TableGPT2-7B and Ovis2-8B?
- Basis in paper: The paper evaluates only one specific pair of constituent models but claims "TableDART is a general framework that can be seamlessly integrated with a wide range of existing LLMs, VLMs and MLLMs"
- Why unresolved: The gating network's training depends on the specific embedding dimensions and characteristics of the chosen experts (10,112 total dimensions)
- What evidence would resolve it: Experiments with alternative expert pairs showing comparable performance gains

### Open Question 2
- Question: What is the theoretical upper bound on the Fusion path's Synergy Success Rate, and can architectural improvements push beyond the observed 14.0% average?
- Basis in paper: The paper reports 14.0% synergy success rate on hard cases but doesn't investigate whether this represents a fundamental limit
- Why unresolved: The Fusion agent's effectiveness depends on what partial information each failed single-modality output contains
- What evidence would resolve it: Analysis of failure patterns in hard cases and whether enhanced Fusion agent prompting can extract more synergistic solutions

### Open Question 3
- Question: How does TableDART maintain performance when the external Fusion API (Gemini 2.0 Flash) is unavailable, rate-limited, or introduces variable latency?
- Basis in paper: The Fusion path relies entirely on REST API calls to Google Gemini 2.0 Flash, with a measured "0.3s API average overhead"
- Why unresolved: Production deployment requires graceful degradation when external services are unreliable
- What evidence would resolve it: Experiments measuring performance under simulated API outages or with local Fusion model alternatives

### Open Question 4
- Question: What specific input features or table/query characteristics drive the learned routing policy's deviation from the simple greedy heuristic at optimal λ=0.15?
- Basis in paper: The authors note that λ=0.15 "resides at the apex of this performance curve" with "modest Heuristic Alignment Score," showing "the globally optimal strategy... involves strategically investing in more computationally expensive processing paths"
- Why unresolved: Understanding these patterns would enable better gating network design and interpretability
- What evidence would resolve it: Feature importance analysis correlating specific table/query attributes with routing decisions that diverge from the greedy heuristic

## Limitations

- Performance gains need validation on truly out-of-distribution table-query pairs beyond the same datasets used for training
- Resource-aware regularization sensitivity shows non-linear relationship, suggesting aggressive constraints could significantly degrade accuracy
- Cross-modal synthesis reliability lacks systematic error analysis of when and why the Fusion agent fails

## Confidence

- **High Confidence (Mechanism 1 - Gating Network Design):** Well-specified architectural choices directly supported by quantitative evidence showing complementarity between unimodal paths
- **Medium Confidence (Mechanism 2 - Resource-Aware Training):** Theoretically sound framework with ablation studies showing regularization benefit, but limited to single hyperparameter sweep
- **Low Confidence (Mechanism 3 - Fusion Agent Effectiveness):** Relies heavily on black-box LLM agent whose performance varies with prompt quality and context length

## Next Checks

1. **Zero-Shot Transfer Evaluation:** Test TableDART on a completely held-out benchmark (e.g., FinQA or domain-specific scientific table dataset) to measure performance degradation and routing distribution shifts

2. **Latency-Precision Pareto Analysis:** Systematically vary λ from 0.0 to 1.0 on a latency-constrained test set (target <2.0s per query) and plot accuracy vs. average latency to identify true efficiency-accuracy frontier

3. **Fusion Agent Error Attribution:** Implement comprehensive error analysis framework categorizing Fusion agent failures by comparing outputs against both unimodal model outputs and ground truth