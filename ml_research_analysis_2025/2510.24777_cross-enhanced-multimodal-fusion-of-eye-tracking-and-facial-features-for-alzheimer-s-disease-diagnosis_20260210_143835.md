---
ver: rpa2
title: Cross-Enhanced Multimodal Fusion of Eye-Tracking and Facial Features for Alzheimer's
  Disease Diagnosis
arxiv_id: '2510.24777'
source_url: https://arxiv.org/abs/2510.24777
tags:
- facial
- features
- eye-tracking
- fusion
- cognitive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study proposes a multimodal fusion framework that integrates
  eye-tracking and facial features for Alzheimer's disease diagnosis. The approach
  combines a Cross-Enhanced Fusion Attention Module (CEFAM) to model inter-modal interactions
  and a Direction-Aware Convolution Module (DACM) to capture directional facial features.
---

# Cross-Enhanced Multimodal Fusion of Eye-Tracking and Facial Features for Alzheimer's Disease Diagnosis

## Quick Facts
- arXiv ID: 2510.24777
- Source URL: https://arxiv.org/abs/2510.24777
- Reference count: 40
- Primary result: 95.11% classification accuracy for AD diagnosis using synchronized eye-tracking and facial features

## Executive Summary
This study introduces a multimodal fusion framework that integrates eye-tracking and facial features for Alzheimer's disease diagnosis. The approach combines a Cross-Enhanced Fusion Attention Module (CEFAM) to model inter-modal interactions and a Direction-Aware Convolution Module (DACM) to capture directional facial features. The model was evaluated on a synchronized dataset of 50 participants (25 AD, 25 healthy controls) collected during a visual memory-search paradigm. Experimental results demonstrate that the proposed method achieves 95.11% classification accuracy, outperforming traditional late fusion and feature concatenation methods. The framework effectively leverages complementary information from both modalities to enhance diagnostic performance.

## Method Summary
The proposed method fuses eye-tracking data (gaze coordinates, pupil diameters, event types) with facial video through a parallel processing architecture. Facial features are extracted using a DCNN with Direction-Aware Convolution Module (DACM) that captures horizontal and vertical structural patterns, followed by a 2-layer LSTM for temporal modeling. Eye-tracking features are processed through a 2-layer Transformer encoder. The Cross-Enhanced Fusion Attention Module (CEFAM) then performs cross-attention with facial features as Query and eye-tracking as Key/Value, enhanced with global pooling. The fused representation passes through 2 FC layers with dropout before classification. The system uses stratified group 10-fold cross-validation on 50 participants, trained with Adam optimizer (lr=1e-5), batch size 8, and early stopping.

## Key Results
- Achieved 95.11% classification accuracy for AD vs HC discrimination
- Outperformed late fusion (83.78%) and feature concatenation methods
- DACM module showed superior directional feature capture compared to standard 3×3 convolution (1.2% accuracy gain)

## Why This Works (Mechanism)

### Mechanism 1: Cross-Modal Attention with Global Enhancement
The CEFAM module improves multimodal fusion by combining cross-attention (for fine-grained inter-modal interactions) with global pooling (for stable modal-level statistics). Cross-attention computes dependency weights between facial features (Query) and eye-tracking features (Key/Value), allowing facial representations to selectively attend to informative eye-tracking patterns. Global max/average pooling extracts statistical descriptors from eye-tracking to complement local attention, mitigating noise sensitivity in temporal eye-tracking data. Core assumption: Eye-tracking features exhibit higher local variability/noise than facial features, so global statistics provide stabilizing signal; facial features provide more reliable Query representations.

### Mechanism 2: Direction-Aware Convolution for Fine-Grained Facial Features
DACM captures horizontal and vertical structural patterns in facial regions that standard isotropic convolutions (e.g., 3×3) may overlook. DACM splits channels into two branches—horizontal (1×3 conv) and vertical (3×1 conv)—each with stacked convolutions, BN, and ReLU. Outputs are concatenated and combined with input via residual connection. This emphasizes directional textures (e.g., eye alignment, mouth corners, nasal bridge) relevant to facial asymmetry and expression attenuation in AD. Core assumption: AD-related facial differences manifest along directional structures; separable horizontal/vertical processing captures these more effectively than standard convolutions.

### Mechanism 3: Facial-Dominant Fusion Strategy
Using facial features as the dominant modality (Query) with eye-tracking as auxiliary (Key/Value) yields more stable and accurate fusion than the reverse or bidirectional configurations. Facial features provide more stable discriminative information; when used as Query in cross-attention, they filter complementary eye-tracking patterns without introducing eye-tracking's noise into the primary representation. Global enhancement further injects stable eye-tracking statistics. Core assumption: Facial features are less noisy and more semantically stable than eye-tracking sequences for AD discrimination.

## Foundational Learning

- **Cross-Attention in Transformers**: Why needed here: CEFAM uses cross-attention to compute dependencies between facial and eye-tracking modalities; understanding Query/Key/Value roles is essential for debugging fusion behavior. Quick check: If you swap Query and Key modalities, what happens to the attention weight matrix dimensions and semantics?

- **Temporal Alignment of Multimodal Sequences**: Why needed here: Facial video (30 fps) and eye-tracking (250 Hz) must be temporally aligned before fusion; downsampling strategy affects feature granularity. Quick check: Given 10-second recordings at 30 fps (video) and 250 Hz (eye-tracking), what is the temporal alignment strategy that produces matching sequence lengths?

- **Ablation Study Design**: Why needed here: Paper validates DACM and CEFAM contributions through ablation; understanding this helps assess module necessity vs. incremental improvement. Quick check: In Table 3, Model III (CEFAM only) improves accuracy from 83.78% to 93.78%. What does this suggest about the relative contribution of fusion vs. feature extraction?

## Architecture Onboarding

- **Component map**: Input video (N×224×224×3) + eye-tracking (M×6) → DCNN+DACM+LSTM (facial) + Transformer (eye) → CEFAM cross-attention → Global pooling → FC layers → Classification

- **Critical path**: Preprocess and temporally align facial frames and eye-tracking samples → Extract modality-specific features through parallel branches → Apply CEFAM with facial-dominant cross-attention → Concatenate fused representation with global eye-tracking features → Classify via 2-layer FC head

- **Design tradeoffs**: DACM vs. standard 3×3 conv: DACM adds directional sensitivity but increases architectural complexity; ablation shows ~1.2% accuracy gain over Conv3×3. Face→Eye vs. Eye→Face: Facial-dominant fusion is more stable but may underutilize eye-tracking's dynamic information; global enhancement compensates. Small dataset (n=50): High accuracy (95.11%) reported, but risk of overfitting—mitigated by 10-fold cross-validation but limits generalizability.

- **Failure signatures**: Low eye-tracking quality: Missing data >20% or erratic gaze patterns may cause global pooling to produce uninformative features. Facial misalignment: Inconsistent face crops across frames reduce DACM's ability to capture directional patterns. Imbalanced class performance: Model shows high precision (96.75%) but lower recall (90.00%), suggesting some AD false negatives.

- **First 3 experiments**: Reproduce single-modality baselines: Train eye-only (Transformer encoder → FC) and face-only (DCNN+LSTM → FC) models on provided dataset to verify reported accuracies (~77% and ~81%). Ablate CEFAM components: Remove global pooling (GMP+GAP) from CEFAM and measure accuracy drop to quantify global enhancement contribution. Swap dominant modality: Run Eye→Face configuration (eye as Query, face as K/V) with same hyperparameters; expect performance degradation per section 5.3.3 findings.

## Open Questions the Paper Calls Out

### Open Question 1
Can the proposed framework effectively distinguish intermediate stages of cognitive decline, such as Mild Cognitive Impairment (MCI), rather than performing only binary classification? Basis: The authors note that "the present work focuses on binary AD detection without addressing intermediate stages such as MCI, which are clinically important for early intervention." Unresolved because the current dataset and model output layer were constructed solely to differentiate between healthy controls (HC) and Alzheimer's Disease (AD) patients. Evidence needed: Successful evaluation of the framework on a dataset comprising HC, MCI, and AD subjects, demonstrating statistically significant separation between the MCI group and the other classes.

### Open Question 2
Does the cross-modal fusion architecture maintain its performance when applied to cognitive tasks outside the visual memory-search paradigm? Basis: The paper states the framework "currently depends on a single visual memory paradigm, which may not comprehensively capture broader cognitive domains such as executive function, attention, and language processing." Unresolved because the model was trained and validated exclusively on data collected during the specific geometric figure memory task. Evidence needed: Benchmarking the model's accuracy on multimodal data collected during varied cognitive tasks (e.g., verbal fluency tests or executive function challenges).

### Open Question 3
Is the reported 95.11% accuracy robust against overfitting when scaled to larger, multi-center cohorts? Basis: The authors acknowledge that "the modest sample size limits statistical robustness and raises potential risks of overfitting, thereby constraining generalizability." Unresolved because the study utilized a relatively small dataset (50 participants) collected at a single site (Shandong University of Traditional Chinese Medicine). Evidence needed: Validation results from a significantly larger, independent dataset collected from diverse clinical settings and demographic backgrounds.

## Limitations

- Small sample size (50 participants) limits statistical robustness and generalizability
- Single-task paradigm restricts applicability to broader cognitive domains
- Lack of external validation on independent, multi-center cohorts

## Confidence

- **High**: Basic multimodal integration framework, temporal alignment strategy
- **Medium**: DACM directional convolutions, CEFAM cross-attention with global enhancement
- **Low**: Generalization to larger cohorts, different cognitive tasks, clinical deployment scenarios

## Next Checks

1. Test the facial-dominant fusion configuration against alternative configurations (bidirectional, eye-dominant) on a held-out test set to confirm the claimed advantage
2. Evaluate model performance when eye-tracking quality is degraded (simulate 20-40% missing samples) to assess robustness of global pooling enhancement
3. Apply the trained model to a different AD biomarker task (e.g., MMSE score regression) to test transferability of learned representations