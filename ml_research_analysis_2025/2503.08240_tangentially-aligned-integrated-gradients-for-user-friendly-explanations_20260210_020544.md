---
ver: rpa2
title: Tangentially Aligned Integrated Gradients for User-Friendly Explanations
arxiv_id: '2503.08240'
source_url: https://arxiv.org/abs/2503.08240
tags:
- uni00000013
- uni00000011
- base-point
- gradient
- explanations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of choosing base-points in integrated
  gradients (IG) explanations for neural networks. IG explanations depend heavily
  on the choice of base-point, but existing heuristics for selecting base-points often
  produce explanations that are not perceptually meaningful or aligned with the data
  manifold.
---

# Tangentially Aligned Integrated Gradients for User-Friendly Explanations

## Quick Facts
- arXiv ID: 2503.08240
- Source URL: https://arxiv.org/abs/2503.08240
- Authors: Lachlan Simpson; Federico Costanza; Kyle Millar; Adriel Cheng; Cheng-Chew Lim; Hong Gunn Chew
- Reference count: 36
- Primary result: A method to choose base-points for Integrated Gradients that maximizes tangential alignment, producing more meaningful explanations with μ_x > 0.91 compared to standard heuristics

## Executive Summary
This paper addresses the critical problem of choosing base-points for Integrated Gradients (IG) explanations in neural networks. The authors propose selecting base-points that maximize the tangential alignment of explanations—ensuring attributions lie in the tangent space of the data manifold rather than the normal space. This approach produces more perceptually meaningful explanations that better capture features important for classification. The method is validated across four image datasets (MNIST, Fashion-MNIST, CIFAR10, FER2013) and consistently outperforms common base-point choices (zero, maximum distance, uniform, Gaussian) and three gradient explainability models.

## Method Summary
The authors formalize tangential alignment and provide conditions under which base-points yield tangentially aligned explanations. They approximate the tangent and normal spaces of the data manifold using a convolutional autoencoder, then optimize base-points via gradient descent to minimize the component of explanations in the normal space. The autoencoder's decoder Jacobian provides the tangent space basis, while the normal space basis is computed as the null space of the tangent basis matrix. The optimization minimizes E_x(α) = ½||π^⊥_x IG(x,α,F)||² to find base-points that produce explanations lying almost entirely in the tangent space (μ_x → 1).

## Key Results
- Tangentially aligned IG consistently achieves μ_x > 0.91 across all four datasets
- Standard base-points (zero, uniform, Gaussian) often produce explanations close to random noise, particularly on complex datasets like CIFAR10
- On CIFAR10, tangential IG produces μ_x > 0.91 while zero base-point yields μ_x well below the random baseline of √(n/d)
- The method outperforms common base-point choices and three gradient explainability models (Gradient, Smooth Grad, Input×Gradient)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Explanations lying in the tangent space of the data manifold are more perceptually meaningful than those in the normal space.
- Mechanism: The tangent space T_xM at point x captures directions in which features can change while remaining within the valid data distribution. Attributions in the tangent space highlight image components that meaningfully affect classification; attributions in the normal space T_xM^⊥ approximate random noise.
- Core assumption: The manifold hypothesis holds—data lies on a low-dimensional Riemannian manifold embedded in high-dimensional space.
- Evidence anchors:
  - [abstract] "The quality of explanations on a manifold can be measured by the extent to which an explanation for a point lies in its tangent space."
  - [section 1] "The intuition is if an explanation lies in the tangent space of the image, the explanation will contain meaningful components of the image [9]."
  - [corpus] Related work (Bordt et al. [9] referenced in paper) validates this hypothesis on image datasets and user studies.
- Break condition: If the data manifold is poorly approximated or the intrinsic dimensionality is close to ambient dimension, tangent/normal space decomposition becomes meaningless.

### Mechanism 2
- Claim: A convolutional autoencoder's decoder Jacobian provides a computable approximation of the tangent space basis.
- Mechanism: The decoder maps from latent space L to manifold M. The Jacobian J_dec(x): T_xL → T_dec(x)M is a linear map computable via backpropagation. The gradients of the decoder outputs with respect to latent coordinates span the tangent space. The normal space basis is computed as the null space of the tangent basis matrix.
- Core assumption: The autoencoder accurately captures the data manifold geometry.
- Evidence anchors:
  - [section 4.1] "The Jacobian of the decoder can be computed via back-propagation [31]. The tangent space of M is spanned by the gradient of dec [9]."
  - [section 2.1] Equations 9-14 formally define the projection maps and tangential alignment measure.
  - [corpus] "Riemannian Integrated Gradients: A Geometric View of Explainable AI" similarly uses geometric structure for explanations, supporting the manifold-based approach.
- Break condition: If the autoencoder has insufficient capacity or poor reconstruction, the tangent space approximation will be inaccurate, leading to incorrect normal space projections.

### Mechanism 3
- Claim: Optimizing the base-point to minimize the normal space component of IG yields tangentially aligned explanations (μ_x > 0.91).
- Mechanism: Define E_x(x') = ½||H_x(x')||² where H_x projects the attribution into the normal space. Minimizing E_x(α) via gradient descent finds a base-point α such that the IG attribution A(x, α, F) lies almost entirely in the tangent space (E_x(α) ≈ 0 implies μ_x → 1).
- Core assumption: A non-trivial solution exists with α ≠ x and the optimization landscape permits convergence.
- Evidence anchors:
  - [section 3, Theorem 1] "A is tangentially aligned at x, with base-point α, if and only if H_x(α) = 0 or, equivalently, if E_x(α) = 0."
  - [section 4.4, Figure 2] Tangentially aligned IG achieves μ_x > 0.91 across MNIST, Fashion-MNIST, FER2013.
  - [corpus] No direct corpus evidence for this specific optimization approach; it appears novel.
- Break condition: If the classifier F has pathological gradient behavior or the optimization gets stuck in local minima, the base-point may not achieve high tangential alignment.

## Foundational Learning

- Concept: Integrated Gradients and path methods
  - Why needed here: IG is the core explainability method being improved; understanding its sensitivity to base-points is essential.
  - Quick check question: Can you explain why IG integrates gradients along a path from base-point to input, and how different base-points change the attribution?

- Concept: Riemannian manifolds, tangent/normal spaces
  - Why needed here: The paper's core contribution relies on projecting explanations into tangent vs. normal spaces of the data manifold.
  - Quick check question: Given a 2D surface embedded in 3D, can you identify vectors that lie in the tangent plane vs. the normal direction at a point?

- Concept: Autoencoder latent spaces and Jacobians
  - Why needed here: The method uses the decoder Jacobian to approximate the tangent space of the data manifold.
  - Quick check question: How does the Jacobian of a decoder relate changes in latent vectors to changes in the output space?

## Architecture Onboarding

- Component map:
  - Autoencoder -> Classifier CNN -> Tangent/Normal Space Computation -> Base-point Optimizer -> Tangential IG

- Critical path:
  1. Train autoencoder on data → obtain decoder
  2. For input x, compute decoder Jacobian → get tangent basis {τ_1...τ_n}
  3. Compute normal basis {ν_{n+1}...ν_d} via null space
  4. Initialize base-point α; run gradient descent to minimize E_x(α)
  5. Compute IG(x, α*, F) with optimized base-point

- Design tradeoffs:
  - **Latent dimension n**: Larger n captures more manifold complexity but increases computation; paper uses n=144 for CIFAR10/FER2013, n=10 for MNIST32/Fashion-MNIST
  - **Gradient descent iterations**: More iterations → higher tangential alignment but O(ε) cost vs. O(1) for fixed base-points
  - **Autoencoder architecture quality**: Better manifold approximation → better tangent space → more meaningful explanations

- Failure signatures:
  - μ_x close to √(n/d) (random baseline) indicates optimization failed or tangent space approximation is poor
  - Explanations appearing as noise suggests base-point is in normal space-dominated region
  - On CIFAR10, standard base-points (zero, uniform, Gaussian) produce μ_x well below random baseline—indicating they actively generate normal-space attributions

- First 3 experiments:
  1. Reproduce Figure 1a on a simpler dataset (MNIST): compare μ_x distributions for zero, uniform, Gaussian, max-distance, and tangential base-points.
  2. Ablate autoencoder quality: train autoencoders with varying latent dimensions and measure correlation between reconstruction error and tangential alignment quality.
  3. Visual sanity check: replicate Figure 2, verifying that tangential IG highlights semantically meaningful regions while standard base-points produce noise-like attributions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Gaussian smoothing parameter ($\sigma$) impact the tangential alignment of integrated gradients?
- Basis in paper: [explicit] The authors state in Section 4.4, "It is the goal of future work to determine the impact of $\sigma$ on tangential alignment," noting that the Gaussian base-point performed better on complex datasets likely due to this parameter.
- Why unresolved: While the paper demonstrates that the Gaussian base-point outperforms other heuristics on datasets like CIFAR10, the specific sensitivity of this performance to the choice of $\sigma$ was not tested or optimized.
- What evidence would resolve it: An ablation study measuring the fraction of explanation in the tangent space ($\mu_x$) across a range of $\sigma$ values on the CIFAR10 and FER2013 datasets.

### Open Question 2
- Question: To what extent does the approximated dimension of the tangent space ($n$) affect the quality of tangential alignment?
- Basis in paper: [explicit] In Section 4.4, regarding the lower performance on CIFAR10, the authors state: "It is the goal of future work to determine if the dimension of the tangent space of CIFAR10 of $n = 144...$ impacts the tangential alignment."
- Why unresolved: The paper fixes the tangent space dimension ($n$) based on parameters from prior work (e.g., $n=144$ for CIFAR10), but it is unclear if this dimension accurately captures the data manifold or limits the alignment potential.
- What evidence would resolve it: A comparative analysis varying the latent dimension of the convolutional autoencoder (thereby changing $n$) and correlating it with the resulting tangential alignment metrics.

### Open Question 3
- Question: What are the rigorous theoretical conditions required for a base-point to guarantee tangential alignment?
- Basis in paper: [explicit] Section 5 concludes, "In future work we seek to further investigate the theoretical conditions a base-point must have to provide tangential explanations."
- Why unresolved: The paper currently provides sufficient conditions based on Gershgorin circle theorems and Hessian definiteness, but a complete theoretical characterization of the necessary conditions for optimal base-point selection remains undeveloped.
- What evidence would resolve it: Deriving formal mathematical proofs that establish necessary and sufficient conditions for tangential alignment, or identifying specific function classes where these conditions fail.

### Open Question 4
- Question: Can adaptive optimization hyperparameters improve the convergence and alignment quality for individual data points?
- Basis in paper: [explicit] The authors note in Section 4.4: "Some points may require a different learning rate and number of iterations to achieve higher tangential alignment. We leave this to future work."
- Why unresolved: The experiments utilized a fixed learning rate and iteration count across all points to approximate the optimal base-point, potentially resulting in sub-optimal explanations for points that are harder to optimize.
- What evidence would resolve it: Implementing an adaptive gradient descent strategy or a per-point hyperparameter search to verify if the maximum achievable $\mu_x$ increases compared to the fixed-parameter approach.

## Limitations

- **Critical parameter specification**: The paper does not specify learning rate, iteration count, or convergence criteria for the base-point optimization, which are crucial parameters affecting results.
- **Autoencoder architecture details**: Exact filter sizes, channel counts, and training hyperparameters are not provided, making exact reproduction difficult.
- **Tangent space approximation dependency**: The method's success heavily depends on the autoencoder accurately capturing the data manifold; poor manifold approximation could lead to incorrect tangent/normal space decomposition.

## Confidence

- **High confidence** in the core theoretical framework and the mathematical validity of tangential alignment as a measure of explanation quality.
- **Medium confidence** in the empirical results, given the consistent performance across four datasets and comparison to multiple baselines.
- **Low confidence** in reproducibility without the specific optimization parameters and detailed autoencoder architecture specifications.

## Next Checks

1. **Ablation study on optimization parameters**: Systematically vary learning rate and iteration count for base-point optimization to determine their impact on tangential alignment quality and computational cost.
2. **Autoencoder quality vs. explanation quality correlation**: Train autoencoders with varying latent dimensions and reconstruction quality, then measure how reconstruction error correlates with achieved tangential alignment (μ_x) values.
3. **Cross-dataset generalization**: Test the method on a fifth dataset (e.g., SVHN or CIFAR100) to verify that the tangential alignment benefits generalize beyond the four datasets studied.