---
ver: rpa2
title: 'Beyond Translation: LLM-Based Data Generation for Multilingual Fact-Checking'
arxiv_id: '2502.15419'
source_url: https://arxiv.org/abs/2502.15419
tags:
- claim
- data
- claims
- fact-checking
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MultiSynFact, a large-scale multilingual
  fact-checking dataset with 2.2M claim-source pairs across Spanish, German, and English.
  The dataset is generated using a pipeline that leverages Large Language Models (LLMs)
  to create claims from Wikipedia knowledge sentences, followed by rigorous filtering
  and validation steps to ensure quality.
---

# Beyond Translation: LLM-Based Data Generation for Multilingual Fact-Checking

## Quick Facts
- **arXiv ID**: 2502.15419
- **Source URL**: https://arxiv.org/abs/2502.15419
- **Reference count**: 28
- **Primary result**: Introduces MultiSynFact, a 2.2M synthetic claim-source dataset for multilingual fact-checking (es, de, en) generated using LLM-based pipeline with dual validation.

## Executive Summary
This paper introduces MultiSynFact, a large-scale synthetic dataset for multilingual fact-checking. The authors develop a pipeline that uses LLMs to generate claims from Wikipedia knowledge sentences, then applies rigorous filtering and validation steps to ensure data quality. The resulting dataset contains 2.2 million claim-source pairs across Spanish, German, and English. The authors demonstrate that incorporating this synthetic data consistently improves fact-checking performance across monolingual, multilingual, and cross-lingual settings, with higher macro F1 scores particularly for Spanish and German. The dataset and framework are open-sourced to support further research in multilingual fact-checking.

## Method Summary
The authors developed a pipeline for generating synthetic fact-checking data. They parse Wikipedia dumps to extract knowledge sentences, then use Mistral-7B-Instruct to generate claims in three categories (supports, refutes, not-info). Claims are filtered through two validation stages: an LLM-based self-evaluation scoring claims on quality, self-containedness, and category alignment, followed by an MNLI-based filter using mDeBERTa-v3 to validate source-claim relationships. The final dataset contains 2.2M instances after filtering, with additional variants including data without MNLI filtering (3.8M instances) and data without comparative prompts (1.7M instances). Models are fine-tuned on mDeBERTa-v3-base with specific hyperparameters.

## Key Results
- MultiSynFact consistently improves fact-checking performance across monolingual, multilingual, and cross-lingual settings
- Macro F1 scores show consistent gains, particularly for Spanish and German, when incorporating synthetic data
- Cross-lingual transfer shows higher gains for linguistically similar languages (es→pt/it) compared to distant ones (es→hi/ja)
- The dual-filtering approach (LLM + MNLI) produces higher quality data than either method alone, as shown in ablation studies

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A dual-validation pipeline combining LLM-based self-evaluation with MNLI filtering produces higher-quality synthetic training data than either method alone.
- **Mechanism:** The pipeline first uses Mistral-7B to both generate and evaluate claims across semantic dimensions (self-contained, support, objective, quality). A separate mDeBERTa-v3 model then independently validates the source-claim relationship by treating it as premise-hypothesis entailment. Claims must pass both filters with aligned category predictions to be retained, reducing noise and mislabeled examples.
- **Core assumption:** NLI task structure is a reliable proxy for fact-checking veracity assessment; LLM self-evaluation scores correlate with genuine downstream training utility.
- **Evidence anchors:** "incorporating rigorous claim validation steps to ensure data quality" (abstract); "We frame the source sentences as premises and the generated claims as hypotheses" (section 3.3).
- **Break condition:** High disagreement between filters, particularly for the "refutes" class, may systematically reduce data volume or introduce filter-specific biases.

### Mechanism 2
- **Claim:** Instructing the generation LLM to produce claims with comparative/superlative structures improves the trained fact-checker's capacity for fine-grained, relational inference.
- **Mechanism:** By explicitly prompting for comparative forms (e.g., "X is larger than Y"), the generation process creates claims that require understanding relationships rather than simple entity-attribute matching. This forces the downstream fact-checking model to learn contrastive reasoning, improving robustness against claims that exploit subtle quantitative or temporal variations.
- **Core assumption:** Training on synthetically generated comparative claims transfers to improved performance on non-comparative real-world fact-checking tasks.
- **Evidence anchors:** "To enhance the sensitivity of models to contrastive examples... we further instruct Mistral-7B to generate claims containing comparative or superlative adjectives" (section 3.2).
- **Break condition:** The mechanism fails if the LLM hallucinates comparisons not grounded in the source text, introducing systematically incorrect labels.

### Mechanism 3
- **Claim:** Large-scale synthetic multilingual data augmentation facilitates cross-lingual transfer, improving fact-checking performance on unseen languages, especially those linguistically similar to the training set.
- **Mechanism:** Pre-training or co-training on 2.2M synthetic claim-source pairs provides a dense, diverse signal that strengthens the multilingual representations in models like mDeBERTa. When fine-tuned on a smaller target dataset, these strengthened representations enable better zero-shot or few-shot transfer to related languages due to shared linguistic features in the embedding space.
- **Core assumption:** Synthetic Wikipedia-based claims capture core reasoning patterns generalizable to real-world misinformation.
- **Evidence anchors:** "...we observe performance gains in Spanish, German, and Latin-based languages, with macro F1 scores above 0.8 for French and Italian" (section 5).
- **Break condition:** Transfer effectiveness drops for linguistically distant languages, indicating the mechanism is bounded by the underlying multilingual model's pre-training.

## Foundational Learning

- **Concept:** Natural Language Inference (NLI)
  - **Why needed here:** The paper uses an MNLI model as a core validation component, treating fact-checking as an entailment task. Understanding NLI labels (entailment, contradiction, neutral) is essential to interpret the pipeline design.
  - **Quick check question:** Given a premise "Paris is the capital of France," what is the NLI label for the hypothesis "France's capital city is Paris"? What about "Paris is in Germany"?

- **Concept:** Cross-Lingual Transfer Learning
  - **Why needed here:** The primary value proposition is multilingual and cross-lingual performance. You must understand how multilingual models (e.g., mDeBERTa) share representations across languages to interpret the experimental results.
  - **Quick check question:** Why might a model trained on Spanish and English data perform better on Italian than on Japanese, even if neither language was in the training set?

- **Concept:** Synthetic Data Augmentation
  - **Why needed here:** The entire dataset is synthetically generated. You need to evaluate when synthetic data helps (low-data scenarios) versus when it might hurt (domain shift, noise).
  - **Quick check question:** If a synthetic dataset has a systematic class imbalance (e.g., fewer "refutes" examples), how might that bias a model trained on it?

## Architecture Onboarding

- **Component map:** Wikipedia dump parser (wikitextparser) → Mistral-7B-Instruct (claim generation) → Mistral-7B (LLM validation) → mDeBERTa-v3 (MNLI validation) → Filter & Merge logic → mDeBERTa-v3-base (fact-check model fine-tuning)

- **Critical path:** The interaction between the LLM Validator and MNLI Validator is critical. A claim is discarded if either filter rejects it or if their predictions disagree. This dual-gate is the primary lever for data quality vs. quantity.

- **Design tradeoffs:**
  - **Data Volume vs. Quality:** The no_mnli_filtering dataset has 3.8M instances, while mnli_filtering has 2.2M. MNLI filtering improves performance on non-English data, suggesting quality matters more than raw volume for transfer.
  - **Generator-Validator Choice:** Using the same model (Mistral-7B) for both generation and LLM validation is efficient but may compound model-specific errors. Using a separate model (mDeBERTa) for MNLI provides an independent signal.

- **Failure signatures:**
  - **Class Imbalance:** The "refutes" class is consistently smaller due to generation and validation difficulty. Monitor per-class F1, not just macro scores.
  - **Topic Drift:** If synthetic data topics don't overlap with evaluation data topics, gains may be limited.
  - **Hallucinated Comparatives:** If the LLM invents a comparison not in the source, the claim will be mislabeled.

- **First 3 experiments:**
  1. **Ablate the MNLI Filter:** Train on no_mnli_filtering vs. mnli_filtering data for a new target language. Measure the delta in macro F1 to quantify the specific contribution of the MNLI validation gate.
  2. **Analyze Filter Disagreement:** Manually inspect claims where the LLM validator and MNLI validator disagree (especially for "refutes"). Categorize error types to determine which filter is the bottleneck.
  3. **Probe Cross-Lingual Transfer by Family:** Group evaluation languages by linguistic family. Plot performance gain from synthetic data against linguistic distance from training languages to visualize transfer boundaries.

## Open Questions the Paper Calls Out

- **Question:** Does the synthetic data generation pipeline maintain effectiveness for low-resource languages where LLM capabilities are limited?
  - **Basis in paper:** "It is crucial to investigate the effectiveness of our approach for low-resource scenarios for future work."
  - **Why unresolved:** The current study focuses on high-resource languages, and low-resource languages require identifying best-performing LLMs through fine-grained analysis.
  - **What evidence would resolve it:** Applying the MultiSynFact pipeline to low-resource languages and evaluating the linguistic quality and fact-checking utility of the resulting claims.

- **Question:** Can instruction-tuning be utilized to generate diverse claims that improve fact-checking system robustness?
  - **Basis in paper:** "explore methods for further improving fact-checking systems, by generating diverse claims (both similar and dissimilar to current benchmarks) via instruction-tuning."
  - **Why unresolved:** The current dataset relies on a standard generation approach, and the specific benefits of instruction-tuning for creating claim diversity remain untested.
  - **What evidence would resolve it:** Comparing models trained on instruction-tuned synthetic claims against the current baseline on diverse, out-of-domain fact-checking benchmarks.

- **Question:** Does incorporating diverse external sources reduce the biases inherent in Wikipedia-based synthetic datasets?
  - **Basis in paper:** "adding diverse resources (e.g., news and fact-checking websites) could reduce potential biases."
  - **Why unresolved:** The current implementation relies exclusively on Wikipedia dumps, limiting domain diversity.
  - **What evidence would resolve it:** Generating a new dataset using news or verified fact-checking sources and analyzing it for bias markers compared to the Wikipedia-based dataset.

## Limitations

- **Generation-Hallucination Risk:** Reliance on a single LLM for both generation and initial validation creates vulnerability to systematic biases and hallucinations that may propagate through the pipeline.
- **MNLI Filter Design:** Using NLI entailment labels as a proxy for fact-checking veracity introduces a semantic gap, as NLI models are not explicitly trained for world knowledge and temporal reasoning required in fact-checking.
- **Cross-Lingual Generalization Bounds:** While gains are observed for linguistically similar languages, the paper lacks evidence for transfer to truly distant languages and doesn't quantify the relationship between linguistic distance and performance gain.

## Confidence

- **High Confidence:** The core experimental finding that incorporating MultiSynFact data improves fact-checking performance is well-supported by reported macro F1 scores and ablation studies.
- **Medium Confidence:** The mechanism by which dual LLM+MNLI filtering improves data quality is plausible but the specific impact of each filter stage is not fully disentangled.
- **Low Confidence:** The extent to which synthetic data captures the true distribution of real-world misinformation is uncertain, as Wikipedia-derived claims may not reflect the nuance and adversarial nature of actual false claims.

## Next Checks

1. **Filter Ablation and Disagreement Analysis:** Perform a controlled ablation study comparing model performance when trained on LLM-filtered only data, MNLI-filtered only data, and combined dual-filtered data. Additionally, manually analyze claims where filters disagree, particularly for "refutes," to identify the source of error and determine which filter is the primary bottleneck.

2. **Linguistic Family Transfer Study:** Group evaluation languages by linguistic family and plot performance gain from MultiSynFact data against quantitative measures of linguistic distance from training languages. This will empirically validate the observed transfer pattern and define the boundaries of cross-lingual generalization.

3. **Hallucination Detection for Comparative Claims:** Design an evaluation to quantify the rate of hallucinated comparisons in synthetic data. For a sample of comparative claims, verify whether the comparative relationship is explicitly supported by the source sentence and calculate the proportion of unsupported, mislabeled, or contradicted claims to assess the risk of introducing systematic noise.