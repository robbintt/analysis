---
ver: rpa2
title: The Medical Metaphors Corpus (MCC)
arxiv_id: '2508.07993'
source_url: https://arxiv.org/abs/2508.07993
tags:
- metaphor
- metaphors
- scientific
- language
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Medical Metaphors Corpus (MCC), the first
  comprehensive dataset of 792 annotated scientific metaphors spanning medical and
  biological domains. The corpus aggregates metaphors from diverse sources including
  peer-reviewed literature, news media, social media, and crowdsourced contributions,
  providing both binary and graded metaphoricity judgments validated through human
  annotation.
---

# The Medical Metaphors Corpus (MCC)

## Quick Facts
- **arXiv ID:** 2508.07993
- **Source URL:** https://arxiv.org/abs/2508.07993
- **Reference count:** 11
- **Primary result:** First comprehensive dataset of 792 annotated scientific metaphors across medical/biological domains with graded metaphoricity judgments and confidence-weighted evaluation benchmarks

## Executive Summary
This paper introduces the Medical Metaphors Corpus (MCC), the first comprehensive dataset of 792 annotated scientific metaphors spanning medical and biological domains. The corpus aggregates metaphors from diverse sources including peer-reviewed literature, news media, social media, and crowdsourced contributions, providing both binary and graded metaphoricity judgments validated through human annotation. Each instance includes source-target conceptual mappings and perceived metaphoricity scores on a 0-7 scale. Evaluation using state-of-the-art language models demonstrates modest performance on scientific metaphor detection, revealing substantial room for improvement in domain-specific figurative language understanding.

## Method Summary
The study constructs MCC by aggregating sentences from nine heterogeneous sources and annotating them through a Qualtrics survey where annotators provide binary metaphoricity judgments and 0-7 metaphoricity ratings. The corpus is processed into a "silver standard" by calculating majority votes for binary labels and mean ratings for metaphoricity scores. Confidence weights are computed from annotator agreement patterns, with tied votes (16.9%) excluded from evaluation. Five proprietary LLMs (GPT-4, o1-preview, o3-mini, Claude Opus 4, DeepSeek) are evaluated zero-shot on the filtered dataset using both standard and confidence-weighted metrics.

## Key Results
- MCC contains 792 annotated metaphors with source-target conceptual mappings and 0-7 metaphoricity ratings
- LLMs show conservative precision-recall profiles (high precision 0.71-0.79 but lower recall) on scientific metaphor detection
- Confidence-weighted evaluation reveals 3-4.6% performance improvement across all models on high-consensus items
- Scientific terminology with latent metaphorical readings and context-dependent domain phrases drive human disagreement

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Metaphoricity exists on a gradient continuum rather than as a binary property, and capturing this gradation improves evaluation validity.
- **Mechanism:** The annotation protocol elicits both binary judgments (M/L) and perceived metaphoricity scores (0–7 scale) from multiple annotators. The mean metaphoricity rating serves as a continuous signal, while binary labels enable classification benchmarking. The strong separation between ratings for metaphorical (μ = 3.41) versus non-metaphorical expressions (μ = 0.16) confirms that annotators perceive a spectrum.
- **Core assumption:** Annotators can reliably distinguish degrees of figurativeness; the gradient reflects genuine cognitive perception rather than random noise.
- **Evidence anchors:**
  - [abstract] "Each instance includes source-target conceptual mappings and perceived metaphoricity scores on a 0–7 scale."
  - [Section 6] "Sentences judged metaphorical receive substantially higher metaphoricity ratings than non-metaphorical ones (μMETA = 3.41 vs. μNON = 0.16, Δ = 3.25 points), a pattern that holds for 95% of question pairs."
  - [corpus] Neighboring work (Figurative Archive, MultiCMET) similarly adopts graded norming, suggesting cross-domain validity of continuum-based approaches.

### Mechanism 2
- **Claim:** Confidence-weighted evaluation metrics reveal that LLM performance is systematically better on high-consensus items, exposing a specific failure mode at boundary cases.
- **Mechanism:** For each annotated item, compute `confidence = max(yes_count, no_count) / total_votes`. Transform confidence ∈ [0.5, 1.0] to weight ∈ [0.0, 1.0]. Items with tied votes (16.9% of corpus) receive zero weight and are excluded from weighted metrics. The consistent 3–4.6% improvement in weighted accuracy across all tested LLMs indicates models handle clear-cut cases reliably but struggle where humans disagree.
- **Core assumption:** Human disagreement signals genuine ambiguity rather than annotation error; excluding ties does not systematically bias evaluation.
- **Evidence anchors:**
  - [Section 7.1] "This linear mapping transforms confidence scores from the range [0.5, 1.0] to weights in [0.0, 1.0], ensuring that items with perfect consensus receive full weight."
  - [Section 8] "The consistent 3–4.6% improvement across all models when weighted by human consensus demonstrates that current LLMs perform systematically better on cases where humans strongly agree."
  - [corpus] Related metaphor detection benchmarks (VUA, MOH-X) do not explicitly model annotator confidence, limiting comparability.

### Mechanism 3
- **Claim:** Scientific metaphors pose distinct challenges for LLMs due to domain-specific conventionalization and technical terminology with latent metaphorical readings.
- **Mechanism:** The corpus aggregates metaphors from heterogeneous channels (peer-reviewed literature, news, social media, crowdsourcing), preserving source–target mappings per Conceptual Metaphor Theory (CMT). LLMs tested in zero-shot settings show conservative precision–recall profiles: high precision (0.71–0.79) but lower recall, indicating a bias toward literal interpretation unless explicit figurative cues are present. Boundary cases—scientific terminology with possible metaphorical readings, highly lexicalized expressions, and context-dependent domain phrases—drive both human and model uncertainty.
- **Core assumption:** Zero-shot evaluation approximates real-world deployment where domain-specific training data is scarce; performance gaps generalize to other scientific subdomains.
- **Evidence anchors:**
  - [Section 8] "The conservative precision–recall profiles observed across all models (high precision but low recall for metaphor detection) reflect a systematic bias toward literal interpretation."
  - [Section 6] "The highest binary disagreement (perfect 50/50 splits) arises in three main situations: (i) scientific terminology with a possible metaphorical reading (e.g., 'drug transport'), (ii) highly lexicalised conventional metaphors, and (iii) domain-specific phrases."
  - [corpus] Cross-cultural metaphor datasets (Cultural Bias Matters benchmark) suggest domain and cultural specificity compound detection difficulty, though direct comparison to MCC is not available.

## Foundational Learning

- **Conceptual Metaphor Theory (CMT):**
  - **Why needed here:** MCC structures all annotations around source–target domain mappings (e.g., DISEASE IS AN ENEMY). Understanding CMT is prerequisite to interpreting dataset labels and designing metaphor-aware systems.
  - **Quick check question:** Given the sentence "The virus invades the lungs," identify the source domain, target domain, and the mapped relation.

- **Inter-Annotator Agreement Metrics (Fleiss' κ, Pearson r):**
  - **Why needed here:** The paper reports κ = 0.23 (binary) and Pearson r = 0.4 (Likert), framing these as expected for subjective figurativeness judgments rather than failure. Knowing how to interpret these metrics is essential for quality assessment.
  - **Quick check question:** A Fleiss' κ of 0.23 indicates what level of agreement? What does a Pearson r of 0.4 suggest about rating correlation?

- **Confidence-Weighted Evaluation Design:**
  - **Why needed here:** The paper introduces a custom weighting scheme to handle annotator disagreement. Understanding how weights are computed and applied is necessary to reproduce or extend the evaluation.
  - **Quick check question:** For an item with 4 "yes" votes and 2 "no" votes, compute the confidence score and the corresponding weight using the paper's linear mapping.

## Architecture Onboarding

- **Component map:**
  - Data Collection Layer: Nine curated sources (literature, news, social media, crowdsourcing) → standardized sentence extraction with provenance tags
  - Annotation Layer: Qualtrics survey → binary judgment + 0–7 metaphoricity rating → minimum 2 annotators per item
  - Quality Control Layer: Consistency filtering (literal → must score 0), manual empty-submission removal, inter-annotator agreement calculation
  - Silver Standard Construction: Majority vote for binary label, mean for metaphoricity, confidence-weight computation, tie exclusion (134 items)
  - Evaluation Layer: Zero-shot LLM prompting → predictions → standard + confidence-weighted metrics (accuracy, precision, recall, F1)

- **Critical path:**
  1. Source aggregation and sentence extraction (preserving CMT mappings)
  2. Recruiting annotators with adequate English proficiency and providing explicit metaphor/metaphoricity instructions
  3. Computing disagreement metrics to identify boundary cases
  4. Building the confidence-weighted silver standard
  5. Running LLM evaluation and comparing weighted vs. unweighted performance

- **Design tradeoffs:**
  - Breadth vs. depth: Aggregating from nine heterogeneous sources maximizes domain and genre coverage but introduces variable annotation density per source (Van Rijn-van Tongeren contributes 455 items; Metamia contributes 16)
  - Binary vs. graded: Including both captures categorical and intensity signals, but doubles annotation effort and complicates silver standard interpretation
  - Tie handling: Excluding 16.9% of items (ties) yields a cleaner evaluation signal but discards potentially informative ambiguity data

- **Failure signatures:**
  - Low inter-annotator agreement on specific sources (e.g., Gibbs Jr and Franks interviews show extreme rating variability: 0.00–6.75)
  - Systematic model underperformance on items with scientific terminology possessing latent metaphorical readings
  - Weighted metrics that diverge sharply from standard metrics, indicating model brittleness on ambiguous cases

- **First 3 experiments:**
  1. **Baseline reproduction:** Run the five evaluated LLMs (GPT-4, o1-preview, o3-mini, DeepSeek, Claude Opus 4) on the 589 non-tie items, confirming reported accuracy (0.655–0.716) and weighted accuracy (0.690–0.758) ranges
  2. **Disagreement analysis:** Isolate the 134 tie items and run qualitative error analysis—determine whether models predict "yes" or "no" and compare to annotator rationales
  3. **Domain stratification:** Subsample by source channel (literature vs. news vs. social media) and compute per-channel metrics to identify whether LLMs perform differently on formal vs. informal scientific discourse

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** To what extent can supervised fine-tuning on the MCC dataset mitigate the "conservative bias" (high precision, low recall) observed in zero-shot LLM evaluations?
- **Basis in paper:** [explicit] The authors state that LLMs exhibit a "systematic bias toward literal interpretation" and list "fine-tuning or continued pre-training on the MCC dataset" as a promising avenue for future work.
- **Why unresolved:** The current study only evaluates zero-shot performance; it does not test whether training on the domain-specific data improves recall for subtle scientific metaphors.
- **What evidence would resolve it:** Comparative benchmarks of models before and after fine-tuning on MCC, specifically analyzing shifts in the precision-recall trade-off.

### Open Question 2
- **Question:** Can neurosymbolic approaches (integrating LLMs with symbolic ontologies) outperform purely neural models in resolving context-dependent scientific metaphors?
- **Basis in paper:** [explicit] Section 8.1 identifies "integrating symbolic ontologies with LLMs to bias inference toward structured, yet context-based metaphor understanding" as a promising research avenue.
- **Why unresolved:** The paper evaluates standard LLMs but does not implement or test hybrid logic-augmented systems against the MCC benchmark.
- **What evidence would resolve it:** A comparative study evaluating a neurosymbolic model against baseline LLMs on the MCC dataset, particularly on items with high annotator disagreement.

### Open Question 3
- **Question:** Does the inclusion of fine-grained annotation dimensions (such as creativity or appropriateness) improve the utility of metaphor generation systems for patient-centered communication?
- **Basis in paper:** [explicit] Section 8.2 (Limitations) notes that expanding the corpus with "more fine-grained annotation dimensions (e.g. quality ones: clarity, creativity, appropriateness)" would enhance utility for research applications.
- **Why unresolved:** The current dataset includes metaphoricity scores but lacks these qualitative dimensions, limiting the ability to train models to generate "appropriate" rather than just "present" metaphors.
- **What evidence would resolve it:** A dataset extension with quality ratings and subsequent human evaluation of generated metaphors based on these new criteria.

## Limitations
- The zero-shot evaluation may underestimate domain-specific performance if evaluated LLMs were exposed to biomedical text during pretraining
- Inter-annotator agreement (κ = 0.23) is low but characterized as expected for subjective figurativeness judgments without cross-domain benchmarks
- The exclusion of 134 tied items (16.9%) from evaluation, while methodologically justified, discards potentially informative ambiguity data

## Confidence

**Confidence: High**

- **Data construction methodology:** High - Explicit annotation protocol, provenance tracking, and quality control procedures are detailed
- **Evaluation design:** High - Confidence-weighted metrics are transparently defined and consistently applied across models
- **Performance claims:** High - Results are well-supported by corpus statistics and systematic evaluation
- **Interpretation of agreement:** Medium - Characterization of κ = 0.23 as "expected" lacks cross-domain comparison
- **Generalizability claims:** Medium - Zero-shot evaluation approximates real-world deployment but may not capture full domain gap

## Next Checks

1. **Cross-corpus generalization:** Evaluate the same LLMs on non-scientific metaphor datasets (e.g., VUA, MOH-X) to isolate whether MCC-specific challenges stem from domain or genre
2. **Human expert validation:** Recruit medical domain experts to re-annotate a stratified sample of high-disagreement and scientific-terminology items; compare expert vs. crowdworker consensus patterns
3. **Error analysis by source:** Compute per-source weighted metrics and perform qualitative error analysis to determine whether LLM performance degradation is uniform across channels or concentrated in specific genres (e.g., social media vs. peer-reviewed literature)