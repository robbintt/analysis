---
ver: rpa2
title: Efficient Parametric SVD of Koopman Operator for Stochastic Dynamical Systems
arxiv_id: '2507.07222'
source_url: https://arxiv.org/abs/2507.07222
tags:
- singular
- operator
- koopman
- lora
- nesting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a scalable and numerically stable method for
  learning the top-k singular functions of the Koopman operator for stochastic dynamical
  systems. The key idea is to formulate the learning objective as a low-rank approximation
  problem that avoids numerically unstable linear-algebraic operations (e.g., matrix
  inversion, SVD) required by existing methods like VAMPnet and DPNet.
---

# Efficient Parametric SVD of Koopman Operator for Stochastic Dynamical Systems

## Quick Facts
- **arXiv ID:** 2507.07222
- **Source URL:** https://arxiv.org/abs/2507.07222
- **Reference count:** 40
- **Primary result:** Scalable and numerically stable method for learning top-k singular functions of the Koopman operator, avoiding unstable matrix inversion/SVD operations used by VAMPnet and DPNet.

## Executive Summary
This paper proposes a scalable and numerically stable method for learning the top-k singular functions of the Koopman operator for stochastic dynamical systems. The key idea is to formulate the learning objective as a low-rank approximation problem that avoids numerically unstable linear-algebraic operations (e.g., matrix inversion, SVD) required by existing methods like VAMPnet and DPNet. The proposed LoRA objective is expressed as a simple polynomial in second-moment matrices, enabling efficient and unbiased gradient estimation. Empirical results on benchmark tasks demonstrate that the learned singular subspaces are reliable and effective for downstream tasks such as eigen-analysis and multi-step prediction.

## Method Summary
The paper introduces LoRA (Low-Rank Approximation) as a loss function that learns the top-k singular functions of the Koopman operator for stochastic dynamical systems. Instead of computing unstable matrix operations like SVD or inversion, LoRA minimizes a polynomial in second-moment matrices: $L_{lora} = -2 \text{tr}(T[f,g]) + \text{tr}(M_{\rho_0}[f]M_{\rho_1}[g])$. This formulation enables unbiased gradient estimation from mini-batches and improves training stability. The method includes centering to handle the constant singular function and optional nesting techniques to explicitly order learned singular functions by importance. Inference can be done via CCA+LoRA or EDMD on the learned bases.

## Key Results
- LoRA variants consistently outperform VAMPnet and DPNet on Chignolin molecular dynamics, achieving higher VAMP-E scores and better generalization to test data
- The proposed nesting techniques improve training stability and performance, particularly under challenging conditions with small batch sizes
- On Langevin dynamics, the method accurately recovers eigenfunctions and eigenvalues, demonstrating spectral accuracy
- LoRA eliminates the numerical instability issues (divergence/NaNs) that plague VAMPnet-1 and DPNet during training

## Why This Works (Mechanism)

### Mechanism 1
The LoRA objective enables stable optimization by converting the singular subspace learning problem into a polynomial minimization of second-moment matrices. Unlike VAMPnet or DPNet, which require computing matrix square roots or inverses (unstable operations), the LoRA objective minimizes the Hilbert-Schmidt norm of the approximation error via a simple polynomial expression, avoiding ill-conditioned linear algebra during the backward pass.

### Mechanism 2
Using the LoRA objective allows for unbiased gradient estimation from mini-batches, improving convergence scalability compared to biased inverse-based methods. Standard approaches backpropagate through SVD/inversion of empirical matrices estimated from mini-batches, introducing bias. The LoRA polynomial is linear in the expectations of matrix products, meaning the gradient of the empirical objective is an unbiased estimate of the population gradient.

### Mechanism 3
Nesting techniques (joint or sequential) stabilize training and explicitly order the learned singular functions by modifying the objective to prioritize dominant modes. This forces the network to capture variance in a hierarchical manner (singular value 1, then 2, etc.), preventing the optimizer from "hopping" between modes and ensuring the most important singular functions are learned first.

## Foundational Learning

- **Concept: Koopman Operator Theory**
  - **Why needed here:** The paper frames nonlinear dynamics as a linear operator acting on observables (functions of state). You must understand that the goal is not to predict the next state $x$ directly, but to predict the evolution of observables $f(x)$ linearly.
  - **Quick check question:** If I have a trajectory, am I trying to learn a matrix $K$ such that $x_{t+1} \approx K x_t$, or $f(x_{t+1}) \approx K f(x_t)$? (It is the latter).

- **Concept: Second-Moment Matrices (Covariance & Cross-Covariance)**
  - **Why needed here:** The entire LoRA objective is built on $M$ (auto-covariance) and $T$ (cross-covariance) matrices. You need to grasp that $T[f,g]$ captures the correlation between the current state's encoding $f(x)$ and the future state's encoding $g(x')$.
  - **Quick check question:** In the LoRA loss $-2 \text{tr}(T) + \text{tr}(MM)$, does maximizing the trace of $T$ encourage the embeddings of $x$ and $x'$ to align or diverge? (Align).

- **Concept: Eckart–Young–Mirsky Theorem**
  - **Why needed here:** This theorem is the mathematical justification for Mechanism 1. It proves that minimizing the Frobenius norm (or HS norm) of the reconstruction error of a matrix is equivalent to finding its SVD.
  - **Quick check question:** Why does minimizing $\| K - \sum f_i \otimes g_i \|^2$ yield the top singular functions? (Because low-rank approximation error is minimized by the SVD).

## Architecture Onboarding

- **Component map:** Encoder $f_\theta(x)$ -> Lagged Encoder $g_\theta(x')$ -> Centering (prepend constant 1) -> LoRA Loss Module

- **Critical path:** The implementation of the **Nesting Loss** (Appendix F.1). If using *Sequential Nesting*, you must implement partial stop-gradients: the gradient for the $i$-th row/column of the second-moment matrix must not flow back through the $j$-th component if $i \neq j$.

- **Design tradeoffs:**
  - **Inference Approach 1 (CCA + LoRA) vs. Approach 2 (EDMD):** Approach 1 (CCA) is robust to misalignment but requires an expensive post-hoc SVD step on the embeddings. Approach 2 (EDMD) is faster (just OLS) but performance depends heavily on which basis ($f$ or $g$) is used.
  - **Joint vs. Sequential Nesting:** Joint is theoretically robust for shared weights; Sequential is a strong heuristic for separate networks.

- **Failure signatures:**
  - **VAMPnet-1/DPNet divergence:** Training loss goes to NaN or doesn't converge due to gradient instability (fixed by LoRA).
  - **Constant Mode Collapse:** The model learns $f(x)=1$ for all modes (fixed by explicit centering/regularization).
  - **Mode Swapping:** During training, the index of the "slowest" mode changes; fixed by Nesting.

- **First 3 experiments:**
  1. **Noisy Logistic Map (Sanity Check):** Implement the simple MLP LoRA loss on this low-dimensional system. Verify that the learned singular values match the ground truth (finite rank).
  2. **Ordered MNIST (Prediction):** Test the multi-step prediction capability. This validates the "Inference" step (EDMD vs. CCA) on a structured prediction task.
  3. **Chignolin (Scalability):** Run the SchNet + LoRA implementation. This is the stress test for stability (small batch sizes) and the "nesting" mechanism's impact on convergence speed.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What is the theoretical optimization landscape of the LoRA objective, and can stronger convergence guarantees be established for neural network training?
- **Basis in paper:** [explicit] "A deeper understanding of the landscape of the LoRA objective will also be crucial for advancing this line of work."
- **Why unresolved:** The paper provides a statistical learning guarantee (Theorem C.1) but does not analyze optimization dynamics or non-convex landscape properties for deep networks.
- **What evidence would resolve it:** Theoretical analysis of LoRA loss geometry; empirical convergence rate studies across network architectures and initialization schemes.

### Open Question 2
- **Question:** Why do nesting techniques stabilize training, and can this effect be rigorously characterized?
- **Basis in paper:** [inferred] "We conjecture that the nesting technique helps the parametric models to focus on the most important signals" and Figure 5 shows nesting prevents divergence without batch normalization, but no formal explanation is provided.
- **Why unresolved:** The empirical stabilization is observed but the mechanism (e.g., implicit regularization, gradient structure) remains a conjecture.
- **What evidence would resolve it:** Ablation studies isolating gradient properties; theoretical analysis of sequential vs. joint nesting optimization dynamics.

### Open Question 3
- **Question:** Can the LoRA framework be extended to robustly identify coherent structures in highly non-normal or chaotic dynamical systems?
- **Basis in paper:** [explicit] "robustly identifying coherent structures in highly non-normal or chaotic systems remains an open challenge."
- **Why unresolved:** Current experiments focus on stationary, mostly normal or reversible systems; chaotic systems may have different spectral properties and require different methodological treatment.
- **What evidence would resolve it:** Application to benchmark chaotic systems (e.g., Lorenz, turbulent flows) with comparison to existing Koopman methods; analysis of singular value degeneracy in chaotic regimes.

### Open Question 4
- **Question:** Does the explicit parameterization g(x′) = Pf(x′) with learnable matrix P offer systematic benefits over joint encoder parameterization?
- **Basis in paper:** [explicit] "A systematic study of the benefits of this parameterization is an interesting direction for future work."
- **Why unresolved:** Turri et al. [41] proposed this but the current paper uses separate encoders without comparative analysis.
- **What evidence would resolve it:** Controlled experiments comparing parameterizations on identical tasks; analysis of sample efficiency and expressivity trade-offs.

## Limitations
- The empirical evaluation relies on datasets where ground-truth Koopman eigenfunctions are either analytically known or indirectly validated via downstream prediction tasks, lacking direct spectral verification for complex systems
- The paper's nesting techniques are primarily justified through ablation studies rather than theoretical guarantees
- Improvements over baselines on Chignolin are relatively modest (VAMP-E score gains of ~0.02-0.03), and alternative explanations (e.g., architectural differences) are not fully ruled out

## Confidence

- **High Confidence:** The core claim that LoRA avoids numerically unstable operations (matrix inversion/SVD) is strongly supported by both theoretical analysis and empirical comparison with VAMPnet/DPNet, which are known to suffer from such instabilities.
- **Medium Confidence:** The assertion that LoRA enables unbiased gradient estimation is plausible given the polynomial structure of the loss, but the paper does not provide empirical evidence (e.g., gradient variance comparisons) to fully substantiate this advantage.
- **Medium Confidence:** The superiority of LoRA variants on Chignolin is well-demonstrated, but the improvements over baselines are relatively modest, and the paper does not rule out alternative explanations such as architectural differences beyond the loss function.

## Next Checks
1. **Ground-Truth Eigenfunction Recovery:** For the Langevin task, perform a more rigorous analysis of eigenfunction approximation error (e.g., L2 distance between learned and true eigenfunctions) to quantify the quality of spectral recovery.
2. **Bias-Variance Tradeoff:** Conduct experiments comparing the variance of gradient estimates between LoRA and VAMPnet/DPNet under varying batch sizes to empirically validate the unbiased gradient claim.
3. **Alternative Datasets:** Test LoRA on additional molecular dynamics datasets (e.g., alanine dipeptide) or fluid dynamics systems to assess robustness and generalizability beyond the Chignolin benchmark.