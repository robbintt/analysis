---
ver: rpa2
title: 'Scaling Laws Meet Model Architecture: Toward Inference-Efficient LLMs'
arxiv_id: '2510.18245'
source_url: https://arxiv.org/abs/2510.18245
tags:
- arxiv
- scaling
- inference
- preprint
- size
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work addresses the challenge of designing large language\
  \ models that balance inference efficiency with accuracy under fixed training budgets.\
  \ The authors introduce a conditional scaling law that extends the Chinchilla framework\
  \ by incorporating key architectural factors\u2014hidden size, the MLP-to-attention\
  \ ratio, and grouped-query attention\u2014and develop a search framework for identifying\
  \ optimal architectures."
---

# Scaling Laws Meet Model Architecture: Toward Inference-Efficient LLMs

## Quick Facts
- **arXiv ID:** 2510.18245
- **Source URL:** https://arxiv.org/abs/2510.18245
- **Authors:** Song Bian; Tao Yu; Shivaram Venkataraman; Youngsuk Park
- **Reference count:** 40
- **Primary result:** Introduces conditional scaling laws to optimize LLM architectures for inference efficiency, achieving up to 42% higher throughput and 2.1% better accuracy than LLaMA-3.2 baselines.

## Executive Summary
This work addresses the challenge of designing large language models that balance inference efficiency with accuracy under fixed training budgets. The authors introduce a conditional scaling law that extends the Chinchilla framework by incorporating key architectural factors—hidden size, the MLP-to-attention ratio, and grouped-query attention—and develop a search framework for identifying optimal architectures. Through training over 200 models ranging from 80M to 3B parameters, they validate that their conditional scaling law reliably predicts optimal architectural choices. Optimized models achieve up to 42% higher inference throughput and 2.1% better accuracy compared to LLaMA-3.2 baselines when trained under the same budget.

## Method Summary
The authors extend Chinchilla scaling laws with a conditional formulation that incorporates architectural factors (hidden size $d_{model}$, MLP-to-attention ratio $r$, and grouped-query attention) through multiplicative calibration factors. They train over 200 models across 80M to 3B parameters on Dolma-v1.7 data, using Megatron-LM with LLaMA-3.2 architecture and standard hyperparameters. The scaling law is fitted on small models (80M-145M) and used to predict optimal architectures at larger scales (1B-3B). An architecture search framework combines analytical optimization for hidden size and MLP-ratio with local search for GQA configuration, validated through extensive ablation studies measuring both validation loss and inference throughput.

## Key Results
- Conditional scaling law predicts optimal architectures with MSE 0.0005 (hidden size) and 0.0027 (MLP-ratio)
- Optimized models achieve 42% higher inference throughput versus LLaMA-3.2 baselines
- Models show 2.1% better average accuracy across 9 downstream tasks
- Inference efficiency gains primarily driven by reduced KV cache size and computational FLOPs

## Why This Works (Mechanism)

### Mechanism 1
Increasing hidden size ($d_{model}$) and MLP-to-attention ratio ($r_{mlp/attn}$) reduces attention computational cost and shrinks KV cache, lowering I/O bottleneck during inference. This assumes inference throughput is primarily limited by computational FLOPs and memory bandwidth.

### Mechanism 2
A conditional scaling law models loss as reference optimal loss multiplied by separable calibration factors for $d_{model}/\sqrt{N}$ and $r_{mlp/attn}$. This assumes Chinchilla provides a valid baseline and architectural effects on loss are separable.

### Mechanism 3
Grouped-Query Attention improves inference efficiency by reducing KV cache size through shared heads, but requires local search due to complex relationship with model loss. This assumes consistent throughput gains from KV cache reduction.

## Foundational Learning

- **Concept: Chinchilla Scaling Laws**
  - **Why needed here:** The conditional law augments Chinchilla laws. Understanding model size, data, and compute relationships is prerequisite.
  - **Quick check question:** How does optimal training token count change if you double model size according to Chinchilla?

- **Concept: Key-Value (KV) Cache in Transformer Inference**
  - **Why needed here:** Primary throughput gain mechanism is reducing KV cache size. Understanding its role in attention and memory footprint is essential.
  - **Quick check question:** How does increasing sequence length affect memory required for KV cache?

- **Concept: Grouped-Query Attention (GQA)**
  - **Why needed here:** GQA is a key architectural parameter. Understanding how it differs from Multi-Head and Multi-Query Attention is necessary to interpret its impact.
  - **Quick check question:** In GQA, what is shared between multiple query heads?

## Architecture Onboarding

- **Component map:** Chinchilla Estimator -> Conditional Calibration (hidden size, MLP-ratio) -> Loss Predictor -> Architecture Search (with GQA enumeration)
- **Critical path:** Fitting conditional scaling law parameters ($a_i, b_i$) on small proxy models before any architecture search can occur
- **Design tradeoffs:**
  - Separability vs. Accuracy: Assumes separable effects for $d_{model}$ and $r_{mlp/attn}$ on loss (Equation 3), simplifying model but potentially sacrificing accuracy
  - Generality vs. Specificity: Law parameters shared across all scales for fitting, but ablation suggests refitting on target-scale data may be more accurate
- **Failure signatures:**
  - High MSE/Spearman degradation during cross-validation indicates poor generalization or flawed separability assumption
  - Optimal architecture yields higher actual loss than predicted, suggesting inaccurate law prediction or narrow search space
  - No Pareto-optimal point for GQA search if throughput gains offset by drastic accuracy loss
- **First 3 experiments:**
  1. Validate Inference Throughput: Reproduce Figure 2 ablations by measuring throughput while varying only $d_{model}$, then $r_{mlp/attn}$, then GQA to confirm directional impact
  2. Fit and Cross-Validate: Train 80M and 145M variants, fit conditional law on 80M data, predict 145M loss, calculate MSE and Spearman to validate predictive power
  3. Run Architecture Search: Use fitted parameters to run search framework for 145M target, train predicted optimal vs baseline, compare actual loss and throughput

## Open Questions the Paper Calls Out

### Open Question 1
Can conditional scaling laws extend to Mixture-of-Experts (MoE) architectures to predict optimal expert granularity and active parameter counts? [explicit] Section 7 restricts analysis to dense models, noting uncertainty about extension to MoE. Unresolved because MoE decouples total from active parameters, introducing sparsity factors that alter architectural ratio relationships. Resolution requires fitting law on MoE models with varying expert counts.

### Open Question 2
How do optimal architectural configurations impact performance during post-training stages like supervised fine-tuning or RLHF? [explicit] Section 7 notes analysis limited to pre-training, uncertainty about changes under post-training. Unresolved because architectures optimized for pre-training might exhibit different convergence or alignment capacity compared to baselines. Resolution requires comparing Panda/Surefire vs LLaMA after identical post-training alignment.

### Open Question 3
Do optimal configurations require architecture-specific hyperparameter tuning to maintain accuracy-efficiency trade-off? [explicit] Section 7 mentions adopting prior experimental setup, uncertainty about whether different architectures warrant different hyperparameters. Unresolved because superior performance might rely on specific hyperparameters used; suboptimal tuning for new architectures could misrepresent true potential. Resolution requires hyperparameter search for each variant to verify consistent optimal loss ranking.

## Limitations
- Predictive accuracy for MLP-to-attention ratio weaker than for hidden size (MSE 0.0027 vs 0.0005), suggesting separability assumption may be violated
- Extrapolation from small (80M-145M) to large (1B-3B) models not validated across broader scale ranges or architectural families
- Model applicability limited to LLaMA-3.2-style architectures, restricting generalizability to other architectural families

## Confidence

**High Confidence:**
- Mechanism linking architectural ratios to inference throughput via compute and memory access is well-established and supported by consistent ablation study results
- Conditional scaling law framework as general approach for incorporating architectural factors is validated by successful architecture prediction and measured improvements

**Medium Confidence:**
- Predictive accuracy for hidden size selection is strong, but weaker MLP-ratio performance introduces uncertainty in overall architecture predictions
- Inference efficiency gains are directly measured but depend on specific A100 hardware characteristics

**Low Confidence:**
- Extrapolation of optimal ratios from small to large models has not been validated across broader scales or architectural families
- Claim that single parameter set works across all scales is questionable, as ablation suggests refitting on target-scale data may yield better predictions

## Next Checks

1. **Cross-Scale Validation:** Train intermediate-scale (500M) models using small-model fitted parameters, compare predicted vs fitted-optimal architectures to quantify prediction degradation

2. **Generalization to New Architectures:** Apply fitted conditional law to different architectural family (e.g., different feed-forward designs), measure if loss predictions and throughput gains hold or require refitting

3. **Hardware Dependency Analysis:** Repeat inference measurements on different platforms (H100, CPU), quantify how relative throughput gains from architectural optimizations change across hardware to assess robustness of efficiency claims