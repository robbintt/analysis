---
ver: rpa2
title: 'Loci Similes: A Benchmark for Extracting Intertextualities in Latin Literature'
arxiv_id: '2601.07533'
source_url: https://arxiv.org/abs/2601.07533
tags:
- retrieval
- source
- text
- query
- latin
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Loci Similes is a benchmark dataset for detecting intertextual
  references in Latin literature, containing ~172k text segments and 545 expert-verified
  links between Late Antique and classical authors. The dataset enables evaluation
  of methods that go beyond exact lexical matching to identify semantic and stylistic
  similarities in text reuse.
---

# Loci Similes: A Benchmark for Extracting Intertextualities in Latin Literature

## Quick Facts
- arXiv ID: 2601.07533
- Source URL: https://arxiv.org/abs/2601.07533
- Reference count: 40
- Loci Similes is a benchmark dataset for detecting intertextual references in Latin literature, containing ~172k text segments and 545 expert-verified links between Late Antique and classical authors

## Executive Summary
Loci Similes addresses the challenge of automatically detecting intertextual references in Latin literature, where authors intentionally echo earlier works through varying degrees of lexical similarity. The dataset enables evaluation of methods that go beyond exact lexical matching to identify semantic and stylistic similarities in text reuse. Using this benchmark, we establish baselines for retrieval and classification of intertextualities using state-of-the-art language models, demonstrating their potential while highlighting challenges in distinguishing meaningful reuse from coincidental lexical overlap.

## Method Summary
The approach uses a two-stage pipeline: first, a bi-encoder retrieves candidate intertextual matches from a large corpus using semantic similarity in dense vector space; second, a cross-encoder re-ranks and classifies these candidates to filter false positives. The bi-encoder (E5-large) processes query and source segments independently with prefix markers to distinguish asymmetric roles, while the cross-encoder (XLM-RoBERTa Large) examines token-level interactions between paired segments. Hard negative sampling during training improves discrimination between genuine intertextuality and coincidental overlap.

## Key Results
- E5-large bi-encoder retrieves intertextual references beyond exact lexical matching with Recall@100 of 76.7%
- XLM-RoBERTa Large cross-encoder reduces false positives by 87.6% while maintaining high recall
- The two-stage pipeline achieves Segment-Misclassification Rate (SMR) reduction from 20% to 2.4%
- Manual review workload reduced from 93,700 to 780 candidates (99% reduction)

## Why This Works (Mechanism)

### Mechanism 1: Asymmetric Dense Retrieval via Bi-Encoder
- Claim: Fine-tuned multilingual embeddings can retrieve intertextual references that lack exact lexical overlap by encoding semantic similarity into dense vectors.
- Mechanism: The bi-encoder produces separate embeddings for query and source segments using E5-large. Cosine similarity ranks candidates by semantic proximity. Prefixes ("Query:", "Candidate:") distinguish asymmetric roles in the intertextual relationship.
- Core assumption: Intertextual references share sufficient semantic structure to cluster in embedding space even when surface forms diverge.
- Evidence anchors:
  - [abstract] "Language models offer a promising path forward due to their capability of capturing semantic similarity beyond lexical overlap."
  - [Section 5.1] "Consistent with state-of-the-art dense retrieval approaches (Su et al., 2023), we distinguish the asymmetric roles of text pairs by prepending the prefixes 'Query: ' to target text segments and 'Candidate: ' to source text segments prior to encoding."
  - [corpus] Related work on intertextual parallel detection in Biblical Hebrew (FMR=0.48) supports transferability of transformer-based approaches across historical languages.
- Break condition: Minimal lexical reuse with divergent contexts yields low similarity scores, causing valid references to rank poorly and be discarded before re-ranking.

### Mechanism 2: Cross-Encoder Re-ranking for False Positive Filtering
- Claim: Joint processing of query-candidate pairs via cross-attention reduces spurious matches that bi-encoders cannot distinguish from genuine intertextuality.
- Mechanism: The cross-encoder (XLM-RoBERTa Large) concatenates query and candidate segments into a single input sequence. Self-attention enables token-level interaction modeling, allowing the classifier to detect whether lexical overlap reflects intentional reuse or coincidental phrase co-occurrence.
- Core assumption: Genuine intertextuality exhibits interaction patterns between query and source tokens that differ from random overlap.
- Evidence anchors:
  - [Section 5.2] "Unlike the bi-encoder, which generates static embeddings for query and source segments independently, the cross-encoder processes the query and candidate as a single paired input. This allows the self-attention mechanism to directly compare the two segments at the token level."
  - [Section 8] "The classifier correctly identifies these low-overlap references if they pass the retrieval step."
  - [corpus] Limited direct corpus evidence; related work on quotation detection (ACT algorithm) similarly combines retrieval with verification stages but for different text reuse patterns.
- Break condition: Common collocations (e.g., "puncto temporis") that are semantically plausible as references but lack genuine intertextual intent will generate false positives.

### Mechanism 3: Hard Negative Sampling for Discrimination Calibration
- Claim: Training with semantically similar non-matches forces models to learn finer distinctions between topical similarity and intentional intertextuality.
- Mechanism: Negative examples are sampled using embedding similarity to find "hard negatives"—candidates close in vector space but not true intertextual links. The 1:10 positive-to-negative ratio during training biases models toward conservative predictions.
- Core assumption: The boundary between intertextuality and coincidental overlap can be learned from contrastive examples.
- Evidence anchors:
  - [Section 7.2] "Training with hard negatives (⟨qry,sim⟩) minimized error rates more effectively than random pairing. This approach enforces finer semantic distinctions."
  - [Section 7.1] "Performance improved consistently with more negative examples, and the 1:10 ratio yielded optimal results."
  - [corpus] No direct corpus evidence on hard negative sampling for historical text reuse; this is a methodological contribution requiring validation.
- Break condition: When the distribution of hard negatives does not reflect the true distribution of confounders in scholarly practice, the calibration will not transfer.

## Foundational Learning

- **Bi-Encoder vs. Cross-Encoder Architectures**
  - Why needed here: The two-stage pipeline relies on fundamentally different encoding strategies with distinct computational and representational trade-offs.
  - Quick check question: Can you explain why bi-encoders scale to large corpora but cross-encoders capture finer-grained interactions?

- **Contrastive Learning and Negative Sampling**
  - Why needed here: Model performance hinges on how negative examples are constructed during fine-tuning.
  - Quick check question: What is the difference between random negatives and hard negatives, and when would each fail?

- **Information Retrieval Metrics (Recall@k, MRR, SMR)**
  - Why needed here: Evaluation requires understanding ranking quality and error decomposition for imbalanced data.
  - Quick check question: Why does the paper report SMR (Segment-Misclassification Rate) instead of standard F1 for the full pipeline?

## Architecture Onboarding

- **Component map:** Query corpus (Jerome, Lactantius) → ~83k segments; Source corpus (classical authors) → ~88k segments → E5-large bi-encoder → cosine similarity ranking → top-k candidates → XLM-RoBERTa Large cross-encoder → binary prediction (Reference/No Reference) → threshold filtering → output matches

- **Critical path:** Input preprocessing → embedding generation → top-k retrieval → cross-encoder classification → threshold filtering → output matches

- **Design tradeoffs:**
  - Retrieval depth (k): Higher k improves recall but increases classification workload and false positives
  - Model size: Larger multilingual models outperform Latin-specific models, but at higher computational cost
  - Negative sampling ratio: Higher ratios improve robustness but require more training data

- **Failure signatures:**
  - Low-overlap allusions ranked poorly by bi-encoder → missed true positives
  - Common collocations surviving both stages → false positives
  - Domain mismatch between training negatives and test confounders → calibration drift

- **First 3 experiments:**
  1. **Baseline retrieval only:** Run E5-large on held-out fold, measure Recall@10, @100, @1000 without classification. Establish upper bound on recall.
  2. **Classification ablation:** Fix retrieval at k=100, compare random vs. hard negative sampling strategies. Measure SMR reduction.
  3. **End-to-end workload analysis:** Calculate manual review burden at k=100 with and without re-ranking. Validate the claimed 99% workload reduction (780 vs. 93,700 candidates).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What dedicated architectures can effectively detect subtle "two-word congruencies" that current bi-encoder retrieval models miss?
- Basis in paper: [explicit] Conclusion states: "detecting subtle 'two-word congruencies' remains challenging" and requires "development of dedicated architectures for the detection of intertextualities."
- Why unresolved: Current bi-encoder models produce low similarity scores for valid references with minimal lexical overlap in divergent contexts, causing them to be discarded before the classification stage.
- What evidence would resolve it: New architectures that improve Recall@k for low-overlap pairs without degrading performance on verbatim and paraphrase cases.

### Open Question 2
- Question: How can models better distinguish meaningful intertextual reuse from coincidental lexical overlap of common Latin phrases?
- Basis in paper: [explicit] Discussion notes: "classification models struggle to distinguish meaningful intertextuality from the coincidental use of common phrases."
- Why unresolved: The classifier achieves high recall but low precision, over-generating spurious matches that require manual filtering by scholars.
- What evidence would resolve it: Improved precision (F1) on the benchmark without substantial recall loss; lower Global False-Positive Rate.

### Open Question 3
- Question: Will incorporating additional intertextual link collections (e.g., Dexter et al., Schropp et al.) enable models to generalize across wider stylistic deviations?
- Basis in paper: [explicit] Future Work: "We aim to expand our dataset by incorporating existing collections of intertextual links in Latin literature... to better enable models to capture wide ranges of stylistic deviations."
- Why unresolved: Current 545 verified links may not cover the full spectrum from verbatim quotation to subtle allusion, limiting model robustness.
- What evidence would resolve it: Evaluation on expanded benchmark showing consistent performance across the taxonomy in Figure 2 (verbatim, paraphrase, allusion).

### Open Question 4
- Question: How do ambiguous boundary cases in scholarly annotation affect benchmark reliability and model evaluation?
- Basis in paper: [inferred] Limitations section: "The boundaries between literal citation, subtle allusion, and general thematic resonance are fluid... Many cases exhibit characteristics of multiple categories."
- Why unresolved: Undocumented valid references among "false positives" may skew reported precision, and annotator disagreement affects ground truth consistency.
- What evidence would resolve it: Inter-annotator agreement analysis and comparison of model performance on unambiguously vs. ambiguously classified instances.

## Limitations

- Precision-recall tradeoff remains context-dependent, requiring manual threshold calibration for different scholarly use cases
- Hard negative sampling effectiveness depends on whether negatives reflect true confounding patterns scholars encounter
- Computational scalability claims require validation for production-scale deployment without hardware specifications

## Confidence

**High Confidence Claims:**
- The bi-encoder architecture with multilingual embeddings can retrieve intertextual references beyond exact lexical matching
- Cross-encoders outperform bi-encoders in filtering false positives through token-level interaction modeling
- Hard negative sampling improves discrimination between genuine intertextuality and coincidental overlap

**Medium Confidence Claims:**
- The 1:10 positive-to-negative ratio represents an optimal balance for this specific task
- The reported 99% workload reduction translates to equivalent reduction in manual verification effort
- The framework generalizes to other historical languages with similar text reuse patterns

**Low Confidence Claims:**
- The precise recall values for low-overlap allusions (specific percentages may vary with different evaluation folds)
- The computational efficiency claims for production-scale deployment without hardware specifications
- The direct applicability to non-Latin historical text reuse without validation

## Next Checks

1. **Cross-domain robustness test:** Apply the trained models to a held-out corpus of classical Latin authors (not in the training data) and measure performance degradation. This validates whether the multilingual models truly capture intertextual patterns rather than memorizing specific author styles.

2. **Threshold sensitivity analysis:** Systematically vary the classification threshold from 0.0 to 1.0 and plot precision-recall curves. Identify the threshold that maximizes F1-score for each retrieval depth (k=10, 100, 1000) to understand the precision-recall tradeoff better.

3. **Negative sampling ablation study:** Compare performance using three different negative sampling strategies: random sampling, embedding-based hard negatives, and expert-curated non-matches. This validates whether the computational hard negative approach aligns with scholarly judgment of false positives.