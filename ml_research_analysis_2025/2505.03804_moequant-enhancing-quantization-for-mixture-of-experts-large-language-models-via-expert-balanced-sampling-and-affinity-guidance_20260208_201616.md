---
ver: rpa2
title: 'MoEQuant: Enhancing Quantization for Mixture-of-Experts Large Language Models
  via Expert-Balanced Sampling and Affinity Guidance'
arxiv_id: '2505.03804'
source_url: https://arxiv.org/abs/2505.03804
tags:
- quantization
- experts
- llms
- expert
- moequant
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of post-training quantization
  for Mixture-of-Experts (MoE) large language models, which suffer from accuracy degradation
  due to inter-expert and intra-expert imbalances. The authors propose MoEQuant, a
  framework that introduces Expert-Balanced Self-Sampling (EBSS) to generate balanced
  calibration datasets and Affinity-Guided Quantization (AGQ) to incorporate token-expert
  affinities into the quantization process.
---

# MoEQuant: Enhancing Quantization for Mixture-of-Experts Large Language Models via Expert-Balanced Sampling and Affinity Guidance

## Quick Facts
- arXiv ID: 2505.03804
- Source URL: https://arxiv.org/abs/2505.03804
- Reference count: 26
- Key outcome: Over 10 points accuracy improvement in HumanEval under 4-bit quantization for MoE models

## Executive Summary
This paper addresses the challenge of post-training quantization for Mixture-of-Experts (MoE) large language models, which suffer from accuracy degradation due to inter-expert and intra-expert imbalances. The authors propose MoEQuant, a framework that introduces Expert-Balanced Self-Sampling (EBSS) to generate balanced calibration datasets and Affinity-Guided Quantization (AGQ) to incorporate token-expert affinities into the quantization process. EBSS leverages the model's self-sampling capabilities and uses cumulative probabilities and expert balance metrics to efficiently construct calibration sets, while AGQ integrates gating coefficients into layer-wise calibration to reduce quantization errors. Experiments on models like DeepSeekMoE-16B, Qwen-MoE-14B, and Mixtral-8x7B demonstrate significant performance gains, with over 10 points accuracy improvement in HumanEval under 4-bit quantization, along with 3.2× memory savings and 1.2× inference speedup. MoEQuant effectively bridges the gap between quantization efficiency and MoE model requirements, enabling practical deployment on resource-constrained devices.

## Method Summary
MoEQuant consists of two plug-and-play components: Expert-Balanced Self-Sampling (EBSS) for generating balanced calibration data and Affinity-Guided Quantization (AGQ) for incorporating token-expert affinities into quantization. EBSS uses beam search with probability-guided pruning to generate calibration sequences that optimize both perplexity and expert balance, reducing search complexity from O(m^n) to O(wn). AGQ modifies the quantization loss by weighting it with gating coefficients, effectively scaling the Hessian matrix as H=(X·√c)(X·√c)^T. The framework is integrated with standard PTQ methods like GPTQ and AWQ, using symmetric uniform per-channel quantization. EBSS generates calibration data via autoregressive sampling, while AGQ applies affinity weighting during the layer-wise quantization process, resulting in improved accuracy while maintaining the memory and speed benefits of quantization.

## Key Results
- Over 10 points accuracy improvement in HumanEval under 4-bit quantization for DeepSeek-MoE-16B
- 3.2× memory savings and 1.2× inference speedup compared to FP16
- Expert-Balanced Self-Sampling reduces expert imbalance from 2.01 to 0.83 in DeepSeek-MoE-16B
- Combined EBSS+AGQ improves average accuracy by ~2.6% on 7 reasoning tasks

## Why This Works (Mechanism)

### Mechanism 1: Expert-Balanced Self-Sampling (EBSS)
Constructing calibration datasets with balanced expert usage reduces quantization error for underutilized experts. EBSS generates samples via the model's own sampling process, using cumulative probability and expert balance metrics to guide a beam-style search. It jointly optimizes for low perplexity (distribution alignment) and balanced expert activation, reducing search complexity from O(m^n) to O(wn) via probability-guided path pruning.

### Mechanism 2: Affinity-Guided Quantization (AGQ)
Weighting quantization error by token-expert affinity scores preserves more of the expert's functional output during compression. For each token routed to expert E with affinity coefficient c_i, AGQ scales the token's contribution to quantization loss by c_i. This propagates through FFN layers, and for Hessian-based methods like GPTQ, the Hessian becomes H = (X·√c)(X·√c)^T, giving higher-weight tokens more influence on weight updates.

### Mechanism 3: Orthogonal Integration with Existing PTQ Methods
EBSS and AGQ are modular components that improve any layer-wise PTQ method without architectural changes. EBSS replaces the calibration dataset while AGQ modifies the loss/Hessian computation. Neither changes the quantization algorithm itself (e.g., GPTQ's error compensation or AWQ's activation-aware smoothing).

## Foundational Learning

- **Mixture-of-Experts Routing and Gating**
  - Why needed: Understanding how tokens are routed to experts via softmax gating and how outputs are aggregated as weighted sums is essential to grasp why inter/intra-expert imbalance affects quantization.
  - Quick check: Given a token x routed to experts E_1, E_2 with gating scores 0.8 and 0.2, what is the final output?

- **Post-Training Quantization (PTQ) Basics**
  - Why needed: MoEQuant builds on standard PTQ methods (GPTQ, AWQ); you need to understand layer-wise calibration, Hessian-based error compensation, and symmetric uniform quantization.
  - Quick check: What is the role of calibration data in PTQ, and how does the Hessian matrix guide weight updates in GPTQ?

- **Perplexity as Distribution Alignment Metric**
  - Why needed: EBSS uses perplexity to ensure generated calibration data matches the model's learned distribution.
  - Quick check: Lower perplexity indicates what about the relationship between calibration data and model distribution?

## Architecture Onboarding

- **Component map:**
  Input tokens → MoE Layer → Gating Network (produces affinity scores c_i) → Shared Experts (always active) → Routing Experts (top-k selected)
  Calibration Pipeline: [EBSS: Self-sampling → Beam search w/ PPL + balance scores] → Balanced calibration set → [AGQ: Affinity-weighted loss/Hessian] → Standard PTQ method (GPTQ/AWQ) → Quantized weights

- **Critical path:**
  1. Generate calibration data using EBSS (requires model forward passes, track expert usage per layer)
  2. For each expert's linear layer, compute affinity-weighted activations (X·√c)
  3. Run standard PTQ (GPTQ/AWQ) with modified inputs

- **Design tradeoffs:**
  - Beam width (w): Larger w increases diversity but costs more generation time (Table 7 shows w=4 is near-optimal)
  - Temperature (τ): Controls expert balance vs. perplexity tradeoff (τ=1.2 optimal in experiments)
  - Calibration set size: 128×512 tokens used; larger sets may help but with diminishing returns

- **Failure signatures:**
  - Severe accuracy drop on specific tasks (e.g., code/math) → Likely calibration data doesn't cover those domains
  - C4 perplexity much higher than WikiText2 → Calibration overfit to WikiText2 (use EBSS to fix)
  - Expert usage still highly imbalanced after EBSS → Check τ value or beam width

- **First 3 experiments:**
  1. Replicate baseline: Apply GPTQ/AWQ to a small MoE model (e.g., Mixtral-8x7B) with WikiText2 calibration, measure C4 PPL and task accuracy to confirm degradation.
  2. EBSS ablation: Generate calibration data with EBSS (τ=1.2, w=4), apply standard GPTQ, measure expert balance (σ) and accuracy improvement.
  3. Full MoEQuant: Combine EBSS + AGQ with GPTQ, verify results match Table 1 (e.g., DeepSeek-MoE-16B HumanEval should show ~2-3 point gain over vanilla GPTQ).

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but the methodology raises several important unresolved issues regarding the generalizability and limitations of the approach.

## Limitations
- EBSS calibration data generation process lacks precise specification of termination criteria and exact treatment of shared versus routing experts
- AGQ implementation details are insufficiently described, particularly regarding how gating coefficients are applied during layer-wise quantization
- Modularity claim that EBSS and AGQ can be seamlessly integrated with any existing PTQ method is asserted but not rigorously tested across multiple quantization frameworks

## Confidence

- **High Confidence**: The core diagnosis that MoE models suffer from inter-expert and intra-expert imbalances during quantization is well-supported by related work and experimental observations. The reported memory savings (3.2×) and inference speedup (1.2×) are directly measurable outcomes.

- **Medium Confidence**: The mechanism by which EBSS generates balanced calibration data through probability-guided beam search is plausible and aligns with established sampling techniques, though the specific implementation details remain unclear. The affinity-weighted quantization approach (AGQ) has theoretical grounding but lacks independent validation of the Hessian modification approach.

- **Low Confidence**: The modularity claim that EBSS and AGQ can be seamlessly integrated with any existing PTQ method is asserted but not rigorously tested across multiple quantization frameworks. The optimal hyperparameter choices (beam width w=4, temperature τ=1.2) may be model-specific rather than universally applicable.

## Next Checks

1. **EBSS Implementation Fidelity**: Implement the exact EBSS calibration data generation process and verify that it produces the claimed balanced expert distributions. Measure and compare expert usage statistics (σ) before and after EBSS application across multiple MoE models.

2. **AGQ Hessian Modification Verification**: Independently verify the affinity-weighted Hessian computation by implementing a minimal GPTQ version with AGQ modifications. Compare quantization error distributions with and without affinity weighting on the same calibration data.

3. **Cross-Model Generalization Test**: Apply MoEQuant to a MoE model not used in the original experiments (e.g., LLaMA-MoE or a different variant of DeepSeek/Qwen). Verify that the ~10-point HumanEval improvement and expert balance gains are reproducible across architectures.