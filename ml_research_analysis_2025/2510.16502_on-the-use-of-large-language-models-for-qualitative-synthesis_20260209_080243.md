---
ver: rpa2
title: On the Use of Large Language Models for Qualitative Synthesis
arxiv_id: '2510.16502'
source_url: https://arxiv.org/abs/2510.16502
tags:
- llms
- synthesis
- qualitative
- research
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explores the challenges of using large language models
  (LLMs) for qualitative synthesis in systematic reviews. Through two trials and a
  collaborative autoethnographic approach, the authors evaluate LLM-assisted synthesis
  against criteria for sound qualitative synthesis.
---

# On the Use of Large Language Models for Qualitative Synthesis

## Quick Facts
- arXiv ID: 2510.16502
- Source URL: https://arxiv.org/abs/2510.16502
- Reference count: 40
- Primary result: LLMs struggle with interpretive qualitative synthesis, showing biases and hallucinations, and require as much effort to verify as manual synthesis

## Executive Summary
This study explores the challenges of using large language models (LLMs) for qualitative synthesis in systematic reviews. Through two trials and a collaborative autoethnographic approach, the authors evaluate LLM-assisted synthesis against criteria for sound qualitative synthesis. Key findings reveal that LLMs struggle with interpretation, lack transparency, exhibit inherent biases, and can produce incorrect or superficial outputs. While useful for tasks like paper summaries or descriptive categorizations, verifying LLM-supported synthesis is labor-intensive and often requires as much effort as manual synthesis. The study concludes that, despite their appeal, LLMs are not yet suitable for complex qualitative synthesis tasks without significant human oversight and risk management.

## Method Summary
The authors conducted two qualitative synthesis trials: Trial 1 applied thematic synthesis to two EBSE position papers using ChatGPT-4o with zero-shot and expert persona prompts; Trial 2 performed content analysis/categorization on 12 LLM-QA empirical studies using ChatGPT 5 Thinking and Gemini 2.5 Pro. Both trials used prompt chaining and evaluated outputs against "Characteristics of a Sound QS" (groundedness, relevance, transparency, subjectivity). Trial 2 specifically measured Matthews Correlation Coefficient against a manual baseline. The study employed a collaborative autoethnographic approach with researchers experienced in both qualitative synthesis and LLM usage.

## Key Results
- LLMs perform adequately on narrow aggregative tasks (summaries, basic categorization) when outputs are verifiable against source data
- Human-in-the-loop validation can identify LLM hallucinations but requires effort comparable to manual synthesis
- LLMs struggle with interpretive synthesis requiring conceptual innovation and show systematic biases and transparency issues

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can support narrow, aggregative qualitative synthesis tasks (summaries, descriptive categorizations) when outputs are verifiable against source data.
- Mechanism: Autoregressive generation produces probable continuations that perform adequately for pattern-matching and extraction tasks where success criteria are explicit and grounded in provided text.
- Core assumption: Task success depends on recognition and categorization rather than conceptual innovation or interpretive integration across studies.
- Evidence anchors:
  - [abstract] "While useful for tasks like paper summaries or descriptive categorizations"
  - [section] Table 2, 2nd Trial: "Paper summaries were very good" and RQa categorizations were "relevant and data-grounded"
  - [corpus] Weak evidence; corpus focuses on screening and data extraction, not qualitative synthesis specifically
- Break condition: Tasks requiring interpretation beyond description, reconciliation of contradictory findings, or conceptual innovation cause systematic degradation in output quality.

### Mechanism 2
- Claim: Human-in-the-loop validation can identify and correct LLM hallucinations, but the effort required may equal or exceed manual synthesis.
- Mechanism: Prompt chaining creates intermediate checkpoints that enable step-by-step verification, making the synthesis process more auditable and allowing researchers to detect grounding failures.
- Core assumption: Researchers have sufficient expertise to recognize when LLM outputs diverge from primary study findings.
- Evidence anchors:
  - [abstract] "verifying LLM-supported synthesis is labor-intensive and often requires as much effort as manual synthesis"
  - [section] Finding 11: "Assessing whether a QS result is sound... is labor-intensive... LLMs tend to be more verbose... we frequently had to request corrections"
  - [corpus] "Large Language Models with Human-in-The-Loop Validation" paper validates HITL approach for data extraction, suggesting conditional transfer
- Break condition: Validation breaks down when researchers lack deep familiarity with primary studies, when LLM outputs appear plausible but contain subtle distortions, or when volume of verbiage overwhelms verification capacity.

### Mechanism 3
- Claim: Providing complete methodological context through rich prompting partially mitigates LLMs' inability to follow synthesis methods consistently.
- Mechanism: Few-shot examples and explicit step-by-step instructions create scaffolding that constrains generation toward method-appropriate outputs, reducing (but not eliminating) procedural drift.
- Core assumption: LLMs have encountered sufficient examples of the target synthesis method during pretraining to enable in-context learning.
- Evidence anchors:
  - [abstract] Not directly addressed
  - [section] Finding 6: "Providing rich, precise context is therefore essential, especially for less-used methods that are likely underrepresented in LLM training data. We also found that prompt chaining helped validate intermediate results"
  - [corpus] "Chained Prompting" paper addresses systematic review search strategies, supporting prompt chaining efficacy in related tasks
- Break condition: Novel or rarely-used synthesis methods without adequate representation in training data; tasks requiring iteration that conflicts with linear prompt-response structure.

## Foundational Learning

- Concept: Autoregressive generation (next-token prediction based on learned probability distributions)
  - Why needed here: LLMs do not "understand" or "interpret" - they generate statistically probable continuations, which fundamentally limits their capacity for true conceptual innovation
  - Quick check question: Can you explain why asking an LLM to "interpret findings across studies" is fundamentally different from asking it to "extract findings from each study"?

- Concept: Hallucination (generation of text unfaithful to source input)
  - Why needed here: LLMs lack inherent truth-tracking mechanisms; outputs may appear plausible while containing factual errors or fabrications
  - Quick check question: When an LLM produces a synthesis output, what specific validation steps would detect a hallucinated finding?

- Concept: Qualitative synthesis spectrum (aggregative vs. interpretive approaches)
  - Why needed here: LLM suitability varies dramatically across this spectrum; aggregative tasks are more tractable than interpretive ones requiring conceptual innovation
  - Quick check question: Given a synthesis requiring "problematizing the literature" versus one requiring "thematic summaries," which would you expect LLMs to handle more reliably and why?

## Architecture Onboarding

- Component map: Prompt engineering layer -> LLM inference engine -> Output validation layer -> Human verification checkpoint -> Iterative refinement loop (if needed)

- Critical path: Define synthesis characteristics -> Design prompt strategy with methodological context -> Execute with prompt chaining -> Validate each intermediate output against primary studies -> Assess against sound QS criteria (Table 1)

- Design tradeoffs: Zero-shot vs. few-shot (few-shot improves accuracy but requires quality examples); single-prompt vs. chained (chained improves auditability but increases complexity); open-source vs. commercial (open-source enables reproducibility but requires technical expertise per Finding 9)

- Failure signatures: Outputs not grounded in primary studies; method steps skipped or distorted; superficial themes lacking conceptual depth; findings that appear plausible but misrepresent source material; inability to handle contradictory evidence

- First 3 experiments:
  1. Replicate the paper's finding on paper summaries: Provide 3-5 primary studies to an LLM and evaluate summary quality against manual summaries to establish baseline performance on aggregative tasks.
  2. Test prompt sensitivity: Run the same categorization task with 3 different prompt wordings and measure output consistency to quantify reliability constraints.
  3. Validate hallucination detection: Have an LLM perform content analysis on studies you know well, then systematically identify every instance where outputs diverge from source material to calibrate your verification burden.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can open-source LLMs effectively address the reproducibility and transparency deficits observed in commercial models for qualitative synthesis?
- Basis in paper: [explicit] "...one way to address reproducibility concerns is to use open-source models... further research is needed that... evaluates open-source alternatives"
- Why unresolved: The authors identified reproducibility and "black box" opacity as major challenges (Challenge 9) but did not test open-source alternatives during their trials.
- What evidence would resolve it: A replication of the described trials using open-source models, measuring the consistency of outputs and the ability to audit the generation process compared to the manual baseline.

### Open Question 2
- Question: Do LLM-based tools specifically designed for qualitative synthesis offer improved methodological rigor over general-purpose chatbots?
- Basis in paper: [explicit] "Further research is needed that... evaluates... LLM-based tools specifically designed to support QS."
- Why unresolved: The authors primarily tested general-purpose tools (ChatGPT, Gemini); their single test of a specialized tool (Elicit) failed to support the specific task.
- What evidence would resolve it: Comparative evaluations of purpose-built LLM tools (e.g., research agents) against the "characteristics of a sound QS" criteria defined in the paper.

### Open Question 3
- Question: Can "reasoning" models overcome the inability of standard LLMs to perform interpretive synthesis requiring conceptual innovation?
- Basis in paper: [explicit] "Further research is needed that covers QS with diverse characteristics..." / [inferred] "LLMs do not truly interpret information... [and] seem poor fit for tasks requiring conceptual innovation."
- Why unresolved: The paper notes that LLMs struggle with interpretation (Challenge 7), but acknowledges the emergence of "reasoning models" (Section 3) which were not evaluated for this specific capacity.
- What evidence would resolve it: Trials applying reasoning models to idealist synthesis methods to test if they can generate valid conceptual insights rather than just descriptive aggregations.

## Limitations
- Limited generalizability due to small sample sizes (2 EBSE papers and 12 LLM-QA studies)
- Specific prompt formulations not fully specified, creating reproducibility challenges
- Heavy reliance on researcher judgment rather than objective metrics for interpretive synthesis quality

## Confidence

**High confidence:** LLM performance on narrow, aggregative tasks (summaries, basic categorization) when outputs can be verified against source data
**Medium confidence:** Human-in-the-loop validation requirements and associated effort levels, supported by multiple evidence anchors across trials
**Medium confidence:** Challenges with LLM interpretation and transparency, based on consistent findings across both trials
**Low confidence:** Generalizability to novel synthesis methods or interpretive synthesis approaches not tested in the trials

## Next Checks
1. Replicate the paper summary task using 3-5 diverse primary studies with different LLM models and prompt variations to establish baseline performance boundaries
2. Test hallucination detection sensitivity by having LLM perform content analysis on known studies and systematically cataloging all deviations from source material
3. Evaluate prompt chaining effectiveness by comparing single-prompt vs. chained approaches on identical synthesis tasks, measuring both output quality and verification effort