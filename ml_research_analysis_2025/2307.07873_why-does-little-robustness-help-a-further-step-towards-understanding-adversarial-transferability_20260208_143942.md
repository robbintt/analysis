---
ver: rpa2
title: Why Does Little Robustness Help? A Further Step Towards Understanding Adversarial
  Transferability
arxiv_id: '2307.07873'
source_url: https://arxiv.org/abs/2307.07873
tags:
- transferability
- adversarial
- gradient
- smoothness
- similarity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper provides a deeper understanding of adversarial transferability\
  \ by focusing on surrogate model aspects. The authors attribute the \"little robustness\"\
  \ phenomenon\u2014where models adversarially trained with small perturbation budgets\
  \ serve as better surrogates\u2014to a trade-off between model smoothness and gradient\
  \ similarity."
---

# Why Does Little Robustness Help? A Further Step Towards Understanding Adversarial Transferability

## Quick Facts
- arXiv ID: 2307.07873
- Source URL: https://arxiv.org/abs/2307.07873
- Reference count: 40
- This paper attributes "little robustness" in adversarial transferability to a trade-off between model smoothness and gradient similarity, proposing SAM&IR/JR to optimize both.

## Executive Summary
This paper investigates the "little robustness" phenomenon in adversarial transferability, where models adversarially trained with small perturbation budgets serve as better surrogate attackers. The authors identify a trade-off between model smoothness (flatness of loss landscape) and gradient similarity (alignment of attack directions) as the key factor. They show that small-budget adversarial training improves smoothness but introduces data distribution shifts that degrade gradient similarity. To address this, they propose integrating Sharpness-Aware Minimization (SAM) and input regularization techniques, demonstrating significant improvements in transferability across multiple datasets and architectures.

## Method Summary
The authors propose a surrogate model construction approach based on the trade-off between model smoothness and gradient similarity. They train surrogates using combinations of SAM and input/jacobian regularization (IR/JR) to simultaneously optimize both factors. The method is evaluated on CIFAR-10 and ImageNette datasets using ResNet architectures, with transferability measured against various black-box target models through PGD, MI-FGSM, and DIM attacks.

## Key Results
- Small-budget adversarial training maximizes transferability by balancing smoothness gains and similarity retention
- SAM and input regularization significantly improve both smoothness and gradient similarity
- The proposed SAM&IR and SAM&JR methods achieve state-of-the-art transferability results
- Data distribution shifts from off-manifold samples degrade gradient similarity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adversarial transferability is regulated by a trade-off between model smoothness and gradient similarity.
- Mechanism: Small-budget adversarial training implicitly regularizes loss curvature (improving smoothness) but introduces data distribution shifts (degrading gradient similarity). As the budget increases, smoothness gains saturate while similarity continues to drop, creating a "transferability circuit" where optimal transfer occurs at intermediate robustness.
- Core assumption: The theoretical lower bound linking transferability to these factors (Theorem 1) holds empirically for deep networks.
- Evidence anchors:
  - [abstract] "attribute it to a trade-off between two predominant factors: model smoothness and gradient similarity."
  - [section 2.3] "conjecture that... the trade-off between model smoothness and gradient similarity is strongly correlated with the transferability."
  - [corpus] "Seeking Flat Minima over Diverse Surrogates..." supports the focus on flat minima (smoothness) for transferability.
- Break condition: If the target model shares the exact same shifted data distribution as the surrogate, the similarity penalty may vanish, potentially reversing the trade-off.

### Mechanism 2
- Claim: Data distribution shifts caused by off-manifold samples impair gradient similarity.
- Mechanism: Adversarial training and certain augmentations (Mixup, Cutmix) introduce off-manifold samples ($x+\delta \notin M$). Surrogates trained on this shifted distribution ($D'$) learn gradient directions ($\nabla \ell_F$) that misalign with target models trained on the natural distribution ($D$).
- Core assumption: Assumption: Adversarial examples and strong augmentations push data off the underlying natural data manifold.
- Evidence anchors:
  - [section 2.3] Hypothesis 1: "data distribution shift in adversarial training explains the degradation of gradient similarity."
  - [section 3.2] "results... support the hypothesis that data distribution shift impairs gradient similarity."
  - [corpus] Evidence is primarily internal to the paper; external corpus links are weak for this specific mechanism.
- Break condition: If the target model is also trained on the same augmented data ($D'$), similarity degradation may not occur (as seen in Table 7 where $D'$-surrogates align well with $D'$-targets).

### Mechanism 3
- Claim: Sharpness-Aware Minimization (SAM) promotes superior gradient similarity compared to standard training.
- Mechanism: Unlike standard SGD or input regularization, SAM seeks flat minima in weight space, which correlates with a general alignment of input gradients towards diverse solutions. This effectively simulates attacking an ensemble of models, boosting transferability.
- Core assumption: Assumption: SAM functions as a relaxation of Bayesian model averaging, capturing a distribution of valid solutions.
- Evidence anchors:
  - [section 5.1] "SAM yields the best gradient alignments when input regularizations are not involved."
  - [section 6 Q7] "SAM as an optimal relaxation of Bayes object... attacking a SAM solution is somewhat equivalent to attacking an ensemble."
  - [corpus] "Seeking Flat Minima over Diverse Surrogates..." validates the "flat minima" approach to transferability.
- Break condition: If the surrogate architecture or loss landscape prevents SAM from finding a suitable flat basin (e.g., extreme non-convexity), the alignment benefit may be lost.

## Foundational Learning

- Concept: **Model Smoothness (Input Space)**
  - Why needed here: A core metric defined as the dominant eigenvalue of the Hessian ($\nabla^2_x \ell$). Lower values indicate a flatter loss surface, meaning gradients are more stable around the input.
  - Quick check question: How does penalizing the input gradient norm (IR) directly affect the Hessian eigenvalues?

- Concept: **Gradient Similarity (Cosine)**
  - Why needed here: Measures the alignment between the surrogate's attack direction ($\nabla_x \ell_F$) and the target's vulnerable direction ($\nabla_x \ell_G$). High similarity suggests the surrogate's attack vector is generalizable.
  - Quick check question: Why might two models trained on different data distributions (e.g., normal vs. adversarial) have low gradient similarity?

- Concept: **Sharpness-Aware Minimization (SAM)**
  - Why needed here: An optimization technique that simultaneously minimizes loss value and loss sharpness (max perturbation within radius $\rho$). It is a key tool for boosting transferability without data distribution shifts.
  - Quick check question: How does SAM differ from explicit gradient regularization (ER) in terms of where the regularization pressure is applied?

## Architecture Onboarding

- Component map: Surrogate Training (SAM&JR) -> Attack Generation (PGD) -> Transfer Evaluation (ASR on Target)
- Critical path: Train Surrogate with SAM&JR → Generate AEs via PGD → Evaluate ASR on Target
- Design tradeoffs:
  - **Input Reg (IR/JR)** vs. **SAM**: IR provides the best smoothness but moderate similarity. SAM provides superior similarity and good smoothness. Combining them (SAM&JR) yields the highest transferability but adds training complexity.
  - **Robustness vs. Transferability**: High robustness (large $\epsilon$ AT) destroys transferability; "little robustness" (small $\epsilon$) maximizes it.
- Failure signatures:
  - **Low ASR on high $\epsilon$ AT surrogates:** Indicates the "transferability circuit" has broken due to destroyed gradient similarity (distribution shift).
  - **High Variance in Transfer:** Suggests the surrogate lies in a sharp minimum (low smoothness) or the augmentation magnitude is unstable.
- First 3 experiments:
  1. Reproduce the "Little Robustness" curve (Fig 2) by training surrogates with varying AT budgets ($\epsilon \in [0, 1]$) and plotting transfer ASR.
  2. Compare the "Intra-architecture Similarity" (Fig 11) of a SAM-trained model vs. a Standard model to validate the gradient alignment hypothesis.
  3. Integrate SAM&JR with an ensemble method (LGV) to verify if the "flat minima" property of SAM complements geometric vicinity strategies.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact theoretical mechanism by which Sharpness-Aware Minimization (SAM) yields superior gradient alignment towards every training solution?
- Basis in paper: [explicit] The authors state their interpretation of SAM's alignment effect is "merely intuitive and speculative" and that "uncovering the exact reason is far beyond the scope of our study" (Sec. 6, Q7).
- Why unresolved: The paper empirically observes that SAM aligns gradients better than standard training (Fig. 11), likely due to an "expectation" effect similar to Bayesian learning, but lacks a rigorous theoretical derivation connecting SAM to general gradient similarity.
- What evidence would resolve it: A theoretical proof linking SAM's optimization of flat minima to the cosine similarity of gradients across the loss landscape, or ablation studies isolating the "Bayes relaxation" properties of SAM.

### Open Question 2
- Question: Can the proposed surrogate construction methods (e.g., SAM&JR) be integrated with other attack strategies, such as loss object modification or unrestricted generation, to further boost transferability?
- Basis in paper: [explicit] The authors identify factors like loss object modification and patch-based AEs as contributors to transferability but state, "We leave it to our future work to investigate whether they can be mildly integrated with our design" (Sec. 5.2.3).
- Why unresolved: The study focuses on surrogate training mechanisms (gradient regularization) and generation strategies (MI, DIM), but does not test combinations with loss function modifications or spatially unrestricted attacks.
- What evidence would resolve it: Experimental results combining SAM-trained surrogates with diverse loss functions (e.g., transforming logits) or unrestricted perturbation norms to measure additive improvements in transfer success rates.

### Open Question 3
- Question: How can a unified framework reason adversarial transferability by combining surrogate manipulation, attack generation, and test-time transformations?
- Basis in paper: [explicit] The authors conclude by noting they "leave the exploration and discussion regarding a unified framework that can reason adversarial transferability from all perspectives for future work" (Sec. 7.2).
- Why unresolved: The current work focuses heavily on the surrogate aspect (smoothness vs. similarity). It acknowledges but does not mathematically integrate the roles of attack generation (e.g., momentum) or test-time transformations (e.g., ghost networks) into the transferability lower bound.
- What evidence would resolve it: A comprehensive theoretical model that incorporates generation strategy variance and test-time transformation variance into the smoothness-similarity trade-off bound defined in Theorem 1.

## Limitations
- The theoretical lower bound (Theorem 1) assumes specific data distribution shift scenarios and may not generalize to all surrogate-target pairs
- The mechanism explaining small-budget adversarial training's effects is primarily supported by correlation rather than causal intervention studies
- The analysis of off-manifold samples is largely conceptual without direct empirical validation of manifold distances

## Confidence
- **High Confidence:** The empirical observation that small-budget adversarial training maximizes transferability (Fig 2), and the effectiveness of SAM&JR in improving both smoothness and similarity metrics.
- **Medium Confidence:** The theoretical framework connecting smoothness and similarity to transferability (Theorem 1 and Hypothesis 1). While internally consistent, its universal applicability requires broader validation.
- **Medium Confidence:** The claim that SAM seeks flat minima leading to better gradient similarity. This is supported by experiments but relies on an implicit connection to Bayesian model averaging.

## Next Checks
1. **Intervention Study:** Systematically manipulate data augmentation strength and measure the causal impact on both smoothness and gradient similarity to directly test the distribution shift hypothesis.
2. **Theoretical Stress Test:** Attempt to falsify Theorem 1 by constructing a surrogate-target pair where high robustness surprisingly improves transferability, or where smoothness and similarity are misaligned.
3. **Extended Evaluation:** Validate the SAM&JR findings on a larger-scale dataset (e.g., full ImageNet) and against more diverse target models (e.g., Vision Transformers, MLaaS APIs) to assess scalability and generalizability.