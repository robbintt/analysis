---
ver: rpa2
title: Ensembles provably learn equivariance through data augmentation
arxiv_id: '2410.01452'
source_url: https://arxiv.org/abs/2410.01452
tags:
- equivariant
- neural
- training
- data
- then
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper extends the finding that ensemble neural networks trained
  with data augmentation become equivariant in mean, even when individual networks
  are not. Unlike prior work which relied on the neural tangent kernel limit, the
  authors prove this phenomenon holds for general architectures and training algorithms,
  provided the architecture space is compatible with the group symmetry.
---

# Ensembles provably learn equivariance through data augmentation

## Quick Facts
- arXiv ID: 2410.01452
- Source URL: https://arxiv.org/abs/2410.01452
- Authors: Oskar Nordenfors; Axel Flinth
- Reference count: 40
- Primary result: Proves that ensemble neural networks trained with data augmentation become equivariant in mean, extending beyond the NTK limit to general architectures and training algorithms.

## Executive Summary
This paper establishes theoretical foundations for why ensembles of neural networks trained with data augmentation become equivariant in their mean prediction, even when individual networks are not. The key insight is that if the architecture space is invariant under the group action and the training algorithm is equivariant, then the ensemble mean inherits this symmetry. The authors prove this phenomenon holds for general architectures and training algorithms, not just in the neural tangent kernel limit. They validate their findings through experiments with CNNs and PointNets, demonstrating that ensembles with symmetric filters outperform asymmetric ones on rotated image datasets, and that ensembling increases equivariance in point cloud classification.

## Method Summary
The authors train ensembles of neural networks using SGD with random data augmentation drawn from the Haar measure of the symmetry group. They analyze architectures (CNNs, PointNets) as constrained subspaces of the full parameter space, focusing on whether these architecture spaces are invariant under group actions. The training uses standard initialization with Gaussian weights and LayerNorm activations. For CNNs, they test different filter support patterns (symmetric vs asymmetric), while for PointNets they use random SO(3) rotations. The ensemble size varies from small (5 members) to large (1000 members) to study the scaling of equivariance error.

## Key Results
- Proves ensemble means become equivariant even for non-invariant architectures, provided the architecture space is ρ-invariant and training is equivariant
- Shows logarithmic scaling of equivariance error with ensemble size for finite groups (C₄, C₁₆)
- Demonstrates that invariant architectures (symmetric filters) achieve better equivariance than non-invariant ones, though both benefit from ensembling
- Validates theory with experiments on MNIST (rotated images) and ModelNet10/40 (point clouds)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The ensemble mean becomes equivariant even if individual members are not, provided the training algorithm is equivariant.
- **Mechanism:** When a neural network is trained on fully augmented data using an equivariant algorithm (like gradient descent), the distribution of possible trained models becomes symmetric. While a single sample (individual network) from this distribution may lie off the symmetry manifold, the average of the distribution (the ensemble mean) theoretically centers on the equivariant solution.
- **Core assumption:** The training algorithm is equivariant, and the initialization distribution is invariant under the group action.
- **Evidence anchors:**
  - [abstract] "ensembles of neural networks trained with data augmentation become equivariant in mean"
  - [section 2] Theorem 2.3: "Equivariant training leads to equivariant ensembles"
  - [corpus] "Emergent equivariance in deep ensembles" (prior work cited in intro) established this in the NTK limit; this paper extends it to finite width.
- **Break condition:** If the training algorithm is not equivariant (e.g., specific non-standard optimizers) or if initialization is heavily biased toward non-symmetric weights.

### Mechanism 2
- **Claim:** The architecture space $L$ (the set of permissible weights) must be invariant under the group action ($\rho(g)L \subseteq L$) for the ensemble to guarantee equivariance.
- **Mechanism:** The paper models architectures (CNNs, Transformers) as affine subspaces $L$ of the full parameter space. If a symmetry transformation (e.g., rotation) maps a valid weight configuration to an invalid one (outside $L$), the gradient flow cannot preserve the symmetry. If $L$ is $\rho$-invariant, the projection of the gradient onto $L$ commutes with the group action, preserving the symmetry dynamics throughout training.
- **Core assumption:** The projection operator $\Pi_L$ onto the architecture space commutes with the group action $\rho(g)$.
- **Evidence anchors:**
  - [section 3.1] Definition 5: "L is $\rho$-invariant if $\rho(g)L \subseteq L$"
  - [section 3.1] Lemma 3.2: "L is $\rho$-invariant if and only if $\Pi_L$ is $\rho$-equivariant"
  - [corpus] "Identifiable Equivariant Networks are Layerwise Equivariant" supports the importance of architectural constraints on symmetry.
- **Break condition:** Using architectures where weight sharing or filter support breaks under transformation (e.g., strictly asymmetric filters for rotation tasks), causing $\rho(g)L \not\subseteq L$.

### Mechanism 3
- **Claim:** Mini-batch Stochastic Gradient Descent (SGD) with random data augmentation acts as an equivariant training algorithm.
- **Mechanism:** By sampling augmentations uniformly (according to the Haar measure) during training, the expected gradient update becomes invariant to the specific orientation of the input. This ensures that the probability distribution of the weights evolves symmetrically over time, satisfying the conditions for ensemble equivariance even in stochastic, finite-batch settings.
- **Core assumption:** Augmentations are drawn i.i.d. according to the Haar measure (uniformly over the group), and the loss function is invariant.
- **Evidence anchors:**
  - [section 4.2] Theorem 4.10: "Ensembles... trained with mini-batch SGD with random augmentations are equivariant."
  - [section 4.2] Lemma 4.7: The risk $\mathcal{R}_{aug}$ is invariant under group transformations when augmentation is random.
  - [corpus] "Data Augmentation and Regularization for Learning Group Equivariance" explores the link between augmentation and symmetry.
- **Break condition:** Non-uniform or biased sampling of augmentations; or deterministic augmentation schedules that break the statistical symmetry of the gradient estimator.

## Foundational Learning

- **Concept: Group Equivariance vs. Invariance**
  - **Why needed here:** The paper distinguishes between tasks where outputs rotate with inputs (equivariance) versus staying fixed (invariance). The theoretical proofs rely heavily on representation theory ($\rho(g)$) to define how groups act on network weights and feature spaces.
  - **Quick check question:** If a network is invariant to rotation, what is the output of `Network(Rotated_Image)` compared to `Network(Image)`?

- **Concept: The Architecture Space ($L$)**
  - **Why needed here:** Unlike standard analyses treating networks as generic function approximators, this paper frames specific architectures (CNNs, PointNets) as constraints ($L$) on a broader parameter space. Understanding $L$ is critical to verifying the $\rho$-invariance condition.
  - **Quick check question:** In a CNN, does the set of all possible 3x3 filters form a linear subspace? How does restricting filter support (e.g., symmetric vs. asymmetric) define different architecture spaces?

- **Concept: Haar Measure**
  - **Why needed here:** The theoretical guarantees for stochastic training rely on "random augmentations" being drawn uniformly over the group. The Haar measure formalizes this "uniformity" for continuous and finite groups, ensuring the augmentation process itself is unbiased.
  - **Quick check question:** Why is "randomly rotating by 0 to 360 degrees" preferred over "rotating by 0 or 90 degrees only" when training for continuous rotation equivariance?

## Architecture Onboarding

- **Component map:** Architecture Space ($L$) -> Symmetry Group ($G$) -> Lifted Representation ($\rho$) -> Training Algorithm (SGD + random augmentation)

- **Critical path:**
  1. **Define Symmetry:** Identify the group $G$ relevant to the data.
  2. **Check Architecture:** Verify if the architecture space $L$ is $\rho$-invariant. (e.g., do rotated filters still belong to the valid filter set?).
  3. **Initialization:** Ensure weight distribution is symmetric ($p_0 = p_0 \circ \rho(g)^{-1}$).
  4. **Training:** Train an ensemble of $M$ models using SGD with random augmentation.
  5. **Inference:** Average the outputs of the $M$ models.

- **Design tradeoffs:**
  - **Invariant vs. Non-invariant $L$:** Explicitly equivariant architectures (GDL) satisfy invariance by design but are complex to implement. Ensembles allow "softer" equivariance but require multiple forward passes and storage of $M$ models.
  - **Ensemble Size ($M$):** Theory suggests $O(\log|G|)$ members for finite groups. Smaller ensembles are cheaper but yield "approximate" equivariance (noisy mean).

- **Failure signatures:**
  - **Asymmetric Support:** Using filters that cannot represent rotated versions of themselves (e.g., strictly triangular kernels for 90-degree rotation tasks) causes the ensemble mean to drift from perfect equivariance.
  - **Biased Augmentation:** If augmentation misses parts of the group, the ensemble will fail to learn equivariance for those missing transformations.

- **First 3 experiments:**
  1. **Baseline Ensemble:** Train a standard MLP ensemble on MNIST with full rotation augmentation. Measure the "equivariance error" of the mean prediction vs. single models to verify the basic "emergence" phenomenon.
  2. **Architecture Ablation ($L$-invariance):** Train two CNN ensembles: one with rotation-invariant filter supports (e.g., circular/plus-shape) and one with asymmetric supports (e.g., strict triangles). Compare their equivariance error on a rotated dataset to validate Mechanism 2.
  3. **Sample Complexity:** Measure the decay of equivariance error as the ensemble size $M$ increases. Check if the error scales logarithmically with group size $|G|$ (e.g., compare $C_4$ vs $C_{16}$) as predicted in Section 4.3.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the theoretical guarantees for equivariant ensembles be extended to non-compact groups, such as the group of rigid motions $SE(d)$?
  - **Basis in paper:** Explicit. Section 1.2 ("Limitations") states the assumption that the group $G$ is compact excludes $SE(d)$.
  - **Why unresolved:** The proofs rely on the Haar measure being normalizable to a probability measure, a property that holds for compact groups but requires specific handling for non-compact ones.
  - **What evidence would resolve it:** An extension of Theorem 4.10 to locally compact groups or empirical validation showing equivariance emergence on $SE(3)$ tasks.

- **Open Question 2:** How does the violation of equivariance in non-linear activation functions (e.g., using ReLU instead of tanh) impact the emergence of equivariance in deep ensembles?
  - **Basis in paper:** Explicit. Section 1.2 ("Limitations") and Assumption 1 note that all non-linearities $\sigma_i$ are assumed to be equivariant.
  - **Why unresolved:** The theoretical results depend on Lemma 2.1, which requires equivariant non-linearities to show the network is equivariant to the lifted representation.
  - **What evidence would resolve it:** Theoretical bounds quantifying the equivariance error when using non-equivariant activations, or empirical studies comparing equivariance emergence between ReLU and equivariant non-linearities.

- **Open Question 3:** Can the "approximate equivariance" observed in non-invariant architectures be formally quantified by the commutator norm between the projection operator and the group action?
  - **Basis in paper:** Inferred. Section 5.1 notes that non-invariant architectures (like asymmetric CNN filters) become surprisingly equivariant, speculating that $\rho(g)\Pi_L \approx \Pi_L\rho(g)$.
  - **Why unresolved:** The current theoretical framework provides a binary condition (invariant vs. non-invariant), whereas experiments suggest a continuous spectrum of equivariance based on the geometry of $L$.
  - **What evidence would resolve it:** A correlation analysis showing that the magnitude of the commutator $||\rho(g)\Pi_L - \Pi_L\rho(g)||$ predicts the ensemble's final equivariance error rate.

## Limitations

- The theory requires the architecture space $L$ to be ρ-invariant, which may not hold for standard CNNs with asymmetric filters
- The logarithmic scaling bound for finite groups assumes the optimal ensemble member is exactly at the equivariant point, but in practice the ensemble mean only approximates this
- For Lie groups like SO(3), the linear scaling with dimension assumes the initialization covers the full orbit, which may not be realistic with practical initialization schemes

## Confidence

- **High Confidence**: The theoretical framework for understanding when ensembles become equivariant (ρ-invariance of $L$, equivariant training algorithms) is sound and well-supported by the proofs in Sections 3-4
- **Medium Confidence**: The empirical demonstrations show the phenomenon exists, but the numerical results for architecture-dependent equivariance are limited to specific filter patterns without exhaustive ablation
- **Medium Confidence**: The sample complexity bounds are theoretically derived but not thoroughly validated across diverse architectures and group types in the experiments

## Next Checks

1. **Architecture Space Verification**: Systematically test whether rotated asymmetric filters remain within the architecture space constraints for different filter patterns (e.g., 2x2, 3x3, 5x5 with various asymmetric patterns)
2. **Gradient Flow vs SGD Dynamics**: Compare the equivariance error evolution between idealized gradient flow (as used in proofs) and practical SGD with different batch sizes and learning rates to quantify the approximation gap
3. **Continuous Group Scaling**: Extend the finite group experiments (C4, C16) to test the Lie group scaling prediction by training ensembles for SO(2) and SO(3) with varying ensemble sizes to verify linear scaling with dimension