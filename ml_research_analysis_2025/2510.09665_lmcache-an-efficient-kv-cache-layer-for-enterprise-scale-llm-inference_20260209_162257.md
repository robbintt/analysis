---
ver: rpa2
title: 'LMCache: An Efficient KV Cache Layer for Enterprise-Scale LLM Inference'
arxiv_id: '2510.09665'
source_url: https://arxiv.org/abs/2510.09665
tags:
- cache
- inference
- memory
- lmcache
- vllm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LMCACHE addresses the problem of efficiently storing and transferring
  KV caches in LLM inference, as traditional GPU memory storage becomes insufficient
  for growing cache sizes and reuse demands. It introduces a high-performance, modular
  KV caching layer that supports offloading caches to CPU/disk/remote storage and
  transferring them across inference engines and queries.
---

# LMCache: An Efficient KV Cache Layer for Enterprise-Scale LLM Inference

## Quick Facts
- **arXiv ID:** 2510.09665
- **Source URL:** https://arxiv.org/abs/2510.09665
- **Reference count:** 24
- **Primary result:** Achieves up to 15× higher throughput and at least 2× lower latency compared to open-source and commercial baselines for LLM KV cache management

## Executive Summary
LMCACHE addresses the challenge of efficiently storing and transferring KV caches in large language model (LLM) inference, where traditional GPU memory storage becomes insufficient for growing cache sizes and reuse demands. It introduces a high-performance, modular KV caching layer that supports offloading caches to CPU/disk/remote storage and transferring them across inference engines and queries. The system employs three key optimizations: chunk-based batched data movement to maximize interconnect bandwidth, layer-wise compute-I/O pipelining to hide I/O latency behind GPU computation, and zero-copy operations to minimize memory duplication during concurrent transfers. LMCACHE has been widely adopted in enterprise deployments for large-scale, cache-aware LLM serving.

## Method Summary
LMCACHE provides a modular KV caching layer that integrates with vLLM and SGLang inference engines to enable efficient cache offloading and transfer. The system uses chunk-based batched data movement (default 256 tokens) to saturate interconnect bandwidth, layer-wise pipelining to overlap I/O with computation using separate CUDA streams, and zero-copy operations with reference counting for concurrent transfers. The implementation is in Python (v0.3.6) with custom CUDA kernels, supporting multiple storage backends including CPU memory, disk, and remote object storage. The architecture includes components for KV cache management, token processing, storage management, and transfer coordination, with dynamic offloading strategies to minimize memory duplication.

## Key Results
- Achieves up to 15× higher throughput and at least 2× lower latency compared to open-source and commercial baselines
- Supports efficient KV cache offloading to CPU (500GB DRAM) and disk, reducing GPU memory pressure
- Enables prefix caching for multi-round Q&A scenarios, improving cache hit rates to 85%
- Successfully deployed across enterprise environments for large-scale LLM inference

## Why This Works (Mechanism)

### Mechanism 1: Chunk-Based Batched Data Movement
LMCACHE groups multiple pages from multiple layers into larger chunks (default 256 tokens) to saturate interconnect bandwidth, achieving 400 Gbps vs vLLM native's 88 Gbps loading bandwidth. This reduces per-transfer overhead and maximizes bandwidth utilization.

### Mechanism 2: Layer-Wise Compute-I/O Pipelining
Separate CUDA streams for computation and data movement per layer enable overlapping KV cache loading with inference computation, reducing end-to-end delay by 1.46× by hiding I/O latency behind GPU compute.

### Mechanism 3: Zero-Copy Multi-Destination Transfers
Reference counting instead of data duplication for concurrent transfers to multiple destinations reduces memory pressure and copy overhead when KV cache is written to multiple storage tiers simultaneously.

## Foundational Learning

- **Concept: PagedAttention memory management** - Needed to translate between small fixed-size pages (16-64KB) used by vLLM/SGLang and larger chunks for efficient transfer. *Quick check:* Can you explain why transferring 16KB pages individually underutilizes PCIe/network bandwidth compared to 16MB chunks?

- **Concept: Prefill-Decode disaggregation** - Needed for scenarios where prefill and decode run on separate GPUs, requiring KV cache transfer from prefiller to decoder. *Quick check:* Why does separating prefill and decode improve tail latency for the decode phase?

- **Concept: CUDA streams and asynchronous execution** - Needed for layer-wise pipelining requiring concurrent compute and memory transfer in separate streams without blocking. *Quick check:* What happens if compute and I/O share the same default CUDA stream instead of separate streams?

## Architecture Onboarding

- **Component map:** Query arrives → KV connector gets metadata → Token processor checks cache hits → Storage manager retrieves from backend via transfer channel → GPU connector loads into paged memory → Inference proceeds

- **Critical path:** Query → KV Connector → Token Processor → Storage Manager → Transfer Channel → GPU Connector → Inference

- **Design tradeoffs:** Chunk size vs. latency and buffer requirements; dynamic offloading window size vs. allocation stalls; Python implementation vs. potential performance gains from Rust/C++

- **Failure signatures:** High TTFT despite cache hits (bandwidth underutilization); unexpectedly low prefix cache hit ratio (~45% vs 85%, likely due to context truncation); allocation stalls under load (dynamic offloading window too small)

- **First 3 experiments:**
  1. Benchmark transfer throughput with varying chunk sizes (64, 256, 1024 tokens) to find saturation point for your storage backend
  2. Measure TTFT reduction with layer-wise pipelining enabled vs disabled on your typical context lengths
  3. Profile cache hit rates on your workload with and without context truncation to quantify reuse loss

## Open Questions the Paper Calls Out

1. **Adaptive policy for loading vs. recomputing prefill:** What is the optimal adaptive policy for deciding between loading KV cache from remote storage versus recomputing prefill in real-time when bandwidth is low?

2. **Dynamic offloading to lower storage tiers:** Can the dynamic offloading strategy (start/current/end pointers) be effectively extended to lower storage tiers like remote disks or object storage beyond GPU-CPU?

3. **Lossy compression integration:** How can lossy compression be optimally integrated into the KV cache lifecycle to maximize storage efficiency without degrading task performance in open-ended chatbots?

## Limitations

- Evaluation relies heavily on synthetic benchmarks with limited real-world traces due to privacy constraints, restricting independent validation of enterprise-scale claims
- Zero-copy multi-destination transfer optimization lacks direct external validation and empirical evidence of real-world frequency
- Python implementation may introduce performance limitations compared to native Rust/C++ alternatives, though not quantified

## Confidence

- **High Confidence:** Chunk-based batched data movement mechanism and performance benefits (400 Gbps vs 88 Gbps) are well-supported by theoretical analysis and empirical evidence
- **Medium Confidence:** Layer-wise compute-I/O pipelining shows promising results (1.46× delay reduction) but effectiveness depends on workload characteristics
- **Low Confidence:** Zero-copy multi-destination transfer optimization has weakest empirical support with limited evidence of real-world frequency and impact

## Next Checks

1. Benchmark transfer throughput with varying chunk sizes (64, 256, 1024 tokens) on your specific storage backend to identify saturation point
2. Measure Time To First Token (TTFT) reduction with layer-wise pipelining enabled vs disabled on your typical context lengths
3. Profile cache hit rates on your production workload with and without context truncation to quantify reuse loss