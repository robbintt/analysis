---
ver: rpa2
title: 'PICOs-RAG: PICO-supported Query Rewriting for Retrieval-Augmented Generation
  in Evidence-Based Medicine'
arxiv_id: '2510.23998'
source_url: https://arxiv.org/abs/2510.23998
tags:
- medical
- evidence
- query
- llms
- more
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of handling complex medical
  queries in retrieval-augmented generation (RAG) systems for evidence-based medicine
  (EBM). The authors propose PICOs-RAG, a method that expands and normalizes user
  queries into professional formats using the PICO (Population, Intervention, Comparison,
  Outcome) framework to extract critical information for targeted evidence retrieval.
---

# PICOs-RAG: PICO-supported Query Rewriting for Retrieval-Augmented Generation in Evidence-Based Medicine

## Quick Facts
- arXiv ID: 2510.23998
- Source URL: https://arxiv.org/abs/2510.23998
- Authors: Mengzhou Sun; Sendong Zhao; Jianyu Chen; Bin Qin
- Reference count: 27
- Primary result: Up to 8.8% accuracy and 8.6% relevance improvement over baseline RAG methods for evidence-based medicine queries

## Executive Summary
This study addresses the challenge of handling complex medical queries in retrieval-augmented generation (RAG) systems for evidence-based medicine (EBM). The authors propose PICOs-RAG, a method that expands and normalizes user queries into professional formats using the PICO (Population, Intervention, Comparison, Outcome) framework to extract critical information for targeted evidence retrieval. The approach combines query classification, LLM-based expansion, and PICO extraction to improve retrieval relevance and accuracy. Experiments on the Chinese webMedQA dataset demonstrate significant performance gains compared to baseline RAG methods, as evaluated by two different metrics. Ablation studies confirm the effectiveness of each component, particularly query classification and PICO extraction.

## Method Summary
The PICOs-RAG method processes medical queries through a three-stage pipeline: first, queries are classified into one of 22 medical disciplines using a BioBERT classifier (70.2% accuracy); second, an LLM expands and normalizes the query using discipline-specific prompts to add professional terminology and context; third, a second LLM pass extracts PICO elements from the expanded query, which are then vectorized and used for similarity-based retrieval from a medical guideline index. The retrieved evidence is combined with the original query for final response generation. The system is evaluated on Chinese webMedQA using two metrics: relevance to gold answers and overall accuracy, showing significant improvements over baseline RAG approaches.

## Key Results
- PICOs-RAG achieves 84.8% relevance to gold answers and 39.4% overall accuracy on webMedQA
- Performance improvements of up to 8.8% accuracy and 8.6% relevance compared to baseline RAG methods
- Ablation studies confirm each component's contribution, with query classification and PICO extraction being particularly impactful
- The method successfully addresses the gap between non-professional user queries and professional medical terminology required for evidence retrieval

## Why This Works (Mechanism)

### Mechanism 1: Query Classification Constrains Domain-Specific Expansion
Pre-classifying queries by medical discipline before expansion improves relevance by constraining the LLM's generation space. A classifier routes queries to specialty-specific prompts, guiding the LLM to expand using relevant terminology (e.g., internal medicine prompts emphasize dietary habits and medical history), preventing generic expansions that dilute retrieval signals.

### Mechanism 2: PICO Extraction Filters Retrieval Noise
Extracting structured PICO elements from expanded queries reduces irrelevant retrieval by focusing on clinically salient components. After LLM expansion, a second LLM pass extracts only Population, Intervention, Comparison, and Outcome elements for similarity search, filtering out expanded but non-essential terms.

### Mechanism 3: Expansion + PICO Sequential Pipeline Bridges Lay-to-Professional Gap
The two-stage process (expansion then PICO extraction) outperforms either alone by first enriching context, then focusing signals. Direct PICO extraction from lay queries fails due to insufficient signal; expansion alone adds noise. The sequential pipeline captures both breadth (expansion) and precision (PICO).

## Foundational Learning

- **Concept: Evidence-Based Medicine (EBM)**
  - Why needed here: PICOs-RAG is designed specifically for EBM scenarios where responses require supporting literature evidence, not just generated answers.
  - Quick check question: Can you explain why EBM requires both an answer AND supporting evidence, unlike general Q&A?

- **Concept: PICO Framework**
  - Why needed here: The entire extraction mechanism depends on understanding Population, Intervention, Comparison, Outcome as structured clinical query components.
  - Quick check question: Given the query "Is aspirin better than ibuprofen for headache in adults?", can you identify the PICO elements?

- **Concept: Vector Similarity Retrieval**
  - Why needed here: The system uses LLM-generated embeddings to match PICO elements against a vectorized medical guideline index.
  - Quick check question: Why might exact keyword matching fail where semantic vector similarity succeeds in medical retrieval?

## Architecture Onboarding

- **Component map:**
  User Query → BioBERT Classifier → Medical Specialty → LLM Expansion Module → Expanded Query → LLM PICO Extractor → PICO Elements → Vector Similarity Search → Retrieved Evidence → Response LLM → Final Answer

- **Critical path:** Query classification accuracy → Expansion relevance → PICO extraction completeness → Retrieval precision. Failures cascade: misclassification leads to off-topic expansion, which yields incomplete PICO extraction.

- **Design tradeoffs:**
  - Single-model deployment (Baichuan2-13B for both expansion and response) vs. specialized models per task (authors chose single model for practical deployment)
  - PICO vs. PIO (Comparison element optional): Minimal performance difference (2.2% Method A), but authors selected PICO
  - Evaluation Method A (relevance to gold answer) vs. Method B (comprehensive accuracy): Different ablation patterns suggest tradeoffs between targeted vs. comprehensive responses

- **Failure signatures:**
  - Classification failure: Responses become generic (Method A drops, Method B may slightly improve)
  - Expansion failure: Low retrieval relevance (largest ablation drop: 64.6%/22.6%)
  - PICO extraction failure: Noisy retrieval (Figure 8 shows irrelevant case retrieval)
  - Model selection failure: Huatuo2 generates repetitive text; ZIYA copies evidence verbatim; LLaMA3 responds in wrong language for Chinese queries

- **First 3 experiments:**
  1. Reproduce ablation on held-out queries: Test 100 new queries from webMedQA with w/o classification, w/o expansion, w/o PICO to validate component contributions on unseen data.
  2. PICO vs. PIO comparison with statistical testing: The 2.2% Method A difference may not be significant—run bootstrap sampling to confirm.
  3. Cross-domain generalization: Apply PICOs-RAG to English medical QA datasets (e.g., MedQA) to test if the method transfers beyond Chinese webMedQA.

## Open Questions the Paper Calls Out
- The authors explicitly state they chose a Chinese dataset because current LLMs are "highly trained on [English] datasets" and achieve high accuracy without support, leaving English performance unverified.
- The method's reliance on specific model behaviors (instruction following and stability) was not overcome for other architectures tested (Llama3 responded in wrong language, Huatuo2 generated repetitive text).
- The dataset used (webMedQA) consists solely of patient queries, so performance on professionally phrased clinical queries which may require no expansion is not tested.

## Limitations
- Critical prompt templates for expansion and PICO extraction are not provided, essential for reproducing results
- Medical guidelines corpus source, preprocessing, and indexing parameters are unspecified
- BioBERT classifier training data for 22 medical disciplines is not described
- PICO vs. PIO ablation shows minimal performance difference (2.2%), suggesting Comparison element may be unnecessary

## Confidence
- **High**: The sequential expansion-then-PICO pipeline improves retrieval relevance compared to direct PICO extraction from lay queries
- **Medium**: Query classification meaningfully constrains expansion and affects response focus
- **Medium**: The overall 6.2–8.8% accuracy and 8.6% relevance improvements are valid within Chinese webMedQA dataset

## Next Checks
1. Reproduce component ablations on held-out queries to verify each component's contribution on unseen data
2. Statistically validate PICO vs. PIO performance with bootstrap sampling across multiple runs
3. Cross-domain generalization test by applying PICOs-RAG to English medical QA datasets (e.g., MedQA-USMLE)