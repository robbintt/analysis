---
ver: rpa2
title: 'PuzzleWorld: A Benchmark for Multimodal, Open-Ended Reasoning in Puzzlehunts'
arxiv_id: '2506.06211'
source_url: https://arxiv.org/abs/2506.06211
tags:
- reasoning
- puzzle
- puzzles
- arxiv
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PuzzleWorld is a benchmark of 667 puzzlehunt-style problems designed
  to evaluate multimodal, open-ended reasoning. Unlike conventional benchmarks with
  well-defined tasks, puzzlehunts require discovering the underlying problem structure
  from ambiguous, multimodal cues.
---

# PuzzleWorld: A Benchmark for Multimodal, Open-Ended Reasoning in Puzzlehunts

## Quick Facts
- arXiv ID: 2506.06211
- Source URL: https://arxiv.org/abs/2506.06211
- Reference count: 40
- Most state-of-the-art models achieve only 1-2% final answer accuracy on this benchmark

## Executive Summary
PuzzleWorld is a benchmark of 667 puzzlehunt-style problems designed to evaluate multimodal, open-ended reasoning. Unlike conventional benchmarks with well-defined tasks, puzzlehunts require discovering the underlying problem structure from ambiguous, multimodal cues. Each puzzle includes fine-grained annotations: final solution, detailed reasoning traces, and cognitive skill labels. Most state-of-the-art models achieve only 1-2% final answer accuracy, with the best model solving 14% of puzzles and reaching 40% stepwise accuracy.

## Method Summary
PuzzleWorld contains 667 puzzlehunt-style problems sourced from Puzzled Pint with detailed annotations including final solutions, reasoning traces, and cognitive skill labels. The dataset is publicly available and includes original puzzle images with metadata. Evaluation uses an LLM judge (GPT-4o) to assess stepwise accuracy by comparing candidate reasoning traces against ground truth. The benchmark tests multimodal reasoning across six cognitive skills and three modalities. Fine-tuning experiments use LoRA on an 8B InternVL3 model with reasoning traces, avoiding training on final answers alone which degrades performance.

## Key Results
- Most state-of-the-art models achieve only 1-2% final answer accuracy on PuzzleWorld
- The best model solves 14% of puzzles with 40% stepwise accuracy
- Fine-tuning on reasoning traces improves stepwise reasoning from 4% to 11%, while training on final answers alone degrades performance to near zero
- 53.33% of reasoning failures involve spatial reasoning or sketching-related capabilities

## Why This Works (Mechanism)

### Mechanism 1: Reasoning Trace Fine-tuning
Fine-tuning on detailed reasoning traces improves stepwise reasoning capability, while answer-only training degrades it. Models learn decomposed cognitive processes (pattern discovery → manipulation → extraction → combination) rather than memorizing sparse input-output pairs. The annotation schema captures atomic operations that compose complex reasoning chains.

### Mechanism 2: Open-Ended Discovery as a Capability Probe
Puzzles without explicit instructions test a distinct capability from conventional benchmarks with well-defined tasks. Models must first infer the problem structure from ambiguous multimodal cues before executing solutions. The modality × reasoning skill taxonomy enables targeted capability assessment across 667 diverse puzzles.

### Mechanism 3: Language-Centric Bottleneck in Spatial Reasoning
Current multimodal models lose critical spatial information when converting visual inputs into language-based reasoning chains. Models default to textual chain-of-thought, which cannot preserve complex spatial layouts. Absence of sketching capabilities—maintaining persistent visual representations during reasoning—causes systematic failures in spatial tasks.

## Foundational Learning

- **Puzzlehunts vs. Conventional Benchmarks**: Why needed here - Conventional benchmarks provide explicit instructions; puzzlehunts require discovering both task and solution method from ambiguous multimodal clues, mirroring real-world open-ended problem-solving. Quick check question: Why is a Sudoku puzzle "conventional" while a clue showing only musical notes with no instructions is "open-ended"?

- **Stepwise vs. Final Answer Evaluation**: Why needed here - Final answer accuracy (1-2% for most models) is too sparse for diagnostics. The LLM judge (r=0.829 correlation with human evaluation) enables fine-grained reasoning trajectory analysis. Quick check question: If a model correctly identifies 4 of 7 reasoning steps but fails final extraction, what is its stepwise accuracy?

- **Atomic Cognitive Operations**: Why needed here - Annotations decompose reasoning into five actions (pattern discovery, sketching, manipulation, combining, extraction). This taxonomy enables targeted failure analysis and improvement. Quick check question: Which operation fails when a model solves crossword clues correctly but cannot trace a path through the filled grid?

## Architecture Onboarding

- **Component map**: Puzzle image + metadata JSON -> Standardized prompt -> Model output -> LLM judge evaluation -> Stepwise accuracy
- **Critical path**: 1) Load puzzle image + metadata, 2) Prompt with standardized format, 3) Extract candidate solution and reasoning, 4) Run LLM judge for stepwise accuracy, 5) Optional: Fine-tune on reasoning traces
- **Design tradeoffs**: Preserving original images maintains spatial relationships but accepts OCR variability; single canonical annotation path vs. multiple valid solution approaches; LLM judge enables scale but introduces potential evaluation bias
- **Failure signatures**: Myopic reasoning (committing to first hypothesis without backtracking), language bottleneck (spatial layouts garbled in text conversion), sketching gap (correct operations proposed but visual execution fails), answer-only collapse (stepwise accuracy 4.78% → 2.96%, final accuracy → 0%)
- **First 3 experiments**: 1) Baseline evaluation: Run model on all 667 puzzles; compute final and stepwise accuracy by skill and modality, 2) Error taxonomy analysis: Categorize root cause of step-0 failures, 3) Reasoning trace fine-tuning pilot: Fine-tune on 80% training split using traces only; evaluate on 20% held-out

## Open Questions the Paper Calls Out

- **Incorporating sketch-like visual memory**: Can adding visual sketching mechanisms significantly improve performance on visual and spatial reasoning tasks? The authors identify this as a critical gap that may offer a promising direction.

- **Overcoming myopic commitment**: How can models be adjusted to enable effective backtracking when early hypotheses are incorrect? Current models lack mechanisms for self-correction and rarely recover from initial errors.

- **LLM judge bias**: To what extent does reliance on GPT-4o as an LLM-judge introduce bias or instability in evaluating ambiguous reasoning traces? While correlation with human judgment is high, automated evaluation may fail on novel or unconventional reasoning steps.

- **Generalizability beyond static inputs**: Does the exclusion of temporal and interactive modalities limit the generalizability of reasoning capabilities assessed by the benchmark? Real-world problem solving often involves dynamic or auditory data not captured in current static-image benchmarks.

## Limitations
- Evaluation relies on LLM judge (GPT-4o) with potential bias, though correlation with human evaluation is r=0.829
- Single canonical annotation per puzzle may not capture valid alternative solution paths
- Puzzle difficulty distribution from Puzzled Pint may not generalize to broader puzzlehunt domains

## Confidence
- **High Confidence**: PuzzleWorld contains 667 annotated puzzles; most models achieve 1-2% final accuracy; fine-tuning on reasoning traces improves stepwise reasoning from 4% to 11%; training on final answers alone degrades performance to near zero; language bottleneck is primary failure mode for spatial reasoning
- **Medium Confidence**: Puzzlehunts uniquely test problem structure discovery; spatial reasoning failures stem from lack of sketching capabilities; stepwise accuracy correlates meaningfully with reasoning capability
- **Low Confidence**: Puzzlehunt reasoning directly translates to real-world scientific discovery; current models' failures reflect fundamental reasoning limitations vs. architectural gaps; puzzlehunt paradigm is superior to existing reasoning benchmarks

## Next Checks
1. **Cross-Annotation Validation**: Have multiple human annotators create reasoning traces for 50 randomly selected puzzles to measure inter-annotator agreement and identify genuinely alternative solution paths.

2. **Evaluation Method Sensitivity**: Compare LLM judge results against a smaller set of human evaluations on identical puzzles to establish the true correlation and identify potential systematic biases in automated evaluation.

3. **Sketching Capability Intervention**: Implement a simple visual working memory mechanism (e.g., maintaining a persistent canvas representation) and evaluate its impact on spatial reasoning performance across the visual puzzle subset.