---
ver: rpa2
title: Cluster-Based Generalized Additive Models Informed by Random Fourier Features
arxiv_id: '2512.19373'
source_url: https://arxiv.org/abs/2512.19373
tags:
- mixture
- random
- fourier
- data
- clustering
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a mixture-of-GAMs framework that combines
  random Fourier feature embeddings with interpretable local modeling. The method
  learns a latent Fourier representation from data, reduces it via PCA, and applies
  Gaussian mixture modeling to partition the input space into clusters.
---

# Cluster-Based Generalized Additive Models Informed by Random Fourier Features

## Quick Facts
- arXiv ID: 2512.19373
- Source URL: https://arxiv.org/abs/2512.19373
- Reference count: 40
- Primary result: Mixture-of-GAMs framework combining RFF embeddings with cluster-specific interpretable models achieves performance comparable to state-of-the-art black-box models on four regression benchmarks

## Executive Summary
This work introduces a mixture-of-GAMs framework that combines random Fourier feature embeddings with interpretable local modeling. The method learns a latent Fourier representation from data, reduces it via PCA, and applies Gaussian mixture modeling to partition the input space into clusters. Within each cluster, a GAM captures nonlinear univariate effects via spline-based smoothers. Numerical experiments on four regression benchmarks show that the proposed method consistently outperforms global interpretable models and mixture-of-linear baselines, achieving performance comparable to state-of-the-art black-box and explainable models on several tasks. This approach provides a principled way to integrate representation learning with transparent statistical modeling.

## Method Summary
The framework operates in four stages: (1) Train a Random Fourier Feature (RFF) model to approximate the target function, extracting intermediate complex-valued features; (2) Apply PCA to reduce dimensionality of the RFF representation, then use Gaussian mixture models for soft clustering; (3) For each cluster, train a cluster-specific GAM using B-splines with quantile-based knots and 2nd-difference smoothness penalties; (4) Aggregate predictions from local GAMs using the soft cluster assignments. The approach leverages RFFs to learn a spectral approximation of the target function, then uses the learned representation to guide interpretable local modeling.

## Key Results
- Mixture-of-GAMs consistently outperforms global interpretable models (single GAM) and mixture-of-linear baselines across all four tested regression datasets
- Clustering in learned RFF space provides better partitioning than unsupervised clustering in raw input space
- The method achieves test RMSE comparable to state-of-the-art black-box and explainable models on several benchmarks
- Local nonlinear modeling within clusters captures heterogeneous covariate effects that global models miss

## Why This Works (Mechanism)

### Mechanism 1
Clustering in a learned Random Fourier Feature (RFF) latent space partitions data more effectively for local regression than clustering in the raw input space. The RFF model learns a spectral approximation of the target function, where intermediate representations (amplitude-weighted Fourier features) encode high-level structural information about the regression surface. Applying PCA and GMM in this space groups data points based on learned functional similarities rather than just input proximity. Core assumption: The variance in the high-dimensional RFF representation corresponds to distinct, learnable regimes in the data distribution suitable for specialized local models.

### Mechanism 2
Replacing local linear models with cluster-specific Generalized Additive Models (GAMs) captures heterogeneous, nonlinear marginal effects without sacrificing interpretability. While global GAMs assume additive effects hold everywhere and mixture-of-linear-models assume local linearity, the proposed method fits flexible spline functions within each cluster. This allows the shape of the relationship between a predictor and the response to vary across different regions of the input space. Core assumption: The data heterogeneity is such that the underlying function is locally additive (or approximately so), even if the global function is not.

### Mechanism 3
Dimensionality reduction via PCA on RFF intermediate features acts as a spectral filter that improves clustering robustness. The raw RFF representation is extremely high-dimensional (K ≈ 2000-4000). Direct clustering in this space is susceptible to noise and the curse of dimensionality. PCA retains only the top d components (where d ≈ 2-6), filtering out high-frequency noise directions that are unlikely to represent stable cluster structures. Core assumption: The principal components of the centered intermediate feature matrix correspond to meaningful macro-structures in the data, while minor components represent noise.

## Foundational Learning

- **Random Fourier Features (RFF)**: Spectral approximation technique for kernel methods. Why needed: Forms the "supervision" signal for clustering. Quick check: Can you explain why the inner product of RFF vectors approximates a kernel function?
- **Generalized Additive Models (GAMs)**: Additive models with flexible univariate smooth functions. Why needed: These are the local engines of prediction. Quick check: How does a GAM differ from a simple linear model, and what constraint makes it "additive"?
- **Soft Clustering (GMM)**: Probabilistic clustering method providing soft assignments. Why needed: The framework uses probabilistic assignment (γ_ℓ) rather than hard boundaries. Quick check: In a GMM, what does the posterior responsibility γ_ℓ(x) represent in the final prediction equation m̃(x)?

## Architecture Onboarding

- **Component map**: Input -> RFF Core (Train RFF, extract intermediate features) -> Latent Clustering (PCA, GMM) -> Local Experts (Train local GAM per cluster) -> Aggregation (Weight by soft assignments)
- **Critical path**: The accuracy of the RFF model (Stage 1) is the bottleneck. If the RFF model fails to learn a good representation of the target function, the subsequent PCA and GMM will cluster noise, rendering the local GAMs useless.
- **Design tradeoffs**: Latent dimension (d) vs. Cluster granularity (L): Low d provides smoother clusters but may merge distinct regimes; High L increases local flexibility but risks fragmenting data into sparse clusters where GAMs cannot fit reliably. Transparency vs. Accuracy: While better than black boxes, local GAMs are harder to aggregate for a global explanation than a single global GAM.
- **Failure signatures**: Cluster Collapse (GMM converges to single component - check priors π_ℓ); Sparse GAM Fits (Test error explodes - check if any cluster has too few data points); PCA Bottleneck (Validation error plateaus regardless of cluster count - implies retained principal components don't capture needed variance)
- **First 3 experiments**: Ablation (Clustering Source): Compare clustering on Raw Input vs. RFF Latent Space; Hyperparameter Sensitivity: Grid search over Cluster count (L) and PCA dimension (d); Local vs. Global: Compare proposed mixture against Global GAM and Mixture of Linear Models

## Open Questions the Paper Calls Out

### Open Question 1
Can sparsity-promoting regularization (e.g., ℓ₁ or ℓₚ penalties with p ≤ 1) on the random Fourier feature coefficients improve interpretability and generalization in the mixture-of-GAMs framework? The paper states: "While we mainly focus on Tikhonov regularization in this work, alternative regularization schemes such as ℓₚ-type penalties with p ≤ 1 can be considered to promote sparsity in the random feature coefficients and could be investigated in future work." Empirical comparison of sparse vs. ridge regularization across benchmarks would resolve this.

### Open Question 2
How can the number of mixture components L and the PCA embedding dimension d be selected in a principled, data-driven manner rather than via grid search? Hyperparameters L and d are tuned via grid search, but no theoretical or automatic selection criterion is provided. Development and validation of model selection criteria (e.g., information-theoretic, cross-validation-based, or stability-based methods) would resolve this.

### Open Question 3
Does extending the mixture-of-GAMs framework to spatio-temporal settings, where random Fourier features encode both spatial and temporal structure, yield interpretable and accurate localized models for physical or environmental systems? The conclusion states: "One promising avenue is the extension of the proposed framework to spatio-temporal settings, where random Fourier features can encode both spatial structure and temporal dynamics through joint spectral representations." Application to spatio-temporal benchmarks with evaluation of predictive skill and interpretability would resolve this.

## Limitations

- Performance depends critically on RFF model quality - poor spectral approximation leads to ineffective clustering and local modeling
- Assumes local additivity which may not hold for datasets with strong interaction effects
- Computational cost scales poorly with feature dimension and sample size due to RFF intermediate feature computation and multiple local GAM fits
- The framework is currently limited to regression tasks and hasn't been validated for classification problems

## Confidence

- **High confidence**: Clustering in learned RFF space outperforms raw input clustering (supported by direct ablation experiments)
- **Medium confidence**: Local GAMs consistently outperform local linear models (evidence from numerical experiments but limited to tested datasets)
- **Medium confidence**: PCA-based dimensionality reduction improves clustering robustness (supported empirically but mechanism not deeply validated)

## Next Checks

1. **Interaction Effect Test**: Evaluate performance on datasets with known strong interaction effects (e.g., Friedman #3) to test the limits of the additive local model assumption
2. **RFF Quality Ablation**: Compare against standard RFF (without resampling optimization) to quantify the impact of the learned frequency selection on downstream clustering performance
3. **Global Interpretability Analysis**: Conduct systematic comparison of global explainability between single GAM, mixture-of-GAMs, and black-box models using SHAP or similar methods to validate the interpretability tradeoff claim