---
ver: rpa2
title: 'InfraGPT Smart Infrastructure: An End-to-End VLM-Based Framework for Detecting
  and Managing Urban Defects'
arxiv_id: '2510.16017'
source_url: https://arxiv.org/abs/2510.16017
tags:
- detection
- language
- reasoning
- vision
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes InfraGPT, an end-to-end vision-language framework
  for automated urban infrastructure defect detection and management. It integrates
  YOLO-based multi-defect detection with a vision-language model (VLM) to generate
  structured JSON-based maintenance plans from CCTV footage.
---

# InfraGPT Smart Infrastructure: An End-to-End VLM-Based Framework for Detecting and Managing Urban Defects

## Quick Facts
- arXiv ID: 2510.16017
- Source URL: https://arxiv.org/abs/2510.16017
- Reference count: 24
- Primary result: YOLOv11 achieves 91% mAP@0.5 on urban defect detection; LLaVA:7B achieves ROUGE-L 0.36 for structured maintenance plan generation

## Executive Summary
InfraGPT is an end-to-end vision-language framework that automates urban infrastructure defect detection and management. It combines YOLO-based multi-defect detection with vision-language models to generate structured JSON-based maintenance plans from CCTV footage. The system identifies cracks, potholes, and leaks, then uses VLM reasoning to output actionable maintenance instructions including defect type, location, severity, required tools, and urgency. Experimental evaluation shows strong performance with YOLOv11 achieving 91% mAP@0.5 and VLM-generated summaries exhibiting ROUGE-L F1 scores up to 0.36.

## Method Summary
The pipeline processes CCTV frames through a VLM controller that performs initial screening and binary decision vector generation (S = [s_c, s_l, s_o]) to determine which YOLO variant to activate. Detected defects are then passed to a reasoning VLM that generates structured JSON maintenance plans with fields for incident descriptions, tools, dimensions, repair plans, and urgency alerts. The system uses YOLOv8/v11 for detection and LLaVA:7B or Qwen2.5-VL:7B for language generation, trained on a merged dataset of Crack500, Road Damage Dataset 2022, and Pothole-600.

## Key Results
- YOLOv11 achieves 91% mAP@0.5 and 95% precision on multi-defect detection
- LLaVA:7B achieves ROUGE-L 0.36 for structured JSON generation vs Qwen2.5-VL:7B at 0.24
- End-to-end system achieves 94% structural accuracy in JSON schema compliance
- Inference time averages 3 seconds per frame with YOLO running at 15 FPS on RTX 4060

## Why This Works (Mechanism)

### Mechanism 1: VLM-Guided Adaptive Detection Selection
A vision-language model performs initial coarse semantic analysis on input images, producing a binary decision vector S = [s_c, s_l, s_o] indicating likelihood of cracks, leaks, or other anomalies. This vector dynamically controls which YOLO variant is activated for fine-grained detection and localization, improving efficiency by avoiding unnecessary computation.

### Mechanism 2: Structured Schema-Constrained Generation
After YOLO detection produces bounding boxes and class labels, the VLM receives both the original image and detection results, then generates structured output conforming to a predefined JSON schema with fields for type, class, bbox, severity, tools, actions, and urgency.

### Mechanism 3: Cross-Modal Attention Grounding
The VLM processes visual features and YOLO detections jointly, with attention mechanisms focusing on regions corresponding to detected defects during reasoning. Heatmap visualization confirms that attention peaks align with YOLO bounding boxes, validating precise visual grounding.

## Foundational Learning

- **YOLO Object Detection Architecture**: Understanding how single-stage detectors produce bounding boxes, confidence scores, and class predictions enables debugging detection quality issues. Quick check: Can you explain why YOLO divides images into grid cells and how this affects detection of small defects like thin cracks?

- **Vision-Language Model Fusion**: The VLM must jointly process visual features and text prompts; understanding cross-attention mechanisms helps diagnose reasoning failures. Quick check: How does a VLM like LLaVA combine image encoder outputs with language model inputs, and what does this imply for prompt design?

- **Structured Output Generation and Schema Validation**: JSON schema alignment is critical for integration; understanding constrained decoding helps reduce hallucination. Quick check: What validation steps would catch a VLM generating syntactically valid JSON with semantically inconsistent field values?

## Architecture Onboarding

- **Component map**: Input acquisition: CCTV frames → preprocessing (normalization, resize) → VLM Controller (Qwen2.5-VL or LLaVA) → YOLO Detection (v8 or v11) → VLM Reasoning Module → Output: JSON maintenance plan with schema validation

- **Critical path**: 1. Frame capture and preprocessing 2. VLM coarse analysis (produces S vector) 3. Conditional YOLO activation based on S 4. Detection result fusion with original image 5. VLM structured reasoning with prompt template 6. JSON output and schema validation

- **Design tradeoffs**: LLaVA:7B vs Qwen2.5-VL:7B: LLaVA shows higher ROUGE-L (0.36 vs 0.24) but paper notes Qwen provides richer contextual descriptions; YOLOv8 vs YOLOv11: v11 achieves higher mAP (91%) and better leak/crack detection at similar inference speed (15 FPS); Two-stage vs single-stage: Added VLM screening introduces ~3 second latency per frame but reduces false positives by ~10%

- **Failure signatures**: High false-negative rate in VLM screening: defects present but S vector indicates no anomaly; JSON schema violations: missing required fields, incorrect data types, hallucinated keys; Detection localization errors in cluttered/reflective environments; ROUGE-L variance spike on complex multi-defect scenes

- **First 3 experiments**: 1. Reproduce YOLO detection baseline: Train YOLOv11 on combined defect dataset, verify mAP@0.5 approaches 91% on held-out validation set; 2. VLM screening accuracy test: Run VLM controller on 100 frames with known ground-truth labels; measure false-positive and false-negative rates for the binary decision vector; 3. End-to-end JSON validity evaluation: Process 50 test frames through full pipeline; manually verify each JSON output for schema compliance, semantic accuracy, and grounding against YOLO detections

## Open Questions the Paper Calls Out

- **Open Question 1**: What specific mechanisms can effectively eliminate hallucinations and ensure strict schema alignment in the VLM-generated JSON maintenance plans? The authors identify "hallucination control" and "schema alignment" as persistent limitations where models may output factually incorrect entries or deviate from field structures.

- **Open Question 2**: How can inference latency be reduced to support real-time processing for high-resolution imagery without compromising detection accuracy? The discussion notes that "the reasoning stage increases processing latency, particularly with high-resolution imagery."

- **Open Question 3**: Can hierarchical temporal fusion for video analysis resolve ambiguities in cluttered or reflective environments? The authors state that performance "declines in cluttered or reflective environments where defect boundaries become ambiguous."

## Limitations

- VLM-based model selection efficiency gains are theoretical without quantified false-negative/false-positive rates for the initial screening stage
- Cross-modal attention grounding lacks independent validation beyond internal heatmaps
- Schema-constrained generation may fail on complex multi-defect scenes where required fields conflict

## Confidence

- **High Confidence**: YOLO detection performance metrics (mAP@0.5, FPS) with specified hardware
- **Medium Confidence**: VLM generation quality metrics (ROUGE-L scores) comparing LLaVA vs Qwen models
- **Low Confidence**: End-to-end pipeline reliability claims without specified validation procedures

## Next Checks

1. **Grounding Validation**: Run 50 test frames through full pipeline and manually verify that VLM attention peaks align with actual YOLO detection regions for each defect instance

2. **Schema Compliance Audit**: Generate JSON outputs for 100 diverse frames and compute exact structural accuracy by checking field presence, data types, and semantic consistency against ground truth

3. **Failure Mode Analysis**: Systematically induce challenging conditions (cluttered scenes, poor lighting, multiple overlapping defects) and measure detection/recognition performance degradation across all components