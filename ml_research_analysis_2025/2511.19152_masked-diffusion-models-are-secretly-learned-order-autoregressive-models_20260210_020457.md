---
ver: rpa2
title: Masked Diffusion Models are Secretly Learned-Order Autoregressive Models
arxiv_id: '2511.19152'
source_url: https://arxiv.org/abs/2511.19152
tags:
- uni00000013
- uni00000011
- arxiv
- diffusion
- uni00000015
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Masked Diffusion Models (MDMs) train by decoding tokens in random
  order, but this uniform weighting ignores that some orders may be more favorable
  than others. This work shows that extending MDMs with multivariate noise schedules
  enables the model to learn an optimal state-independent decoding order during training.
---

# Masked Diffusion Models are Secretly Learned-Order Autoregressive Models

## Quick Facts
- **arXiv ID:** 2511.19152
- **Source URL:** https://arxiv.org/abs/2511.19152
- **Authors:** Prateek Garg; Bhavya Kohli; Sunita Sarawagi
- **Reference count:** 31
- **Primary result:** Learned multivariate noise schedules enable MDMs to discover optimal decoding orders, achieving competitive tabular data generation performance with far fewer parameters than baselines

## Executive Summary
Masked Diffusion Models (MDMs) train by decoding tokens in random order, but this uniform weighting ignores that some orders may be more favorable than others. This work shows that extending MDMs with multivariate noise schedules enables the model to learn an optimal state-independent decoding order during training. The authors prove that the MDM objective decomposes into a weighted sum of autoregressive losses over all possible orders, with the probability of each order determined by the noise schedule. This establishes MDMs as learned-order autoregressive models. Experiments on tabular data generation show that MDM with learned schedules achieves competitive performance against strong baselines (e.g., TabDiff, TabSYN) across metrics like Trend, Shape, α-Precision, and β-Recall, while using far fewer parameters.

## Method Summary
The method extends standard MDMs by introducing per-feature noise schedules α_{t,ℓ} = 1 - t^{w_ℓ} where w_ℓ are learnable parameters. During training, each feature ℓ transitions from masked to unmasked at a random time t*_ℓ following distribution P(t*_ℓ ≤ t) = 1 - α_{t,ℓ}. The intersection of all L schedules induces a probability distribution over permutations π. The ELBO telescopes so only the log-probability of the correct token at its transition time contributes, yielding an AR-style conditional prediction loss weighted by P(π). Schedules are learned via gradient descent through the masking process using the RLOO estimator. At inference, Algorithm 1 samples order π from learned schedules and decodes tokens sequentially using the reverse model.

## Key Results
- MDM with learned schedules achieves competitive performance vs TabDiff/TabSYN across Trend, Shape, α-Precision, and β-Recall metrics
- Learned schedules use far fewer parameters (~86K) compared to baseline methods
- Validation loss consistently improves with learned schedules, but downstream metric gains are modest
- Learned schedules increase training loss variance compared to fixed linear schedules

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The MDM training objective decomposes into a weighted expectation over autoregressive losses for all possible token orderings, with weights determined by the multivariate noise schedule.
- **Mechanism:** Each per-position noise schedule α_{t,ℓ} defines a distribution over when that token transitions from masked to unmasked. The intersection of all L schedules induces a probability distribution over permutations π via P(π) = ∫_{Ω_π} ∏_ℓ(-α'_{t,ℓ}) dt where Ω_π is the region where transition times respect order π. The ELBO telescopes so only the log-probability of the correct token at its transition time contributes, yielding an AR-style conditional prediction loss weighted by P(π).
- **Core assumption:** Time-independent network parameterization (or near-independence) so that the loss at transition time depends primarily on the conditioning context rather than absolute time.
- **Evidence anchors:**
  - [abstract] "We prove that the MDM objective decomposes precisely into a weighted auto-regressive losses over these orders"
  - [Section 3.1, Proposition 3.1] Full derivation with P(π) integral formula
  - [corpus] Related work "Train for the Worst, Plan for the Best" confirms ordering sensitivity but focuses on inference-time heuristics; weak direct validation of the learned-order decomposition theory
- **Break condition:** If network predictions vary substantially with absolute time t (not just context), the AR interpretation degrades since the loss becomes time-dependent beyond conditioning context.

### Mechanism 2
- **Claim:** Per-dimension noise schedules α_{t,ℓ} = 1 - t^{w_ℓ} directly control the probability of each decoding order, breaking the standard MDM invariance to noise schedule choice.
- **Mechanism:** Corollary 3.1 shows each token's transition time t*_ℓ follows pdf -α'_{t,ℓ}. Schedule parameter w_ℓ controls the concentration of this distribution: larger w_ℓ pushes expected transition time later (decoded earlier in reverse process). By learning {w_ℓ} via gradient descent through the masking process (using RLOO estimator), the model discovers which features should be decoded first to maximize likelihood.
- **Core assumption:** Gradient signal through the discrete masking process can be sufficiently low-variance to learn meaningful schedule differences; RLOO estimator provides workable variance reduction.
- **Evidence anchors:**
  - [Section 3.2, Proposition 3.2] P(t* ≤ t) = 1 - α_t establishes transition time distribution
  - [Section 4, Figure 2b] Visualizes learned schedules on Adult dataset showing learned differentiation across features
  - [corpus] "Optimizing Decoding Paths in Masked Diffusion Models by Quantifying Uncertainty" provides complementary evidence that order matters, though via uncertainty metrics rather than learned schedules
- **Break condition:** If learned schedules converge to near-identical values (symmetry not broken), the model reverts to uniform order distribution equivalent to standard MDM.

### Mechanism 3
- **Claim:** Lower validation loss from learned schedules does not guarantee proportional improvements in downstream fidelity metrics due to increased training variance.
- **Mechanism:** Learned schedules introduce per-dimension weighting variations that increase loss variance during training compared to fixed linear schedules. While the expected gradient is correct, higher variance may slow effective convergence on the downstream task. The paper observes modest validation loss improvements (e.g., Adult: 13.1524 vs 13.1861) but limited metric gains.
- **Core assumption:** The linear schedule's lower variance (observed in prior work for univariate case) extends to multivariate case as explanation for limited gains.
- **Evidence anchors:**
  - [Section 5, Discussion] "learned schedulers increase the loss variance during training"
  - [Table 2a] Validation losses show consistent but small improvements for MDM(LS) over MDM
  - [corpus] No direct corpus validation of the variance hypothesis; this remains an open question per authors
- **Break condition:** If variance is not the limiting factor, or if downstream metrics are insensitive to the order-optimization objective, learned schedules may not provide practical benefits despite theoretical correctness.

## Foundational Learning

- **Concept: Variational Lower Bound (ELBO) for Diffusion Models**
  - Why needed here: The entire theoretical framework builds on decomposing the ELBO (Equation 3) into order-weighted components; understanding how ELBO relates to likelihood is essential.
  - Quick check question: Can you explain why maximizing the ELBO approximately maximizes log-likelihood, and how the discrete-time ELBO connects to the continuous-time loss in Equation 4?

- **Concept: Masked Diffusion Forward/Reverse Process**
  - Why needed here: Proposition 3.2's proof relies on the mechanics of masking probability α_{t|s} and the reverse process jump behavior; misunderstanding this invalidates the order-schedule correspondence.
  - Quick check question: Given α_t decreases from 1 to 0 over t∈[0,1], what is the probability a token is masked at time t=0.5 if α_t = 1-t?

- **Concept: RLOO Gradient Estimator for Discrete Variables**
  - Why needed here: Learning schedules requires differentiating through discrete masking; RLOO (REINFORCE Leave-One-Out) provides the variance-reduced gradient estimator used in MDM(LS).
  - Quick check question: Why can't we directly backpropagate through discrete sampling, and how does a baseline (like RLOO's leave-one-out mean) reduce variance?

## Architecture Onboarding

- **Component map:** Input features x_1:L -> Noise schedule module {w_ℓ} -> Forward masking with α_{t,ℓ} -> Reverse model µ_θ -> Loss module with weights α'_{t,ℓ}/(1-α_{t,ℓ}) -> Order sampler (Algorithm 1)

- **Critical path:**
  1. Initialize schedules uniformly (e.g., w_ℓ = 1 for all ℓ)
  2. For each training batch: sample t uniformly, apply forward masking per-position using α_{t,ℓ}
  3. Compute reverse model predictions, weight losses by per-position schedule derivatives
  4. Backpropagate through both network θ and schedule parameters {w_ℓ} using RLOO estimator
  5. At inference: use Algorithm 1 to sample order π, decode tokens in that order using reverse model

- **Design tradeoffs:**
  - Schedule parameterization: α_{t,ℓ} = 1 - t^{w_ℓ} is simple but restricts expressivity; alternatives (learned splines, neural schedules) may capture more complex order distributions at cost of optimization difficulty
  - State-independent vs state-dependent orders: This work learns state-independent P(π); state-dependent (conditioned on partial generation) may be more expressive but harder to train (see LO-ARMs in references)
  - Network parameterization: Time-independent networks simplify theory but may limit expressivity; time-conditioned networks are standard but complicate the AR interpretation

- **Failure signatures:**
  - Schedules collapse to near-identical values → reverts to uniform MDM behavior (Figure 3 shows TabDiff fails this way)
  - High gradient variance prevents schedule differentiation → use stronger variance reduction or pretrain with fixed schedules
  - Generated samples have poor β-Recall despite good α-Precision → model may be overfitting to training distribution modes; check schedule diversity and sampling temperature

- **First 3 experiments:**
  1. **Verify order-schedule correspondence:** Train MDM(LS) on a small synthetic dataset with known optimal order; confirm learned schedules recover that order (sample orders via Algorithm 1 and compare to ground-truth optimal ordering).
  2. **Ablate schedule learning:** Compare (a) fixed uniform schedules, (b) fixed random schedules, (c) learned schedules on same architecture; measure both validation loss and downstream metrics to quantify learning benefit vs variance cost.
  3. **Scale analysis:** Train on datasets with varying L (number of features); plot learned schedule entropy vs L to test whether order learning becomes harder/more beneficial with more dimensions. Compare to TabDiff's reported failure to learn schedules on same data to diagnose parameterization or optimization differences.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can masked diffusion models with learned schedules be utilized to discover independencies and latent structures within tabular datasets?
  - Basis in paper: [explicit] The authors explicitly state in the Discussion section: "An interesting open question is whether masked diffusion with learned schedules can discover independencies and structures in tabular data."
  - Why unresolved: The current work focuses on establishing the theoretical equivalence between schedules and ordering, and verifying generation quality, without analyzing if the learned order corresponds to semantic data structure.
  - What evidence would resolve it: Experiments on datasets with known ground-truth structures or conditional independencies, demonstrating that the learned noise schedules align with the underlying graphical model.

- **Open Question 2:** How can the high loss variance induced by learned multivariate noise schedules be mitigated to improve training stability?
  - Basis in paper: [explicit] The Discussion notes that limited performance gains may be due to learned schedulers increasing loss variance, suggesting "Future work should focus on how these effects can be mitigated for multivariate schedules."
  - Why unresolved: The authors identify high variance as a likely cause for the discrepancy between lower validation loss and stable downstream performance, but do not propose a solution.
  - What evidence would resolve it: The development of variance-reduction techniques or regularizers for the noise schedule parameters that result in smoother training curves and improved fidelity metrics.

- **Open Question 3:** Why does the improvement in validation loss achieved by MDM(LS) not translate into significant gains in downstream data fidelity metrics?
  - Basis in paper: [inferred] The Results section shows MDM(LS) achieves lower validation loss (Table 2a) but performs similarly or only marginally better than baselines on metrics like Trend and Shape (Tables 1-3).
  - Why unresolved: The paper observes this phenomenon and hypothesizes causes like variance, but the disconnect between the optimized objective (likelihood/loss) and the evaluation criteria remains an unstated assumption requiring investigation.
  - What evidence would resolve it: Theoretical analysis explaining the link between the learned-order ELBO and specific fidelity metrics, or empirical results showing a schedule that significantly bridges this gap.

## Limitations

- The learned schedule mechanism assumes sufficient network capacity to differentiate predictions across token orders; simple architectures may collapse schedules to uniform values
- Learned schedules increase training loss variance compared to fixed linear schedules, which may explain why validation loss improvements don't translate to proportional gains in downstream metrics
- The practical benefit of learned schedules is modest in current experiments, with validation loss improvements not fully translating to downstream metric gains

## Confidence

**High confidence:** The theoretical decomposition of the MDM objective into order-weighted AR losses (Proposition 3.1) and the characterization of transition time distributions (Proposition 3.2) are mathematically rigorous and well-supported by the derivations.

**Medium confidence:** The practical implementation of learned schedules via RLOO gradient estimation and the empirical observations of modest performance improvements are supported by experimental results, though the variance hypothesis explaining limited gains requires further validation.

**Low confidence:** The claim that learned schedules will consistently provide meaningful practical benefits across different tabular datasets and architectures, given the observed small improvements and the open question about variance effects.

## Next Checks

1. **Schedule differentiation diagnostic:** Train MDM(LS) on a synthetic dataset where the optimal generation order is known (e.g., features with clear causal relationships). Visualize the learned w_ℓ values and sample generation orders to verify they align with the optimal order, confirming the learning mechanism works as intended.

2. **Variance sensitivity analysis:** Systematically vary the number of RLOO samples K and monitor both training loss variance and downstream metrics. Quantify the tradeoff between variance reduction and gradient accuracy to validate whether variance is indeed the limiting factor for performance gains.

3. **Architecture capacity ablation:** Train models with varying neural network capacities (different depths/widths) on the same datasets and compare the degree of schedule differentiation (entropy of sampled orders) and the resulting performance improvements. This would establish whether the learned schedule benefits are architecture-dependent.