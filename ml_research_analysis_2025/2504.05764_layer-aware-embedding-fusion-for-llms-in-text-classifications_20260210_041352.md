---
ver: rpa2
title: Layer-Aware Embedding Fusion for LLMs in Text Classifications
arxiv_id: '2504.05764'
source_url: https://arxiv.org/abs/2504.05764
tags:
- performance
- fusion
- embedding
- embeddings
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates layer-aware embedding fusion strategies
  for text classification using large language models (LLMs). The authors propose
  a method to select optimal layers for embedding extraction and combine embeddings
  from multiple LLMs without fine-tuning.
---

# Layer-Aware Embedding Fusion for LLMs in Text Classifications

## Quick Facts
- arXiv ID: 2504.05764
- Source URL: https://arxiv.org/abs/2504.05764
- Reference count: 11
- Primary result: Layer-aware embedding fusion improves text classification accuracy by selecting optimal layers from decoder-based LLMs and combining embeddings from complementary models without fine-tuning.

## Executive Summary
This paper introduces a layer-aware embedding fusion approach for text classification using large language models (LLMs). The method extracts hidden states from specific transformer layers, identifies optimal layers for classification tasks (typically mid-to-late layers in decoder models rather than final layers), and combines embeddings from multiple complementary models through learned projection and fusion operations. Experiments on four English text classification datasets demonstrate that this approach consistently outperforms single-layer or single-model baselines, with the best results achieved by fusing two models with optimally selected layers. The research highlights the importance of layer selection and model complementarity in embedding fusion strategies.

## Method Summary
The method involves extracting hidden states from specified layers of generation models (LLaMA2, Qwen2.5, Falcon3, Mistral, Gemma2) and embedding models (NV-Embed-v2, e5-large-v2), projecting embeddings to a unified dimension via learnable linear transformation with ReLU activation, and applying fusion operators (concatenation, sum, Hadamard, multiplication, quaternion, MoE, residual-enhanced) before training an MLP classifier. The approach does not fine-tune backbone models, instead learning only the projection and fusion components. Training uses batch size 100, learning rate 1e-4, Adam optimizer, and 120 epochs.

## Key Results
- Mid-to-late layers in decoder-based LLMs (layers 20-27 in 32-layer models) consistently outperform final layers for text classification tasks.
- Combining embeddings from two complementary models with optimal layer selection achieves highest accuracy on SST-2, MR, and R8 datasets.
- For the R52 dataset (52 classes), fusing embeddings from four models achieved peak performance.
- Quaternion fusion and Hadamard with residual connections performed best among tested fusion methods.

## Why This Works (Mechanism)

### Mechanism 1: Layer-Aware Selection in Decoder-Based LLMs
Decoder-based LLMs are trained for next-token prediction, causing final layers to specialize in token-level interactions and localized patterns rather than global semantic representations. Intermediate layers capture richer contextual understanding before the representation becomes overly specialized for generation. Task-relevant semantic information follows a non-monotonic trajectory through decoder layers, peaking before the final layer.

### Mechanism 2: Complementary Model Fusion Without Fine-Tuning
Different models develop distinct representational biases based on their pretraining data, architecture, and objectives. When models capture different aspects of semantic meaning, fusion creates a richer combined representation. However, redundant models add noise and computational cost without benefit. Model complementarity is more critical than standalone strength for successful fusion.

### Mechanism 3: Dimension-Unified Projection for Heterogeneous Embeddings
Linear projection with non-linear activation enables effective fusion of embeddings with different dimensions while preserving task-relevant information. The paper projects embeddings to a unified dimension (typically 1024) using learnable weight matrices with ReLU activation, allowing embeddings from models with different dimensions to be combined via various fusion operations.

## Foundational Learning

- **Hidden states vs. embeddings in transformer models**
  - Why needed here: The paper extracts hidden states from specific transformer layers rather than using model outputs. Understanding that hidden states at layer n capture intermediate representations is essential for layer selection.
  - Quick check question: Given a 32-layer decoder model, would you expect layer 32's hidden state to be better for sentiment classification than layer 24? Based on this paper, explain why or why not.

- **Linear projection with non-linearity**
  - Why needed here: The fusion pipeline requires projecting heterogeneous embeddings to a shared dimension. Understanding why ReLU follows the linear transformation (to introduce non-linearity before fusion) clarifies the design choice.
  - Quick check question: Why might a purely linear projection (without ReLU) be insufficient for fusing embeddings from architecturally different models?

- **Fusion operators (concatenation, Hadamard, quaternion)**
  - Why needed here: The paper tests seven fusion methods. Understanding what each operation preserves (concatenation preserves all information but increases dimensions; Hadamard emphasizes element-wise agreement; quaternion captures multi-dimensional relationships) enables informed method selection.
  - Quick check question: If two models produce highly correlated embeddings, which fusion method would likely perform worst, and why?

## Architecture Onboarding

- **Component map**: Embedding extraction -> Dimension projection -> Fusion operators -> Classifier head
- **Critical path**: 1) Identify optimal layer per model-dataset combination through exhaustive layer-wise evaluation 2) Extract embeddings from selected layers 3) Project to unified dimension via learned linear transformation + ReLU 4) Apply fusion operator 5) Train classifier on fused embeddings
- **Design tradeoffs**:
  - Single optimal layer vs. multi-layer averaging: Single layer achieves ~0.4% higher accuracy; multi-layer averaging provides stability but adds memory/compute overhead
  - Two-model vs. multi-model fusion: Two models with optimal layers achieve best performance on 3/4 datasets; four models provide stability and peak performance on R52 but require 4× memory
  - Fusion method selection: Quaternion and Hadamard with residual connections performed best; concatenation most scalable; multiplication and MoE showed inconsistent results
- **Failure signatures**:
  - Using final layer instead of mid-late layer causes ~4% accuracy drop on classification tasks
  - Combining models with similar representational patterns can degrade performance vs. single-model baseline
  - Five-model fusion requires 16,896 dimensions (~4.3GB for SST-2) with diminishing accuracy returns
- **First 3 experiments**:
  1. Layer sweep baseline: For each model, extract embeddings from all layers on SST-2 validation set. Plot accuracy vs. layer index to identify optimal layer.
  2. Two-model complementarity test: Select two architecturally different models. Compare fusion performance using final layers vs. optimal layers. Test concatenation, Hadamard, and quaternion fusion methods.
  3. Scaling limit analysis: Incrementally add models (2→3→4→5) and measure both accuracy and memory consumption. Identify the point where accuracy gains plateau relative to memory cost.

## Open Questions the Paper Calls Out

### Open Question 1
Can automated methods for identifying task-optimal embedding layers be developed that eliminate the need for exhaustive empirical layer-by-layer evaluation? The authors aim to develop "frameworks for automatically identifying the most suitable layers for specific tasks or domains" to reduce the current "trial-and-error" requirement.

### Open Question 2
Does layer-aware embedding fusion generalize to multilingual datasets and specialized domains such as medical or legal text classification? The study is "limited by its focus on widely used English datasets (SST-2, MR, R8, R52), leaving the effectiveness in multilingual or domain specific contexts (e.g., medical, legal) largely unverified."

### Open Question 3
Can quantitative metrics be developed to predict model complementarity before fusion, rather than relying on post-hoc performance evaluation? The paper notes that "combining two individually strong models did not always result in superior performance" and that "model complementarity is more critical than standalone strength," but provides no principled method for predicting which models will be complementary.

## Limitations
- Focus on English text classification with relatively small datasets (max 10k test samples)
- Missing implementation details for MLP classifier architecture and exact embedding extraction method
- Requires computational resources for exhaustive layer-wise sweeps across multiple LLMs
- Uncertainty about generalization to multilingual tasks, larger datasets, or different NLP tasks

## Confidence

**High confidence**: The core finding that mid-to-late decoder layers outperform final layers for classification tasks is strongly supported by consistent experimental evidence across all four datasets.

**Medium confidence**: The specific optimal layers identified may be dataset-dependent and could shift with different training objectives or model architectures. The projection dimension of 1024 is chosen but not systematically validated.

**Low confidence**: The generalizability of quaternion fusion being optimal for two-model combinations and concatenation for multi-model fusion across different task domains or model families remains uncertain without broader experimental validation.

## Next Checks

1. **Cross-dataset layer transferability test**: Validate whether optimal layers identified on SST-2 transfer to similar sentiment analysis datasets (e.g., IMDB, Amazon reviews) without retraining.

2. **Memory-accuracy tradeoff analysis**: Systematically evaluate the 4-model fusion configuration on smaller datasets to determine if accuracy gains justify 4× memory cost in typical deployment scenarios with memory constraints.

3. **Architecture dependency validation**: Test the layer-aware fusion approach with encoder-only models (BERT, RoBERTa) and encoder-decoder models (T5) to determine if mid-to-late layer superiority holds across different transformer architectures beyond decoder-only LLMs.