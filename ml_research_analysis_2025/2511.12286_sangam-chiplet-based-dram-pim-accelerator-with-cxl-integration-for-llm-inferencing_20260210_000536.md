---
ver: rpa2
title: 'Sangam: Chiplet-Based DRAM-PIM Accelerator with CXL Integration for LLM Inferencing'
arxiv_id: '2511.12286'
source_url: https://arxiv.org/abs/2511.12286
tags:
- sangam
- logic
- dram
- memory
- inference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Sangam addresses memory bottlenecks in large language model inference
  by introducing a chiplet-based DRAM-PIM accelerator with CXL integration. The approach
  decouples logic and memory into heterogeneous chiplets, enabling advanced processing
  elements like systolic arrays without sacrificing DRAM capacity.
---

# Sangam: Chiplet-Based DRAM-PIM Accelerator with CXL Integration for LLM Inferencing

## Quick Facts
- arXiv ID: 2511.12286
- Source URL: https://arxiv.org/abs/2511.12286
- Reference count: 40
- Key outcome: Sangam achieves 3.93x, 4.22x, and 2.82x speedup in end-to-end query latency and 10.3x, 9.5x, and 6.36x greater decoding throughput compared to H100 GPU across LLaMA 2-7B, Mistral-7B, and LLaMA 3-70B.

## Executive Summary
Sangam addresses memory bottlenecks in large language model inference by introducing a chiplet-based DRAM-PIM accelerator with CXL integration. The approach decouples logic and memory into heterogeneous chiplets, enabling advanced processing elements like systolic arrays without sacrificing DRAM capacity. Sangam achieves 3.93x, 4.22x, and 2.82x speedup in end-to-end query latency, 10.3x, 9.5x, and 6.36x greater decoding throughput, and order-of-magnitude energy savings compared to an H100 GPU across various configurations of LLaMA 2-7B, Mistral-7B, and LLaMA 3-70B.

## Method Summary
Sangam uses HARMONI simulation framework modeling hierarchical PIM architecture with DDR5 timing (DRAMsim3), SRAM modeling (FN-CACTI), and logic synthesis (Synopsys DC). The configuration includes 4 modules, 4 ranks per module (2 wt, 2 kv), 16 chips per module. Each logic chiplet contains 8×8 FP16 systolic arrays per bank, 16-lane SIMD multipliers, 256KB SRAM, and operates at 400MHz. Tensor mapping follows rank-level (KV cache vs weights separation), chip-level (head-wise/column-wise), bank-level (row-wise partitioning), and input-stationary systolic dataflow.

## Key Results
- 3.93x, 4.22x, and 2.82x speedup in end-to-end query latency for LLaMA 2-7B, Mistral-7B, and LLaMA 3-70B respectively
- 10.3x, 9.5x, and 6.36x greater decoding throughput compared to H100 GPU
- Order-of-magnitude energy savings across all tested configurations

## Why This Works (Mechanism)

### Mechanism 1: Chiplet-Based Logic-Memory Decoupling
Separating DRAM banks from peripheral logic into chiplets fabricated in heterogeneous technology nodes enables advanced compute units without sacrificing memory capacity. DRAM banks remain in DRAM-process chiplets optimized for density; the "center stripe" moves to logic-process chiplets connected via interposer. This exposes full bank-level bandwidth (~400 GB/s per chip) to logic-process compute while preserving 100% DRAM capacity. The interposer wire pitch (≥4.7 µm) maintains signal integrity at DDR5 bank-interface bandwidth without timing violations.

### Mechanism 2: Small Systolic Arrays for Flat GEMMs
Many small (8×8) systolic arrays distributed across banks outperform few large arrays for LLM inference's "flat" GEMM shapes (small M, large K/N). LLM decode-phase GEMVs have M=1; even batched decode with B=8 yields M=8. Large systolic arrays underutilize. An 8×8 array width matches DDR5's 128-bit bank interface, enabling rate-matched streaming at 400 MHz lockstep with DRAM tCCD. Input-stationary dataflow maximizes reuse of input tiles while weights stream through.

### Mechanism 3: Hierarchical Tensor Mapping with Weight/KV Separation
Disaggregating weights and KV cache into separate ranks with hierarchical partitioning minimizes inter-chip communication and maximizes bank-level parallelism. Rank-level: "wt ranks" hold static weights; "kv ranks" hold dynamically-growing KV cache. Chip-level: head-wise partitioning for attention, column-wise for projections. Bank-level: row-wise K-dimension partitioning enables parallel reduction via adder trees. Array-level: input-stationary dataflow.

## Foundational Learning

- **LLM Inference Phases (Prefill vs. Decode)**: Why needed: Sangam optimizes differently for each phase—decode benefits most from PIM (memory-bound GEMV), prefill can be compute-bound and may favor GPU for large inputs. Quick check: For batch=1, input=2048, output=128, which phase dominates E2E latency on Sangam vs. H100?

- **Operational Intensity and Roofline Analysis**: Why needed: Determines whether a kernel is memory-bound (OI < ridge point) or compute-bound. Sangam targets the memory-bound region; Figure 3 roofline shows crossover points. Quick check: Given OI=8 FLOPs/byte for decode projections, is H100 or Sangam D1 bandwidth-bound?

- **Systolic Array Dataflows (Weight/Input/Output Stationary)**: Why needed: Sangam chooses input-stationary for flat GEMMs—understanding why requires knowing reuse patterns and SRAM constraints. Quick check: For GEMM(M=8, K=4096, N=4096), which element has highest reuse, and which dataflow exploits it?

## Architecture Onboarding

- **Component map**: Host CPU ←→ CXL Switch ←→ [Sangam Module] × N ←→ Rank 0-3 (2 wt, 2 kv) ←→ MCoOI ←→ DRAM Bank Chiplets ×64 (2 banks each) + Logic Chiplet (center stripe + PIM) ←→ 8×8 Systolic Array ×32 banks + SIMD Multiplier ×32 banks + Adder Tree + Max Tree + SRAM Buffer

- **Critical path**: Host → CXL → DMA load model weights to wt ranks (one-time) → Per-request: Input tokens → DMA to Sangam → Prefill (attention + projections) → Decode loop (GEMV-heavy) → Output tokens → Interrupt host → Bottleneck: Decode-phase GEMV bandwidth (10.3× throughput gain over H100 for LLaMA 2-7B)

- **Design tradeoffs**: D1 (128 GB, 16 chips/rank) vs. D3 (128 GB, 8 chips/rank): D1 reduces queueing delay (21%→23%) but increases communication overhead; D3 better for GQA models (1 head/chip). Systolic array size: 8×8 maximizes bank utilization for small M; larger arrays would idle for batch=1 decode. KV/wt rank split: Simplifies allocation but reserves capacity; long-context workloads may need >2 kv ranks.

- **Failure signatures**: TTFT SLO violation: Input length > crossover point (~256 for B=1, ~32 for B=8 on D1) without GPU prefill offload. Queueing spike: Reducing chips from 16→8 (D1→D3) increases contention; observe queueing % in latency breakdown. Communication-dominated E2E: Large inputs (2048) with small outputs show 28.5% communication overhead in D2/D4.

- **First 3 experiments**: 1) Bandwidth microbenchmark: Measure sustained GB/s for all-bank parallel reads at 400 MHz lockstep; verify ~51.2 TB/s aggregate for D1. 2) Flat GEMM sweep: Benchmark GEMM(M, 4096, 4096) for M∈{1, 8, 16, 32, 64} on systolic array; plot utilization vs. H100 baseline. 3) Crossover calibration: For target SLO (e.g., TTFT < 1.5s), sweep input length and batch size to find where Sangam requires GPU prefill offload.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal dynamic scheduling policy to partition prefill and decode phases between Sangam and a host GPU based on input length?
- Basis in paper: Section V.C states, "One solution is to use the GPU for prefill... perhaps only when the input length exceeds the TTFT crossover point," suggesting a hybrid approach to mitigate Sangam's slowdown on large inputs.
- Why unresolved: The current evaluation compares the architectures in isolation; the runtime algorithm required to dynamically switch execution devices mid-inference is not implemented or modeled.
- What evidence would resolve it: A simulation demonstrating end-to-end latency improvements when a runtime switches execution targets at the identified crossover boundaries (e.g., input size > 2048).

### Open Question 2
- Question: How does the CXL interconnect bandwidth constrain performance scaling as the number of Sangam modules increases for large models?
- Basis in paper: Section V.D (Observation O2) notes that increasing capacity (modules) improves compute time but causes communication overhead to rise from 14.5% to 28.5% of E2E latency.
- Why unresolved: The study evaluates fixed configurations (up to 512GB) but does not determine the saturation point where CXL link contention overwhelms the benefits of added PIM compute resources.
- What evidence would resolve it: Scaling sensitivity analysis showing throughput degradation curves relative to the number of active modules sharing the CXL switch bandwidth.

### Open Question 3
- Question: What are the data coherence and synchronization overheads for the proposed "co-execution" mode where Sangam offloads large GEMMs to a GPU?
- Basis in paper: The Abstract and Introduction state Sangam can "co-execute alongside the GPUs" and offload large GEMMs, but the evaluation focuses on Sangam as a standalone unit or drop-in replacement.
- Why unresolved: The mechanism for sharing memory spaces between CXL-attached PIM and a discrete GPU (likely requiring data movement over PCIe/CXL) is proposed but not benchmarked.
- What evidence would resolve it: Micro-benchmarks quantifying the latency and energy cost of moving activation tensors from Sangam to a GPU and back during a single inference pass.

## Limitations

- Chiplet Integration Bandwidth: Performance claims rely critically on interposer D2D bandwidth sustaining ~32 GB/s per lane without timing violations, which lacks empirical validation.
- Systolic Array Granularity Trade-off: 8×8 array selection assumes small M dimensions (<64) remain typical, but may not generalize to larger batch sizes or different workload patterns.
- CXL Switch Overhead: 28.5% communication overhead for large inputs suggests non-negligible CXL switch contention not fully explored in scaling analysis.

## Confidence

**High Confidence**: Chiplet-based logic-memory decoupling mechanism and its fundamental benefits (100% DRAM capacity preservation, full bank bandwidth exposure) are well-grounded in established chiplet design principles and supported by detailed architectural specifications.

**Medium Confidence**: Systolic array size selection (8×8) and input-stationary dataflow optimization are theoretically sound given flat GEMM characteristics, but rely on workload assumptions that may not hold across all LLM inference scenarios.

**Low Confidence**: Energy consumption comparisons assume specific power models without detailed measurement methodologies; 80% TDP approximation for H100 and logic chiplet power estimation lack empirical validation.

## Next Checks

1. **Interposer Bandwidth Validation**: Implement microbenchmark measuring sustained D2D bandwidth across interposer under all-bank parallel access patterns at 400 MHz lockstep. Verify 51.2 TB/s aggregate bandwidth claim for D1 configuration holds under realistic thermal and signal integrity conditions.

2. **Crossover Point Calibration**: For target SLO (e.g., TTFT < 1.5s), systematically sweep input length (32-2048 tokens) and batch size (1-64) to identify where Sangam requires GPU prefill offload. Validate that reported crossover points (256 tokens for B=1, 32 tokens for B=8 on D1) accurately predict performance degradation.

3. **Flat GEMM Utilization Study**: Benchmark GEMM(M, 4096, 4096) across M ∈ {1, 8, 16, 32, 64, 128, 256} on both 8×8 systolic arrays and simulated larger array implementation. Plot utilization curves to verify small arrays maintain higher efficiency for M<64 while larger arrays become competitive at higher M values.