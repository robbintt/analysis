---
ver: rpa2
title: 'ESGBench: A Benchmark for Explainable ESG Question Answering in Corporate
  Sustainability Reports'
arxiv_id: '2511.16438'
source_url: https://arxiv.org/abs/2511.16438
tags:
- esgbench
- reports
- question
- evidence
- numeric
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ESGBench is a benchmark for explainable ESG question answering
  using corporate sustainability reports. It provides a reproducible pipeline to collect
  ESG/TCFD reports, build searchable indices, generate QA pairs with verbatim evidence,
  and evaluate predictions across exact match, F1, numeric accuracy, and retrieval
  recall.
---

# ESGBench: A Benchmark for Explainable ESG Question Answering in Corporate Sustainability Reports

## Quick Facts
- **arXiv ID**: 2511.16438
- **Source URL**: https://arxiv.org/abs/2511.16438
- **Reference count**: 9
- **Primary result**: A retrieval-augmented generation baseline achieves 21% exact match, 55% string F1, and 45% numeric accuracy on ESG question answering over corporate sustainability reports.

## Executive Summary
ESGBench introduces a reproducible pipeline for explainable ESG question answering using corporate sustainability and TCFD reports. The benchmark provides a comprehensive framework including PDF ingestion, dual-view indexing (narrative chunks and table rows), automatic QA pair generation with verbatim evidence anchoring, and evaluation across exact match, string F1, numeric accuracy, and retrieval recall metrics. A simple retrieval-augmented generation baseline demonstrates modest performance, particularly struggling with numeric KPI extraction and table-based data. The benchmark aims to accelerate research in transparent, accountable ESG-focused AI systems by providing both the evaluation framework and initial baseline results.

## Method Summary
The benchmark follows a reproducible pipeline: seed CSV metadata drives PDF downloads, which are then preprocessed into narrative chunks (600-1200 characters with page indices) and parsed table rows using pdfplumber and Camelot. Constrained prompts generate QA pairs requiring verbatim answers with preserved numbers and units, along with exact evidence quotes. A retrieval-augmented generation baseline uses dense embeddings over chunks with top-k retrieval, followed by constrained generation enforcing single-span answers from retrieved context. Evaluation metrics include exact match, string F1, numeric accuracy (±2% tolerance with unit awareness), and retrieval recall@K, with per-category scores for Environmental, Social, Governance, Strategy, and Risk domains.

## Key Results
- Baseline RAG system achieves 21% exact match and 55% string F1 across all ESG categories
- Numeric accuracy remains challenging at 45%, with particular difficulty handling unit variations and scale words
- Retrieval recall@5 ranges from 70-80%, indicating retriever finds relevant pages but reader struggles with extraction
- Environmental category shows lower performance than Governance, suggesting domain-specific challenges
- Table-derived QAs underperform significantly, highlighting limitations in current table parsing approaches

## Why This Works (Mechanism)

### Mechanism 1: Retrieval-Constrained Generation for Grounding
- Claim: Dense retrieval over chunked documents constrains LLM outputs to source content, enabling traceable answers.
- Mechanism: Embed passages/tables → top-k retrieval per question → constrained prompt forces single-span answer from retrieved context only.
- Core assumption: Relevant evidence exists within retrieved chunks; retrieval quality bounds answer quality.
- Evidence anchors: [abstract] shows 21% exact match baseline; [section 4] describes dense embeddings and top-k retrieval; [corpus] confirms related benchmarks use similar RAG patterns.
- Break condition: When gold evidence spans multiple non-adjacent chunks or requires synthesis across pages, single-span constraint fails.

### Mechanism 2: Verbatim Evidence Anchoring
- Claim: Requiring exact evidence quotes creates an auditable verification chain for each answer.
- Mechanism: QA generation prompts enforce: (a) answerable only from provided text, (b) answers should be verbatim with preserved numbers/units, (c) evidence_quote copied exactly with page number.
- Core assumption: Correct answers correspond to extractable verbatim spans; no synthesis or inference required.
- Evidence anchors: [section 3.3] specifies prompt constraints; [section 5] evaluates retrieval recall; [corpus] shows ESGenius-QA uses different format.
- Break condition: When context is distributed or implicit (e.g., inferring scope from document structure), verbatim extraction under-specifies the task.

### Mechanism 3: Dual-View Document Indexing
- Claim: Separately processing narrative chunks and table rows addresses heterogeneity in sustainability reports.
- Mechanism: PDF → passage chunks (600–1200 chars with page indices) + table rows (parsed via pdfplumber/Camelot, normalized with optional value/unit).
- Core assumption: Tables can be flattened to row-level summaries without losing semantic relationships.
- Evidence anchors: [section 3.2] describes dual-view creation; [section 6] identifies table parsing challenges; [corpus] shows MMESGBench addresses multimodal ESG understanding.
- Break condition: Complex tables with merged cells, multi-row headers, or implicit unit inheritance fail standard extraction tools.

## Foundational Learning

- **Concept: Retrieval-Augmented Generation (RAG)**
  - Why needed here: ESGBench's baseline architecture is a RAG system; understanding retrieval-reader interaction is prerequisite for improvement.
  - Quick check question: If retrieval recall@5 is 75% but exact match is 21%, where is the performance bottleneck?

- **Concept: ESG/TCFD Disclosure Framework**
  - Why needed here: Benchmark targets domain-specific KPIs (Scope 1–3 emissions, renewable energy %, diversity metrics); lacking domain knowledge misinterprets failure modes.
  - Quick check question: Why might Scope 3 emissions be harder to extract than Scope 1?

- **Concept: Unit-Aware Numeric Evaluation**
  - Why needed here: 45% numeric accuracy; errors stem from unit mismatches ("tCO2e" vs "ktCO2e") and scale words ("million").
  - Quick check question: How would you normalize "1.5 MtCO2e" and "1,500,000 tCO2e" for comparison?

## Architecture Onboarding

- **Component map:**
  Seed CSV → PDF Ingestion → chunks.json + {DOC}_tables.json → QA Generation (constrained prompts) → esgbench_open_source.jsonl (gold) → RAG Baseline: Embedder → Retriever → Reader → Evaluation: EM / F1 / Numeric / Recall@K

- **Critical path:** Table parsing quality → Embedding representation → Retrieval recall → Constrained generation fidelity

- **Design tradeoffs:**
  - Chunk size 600–1200 chars: Smaller chunks improve precision but may fragment context across boundaries.
  - General-purpose vs domain embeddings: Current baseline uses general embeddings; ClimateBERT/FinBERT may improve recall but add complexity.
  - Automatic QA generation vs human validation: Speed and scale vs reliability; noise acknowledged in section 7.

- **Failure signatures:**
  - Low numeric accuracy (45%) with high string F1 (55%): Unit handling broken, text content partially correct.
  - Recall@5 = 70–80% but EM = 21%: Retriever finds page but reader fails extraction—likely prompt or span-constraint issue.
  - Table-derived QAs underperforming: Parsing tool limitations on complex layouts.

- **First 3 experiments:**
  1. Replace general embeddings with ClimateBERT or FinBERT; measure retrieval recall@5 change on Environmental vs Governance categories.
  2. Add explicit unit normalization pre-processing (detect scale words, convert to base units); re-evaluate numeric accuracy@±2%.
  3. Swap pdfplumber/Camelot for layout-aware table parser (e.g., Table Transformer); isolate table-derived QA accuracy improvement.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does human validation of the automatically generated QA pairs reveal systematic context gaps or hallucinations that the current synthetic generation pipeline misses?
- Basis in paper: [explicit] The authors explicitly list "Human validation of a stratified QA subset" as a primary avenue for future work, noting that "automatic QA generation can miss context."
- Why unresolved: The current benchmark relies entirely on LLM-generated questions and answers, which introduces noise and potential "easily extractable span" bias that automated metrics cannot fully assess.
- What evidence would resolve it: A comparative study evaluating model performance on the current synthetic dataset versus a new, human-curated "gold standard" subset.

### Open Question 2
- Question: To what extent can layout-aware table parsing methods improve the low numeric accuracy (45.3%) observed in the baseline experiments?
- Basis in paper: [explicit] The paper identifies "layout-aware table parsing" as a specific future work item and highlights that "variants like 'tCO2e' and scale words ('million') create fragile failure modes."
- Why unresolved: The baseline uses general-purpose chunking and basic parsing (pdfplumber/Camelot), which fails to reliably capture the complex structures and units common in ESG tables.
- What evidence would resolve it: Benchmark results from a system employing advanced document AI (e.g., vision-language models for tables) demonstrating significantly higher Numeric Accuracy and F1 scores on table-derived questions.

### Open Question 3
- Question: How does model performance and retrieval robustness change when aligning the corpus with emerging non-Western or strict regulatory taxonomies (e.g., BRSR, CSRD)?
- Basis in paper: [explicit] The authors state the "seed corpus favors large multinationals with English-language reports" and explicitly call for "multilingual reports" and "alignment to CSRD/BRSR/ISSB taxonomies."
- Why unresolved: It is unclear if the current modest performance (21% Exact Match) is a result of model limitations or the heterogeneous, loosely structured nature of the current report collection.
- What evidence would resolve it: Evaluation results from a version of ESGBench explicitly mapped to CSRD/BRSR standards, measuring retrieval recall improvements and domain alignment consistency.

## Limitations

- Benchmark relies entirely on automatically generated QA pairs without human validation, introducing potential noise in gold labels and evidence quotes.
- Numeric accuracy evaluation uses ±2% tolerance, which may mask systematic errors in unit conversion or scale word interpretation.
- Table parsing quality heavily depends on pdfplumber/Camelot performance, which struggles with complex layouts, merged cells, and implicit unit inheritance.

## Confidence

- **High confidence**: Retrieval-augmented generation as baseline approach, evaluation metrics (EM/F1/numeric accuracy/recall@K), and general benchmark construction pipeline.
- **Medium confidence**: Domain-specific performance differences across ESG categories (Environmental vs Governance), given limited per-category data points.
- **Low confidence**: Claims about "explainability" benefits, as the benchmark focuses on traceable evidence rather than interpretability of model decisions.

## Next Checks

1. Manually validate 20 random QA pairs to estimate gold label accuracy and identify systematic generation errors.
2. Compare retrieval recall@5 using general-purpose vs domain-specific embeddings (ClimateBERT/FinBERT) on Environmental and Governance subsets.
3. Measure performance impact of explicit unit normalization preprocessing by isolating numeric QA pairs with scale words and unit mismatches.