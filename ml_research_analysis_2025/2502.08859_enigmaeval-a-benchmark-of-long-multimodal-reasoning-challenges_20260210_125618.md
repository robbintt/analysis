---
ver: rpa2
title: 'EnigmaEval: A Benchmark of Long Multimodal Reasoning Challenges'
arxiv_id: '2502.08859'
source_url: https://arxiv.org/abs/2502.08859
tags:
- puzzle
- puzzles
- reasoning
- answer
- solution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Puzzle-solving events offer complex multimodal challenges ideal
  for evaluating frontier language models, but existing benchmarks focus on specific
  puzzle types or text-only problems. This work introduces EnigmaEval, a dataset of
  1,184 puzzles from puzzle hunts with varying difficulty levels, featuring text and
  images that require multi-step reasoning and hidden connection discovery.
---

# EnigmaEval: A Benchmark of Long Multimodal Reasoning Challenges

## Quick Facts
- arXiv ID: 2502.08859
- Source URL: https://arxiv.org/abs/2502.08859
- Reference count: 40
- Models achieve 7.0% accuracy on normal puzzles and 0% on hard puzzles, significantly underperforming expert humans

## Executive Summary
EnigmaEval is a benchmark dataset of 1,184 puzzle hunt challenges designed to evaluate frontier language models' capabilities in long multimodal reasoning. Unlike existing benchmarks focused on specific puzzle types or text-only problems, EnigmaEval puzzles require discovering hidden connections between seemingly unrelated information without explicit instructions. The dataset includes both original PDF/webpage formats and human-transcribed text-image representations, enabling researchers to distinguish between parsing and reasoning limitations. Current models show extremely poor performance (7.0% on normal puzzles, 0% on hard puzzles), revealing significant gaps in their ability to handle unstructured multimodal reasoning challenges.

## Method Summary
EnigmaEval evaluates frontier multimodal LLMs on puzzle hunt challenges requiring multi-step reasoning, hidden connection discovery, and integration of text/images without explicit instructions. The dataset contains 1,184 puzzles from 8 sources (PuzzledPint, MIT Mystery Hunt, Labor Day Extravaganza, etc.) in two formats: original PDFs/webpage screenshots as PNGs, and human-transcribed standardized text-image representations. Puzzles are split into normal (949) and hard (235) subsets based on source difficulty. Models are prompted with system templates requiring step-by-step reasoning and standardized answer format ("Answer: <answer>"), with default temperature. For meta-puzzles (77 total), models receive correct component answers in the prompt. Accuracy is measured via string matching against ground-truth solutions across three evaluation runs.

## Key Results
- Tested models achieved 7.0% accuracy on normal puzzles and 0% on hard puzzles
- Performance dropped significantly for some models when moving from transcribed to raw PDF formats (e.g., Llama 3.2 90B Vision: 0.5% → 0.1%)
- All models completely failed on hard puzzles (0% accuracy), highlighting the benchmark's ability to expose reasoning limitations
- Results significantly underperform expert human solvers and other difficult benchmarks like Humanity's Last Exam

## Why This Works (Mechanism)

### Mechanism 1: Unstructured Multimodal Reasoning Probe
If models are presented with puzzles that lack explicit instructions and combine text/images, performance drops significantly because models must infer both the task structure and solution path simultaneously. EnigmaEval puzzles deliberately omit solving instructions, forcing models to engage in lateral thinking (discovering hidden connections) rather than applying known solution templates. The multimodal aspect (text + images + layout) compounds difficulty as models must integrate visual parsing with semantic reasoning.

### Mechanism 2: Separation of Parsing vs. Reasoning via Dual Format
Providing both raw PDF/PNG formats and human-transcribed text-image formats enables attribution of failures to OCR/parsing vs. reasoning deficits. The transcribed format removes document-layout complexity while preserving semantic content. Performance deltas between formats isolate preprocessing bottlenecks.

### Mechanism 3: Difficulty Stratification via Domain Expertise Requirements
Puzzles stratified by source difficulty (Normal vs. Hard) reveal that current models completely fail on problems requiring multi-step deduction with minimal intermediate verification. Hard puzzles require "five or more non-trivial steps with minimal verification" and may have only thematic hints for intermediate answers.

## Foundational Learning

- **Concept: Lateral / Creative Reasoning**
  - Why needed here: EnigmaEval puzzles require solvers to identify non-obvious connections (e.g., acrostics, indexing, alpha-numeric codes) without being told which mechanism to apply.
  - Quick check question: Can you explain why a model that aces MATH might still fail on a puzzle where the first step is to notice the first letters of each clue spell a hint?

- **Concept: Multimodal Integration**
  - Why needed here: Puzzles include images, grids, and layout cues that are semantically meaningful. Models must fuse visual parsing with textual reasoning.
  - Quick check question: If a puzzle's answer depends on the spatial arrangement of images relative to text, would a text-only model have sufficient information?

- **Concept: OCR / Document Parsing Bottlenecks**
  - Why needed here: Some performance gaps are due to failure to extract text from PDFs rather than failure to reason.
  - Quick check question: Why does providing a human transcription sometimes improve performance for the same underlying puzzle?

## Architecture Onboarding

- **Component map:** Dataset / 1,184 puzzles → split into Normal (949) / Hard (235) → Formats → Raw (PDF/PNG) vs. Transcribed (text + image placeholders) → Evaluation Pipeline → Prompt with tips → Model response → Answer extraction (regex for "Answer: ...") → String match vs. ground truth → Meta-puzzle handling → Provide correct component answers in prompt, test meta-reasoning in isolation

- **Critical path:** 1. Load puzzle (select format: raw or transcribed). 2. Apply format-specific system prompt (with tips for mechanisms like indexing, acrostics). 3. Generate model response at temperature=0 (or default). 4. Extract answer via regex patterns (single / pair / list). 5. Compare to ground-truth solution (exact string match).

- **Design tradeoffs:** Providing solving tips in the prompt helps standardize prompting but may artificially boost models that are good at instruction-following rather than discovery. Human transcription isolates reasoning from parsing but risks losing spatial/layout signals and is not scalable to new puzzles. Private dataset prevents memorization but limits reproducibility and community auditing.

- **Failure signatures:** Parsing bottleneck: Large performance drop from transcribed to raw format (e.g., Llama 3.2 90B Vision: 0.5% → 0.1%). Short-horizon reasoning: Model proposes plausible but incorrect answer early, fails to explore alternatives. Knowledge gap: Model identifies mechanism but lacks cultural/trivia knowledge to complete (e.g., fails on TV show references). Attention failure on meta-puzzles: Model ignores provided component answers (low meta-puzzle solve rates in Table C.1).

- **First 3 experiments:**
  1. **Format Ablation:** Run the same set of puzzles in both raw and transcribed formats on a single model. Quantify the performance delta to isolate parsing vs. reasoning contribution.
  2. **Prompt Sensitivity Test:** Remove the general tips from the system prompt and compare accuracy. This tests how much performance relies on explicit mechanism hints vs. intrinsic reasoning.
  3. **Step-Depth Analysis:** Sample N normal puzzles and N hard puzzles. For each, use a strong model to generate a step-by-step solution attempt. Manually annotate at which step the model fails (mechanism identification, knowledge retrieval, indexing errors, etc.) to characterize the failure distribution.

## Open Questions the Paper Calls Out

### Open Question 1
What specific capabilities cause models to fail completely (0% accuracy) on the hard puzzle split while achieving non-trivial performance on normal puzzles? The paper documents the performance gap but does not conduct fine-grained error analysis identifying which specific reasoning steps, knowledge types, or multi-step coordination abilities are missing.

### Open Question 2
Why do some frontier models show dramatic performance degradation on raw PDF formats while others do not? The paper notes the variance across models but does not identify which specific document processing components (OCR accuracy, layout understanding, figure-text alignment) cause the performance gap.

### Open Question 3
What prevents models from effectively utilizing provided component answers when solving meta-puzzles? Table C.1 shows meta-puzzle solve rates remain very low (0.0–8.7%) even when models are given correct answers to preceding puzzles, suggesting attention or integration failures not addressed in the paper.

## Limitations

- The private nature of EnigmaEval prevents independent verification and community auditing of the benchmark design and methodology
- Human transcription may introduce artifacts or lose critical spatial/layout information that is semantically meaningful in puzzle-solving contexts
- Exact string matching evaluation may be overly strict, potentially penalizing models for legitimate answer variants

## Confidence

**High Confidence**: The claim that current frontier models perform extremely poorly on EnigmaEval (7.0% normal, 0% hard) is well-supported by the presented results and represents a direct empirical observation.

**Medium Confidence**: The separation of parsing vs. reasoning limitations through dual-format presentation is logically sound and supported by observed performance deltas, though the assumption that human transcription perfectly preserves all task-relevant information remains unverified.

**Low Confidence**: The assertion that hard puzzles specifically require "five or more non-trivial steps with minimal verification" is described in methodology but not empirically validated through model failure analysis.

## Next Checks

1. **Error Mode Analysis**: Conduct systematic error analysis of model failures on stratified samples of normal and hard puzzles, categorizing failures into mechanism identification, knowledge retrieval, indexing errors, OCR failures, and attention failures.

2. **String Matching Robustness Test**: Implement and test alternative answer matching schemes (fuzzy matching, canonicalization rules) to assess whether exact string matching is overly penalizing models for legitimate answer variants.

3. **Cross-Benchmark Comparison**: Evaluate the same models on both EnigmaEval and Humanity's Last Exam using identical prompt templates and answer extraction methods to validate whether EnigmaEval is genuinely more challenging.