---
ver: rpa2
title: 'Beyond ImageNet: Understanding Cross-Dataset Robustness of Lightweight Vision
  Models'
arxiv_id: '2511.00335'
source_url: https://arxiv.org/abs/2511.00335
tags:
- vision
- datasets
- mobile
- imagenet
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the gap in benchmarking lightweight vision
  models beyond ImageNet, showing that ImageNet performance does not reliably predict
  robustness on fine-grained or medical datasets. The authors evaluate 11 mobile models
  (2.5M parameters) across 7 diverse datasets under a unified training protocol.
---

# Beyond ImageNet: Understanding Cross-Dataset Robustness of Lightweight Vision Models

## Quick Facts
- arXiv ID: 2511.00335
- Source URL: https://arxiv.org/abs/2511.00335
- Reference count: 40
- This paper addresses the gap in benchmarking lightweight vision models beyond ImageNet, showing that ImageNet performance does not reliably predict robustness on fine-grained or medical datasets.

## Executive Summary
This paper addresses the gap in benchmarking lightweight vision models beyond ImageNet, showing that ImageNet performance does not reliably predict robustness on fine-grained or medical datasets. The authors evaluate 11 mobile models (2.5M parameters) across 7 diverse datasets under a unified training protocol. They introduce the Cross-Dataset Score (xScore) to measure accuracy and consistency, revealing that ImageNet accuracy does not guarantee generalization. The four best proxy datasets for xScore evaluation are identified. Architectural elements such as isotropic convolutions with higher spatial resolution and channel-wise attention promote broader generalization, while transformer-based blocks yield little additional benefit under tight parameter constraints. The study provides a reproducible framework and actionable insights for designing robust, efficient mobile vision models.

## Method Summary
The authors evaluate 11 lightweight mobile models with approximately 2.5 million parameters across 7 diverse datasets including ImageNet, fine-grained classification datasets, and medical imaging datasets. They implement a unified training protocol across all experiments and introduce the Cross-Dataset Score (xScore) metric to quantify both accuracy and consistency across datasets. The study employs systematic ablation studies to identify architectural elements that promote cross-dataset generalization, comparing various model architectures and components while controlling for parameter constraints.

## Key Results
- ImageNet performance does not reliably predict model robustness on fine-grained or medical datasets
- The four best proxy datasets for xScore evaluation were identified through systematic analysis
- Isotropic convolutions with higher spatial resolution and channel-wise attention improve cross-dataset generalization
- Transformer-based blocks show limited additional benefit under tight parameter constraints

## Why This Works (Mechanism)
The study reveals that cross-dataset robustness requires different architectural considerations than single-dataset optimization. Models optimized solely for ImageNet performance develop features that may not transfer well to datasets with different characteristics, such as fine-grained distinctions or medical imaging modalities. The introduction of the xScore metric captures this gap by measuring both accuracy and consistency across diverse datasets, providing a more holistic evaluation framework than traditional single-dataset benchmarks.

## Foundational Learning
- **Cross-dataset generalization**: Understanding how models perform across different data distributions - needed because real-world deployment requires robustness beyond training domains; quick check: compare accuracy drop across dataset pairs
- **Mobile model constraints**: Working within tight parameter budgets (2.5M) - needed to address practical deployment scenarios; quick check: verify parameter count per model
- **Unified training protocols**: Standardized experimental setup across diverse datasets - needed to ensure fair comparison; quick check: confirm consistent data augmentation and optimization settings
- **xScore metric**: Novel evaluation combining accuracy and consistency - needed to quantify cross-dataset performance; quick check: validate metric calculation across dataset pairs
- **Architectural ablation studies**: Systematic component evaluation - needed to identify generalization-promoting elements; quick check: review ablation study design and results
- **Isotropic convolution design**: Equal spatial and channel dimensions - needed for better feature representation; quick check: verify spatial resolution settings

## Architecture Onboarding

Component Map:
Input -> Data Augmentation -> Mobile Vision Model (2.5M params) -> Classification Head -> Loss Function -> xScore Evaluation

Critical Path:
Data preprocessing → Model inference → Cross-dataset evaluation → xScore computation

Design Tradeoffs:
The study prioritizes parameter efficiency over model capacity, which limits the applicability of transformer-based architectures that typically require more parameters to show benefits. This constraint necessitates careful architectural choices that maximize cross-dataset generalization within tight computational budgets.

Failure Signatures:
Models that overfit to ImageNet-specific features show poor performance on fine-grained and medical datasets, indicated by high accuracy variance across datasets. Transformer-based models under parameter constraints fail to provide additional benefits, suggesting that their advantages require larger capacity to manifest.

First Experiments:
1. Reproduce the baseline ImageNet-only training results to establish performance baseline
2. Evaluate a simple mobile model on all 7 datasets to understand cross-dataset performance gaps
3. Implement and test isotropic convolutions with higher spatial resolution on a baseline model

## Open Questions the Paper Calls Out
None identified in the provided information.

## Limitations
- The mobile model size constraint (2.5M parameters) may limit applicability to larger architectures
- The specific datasets chosen may not capture all real-world deployment scenarios
- The xScore metric, while novel, requires further validation across broader domains

## Confidence

High Confidence Claims:
- ImageNet performance poorly predicts robustness on fine-grained and medical datasets
- Isotropic convolutions and channel-wise attention promote cross-dataset generalization
- Transformer-based blocks show limited benefit under parameter constraints

Medium Confidence Claims:
- Architectural insights about specific components may have dataset-specific effects
- Identified proxy datasets may not generalize to all possible dataset combinations

## Next Checks
1. Test the xScore metric and identified proxy datasets on larger model families (e.g., 10-50M parameters) to verify scalability
2. Evaluate the proposed architectures on additional medical and fine-grained datasets not included in the original study to assess generalizability
3. Conduct ablation studies on the specific configurations of channel-wise attention and isotropic convolutions to isolate their individual contributions to cross-dataset robustness