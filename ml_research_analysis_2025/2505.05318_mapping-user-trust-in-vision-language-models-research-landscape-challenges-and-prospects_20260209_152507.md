---
ver: rpa2
title: 'Mapping User Trust in Vision Language Models: Research Landscape, Challenges,
  and Prospects'
arxiv_id: '2505.05318'
source_url: https://arxiv.org/abs/2505.05318
tags:
- trust
- language
- user
- studies
- vlms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey systematically reviews studies on trust dynamics in
  user-VLM interactions, introducing a multi-disciplinary taxonomy that bridges cognitive
  science capabilities, collaboration modes, and agent behaviors. It identifies a
  significant gap between trust research in LLMs and VLMs, noting that current VLM
  studies focus heavily on hallucinations and adversarial attacks while neglecting
  broader aspects like fairness, explainability, and user engagement.
---

# Mapping User Trust in Vision Language Models: Research Landscape, Challenges, and Prospects

## Quick Facts
- arXiv ID: 2505.05318
- Source URL: https://arxiv.org/abs/2505.05318
- Reference count: 40
- This survey systematically reviews studies on trust dynamics in user-VLM interactions, introducing a multi-disciplinary taxonomy that bridges cognitive science capabilities, collaboration modes, and agent behaviors.

## Executive Summary
This survey systematically reviews studies on trust dynamics in user-VLM interactions, introducing a multi-disciplinary taxonomy that bridges cognitive science capabilities, collaboration modes, and agent behaviors. It identifies a significant gap between trust research in LLMs and VLMs, noting that current VLM studies focus heavily on hallucinations and adversarial attacks while neglecting broader aspects like fairness, explainability, and user engagement. A workshop with prospective VLM users reveals key requirements for future studies, including prioritizing user agency, contextual trust metrics, and exploring hybrid modalities like scene graphs. The findings emphasize the need for multidisciplinary collaboration and user-centered approaches to advance VLM trustworthiness research.

## Method Summary
The authors conducted a systematic literature review using Dimensions AI with keywords including "trustworthy," "user study," "human in the loop," and various VLM/LLM variants, screening 157 candidates down to 43 papers. They also conducted an exploratory workshop with 8 participants using STAR benchmark videos to compare blindfolded human, LLM, and VLM responses. The study involved open-ended questions across perception, temporal reasoning, and prediction tasks, with trust metrics captured through TrustMeter and free-text annotations.

## Key Results
- A multi-disciplinary taxonomy bridges cognitive science capabilities, collaboration modes, and agent behaviors to analyze VLM trust research gaps
- Current VLM trust studies focus disproportionately on hallucinations and adversarial attacks, neglecting broader trust dimensions like fairness and explainability
- Workshop participants emphasized the need for user agency, contextual trust metrics, and hybrid modalities like scene graphs for improved interaction

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A taxonomy bridging cognitive science and agent behavior provides structured coverage analysis for VLM trust research.
- Mechanism: The paper extends Mayer's ABI framework (Ability, Benevolence, Integrity) through Situated Cognition and Theory of Mind concepts, mapping VLM capabilities (intuitive physics, compositionality, causality) and collaboration modes (planning, sensemaking, deliberation) to trust dimensions. This enables systematic gap identification in literature coverage.
- Core assumption: Trust in VLMs can be decomposed into measurable cognitive and behavioral factors that generalize across application contexts.
- Evidence anchors:
  - [section] "We extend the ABI framework through key concepts in Situated Cognition [10, 29] and Theory of Mind [56], proposing the taxonomy in Figure 1."
  - [section] "The proposed taxonomy allows us to evaluate the current coverage of trust-building factors from a nuanced viewpoint that spans visual cognition capabilities, collaborative thought modes, and AI behavioural motifs."
  - [corpus] Related work on trust in VLMs from participatory workshops exists but remains nascent; limited corpus validation for this specific taxonomy structure.
- Break condition: If trust factors are highly context-dependent and cannot generalize across application domains, the taxonomy's diagnostic utility degrades.

### Mechanism 2
- Claim: Scene graphs serve as a hybrid modality bridging visual grounding and textual interpretability for user-VLM interaction.
- Mechanism: Scene graphs represent video/image content as structured text triples (subject-predicate-object), enabling decomposable verification of model outputs while maintaining visual grounding. Users can annotate individual nodes/edges rather than rating holistic responses.
- Core assumption: Graph-structured outputs improve user ability to verify model reasoning compared to unstructured text or raw visuals alone.
- Evidence anchors:
  - [section] "Scene graphs can thus also be seen as collections of text triples composed by a subject, a predicate, and an object... this representation format can support semantic concept abstraction for meta-learning."
  - [section] Workshop participants "rated textual and graph-based responses from the VLM as equally important, with 71% preferring to display both on a single page" and "preferred annotating individual graph nodes/edges over rating the entire graph."
  - [corpus] Weak direct corpus evidence; no comparative studies in neighbors validating scene graphs against other modalities for trust outcomes.
- Break condition: If graph generation introduces additional hallucination or abstraction errors, the verification benefit may be offset by new failure modes.

### Mechanism 3
- Claim: Prioritizing user agency and multi-turn interaction over task completion yields more valid trust measurements.
- Mechanism: Workshop participants expressed frustration when constrained to single-query evaluation and recommended prompt refinement opportunities. Trust drops sharply after early failures (e.g., Video-LLaMa's 29.63% accuracy), making ice-breaking rounds and continuous trust tracking essential for calibration.
- Core assumption: Trust formation is dynamic and requires longitudinal interaction patterns to capture accurately.
- Evidence anchors:
  - [section] "One core pre-requisite emerging from the workshop is prioritising user agency over task completion, allowing multi-turn interactions and prompt refinement."
  - [section] "As trust can drop sharply after initial failures (see the case of Video-LLaMa), the continuous tracking of trust metrics throughout the evaluation tasks is essential."
  - [corpus] Neighbor paper on participatory user workshops aligns with this finding; limited corpus validation for multi-turn vs. single-turn trust measurement.
- Break condition: If extended interaction introduces confounds (learning effects, fatigue), trust measurement validity may decrease.

## Foundational Learning

- Concept: **ABI Trust Framework (Ability, Benevolence, Integrity)**
  - Why needed here: The taxonomy builds directly on this organizational trust model; understanding its interpersonal origins helps recognize where human-AI analogies hold or break down.
  - Quick check question: Can you distinguish between "trustworthiness" as a system property vs. "trust" as user perception?

- Concept: **Zero-Shot Inference in VLMs**
  - Why needed here: VLMs' defining capability is performing unseen tasks without fine-tuning; this creates unique trust challenges since users cannot rely on task-specific validation.
  - Quick check question: What makes zero-shot inference different from traditional supervised learning in terms of failure mode predictability?

- Concept: **Hallucination in Vision-Language Models**
  - Why needed here: The survey identifies hallucination mitigation as the dominant research focus; understanding what hallucinations are (objects/concepts absent from visual input) is prerequisite to evaluating Integrity dimensions.
  - Quick check question: How does a visual hallucination differ from a language-only model's factual error?

## Architecture Onboarding

- Component map:
  - Taxonomy Layer: Cognitive capabilities (intuitive physics, compositionality, causality, meta-learning) → Collaboration modes (planning, sensemaking, deliberation, creation) → Agent behaviors (explicability, predictability, legibility/security, fairness)
  - Evaluation Layer: Datasets/benchmarks (ability-focused: STAR, RoboVQA, SpatialEval; integrity-focused: VHELM, SPA-VL) → Methods (hallucination reduction, robustness testing, preference alignment)
  - User Study Layer: Task design → Trust metric capture (TrustMeter, free-text annotations) → Interaction modality (text, visual, graph)

- Critical path:
  1. Map research question to taxonomy dimensions (which cognitive/collaboration/behavioral factors are relevant?)
  2. Select or construct benchmark covering those dimensions
  3. Design user study with agency-preserving interaction (multi-turn, prompt refinement allowed)
  4. Include ice-breaking rounds before trust measurement
  5. Capture continuous trust metrics, not just endpoint scores

- Design tradeoffs:
  - **Human-in-the-Loop vs. Model-in-the-Loop**: Human feedback provides authentic trust signals but is costly and logistically complex; LLM-generated feedback scales but may not capture genuine user trust dynamics.
  - **Graph vs. Text modality**: Graphs enable targeted verification but add generation complexity; text alone is simpler but harder to decompose.
  - **Breadth vs. depth in capability coverage**: Comprehensive evaluation across all taxonomy dimensions is resource-intensive; focused evaluation risks missing emergent trust issues.

- Failure signatures:
  - **Trust collapse after early failure**: Video-LLaMa's low accuracy (29.63%) caused participants to recommend removing the model entirely.
  - **Language bias dominance**: VLMs may rely on textual priors rather than visual grounding, producing plausible but ungrounded answers.
  - **Metric fatigue**: Users skip TrustMeter or free-text fields if placed too early or too frequently.

- First 3 experiments:
  1. **Calibration study**: Run ice-breaking rounds with known-accuracy VLM outputs, measure trust score stabilization point before main evaluation.
  2. **Modality comparison**: Within-subjects design comparing text-only, visual-only, and graph-enhanced outputs on same tasks; measure annotation precision and trust calibration.
  3. **Agency manipulation**: Randomize participants to single-turn vs. multi-turn interaction conditions; compare trust metric validity against ground-truth model accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does incorporating hybrid modalities like scene graphs into VLM outputs improve user trust and verification capabilities compared to standard textual responses?
- Basis in paper: [explicit] The authors state that exploring scene graphs as a "hybrid modality... opens up interesting opportunities of investigation" to address over-reliance on language.
- Why unresolved: While scene graphs structure reasoning chains, their efficacy in enhancing user engagement and trust relative to text-only outputs remains unverified.
- What evidence would resolve it: Comparative user studies measuring trust calibration and verification accuracy when users interact with scene graph outputs versus text-only explanations.

### Open Question 2
- Question: How does prioritizing user agency and prompt refinement over task completion affect the development of appropriate trust in VLMs?
- Basis in paper: [explicit] Workshop participants expressed distrust when the study structure prioritized task completion, leading to the requirement of "prioritising user agency over task completion."
- Why unresolved: Current evaluations often optimize for benchmark accuracy (task completion) rather than the interactive dynamics required for trust formation.
- What evidence would resolve it: Experimental results comparing trust dynamics in rigid single-turn tasks versus open-ended, multi-turn interaction scenarios.

### Open Question 3
- Question: Can VLMs be engineered to robustly handle advanced reasoning tasks, specifically causal inference and meta-learning, to bridge the gap with human visual intelligence?
- Basis in paper: [explicit] The survey highlights that "advanced reasoning challenges like causality and meta-learning... remain under-explored" in current VLM research.
- Why unresolved: Existing benchmarks predominantly focus on perceptual capabilities and hallucination reduction, neglecting higher-order cognitive functions.
- What evidence would resolve it: Performance evaluations on novel benchmarks designed to test causal prediction and rapid adaptation to new visual concepts.

## Limitations
- The proposed taxonomy lacks empirical validation across diverse application domains.
- Workshop findings are based on a small sample (n=8) and specific task contexts, limiting generalizability.
- Trust measurement validity relies heavily on qualitative feedback rather than quantitative validation.

## Confidence
- High confidence: The identification of a significant gap between LLM and VLM trust research, and the observation that hallucination mitigation dominates current VLM trust literature.
- Medium confidence: The proposed taxonomy structure and its utility for gap analysis, as it is theoretically grounded but lacks extensive empirical validation.
- Medium confidence: Workshop findings on user requirements (agency, multi-turn interaction, contextual metrics), though these are insightful, the small sample size limits generalizability.

## Next Checks
1. Conduct cross-domain validation study applying the taxonomy to VLM trust research across different application areas (medical, autonomous vehicles, education) to test generalizability.
2. Replicate the user workshop with a larger, more diverse participant pool and varied task contexts to strengthen the validity of user requirement findings.
3. Design controlled experiment comparing trust measurement validity between single-turn and multi-turn interaction conditions with ground-truth model accuracy as reference.