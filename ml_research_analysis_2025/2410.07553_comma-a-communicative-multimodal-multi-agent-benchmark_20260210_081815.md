---
ver: rpa2
title: 'COMMA: A Communicative Multimodal Multi-Agent Benchmark'
arxiv_id: '2410.07553'
source_url: https://arxiv.org/abs/2410.07553
tags:
- solver
- agents
- press
- expert
- button
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces COMMA, a benchmark designed to evaluate\
  \ collaborative multimodal agent performance through language communication. The\
  \ benchmark simulates a scenario where two agents\u2014Solver and Expert\u2014must\
  \ work together to solve vision-language puzzles, with the Solver having access\
  \ to visual information and the Expert possessing manuals."
---

# COMMA: A Communicative Multimodal Multi-Agent Benchmark

## Quick Facts
- arXiv ID: 2410.07553
- Source URL: https://arxiv.org/abs/2410.07553
- Authors: Timothy Ossowski; Danyal Maqbool; Jixuan Chen; Zefan Cai; Tyler Bradshaw; Junjie Hu
- Reference count: 36
- Primary result: AI models significantly underperform humans on collaborative multimodal tasks, with o4-mini achieving 53.98% success rate vs human baseline of 69.01%

## Executive Summary
COMMA introduces a benchmark to evaluate collaborative multimodal agent performance through language communication. The benchmark simulates a scenario where two agents—Solver (with visual access) and Expert (with manuals)—must work together to solve vision-language puzzles. Even state-of-the-art models like GPT-4o and Gemini 2.0 significantly underperform human baselines, revealing critical weaknesses in inter-agent communication, information asymmetry handling, and privacy adherence. The study identifies specific failure modes including roleplay errors, miscommunication, repetition loops, and privacy leaks.

## Method Summary
COMMA evaluates agent collaboration through 10 puzzle types categorized by cognitive capabilities (memory, grounding, reasoning). Two agents interact via language: Solver has visual information but no instructions, while Expert has manuals but no visuals. Success requires precise description, rule retrieval, and visual grounding across multiple turns. The benchmark uses conversation history as episodic memory and constrains interactions to 10 turns with 3 mistake limits. Models are evaluated on success rate, efficiency, and privacy adherence.

## Key Results
- AI models achieve 53.98% average success rate vs human baseline of 69.01%
- Chain-of-thought models (LLaVA-CoT, R1-OneVision) underperform random baseline (14.97% and 16.81% vs 18.70%)
- GPT-4o and Gemini 2.0 frequently reveal private information despite explicit instructions
- Common failure modes include roleplay errors, miscommunication, repetition loops, and privacy leaks
- Even top-performing models show plateauing performance after 4-5 conversation turns

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Information asymmetry forces explicit language-based communication, exposing collaboration weaknesses
- Mechanism: Solver has visuals but no instructions; Expert has manuals but no visuals. Success requires describing observations precisely, retrieving relevant rules, and grounding language to visual context
- Core assumption: Models trained on single-agent, full-information tasks struggle when information is partitioned
- Evidence: "Existing agentic benchmarks fail to address key aspects of inter-agent communication" and "domain shift in pretraining data"

### Mechanism 2
- Claim: Episodic memory enables error correction but current models underutilize it
- Mechanism: Conversation history allows learning from mistakes, but models show plateauing performance after 4-5 turns
- Core assumption: Models capable of in-context learning can leverage conversation history
- Evidence: "Overall the results show a plateau after about 4-5 conversation turns" and repetition loops as major error type

### Mechanism 3
- Claim: Chain-of-thought models underperform due to self-reasoning optimization conflicting with communication
- Mechanism: CoT models attempt independent problem-solving rather than querying experts
- Core assumption: Reasoning training emphasizes self-contained solutions
- Evidence: "Many chain of thought reasoning models... struggle to outperform even a random baseline"

## Foundational Learning

- Concept: Asymmetric information games (e.g., "Keep Talking and Nobody Explodes")
  - Why needed: COMMA directly adapts this game structure; understanding human success patterns provides mental model for AI failures
  - Quick check: Can you explain why giving Expert full visual access would invalidate the benchmark?

- Concept: Episodic vs. working memory in cognitive architectures
  - Why needed: Benchmark structures agents with both memory types; understanding their interaction explains success/failure patterns
  - Quick check: If you removed episodic memory, which puzzle categories would be most affected and why?

- Concept: Grounding language in visual context
  - Why needed: Solver must map Expert's textual instructions to specific visual elements
  - Quick check: How would you distinguish a grounding error from a miscommunication error in a failure log?

## Architecture Onboarding

- Component map: Solver Agent [task_prompt, puzzle_image, conversation_history] -> text_description OR action_command -> Environment -> feedback -> Expert Agent [task_prompt, instruction_manual, conversation_history] -> guidance_text OR clarification_request

- Critical path: Solver describes visual state → Expert retrieves relevant manual rules → Expert provides guidance → Solver grounds guidance to image → Solver executes action → Environment validates

- Design tradeoffs: 10-turn limit standardizes evaluation but may underestimate human performance; greedy decoding vs temperature 0.6 trade-off between reproducibility and avoiding repetition loops

- Failure signatures: Roleplay error (Expert describes visuals), Miscommunication (Solver ignores Expert), Repetition loop (identical (state, action) pairs), Privacy leak (Solver reveals prohibited information)

- First 3 experiments:
  1. Establish baseline with GPT-4o as both roles on 10 puzzles, log success rate and manually annotate first 20 failures
  2. Ablate episodic memory by restricting to most recent message, compare success rate drop across puzzle categories
  3. Test privacy adherence on ATM/Health puzzles, scan Solver outputs for private information using regex and LLM judge

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can pretraining on communication-focused tasks improve reasoning models' collaborative performance?
- Basis: Authors state adding communication tasks to pretraining will likely address reasoning models' underperformance
- Evidence needed: Fine-tune reasoning models on communication datasets and evaluate on COMMA

### Open Question 2
- Question: What mechanisms can enforce privacy constraints without degrading task performance?
- Basis: GPT-4o/Gemini 2.0 frequently reveal private information despite instructions
- Evidence needed: Privacy-preserving mechanisms achieving both high success rates and zero information leakage

### Open Question 3
- Question: Can hybrid thinking approaches improve efficiency while maintaining accuracy?
- Basis: o4-mini achieves highest performance but lowest efficiency (0.15) vs human efficiency of 0.78
- Evidence needed: Models with hybrid thinking showing high partial success rates (>50%) and efficiency (>0.5)

### Open Question 4
- Question: How does performance scale with number of agents and hierarchical roles?
- Basis: Real-world scenarios involve more than 2 agents and hierarchical structures
- Evidence needed: 3+ agent configurations showing whether failure modes compound

### Open Question 5
- Question: What are root causes of communication gap between reasoning and general-purpose models?
- Basis: Reasoning models underperform random baseline despite strong reasoning capabilities
- Evidence needed: Ablation studies isolating reasoning capacity vs instruction-following in collaborative settings

## Limitations
- Synthetic puzzle environments may not fully capture real-world collaborative complexity
- Using same model for both roles creates artificial constraint not present in real deployments
- 10-turn limit and 3-mistake threshold may artificially constrain human-level performance
- Privacy leakage findings limited to specific puzzle types and may not generalize

## Confidence

**High Confidence**: AI models significantly underperform humans (53.98% vs 69.01% success rate); specific failure modes are directly observable

**Medium Confidence**: Chain-of-thought models systematically underperform due to domain shift; privacy leakage findings are concerning but context-limited

**Low Confidence**: Episodic memory utilization plateau hypothesis; mechanism explaining reasoning models' communication struggles

## Next Checks

1. **Generalization Test**: Evaluate COMMA with specialist models (separate models for Solver vs Expert) to determine if gaps are due to model capability or single-model constraint

2. **Real-World Translation**: Design pilot study applying COMMA-style protocols to technical support scenario to validate benchmark insights translate to real applications

3. **Privacy Robustness Analysis**: Systematically vary explicitness of privacy instructions and measure relationship between instruction clarity and privacy adherence across model families