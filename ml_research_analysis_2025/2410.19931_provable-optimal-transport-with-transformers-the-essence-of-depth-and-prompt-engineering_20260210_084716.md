---
ver: rpa2
title: 'Provable optimal transport with transformers: The essence of depth and prompt
  engineering'
arxiv_id: '2410.19931'
source_url: https://arxiv.org/abs/2410.19931
tags:
- attention
- transformer
- layers
- translation
- solution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a mechanistic and theoretical explanation for
  how transformers align tokens in language tasks, particularly machine translation.
  The authors show empirically that attention weights in transformers progressively
  align translated word pairs across layers, closely approximating optimal transport
  (OT) between word embeddings.
---

# Provable optimal transport with transformers: The essence of depth and prompt engineering

## Quick Facts
- **arXiv ID**: 2410.19931
- **Source URL**: https://arxiv.org/abs/2410.19931
- **Reference count**: 40
- **Key outcome**: Transformers align translated word pairs via progressive OT approximation across layers, with depth and prompt engineering controlling accuracy

## Executive Summary
This paper provides a mechanistic and theoretical explanation for how transformers align tokens in language tasks, particularly machine translation. The authors show empirically that attention weights in transformers progressively align translated word pairs across layers, closely approximating optimal transport (OT) between word embeddings. Theoretically, they prove that softmax self-attention layers can simulate gradient descent on the dual of the entropy-regularized OT problem, establishing that transformer depth controls OT approximation accuracy. This explains how standard transformers can sort lists of varying lengths without parameter adjustment, up to an error term vanishing with depth. The work also reveals that careful prompt engineering is essential for this capability, effectively extending the transformer's memory to implement OT.

## Method Summary
The paper trains transformers to solve discrete optimal transport problems by minimizing MSE between output attention weights and entropy-regularized OT solutions. The method uses engineered prompts with specific memory columns to store dual variables, two-head self-attention with residual connections, and reparameterizes attention weights as products of key/query matrices. For translation experiments, the approach extracts attention weights from pretrained Marian models on WMT14 English-French sentence pairs and compares alignment to MUSE dictionary entries using MRR and Hits@k metrics.

## Key Results
- OT on word embeddings aligns translated words with approximately 90% accuracy
- Attention dynamics in pretrained translation models show progressive alignment improvement across layers
- Theoretical proof establishes softmax self-attention can simulate gradient descent on dual entropy-regularized OT
- Transformer depth controls OT approximation accuracy with error vanishing as O(1/âˆšdepth)

## Why This Works (Mechanism)

### Mechanism 1: Attention as Gradient Descent on Dual OT
The paper constructs a specific parameterization where the attention operation updates the dual variables ($u, v$) of the OT problem. The residual connection implements the gradient update, with attention weights converging to the transport plan $P^*_\lambda$. This requires fixed parameters independent of input size and engineered prompt columns for memory storage.

### Mechanism 2: Depth Controls Approximation Accuracy
Each layer corresponds to an iteration of the optimization algorithm, with error scaling as $O(n^{3/2}/\sqrt{\text{depth}})$. Deeper networks perform more iterations of gradient descent, refining the alignment matrix towards the optimal transport plan.

### Mechanism 3: Prompt Engineering as Memory Extension
Engineered prompt construction provides "memory registers" needed to execute the OT algorithm. Input matrices are padded with constant columns and specific row structures that serve as storage locations for dual variables $u_\ell$ and $v_\ell$ during the forward pass.

## Foundational Learning

- **Entropy-Regularized Optimal Transport**: The core mathematical problem the transformer solves. Understanding primal vs dual formulations is crucial. *Quick check*: Can you explain why adding entropy makes OT differentiable and solvable via gradient descent?

- **Residual Connections as Iterative Updates**: Residual connections implement gradient descent steps $x_{t+1} = x_t - \eta \nabla f$. Viewing depth as "time" in an optimization loop is central. *Quick check*: Would transformers still implement iterative refinement without residual connections?

- **Softmax as Probability Generator**: Softmax produces doubly stochastic matrices required by OT, mathematically linked to the optimal solution form $P_{ij} \propto e^{-C_{ij} + u_i + v_j}$. *Quick check*: How does softmax ensure rows/columns sum to 1 for transport plans?

## Architecture Onboarding

- **Component map**: Input $Z_0$ (structured matrix with engineered columns) -> Two attention heads (compute $u$- and $v$-potentials) -> Residual stream (state vector for optimization) -> Attention weights $A^{(\ell)}$ (transport plan)

- **Critical path**: 
  1. Initialize input $Z_0$ with engineered columns
  2. Pass through $L$ layers without causal masking (for full bi-directional transport)
  3. Extract attention weights $A^{(\ell)}$ at final layer as transport plan

- **Design tradeoffs**: 
  - Depth vs Efficiency: $O(1/\sqrt{\text{depth}})$ convergence is slow compared to Sinkhorn's $O(\log(1/\epsilon))$
  - Prompt Overhead: Requires increasing embedding dimension and sequence length, consuming context window

- **Failure signatures**: 
  - Collapse to diagonal (identity matrix) if parameters incorrect or depth too low
  - Memory overflow preventing dual variable storage without engineered prompt columns

- **First 3 experiments**: 
  1. Implement specific parameter configuration on synthetic sorting task to reproduce OT solution
  2. Ablate engineered prompt vs natural encoding to quantify performance gap
  3. Train on $n=7$ points, test on $n=15$ to verify out-of-distribution generalization

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can standard softmax attention solve OT with general cost functions beyond Euclidean distance?
- **Basis**: Discussion asks about general cost functions $C(x_i, y_j)$
- **Why unresolved**: Current proof relies on Euclidean cost properties
- **Resolution evidence**: Theoretical extension for general costs or empirical demonstrations with non-Euclidean metrics

### Open Question 2
- **Question**: Can theoretical convergence bounds be tightened to match Sinkhorn efficiency?
- **Basis**: Current bound yields $O(\epsilon^{-2})$ depth vs Sinkhorn's $O(\log(1/\epsilon))$
- **Why unresolved**: Uses general convex optimization rates that may be too broad
- **Resolution evidence**: Refined proof showing $O(\log(\text{depth}))$ convergence or theoretical barriers

### Open Question 3
- **Question**: What is the minimal prompt size $n$ sufficient for learning assignment tasks?
- **Basis**: "Learning with Small Prompts" discussion asks about minimal $n$
- **Why unresolved**: Lower bounds on training data complexity remain uncharacterized
- **Resolution evidence**: Empirical studies determining smallest training sequence length enabling generalization

## Limitations
- Theoretical claims rely on highly specific parameter configurations unlikely to emerge from standard training
- Error bound scales as $O(n^{3/2}/\sqrt{\text{depth}})$, suggesting slow practical convergence
- Prompt engineering requirement demands specific input formatting not available in pretrained models without retraining

## Confidence
- **High confidence**: Empirical observation of progressive alignment in translation models (Fig. 3, MRR/Hits@k metrics)
- **Medium confidence**: Theoretical proof of softmax attention simulating dual OT gradient descent (Theorem 3.1)
- **Medium confidence**: Depth controlling accuracy with error vanishing as $O(1/\sqrt{\text{depth}})$ (Theorem 3.2)

## Next Checks
1. **Parameter discovery experiment**: Train transformers on OT tasks using standard initialization to see if attention naturally converges to transport plans
2. **Scaling study**: Evaluate error bound empirically across varying sequence lengths and depths to verify convergence rate
3. **Prompt ablation robustness**: Test pretrained models' alignment when engineered prompt components are perturbed or partially removed