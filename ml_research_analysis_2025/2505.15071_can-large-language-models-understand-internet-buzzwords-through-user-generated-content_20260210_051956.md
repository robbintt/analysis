---
ver: rpa2
title: Can Large Language Models Understand Internet Buzzwords Through User-Generated
  Content
arxiv_id: '2505.15071'
source_url: https://arxiv.org/abs/2505.15071
tags:
- definition
- buzzword
- definitions
- buzzwords
- ress
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether large language models (LLMs) can
  generate accurate definitions for Chinese internet buzzwords using user-generated
  content (UGC) as examples. The authors introduce CHEER, the first dataset of Chinese
  internet buzzwords with definitions and corresponding UGC, and propose RESS, a novel
  method that guides LLMs through aspect-specific definition generation inspired by
  child language acquisition skills.
---

# Can Large Language Models Understand Internet Buzzwords Through User-Generated Content

## Quick Facts
- arXiv ID: 2505.15071
- Source URL: https://arxiv.org/abs/2505.15071
- Authors: Chen Huang; Junkai Luo; Xinzuo Wang; Wenqiang Lei; Jiancheng Lv
- Reference count: 40
- Key outcome: RESS method achieves +2.51% semantic accuracy and +3.31% semantic completeness improvements over baselines

## Executive Summary
This paper investigates whether large language models (LLMs) can generate accurate definitions for Chinese internet buzzwords using user-generated content (UGC) as examples. The authors introduce CHEER, the first dataset of Chinese internet buzzwords with definitions and corresponding UGC, and propose RESS, a novel method that guides LLMs through aspect-specific definition generation inspired by child language acquisition skills. RESS outperforms existing methods, achieving an average improvement of +2.51% in semantic accuracy and +3.31% in semantic completeness over the best baseline. However, results reveal key challenges: LLMs over-rely on prior exposure to buzzwords, have underdeveloped inferential abilities for unseen terms, and struggle to identify high-quality UGC to facilitate comprehension.

## Method Summary
The study introduces CHEER, a dataset of Chinese internet buzzwords paired with definitions and user-generated content examples. The proposed RESS method guides LLMs through aspect-specific definition generation, inspired by child language acquisition patterns. This approach breaks down definition generation into distinct semantic aspects, allowing the model to focus on different dimensions of meaning systematically. The method is evaluated against multiple baseline approaches using automatic metrics (BLEU, ROUGE) on the Chinese language models CPM-2 and GLM-4.

## Key Results
- RESS achieves +2.51% improvement in semantic accuracy over best baseline
- RESS achieves +3.31% improvement in semantic completeness over best baseline
- LLMs show significant performance degradation when buzzwords are excluded from training contexts

## Why This Works (Mechanism)
The RESS method works by decomposing the complex task of buzzword definition generation into manageable semantic aspects, mirroring how children acquire language through progressive understanding of different meaning dimensions. By guiding the LLM to focus on specific aspects sequentially, the model can build a more comprehensive understanding of the buzzword's semantic space rather than attempting to capture all meanings simultaneously.

## Foundational Learning
- Chinese internet culture and linguistic patterns - needed to understand the domain specificity of buzzwords; quick check: familiarity with common Chinese internet slang
- Child language acquisition theories - needed to inform the aspect-based approach; quick check: understanding of developmental linguistics principles
- Semantic aspect decomposition - needed to structure the definition generation process; quick check: ability to identify distinct semantic dimensions of word meanings

## Architecture Onboarding
- Component map: UGC examples -> Aspect identification -> Sequential definition generation -> Semantic validation
- Critical path: Quality UGC selection → Aspect decomposition → Guided generation → Evaluation
- Design tradeoffs: The aspect-based approach sacrifices some efficiency for improved semantic coverage and accuracy
- Failure signatures: Over-reliance on prior exposure, inability to infer meanings for unseen terms, confusion when UGC quality is low
- First experiments: 1) Test aspect decomposition on diverse buzzword categories, 2) Compare aspect-guided vs holistic generation approaches, 3) Evaluate performance across different UGC quality levels

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset focuses exclusively on Chinese internet buzzwords, limiting generalizability
- Automatic metrics may not fully capture semantic quality of culturally nuanced definitions
- Only Chinese language models tested, preventing cross-architecture comparisons
- Modest performance improvements suggest substantial room for improvement

## Confidence
**High confidence**: LLMs over-rely on prior exposure to buzzwords and struggle with unseen terms is well-supported by experimental results showing significant performance drops when buzzwords are excluded from training contexts.

**Medium confidence**: LLMs have "underdeveloped inferential abilities" for generating definitions is supported but requires more rigorous comparison against human baseline performance to establish whether the gap is truly developmental rather than inherent to the task.

**Low confidence**: LLMs "struggle to identify high-quality UGC" is based on indirect evidence from definition quality rather than direct assessment of UGC selection mechanisms, requiring further validation.

## Next Checks
1. Conduct human evaluation studies comparing LLM-generated definitions against expert-created definitions across diverse buzzword categories to validate automatic metric findings.

2. Test the RESS methodology on multilingual datasets to assess cross-linguistic generalizability and identify whether aspect-based generation principles transfer across languages.

3. Implement controlled experiments varying UGC quality and quantity to determine optimal conditions for LLM comprehension and identify failure modes in the generation process.