---
ver: rpa2
title: Relative Trajectory Balance is equivalent to Trust-PCL
arxiv_id: '2509.01632'
source_url: https://arxiv.org/abs/2509.01632
tags:
- prior
- distribution
- soft
- reward
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper establishes that the Relative Trajectory Balance (RTB)
  objective, introduced for fine-tuning sequential generative models in GFlowNets,
  is mathematically equivalent to Trust-PCL, an off-policy reinforcement learning
  algorithm with KL regularization. This equivalence is shown through direct algebraic
  manipulation, demonstrating that both methods optimize the same loss function up
  to a constant factor.
---

# Relative Trajectory Balance is equivalent to Trust-PCL

## Quick Facts
- arXiv ID: 2509.01632
- Source URL: https://arxiv.org/abs/2509.01632
- Reference count: 21
- Primary result: Establishes mathematical equivalence between Relative Trajectory Balance (RTB) and Trust-PCL through direct algebraic manipulation.

## Executive Summary
This paper establishes that Relative Trajectory Balance (RTB), a fine-tuning objective for sequential generative models introduced in GFlowNets, is mathematically equivalent to Trust-PCL, an off-policy reinforcement learning algorithm with KL regularization. The equivalence is demonstrated through direct algebraic manipulation, showing both methods optimize the same loss function up to a constant factor. The authors use this insight to revisit an illustrative example where KL-regularized RL methods were claimed to perform poorly, demonstrating that the failure stemmed from algorithmic and reward function choices rather than fundamental limitations of KL-regularized RL. When corrected, standard REINFORCE with KL regularization achieved comparable performance to RTB on this task.

## Method Summary
The paper establishes the mathematical equivalence between RTB and Trust-PCL by directly manipulating the algebraic forms of their respective residuals. Both methods optimize the same objective: minimizing the KL divergence between the marginal distribution of trajectories under the policy and a target "tilted distribution" defined by the prior distribution times the exponential of negative energy. The paper then revisits a 2D generation task from previous work where KL-regularized RL was claimed to fail, showing that the failure was due to an incorrect reward definition (double exponentiation instead of linear energy) rather than algorithmic limitations. The corrected approach using proper reward definition achieves the same performance as RTB.

## Key Results
- RTB and Trust-PCL losses are equal up to a constant factor (α²) through direct algebraic manipulation
- The failure of KL-regularized RL on the illustrative task was caused by incorrect reward definition (double exponentiation), not algorithmic limitations
- Standard REINFORCE with KL regularization and corrected reward achieves comparable performance to RTB on the 2D task
- RTB is not a fundamentally new approach but rather a reformulation of existing KL-regularized RL methods within the GFlowNet framework

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Relative Trajectory Balance (RTB) and Trust-PCL optimize the exact same mathematical objective, differing only by a constant scaling factor.
- **Mechanism:** The RTB residual (log-ratio of prior to current trajectory, scaled by a partition function and energy term) can be algebraically rearranged into the Trust-PCL residual (sum of rewards plus log-policy ratio, centered by a value function). The relationship is defined as $L_{T-PCL}(\phi, \psi) = \alpha^2 L_{RTB}(\phi, \psi)$.
- **Core assumption:** The MDP structure is finite-horizon and deterministic, and the correspondence $V^\psi_{soft}(s_0) = \alpha \log Z_\psi$ holds.
- **Evidence anchors:**
  - [abstract] "establish... equivalence... through direct algebraic manipulation"
  - [section 3] "Proposition 1... losses are equal up to a constant factor"
  - [corpus] The corpus signals confirm RTB is an off-policy RL objective, supporting the classification.

### Mechanism 2
- **Claim:** Optimizing the KL-regularized objective forces the policy's marginal distribution to converge to the "tilted distribution" (Prior × $\exp(-\text{Energy}/\alpha)$).
- **Mechanism:** The optimal KL-regularized policy $\pi^*$ relates to the prior via an exponential term containing the Q-function. Due to the specific reward definition (sum = $-E$), intermediate Q-values cancel via telescoping sums, leaving only the terminal energy and the initial state value constant.
- **Core assumption:** The reward function is strictly defined such that the trajectory sum equals the negative terminal energy (Eq. 3).
- **Evidence anchors:**
  - [section 2.1] "marginal distribution... is the tilted distribution" (Eq. 5)
  - [appendix a] Derivation showing the proportionality $\pi^\top_{RelEnt}(s_T) \propto \pi^\top_{prior}(s_T) \exp(-E(s_T)/\alpha)$ (Eq. 21)
  - [corpus] "A Theoretical Lens for RL-Tuned LLMs" confirms the energy-based model structure of optimal KL-regularized policies.

### Mechanism 3
- **Claim:** The previously reported failure of KL-regularized RL on specific tasks was caused by an incorrect reward definition (double exponentiation), not a fundamental algorithmic deficiency.
- **Mechanism:** The baseline comparison used a sparse reward $\tilde{r} = \exp(-E)$ instead of the correct $r = -E$. This creates an optimal target distribution $\propto \pi_{prior} \exp(\exp(-E)/\alpha)$, which fundamentally differs from the intended target and hampers learning.
- **Core assumption:** The goal is to sample from the standard tilted distribution (Eq. 2).
- **Evidence anchors:**
  - [section 3.1] "this happens to be caused by their reward function not satisfying (3)"
  - [appendix b] "This error likely came from the naming conflict... extra inner 'exp'" (Eq. 23)
  - [corpus] Corpus signals are weak on this specific historical error; reliance is primarily on the paper's retrospective analysis.

## Foundational Learning

- **Concept: KL-Regularized Reinforcement Learning (MaxEnt-RL)**
  - **Why needed here:** This is the theoretical framework into which RTB is mapped. Understanding how the KL penalty trades off reward maximization against deviation from a prior is essential.
  - **Quick check question:** If you increase the temperature $\alpha$, does the optimal policy become more or less concentrated on high-reward states compared to the prior?

- **Concept: Trajectory Balance (TB) in GFlowNets**
  - **Why needed here:** RTB is a modification of TB. TB frames generative modeling as minimizing the difference in "flow" or log-probability ratios between complete trajectories.
  - **Quick check question:** In TB, what does the log-partition function $\log Z$ represent in relation to the trajectory probabilities?

- **Concept: Off-Policy Learning & Importance Sampling**
  - **Why needed here:** Trust-PCL and RTB are off-policy methods. The paper corrects the baseline comparison by using Self-Normalized Importance Sampling (SNIS) to handle the data distribution shift.
  - **Quick check question:** Why is a behavior policy $\pi_b$ distinct from the target policy $\pi_\phi$ necessary for efficient exploration in this context?

## Architecture Onboarding

- **Component map:** Prior ($\pi_{prior}$) → Policy ($\pi_\phi$) → Trajectory → Reward ($r = -E$) → Value Function ($V_{soft}$ / $\log Z_\psi$)
- **Critical path:** The strict definition of the Reward Function (Mechanism 3). Engineering teams must ensure the reward sum over a trajectory equals the negative energy of the final state. Using a raw "GFlowNet reward" $R(x)=\exp(-E(x))$ directly as an RL reward introduces the double-exponential error.
- **Design tradeoffs:**
  - **On-Policy vs. Off-Policy:** On-policy (standard REINFORCE) struggles with exploration in high-dimensional spaces. Off-policy (Trust-PCL/RTB) decouples exploration (behavior policy) from optimization but requires importance sampling correction.
  - **Control Variates:** REINFORCE uses a local baseline (batch average); RTB uses a global baseline ($Z_\psi$). Global baselines may reduce variance but introduce additional approximation error.
- **Failure signatures:**
  - **Mode Collapse (RL failure):** Policy ignores low-reward modes.
  - **Distorted Target Distribution:** Caused by reward function mismatch (e.g., $\exp(\exp(-E))$).
  - **High Variance:** Caused by poor importance sampling weights or missing baselines.
- **First 3 experiments:**
  1.  **Verify Reward Mapping:** Train an agent on the "Illustrative Example" (2D Gaussians) using $r = -E$ vs. $r = \exp(-E)$. Confirm that only $r = -E$ matches the target contour.
  2.  **Loss Equivalence Check:** Implement both RTB and Trust-PCL losses. On a fixed batch of trajectories, verify $L_{T-PCL} \approx \alpha^2 L_{RTB}$ numerically.
  3.  **Baseline Comparison:** Compare on-policy REINFORCE (with correct reward) against off-policy RTB/Trust-PCL on the 2D task to validate the exploration benefits of the off-policy approach.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific performance benefits do advanced methods like RTB or Trust-PCL offer over simpler KL-regularized RL baselines on complex, high-dimensional tasks?
- Basis in paper: [Explicit] The conclusion states that "on more challenging tasks, advanced methods like RTB/Trust-PCL are likely to offer clearer benefits" compared to the simple illustrative example analyzed in the main text.
- Why unresolved: The paper primarily focused on refuting the failure modes of RL in a simple 2D task; it did not empirically validate the superiority of these methods on the complex language or diffusion models mentioned in the introduction.
- What evidence would resolve it: Empirical benchmarks comparing RTB, Trust-PCL, and REINFORCE with KL regularization on large-scale diffusion or autoregressive language models.

### Open Question 2
- Question: How can advanced stability techniques like update clipping (inspired by PPO) be integrated into the RTB/Trust-PCL framework to improve fine-tuning?
- Basis in paper: [Explicit] The conclusion suggests promising directions lie in "exploring new algorithms for generative model fine-tuning" by incorporating techniques such as "update clipping" mentioned in recent works like DDPO and DPOK.
- Why unresolved: The paper establishes the mathematical equivalence of the base objectives but does not explore how higher-level optimization heuristics (like clipping) used in practical RL fine-tuning translate to or improve the RTB formulation.
- What evidence would resolve it: A study analyzing the convergence and sample efficiency of RTB/Trust-PCL when augmented with trust-region or clipping constraints.

### Open Question 3
- Question: Does the mathematical equivalence between RTB and Trust-PCL hold empirically in large-scale continuous domains, or do optimization dynamics cause divergence?
- Basis in paper: [Inferred] The paper validates its theoretical claim on a "2D generation task" and "illustrative example." While the algebraic equivalence is proven, the introduction highlights applications in "vision" and "scientific discovery" involving high-dimensional data where optimization landscapes differ significantly.
- Why unresolved: Theoretical equivalence does not guarantee identical performance under function approximation (neural networks) and stochastic gradient descent in high dimensions.
- What evidence would resolve it: Experiments demonstrating that minimizing the RTB loss and Trust-PCL loss results in identical learning curves and final sample quality in high-dimensional generative tasks.

## Limitations

- The paper relies on retrospective analysis of a historical error rather than independent reproduction of the original failed experiments
- Practical implementation details like policy architectures, exploration strategies, and hyperparameter choices remain underspecified
- Theoretical equivalence does not guarantee identical performance under function approximation and stochastic gradient descent in high-dimensional settings

## Confidence

- **High Confidence:** The algebraic equivalence between RTB and Trust-PCL losses (Mechanism 1). The mathematical derivation is complete and verifiable.
- **Medium Confidence:** The explanation of the historical failure (Mechanism 3). The paper's diagnosis is internally consistent and the fix is demonstrated, but the original experimental conditions are not fully accessible for independent verification.
- **Medium Confidence:** The convergence properties of KL-regularized RL to the tilted distribution (Mechanism 2). The theoretical foundation is sound, but the finite-sample behavior and variance properties in high-dimensional settings require further empirical validation.

## Next Checks

1. **Independent Reproduction:** Implement both the "incorrect" (double-exponential) and "correct" (linear energy) reward functions on the 2D task. Confirm that only the correct reward recovers the target distribution, and quantify the learning curves and sample efficiency.
2. **Equivalence Verification:** For a fixed set of trajectories, compute both the RTB and Trust-PCL losses using their respective formulations. Numerically verify the claimed relationship $L_{T-PCL} = \alpha^2 L_{RTB}$ to within numerical precision.
3. **Architecture Stress Test:** Evaluate the corrected KL-regularized RL (and RTB) on a higher-dimensional extension of the task (e.g., 5D or 10D mixtures). Assess whether the performance gap between on-policy and off-policy methods widens with dimensionality, and whether variance reduction techniques (e.g., baselines, importance sampling tuning) become critical.