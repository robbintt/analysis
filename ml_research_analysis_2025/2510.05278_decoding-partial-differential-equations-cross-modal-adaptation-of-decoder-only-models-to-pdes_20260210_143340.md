---
ver: rpa2
title: 'Decoding Partial Differential Equations: Cross-Modal Adaptation of Decoder-only
  Models to PDEs'
arxiv_id: '2510.05278'
source_url: https://arxiv.org/abs/2510.05278
tags:
- decoder-only
- adaptation
- performance
- cross-modal
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates how model architecture impacts cross-modal
  adaptation of large language models to time-dependent partial differential equation
  (PDE) simulation tasks. We find that directly applying existing cross-modal adaptation
  methods (FPT and ORCA) to decoder-only models results in significantly worse performance
  than encoder-only models, and scaling decoder-only models does not close this gap.
---

# Decoding Partial Differential Equations: Cross-Modal Adaptation of Decoder-only Models to PDEs

## Quick Facts
- **arXiv ID**: 2510.05278
- **Source URL**: https://arxiv.org/abs/2510.05278
- **Reference count**: 16
- **One-line primary result**: Decoder-only LLMs underperform encoder-only models on PDE tasks due to autoregressive attention limitations; Sequence Doubling method closes this performance gap.

## Executive Summary
This work investigates how model architecture impacts cross-modal adaptation of large language models to time-dependent partial differential equation (PDE) simulation tasks. We find that directly applying existing cross-modal adaptation methods (FPT and ORCA) to decoder-only models results in significantly worse performance than encoder-only models, and scaling decoder-only models does not close this gap. We identify that the autoregressive attention mechanism in decoder-only models limits their ability to condition on bidirectional context, which is important for PDE tasks. To address this, we introduce two novel methods: Parallel Flipping, which runs the pipeline with inverted sequences and combines predictions, and Sequence Doubling, which concatenates each sequence with itself to expand context. Both methods improve decoder-only model performance across all tasks and adaptation methods, with Sequence Doubling achieving the largest gains and closing the performance gap to encoder-only models.

## Method Summary
The study adapts pre-trained LLMs (RoBERTa-base, BERT-base for encoder-only; GPT-2 family up to 1.61B, Pythia family 14M–1.4B for decoder-only) to four PDEBench datasets (Advection, Diffusion-Reaction, Diffusion-Sorption, Navier-Stokes) using two cross-modal adaptation methods: FPT (fine-tunes only input/output layers and layer norms) and ORCA (first aligns embeddings using optimal transport distance before full fine-tuning). Task-specific embedders and predictors are used to match model and task dimensions. The proposed solutions are Parallel Flipping (run original and inverted sequences, merge second halves) and Sequence Doubling (concatenate sequence with itself, use second half of final hidden layer for prediction). Training uses task-specific optimizers and NVIDIA A100 GPUs, with longest runs taking 140 GPU hours.

## Key Results
- Decoder-only models with FPT and ORCA perform significantly worse than encoder-only models on all four PDE tasks.
- Scaling decoder-only models (up to 71× parameters) does not close the performance gap to encoder-only models.
- Sequence Doubling achieves the largest performance gains and closes the gap to encoder-only performance across all tasks and adaptation methods.
- Parallel Flipping also improves decoder-only performance but with smaller gains than Sequence Doubling.
- ORCA with decoder-only models can underperform randomly-initialized versions on some tasks, suggesting pre-training provides no benefit.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Decoder-only models underperform encoder-only models on PDE tasks because autoregressive (causal) attention prevents bidirectional context conditioning.
- **Mechanism**: In decoder-only models, position $t$ can only attend to positions $1 \ldots t$. For PDE sequences where spatial/temporal points relate symmetrically, this unidirectional constraint produces "spikier" predictions early in sequences where context is limited, compared to encoder-only models that can attend to the full sequence at every position.
- **Core assumption**: PDE simulation tasks fundamentally benefit from bidirectional context access, similar to how encoder representations work in NLP.
- **Evidence anchors**:
  - [abstract]: "We identify that the autoregressive attention mechanism in decoder-only models limits their ability to condition on bidirectional context, which is important for PDE tasks."
  - [section]: "decoder-only models are penalized for being autoregressive, since each point in the sequence is treated as an individual token, and GPT-2 and PYTHIA cannot condition on the sequence bidirectionally" (Page 6)
  - [corpus]: Weak direct evidence—corpus neighbors focus on PDE solving methods rather than architectural attention patterns.
- **Break condition**: This mechanism would break if PDE data were inherently causal/sequential rather than symmetric, or if outputs were computed via autoregressive generation rather than averaged hidden representations.

### Mechanism 2
- **Claim**: Sequence Doubling enables decoder-only models to access full bidirectional context by giving each position access to the complete sequence during second-half processing.
- **Mechanism**: By concatenating sequence $S$ as $[S, S]$, when the model processes position $t$ in the second copy, it can attend to all of the first copy plus positions up to $t$ in the second copy. The second half of the final hidden layer is therefore conditioned on the complete sequence, simulating bidirectional attention.
- **Core assumption**: The model effectively uses the first sequence copy as context without interference from repetition.
- **Evidence anchors**:
  - [abstract]: "Sequence Doubling... concatenates each sequence with itself to expand context... achieving the largest gains and closing the performance gap to encoder-only models."
  - [section]: "This half of the hidden layer is conditioned on the first instance of the entire sequence and should therefore be a much richer representation of the data" (Page 7)
  - [corpus]: Springer et al. (2025, cited in paper) shows "Repetition improves language model embeddings"—principle of beneficial repetition is supported.
- **Break condition**: Would break if position encodings conflict with repetition, or if the model attends preferentially to the second (closer) copy rather than the first.

### Mechanism 3
- **Claim**: Parallel Flipping improves decoder-only predictions by ensuring both halves of the output sequence have access to their "smooth" region with maximum context.
- **Mechanism**: Error analysis showed decoder predictions are irregular at sequence beginnings (low context) but smooth toward the end (high context). Running the pipeline on both original and inverted sequences, then taking the second half from each run, gives both output halves the benefit of full-past-context conditioning.
- **Core assumption**: The "spiky beginning / smooth end" pattern is consistent and predictable across runs.
- **Evidence anchors**:
  - [abstract]: "Parallel Flipping, which runs the pipeline with inverted sequences and combines predictions"
  - [section]: "the beginnings of the output sequences were generally more spiky but they became smoother as the sequence progressed" (Page 6)
  - [section]: Appendix Figures 6–7 visualize the spiking pattern in Advection/Diffusion-Reaction predictions.
  - [corpus]: No direct corpus evidence for this specific technique.
- **Break condition**: Would break if the junction point between halves introduces artifacts, or if the spiking pattern is task-dependent and unpredictable.

## Foundational Learning

- **Autoregressive vs. Bidirectional Attention**
  - **Why needed here**: This distinction explains the core architectural limitation being addressed.
  - **Quick check question**: Can you explain why a decoder at position $t$ cannot attend to position $t+1$, and how this asymmetry affects processing of symmetric wave-like data?

- **Cross-Modal Adaptation Methods (FPT and ORCA)**
  - **Why needed here**: These are the adaptation methods being compared across architectures.
  - **Quick check question**: What is the key difference—FPT fine-tunes only input/output layers and layer norms, while ORCA first aligns embeddings using optimal transport distance before full fine-tuning?

- **Non-Generative Output Computation**
  - **Why needed here**: Unlike typical LLM usage, these tasks compute outputs by averaging final hidden representations rather than autoregressive token generation.
  - **Quick check question**: Why might averaging hidden representations fail to leverage decoder-only models' generative strengths?

## Architecture Onboarding

- **Component map**:
  - Input: PDE time-series → task-specific embedder → transformer (encoder-only: RoBERTa/BERT; decoder-only: GPT-2/Pythia at multiple scales) → Final hidden layer → average representations → task-specific predictor → PDE predictions

- **Critical path**:
  1. Establish encoder-only baseline with FPT/ORCA on target PDE dataset
  2. Apply unmodified pipeline to decoder-only to confirm performance gap
  3. Implement Sequence Doubling and verify gap closure

- **Design tradeoffs**:
  - **Parallel Flipping**: Parallelizable (2× compute resources or 2× time); potential artifact at concatenation point
  - **Sequence Doubling**: Not parallelizable; doubles memory/time due to longer sequences; produces smoother outputs without hard boundaries
  - **Scaling**: 12–71× parameter increases yielded minimal gains—architecture, not scale, is the bottleneck

- **Failure signatures**:
  - Decoder-only with ORCA may not beat randomly-initialized versions (pre-training provides no benefit)
  - High variance on Navier-Stokes (ORCA) and Advection (FPT)
  - Scaling shows no consistent improvement trends

- **First 3 experiments**:
  1. Replicate encoder vs. decoder comparison on one PDE dataset (e.g., Advection) to quantify the baseline gap
  2. Implement Sequence Doubling on smallest decoder-only model and verify gap closure to encoder-only performance
  3. Test scaling by evaluating multiple GPT-2 sizes with Sequence Doubling on Navier-Stokes to confirm limited scaling benefits

## Open Questions the Paper Calls Out

- **Open Question 1**: Do the performance gaps between decoder-only and encoder-only models persist when using other cross-modal adaptation methods like PARE or UPS?
  - **Basis in paper**: [explicit] The authors explicitly state in Section 10 (Limitations) that they only experimented with FPT and ORCA, leaving the investigation of whether these patterns hold for PARE (Cai et al., 2024) and UPS (Shen et al., 2024) to future work.
  - **Why unresolved**: The current study restricts its scope to two specific adaptation methods, so it remains unclear if the architectural limitations identified are intrinsic to decoder-only models or specific to how FPT and ORCA utilize them.
  - **What evidence would resolve it**: Running the same ablation studies using PARE and UPS adaptation methods on the PDEBench datasets to observe if the decoder-only performance gap remains.

- **Open Question 2**: What specific factors (e.g., optimizer choice, initial checkpoint randomness) cause the high variance and instability observed in cross-modal adaptation performance?
  - **Basis in paper**: [explicit] In Section 8, the authors identify diagnosing these instabilities as "the most important direction for future work," suggesting that optimizers or weight initialization might be causes.
  - **Why unresolved**: The paper reports high variance in results (shown via large error bars in figures) but does not perform the systematic ablations necessary to isolate the root cause of this instability.
  - **What evidence would resolve it**: A series of controlled experiments varying optimizers and random seeds independently to determine which variable correlates most strongly with performance variance.

- **Open Question 3**: Can explicitly enabling bidirectional attention in decoder-only models (e.g., via LLM2Vec) outperform the proposed Sequence Doubling method for PDE tasks?
  - **Basis in paper**: [explicit] Section 8 mentions that an alternative to simulating bidirectionality through data manipulation (Sequence Doubling) would be to actually enable bidirectional attention using methods like LLM2Vec, leaving this comparison for future work.
  - **Why unresolved**: The authors focused on input-side solutions (Parallel Flipping and Sequence Doubling) rather than modifying the model's internal attention mechanism.
  - **What evidence would resolve it**: A comparative study evaluating the performance of LLM2Vec-adapted decoders against standard decoders using Sequence Doubling on the same PDE benchmarks.

## Limitations

- **Limited hyperparameter transparency**: The paper provides optimizer choices per dataset but omits learning rates, batch sizes, epoch counts, weight decay values, and learning rate schedules, forcing researchers to tune these empirically.
- **Architectural ambiguity**: Task-specific embedder and predictor components are underspecified, including layer counts, activation functions, and initialization schemes, which may impact ORCA's two-stage training effectiveness.
- **Proxy dataset selection rationale**: The CoNLL-2000 proxy dataset with 2000 sequences padded to 32 tokens is used across all four PDE tasks without justification for why this proxy is suitable for all tasks.

## Confidence

- **High confidence**: The core architectural finding that decoder-only models underperform encoder-only models on PDE tasks due to autoregressive attention limitations, supported by direct experimental evidence and sound theoretical mechanism.
- **Medium confidence**: The Sequence Doubling mechanism explanation, as empirical results are strong but theoretical justification relies on unverified assumptions about attention patterns across sequence copies.
- **Low confidence**: The claim that scaling decoder-only models won't close the gap, as the paper only tests up to 1.61B parameters while modern LLMs reach 175B+ parameters.

## Next Checks

1. **Replicate the baseline gap** by implementing FPT and ORCA on Advection with RoBERTa-base and GPT-2 (160M) using 5 seeds each. This validates the core finding before testing proposed solutions and establishes the exact magnitude of the performance gap under reproducible conditions.

2. **Test Sequence Doubling implementation** on the smallest decoder-only model by creating the concatenated input format and extracting the second half of the final hidden layer for prediction. Verify that this produces the claimed performance improvement and compare against the baseline decoder-only results.

3. **Conduct ablation on proxy dataset quality** by training the same decoder-only model with different proxy datasets (varying size, token count, and domain similarity to PDE tasks). This tests whether the proxy dataset choice explains any remaining performance differences between architectures.