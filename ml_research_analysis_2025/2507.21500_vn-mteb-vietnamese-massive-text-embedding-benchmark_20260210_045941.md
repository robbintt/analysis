---
ver: rpa2
title: 'VN-MTEB: Vietnamese Massive Text Embedding Benchmark'
arxiv_id: '2507.21500'
source_url: https://arxiv.org/abs/2507.21500
tags:
- datasets
- retrieval
- https
- huggingface
- mteb
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces VN-MTEB, a comprehensive Vietnamese benchmark
  for evaluating text embedding models, addressing the lack of large-scale Vietnamese
  test datasets. The authors developed an automated translation pipeline leveraging
  LLMs and embedding models to translate and filter high-quality samples from the
  English Massive Text Embedding Benchmark (MTEB) into Vietnamese, preserving semantic
  fidelity, named entities, and code snippets.
---

# VN-MTEB: Vietnamese Massive Text Embedding Benchmark
## Quick Facts
- arXiv ID: 2507.21500
- Source URL: https://arxiv.org/abs/2507.21500
- Reference count: 21
- Primary result: Automated pipeline translating English MTEB to Vietnamese, enabling evaluation of 18 embedding models across 41 datasets

## Executive Summary
VN-MTEB addresses the critical gap in Vietnamese natural language processing evaluation by providing a comprehensive benchmark for text embedding models. The benchmark comprises 41 datasets spanning six tasks including retrieval, classification, and semantic textual similarity, enabling systematic evaluation of embedding models on Vietnamese language data. Through automated translation using LLMs and embedding models, the authors created a high-quality Vietnamese counterpart to the English MTEB benchmark, preserving semantic fidelity, named entities, and code snippets.

The evaluation of 18 embedding models revealed performance variations based on model architecture, with larger models incorporating Rotary Positional Embedding (RoPE) demonstrating superior performance compared to those using Absolute Positional Embedding (APE) for Vietnamese tasks. This finding provides important insights for developing and selecting embedding models for Vietnamese language applications.

## Method Summary
The authors developed an automated translation pipeline to create VN-MTEB by translating English MTEB datasets into Vietnamese. The pipeline leverages large language models (LLMs) and embedding models to perform translations while filtering for high-quality samples that preserve semantic fidelity, named entities, and code snippets. This systematic approach enables the creation of a comprehensive Vietnamese benchmark covering 41 datasets across six task categories: retrieval, reranking, classification, clustering, pair classification, and semantic textual similarity. The benchmark provides a standardized evaluation framework for comparing embedding model performance on Vietnamese language tasks.

## Key Results
- VN-MTEB contains 41 Vietnamese datasets covering six task categories, enabling comprehensive evaluation of text embedding models
- Larger embedding models with Rotary Positional Embedding (RoPE) significantly outperform those with Absolute Positional Embedding (APE) on Vietnamese tasks
- The automated translation pipeline successfully preserves semantic fidelity, named entities, and code snippets while creating high-quality Vietnamese test datasets

## Why This Works (Mechanism)
The benchmark's effectiveness stems from leveraging advanced translation technology to create high-quality Vietnamese datasets that maintain the semantic structure and complexity of the original English MTEB tasks. The automated pipeline ensures consistency across translations while filtering for quality, creating a reliable evaluation framework. The architectural differences between RoPE and APE directly impact how models handle Vietnamese's unique linguistic features, with RoPE providing better positional context for the language's tonal and morphological characteristics.

## Foundational Learning
- **Text embedding benchmarks**: Standardized evaluation frameworks that measure how well models capture semantic relationships in text; needed to objectively compare model performance across different architectures and training approaches; quick check: verify benchmark includes diverse task types and sufficient dataset coverage.
- **Rotary Positional Embedding (RoPE)**: Position encoding method that incorporates relative position information through rotation matrices; needed for better handling of sequential dependencies in Vietnamese text; quick check: confirm RoPE implementation matches original paper specifications.
- **Absolute Positional Embedding (APE)**: Position encoding using fixed absolute position vectors; provides baseline positional information but may be less effective for Vietnamese morphology; quick check: compare APE performance against RoPE across different sequence lengths.
- **Large Language Model (LLM) translation**: Advanced neural translation systems that capture context and nuance; needed for high-quality Vietnamese dataset creation; quick check: validate translation quality through human evaluation on sample subsets.
- **Semantic textual similarity**: Task measuring how well models capture meaning equivalence between text pairs; fundamental for evaluating embedding quality; quick check: ensure benchmark includes multiple similarity datasets with varying difficulty levels.

## Architecture Onboarding
- **Component map**: LLMs -> Translation Pipeline -> Quality Filter -> VN-MTEB Datasets -> Embedding Models -> Performance Evaluation
- **Critical path**: Translation generation → Quality filtering → Dataset validation → Model evaluation → Performance comparison
- **Design tradeoffs**: Automated translation enables scale but may miss cultural nuances; quality filtering ensures reliability but reduces dataset size; standardized tasks enable comparison but may not capture all Vietnamese linguistic phenomena
- **Failure signatures**: Poor translation quality manifests as semantic drift; inadequate filtering produces noisy datasets; missing named entity preservation reduces task relevance; positional encoding mismatches cause performance degradation
- **First experiments**: 1) Validate translation quality on sample datasets using human evaluation, 2) Compare RoPE vs APE performance on Vietnamese similarity tasks, 3) Test embedding model performance across different Vietnamese text lengths

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- The automated translation approach may not fully capture Vietnamese cultural context and idiomatic expressions that require human understanding
- Quality filtering may exclude challenging but valid examples that could provide more comprehensive evaluation
- The benchmark may not cover all Vietnamese linguistic phenomena, particularly domain-specific terminology and dialect variations
- Translation quality depends on the capabilities of the LLMs used, which may evolve over time

## Confidence
- **Paper rigor**: Moderate confidence - the paper presents a systematic approach with clear methodology, but lacks detailed statistical analysis of translation quality and performance differences
- **Result soundness**: Moderate confidence - results are internally consistent with the methodology, but external validation through independent replication would strengthen claims
- **Clarity of claims**: High confidence - claims are clearly stated and directly supported by the presented results
- **Practical utility**: High confidence - the benchmark addresses a genuine need in the Vietnamese NLP community and provides actionable insights for model selection

## Next Checks
- Verify translation quality by comparing sample Vietnamese datasets with human translations
- Replicate RoPE vs APE performance comparison using different Vietnamese text domains
- Test benchmark scalability by adding new Vietnamese datasets and evaluating impact on model rankings
- Conduct ablation studies to quantify the contribution of quality filtering to benchmark reliability
- Evaluate benchmark performance on Vietnamese dialect variations and domain-specific terminology