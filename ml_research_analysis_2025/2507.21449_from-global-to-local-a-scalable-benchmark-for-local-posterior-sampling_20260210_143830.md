---
ver: rpa2
title: 'From Global to Local: A Scalable Benchmark for Local Posterior Sampling'
arxiv_id: '2507.21449'
source_url: https://arxiv.org/abs/2507.21449
tags:
- local
- deep
- sgld
- learning
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper argues that existing global convergence guarantees for
  stochastic gradient MCMC (SGMCMC) algorithms are incompatible with degenerate loss
  landscapes of neural networks, and shifts focus to local posterior sampling. To
  evaluate local sampling performance, the authors introduce a scalable benchmark
  based on estimating the local learning coefficient (LLC) for deep linear networks
  (DLNs), where ground-truth LLC values are analytically known.
---

# From Global to Local: A Scalable Benchmark for Local Posterior Sampling

## Quick Facts
- **arXiv ID:** 2507.21449
- **Source URL:** https://arxiv.org/abs/2507.21449
- **Reference count:** 40
- **Primary result:** Preconditioned SGMCMC (RMSPropSGLD, AdamSGLD) outperform unpreconditioned methods at estimating Local Learning Coefficients in degenerate deep linear networks

## Executive Summary
This paper addresses a fundamental tension in Bayesian deep learning: existing theoretical convergence guarantees for stochastic gradient MCMC (SGMCMC) require assumptions that fail for neural networks with degenerate loss landscapes. The authors shift focus from global to local posterior sampling, introducing a scalable benchmark based on estimating the Local Learning Coefficient (LLC) in deep linear networks (DLNs) where ground-truth values are analytically known. Their experiments show that preconditioned methods (RMSPropSGLD, AdamSGLD) consistently outperform other SGMCMC variants at this task, demonstrating that these algorithms can extract meaningful local geometric information from highly degenerate posteriors despite theoretical divergence concerns.

## Method Summary
The authors develop a benchmark for evaluating local posterior sampling by computing the Local Learning Coefficient (LLC) in deep linear networks. The LLC measures how parameter-space volume scales near a minimum as a function of loss tolerance, providing a ground-truth metric for local geometry. They implement several SGMCMC algorithms (SGLD, RMSPropSGLD, AdamSGLD, SGHMC, SGNHT) and evaluate their ability to estimate LLC values for DLNs with up to 100M parameters. The evaluation uses synthetic data and a Gaussian localizing prior, with samplers running for 50,000 steps from a known low-rank true parameter. The key innovation is demonstrating that this approach scales to large models while maintaining analytical tractability of the ground truth.

## Key Results
- RMSProp-preconditioned SGLD and AdamSGLD consistently outperform other SGMCMC methods at LLC estimation
- Preconditioned methods show less sensitivity to step size and maintain better mean-variance trade-offs
- These algorithms achieve superior order preservation of true LLC values across different network architectures
- Results demonstrate that SGMCMC can extract non-trivial local information about posterior geometry in highly degenerate models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Preconditioned SGMCMC captures local posterior geometry more reliably than unpreconditioned methods in degenerate landscapes.
- **Mechanism:** Per-parameter adaptive step sizes compensate for non-uniform curvature and degenerate directions where gradients provide poor scaling information.
- **Core assumption:** Gradient variance statistics correlate with local geometric structure affecting volume scaling.
- **Evidence anchors:** Abstract mentions RMSPropSGLD/AdamSGLD outperform others with less sensitivity to step size and better mean-variance trade-offs.
- **Break condition:** If degeneracy is so severe that gradient information is uncorrelated with local volume structure, preconditioning may not help.

### Mechanism 2
- **Claim:** The Local Learning Coefficient serves as a tractable ground-truth benchmark for local sampler performance.
- **Mechanism:** LLC quantifies volume scaling near degenerate minima. Sampler samples from tempered posterior yield estimator that converges to true LLC if sampler correctly explores local geometry.
- **Core assumption:** DLN LLC formula correctly captures relevant geometric structure samplers must navigate.
- **Evidence anchors:** Section 3.1 provides volume scaling formula; Section 3.2 establishes DLNs as scalable setting with known LLC values.
- **Break condition:** If samplers explore beyond local neighborhood where asymptotic LLC formula applies, estimator becomes biased.

### Mechanism 3
- **Claim:** Global convergence guarantees for SGLD provably fail when log-likelihood has degenerate critical points.
- **Mechanism:** Standard convergence proofs require global Lipschitz conditions on gradient log-posterior. Degenerate critical points cause polynomial loss landscapes with degree >2, leading to superlinear gradient growth that violates these conditions.
- **Core assumption:** DLN loss landscape is representative of degeneracy patterns in practical neural networks.
- **Evidence anchors:** Section 2.1 cites Hutzenthaler et al. showing divergence for superlinear growth; Lemma 2.1 shows DLN loss is polynomial of degree 2M.
- **Break condition:** If practical samplers use modifications (taming, truncation) that control divergence, theoretical negative result may not predict empirical failure.

## Foundational Learning

- **Concept: Stochastic Gradient MCMC (SGMCMC)**
  - **Why needed here:** Entire paper evaluates SGMCMC variants; understanding gradient-driven diffusion with noise injection is prerequisite.
  - **Quick check question:** Can you explain why adding noise to stochastic gradient descent produces posterior samples rather than just optimization?

- **Concept: Loss Landscape Degeneracy**
  - **Why needed here:** Central thesis is that degeneracy invalidates standard convergence assumptions.
  - **Quick check question:** Why does non-invertible Hessian at a minimum imply standard Gaussian approximation fails?

- **Concept: Local Learning Coefficient (LLC) and Volume Scaling**
  - **Why needed here:** LLC is the evaluation metric; understanding it as generalization of "effective dimensionality" at degenerate minima is essential.
  - **Quick check question:** For non-degenerate minimum in d dimensions, what is the LLC? Why is it smaller for degenerate minima?

## Architecture Onboarding

- **Component map:** DLN Benchmark Generator → True LLC (analytical) → SGMCMC Sampler → Posterior Samples → LLC Estimator → Relative Error

- **Critical path:**
  1. Generate DLN architecture + true parameter w₀ (low rank for interesting degeneracy)
  2. Compute ground-truth LLC via Theorem 3.4
  3. Run sampler from w₀ for T steps with Gaussian localizing prior
  4. Estimate LLC from average loss difference: ̂λ = nβ(L̄ - Lₙ(w₀))
  5. Compare ̂λ to true λ across step sizes, architectures, samplers

- **Design tradeoffs:**
  - RMSPropSGLD vs AdamSGLD: RMSProp has clearer failure signal (catastrophic NaN) vs Adam's gradual degradation
  - Step size: Smaller sizes give more accurate LLC but require more samples; preconditioning widens viable range
  - Burn-in: 90% burn-in (B = 0.9T) used; non-localized initial samples would bias estimates

- **Failure signatures:**
  - NaN estimates → step size too large (gradient explosion)
  - ̂λ ≈ d/2 for degenerate minima → sampler stuck in local mode
  - High variance across runs → step size or localization parameter poorly tuned
  - Loss trace spikes → numerical instability, common in SGLD without preconditioning

- **First 3 experiments:**
  1. Reproduce 100K DLN benchmark: Implement RMSPropSGLD, sweep step size 10⁻¹⁴ to 10⁻⁶, plot relative error vs step size.
  2. Ablate preconditioning: Compare SGLD vs RMSPropSGLD on 1M DLN, quantify proportion of NaN runs and variance of error.
  3. Order preservation test: Generate 10 DLNs with varying true LLCs, run each sampler at fixed step size, compute rank correlation between estimated and true LLC order.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What theoretical mechanism explains empirical success of SGMCMC in extracting local geometric information from degenerate posteriors where global convergence guarantees fail?
- **Basis in paper:** Section 5 labels "A theoretical explanation for the empirical success of local SGMCMC" as open problem, noting empirical success despite violations of assumptions required by standard theory.
- **Why unresolved:** Current theory predicts divergence or requires assumptions that don't hold for deep linear networks, yet samplers successfully estimate LLC in these settings.
- **What evidence would resolve it:** Formal convergence proof for local sampling that accounts for degenerate geometry without relying on global Lipschitz constraints.

### Open Question 2
- **Question:** What determines the "effective support" of SGMCMC sampling chains in practice when non-compact priors are used?
- **Basis in paper:** Section 5 identifies understanding "effective support" as central issue because experiments use Gaussian priors but succeed locally, contradicting theoretical requirements for compactness.
- **Why unresolved:** Theoretical validity of LLC estimation relies on compact supports, creating disconnect with empirical methodology using unbounded priors.
- **What evidence would resolve it:** Analytical characterization of implicit boundaries or potential functions confining sampler to local neighborhood in high-dimensional degenerate spaces.

### Open Question 3
- **Question:** Do performance rankings of SGMCMC algorithms generalize to non-linear networks or different classes of degeneracy?
- **Basis in paper:** Section 5 notes DLNs "may not capture all forms of degeneracy" and suggests developing wider set of degeneracy-aware benchmarks.
- **Why unresolved:** Benchmark relies on analytically tractable LLCs in DLNs; unverified if results hold for complex non-linear degeneracies in standard transformers or CNNs.
- **What evidence would resolve it:** Experiments on non-linear models with known degeneracy structures confirming adaptive preconditioners still outperform standard methods.

## Limitations
- The DLN benchmark may not capture full complexity of degeneracy in practical neural networks compared to general deep networks with nonlinear activations
- Experimental validation is limited to specific model scales (100K-100M parameters) and architectures (deep linear networks)
- The mechanism by which preconditioning compensates for degeneracy could involve factors beyond gradient variance adaptation

## Confidence
- **High Confidence:** Theoretical argument about superlinear gradient growth causing divergence in degenerate landscapes is mathematically sound given assumptions
- **Medium Confidence:** Empirical claim that RMSPropSGLD/AdamSGLD outperform other methods is well-supported within DLN setting
- **Low Confidence:** Claim that gradient variance statistics directly capture relevant local geometric structure in degenerate landscapes is plausible but not rigorously established

## Next Checks
1. **Generalization to Nonlinear Networks:** Apply benchmark methodology to deep networks with nonlinear activations (e.g., fully-connected ReLU networks) to determine if preconditioned methods maintain advantage.

2. **Mechanism Dissection:** Design experiments isolating effect of gradient variance adaptation versus other adaptive mechanisms (momentum, bias correction) by creating variants of RMSPropSGLD using different statistics.

3. **Real-World Task Validation:** Implement DLN-based image classification or regression task on real data, comparing downstream task performance when using posterior samples from different SGMCMC algorithms to validate correlation between better LLC estimation and predictive uncertainty calibration.