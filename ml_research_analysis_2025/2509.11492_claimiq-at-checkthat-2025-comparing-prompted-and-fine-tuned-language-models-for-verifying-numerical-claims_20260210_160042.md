---
ver: rpa2
title: 'ClaimIQ at CheckThat! 2025: Comparing Prompted and Fine-Tuned Language Models
  for Verifying Numerical Claims'
arxiv_id: '2509.11492'
source_url: https://arxiv.org/abs/2509.11492
tags:
- evidence
- llama
- claim
- language
- fine-tuned
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of verifying numerical and temporal
  claims using retrieved evidence in the CLEF 2025 CheckThat! Lab Task 3.
---

# ClaimIQ at CheckThat! 2025: Comparing Prompted and Fine-Tuned Language Models for Verifying Numerical Claims

## Quick Facts
- **arXiv ID:** 2509.11492
- **Source URL:** https://arxiv.org/abs/2509.11492
- **Reference count:** 25
- **Primary result:** Best model achieved 0.945 macro-F1 on validation but dropped to 0.424 on test, highlighting severe generalization gap

## Executive Summary
This paper addresses the challenge of verifying numerical and temporal claims using retrieved evidence in the CLEF 2025 CheckThat! Lab Task 3. The authors explore two complementary approaches: zero-shot prompting with instruction-tuned large language models (LLMs) and supervised fine-tuning using parameter-efficient LoRA. They investigate evidence selection strategies, including full-document input and top-k sentence filtering using BM25 and MiniLM. Their best-performing model, LLaMA fine-tuned with LoRA using full-document evidence, achieved a macro-F1 of 0.945 on the English validation set. However, the test set performance dropped to a macro-F1 of 0.424, highlighting a generalization challenge. The findings underscore the importance of evidence granularity and model adaptation for robust numerical fact verification.

## Method Summary
The authors investigate numerical claim verification using three model approaches: prompted LLaMA-3.1-8B-Instruct (zero-shot), fine-tuned RoBERTa-base, and fine-tuned LLaMA-3.1-8B with LoRA. They employ three evidence selection strategies: full document passthrough, top-3 BM25 sentences, and top-3 MiniLM sentences via cosine similarity. The fine-tuning uses LoRA with rank=8, alpha=16, dropout=0.05 on query, key, value, and output projections. Training uses FP16, gradient checkpointing, batch size 2 with gradient accumulation of 4, and 3 epochs. Evidence comes from up to 100 BM25-ranked documents per claim from the CLEF 2025 Task 3 English dataset (15,514 claims) with a 90/10 train/validation split.

## Key Results
- LLaMA fine-tuned with LoRA using full-document evidence achieved 0.945 macro-F1 on validation
- Test set performance dropped to 0.424 macro-F1, indicating severe generalization gap
- Zero-shot prompted LLaMA showed more stable performance across distribution shifts compared to fine-tuned models
- Conflicting class showed the lowest test F1 (0.32), with models defaulting to binary True/False decisions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Parameter-efficient fine-tuning (LoRA) with full-document evidence enables stronger numerical claim verification than zero-shot prompting on in-distribution data.
- **Mechanism:** LoRA adapts query, key, value, and output projection layers (rank=8, α=16, dropout=0.05) to align model behavior with the 3-class classification task, preserving pretrained knowledge while learning task-specific patterns from claim-evidence-label triplets.
- **Core assumption:** The validation set distribution approximates the test set; task-specific supervision will generalize.
- **Evidence anchors:**
  - [abstract] "Our best-performing model, LLaMA fine-tuned with LoRA using full-document evidence, achieved a macro-F1 of 0.945 on the English validation set."
  - [section 4.4] "LoRA applies to query, key, value, and output projections (r=8, α=16, dropout=0.05)."
  - [corpus] Related CheckThat! 2025 systems (DS@GT, CEA-LIST) similarly explore fine-tuning vs prompting trade-offs, with mixed generalization results.
- **Break condition:** When test distribution shifts significantly from validation (as observed), LoRA adaptation overfits to spurious validation patterns, causing performance collapse (0.945 → 0.424 macro-F1).

### Mechanism 2
- **Claim:** Evidence granularity—full document vs. top-k sentence filtering—interacts with model capacity to affect class-wise performance differently across validation and test.
- **Mechanism:** Full-document evidence provides richer context for resolving ambiguous/conflicting claims but introduces noise that smaller models (RoBERTa) struggle with; sentence-level filtering (BM25/MiniLM) reduces noise but may exclude critical context for numerical reasoning.
- **Core assumption:** Semantic similarity (MiniLM) or lexical overlap (BM25) correlates with evidence relevance for claim verification.
- **Evidence anchors:**
  - [abstract] "They investigate evidence selection strategies, including full-document input and top-k sentence filtering using BM25 and MiniLM."
  - [section 6.1] "RoBERTa performs well across both BM25 and MiniLM sentence-level evidence... suggesting that sentence-level semantic filtering benefits models lacking strong pretraining on numerical reasoning."
  - [corpus] DS@GT at CheckThat! 2025 similarly evaluates context and tokenization strategies, finding that context window management impacts numerical fact verification.
- **Break condition:** When key numerical evidence is distributed across multiple sentences or requires document-level context, sentence filtering excludes necessary information.

### Mechanism 3
- **Claim:** Zero-shot prompting exhibits more stable (though lower) performance across distribution shifts compared to fine-tuned models that overfit to training patterns.
- **Mechanism:** Zero-shot models rely on pretrained reasoning capabilities without dataset-specific adaptation, avoiding overfitting but lacking task alignment; fine-tuned models learn dataset-specific patterns that may not transfer.
- **Core assumption:** The instruction-tuned model's pretrained knowledge includes sufficient numerical reasoning capability for the task.
- **Evidence anchors:**
  - [section 6.2] "Zero-shot prompting generalized better than fine-tuned RoBERTa, whose macro-F1 dropped to 0.35."
  - [section 6.3] "The model may have overfit to patterns in the validation data or faced difficulties adapting to shifts in evidence structure and language style in the test set."
  - [corpus] Corpus evidence weak for this specific generalization claim; related systems do not consistently report validation-test gaps.
- **Break condition:** When task requires domain-specific knowledge or label conventions not captured in pretrained instruction-following, zero-shot fails to produce correct labels.

## Foundational Learning

- **Concept: Low-Rank Adaptation (LoRA)**
  - **Why needed here:** Core technique enabling efficient LLaMA fine-tuning; understanding rank, alpha, and target modules is necessary to interpret results and reproduce experiments.
  - **Quick check question:** If LoRA rank were increased from 8 to 32, would you expect more or less overfitting to the validation set, and why?

- **Concept: Evidence Retrieval and Filtering (BM25 vs. Dense Retrieval)**
  - **Why needed here:** The paper compares lexical (BM25) and semantic (MiniLM) sentence selection; understanding their failure modes explains performance differences.
  - **Quick check question:** For a claim like "Unemployment rose 15% in 2023," would BM25 or MiniLM be more likely to retrieve a sentence stating "Joblessness increased significantly last year"?

- **Concept: Distribution Shift and Generalization Gaps**
  - **Why needed here:** The 0.945→0.424 validation-test gap is the central negative finding; understanding overfitting vs. distribution shift is critical for future work.
  - **Quick check question:** If test-set claims used different numerical formats (e.g., "1.5 million" vs. "1,500,000") than validation, which mechanism—overfitting or distribution shift—better explains the performance drop?

## Architecture Onboarding

- **Component map:** Input Layer (Claim + evidence set) -> Evidence Selection Module (Full document / Top-3 BM25 / Top-3 MiniLM) -> Model Backbone (Prompted LLaMA / Fine-tuned RoBERTa / Fine-tuned LLaMA+LoRA) -> Output Processing (Regex extraction + label mapping) -> Macro-F1 evaluation

- **Critical path:** Evidence selection → Prompt construction (for LLaMA) or tokenization (for RoBERTa) → Model inference → Label extraction → Macro-F1 evaluation. The LoRA fine-tuning path additionally requires: Prompt-response pair formatting → 3-epoch training with gradient accumulation (batch size 2, accum 4) → LoRA checkpoint loading

- **Design tradeoffs:**
  - Full-document evidence: Higher context completeness vs. noise dilution; benefits large models more than smaller ones
  - Sentence-level filtering: Reduced context length and noise vs. potential exclusion of distributed evidence; MiniLM captures semantics, BM25 captures lexical precision
  - LoRA fine-tuning: Strong in-distribution performance vs. overfitting risk; zero-shot: Lower peak performance but more robust to distribution shift

- **Failure signatures:**
  - **Validation-test gap >0.4 macro-F1:** Indicates overfitting or significant distribution shift; examine claim types, evidence sources, and label distributions across splits
  - **Conflicting class F1 <0.15:** Model defaults to binary True/False; evidence may be insufficiently ambiguous or model lacks calibration for uncertainty
  - **True class F1 drops disproportionately on test:** Numerical reasoning patterns learned during fine-tuning may not generalize; check for format/genre shifts in numerical expressions

- **First 3 experiments:**
  1. **Baseline reproduction:** Run zero-shot prompted LLaMA with full-document evidence on validation split; target macro-F1 ~0.609. This confirms infrastructure and prompt formatting.
  2. **Ablation on evidence granularity:** Compare Top-3 MiniLM vs. full-document for fine-tuned LoRA on validation; expect full-document to outperform (~0.945 vs. ~0.660).
  3. **Generalization probe:** Evaluate the best validation configuration (LoRA + full-document) on a held-out slice with manually characterized distribution shifts (e.g., different numerical formats, evidence sources); diagnose whether drop stems from overfitting or shift.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can domain-adaptive training mitigate the severe generalization gap observed between the validation and test sets?
- **Basis in paper:** [explicit] The Conclusion identifies "domain-adaptive training" as a necessary direction to address the "performance drop on the test set."
- **Why unresolved:** The best model dropped from a validation Macro-F1 of 0.945 to a test Macro-F1 of 0.424, suggesting overfitting to specific evidence patterns that standard fine-tuning could not overcome.
- **What evidence would resolve it:** Experiments applying domain adaptation techniques to the fine-tuned LLaMA model to measure if test set Macro-F1 stabilizes closer to validation performance.

### Open Question 2
- **Question:** To what extent do reasoning-aware modeling strategies improve performance on the "Conflicting" class?
- **Basis in paper:** [explicit] The Conclusion suggests exploring "reasoning-aware modeling strategies" to enhance robustness.
- **Why unresolved:** The "Conflicting" class showed the lowest test F1 (0.32), and the discussion links failures to "numerical reasoning challenges" and ambiguous evidence that standard modeling failed to parse.
- **What evidence would resolve it:** Implementation of explicit reasoning modules (e.g., Chain-of-Thought) or numerical pre-training tasks, resulting in significantly higher F1 scores for the Conflicting class.

### Open Question 3
- **Question:** Can improved retrieval filtering prevent the context dilution observed in full-document evidence without losing necessary context?
- **Basis in paper:** [explicit] The Conclusion calls for "improved retrieval filtering" as a key area for future work.
- **Why unresolved:** While full-document evidence yielded the best validation score, it failed to generalize; conversely, top-k sentence filtering was too restrictive.
- **What evidence would resolve it:** Comparing advanced re-ranking methods against the BM25 and MiniLM baselines to find a balance that maintains high validation F1 without overfitting.

## Limitations
- Severe generalization gap between validation (0.945 macro-F1) and test (0.424 macro-F1) performance suggests overfitting to validation patterns or distribution shift
- Conflicting class severely underperforms (F1 0.085 on test), with models defaulting to binary decisions rather than handling ambiguity
- Root cause of generalization failure not fully characterized—could be overfitting, distribution shift, or both

## Confidence
- **High confidence:** The mechanism that LoRA fine-tuning with full-document evidence achieves strong in-distribution performance (validation macro-F1 0.945) is well-supported by the experimental results and aligns with established understanding of parameter-efficient adaptation.
- **Medium confidence:** The generalization failure (validation-to-test drop from 0.945 to 0.424) is observed but the root cause remains unclear—it could be overfitting, distribution shift, or both. The paper's attribution to "difficulties adapting to shifts in evidence structure and language style" is plausible but not definitively proven.
- **Medium confidence:** The claim that zero-shot prompting shows more stable (though lower) performance across distribution shifts is supported by the relative performance drop being less severe for prompted LLaMA, but this requires more systematic distribution shift analysis to confirm.

## Next Checks
1. **Distribution Shift Analysis:** Characterize the validation and test sets by claim types, numerical formats, evidence source domains, and label distributions. This would determine whether the performance gap stems from overfitting or fundamental distributional differences.

2. **Conflicting Class Calibration:** Implement focal loss or class-weighted training to address the severe underperformance on the conflicting class (F1 0.085 on test). Analyze whether this stems from class imbalance or model uncertainty calibration issues.

3. **Evidence Granularity Ablation:** Systematically compare full-document vs. sentence-level evidence (BM25 and MiniLM) across validation and test sets with detailed class-wise breakdown. This would clarify whether the drop is due to noise in full documents or exclusion of distributed evidence in sentence filtering.