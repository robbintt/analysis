---
ver: rpa2
title: 'From Polyester Girlfriends to Blind Mice: Creating the First Pragmatics Understanding
  Benchmarks for Slovene'
arxiv_id: '2510.21575'
source_url: https://arxiv.org/abs/2510.21575
tags:
- language
- understanding
- slovene
- llms
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces SloPragEval and SloPragMega, the first benchmarks
  for evaluating pragmatic understanding in Slovene, covering 405 multiple-choice
  questions across various nuanced language phenomena. The authors translate and culturally
  adapt existing English pragmatics datasets, addressing challenges of machine translation
  and cultural specificity.
---

# From Polyester Girlfriends to Blind Mice: Creating the First Pragmatics Understanding Benchmarks for Slovene

## Quick Facts
- **arXiv ID**: 2510.21575
- **Source URL**: https://arxiv.org/abs/2510.21575
- **Reference count**: 0
- **Primary result**: Introduced SloPragEval and SloPragMega, the first benchmarks for evaluating pragmatic understanding in Slovene, with GPT-5 achieving near-human performance while open-source models lag significantly behind.

## Executive Summary
This work introduces SloPragEval and SloPragMega, the first benchmarks for evaluating pragmatic understanding in Slovene. The authors translate and culturally adapt existing English pragmatics datasets, creating 405 multiple-choice questions covering various nuanced language phenomena including Gricean maxim violations and pragmatic categories like irony, metaphor, and humor. Human evaluation establishes a baseline accuracy of around 0.85. LLM testing reveals significant performance gaps between proprietary models (GPT-5 achieving 0.81-0.83 accuracy) and open-source models, with smaller models lagging notably behind. The results indicate that while current models have improved in understanding nuanced language, they still struggle with culture-specific implied meanings, particularly in Manner-flouting utterances.

## Method Summary
The authors created Slovene pragmatics benchmarks by translating and culturally adapting existing English datasets (SloPragEval with 300 items, SloPragMega with 105 items). Human evaluators established a baseline accuracy of ~0.85. For LLM evaluation, they used zero-shot multiple-choice question answering with both Slovene and English prompts, testing proprietary models (GPT-5, GPT-5-chat) and open-source models (DeepSeek-R1-Distill-Qwen-14B, Gemma3-27B, GaMS-27B, Llama3.3-70B). Answer extraction used regex matching on model outputs, with accuracy computed over three runs per model per dataset.

## Key Results
- GPT-5 achieves near-human performance (0.81-0.83 accuracy) on Slovene pragmatics benchmarks
- Open-source models show significant performance gaps, with smaller models lagging notably behind
- Manner-flouting utterances are particularly challenging, with both humans and LLMs achieving only 0.67-0.68 accuracy
- Models show improved pragmatic understanding but struggle with culture-specific implied meanings

## Why This Works (Mechanism)
The benchmarks work by providing structured multiple-choice questions that test whether models can identify the intended meaning behind utterances that violate Gricean maxims or employ pragmatic phenomena. The translation and cultural adaptation process aims to preserve the pragmatic intent while making scenarios relevant to Slovene speakers. Model performance differences likely reflect both language-specific pretraining and the ability to recognize cross-linguistic pragmatic patterns.

## Foundational Learning
- **Gricean maxims**: Four principles (Quality, Quantity, Relevance, Manner) that govern cooperative conversation; needed to understand what constitutes pragmatic violations
- **Pragmatic phenomena**: Types of non-literal language including irony, metaphor, and humor; needed to categorize the different ways meaning can be implied
- **Cultural adaptation**: Process of modifying translated content to fit target culture; needed to ensure benchmarks test genuine Slovene pragmatic understanding
- **Zero-shot prompting**: Evaluating models without task-specific training examples; needed to assess baseline pragmatic capabilities
- **Quantization**: Reducing model precision for efficient inference; needed to run large models on limited hardware
- **Regex extraction**: Using regular expressions to parse model outputs; needed to automatically evaluate multiple-choice answers

## Architecture Onboarding
- **Component map**: Translation process -> Cultural adaptation -> Human evaluation -> LLM zero-shot MCQA -> Answer extraction -> Accuracy computation
- **Critical path**: Human evaluation baseline -> LLM testing -> Performance comparison across model families and pragmatic categories
- **Design tradeoffs**: Translation preserves original pragmatic intent but may retain English patterns; cultural adaptation improves relevance but introduces subjective decisions
- **Failure signatures**: Low accuracy on Manner-flouting suggests genuine difficulty vs. translation artifacts; performance gaps between models indicate pretraining limitations
- **First experiments**:
  1. Run human evaluation with explicit inter-annotator agreement metrics
  2. Compare model performance on translated vs. natively Slovene pragmatic scenarios
  3. Analyze model-generated explanations for correct vs. incorrect answers

## Open Questions the Paper Calls Out
- Do LLMs achieve high performance on translated pragmatics benchmarks by pivoting through English representations rather than genuinely understanding the target language's pragmatic phenomena?
- Why do both humans and LLMs show markedly lower accuracy (0.67â€“0.68) on Manner-flouting utterances compared to other pragmatic phenomena?
- Can open-source models close the performance gap with proprietary models on pragmatic understanding for lower-resource languages through language-specific pretraining?
- What insights into LLM pragmatic reasoning capabilities can be gained from analyzing the explanations and reasoning chains models generate alongside their answer selections?

## Limitations
- Translation and cultural adaptation process introduces potential noise and may preserve English pragmatic patterns
- Human baseline lacks explicit inter-annotator agreement metrics, making reliability assessment difficult
- Manner-flouting category's poor performance across all models suggests either genuine difficulty or misalignment between translated scenarios and Slovene pragmatic norms

## Confidence
- **High Confidence**: Benchmark construction methodology and comparative performance ranking between GPT-5 and open-source models
- **Medium Confidence**: Specific accuracy values and category-wise performance differences (may vary with prompt formatting)
- **Low Confidence**: Claims about cultural specificity of pragmatic understanding lack direct evidence distinguishing translation artifacts from genuine cultural differences

## Next Checks
1. Replicate human evaluation with explicit inter-annotator agreement metrics and document cultural adaptation decision process
2. Conduct ablation studies comparing model performance on originally Slovene-constructed vs. translated pragmatic scenarios
3. Perform error analysis on Manner-flouting utterances to determine whether failures stem from linguistic ambiguity, cultural unfamiliarity, or reasoning limitations