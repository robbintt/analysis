---
ver: rpa2
title: 'Correcting for Position Bias in Learning to Rank: A Control Function Approach'
arxiv_id: '2506.06989'
source_url: https://arxiv.org/abs/2506.06989
tags:
- click
- bias
- data
- ranking
- position
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses position bias in learning-to-rank (LTR) systems,
  where users are more likely to click on higher-ranked items regardless of their
  true relevance. The authors propose a novel control function-based method called
  CFC that corrects for position bias in a two-stage process.
---

# Correcting for Position Bias in Learning to Rank: A Control Function Approach

## Quick Facts
- arXiv ID: 2506.06989
- Source URL: https://arxiv.org/abs/2506.06989
- Reference count: 40
- Primary result: Novel CFC method corrects position bias in LTR without requiring click/propensity models or modifying loss functions

## Executive Summary
This paper addresses position bias in learning-to-rank systems where users click higher-ranked items regardless of true relevance. The authors propose a two-stage control function approach (CFC) that extracts exogenous variation from ranking residuals to correct for this bias. Unlike previous methods, CFC doesn't require knowledge of the click or propensity model and can be applied to any ranking algorithm. The method also introduces a technique to debias validation clicks for hyperparameter tuning when unbiased labels are unavailable.

## Method Summary
The CFC method works in two stages: first, a model predicts historical ranking positions from features, and the residuals capture exogenous variation; second, these residuals and their interactions with features are used as control functions in a standard LTR algorithm to correct for position bias. The approach doesn't require knowledge of the click or propensity model and allows for nonlinearity in the underlying ranking model. A separate residualization process can debias validation clicks for hyperparameter tuning. The method is evaluated on three benchmark datasets (Yahoo, MSLR-WEB10k, Istella-S) using synthetic clicks generated via Position-Based Model.

## Key Results
- CFC outperforms state-of-the-art position bias correction methods on all three benchmark datasets
- When using true relevance labels for hyperparameter tuning, CFC achieved ERR@10 scores of 0.462 (Yahoo), 0.324 (MSLR-WEB10k), and 0.717 (Istella-S)
- The method is robust to varying degrees of position bias severity, different numbers of clicks, and click noise
- CFC's performance advantage is maintained when using debiased validation clicks for hyperparameter tuning

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Decomposing the historical ranking process into explained and residual components isolates exogenous variation caused by position rather than relevance.
- **Mechanism:** In the first stage, a model $m(\cdot)$ predicts item rank using query-item features. The residuals from this model capture unobserved factors and randomness in the ranking process that are mean-independent of the item features.
- **Core assumption:** The first-stage model is specified well enough that the residuals are mean-independent of the features, and the ranking errors contain the necessary variance structure.
- **Evidence anchors:**
  - [abstract] "...first stage uses exogenous variation from the residuals of the ranking process..."
  - [section 4.1] "We refer to this step as residualization: decomposing the observed rank into a component explained by the observable features... and a residual component."
- **Break condition:** The first-stage model overfits or underfits significantly, causing residuals to contain feature information (failing the exogeneity condition).

### Mechanism 2
- **Claim:** Including ranking residuals and their interactions as control functions in the second-stage click model corrects for endogeneity (position bias) without modifying the ranker's loss function.
- **Mechanism:** The second-stage LTR algorithm takes both original features and the control functions (transformed residuals) as input. By conditioning on the residuals, the model absorbs the correlation between item features and the error term in the click equation, leaving the feature coefficients unbiased.
- **Core assumption:** Position bias induces endogeneity such that unobserved factors affecting ranking are correlated with unobserved determinants of clicks.
- **Evidence anchors:**
  - [abstract] "...use these residuals and feature interaction terms as control functions within the click model to account for position bias."
  - [section 4.2] "Conditioning on a function of the ranking residual controls for this dependence."
- **Break condition:** The control function terms are insufficient to capture the bias (e.g., linear residuals used when non-linear transformations are required).

### Mechanism 3
- **Claim:** A separate residualization process can debias validation clicks, enabling effective hyperparameter tuning when ground-truth relevance labels are unavailable.
- **Mechanism:** A regression model is trained to predict clicks using only the transformed residuals from the first stage. The residuals of this validation model (the part of the click unexplained by position) serve as a proxy for relevance during validation.
- **Core assumption:** The transformed residuals $T(\hat{\epsilon})$ fully explain the position-biased component of the validation clicks.
- **Evidence anchors:**
  - [section 4.4] "To debias the clicks from the position effect, we can partial out the effect of T(...) from clicks."
  - [section 5.2] "CFC achieves the best performance... while CFC-BC [using biased clicks] consistently underperforms."
- **Break condition:** The validation set has a different position-bias distribution than the training set.

## Foundational Learning

- **Concept: Endogeneity in Observational Data**
  - **Why needed here:** The entire paper relies on the premise that position bias creates an endogeneity problem (features correlate with the error term), which standard regression cannot solve.
  - **Quick check question:** If you include "rank" as a feature in a standard click model, does it solve the endogeneity? (Hint: No, because rank is determined by the policy and creates policy-induced variation).

- **Concept: Heteroskedasticity (Lewbel's Method)**
  - **Why needed here:** The CFC method leverages heteroskedasticity (variance of errors changing with features) to construct control function terms when simple residualization is insufficient.
  - **Quick check question:** Why is feature-dependent variance in ranking errors useful for bias correction? (Hint: It provides the identification condition needed to separate bias from relevance).

- **Concept: Residual Transformations (IMR/Hazard Ratio)**
  - **Why needed here:** The authors treat the shape of the residual distribution (e.g., assuming normality vs. non-parametric) as a hyperparameter to optimize performance.
  - **Quick check question:** Why might an Inverse Mills Ratio (Hazard Ratio) be a better control function input than a raw residual? (Hint: It models the likelihood of missingness/selection).

## Architecture Onboarding

- **Component map:**
  Stage 1 Regressor -> Transformation Layer -> Stage 2 Ranker -> (Debiased Validator)

- **Critical path:**
  1. Train Stage 1 on `(features, historical_rank)`
  2. Compute residuals and transform them (e.g., via KDE)
  3. Train Stage 2 on `(features, transformed_residuals, clicks)`
  4. Inference: Pass only `features` to the trained Stage 2 model

- **Design tradeoffs:**
  - **CFC vs. CFC-S:** CFC includes interaction terms (doubles input dimensionality); CFC-S uses only the residual. CFC-S may generalize better with limited data or GBDT rankers.
  - **Ridge vs. XGBoost for Stage 1:** Ridge is faster and generally sufficient; complex models (XGBoost) showed no significant gain in the ablation studies.
  - **Transformation choice:** PDF/Hazard ratios assume specific distributions; Min-Max is distribution-agnostic but may capture less information.

- **Failure signatures:**
  - Performance drops as training data increases (suggesting overfitting to the bias structure rather than relevance)
  - Stage 1 residuals show no heteroskedasticity (statistically testable via Fligner-Killeen)
  - Validation performance diverges from test performance (indicating the Debiasing Validator is leaking bias)

- **First 3 experiments:**
  1. **Heteroskedasticity Validation:** Run a Fligner-Killeen test on the Stage 1 residuals to confirm the assumption required for CFC interaction terms.
  2. **Ablation on Stage 1 Model:** Swap Ridge for a simple mean predictor to confirm that modeling the ranking process adds value over naive baseline.
  3. **Hyperparameter Sensitivity:** Compare CFC using debiased validation clicks vs. raw biased clicks to quantify the gain from the proposed tuning method.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the Control Function-based Correction (CFC) framework be generalized to simultaneously correct for other biases common in Learning to Rank, such as selection bias and trust bias?
- **Basis in paper:** [explicit] The Conclusion states, "Our CFC method is flexible and can be extended to incorporate more biases encountered in LTR systems, such as selection bias and trust bias."
- **Why unresolved:** The current study focuses exclusively on isolating and correcting position bias, leaving the interaction and integration of other bias-correction mechanisms within the control function framework unexplored.
- **What evidence would resolve it:** An extension of the CFC model that incorporates variables or residual transformations for selection/trust, evaluated on datasets where these biases are known to exist or are simulated.

### Open Question 2
- **Question:** Can the proposed residual-based debiasing technique for validation clicks be applied to propensity-based baselines to improve their hyperparameter tuning stability?
- **Basis in paper:** [inferred] The authors note that existing baselines "rely on biased clicks for validation" and introduce a debiasing method specific to their framework (Eq. 9). It is unstated if this validation correction is theoretically transferable to IPW-based methods.
- **Why unresolved:** The experiments show CFC outperforming baselines that use biased validation data, but it's unclear if the performance gap is due to the core ranking method or the baselines' lack of a debiased validation mechanism.
- **What evidence would resolve it:** Experimental results where baseline methods (e.g., DLA, PairD) are tuned using the proposed click debiasing technique, compared against their standard tuning process.

### Open Question 3
- **Question:** How robust is the CFC method to violations of the "covariate exogeneity" assumption in complex, dynamic ranking environments?
- **Basis in paper:** [inferred] Section 4.2 states that the method assumes features are uncorrelated with structural errors ($E[\mathbf{x} \epsilon]=0$). The authors note this is "not directly testable" and relies on the assumption that features are determined prior to ranking.
- **Why unresolved:** In real-world systems, features can be endogenous (e.g., features derived from user behavior which correlates with the error term), potentially invalidating the identification strategy.
- **What evidence would resolve it:** Sensitivity analysis using synthetic data where the degree of feature-endogeneity is controlled to observe the point of failure for the CFC estimator.

## Limitations
- The core mechanism relies on heteroskedasticity in Stage 1 residuals, which is untested and may not hold across datasets
- Performance gains appear sensitive to choice of residual transformation with no systematic guidance for selection
- Validation debiasing assumes position-bias structure matches between training and validation sets
- Control function approach increases input dimensionality, potentially causing overfitting on high-dimensional features

## Confidence

**Confidence labels:**
- **High confidence:** The two-stage residualization framework is technically sound and the core identification argument is valid
- **Medium confidence:** Experimental results show consistent improvements over baselines, but magnitude of gains varies significantly across datasets
- **Low confidence:** The theoretical justification for residual transformations as control functions lacks empirical validation of the underlying distributional assumptions

## Next Checks

1. Test the heteroskedasticity assumption by running Fligner-Killeen tests on Stage 1 residuals across all datasets to verify the identification condition.
2. Conduct ablation studies comparing CFC performance when using a naive mean predictor vs. Ridge in Stage 1 to quantify the value of modeling the ranking process.
3. Validate the debiased validation technique by comparing hyperparameter tuning results using debiased clicks vs. true relevance labels across multiple runs to measure the practical benefit.