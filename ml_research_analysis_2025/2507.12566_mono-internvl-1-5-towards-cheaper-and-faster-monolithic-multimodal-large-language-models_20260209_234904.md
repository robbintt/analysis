---
ver: rpa2
title: 'Mono-InternVL-1.5: Towards Cheaper and Faster Monolithic Multimodal Large
  Language Models'
arxiv_id: '2507.12566'
source_url: https://arxiv.org/abs/2507.12566
tags:
- visual
- arxiv
- data
- mono-internvl
- mono-internvl-1
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenges of training monolithic multimodal
  large language models (MLLMs), which integrate visual encoding and language decoding
  into a single model but often suffer from unstable optimization and catastrophic
  forgetting. The authors propose Mono-InternVL, which embeds visual experts into
  a pre-trained LLM using a multimodal mixture-of-experts architecture, enabling stable
  learning of visual knowledge via delta tuning.
---

# Mono-InternVL-1.5: Towards Cheaper and Faster Monolithic Multimodal Large Language Models

## Quick Facts
- **arXiv ID:** 2507.12566
- **Source URL:** https://arxiv.org/abs/2507.12566
- **Reference count:** 40
- **Primary result:** Mono-InternVL-1.5 achieves similar multimodal performance to InternVL-1.5 while reducing first-token latency by up to 69%

## Executive Summary
This paper addresses the challenges of training monolithic multimodal large language models (MLLMs) that integrate visual encoding and language decoding into a single model. The authors propose Mono-InternVL, which embeds visual experts into a pre-trained LLM using a multimodal mixture-of-experts architecture, enabling stable learning of visual knowledge via delta tuning. To further improve efficiency, they introduce Mono-InternVL-1.5, which incorporates visual attention experts and an improved Endogenous Visual Pre-training (EViP++) strategy, reducing training data by 58% while maintaining competitive performance. Experimental results show that Mono-InternVL outperforms existing monolithic MLLMs on 12 out of 15 benchmarks, with a +114-point improvement over Emu3 on OCRBench.

## Method Summary
Mono-InternVL uses a monolithic architecture where visual experts are embedded into a pre-trained LLM (InternLM2-1.8B) through a multimodal mixture-of-experts (MMoE) approach. The model employs static hard routing—visual tokens route exclusively to visual FFN experts, textual tokens to text FFN experts. During pre-training stages S1.1 and S1.2, LLM parameters remain frozen while visual experts and patch embeddings update via delta tuning. In S1.3, attention layers unfreeze for vision-language alignment. Mono-InternVL-1.5 extends this with visual attention experts and EViP++, a progressive curriculum that organizes training data from noisy/simple to high-quality/complex, reducing total training data by 58% to approximately 500M pairs. The architecture also includes a fused CUDA kernel for MoE operations that exploits token-type sparsity to achieve up to 26% inference speedup.

## Key Results
- Mono-InternVL outperforms existing monolithic MLLMs on 12 out of 15 benchmarks, with +114-point improvement over Emu3 on OCRBench
- Mono-InternVL-1.5 achieves similar multimodal performance to InternVL-1.5 while reducing first-token latency by up to 69%
- EViP++ reduces training data by 58% while maintaining competitive performance
- Fused CUDA kernel achieves up to 26% inference speedup through exploiting token-type sparsity

## Why This Works (Mechanism)

### Mechanism 1: Separating visual and textual parameters via mixture-of-experts prevents catastrophic forgetting while enabling visual learning in monolithic MLLMs.
The MMoE architecture uses static hard routing—visual tokens route exclusively to visual FFN experts, textual tokens to text FFN experts. During pre-training stages S1.1 and S1.2, the pre-trained LLM parameters remain frozen; only visual experts and patch embeddings update via delta tuning. In S1.3, attention layers unfreeze for vision-language alignment. Core assumption: Visual and textual modalities require fundamentally different parameter spaces to avoid interference, and freezing language parameters sufficiently preserves capabilities while visual experts learn from scratch. Evidence: [abstract] "embed a new visual parameter space into a pre-trained LLM, enabling stable learning of visual knowledge from noisy data via delta tuning" - [section III.A] Eq. 5 defines static routing: FFNv for visual tokens, FFNt for text tokens - [corpus] BREEN paper corroborates encoder-free approaches require careful data/parameter management but lacks specific MoE-delta tuning comparison. Break condition: If visual and textual tokens require significant cross-modal reasoning during pre-training, static routing may underperform vs. soft routing.

### Mechanism 2: Progressive endogenous visual pre-training (EViP/EViP++) enables data-efficient visual learning by organizing data from noisy/simple to high-quality/complex.
Three-stage curriculum: (1) Concept learning on 922M noisy image-text pairs (S1.1), (2) Semantic learning on 258M synthetic captions from InternVL2-8B (S1.2), (3) Alignment learning on 143M task-specific data (S1.3). EViP++ reduces data by 58% (to ~500M total) by adding visual attention experts that accelerate early-stage learning, allowing higher-quality data to compensate for quantity reduction. Core assumption: Visual concepts can be learned hierarchically (objects → semantics → task alignment), and synthetic captions provide cleaner semantic knowledge than noisy web-scraped pairs. Evidence: [abstract] "EViP++...reduces training data by 58% while maintaining competitive performance" - [section III.B & IV.A] Fig. 3 shows stage-wise data organization; Tab. VI shows delta tuning outperforms full tuning by +16-18% on specific benchmarks - [corpus] Limited direct corpus evidence on progressive visual pre-training; BREEN focuses on learnable queries, not curriculum strategies. Break condition: If task-specific alignment data is insufficient or synthetic captions introduce systematic biases, performance gains from curriculum may plateau or degrade on specialized benchmarks.

### Mechanism 3: Fused CUDA kernels for modality-specific MoE operations achieve up to 2.3x speedup by exploiting token-type sparsity within sequence blocks.
The kernel partitions input sequences into fine-grained blocks. Each block assigns two thread blocks (visual/text). On entry, each thread checks for relevant token presence—if absent, exits immediately. This achieves near-single-branch efficiency when tokens cluster spatially (common in multimodal sequences where images appear contiguously). Core assumption: Visual and textual tokens are spatially clustered in input sequences, making per-block token-type checking efficient; GPU thread block overhead is negligible for early-exit paths. Evidence: [abstract] "includes a fused CUDA kernel to speed up inference by up to 26%" - [section IV.B & Tab. XII] Linear MoE achieves 1.84-2.32x speedup; MLP MoE achieves 1.71-1.82x speedup across sequence lengths 2K-128K - [corpus] "Lifting the Veil on Visual Information Flow" paper analyzes visual token processing but does not address kernel-level optimization—mechanism is novel to this work. Break condition: If sequences have highly interleaved visual-text tokens, per-block sparsity decreases, reducing early-exit benefits and approaching standard PyTorch performance.

## Foundational Learning

### Concept: Mixture-of-Experts (MoE) with Hard Routing
- **Why needed here:** Unlike standard MoE with learned token-to-expert routing, Mono-InternVL uses modality-determined static routing. Engineers must understand this design choice avoids routing instability but sacrifices expert sharing flexibility.
- **Quick check question:** If you observe visual tokens being routed to text experts during inference, what component failed? (Answer: Token type identification in the routing logic or input embedding corruption)

### Concept: Delta Tuning (Parameter-Efficient Fine-Tuning)
- **Why needed here:** The core training strategy freezes most LLM parameters while training visual experts. This differs from LoRA/adapter approaches—here, entirely new expert branches are added, not low-rank updates to existing weights.
- **Quick check question:** During S1.1 pre-training, which parameter groups should show gradient updates? (Answer: Patch embedding MLP, visual FFN experts; NOT attention layers or text FFN experts)

### Concept: Catastrophic Forgetting in Continual Learning
- **Why needed here:** The paper explicitly addresses this failure mode. Engineers should recognize that performance drops on text-only benchmarks indicate partial forgetting, even with delta tuning.
- **Quick check question:** If Mono-InternVL-1.5 shows degraded NLP performance vs. baseline, is this a bug or expected tradeoff? (Answer: Expected within limits; Tab. XI shows 2-4% drops on some NLP tasks—monitor if drops exceed ~5%)

## Architecture Onboarding

### Component map:
Image → PatchEmbed (28×28 patches) + PE → MLP → visual tokens xv ∈ R^(h×w×d) → Token type flags (visual vs. text) → Per-layer MMoE → MMHA (multi-head attention with separate Q/K/V projections per modality) → MMoE FFN (hard router → FFNv visual or FFNt text) → Fused Kernel Block (sequence partitioned into blocks → dual thread blocks → early-exit on absent token types) → Standard LLM next-token prediction

### Critical path:
1. Patch embedding initialization from scratch (no pre-trained visual encoder)
2. Visual expert initialization from text FFN weights (Section III.A: "FFNv is initialized from FFNt")
3. S1.1/S1.2: Freeze LLM, train visual FFN experts only
4. S1.3: Unfreeze attention, train visual attention experts (Mono-InternVL-1.5)
5. S2: Unfreeze all, train on instruction data

### Design tradeoffs:
- **Monolithic vs. Modular:** Eliminates visual encoder latency (~69% TTFT reduction vs. InternVL-1.5) but requires training visual capabilities from scratch
- **Static vs. Learned Routing:** Simplifies deployment and enables fused kernel optimization, but cannot adaptively allocate experts based on token content
- **Data efficiency vs. capacity:** EViP++ reduces data by 58% but adds visual attention experts, increasing parameter count by ~1.2B visual params

### Failure signatures:
- **Token routing errors:** Visual tokens processed by text experts → outputs garbage/English text for image regions
- **Forgetting detection:** Text-only benchmark scores drop >5% from InternLM2 baseline → excessive S1.3/S2 unfreezing
- **Kernel fallback:** CUDA kernel fails to load → automatic fallback to sequential PyTorch MoE (2x slower; check logs for "Fused Kernel" initialization)
- **Patch embedding collapse:** High-res images (>8M pixels) cause OOM or token truncation → dynamic resolution strategy failure

### First 3 experiments:
1. **Ablation: Visual Expert Initialization**
   - Train Mono-InternVL with FFNv initialized randomly vs. from FFNt weights
   - Measure: Convergence speed on S1.1 concept learning, final performance on OCRBench
   - Expected: Pre-trained initialization accelerates early learning (paper claims this enables leveraging LLM knowledge)

2. **Stress Test: Interleaved Multimodal Sequences**
   - Create inputs with alternating single visual/text tokens (e.g., interleaved document pages)
   - Measure: Fused kernel speedup vs. PyTorch baseline
   - Expected: Speedup degrades toward 1.0x as block-level sparsity disappears

3. **Forgetting Threshold Detection**
   - Track NLP benchmark performance (MMLU, MATH) after each pre-training stage
   - Compare: Mono-InternVL (frozen attention in S1.3) vs. Mono-InternVL-1.5 (unfrozen attention experts)
   - Expected: Mono-InternVL-1.5 may show slightly higher forgetting due to attention updates, but compensated by better multimodal performance (Tab. XI: MATH improves 12.3→15.1)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the Mono-InternVL architecture overcome its limitations in high-resolution document understanding (e.g., InfoVQA) by scaling model depth, or does the monolithic structure inherently trade off resolution capability for deployment efficiency?
- **Basis in paper:** [Inferred] The authors note Mono-InternVL is inferior to InternVL-1.5 on InfoVQA by -12.4%, explicitly attributing this to "relatively shallow model depth" limiting visual encoding ability.
- **Why unresolved:** The paper demonstrates success with a 1.8B parameter model but does not explore if increasing depth (parameters) resolves the high-resolution deficiency without negating the efficiency benefits.
- **What evidence would resolve it:** Evaluation of a deeper Mono-InternVL variant (e.g., 7B or 8B parameters) on high-resolution benchmarks like InfoVQA to see if the performance gap closes.

### Open Question 2
- **Question:** How can the delta tuning process be refined to fully prevent the slight degradation in general language capabilities (e.g., MMLU, CMMLU) observed when adding visual attention experts?
- **Basis in paper:** [Inferred] Table XI and the text note that Mono-InternVL-1.5 shows a "slight performance drop on some NLP tasks" (e.g., CMMLU drops from 46.1 to 41.7) compared to the original LLM, despite the use of separate experts.
- **Why unresolved:** While the drop is deemed "acceptable" given the cheap training cost, the mechanism causing this interference in a theoretically isolated expert architecture is not fully solved.
- **What evidence would resolve it:** Identifying a training strategy or regularization term that allows Mono-InternVL to match the base LLM's NLP benchmarks exactly while retaining multimodal performance.

### Open Question 3
- **Question:** Is the "static routing" mechanism (hard-routing visual tokens to visual experts) optimal for complex reasoning tasks involving dense interleaved image-text inputs compared to learned dynamic routing?
- **Basis in paper:** [Inferred] The paper utilizes a static routing strategy where visual/textual tokens are assigned to specific experts deterministically.
- **Why unresolved:** While efficient, static routing may limit the model's flexibility to adaptively use visual or linguistic context for tokens that carry cross-modal semantic weight in advanced reasoning.
- **What evidence would resolve it:** A comparative study between the current static routing and a learned dynamic gating mechanism on interleaved multimodal reasoning benchmarks.

## Limitations

- **Limited out-of-distribution validation:** The paper demonstrates performance on benchmark datasets but provides limited analysis of model behavior on real-world deployment scenarios or out-of-distribution data.
- **Forgetting tradeoffs:** While delta tuning is claimed to prevent catastrophic forgetting, Tab. XI shows 2-4% drops on some NLP tasks (MMLU: 47.1→44.7, CMMLU: 37.3→35.1), indicating partial forgetting that may be problematic for production deployment.
- **Kernel performance assumptions:** The 26% inference speedup from fused CUDA kernels assumes spatially clustered visual-text tokens, but the paper doesn't extensively benchmark realistic multimodal inputs where token interleaving patterns may be more complex.

## Confidence

- **High Confidence:** The core architectural claims (MoE with hard routing, delta tuning strategy, EViP++ curriculum) are well-supported by experimental results showing consistent improvements across 12/15 benchmarks. The 114-point OCRBench improvement and 69% TTFT reduction are specific, measurable outcomes.
- **Medium Confidence:** The data efficiency claims (58% reduction) and kernel speedup claims (up to 26%) are supported by ablation studies and benchmark comparisons, but depend on specific implementation details not fully specified in the paper. The exact impact may vary with different hardware or input patterns.
- **Low Confidence:** Claims about the generality of the progressive curriculum approach beyond the specific datasets used, and the long-term stability of the model's NLP capabilities, lack extensive validation across diverse scenarios and timeframes.

## Next Checks

1. **Out-of-Distribution Robustness Test** - Evaluate Mono-InternVL-1.5 on visual question answering datasets not included in the training mix (e.g., VQA-CP, GQA variations) to assess whether the 58% data reduction in EViP++ creates blind spots in visual reasoning capabilities.

2. **Interleaved Token Performance** - Create benchmark sequences with alternating visual-text tokens at various granularities (single token interleaving up to sentence-level interleaving) to measure actual CUDA kernel speedup degradation and identify performance breakpoints.

3. **Long-term Forgetting Analysis** - Track NLP benchmark performance (MMLU, MATH) at 1K, 10K, and 100K training steps during S1.3 and S2 to quantify forgetting rates and determine whether the current delta tuning approach maintains acceptable text performance throughout extended training.