---
ver: rpa2
title: 'TAO-Net: Two-stage Adaptive OOD Classification Network for Fine-grained Encrypted
  Traffic Classification'
arxiv_id: '2512.15753'
source_url: https://arxiv.org/abs/2512.15753
tags:
- traffic
- classification
- tao-net
- network
- encrypted
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes TAO-Net, a two-stage framework for fine-grained
  encrypted traffic classification that addresses the challenge of Out-of-Distribution
  (OOD) traffic from emerging applications. The method uses a hybrid OOD detection
  mechanism in stage one to distinguish between In-Distribution (ID) and OOD traffic,
  and leverages Large Language Models (LLMs) with a Semantic-enhanced Prompt Strategy
  (SPS) in stage two to classify OOD traffic without relying on predefined labels.
---

# TAO-Net: Two-stage Adaptive OOD Classification Network for Fine-grained Encrypted Traffic Classification

## Quick Facts
- arXiv ID: 2512.15753
- Source URL: https://arxiv.org/abs/2512.15753
- Reference count: 40
- Key outcome: Achieves 96.81-97.70% macro-precision and 96.77-97.68% macro-F1 on encrypted traffic classification, outperforming previous methods (44.73-86.30% macro-precision) by routing OOD traffic through LLM-based classification

## Executive Summary
This paper addresses the challenge of fine-grained encrypted traffic classification when faced with Out-of-Distribution (OOD) traffic from emerging applications. The proposed TAO-Net framework employs a two-stage approach: first distinguishing between In-Distribution (ID) and OOD traffic using hybrid OOD detection, then classifying ID traffic with a transformer and OOD traffic using a Large Language Model with semantic-enhanced prompts. Experiments on three datasets demonstrate significant improvements over existing methods, achieving 96-97% macro-precision compared to 44-86% for prior approaches.

## Method Summary
TAO-Net implements a two-stage adaptive classification framework. Stage 1 uses a hybrid OOD detector that combines transformer inter-layer transformation smoothness with PCA-based feature residual analysis to compute a hybrid score for routing decisions. If the score is below threshold δ=0.75, traffic routes to a transformer classifier trained on ID categories; otherwise, it routes to an LLM with Semantic-enhanced Prompt Strategy (SPS) for OOD classification. The approach leverages GPT-4o with temperature=0.7 and top-p=0.95, using three prompt modes (Strict, Complete, Extended) to constrain generation space. Training uses Adam/AdamW optimizers with learning rates of 2e-5, evaluated on CHNAPP, ISCXVPN, and ISCXTor datasets.

## Key Results
- Achieves 96.81-97.70% macro-precision and 96.77-97.68% macro-F1 across three datasets
- Outperforms previous methods by 50-52 percentage points in macro-precision
- Strict SPS mode provides 3-5% precision improvement over Extended mode
- OOD detection threshold δ=0.75 provides optimal balance between precision and recall

## Why This Works (Mechanism)

### Mechanism 1: Hybrid OOD Detection via Inter-layer Smoothness and PCA Residuals
The approach combines transformer inter-layer transformation smoothness with PCA-based feature residual analysis to improve ID/OOD discrimination. It computes a hybrid score S(X) = α||P_R·φ(X)||₂ + (1-α)Σ||F_l(X) - F_{l-1}(X)||₂, where the first term captures projection distance onto the residual subspace and the second term measures smooth feature transitions across network layers. ID data should cluster in the principal subspace with smooth inter-layer transitions, while OOD data violates both assumptions.

### Mechanism 2: Two-Stage Adaptive Routing Separates ID and OOD Processing
The framework routes ID and OOD traffic through specialized classifiers (transformer vs. LLM) to prevent error propagation from single-model handling of both distributions. Stage 1 computes the hybrid score and compares to threshold δ. If S(X) ≤ δ, traffic routes to a transformer classifier trained only on ID categories; if S(X) > δ, traffic routes to an LLM with SPS that generates labels without predefined constraints, preventing forced categorization that corrupts ID decision boundaries.

### Mechanism 3: Semantic-Enhanced Prompt Strategy Constrains LLM Generation Space
SPS restricts the LLM's generation space via structured prompts to improve classification precision for OOD traffic while maintaining flexibility. The approach provides three modes—Strict (constrains to specific candidate labels), Complete (includes all dataset-specific categories), Extended (cross-dataset knowledge). Strict mode constrains output space most tightly, reducing hallucination and ambiguity by encoding domain knowledge (protocol behavior, packet structure) to ground generation.

## Foundational Learning

- **Concept: Out-of-Distribution (OOD) Detection**
  - Why needed here: The entire TAO-Net architecture hinges on correctly identifying when test traffic deviates from training distribution to route traffic appropriately
  - Quick check question: Given a feature space where ID data has covariance matrix C with eigenvalues [10, 5, 0.1, 0.05, 0.01], which principal components would you retain to capture 90% variance, and which define the residual subspace for OOD scoring?

- **Concept: Transformer Attention for Sequential Data**
  - Why needed here: The ID classifier uses transformer self-attention to capture dependencies across packet tokens, essential for understanding classification failures
  - Quick check question: If attention weights are uniformly distributed across all token positions rather than concentrated, what might this indicate about the model's ability to discriminate traffic patterns?

- **Concept: LLM Conditional Generation**
  - Why needed here: OOD classification reformulates the problem as P(ŷ|X) = Π P(ŷ_t|X, ŷ_{<t}), generating labels token-by-token, critical for understanding prompt effectiveness
  - Quick check question: If temperature=0.7 and top-p=0.95, what happens to the output distribution compared to greedy decoding (temperature=0)?

## Architecture Onboarding

- **Component map:** Input Traffic X → LSTM Feature Extractor → PCA Projector → Inter-layer Smoothness Module → Hybrid Score → [Transformer Classifier if S ≤ δ] OR [LLM + SPS if S > δ]
- **Critical path:** The OOD detection threshold δ (set to 0.75) is the single most sensitive parameter; incorrect δ causes cascading errors in downstream classification
- **Design tradeoffs:**
  - Strict vs. Extended SPS modes: Strict yields +3-5% precision but fails on truly novel applications; Extended is more flexible but noisier
  - α = 0.6 weighting: Favors PCA residuals slightly over smoothness; adjust if traffic has high feature variance
  - GPT-4o vs. local LLM: Paper uses GPT-4o (cloud API); substitute with LLaMA or ChatGLM for air-gapped networks (15-30% precision drop)
- **Failure signatures:**
  1. High false positive rate in Stage 1: Legitimate new app versions flagged as OOD → over-reliance on LLM → inconsistent labeling
  2. Confusion between similar OOD apps: YouTube vs. Vimeo suggests feature overlap in streaming services
  3. LLM hallucination: Generated labels not matching ground truth when Extended mode allows too many candidates
- **First 3 experiments:**
  1. Baseline validation: Replicate PacRep and ET-BERT scores on target dataset to establish OOD detection need
  2. Threshold sensitivity: Sweep δ ∈ [0.5, 1.0] and plot precision-recall for Stage 1 detection
  3. SPS mode comparison: Run Strict, Complete, Extended on held-out OOD set; if Extended underperforms by >5%, consider domain-specific prompt engineering

## Open Questions the Paper Calls Out

### Open Question 1
How can an adaptive mechanism be developed to dynamically select the optimal Semantic-enhanced Prompt Strategy (SPS) mode based on real-time traffic characteristics? The conclusion states future work will focus on developing an adaptive SPS mechanism. The current implementation requires manual selection among three modes with different precision-flexibility tradeoffs. Evidence would be a feedback loop or classifier that autonomously switches prompt modes based on input traffic uncertainty or entropy.

### Open Question 2
What methodologies can effectively optimize the OOD detection threshold (δ) to maintain robustness across highly variable network traffic distributions? The authors explicitly identify optimizing the OOD detection threshold for different traffic scenarios as future work. The current study uses a fixed threshold (δ=0.75) that may not generalize well without manual tuning. Evidence would be an adaptive threshold algorithm maintaining stable F1 scores across datasets with different class balances or noise levels.

### Open Question 3
Can the framework achieve real-time or near-real-time classification throughput given the high computational latency of Large Language Models (LLMs) like GPT-4o? The paper utilizes GPT-4o for OOD classification but evaluates only accuracy metrics, omitting inference time analysis critical for network deployment. LLMs typically introduce significant latency (hundreds of milliseconds to seconds), potentially prohibitive for high-speed network monitoring. Evidence would be latency benchmarks under high load or model distillation/quantization techniques to reduce inference time.

## Limitations
- The critical threshold δ=0.75 is set empirically without sensitivity analysis shown for different datasets
- The LLM-based OOD classification relies on GPT-4o API, making the approach expensive and unsuitable for air-gapped networks
- Limited evaluation on truly novel applications—OOD samples are still from predefined application families

## Confidence
- **High**: Stage 1 hybrid OOD detection combining PCA residuals and inter-layer smoothness (supported by explicit equations and ablation)
- **Medium**: SPS performance gains (Strict vs Extended modes show consistent improvement, but prompt templates are not fully specified)
- **Low**: Generalization to zero-day applications (OOD detection works for known families but may fail on structurally different traffic)

## Next Checks
1. **Threshold sensitivity test**: Sweep δ across [0.5, 1.0] on validation sets to verify 0.75 is optimal for each dataset
2. **Cross-dataset consistency**: Train on CHNAPP, test on ISCXVPN/ISCXTor to evaluate robustness to different traffic characteristics
3. **Local LLM substitution**: Replace GPT-4o with LLaMA-2-7B and measure performance degradation to assess dependency on commercial API