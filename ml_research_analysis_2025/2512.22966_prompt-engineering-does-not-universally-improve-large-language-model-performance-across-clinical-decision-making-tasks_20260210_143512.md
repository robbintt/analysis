---
ver: rpa2
title: Prompt engineering does not universally improve Large Language Model performance
  across clinical decision-making tasks
arxiv_id: '2512.22966'
source_url: https://arxiv.org/abs/2512.22966
tags:
- clinical
- performance
- accuracy
- reasoning
- were
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study systematically evaluated three leading Large Language\
  \ Models\u2014ChatGPT-4o, Gemini 1.5 Pro, and Llama 3.3 70B\u2014across five sequential\
  \ clinical decision-making tasks using real-world patient case studies. The models\
  \ showed strong performance on final diagnosis but poor results on relevant diagnostic\
  \ testing, with accuracy varying significantly by task and model."
---

# Prompt engineering does not universally improve Large Language Model performance across clinical decision-making tasks

## Quick Facts
- arXiv ID: 2512.22966
- Source URL: https://arxiv.org/abs/2512.22966
- Authors: Mengdi Chai; Ali R. Zomorrodi
- Reference count: 0
- Primary result: Prompt engineering improves performance on most challenging clinical task but degrades results on others

## Executive Summary
This study systematically evaluated three leading Large Language Models—ChatGPT-4o, Gemini 1.5 Pro, and Llama 3.3 70B—across five sequential clinical decision-making tasks using real-world patient case studies. The models showed strong performance on final diagnosis but poor results on relevant diagnostic testing, with accuracy varying significantly by task and model. While prompt engineering using the MedPrompt framework improved performance on the most challenging task (relevant diagnostic testing), it often degraded results on others, demonstrating that advanced prompting is not universally beneficial. Targeted dynamic few-shot learning did not consistently outperform random selection, suggesting that tightly matched examples may limit contextual diversity.

## Method Summary
The study used 36 clinical cases from MSD Manual Professional Version, evaluating three LLMs across five sequential clinical tasks: differential diagnosis, essential immediate steps, relevant diagnostic testing, final diagnosis, and treatment recommendation. Baseline prompting was compared against MedPrompt variants including self-generated chain-of-thought reasoning and dynamic few-shot learning (both KNN and random selection). Each question was run three times per condition, with accuracy measured as overlap between model selections and ground truth. Statistical comparisons used paired Mann-Whitney U tests with p<0.05 significance threshold.

## Key Results
- Models achieved 96-98% accuracy on final diagnosis but only 52-78% on relevant diagnostic testing
- MedPrompt significantly improved performance on diagnostic testing (lowest baseline accuracy) but degraded results on other tasks
- KNN-based few-shot selection did not consistently outperform random selection
- Task-dependent and model-specific effects observed across all prompt engineering interventions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prompt engineering provides cognitive scaffolding for high-uncertainty tasks where baseline model capabilities are weakest.
- Mechanism: When models lack robust internal strategies for tasks requiring reasoning under uncertainty, structured prompting (CoT + few-shot examples) supplies explicit reasoning pathways that compensate for the deficiency. The framework decomposes complex decisions into intermediate steps the model can follow.
- Core assumption: The gap between knowledge encoding and practical application manifests most strongly when models must weigh multiple possibilities without definitive constraints.
- Evidence anchors:
  - [abstract]: "While it significantly improved the performance on the task with lowest baseline accuracy (relevant diagnostic testing), it was counterproductive for others."
  - [section Page 13]: "Prompt engineering provided substantial benefits for the single most challenging task: relevant diagnostic testing... the prompting frameworks acted as a form of 'cognitive scaffolding', guiding the models through a reasoning process where their baseline capabilities were weakest."
  - [corpus]: Related work on instruction tuning and CoT for medical QA (arxiv 2506.12182) supports conditional benefits of structured prompting in domain-specific contexts, though evidence for generalization remains mixed.
- Break condition: When baseline accuracy is already high (e.g., final diagnosis at 96-98%), scaffolding provides no marginal benefit and may introduce unnecessary constraints.

### Mechanism 2
- Claim: Structured prompting can degrade performance through cognitive forcing, overriding effective pre-trained heuristics.
- Mechanism: Chain-of-thought reasoning and few-shot scaffolds impose rigid reasoning paths that may conflict with models' learned inference patterns. The model mimics the prompted reasoning structure rather than engaging its natural problem-solving capabilities.
- Core assumption: Models develop implicit reasoning heuristics during pre-training that can be disrupted by explicit structural constraints.
- Evidence anchors:
  - [abstract]: "Prompt engineering using the MedPrompt framework improved performance on the most challenging task (relevant diagnostic testing), it often degraded results on others."
  - [section Page 13]: "This suggests a phenomenon of 'cognitive forcing' effect, where the structured, step-by-step reasoning imposed by the MedPrompt framework may interfere with and override a model's effective, pre-trained heuristics."
  - [corpus]: "Prompting Science Report 1: Prompt Engineering is Complicated and Contingent" (arxiv 2503.04818) explicitly notes no universal standard exists for prompt effectiveness—reinforcing contingency.
- Break condition: When model-task pairing already exhibits strong natural alignment, adding structural constraints is more likely to hurt than help.

### Mechanism 3
- Claim: Semantic similarity-based example selection does not consistently outperform random selection due to a diversity-precision trade-off.
- Mechanism: KNN-based few-shot selection prioritizes closely matched examples, which can narrow the reasoning context and eliminate alternative solution pathways. Random selection preserves contextual diversity that may surface useful analogical reasoning strategies.
- Core assumption: Diverse examples encode a broader range of reasoning patterns that provide coverage across edge cases.
- Evidence anchors:
  - [abstract]: "Targeted dynamic few-shot learning did not consistently outperform random selection, suggesting that tightly matched examples may limit contextual diversity."
  - [section Page 13-14]: "The reason for this unexpected observation could be that the presumed advantage of closely matched examples can be offset in some cases by the loss of broader contextual diversity or alternative reasoning strategies."
  - [corpus]: Limited direct corpus evidence on this specific mechanism; assumption remains plausible but under-tested in clinical domains.
- Break condition: When task requires highly specialized reasoning patterns not well-represented in random samples, targeted selection may regain advantage.

## Foundational Learning

- Concept: Chain-of-Thought (CoT) Reasoning
  - Why needed here: MedPrompt relies on self-generated CoT to create training examples; understanding how CoT structures inference is essential for diagnosing why it helps in some tasks and hurts in others.
  - Quick check question: Given a clinical vignette, can you distinguish between a CoT explanation that reveals genuine reasoning versus one that post-hoc rationalizes a memorized answer?

- Concept: Dynamic Few-Shot Learning (KNN vs Random)
  - Why needed here: The study explicitly compares KNN-based semantic matching against random selection for example retrieval; this choice determines the diversity-precision trade-off in prompting.
  - Quick check question: For a diagnostic testing question about cardiac chest pain, would you expect semantically similar examples (other cardiac cases) or diverse examples (mixed specialty cases) to yield better generalization—why?

- Concept: Temperature Settings and Output Variance
  - Why needed here: The study evaluates default vs zero temperature across tasks; while differences were not statistically significant, the rationale relates to determinism vs adaptability in clinical reasoning.
  - Quick check question: In a high-stakes diagnostic setting, would you prioritize reproducibility (temperature=0) or exploratory breadth (temperature>0)—what trade-offs does each introduce?

## Architecture Onboarding

- Component map:
  - Input Layer: HPI + ROS + PE + cumulative ground truth from prior tasks (sequential context accumulation)
  - Prompting Layer: Baseline (no engineering) OR MedPrompt variants (CoT + KNN/random few-shot + choice shuffling ensemble)
  - Model Layer: ChatGPT-4o, Gemini 1.5 Pro, or Llama 3.3 70B (accessed via respective APIs)
  - Evaluation Layer: Overlap-based accuracy for multi-select tasks; binary accuracy for final diagnosis
  - Training Set Construction: 30% allocation where self-generated CoT matches ground truth; synthetic augmentation if insufficient

- Critical path:
  1. Extract clinical vignettes → structure into sequential task prompts
  2. Run baseline evaluation (default and zero temperature)
  3. Generate CoT explanations for training cases
  4. Construct few-shot examples (KNN or random selection)
  5. Apply MedPrompt with choice shuffling ensemble (3 variants)
  6. Aggregate accuracy across 3 trials per case

- Design tradeoffs:
  - KNN few-shot: Higher semantic precision vs potential loss of reasoning diversity
  - Random few-shot: Higher diversity vs risk of irrelevant examples
  - Zero temperature: Higher reproducibility vs potential conservative bias
  - Ensemble size: More shuffles increase robustness but add API cost

- Failure signatures:
  - Accuracy drops after prompt engineering (observed for ChatGPT-4o on treatment recommendation: 71% → 50.2%)
  - Near-identical performance across temperature settings despite task complexity differences
  - High variance across clinical vignettes within same task category (indicating task-specific but not case-general learning)

- First 3 experiments:
  1. Replicate baseline evaluation on a held-out vignette to validate model behavior under your API configuration and temperature settings.
  2. Ablate CoT alone (without few-shot examples) to isolate the contribution of reasoning scaffolding from example selection.
  3. Compare K=3 vs K=5 few-shot examples on the relevant diagnostic testing task to test whether example quantity modifies the diversity-precision trade-off.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What mechanisms explain why targeted KNN-based dynamic few-shot selection fails to consistently outperform random example selection in clinical reasoning tasks?
- Basis in paper: [explicit] The authors found that "targeted dynamic few-shot prompting did not consistently outperform random selection, indicating that the presumed benefits of closely matched examples may be counterbalanced by loss of broader contextual diversity."
- Why unresolved: The paper identifies the phenomenon but does not experimentally disentangle semantic similarity from diversity effects, nor test alternative selection strategies.
- What evidence would resolve it: Ablation studies varying example diversity while controlling for similarity, comparing multiple embedding strategies and selection algorithms across tasks.

### Open Question 2
- Question: How do findings generalize to larger, messier real-world clinical datasets with incomplete or contradictory information?
- Basis in paper: [explicit] The authors note: "Future research can expand on this work by incorporating larger-scale and more diverse datasets to better reﬂect real-world clinical complexity" and that "these vignettes do not fully represent the diversity, complexity, ambiguity, and noise of real-world clinical practice."
- Why unresolved: The study used only 36 curated educational vignettes from the MSD Manual, limiting external validity.
- What evidence would resolve it: Replication across electronic health record-derived cases, multi-institution datasets, and cases with missing or conflicting data.

### Open Question 3
- Question: Does integrating human-in-the-loop feedback or RLHF improve safety and adaptability of LLMs in sequential clinical decision-making?
- Basis in paper: [explicit] The authors state: "integrating human-in-the-loop reﬁnement and reinforcement learning from human feedback (RLHF) are essential for building clinician trust and ensuring safe implementation of LLMs in healthcare."
- Why unresolved: The study relied on static LLM-generated chain-of-thought examples without real-time clinician feedback or reinforcement mechanisms.
- What evidence would resolve it: Comparative studies of static prompting versus iterative RLHF or clinician-in-the-loop refinement on the same clinical tasks.

### Open Question 4
- Question: What qualitative aspects of clinical reasoning—such as coherence, safety, or appropriateness—are missed by quantitative overlap metrics?
- Basis in paper: [inferred] The authors acknowledge their "quantitative approach, while objective, does not capture the qualitative aspects of the models' outputs, such as the coherence, safety, or clinical appropriateness of their underlying reasoning."
- Why unresolved: No qualitative or clinician-rated evaluation was performed; only answer overlap was measured.
- What evidence would resolve it: Expert clinician review of reasoning traces, safety assessments, and structured qualitative scoring rubrics alongside quantitative metrics.

## Limitations

- Study based on only 36 curated clinical cases from a single source (MSD Manual), limiting external validity
- Evaluation focuses exclusively on multiple-choice question answering rather than open-ended clinical reasoning
- Quantitative metrics do not capture qualitative aspects of reasoning such as coherence, safety, or clinical appropriateness

## Confidence

- High confidence: Core finding that prompt engineering effects are task-dependent and model-specific
- Medium confidence: Proposed mechanisms (cognitive scaffolding vs. cognitive forcing) based on observed patterns
- Low confidence: Generalizability of diversity-precision trade-off finding for few-shot selection

## Next Checks

1. Replicate the study with a larger, more diverse clinical case corpus (e.g., MIMIC-IV, clinical vignettes from multiple sources) to test robustness across case distributions.
2. Conduct ablation studies isolating CoT effects from few-shot examples to directly test the cognitive scaffolding hypothesis.
3. Systematically vary K values for few-shot selection and embedding similarity thresholds to characterize the diversity-precision trade-off more precisely.