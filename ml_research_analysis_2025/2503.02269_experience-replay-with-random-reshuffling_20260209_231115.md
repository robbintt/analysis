---
ver: rpa2
title: Experience Replay with Random Reshuffling
arxiv_id: '2503.02269'
source_url: https://arxiv.org/abs/2503.02269
tags:
- replay
- experience
- sampling
- buffer
- transitions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper bridges supervised learning and reinforcement learning
  by extending random reshuffling (RR) to experience replay, addressing the gap where
  standard replay samples with replacement while supervised learning uses epoch-wise
  without-replacement sampling for better convergence. Two methods are proposed: RR-C
  applies RR to the indices of a circular buffer in uniform experience replay, ensuring
  each transition is sampled exactly once per epoch, and RR-M adapts RR to prioritized
  experience replay by masking oversampled transitions based on actual versus expected
  sample counts.'
---

# Experience Replay with Random Reshuffling
## Quick Facts
- arXiv ID: 2503.02269
- Source URL: https://arxiv.org/abs/2503.02269
- Authors: Yasuhiro Fujita
- Reference count: 40
- Primary result: Bridges supervised learning's random reshuffling to experience replay, improving convergence through reduced variance in sample counts

## Executive Summary
This paper addresses a fundamental gap between supervised learning and reinforcement learning by extending random reshuffling (RR) to experience replay mechanisms. While supervised learning uses epoch-wise without-replacement sampling for better convergence, standard experience replay samples with replacement, leading to higher variance in sample frequencies. The work introduces two methods that adapt RR to both uniform and prioritized experience replay, providing theoretical guarantees on variance reduction and empirical improvements on Atari benchmarks.

## Method Summary
The paper proposes two approaches to implement random reshuffling in experience replay. RR-C applies RR directly to indices of a circular buffer in uniform experience replay, ensuring each transition is sampled exactly once per epoch. RR-M adapts RR to prioritized experience replay by maintaining sample count statistics and masking oversampled transitions based on the difference between actual and expected sample counts. Theoretical analysis proves both methods reduce variance in sample frequencies, with RR-C achieving unbiased sampling and RR-M converging to expected frequencies asymptotically. The implementations are straightforward and can replace standard sampling mechanisms in existing RL codebases.

## Key Results
- RR-C reduces sample count variance to zero, achieving unbiased sampling across epochs
- Both methods demonstrate variance reduction without significant bias in simulations
- RR-C improves performance over with-replacement sampling in C51 and DQN across most Atari games
- RR-M provides modest gains in DDQN+LAP on Atari benchmarks

## Why This Works (Mechanism)
The core mechanism leverages the statistical properties of without-replacement sampling to reduce variance in transition sampling frequencies. In standard experience replay, sampling with replacement creates high variance in how often each transition is selected within an epoch. By ensuring each transition is sampled exactly once per epoch (RR-C) or by masking oversampled transitions (RR-M), the methods reduce this variance while maintaining the benefits of experience replay. The theoretical analysis shows that variance reduction directly contributes to more stable learning and better convergence properties.

## Foundational Learning
- **Experience Replay**: Stores past transitions for off-policy learning; needed because RL agents learn from limited interactions and benefit from revisiting past experiences
- **Random Reshuffling**: Epoch-wise without-replacement sampling; needed for variance reduction compared to sampling with replacement
- **Prioritized Experience Replay**: Samples transitions based on TD error; needed to focus learning on more informative experiences
- **Variance in Sample Counts**: Measures how much sampling deviates from uniform frequency; needed to understand convergence properties
- **Masking Mechanism**: Hides oversampled transitions in RR-M; needed to adapt RR to prioritized sampling where some transitions are inherently more important
- **Circular Buffer**: Fixed-size storage for transitions; needed to implement epoch-based sampling in RR-C

## Architecture Onboarding
Component map: Replay Buffer -> Sampling Mechanism -> Masking Layer (RR-M only) -> Agent Training Loop
Critical path: New transition arrives → Buffer stores it → Sampling mechanism selects batch → Masking applied (RR-M) → Agent updates policy/value
Design tradeoffs: RR-C requires buffer size to be multiple of epoch length vs. simpler implementation; RR-M adds computational overhead for tracking sample counts vs. maintaining prioritized sampling benefits
Failure signatures: High variance in sample counts indicates RR not working; biased sampling indicates masking threshold too aggressive
First experiments: 1) Verify uniform sampling counts per epoch with RR-C on small buffer, 2) Test masking effectiveness in RR-M by comparing actual vs expected sample frequencies, 3) Measure wall-clock overhead of sample count tracking in RR-M

## Open Questions the Paper Calls Out
None specified in the source material.

## Limitations
- Performance improvements may not transfer to more complex off-policy algorithms beyond C51 and DQN
- Computational overhead of maintaining sample count statistics could be significant for large replay buffers
- Implementation complexity in distributed training settings where experience tuples are processed asynchronously
- Buffer size constraints in RR-C may limit flexibility in some applications

## Confidence
- Variance reduction properties: High confidence based on theoretical analysis and simulations
- Performance improvements on Atari: Medium confidence due to limited algorithm scope
- Generalization to other RL domains: Low-Medium confidence pending broader empirical validation
- Implementation overhead: Low-Medium confidence without comprehensive benchmarking

## Next Checks
1. Evaluate RR-C on continuous control tasks (e.g., DDPG, TD3) to assess performance beyond discrete action spaces
2. Test RR-M with modern prioritized methods (e.g., proportional, rank-based) and compare against recent advances in experience replay efficiency
3. Measure wall-clock time overhead and memory usage for both methods in large-scale distributed RL training scenarios