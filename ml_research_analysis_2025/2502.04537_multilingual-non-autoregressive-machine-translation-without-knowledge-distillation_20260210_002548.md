---
ver: rpa2
title: Multilingual Non-Autoregressive Machine Translation without Knowledge Distillation
arxiv_id: '2502.04537'
source_url: https://arxiv.org/abs/2502.04537
tags:
- translation
- m-dat
- m-at
- machine
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of non-autoregressive multilingual
  machine translation, aiming to improve both efficiency and translation quality without
  relying on knowledge distillation. The proposed method, M-DAT, leverages a directed
  acyclic Transformer architecture that generates multiple translation fragments in
  parallel and selects them using predicted linkages, eliminating the need for knowledge
  distillation.
---

# Multilingual Non-Autoregressive Machine Translation without Knowledge Distillation

## Quick Facts
- arXiv ID: 2502.04537
- Source URL: https://arxiv.org/abs/2502.04537
- Authors: Chenyang Huang, Fei Huang, Zaixiang Zheng, Osmar R. ZaÃ¯ane, Hao Zhou, Lili Mou
- Reference count: 23
- One-line primary result: M-DAT achieves state-of-the-art performance in non-autoregressive multilingual translation, outperforming previous methods in both supervised and zero-shot settings

## Executive Summary
This paper introduces M-DAT (Multilingual Directed Acyclic Transformer), a novel approach to non-autoregressive multilingual machine translation that eliminates the need for knowledge distillation. The method employs a directed acyclic Transformer architecture that generates multiple translation fragments in parallel and selects them using predicted linkages. Additionally, a pivot back-translation approach is introduced to enhance generalization to unseen translation directions. Experiments demonstrate that M-DAT achieves superior performance compared to previous state-of-the-art methods in both supervised and zero-shot multilingual translation tasks.

## Method Summary
M-DAT leverages a directed acyclic Transformer architecture that generates multiple translation fragments in parallel, which are then selected using predicted linkages. This eliminates the dependency on knowledge distillation typically required in non-autoregressive models. The method also introduces pivot back-translation to improve zero-shot translation performance by using intermediate languages to generate pseudo-parallel data for unseen language pairs. The architecture processes multiple translation fragments simultaneously while maintaining translation quality through an intelligent selection mechanism based on predicted linkages between fragments.

## Key Results
- M-DAT achieves 0.4 higher BLEU scores than previous state-of-the-art in supervised translation
- M-DAT outperforms strong autoregressive baselines in zero-shot translation settings
- The method eliminates the need for knowledge distillation while maintaining high translation quality

## Why This Works (Mechanism)
The directed acyclic Transformer architecture enables parallel generation of multiple translation fragments, which are then intelligently selected using predicted linkages. This approach overcomes the conditional independence assumption problem in standard non-autoregressive models. The pivot back-translation mechanism leverages intermediate languages to create pseudo-parallel data for unseen translation directions, effectively expanding the model's capability beyond its direct training pairs. By combining these two innovations, M-DAT achieves both efficiency and quality improvements without relying on knowledge distillation.

## Foundational Learning

**Non-autoregressive translation**: Why needed - to improve inference speed by generating all tokens in parallel rather than sequentially; Quick check - compare latency between autoregressive and non-autoregressive models

**Knowledge distillation**: Why needed - typically used to improve non-autoregressive translation quality by transferring knowledge from autoregressive teacher models; Quick check - verify whether distillation is actually required for comparable performance

**Multilingual translation**: Why needed - to handle multiple language pairs with a single model for efficiency and cross-lingual transfer; Quick check - measure performance across different language families

**Pivot back-translation**: Why needed - to generate training data for unseen language pairs by translating through intermediate languages; Quick check - evaluate quality of pivot-generated translations

**Directed acyclic architectures**: Why needed - to enable parallel processing while maintaining dependencies between translation fragments; Quick check - analyze how fragment selection affects final output quality

## Architecture Onboarding

**Component map**: Input -> Encoder -> Fragment Generator -> Link Predictor -> Fragment Selector -> Output

**Critical path**: The model generates multiple translation fragments in parallel (Fragment Generator), predicts linkages between them (Link Predictor), and selects the optimal combination (Fragment Selector) to produce the final translation.

**Design tradeoffs**: Parallel generation enables faster inference but requires sophisticated fragment selection mechanisms; eliminating knowledge distillation reduces training complexity but demands architectural innovations to maintain quality.

**Failure signatures**: Poor fragment selection leading to incoherent translations; degraded performance on rare language pairs; increased computational overhead from generating multiple fragments.

**First experiments**:
1. Compare BLEU scores with and without the directed acyclic architecture on a single language pair
2. Test fragment selection quality by evaluating translations using different numbers of generated fragments
3. Measure zero-shot performance improvement when adding pivot back-translation to the base model

## Open Questions the Paper Calls Out
None

## Limitations
- Experimental evaluation limited to specific datasets (WMT14 En-De, WMT16 Ro-En, OPUS-100, TED Talks) and language pairs
- Pivot back-translation introduces additional computational complexity without detailed efficiency analysis
- Directed acyclic architecture scalability for extremely large-scale multilingual systems not characterized
- Evaluation focuses primarily on BLEU scores without extensive linguistic quality analysis

## Confidence

**High confidence**: The architectural innovation of M-DAT and its ability to eliminate knowledge distillation in MNMT

**Medium confidence**: The claimed improvements over state-of-the-art baselines, particularly in zero-shot settings

**Medium confidence**: The effectiveness of the pivot back-translation approach for unseen language pairs

## Next Checks

1. Test M-DAT on additional language pairs from diverse language families (e.g., Asian, African languages) and low-resource settings to assess robustness

2. Conduct human evaluation studies to verify that BLEU improvements correspond to meaningful translation quality gains across linguistic phenomena

3. Perform ablation studies to quantify the relative contributions of the directed acyclic architecture versus pivot back-translation to overall performance gains