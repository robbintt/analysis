---
ver: rpa2
title: 'BladeSDF : Unconditional and Conditional Generative Modeling of Representative
  Blade Geometries Using Signed Distance Functions'
arxiv_id: '2601.13445'
source_url: https://arxiv.org/abs/2601.13445
tags:
- latent
- training
- design
- blade
- distance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a DeepSDF-based generative framework for turbine
  blade geometries, addressing the need for performance-aware modeling with quantified
  reconstruction accuracy. The method learns a continuous signed distance function
  (SDF) representation, establishing an interpretable latent space aligned with blade-relevant
  parameters (taper and chord ratios) and enabling both unconditional synthesis and
  conditional generation from engineering descriptors like maximum directional strains.
---

# BladeSDF : Unconditional and Conditional Generative Modeling of Representative Blade Geometries Using Signed Distance Functions

## Quick Facts
- arXiv ID: 2601.13445
- Source URL: https://arxiv.org/abs/2601.13445
- Reference count: 40
- Primary result: DeepSDF-based generative framework achieving sub-percent reconstruction accuracy for turbine blade geometries with interpretable latent space aligned to engineering parameters

## Executive Summary
BladeSDF presents a DeepSDF-based generative modeling framework for turbine blade geometries that addresses the need for performance-aware modeling with quantified reconstruction accuracy. The method learns a continuous signed distance function (SDF) representation from synthetic point clouds, establishing an interpretable latent space aligned with blade-relevant parameters (taper and chord ratios) and enabling both unconditional synthesis and conditional generation from engineering descriptors like maximum directional strains. The framework achieves high reconstruction fidelity with surface distance errors concentrated within 1% of the maximum blade dimension, and demonstrates robust generalization to unseen designs. By integrating constraints, objectives, and performance metrics, this approach advances beyond traditional 2D-guided or unconstrained 3D pipelines, offering a practical and interpretable solution for data-driven turbine blade modeling and concept generation.

## Method Summary
BladeSDF uses an auto-decoder DeepSDF formulation to learn a continuous SDF representation from synthetic turbine blade point clouds. The method normalizes geometries to [-1,1]³, computes truncated SDF labels using a convex-hull sign proxy and KD-tree distance, and samples 20,000 points per blade (50% near-surface band |s|≤0.1, 50% uniform). Joint optimization of decoder weights and latent codes with L2 regularization produces an interpretable latent space. A neural network maps strain triplets to latent codes for conditional generation, while Gaussian sampling and interpolation enable unconditional synthesis. Reconstruction quality is evaluated using surface distance metrics targeting <1% of maximum blade dimension.

## Key Results
- Reconstruction fidelity achieves surface distance errors within 1% of maximum blade dimension
- Latent space shows strong correlation with physical parameters (PC1 with taper ratio K₁, PC2 with chord ratio K₃)
- Conditional generation from strain triplets achieves NRMSE ~4% on training and 2-7% on test data
- Framework demonstrates robust generalization across 300 held-out test designs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Truncated SDF supervision with near-surface biased sampling yields smooth, watertight blade reconstructions with sub-percent accuracy.
- **Mechanism:** The clamped loss `clip(s*, -δ, δ)` bounds residuals, preventing distant points from dominating gradients. Combined with 50% sampling in the narrow band `|s*| ≤ δ = 0.1`, supervision focuses on surface geometry where it matters for manufacturing. The convex-hull sign assignment + KD-tree distance provides stable inside/outside labels even for sparse point clouds.
- **Core assumption:** Blades are near-convex such that convex-hull signs mildly over-approximate but don't flip critical regions; point clouds contain sufficient near-surface samples.
- **Evidence anchors:** Surface distance errors within 1% of maximum blade dimension; clamped loss and near-surface sampling strategy described in methodology.

### Mechanism 2
- **Claim:** The auto-decoder formulation with L2 latent prior produces an approximately Gaussian, interpretable latent manifold aligned with blade parameters.
- **Mechanism:** Joint optimization of decoder θ and latent codes {z_i} with regularization `λ_z ||z_i||²` keeps codes near a zero-mean distribution. The 256-D latent dimension + 8-layer MLP decoder provides sufficient capacity without overfitting. PCA reveals PC1 correlates with taper ratio K₁ and PC2 with chord ratio K₃.
- **Core assumption:** The blade design space is smooth and low-dimensional enough that a shared decoder can interpolate between all designs.
- **Evidence anchors:** Latent codes kept near zero-mean Gaussian; PC1 strongly correlated with taper ratio K₁ and PC2 aligned with chord ratio K₃.

### Mechanism 3
- **Claim:** A compact neural map from strain triplets to latent codes enables performance-conditional synthesis with sub-percent reconstruction degradation.
- **Mechanism:** The NN g_φ: R³ → R²⁵⁶ learns an inverse mapping from engineering descriptors (ε_x, ε_y, ε_z) to latent codes. By training on optimized latents from the DeepSDF model, the map learns to navigate to regions of the manifold associated with target mechanical responses.
- **Core assumption:** Strain triplets contain sufficient information to disambiguate blade geometries; the mapping is approximately single-valued.
- **Evidence anchors:** NRMSE decreases to ~4% by ≈40,000 epochs; surface-distance comparisons typically within 1%.

## Foundational Learning

- **Concept: Signed Distance Functions (SDFs)**
  - **Why needed here:** The entire representation is built on SDFs; understanding that s(x) < 0 inside, s(x) > 0 outside, and |s(x)| gives distance to surface is prerequisite for interpreting loss functions and sampling strategies.
  - **Quick check question:** Given a point at coordinates (0.5, 0.5, 0.5) and SDF value s = -0.03, is this point inside or outside the blade, and how far from the surface?

- **Concept: Auto-decoder vs Auto-encoder**
  - **Why needed here:** BladeSDF uses auto-decoder (no encoder; latents optimized jointly with decoder), which differs from standard auto-encoders. This affects training (iterative latent optimization per design) and inference (test-time code fitting).
  - **Quick check question:** In auto-decoder inference, what is optimized and what is frozen? Why does this lead to broader error distributions on test data?

- **Concept: Latent Priors and Regularization**
  - **Why needed here:** The L2 prior `λ_z ||z||²` is what makes the latent space approximately Gaussian and enables simple Gaussian sampling for generation. Without it, codes could drift arbitrarily.
  - **Quick check question:** If λ_z were set to 0, what would happen to the latent code distribution during training? What generative sampling strategies would become unreliable?

## Architecture Onboarding

- **Component map:** Point Cloud → [Convex Hull + KD-Tree] → Ground Truth SDF Samples → [Joint Optimization] → Latent Code z_i → Decoder Weights θ → SDF Field → [Marching Cubes] → Mesh

- **Critical path:**
  1. Data preparation: Normalize to [-1,1]³, compute truncated SDF labels (δ=0.1, 50% near-surface + 50% uniform)
  2. Auto-decoder training: 222 designs × 20K samples each, ~100-120 epochs to convergence
  3. Latent space validation: Check Gaussian marginals (should be ~N(0, 0.1)), verify PC axes align with known parameters
  4. Conditional map training: Train g_φ on {(ε_i, z_i)} pairs; target <5% NRMSE

- **Design tradeoffs:**
  - **Latent dimension (k=256):** Higher → better reconstruction but risk of overfitting; lower → compression but may lose fine features
  - **Truncation distance (δ=0.1):** Larger → more supervision but weaker surface focus; smaller → tight band but risks sparse supervision
  - **Near-surface sampling ratio (50%):** Higher → better surface detail; lower → better global shape
  - **Strain input dimensionality (3):** Current uses max strains only; could extend to full field but requires consistent sampling across designs

- **Failure signatures:**
  - Non-Gaussian latent marginals with heavy tails → λ_z too low or decoder under-capacity
  - Reconstructions with spurious surface artifacts → SDF sampling too sparse near surface or δ too large
  - Conditional map produces non-physical geometries → training strains outside distribution or insufficient training diversity
  - PC axes show no correlation with known parameters → latent dimension too high (modes split artificially) or training collapsed to single mode

- **First 3 experiments:**
  1. **Reconstruction baseline:** Train on subset (e.g., 100 designs), reconstruct held-out designs; plot distance metric histogram. Target: 80%+ of reconstructions <1% D_max.
  2. **Latent space traversal:** Select two designs with known K₁, K₃ differences; interpolate z and decode intermediate shapes. Verify smooth morphological transition and monotonic parameter change.
  3. **Conditional map sanity check:** Train g_φ, then for each training design compare reconstruction from optimized z_i vs. predicted z* = g_φ(ε_i). Target: surface distance <0.5% D_max for 90% of designs.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the strain-conditioned generation actually yield geometries that satisfy the target mechanical responses when verified by high-fidelity physics solvers?
- **Basis:** The authors state that "A full structural analysis would be required to quantitatively verify that the synthesized geometry achieves the desired strain extrema."
- **Why unresolved:** The paper maps strains to latent codes using a neural network, but this acts as a surrogate; the inverse design loop is not closed with a finite element analysis (FEA) to confirm physical accuracy.

### Open Question 2
- **Question:** How can predictive uncertainty be integrated into the reconstruction and synthesis pipeline to enable risk-aware design exploration?
- **Basis:** The conclusion explicitly identifies this as a future direction: "integrating predictive uncertainty into reconstruction and conditional synthesis would enable risk-aware exploration and selection of candidate geometries."
- **Why unresolved:** The current DeepSDF auto-decoder formulation produces deterministic latent codes and surfaces without confidence intervals, making it difficult to assess the reliability of novel generated designs.

### Open Question 3
- **Question:** Can the SDF labeling pipeline maintain robustness when applied to real-world blade scans that deviate from the idealized "near-convex" assumption?
- **Basis:** Section II.A.2 notes the method relies on a convex-hull proxy for sign computation, justified by the assumption that "blades designs are near-convex," which implies potential failure modes for highly non-convex or damaged real-world geometries.
- **Why unresolved:** The current results are limited to clean, synthetic datasets where the convex-hull approximation is valid.

## Limitations

- **Convexity assumption:** The convex-hull sign assignment method may fail for blades with significant concavities or internal cooling channels
- **Conditional mapping ambiguity:** The 3→256 dimensional mapping from strain to latent space may suffer from ambiguity or extrapolation failures beyond the training distribution
- **Deterministic outputs:** The current framework produces deterministic geometries without predictive uncertainty estimates, limiting risk-aware design exploration

## Confidence

- **High Confidence:** Reconstruction accuracy claims (1% surface distance error) are well-supported by quantitative metrics and extensive testing on 300 held-out designs. The auto-decoder formulation and joint optimization mechanism are clearly specified and validated.
- **Medium Confidence:** The interpretability of the latent space (alignment with taper/chord ratios) is demonstrated but relies on correlation rather than causal analysis. The Gaussianity of the latent distribution is observed but not rigorously tested against null distributions.
- **Low Confidence:** The conditional generation performance on truly unseen strain profiles is not extensively validated. The 3→256 dimensional mapping from strain to latent space may suffer from ambiguity or extrapolation failures beyond the training distribution.

## Next Checks

1. **Robustness to Concavity:** Test the SDF generation pipeline on blade geometries with known internal features (cooling channels, fillets). Measure sign assignment accuracy and reconstruction fidelity compared to ground truth.
2. **Latent Space Gaussianity Test:** Perform formal statistical tests (e.g., Kolmogorov-Smirnov) on latent marginals to confirm N(0,0.1) distribution. Compare against alternative regularization schemes (e.g., Gaussian mixture priors).
3. **Conditional Extrapolation:** Generate conditional designs for strain profiles systematically varied beyond the convex hull of training strains. Quantify reconstruction degradation and check for physical plausibility of generated geometries.