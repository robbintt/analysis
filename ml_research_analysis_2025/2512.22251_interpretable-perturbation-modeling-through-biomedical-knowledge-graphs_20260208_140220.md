---
ver: rpa2
title: Interpretable Perturbation Modeling Through Biomedical Knowledge Graphs
arxiv_id: '2512.22251'
source_url: https://arxiv.org/abs/2512.22251
tags:
- graph
- drug
- expression
- biological
- embeddings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We develop a multimodal biomedical knowledge graph that integrates
  drug, gene, protein, disease, pathway, and cellular context embeddings with LINCS
  L1000 perturbation data to predict differential gene expression profiles for drug-cell
  pairs. Our graph attention network outperforms MLP baselines by 2.5% in DEG correlation
  (0.708 vs.
---

# Interpretable Perturbation Modeling Through Biomedical Knowledge Graphs

## Quick Facts
- arXiv ID: 2512.22251
- Source URL: https://arxiv.org/abs/2512.22251
- Reference count: 5
- Graph attention network outperforms MLP baselines by 2.5% in DEG correlation (0.708 vs. 0.683) under scaffold splits

## Executive Summary
This work develops a multimodal biomedical knowledge graph that integrates drug, gene, protein, disease, pathway, and cellular context embeddings with LINCS L1000 perturbation data to predict differential gene expression profiles for drug-cell pairs. The graph attention network (GATv2) architecture leverages heterogeneous biological relationships to generalize to structurally novel compounds, achieving 0.708 DEG correlation versus 0.683 for MLP baselines under Bemis-Murcko scaffold splits. Ablation experiments demonstrate that both graph topology and pretrained multimodal node features are essential—edge shuffling or node randomization causes performance to fall below MLP levels. Attention analysis reveals the model focuses on protein-mediated interactions and biological processes, mirroring known drug mechanisms and enabling interpretable perturbation modeling.

## Method Summary
The method constructs a heterogeneous knowledge graph from PrimeKG++ and LINCS L1000 data, with 7 node types (drugs, genes/proteins, diseases, biological processes, molecular functions, cellular components, pathways, cells) and multimodal embeddings from foundation models (MolFormer-XL for SMILES, BioBERT for text, ProtBERT for proteins). GATv2 with 4 attention heads processes the graph, routing messages through biologically meaningful edges. Node-type-specific 3-layer MLPs project embeddings to shared space, and a delta predictor adds perturbation effects to baseline expression. The model is trained on 80/20 scaffold splits with MSE loss and AdamW optimizer, evaluating DEG correlation on top-50 perturbed genes to test generalization to structurally novel compounds.

## Key Results
- GATv2 achieves 0.708 DEG correlation versus MLP 0.683 on scaffold splits (2.5% improvement)
- Edge shuffling ablation drops performance to 0.537, below MLP baseline, proving graph topology is essential
- Node randomization with intact topology collapses performance to 0.572, proving pretrained embeddings are essential
- Attention analysis shows 98.8% routes through protein nodes without explicit supervision

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Graph structure provides mechanistic inductive bias that enables generalization to structurally novel compounds
- Mechanism: Message passing across protein, pathway, and ontology nodes creates drug representations enriched with biological context, allowing the model to infer perturbation effects for unseen scaffolds by reasoning through shared biological intermediaries rather than chemical similarity
- Core assumption: Drugs with different structures but similar protein targets/pathway interactions produce similar transcriptional perturbations
- Evidence anchors:
  - [abstract] "Ablation experiments show both graph topology and pretrained multimodal node features are essential—edge shuffling or node randomization causes performance to fall below MLP levels"
  - [section 3.2.1] "When graph edges were shuffled—removing all meaningful biological relationships while preserving node features—GAT performance dropped from 0.708 to 0.537, falling below even the MLP baselines"
  - [corpus] Related work on causal knowledge graphs (FMR=0.586) suggests structured biological relationships support mechanistic reasoning, though causal formalization remains limited
- Break condition: If test compounds share scaffolds with training data (random split), graph provides no advantage—model succeeds via chemical similarity instead

### Mechanism 2
- Claim: Protein-mediated attention emerges without explicit supervision as the primary reasoning pathway
- Mechanism: GATv2 learns to route 98.8% of attention through protein nodes, mirroring biological reality that drugs act via protein targets; this pattern emerges from graph structure alone rather than task-specific supervision
- Core assumption: Biomedical knowledge graph edges encode biologically meaningful relationships that map onto true drug mechanisms
- Evidence anchors:
  - [abstract] "Attention analysis reveals that the model focuses on protein-mediated interactions and biological processes, mirroring known drug mechanisms"
  - [section 3.3, Figure 2] "98.8% of attention routes through protein nodes...This protein-centric pattern emerges without explicit supervision"
  - [corpus] Weak direct corpus evidence for unsupervised attention emergence in drug perturbation specifically
- Break condition: Edge shuffling causes attention to fragment (23.2% drug-to-drug), demonstrating that coherent attention requires meaningful topology

### Mechanism 3
- Claim: Pretrained multimodal embeddings provide semantic content that structure alone cannot supply
- Mechanism: Foundation model embeddings (MolFormer-XL for SMILES, BioBERT for text, ProtBERT for proteins) encode domain knowledge into node features; message passing then propagates this semantic information across biologically meaningful edges
- Core assumption: Pretrained embeddings capture biologically relevant features that generalize across contexts
- Evidence anchors:
  - [abstract] "both graph topology and pretrained multimodal node features are essential"
  - [section 3.2.2] "Randomizing all node embeddings except for the drug and cell representations produced a similar performance collapse (DEG correlation: 0.572), despite leaving the graph topology intact"
  - [corpus] Multimodal contrastive learning in BKGs (PrimeKG++) shows pretrained embeddings improve link prediction, supporting transfer to perturbation task
- Break condition: Node randomization with intact topology causes performance collapse, proving structure without semantics is insufficient

## Foundational Learning

- Concept: **Graph Attention Networks (GATv2)**
  - Why needed here: Core architecture for heterogeneous message passing with learned edge-type-specific attention weights
  - Quick check question: Can you explain how attention coefficients are computed per edge type and aggregated across heads?

- Concept: **Scaffold Splitting (Bemis-Murcko)**
  - Why needed here: Critical evaluation methodology—random splits inflate performance by allowing chemical memorization; scaffold splits test true generalization
  - Quick check question: Why would random splitting make graph structure appear unnecessary?

- Concept: **Delta Prediction Formulation (ChemCPA-style)**
  - Why needed here: Model predicts ∆ (perturbation effect) added to baseline expression, disentangling drug effect from cellular context
  - Quick check question: What happens if you predict expression directly instead of delta?

## Architecture Onboarding

- Component map:
  - **Input layer**: Multimodal embeddings [2, 768] per node (MolFormer-XL + BioBERT for drugs; ProtBERT + BioBERT for proteins; BioBERT for diseases/GO terms); cells use [2, 978] baseline expression
  - **Encoder**: Node-type-specific 3-layer MLPs (1536→1024→1024→256) projecting to shared space
  - **GATv2 layer**: 4 attention heads with edge-type-specific attention, neighbor sampling (20/10 per hop)
  - **Prediction head**: 3-layer MLP (drug embedding ⊕ baseline expression → 978-dim delta)
  - **Output**: Predicted perturbed expression = baseline + delta

- Critical path: Drug node → GATv2 message passing through protein/pathway neighbors → contextualized drug embedding z_drug → concatenate with baseline expression → delta predictor → add to baseline

- Design tradeoffs:
  - Scaffold split is harder but realistic; random split inflates performance via chemical similarity
  - Neighbor sampling enables scalability but may miss long-range dependencies
  - Mean-pooling target embeddings (MLP+Targets) underperforms GAT, suggesting attention-weighted aggregation matters

- Failure signatures:
  - Performance ≈0.54 DEG correlation: Check if edges were shuffled or graph is disconnected
  - Performance ≈0.57 with intact graph: Check if node features are randomized or encoder failing
  - Random/scaffold split performance identical: Model may be overfitting to chemical features, graph not utilized

- First 3 experiments:
  1. **Baseline reproduction**: Train MLP with concatenated drug+baseline features on scaffold split; target ~0.68 DEG correlation
  2. **Edge shuffle ablation**: Randomize graph edges while preserving node features; expect collapse to ~0.54 (below MLP baseline)
  3. **Attention visualization**: For a known drug (e.g., tamoxifen), extract 1-hop and 2-hop attention distributions; verify protein-centric routing (>90%) and identify mechanistic intermediaries

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can inverting the learned drug-to-gene mapping enable accurate prediction of candidate drugs from desired transcriptional profiles?
- Basis in paper: [explicit] The authors state "an intriguing future direction is the inverse perturbation problem: predicting candidate drugs capable of inducing a desired transcriptional effect."
- Why unresolved: The current framework only addresses the forward direction (drug → expression); the inverse mapping requires additional constraints and validation.
- What evidence would resolve it: Successful recovery of known drugs from their perturbation signatures, and experimental validation of novel drug predictions.

### Open Question 2
- Question: Would learning edge-specific importance weights or pruning uninformative edges improve perturbation prediction accuracy?
- Basis in paper: [explicit] The authors note "large portions of the graph are either uninformative or not encoded in a way the model can leverage" and suggest "learning edge-specific importance weights could substantially strengthen representation quality."
- Why unresolved: Current attention treats edge types uniformly; attention analysis shows only a subset of edges (primarily protein-protein and GO terms) contribute meaningfully.
- What evidence would resolve it: Ablation comparing learned edge weighting against uniform attention, with improvements in DEG correlation on scaffold splits.

### Open Question 3
- Question: Can more expressive heterogeneous graph transformers outperform GATv2 on this task?
- Basis in paper: [explicit] "Our architectural exploration was limited to GATv2. Recent heterogeneous graph transformers, relation-aware message-passing models, and hybrid neural-symbolic architectures may more effectively exploit typed multimodal graphs."
- Why unresolved: Only GATv2 was tested; the contribution of architecture choice versus graph structure remains unclear.
- What evidence would resolve it: Benchmarking alternative architectures (e.g., HGT, RGCN) on identical splits with statistical comparison of DEG correlation.

### Open Question 4
- Question: Would incorporating cell-type-specific or condition-specific edge context improve mechanistic reasoning?
- Basis in paper: [inferred] The authors acknowledge that "many edges encode broad associations but lack the context (cell type, condition, directionality) required for precise mechanistic reasoning."
- Why unresolved: Current edges are static and context-agnostic, potentially limiting the model's ability to capture context-dependent drug effects.
- What evidence would resolve it: Performance comparison between static-edge and context-conditioned-edge graphs on cell-line-stratified perturbation predictions.

## Limitations

- Performance advantage over MLP baselines is modest (2.5% in DEG correlation) and may not justify added complexity in all applications
- Scaffold split evaluation tests generalization within same biological space rather than truly novel biological contexts
- Edge shuffling and node randomization ablations confirm necessity of structure and semantics but don't isolate which specific edge types or node modalities are most critical

## Confidence

- High confidence in GATv2 architecture and scaffold split methodology
- Medium confidence in interpretation of attention patterns as mechanistic reasoning
- Medium confidence that performance gains stem from biological generalization rather than memorization
- Low confidence in absolute performance numbers without code release for exact reproduction

## Next Checks

1. **Ablation of specific edge types**: Remove drug-protein edges versus protein-protein edges versus ontology edges to identify which relationships drive performance gains.
2. **Cross-cell generalization test**: Train on subset of cell lines and test on held-out cell types to verify the model learns perturbation mechanisms rather than cell-specific responses.
3. **Mechanistic validation**: For top-attended protein nodes in known drug cases, verify whether these proteins are established targets or pathway mediators through independent literature review.