---
ver: rpa2
title: 'IMRNNs: An Efficient Method for Interpretable Dense Retrieval via Embedding
  Modulation'
arxiv_id: '2601.20084'
source_url: https://arxiv.org/abs/2601.20084
tags:
- retrieval
- imrnns
- query
- modulation
- dense
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "IMRNNs addresses the lack of interpretability in dense retrieval\
  \ by introducing a lightweight framework that dynamically modulates query and document\
  \ embeddings at inference time. It employs two independent adapters\u2014one conditioning\
  \ document embeddings on the current query, and another refining the query embedding\
  \ using corpus-level feedback from retrieved documents."
---

# IMRNNs: An Efficient Method for Interpretable Dense Retrieval via Embedding Modulation

## Quick Facts
- **arXiv ID**: 2601.20084
- **Source URL**: https://arxiv.org/abs/2601.20084
- **Reference count**: 16
- **Key outcome**: Improves retrieval effectiveness with average gains of +6.35% nDCG, +7.14% recall, and +7.04% MRR over state-of-the-art baselines across seven benchmark datasets

## Executive Summary
IMRNNs introduces a lightweight framework for interpretable dense retrieval by dynamically modulating query and document embeddings at inference time. The method employs two independent adapters—one conditioning document embeddings on the current query, and another refining the query embedding using corpus-level feedback from retrieved documents. This bidirectional modulation enables the model to adapt representations dynamically while exposing interpretable semantic dependencies between queries and documents. Empirically, IMRNNs demonstrates consistent improvements across seven BEIR benchmark datasets, with average gains of +6.35% nDCG, +7.14% recall, and +7.04% MRR over state-of-the-art baselines.

## Method Summary
IMRNNs is a bidirectional adapter framework that modulates static query and document embeddings to improve dense retrieval effectiveness while providing interpretability. The method uses two 2-layer MLPs: a Query Adapter that conditions document embeddings on the current query, and a Document Adapter that aggregates corpus-level feedback to refine the query embedding. A learnable projection matrix reduces embedding dimensions from 1024 to 256 for efficient adapter learning. The model is trained with margin-based contrastive loss and evaluated on seven BEIR benchmark datasets. During inference, document embeddings are pre-computed and stored, while modulation occurs dynamically based on the query context.

## Key Results
- Improves nDCG by +6.35% average across seven benchmark datasets
- Increases recall by +7.14% and MRR by +7.04% over state-of-the-art baselines
- Achieves better retrieval effectiveness than unidirectional adapter baselines
- Provides interpretable semantic explanations through back-projection of modulation vectors

## Why This Works (Mechanism)

### Mechanism 1: Query-Conditioned Document Modulation
Dynamically modulating document embeddings based on query context improves retrieval discrimination by pulling relevant documents closer and pushing irrelevant ones away in the embedding space. The Query Adapter (Aq) is a lightweight MLP that takes the projected query embedding and produces a weight matrix W_q and bias b_q, which transform each document embedding as d_mod = W_q · d_proj + b_q.

### Mechanism 2: Corpus-Level Query Refinement
Aggregating document-specific transformations provides corpus-level feedback that aligns query embeddings with the vocabulary and semantic structure of available documents. The Document Adapter (A_d) processes each document independently to produce (W_d, b_d) pairs, which are averaged across the corpus and applied to the query: q_mod = W̄_d · q_proj + b̄_d.

### Mechanism 3: Moore-Penrose Back-Projection for Token Attribution
Modulation vectors can be back-projected to the encoder's token embedding space to identify human-interpretable keywords that explain retrieval decisions. By computing cosine similarity between the back-projected vector and every token embedding in the encoder's vocabulary, the method identifies tokens whose semantics align with the modulation direction.

## Foundational Learning

- **Moore-Penrose Pseudoinverse**
  - Why needed here: Core to the interpretability mechanism; maps modulation vectors from reduced working space back to original embedding dimensions
  - Quick check question: Given projection matrix P (m×n, m<n), how would you compute P^+ and what does it geometrically represent?

- **Margin-Based Contrastive Learning**
  - Why needed here: Training objective that enforces separation between relevant and irrelevant query-document pairs in modulated space
  - Quick check question: In the loss L = max{0, γ − cos(q_mod, d_mod^+) + cos(q_mod, d_mod^−)}, what happens when γ is too large vs. too small?

- **Embedding Space Geometry**
  - Why needed here: Understanding cosine similarity, vector addition/subtraction for modulation deltas, and how affine transformations reshape embedding neighborhoods
  - Quick check question: Why does layer normalization before cosine similarity matter for stable gradient flow?

## Architecture Onboarding

- **Component map**: Base encoder (frozen) → Projection matrix (1024→256) → Query Adapter + Document Adapter → Modulated embeddings → Cosine similarity scoring

- **Critical path**:
  1. Pre-compute document projections d_proj offline
  2. For new query: encode → project → pass through both adapters
  3. Apply W_q to all documents (vectorized matrix-vector products)
  4. Aggregate document adapter outputs → apply to query
  5. Compute cosine similarities → rank

- **Design tradeoffs**:
  - Dimension reduction (m=256) trades representational capacity for faster adapter learning and generalization
  - Bidirectional modulation adds ~1.6ms latency vs. ~1.0ms for unidirectional adapters
  - Interpretability via back-projection is approximate; some tokens will be noisy

- **Failure signatures**:
  - **Noisy token attribution**: Spurious tokens like "Ridges" appearing in keyword lists—filter or post-process
  - **Domain mismatch**: Poor gains on specialized domains if base encoder lacks domain semantics—switch base retriever first
  - **Corpus scaling**: Linear cost with corpus size for document adapter—requires caching or ANN pre-filtering

- **First 3 experiments**:
  1. **Baseline replication**: Run IMRNNs on MS MARCO with e5-large-v2; verify nDCG improvement (~0.85→0.88) and inspect token attributions for 5 queries
  2. **Ablation**: Disable document adapter (unidirectional modulation only) to isolate contribution of corpus-level feedback
  3. **Base encoder sensitivity**: Swap e5-large-v2 for BGE on a domain-specific dataset (e.g., SciFact) to confirm modulation amplifies rather than replaces base quality

## Open Questions the Paper Calls Out

### Open Question 1
Can systematic methods be developed to automatically identify and filter spurious tokens from back-projected modulation vectors? The Moore-Penrose pseudoinverse back-projection is inherently approximate, producing noisy mappings where tokens like "Ridges" and "Innate" appear semantically unrelated to query-document interactions.

### Open Question 2
How does IMRNNs' inference latency scale on corpora exceeding 10M documents, and what infrastructure optimizations are necessary for deployment? The Document Adapter processes each corpus document individually, scaling linearly with corpus size, "potentially limiting deployment on extremely large corpora (>10M documents) without infrastructure optimizations."

### Open Question 3
Does the simple averaging aggregation for Document Adapter optimally capture corpus-level signals, or would relevance-weighted aggregation improve performance? The paper uses uniform averaging but does not ablate or justify this choice against alternatives like weighting document transformations by initial retrieval scores.

## Limitations

- **Computational complexity**: The document adapter requires O(N) complexity per query, making it prohibitive for large-scale corpora without batching or ANN indexing strategies
- **Noisy interpretability**: Back-projected tokens often include semantically spurious results like "Ridges" and "Innate" that don't meaningfully explain retrieval decisions
- **Base encoder dependency**: The method amplifies rather than replaces base encoder quality, showing poor performance on specialized domains when the base retriever lacks domain-specific semantics

## Confidence

- **High confidence**: The bidirectional modulation mechanism and its architectural implementation are well-specified with clear equations and component descriptions. The core retrieval effectiveness improvements are supported by comprehensive experiments across seven datasets.
- **Medium confidence**: The training procedure and hyperparameters are detailed, but implementation specifics like MLP hidden dimensions and initialization strategies are not provided. The interpretability mechanism is theoretically sound but produces noisy results in practice.
- **Low confidence**: The computational complexity analysis and scalability solutions for large corpora are not thoroughly addressed. The paper mentions latency increases but doesn't provide solutions for handling corpora with millions of documents during inference.

## Next Checks

1. **Computational Scalability Test**: Implement the document adapter inference pipeline and measure latency scaling with corpus size (e.g., test on subsets of 10K, 100K, 1M documents from MS MARCO). Compare against baseline dense retrieval and evaluate whether batching strategies or ANN indexing are necessary for practical deployment.

2. **Interpretability Quality Assessment**: For 20 random queries, extract top-10 tokens from back-projected modulation vectors and have human annotators rate semantic relevance (0-5 scale). Calculate precision@k for interpretable tokens and compare against a baseline of random token sampling to quantify whether the method provides meaningful explanations beyond noise.

3. **Base Encoder Sensitivity Analysis**: Systematically replace the base encoder across three quality tiers (e5-large-v2, MiniLM, BGE) on a domain-specific dataset like SciFact. Measure how IMRNNs modulation amplifies base quality differences and determine whether the method's gains are additive to base retriever performance or if it introduces new failure modes with weaker base encoders.