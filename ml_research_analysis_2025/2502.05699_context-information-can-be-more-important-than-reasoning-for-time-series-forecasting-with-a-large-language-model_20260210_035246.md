---
ver: rpa2
title: Context information can be more important than reasoning for time series forecasting
  with a large language model
arxiv_id: '2502.05699'
source_url: https://arxiv.org/abs/2502.05699
tags:
- time
- series
- prompting
- forecasting
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the effectiveness of prompting techniques
  for time series forecasting using large language models, specifically GPT-4o-mini.
  The research evaluates various prompting strategies including zero-shot and one-shot
  chain-of-thought, plan-and-solve, long-short-term prompting, and a novel SARIMA-based
  approach.
---

# Context information can be more important than reasoning for time series forecasting with a large language model

## Quick Facts
- arXiv ID: 2502.05699
- Source URL: https://arxiv.org/abs/2502.05699
- Authors: Janghoon Yang
- Reference count: 21
- Primary result: Simple baseline prompts with contextual information often achieve performance comparable to or better than complex reasoning prompts for LLM-based time series forecasting

## Executive Summary
This study investigates how different prompting strategies affect time series forecasting performance when using large language models, specifically GPT-4o-mini. The research systematically evaluates various approaches including zero-shot and one-shot chain-of-thought, plan-and-solve, long-short-term prompting, and a novel SARIMA-based method. Experiments on three short time series datasets (PISA) and one long time series dataset (IHEPC household electricity consumption) reveal that providing appropriate context information about the domain and temporal characteristics often yields performance comparable to or better than more complex reasoning-based prompts. The findings challenge the assumption that sophisticated reasoning prompts necessarily improve forecasting accuracy.

## Method Summary
The study uses GPT-4o-mini to forecast time series data using various prompting strategies. Baseline prompts include domain context and time resolution information following the PromptCast format. Chain-of-thought prompts instruct the model to "think step by step" with and without one-shot examples. The Plan-and-Solve+ approach guides decomposition of forecasting tasks. Long-Short-Term prompting processes recent and historical values separately. A novel SARIMA-based prompt requests explicit trend, seasonality, and short-term component decomposition before forecasting. The PISA dataset provides three short time series (SG, CT, ECL), while the IHEPC dataset is preprocessed into 96-timestep windows shifted by 10 points for 6-step forecasting. Performance is measured using RMSE and MAE across multiple samples.

## Key Results
- No single prompting method consistently outperformed others across all datasets
- Baseline prompts with contextual information achieved best RMSE on SG dataset and best RMSE on IHEPC steps 2, 4, 5, 6
- One-shot chain-of-thought outperformed baseline only on CT dataset
- SARIMA-based prompts degraded performance 2-3x due to failure to properly subtract trend before seasonality
- LST prompting produced negative predictions when recent values decreased and sometimes generated insufficient output values

## Why This Works (Mechanism)

### Mechanism 1: Context Anchoring Reduces Reasoning Overhead
- Claim: Providing domain and temporal context information yields forecasting performance comparable to or better than complex reasoning prompts.
- Mechanism: Contextual framing allows the LLM to leverage pretrained world knowledge about temporal patterns without requiring explicit multi-step reasoning procedures that invite calculation errors.
- Core assumption: The model has internalized sufficient temporal reasoning during pretraining; additional reasoning prompts add noise rather than signal.
- Evidence anchors:
  - [abstract] "simply providing proper context information related to the time series, without additional reasoning prompts, can achieve performance comparable to the best-performing prompt for each case"
  - [Page 4] Baseline prompting achieved best RMSE on SG dataset (9.665) and best RMSE on IHEPC steps 2, 4, 5, 6
  - [corpus] Related work (DP-GPT4MTS) confirms contextual textual information significantly affects forecasting accuracy
- Break condition: When domain-specific physics or strong seasonality patterns require explicit decomposition, baseline context alone may underperform (e.g., one-shot CoT outperformed baseline on CT dataset).

### Mechanism 2: Tokenization Fragmentation Disrupts Numerical Pattern Recognition
- Claim: BPE-based tokenization fragments multi-digit numbers, impairing the model's ability to recognize numerical patterns in time series.
- Mechanism: Numbers are split into multiple tokens (e.g., 13245 → [132, 45]; 12.992 → [12, ., 992]), breaking the numerical coherence needed for pattern extraction and algebraic operations.
- Core assumption: Numerical reasoning requires perceiving complete values; fragmentation forces the model to reconstruct meaning from incoherent tokens.
- Evidence anchors:
  - [Page 1] "numbers with long digits can be split into multiple tokens, hindering the extraction of precise patterns in numerical sequences"
  - [Page 4] "The tokenizer does not treat numeric values as a single token but includes vocabularies for numbers from 0 to 999"
  - [corpus] "Revisiting LLMs as Zero-Shot Time-Series Forecasters" finds small noise can break large models, suggesting numerical fragility
- Break condition: Models with numerical-aware tokenization or patch-based approaches would mitigate this mechanism.

### Mechanism 3: Procedural Misalignment Between Prompt Specification and Learned Behavior
- Claim: Complex reasoning prompts can degrade performance when their procedural requirements conflict with the model's learned reasoning patterns.
- Mechanism: Detailed prompts (e.g., SARIMA decomposition) require sequential execution of subtasks; models frequently skip steps, misapply operations (e.g., not subtracting trend before seasonality), or revert to simple heuristics (recent-values-only forecasting).
- Core assumption: LLMs lack reliable procedural execution for unfamiliar multi-step numerical tasks.
- Evidence anchors:
  - [Page 2] "LLMs often fail to follow the procedures described by the prompt... fail to calculate accurately... misunderstand the semantics of prompts"
  - [Page 5] One-shot SARIMA produced 2-3x larger predictions due to failing to subtract trend/seasonality components
  - [corpus] "Beyond Naïve Prompting" notes LLMs can be effective context-aided forecasters but full potential requires better strategies
- Break condition: When one-shot examples align well with model's internal reasoning (as with one-shot CoT on CT dataset), procedural prompts can help.

## Foundational Learning

- **Concept: Time Series Decomposition (Trend/Seasonality/Residual)**
  - Why needed here: The SARIMA prompting method explicitly requests this decomposition; understanding why it failed requires knowing what proper decomposition looks like.
  - Quick check question: Given [10, 12, 14, 11, 13, 15, 12, 14, 16], what is the approximate trend and what might be the seasonality period?

- **Concept: Tokenization for Numerical Data**
  - Why needed here: The paper identifies BPE tokenization as a fundamental limitation; practitioners must understand how their tokenizer handles numbers before attempting numerical tasks.
  - Quick check question: How would a BPE tokenizer with vocabulary 0-999 tokenize the sequence [1000, 23.45, 78910]?

- **Concept: Zero-Shot vs. One-Shot Prompting**
  - Why needed here: The study compares these approaches; one-shot examples helped CoT but hurt SARIMA—understanding when examples help vs. hurt is critical.
  - Quick check question: What characteristics of a one-shot example might cause it to degrade rather than improve performance?

## Architecture Onboarding

- **Component map:**
  Time Series Input → Context Enrichment (domain, resolution, dates) → Prompt Construction (baseline vs. reasoning-based) → LLM (GPT-4o-mini in study) → Response Parsing (regex extraction, handle missing) → Evaluation (RMSE, MAE)

- **Critical path:** Context information quality → Prompt complexity calibration → Response parsing robustness. The study shows over-engineering the middle step (complex prompts) often hurts more than helps.

- **Design tradeoffs:**
  - Simple prompts with rich context vs. complex reasoning prompts with minimal context: Paper favors the former
  - One-shot examples: Can improve (CT dataset with CoT) or severely degrade (SARIMA across all datasets) depending on alignment
  - Response format enforcement vs. reasoning freedom: Strict format requests may interfere with forecasting quality

- **Failure signatures:**
  - **Quantized forecasting:** Model outputs only a small set of repeated values
  - **Missing steps:** LST prompting sometimes generates 5 values instead of 6
  - **Component addition errors:** SARIMA produces 2-3x inflated predictions when trend not subtracted before seasonality
  - **Negative predictions:** LST produced negative values when recent trend was decreasing
  - **Recent-value anchoring:** Model ignores long-term patterns regardless of prompt instructions

- **First 3 experiments:**
  1. **Baseline context-only test:** Implement the PromptCast-style baseline with domain and time resolution; establish this as your floor performance before trying complex prompts.
  2. **Tokenization audit:** Test how your model's tokenizer handles representative numbers from your domain; if fragmentation is severe, consider scaling/offsetting as the paper suggests.
  3. **Ablation on your specific domain:** Test at least 3 prompt types (baseline, zero-shot CoT, one-shot CoT) on your data—do not assume results transfer across datasets, as the paper shows no universal winner.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can joint fine-tuning and prompting achieve better performance than either approach alone for LLM-based time series forecasting?
  - Basis in paper: [explicit] "Moreover, joint fine-tuning and prompting may have potential for further improvement, such that fine-tuning can support the requirements of the prompting better, and datasets for fine-tuning can be more fitted to a specific type of prompting."
  - Why unresolved: This study only evaluated prompting strategies on a frozen model (GPT-4o-mini) without any fine-tuning, leaving the interaction between fine-tuning and prompting unexplored.
  - What evidence would resolve it: Experiments comparing (1) prompting-only, (2) fine-tuning-only, and (3) joint fine-tuning-with-prompting conditions across multiple datasets, measuring RMSE/MAE improvements.

- **Open Question 2:** What training corpus design would most effectively address LLM shortcomings in time series forecasting?
  - Basis in paper: [explicit] "Proper fine-tuning is required to fit time series forecasting, which requires a training corpus that addresses the shortcomings of LLMs for time series forecasting."
  - Why unresolved: The paper identifies specific weaknesses (calculation errors, failure to exploit seasonality, over-reliance on recent values) but does not investigate what training data could remediate these issues.
  - What evidence would resolve it: Ablation studies with training corpora specifically designed to target each identified weakness, comparing performance gains from each corpus variant.

- **Open Question 3:** Can scaling and offsetting techniques effectively mitigate tokenization limitations for numerical time series data without requiring patching/adaptor modules?
  - Basis in paper: [explicit] "Additionally, fundamental limitations by tokenization need to be addressed further. Proper scaling and offsetting can be an alternative way to deal with this issue approximately, rather than having an adaptor module for patching, which incurs additional complexity for training."
  - Why unresolved: The paper acknowledges tokenization as a fundamental limitation and suggests scaling/offsetting as a simpler alternative, but provides no empirical validation of this approach.
  - What evidence would resolve it: Comparative experiments testing various scaling and offsetting strategies against patching-based methods, evaluating both forecasting accuracy and computational overhead.

- **Open Question 4:** What mechanisms cause context information to outperform explicit reasoning prompts in LLM-based forecasting?
  - Basis in paper: [inferred] The paper finds that "providing proper context information can be more crucial than a prompt for specific reasoning" but does not explain the underlying mechanism—whether it relates to reduced cognitive load, better alignment with pre-training, or avoidance of accumulated reasoning errors.
  - Why unresolved: The empirical finding is presented, but no controlled experiments isolate the causal factors behind this counterintuitive result.
  - What evidence would resolve it: Systematic ablation of context elements (temporal metadata, domain information, units) combined with analysis of internal reasoning traces to identify which factors most influence prediction quality.

## Limitations

- Experiments were conducted exclusively on GPT-4o-mini, a small model that may not represent larger frontier models' capabilities
- Evaluation covers only four datasets (three short time series from PISA and one long series from IHEPC), limiting generalizability
- Specific prompt templates and one-shot examples are referenced externally rather than fully specified in the paper
- The tokenization analysis provides observational evidence rather than controlled experiments measuring quantitative impact

## Confidence

- **High Confidence:** Context information importance - The consistent finding across multiple datasets that baseline prompts with proper contextual framing achieve performance comparable to or better than complex reasoning prompts is well-supported by the experimental results.
- **Medium Confidence:** Tokenization fragmentation impact - While the mechanism is plausible and theoretically sound, the paper provides observational evidence rather than controlled experiments measuring the quantitative impact of tokenization on numerical pattern recognition accuracy.
- **Medium Confidence:** Procedural misalignment effects - The documented failures of complex reasoning prompts (particularly SARIMA) are well-demonstrated, but the extent to which these generalize to other multi-step numerical tasks or larger models remains uncertain.
- **Low Confidence:** Universal superiority of context over reasoning - The paper shows no single method consistently wins, suggesting the "context more important than reasoning" framing may be overstated. Results are highly dataset-dependent.

## Next Checks

1. **Tokenization Impact Validation:** Conduct controlled experiments comparing numerical accuracy across different tokenization schemes (BPE vs. word-based vs. numerical-aware) on identical forecasting tasks to quantify the fragmentation effect claimed in the paper.

2. **Model Size Generalization:** Replicate the core experiments (baseline, zero-shot CoT, one-shot CoT) on at least two additional model sizes (e.g., GPT-4o and a smaller model than 4o-mini) to determine whether the context vs. reasoning tradeoff identified holds across the capability spectrum.

3. **Domain Transferability Test:** Apply the same prompting strategies to time series datasets from fundamentally different domains (e.g., financial markets, medical monitoring, sensor data) to validate whether the dataset-specific performance variations observed are due to domain characteristics or dataset-specific quirks.