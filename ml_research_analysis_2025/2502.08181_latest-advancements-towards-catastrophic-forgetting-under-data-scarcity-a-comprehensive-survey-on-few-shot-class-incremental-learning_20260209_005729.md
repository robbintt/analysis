---
ver: rpa2
title: 'Latest Advancements Towards Catastrophic Forgetting under Data Scarcity: A
  Comprehensive Survey on Few-Shot Class Incremental Learning'
arxiv_id: '2502.08181'
source_url: https://arxiv.org/abs/2502.08181
tags:
- fscil
- learning
- prototype
- few-shot
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This survey comprehensively examines few-shot class incremental
  learning (FSCIL), addressing the challenge of catastrophic forgetting under data
  scarcity. It introduces five main FSCIL approaches: backbone tuning, meta-learning,
  prototype-tuning, dynamic architecture, and parameter-efficient fine-tuning (PEFT).'
---

# Latest Advancements Towards Catastrophic Forgetting under Data Scarcity: A Comprehensive Survey on Few-Shot Class Incremental Learning

## Quick Facts
- arXiv ID: 2502.08181
- Source URL: https://arxiv.org/abs/2502.08181
- Reference count: 11
- Primary result: PEFT approaches with frozen ViT backbones achieve up to 95.27% average accuracy on MiniImageNet

## Executive Summary
This survey comprehensively examines few-shot class incremental learning (FSCIL), addressing the challenge of catastrophic forgetting under data scarcity. It introduces five main FSCIL approaches: backbone tuning, meta-learning, prototype-tuning, dynamic architecture, and parameter-efficient fine-tuning (PEFT). The PEFT approach, utilizing pre-trained vision transformers (ViT) with prompt-based fine-tuning, emerges as particularly effective, achieving up to 95.27% average accuracy on MiniImageNet. The survey highlights the importance of prototype rectification mechanisms to mitigate bias from limited samples and discusses the potential of language-guided learning using vision-language models. Despite advancements, stability-plasticity balance remains a challenge, with significant performance drops between base and novel classes.

## Method Summary
The survey identifies Parameter-Efficient Fine-Tuning (PEFT) as the leading FSCIL approach, utilizing frozen pre-trained ViT backbones with small learnable prompts injected into self-attention layers. The method employs prototype-based classifiers with rectification mechanisms to address bias from limited samples. Training involves freezing the ViT backbone while updating only prompt parameters and prototypes across base and incremental sessions. Standard benchmarks include MiniImageNet and CUB-200-2011, with performance measured by Average Accuracy (AA), Performance Drop (PD), and Average Harmonic Mean (AHM).

## Key Results
- PEFT approaches achieve state-of-the-art performance with up to 95.27% average accuracy on MiniImageNet
- Prototype rectification mechanisms are essential for mitigating bias from limited samples
- Significant stability-plasticity challenges persist, with up to 60% accuracy gaps between base and novel classes
- Language-guided learning using vision-language models shows promise for enhancing discriminative representations

## Why This Works (Mechanism)

### Mechanism 1: Parameter-Efficient Fine-Tuning (PEFT) with Frozen Backbone
- Claim: Freezing pre-trained backbones while tuning only small prompt parameters appears to reduce catastrophic forgetting in FSCIL settings.
- Mechanism: A pre-trained ViT backbone (86.57M parameters) remains completely frozen. Small learnable prompts (~0.21-0.61M parameters) are injected into the self-attention mechanism via concatenation to query/key/value vectors. This preserves existing feature representations while adapting to new classes.
- Core assumption: The pre-trained backbone contains sufficiently general features that can be repurposed via prompt modulation without weight updates.
- Evidence anchors:
  - [abstract]: "The PEFT approach, utilizing pre-trained vision transformers (ViT) with prompt-based fine-tuning, emerges as particularly effective, achieving up to 95.27% average accuracy on MiniImageNet"
  - [Section 3.5]: "Prompt-based method attaches a small learnable prompts into a frozen pre-trained ViT model (fixed ψ)"
  - [corpus]: Limited independent validation; neighbor papers explore alternatives (Diffusion-FSCIL, MoTiC) rather than PEFT replication
- Break condition: When novel classes diverge significantly from pre-training distribution (cross-domain), frozen features may lack necessary discriminative capacity.

### Mechanism 2: Prototype Rectification for Bias Mitigation
- Claim: Few-shot samples produce biased class prototypes; rectification mechanisms may help approximate true class distributions.
- Mechanism: With limited samples, computed prototypes represent only observed instances. Rectification adjusts prototypes via loss functions, pseudo-prototypes, training-free calibration (weighted similarity to base prototypes), or trainable networks.
- Core assumption: True class distributions can be approximated through rectification despite sample scarcity.
- Evidence anchors:
  - [abstract]: "The survey highlights the importance of prototype rectification mechanisms to mitigate bias from limited samples"
  - [Section 4.4]: "a few observed samples have a high chance of producing a biased prototype i.e. a prototype that only represents the few observed samples but not the whole distribution"
  - [corpus]: "Brain-inspired analogical mixture prototypes" (neighbor, FMR 0.69) supports prototype-based FSCIL approaches
- Break condition: At extreme scarcity (1-shot) or with highly unrepresentative samples, rectification may fail.

### Mechanism 3: Language-Guided Vision-Language Synergy
- Claim: Text embeddings from VLMs may provide complementary discriminative information to visual features.
- Mechanism: Pre-trained VLMs (e.g., CLIP) encode class semantics into text embeddings. These serve as loss anchors and/or classifiers alongside visual prototypes, leveraging linguistic knowledge to compensate for visual data scarcity.
- Core assumption: Language descriptions capture class-discriminative information that correlates with visual features.
- Evidence anchors:
  - [Section 1]: "Language-guided learning offers discriminative representations through encoded text embedding of respective classes by a pre-trained vision-language model (VLM) such as CLIP"
  - [Table 1]: PriViLege and FineFMPL (VL-Prototype) achieve 95.27% and 93.46% AA on MiniImageNet
  - [corpus]: No direct neighbor validation; this remains an emerging direction with limited replication
- Break condition: When visual classes lack clear linguistic descriptions or cross-modal alignment is weak.

## Foundational Learning

- **Concept: Catastrophic Forgetting**
  - Why needed here: FSCIL specifically addresses forgetting when learning new classes from limited samples; understanding baseline forgetting dynamics is essential.
  - Quick check question: Why does standard fine-tuning on new classes degrade performance on previously learned classes?

- **Concept: Stability-Plasticity Dilemma**
  - Why needed here: The paper identifies this as the core unresolved challenge—balancing retention (stability) with new learning (plasticity), evidenced by up to 60% base-to-novel accuracy gaps.
  - Quick check question: What does a large gap between average accuracy and harmonic mean accuracy indicate about a model's stability-plasticity balance?

- **Concept: Prototypical Classification**
  - Why needed here: Prototype-based classifiers are "highly preferable" in FSCIL; understanding nearest-prototype vs MLP classification is fundamental.
  - Quick check question: Why does a prototype-based classifier reduce forgetting risk compared to a network-based classifier that updates each task?

## Architecture Onboarding

- **Component map:**
  - Frozen ViT-B/16 (86.57M) -> Prompt modules (0.21-0.61M) -> Prototype classifier -> Rectification module

- **Critical path:**
  1. Select approach: PEFT (if pre-trained ViT available) or prototype-tuning (if training from scratch)
  2. Implement prototype-based classifier (not network-based) to minimize forgetting
  3. Add rectification: Start with loss-based; escalate to training-free calibration if needed
  4. For PEFT: Choose prompt structure (pool-based L2P, task-wise DualP, growing CODA-P)

- **Design tradeoffs:**
  - PEFT vs Traditional: Lower trainable parameters (0.2M vs 11.7M+) but requires pre-trained backbone
  - Prototype vs Network classifier: Better stability vs potentially richer boundaries
  - Rectification complexity: Simple loss (fast, may underfit) vs trainable network (slower, more capacity)

- **Failure signatures:**
  - Novel class accuracy near 0%: Model ignoring new classes (see L2P/DualP/CodaP base-only behavior in Table 2)
  - Base-to-novel gap >40%: Stability-plasticity failure
  - Performance drop >15% across sessions: Insufficient rectification or overfitting to base classes

- **First 3 experiments:**
  1. Establish baseline: Replicate L2P+ on CUB with ViT-B/16 pre-trained backbone, prototype classifier, loss-based rectification.
  2. Ablate rectification: Compare loss-based vs training-free calibration on novel class accuracy to quantify rectification contribution.
  3. Test stability-plasticity: Report base vs novel accuracy at final task across approaches; target <20% gap for acceptable balance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can prototype rectification mechanisms be advanced beyond simple loss-based calibration to effectively narrow the performance gap (up to 60%) between base and novel classes?
- Basis in paper: [explicit] The authors identify the stability-plasticity dilemma as the most important open challenge, noting that Parameter-Efficient Fine-Tuning (PEFT) methods still suffer from a significant disparity between base and novel class accuracy.
- Why unresolved: Current PEFT methods mostly rely on basic loss-based rectification, which fails to adequately correct prototype bias caused by extreme data scarcity in novel tasks.
- What evidence would resolve it: The development of a rectification mechanism (e.g., using pseudo-prototypes or trainable networks) that consistently maintains a less than 10% accuracy gap between base and novel classes on standard benchmarks.

### Open Question 2
- Question: How does class imbalance in the base training phase impact the generalization and forgetting rates of current Few-Shot Class Incremental Learning (FSCIL) methods?
- Basis in paper: [explicit] The survey lists "imbalance learning" as an overlooked aspect, noting that current FSCIL settings assume balanced base classes, whereas real-world scenarios (e.g., medical data) usually feature long-tailed distributions.
- Why unresolved: Existing methods have primarily been validated against balanced datasets like MiniImageNet and CUB, leaving their robustness against skewed class distributions untested.
- What evidence would resolve it: A comprehensive evaluation of existing FSCIL topologies on a modified benchmark featuring a long-tailed distribution in the base task.

### Open Question 3
- Question: To what extent does the superior performance of PEFT approaches stem from the pre-trained ViT backbone rather than the incremental learning methodology itself?
- Basis in paper: [explicit] The authors raise the issue of "Setting Fairness," pointing out that recent PEFT methods utilize significantly larger backbones (ViT-B/16 with 86M parameters) compared to non-PEFT methods (ResNet18 with 11.7M parameters).
- Why unresolved: Direct comparisons are currently flawed because differences in model capacity and pre-training data confound the isolation of the incremental learning strategy's specific contribution.
- What evidence would resolve it: A rigorous comparative study where PEFT and non-PEFT methods are evaluated using comparable parameter budgets or identical pre-trained backbones.

## Limitations
- Independent replication of specific PEFT methods (PriViLege, ASP, ApproxFSCIL) remains limited
- Survey scope constrained to vision-centric FSCIL with minimal coverage of cross-modal or generative approaches
- Several claims about SOTA performance require verification against updated benchmarks and potential newer methods

## Confidence
- **High confidence**: Core FSCIL problem definition, benchmark datasets (MiniImageNet, CUB), and stability-plasticity dilemma as fundamental challenge
- **Medium confidence**: PEFT approach effectiveness and prototype rectification mechanisms, pending independent replication
- **Low confidence**: Specific performance numbers for newer methods and cross-modal learning capabilities

## Next Checks
1. **Replication Verification**: Independently reproduce PriViLege's 95.27% AA on MiniImageNet using frozen ViT-B/16 with prompt tuning
2. **Cross-Domain Testing**: Evaluate PEFT approach performance when novel classes significantly diverge from pre-training distribution
3. **Rectification Impact**: Quantify the contribution of different rectification mechanisms (loss-based vs training-free calibration) to novel class accuracy