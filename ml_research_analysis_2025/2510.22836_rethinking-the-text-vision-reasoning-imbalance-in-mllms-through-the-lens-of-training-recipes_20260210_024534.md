---
ver: rpa2
title: Rethinking the Text-Vision Reasoning Imbalance in MLLMs through the Lens of
  Training Recipes
arxiv_id: '2510.22836'
source_url: https://arxiv.org/abs/2510.22836
tags:
- training
- reasoning
- visual
- arxiv
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper analyzes and addresses the modality gap in multimodal
  large language models (MLLMs), where models over-rely on textual cues while under-attending
  to visual content. The authors identify that standard training recipes exacerbate
  this gap and propose a two-pronged approach to mitigate it: a curriculum training
  strategy that alternates between text-centric and vision-centric data, and a KL-based
  self-distillation loss that transfers reasoning competence from text-rich to vision-centric
  inputs.'
---

# Rethinking the Text-Vision Reasoning Imbalance in MLLMs through the Lens of Training Recipes

## Quick Facts
- **arXiv ID**: 2510.22836
- **Source URL**: https://arxiv.org/abs/2510.22836
- **Authors**: Guanyu Yao; Qiucheng Wu; Yang Zhang; Zhaowen Wang; Handong Zhao; Shiyu Chang
- **Reference count**: 13
- **Primary result**: MLLMs over-rely on text cues and under-attend to visual content; proposed training recipes balance modality performance

## Executive Summary
This paper addresses a fundamental imbalance in multimodal large language models (MLLMs) where models exhibit stronger reasoning performance on text-only inputs compared to vision-centric multimodal inputs. The authors identify that standard training recipes exacerbate this modality gap by implicitly encouraging over-reliance on textual information. They propose a two-pronged approach: a curriculum training strategy that alternates between text-rich and vision-centric data, and a KL-based self-distillation loss that transfers reasoning competence from text to vision. Their experiments demonstrate that this combined approach effectively narrows the modality gap while preserving overall reasoning performance.

## Method Summary
The authors tackle the text-vision reasoning imbalance through two complementary mechanisms. First, they introduce curriculum training that progressively shifts from text-centric to vision-centric data, allowing the model to first learn reasoning capabilities in text-rich environments before applying them to more challenging visual scenarios. Second, they implement a KL-based self-distillation loss that encourages the model to transfer reasoning competence from text-rich to vision-centric inputs. This self-distillation approach uses the model's own predictions on text-rich data as soft targets for training on vision-centric inputs, effectively bridging the reasoning gap between modalities.

## Key Results
- Curriculum training strategy effectively balances performance across text and vision modalities
- KL-based self-distillation successfully transfers reasoning competence from text-rich to vision-centric inputs
- Combined approach narrows the modality gap while maintaining overall reasoning performance
- Vision-centric performance improves significantly without degrading text-only reasoning

## Why This Works (Mechanism)
The approach works by addressing the fundamental training imbalance that leads MLLMs to over-rely on textual cues. The curriculum strategy gradually exposes the model to increasingly visual scenarios after establishing strong reasoning foundations in text-rich contexts. The KL-based self-distillation creates a knowledge transfer pathway that allows the model to leverage its stronger text-based reasoning capabilities when processing visual information. Together, these mechanisms systematically reduce the modality gap by both preventing over-reliance on text during training and explicitly transferring reasoning competence across modalities.

## Foundational Learning
- **Multimodal training data composition**: Understanding how vision-text ratios affect model learning; quick check: verify data generation follows controlled vision-text distributions
- **KL divergence in self-distillation**: Measures distribution similarity between text-rich and vision-centric predictions; quick check: monitor KL loss convergence during training
- **Curriculum learning principles**: Progressive difficulty adjustment in training sequences; quick check: validate curriculum schedule effectiveness on small-scale experiments
- **Modality attention mechanisms**: How MLLMs distribute focus between visual and textual inputs; quick check: analyze attention patterns across different input types
- **Reasoning transfer learning**: Knowledge transfer across different input modalities; quick check: measure performance consistency across reasoning tasks
- **Synthetic data generation for MLLMs**: Creating controlled multimodal datasets; quick check: validate synthetic data diversity and quality

## Architecture Onboarding

**Component map**: Data Generator -> Curriculum Scheduler -> MLLM Backbone -> KL Self-Distillation Module -> Performance Evaluator

**Critical path**: Data generation with controlled vision-text ratios → curriculum scheduling → model training with KL self-distillation → performance evaluation across modalities

**Design tradeoffs**: 
- Balancing synthetic data complexity versus real-world applicability
- Trade-off between curriculum duration and overall training efficiency
- KL temperature selection affecting knowledge transfer effectiveness
- Computational overhead of self-distillation versus performance gains

**Failure signatures**:
- Over-reliance on text persists despite training interventions
- KL loss divergence indicating unstable knowledge transfer
- Curriculum schedule mismatch causing degraded performance
- Performance degradation in either modality during balancing

**3 first experiments**:
1. Baseline evaluation of text-only vs vision-centric performance gap
2. Single-stage curriculum training effectiveness assessment
3. KL self-distillation ablation study with fixed curriculum

## Open Questions the Paper Calls Out
None identified in the provided materials.

## Limitations
- Synthetic data generation may not fully capture real-world multimodal distribution characteristics
- Limited ablation studies on the relative contributions of curriculum training versus self-distillation
- No evaluation of model generalization to out-of-distribution multimodal scenarios

## Confidence
- **High confidence**: The identification of modality imbalance as a systematic issue in standard MLLM training approaches
- **Medium confidence**: The effectiveness of the proposed curriculum training strategy in balancing performance across modalities
- **Medium confidence**: The utility of KL-based self-distillation for transferring reasoning competence from text to vision

## Next Checks
1. Evaluate the proposed methods on real-world multimodal datasets with naturally occurring vision-text distributions
2. Conduct ablation studies to isolate the individual contributions of curriculum training and KL-based self-distillation
3. Test model robustness across varying levels of visual complexity and text-image alignment quality