---
ver: rpa2
title: 'nvBench 2.0: Resolving Ambiguity in Text-to-Visualization through Stepwise
  Reasoning'
arxiv_id: '2503.12880'
source_url: https://arxiv.org/abs/2503.12880
tags:
- data
- visualization
- ambiguity
- query
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: nvBench 2.0 is a benchmark for evaluating Text-to-Visualization
  systems on ambiguous natural language queries. It includes 7,878 queries and 24,076
  visualizations derived from 780 tables across 153 domains.
---

# nvBench 2.0: Resolving Ambiguity in Text-to-Visualization through Stepwise Reasoning

## Quick Facts
- arXiv ID: 2503.12880
- Source URL: https://arxiv.org/abs/2503.12880
- Authors: Tianqi Luo; Chuhan Huang; Leixian Shen; Boyan Li; Shuyu Shen; Wei Zeng; Nan Tang; Yuyu Luo
- Reference count: 40
- nvBench 2.0 is a benchmark for evaluating Text-to-Visualization systems on ambiguous natural language queries. It includes 7,878 queries and 24,076 visualizations derived from 780 tables across 153 domains. A controlled ambiguity-injection pipeline generates queries with multiple valid interpretations, each traceable through step-wise reasoning paths. Step-Text2Vis, a model trained on nvBench 2.0 using step-wise preference optimization, achieves state-of-the-art performance with F1@3 of 81.50% and F1@5 of 80.88%, outperforming baselines like GPT-4o by over 20%.

## Executive Summary
nvBench 2.0 addresses the critical challenge of ambiguous natural language queries in Text-to-Visualization (Text2VIS) tasks, where a single query can map to multiple valid visualizations. The benchmark introduces a controlled ambiguity-injection pipeline that generates queries with traceable step-wise reasoning paths, enabling systematic evaluation of ambiguity resolution. The pipeline converts seed visualizations to Abstract Syntax Trees (ASTs), injects ambiguity through semantic alias discovery and implicit node addition, and uses Answer Set Programming (ASP) to exhaustively enumerate valid interpretations. Step-Text2Vis, a fine-tuned model using step-wise preference optimization, demonstrates state-of-the-art performance by achieving F1@3 of 81.50% and F1@5 of 80.88%, outperforming strong baselines by over 20%. This work advances the ability to handle ambiguity in Text2VIS tasks through systematic enumeration and step-level supervision.

## Method Summary
nvBench 2.0 employs a three-step pipeline to generate ambiguous Text2VIS queries with traceable reasoning paths. First, seed visualizations from existing datasets are converted to ASTs and transformed into ambiguity-aware trees by injecting ambiguous nodes (multi-choice options via ConceptNet + LLM refinement) and implicit nodes (unspecified but required components). Second, an ASP solver (Clingo) encodes these trees as choice rules and integrity constraints, exhaustively enumerating all valid visualizations as stable models. Third, an LLM synthesizes ambiguous natural language queries from the ambiguity-aware trees, which are verified for completeness and type preservation. Step-Text2Vis, trained on this data using two-stage fine-tuning (SFT cold-start followed by Step-DPO preference optimization), achieves state-of-the-art performance by optimizing step-level reasoning decisions.

## Key Results
- Step-Text2Vis achieves F1@3 of 81.50% and F1@5 of 80.88% on nvBench 2.0 test set
- Outperforms GPT-4o by 22.54% in F1@3 and Qwen2.5-7B-SFT by 11.35% in F1@5
- Demonstrates significant performance gains across all ambiguity types (DT, CM, DS, CT) and chart types
- Step-wise preference optimization provides consistent improvements over standard SFT training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing visualization intent into an ambiguity-aware tree enables systematic enumeration of valid interpretations.
- Mechanism: Seed visualizations are converted to Abstract Syntax Trees (ASTs) encoding components (data mappings, mark types, channels). Nodes are labeled as explicit, ambiguous, or implicit. Ambiguity is injected by: (1) replacing explicit nodes with multi-choice ambiguous nodes using semantic alias discovery (ConceptNet + LLM refinement), and (2) adding implicit nodes for unspecified but required components.
- Core assumption: Visualization grammar can be fully captured as a tree structure where each node's ambiguity type is classifiable.
- Evidence anchors:
  - [section] Section 2.1, Step 1 describes the VIS Tree Synthesis process converting seed VIS to AST with ambiguity node injection.
  - [section] Appendix B.1 formalizes: T = {A | A = [a₁, a₂, ..., aₜ]} where each node aᵢ = (τ, op, params) with τ ∈ {explicit, ambiguous, implicit}.
  - [corpus] Weak direct evidence in corpus; related work "DeepVIS" also uses step-wise reasoning but without the tree formalism.
- Break condition: Fails if visualization intent cannot be decomposed into discrete, classifiable nodes (e.g., highly creative or domain-specific visual encodings).

### Mechanism 2
- Claim: Answer Set Programming (ASP) provides exhaustive, constraint-compliant enumeration of valid visualizations.
- Mechanism: The ambiguity-aware tree T′ is encoded as ASP rules. Ambiguous nodes become choice rules (exactly one option selected). Implicit nodes become placeholders. Hard constraints enforce visualization grammar (e.g., "temporal fields must be binned"). The ASP solver enumerates all stable models—each representing a valid visualization.
- Core assumption: All valid visualizations can be expressed as stable models satisfying declared constraints; no valid interpretation exists outside the constraint system.
- Evidence anchors:
  - [section] Section 2.1, Step 2 states the ASP solver "applies grammar constraints to transform the ambiguous tree into a resolved VIS set."
  - [section] Appendix B.2 details ASP encoding: choice rules for ambiguous nodes, integrity constraints for hard rules, and completeness guarantees via exhaustive enumeration.
  - [corpus] No corpus papers use ASP for this purpose; this appears novel to nvBench 2.0.
- Break condition: Fails if grammar constraints are incomplete or overly restrictive, excluding valid real-world visualizations.

### Mechanism 3
- Claim: Step-wise preference optimization improves ambiguous Text2VIS by reinforcing correct intermediate reasoning steps.
- Mechanism: Step-Text2Vis is fine-tuned in two stages: (1) SFT cold-start to learn structured step-wise output format; (2) Step-DPO training where preference pairs are constructed by identifying the first erroneous step in model outputs vs. ground truth. The loss maximizes probability of correct next step s_win given previous correct steps, and minimizes probability of incorrect step s_lose.
- Core assumption: Errors in visualization reasoning are localized to specific steps; correcting step-level decisions propagates to correct final outputs.
- Evidence anchors:
  - [section] Section 3.1 formalizes the Step-DPO loss function (Equation 1).
  - [section] Section 4.2 reports Step-Text2Vis achieves F1@3 of 81.50%, outperforming GPT-4o-Step by 22.54%.
  - [corpus] "Aligning Text, Code, and Vision" uses reinforcement learning for Text2VIS but not step-wise preference optimization.
- Break condition: Fails if reasoning errors are not step-localizable (e.g., global coherence issues across steps).

## Foundational Learning

- Concept: **Abstract Syntax Trees (ASTs) for visualization grammar**
  - Why needed here: The pipeline represents visualizations as trees where each node corresponds to a construction action (mark type, encoding channel, data transformation). Understanding AST structure is prerequisite to comprehending ambiguity injection.
  - Quick check question: Given a bar chart with X=Date (binned by year), Y=AVG(Sales), Color=Region, can you sketch the AST nodes and their relationships?

- Concept: **Answer Set Programming (ASP) and stable model semantics**
  - Why needed here: ASP is used to exhaustively enumerate valid visualizations. You must understand choice rules, integrity constraints, and how stable models represent valid solutions.
  - Quick check question: Write an ASP choice rule that selects exactly one column from {World_Gross, Local_Gross} for an ambiguous Y-channel.

- Concept: **Direct Preference Optimization (DPO) and step-wise variants**
  - Why needed here: Step-Text2Vis uses Step-DPO to optimize reasoning at the step level. Understanding DPO's objective (maximizing log-likelihood ratio of preferred vs. dispreferred outputs) is essential.
  - Quick check question: In Step-DPO, how does the training signal differ when the error occurs at step 2 vs. step 4 of a 5-step reasoning chain?

## Architecture Onboarding

- Component map:
  Data Preparation -> Ambiguity Metadata Generator -> VIS Tree Synthesizer -> ASP Solver -> Query Generator -> Query Verifier -> Reasoning Path Generator -> Step-Text2Vis Training Pipeline

- Critical path:
  1. Seed VIS -> AST conversion (must preserve all design decisions)
  2. Ambiguity injection -> T′ (controlled ambiguity levels 2-5)
  3. ASP solving -> Valid VIS enumeration (completeness depends on constraint coverage)
  4. Query synthesis + verification -> Ambiguous NL query
  5. Step-wise reasoning generation -> Training data for Step-Text2Vis

- Design tradeoffs:
  - **Ambiguity level cap (k≤5)**: Balances dataset manageability against real-world complexity. Higher levels increase combinatorial explosion and evaluation difficulty.
  - **LLM-based vs. rule-based generation**: LLMs (GPT-4o-mini) provide query diversity but introduce ~5% unintended facts requiring verification.
  - **ASP vs. neural enumeration**: ASP guarantees completeness and correctness but requires manual constraint encoding; neural approaches are more flexible but lack guarantees.
  - **Step-DPO vs. standard DPO**: Step-DPO provides finer-grained supervision but requires structured step-wise ground truth.

- Failure signatures:
  - **Over-generalized data selection**: Model selects irrelevant columns (e.g., treating "store_name" as an ID column).
  - **Incorrect aggregation inference**: Applying COUNT(*) when data column already contains aggregated values.
  - **Invalid chart-type-task mapping**: Selecting boxplot for single-value-per-category distributions.
  - **Channel mapping violations**: Mapping temporal field to color in bar charts instead of X-axis.

- First 3 experiments:
  1. **Baseline comparison**: Evaluate GPT-4o-mini, GPT-4o, Claude-3.5-Haiku, Qwen2.5-7B with direct prompting vs. step prompting on nvBench 2.0 test set. Report P@K, R@K, F1@K for K∈{1,3,5}.
  2. **Ablation on ambiguity types**: Isolate performance on each ambiguity type (DT, CM, DS, CT) to identify which types are most challenging. Expect DT (50.55% of data) to dominate error patterns.
  3. **Step-DPO vs. SFT-only**: Train Qwen2.5-7B with SFT only vs. SFT+Step-DPO. Measure recall improvement at higher ambiguity levels (k=4,5) where Step-Text2Vis shows largest gains.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the step-wise reasoning and preference optimization approach from Step-Text2Vis be adapted to handle ambiguous Text2SQL queries, or unified within a framework that jointly resolves visualization and SQL ambiguity?
- Basis in paper: [explicit] Section F (Limitations) states: "Although our benchmark focuses on Text2VIS ambiguity resolution, it does not extend to related domains such as Text2SQL... Future work could explore the integration of both Text2VIS and Text2SQL ambiguity resolution within a unified framework."
- Why unresolved: nvBench 2.0 is designed exclusively for visualization outputs, and the ASP solver encodes visualization-specific grammar constraints, so cross-domain applicability remains untested.
- What evidence would resolve it: A unified ambiguity benchmark covering both Text2VIS and Text2SQL tasks, with experiments showing whether Step-DPO improves SQL generation under ambiguity comparably to visualization tasks.

### Open Question 2
- Question: To what extent does the synthetic ambiguity-injection pipeline capture the distribution and nature of ambiguity encountered in real-world user queries?
- Basis in paper: [inferred] The pipeline generates ambiguous queries through controlled injection starting from unambiguous seed visualizations (Section 2.1), but no comparison to human-generated ambiguous queries validates ecological validity.
- Why unresolved: The benchmark relies entirely on programmatically injected ambiguity types rather than empirically observed user ambiguity patterns.
- What evidence would resolve it: A comparative study correlating model performance on nvBench 2.0 with performance on a benchmark of naturally occurring ambiguous user queries from real visualization tools.

### Open Question 3
- Question: Can adaptive reasoning strategies that dynamically select different reasoning paths based on query characteristics and target chart type outperform the fixed five-step process used in Step-Text2Vis?
- Basis in paper: [explicit] Section 4.2 (Key Implications) states: "The observed performance variations across different chart types and ambiguity levels suggest that future Text2VIS systems should adaptively select reasoning strategies based on both the queries' characteristics and the target vis type."
- Why unresolved: Step-Text2Vis uses a uniform step-wise process regardless of query or chart type, yet performance varies significantly (e.g., Line charts consistently score lower; higher ambiguity levels cause degradation).
- What evidence would resolve it: Experiments comparing fixed step-wise reasoning against query-conditioned adaptive reasoning across chart types and ambiguity levels, demonstrating measurable gains.

### Open Question 4
- Question: How does incorporating conversational context from prior dialogue turns affect ambiguity resolution accuracy compared to the single-query setting in nvBench 2.0?
- Basis in paper: [explicit] Section F (Limitations) states: "nvBench 2.0 evaluates standalone queries without considering the conversational context in which they might appear... The benchmark does not account for these contextual dependencies."
- Why unresolved: Real visualization systems operate in multi-turn interactions, but nvBench 2.0 treats each query in isolation.
- What evidence would resolve it: A multi-turn extension of nvBench 2.0 with dialogue history, and experiments showing whether context-aware models outperform context-agnostic baselines on ambiguous queries.

## Limitations
- **Controlled ambiguity vs. natural ambiguity**: The benchmark relies on synthetically injected ambiguity rather than naturally occurring ambiguous queries from real users, potentially limiting real-world applicability.
- **Constraint system completeness**: ASP-based enumeration guarantees completeness only within the declared grammar constraints, which may not capture all valid visualization possibilities.
- **Single-query evaluation**: The benchmark evaluates standalone queries without considering conversational context or multi-turn interactions common in real visualization systems.

## Confidence

- **High Confidence**: The dataset construction methodology (AST-based ambiguity injection + ASP enumeration) is well-specified and reproducible. The step-wise reasoning formalization and Step-DPO training procedure are clearly defined.
- **Medium Confidence**: The reported performance gains (F1@3 of 81.50%, F1@5 of 80.88%) are convincing but depend on the quality of ground truth step-wise reasoning paths. The assumption that errors are step-localizable may not hold for all visualization tasks.
- **Low Confidence**: The real-world impact of handling controlled ambiguity vs. natural ambiguity in user queries. The dataset's domain coverage (153 domains from 780 tables) may not represent the full diversity of visualization use cases.

## Next Checks

1. **Ground Truth Verification**: Conduct inter-annotator agreement study on step-wise reasoning paths for a subset of queries to validate the consistency and correctness of the automated reasoning path generation pipeline.

2. **Natural Ambiguity Testing**: Evaluate Step-Text2Vis on naturally occurring ambiguous queries from sources like BIRD or real user logs, comparing performance against synthetic nvBench 2.0 queries to assess real-world applicability.

3. **Constraint System Completeness**: Systematically test ASP solver coverage by attempting to generate visualizations that violate current grammar constraints (e.g., novel chart types, unconventional encoding channels) to identify gaps in the constraint system.