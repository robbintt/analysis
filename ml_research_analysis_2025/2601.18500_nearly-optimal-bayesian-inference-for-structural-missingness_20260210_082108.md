---
ver: rpa2
title: Nearly Optimal Bayesian Inference for Structural Missingness
arxiv_id: '2601.18500'
source_url: https://arxiv.org/abs/2601.18500
tags:
- missing
- missingness
- posterior
- data
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles Bayesian inference under structural missingness,
  where values can be logically undefined and masks depend on observed/unobserved
  variables and other missingness indicators. The proposed approach uses Prior-Fitted
  Networks (PFNs) to directly learn the posterior predictive distribution from incomplete
  data, and optionally employs Flow Matching to model the posterior over missing values.
---

# Nearly Optimal Bayesian Inference for Structural Missingness

## Quick Facts
- arXiv ID: 2601.18500
- Source URL: https://arxiv.org/abs/2601.18500
- Reference count: 40
- Achieves state-of-the-art results on 43 classification and 15 imputation benchmarks with near-Bayes-optimality guarantees

## Executive Summary
This paper addresses Bayesian inference under structural missingness, where values can be logically undefined and masks depend on observed/unobserved variables and other missingness indicators. The authors propose using Prior-Fitted Networks (PFNs) to directly learn the posterior predictive distribution from incomplete data, optionally employing Flow Matching to model the posterior over missing values. This decoupled approach enables posterior integration and avoids plug-in imputation bias and MNAR bias, achieving state-of-the-art performance across both low and high missingness regimes.

## Method Summary
The framework uses PFNs to learn the posterior predictive distribution directly from incomplete data, bypassing traditional imputation approaches. The method decouples learning the missing-value posterior from label prediction, allowing for efficient posterior integration. Optionally, Flow Matching is employed to model the posterior over missing values, further improving performance. The approach is theoretically grounded with near-Bayes-optimality guarantees and demonstrates superior empirical performance on extensive benchmark datasets.

## Key Results
- Achieves state-of-the-art performance on 43 classification and 15 imputation benchmarks
- PFN-Flow ranks first in both low and high missingness regimes
- Decoupled design shown to be more sample-efficient than end-to-end learning in limited-context settings
- Outperforms strong baselines including TabPFN and PFN-NSM

## Why This Works (Mechanism)
The decoupled architecture separates posterior learning from prediction, enabling more efficient sample usage and avoiding biases from plug-in imputation. By directly modeling the posterior predictive distribution rather than imputing missing values, the method sidesteps the MNAR bias that plagues traditional approaches.

## Foundational Learning
- **Prior-Fitted Networks (PFNs)**: Neural networks trained to approximate posterior distributions; needed to bypass explicit imputation and achieve Bayesian consistency; quick check: verify PFN can approximate posteriors on synthetic missingness patterns
- **Structural Missingness**: Missingness determined by logical relationships between variables; needed to handle realistic scenarios where data can be undefined; quick check: test on data with missingness depending on variable combinations
- **Flow Matching**: Technique for modeling complex posterior distributions; needed to accurately represent the posterior over missing values; quick check: compare posterior quality with/without flow matching on controlled datasets
- **Second-Order Structural Causal Models (SCMs)**: Causal framework with probabilistic mechanisms; needed for theoretical guarantees of near-Bayes-optimality; quick check: verify theoretical assumptions hold on synthetic SCM-generated data
- **Decoupled Inference**: Separate learning of missing-value posterior and prediction; needed for sample efficiency in limited-context settings; quick check: ablate by comparing to end-to-end architecture on small datasets

## Architecture Onboarding

**Component Map**
PFN -> Flow Matching -> Posterior Predictive Distribution

**Critical Path**
1. Input data with structural missingness
2. PFN learns approximate posterior predictive distribution
3. (Optional) Flow Matching refines posterior over missing values
4. Posterior predictive distribution used for downstream tasks

**Design Tradeoffs**
- Decoupling enables sample efficiency but adds architectural complexity
- Flow Matching improves posterior quality but increases computational overhead
- Theoretical guarantees require SCM assumptions that may not hold in practice

**Failure Signatures**
- Poor performance when true data-generating process deviates substantially from SCM prior
- Computational bottlenecks with Flow Matching on large datasets
- Degraded accuracy when structural missingness patterns are highly complex

**First Experiments**
1. Ablation study comparing decoupled vs. end-to-end architectures on datasets with varying context sizes
2. Robustness test on synthetic data where missingness patterns change over time
3. Scalability benchmark measuring Flow Matching efficiency on datasets >1M samples

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the method perform under prior misspecification when the true data-generating process deviates substantially from the second-order SCM prior?
- Basis in paper: [explicit] Theorem 6.1(iii) acknowledges the misspecified case where the PFN learns "the best approximation in Φ in the above expected conditional KL sense," but does not characterize how performance degrades as prior mismatch grows.
- Why unresolved: The theoretical guarantees assume the SCM prior is well-specified or provide only best-approximation bounds; empirical evaluation uses real datasets but does not systematically vary the gap between prior assumptions and ground truth.
- What evidence would resolve it: Controlled experiments where data is generated from known distributions outside the SCM prior family, measuring performance degradation as a function of quantifiable prior misspecification (e.g., KL divergence between true and prior-supported mechanisms).

### Open Question 2
- Question: Can the finite-sample near-Bayes-optimality guarantees be extended to settings where the Markov structure itself is unknown and must be learned?
- Basis in paper: [explicit] Section 4.1 states the method assumes "a second-order SCM with randomized missingness mechanisms but a fixed Markov structure"; Section 1 notes that traditional SCMs "rely on independence testing assumptions and scale poorly."
- Why unresolved: The current framework amortizes inference within a fixed causal structure, leaving open whether structure uncertainty can be incorporated without sacrificing the sample-complexity advantages proved in Theorem 6.7.
- What evidence would resolve it: Theoretical analysis extending Theorem 6.1 to a prior over graph structures, plus experiments comparing fixed-structure vs. structure-learning variants on data with unknown causal graphs.

### Open Question 3
- Question: What are the precise conditions under which the decoupled PFN + Flow Matching architecture outperforms end-to-end training in practice?
- Basis in paper: [explicit] Theorem 6.7 provides a "sample complexity advantage of decoupled inference" using an "effective complexity proxy" with coupling penalty L_cpl, stating the advantage holds "when L_g is large and context is limited (large L_cpl)."
- Why unresolved: The theorem uses asymptotic proxy assumptions (N(L,ε) = O((L/ε)^d)) and does not specify concrete thresholds or how to measure L_g, L_cpl empirically; empirical section does not ablate this design choice.
- What evidence would resolve it: Ablation studies comparing decoupled vs. end-to-end architectures across tasks with systematically varied context sizes and missingness complexity, plus empirical estimation of the coupling penalty from training dynamics.

## Limitations
- Theoretical guarantees assume well-specified SCM prior, with limited characterization of misspecification effects
- Computational overhead of Flow Matching may limit scalability to very large datasets
- Empirical validation relies on benchmark datasets that may not fully capture real-world structural missingness complexity

## Confidence

**Major Claim Clusters:**
- **Bayesian Inference Framework**: High - Well-established theoretical foundation
- **State-of-the-Art Performance**: Medium - Impressive results but dependent on specific baselines and metrics
- **Sample Efficiency**: Medium - Claim needs further validation across diverse problem domains

## Next Checks
1. **Robustness Testing**: Evaluate the framework on datasets with varying degrees of structural missingness complexity, including cases where missingness patterns change over time or across subpopulations.
2. **Scalability Assessment**: Benchmark the computational efficiency of the Flow Matching component on large-scale datasets (e.g., >1M samples) to assess practical applicability.
3. **Generalization Study**: Test the model's performance on out-of-distribution data and tasks not seen during meta-training to evaluate the robustness of the learned posterior predictive distribution.