---
ver: rpa2
title: 'ICH-Qwen: A Large Language Model Towards Chinese Intangible Cultural Heritage'
arxiv_id: '2505.23831'
source_url: https://arxiv.org/abs/2505.23831
tags:
- cultural
- data
- heritage
- intangible
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ICH-Qwen, a large language model designed
  specifically for Chinese intangible cultural heritage (ICH). It addresses the challenge
  of preserving and disseminating ICH, which is threatened by modernization.
---

# ICH-Qwen: A Large Language Model Towards Chinese Intangible Cultural Heritage

## Quick Facts
- arXiv ID: 2505.23831
- Source URL: https://arxiv.org/abs/2505.23831
- Reference count: 21
- This paper introduces ICH-Qwen, a large language model designed specifically for Chinese intangible cultural heritage (ICH).

## Executive Summary
This paper introduces ICH-Qwen, a large language model designed specifically for Chinese intangible cultural heritage (ICH). It addresses the challenge of preserving and disseminating ICH, which is threatened by modernization. The model was trained on a substantial corpus of open-source Chinese ICH data and fine-tuned with synthetic data. The authors developed knowledge entity annotation and part-of-speech tagging schemes tailored for ICH. The model was evaluated on three tasks: knowledge Q&A, context-aware knowledge Q&A, and terminology interpretation. ICH-Qwen achieved superior performance compared to several state-of-the-art models, with ROUGE-1-F scores of 25.04, 37.21, and 23.82, and BLEU-4 scores of 6.32, 11.99, and 8.41 for the respective tasks. The study demonstrates the potential of LLMs for intelligent ICH conservation and digital humanities research.

## Method Summary
ICH-Qwen is built on Qwen2.5-7B-Chat through continued pre-training on a 49 million token corpus of Chinese ICH data from authoritative sources, followed by instruction fine-tuning using LoRA on synthetic Q&A pairs generated by Qwen2-72B-Instruct. The training pipeline includes manual verification of synthetic data to ensure cultural accuracy. The model is evaluated on three domain-specific tasks using ROUGE and BLEU metrics, demonstrating superior performance compared to baselines like Llama3.1-8B and GLM4-9B-Chat.

## Key Results
- ICH-Qwen achieves ROUGE-1-F scores of 25.04, 37.21, and 23.82 on Knowledge Q&A, Context-aware Q&A, and Terminology Interpretation tasks respectively
- The model outperforms baselines including Llama3.1-8B, GLM4-9B-Chat, and others across all three evaluation tasks
- Context-aware prompting significantly improves performance, with a 48.6% improvement in ROUGE-1-F compared to standard Q&A

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Continued pre-training on a curated, domain-specific corpus injects specialized lexical and semantic knowledge into a general-purpose LLM.
- Mechanism: By exposing the base model (Qwen2.5-7B) to 49 million tokens of authoritative ICH text (policies, inventories, academic abstracts), the model updates its weights to minimize loss on domain-specific n-grams and entity relationships, effectively aligning its latent space with ICH concepts.
- Core assumption: The base model possesses sufficient general reasoning capabilities, and the domain corpus is clean and representative enough to shift probability distributions without catastrophic forgetting.
- Evidence anchors:
  - [abstract] Mentions utilizing a "substantial corpus of open-source Chinese ICH data" and "continued pre-training."
  - [section 3.2.1] Details the 49 million-token dataset sourced from the China Intangible Cultural Heritage Network and academic databases.
  - [corpus] Direct architectural validation is weak in immediate neighbors; however, related work like "Fusing Bidirectional Chains..." supports LLM efficacy in this domain.
- Break condition: If the domain corpus contains significant noise or contradictory definitions (e.g., conflicting regional ICH accounts), the model may suffer from semantic drift or increased hallucination rates.

### Mechanism 2
- Claim: Synthetic instruction data generated by a larger "teacher" model (Qwen2-72B) effectively aligns the "student" model (ICH-Qwen) for downstream conversational tasks.
- Mechanism: The teacher model transforms raw ICH domain texts into structured Q&A pairs (instruction-output). The student model is then fine-tuned on this synthetic data via LoRA (Low-Rank Adaptation), optimizing it to follow instructions while retaining the specific knowledge injected during pre-training.
- Core assumption: The teacher model (Qwen2-72B) accurately extracts key information and generates high-quality instruction pairs without introducing hallucinations that the student model would blindly learn.
- Evidence anchors:
  - [abstract] States the model employs "natural language understanding... augmented with synthetic data."
  - [section 3.3] Describes using Qwen2-72B-Instruct to generate question-answer pairs from ICH texts, followed by manual verification.
  - [corpus] Corpus neighbors focus more on application (VR/Design) than synthetic data pipelines; validation relies primarily on the paper's internal reporting.
- Break condition: If the synthetic generation prompt allows the teacher model to "hallucinate" details not present in the source text, the student model will reinforce factual errors.

### Mechanism 3
- Claim: Context-aware prompting significantly improves retrieval accuracy and factual grounding compared to zero-shot knowledge retrieval.
- Mechanism: Providing the model with a reference snippet (context) constrains the generation space, forcing the model to extract and synthesize answers from the provided evidence rather than relying solely on parametric memory. This is evidenced by the performance lift in context-aware tasks.
- Core assumption: The model's attention mechanism can effectively prioritize the provided context window over its internal pre-trained weights (i.e., it has learned to attend to context).
- Evidence anchors:
  - [section 4.4] Reports a performance jump in ROUGE-1-F from 25.04 (Knowledge Q&A) to 37.21 (Context-aware Q&A), a 48.6% improvement attributed to "contextual cues."
  - [table 2] Shows the input format for Context-aware Q&A explicitly demanding the output "strictly derives from the given material."
  - [corpus] Not explicitly detailed in corpus neighbors.
- Break condition: If the retrieved context is irrelevant or if the context window exceeds the model's effective attention span, performance may degrade below standard zero-shot levels due to distraction.

## Foundational Learning

- Concept: **Low-Rank Adaptation (LoRA)**
  - Why needed here: The paper fine-tunes a 7B parameter model. Full fine-tuning is computationally expensive. LoRA allows efficient adaptation by freezing pre-trained weights and injecting trainable rank-decomposition matrices.
  - Quick check question: Can you explain why LoRA prevents "catastrophic forgetting" better than full parameter fine-tuning in this specific domain-adaptation context?

- Concept: **Synthetic Data Generation (SDG)**
  - Why needed here: High-quality, instruction-following data for niche domains like ICH is scarce. SDG uses a stronger model to bootstrap training data for the smaller, specialized model.
  - Quick check question: What are the risks of "model collapse" when training a model on data generated by another model, and how does the paper attempt to mitigate this?

- Concept: **ROUGE/BLEU Metrics**
  - Why needed here: The paper relies on these metrics to claim "state-of-the-art" performance. Understanding that ROUGE focuses on recall (coverage) and BLEU on precision (n-gram accuracy) is vital for interpreting the results.
  - Quick check question: Why might a high BLEU score still result in a response that is factually incorrect or culturally insensitive in the context of Intangible Cultural Heritage?

## Architecture Onboarding

- Component map:
  Data Layer: China ICH Network (Policies/News) + Academic Abstracts
  -> Processing Layer: Rule-based cleaning -> Knowledge Entity Annotation (Manual/Expert) -> Synthetic Generation (Qwen2-72B)
  -> Model Layer: Base (Qwen2.5-7B) -> Continued Pre-training -> Instruction Fine-tuning (LoRA)
  -> Evaluation: 3-task benchmark (Knowledge Q&A, Context Q&A, Terminology)

- Critical path: The **Manual Verification of Synthetic Data** (Section 3.3). The pipeline relies on Qwen2-72B to generate Q&A pairs, but quality is gated by human review to ensure "authentic reflection of ICH culture."

- Design tradeoffs:
  - **Base Model Size:** Selected Qwen2.5-7B over larger models (like the 72B teacher) likely for deployment efficiency in resource-constrained digital humanities settings, trading off some raw reasoning power.
  - **Metric Selection:** Reliance on lexical overlap (ROUGE/BLEU) trades off semantic nuance measurement; the paper admits these metrics focus on "character-level overlap" rather than cultural deep understanding.

- Failure signatures:
  - **Low ROUGE-1-F on Terminology:** Indicates the model is failing to align specific ICH terms with their definitions.
  - **Performance Drop in Context-Aware Mode:** Would indicate the model is failing to attend to the provided context window.
  - **Llama3.1-8B Underperformance:** The paper notes Llama3.1-8B failed significantly; if you replicate this, check for tokenization mismatches or lack of Chinese pre-training corpus in the baseline.

- First 3 experiments:
  1. **Baseline Validation:** Reproduce the "Knowledge Q&A" benchmark using the provided prompts (Table 2) against the base Qwen2.5-7B model to quantify the delta introduced by the ICH-specific training.
  2. **Context Ablation:** Run the Context-aware Q&A task with *irrelevant* context to test if the model is actually reading the context or just using the question (distractor testing).
  3. **Entity Extraction Test:** Input raw ICH text and verify if the model correctly identifies `<ICH-TITLE>`, `<ICH-PLACE>`, and `<ICH-TERM>` tags as described in the annotation framework (Section 3.2.2).

## Open Questions the Paper Calls Out
- How can multimodal data (e.g., speech, video) be effectively integrated into ICH-specific LLMs to capture the non-textual aspects of heritage?
- Can the model be effectively adapted to handle the diverse range of dialects and minority languages associated with specific ICH practices?
- Do high n-gram overlap scores (ROUGE/BLEU) correlate with the factual accuracy and cultural authenticity required for reliable ICH dissemination?

## Limitations
- The paper relies heavily on synthetic data generation without providing rejection rates or comprehensive quality metrics
- Performance is measured using ROUGE and BLEU metrics that focus on lexical overlap rather than semantic understanding or cultural appropriateness
- Critical hyperparameters for LoRA and continued pre-training are not specified, limiting reproducibility

## Confidence
- **Performance Claims (High):** The ROUGE and BLEU scores are explicitly reported with methodology. The relative performance gaps between ICH-Qwen and baselines are clearly demonstrated.
- **Mechanism Claims (Medium):** The paper describes the pre-training and fine-tuning pipeline, but lacks ablation studies to isolate the contribution of each component (domain corpus vs. synthetic data vs. LoRA).
- **Cultural Appropriateness Claims (Low):** The paper does not evaluate whether the model's responses are culturally sensitive or accurate beyond lexical matching, despite this being critical for ICH applications.

## Next Checks
1. **Synthetic Data Audit:** Sample 50 synthetic Q&A pairs from the final training set and verify whether they accurately reflect the source ICH texts without hallucination or fabrication.
2. **Human Evaluation Study:** Recruit domain experts to rate a subset of ICH-Qwen responses on cultural accuracy, sensitivity, and completeness, comparing against baseline models.
3. **Ablation Experiment:** Train three variants of the model: (a) base Qwen2.5-7B, (b) continued pre-training only, and (c) instruction fine-tuning only, to quantify the individual contribution of each training phase.