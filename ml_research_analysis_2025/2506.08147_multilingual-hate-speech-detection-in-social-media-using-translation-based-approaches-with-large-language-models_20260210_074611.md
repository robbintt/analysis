---
ver: rpa2
title: Multilingual Hate Speech Detection in Social Media Using Translation-Based
  Approaches with Large Language Models
arxiv_id: '2506.08147'
source_url: https://arxiv.org/abs/2506.08147
tags:
- urdu
- english
- speech
- spanish
- hate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of multilingual hate speech
  detection, focusing on low-resource languages like Urdu, which are underrepresented
  in existing research. The authors introduce a novel trilingual dataset of 10,193
  tweets in English, Urdu, and Spanish, balanced across hateful and non-hateful labels.
---

# Multilingual Hate Speech Detection in Social Media Using Translation-Based Approaches with Large Language Models

## Quick Facts
- arXiv ID: 2506.08147
- Source URL: https://arxiv.org/abs/2506.08147
- Authors: Muhammad Usman; Muhammad Ahmad; M. Shahiki Tash; Irina Gelbukh; Rolando Quintero Tellez; Grigori Sidorov
- Reference count: 15
- One-line primary result: Translation-based pipeline with LLMs achieves F1-scores of 0.87 (English), 0.85 (Spanish), 0.81 (Urdu), and 0.88 (joint multilingual)

## Executive Summary
This study addresses multilingual hate speech detection for low-resource languages like Urdu through a translation-based approach using large language models. The authors introduce a balanced trilingual dataset of 10,193 tweets across English, Urdu, and Spanish, and employ a pipeline that standardizes data through translation, enhances feature extraction with attention layers, and leverages transformer and LLM architectures. The framework achieves significant performance gains over baseline models, demonstrating the potential of translation-based methods for improving hate speech detection in underrepresented languages.

## Method Summary
The methodology involves translating Urdu and Spanish tweets to English using Google Translate API to create a unified input space for pre-trained models. A custom attention layer is prepended to transformers and LLMs to enhance feature extraction. The pipeline includes preprocessing (cleaning, tokenization, stopword removal, stemming) and uses 80-20 train-test splits for traditional models, with full dataset few-shot prompting for LLMs. Models tested include SVM with TF-IDF, BERT variants, GPT-3.5 Turbo, and Qwen 2.5 72B. Hyperparameters are optimized via grid/random search, and macro F1-score is the primary evaluation metric.

## Key Results
- Translation-based pipeline achieves macro F1-scores of 0.87 for English, 0.85 for Spanish, 0.81 for Urdu, and 0.88 for joint multilingual model
- Performance improvements of up to 10.17% over baseline SVM models
- Qwen 2.5 72B outperforms other models in joint multilingual setting with F1 of 0.88
- Dataset is balanced with 4,849 hateful and 5,344 non-hateful labels across 10,193 total tweets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Translation-based standardization enables low-resource language processing by mapping diverse inputs to a high-resource semantic space.
- **Mechanism:** Google Translate API converts Urdu and Spanish tweets into English, creating a unified input space for pre-trained models robust in English but underperforming on raw low-resource scripts.
- **Core assumption:** Machine translation preserves semantic markers of hate speech across languages without significant information loss.
- **Evidence anchors:** [abstract] mentions "translation-based pipeline standardizes the data, enabling joint multilingual processing"; [section 5.2] describes cross-lingual alignment through translation.
- **Break condition:** If translation API dilutes intensity of localized slang, downstream classifier will fail to detect hate.

### Mechanism 2
- **Claim:** Joint multilingual training improves low-resource performance via cross-lingual transfer learning.
- **Mechanism:** Training a single model (Qwen 2.5 72B) on combined dataset of all three languages leverages shared semantic features from high-resource languages to regularize low-resource task.
- **Core assumption:** Hate speech features share latent cross-lingual structure that model can abstract.
- **Evidence anchors:** [abstract] reports "0.88 for the joint multilingual model"; [section 7.4] shows Qwen 2.5 72B achieved highest F1 (0.88) in joint setting.
- **Break condition:** If languages are too linguistically distant or dataset imbalanced, model may bias toward high-resource language, degrading Urdu nuance detection.

### Mechanism 3
- **Claim:** Prepended attention layers enhance feature extraction for transformer models by prioritizing hate-relevant tokens.
- **Mechanism:** Multi-head attention mechanism (Linformer-based sparse attention) prepended as contextual filter before transformer encoder, forcing model to weight critical tokens higher than standard filler words.
- **Core assumption:** Standard transformer attention might dilute signal of short, hateful keywords in long sequences; dedicated filter layer explicitly optimizes signal-to-noise ratio.
- **Evidence anchors:** [section 5.4.4] "Our approach integrates a multi-head self-attention mechanism... prepended as a contextual filter"; [abstract] claims "attention layers as a precursor... enhancing feature extraction."
- **Break condition:** If attention layer adds computational overhead or overfits to specific keywords appearing in non-hateful contexts, false positives will rise.

## Foundational Learning

- **Concept: Transfer Learning (Cross-Lingual)**
  - **Why needed here:** Architecture relies on applying model trained on massive English corpora to Urdu; understanding weight sharing/fine-tuning across languages is key to understanding "Joint" results.
  - **Quick check question:** Why does Qwen 2.5 72B perform better on Urdu in "Joint" setting than on Urdu alone?

- **Concept: Macro F1-Score**
  - **Why needed here:** Authors use this as primary metric because hate speech datasets often have class imbalance; macro F1 treats both classes equally, critical for safety systems where missing "Hateful" content is costly.
  - **Quick check question:** If model predicts "Not-Hateful" for 99% of tweets and achieves 90% accuracy, why is F1-score likely near zero?

- **Concept: Pipeline/Workflow Architecture**
  - **Why needed here:** System isn't single model but chain (Preprocessing -> Translation -> Attention -> Classification); understanding dependencies is key to debugging.
  - **Quick check question:** At which step in pipeline does "attention layer" get applied relative to translation step?

## Architecture Onboarding

- **Component map:** Raw Tweets -> Preprocessing (cleaning/tokenization) -> Translation Node (Google Translate) -> Attention Filter (custom multi-head) -> Backbone (Transformer/LLM) -> Binary Classification (Hateful/Not-Hateful)
- **Critical path:** Translation Node and Attention Filter. System creates bottleneck at translation; if slang is mistranslated, powerful LLM downstream cannot recover intent.
- **Design tradeoffs:**
  - Translation vs. Multilingual Native Models: Authors chose to translate rather than use native multilingual model exclusively, trading "native nuance" for "high-resource model performance"
  - LLM vs. SVM: Using Qwen/GPT offers high accuracy (0.88 F1) but high latency/cost compared to SVM (0.82 F1), which is faster and cheaper
- **Failure signatures:**
  - Semantic Dampening: Misclassification when strong cultural slurs are translated to mild English terms, causing False Negatives
  - Sarcasm Misses: High False Negatives in English where "Great work, morons" is missed because model relies on literal translation artifacts
- **First 3 experiments:**
  1. Ablation on Translation: Run pipeline on native Urdu text (without translation) using XLM-RoBERTa to isolate information loss caused by translation step
  2. Attention Layer Validation: Compare performance of model with and without custom prepended attention layer to verify claimed contribution
  3. Error Analysis on Slang: Inject specific slang terms into translation API and measure variance in output hostility to quantify translation noise

## Open Questions the Paper Calls Out

- **Open Question 1:** To what extent would developing Urdu-specific pre-trained embeddings tailored to code-mixed and Roman texts improve detection accuracy compared to standard multilingual transformers?
  - **Basis in paper:** [Explicit] Section 10.3 suggests developing pre-trained models tailored to Urdu would improve detection of cultural nuances
  - **Why unresolved:** Current models like bert-base-uncased achieve only 0.50 F1 for Urdu due to struggles with morphological complexity and Roman Urdu slang
  - **What evidence would resolve it:** Comparative study evaluating custom Urdu fine-tuned transformer against standard XLM-RoBERTa on dataset rich in Roman Urdu slang

- **Open Question 2:** Can replacing general-purpose translation APIs with domain-specific translation models effectively preserve hostility intensity of slang and dialectal insults during cross-lingual transfer?
  - **Basis in paper:** [Explicit] Section 10.3 asks if replacing Google Translate with domain-specific models could reduce errors like "hijo de puta" to "jerk"
  - **Why unresolved:** Error analysis shows translation ambiguities currently soften insults (e.g., "What shitty people" becomes "What bad people"), causing false negatives
  - **What evidence would resolve it:** Experiment measuring correlation between human-rated hostility scores in original tweets versus domain-adapted translations

- **Open Question 3:** How can attention visualization or feature attribution techniques be utilized to diagnose and correct misclassification of implicit hate speech (e.g., sarcasm) in Large Language Models?
  - **Basis in paper:** [Explicit] Section 9 states LLMs lack transparency, complicating error attribution; future work includes enhancing model interpretability through attention visualization
  - **Why unresolved:** Models like GPT-3.5 Turbo struggle with implicit hate speech (e.g., "Great work, morons") due to prioritizing literal meanings, yet internal decision-making remains "black box"
  - **What evidence would resolve it:** Application of explainability tools to misclassified examples to identify if models attend to sarcastic markers or are distracted by non-hateful tokens

## Limitations

- Dataset unavailability blocks independent verification of reported results
- Translation-based approach introduces potential semantic loss, particularly for culturally specific slang and profanity in Urdu and Spanish
- Custom attention mechanism's contribution remains partially unverified due to limited evidence in corpus
- Binary classification focus may oversimplify nuanced nature of hate speech requiring more granular categorization

## Confidence

- Translation-Based Standardization Effectiveness: Medium - Reported improvements are substantial but reliance on Google Translate API without detailed error analysis limits confidence
- Joint Multilingual Training Gains: Medium - 0.88 F1 score is impressive but lack of ablation studies comparing to multilingual native models reduces confidence
- Attention Layer Enhancement: Low - Mechanism is described but lacks empirical validation in provided corpus, and sparse attention implementation details are incomplete

## Next Checks

1. **Ablation Study on Translation:** Replicate pipeline using native multilingual models (XLM-RoBERTa) on Urdu text without translation to quantify information loss from translation step
2. **Attention Layer Validation:** Conduct controlled experiments comparing model performance with and without custom prepended attention layer to isolate its contribution
3. **Translation Error Analysis:** Systematically test Google Translate API with curated list of Urdu and Spanish hate speech terms to measure semantic preservation and identify consistent mistranslations