---
ver: rpa2
title: Enhancing Spatial Reasoning in Large Language Models for Metal-Organic Frameworks
  Structure Prediction
arxiv_id: '2601.09285'
source_url: https://arxiv.org/abs/2601.09285
tags:
- structure
- building
- structures
- rotation
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Metal-organic frameworks (MOFs) are crystalline materials with
  diverse applications, but predicting their 3D structures is challenging due to their
  atomic complexity. Large language models (LLMs) have been effective in generating
  simpler crystals but struggle with MOFs' intricate 3D geometries.
---

# Enhancing Spatial Reasoning in Large Language Models for Metal-Organic Frameworks Structure Prediction

## Quick Facts
- **arXiv ID:** 2601.09285
- **Source URL:** https://arxiv.org/abs/2601.09285
- **Reference count:** 26
- **Primary result:** MOF-LLM achieves 35.78% match rate on unseen MOFs, outperforming state-of-the-art methods while generating structures in 0.04 seconds.

## Executive Summary
Metal-organic frameworks (MOFs) are crystalline materials with diverse applications, but predicting their 3D structures is challenging due to their atomic complexity. Large language models (LLMs) have been effective in generating simpler crystals but struggle with MOFs' intricate 3D geometries. To address this, we propose MOF-LLM, the first LLM framework specifically designed for block-level MOF structure prediction. Our method uses spatial-aware continual pre-training (CPT) to improve LLMs' spatial reasoning, structural supervised fine-tuning (SFT) to enable block assembly, and matching-driven reinforcement learning (RL) to refine structure stability. The model generates stable MOF structures with a match rate of 35.78% on unseen MOFs, outperforming state-of-the-art denoising-based and LLM-based methods. Additionally, MOF-LLM achieves a high inference speed of 0.04 seconds per structure, making it highly efficient for high-throughput MOF discovery.

## Method Summary
MOF-LLM employs a three-stage pipeline on a Qwen-3 8B backbone: spatial-aware continual pre-training with 3D geometric priors, structural supervised fine-tuning for block assembly, and matching-driven reinforcement learning for stability refinement. The method treats MOFs as assemblies of rigid building blocks, reducing dimensionality from atomic coordinates to block-level representations. Spatial descriptors (PCA spans, molecular weights, rotated principal axes) are encoded as text during pre-training to improve spatial reasoning. The final RL stage uses pymatgen's StructureMatcher for geometric rewards, achieving 35.78% match rate on unseen MOFs with inference times of 0.04 seconds per structure.

## Key Results
- Achieves 35.78% match rate on unseen MOFs, outperforming state-of-the-art methods
- Generates structures in 0.04 seconds per structure, demonstrating high efficiency
- Spatial-aware CPT provides incremental 1% improvement in match rate
- RL stage significantly reduces RMSE and improves geometric fidelity

## Why This Works (Mechanism)

### Mechanism 1: Spatial-Aware Continual Pre-training Injects Geometric Priors
- Claim: Explicitly encoding 3D geometry as text descriptors (PCA spans, molecular weights, rotated principal axes) during CPT improves spatial reasoning.
- Mechanism: The model learns associations between SMILES strings and their physical dimensions/masses, building an implicit "bounding box" representation before assembly training.
- Core assumption: Text-based spatial descriptors can substitute for native 3D representations in transformer attention.
- Evidence anchors: [abstract] "spatial-aware continual pre-training (CPT) to improve LLMs' spatial reasoning"; [section 4.2] "augment the description of each building block with its molecular weight and spatial span along three PCA axes"; [section 5.3] Ablation shows ~1% MR improvement from CPT; removing spatial info causes "pronounced degradation."

### Mechanism 2: Block-Level Representation Reduces Dimensionality
- Claim: Representing MOFs as building block assemblies (vs. atomic coordinates) makes the generation tractable for autoregressive LLMs.
- Mechanism: A MOF with hundreds of atoms becomes M blocks (typically <200), reducing sequence length and constraining output to chemically meaningful units.
- Core assumption: Building blocks can be treated as rigid bodies without losing critical structural information.
- Evidence anchors: [section 1] "block-level generation paradigm substantially reduces the dimensionality of the design space"; [table 1] All-atom methods dramatically underperform block-level methods.

### Mechanism 3: Matching-Driven RL Refines Global Structure Stability
- Claim: Reinforcement learning with structure-matching rewards improves geometric fidelity beyond token-level SFT.
- Mechanism: SAPO samples multiple outputs, computes rewards based on pymatgen's StructureMatcher tolerance, and updates policy to favor low-RMSE structures.
- Core assumption: The reward function (matching tolerance + RMSE bonus) correlates with physically stable MOFs.
- Evidence anchors: [section 4.4] "SFT inherently fails to enforce global structural plausibility"; [section 5.3] RL stage yields "substantial increase in MR and marked reduction in RMSE."

## Foundational Learning

- **Fractional coordinates and lattice parameters**
  - Why needed here: The model outputs translations in fractional coordinates (TL⁻¹) and lattice as (a, b, c, α, β, γ); understanding periodic boundary conditions is essential.
  - Quick check question: Given lattice vectors, can you convert a fractional coordinate [0.5, 0.5, 0.5] to Cartesian?

- **Euler angles vs. rotation matrices (SO(3))**
  - Why needed here: The paper uses extrinsic x-y-z Euler angles (roll, pitch, yaw) because rotation matrices are "redundant, conceptually abstract, and strictly constrained."
  - Quick check question: What is gimbal lock, and does the [−π/2, π/2] pitch range avoid it?

- **SMILES canonicalization**
  - Why needed here: Building blocks are encoded as canonical SMILES; this ensures unique representation for each molecule.
  - Quick check question: Why might SMILES lose 3D geometry information that PCA spans attempt to recover?

## Architecture Onboarding

- **Component map**: Qwen-3 8B -> Spatial-aware CPT -> Structural SFT -> Matching-driven RL
- **Critical path**: CPT → SFT → RL. Ablation confirms each stage contributes incrementally; skipping CPT reduces MR by ~1 point.
- **Design tradeoffs**:
  - Euler angles vs. axis-angle: Paper ablates both; Euler angles win (Table 3). Assumption: Extrinsic x-y-z convention handles MOF rotations better.
  - SMILES vs. 3D-native tokens: SMILES leverages pre-trained chemical knowledge but loses geometry; compensated by CPT spatial descriptors.
  - Rigid blocks vs. flexible: Current approach assumes rigidity; future work needed for torsion-aware generation.
- **Failure signatures**:
  - Parsing failures (reward = -1): Model generates malformed text that doesn't decode to valid structures.
  - Low MR at strict tolerance (stol=0.5) but high at lenient (stol=1.0): Imprecise block placement.
  - High RMSE on geometric properties (VSA, PLD): Model doesn't capture pore structure correctly.
- **First 3 experiments**:
  1. **Reproduce ablation**: Train SFT-only model and compare to SFT+CPT+RL. Expected: ~1-2% MR gap.
  2. **Rotation representation test**: Swap Euler angles for axis-angle (code in Appendix A.4). Expected: ~3% MR drop based on Table 3.
  3. **Reward function sensitivity**: Vary RMSE bonus weight (currently 0.5·e^(-4·RMSE)). If too high, may overfit to training structures; if too low, may not refine precision.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the framework be extended to model the conformational flexibility of building blocks?
- **Basis in paper:** [explicit] The authors state they "treat building blocks as rigid bodies" and identify modeling "conformational flexibility" as a necessary step to enhance practicality.
- **Why unresolved:** The current methodology relies on fixed geometries, ignoring the rotational freedom of bonds within organic linkers, which limits accuracy for flexible materials.
- **What evidence would resolve it:** A modified model that successfully predicts structures with torsional adjustments, showing higher fidelity on flexible MOF benchmarks.

### Open Question 2
- **Question:** Can advanced tokenization methods natively preserve 3D geometries better than SMILES strings?
- **Basis in paper:** [explicit] The paper acknowledges that "flattening 3D blocks into 1D SMILES strings inherently simplifies geometric details" and suggests "advanced tokenization" as a research direction.
- **Why unresolved:** Converting complex 3D structures to 1D text strings creates a semantic bottleneck, forcing the model to rely on implicit priors rather than explicit spatial data.
- **What evidence would resolve it:** A comparison showing that 3D-native tokens (e.g., geometric vectors) yield lower coordinate RMSE than the text-based approach.

### Open Question 3
- **Question:** Does explicit Chain-of-Thought (CoT) reasoning improve spatial planning for MOF assembly?
- **Basis in paper:** [explicit] The authors propose "integrating explicit Chain-of-Thought reasoning" as a promising direction for improving "interpretability and complex spatial planning."
- **Why unresolved:** The current model generates solutions autoregressively in a single pass without intermediate planning steps, which may limit performance on highly complex topologies.
- **What evidence would resolve it:** Experiments demonstrating that CoT-augmented generation increases match rates for structures with high block counts.

## Limitations

- **Building block rigidity assumption**: The block-level representation explicitly treats building blocks as rigid bodies, which "may limit applicability to MOFs where conformational flexibility within building blocks is critical."
- **Spatial encoding generalizability**: The 1% MR gain from spatial-aware CPT is modest, and it's unclear whether this improvement generalizes beyond the Boyd dataset or to MOFs with significantly different building block geometries.
- **Reward function validity**: The matching-driven RL uses pymatgen's StructureMatcher with tolerance-based rewards and RMSE bonuses, which may not capture chemically relevant stability criteria.

## Confidence

**High Confidence**:
- Block-level representation substantially reduces dimensionality and enables LLM generation of MOFs
- Spatial-aware CPT provides incremental improvement in spatial reasoning
- Matching-driven RL refines structural fidelity beyond SFT

**Medium Confidence**:
- The specific choice of extrinsic x-y-z Euler angles over rotation matrices or axis-angle representations
- The sufficiency of rigid building blocks for diverse MOF applications
- The correlation between StructureMatcher-based rewards and chemical stability

**Low Confidence**:
- Generalization of spatial encoding benefits to unseen MOF topologies
- The practical utility of generated structures for real-world applications (no synthesis or property validation reported)
- Performance relative to non-LLM methods on the same evaluation metrics

## Next Checks

1. **Rigidity Impact Assessment**: Systematically evaluate MOF-LLM performance on MOF families known for conformational flexibility (e.g., pillared-layer structures, flexible MIL-53 variants). Compare match rates and geometric properties against ground truth to quantify the practical limitations of the rigid block assumption.

2. **Reward Function Ablation**: Replace the StructureMatcher-based reward with alternative stability criteria (e.g., metal-ligand bond distance constraints, void space preservation metrics) and measure impact on MR, RMSE, and downstream property prediction. This would validate whether geometric matching correlates with chemical stability.

3. **Cross-Dataset Generalization**: Test MOF-LLM on independent MOF datasets not represented in the Boyd et al. corpus (e.g., CSD-derived MOFs, recent high-performance MOFs from literature). Evaluate whether spatial-aware CPT provides consistent benefits across diverse building block chemistries and topologies.