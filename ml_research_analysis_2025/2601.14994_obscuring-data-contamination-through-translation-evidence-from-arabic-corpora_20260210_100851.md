---
ver: rpa2
title: 'Obscuring Data Contamination Through Translation: Evidence from Arabic Corpora'
arxiv_id: '2601.14994'
source_url: https://arxiv.org/abs/2601.14994
tags:
- contamination
- mmlu
- xquad
- data
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work examines whether translating benchmarks into Arabic can
  mask data contamination in large language model evaluation. The authors fine-tune
  four open-weight models on varying proportions of Arabic-translated benchmark data
  and evaluate them on original English tests, extending the Tested Slot Guessing
  method with choice re-ordering and Min-K% probability analysis to detect memorization.
---

# Obscuring Data Contamination Through Translation: Evidence from Arabic Corpora
## Quick Facts
- arXiv ID: 2601.14994
- Source URL: https://arxiv.org/abs/2601.14994
- Reference count: 6
- Primary result: Arabic translation of benchmarks can mask data contamination in LLM evaluation, requiring multilingual detection methods.

## Executive Summary
This work investigates whether translating evaluation benchmarks into Arabic can obscure data contamination in large language model assessment. The authors fine-tune four open-weight models on varying proportions of Arabic-translated benchmark data and evaluate them on original English tests. They find that while Arabic translation suppresses conventional contamination indicators, models still benefit from contaminated data—especially those with stronger Arabic capabilities—as reflected in rising Min-K% scores and cross-lingual answer consistency. To address this blind spot, they propose Translation-Aware Contamination Detection (TACD), which compares behavioral signals across multiple translated benchmark variants rather than relying solely on English.

## Method Summary
The study employs a controlled fine-tuning approach where four open-weight models are trained on different proportions of Arabic-translated benchmark data. The authors extend the Tested Slot Guessing method with choice re-ordering and Min-K% probability analysis to detect memorization. They evaluate models on original English tests and introduce TACD, which compares behavioral signals across multiple translated benchmark variants. The methodology focuses on detecting contamination effects that English-only methods miss, particularly for models with varying Arabic language capabilities.

## Key Results
- Arabic translation suppresses conventional contamination indicators in English-only evaluation methods
- Models with stronger Arabic capabilities show greater benefit from contaminated data, as reflected in rising Min-K% scores
- TACD reliably exposes contamination effects even when English-only methods fail

## Why This Works (Mechanism)
Translation obscures contamination signals by shifting the evaluation context away from the language space where memorization was learned. Models trained on Arabic-translated data retain knowledge from contamination but express it differently when evaluated in English, reducing detection through standard metrics. The effect is amplified in models with stronger cross-lingual capabilities, which can better leverage memorized patterns across language boundaries.

## Foundational Learning
- **Data contamination in LLMs**: Why needed - understanding how training data overlap affects model evaluation integrity; Quick check - verify benchmark examples appear in training corpora
- **Cross-lingual transfer**: Why needed - models with multilingual capabilities can exploit contamination across languages; Quick check - compare model performance across language pairs
- **Translation artifacts**: Why needed - translation quality and domain specificity affect contamination detection; Quick check - assess translation accuracy and cultural context preservation
- **Min-K% probability analysis**: Why needed - measures consistency in model predictions to detect memorization; Quick check - track probability score distributions across evaluations
- **Tested Slot Guessing method**: Why needed - provides baseline contamination detection framework; Quick check - validate against known contamination scenarios
- **Multilingual evaluation pipelines**: Why needed - ensure fair assessment across language boundaries; Quick check - implement parallel testing in multiple languages

## Architecture Onboarding
Component map: Arabic translation -> Model fine-tuning -> English evaluation -> Contamination detection -> TACD analysis

Critical path: Data translation → Fine-tuning with contamination → Cross-lingual evaluation → Signal comparison → Contamination identification

Design tradeoffs: Translation quality vs. detection sensitivity; model capability vs. contamination benefit; English-only vs. multilingual evaluation

Failure signatures: False negatives in English-only detection; inconsistent cross-lingual patterns; translation artifacts masking contamination

First experiments:
1. Test contamination detection with varying translation quality levels
2. Compare TACD effectiveness across different language pairs
3. Evaluate contamination benefits in models with different multilingual proficiencies

## Open Questions the Paper Calls Out
None

## Limitations
- Generalizability limited to Arabic-English pair and four specific models studied
- Translation quality and domain specificity effects not systematically validated
- Effectiveness of TACD dependent on availability of diverse translated benchmarks

## Confidence
- High confidence: Arabic translation suppresses conventional contamination indicators in English-only evaluation methods
- Medium confidence: Models with stronger Arabic capabilities benefit more from contaminated data
- Medium confidence: TACD reliably exposes contamination effects where English-only methods fail

## Next Checks
1. Replicate the study with additional language pairs beyond Arabic-English to test generalizability
2. Conduct ablation studies varying translation quality, domain specificity, and cultural context
3. Test TACD on proprietary models and larger model families beyond the four open-weight models studied