---
ver: rpa2
title: 'Look Beyond: Two-Stage Scene View Generation via Panorama and Video Diffusion'
arxiv_id: '2509.00843'
source_url: https://arxiv.org/abs/2509.00843
tags:
- diffusion
- view
- video
- panorama
- scene
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a two-stage scene view generation method that
  decomposes single-view novel view synthesis into panoramic scene extrapolation followed
  by trajectory-aware view interpolation. The method first generates a 360-degree
  panorama from a single image using a diffusion transformer, then extracts keyframes
  and uses them to condition a video diffusion model for consistent novel view synthesis.
---

# Look Beyond: Two-Stage Scene View Generation via Panorama and Video Diffusion

## Quick Facts
- arXiv ID: 2509.00843
- Source URL: https://arxiv.org/abs/2509.00843
- Reference count: 40
- Key outcome: Proposes two-stage NVS method achieving superior geometric consistency (mTSED: 0.835 vs. 0.740 baseline) and perceptual quality (LPIPS: 0.218 vs. 0.456) on Matterport3D and RealEstate10K datasets

## Executive Summary
This paper introduces a two-stage approach to novel view synthesis from a single image that decomposes the task into panoramic scene extrapolation followed by trajectory-aware view interpolation. The method first generates a 360-degree panorama using a diffusion transformer, then extracts keyframes and conditions a video diffusion model for consistent novel view synthesis across extended trajectories. By grounding view generation in a unified panoramic representation, the approach addresses long-term consistency challenges that plague autoregressive video-only methods, achieving state-of-the-art performance in both perceptual quality and geometric fidelity.

## Method Summary
The method operates in two stages: (1) panorama generation using a DiT-XL model conditioned on CLIP embeddings to create a 360° scene representation from a single perspective image, and (2) video diffusion using a frozen SVD-Interpolation backbone with learnable Plücker raymap conditioning and spatial noise weighting. Keyframes are extracted from the generated panorama through perspective projection and depth-based warping, then used to anchor trajectory generation. The approach employs spatial weighting of noise estimates based on camera positional proximity and orientation similarity rather than purely temporal diffusion, improving geometric consistency across long trajectories.

## Key Results
- Achieves superior geometric consistency (mTSED: 0.835 vs. 0.740 baseline)
- Shows perceptual quality improvements (LPIPS: 0.218 vs. 0.456 baseline)
- Demonstrates better temporal consistency (FVD: 113.561 vs. 198.269) across extended trajectories

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing single-view NVS into panorama generation followed by view interpolation improves long-term consistency
- Mechanism: The panorama provides a unified geometric prior that constrains all subsequent view generation. Keyframes extracted from this single panoramic representation share consistent scene geometry, lighting, and semantics by construction—eliminating drift that accumulates in autoregressive approaches.
- Core assumption: The generated panorama accurately captures scene structure and appearance from the input perspective.
- Evidence anchors:
  - [abstract] "decomposing single-view NVS into a 360-degree scene extrapolation followed by novel view interpolation... ensures long-term view and scene consistency by conditioning on keyframes extracted and warped from a generated panoramic representation"
  - [section 3] "This panoramic representation serves as a geometric prior, overcoming the long-term view consistency limitations of existing single-view video generation methods"
  - [corpus] Scene Splatter identifies similar scene inconsistency issues with video-only approaches; MVRoom uses multi-view diffusion with 3D layout for consistency—supporting the geometric prior hypothesis
- Break condition: If panorama generation produces geometrically incorrect or inconsistent scene layouts, downstream view interpolation will inherit these errors. Failures observed under "extreme lighting, motion blur, occlusion, cluttered or cases with dynamic objects" (Section 5).

### Mechanism 2
- Claim: Spatial noise diffusion with keyframe-anchored weighting improves view alignment over purely temporal diffusion
- Mechanism: Rather than weighting frames linearly by time, Algorithm 1 computes per-frame weights based on camera positional proximity (Eq. 9) and orientation similarity (Eq. 10) to anchor keyframes. This spatial weighting grounds each generated frame's noise estimate to its nearest geometric reference, reducing drift.
- Core assumption: Camera poses are known or accurately estimated for all frames in the trajectory.
- Evidence anchors:
  - [section 3.4] Algorithm 1 lines 8-11: spatial interpolation computes "position weights" and "orientation weights" normalized for spatial consistency guidance
  - [table 4] Ablation shows "Spatial Diff + Raymap" achieves mTSED 0.88 vs. "Temporal Diff + Raymap" at 0.83, confirming spatial weighting improves geometric consistency
  - [corpus] Limited direct comparison—most related work uses temporal attention without explicit spatial pose-weighted diffusion
- Break condition: If camera pose interpolation between keyframes is inaccurate (e.g., incorrect SLERP for rotations), spatial weights will misguide denoising, producing misaligned views.

### Mechanism 3
- Claim: Plücker raymap embedding provides sufficient camera pose conditioning for controllable view synthesis
- Mechanism: Camera intrinsics K and extrinsics (R, T) are converted to 6D Plücker ray representations per pixel (Eq. 26), encoding both ray direction and moment vectors. These raymaps are concatenated across frames and compressed via learned encoder, then injected via cross-attention into the frozen video diffusion backbone.
- Core assumption: The pre-trained SVD model's latent space can be steered by raymap conditioning without fine-tuning the core denoising network.
- Evidence anchors:
  - [section 3.3] "Each view is represented by a raymap r_i ∈ R^(H×W×6), initialized using the Plücker 6D embedding... These features serve as conditioning input to the video diffusion model via cross-attention"
  - [table 4] "Temporal Diff + Raymap" (without spatial weighting) already achieves mTSED 0.83 vs. 0.75 without raymaps
  - [corpus] CameraCtrl and related work use similar camera conditioning strategies, suggesting ray-based embeddings are an established approach
- Break condition: If trajectories include extreme camera motions beyond training distribution (e.g., 360° rapid pans), raymap conditioning may fail—the paper notes video diffusion models "still face challenges under large camera shifts such as 360-degree pans" (Section 2).

## Foundational Learning

- Concept: **Equirectangular panorama projection**
  - Why needed here: Understanding how perspective views map to/from panoramas is essential for keyframe extraction (Eq. 6, 12-16) and walk-in motion simulation (Eq. 7-8)
  - Quick check question: Given yaw θ=90° and pitch φ=0°, which panorama region does the "right" perspective view sample from?

- Concept: **Diffusion model conditioning via cross-attention**
  - Why needed here: Both panorama DiT (CLIP conditioning) and video diffusion (raymap conditioning) inject external signals through cross-attention rather than concatenation
  - Quick check question: Why does cross-attention allow freezing the pre-trained SVD backbone while still adapting to new conditioning modalities?

- Concept: **Plücker coordinates for ray representation**
  - Why needed here: The 6D Plücker embedding encodes camera rays with both direction and moment, providing a complete geometric description for pose conditioning
  - Quick check question: What information does the "moment vector" in Plücker coordinates capture that direction alone does not?

## Architecture Onboarding

- Component map:
  - Input image → CLIP embedding → Panorama DiT (XL, 24 blocks, 32×64×4 latents) → outpainting with cycle consistency loss → Keyframe extraction via perspective projection (Eq. 12-16) → Depth-based warping (Eq. 7-8) → Masked anchor frames → Raymap encoding (Plücker 6D) → Video diffusion (frozen SVD + LoRA fine-tuning) → Spatial-weighted DDPM sampling (Algorithm 1) → Composed video clips

- Critical path:
  1. Input image → CLIP embedding → panorama outpainting (inference: ~34s)
  2. Panorama → keyframe extraction + warping (depends on trajectory)
  3. Keyframes + raymaps → video diffusion inference (~620s for 100 frames)
  4. Compose video clips (48 frames/pass) into full trajectory

- Design tradeoffs:
  - **Panorama resolution vs. speed**: 256×512 input limits detail in extracted perspective views; higher resolution increases panorama generation time
  - **Keyframe overlap vs. diversity**: Higher overlap improves consistency (Fig. 17-19) but requires more keyframes per trajectory
  - **Walk-in depth factor**: 0.8× max depth used; larger factors reduce known regions, degrading consistency (Fig. 10a)

- Failure signatures:
  - **Panorama boundary artifacts**: Red gaps in Fig. 8 without cycle consistency loss—check panorama edges for discontinuities
  - **View misalignment after long trajectories**: FVD/mTSED degradation in Fig. 7 for baselines—monitor these metrics over frame count
  - **Hallucinated content in corners**: ViewCrafter creates phantom rooms (Fig. 23)—compare generated views against panorama-projected reference

- First 3 experiments:
  1. **Validate panorama quality**: Generate panoramas from held-out test views; compute PSNR/SSIM against ground-truth panoramas to establish Stage 1 upper bound
  2. **Ablate keyframe strategy**: Compare (a) panorama-only keyframes vs. (b) panorama + walk-in keyframes on mTSED to quantify warping contribution
  3. **Spatial vs. temporal weighting**: Replicate Table 4 on a subset; sweep temperature parameters τ_T and τ_q (Fig. 20) to find optimal spatial weighting for your target dataset

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can dynamic foreground entities be effectively incorporated into the current static scene framework?
- **Basis in paper:** [Explicit] The conclusion states, "While we currently model static scenes, dynamic foreground entities could be incorporated as overlaid layers."
- **Why unresolved:** The current two-stage pipeline (panorama generation followed by view interpolation) relies on the assumption of a static environment to maintain geometric consistency across keyframes; introducing non-rigid dynamics breaks the warping and inpainting assumptions used in the diffusion process.
- **What evidence would resolve it:** A modified architecture that segments and processes dynamic objects as separate layers, successfully generating temporally consistent motion for foreground entities without artifacts or "ghosting" in the static background.

### Open Question 2
- **Question:** Can autonomous trajectory planning be integrated to enable topology-aware navigation?
- **Basis in paper:** [Explicit] The authors suggest that "Integrating autonomous trajectory planning would enable topology-aware navigation" as a future direction.
- **Why unresolved:** The current method relies on user-defined trajectories or pre-existing paths in the dataset. Real-time autonomous planning requires the model to infer navigable space (topology) and obstacles directly from the generated or hallucinated geometry, which introduces potential error accumulation.
- **What evidence would resolve it:** The demonstration of an agent using the generated panoramic representation to successfully plan and execute collision-free paths in novel, synthesized environments without human intervention.

### Open Question 3
- **Question:** How can the model's robustness be improved regarding extreme lighting, motion blur, and occlusion?
- **Basis in paper:** [Explicit] The authors note, "we observed failures under extreme lighting, motion blur, occlusion, [and] cluttered... cases."
- **Why unresolved:** Diffusion models often struggle with high-uncertainty regions where structural priors are weak or undefined. The current training data (Matterport3D, RealEstate10K) may lack sufficient examples of these adverse conditions to learn robust invariance.
- **What evidence would resolve it:** Quantitative results (e.g., FID, FVD) on a specialized test set of challenging scenes featuring low light or high occlusion, showing that the method retains structural consistency where baseline models fail.

## Limitations
- **Depth estimation dependency**: Performance relies heavily on accurate depth estimation for panorama-to-perspective projection, with errors propagating to final view quality
- **Large motion challenges**: Method faces difficulties with extreme camera motions like 360-degree pans where raymap conditioning may fail
- **Challenging imaging conditions**: System shows failures under extreme lighting, motion blur, occlusion, and cluttered scenes with dynamic objects

## Confidence
- **High confidence**: Panorama-first decomposition strategy well-supported by ablation studies and comparison to baselines with substantial geometric consistency improvements
- **Medium confidence**: Spatial noise diffusion weighting mechanism shows measurable improvements but optimal parameters may be dataset-specific
- **Low confidence**: Handling of extreme camera motions and failure modes under challenging imaging conditions insufficiently characterized

## Next Checks
1. **Depth estimation sensitivity**: Systematically vary depth estimation quality and measure impact on final view synthesis metrics to quantify dependency on depth accuracy

2. **Trajectory extremity analysis**: Generate and evaluate trajectories with progressively larger camera motions to characterize method's breaking point and identify where spatial consistency degrades

3. **Cross-dataset generalization**: Evaluate trained model on third dataset with different scene characteristics to test whether spatial weighting parameters and panorama generation generalize beyond original datasets