---
ver: rpa2
title: Intuitions of Machine Learning Researchers about Transfer Learning for Medical
  Image Classification
arxiv_id: '2510.00902'
source_url: https://arxiv.org/abs/2510.00902
tags:
- learning
- transfer
- medical
- source
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates how machine learning practitioners select
  source datasets for transfer learning in medical imaging. A task-based survey with
  15 participants examined their intuition across two case studies using different
  target tasks but identical source options.
---

# Intuitions of Machine Learning Researchers about Transfer Learning for Medical Image Classification

## Quick Facts
- arXiv ID: 2510.00902
- Source URL: https://arxiv.org/abs/2510.00902
- Reference count: 40
- Primary result: Machine learning practitioners' source dataset selection for transfer learning is task-dependent, influenced by community practices, dataset properties, and perceived similarity, but actual performance often diverges from intuition.

## Executive Summary
This study investigates how machine learning researchers select source datasets for transfer learning in medical imaging through a task-based survey with 15 participants across two case studies. The research reveals that choices are significantly influenced by community practices, dataset properties, and perceived visual or semantic similarity, rather than purely objective data fitness. Crucially, the study challenges the traditional "more similar is better" view by demonstrating that similarity ratings and expected performance are not always aligned, highlighting the need for clearer definitions and tools to operationalize vague concepts like "domain gap" and "image quality."

## Method Summary
The study employed a task-based survey approach where 15 machine learning researchers were presented with two case studies involving different target tasks but identical source options. Participants were asked to select source datasets for transfer learning and explain their reasoning, covering factors such as semantic similarity, visual similarity, embedding similarity, availability, and community norms. The survey captured both quantitative ratings and qualitative responses to understand the decision-making process.

## Key Results
- Source dataset selection is strongly influenced by community practices and reviewer expectations rather than purely objective data fitness
- Perceived visual or semantic similarity does not reliably predict transfer performance, challenging the "more similar is better" heuristic
- Participants frequently used ambiguous terminology like "good image quality" and "domain gap," highlighting the need for clearer operational definitions

## Why This Works (Mechanism)

### Mechanism 1: Social Proof and Community Inertia
Researchers select established baseline sources like ImageNet-1K because community norms and reviewer expectations create external validation of these choices, effectively reducing the risk of rejection.

### Mechanism 2: The "More Similar is Better" Heuristic (and its failure modes)
Practitioners rely on perceived visual/semantic similarity to predict performance, but this surface-level intuition often fails to account for underlying feature space distributions, leading to misalignment between expectations and actual results.

### Mechanism 3: Usability and Friction Reduction
Practical availability of pretrained models and ease of implementation often override theoretical dataset suitability, with readily available standard-format models being preferred despite potential performance tradeoffs.

## Foundational Learning

- **Concept: Transfer Learning Factors (Source vs. Target)**
  - Why needed here: The paper deconstructs selection into semantic similarity, visual similarity, and embedding similarity. You must distinguish these to understand why "medical to medical" transfer isn't always better.
  - Quick check question: Can you explain why a dataset of 3D MRI images might be semantically similar but visually dissimilar to 2D histology patches?

- **Concept: Tacit Knowledge in ML**
  - Why needed here: The core argument is that dataset selection relies on "intuition" (tacit knowledge). Understanding this helps you operationalize vague terms into measurable metrics.
  - Quick check question: If a colleague says a dataset has "good image quality" for transfer learning, what two specific follow-up questions should you ask to make that explicit?

- **Concept: Medical Imaging Modalities**
  - Why needed here: The case studies compare Chest X-rays (grayscale, projection) with H&E patches (RGB, texture-heavy). Visual similarity depends entirely on these modality differences.
  - Quick check question: Does "visual similarity" refer to color space similarity or structural texture similarity in this context?

## Architecture Onboarding

- **Component map:**
  - Target Task Definition -> Candidate Pool -> Evaluation Matrix [Domain Sim, Visual Sim, Availability, Size] -> Selector (Human Intuition or automated metric) -> Chosen Pretrained Weights

- **Critical path:**
  1. Define target task constraints (modalities, data scarcity)
  2. Shortlist sources based on Availability and Community Norms
  3. Apply Visual/Semantic Filters to narrow down
  4. Validate intuition empirically, as intuition often misaligns with performance

- **Design tradeoffs:**
  - General vs. Domain Specific: ImageNet offers robustness and ease; domain-specific sources offer specificity but may lack modality match
  - Intuition vs. Automation: Gut feeling is fast but error-prone; calculating embedding similarity is slower but more grounded

- **Failure signatures:**
  - The "Domain Gap" Fallacy: Assuming "Medical Source = Better" for all medical targets
  - Terminology Drift: Team members using "similarity" to mean different things

- **First 3 experiments:**
  1. Have 3 team members rank 3 potential source datasets for a new target task based on gut feeling and visual inspection
  2. Compare the team's ranking against actual fine-tuning performance to test the "More Similar is Better" heuristic
  3. Replace vague terms in project docs with specific metrics like "RGB color distribution match"

## Open Questions the Paper Calls Out
None

## Limitations
- Small sample size (15 participants) limits statistical power for quantitative conclusions
- Survey format with predefined options may miss nuanced reasoning patterns
- Focus on specific medical imaging domains may not capture broader transfer learning dynamics

## Confidence
- High confidence: Community practice influence on source selection
- Medium confidence: Similarity-performance misalignment
- Low confidence: Quantitative strength of usability factor

## Next Checks
1. Replicate findings with larger, more diverse participant pool across different medical specialties
2. Conduct empirical validation study comparing intuitive selections against actual transfer learning performance across 20+ source-target pairs
3. Develop and test HCI tools for operationalizing vague terms like "domain gap" and "image quality" in real research workflows