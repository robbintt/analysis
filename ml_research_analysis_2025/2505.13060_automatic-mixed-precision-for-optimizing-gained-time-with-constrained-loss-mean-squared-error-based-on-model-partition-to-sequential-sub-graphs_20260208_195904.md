---
ver: rpa2
title: Automatic mixed precision for optimizing gained time with constrained loss
  mean-squared-error based on model partition to sequential sub-graphs
arxiv_id: '2505.13060'
source_url: https://arxiv.org/abs/2505.13060
tags:
- time
- gain
- loss
- quantization
- layers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a mixed precision (MP) quantization method
  for optimizing inference time while maintaining loss accuracy in neural networks.
  The key idea is to partition the model into sequential subgraphs and formulate an
  Integer Programming (IP) problem to maximize a chosen performance metric (e.g.,
  empirical time gain, theoretical time gain, or memory gain) under a constrained
  mean-squared-error (MSE) of the loss function.
---

# Automatic mixed precision for optimizing gained time with constrained loss mean-squared-error based on model partition to sequential sub-graphs

## Quick Facts
- arXiv ID: 2505.13060
- Source URL: https://arxiv.org/abs/2505.13060
- Reference count: 31
- Proposes a mixed precision quantization method that partitions models into sequential subgraphs and formulates an Integer Programming problem to maximize performance under a constrained loss MSE.

## Executive Summary
This paper introduces a mixed precision quantization framework for optimizing inference time while maintaining loss accuracy in neural networks. The key innovation is partitioning the model into sequential subgraphs and formulating an Integer Programming problem to maximize performance metrics under a constrained mean-squared-error of the loss function. The method employs a novel sensitivity metric based on first-order Taylor expansion, efficiently computed via forward and backward passes, and demonstrates consistent improvements over baseline strategies on Intel Gaudi 2 accelerator.

## Method Summary
The method partitions a model into sequential subgraphs via DAG analysis, then calibrates per-layer sensitivities using forward/backward passes on a small calibration dataset. It measures empirical time gains for each subgraph across precision configurations and solves an Integer Programming problem to select the optimal mixed-precision configuration under a loss MSE constraint. The approach balances accuracy preservation with performance optimization through hardware-aware timing predictions and analytical sensitivity estimation.

## Key Results
- IP-ET strategy achieves 30% speedup at comparable accuracy for 8B model compared to Random/Prefix baselines
- Sensitivity metric accurately predicts loss MSE across random MP configurations (measured vs predicted shows y≈x relationship)
- Time gains are additive across sequential subgraphs but not across individual layers within subgraphs
- Method validated on Meta-Llama-3.1-8B and Meta-Llama-3.2-1B models across multiple tasks

## Why This Works (Mechanism)

### Mechanism 1: First-Order Taylor Sensitivity Metric
The loss sensitivity to quantization noise is estimated per-layer via gradient-weighted input magnitudes, enabling prediction of loss MSE for arbitrary mixed-precision configurations. Quantization noise is modeled as scaled uniform random variable, and first-order Taylor expansion yields loss perturbation as dot product of quantization noise with loss gradient. The sensitivity metric s_ℓ = ‖z_ℓ ⊙ ż_ℓ‖₂ captures loss change per unit quantization error, computed via standard forward and backward passes.

### Mechanism 2: Sequential Sub-graph Additivity for Time Prediction
Empirical time gains from mixed precision are additive across sequential subgraphs but NOT across individual layers within a subgraph. Modern accelerators execute concurrent operations in parallel and apply kernel fusion, so per-layer time measurements don't sum correctly. Partitioning into single-entry/single-exit sequential subgraphs bounded by branching/merging nodes ensures strict serial execution, making time gains additive across subgraph boundaries.

### Mechanism 3: Integer Programming for Constrained Optimization
Formulating MP configuration selection as an IP problem yields provably optimal solutions under the loss MSE constraint, outperforming heuristic layer-ordering strategies. The optimization maximizes total performance subject to: each group selects exactly one precision configuration, and total loss MSE ≤ threshold. The loss MSE constraint uses the additive sensitivity model, creating a constrained combinatorial optimization solvable via standard IP solvers.

## Foundational Learning

- **Concept: First-order Taylor expansion for sensitivity analysis**
  - Why needed here: The core metric relies on linearizing the loss function around quantization noise. Without understanding that ∂g/∂z_ℓ captures local sensitivity, the additive property is opaque.
  - Quick check question: Given a function f(x) and small perturbation δx, what is the first-order approximation of f(x + δx) - f(x)?

- **Concept: Floating-point quantization noise model**
  - Why needed here: The paper models quantization error as proportional to value magnitude with uniform distribution. Understanding why α_f = 2^(-2m_f)/12 arises from mantissa bit-width is essential.
  - Quick check question: Why does FP8-E4M3 (3 mantissa bits) produce larger quantization noise than BF16 (7 mantissa bits), and how does this relate to Eq. 16?

- **Concept: Directed Acyclic Graph (DAG) partitioning**
  - Why needed here: The sub-graph identification algorithm traverses the computation DAG to find sequential regions. Understanding BFS, path lengths, and single-entry/single-exit properties is prerequisite.
  - Quick check question: In a DAG, what property ensures that two nodes can execute in parallel vs. must execute sequentially?

## Architecture Onboarding

- **Component map:** Sensitivity Calibration Module -> Partition Engine -> Time Measurement Harness -> IP Solver
- **Critical path:** 1) Partition model → identify sub-graphs (determines granularity of optimization) 2) Calibrate sensitivities → one forward/backward pass (defines loss constraint surface) 3) Measure time gains → benchmark each sub-graph configuration (defines objective) 4) Solve IP → produce deployment configuration
- **Design tradeoffs:** Sub-graph granularity: Finer partitions increase configuration space (exponential) but capture more local timing effects. Coarser partitions reduce IP complexity but may miss optimization opportunities. Calibration dataset size: More samples improve sensitivity estimates but increase setup time. Paper uses 20% of evaluation data. Threshold τ: Lower values preserve accuracy but reduce time gains.
- **Failure signatures:** Loss MSE predictions systematically lower than measured → sensitivity underestimates true noise impact. Time gains not additive across sub-graphs → partition incorrectly identified sequential regions. IP infeasible → threshold τ too tight for any valid MP configuration.
- **First 3 experiments:** 1) Validate sensitivity additivity: Compare predicted loss MSE vs. measured loss MSE across random MP configurations. Plot should show y ≈ x with low scatter. 2) Validate time additivity: For a partitioned model, compare sum of per-sub-graph time gains vs. end-to-end measured time gain. Should match within measurement noise. 3) Ablate partition granularity: Compare optimization quality when treating each layer as a sub-graph vs. using the partitioning algorithm. Expect naive per-layer approach to show time prediction errors due to parallelism.

## Open Questions the Paper Calls Out

### Open Question 1
How does the IP-ET strategy's reliance on empirical time measurements for sequential sub-graphs generalize to hardware architectures with different kernel fusion scheduling or memory hierarchies? The partitioning algorithm assumes specific sequential dependencies to ensure time-gain additivity; this assumption might break or require re-partitioning on hardware with vastly different parallel execution strategies.

### Open Question 2
Can the optimization objective be refined to ensure improvements in loss-MSE consistently translate to accuracy gains across all tasks, given the observed divergence in LAMBADA? The method treats loss-MSE as the sole constraint proxy for model quality, but results show this proxy can decouple from the target metric (accuracy) depending on the task.

### Open Question 3
Does the relative performance advantage of the proposed method diminish or shift as model scale increases beyond 8B parameters? The paper implies a correlation between the "gap" (quantization sensitivity) and the method's utility, but it remains unclear if the sensitivity-based IP approach offers significant gains over simpler baselines for models where lower precision is naturally more stable.

## Limitations
- Sub-graph partitioning method assumes hardware-specific timing behaviors are static across MP configurations; if compiler optimizations cross sub-graph boundaries, additive time-gain assumption may fail.
- Sensitivity metric relies on first-order Taylor approximation, which degrades when quantization noise is large relative to full-precision values.
- IP formulation assumes exact sensitivity predictions; systematic bias in sensitivity estimates could lead to constraint violations in practice.

## Confidence

**High confidence**: First-order Taylor sensitivity metric derivation and implementation (well-established technique, clear mathematical framework)

**Medium confidence**: Sub-graph additivity assumption for time prediction (empirically validated but hardware-dependent)

**Medium confidence**: IP formulation yields optimal solutions (correct formulation but sensitive to input accuracy)

## Next Checks

1. **Sensitivity monotonicity test**: Verify that sensitivity metric s_ℓ increases monotonically with quantization noise magnitude for individual layers. This validates the first-order approximation assumption.

2. **Cross-hardware reproducibility**: Implement the method on a different accelerator (e.g., NVIDIA A100) and measure whether sub-graph time additivity holds. If not, the method requires hardware-specific calibration.

3. **Constraint satisfaction verification**: After deploying IP-selected MP configuration, measure actual loss MSE on held-out data and compare against the threshold τ. This validates the sensitivity prediction accuracy in practice.