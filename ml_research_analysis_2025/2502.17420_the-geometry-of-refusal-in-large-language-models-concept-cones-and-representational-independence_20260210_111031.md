---
ver: rpa2
title: 'The Geometry of Refusal in Large Language Models: Concept Cones and Representational
  Independence'
arxiv_id: '2502.17420'
source_url: https://arxiv.org/abs/2502.17420
tags:
- refusal
- directions
- arxiv
- direction
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# The Geometry of Refusal in Large Language Models: Concept Cones and Representational Independence

## Quick Facts
- arXiv ID: 2502.17420
- Source URL: https://arxiv.org/abs/2502.17420
- Reference count: 37
- Key outcome: Refutation of single linear refusal direction; discovery of multi-dimensional refusal cones with representational independence properties

## Executive Summary
This paper challenges the prevailing assumption that refusal in large language models is mediated by a single linear activation direction. Through gradient-based optimization, the authors identify multi-dimensional "refusal cones" containing infinitely many effective directions, and introduce the concept of representational independence to capture true causal independence beyond mere orthogonality. The work provides both practical jailbreaking methods and theoretical insights into the geometric structure of LLM safety mechanisms.

## Method Summary
The authors propose a gradient-based representation engineering approach (RDO) that optimizes refusal directions using three loss terms: ablation loss (forcing answers to harmful prompts when direction is removed), addition loss (forcing refusals to harmless prompts when direction is added), and retain loss (preserving normal behavior via KL divergence). They extend this to optimize orthonormal bases forming refusal cones (RCO) and introduce representational independence constraints that ensure ablation of one direction doesn't affect the representation strength of another at any layer. The method is evaluated on JAILBREAKBENCH, TruthfulQA, and other benchmarks across multiple model families.

## Key Results
- RDO achieves 86.8% ASR on JAILBREAKBENCH versus 70.2% for DIM baseline
- TruthfulQA degradation reduced from 9.1% (DIM) to 5.3% (RDO)
- Multi-dimensional cones up to 5D maintain high ASR; larger models support higher-dimensional cones
- Orthogonal directions can still causally interfere through non-linear layer transformations, requiring representational independence constraints

## Why This Works (Mechanism)

### Mechanism 1: Gradient-based Refusal Direction Optimization (RDO)
Gradient descent on custom loss functions yields refusal directions with fewer side effects than contrastive prompting (DIM). The method encodes refusal properties into three loss terms and optimizes with unit norm constraint. Core assumption: refusal behavior can be characterized functionally through monotonic scaling and surgical ablation properties that transfer across prompt distributions. Break condition: if optimization targets don't generalize across harmful/harmless prompt distributions, the learned direction overfits.

### Mechanism 2: Multi-dimensional Refusal Cones
Refusal is mediated by polyhedral cones containing infinitely many effective directions, not a single axis. The method extends RDO to optimize an orthonormal basis where all convex combinations with λᵢ ≥ 0 satisfy refusal properties. Core assumption: multiple conceptually distinct aspects of refusal can be linearly combined without interference. Break condition: if the model's residual stream dimension is too small or refusal is encoded non-linearly, cone basis vectors will degrade in effectiveness as dimensionality increases.

### Mechanism 3: Representational Independence (RepInd)
Orthogonal directions can still causally interfere through non-linear layer transformations; true independence requires that ablating one direction leaves the cosine similarity of another unchanged at all layers. The method defines RepInd via layer-wise cosine similarity constraints and adds independence loss terms. Core assumption: non-linear interactions between layers cause orthogonal directions to influence each other's representation strength downstream. Break condition: if cosine similarity doesn't capture causal relevance, RepInd constraints may be too strict or insufficient.

## Foundational Learning

- **Concept: Residual Stream Activation Space**
  - Why needed here: All interventions operate on x^(l)_i, the residual stream at layer l and token position i. Understanding that information flows through additive residual connections is prerequisite.
  - Quick check question: If you ablate direction r at layer 5, which layers' activations are affected?

- **Concept: Directional Ablation vs. Activation Subtraction**
  - Why needed here: The paper distinguishes projection-based ablation (removing component along r) from subtraction (shifting by fixed amount). These have different effects.
  - Quick check question: Which intervention preserves the norm of the activation vector?

- **Concept: Linear Representation Hypothesis**
  - Why needed here: The paper challenges and extends the assumption that concepts are encoded as single linear directions. You need to understand what "linear direction" means before appreciating multi-dimensional cones.
  - Quick check question: If refusal were purely linear and one-dimensional, what would happen if you found two orthogonal refusal directions?

## Architecture Onboarding

- **Component map:**
  - SALADBENCH/ALPACA datasets -> Target generation (DIM baseline) -> RDO/RCO optimizer -> Unit vector/basis B -> Evaluation (JAILBREAKBENCH, TruthfulQA)

- **Critical path:**
  1. Generate targets (t_answer, t_refusal, t_retain) using DIM direction on frozen model
  2. Initialize r randomly, optimize with AdamW
  3. Select best direction from final 20 steps using refusal score heuristic
  4. For cones: extend to basis B, sample and evaluate at test time

- **Design tradeoffs:**
  - Higher λ_ret → fewer side effects but potentially lower ASR (Figure 15 shows Pareto frontier)
  - Higher cone dimension → captures more refusal aspects but individual basis vectors may weaken
  - More RepInd constraints → cleaner mechanistic interpretation but harder optimization

- **Failure signatures:**
  - RDO direction causes benchmark collapse → λ_ret too low
  - Cone samples have high variance in ASR → basis vectors poorly cover refusal space
  - RepInd direction has low ASR → over-constrained; relax layer cutoff or λ_ind

- **First 3 experiments:**
  1. Reproduce Figure 2: Train RDO on Qwen2.5-3B with default hyperparameters; compare ASR on JAILBREAKBench against DIM baseline
  2. Ablate retain loss: Set λ_ret = 0 and measure TruthfulQA degradation vs. default; confirms retain loss purpose
  3. Find one RepInd direction: Extend RDO with L_RepInd loss relative to DIM direction; verify Figure 6 pattern (cosine similarity stability under intervention)

## Open Questions the Paper Calls Out

### Open Question 1
Can the gradient-based RDO method identify additional independent refusal mechanisms when using diverse optimization targets or reinforcement learning with judge-based rewards? All refusal directions were optimized against identical targets, potentially constraining discovery of mechanistically distinct refusal pathways. Experiments with varied targets or RL rewards could identify additional independent mechanisms.

### Open Question 2
What determines the maximum dimensionality of refusal cones in LLMs, and why does cone performance degrade faster in smaller models? Figure 4 shows cone performance degrading with dimensionality more rapidly for smaller Qwen models. The relationship between model capacity and refusal representation dimensionality is observational rather than theoretically grounded.

### Open Question 3
Are there other undiscovered regions in activation space beyond the identified refusal cones that mediate refusal behavior? While the paper establishes the existence of higher-dimensional refusal cones, it cannot rule out the possibility of other yet-undiscovered regions that mediate refusal. Exhaustive search using diverse initialization strategies could reveal refusal-mediating regions outside discovered cones.

### Open Question 4
Can the representational independence framework and gradient-based representation engineering generalize to concepts beyond refusal? The conclusion states the approach can be extended to identify various concepts beyond refusal by changing optimization targets. While theoretically generalizable, the method has only been validated on refusal behavior; its effectiveness for other concepts remains untested.

## Limitations
- Limited empirical validation of Representational Independence beyond a single baseline direction
- Dataset and target generation opacity due to unspecified chat templates and tokenization handling
- Scaling assumptions for cone dimensions based on limited model size range without theoretical grounding

## Confidence
- **High confidence:** Core RDO mechanism is well-specified and validated across multiple models and benchmarks with robust TruthfulQA degradation reduction
- **Medium confidence:** Multi-dimensional cone extension logically follows from RDO but practical benefits versus multiple independent directions are not clearly established
- **Low confidence:** Representational Independence framework has the least empirical support, showing plausibility against one baseline but lacking systematic validation

## Next Checks
1. **RepInd independence validation:** Train multiple RDO directions on the same model, then attempt to find RepInd directions for each pair. Measure whether RepInd directions truly show independence (cosine stability) when ablating non-corresponding directions, versus orthogonal but dependent directions that fail this test.

2. **Template ablation study:** Implement RDO with identical hyperparameters but systematically vary chat templates and tokenization schemes across the same model family. Quantify how much refusal direction effectiveness varies with these implementation details to establish sensitivity bounds.

3. **Cone vs. independent directions comparison:** For a fixed model and dataset, generate d independent RDO directions and compare their combined ASR to a d-dimensional cone from RCO. Measure both average ASR and variance across samples to determine if the cone structure provides practical advantages over simpler multi-direction approaches.