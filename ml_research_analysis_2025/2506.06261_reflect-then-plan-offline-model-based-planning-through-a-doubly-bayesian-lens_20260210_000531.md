---
ver: rpa2
title: 'Reflect-then-Plan: Offline Model-Based Planning through a Doubly Bayesian
  Lens'
arxiv_id: '2506.06261'
source_url: https://arxiv.org/abs/2506.06261
tags:
- offline
- refplan
- planning
- policy
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RefPlan, a novel doubly Bayesian approach
  to offline model-based planning that integrates epistemic uncertainty modeling with
  planning in a unified probabilistic framework. The method addresses the challenge
  of limited data coverage in offline RL by explicitly modeling the agent's belief
  over environment dynamics and marginalizing over this uncertainty during planning.
---

# Reflect-then-Plan: Offline Model-Based Planning through a Doubly Bayesian Lens

## Quick Facts
- arXiv ID: 2506.06261
- Source URL: https://arxiv.org/abs/2506.06261
- Reference count: 40
- One-line primary result: RefPlan improves prior policy performance by 11.6% on average vs 5.3% for baselines on D4RL benchmarks

## Executive Summary
This paper introduces RefPlan, a novel doubly Bayesian approach to offline model-based planning that integrates epistemic uncertainty modeling with planning in a unified probabilistic framework. The method addresses the challenge of limited data coverage in offline RL by explicitly modeling the agent's belief over environment dynamics and marginalizing over this uncertainty during planning. At test time, RefPlan updates its belief using real-time observations and uses this posterior to guide model-based planning, improving upon fixed conservative policies.

The key innovation lies in recasting planning as Bayesian posterior estimation, allowing the agent to consider multiple plausible scenarios beyond its immediate knowledge. Experimental results on standard D4RL benchmarks demonstrate that RefPlan consistently outperforms existing methods, particularly under challenging conditions such as out-of-distribution states, limited data availability, and changing environment dynamics. For example, RefPlan improved prior policy performance by 11.6% on average compared to 5.3% for baseline methods, with statistically significant improvements across multiple metrics.

## Method Summary
RefPlan operates in two stages: first, a prior policy is trained using any offline RL algorithm on the dataset; second, a VAE (encoder + probabilistic decoder ensemble) is trained to capture epistemic uncertainty. At test time, RefPlan encodes the trajectory history into a latent belief distribution, samples multiple dynamics hypotheses, and performs model-based planning by marginalizing over these samples to compute the final action. This approach recasts planning as Bayesian posterior estimation, allowing the agent to reason about multiple plausible scenarios and improve upon fixed conservative policies.

## Key Results
- RefPlan improves prior policy performance by 11.6% on average vs 5.3% for baselines on D4RL benchmarks
- Superior performance under out-of-distribution states, limited data, and changing environment dynamics
- Statistically significant improvements across multiple metrics including normalized score and RLiable metrics

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Test-time belief updating over environment dynamics reduces performance degradation under epistemic uncertainty.
- **Mechanism:** A variational encoder (GRU-based RNN) maps real-time trajectory history $\tau_{:t} = (s_0, a_0, r_0, \ldots, s_t)$ to a latent Gaussian distribution $q_\phi(m_t | \tau_{:t})$ that approximates the posterior over possible MDPs. This belief state is updated incrementally as new observations arrive, enabling the agent to adapt its model to the actual deployment dynamics rather than relying solely on the offline dataset distribution.
- **Core assumption:** The latent variable $m_t$ is sufficient to capture task-relevant epistemic uncertainty, and the variational approximation $q_\phi$ is expressive enough to represent the true posterior structure.
- **Evidence anchors:**
  - [Abstract] "At deployment, it updates a belief over environment dynamics using real-time observations, incorporating uncertainty into MB planning via marginalization."
  - [Section 3.2] "We assume that knowing the posterior distribution $p(m|\tau_{:t})$ is sufficient for planning under epistemic uncertainty."
  - [Corpus] Neighbor paper "On Efficient Bayesian Exploration in Model-Based Reinforcement Learning" explicitly targets epistemic uncertainty for exploration, suggesting this is an active research direction but corpus does not directly validate this specific mechanism.
- **Break condition:** If the encoder fails to distinguish meaningfully different dynamics (e.g., collapsed latent space with near-zero variance), or if out-of-distribution trajectories produce unreliable posteriors, belief-guided planning will degrade toward prior-only performance.

### Mechanism 2
- **Claim:** Recasting planning as Bayesian posterior inference over action sequences enables principled integration of offline-learned policies as priors.
- **Mechanism:** Control-as-inference framework treats optimal trajectories as samples from $p(\tau | O)$ where $O$ denotes optimality variables. The prior policy $\pi_p$ learned offline defines the action prior $p(\tau) = \prod_h \pi_p(a_{t+h}|s_{t+h}) \hat{p}_\psi(s_{t+h+1}|s_{t+h}, a_{t+h})$. Posterior estimation via importance sampling weights action sequences by $\exp(\kappa \sum_h r_{t+h})$, naturally balancing reward maximization with staying near the offline-learned behavior.
- **Core assumption:** The exponential form $p(O|\tau) \propto \exp(\kappa \sum_h r_{t+h})$ appropriately captures optimality, and the prior policy provides meaningful guidance (not arbitrarily bad).
- **Evidence anchors:**
  - [Section 3.1] "This approach enables the agent to optimize its actions by reasoning over the learned dynamics model and prior knowledge obtained from offline training."
  - [Equation 6-7] Derivation shows importance sampling with prior policy as proposal distribution.
  - [Corpus] No direct corpus validation of this specific inference formulation for offline MB planning.
- **Break condition:** If the prior policy is severely suboptimal or the reward model is miscalibrated, the weighted sampling will either fail to improve over prior or exploit model inaccuracies.

### Mechanism 3
- **Claim:** Marginalizing over the latent belief during planning produces more robust action selection than single MAP or mean estimates.
- **Mechanism:** Rather than conditioning on a single dynamics hypothesis $\hat{p}_\psi(\cdot|\cdot, \cdot, \mu_t)$, RefPlan computes $\mathbb{E}_{p(\tau|O)}[a_{t:t+H}] = \mathbb{E}_{m_t \sim q_\phi}[\mathbb{E}_{p(\tau|O, m_t)}[a_{t:t+H} | m_t]]$ via Monte Carlo with $\bar{n}$ latent samples. This averages planned actions across multiple plausible dynamics, hedging against model uncertainty.
- **Core assumption:** Monte Carlo sampling with moderate $\bar{n}$ (1-16 in experiments) sufficiently approximates the marginal, and action-space averaging produces coherent behavior.
- **Evidence anchors:**
  - [Section 3.3, Equation 10] Explicit marginalization formula with practical sampling approximation.
  - [Table 3] RefPlan outperforms LOOP even when LOOP is given 16x computational budget, suggesting gains come from principled uncertainty handling, not just more samples.
  - [Corpus] "Long-Horizon Model-Based Offline Reinforcement Learning Without Conservatism" revisits Bayesian perspectives as alternatives to conservatism, providing indirect support but no direct validation of marginalization.
- **Break condition:** If the latent space is high-dimensional or multimodal, small $\bar{n}$ may inadequately capture uncertainty; conversely, large $\bar{n}$ may introduce variance that destabilizes action selection.

## Foundational Learning

- **Concept: Control-as-Inference / Probabilistic Planning**
  - **Why needed here:** RefPlan derives its planning objective from this framework; understanding how optimality variables induce posterior distributions over trajectories is essential to grasp why the algorithm samples and weights trajectories rather than optimizing directly.
  - **Quick check question:** Given a trajectory $\tau$ with rewards $[1, 2, 1]$ and inverse temperature $\kappa = 1$, what is the relative weight $w \propto p(O|\tau)$ compared to a trajectory with rewards $[0, 0, 0]$?

- **Concept: Variational Autoencoders for Sequential Data (Amortized Inference)**
  - **Why needed here:** The encoder $q_\phi(m_t|\tau_{:t})$ amortizes posterior inference over trajectory histories; recognizing that this is learned via ELBO optimization on reconstruction helps explain the two-stage training procedure and why the decoder matters for planning.
  - **Quick check question:** Why does the ELBO for this VAE include a KL term $KL(q_\phi(m_t|\tau_{:t}) || p(m_t))$, and what prior $p(m_t)$ is used at $t > 0$?

- **Concept: Epistemic vs. Aleatoric Uncertainty in RL**
  - **Why needed here:** The paper explicitly frames offline RL as an epistemic POMDP where uncertainty stems from limited data coverage, not inherent environment stochasticity; distinguishing these is critical for understanding what the latent variable captures and what it doesn't.
  - **Quick check question:** If an environment has stochastic transitions with known probabilities, does RefPlan's latent variable primarily model epistemic or aleatoric uncertainty?

## Architecture Onboarding

- **Component map:** Prior Policy $\pi_p$ -> Variational Encoder $q_\phi$ -> Probabilistic Ensemble Decoder $\hat{p}_\psi$ -> Trajectory Optimizer

- **Critical path:**
  1. Pretrain prior policy with chosen offline RL algorithm on dataset $\mathcal{D}$.
  2. Train VAE (encoder + decoder) via ELBO on trajectory segments from $\mathcal{D}$ (Stage 1).
  3. Freeze encoder, fine-tune decoder via MLE for next-state prediction (Stage 2).
  4. At deployment: each step $t$, encode history $\tau_{:t}$, sample $\bar{n}$ latents, generate $\bar{N}$ prior plans, rollout under each latent, compute marginal posterior action.

- **Design tradeoffs:**
  - **Planning horizon $H$**: Longer $H$ captures more lookahead but increases compounding model error; paper uses $H \in \{2, 4\}$.
  - **Number of latent samples $\bar{n}$**: Higher $\bar{n}$ better approximates marginal but increases compute; experiments show diminishing returns beyond 8-16 (Figure 5).
  - **Inverse temperature $\kappa$**: Controls exploitation-exploration in importance weights; higher $\kappa$ focuses on high-reward trajectories but may over-exploit model errors.
  - **Ensemble size**: 20 models with 14 elite balances diversity and quality; smaller ensembles reduce variance but may miss uncertainty.

- **Failure signatures:**
  - **Latent collapse**: Encoder outputs near-zero variance $\sigma_t^2 \approx 0$—belief stops updating, marginalization becomes meaningless. Check $\sigma_t$ statistics during rollout.
  - **Prior-exploitation gap**: RefPlan barely improves over prior policy—suggests encoder is not capturing meaningful uncertainty or reward model is uninformative.
  - **OOD trajectory explosion**: Large state prediction errors when rolling out—decoder generalization failure, may need uncertainty penalty (variance-based, as in LOOP).
  - **Computational bottleneck**: Runtime scales with $\bar{n} \times \bar{N} \times H$—if real-time constraint violated, reduce $\bar{n}$ first (sub-linear impact, per Table 15).

- **First 3 experiments:**
  1. **Encoder sanity check**: Train VAE on a single dataset, visualize latent space (t-SNE/PCA) colored by trajectory characteristics (reward sum, state coverage). Verify latent encodes meaningful variation, not collapsed to single point.
  2. **Ablation on $\bar{n}$**: Run RefPlan with $\bar{n} \in \{1, 2, 4, 8, 16\}$ on one environment (e.g., HalfCheetah-MR) with fixed prior. Plot normalized score vs. $\bar{n}$ and sample variance of posterior mean actions. Confirm marginalization benefit.
  3. **OOD robustness test**: Train prior on medium-expert (ME) dataset, evaluate on initial states sampled from random (R) dataset (replicating Table 1 setup). Compare RefPlan vs. prior-only vs. LOOP. Verify RefPlan mitigates degradation.

## Open Questions the Paper Calls Out

- **Question:** Would direct sampling methods (e.g., SIR) yield better planning performance than the current Monte Carlo marginalization approach in environments with highly multi-modal uncertainty?
- **Question:** Can integrating data augmentation specifically for single-task offline RL further improve RefPlan's adaptability to changing environment dynamics?
- **Question:** Does the doubly Bayesian framework scale effectively to more complex environments and model architectures?

## Limitations
- The paper's claims about epistemic uncertainty modeling rely on untested assumptions about the encoder's ability to capture task-relevant dynamics uncertainty
- Computational overhead of marginalization (scaling with $\bar{n} \times \bar{N} \times H$) is not thoroughly analyzed
- The paper doesn't address potential negative transfer when the prior policy is severely suboptimal or when the reward model is miscalibrated

## Confidence
- **High confidence:** Experimental results showing RefPlan's superior performance on D4RL benchmarks across multiple metrics
- **Medium confidence:** The theoretical framework of recasting planning as Bayesian posterior inference and the two-stage VAE training procedure
- **Medium confidence:** Claims about robustness to OOD states and limited data, though these could benefit from more rigorous analysis of failure modes
- **Low confidence:** Assertions about the encoder capturing meaningful epistemic uncertainty without direct validation of latent space structure or uncertainty quantification

## Next Checks
1. **Latent space analysis:** Visualize the encoder's latent distribution using t-SNE/PCA on trajectory embeddings, colored by reward sum and state coverage. Verify the latent space captures meaningful variation rather than collapsing to a single point.
2. **Ablation on marginalization samples:** Systematically vary $\bar{n} \in \{1, 2, 4, 8, 16\}$ and plot normalized score vs. sample variance of posterior mean actions. Confirm that marginalization provides measurable benefits beyond single-sample planning.
3. **OOD generalization stress test:** Train prior on medium-expert dataset, then evaluate RefPlan on initial states sampled from random dataset. Compare performance degradation against prior-only and LOOP baselines to quantify OOD robustness.