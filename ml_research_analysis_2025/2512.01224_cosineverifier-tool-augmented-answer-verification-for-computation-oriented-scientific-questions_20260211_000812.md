---
ver: rpa2
title: 'CoSineVerifier: Tool-Augmented Answer Verification for Computation-Oriented
  Scientific Questions'
arxiv_id: '2512.01224'
source_url: https://arxiv.org/abs/2512.01224
tags:
- answer
- arxiv
- verification
- data
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'CoSineVerifier introduces a tool-augmented verifier for computation-oriented
  scientific questions, leveraging external executors for precise computations and
  symbolic simplifications. It employs a two-stage pipeline: cold-start fine-tuning
  followed by multi-turn reinforcement learning with tool integration.'
---

# CoSineVerifier: Tool-Augmented Answer Verification for Computation-Oriented Scientific Questions

## Quick Facts
- **arXiv ID:** 2512.01224
- **Source URL:** https://arxiv.org/abs/2512.01224
- **Reference count:** 40
- **Primary result:** Tool-augmented verifier achieves state-of-the-art performance on VerifyBench-Hard and SCI-Bench, and improves RLVR performance on AIME benchmarks.

## Executive Summary
CoSineVerifier introduces a tool-augmented verifier specifically designed for computation-oriented scientific questions. The system addresses the calculation inaccuracy problem in large language models by integrating external executors for precise computations and symbolic simplifications. Through a two-stage pipeline combining cold-start fine-tuning with multi-turn reinforcement learning, the verifier learns to effectively use tools while avoiding common failure modes like tool loops. The model demonstrates superior performance compared to both rubric-based and model-based verifiers, showing strong generalization capabilities when deployed as a reward model in reinforcement learning with verifiable rewards (RLVR) settings.

## Method Summary
CoSineVerifier employs a two-stage training approach: cold-start fine-tuning followed by multi-turn reinforcement learning. The model starts with Qwen3-4B-Instruct as its backbone and undergoes SFT on 8.5k tool-use traces to learn proper syntax and usage patterns. This is followed by DAPO-based RL training on 9.4k "hard" samples with a composite reward function combining answer correctness and tool usage encouragement. The system uses a sandbox environment for executing Python code and a unit conversion utility as its primary tools. Data preparation involves filtering 1.14M candidates down to 63k disagreement cases, which are then augmented with synthetic long-tail error examples to ensure comprehensive coverage of verification failure modes.

## Key Results
- Achieves state-of-the-art performance on VerifyBench-Hard and SCI-Bench benchmarks
- Outperforms both rubric-based and model-based verifiers on AIME'24 and AIME'25 when used as reward model in RLVR
- Demonstrates strong generalization across different domains and question types
- Shows consistent improvement over parametric-only models, particularly on calculation-heavy tasks

## Why This Works (Mechanism)

### Mechanism 1: External Tool Integration
External tool integration mitigates calculation inaccuracy by outsourcing high-precision arithmetic and symbolic manipulation to a deterministic Python interpreter. The model generates executable Python code to evaluate both candidate and reference answers numerically, comparing floating-point results rather than relying on parametric guessing. This approach is particularly effective for tasks like verifying algebraic equivalences that are difficult for LLMs to handle internally.

### Mechanism 2: Cold-Start + RL Training Pipeline
The cold-start fine-tuning followed by multi-turn reinforcement learning stabilizes agentic tool use and prevents the model from hallucinating tool outputs or entering infinite loops. SFT first teaches the model proper tool invocation syntax, while subsequent RL optimization using shaped rewards ($R_{ans} + R_t$) encourages effective tool use only when they lead to correct verdicts. This staged approach is critical for preventing the tool-loop failures that occur with direct RL training.

### Mechanism 3: Long-Tail Error Synthesis
Targeted synthesis of long-tail error types improves robustness against diverse failure modes by addressing the skewed distribution of verification errors. By analyzing 63k disagreement cases and identifying that top 5 error categories account for 85% of failures, the authors use a teacher model to synthesize examples of under-represented errors. This data augmentation balances the training distribution and forces the model to learn specific correction strategies for rare cases.

## Foundational Learning

**Reinforcement Learning with Verifiable Rewards (RLVR)**
- *Why needed:* CoSineVerifier serves as the "Reward Model" in an RLVR loop, where its binary output drives the policy update of a separate reasoning model.
- *Quick check:* How does a binary verification signal (Correct/Incorrect) translate into a gradient update for a generative LLM?

**Agentic Tool Use (Reasoning-Action Interleaving)**
- *Why needed:* The verifier is an active agent that must decide when to think, when to code, and how to interpret results, not just a passive classifier.
- *Quick check:* What's the difference between a standard Chain-of-Thought trace and a Tool-Integrated Reasoning trace in terms of token sequence?

**Outcome Reward Models (ORM) vs. Process Reward Models (PRM)**
- *Why needed:* CoSineVerifier is an outcome verifier focusing on final answer correctness rather than step-by-step validity.
- *Quick check:* Why might an ORM be sufficient for scientific questions with definitive ground-truth answers, unlike open-ended creative writing?

## Architecture Onboarding

**Component map:**
Qwen3-4B-Instruct -> OpenRLHF (SFT) -> Verl (DAPO/RL) -> Sandbox (Python interpreter + Unit Conversion Utility)

**Critical path:**
1. Data Prep: Filter 1.14M samples to find "disagreement" cases
2. Cold Start: SFT on short, correct tool-call traces (max 200 tokens) to ground syntax
3. RL Phase: Train using DAPO on "hard" synthetic data with $R_{ans} + R_{tool\_encourage}$ rewards
4. Inference: Model receives (Q, A_model, A_ref), generates reasoning/code, executes in sandbox, outputs verdict

**Design tradeoffs:**
- 4B + Tool vs. 32B No-Tool: Tool-augmentation allows small model to outperform large parametric model on calculation tasks, trading inference latency for accuracy and lower parameter cost
- Cold Start Necessity: Direct RL fails; SFT is strictly required to prevent "tool loops"

**Failure signatures:**
- Tool Loops: Model repeatedly calls tools without resolving (mitigated by Cold Start)
- Calculation Hallucination: Model outputs verdict without using tools on obvious math problems (mitigated by RL tool-encouragement)
- Format False Negatives: Strict rubric-based verification failing on correct but poorly formatted answers (solved by model-based approach)

**First 3 experiments:**
1. Reproduce the "Cold Start vs. Direct RL" Ablation to verify tool loop degradation
2. Sandbox Latency Profiling to measure Python interpreter overhead vs. pure token generation
3. Cross-Domain Generalization testing on held-out datasets to verify long-tail augmentation generalization

## Open Questions the Paper Calls Out

**Open Question 1:** Can the proposed data synthesis strategy effectively saturate the long-tail distribution of verification errors?
- Basis: The introduction notes error types follow a long-tail distribution, making comprehensive coverage difficult
- Why unresolved: While 10k synthetic examples are generated, the remaining coverage gap is not quantified
- Evidence needed: Ablation study measuring accuracy specifically on held-out, synthetic long-tail error types

**Open Question 2:** Does the specific reward shaping for tool usage inadvertently penalize valid internal reasoning?
- Basis: Section 4.4.2 defines a tool-encouragement reward that penalizes incorrect answers with zero tool usage
- Why unresolved: This may discourage learning to verify simple instances using internal knowledge
- Evidence needed: Analysis of tool call rate on trivial verification tasks to detect unnecessary tool dependencies

**Open Question 3:** How robust is the verifier when integrated into RLVR pipelines if the sandbox environment encounters execution errors or timeouts?
- Basis: The method relies on a "Sandbox" to execute code, but doesn't discuss system behavior when tools fail
- Why unresolved: Tool crashes could result in null rewards or misleading penalties, destabilizing policy training
- Evidence needed: Experiments measuring RL training stability with probabilistic tool execution failures

## Limitations

- **Sandbox Reliability:** Specific implementation details for secure, deterministic code execution are not provided, affecting reproducibility and safety
- **Generalization to Non-Computational Tasks:** Performance on pure logical reasoning or multi-step proofs without numerical components remains untested
- **Synthetic Data Fidelity:** Risk that GPT-o3-generated synthetic examples don't accurately represent real-world failure modes, potentially limiting robustness

## Confidence

- **High Confidence:** Core claim that tool-augmented verification outperforms pure parametric models on computation-heavy tasks is well-supported by experimental results
- **Medium Confidence:** Assertion that cold-start SFT is strictly necessary to prevent tool loops is supported by ablation studies, but exact failure dynamics may vary
- **Low Confidence:** Claim about synthetic data augmentation being essential for handling long-tail errors is based on internal ablation studies without independent validation

## Next Checks

1. **Cross-Domain Transfer:** Test CoSineVerifier on held-out dataset of pure mathematical proofs or logical reasoning questions to assess generalization beyond numerical computation

2. **Sandbox Failure Modes:** Conduct controlled experiments where Python interpreter is deliberately made to fail (timeouts, memory errors, security violations) to verify model robustness to tool execution failures

3. **Reward Function Sensitivity:** Systematically vary relative weights of $R_{ans}$ and $R_t$ in shaped reward to determine if claimed optimal balance is robust to hyperparameter changes