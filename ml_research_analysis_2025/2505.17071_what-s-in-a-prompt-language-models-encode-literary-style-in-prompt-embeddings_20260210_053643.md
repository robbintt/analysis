---
ver: rpa2
title: What's in a prompt? Language models encode literary style in prompt embeddings
arxiv_id: '2505.17071'
source_url: https://arxiv.org/abs/2505.17071
tags:
- embeddings
- style
- information
- language
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates how information from an entire prompt\
  \ is encoded in the embeddings of large language models (LLMs), focusing on intangible\
  \ aspects like literary style. Using short excerpts (10\u2013100 tokens) from various\
  \ 19th- and early 20th-century novels, the authors show that embeddings from later\
  \ transformer layers contain distinctive stylistic features specific to individual\
  \ authors."
---

# What's in a prompt? Language models encode literary style in prompt embeddings

## Quick Facts
- arXiv ID: 2505.17071
- Source URL: https://arxiv.org/abs/2505.17071
- Reference count: 17
- Authors identify that LLM embeddings encode distinctive stylistic features specific to individual authors, enabling authorship classification with over 90% accuracy

## Executive Summary
This study investigates how information from an entire prompt is encoded in the embeddings of large language models (LLMs), focusing on intangible aspects like literary style. Using short excerpts (10–100 tokens) from various 19th- and early 20th-century novels, the authors show that embeddings from later transformer layers contain distinctive stylistic features specific to individual authors. They demonstrate that excerpts can be classified by authorship with over 90% accuracy using an MLP probe, especially when context length and layer depth increase. Notably, books by the same author are more entangled than those by different authors, suggesting that embeddings encode lexical and syntactic style rather than factual content. This effect persists across multiple LLM architectures (Llama-3.2-1B, gemma-3-1b-pt, Qwen3-1.7B-Base, SmolLM2-1.7B). Shuffling token order preserves separability, indicating that style is more lexical than syntactic. These findings reveal that LLMs compress and encode abstract stylistic information in deep representations, opening potential applications in authorship attribution and literary analysis.

## Method Summary
The authors extracted embeddings from later transformer layers of various LLMs (Llama-3.2-1B, gemma-3-1b-pt, Qwen3-1.7B-Base, SmolLM2-1.7B) when processing short literary excerpts (10-100 tokens) from 13 19th-20th century novels from Project Gutenberg. They split texts into non-overlapping chunks of varying lengths (N=8, 16, 32, 64, 128 tokens) and extracted the last token embedding after each transformer layer. For binary classification (GE vs VW), they used a linear SVM on PCA-reduced (64-dim) embeddings with 70/30 train/val split. For multiclass classification (13 books), they used an MLP with a 32-dim penultimate layer, cross-entropy loss, Adam optimizer. They also tested token shuffling with block sizes B=1, 4, 32 to assess the contribution of syntactic structure versus lexical content to style encoding.

## Key Results
- Authorship classification accuracy exceeds 90% using MLP probes on later transformer layers
- Books by the same author are more entangled than those by different authors, suggesting embeddings encode stylistic rather than factual content
- Shuffling token order preserves separability, indicating style is more lexical than syntactic
- Results persist across multiple LLM architectures (Llama-3.2-1B, gemma-3-1b-pt, Qwen3-1.7B-Base, SmolLM2-1.7B)

## Why This Works (Mechanism)
The authors demonstrate that transformer models encode abstract literary style information in their deep representations. As input passes through transformer layers, the model progressively compresses and abstracts information from the prompt. The later layers appear to capture stylistic patterns that are distinctive to individual authors, independent of semantic content. This encoding manifests as a low-dimensional subspace structure that can be probed for authorship classification. The persistence of this signal across multiple model architectures suggests it's a fundamental property of how transformers process and represent language, rather than an artifact of a specific model.

## Foundational Learning
- **Transformer architecture**: Understanding how transformers process input through multiple layers to produce contextualized embeddings is essential for interpreting where and how style information is encoded
- **Prompt embeddings**: Knowing how embeddings are generated from prompts and how different token positions are represented helps understand why last-token embeddings were used
- **PCA dimensionality reduction**: Understanding principal component analysis is crucial for interpreting how high-dimensional embeddings are visualized and reduced for probing
- **Linear probing with SVM**: Familiarity with probe-based classification methods helps evaluate how well embeddings encode separable information
- **Token shuffling experiments**: Understanding how to systematically manipulate input order to isolate syntactic versus lexical contributions is key to interpreting the style findings

## Architecture Onboarding

**Component Map**: Input text → Tokenizer → Transformer layers (L1→L16) → Last token embedding → PCA(64) → Probe (SVM/MLP)

**Critical Path**: Text → Tokenizer → Transformer forward pass → Last token embedding extraction → Probe training/testing

**Design Tradeoffs**: Using last token embedding rather than mean pooling simplifies implementation but may lose some positional information; base models chosen over instruct-tuned versions to avoid task-specific adaptations

**Failure Signatures**: 
- Accuracy near random (~50%) even at deep layers → check last token extraction method
- No improvement with layer depth → verify base model usage and forward pass configuration
- High variance across runs → ensure sufficient non-overlapping excerpts per class (≥500 minimum)

**First Experiments**:
1. Extract last token embeddings at layers L=10-16 from Llama-3.2-1B for GE and VW excerpts
2. Train linear SVM on PCA(64) embeddings with 70/30 split, targeting >85% accuracy
3. Visualize top 16 PCA components to observe style separation

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: To what extent is the stylistic signature inferred from syntactic structure versus lexical content?
- **Basis in paper**: The authors note that shuffling conserves separability, "suggesting the main signal might be about lexical content rather than syntax."
- **Why unresolved**: The shuffling experiment provides a hint but does not quantify the specific contribution of syntax versus vocabulary to the vector separation.
- **What evidence would resolve it**: Systematic ablation studies where syntactic structure is preserved but vocabulary is substituted, and vice-versa, to measure the resulting change in probe accuracy.

### Open Question 2
- **Question**: Does the geometric encoding of style generalize to non-literary text domains, such as technical writing, code, or informal speech?
- **Basis in paper**: The methodology is restricted to a curated corpus of 19th and 20th-century novels, leaving the behavior on diverse domains untested.
- **Why unresolved**: Literary style may possess unique formal properties; it is unclear if the observed low-dimensional subspace structure exists for functional or unstructured text types.
- **What evidence would resolve it**: Applying the same PCA and probing methodology to datasets of software code, scientific abstracts, or social media posts to observe if similar clustering occurs.

### Open Question 3
- **Question**: How can stylistic features be definitively disentangled from semantic content and topic in the embedding space?
- **Basis in paper**: The authors acknowledge that disentangling style from topic is a recognized challenge, and high separability might still conflate the two.
- **Why unresolved**: While same-author confusion suggests style is a factor, the embeddings simultaneously contain semantic information that could drive the classification logic.
- **What evidence would resolve it**: Training probes on datasets where the topic is held constant across different authors, or varied within a single author's work, to isolate pure stylistic directions.

## Limitations
- Analysis is limited to 13 novels from 19th-20th century literature, potentially limiting generalizability to modern writing or other literary traditions
- The study uses only base models rather than instruction-tuned variants, which may behave differently
- Probe-based classification measures separability but doesn't directly reveal what specific stylistic features are being encoded

## Confidence

- **High Confidence**: The claim that embeddings from later transformer layers contain distinctive stylistic features specific to individual authors is strongly supported by the >90% classification accuracy results across multiple model architectures
- **Medium Confidence**: The finding that books by the same author are more entangled than those by different authors, suggesting encoding of lexical and syntactic style rather than factual content, is well-supported but could benefit from additional controls
- **Medium Confidence**: The claim that shuffling token order preserves separability, indicating style is more lexical than syntactic, is supported but the choice of block sizes leaves some uncertainty about the precise nature of this effect

## Next Checks
1. Perform k-fold cross-validation (k=5 or 10) on the authorship classification task to assess stability and generalizability, reporting mean and standard deviation across folds
2. Apply integrated gradients or attention visualization to identify which specific tokens or token combinations contribute most to authorship classification
3. Test classification accuracy when training on earlier 19th-century authors and testing on later 20th-century authors to assess whether embeddings capture time-invariant stylistic features or period-specific patterns