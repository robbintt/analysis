---
ver: rpa2
title: Incentivizing Dual Process Thinking for Efficient Large Language Model Reasoning
arxiv_id: '2505.16315'
source_url: https://arxiv.org/abs/2505.16315
tags:
- reasoning
- thinking
- acpo
- length
- fast
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the overthinking problem in large reasoning
  models, where models generate redundant reasoning regardless of task difficulty.
  The authors propose Adaptive Cognition Policy Optimization (ACPO), a reinforcement
  learning framework that enables dynamic system switch between fast and slow thinking
  modes based on task complexity.
---

# Incentivizing Dual Process Thinking for Efficient Large Language Model Reasoning

## Quick Facts
- arXiv ID: 2505.16315
- Source URL: https://arxiv.org/abs/2505.16315
- Reference count: 39
- Key result: Reduces token usage by 60.5% on AIME 2024 while maintaining accuracy

## Executive Summary
This paper addresses the overthinking problem in large reasoning models, where models generate redundant reasoning regardless of task difficulty. The authors propose Adaptive Cognition Policy Optimization (ACPO), a reinforcement learning framework that enables dynamic system switch between fast and slow thinking modes based on task complexity. ACPO introduces system-aware reasoning tokens to explicitly represent thinking modes and integrates online difficulty estimation with token length budget to guide adaptive reasoning. Experiments show ACPO effectively reduces reasoning length while maintaining accuracy, achieving significant improvements in accuracy per computation unit on benchmark tasks.

## Method Summary
The paper proposes Adaptive Cognition Policy Optimization (ACPO), a reinforcement learning framework that addresses the overthinking problem in large reasoning models. ACPO enables dynamic switching between fast and slow thinking modes based on task complexity by introducing system-aware reasoning tokens that explicitly represent these thinking modes. The framework integrates online difficulty estimation with token length budgets to guide adaptive reasoning. The method is trained on carefully curated data with varying reasoning lengths to learn when to engage in detailed reasoning versus quick responses, effectively balancing accuracy and computational efficiency.

## Key Results
- Reduces token usage by 60.5% on AIME 2024 (from 16,894 to 6,670 tokens) with only 2% accuracy drop
- Achieves 3.22Ã— improvement in accuracy per computation unit on MATH 500 compared to baseline models
- Successfully balances accuracy and computational efficiency through adaptive reasoning length

## Why This Works (Mechanism)
The framework works by explicitly modeling two distinct thinking modes (fast and slow) through system-aware tokens, allowing the model to dynamically allocate computational resources based on task difficulty. The online difficulty estimation module provides real-time assessment of problem complexity, enabling appropriate system switching. The token length budget constraint ensures the model remains within computational limits while maintaining accuracy. The reinforcement learning framework optimizes for both reasoning quality and efficiency, creating a balanced approach that avoids both underthinking and overthinking.

## Foundational Learning
- Reinforcement Learning: Why needed - Optimizes the balance between reasoning quality and computational efficiency; Quick check - Verify the reward function properly weights both accuracy and token efficiency
- System 1 vs System 2 Thinking: Why needed - Provides theoretical foundation for fast vs slow reasoning modes; Quick check - Confirm the framework correctly implements distinct cognitive strategies
- Online Difficulty Estimation: Why needed - Enables real-time adaptation to problem complexity; Quick check - Validate the accuracy of difficulty predictions across diverse problem types

## Architecture Onboarding

**Component Map:** Input Task -> Difficulty Estimator -> System Selector -> Reasoning Module -> Output

**Critical Path:** The critical path involves task input flowing through the difficulty estimator, which informs the system selector that determines whether to engage fast or slow reasoning. The reasoning module then generates appropriate responses based on this selection, with the output being the final answer.

**Design Tradeoffs:** The framework trades some accuracy for significant computational efficiency gains. The dual system approach adds complexity compared to single-mode reasoning but enables better resource allocation. The online estimation introduces latency but provides more adaptive responses compared to static approaches.

**Failure Signatures:** The system may fail when difficulty estimation is inaccurate, leading to inappropriate system selection. Complex problems may be under-solved if classified as simple, while simple problems may be over-solved if misclassified as complex. The token budget constraint may also lead to premature termination of necessary reasoning.

**3 First Experiments:**
1. Ablation study removing the difficulty estimation module to quantify its contribution
2. Comparison of single-system vs dual-system performance on varied difficulty problems
3. Analysis of token usage distribution across problem difficulty levels

## Open Questions the Paper Calls Out
None

## Limitations
- Limited evaluation scope focused on domains where reasoning can be decomposed into token sequences
- Computational overhead from online difficulty estimation may offset some efficiency gains
- System-aware token approach assumes reasoning can be captured through discrete token generation

## Confidence
- High confidence in technical implementation of ACPO and system-aware reasoning tokens
- Medium confidence in claimed efficiency improvements on benchmark tasks under controlled conditions
- Low confidence in generalization to real-world applications and diverse reasoning tasks

## Next Checks
1. Evaluate ACPO on a broader range of reasoning tasks including code generation, scientific reasoning, and open-ended problem solving
2. Conduct ablation studies to quantify individual contributions of online difficulty estimation versus system-aware reasoning tokens
3. Measure actual computational efficiency gains including memory usage and inference latency in real deployment scenarios