---
ver: rpa2
title: 'Tracing LLM Reasoning Processes with Strategic Games: A Framework for Planning,
  Revision, and Resource-Constrained Decision Making'
arxiv_id: '2506.12012'
source_url: https://arxiv.org/abs/2506.12012
tags:
- damage
- cost
- attack
- arxiv
- rate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AdvGameBench, a framework for evaluating
  LLM reasoning processes in strategic games, focusing on planning, revision, and
  resource-constrained decision-making. Instead of just measuring win rates, it tracks
  how models form and adapt strategies, manage constraints, and respond to feedback.
---

# Tracing LLM Reasoning Processes with Strategic Games: A Framework for Planning, Revision, and Resource-Constrained Decision Making

## Quick Facts
- arXiv ID: 2506.12012
- Source URL: https://arxiv.org/abs/2506.12012
- Authors: Xiaopeng Yuan; Xingjian Zhang; Ke Xu; Yifan Xu; Lijun Yu; Jindong Wang; Yushun Dong; Haohan Wang
- Reference count: 40
- Primary result: Introduced AdvGameBench to evaluate LLM reasoning in strategic games via planning, revision, and resource constraints.

## Executive Summary
This paper presents AdvGameBench, a novel framework for evaluating large language models' reasoning processes in strategic games by focusing on planning, revision behaviors, and resource-constrained decision-making. Rather than measuring win rates alone, the framework traces how models form, adapt, and manage strategies over multiple adversarial rounds. Across 4,320 rounds with 12 models, the study reveals that disciplined, selective revisions correlate with higher success rates, while frequent or impulsive revisions tend to degrade performance. ChatGPT-o3-mini emerged as the top performer, demonstrating both high win rates and effective resource management.

## Method Summary
The AdvGameBench framework evaluates LLMs through adversarial rounds in five turn-based strategic games, tracking three core process metrics: planning depth, strategy revision patterns, and adherence to resource budgets. Each round records not only the final move but also intermediate reasoning, enabling assessment of how models adapt strategies in response to opponent actions. Models are scored on win rates, correction success rates, and improvement slopes. The framework emphasizes tracing reasoning processes rather than just outcomes, with synthetic opponents providing controlled variability in play styles.

## Key Results
- ChatGPT-o3-mini achieved the highest composite score: 74.7% win rate, 78.6% correction success, and +0.041 improvement slope, with perfect budget adherence.
- Models that revised strategies too frequently (e.g., Qwen-Plus) showed lower correction success and higher over-budget rates.
- A negative correlation was observed between over-correction risk rate and correction success rate (r = -0.51, p = 0.093), suggesting that selective revisions are more effective than impulsive ones.

## Why This Works (Mechanism)
The AdvGameBench framework works by embedding evaluation within the reasoning process itself, rather than treating game outcomes as black-box results. By tracing each model's strategic adaptations move-by-move, the framework exposes the relationship between revision discipline and performance. The use of synthetic opponents allows for consistent, reproducible adversarial pressure, while the three-tier scoring system for revisions captures qualitative differences in reasoning quality. This process-level visibility enables identification of high-performing behaviors (selective revision, budget adherence) and failure modes (over-correction, resource overuse) that are invisible in outcome-only evaluations.

## Foundational Learning
- **Strategic game theory**: Needed to understand adversarial decision-making structures; quick check: can identify dominant strategies and Nash equilibria.
- **Reinforcement learning concepts**: Needed to grasp how models adapt strategies based on feedback; quick check: can explain exploration vs. exploitation.
- **Process tracing methodology**: Needed to follow and evaluate reasoning steps; quick check: can differentiate between planning and revision phases.
- **Resource constraint modeling**: Needed to interpret budget adherence as a performance metric; quick check: can model limited-turn or limited-resource scenarios.
- **Statistical correlation analysis**: Needed to interpret relationships between behaviors and outcomes; quick check: can compute and interpret Pearson r and p-values.

## Architecture Onboarding

**Component map:** Game environment -> Move generator -> Strategy tracker -> Revision evaluator -> Resource budget monitor -> Outcome scorer

**Critical path:** Game state → Move decision → Strategy revision check → Resource budget validation → Outcome recording

**Design tradeoffs:** The framework prioritizes process visibility over raw win rate, sacrificing some realism (synthetic opponents) for reproducibility and granularity. This enables deeper behavioral analysis but may not fully capture human-like strategic unpredictability.

**Failure signatures:** Models that over-correct show high revision counts but low correction success; models that under-plan show early-game wins but late-game collapse; models that exceed budgets achieve short-term gains but incur penalties.

**First experiments:**
1. Compare AdvGameBench results with standard win-rate-only benchmarks to quantify the added value of process metrics.
2. Vary synthetic opponent difficulty to test model robustness across skill levels.
3. Introduce cooperative game variants to assess whether revision discipline transfers to collaborative contexts.

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Does the negative correlation between over-correction risk rate (ORR) and correction success rate (CSR) achieve statistical significance when evaluated across a larger population of models?
- Basis in paper: [explicit] Section 4.4 reports a negative correlation (r = -0.51, p = 0.093) but notes that "none of these effects reach conventional thresholds for statistical significance due to the limited sample size (n = 12)."
- Why unresolved: The study was constrained to 12 models, lacking the statistical power to confirm if frequent edits definitively reduce correction efficacy across the general model population.
- What evidence would resolve it: Replicating the benchmark with a significantly larger set of models (n > 30) to determine if the correlation coefficient achieves p < 0.05.

### Open Question 2
- Question: Do the process-level behaviors (e.g., disciplined revision, budget adherence) observed in synthetic adversarial settings transfer effectively to gameplay against human opponents?
- Basis in paper: [explicit] The Limitations section states the framework "relies on synthetic opponents, which—although diversified—cannot fully mirror human play styles."
- Why unresolved: While models like ChatGPT-o3-mini excel against other LLMs, it is unknown if the "selective revision" strategies remain effective against the non-optimal or deceptive play styles typical of humans.
- What evidence would resolve it: A follow-up evaluation measuring the win rates and process metrics of the top-performing models in matches against human players.

### Open Question 3
- Question: How does the reliance on "selective revision" for high win rates change in cooperative or real-time game environments?
- Basis in paper: [explicit] The Limitations section notes the current scope "covers three turn-based genres but no real-time or cooperative play."
- Why unresolved: It is unclear if the cognitive load of real-time constraints or the social dynamics of cooperation penalize "impulsive" revision strategies differently than turn-based adversarial games.
- What evidence would resolve it: Applying the AdvGameBench metrics (ORR, Improvement Slope) to cooperative benchmarks (e.g., Overcooked) or real-time strategy games to compare the correlation between revision style and success.

## Limitations
- Limited sample size (n = 12 models) constrains statistical power for detecting behavioral correlations.
- Focus on five turn-based games may not generalize to real-time or cooperative strategic contexts.
- Synthetic opponents, while controlled, cannot fully replicate the unpredictability of human play styles.

## Confidence
- **High Confidence**: Win rate measurements, budget adherence statistics, and basic descriptive findings about model performance differences
- **Medium Confidence**: The framework's ability to differentiate between revision strategies and the observed relationships between revision frequency and success rates
- **Low Confidence**: Causal interpretations of why certain models perform better, and generalization of findings to non-game strategic reasoning tasks

## Next Checks
1. **Expand Game Diversity**: Test the AdvGameBench framework across 10+ additional strategic game types (e.g., cooperative games, incomplete information games) to assess generalizability of observed revision patterns.
2. **Longitudinal Analysis**: Conduct extended play sessions (100+ rounds per model) to determine whether observed behaviors represent stable strategies or transient adaptations.
3. **Cross-Paradigm Validation**: Compare AdvGameBench results with performance on non-game strategic reasoning benchmarks (e.g., multi-step planning tasks, resource allocation problems) to validate the framework's external validity.