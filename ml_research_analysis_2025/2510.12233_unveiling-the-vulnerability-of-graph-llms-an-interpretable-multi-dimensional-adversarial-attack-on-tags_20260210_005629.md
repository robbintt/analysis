---
ver: rpa2
title: 'Unveiling the Vulnerability of Graph-LLMs: An Interpretable Multi-Dimensional
  Adversarial Attack on TAGs'
arxiv_id: '2510.12233'
source_url: https://arxiv.org/abs/2510.12233
tags:
- graph
- attack
- adversarial
- node
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work exposes a critical vulnerability in Graph-LLMs by proposing
  IMDGA, a multi-dimensional adversarial attack framework that manipulates both textual
  and structural components of text-attributed graphs. It introduces three modules:
  Topological SHAP to identify pivotal words, Semantic Perturbation to substitute
  them with contextually plausible alternatives, and Edge Pruning to disrupt critical
  message-passing paths.'
---

# Unveiling the Vulnerability of Graph-LLMs: An Interpretable Multi-Dimensional Adversarial Attack on TAGs

## Quick Facts
- **arXiv ID:** 2510.12233
- **Source URL:** https://arxiv.org/abs/2510.12233
- **Reference count:** 40
- **Primary result:** IMDGA achieves up to 95% ASR by manipulating both text and structure in TAGs

## Executive Summary
This work exposes a critical vulnerability in Graph-LLMs by proposing IMDGA, a multi-dimensional adversarial attack framework that manipulates both textual and structural components of text-attributed graphs. It introduces three modules: Topological SHAP to identify pivotal words, Semantic Perturbation to substitute them with contextually plausible alternatives, and Edge Pruning to disrupt critical message-passing paths. IMDGA achieves superior attack success rates (up to 95% ASR) while maintaining stealthiness and interpretability across multiple datasets and Graph-LLM architectures, outperforming existing text and graph-based baselines.

## Method Summary
IMDGA is a black-box adversarial attack framework targeting text-attributed graphs (TAGs) that combines text-based and structure-based perturbations. The method operates through three sequential modules: (1) Topological SHAP identifies pivotal words using Shapley values to quantify their contribution to node classification through neighborhood aggregation; (2) Semantic Perturbation generates contextually plausible substitutes using an MLM constrained by a graph-aware scoring function that maximizes confidence gaps; (3) Edge Pruning disrupts critical message-passing paths by pruning edges connected to vulnerable nodes identified through predictive disparity and feature influence metrics. The attack maintains stealthiness through controlled modification ratios and preserves semantic coherence while achieving high success rates.

## Key Results
- Achieves up to 95% Attack Success Rate across Cora, Citeseer, PubMed, and ogbn-arxiv datasets
- Outperforms text-only baselines (TextHoaxer) and graph-only baselines (FGSM) by significant margins
- Maintains stealthiness with controlled perplexity and high semantic similarity while being imperceptible to human evaluators

## Why This Works (Mechanism)

### Mechanism 1
The framework achieves high-impact adversarial attacks by identifying pivotal words based on their contribution to the graph's message-passing mechanism, rather than treating text in isolation. The Topological SHAP Module adapts Shapley values to the graph domain, calculating the marginal contribution of a word to the aggregated prediction scores of the target node and its neighbors. By masking words and observing the resulting drop in confidence across the local subgraph, it identifies tokens that are structurally critical for classification.

### Mechanism 2
The attack maintains stealthiness (low perplexity, high semantic similarity) while maximizing disruption by leveraging a Masked Language Model (MLM) constrained by a graph-aware scoring function. The Semantic Perturbation Module uses an MLM to generate contextually plausible substitutes for the identified pivotal words, but doesn't pick the most likely linguistic substitute. Instead, it selects the candidate that maximizes the "confidence gap" - the difference between the top two predicted classes - aggregated over the node's neighborhood, ensuring the perturbation pushes the decision boundary specifically for that graph structure.

### Mechanism 3
When text perturbations are insufficient, the attack exploits structural vulnerabilities by pruning edges that act as "nexus" points for information flow. The Edge Pruning Module identifies a subset of nodes (the "nexus of vulnerability") that are highly susceptible to feature changes. It computes a score based on predictive disparity, feature influence (Jacobian norm), and centrality, then estimates Shapley values for edges connected to this nexus and prunes those with the highest attribution to disrupt critical message-passing paths.

## Foundational Learning

- **Text-Attributed Graphs (TAGs) & Message Passing**: Standard GNN attacks modify anonymous feature vectors; IMDGA attacks *raw text*. You must understand how an LLM encodes text into node features $X$ and how a GNN aggregates $X$ from neighbors $N(v)$ to see where the "Semantic Perturbation" strikes.
  - **Quick check**: How does changing a word in Node A's text potentially alter the feature representation of its neighbor, Node B?

- **Shapley Values (Game Theory)**: The paper relies on "Topological SHAP" for interpretability. You need to understand Shapley values as a method to fairly distribute the "payout" (prediction score) among "players" (words/edges) to grasp why this attack is considered "interpretable."
  - **Quick check**: In the context of this paper, what represents the "coalition" and what represents the "marginal contribution" when calculating a word's importance?

- **Black-Box Adversarial Attacks**: The paper explicitly assumes a black-box setting (access only to model outputs/probabilities, not gradients). Understanding this constraint explains why the authors use score-based search (confidence gaps) and MLM substitution rather than gradient-based optimization (like FGSM).
  - **Quick check**: Why does the lack of gradient access force the attacker to rely on "confidence gaps" and sampling strategies?

## Architecture Onboarding

- **Component map:** Input TAG → Topological SHAP (identify pivotal words) → Semantic Perturbation (substitute words) → Edge Pruning (if needed) → Perturbed TAG
- **Critical path:** The dependency chain is sequential: SHAP Analysis → Perturbation. You cannot effectively perturb the text without first identifying the topologically significant words. Edge pruning is a conditional refinement stage, not a parallel path.
- **Design tradeoffs:**
  - Interpretability vs. Speed: Computing Shapley values for every word is O(2^m). The system uses partition-based sampling to approximate this, trading exact attribution for tractable run times.
  - Stealthiness vs. Effectiveness: Aggressive word substitution (high modification ratio β) increases Attack Success Rate (ASR) but risks detection (high Perplexity). The module balances this via the candidate scoring function.
- **Failure signatures:**
  - Low ASR + High Perplexity: Semantic substitutes are grammatically correct but contextually wrong for the graph.
  - High Computational Cost: SHAP sampling space is too large for high-degree nodes.
  - Robustness of Target: If the target is GNNGuard or similar robust architectures, expect lower ASR.
- **First 3 experiments:**
  1. Baselines Comparison: Run IMDGA against text-only baselines (e.g., TextHoaxer) and graph-only baselines (e.g., FGSM) on Cora/Citeseer to validate the "Multi-Dimensional" advantage.
  2. Ablation Study: Disable the Topological SHAP (use random word selection) and the Edge Pruning module separately to quantify their specific contribution to the ASR.
  3. Stealthiness Check: Measure Perplexity (PPL) and Cosine Similarity of the perturbed text against the original to ensure the attack meets the "imperceptible" criteria.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What specific defense mechanisms or robust training paradigms can effectively mitigate multi-dimensional attacks like IMDGA without significantly compromising the performance of Graph-LLMs on clean data?
- **Basis in paper:** The conclusion explicitly states there is an "urgent need for systematic defenses to guide the development of more resilient Graph-LLMs against increasingly sophisticated adversarial threats."
- **Why unresolved:** The paper focuses exclusively on the attack methodology (IMDGA) and demonstrating vulnerabilities, without proposing, implementing, or testing any specific defense strategies.
- **What evidence would resolve it:** A study introducing a defense framework (e.g., adversarial training or graph purification) that successfully lowers IMDGA's Attack Success Rate (ASR) while maintaining high clean accuracy on TAGs.

### Open Question 2
- **Question:** Can the Semantic Perturbation module be effectively adapted for decoder-only Large Language Models (e.g., GPT, LLaMA) that lack a native Masked Language Model (MLM) objective?
- **Basis in paper:** The methodology in Section 3.2 relies on an MLM to generate candidate words based on bidirectional conditional probabilities, which is inherently compatible with BERT-style encoders but technically incompatible with generative decoder architectures.
- **Why unresolved:** The framework's reliance on the `[MASK]` token for context-aware substitution suggests the current implementation cannot directly transfer to decoder-only Graph-LLMs, which are increasingly popular.
- **What evidence would resolve it:** An experiment applying IMDGA to a Graph-LLM utilizing a decoder-only backbone (like LLaMA) using a modified generation-based perturbation strategy that achieves comparable ASR.

### Open Question 3
- **Question:** Does the IMDGA framework generalize to other graph learning tasks, such as link prediction or graph classification, where the dependency on neighborhood message passing differs from node classification?
- **Basis in paper:** Section 2 states that node representations are "suitable for various downstream tasks," yet the experimental evaluation is restricted strictly to node classification accuracy and ASR.
- **Why unresolved:** The specific scoring functions are tailored to node-level confidence gaps and label flips; their efficacy in disrupting edge-level probabilities or graph-level properties remains untested.
- **What evidence would resolve it:** Empirical results showing the attack's impact on metrics like Hits@K or AUC for link prediction tasks on standard TAG datasets.

## Limitations
- **Parameter Sensitivity**: The framework's effectiveness depends heavily on several unspecified hyperparameters including SHAP coalition sampling count, pivotal word selection threshold, and edge pruning weight parameters.
- **Computational Scalability**: While the paper claims O(2^m) complexity is mitigated through sampling, the actual computational cost of Topological SHAP remains significant for nodes with many words.
- **Generalization Across Architectures**: The attack's effectiveness may vary significantly depending on the target Graph-LLM's architecture, but this hasn't been thoroughly explored.

## Confidence

**High Confidence Claims:**
- The multi-dimensional approach combining text and structural perturbations outperforms single-dimension attacks
- IMDGA achieves superior ASR across multiple datasets
- The framework maintains stealthiness through controlled perplexity and semantic similarity
- Robustness against defenses like GNNGuard is demonstrated

**Medium Confidence Claims:**
- The specific quantitative improvements over baselines are context-dependent on exact parameter settings
- The interpretability claims rely on SHAP values, but the approximation quality and sampling sufficiency are not fully validated
- Edge pruning's contribution is conditional and may vary based on graph density and homophily

**Low Confidence Claims:**
- The claim that IMDGA is "imperceptible" to human evaluators is based on limited samples without confidence intervals
- The scalability to extremely large graphs is demonstrated but computational costs are not detailed

## Next Checks

1. **Parameter Sensitivity Analysis**: Systematically vary the SHAP sampling count (s), pivotal word threshold (τ), and edge pruning weights (α1, α2, α3) across their plausible ranges to identify which parameters most critically affect ASR and stealthiness. Document the trade-offs between effectiveness and computational cost.

2. **Architectural Robustness Test**: Apply IMDGA against a broader range of Graph-LLM architectures including those with attention-based GNNs, graph pooling methods, and models with built-in robustness features. Compare attack success rates to identify architectural vulnerabilities.

3. **Human Evaluation Replication**: Replicate the human evaluation study with a larger sample size (n > 200) and multiple evaluation rounds to establish confidence intervals for the "imperceptibility" claims. Test whether perturbed text remains contextually appropriate across different domains and expertise levels.