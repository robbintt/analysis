---
ver: rpa2
title: Adaptive Symmetrization of the KL Divergence
arxiv_id: '2511.11159'
source_url: https://arxiv.org/abs/2511.11159
tags:
- training
- learning
- distribution
- logp
- divergence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a method for learning a probability distribution
  given a finite set of samples. The method minimizes the symmetric Jeffreys divergence,
  a sum of forward and reverse KL divergences, using a proxy model and constrained
  optimization.
---

# Adaptive Symmetrization of the KL Divergence

## Quick Facts
- arXiv ID: 2511.11159
- Source URL: https://arxiv.org/abs/2511.11159
- Reference count: 40
- Key outcome: Novel method for learning probability distributions from samples by minimizing symmetric Jeffreys divergence using proxy model and adaptive symmetrization

## Executive Summary
This paper introduces an adaptive symmetrization method for learning probability distributions from finite samples by minimizing the symmetric Jeffreys divergence. The approach addresses the challenge of estimating reverse KL divergence from samples by employing a proxy model within a constrained optimization framework. The method adaptively adjusts the balance between forward and reverse KL divergences during training, demonstrating improved stability and robustness across density estimation, image generation, and simulation-based inference tasks.

## Method Summary
The method learns a probability distribution by minimizing the symmetric Jeffreys divergence, which is the sum of forward and reverse KL divergences. Since reverse KL divergence is difficult to compute directly from samples, the authors employ a proxy model to estimate it. The optimization problem is formulated as a constrained optimization where the proxy model is trained to minimize the reverse KL divergence while the main model is trained to minimize the overall Jeffreys divergence. The key innovation is an adaptive mechanism that adjusts the relative weighting between forward and reverse KL terms during training to maintain stability and prevent mode collapse or mode seeking behaviors that can occur with fixed weighting schemes.

## Key Results
- Demonstrates superior performance compared to existing methods in density estimation tasks
- Shows improved stability in image generation without mode collapse
- Achieves better accuracy in simulation-based inference applications

## Why This Works (Mechanism)
The adaptive symmetrization approach works by dynamically balancing the forward and reverse KL divergence terms based on the current state of the learning process. This prevents the optimization from becoming dominated by either term, which can lead to pathological behaviors such as mode collapse (when forward KL dominates) or mode seeking (when reverse KL dominates). By using a proxy model to estimate the reverse KL, the method can effectively optimize both terms even when direct computation of reverse KL is intractable from samples.

## Foundational Learning
- **KL Divergence**: Why needed - measures difference between probability distributions; Quick check - understand forward vs reverse KL properties
- **Jeffreys Divergence**: Why needed - symmetric version of KL divergence for stable optimization; Quick check - know its relation to forward/reverse KL
- **Proxy Models**: Why needed - estimate intractable reverse KL from samples; Quick check - understand how proxy models can approximate reverse KL
- **Constrained Optimization**: Why needed - framework for joint training of main and proxy models; Quick check - know basic constrained optimization principles
- **Adaptive Weighting**: Why needed - dynamically balance optimization terms; Quick check - understand how adaptive weighting prevents optimization pathologies
- **Mode Collapse/Seeking**: Why needed - understand failure modes in distribution learning; Quick check - know how fixed KL weighting can cause these issues

## Architecture Onboarding

Component Map: Samples -> Main Model -> Forward KL -> Jeffreys Divergence; Proxy Model -> Reverse KL -> Jeffreys Divergence; Adaptive Controller -> Weight Adjustment

Critical Path: Samples → Main Model → Forward KL → Jeffreys Divergence (primary objective); Proxy Model → Reverse KL → Jeffreys Divergence (auxiliary objective)

Design Tradeoffs:
1. Proxy model complexity vs. estimation accuracy
2. Adaptation frequency vs. computational overhead
3. Forward/reverse KL balance range vs. stability/robustness

Failure Signatures:
- Mode collapse: forward KL dominates
- Mode seeking: reverse KL dominates
- Optimization instability: improper adaptive weighting
- Poor proxy estimation: inaccurate reverse KL approximation

First Experiments:
1. Simple Gaussian distribution estimation with known parameters
2. Mixture of Gaussians to test mode coverage
3. Synthetic image generation with known data distribution

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on proxy model accuracy and computational overhead
- Unclear adaptive mechanism details for balancing forward/reverse KL
- Limited experimental scope and comparison with broader range of methods

## Confidence
- Theoretical foundation: High
- Practical effectiveness: Medium
- Stability and robustness claims: Medium

## Next Checks
1. Conduct ablation studies to quantify impact of proxy model choice and hyperparameters on final performance
2. Perform extensive computational complexity analysis to determine practical feasibility for large-scale applications
3. Expand experimental validation to include wider range of data types and comparison with additional state-of-the-art methods in distribution learning