---
ver: rpa2
title: Using causal abstractions to accelerate decision-making in complex bandit problems
arxiv_id: '2509.04296'
source_url: https://arxiv.org/abs/2509.04296
tags:
- causal
- abstraction
- at-ucb
- regret
- base
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the problem of accelerating decision-making
  in complex bandit problems by exploiting shared information between multi-armed
  bandit problem instances defined at different levels of abstraction. The authors
  propose AT-UCB, an algorithm that leverages causal abstraction theory to first explore
  within a cheap-to-simulate and coarse-grained CMAB instance, before employing the
  traditional upper confidence bound (UCB) algorithm on a restricted set of potentially
  optimal actions in the CMAB of interest.
---

# Using causal abstractions to accelerate decision-making in complex bandit problems

## Quick Facts
- arXiv ID: 2509.04296
- Source URL: https://arxiv.org/abs/2509.04296
- Reference count: 40
- One-line primary result: AT-UCB algorithm achieves lower regret than standard UCB by leveraging cheap, low-fidelity abstract models to filter suboptimal actions before running expensive high-fidelity optimization.

## Executive Summary
This paper proposes AT-UCB, a novel algorithm that accelerates decision-making in complex bandit problems by exploiting shared information between multi-armed bandit problem instances at different levels of abstraction. The approach uses a cheap-to-simulate, coarse-grained causal abstraction model to pre-filter sub-optimal actions before applying the traditional upper confidence bound algorithm on the expensive base model. This method leads to significant reductions in cumulative regret compared to standard UCB, particularly when the abstraction error is small relative to the action reward gaps. The authors provide theoretical regret bounds and validate their approach empirically using epidemiological simulators with varying computational costs.

## Method Summary
The AT-UCB algorithm operates in two phases: first, it explores a cheap abstract model uniformly to estimate mean rewards and construct a discard set of low-performing abstract arms using a threshold ε; second, it applies standard UCB on the restricted set of base actions corresponding to non-discarded abstract arms. The theoretical analysis shows that regret can be decomposed into the cost of abstract exploration plus UCB regret on the filtered action set, with the bound scaling only with the number of surviving arms rather than all arms. The approach requires knowledge of the abstraction error ε(α) to set the threshold, and assumes the intervention map between abstract and base models is well-defined.

## Key Results
- AT-UCB achieves lower regret than standard UCB when the abstraction error ε(α) is small relative to reward gaps
- The theoretical regret bound scales with |ω⁻¹(Dᶜ)| rather than total arms k when filtering is effective
- Empirical validation on epidemic simulators shows consistent regret reduction across different threshold values
- For sufficiently small ε, the method demonstrates significant computational acceleration while preserving the optimal base arm

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Pre-filtering sub-optimal actions in a cheap, low-fidelity model reduces the effective search space for the expensive model.
- **Mechanism:** The algorithm first executes uniform exploration in the abstract CMAB (M') to estimate mean rewards (μ̂ₐ'). It constructs a discard set D̂ of abstract arms falling below a threshold ε relative to the best-observed abstract arm. Using the intervention map ω, it projects this discard set onto the base model, effectively eliminating a subset of expensive actions before the main UCB phase begins.
- **Core assumption:** The abstract model M' is significantly cheaper to simulate than the base model M (cost C ≪ 1), and the intervention map ω reliably groups base actions into abstract clusters.
- **Evidence anchors:** [abstract]: Mentions exploring "within a cheap-to-simulate abstracted model... before employing... UCB on a restricted set." [section]: Algorithm 1 (Lines 3–5) explicitly details the uniform sampling, thresholding, and set restriction steps. [corpus]: Related work like "Causal Abstractions, Categorically Unified" supports the feasibility of defining the necessary τ, ω maps, though it does not address the bandit optimization itself.
- **Break condition:** If the abstract model is not sufficiently faster than the base model, the overhead cost n' · C may exceed the regret reduction gained from filtering.

### Mechanism 2
- **Claim:** Cumulative regret can be bounded by the abstraction error and the reduction in action set size, theoretically guaranteeing acceleration.
- **Mechanism:** The theoretical analysis (Proposition 2) separates total regret into the cost of abstract exploration (n' · C) and the regret incurred by the UCB phase. Crucially, the UCB regret term scales only with the number of *surviving* arms |ω⁻¹(Dᶜ)| rather than the total arms k. If the abstraction is high-quality (small ε(α)), the surviving set is small, and the logarithmic regret term is significantly reduced compared to standard UCB.
- **Core assumption:** The abstraction error ε(α) is small enough that the "good" abstract arms map to a small subset of "good" base arms (formally, |ω⁻¹(Dᶜ)| ≪ k).
- **Evidence anchors:** [section]: Section 3.1 states that when ε(α) is small, "Proposition 2 gives a lower regret bound than the standard upper bound for UCB." [proposition 2]: Equation (7) explicitly shows the regret summing over ω⁻¹(Dᶜ) rather than all arms. [corpus]: Weak signal; standard bandit literature confirms UCB scaling, but the abstraction-augmented bound is specific to this paper.
- **Break condition:** If the abstraction error ε(α) is large, the set of "potentially optimal" base arms ω⁻¹(Dᶜ) remains large, negating the reduction in the regret bound.

### Mechanism 3
- **Claim:** The filtering mechanism preserves the true optimal base arm with high probability by exploiting approximate rank preservation.
- **Mechanism:** Proposition 1 guarantees that the mean reward of the abstract optimal arm μₐ'₁ is within ε(α) of the image of the base optimal arm μω(ₐ₁). By setting the threshold ε ≥ ε(α), the algorithm ensures that the image of the base optimal arm is unlikely to fall into the discard set D̂, even if it is not the absolute best arm in the abstract model.
- **Core assumption:** The learner has prior knowledge of (or can safely overestimate) the abstraction error ε(α) to set the hyperparameter ε correctly (Assumption 1).
- **Evidence anchors:** [section]: Section 3 utilizes Proposition 1 to justify restricting attention to the interval defined by the threshold. [appendix]: Appendix D (Limitations) explicitly notes that the guarantee relies on the learner knowing ε(α). [corpus]: "Causal Abstraction Inference under Lossy Representations" suggests that estimating this error is a non-trivial prerequisite.
- **Break condition:** If the threshold ε is set lower than the true abstraction error ε(α), the algorithm risks filtering out the optimal base arm, leading to linear regret.

## Foundational Learning

- **Concept: Upper Confidence Bound (UCB) Algorithm**
  - **Why needed here:** This is the base algorithm being modified. You must understand how UCB balances exploration (pulling less-seen arms) and exploitation (pulling high-reward arms) to see how AT-UCB accelerates this by reducing the initial "search space."
  - **Quick check question:** In standard UCB, how does the confidence interval width change as you pull an arm more often?

- **Concept: Structural Causal Models (SCM) & Interventions (do-calculus)**
  - **Why needed here:** The "arms" in this bandit problem are not just arbitrary levers; they are causal interventions (do(X=x)) on a structural model. Understanding the difference between observing and intervening is critical to defining the CMAB.
  - **Quick check question:** What is the difference between P(Y|X) and P(Y|do(X))?

- **Concept: Wasserstein Distance**
  - **Why needed here:** The paper quantifies "abstraction error" using the 2-Wasserstein distance (W₂) in Definition 5. You need a conceptual grasp of W₂ as a metric between probability distributions to understand the cost of using a low-fidelity model.
  - **Quick check question:** Does Wasserstein distance require the distributions to have the same support (unlike KL divergence)?

## Architecture Onboarding

- **Component map:** Base Simulator (M) -> Abstract Simulator (M') -> Abstraction Layer (α = ⟨τ, ω⟩) -> Controller (AT-UCB)
- **Critical path:** The definition of the **Intervention Map (ω)**. If the mapping between high-res interventions and low-res interventions is poorly defined (e.g., mapping a complex lockdown policy to a simple parameter incorrectly), the information transfer fails.
- **Design tradeoffs:**
  - **Threshold ε:** Must be tuned. Too low → false negatives (dropping the optimal arm). Too high → false positives (no acceleration).
  - **Abstract Horizon n':** Spending too long in the abstract model incurs cost C without benefit if the optimal arm is already identified; spending too little results in high variance estimates that lead to incorrect filtering.
- **Failure signatures:**
  - **High Regret in Early Steps:** Indicates the abstract model failed to filter sub-optimal arms (threshold too high or abstraction too coarse).
  - **Convergence to Sub-optimal Arm:** Indicates the threshold ε was too tight relative to the true abstraction error ε(α), causing the optimal arm to be permanently filtered out.
  - **Slow Convergence:** The cost C of the abstract model is not significantly lower than the base model, or the abstraction provides no useful signal (IC error is near random).
- **First 3 experiments:**
  1. **Synthetic Validation:** Create a simple linear-Gaussian SCM and a known abstract version. Verify Proposition 2's regret bound holds numerically.
  2. **Threshold Sensitivity:** Reproduce Figure 1 but vary the *true* abstraction error (by adding noise to the abstract model) to observe how performance degrades as ε(α) increases.
  3. **Mapping Stress Test:** Introduce a "broken" intervention map ω that maps base arms to random abstract arms. Verify that performance drops to worse than standard UCB due to misleading information.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the causal graph structure of the abstract SCM be exploited to improve the efficiency of the uniform exploration phase?
- **Basis in paper:** [explicit] Section 5 and Appendix D state that AT-UCB "does not exploit causal information provided by the SCMs" to expedite exploration.
- **Why unresolved:** The current method relies on uniform sampling in the abstract model, ignoring potential efficiency gains available through the causal relationships within the abstract SCM.
- **What evidence would resolve it:** An augmented algorithm that utilizes graph-based causal bandit techniques during the abstract phase to reduce sample complexity compared to uniform sampling.

### Open Question 2
- **Question:** Can the AT-UCB algorithm be adapted to provide theoretical guarantees without requiring oracle knowledge of the abstraction error ε(α)?
- **Basis in paper:** [explicit] Section 5 and Appendix D note that the regret guarantee relies on setting the threshold ε = ε(α), which may be "unknown" in black-box settings.
- **Why unresolved:** The algorithm's safety depends on filtering arms based on ε(α), but actively learning this error is described as costly and non-trivial.
- **What evidence would resolve it:** A regret analysis for a variant algorithm that maintains performance guarantees while adaptively estimating or remaining robust to unknown abstraction errors.

### Open Question 3
- **Question:** How can the required abstract horizon n' be determined without prior knowledge of the abstract optimality gaps Δ'ₐ'?
- **Basis in paper:** [explicit] Appendix D highlights that assessing the validity of Assumption 1 requires knowledge of gap sizes which is "unlikely to be available a priori."
- **Why unresolved:** Theoretical bounds currently assume a fixed n' derived from unknown parameters, making the tuning of the abstract exploration length challenging in practice.
- **What evidence would resolve it:** An adaptive stopping criterion for the abstract exploration phase that ensures sufficient information is gathered to filter arms correctly without prior gap knowledge.

## Limitations

- The algorithm requires prior knowledge or accurate estimation of the abstraction error ε(α) to set the threshold parameter correctly
- The effectiveness critically depends on the intervention map ω being well-defined and preserving the relative ranking of arms
- The method assumes the abstract model is substantially cheaper to simulate than the base model, which may not hold for all problem domains

## Confidence

- **High confidence:** The core theoretical mechanism of regret decomposition and the benefit of action filtering when ε is small (Proposition 2)
- **Medium confidence:** The empirical demonstration that AT-UCB outperforms standard UCB on epidemic simulators, though limited to specific parameters
- **Low confidence:** The practical feasibility of estimating abstraction error ε(α) in real-world applications without oracle knowledge

## Next Checks

1. **Abstraction error estimation:** Implement a method to estimate ε(α) empirically from finite samples of both models, then test whether AT-UCB remains effective when using this estimated error rather than oracle knowledge.

2. **Mapping robustness:** Systematically degrade the quality of the intervention map ω (e.g., random permutations, noise injection) to quantify how mapping errors impact regret performance and identify the threshold where benefits disappear.

3. **Cross-domain validation:** Apply AT-UCB to a non-epidemiological domain (e.g., recommender systems or hyperparameter optimization) with known causal structure to test generalizability beyond the demonstrated case.