---
ver: rpa2
title: Enhancing Fairness in Skin Lesion Classification for Medical Diagnosis Using
  Prune Learning
arxiv_id: '2509.00745'
source_url: https://arxiv.org/abs/2509.00745
tags:
- skin
- fairness
- pruning
- classification
- lesion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses fairness bias in skin lesion classification
  models, particularly related to skin color. The authors propose a prune learning
  method that improves fairness by identifying and removing convolutional filters,
  patches, and attention heads that disproportionately focus on skin tone features.
---

# Enhancing Fairness in Skin Lesion Classification for Medical Diagnosis Using Prune Learning

## Quick Facts
- arXiv ID: 2509.00745
- Source URL: https://arxiv.org/abs/2509.00745
- Reference count: 0
- Primary result: Improved fairness metrics (EOpp1 and EOdd) by 1.5-6.5% while maintaining predictive performance and reducing computational costs by 10-20%

## Executive Summary
This study addresses fairness bias in skin lesion classification models, particularly related to skin color. The authors propose a prune learning method that improves fairness by identifying and removing convolutional filters, patches, and attention heads that disproportionately focus on skin tone features. Their approach uses skewness analysis of feature maps and attention distributions to detect and eliminate skin-color-dependent components without requiring explicit skin tone labels. Evaluated on the ISIC2019 dataset using VGG11 and ViT-B16 models, the method improved fairness metrics while maintaining predictive performance and reducing computational costs by 10-20%. This provides a practical solution for developing equitable medical AI that works across diverse skin tones without sacrificing accuracy or efficiency.

## Method Summary
The proposed "Prune Learning" method uses statistical skewness to identify and remove model components that focus on skin tone rather than lesion pathology. For CNN models like VGG11, it calculates skewness of feature map activations per convolutional filter and prunes those with negative skewness (indicating widespread activation across skin background). For Vision Transformers, it prunes both input patch embeddings and attention heads with negative skewness. The pruned models are then fine-tuned with partial training strategies, including freezing pruned components to prevent re-learning of skin-tone features. This structural pruning approach physically removes weights rather than masking them, achieving both fairness improvements and computational efficiency gains.

## Key Results
- VGG11: 229 filters pruned (10.3% reduction), achieving 1.52% improvement in EOpp1 and 1.54% in EOdd
- ViT-B16: 450 patches pruned (54% reduction) plus 4 attention heads, achieving 1.18% improvement in EOpp1 and 1.25% in EOdd
- Computational efficiency: VGG11 saw 18.67% FLOPs reduction and 10.73% parameter reduction; ViT-B16 achieved 29.69% FLOPs reduction and 20.14% parameter reduction
- Accuracy maintained: F1 scores remained comparable to baseline models (VGG11: 0.8708 vs 0.8757 baseline; ViT-B16: 0.8910 vs 0.9065 baseline)

## Why This Works (Mechanism)

### Mechanism 1: Skewness as a Proxy for Feature Locality
If skin lesions occupy a small, localized area of an image while skin tone dominates the background, the statistical skewness of feature map activations can serve as a proxy to distinguish lesion-relevant features from skin-tone features. The method calculates the skewness of activation distributions in convolutional filters (CNN) or attention maps (ViT). Positive skew indicates a long tail of high activations in a small region (assumed to be the lesion), while negative skew indicates moderate activation spread across a large area (assumed to be skin tone). By pruning components with negative skew, the model is structurally forced to ignore background skin features. This mechanism likely fails if input images are cropped so tightly that skin background is minimal, or if lesions span very large surface areas, flattening the skewness distribution.

### Mechanism 2: Structural Capacity Reduction for Bias Mitigation
Physically removing model parameters (structural pruning) reduces the network's capacity to learn spurious correlations (like skin tone) more effectively than masking or regularization, while simultaneously lowering computational cost. Unlike methods that mask channels or add regularization terms, this approach physically deletes filters and attention heads identified via the skewness criterion. This prevents the model from "re-learning" the pruned skin-tone features during fine-tuning. The features encoding skin tone are assumed to be functionally separable from those encoding lesion pathology, residing in distinct filters or heads. If skin tone features are deeply entangled with lesion texture features in the same filters, pruning may degrade diagnostic accuracy.

### Mechanism 3: Disentangling Patch Embeddings from Attention Heads (ViT)
In Vision Transformers, fairness can be enhanced by sequentially pruning input patches (spatial focus) and attention heads (latent focus), particularly by freezing the pruned patch embeddings during fine-tuning. The method reduces the input dimension by removing channels related to skin color before the first encoder block, then removes attention heads that over-emphasize skin tone in the latent space. Freezing the patch embedding weights after pruning prevents the model from re-integrating skin-tone signals during the fine-tuning phase. The initial patch embedding layer is assumed to capture sufficient skin-tone information that can be isolated, and that later encoder blocks can adapt to the reduced dimensionality without losing lesion fidelity. Aggressive patch pruning may discard subtle lesion context if the "keep indices" are too restrictive.

## Foundational Learning

**Concept: Statistical Skewness (3rd Moment)**
Why needed here: This is the core mathematical heuristic used to identify "skin" vs. "lesion" channels without explicit labels.
Quick check question: Given a feature map where 90% of pixels are near zero and 10% are highly activated (lesion), would this distribution exhibit positive or negative skewness?

**Concept: Equalized Odds (EOdd) vs. Equal Opportunity (EOpp)**
Why needed here: To distinguish between fairness in True Positive Rates (catching disease) vs. True Negative Rates (avoiding false alarms) across skin tone groups.
Quick check question: If a model detects cancer equally well in light and dark skin but has a higher False Positive Rate for dark skin, is Equalized Odds violated?

**Concept: Model Pruning (Structured vs. Unstructured)**
Why needed here: The paper relies on *structured* pruning (removing entire filters/channels) to achieve hardware efficiency, not just zeroing individual weights.
Quick check question: Does removing an entire convolutional filter result in a "sparse" weight matrix or a physically smaller dense matrix?

## Architecture Onboarding

**Component map:** Input -> Backbone (VGG11/ViT-B16) -> Skewness Analyzer -> Pruner -> Fine-tuner -> Output

**Critical path:**
1. Train Vanilla Model to convergence
2. Run inference on Validation Set to collect feature maps/attention stats
3. Compute Skewness and determine "Keep Indices" (Median Skew > 0)
4. Re-initialize model with reduced dimensions (physically delete weights)
5. Fine-tune (Critical: Use Frozen Embeddings for ViT optimal fairness)

**Design tradeoffs:**
- Aggressiveness: Higher pruning rates improve efficiency and fairness but risk accuracy collapse
- Fine-tuning Strategy: Full re-training might recover accuracy but re-learn bias; Frozen layers preserve fairness but may cap peak accuracy
- Architecture: VGG is easier to prune (clear filter hierarchy); ViT requires handling both Patch (input) and Head (latent) dimensions

**Failure signatures:**
- Performance Drop: F1 score drops significantly → Skewness threshold is too aggressive (pruning lesion features)
- No Fairness Gain: EOdd stays high → Skewness assumption fails (skin tone is not negatively skewed in your specific data distribution)
- Dimension Mismatch: Runtime error in Residual Add → Algorithm 3 (ViT Residual Add) not implemented correctly for reduced dimensions

**First 3 experiments:**
1. Baseline Profile: Train Vanilla VGG11 on ISIC2019 and plot the distribution of skewness values for the final MaxPool layer to see if "skin" and "lesion" channels are visibly separable
2. Sensitivity Analysis (VGG): Apply SkewPrune with varying thresholds (e.g., Median > 0, > 0.1, > -0.1) and plot the trade-off curve between FLOPs reduction and EOpp1 score
3. ViT Ablation: Implement "Pattern 5" (Patch+Head Prune with Full Fine-tuning) vs. "Pattern 6" (Frozen Embeddings) from Table I to verify the authors' claim that freezing improves fairness

## Open Questions the Paper Calls Out

**Open Question 1**
- Question: Does skewness-based pruning generalize to other medical imaging domains (e.g., chest X-rays, retinal imaging) where background tissue features may correlate with patient demographics?
- Basis in paper: "In future work, we plan to: Extend the pruning approach to other medical image classification tasks"
- Why unresolved: Skin lesion images have relatively homogeneous backgrounds compared to other medical imaging modalities; the assumption that background features occupy large spatial regions may not hold for modalities with complex anatomical structures
- What evidence would resolve it: Evaluation of the same pruning methodology on diverse medical imaging datasets with demographic annotations, measuring both fairness improvements and diagnostic accuracy retention

**Open Question 2**
- Question: What are the unintended consequences of removing "skin-focused" features when those features may carry diagnostic information relevant to lesion type or severity?
- Basis in paper: The paper assumes channels with negative skewness represent irrelevant skin features, but does not verify whether any diagnostic information is inadvertently discarded alongside skin tone information
- Why unresolved: The method does not analyze what semantic information the pruned channels encode beyond spatial distribution characteristics; some skin-related features may be correlated with lesion presentation
- What evidence would resolve it: Probing experiments that compare the information content of pruned vs. retained channels, or analysis of cases where pruning degrades per-class accuracy

**Open Question 3**
- Question: How stable are fairness improvements under distribution shift, such as when deployed on populations with different skin tone distributions than ISIC2019?
- Basis in paper: The method is evaluated on a single dataset with a specific demographic composition; real-world deployment may encounter populations with different skin tone distributions or imaging conditions
- Why unresolved: No out-of-distribution or cross-dataset evaluation was conducted; the pruning decisions are made on the validation set of ISIC2019
- What evidence would resolve it: Cross-dataset evaluation on dermatology datasets from different geographic regions, or evaluation on synthetic distribution shifts that alter the skin tone distribution

## Limitations

- **Data dependency concerns**: The method relies on the assumption that lesions are small and localized while skin tone occupies a larger background area, which may not hold for all lesion types or imaging protocols
- **Evaluation scope limitations**: The study only evaluates on the ISIC2019 dataset with VGG11 and ViT-B16 architectures, leaving generalizability to other medical imaging domains untested
- **Implementation complexity for ViT**: The ViT pruning approach involves complex dimension management, particularly for the patch embedding layer and residual connections, making faithful reproduction challenging

## Confidence

**High confidence**: The core mechanism of using skewness to identify and prune skin-tone-related components is well-founded theoretically and demonstrates consistent results across both VGG and ViT architectures. The 1.5-6.5% improvement in fairness metrics while maintaining accuracy is well-supported by the experimental results.

**Medium confidence**: The claim of achieving fairness improvements without accuracy trade-offs is supported within the tested parameters but may be architecture- and dataset-dependent. The structural pruning approach's superiority over masking methods is demonstrated but could vary with different pruning strategies.

**Low confidence**: The generalizability of the skewness assumption across diverse medical imaging contexts and the long-term stability of pruned models during extended deployment are not established. The method's robustness to different image preprocessing pipelines and acquisition protocols remains uncertain.

## Next Checks

1. **Ablation study on skewness threshold**: Systematically vary the skewness threshold to identify the optimal balance between fairness improvement and accuracy preservation, and determine if the binary threshold is optimal

2. **Cross-dataset validation**: Test the pruned models on external skin lesion datasets (e.g., Derm7pt, SD-198) to assess whether the fairness improvements transfer beyond ISIC2019 and validate the method's robustness to different data distributions

3. **Extended lesion type analysis**: Evaluate the method's performance across specific lesion categories (melanoma vs. benign nevi vs. seborrheic keratosis) to identify if certain lesion types are more sensitive to skewness-based pruning, potentially revealing hidden biases in the pruning mechanism