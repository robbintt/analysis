---
ver: rpa2
title: 'Dagstuhl Perspectives Workshop 24352 -- Conversational Agents: A Framework
  for Evaluation (CAFE): Manifesto'
arxiv_id: '2506.11112'
source_url: https://arxiv.org/abs/2506.11112
tags:
- system
- user
- information
- conversational
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces the CONversational Information ACcess (CONIAC)
  World Model and the Conversational Agents Framework for Evaluation (CAFE) to address
  the need for more advanced evaluation of conversational information access systems.
  CAFE is a six-component framework consisting of stakeholder goals, user tasks, user
  aspects, evaluation criteria, methodology, and measures.
---

# Dagstuhl Perspectives Workshop 24352 -- Conversational Agents: A Framework for Evaluation (CAFE): Manifesto

## Quick Facts
- **arXiv ID**: 2506.11112
- **Source URL**: https://arxiv.org/abs/2506.11112
- **Reference count**: 40
- **Primary result**: Introduces CONIAC World Model and CAFE framework for evaluating conversational information access systems

## Executive Summary
This manifesto presents the Conversational Agents Framework for Evaluation (CAFE) to address the growing need for advanced evaluation of conversational information access systems. The framework introduces a six-component evaluation workflow that moves beyond traditional static assessments to support dynamic, continuous probing throughout conversational interactions. Built on the CONIAC World Model, CAFE emphasizes the importance of state tracking, multi-turn context, and the complex interplay between system capabilities and user needs in conversational systems.

## Method Summary
The paper proposes the CONIAC World Model and CAFE framework as a conceptual approach rather than a specific technical implementation. The method centers on instantiating a six-component evaluation workflow (Stakeholder Goals → User Aspects → Tasks → Criteria → Methodology → Measures) rather than training a specific model. The framework emphasizes continuous evaluation probes over single-point assessments, with probe instrumentation left as an implementation detail. Evaluation criteria are organized into system-centric (hardware/software) and user-centric (conversation/content/consequences) categories, with methodology selection constrained by the temporal nature of the criterion being measured.

## Key Results
- CAFE provides a structured six-component framework for evaluating conversational information access systems
- The framework supports dynamic evaluation through continuous probes rather than static assessments
- CONIAC World Model emphasizes state tracking, user information, and world knowledge in conversational systems
- Evaluation methodologies must align with both the evaluation focus (system vs. user) and the time model (memoryless, discrete event, discrete time, continuous)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Conversational information access systems require continuous evaluation probes rather than single-point assessments to capture interaction quality.
- Mechanism: Probes function as sensors that capture snapshots of system behavior and user reactions throughout the conversation, from start event to end event. These can be single-valued measurements or complex multi-modal objects (logs, video, eye-tracking). Probes may operate synchronously with conversational events or asynchronously, enabling both real-time monitoring and post-hoc analysis.
- Core assumption: Conversation quality emerges across multiple turns and cannot be reduced to a final satisfaction score or aggregate accuracy metric.
- Evidence anchors:
  - [abstract] "The framework supports dynamic evaluation through continuous probes rather than static assessments."
  - [section 3.1] "Evaluation should be inherently dynamic and multi-faceted... We define probes that act as sensors that constantly capture snapshots of the actions and reactions inherent to the conversational process."
  - [corpus] No direct corpus evidence on continuous probing; related work on conversational evaluation remains limited to single-turn or end-point metrics.
- Break condition: When evaluation methodology collapses all probes into a single aggregate score, losing temporal granularity and the ability to diagnose where interaction breakdowns occur.

### Mechanism 2
- Claim: A shared state across system components enables coherent multi-turn conversations that adapt to evolving user needs.
- Mechanism: The CONIAC System Layer maintains three state types: (1) world knowledge (external/domain context), (2) user information (preferences, goals, private data), and (3) conversation context (event history, dynamically updated beliefs about user intent). This state persists across sessions and informs proactive system events (clarifications, recommendations, acknowledgments).
- Core assumption: Users may abandon and resume conversations at arbitrary times; state must persist to maintain continuity without requiring users to re-articulate context.
- Evidence anchors:
  - [section 2.1] "We assume that an arbitrary amount of time may pass between two conversational events... the 'state' of the conversation is maintained for arbitrary lengths of time."
  - [section 2.2] "State tracking involves the context that tracks the flow (both user and system events)... and dynamically updated beliefs in the user information state."
  - [corpus] No corpus evidence directly addresses cross-session state persistence in conversational systems.
- Break condition: When state resets between turns (as in traditional query reformulation), the system cannot build on prior context, forcing users to re-explain needs or accept generic responses.

### Mechanism 3
- Claim: Effective evaluation requires selecting methodology based on both the evaluation focus (system vs. user) and the time model (memoryless, discrete event, discrete time, continuous).
- Mechanism: CAFE organizes methodologies along two axes: (1) user involvement continuum from offline simulation to qualitative interviews, and (2) time model from stationary (single measurement) to continuous (high-frequency sampling). This mapping constrains which criteria can be measured—for example, trust development requires longitudinal studies with continuous or discrete-time probing.
- Core assumption: Evaluation criteria are time-agnostic in definition but time-dependent in measurement; the chosen methodology must match the temporal nature of the criterion.
- Evidence anchors:
  - [section 8] "Evaluation methodologies must be selected or adapted to fit the concept of time the research question demands."
  - [section 7] "The temporal dimension of the interaction is particularly important in CONIAC systems... evaluation can address a single turn, a conversation, or system usage across multiple conversations."
  - [corpus] Corpus shows limited attention to time models; related work on conversational coaching agents (Substance over Style) notes similar challenges with multi-turn evaluation but does not formalize time models.
- Break condition: When methodology misaligns with the criterion's temporal nature—for instance, measuring trust (which develops over sessions) using a single-interaction experiment—the resulting data lacks validity for the intended inference.

## Foundational Learning

- Concept: **Mixed-initiative interaction**
  - Why needed here: CONIAC systems differ from traditional search by proactively asking clarifying questions, suggesting alternatives, and guiding users—not just responding to queries.
  - Quick check question: Can your system ask the user a question without being prompted, or does it only respond to explicit user inputs?

- Concept: **Task definiteness vs. complexity**
  - Why needed here: The framework distinguishes well-defined tasks (requiring iterative refinement) from ill-defined tasks (requiring exploratory search), which determines appropriate evaluation criteria.
  - Quick check question: For your target task, would users know their exact information need at the start, or would it evolve through conversation?

- Concept: **Multi-stakeholder evaluation**
  - Why needed here: Users, platform operators, content creators, and advertisers have overlapping and conflicting goals; evaluation must account for whose success is being measured.
  - Quick check question: Have you identified at least three distinct stakeholders for your system, and do your evaluation criteria address each?

## Architecture Onboarding

- Component map:
  CONIAC World Model
  ├── Process Layer: User Flow ↔ System Flow (via Conversational Events)
  ├── System Layer: IR + RS + QA components → Shared State (world knowledge, user info, context)
  └── Evaluation Layer: CAFE Framework → Continuous Probes

  CAFE Framework (workflow order):
  Stakeholder Goals → User Aspects → Tasks → Criteria → Methodology → Measures

- Critical path: Start with Stakeholder Goals (whose success matters?), then User Aspects (who are the users?), then Tasks (what are they trying to accomplish?). Only then define Criteria (what counts as success?), which constrains Methodology (how will we measure?), which determines Measures (what specific metrics?).

- Design tradeoffs:
  - **Offline simulation vs. user studies**: Simulation enables rapid iteration but cannot capture user-centric criteria like trust or enjoyment; user studies are expensive but necessary for validation.
  - **System-centric vs. user-centric criteria**: Optimizing for accuracy may harm perceived helpfulness; optimizing for engagement may reduce task completion.
  - **Granularity of probes**: More probes capture more detail but increase measurement overhead and may intrude on the user experience.

- Failure signatures:
  - **Context collapse**: System treats each utterance independently, asking users to repeat information or losing thread mid-conversation.
  - **Metric myopia**: Evaluation focuses exclusively on accuracy/relevance while ignoring conversation quality, trust, or decision confidence.
  - **Stakeholder blindspots**: System succeeds for platform metrics (engagement, ad impressions) but fails user goals (task completion, learning).
  - **Temporal mismatch**: Using memoryless methodology to evaluate criteria that develop over time (trust, skill acquisition).

- First 3 experiments:
  1. **Stakeholder goal mapping**: Document at least 4 stakeholder types for your system, list 2-3 goals per stakeholder, identify at least one goal conflict (e.g., user wants minimal friction, platform wants engagement).
  2. **Probe instrumentation pilot**: For a simple 5-turn product search conversation, define 3 continuous probes (e.g., response latency, user clarification frequency, perceived relevance per turn). Run 5 test conversations and verify probes can be collected without disrupting interaction.
  3. **Time model alignment check**: Select one criterion from each CAFE category (hardware, software, conversation, content, consequences). For each, determine the appropriate time model and verify your planned methodology supports it. Flag any mismatches for redesign.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can user-centric evaluation criteria be accurately assessed using LLMs in simulation settings during system development?
- Basis in paper: [explicit] Section 10 (Research Directions - Criteria) asks if we can foresee a future where user-centric criteria are assessed using LLMs in simulation.
- Why unresolved: It is currently unknown if LLMs can reliably simulate complex, subjective human experiences (e.g., satisfaction, trust) without the presence of actual users.
- What evidence would resolve it: Validation studies demonstrating a high correlation between LLM-based simulation scores and actual human user feedback on subjective criteria.

### Open Question 2
- Question: What offline evaluation measures can effectively assess continuous criteria such as conversation flow and continuance without user studies?
- Basis in paper: [explicit] Section 10 (Research Directions - Measures) asks what offline measures can be developed to assess continuous criteria besides user studies.
- Why unresolved: Current measures are often static or require expensive live user studies; dynamic conversational properties like "flow" are difficult to quantify using traditional offline metrics.
- What evidence would resolve it: The development of new computational metrics that robustly correlate with human judgments of conversation quality and engagement across multi-turn interactions.

### Open Question 3
- Question: How can experimental reproducibility be ensured when evaluating CONIAC systems that involve stochastic components like LLMs?
- Basis in paper: [explicit] Section 10 (Research Directions - Methodology) explicitly lists ensuring reproducibility in experiments involving users, simulation, and stochastic systems as an open question.
- Why unresolved: The inherent randomness of LLMs and the variability of user interactions complicate the ability to replicate specific experimental outcomes consistently.
- What evidence would resolve it: The establishment of standardized evaluation protocols or frameworks that yield statistically consistent results across repeated runs of identical evaluation scenarios.

## Limitations
- The framework lacks concrete implementation details for probe instrumentation and aggregation protocols
- Limited empirical validation beyond the illustrative swimsuit dialogue example
- Applicability across diverse conversational domains remains unproven
- Trade-offs between different evaluation methodologies are not systematically quantified

## Confidence

**High Confidence**: The six-component CAFE workflow structure and the distinction between system-centric and user-centric evaluation criteria are well-established and clearly articulated. The emphasis on continuous probing over static assessment aligns with established principles in conversational system evaluation.

**Medium Confidence**: The CONIAC World Model's state-tracking mechanisms and the time-model methodology mapping are logically sound but lack empirical validation. The probe instrumentation approach, while conceptually clear, requires concrete implementation details for practical application.

**Low Confidence**: The framework's ability to handle conflicting stakeholder goals and the specific protocols for aggregating continuous probe data remain largely theoretical without demonstrated solutions.

## Next Checks

1. **Probe Implementation Validation**: Implement the three continuous probes (response latency, clarification frequency, relevance perception) from the swimsuit dialogue example with 20 real users. Verify probe collection feasibility and analyze the relationship between probe measurements and conversation quality.

2. **Time Model Alignment Test**: Select one criterion from each CAFE category and systematically test whether the proposed time models (memoryless, discrete event, discrete time, continuous) produce different evaluation outcomes when applied to the same conversational system.

3. **Stakeholder Goal Conflict Resolution**: Design a simple conversational system (e.g., product recommendation) where user goals (quick task completion) conflict with platform goals (engagement). Apply CAFE to evaluate both perspectives and document the trade-offs revealed by the framework.