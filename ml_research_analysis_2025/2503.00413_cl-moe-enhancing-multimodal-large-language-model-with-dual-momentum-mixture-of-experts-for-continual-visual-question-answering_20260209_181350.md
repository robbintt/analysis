---
ver: rpa2
title: 'CL-MoE: Enhancing Multimodal Large Language Model with Dual Momentum Mixture-of-Experts
  for Continual Visual Question Answering'
arxiv_id: '2503.00413'
source_url: https://arxiv.org/abs/2503.00413
tags:
- experts
- task
- tasks
- expert
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses catastrophic forgetting in multimodal large
  language models (MLLMs) during continual learning for visual question answering
  (VQA). The proposed CL-MoE framework integrates MLLMs with continual learning using
  a Dual-Router Mixture-of-Experts (MoE) strategy.
---

# CL-MoE: Enhancing Multimodal Large Language Model with Dual Momentum Mixture-of-Experts for Continual Visual Question Answering

## Quick Facts
- arXiv ID: 2503.00413
- Source URL: https://arxiv.org/abs/2503.00413
- Reference count: 40
- Primary result: 14.36% improvement in average precision (51.34% vs 36.98%) and -0.02% forgetting (vs 20.69%) on 10 VQA tasks with LLaVA-7B

## Executive Summary
CL-MoE introduces a Dual-Router Mixture-of-Experts strategy with Dynamic Momentum updates to address catastrophic forgetting in multimodal large language models during continual visual question answering. The framework combines task-level and instance-level routers to robustly assign experts to tasks, while momentum-based parameter updates preserve knowledge from previous tasks. When applied to LLaVA-7B on VQA v2, CL-MoE achieves state-of-the-art performance with significantly reduced forgetting compared to baseline methods.

## Method Summary
CL-MoE integrates LLaVA-7B with continual learning using LoRA-based MoE layers. The Dual-Router MoE (RMoE) employs task-level and instance-level routers that are combined via weighted sum to select appropriate experts. The Dynamic Momentum MoE (MMoE) updates expert parameters based on task-expert relationships, categorizing experts as task-shared, task-specific, or irrelevant. During sequential training on 10 VQA tasks, the framework achieves parameter-efficient continual learning while maintaining strong forward and backward transfer capabilities.

## Key Results
- Average Precision (AP): 51.34% vs 36.98% baseline (14.36% improvement)
- Average Forgetting (AF): -0.02% vs 20.69% baseline
- Outperforms multitask learning upper bound (55.15% vs 51.34% AP) while requiring only sequential access
- Effective with 4-8 experts, LoRA rank=64, α=128, top-K=2

## Why This Works (Mechanism)

### Mechanism 1
Dual-router selection combining task-level and instance-level routing improves expert assignment over single-router approaches in continual VQA. The task-level router computes average expert weights across entire task datasets (global perspective), while the instance-level router makes fine-grained per-sample decisions (local perspective). These are combined via weighted sum (β hyperparameter) to produce final expert selection.

### Mechanism 2
Dynamic momentum updates based on expert-task relationships mitigate catastrophic forgetting while enabling knowledge integration. After training on task t, experts are categorized as task-shared (appear in current and previous tasks), task-specific (only in current task), or irrelevant (not in current task). Parameters update via θt = λ·θt-1 + (1-λ)·φt with different λ weights for each category.

### Mechanism 3
LoRA-based MoE enables parameter-efficient continual learning in large MLLMs while preserving base model capabilities. Original FFN weights W are frozen. Each expert Ei is implemented as low-rank matrices BiAi (rank r). MoE output becomes: f(x) = Wx + (α/r)ΣG(x)i·Ei(x), adding only ~r×n trainable parameters per MoE layer.

## Foundational Learning

- **Catastrophic Forgetting in Neural Networks**: Why needed here: The entire paper addresses forgetting in MLLMs during sequential task learning. Understanding why neural networks forget (gradient updates overwrite previous task parameters) is essential to appreciate why momentum preservation and expert isolation help. Quick check: Can you explain why training on task B after task A causes performance drops on task A, and why simply reducing learning rate doesn't solve this?

- **Mixture-of-Experts (MoE) with Gating/Router Networks**: Why needed here: CL-MoE builds on MoE architecture where a router (gating function) assigns weights to experts. Understanding sparse/gated MoE is prerequisite to grasping dual-router modifications. Quick check: In a standard MoE layer, how does the router decide which experts to use, and what does the output G(x) represent?

- **Low-Rank Adaptation (LoRA) for LLMs**: Why needed here: The paper implements experts as LoRA modules rather than full networks. Understanding LoRA's hypothesis (weight changes are low-rank) explains why this is parameter-efficient. Quick check: If a weight matrix W is R^(1024×4096), and LoRA uses rank r=8, how many trainable parameters does LoRA add versus fine-tuning W directly?

## Architecture Onboarding

- **Component map**:
  ```
  Input (image + question) → MLLM Encoder → Intermediate repr x
                                        ↓
                    ┌───────────────────┴────────────────────┐
                    │              RMoE Layer                 │
                    │  ┌─────────────┐  ┌──────────────┐     │
                    │  │ Instance    │  │ Task-level   │     │
                    │  │ Router GI   │  │ Router GT    │     │
                    │  └──────┬──────┘  └──────┬───────┘     │
                    │         │                │              │
                    │         └───────┬────────┘              │
                    │                 ↓ β-weighted sum        │
                    │         Expert weights w                │
                    └─────────────────┬───────────────────────┘
                                      ↓
                    ┌─────────────────┴───────────────────────┐
                    │    Experts E1...En (LoRA: BiAi)         │
                    │    + Frozen FFN W                       │
                    └─────────────────┬───────────────────────┘
                                      ↓
                    After task t training:
                                      ↓
                    ┌─────────────────┴───────────────────────┐
                    │              MMoE Update                 │
                    │  Categorize: task-shared/task-specific  │
                    │  θt = λ·θt-1 + (1-λ)·φt                │
                    └─────────────────────────────────────────┘
  ```

- **Critical path**:
  1. During training on task t: forward pass through RMoE → train instance router GI and expert params φt
  2. After task t: compute task-level weights GT(t), identify top-K experts Et
  3. Categorize experts (shared vs specific vs none), apply MMoE momentum updates
  4. At inference: identify task via cluster center distance → retrieve GT(t) → combine with GI(x)

- **Design tradeoffs**:
  - **n (number of experts)**: More experts → more specialization but more parameters. Paper finds n=4-8 sufficient; n=1 severely underperforms (32.51% vs 51.34% AP)
  - **K (top experts per task)**: Too few → insufficient capacity; too many → redundancy. Paper uses K=2, minimal difference from K=3-4
  - **γ (momentum weight)**: Controls plasticity-stability tradeoff. γ=0.7 optimal; higher preserves old knowledge but limits new learning
  - **β (router balance)**: β=0.5 balances task vs instance routing; extreme values drop performance ~8-10% AP
  - **Task order**: Paper shows order matters significantly (reverse order: 57.08% vs forward: 51.34% AP)

- **Failure signatures**:
  - AF (Average Forgetting) >> 0: momentum weight γ too low, or MMoE not being applied
  - AP drops on specific tasks: task-level router not identifying correct experts, or those experts' params corrupted
  - Inference errors on all tasks: task identification via cluster centers failing (check Rt computation)
  - Training instability: learning rate too high for LoRA layers (paper uses 2e-4)
  - Performance similar to Vanilla: either routers not trained or MMoE not applied

- **First 3 experiments**:
  1. **Baseline reproduction**: Run Vanilla LLaVA-7B on 10 VQA tasks sequentially, compute AP and AF. Verify you see ~36.98% AP and ~20.69% AF as reported.
  2. **Ablation on components**: Run CL-MoE with (a) only MMoE, (b) only RMoE, (c) both. Expect: MMoE alone (~48.43% AP), RMoE alone (~41.23% AP), both (~51.34% AP).
  3. **Hyperparameter sensitivity**: Sweep γ ∈ {0.5, 0.6, 0.7, 0.8, 0.9, 1.0} with fixed β=0.5, n=8, K=2. Verify optimal at γ=0.7.

## Open Questions the Paper Calls Out

**How can the CL-MoE framework be effectively extended to other diverse multimodal tasks beyond Visual Question Answering (VQA)?**
The conclusion states, "In the future, we aim to extend continual learning-based MLLMs to other diverse tasks, further addressing the forgetting problem in MLLMs for continual multitask learning." The current study exclusively validates the method on VQA v2 using LLaVA-7B, leaving its applicability to tasks like image captioning, video understanding, or audio-language tasks unverified.

**How can the performance gap between the sequential CL-MoE training and the Multitask upper bound be further minimized?**
The authors note in the Experimental Analysis that "compared with the upper bound method Multitask... our model still has room to improve. We would like to explore a more effective algorithm... in the future." While CL-MoE reduces forgetting (AF), its Average Precision (51.34%) remains significantly lower than the Multitask upper bound (55.15%), indicating that sequential learning still incurs a penalty compared to joint training.

**How robust is the framework when the unsupervised task indexing module misclassifies the task identity of a test instance?**
Section 4.2 introduces a task indexing module using Euclidean distance to cluster centroids because task indices are unknown at inference. The paper assumes this mapping is reliable but provides no error analysis for this step. If the text feature of a test question is equidistant to multiple task clusters or falls outside learned distributions, the wrong experts may be selected, potentially degrading performance.

## Limitations

- **Implementation ambiguity**: Critical architectural details like LoRA placement, router dimensions, and task division specifics are not fully specified, creating uncertainty for faithful reproduction.
- **Task order sensitivity**: Performance varies significantly with task sequence (57.08% vs 51.34% AP for reverse vs forward order), indicating potential fragility to training order.
- **Momentum weight dependence**: The optimal γ=0.7 requires careful tuning and may not generalize across different task sequences or dataset characteristics.

## Confidence

**High Confidence (Mechanism Understanding)**: The core dual-router and momentum update mechanisms are clearly described with explicit mathematical formulations (Equations 9-12). The conceptual framework for combining task-level and instance-level routing is well-established in MoE literature, though the specific dual-router application to continual learning is novel.

**Medium Confidence (Reported Results)**: The experimental results showing 51.34% AP and 0.02% AF are impressive and internally consistent with reported ablations. However, the lack of implementation details creates uncertainty about exact reproduction.

**Low Confidence (Generalizability)**: Limited ablation studies on task order sensitivity and the narrow range of tested hyperparameters constrain confidence in performance claims across different scenarios. The heavy dependence on task identification via cluster centers may not generalize to more complex task boundaries.

## Next Checks

1. **Implement Task-Level Router from Scratch**: Reproduce the GT(t) computation by averaging instance-level router outputs over entire task datasets. Verify that the top-K expert selection aligns with task semantics by visualizing router weight distributions across tasks. Test whether task identification via cluster center distance (Rt) reliably matches ground-truth task boundaries.

2. **Systematically Sweep Momentum Weight γ**: Run experiments with γ ∈ {0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 1.0} while keeping all other hyperparameters fixed. Plot AP and AF curves to verify the claimed optimal at γ=0.7, and determine whether the relationship is smooth or contains sharp transitions that could indicate implementation issues.

3. **Cross-Validate Task Order Sensitivity**: Train CL-MoE on all 10! possible task orders (or at minimum 5-10 diverse orders including reverse order) to quantify the true variance in performance. Compare this variance against the forward/reverse order results reported in the paper to determine whether the task order sensitivity represents a fundamental limitation of the approach.