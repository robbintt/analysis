---
ver: rpa2
title: LLM-Assisted Formalization Enables Deterministic Detection of Statutory Inconsistency
  in the Internal Revenue Code
arxiv_id: '2511.11954'
source_url: https://arxiv.org/abs/2511.11954
tags:
- prolog
- reasoning
- legal
- inconsistency
- statutory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study demonstrates that LLM-assisted formalization, anchored
  in Prolog symbolic logic, enables deterministic detection of statutory inconsistencies
  in the U.S. Internal Revenue Code.
---

# LLM-Assisted Formalization Enables Deterministic Detection of Statutory Inconsistency in the Internal Revenue Code
## Quick Facts
- arXiv ID: 2511.11954
- Source URL: https://arxiv.org/abs/2511.11954
- Reference count: 0
- Demonstrates LLM-assisted Prolog formalization enables deterministic statutory inconsistency detection in tax law

## Executive Summary
This study demonstrates that LLM-assisted formalization, anchored in Prolog symbolic logic, enables deterministic detection of statutory inconsistencies in the U.S. Internal Revenue Code. When GPT-4o was prompted with natural language alone, it achieved 33% accuracy and 100% rule coverage in identifying the inconsistency; Prolog-augmented prompting yielded the same accuracy but only 66% rule coverage. In contrast, a Prolog model refined with GPT-5 produced reproducible, logically consistent results across multiple fact patterns and matched outcomes from an independently verified Z3 theorem-prover implementation. This hybrid approach shows that while LLMs can aid in rule translation and refinement, deterministic symbolic reasoning is essential for reliable statutory inconsistency detection.

## Method Summary
The research employed a hybrid approach combining large language models with formal logic systems to detect statutory inconsistencies. GPT-4o was first used to translate natural language tax provisions into Prolog rules, with and without direct Prolog prompting. The resulting rule sets were tested against fact patterns to identify inconsistencies. Subsequently, GPT-5 was used to manually refine the Prolog rules, improving their logical consistency. The Prolog implementation was then independently verified using the Z3 theorem prover to ensure deterministic correctness. The approach was tested on a specific inconsistency within the Internal Revenue Code.

## Key Results
- GPT-4o alone achieved 33% accuracy and 100% rule coverage in detecting inconsistencies
- Prolog-augmented prompting with GPT-4o maintained 33% accuracy but reduced rule coverage to 66%
- GPT-5-refined Prolog rules produced reproducible, logically consistent results matching Z3 verification across multiple test cases

## Why This Works (Mechanism)
The hybrid approach works by leveraging LLMs' natural language processing strengths to translate statutory text into formal logic, while using Prolog's deterministic reasoning to ensure logical consistency. LLMs can parse complex legal language and identify relevant rules, but their probabilistic nature can introduce inconsistencies. Prolog provides a formal framework where rules are evaluated deterministically, eliminating the uncertainty inherent in pure LLM approaches. The refinement step with GPT-5 helps bridge the gap between natural language interpretation and formal logical representation.

## Foundational Learning
- Prolog symbolic logic - Essential for deterministic rule evaluation and inconsistency detection in legal text
- LLM prompt engineering - Critical for extracting accurate formal representations from natural language statutes
- Theorem proving (Z3) - Provides independent verification of logical consistency in formal representations
- Tax law structure - Understanding hierarchical and cross-referenced nature of statutory provisions
- Rule coverage metrics - Important for measuring completeness of extracted legal rules

## Architecture Onboarding
- Component map: Natural Language Text -> GPT-4o Translation -> Prolog Rules -> Consistency Checking -> Results
- Critical path: Text input → LLM processing → Prolog formalization → Rule application → Inconsistency detection
- Design tradeoffs: LLM flexibility vs Prolog determinism; coverage vs accuracy; manual refinement cost vs automation
- Failure signatures: Inconsistent rule extraction, incomplete coverage, logical contradictions in Prolog representation
- First experiments: 1) Test single provision translation accuracy, 2) Verify Prolog rule completeness, 3) Validate Z3 theorem prover integration

## Open Questions the Paper Calls Out
None

## Limitations
- Extremely small corpus (one statutory provision) limits generalizability to broader tax law or other legal domains
- Performance heavily dependent on careful prompt engineering with no quantification of sensitivity
- Manual GPT-5 refinement introduces human-in-the-loop variability without independent benchmarking
- Lack of human-ground-truth labels makes absolute accuracy assessment difficult
- Z3 comparison limited to single inconsistency, leaving scalability unverified

## Confidence
- Deterministic symbolic reasoning superiority: **High** - Supported by reproducible Prolog and Z3 results matching across test cases
- LLM-only accuracy and coverage: **Medium** - Based on single-shot testing with no cross-validation or sensitivity analysis
- GPT-5 refinement effectiveness: **Low** - Dependent on subjective human input and not independently benchmarked

## Next Checks
1. Test the hybrid LLM-Prolog pipeline on 10–20 additional statutory provisions with varying complexity to assess scalability and generalizability
2. Conduct prompt ablation studies to quantify sensitivity of LLM-generated rules to small wording changes
3. Implement a blinded human audit of detected inconsistencies to establish ground truth and measure absolute accuracy of the approach