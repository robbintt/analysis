---
ver: rpa2
title: 'Enhancing Adversarial Robustness with Conformal Prediction: A Framework for
  Guaranteed Model Reliability'
arxiv_id: '2506.07804'
source_url: https://arxiv.org/abs/2506.07804
tags:
- prediction
- adversarial
- size
- training
- conformal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving adversarial robustness
  in deep learning models by integrating conformal prediction with adversarial training.
  The authors propose OPSA, an adversarial attack method that maximizes model uncertainty
  by targeting the size of conformal prediction sets without requiring knowledge of
  the significance level.
---

# Enhancing Adversarial Robustness with Conformal Prediction: A Framework for Guaranteed Model Reliability

## Quick Facts
- arXiv ID: 2506.07804
- Source URL: https://arxiv.org/abs/2506.07804
- Reference count: 40
- Key outcome: OPSA attack maximizes uncertainty via conformal set size; OPSA-AT defense improves robustness while maintaining coverage guarantees.

## Executive Summary
This paper addresses the challenge of improving adversarial robustness in deep learning models by integrating conformal prediction with adversarial training. The authors propose OPSA, an adversarial attack method that maximizes model uncertainty by targeting the size of conformal prediction sets without requiring knowledge of the significance level. They also introduce OPSA-AT, a defense strategy that incorporates OPSA into a novel conformal training paradigm to minimize prediction set sizes while maintaining coverage guarantees. Experimental results on CIFAR-10, CIFAR-100, and mini-ImageNet datasets demonstrate that OPSA generates larger prediction sets than baseline attacks, increasing uncertainty, while OPSA-AT significantly enhances robustness against various adversarial attacks and maintains reliable prediction with smaller prediction set sizes.

## Method Summary
The framework combines conformal prediction with adversarial training through two key components: OPSA attack and OPSA-AT defense. OPSA optimizes perturbations to maximize soft prediction set size using a temperature-scaled sigmoid surrogate for discrete set cardinality, allowing attacks without knowledge of the defender's significance level α. OPSA-AT splits mini-batches into training and calibration subsets, generating OPSA perturbations on the training portion while computing thresholds from the calibration portion. The total loss jointly optimizes for true-label inclusion and set compactness against perturbed inputs. The method maintains coverage guarantees through exchangeability preservation between calibration and test sets under consistent perturbation application.

## Key Results
- OPSA attack produces larger prediction sets (7.29 vs ~5-7) than baseline attacks on CIFAR-10
- OPSA-AT significantly improves robustness against various adversarial attacks while maintaining coverage
- Coverage remains stable around 90% target while reducing prediction set sizes under attack
- OPSA-AT outperforms standard adversarial training and conformal training baselines

## Why This Works (Mechanism)

### Mechanism 1: Attack via Soft Set Size Maximization
- Claim: Maximizing soft prediction set size induces greater uncertainty than standard adversarial objectives.
- Mechanism: OPSA optimizes perturbations to maximize $M_T(x+\epsilon; f, f_y(x+\epsilon)) = \sum_k \sigma((f_k(x+\epsilon) - f_y(x+\epsilon))/T)$, a differentiable surrogate for hard set cardinality. By using the perturbed true-class logit as the internal threshold, the attacker need not know the defender's significance level $\alpha$.
- Core assumption: The temperature-scaled sigmoid adequately approximates the indicator function for set membership, and gradient-based optimization can traverse this surrogate landscape effectively.
- Evidence anchors:
  - [abstract] "OPSA... maximizes model uncertainty by targeting the size of conformal prediction sets without requiring knowledge of the significance level."
  - [section 3.3] "The optimization problem is thus formulated as: $\epsilon^* = \arg\max_{\|\epsilon\|_p \leq r} M_T(x+\epsilon; f, f_y(x+\epsilon))$"
  - [corpus] Limited direct corroboration; neighbor papers focus on CP for GNNs and fairness, not adversarial set-size attacks.
- Break condition: If the temperature $T$ is poorly calibrated, the sigmoid poorly approximates hard set inclusion, degrading attack efficacy.

### Mechanism 2: Defense via Bi-Objective Conformal Training
- Claim: Adversarial training against set-size attacks reduces prediction set size under attack while preserving coverage.
- Mechanism: OPSA-AT splits each mini-batch into $B_{train}$ (for adversarial perturbation generation) and $B_{cal}$ (for threshold computation). The total loss $L_{total} = L_{class} + \lambda M_{T_2}$ jointly encourages true-label inclusion and set compactness against perturbed inputs.
- Core assumption: Exchangeability between $B_{train}$ and $B_{cal}$ within each batch sufficiently approximates the population-level calibration required for valid coverage.
- Evidence anchors:
  - [abstract] "OPSA-AT... minimizes prediction set sizes while maintaining coverage guarantees."
  - [section 3.4] "The threshold $\tau$ in our method is obtained by computing the exact quantile, rather than being determined via a sigmoid-approximated mechanism."
  - [corpus] Indirect support; "Enhancing Trustworthiness of Graph Neural Networks with Rank-Based Conformal Training" similarly uses conformal training but in non-adversarial GNN contexts.
- Break condition: If mini-batch size is too small relative to class count $K$, threshold estimation becomes unstable, violating coverage.

### Mechanism 3: Coverage Robustness via Exchangeability Preservation
- Claim: Coverage guarantees persist when calibration and test data share the same perturbation mechanism.
- Mechanism: CP validity requires exchangeability, not clean data. If both calibration and test sets are OPSA-perturbed (or both clean), exchangeability holds, and $P(y \in \Gamma(x)) \geq 1-\alpha$ remains valid.
- Core assumption: The perturbation mechanism is applied consistently or randomly in a way that preserves exchangeability between calibration and test distributions.
- Evidence anchors:
  - [section 3.5] "The validity of CP relies solely on this exchangeability property of the (potentially transformed) data points."
  - [corpus] Neighbor papers (e.g., "Non-exchangeable Conformal Prediction for Temporal Graph Neural Networks") highlight challenges when exchangeability fails, underscoring its necessity.
- Break condition: If perturbation is applied asymmetrically (e.g., only to test, not calibration), exchangeability breaks and coverage guarantees are void.

## Foundational Learning

- **Conformal Prediction (Threshold Method)**:
  - Why needed here: The entire framework builds on THR-based prediction sets $\Gamma(x) = \{k: f_k(x) \geq \tau\}$ where $\tau$ is a calibration quantile.
  - Quick check question: Given calibration scores $\{0.2, 0.5, 0.8\}$ and $\alpha=0.1$, what threshold guarantees 90% coverage?

- **Adversarial Training as Min-Max Optimization**:
  - Why needed here: OPSA-AT reformulates AT within CP as optimizing model parameters against worst-case set-expanding perturbations.
  - Quick check question: Why does standard cross-entropy loss fail to capture prediction set efficiency?

- **Soft Set Size via Temperature-Scaled Sigmoid**:
  - Why needed here: The attack and defense both rely on differentiable surrogates for discrete set cardinality.
  - Quick check question: What happens to $\sigma((f_k - \tau)/T)$ as $T \to 0$? How does this relate to hard set membership?

## Architecture Onboarding

- **Component map**:
  - Backbone classifier -> OPSA attack module -> Calibration splitter -> Loss aggregator

- **Critical path**:
  1. Pre-train classifier on clean data (5 epochs CIFAR, 10 mini-ImageNet)
  2. For each batch: split -> generate OPSA perturbations on $B_{train}$ -> compute $\tau$ from $B_{cal}$ -> update $\theta$ via $L_{total}$
  3. At test time: apply OPSA to calibration set, compute $\tau$, then evaluate on perturbed test set

- **Design tradeoffs**:
  - **Mini-batch size**: Larger batches (500–1000) better approximate exchangeability but increase memory; paper used 64–200 due to hardware limits
  - **Temperature $T_1$ (attack)**: Higher values smooth the objective but may reduce attack potency; $T_1=1$ worked well
  - **Weight $\lambda$**: Balances coverage vs. efficiency; follow Stutz et al. (2021) for tuning guidance

- **Failure signatures**:
  - **Coverage collapse**: Check if $\tau$ is computed from a batch too small or contaminated with training data
  - **Set explosion under attack**: Indicates defense undertrained against OPSA; increase attack iterations $J$ during training
  - **Gradient instability**: Verify sigmoid temperature matches THR approximation; mismatch causes surrogates to diverge from hard sets

- **First 3 experiments**:
  1. **Sanity check**: On CIFAR-10, apply OPSA to a pre-trained ResNet34; verify prediction set size increases vs. PGD/AutoAttack baselines (Table 1 shows OPSA yields size 7.29 vs. ~5–7 for others).
  2. **Ablation on $T_1$**: Sweep $T_1 \in \{0.001, 0.1, 1, 10, 100\}$ on CIFAR-100; confirm size plateaus near $T_1 \approx 10$ (Table 10).
  3. **Mini-batch sensitivity**: Train OPSA-AT with batch sizes 64 vs. 200 vs. 500 (if memory permits); measure SSCV and coverage stability (Remark 2 suggests 5–10× class count as target).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of OPSA-AT generalize to different deep learning architectures, specifically PreAct ResNet, compared to the ResNet34 and ResNet50 architectures used in this study?
- Basis in paper: [explicit] The conclusion states, "We plan to expand our experiments by including the PreAct ResNet network in future studies."
- Why unresolved: The current experimental scope is limited to standard ResNet architectures, and it is unclear if the method's robustness gains transfer to architectures with different residual connections or activation functions.
- What evidence would resolve it: Empirical results measuring Coverage, Size, and SSCV on CIFAR-10/100 and mini-ImageNet using PreAct ResNet models trained with the OPSA-AT strategy.

### Open Question 2
- Question: Does utilizing the theoretically recommended mini-batch sizes of 500–1000 significantly improve the approximation fidelity and robustness of OPSA-AT compared to the smaller batches (64–200) used due to hardware constraints?
- Basis in paper: [explicit] The authors note, "theoretical analysis suggests optimal performance would be achieved with mini-batch sizes between 500−1000," but computational constraints restricted them to smaller batches.
- Why unresolved: The practical trade-off between computational cost and the theoretical benefits of larger batch sizes for exchangeability approximation remains untested.
- What evidence would resolve it: Ablation studies comparing the convergence rate and final robustness metrics of OPSA-AT when trained with batch sizes of 500+ versus the reported 200, using high-memory hardware.

### Open Question 3
- Question: Can the OPSA framework effectively minimize prediction set sizes when applied to alternative differentiable non-conformity score functions, such as Adaptive Prediction Sets (APS), rather than the Threshold (THR) method?
- Basis in paper: [inferred] Remark 1 claims the method "can be readily extended to accommodate alternative score functions," yet all experiments exclusively utilize the THR score ($s(x, k) = 1 - f_k(x)$).
- Why unresolved: While the gradient-based attack is theoretically compatible, the interaction between OPSA and the sorting mechanisms inherent in methods like APS or RAPS is not empirically verified.
- What evidence would resolve it: Experiments applying OPSA-AT using the APS or RAPS scores on the same benchmarks, comparing the resulting set sizes and coverage violations against the THR baseline.

## Limitations

- Computational constraints limited mini-batch sizes to 64-200, below the theoretically optimal 500-1000 for exchangeability approximation
- Framework has only been validated on CIFAR-10/100 and mini-ImageNet with ResNet architectures, limiting scalability assessment
- Exchangeability assumption may not hold in real-world scenarios with non-i.i.d. data streams or asymmetric perturbation application

## Confidence

- **High Confidence**: The core mechanism of OPSA-AT (joint optimization of coverage and set size) is well-specified and theoretically grounded. The attack strategy (OPSA) is clearly defined and its purpose is unambiguous.
- **Medium Confidence**: The empirical results demonstrating improved robustness and maintained coverage are compelling, but the lack of specified hyperparameters (e.g., λ, training epochs, learning rates) limits reproducibility and full validation.
- **Low Confidence**: Claims about the attack's ability to maximize uncertainty without knowing α are based on the theoretical formulation; however, the practical impact on real-world adversaries is not explored.

## Next Checks

1. **Ablation on Temperature T₁**: Systematically sweep T₁ values to quantify the attack's sensitivity and identify optimal settings.
2. **Mini-Batch Size Impact**: Experiment with larger mini-batch sizes (e.g., 500-1000) to assess the trade-off between exchangeability approximation and computational feasibility.
3. **Non-I.I.D. Data Evaluation**: Test the framework's coverage guarantees on temporally or spatially dependent data to validate the exchangeability assumption under realistic conditions.