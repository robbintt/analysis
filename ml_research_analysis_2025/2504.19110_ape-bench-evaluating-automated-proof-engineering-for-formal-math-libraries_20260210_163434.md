---
ver: rpa2
title: 'APE-Bench: Evaluating Automated Proof Engineering for Formal Math Libraries'
arxiv_id: '2504.19110'
source_url: https://arxiv.org/abs/2504.19110
tags:
- task
- proof
- evaluation
- engineering
- semantic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: APE-Bench introduces the first systematic framework for evaluating
  repository-scale proof engineering in formal mathematics libraries. Unlike existing
  benchmarks focused on isolated theorem proving, APE-Bench automates task extraction
  from real Mathlib commit histories, requiring both syntactic compilation and semantic
  validation of natural-language instructions.
---

# APE-Bench: Evaluating Automated Proof Engineering for Formal Math Libraries

## Quick Facts
- **arXiv ID**: 2504.19110
- **Source URL**: https://arxiv.org/abs/2504.19110
- **Reference count**: 25
- **Key outcome**: Introduces first systematic framework for evaluating repository-scale proof engineering in formal mathematics libraries, achieving 24-47% success rates with frontier models

## Executive Summary
APE-Bench addresses the gap between existing theorem-proving benchmarks focused on isolated problems and real-world proof engineering tasks in large mathematical libraries. The framework extracts tasks from actual Mathlib commit histories, requiring agents to perform repository-scale activities like multi-file coordination, library evolution, and maintenance. Using a unified execution framework (APE-Harness) based on task contract abstraction, the benchmark enables standardized evaluation across diverse formal mathematics tasks and fair systematic comparison of different agent implementations. Comprehensive evaluation with 100 tasks from 67 Mathlib commits shows that frontier models achieve 24-47% success rates under dual verification, with Gemini 3 Flash demonstrating superior cost-efficiency.

## Method Summary
APE-Bench constructs tasks from real Mathlib commit histories through commit filtering, natural language instruction synthesis, and validation. The APE-Harness execution framework uses task contract abstraction to separate task specification from execution strategy, enabling standardized evaluation across diverse formal mathematics tasks. Each task includes environment bindings (commit hash, toolchain), objectives (natural-language instructions), boundaries (read-only/blocked paths), and verification protocols. Evaluation employs dual verification: syntactic compilation through Lean's type-checker and semantic validation via three-dimensional LLM-as-judge assessment (semantic correctness, requirement alignment, scope control). The infrastructure uses content-addressable storage to manage multi-version environments efficiently, reducing storage from 480GB to 71GB through deduplication.

## Key Results
- **Performance gap**: Frontier models achieve 24-47% success rates, with Gemini 3 Flash at 24% and GPT-5.2 at 41% pass rates under dual verification
- **Cost-efficiency**: Gemini 3 Flash demonstrates superior cost-efficiency while maintaining competitive performance
- **Judge reliability**: Semantic validation judges achieve 90.6% agreement with human experts across three dimensions
- **Storage optimization**: Content-addressable storage reduces storage from 480GB to 71GB (85% reduction) and embedding costs from $12.9K to $197 (98.5% reduction)

## Why This Works (Mechanism)

### Mechanism 1: Task Contract Abstraction Enables Scaffold Interchangeability
Declarative task specifications separate from execution strategies allow diverse agents to be fairly compared on identical tasks. Contracts specify environment bindings, objectives, boundaries, and verification protocols—without prescribing how agents achieve them. Different scaffolds (APE-Agent, Claude Code, Codex CLI) execute through shared infrastructure services. Core assumption: Task requirements can be fully captured declaratively; execution logic is separable. Evidence: [abstract] "APE-Harness... enables standardized evaluation across diverse formal mathematics tasks"; [section 4.1] "task contract abstraction that separates task specification from execution strategy". Break condition: Tasks requiring scaffold-specific logic that cannot be expressed declaratively.

### Mechanism 2: Dual Verification Captures Compilation Gaps
Lean compilation is necessary but insufficient; semantic validation via LLM judges captures requirement satisfaction and scope control. Syntactic verification filters type-errors; three-dimensional LLM-as-judge assessment validates intent fulfillment beyond type-correctness. Core assumption: LLM judges align sufficiently with human expert judgment for reliable automated evaluation. Evidence: [abstract] "dual verification that validates both syntactic compilation and semantic requirement satisfaction"; [section 6.3 Table 5] "Semantic judges demonstrate consistent evaluation... Overall (64) 90.6%". Break condition: Systematic LLM judge failures on edge cases—false positives from incomplete implementations or scope violations; false negatives from calibration variance.

### Mechanism 3: Content-Addressable Storage Enables Multi-Version Evaluation
Hash-based deduplication across library versions makes multi-commit evaluation computationally tractable. Library files indexed by cryptographic hash; identical files across commits share compiled artifacts. Version manifests map commits to content hashes without duplicating artifacts. Core assumption: Significant content overlap exists across consecutive Mathlib commits. Evidence: [section 6.6] "deduplication reduces storage from 480GB to 71GB (85% reduction)"; [section 4.2] "This design transforms linear version scaling into logarithmic growth". Break condition: Repository evolution patterns with low inter-version content overlap would eliminate deduplication benefits.

## Foundational Learning

- **Concept: Proof Engineering vs. Isolated Theorem Proving**
  - Why needed here: APE-Bench explicitly targets repository-scale activities (multi-file coordination, library evolution, maintenance) that existing benchmarks miss.
  - Quick check question: Can you name three proof engineering activities that miniF2F cannot evaluate?

- **Concept: Lean Compilation Semantics**
  - Why needed here: Understanding why type-checking ≠ correctness is essential for grasping why dual verification is required.
  - Quick check question: A proof compiles successfully but semantic judges reject it—what failure modes from Appendix B might explain this?

- **Concept: LLM-as-Judge Reliability**
  - Why needed here: The entire semantic validation layer depends on judge alignment with human experts.
  - Quick check question: Why does scope control (89% accuracy) show lower reliability than semantic correctness (92%)?

## Architecture Onboarding

- **Component map:**
  ```
  APE-Bench (construction)    APE-Harness (execution)
  ├─ Commit filtering         ├─ Task contracts
  ├─ Instruction synthesis    │   ├─ Environment bindings
  └─ Validation               │   ├─ Objectives
                              │   ├─ Boundaries
                              │   └─ Verification protocols
                              ├─ Services
                              │   ├─ Execute (compilation)
                              │   └─ Retrieve (semantic search)
                              ├─ Workspaces
                              │   ├─ Scratch (writable)
                              │   ├─ Target (immutable library)
                              │   └─ Reference (read-only deps)
                              └─ Scaffolds
                                  ├─ APE-Agent (ReAct, integrated verification)
                                  ├─ Claude Code adapter
                                  └─ Codex CLI adapter
  ```

- **Critical path:**
  1. Benchmark construction: `Commits → Filter → Instruction synthesis (as task contract) → Validation`
  2. Evaluation: `Contract → Workspace init → Agent execution → Compile check → LLM judge → Result`

- **Design tradeoffs:**
  - Integrated verification (APE-Agent: edit returns compile results) vs. separate calls (Claude Code/Codex: edit + execute) — Section 6.4 shows 5.6% vs 16.9-26.6% execute calls
  - Fixed budget evaluation ($3/task) vs. unconstrained performance — enforces practical cost constraints
  - Self-hosted infrastructure (instruction synthesis runs through APE-Harness) vs. external pipeline — demonstrates generality but adds complexity

- **Failure signatures:**
  - Judge false positives: Incomplete implementations with correct-looking snippets, excessive refactoring masked by correct core
  - Judge false negatives: Over-penalizing beneficial minor refactorings
  - Agent budget exhaustion: Higher per-turn costs limit exploration (Pro/GPT-5.2) vs. cost-efficient Flash

- **First 3 experiments:**
  1. Run `ape_622f41ab_Sqrt` task manually through APE-Harness; trace the contract → execution → verification flow
  2. Compare tool usage distributions (Figure 4) by running identical tasks on APE-Agent vs. Claude Code—observe integrated vs. separate verification patterns
  3. Validate judge reliability: Sample 10 agent solutions, annotate yourself across three dimensions, compare against LLM judge outputs to calibrate trust thresholds

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How can semantic validation protocols be improved to reliably detect incomplete implementations and scope violations?
- **Basis in paper**: [explicit] Section 6.3 states future work should "refine scope assessment criteria," and Appendix B details false positives where judges accepted truncated files or excessive refactoring.
- **Why unresolved**: Current LLM-as-Judge setups focus on local mathematical correctness, often missing global file integrity issues or subtle scope creep.
- **What evidence would resolve it**: A modified judge pipeline integrating explicit scope metrics (diff size bounds, deletion warnings) that achieves >95% accuracy on the "Scope Control" dimension.

### Open Question 2
- **Question**: Does the APE framework and task contract abstraction generalize to other proof assistants like Coq or Isabelle?
- **Basis in paper**: [inferred] The evaluation is exclusively conducted on Mathlib (Lean 4), and the infrastructure (Section 4.2) relies on Lean-specific mechanisms (Lake, .lake directories) for content-addressable storage.
- **Why unresolved**: While the abstraction is theoretically general, the "execute" and "retrieve" services are implemented specifically for Lean's compilation model.
- **What evidence would resolve it**: Successful instantiation of APE-Bench on a non-Lean library (e.g., Coq's Mathematical Components) using the APE-Harness infrastructure.

### Open Question 3
- **Question**: What architectural advances are required to overcome the current reasoning capability ceiling in repository-scale proof engineering?
- **Basis in paper**: [inferred] Section 6.5 (Ablation Studies) shows that varying tool configurations (File, Shell, Retrieval) results in nearly identical success rates (44-47%), indicating the bottleneck is reasoning capability rather than tool access.
- **Why unresolved**: The paper demonstrates that current frontier models hit a performance ceiling regardless of tooling, but does not propose methods to transcend this specific limit.
- **What evidence would resolve it**: A new agent architecture or training methodology that achieves >60% success on APE-Bench without significantly increasing cost-per-task.

## Limitations

- **API availability**: Reliance on proprietary model APIs (GPT-5.2, Gemini 3 Pro/Flash) that may not be publicly accessible
- **Domain specificity**: Benchmark focus on Lean 4/Mathlib limits generalizability to other formal systems
- **Judge uncertainty**: 10% systematic disagreement with human experts suggests non-negligible evaluation uncertainty

## Confidence

**High Confidence** (Evidence directly from paper results):
- Dual verification captures more comprehensive evaluation than compilation-only approaches (90.6% judge accuracy, supported by systematic comparison in Table 5)
- Content-addressable storage achieves significant deduplication (85% storage reduction, 98.5% embedding cost reduction)
- Gemini 3 Flash demonstrates superior cost-efficiency at 24% pass rate vs 41% for GPT-5.2 but at lower per-task cost

**Medium Confidence** (Evidence from controlled experiments with known limitations):
- Task contract abstraction enables fair scaffold comparison (supported by execution pattern differences in Section 6.4, but limited to three scaffolds tested)
- Separate verification calls increase computational overhead (5.6% vs 16.9-26.6% execute call rates, but only three agents compared)

**Low Confidence** (Evidence primarily from design rationale or limited samples):
- The framework generalizes to other formal systems beyond Lean 4/Mathlib (no empirical validation provided)
- Judge reliability holds across diverse mathematical domains (validation only on Mathlib tasks from single time window)

## Next Checks

1. **Judge Reliability Calibration**: Manually evaluate 20 randomly sampled agent solutions across all three semantic dimensions to establish confidence intervals for LLM judge accuracy in your specific domain context.

2. **Deduplication Effectiveness**: Analyze content overlap patterns in your target repository to verify that the logarithmic scaling assumption holds before investing in content-addressable infrastructure.

3. **Scaffold Interchangeability**: Test task contract execution across at least two different agent scaffolds on identical tasks to empirically validate that declarative specifications truly separate from execution strategies.