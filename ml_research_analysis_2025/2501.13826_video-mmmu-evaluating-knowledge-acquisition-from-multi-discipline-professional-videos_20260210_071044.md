---
ver: rpa2
title: 'Video-MMMU: Evaluating Knowledge Acquisition from Multi-Discipline Professional
  Videos'
arxiv_id: '2501.13826'
source_url: https://arxiv.org/abs/2501.13826
tags:
- video
- knowledge
- question
- adaptation
- videos
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Video-MMMU introduces a benchmark to evaluate how large multimodal
  models acquire knowledge from educational videos. It features 300 expert-level videos
  across six disciplines with 900 human-annotated questions aligned to three cognitive
  stages: Perception, Comprehension, and Adaptation.'
---

# Video-MMMU: Evaluating Knowledge Acquisition from Multi-Discipline Professional Videos

## Quick Facts
- **arXiv ID**: 2501.13826
- **Source URL**: https://arxiv.org/abs/2501.13826
- **Reference count**: 40
- **Primary result**: Large multimodal models struggle to learn from educational videos, with significant performance gaps compared to humans across all cognitive stages.

## Executive Summary
Video-MMMU introduces a benchmark to evaluate how large multimodal models acquire knowledge from educational videos. It features 300 expert-level videos across six disciplines with 900 human-annotated questions aligned to three cognitive stages: Perception, Comprehension, and Adaptation. A novel metric, Δknowledge, quantifies knowledge gains after video viewing. Results show models struggle to effectively learn from videos, with significant performance gaps compared to humans, particularly in applying acquired knowledge to novel scenarios. The findings highlight the need for improved methods to enhance models' video-based learning capabilities.

## Method Summary
Video-MMMU evaluates knowledge acquisition through a three-stage cognitive framework using 300 college-level educational videos across 6 disciplines and 30 subjects. The benchmark employs 900 human-annotated multiple-choice questions (3 per video, 10 options each) aligned to Perception, Comprehension, and Adaptation stages. Models are tested with and without video context to compute Δknowledge, which measures improvement on adaptation questions after video viewing. The evaluation uses LMMs-Eval framework with rule-based regex answer extraction, and frame sampling varies by model (32-64 frames for open-source, 20-50 for proprietary).

## Key Results
- Models show steep performance decline across cognitive stages, dropping 10-20% accuracy between perception, comprehension, and adaptation
- Average Δknowledge across 15 models is 13.5%, significantly lower than human performance (37.0%)
- High Right-to-Wrong rates offset learning gains, with models like InternVL2-8B showing 55.0% R→W vs 24.3% W→R

## Why This Works (Mechanism)

### Mechanism 1: Cognitive Stage Degradation
- Claim: Model performance systematically declines as cognitive demands increase from perception through comprehension to adaptation.
- Mechanism: Knowledge acquisition operates as a dependency chain—perception failures propagate to comprehension failures, which cascade to adaptation failures. Models exhibit ~10-20% accuracy drops between stages.
- Core assumption: The three stages represent sequential dependencies rather than independently testable capabilities.
- Evidence anchors: Claude: 72.0% perception → 69.67% comprehension → 55.67% adaptation; open-source models show 10-20% decline per stage.

### Mechanism 2: Knowledge Integration Failure (Right-to-Wrong Override)
- Claim: Models' low Δknowledge stems from overriding correct prior reasoning with new video information.
- Mechanism: Video exposure triggers knowledge updates that corrupt correct priors. Humans show 40.4% W→R vs 10.7% R→W; models like InternVL2-8B show 24.3% W→R vs 55.0% R→W.
- Core assumption: Right-to-Wrong transitions reflect flawed knowledge integration rather than evaluation noise.
- Evidence anchors: 8 models have R→W > W→R; all 15 tested models show R→W rates.

### Mechanism 3: Method Adaptation Gap
- Claim: The dominant failure mode (64% of adaptation errors) is correctly recalling video methods but incorrectly applying them to novel scenarios.
- Mechanism: Models extract surface-level method descriptions but lack compositional reasoning to adapt parameters or recognize structural differences.
- Core assumption: Method Adaptation errors are categorically distinct from comprehension failures.
- Evidence anchors: DFS case study shows model recalls interval containment principle but misapplies to cyclic graph; human succeeds.

## Foundational Learning

- **Concept**: Bloom's Taxonomy Cognitive Stages
  - Why needed here: The benchmark's three-track design maps to Bloom's framework, clarifying why adaptation is structurally harder.
  - Quick check question: Can you explain why "applying a formula with changed inputs" (comprehension) differs from "applying a method to a structurally different problem" (adaptation)?

- **Concept**: Δknowledge Normalization
  - Why needed here: The metric normalizes improvement by ceiling opportunity (100% - Acc_pre) to prevent inflation from easy baselines.
  - Quick check question: If Model A improves 0%→50% and Model B improves 50%→75%, which has higher Δknowledge and why?

- **Concept**: Wrong-to-Right vs Right-to-Wrong Analysis
  - Why needed here: This decomposition reveals whether low Δknowledge reflects learning failure or knowledge corruption.
  - Quick check question: A model with 30% W→R and 40% R→W: is the primary problem learning or retention?

## Architecture Onboarding

- **Component map**: Video Encoder (frames sampled: 10-64 depending on model) → Vision-Language Adapter → LLM Backbone → Text Output → Audio Transcript Pipeline (Whisper) → Text concatenation with visual tokens

- **Critical path**: Frame sampling determines visual coverage (32-64 frames avg); visual-audio fusion affects perception/comprehension; knowledge integration during adaptation requires maintaining priors while incorporating new information; method selection and application determines adaptation success

- **Design tradeoffs**: Frame count vs context length (more frames improve perception but may overwhelm adaptation); audio transcripts boost comprehension (+6-9%) but may hurt adaptation (-4-7%); model scale shows 72B outperforming 7-8B by 15-20 points on perception but adaptation gap narrows to ~10 points

- **Failure signatures**: Negative Δknowledge (LongVA -7.0%, InternVL2-8B -8.5%) indicates video degrades performance; high R→W rate (>40%) shows knowledge corruption dominates; Method Adaptation Error pattern shows correct principle recall but incorrect application to novel scenarios

- **First 3 experiments**:
  1. Baseline reproduction: Run Claude-3.5-Sonnet or GPT-4o on 30-video subset across all three tracks
  2. Frame ablation: Test whether increasing frames from 32→128 improves perception without degrading adaptation
  3. R→W mitigation: Implement "confidence threshold" that ignores video information when prior answer confidence >0.8

## Open Questions the Paper Calls Out
None

## Limitations
- Staged cognitive framework assumes sequential dependencies without testing independent stage performance
- Δknowledge metric doesn't account for varying baseline difficulty across tracks
- R→W vs W→R analysis doesn't distinguish between knowledge corruption and model calibration issues

## Confidence
- **High**: Models show systematic degradation across cognitive stages (consistent numerical patterns across all 15 tested models)
- **Medium**: Knowledge integration failure mechanism (compelling decomposition but alternative explanations not ruled out)
- **Low**: Method Adaptation Gap mechanism (64% error classification based on post-hoc analysis without inter-annotator validation)

## Next Checks
1. **Inter-annotator Reliability**: Re-annotate 50 adaptation questions with independent experts to verify Method Adaptation Error categorization
2. **Ablation of Cognitive Dependencies**: Design experiments testing whether models can succeed at adaptation without comprehension questions
3. **R→W Error Classification**: Categorize Right-to-Wrong errors by question difficulty and prior knowledge certainty to distinguish corruption from calibration issues