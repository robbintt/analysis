---
ver: rpa2
title: Evaluating Data Influence in Meta Learning
arxiv_id: '2501.15963'
source_url: https://arxiv.org/abs/2501.15963
tags:
- data
- influence
- learning
- dval
- meta
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a framework for evaluating data influence in
  meta learning using influence functions. The authors address the challenge of assessing
  data contributions in bilevel optimization settings where traditional influence
  functions fail due to the interdependence between meta-parameters and task-specific
  parameters.
---

# Evaluating Data Influence in Meta Learning

## Quick Facts
- arXiv ID: 2501.15963
- Source URL: https://arxiv.org/abs/2501.15963
- Reference count: 40
- Key outcome: Proposes framework for evaluating data influence in meta learning using influence functions with runtime improvements of 20-25% compared to retraining

## Executive Summary
This paper introduces a framework for evaluating data influence in meta learning using influence functions. The authors address the challenge of assessing data contributions in bilevel optimization settings where traditional influence functions fail due to the interdependence between meta-parameters and task-specific parameters. They propose task influence functions (task-IF) and instance influence functions (instance-IF) to evaluate the impact of specific tasks and data points respectively. The framework captures both direct and indirect effects of data through a two-stage estimation process. Experimental results show the method's effectiveness in identifying harmful data and evaluating data attribution across three datasets (omniglot, MNIST, MINI-Imagenet).

## Method Summary
The method extends classical influence functions to bilevel optimization problems by using total gradients and Hessians instead of partial derivatives. Task-IF computes the influence of entire tasks by capturing how task removal affects meta-parameters through both direct and indirect pathways. Instance-IF uses a two-stage process where first the effect on task-specific parameters is estimated, then the resulting change in outer loss is computed. The framework employs Neumann series for efficient Hessian-vector products and EK-FAC for inner-level Hessian approximations. The approach is evaluated within MAML-style meta-learning on three datasets.

## Key Results
- Runtime reduced from 316.74s (retrain) to 65.69s (task-IF) on Omniglot while maintaining accuracy (0.7804 vs 0.7908)
- Instance influence score (IS) identifies 100% of mislabeled tasks after checking ~40% of data (vs ~45% for random)
- Direct influence function is ~9× faster (7.46s vs 65.69s) but accuracy drops severely (0.5422 vs 0.7804)
- Effective in identifying harmful data and evaluating data attribution across multiple datasets

## Why This Works (Mechanism)

### Mechanism 1: Total Gradient and Hessian Capture Bilevel Dependencies
- Claim: Using total derivatives enables accurate influence estimation in bilevel optimization by capturing how meta-parameters and task-specific parameters co-vary
- Mechanism: The total gradient DλL = ∂λL + (dθ/dλ)·∂θL accounts for the dependency of θ on λ through the inner optimization. The total Hessian Hλ,Total incorporates second-order cross-terms including d²θ/dλ²
- Core assumption: The loss landscape around the optimum is sufficiently smooth for first and second-order Taylor approximations to hold
- Break condition: If the inner optimization has multiple local minima or the outer loss is highly non-convex, linear approximations degrade substantially

### Mechanism 2: Two-Stage Propagation for Training Instance Influence
- Claim: Training instances influence meta-parameters indirectly through task-specific parameters, requiring a cascaded estimation approach
- Mechanism: Stage 1 computes inner-IF(z) = -H^-1_in · ∇ℓ(z; θk) to estimate how removing instance z changes task-parameter θk. Stage 2 defines P(λ, θk; z) = -∇θLO · inner-IF to approximate the resulting change in outer loss
- Core assumption: The outer loss change from θk → θk⁻ can be linearized: LO(θk⁻) - LO(θk) ≈ ∇θLO · (θk⁻ - θk)
- Break condition: If task-specific parameters are highly sensitive to individual instance removal, first-order approximation error grows

### Mechanism 3: Neumann Series Avoids Explicit Hessian Storage
- Claim: The inverse Hessian-vector product H^-1·v can be computed iteratively without materializing or storing the full Hessian matrix
- Mechanism: Truncated Neumann expansion: H^-1·v ≈ v + (I-H)v + (I-H)²v + ... + (I-H)^J·v. Each iteration requires only a Hessian-vector product
- Core assumption: The Hessian spectrum allows reasonable convergence with modest truncation depth J
- Break condition: If Hessian eigenvalues cluster near 1, the Neumann series converges slowly or diverges

## Foundational Learning

- **Bilevel Optimization**
  - Why needed: Meta-learning is formulated as min_λ f(λ, θ*(λ)) s.t. θ*(λ) ∈ argmin_θ' g(λ, θ'). Understanding the hierarchy is essential to grasp why standard influence functions fail
  - Quick check question: Can you identify which variables appear in the outer objective vs. inner objective, and why θ*(λ) is written as a function of λ?

- **Influence Functions (Classical)**
  - Why needed: The base technique IF(z) = -H^-1·∇ℓ(z; θ̂) estimates parameter change from data removal. All extensions build on this
  - Quick check question: Why does the Hessian inverse appear, and what goes wrong if you try to apply this directly to a bilevel problem?

- **Total vs. Partial Derivatives**
  - Why needed: In nested optimization, ∂λL ≠ DλL because θ(λ) depends on λ. The chain rule term dθ/dλ is non-zero and must be computed via implicit differentiation
  - Quick check question: Given DλL = ∂λL + (dθ/dλ)·∂θL, when would the second term dominate?

## Architecture Onboarding

- **Component map:** MAML model training -> Total gradient computation (Algorithm 3) -> Inverse Hessian-vector product (Algorithm 4) -> Influence function calculation -> Model editing or data evaluation

- **Critical path:** 1) Train meta-learning model → obtain λ* and {θi(λ*)} 2) For each evaluation target, compute appropriate IF variant using Algorithms 1, 5, or 6 3) Use IF for model editing (λ* - IF) or compute Influence Score IS = Σ∇θ'ℓ(z; θ')·IF for ranking

- **Design tradeoffs:** Direct IF vs. Task-IF: Direct is ~9× faster (7.46s vs 65.69s) but accuracy drops severely (0.5422 vs 0.7804). Neumann truncation depth J: Higher J improves accuracy but adds linear compute cost. Hessian damping δ: Larger δ stabilizes inversion but may over-smooth influence estimates

- **Failure signatures:** IF values exploding or NaN: Hessian near-singular → increase damping δ. Task-IF accuracy far below retrain baseline: Total Hessian approximation too coarse → increase Neumann order or reduce Γ simplification. Memory overflow despite acceleration: EK-FAC layer-independence assumption violated for very deep or strongly coupled networks

- **First 3 experiments:** 1) Reproduce Table 1 task-IF results on Omniglot (target: accuracy ≥0.75, runtime <100s) to validate implementation 2) Ablate Neumann series depth J ∈ {5, 10, 20, 50} and plot accuracy vs. runtime tradeoff 3) Reproduce harmful task identification (Figure 1): corrupt 80% of task labels, verify IS-based removal reaches 100% mislabeled detection after checking ~40% of data (vs. ~45% for random)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the proposed bilevel influence function framework be adapted to other bilevel optimization domains, such as hyperparameter optimization or reinforcement learning, without significant structural modifications?
- Basis: The Introduction lists "hyperparameter optimization... and reinforcement learning" as key applications of Bilevel Optimization (BLO), but the theoretical derivation and experiments focus exclusively on meta-learning structures
- Why unresolved: The method relies on specific formulations of inner/outer loss and task-specific parameters unique to meta-learning; it is unclear if the "task-IF" logic holds when the inner variable is a hyperparameter or a policy rather than a task-specific weight
- What evidence would resolve it: Theoretical analysis applying task-IF to a different BLO problem (e.g., data cleaning for hyperparameter search) and empirical results showing correlation with retraining similar to the meta-learning results

### Open Question 2
- Question: To what extent does the proposed diagonal approximation of the total Hessian (replacing third-order derivatives with Γ) degrade the accuracy of influence estimates in highly non-convex settings?
- Basis: Theorem 5.1 introduces a heuristic approximation Γ = Σ‖∂θL_O‖₁ · I to replace complex third-order derivative terms to ensure computational tractability, but provides no theoretical error bounds for this simplification
- Why unresolved: While the approximation makes the calculation feasible, discarding the cross-terms and second-order dependencies in d²θ/dλ² could lead to significant estimation errors in complex loss landscapes where these terms are non-negligible
- What evidence would resolve it: A sensitivity analysis comparing the proposed approximate Total Hessian against an exact calculation (on a small network) to quantify the error introduced by the Γ approximation

### Open Question 3
- Question: Does error propagation in the two-stage "instance-IF" estimation process (inner-to-outer) compound inaccuracies compared to single-stage influence methods?
- Basis: Section 4.2 describes the "Training Data Influence" estimation as a two-stage process: first estimating the impact on θk (inner-IF), and then using that result to estimate the impact on λ
- Why unresolved: The framework uses the output of the first estimation (θ^-_k) as input for the second, implying that any approximation errors or linearization inaccuracies in the inner loop are carried forward and potentially magnified in the outer loop calculation
- What evidence would resolve it: Empirical measurement of the "drift" between the two-stage instance-IF prediction and the actual retrained model parameters as the network depth or task complexity increases

## Limitations

- The third-order term approximation (Γ = Σ‖∂θL_O‖₁ · I) in Theorem 5.1 is unverified and could introduce systematic bias in Hessian estimation
- EK-FAC assumptions about layer-wise independence may not hold for deep architectures with cross-layer parameter dependencies
- Framework validity hinges on smoothness assumptions in bilevel optimization landscapes that may not hold in practice

## Confidence

- **High**: Task-IF runtime improvements (20-25%) and basic mechanism of total gradient/Hessian computation
- **Medium**: Instance-IF two-stage estimation framework and indirect effect modeling
- **Low**: Third-order Hessian approximation (Γ simplification) and generalizability across diverse meta-learning architectures

## Next Checks

1. Perform ablation study on Neumann series truncation depth J to quantify approximation error in iHVP computation
2. Test framework on non-MAML meta-learning algorithms (e.g., MetaOptNet) to assess architecture dependence
3. Validate third-order approximation Γ by comparing full third-order term computation vs. simplified form on small-scale problems