---
ver: rpa2
title: 'TRAVELER: A Benchmark for Evaluating Temporal Reasoning across Vague, Implicit
  and Explicit References'
arxiv_id: '2505.01325'
source_url: https://arxiv.org/abs/2505.01325
tags:
- temporal
- event
- question
- time
- events
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces TRAVELER, a synthetic benchmark for evaluating
  temporal reasoning in language models across three types of temporal references:
  explicit, implicit (relative to speech time), and vague. The benchmark includes
  3,300 questions over household event sets of varying lengths (5-100 events).'
---

# TRAVELER: A Benchmark for Evaluating Temporal Reasoning across Vague, Implicit and Explicit References

## Quick Facts
- arXiv ID: 2505.01325
- Source URL: https://arxiv.org/abs/2505.01325
- Reference count: 40
- Primary result: TRAVELER reveals LLMs struggle with event-based temporal reasoning, especially for vague and implicit references, with performance declining sharply as event set length increases.

## Executive Summary
This paper introduces TRAVELER, a synthetic benchmark for evaluating temporal reasoning in language models across explicit, implicit, and vague temporal references. The benchmark includes 3,300 questions over household event sets of varying lengths (5-100 events). Human surveys establish ground truth for vague temporal expressions. Four state-of-the-art LLMs were evaluated, revealing that performance declines significantly with increasing event set length and decreasing temporal explicitness. Chain-of-thought prompting improved results, and larger models (Llama3-70B, GPT-4) achieved the highest accuracy. The findings highlight challenges LLMs face in handling event-based temporal reasoning, especially with ambiguous or extended contexts.

## Method Summary
TRAVELER evaluates temporal reasoning by generating synthetic household event sets and asking questions with explicit (dates), implicit (relative to speech time), or vague (adverbials like "recently") temporal references. Events are tuples with type, subject, location, and timestamp. Questions are generated using templates and evaluated with binary accuracy for explicit/implicit types and probabilistic accuracy for vague types based on human survey distributions. The benchmark tests models across 11 event set lengths and three temporal reference categories using zero-shot and chain-of-thought prompting strategies.

## Key Results
- Larger models (Llama3-70B, GPT-4) achieved highest accuracy (83%-90%) across all temporal reference types
- Accuracy declines sharply as event set length increases from 5 to 100 events
- Vague temporal references show lowest performance (26%-45%) across all models
- Chain-of-thought prompting consistently improves accuracy compared to zero-shot baseline
- Llama3-70B slightly outperforms GPT-4 on explicit and implicit questions despite having far fewer parameters

## Why This Works (Mechanism)

### Mechanism 1: Chain-of-Thought Prompting Reduces Multi-Event Tracking Errors
Chain-of-thought prompting improves temporal reasoning accuracy by enforcing sequential event review rather than holistic pattern matching. By instructing models to "review each event in the event set sequentially" and record matches, CoT forces explicit intermediate computation steps, reducing the likelihood that relevant events are overlooked when processing longer event sets.

### Mechanism 2: Model Scale Enables Better Implicit Reference Resolution
Larger parameter counts correlate with improved accuracy on implicit and vague temporal references, likely due to richer learned temporal associations. Larger models encode more extensive patterns of temporal language use during pretraining, enabling better generalization when explicit cues are absent and inference must rely on context.

### Mechanism 3: Probabilistic Ground Truth Captures Human Variation in Vague Interpretation
Vague temporal references cannot have binary ground truth because human interpretations vary; survey-based probability distributions provide a more appropriate evaluation framework. By collecting Likert-scale ratings from 50 native speakers per event type and computing median probabilities, the benchmark captures the gradient applicability of expressions like "recently" or "long time ago" to events at different time distances.

## Foundational Learning

- Concept: **Temporal Reference Types (Explicit, Implicit, Vague)**
  - Why needed here: The benchmark's core design differentiates these three categories, and understanding their resolution requirements is essential for interpreting results.
  - Quick check question: Given the question "Did Tom wash a mug yesterday?", what additional information is needed to resolve the temporal reference compared to "on 2023-08-16"?

- Concept: **Event-Set Reasoning vs. Single-Event Lookup**
  - Why needed here: Performance degrades with longer event sets, indicating models struggle with multi-event tracking rather than isolated temporal resolution.
  - Quick check question: Why does accuracy drop more sharply for implicit references than explicit references as event-set length increases from 5 to 100?

- Concept: **Probabilistic Evaluation for Subjective Categories**
  - Why needed here: Vague expressions have no binary truth; understanding how probability-weighted accuracy is computed is critical for replicating or extending the benchmark.
  - Quick check question: If an LLM answers "yes" to "Did Tom eat risotto a long time ago?" and one matching event occurred 2 hours ago (p=0) while another occurred 1 week ago (p=0.75), what is the accuracy of this response?

## Architecture Onboarding

- Component map: Event Generator -> Question Generator -> Ground Truth Module -> Evaluation Pipeline
- Critical path:
  1. Generate event set of specified length (5-100 events)
  2. Select question template and temporal expression based on target category
  3. Compute ground truth (match filter for explicit/implicit; probability lookup for vague)
  4. Format prompt with event set and question using chosen strategy (zero-shot or CoT)
  5. Query model at temperature 0 and compare response to ground truth
- Design tradeoffs: Synthetic events enable controlled variation but may not reflect real-world complexity; household domain limits vocabulary but improves interpretability; fixed reference time ensures consistency but prevents evaluation of dynamic "now" references.
- Failure signatures: Sharp accuracy drop from 5 to 50 events indicates lost context or attention degradation; low "Who...?" and "How often...?" accuracy suggests enumeration/counting failures even when matching events exist; vague category near-chance performance (26%-45%) indicates models lack calibrated uncertainty or default to binary responses.
- First 3 experiments:
  1. Replicate the CoT Review prompt on a 20-event set with explicit questions to establish baseline and verify pipeline correctness
  2. Compare zero-shot vs. CoT accuracy on implicit questions across event lengths 10, 30, 50 to quantify prompt-engineering gains
  3. Evaluate a single vague adverbial (e.g., "recently") across event types to validate probabilistic scoring matches survey distributions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does LLM performance vary when extending the benchmark to include question categories requiring common sense knowledge, referential relative to an arbitrary time point, or personal knowledge?
- Basis in paper: The Conclusion states, "the benchmark should be extended to include the other question categories (Questions requiring common sense knowledge, Referential relative to an arbitrary time point, Personal Knowledge) and the performance of LLMs for them should be evaluated."
- Why unresolved: The current TRAVELER benchmark is restricted to explicit, implicit relative to speech time, and vague references; the other categories defined in Section 2.1 were excluded from the dataset generation and experimentation.
- What evidence would resolve it: Accuracy scores for LLMs tested on a modified version of the TRAVELER dataset containing questions grounded in common sense or personal knowledge bases.

### Open Question 2
- Question: What specific model characteristics or training regimens enable Llama3-70B to outperform GPT-4 (1760B parameters) on explicit and implicit relative to speech time tasks?
- Basis in paper: The Discussion section notes, "Additional research is needed to elucidate what specific model characteristics or training regimens make Llama3-70B so good at event-based temporal reasoning."
- Why unresolved: The paper identifies the performance anomaly where a smaller model (70B) surpasses a larger one (1760B), but the experimental setup did not isolate the architectural or data-related causes (e.g., training data composition, fine-tuning techniques).
- What evidence would resolve it: An ablation study or detailed analysis comparing the training data mixtures and architectural inductive biases of Llama3-70B versus GPT-4 specifically on temporal reasoning benchmarks.

### Open Question 3
- Question: Can integrating explicit memory components, such as temporal graphs constructed from historical data, mitigate the performance degradation observed in LLMs processing long event sets?
- Basis in paper: The Conclusion suggests, "One possibility is to enhance LLMs with an explicit memory component... Approaches like that of Xiong et al., where they construct a temporal graph... offer a promising avenue."
- Why unresolved: The paper demonstrates that performance drops as event set length increases (from 5 to 100) because LLMs lack explicit memory, but it does not test external memory solutions.
- What evidence would resolve it: Comparative benchmarks on TRAVELER showing accuracy retention across long event sets (e.g., 100 events) when models are augmented with retrieval mechanisms or graph-based memory.

### Open Question 4
- Question: Can automated prompt optimization tools like DSPy systematically generate prompts that yield higher accuracy than the manually engineered Chain-of-Thought strategies used in the study?
- Basis in paper: Section 6 states, "instead of manually performing prompt engineering like we have done, using automated tools like DSPy is another fruitful research direction."
- Why unresolved: The study relied on manual prompt engineering (Zero-Shot, CoT Review, CoT Step-by-Step), leaving the potential upper bound of automated prompt refinement unexplored.
- What evidence would resolve it: Evaluation results on the TRAVELER dataset using DSPy-optimized prompts compared against the manual "CoT Review" baseline.

## Limitations

- Synthetic benchmark may not reflect real-world temporal reasoning complexity and is limited to household domain
- Performance drops sharply with event set length, but the exact failure modes (attention degradation, memory limitations, enumeration errors) are not clearly distinguished
- Probabilistic evaluation for vague expressions relies on human surveys, but methodology and distributions are not fully specified in the paper

## Confidence

- **High Confidence**: Larger models achieve higher accuracy across all temporal reference types; accuracy declines with event set length and temporal vagueness
- **Medium Confidence**: CoT prompting improves accuracy, but the mechanism is not conclusively established; Llama3-70B's relative performance versus GPT-4 is surprising and warrants further investigation
- **Low Confidence**: Interpretation of vague expression probabilities as capturing true human judgment variation is plausible but not validated against diverse populations or contexts

## Next Checks

1. Replicate the human survey distributions for vague adverbials across event types and time deltas used in the benchmark to ensure accurate probabilistic scoring

2. Diagnose failure modes by event-set length through targeted experiments to distinguish whether accuracy drops are due to attention window limits, memory overload, or enumeration failures

3. Test "now" as a dynamic reference by extending the benchmark to include questions with "now" as the reference time, varying the query time relative to the event set's reference time