---
ver: rpa2
title: Uncovering the Persuasive Fingerprint of LLMs in Jailbreaking Attacks
arxiv_id: '2510.21983'
source_url: https://arxiv.org/abs/2510.21983
tags:
- persuasive
- llms
- prompts
- jailbreak
- harmful
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates how persuasive principles can be leveraged\
  \ to jailbreak aligned LLMs. By systematically rewriting harmful queries using Cialdini\u2019\
  s seven persuasion principles (e.g., Scarcity, Social Proof), the authors generate\
  \ adversarial prompts that significantly increase harmful output generation."
---

# Uncovering the Persuasive Fingerprint of LLMs in Jailbreaking Attacks

## Quick Facts
- arXiv ID: 2510.21983
- Source URL: https://arxiv.org/abs/2510.21983
- Reference count: 21
- One-line primary result: Persuasive prompt rewriting using Cialdini's principles achieved up to 97% ASR improvement across six LLMs

## Executive Summary
This study systematically investigates how persuasion principles can be weaponized to jailbreak aligned LLMs. By rewriting harmful queries using Cialdini's seven persuasion principles, the authors demonstrate that psychologically structured prompts significantly increase harmful output generation compared to original queries. The method achieved up to 97% improvement in attack success rate across six LLMs while maintaining human-readable outputs that evade perplexity-based detection. Different models exhibited distinct "persuasive fingerprints"—varying susceptibility to specific principles like Scarcity or Unity—suggesting model-specific forensic profiling opportunities for jailbreak detection.

## Method Summary
The approach uses WizardLM-Uncensored to rewrite each harmful query from the AdvBench dataset into seven variants, each structured around one of Cialdini's persuasion principles (Authority, Reciprocity, Commitment, Social Proof, Liking, Scarcity, Unity). These variants are then tested against six target LLMs (Vicuna, Llama2, Llama3, Gemma3, DeepSeek-R1, Phi-4) running locally via Ollama. Attack success is determined by keyword-based refusal detection, with informative scores measuring harm severity. The method requires no model training—it operates as black-box attacks only.

## Key Results
- Persuasive prompts achieved up to 97% improvement in attack success rate compared to original harmful queries
- Vicuna was most influenced by Scarcity principle (+59%), while DeepSeek favored Unity principle (+71%)
- Generated prompts had low perplexity (23.62) making them stealthier than gradient-based attacks (PPL=15,895)
- Reciprocity principle consistently underperformed across all models
- Well-aligned models (Llama2, Phi4) showed <30% ASR even with persuasion

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Persuasive structures in prompts increase LLM compliance with harmful requests
- Mechanism: LLMs trained on large-scale human-generated text have learned to respond more compliantly to linguistically structured persuasive patterns (Authority, Scarcity, Social Proof, etc.) that mirror human influence techniques
- Core assumption: Models internalize persuasion-response patterns from training data where humans comply with persuasive requests
- Evidence anchors: "we hypothesize that LLMs, having been trained on large-scale human-generated text, may respond more compliantly to prompts with persuasive structures" [abstract]
- Break condition: If target LLM's training data was heavily filtered for persuasive manipulation patterns, effectiveness would diminish

### Mechanism 2
- Claim: Human-readable persuasive prompts evade perplexity-based detection
- Mechanism: Unlike gradient-based adversarial suffixes (GCG: PPL=15,895), persuasion-based rewrites produce semantically natural text (PPL=23.62) that perplexity filters classify as benign
- Core assumption: Perplexity-based defenses flag statistical anomalies but not semantically coherent persuasive manipulation
- Evidence anchors: "our approach generates jailbreak prompts with low PPL (23.62), indicating that the outputs are more human-readable and fluent" [section 3.4]
- Break condition: If defenders deploy semantic classifiers trained specifically on persuasion principle patterns, stealth advantage degrades

### Mechanism 3
- Claim: Different LLMs exhibit distinct "persuasive fingerprints"—varying susceptibility to specific principles
- Mechanism: Alignment training, architecture, and data composition create model-specific vulnerabilities (e.g., DeepSeek favors Unity +71%; Vicuna favors Scarcity +59%)
- Core assumption: Alignment processes inadvertently create uneven robustness across persuasive dimensions
- Evidence anchors: "DeepSeek uniquely ranks Unity as most influential... Gemma and Phi4 prioritize Authority" [section 3.3]
- Break condition: If alignment training explicitly counter-examples across all seven principles, fingerprint variance would reduce

## Foundational Learning

- Concept: Cialdini's Seven Persuasion Principles (Authority, Reciprocity, Commitment, Social Proof, Liking, Scarcity, Unity)
  - Why needed here: Understanding these principles is essential for interpreting attack vectors and designing targeted defenses
  - Quick check question: Can you distinguish why "Everyone is using this exploit" (Social Proof) differs from "Only 3 spots remain" (Scarcity) in mechanism?

- Concept: Perplexity as Stealth Metric
  - Why needed here: Evaluating jailbreak stealth requires understanding how perplexity scores (lower = more natural) indicate detectability against statistical defenses
  - Quick check question: Why would a PPL of 23.62 evade detection while PPL of 15,895 would not?

- Concept: Attack Success Rate (ASR) vs. Informative Score
  - Why needed here: Binary ASR misses harm severity; informative scores (0-1) capture how much harmful information was actually conveyed
  - Quick check question: If ASR is 70% but average informative score is 0.1, what does this suggest about attack quality?

## Architecture Onboarding

- Component map: Harmful Query Input (AdvBench: 520 queries) → Persuasive Rewrite Engine (WizardLM-Uncensored) → 7 Persuasion Variants per Query → Target LLM (6 models tested) → Response Evaluation (ASR + Informative Score + PPL)

- Critical path:
  1. Rewrite query using each persuasion principle (7 variants)
  2. Query target model with all variants
  3. Success = any single variant bypasses refusal
  4. Calculate Influential Power per principle per model

- Design tradeoffs:
  - Human-readable outputs (low PPL) vs. maximizing ASR (PAIR achieves higher ASR but requires more queries)
  - Single rewrite model (WizardLM-Uncensored) vs. diverse generators (limitation noted by authors)
  - Black-box only access vs. gradient-based optimization (trades transferability for efficiency)

- Failure signatures:
  - Reciprocity principle consistently underperforms across all models (lowest Influential Power)
  - Well-aligned models (Llama2, Phi4) show <30% ASR even with persuasion
  - PPL-based detection fails; semantic pattern detection not tested

- First 3 experiments:
  1. Baseline replication: Run original AdvBench queries against your target model to establish refusal baseline (expected: <25% ASR for aligned models)
  2. Single-principle ablation: Test each persuasion principle independently (7 conditions × 50 queries minimum) to identify your model's fingerprint before full deployment
  3. Transfer test: Generate persuasive variants on one model (e.g., Vicuna), test on another (e.g., Llama3) to quantify cross-model transferability before claiming generalization

## Open Questions the Paper Calls Out

- Question: Do persuasion-based jailbreaks maintain high attack success rates across diverse harm benchmarks beyond AdvBench?
  - Basis in paper: The authors identify the use of a "single jailbreak dataset" as a limitation and suggest that expanding evaluation "would improve the generalizability."
  - Why unresolved: The current study restricts validation to the AdvBench dataset, leaving performance on broader or more recent attack scenarios untested
  - What evidence would resolve it: Evaluating the method on additional datasets such as StrongReject or HarmfulQA to verify consistent ASR improvements

- Question: To what extent do the observed persuasive fingerprints depend on the specific prompt generator (WizardLM-Uncensored) used for rewriting?
  - Basis in paper: The paper notes the dependency on a specific prompt generator and proposes "explore using alternative LLMs or rewriting strategies" for future work
  - Why unresolved: It remains unclear if the distinct susceptibility profiles (e.g., Vicuna favoring Scarcity) are intrinsic to the target models or artifacts of the rewriter's style
  - What evidence would resolve it: Replicating the experiments using alternative rewriting agents (e.g., GPT-4) and comparing the resulting influential power rankings

- Question: What specific training data or alignment mechanisms cause different LLMs to exhibit unique susceptibility profiles to persuasive principles?
  - Basis in paper: The paper identifies distinct "persuasive fingerprints" (e.g., DeepSeek favors Unity) but relies on a behavioral black-box analysis without explaining the causal internal mechanisms
  - Why unresolved: The correlation between model type and susceptibility is established, but the root cause—whether due to training corpus bias or alignment techniques—is not isolated
  - What evidence would resolve it: Ablation studies analyzing internal activations or controlled training experiments to correlate specific data patterns with susceptibility to specific principles

## Limitations

- Single dataset evaluation using only AdvBench queries limits generalizability to other harmful domains
- Dependency on WizardLM-Uncensored as the sole prompt generator may introduce bias in persuasive rewrites
- No testing of semantic classifiers specifically designed to detect persuasion principle patterns

## Confidence

- **High confidence**: The mechanism that persuasive prompts evade perplexity-based detection (PPL=23.62 vs GCG's 15,895) is well-supported by the data. The observation that different models show distinct susceptibility patterns is also robust, given the consistent differences across six models.
- **Medium confidence**: The claim that models learn compliance from persuasive patterns in training data is plausible but not directly tested. The study shows correlation (persuasive prompts work) but doesn't prove the underlying mechanism (models learned this from human text patterns).
- **Low confidence**: Cross-model transferability claims are weak. While the paper notes different fingerprints, it doesn't systematically test whether persuasive variants effective on one model transfer to others, which would be critical for practical attack deployment.

## Next Checks

1. **Dataset generalization test**: Run the same persuasion principle analysis on a completely different jailbreak dataset (e.g., HarmBench or manually crafted prompts) to verify the 97% improvement claim holds beyond AdvBench.

2. **Rewrite engine ablation**: Repeat experiments using a different uncensored model (e.g., GPT-4o-mini or Claude) for prompt generation to determine if WizardLM-Uncensored is introducing bias or if the effect is robust across generators.

3. **Semantic defense evaluation**: Test whether a classifier trained specifically on identifying Cialdini principle patterns can detect persuasive jailbreaks, directly measuring whether the "stealth" advantage degrades against purpose-built semantic defenses.