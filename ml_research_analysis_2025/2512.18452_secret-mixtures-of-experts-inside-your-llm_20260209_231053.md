---
ver: rpa2
title: Secret mixtures of experts inside your LLM
arxiv_id: '2512.18452'
source_url: https://arxiv.org/abs/2512.18452
tags:
- experts
- student
- layer
- variance
- active
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the internal structure of Multilayer Perceptron
  (MLP) layers in transformer models, which are notoriously difficult to interpret
  due to their dense computation and lack of visualizability. The key hypothesis is
  that these MLP layers can be effectively approximated by sparsely-activating Mixture
  of Experts (MoE) models, meaning they compute sparse functions despite their dense
  appearance.
---

# Secret mixtures of experts inside your LLM

## Quick Facts
- arXiv ID: 2512.18452
- Source URL: https://arxiv.org/abs/2512.18452
- Authors: Enric Boix-Adsera
- Reference count: 8
- One-line primary result: MLP layers in transformers can be well-approximated by sparsely-activating MoEs when input activations have dictionary-sparse structure.

## Executive Summary
This paper investigates whether dense Multilayer Perceptron (MLP) layers in transformer models can be approximated by sparsely-activating Mixture of Experts (MoE) models. The core hypothesis is that MLPs compute sparse functions despite their dense appearance, enabled by dictionary-sparse structure in neural activations. The authors establish theoretical connections between MoE models and Sparse Autoencoder (SAE) structure, proving that MLPs are approximable by MoEs under dictionary-sparse conditions but not under Gaussian inputs. Empirical validation through knowledge distillation confirms that MoE students can achieve comparable performance to dense MLP students while using significantly fewer active parameters - up to 8 times fewer - specifically when the input distribution has dictionary-sparse structure.

## Method Summary
The authors use knowledge distillation to test whether MLP layers can be approximated by MoE students. They extract activations from pretrained models (Pythia-410M, Gemma-270M, Pythia-70M) by running Wikitext-103 through layers preceding the target MLP, creating training data D_act. They also generate Gaussian control data D_gauss with matched mean and covariance. MoE student models use a shared expert plus sparse experts with low-rank routers (R = R₁R₂), trained via MSE distillation with Adam for 100 epochs. The key comparison is variance unexplained (1 - R²) versus active parameters, testing whether MoEs achieve similar performance with fewer active neurons than dense MLP students, and whether this advantage disappears for Gaussian data.

## Key Results
- MoE students achieve comparable performance to dense MLP students while using 2-8× fewer active parameters on dictionary-sparse activation data
- The MoE advantage disappears completely when distilling on Gaussian data with matched mean/covariance, validating the importance of activation distribution structure
- Low-rank router reparameterization (R = R₁R₂) surprisingly improves distillation performance despite being less expressive
- MoEs with 128 shared + 128 MoE experts show optimal balance compared to other configurations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MLP layers in trained transformers can be approximated by sparsely-activating MoEs when input activations have dictionary-sparse structure.
- Mechanism: Neural activations are approximately sparse in a learned dictionary (from SAE research). Under this condition, each dictionary element can map to an expert that handles its subspace, enabling sparse routing with O(k) active parameters instead of O(d) dense parameters.
- Core assumption: Activations are (m, k)-dictionary-sparse with approximately orthogonal dictionary vectors.
- Evidence anchors:
  - [abstract] "Our hypothesis is based on a novel theoretical connection between MoE models and Sparse Autoencoder (SAE) structure in activation space."
  - [Section 3.2] Theorem 3.6 proves linear functions are approximable under dictionary-sparsity with γ·‖A‖_op error.
  - [corpus] Related work on SAEs (BTB+23, Gao et al.) supports dictionary-sparsity in activations, but corpus does not directly validate the MoE-SAE connection.
- Break condition: If activations are not dictionary-sparse (e.g., Gaussian), the approximation guarantee fails.

### Mechanism 2
- Claim: Under Gaussian input distributions, sparse MoEs cannot approximate even simple functions like identity.
- Mechanism: Sparse MoEs partition input space into regions with k active experts each. With kd_exp < d/2 active neurons, the MoE projects inputs to a lower-dimensional subspace per region. Gaussian distributions place significant mass across all dimensions, so information loss is unavoidable.
- Core assumption: Input distribution is isotropic Gaussian N(0, I_d/d).
- Evidence anchors:
  - [Section 3.1] Theorem 3.3 proves no (m,k)-MoE with hard gating can c'-approximate identity under Gaussian inputs.
  - [Section 4, Figure 2] Empirical distillation shows MoE advantage disappears for Gaussian data with matched mean/covariance.
  - [corpus] No direct corpus support; this is a novel theoretical contribution.
- Break condition: Dictionary-sparse structure in the input distribution.

### Mechanism 3
- Claim: Low-rank router reparameterization (R = R₁R₂) improves MoE distillation performance despite being strictly less expressive.
- Mechanism: Assumption: The true routing function for dictionary-sparse data lies on a low-dimensional manifold. The Burer-Monteiro factorization provides implicit regularization and smoother optimization landscape.
- Core assumption: Optimal routing matrix has low effective rank for activation distributions.
- Evidence anchors:
  - [Section 4] "Surprisingly, we find that although this reparametrization makes the linear router strictly less expressive, it actually significantly improves performance."
  - [Appendix A.2, Figure 6] Ablation shows low-rank routing outperforms full-rank on activation data but not Gaussian data.
  - [corpus] No direct corpus support for low-rank routing specifically in MoEs.
- Break condition: If routing inherently requires high-rank structure, this reparameterization would hurt.

## Foundational Learning

- Concept: Mixture of Experts (MoE) with top-k gating
  - Why needed here: Core architecture being analyzed; understanding sparse activation is essential.
  - Quick check question: Given m=1000 experts and k=8, how many expert forward passes occur per input?

- Concept: Dictionary learning / Sparse Autoencoders (SAE)
  - Why needed here: Provides the theoretical basis for why activations enable sparse MoE approximation.
  - Quick check question: If activations x can be written as x ≈ D·z with ‖z‖₀ ≤ k, what does this imply about input structure?

- Concept: Knowledge distillation (student-teacher setup)
  - Why needed here: Empirical validation method; distilling MLP→MoE tests the hypothesis.
  - Quick check question: Why use MSE loss between teacher and student outputs rather than matching final task loss?

## Architecture Onboarding

- Component map: Wikitext-103 -> Pretrained transformer (layers 1 to ℓ-1) -> Extract activations at layer ℓ -> Teacher MLP -> Student MoE (shared + sparse experts + low-rank router) -> MSE loss -> Adam optimizer

- Critical path:
  1. Generate D_act by pushing Wikitext through layers preceding target MLP
  2. Generate D_gauss control with matched mean/covariance
  3. Train student MoE via MSE distillation with Adam, 100 epochs, batch size 1024
  4. Compare variance explained vs. active parameters for MoE vs. MLP students

- Design tradeoffs:
  - Shared expert size vs. number of active MoE experts (Table 1 shows 128+128 balance is optimal)
  - Expert dimension d_exp=1 gives best efficiency; larger may help expressiveness
  - More total experts (10k vs 100k) helps at fixed active count

- Failure signatures:
  - MoE shows no advantage over MLP on Gaussian data → input distribution lacks dictionary structure
  - Training instability with MoE → add shared expert, reduce learning rate
  - Full-rank router outperforms low-rank → routing may need higher rank for this data

- First 3 experiments:
  1. Replicate Pythia-70M layer 3 distillation: compare MoE vs. MLP student at 16-256 active neurons on D_act vs. D_gauss.
  2. Ablate shared expert: test 0 shared + 256 MoE vs. 128 shared + 128 MoE vs. 256 shared + 0 MoE.
  3. Test low-rank routing: compare d_proj ∈ {32, 128, 256, full-rank} on activation data.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does the Burer-Monteiro low-rank reparametrization (R = R₁R₂) make linear routers easier to train, despite being strictly less expressive than full-rank routing?
- Basis in paper: [explicit] Section 4, page 8: "Understanding why the Burer-Monteiro reparametrization R = R1R2 makes linear routers easier to train is an interesting question for future study."
- Why unresolved: The paper observes this phenomenon empirically (Appendix A.2) but does not provide theoretical explanation.
- What evidence would resolve it: Theoretical analysis of the optimization landscape for low-rank vs. full-rank routers, or empirical studies isolating the mechanisms involved.

### Open Question 2
- Question: Can MoEs with low-rank routers improve performance in frontier-scale models, particularly in the many-expert regime?
- Basis in paper: [explicit] Section 5.2, page 9: "Further testing this proposed architecture is a promising direction... The many-expert regime is increasingly important, as open-source frontier models scale the number of experts."
- Why unresolved: The distillation experiments are limited to single layers of smaller pretrained models (up to 410M parameters), not full pretraining at scale.
- What evidence would resolve it: Pretraining full transformer models with low-rank router MoEs and comparing to standard MoE architectures on downstream benchmarks.

### Open Question 3
- Question: How can the bidirectional connection between SAE design and MoE router architectures be exploited to improve both?
- Basis in paper: [explicit] Section 6, page 9: "This paper highlights a structural connection between SAEs and MoE models, suggesting a bidirectional opportunity: advances in SAE design may inform better MoE router architectures, and conversely, insights from MoE routing may guide the development of more expressive or efficient SAEs."
- Why unresolved: The paper establishes the theoretical connection but does not implement or test specific cross-domain designs.
- What evidence would resolve it: Experiments transferring architectural innovations (e.g., JumpReLU, gated activations, top-k) between SAEs and MoE routers.

### Open Question 4
- Question: What is the optimal rank for low-rank routers, and how does it depend on model scale or number of experts?
- Basis in paper: [inferred] Appendix A.2 notes: "Since we did not tune the rank of the routing, this indicates that our distillation fraction variance explained by MoEs may be possibly further improved."
- Why unresolved: The paper uses fixed ranks (128 or 256) without systematic tuning or theoretical guidance.
- What evidence would resolve it: Systematic sweep over router ranks across different model sizes and expert counts during distillation.

## Limitations

- The theoretical framework relies on dictionary-sparse assumptions about activations that are not fully characterized or validated mathematically
- Empirical validation is limited to small models (up to 410M parameters) and next-token prediction on Wikitext-103
- The Gaussian control design may not capture all aspects of non-dictionary-sparse distributions that would invalidate the MoE approximation hypothesis

## Confidence

**High confidence**: The experimental demonstration that MoE students achieve comparable performance to dense MLP students with fewer active parameters on activation data, while failing to show this advantage on Gaussian data. This provides strong empirical support for the core hypothesis.

**Medium confidence**: The theoretical connection between dictionary-sparsity and MoE approximability. While the proofs are mathematically sound, they depend on assumptions about activation distributions that are not fully characterized.

**Low confidence**: The claim that low-rank routing (R = R₁R₂) improves MoE distillation despite being less expressive. This counterintuitive finding is presented without a complete theoretical explanation for why the reparameterization helps.

## Next Checks

1. **Test on larger models**: Validate the MoE approximation hypothesis on Llama-7B or Mistral-7B by distilling their MLP layers to sparse MoE students, comparing performance and parameter efficiency across different model scales.

2. **Cross-task generalization**: Evaluate whether the MoE advantage persists when distilling from models trained on diverse tasks (code generation, reasoning, multilingual text) rather than just language modeling.

3. **SAE activation analysis**: Conduct a systematic study of SAE activation sparsity patterns across different layers and models, quantifying how well these activations satisfy the dictionary-sparse conditions required by the theoretical framework.