---
ver: rpa2
title: Multi-level Certified Defense Against Poisoning Attacks in Offline Reinforcement
  Learning
arxiv_id: '2505.20621'
source_url: https://arxiv.org/abs/2505.20621
tags:
- robustness
- poisoning
- arxiv
- learning
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work addresses the vulnerability of offline reinforcement
  learning to data poisoning attacks, which can compromise both per-state action stability
  and overall policy performance. The proposed approach, Multi-level Certified Defense
  (MuCD), leverages differential privacy mechanisms to provide robustness guarantees
  at two levels: action-level robustness ensuring stable state-specific actions and
  policy-level robustness providing lower bounds on expected cumulative reward.'
---

# Multi-level Certified Defense Against Poisoning Attacks in Offline Reinforcement Learning

## Quick Facts
- **arXiv ID**: 2505.20621
- **Source URL**: https://arxiv.org/abs/2505.20621
- **Reference count**: 40
- **Primary result**: Certified defense framework provides policy performance drop ≤50% with 7% poisoned data vs 0.008% in prior work

## Executive Summary
This work addresses the vulnerability of offline reinforcement learning to data poisoning attacks that can compromise both per-state action stability and overall policy performance. The proposed Multi-level Certified Defense (MuCD) leverages differential privacy mechanisms to provide robustness guarantees at two levels: action-level robustness ensuring stable state-specific actions and policy-level robustness providing lower bounds on expected cumulative reward. The framework is applicable to both discrete and continuous action spaces, as well as deterministic and stochastic environments.

The method employs randomized training with DP mechanisms (SGM for transitions, DP-FEDAVG for trajectories) and derives theoretical bounds through outcomes guarantees. Experimental results demonstrate significant improvements over prior work, with certified radii 5 times larger than existing approaches and policy performance drops limited to 50% even with 7% of training data poisoned—a substantial improvement over the 0.008% in previous methods.

## Method Summary
The method trains multiple policy instances using DP-randomized training mechanisms—Sampled Gaussian Mechanism for transition-level poisoning and DP-FEDAVG for trajectory-level poisoning. Using the Opacus framework, the approach generates p=50 independent policy instances for each environment. Certification is performed through two levels: policy-level certification estimates cumulative reward bounds using empirical CDF and DKW inequality, while action-level certification computes per-state robustness radii by checking inferred action scores across instances. The framework handles both discrete and continuous action spaces, with discrete actions using majority voting and continuous actions requiring further research.

## Key Results
- Policy performance drops limited to 50% with 7% poisoned training data vs 0.008% in prior work
- Certified radii 5 times larger than existing approaches (up to 160 vs ~10 for Freeway)
- Framework applicable to both discrete and continuous action spaces with deterministic and stochastic environments
- Substantial improvement in safety and reliability for offline RL systems

## Why This Works (Mechanism)

### Mechanism 1: Differential Privacy Enables Outcomes Guarantee
Randomized training with calibrated noise provides a (K, r)-outcomes guarantee that bounds how much poisoning can shift any output distribution. The training algorithm injects Gaussian noise via Sampled Gaussian Mechanism (for transition-level) or DP-FEDAVG (for trajectory-level), creating DP guarantees that ensure Pr[M(D₁) ∈ S] ≤ K(Pr[M(D₂) ∈ S]) for adjacent datasets differing in up to r elements. The noise multiplier σ and sampling ratio q must be correctly calibrated so the training process preserves the stated (ε, δ)-ADP or (α, ε)-RDP guarantee.

### Mechanism 2: Extending Probability Guarantees to Expected Cumulative Reward
The (K, r)-outcomes guarantee converts to a lower bound on expected cumulative reward J^r(π) using Fubini's theorem and integral bounds over tail distributions. For RDP, this gives J(π̃) ≥ e^{-ε}(b^{-1/α}J(π))^{α/(α-1)}. The cumulative reward must be bounded in a known range [0, b] (or [a, b] for real-valued rewards) for the integral bounds to be valid.

### Mechanism 3: Action Stability via Inferred Score Certification
Per-state action stability is certified by checking if the inferred score of the selected action remains higher than alternatives under worst-case perturbation. Each policy instance votes for an action, and the inferred score I_{A_l}(s_t, π) = Pr[argmax Q^π(s_t, a_i) = A_l]. Action-level robustness holds if K₁⁻¹(I_{a_t}(s_t, π)) > max_{a_l≠a_t} K₂(I_{a_l}(s_t, π)), with the post-processing property ensuring action selection inherits DP guarantees.

## Foundational Learning

- **Differential Privacy (ε, δ)-ADP and (α, ε)-RDP**: The entire certification framework rests on DP guarantees. You must understand how ε (privacy loss), δ (failure probability), and α (Rényi divergence order) trade off, and why RDP provides tighter composition for iterative training. *Quick check*: Given σ=2.0, q=0.01, and 10,000 iterations, can you explain why RDP gives a tighter bound than ADP?

- **Offline RL and MDP Formulation**: The paper models offline RL as episodic finite-horizon MDPs. Understanding state/action spaces, transition functions, and cumulative reward J(π) is essential to interpret certification bounds. *Quick check*: What is the difference between trajectory-level and transition-level poisoning, and why does each require a different DP training algorithm?

- **Certified Robustness vs. Empirical Defenses**: This is a *certified* defense providing worst-case guarantees, not just empirically observed robustness. You must distinguish between "we observed it works" and "we proved it must work." *Quick check*: Why does Theorem 4.2 provide a *lower bound* on J(π̃) rather than an exact prediction?

## Architecture Onboarding

- **Component map**: Input Dataset D → DP Training (SGM/DP-FEDAVG) → p Policy Instances → Certification Module (Policy-level + Action-level) → Certified Bounds

- **Critical path**: DP training hyperparameters (σ, q, C) → privacy budget consumption → available K functions → tightness of certification. Incorrect calibration anywhere cascades to invalid certificates.

- **Design tradeoffs**:
  - Higher noise (σ) → stronger certification but worse clean performance (Table 1: Freeway clean reward drops from 20.1 to 16.0 as σ goes 0→3)
  - More policy instances (p) → better inferred score estimates but more compute
  - RDP vs. ADP: RDP gives tighter bounds for deep networks but requires α parameter tuning

- **Failure signatures**:
  - Certification returns r_t = 0 for most states → inferred scores too close (increase σ or accept weaker robustness)
  - J^r drops to near-zero quickly → reward bounds may be wrong, or noise calibration insufficient
  - Training diverges → clipping threshold C too small relative to gradient magnitudes

- **First 3 experiments**:
  1. **Baseline sanity check**: Train DQN on Freeway with σ=0 (no DP), verify cumulative reward ~20. Compare to COPA baseline from Table 1.
  2. **Noise sweep**: Train with σ ∈ {1.0, 2.0, 3.0} using SGM. For each, compute both action-level mean radii and policy-level J^r at r=0. Plot the tradeoff curve.
  3. **Certification validation**: Synthetically poison 5% of transitions with random reward corruption. Compare empirical J(π̃) to certified lower bound J^r. The empirical should exceed the bound (as in Appendix A.7, Table 5).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can a unified differential privacy training algorithm be developed to provide simultaneous transition- and trajectory-level certified defenses?
- **Basis**: [explicit] The authors state in the conclusion that developing a unified algorithm supporting both defense levels is a potential future direction to enhance robustness.
- **Why unresolved**: The current framework relies on distinct mechanisms (SGM and DP-FEDAVG) for transition and trajectory privacy, which may not be mutually compatible or optimal when combined.
- **What evidence would resolve it**: A single training procedure that generates tight privacy accounting for both adjacency definitions simultaneously.

### Open Question 2
- **Question**: Does a dynamic, adaptive noise injection mechanism improve the trade-off between clean performance and certified robustness compared to the uniform noise used here?
- **Basis**: [explicit] The conclusion suggests that defense performance may be improved by designing sophisticated noise injection mechanisms that adapt noise levels dynamically rather than using uniform noise.
- **Why unresolved**: The current implementation injects noise uniformly, which treats all updates equally regardless of their specific sensitivity or contribution to the policy's robustness.
- **What evidence would resolve it**: Empirical results showing that an adaptive noise schedule achieves higher certified radii or cumulative rewards at equivalent privacy levels.

### Open Question 3
- **Question**: How can action-level robustness certification be defined and computed for continuous action spaces?
- **Basis**: [inferred] While the framework handles continuous spaces for policy-level certification, the action-level certification (Theorem 4.4) explicitly relies on a discrete action set and majority voting over instances.
- **Why unresolved**: The discrete definition of "inferred scores" based on the probability of a specific argmax action does not translate directly to continuous distributions where exact matches are improbable.
- **What evidence would resolve it**: A formulation for action stability in continuous domains (e.g., bounded deviation) with corresponding robustness guarantees.

## Limitations
- Action-level certification often fails when inferred scores for multiple actions are close, limiting applicability to states with clear action preferences
- Framework requires accurate calibration of DP parameters (σ, q, C) for valid certification guarantees
- Theoretical bounds may be loose in practice, potentially overestimating actual robustness

## Confidence
- **Differential Privacy Calibration**: Medium confidence - effectiveness depends on accurate parameter tuning
- **Action-Level Certification**: Low confidence - frequently fails when action scores are close
- **Policy-Level Reward Bounds**: Medium confidence - experimental improvements demonstrated but theoretical bounds may be conservative

## Next Checks
1. Conduct sensitivity analysis varying noise multipliers σ to identify the calibration threshold where certification guarantees become meaningful versus performance degradation begins
2. Test certification robustness across multiple seeds to quantify variance in certified radii and identify conditions where guarantees consistently fail
3. Compare certified bounds against empirical performance under controlled poisoning attacks to validate that theoretical lower bounds are not overly conservative