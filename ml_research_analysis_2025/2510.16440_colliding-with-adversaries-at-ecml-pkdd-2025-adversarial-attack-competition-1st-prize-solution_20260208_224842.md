---
ver: rpa2
title: Colliding with Adversaries at ECML-PKDD 2025 Adversarial Attack Competition
  1st Prize Solution
arxiv_id: '2510.16440'
source_url: https://arxiv.org/abs/2510.16440
tags:
- uni00000013
- uni0000001c
- attack
- uni00000011
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents the first-place solution for Task 1 of the ECML-PKDD
  2025 Adversarial Attack Competition, which involved designing adversarial attacks
  against a TopoDNN classification model for high-energy physics data. The task required
  maximizing misclassification while minimizing perturbations, using a binary classification
  problem between TTJets and W-Boson-jets.
---

# Colliding with Adversaries at ECML-PKDD 2025 Adversarial Attack Competition 1st Prize Solution

## Quick Facts
- arXiv ID: 2510.16440
- Source URL: https://arxiv.org/abs/2510.16440
- Reference count: 3
- First place in ECML-PKDD 2025 Adversarial Attack Competition Task 1

## Executive Summary
This paper presents the winning solution for Task 1 of the ECML-PKDD 2025 Adversarial Attack Competition, achieving a score of 0.976 on a binary classification problem distinguishing TTJets from W-Boson-jets in high-energy physics data. The approach uses a conditional gradient-based attack strategy with multi-round optimization and sample-mixing initialization techniques. The method successfully maximizes misclassification while minimizing perturbations through a carefully designed two-phase objective function and extensive parallel optimization runs.

## Method Summary
The solution employs a conditional objective function that switches between minimizing negative binary cross-entropy (to flip predictions) and minimizing L1 perturbations (once flipped). The attack runs for 150 rounds with 20 parallel runs per round, using vanilla gradient descent with random step sizes that decay exponentially. Initialization strategies include using original data, best solutions from previous rounds, and convex combinations with opposite-class examples. The final submission selects the minimal perturbation for each sample that successfully flipped the prediction.

## Key Results
- Achieved competition score of 0.976, outperforming second place by approximately 0.04
- Successfully flipped predictions for all 5,000 test samples
- Mean L1 perturbation of 1.90 for successful attacks
- 100% fooling ratio achieved across the dataset

## Why This Works (Mechanism)

### Mechanism 1
A conditional two-phase objective function improves attack efficiency by decoupling the misclassification goal from perturbation minimization. The loss function switches behavior based on prediction state - when the model prediction has not yet flipped, the optimizer minimizes negative binary cross-entropy to push predictions across the decision boundary. Once the prediction flips, the objective immediately switches to L1 minimization to reduce perturbation magnitude. This avoids gradient interference between competing objectives.

### Mechanism 2
Multi-round optimization with decaying random step sizes increases the probability of finding low-perturbation adversarial examples. The attack runs 150 rounds with 20 parallel runs per round, using step sizes sampled from uniform distributions with exponentially decaying bounds. Early rounds use larger steps for exploration while later rounds use smaller steps for refinement, maintaining the best solution found so far.

### Mechanism 3
Sample-mixing initialization strategies help escape regions where local gradients provide unreliable attack guidance. Three initialization strategies are used: original data, best solution from previous rounds, and convex combinations of samples with opposite-class examples from the dataset. This leverages label knowledge to initialize optimization closer to the decision boundary.

## Foundational Learning

- **White-box Gradient-Based Attacks**
  - Why needed here: The entire methodology assumes access to model gradients via backpropagation. Without understanding how gradient descent exploits differentiable decision boundaries, the conditional objective mechanism is opaque.
  - Quick check question: Given a neural network f(x) with binary output, how would you compute the gradient of cross-entropy loss with respect to input x?

- **Binary Cross-Entropy Loss**
  - Why needed here: L_fool uses negative BCE as the minimization target to flip predictions. Understanding why minimizing -BCE pushes predictions toward the opposite class is essential.
  - Quick check question: If BCE(y, ŷ) = -[y·log(ŷ) + (1-y)·log(1-ŷ)], what happens to the loss when ŷ approaches 1-y?

- **L₁ Norm Perturbation**
  - Why needed here: The competition metric penalizes L₁ distance between original and adversarial examples. L₁ encourages sparse perturbations, which may differ from L₂-based attacks in feature selection behavior.
  - Quick check question: Why might L₁ minimization produce different adversarial patterns than L₂ minimization on tabular data?

## Architecture Onboarding

- **Component map:**
  Input layer (5,000 samples × 87 features) -> TopoDNN model -> Conditional objective function -> Vanilla gradient descent optimizer -> 150 rounds × 20 parallel runs -> X_best selection

- **Critical path:**
  1. Load model and test data
  2. For each round n ∈ [1,150]: sample 20 step sizes from U(min_n, max_n)
  3. For each parallel run j ∈ [1,20]: initialize per mixing strategy
  4. Run 2,500 gradient descent steps with conditional objective
  5. Run 250 follow-up steps with L_fool only
  6. Update X_best if new perturbation is smaller and prediction flipped
  7. Aggregate final X_best as submission

- **Design tradeoffs:**
  Computational cost vs. score: Full method requires ~3,000 GPU-hours; single-round achieves 0.956 vs. full 0.976. Vanilla GD vs. Adam: Authors note Adam could improve convergence but GD proved sufficient. Opposite-class mixing requires label knowledge: May not generalize to unlabeled attack scenarios.

- **Failure signatures:**
  Fooling ratio plateaus below 100%: Likely hard samples trapped in flat gradient regions; increase opposite-class mixing ratio. L₁ distance not decreasing after early rounds: Step sizes may be too large; verify decay schedule. Out-of-memory errors: Reduce parallel runs from 20 or batch samples.

- **First 3 experiments:**
  1. Baseline validation: Run Core variant (50,000 steps, learning rate 0.01→100) on 100 samples; verify score ≈ 0.93
  2. Ablation on initialization: Compare X-only vs. X_best mixing vs. opposite-class mixing on 500 samples; measure fooling ratio at round 10
  3. Step size sensitivity: Test 3 decay schedules (original, slower decay, fixed small step) on full dataset for 30 rounds; plot score trajectory

## Open Questions the Paper Calls Out

### Open Question 1
Would more sophisticated optimization algorithms (e.g., Adam with momentum and adaptive learning rates) significantly improve attack effectiveness or reduce the number of required optimization rounds compared to vanilla gradient descent? The authors achieved their winning result using only vanilla GD and did not experiment with adaptive optimizers; the potential gains remain unquantified.

### Open Question 2
Can the sample-mixing initialization strategy be made more targeted or theoretically principled to further reduce perturbation size while maintaining 100% fooling ratio? The current mixing uses random uniform weights and random selection of opposite-label samples without optimization of the mixing strategy itself.

### Open Question 3
Does this white-box gradient-based attack methodology generalize effectively to other HEP classification architectures (e.g., Particle Flow Networks, Lorentz Layer Networks) or to non-HEP tabular classification tasks? The method was only evaluated on one specific TopoDNN architecture; transferability to other differentiable models with different inductive biases is unknown.

### Open Question 4
What characterizes the "hardest-to-reverse" samples that require cross-label mixing initialization, and can these be identified a priori to reduce unnecessary parallel optimization runs? The authors note cross-label mixing "proved effective in the hardest-to-reverse cases, where local gradients do not provide reliable guidance"—suggesting some samples have fundamentally different loss landscapes.

## Limitations

- The method depends on specific model architecture and gradient properties, with no empirical evidence about gradient quality or gradient obfuscation in TopoDNN
- Sample-mixing initialization may introduce out-of-distribution artifacts in physical data, but this risk is not quantified
- The competition metric heavily weights fooling ratio over perturbation minimization, potentially favoring aggressive attacks over truly minimal perturbations

## Confidence

- **Mechanism 1 (Conditional objective):** High confidence - The two-phase approach is theoretically sound and well-specified mathematically
- **Mechanism 2 (Multi-round optimization):** Medium confidence - While the approach is logical, the paper lacks ablation studies comparing single-round vs. multi-round performance on the same computational budget
- **Mechanism 3 (Sample-mixing initialization):** Low confidence - The method is described but not validated against simpler initialization strategies, and potential physical data violations are not addressed

## Next Checks

1. **Gradient quality analysis:** Measure the correlation between gradient magnitude and prediction change probability across samples. Compute the fraction of samples where gradients successfully drive predictions toward the opposite class within 100 steps.

2. **Initialization ablation study:** Run controlled experiments comparing four initialization strategies (original data only, X_best mixing only, opposite-class mixing only, and the full multi-strategy approach) on 1,000 samples for 50 rounds. Report fooling ratio and mean L1 perturbation trajectories.

3. **Physical data validity check:** For each mixing strategy, compute the fraction of interpolated points that violate physical constraints (negative energies, invalid angular values). Measure how often the attack produces out-of-distribution samples and whether these affect the competition score.