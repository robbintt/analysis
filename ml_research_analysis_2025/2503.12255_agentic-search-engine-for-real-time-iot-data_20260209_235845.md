---
ver: rpa2
title: Agentic Search Engine for Real-Time IoT Data
arxiv_id: '2503.12255'
source_url: https://arxiv.org/abs/2503.12255
tags:
- data
- iot-ase
- service
- user
- real-time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents IoT-ASE, an agentic search engine designed to
  address the fragmentation of IoT systems by enabling seamless, real-time data sharing
  and retrieval. Leveraging Large Language Models (LLMs) and Retrieval-Augmented Generation
  (RAG), IoT-ASE processes complex queries and delivers contextually relevant results
  using real-time IoT data.
---

# Agentic Search Engine for Real-Time IoT Data

## Quick Facts
- arXiv ID: 2503.12255
- Source URL: https://arxiv.org/abs/2503.12255
- Reference count: 40
- Primary result: 92% intent-based retrieval accuracy using LLM + RAG + multi-agent workflow on 500 services and 37,033 IoT devices

## Executive Summary
This paper presents IoT-ASE, an agentic search engine designed to address the fragmentation of IoT systems by enabling seamless, real-time data sharing and retrieval. Leveraging Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG), IoT-ASE processes complex queries and delivers contextually relevant results using real-time IoT data. Implemented within the SensorsConnect framework, the system was evaluated using a Toronto-based dataset of 500 services and 37,033 IoT devices. Results showed 92% accuracy in intent-based service retrieval, with responses outperforming those from Gemini in relevance and precision. The study highlights IoT-ASE's potential to support effective real-time decision-making by making IoT data more accessible and actionable.

## Method Summary
The method employs a four-agent GA-RAG workflow built with LangGraph, routing queries through a Classifier to either IoT-RAG-SE (for local IoT services), Google Maps, or a Scraper. IoT-RAG-SE uses Sentence-BERT embeddings with HNSW vector indexing for service descriptions, while real-time IoT data is stored in MongoDB with geospatial indexing. A Llama3-8b-8192 model generates responses, and a Reviewer agent checks semantic correctness and preference alignment. The system was evaluated on 25 queries against a simulated Toronto dataset of 500 services and 37,033 IoT devices, achieving 92% intent-based retrieval accuracy.

## Key Results
- 92% accuracy in retrieving intent-based services in the top 3 results
- Median query response time of 2.1 seconds, outperforming Google Maps and Gemini
- Demonstrated effectiveness of agentic workflow with reviewer feedback in improving response relevance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Semantic search over service descriptions enables intent-based retrieval for complex IoT queries.
- Mechanism: Service descriptions are tokenized, embedded using Sentence-BERT, mean-pooled, normalized, and stored in a vector database (HNSW). At query time, the same pipeline transforms the user query into an embedding, and nearest-neighbor search retrieves the top-k matching services, which then route to the appropriate real-time IoT data collections.
- Core assumption: High-quality service descriptions accurately represent the semantic space of user intents; poor descriptions propagate retrieval errors.
- Evidence anchors:
  - [section III-B]: "IoT-RAG-SE carries out two main processes: services descriptions embedding and performing semantic search... 92% accuracy in retrieving intent-based services."
  - [section IV-C1]: "For the 25 queries, the engine infers the correct intent in the list of top 3 matched results, with 92 percent of the retrieved results matching the intent in the first result."
  - [corpus]: Weak direct evidence—neighbor papers discuss IoT architectures but do not validate semantic search for IoT service retrieval.
- Break condition: Ambiguous or overlapping service descriptions cause incorrect top-1 matches; queries outside the semantic coverage of indexed services return irrelevant results.

### Mechanism 2
- Claim: A multi-agent agentic workflow with review feedback reduces hallucination and improves response relevance.
- Mechanism: Four agents operate in sequence—Classifier routes queries (to IoT-RAG-SE, Google Maps, or Scraper), Retriever fetches context, Generator produces responses with retriever-specific prompts, and Reviewer evaluates semantic correctness and preference alignment, potentially reformulating queries or rerouting.
- Core assumption: Reviewer can reliably detect semantic errors and preference mismatches without ground-truth access; LLM-based review sufficiently approximates fact-checking.
- Evidence anchors:
  - [section III-C]: "GA-RAG workflow... incorporates four agents: Classifier, Retriever, Generator, and Reviewer... If the response needs improvement or is unreasonable, it reformulates the query."
  - [section III-D]: "Reviewer checks the user query and the generative response and ensures the response contains the information the user seeks."
  - [corpus]: No direct validation of agentic review loops in IoT contexts from neighbor papers.
- Break condition: Reviewer flags false positives (rejecting correct responses) or misses subtle hallucinations; cascading errors across agents compound latency.

### Mechanism 3
- Claim: Decoupling service embeddings from real-time IoT data enables live updates without re-embedding.
- Mechanism: Static service descriptions are embedded once; dynamic IoT node data (e.g., occupancy, pricing) lives in a separate real-time database. Queries retrieve service matches first, then pull live node documents using geographical indexing and the service name as a collection key.
- Core assumption: IoT data updates are frequent but service taxonomy is stable; geographical and service-type routing suffices for real-time queries.
- Evidence anchors:
  - [section III-B]: "IoT-RAG-SE... returns the nearest k node documents, matching user intent... using 2D or Geographical database indexing."
  - [section III-A]: "The node data entity encapsulates abstracted IoT data... node entity provides descriptions and metadata."
  - [corpus]: Neighbor papers on real-time IoT monitoring (e.g., WSN for industrial environments) support the need for live data but do not validate this decoupling pattern.
- Break condition: Service taxonomy changes require full re-embedding; high-velocity data updates exceed database write throughput for geographical queries.

## Foundational Learning

- Concept: **Vector Embeddings and Semantic Similarity**
  - Why needed here: Core to how IoT-ASE maps natural language queries to service descriptions; cosine similarity in embedding space determines retrieval ranking.
  - Quick check question: Can you explain why normalization is required before computing cosine similarity between query and service embeddings?

- Concept: **RAG (Retrieval-Augmented Generation)**
  - Why needed here: IoT-ASE augments LLM responses with retrieved real-time IoT context; understanding the retriever-generator split is essential for debugging response quality.
  - Quick check question: What happens to response accuracy if the retriever returns irrelevant documents to the generator?

- Concept: **HNSW (Hierarchical Navigable Small World)**
  - Why needed here: The vector database uses HNSW for approximate nearest neighbor search; latency vs. recall tradeoff directly impacts query responsiveness.
  - Quick check question: How does increasing the HNSW `ef` parameter affect recall and latency?

## Architecture Onboarding

- Component map:
  - **IoT-RAG-SE**: Tokenizer → Embedding Model (Sentence-BERT) → Pooling → Normalization → VectorDB (HNSW) → Real-Time IoT Database (MongoDB with geo-indexing).
  - **GA-RAG Workflow**: Classifier → Retriever (IoT-RAG-SE | Google Maps | Scraper) → Generator → Reviewer → Response (or loop back).
  - **SensorsConnect Framework**: Perception Layer (sensors) → Edge Layer (UIDI adapter) → Cloud Layer (real-time DB, historical DB, cache) → Business/User Interface Layers.

- Critical path: User query → Classifier (route to IoT-RAG-SE) → Semantic search on service embeddings → Retrieve top-k services → Query real-time IoT DB with geo filters → Return node documents → Generator (with context + prompts) → Reviewer (check preferences, flag errors) → Final response.

- Design tradeoffs:
  - Embedding service descriptions (static, infrequent re-embedding) vs. embedding live IoT data (computationally infeasible at scale).
  - Concise, preference-aware responses (IoT-ASE) vs. exhaustive lists (Google Maps/Gemini pattern).
  - Coverage zone routing (falls back to Google Maps for uncovered regions) vs. pure IoT-only retrieval.

- Failure signatures:
  - Low top-1 accuracy (e.g., Tire Shop and Museum ranked third instead of first) → likely due to ambiguous service descriptions.
  - Reviewer rerouting loops → generated response fails preference check or retriever returns off-domain context.
  - High latency (>4s for 1% of queries) → potential bottleneck in scraping subnode or multi-agent coordination.

- First 3 experiments:
  1. **Service description quality audit**: Manually inspect top-3 retrieval results for 25 queries; identify where descriptions cause mis-ranking and iterate on description generation prompts.
  2. **Latency breakdown profiling**: Instrument each agent (Classifier, Retriever, Generator, Reviewer) with timing; identify which subnode (IoT-RAG-SE, Google Maps, Scraper) dominates tail latency.
  3. **Reviewer threshold tuning**: Vary the strictness of Reviewer preference-checking logic; measure tradeoff between response rejection rate and user-perceived relevance on a held-out query set.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can service descriptions be automatically optimized and assessed to increase the likelihood of retrieving the intended service as the top result?
- **Basis in paper:** [Explicit] The authors note that the evaluation accuracy depends on the quality of service descriptions and that "finding a method for improvement and assessing the services’ descriptions can enhance the likelihood of the first matching result."
- **Why unresolved:** The current approach relies on descriptions generated by GPT-4, which can be inaccurate, leading to propagation of errors where the correct service is ranked second or third rather than first.
- **What evidence would resolve it:** Demonstration of an automated pipeline that validates or enriches service descriptions, resulting in a statistically significant increase in top-1 retrieval accuracy beyond the reported 92%.

### Open Question 2
- **Question:** How can a dedicated fact-checking component be integrated into the Reviewer agent to verify the correctness of generated responses against real-time data?
- **Basis in paper:** [Explicit] The paper states the Reviewer agent currently has limited capabilities and suggests that "later on, reviewer abilities can be improved by adding a fact-checking component."
- **Why unresolved:** The current Reviewer relies on semantic meaning to catch errors, but it cannot fully validate the factual correctness of the response against the live IoT context.
- **What evidence would resolve it:** An updated architecture where the Reviewer successfully identifies and corrects hallucinated values (e.g., incorrect wait times or prices) in a controlled test set.

### Open Question 3
- **Question:** How does IoT-ASE performance scale regarding latency and cost when applied to the projected tens of billions of IoT devices rather than the simulated 500 services?
- **Basis in paper:** [Inferred] The paper cites a projection of 41.6 billion IoT devices but evaluates the system on a limited dataset of 500 services and 37,033 devices.
- **Why unresolved:** While the system shows acceptable latency in the small-scale Toronto scenario, the efficiency of the vector database and agentic workflow under massive, web-scale data loads remains untested.
- **What evidence would resolve it:** Stress-test results showing query latency and token consumption scaling linearly or sub-linearly as the service vector database grows by orders of magnitude.

## Limitations

- Evaluation relies entirely on a simulated dataset rather than real-world operational IoT data
- 92% retrieval accuracy based on only 25 hand-crafted queries limits generalizability
- Reviewer agent's effectiveness in detecting hallucinations is assumed but not empirically validated with ground-truth annotations

## Confidence

- **High confidence**: The vector embedding and HNSW retrieval pipeline for static service descriptions is well-established and technically sound (Mechanism 1).
- **Medium confidence**: The multi-agent GA-RAG workflow's ability to improve response relevance through review feedback is plausible but not empirically validated against a ground-truth baseline (Mechanism 2).
- **Medium confidence**: The decoupling of static service embeddings from dynamic IoT data is a valid architectural pattern, but performance under high-velocity updates is not tested (Mechanism 3).

## Next Checks

1. **Ground-truth annotation study**: Have human annotators label a held-out query set with correct top-3 service matches and reviewer decisions to measure true accuracy and reviewer precision/recall.
2. **Real-world dataset validation**: Deploy the system against an operational IoT testbed (e.g., open city IoT data) and measure retrieval accuracy, latency, and failure modes under realistic conditions.
3. **Reviewer robustness testing**: Systematically inject known hallucinations or semantic errors into responses and measure the Reviewer's detection rate, false positive rate, and impact on response latency.