---
ver: rpa2
title: 'Self-Guard: Defending Large Reasoning Models via enhanced self-reflection'
arxiv_id: '2602.00707'
source_url: https://arxiv.org/abs/2602.00707
tags:
- safety
- self-guard
- reasoning
- awareness
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces Self-Guard, a lightweight defense framework
  for Large Reasoning Models (LRMs) that addresses the awareness-compliance gap in
  model safety. The framework operates in two stages: safety-oriented prompting to
  activate the model''s latent safety awareness, followed by safety activation steering
  that amplifies directional shifts in the hidden state space to reinforce safety
  compliance.'
---

# Self-Guard: Defending Large Reasoning Models via enhanced self-reflection

## Quick Facts
- arXiv ID: 2602.00707
- Source URL: https://arxiv.org/abs/2602.00707
- Reference count: 40
- Self-Guard effectively bridges awareness-compliance gaps in Large Reasoning Models with minimal utility degradation

## Executive Summary
This paper introduces Self-Guard, a lightweight defense framework for Large Reasoning Models (LRMs) that addresses the awareness-compliance gap in model safety. The framework operates in two stages: safety-oriented prompting to activate the model's latent safety awareness, followed by safety activation steering that amplifies directional shifts in the hidden state space to reinforce safety compliance. Experiments show Self-Guard effectively bridges this gap, achieving competitive or superior safety performance across harmful and jailbreak benchmarks while maintaining high defense precision without over-refusal. The method preserves general reasoning utility with minimal degradation, contrasting sharply with steering-based approaches that suffer catastrophic utility loss.

## Method Summary
Self-Guard employs a two-stage defense mechanism specifically designed for Large Reasoning Models. First, it uses safety-oriented prompting to activate the model's inherent safety awareness capabilities. Second, it applies safety activation steering that amplifies directional shifts in the hidden state space to reinforce compliance with safety standards. The framework focuses on bridging the gap between a model's awareness of harmful content and its actual compliance in refusing to generate such content. Unlike traditional steering methods that can cause catastrophic utility degradation, Self-Guard maintains the model's general reasoning capabilities while enhancing safety responses.

## Key Results
- Achieves competitive or superior safety performance across harmful and jailbreak benchmarks
- Maintains high defense precision without over-refusal of benign requests
- Preserves general reasoning utility with minimal degradation compared to steering-based approaches

## Why This Works (Mechanism)
Self-Guard works by first activating the model's latent safety awareness through carefully designed prompts that trigger the model's internal safety representations. The second stage amplifies directional shifts in the hidden state space that correspond to safety-compliant behavior. This approach leverages the model's existing safety knowledge rather than attempting to override its fundamental reasoning capabilities. By focusing on the awareness-compliance gap, the framework addresses why models sometimes fail to act on safety knowledge they possess. The hidden state amplification technique allows for fine-grained control over safety behavior without disrupting the broader reasoning architecture.

## Foundational Learning
**Hidden state space manipulation**: Understanding how to navigate and modify internal representations without disrupting core functionality. Needed to implement targeted safety interventions while preserving reasoning capabilities. Quick check: Verify hidden state modifications remain confined to safety-relevant dimensions.

**Safety awareness vs. compliance distinction**: Recognizing that models can understand harmful content yet still generate it due to misalignment between awareness and behavior. Critical for identifying the specific failure mode being addressed. Quick check: Test models on safety-knowledge assessment before and after safety-oriented prompting.

**Directional amplification techniques**: Methods for scaling specific hidden state directions to influence model behavior. Essential for strengthening safety responses without wholesale behavioral modification. Quick check: Validate amplification preserves original task performance on non-safety dimensions.

**Prompt-based activation mechanisms**: Understanding how carefully crafted prompts can trigger latent capabilities in pre-trained models. Fundamental to the first stage of the defense framework. Quick check: Test prompt effectiveness across different model scales and safety contexts.

## Architecture Onboarding

**Component map**: Input -> Safety-oriented Prompting -> Hidden State Analysis -> Safety Activation Steering -> Output

**Critical path**: The most time-critical component is the safety-oriented prompting stage, as it must efficiently activate latent safety awareness without introducing excessive latency. This is followed by the hidden state analysis, which requires careful calibration to identify safety-relevant directions without over-processing.

**Design tradeoffs**: The framework balances between aggressive safety enforcement and maintaining reasoning utility. Steering-based approaches were rejected due to catastrophic utility loss, while pure prompting approaches were deemed insufficient for bridging the awareness-compliance gap. The two-stage approach represents a compromise that maximizes safety gains while minimizing utility degradation.

**Failure signatures**: Potential failures include over-refusal of benign content, incomplete safety activation, or hidden state amplification that inadvertently affects non-safety reasoning dimensions. The framework includes calibration mechanisms to detect and mitigate these failure modes through iterative refinement of the steering vectors.

**3 first experiments**:
1. Baseline safety awareness assessment to quantify the initial awareness-compliance gap
2. Prompt effectiveness testing across different harmful content categories
3. Hidden state direction identification validation to ensure safety-relevant amplification

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focuses on binary harmful/non-harmful classification rather than nuanced risk assessment
- Effectiveness on novel attack patterns or multimodal jailbreaks remains untested
- Defense precision claims rely on specific attack datasets that may not generalize to broader adversarial landscapes

## Confidence
- Utility preservation claims: Medium confidence (evaluation lacks depth in task-specific performance across domains)
- Safety performance claims: Medium confidence (evaluation uses established benchmarks that may not represent full spectrum of jailbreak sophistication)
- Generalization across model scales: High confidence (experiments conducted across multiple model sizes)

## Next Checks
1. Evaluate Self-Guard against emerging jailbreak techniques and multimodal attacks to assess real-world robustness beyond static benchmarks
2. Conduct comprehensive utility analysis across diverse task domains to quantify reasoning performance degradation in practical applications
3. Test the framework's effectiveness on out-of-distribution safety risks and novel harmful content categories not represented in training data