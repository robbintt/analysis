---
ver: rpa2
title: 'LT-Soups: Bridging Head and Tail Classes via Subsampled Model Soups'
arxiv_id: '2511.10683'
source_url: https://arxiv.org/abs/2511.10683
tags:
- lt-soups
- imbalance
- training
- should
- classes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses long-tailed class imbalance in vision tasks
  by proposing LT-Soups, a two-stage model averaging framework. It introduces the
  head-tail ratio as a complementary metric to the imbalance ratio, revealing that
  parameter-efficient fine-tuning methods excel in tail-heavy scenarios but degrade
  in more balanced or head-heavy distributions.
---

# LT-Soups: Bridging Head and Tail Classes via Subsampled Model Soups

## Quick Facts
- **arXiv ID:** 2511.10683
- **Source URL:** https://arxiv.org/abs/2511.10683
- **Reference count:** 40
- **Primary result:** Introduces head-tail ratio metric and achieves balanced accuracy across head/tail classes on six long-tailed vision benchmarks

## Executive Summary
This paper addresses the fundamental challenge of long-tailed class imbalance in vision tasks by introducing LT-Soups, a two-stage model averaging framework that systematically combines models trained on progressively less imbalanced subsets. The authors reveal that parameter-efficient fine-tuning methods, while effective for extremely imbalanced distributions, degrade performance as class distributions become more balanced. By averaging models from subsampled datasets and fine-tuning only the classifier on the full dataset, LT-Soups achieves superior performance across the full spectrum of imbalance configurations while maintaining robustness to structural shifts in class distributions.

## Method Summary
LT-Soups operates through a two-stage framework that addresses long-tailed class imbalance by leveraging model averaging with a novel subsampling strategy. The first stage creates multiple models by fine-tuning on progressively less imbalanced subsets of the training data, generated by removing samples from head classes according to the proposed head-tail ratio metric. The second stage averages these models and performs classifier-only fine-tuning on the full dataset. This approach effectively bridges the performance gap between head and tail classes by combining the strengths of models trained on different imbalance configurations, achieving balanced accuracy improvements across six benchmark datasets including CIFAR-100-LT, ImageNet-LT, and Places-LT.

## Key Results
- LT-Soups consistently outperforms both parameter-efficient fine-tuning methods and traditional model soups across six benchmark datasets
- The proposed head-tail ratio metric reveals that PEFT methods excel in tail-heavy scenarios but degrade in balanced or head-heavy distributions
- Achieves balanced accuracy on both head and tail classes while maintaining robustness to structural shifts in class distributions

## Why This Works (Mechanism)
LT-Soups works by systematically combining the complementary strengths of models trained on different imbalance configurations. By averaging models fine-tuned on progressively less imbalanced subsets, the framework captures diverse representations that collectively address both head and tail class challenges. The classifier-only fine-tuning stage on the full dataset allows the model to adapt to the complete distribution while preserving the balanced representations learned during the averaging stage. This two-stage approach effectively mitigates the degradation observed in PEFT methods when class distributions become more balanced, resulting in consistent performance across the full imbalance spectrum.

## Foundational Learning
- **Long-tailed class imbalance:** Why needed - Understanding the fundamental challenge of skewed class distributions in vision tasks; Quick check - Can be measured by imbalance ratio and head-tail ratio metrics
- **Model soups:** Why needed - Ensemble averaging technique for combining multiple fine-tuned models; Quick check - Tested against traditional model averaging baselines
- **Parameter-efficient fine-tuning (PEFT):** Why needed - Essential for adapting large vision models to imbalanced datasets; Quick check - Shows degradation in balanced scenarios
- **Subsampling strategies:** Why needed - Key technique for generating progressively less imbalanced subsets; Quick check - Implemented through head-tail ratio-based sample removal
- **Two-stage training framework:** Why needed - Separates representation learning from classifier adaptation; Quick check - First stage averages models, second stage fine-tunes classifier
- **Head-tail ratio metric:** Why needed - Complementary measure to imbalance ratio for characterizing distribution shifts; Quick check - Validated across six benchmark datasets

## Architecture Onboarding

**Component Map:**
Input data -> Subsampling stage (head-tail ratio) -> Multiple PEFT fine-tuning stages -> Model averaging -> Classifier fine-tuning -> Output predictions

**Critical Path:**
Data subsampling (head-tail ratio) -> PEFT fine-tuning on subsets -> Model averaging -> Classifier fine-tuning on full dataset

**Design Tradeoffs:**
- Computational cost: Higher training overhead but single model inference
- Model complexity: Maintains parameter efficiency while improving balance
- Dataset requirements: Needs sufficient samples for effective subsampling
- Framework flexibility: Works with various PEFT methods and backbones

**Failure Signatures:**
- Performance degradation on extremely imbalanced distributions
- Ineffective subsampling when class imbalance is minimal
- Classifier fine-tuning stage may not adequately adapt to full distribution

**First 3 Experiments to Run:**
1. Validate head-tail ratio metric on real-world long-tailed datasets beyond synthetic configurations
2. Compare LT-Soups against full fine-tuning baselines to assess PEFT-specific advantages
3. Test different subsampling stage counts to optimize computational efficiency

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions for future research, focusing instead on presenting and validating the LT-Soups framework and its empirical performance across benchmark datasets.

## Limitations
- Computational overhead during training may limit practical applicability for resource-constrained scenarios
- Theoretical justification for subsampling strategy's effectiveness across different imbalance configurations needs further development
- Focus exclusively on parameter-efficient fine-tuning methods may restrict generalizability to full fine-tuning approaches

## Confidence

**High confidence:** Experimental methodology and benchmark results are robust and reproducible

**Medium confidence:** Theoretical framework connecting head-tail ratio to performance degradation

**Medium confidence:** Subsampling strategy's effectiveness across different imbalance configurations

**Low confidence:** Generalization claims to real-world long-tailed distributions beyond benchmark datasets

## Next Checks

1. Test LT-Soups on real-world long-tailed datasets (e.g., iNaturalist, LVIS) to validate generalization beyond synthetic imbalance scenarios

2. Conduct ablation studies varying the number of subsampling stages to determine optimal trade-offs between performance and computational cost

3. Evaluate the framework's robustness when applied to non-PEFT baselines and different backbone architectures to assess architectural independence