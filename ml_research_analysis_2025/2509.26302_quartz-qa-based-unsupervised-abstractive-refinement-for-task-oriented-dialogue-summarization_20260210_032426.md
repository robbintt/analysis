---
ver: rpa2
title: 'QUARTZ : QA-based Unsupervised Abstractive Refinement for Task-oriented Dialogue
  Summarization'
arxiv_id: '2509.26302'
source_url: https://arxiv.org/abs/2509.26302
tags:
- dialogue
- summary
- summaries
- quartz
- summarization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: QUARTZ is an unsupervised framework for task-oriented dialogue
  summarization that generates multiple summaries and QA pairs, then selects the best
  summary using LLM-based evaluation. It improves factual consistency and task relevance
  by focusing on QA-based evaluation rather than surface-level metrics.
---

# QUARTZ : QA-based Unsupervised Abstractive Refinement for Task-oriented Dialogue Summarization

## Quick Facts
- arXiv ID: 2509.26302
- Source URL: https://arxiv.org/abs/2509.26302
- Authors: Mohamed Imed Eddine Ghebriout; GaÃ«l Guibon; Ivan Lerner; Emmanuel Vincent
- Reference count: 40
- Primary result: QUARTZ achieves competitive performance to fully supervised state-of-the-art methods across multiple datasets, outperforming them in several cases.

## Executive Summary
QUARTZ is an unsupervised framework for task-oriented dialogue summarization that generates multiple summaries using an LLM and creates corresponding QA pairs. The framework selects the best summary based on LLM-based evaluation focusing on factual consistency and task relevance. QUARTZ demonstrates competitive performance to fully supervised state-of-the-art methods across multiple datasets, with improvements in factual consistency and task relevance through QA-based evaluation rather than surface-level metrics.

## Method Summary
QUARTZ operates as an unsupervised framework that generates multiple summaries and QA pairs, then selects the best summary using LLM-based evaluation. The approach focuses on QA-based evaluation rather than surface-level metrics to improve factual consistency and task relevance. The framework achieves competitive performance to fully supervised state-of-the-art methods, with further improvements possible through fine-tuning the best LLM on the selected summaries. Evaluation shows QUARTZ summaries score higher than reference summaries across coherence, consistency, fluency, and relevance when assessed by LLM-as-judge metrics.

## Key Results
- Achieves +19% BLEU and +3% BERT-Score relative to best supervised method on SAMSum
- LLM-as-judge evaluation shows QUARTZ summaries score higher than reference summaries across multiple quality dimensions
- Human evaluation favors QUARTZ summaries over reference summaries in nearly half of cases

## Why This Works (Mechanism)
QUARTZ leverages unsupervised learning to generate multiple dialogue summaries and uses LLM-based evaluation with QA pairs to select the best summary. This approach improves factual consistency and task relevance by focusing on QA-based evaluation rather than surface-level metrics. The framework demonstrates competitive performance to fully supervised methods while addressing key challenges in task-oriented dialogue summarization.

## Foundational Learning

**Unsupervised Learning**
- Why needed: Enables training without labeled data, reducing dependency on expensive human annotations
- Quick check: Can the model generate reasonable outputs without any supervised training examples?

**QA-based Evaluation**
- Why needed: Provides a more robust assessment of summary quality by testing factual consistency and relevance
- Quick check: Do generated QA pairs accurately reflect the content and purpose of the summaries?

**LLM-as-Judge**
- Why needed: Allows automated evaluation of multiple quality dimensions without human intervention
- Quick check: Are the LLM-based scores consistent with human judgments on sample summaries?

## Architecture Onboarding

**Component Map**
Dialogue -> Multiple Summary Generation -> QA Pair Creation -> LLM-based Evaluation -> Best Summary Selection

**Critical Path**
The critical path flows from generating multiple summaries through QA pair creation to LLM-based evaluation for selecting the optimal summary.

**Design Tradeoffs**
QUARTZ trades the precision of supervised training for the flexibility and scalability of unsupervised learning. The framework prioritizes factual consistency and task relevance over surface-level metrics, potentially sacrificing some grammatical fluency for accuracy.

**Failure Signatures**
Potential failures include LLM evaluation bias, over-reliance on QA pairs that may not capture all relevant information, and possible inconsistencies when different LLMs are used for generation versus evaluation.

**First Experiments**
1. Generate multiple summaries from a single dialogue and evaluate variation in quality
2. Test QA pair accuracy by checking if questions can be answered from their corresponding summaries
3. Compare LLM-based evaluation scores with human judgments on a small sample set

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relies heavily on LLM-based metrics and human evaluation, potentially introducing subjectivity and bias
- Comparative performance may not fully account for differences in training data quality and domain specificity
- Framework's reliance on LLM-based evaluation could introduce inconsistencies if different LLMs are used for generation versus evaluation

## Confidence
High confidence in core methodology of generating multiple summaries and using QA-based evaluation for selection
Medium confidence in comparative performance claims due to potential variations in evaluation conditions
Medium confidence in human evaluation results due to potential subjectivity in human judgment

## Next Checks
1. Conduct ablation studies to isolate the impact of each component in the QUARTZ pipeline
2. Evaluate performance across additional diverse dialogue datasets to test generalizability
3. Implement automated factual consistency checks to complement the LLM-based evaluation