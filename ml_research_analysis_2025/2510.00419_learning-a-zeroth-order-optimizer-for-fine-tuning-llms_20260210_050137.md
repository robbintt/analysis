---
ver: rpa2
title: Learning a Zeroth-Order Optimizer for Fine-Tuning LLMs
arxiv_id: '2510.00419'
source_url: https://arxiv.org/abs/2510.00419
tags:
- learning
- fine-tuner
- loss
- memory
- mezo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ZO Fine-tuner, a learning-based zeroth-order
  optimizer for fine-tuning large language models (LLMs) that learns adaptive per-block
  perturbation variances instead of using static sampling strategies. Motivated by
  the observation that foundation models and their derivatives are widely adopted
  in practice, the method is designed to be trained once per LLM and reused across
  diverse downstream tasks, enabling efficient "train once, reuse widely" deployment.
---

# Learning a Zeroth-Order Optimizer for Fine-Tuning LLMs

## Quick Facts
- arXiv ID: 2510.00419
- Source URL: https://arxiv.org/abs/2510.00419
- Reference count: 35
- One-line primary result: Achieves 2.5% average accuracy gain over baselines while maintaining inference-level memory usage

## Executive Summary
This paper introduces ZO Fine-tuner, a learning-based zeroth-order optimizer for fine-tuning large language models (LLMs) that learns adaptive per-block perturbation variances instead of using static sampling strategies. Motivated by the observation that foundation models and their derivatives are widely adopted in practice, the method is designed to be trained once per LLM and reused across diverse downstream tasks, enabling efficient "train once, reuse widely" deployment. By exploiting the approximately block-diagonal Hessian structure of LLMs, ZO Fine-tuner uses lightweight neural networks to learn shared perturbation variances per parameter block, achieving minimal memory overhead (less than 2MB for OPT-30B) compared to the model size (60GB). Experimental results on four LLMs and seven datasets demonstrate that ZO Fine-tuner outperforms prior zeroth-order baselines in 82.1% of task-model combinations, achieving an average 2.5% improvement in accuracy while maintaining memory usage comparable to inference.

## Method Summary
ZO Fine-tuner is a learning-based zeroth-order optimizer that uses per-block auxiliary networks (PertNN) to predict perturbation variances for each parameter block during optimization. The method exploits the approximately block-diagonal Hessian structure of LLMs by sharing perturbation variances within naturally defined parameter blocks (e.g., embeddings, attention Q/K/V projections). During meta-training (L2L), PertNNs are optimized on one dataset (COPA) to minimize the loss after applying a ZO update with their predicted variances. During deployment, trained PertNNs generate variances for new tasks, enabling "train once, reuse widely" across model derivatives and datasets. The method includes normalization to decouple variance scale from learning rate and uses reparameterization to enable gradient flow through the sampling process.

## Key Results
- Outperforms prior zeroth-order baselines in 82.1% of 28 task-model combinations tested
- Achieves 2.5% average accuracy improvement across all tasks and models
- Maintains memory overhead under 2MB for OPT-30B (vs. 60GB model size), comparable to inference
- Shows strong generalization: PertNNs trained on LLaMA-3.2-1B generalize to LLaMA-3.1-8B-Instruct
- Ablation confirms normalization and learning-to-learn components are critical for performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Per-block adaptive variance yields tighter convergence bounds than isotropic perturbation.
- Mechanism: The paper proves (Theorem 1) that if the Hessian is approximately block-diagonal, assigning distinct perturbation variances per block can reduce the expected loss change upper bound compared to standard MeZO. The PertNN networks learn these block-wise variances conditioned on parameter statistics and recent losses.
- Core assumption: Hessian structure is approximately block-diagonal with blocks aligned to natural parameter groups (e.g., embeddings, attention Q/K/V).
- Evidence anchors:
  - [abstract] "exploiting the approximately block-diagonal Hessian structure of LLMs, ZO Fine-tuner uses lightweight neural networks to learn shared perturbation variances per parameter block"
  - [section 3.1] Theorem 1 and supporting derivation in Appendix D
  - [corpus] Neighbor papers (e.g., "Robust and Efficient Zeroth-Order LLM Fine-Tuning via Adaptive Bayesian Subspace Optimizer") discuss adaptive ZO methods but do not provide this specific block-Hessian theoretical grounding
- Break condition: If Hessian blocks are not approximately diagonal (e.g., architectures with dense cross-layer dependencies), per-block variance may not capture the structure and gains could diminish.

### Mechanism 2
- Claim: Normalization decouples learned variance magnitude from effective learning rate, stabilizing training.
- Mechanism: The method normalizes $\Sigma_t$ such that $\|\Sigma_t\|_F^2 = \|I_d\|_F^2 = d$, ensuring $\|u_t\|$ concentrates around a fixed value. This allows the learning rate $\eta$ to control step size independently of the variance scale.
- Core assumption: High-dimensional concentration of norm holds (reasonable for large $d$).
- Evidence anchors:
  - [section 3.2] Equation 4 and normalization discussion
  - [section 4.2] Ablation Table 2 shows normalization alone improves loss/accuracy across all tested settings
  - [corpus] Weak direct evidence; neighbor papers do not discuss this specific normalization technique
- Break condition: If batch size or perturbation dimension is very small, concentration may weaken and effective learning rate could become unstable.

### Mechanism 3
- Claim: Learning-to-learn (L2L) on a single dataset generalizes across tasks and model derivatives.
- Mechanism: PertNN inputs are task-agnostic state summaries (parameter mean/variance, previous variance, losses). Training on one trajectory (COPA) exposes the optimizer to diverse loss landscapes, enabling transfer without task-specific features.
- Core assumption: The optimizer learns perturbation strategies based on optimization state, not task-specific patterns.
- Evidence anchors:
  - [abstract] "trained on a single dataset is highly generalizable across model derivatives and datasets"
  - [section 4.1] Table 3 shows ZO Fine-tuner trained on LLaMA-3.1-8B generalizes to LLaMA-3.1-8B-Instruct
  - [corpus] Neighbor papers (e.g., "Fine-tuning Quantized Neural Networks with Zeroth-order Optimization") focus on static ZO methods and do not explore L2L generalization
- Break condition: If target tasks have fundamentally different loss geometries (e.g., reinforcement learning vs. classification), generalization may fail.

## Foundational Learning

- **Zeroth-order optimization**
  - Why needed here: Core to understanding how gradient-free updates work via finite differences of forward passes.
  - Quick check question: Can you explain why MeZO requires two forward passes per step and how it estimates gradients?

- **Block-diagonal Hessian structure**
  - Why needed here: Theoretical justification for per-block variance sharing; motivates architecture granularity.
  - Quick check question: What does it mean for a Hessian to be block-diagonal, and how does this affect optimization?

- **Learning-to-learn (L2L) / meta-learning**
  - Why needed here: Framework for training the optimizer itself; requires differentiating through optimization steps.
  - Quick check question: How does L2L differ from standard training, and what are the memory implications?

## Architecture Onboarding

- **Component map**: PertNN (2-layer MLP per block) -> Normalization module -> ZO update core (two-point estimator with reparameterized perturbation)

- **Critical path**:
  1. Compute block statistics from current parameters
  2. Query each block's PertNN to get variances
  3. Normalize variances
  4. Sample perturbation, compute loss at $\theta_t \pm \epsilon u_t$
  5. Estimate gradient, update parameters

- **Design tradeoffs**:
  - Block-wise vs. layer-wise sharing: Block-wise aligns with Hessian structure but requires identifying natural blocks; layer-wise is simpler but theoretically weaker (Table 4)
  - Single-dataset vs. multi-dataset L2L training: Single is cheaper; multi may help for diverse tasks but shows comparable results (Figure 6)
  - Gradient flow truncation: Cutting gradient through finite-difference term saves memory but is an approximation

- **Failure signatures**:
  - Loss oscillates or diverges: Check normalization is applied; verify learning rate is not too high
  - No improvement over MeZO: Verify PertNN is receiving non-constant inputs; check if Hessian structure assumption holds
  - Memory exceeds baseline: Ensure only statistics, not full parameters, are passed to PertNN

- **First 3 experiments**:
  1. Reproduce ablation (Table 2): Run with and without normalization on a single model/dataset to verify stabilization effect
  2. Test transfer: Train ZO Fine-tuner on COPA with LLaMA-3.2-1B, evaluate on SST-2 and SQuAD to confirm generalization
  3. Learning rate sweep (Figure 3): Compare convergence across learning rates $\{10^{-6}, 10^{-7}, 10^{-8}\}$ against MeZO baseline

## Open Questions the Paper Calls Out

- **Open Question 1**
  - Question: Can non-diagonal perturbation variance structures be implemented to improve ZO Fine-tuner performance without exceeding the strict memory constraints required for zeroth-order optimization?
  - Basis in paper: [explicit] The limitations section (A.1) states: "exploring non-diagonal structures is a potential improvement, though it may require additional techniques to mitigate the associated memory overhead."
  - Why unresolved: The current design uses a diagonal variance matrix for memory efficiency, but this may fail to capture complex correlations in the gradient landscape that non-diagonal structures could exploit.
  - What evidence would resolve it: An implementation of ZO Fine-tuner using low-rank or sparse non-diagonal covariance approximations that demonstrates improved convergence or accuracy while maintaining memory usage comparable to inference.

- **Open Question 2**
  - Question: How can specific geometric properties of LLM gradients, such as low-rank structures, be explicitly integrated into the perturbation generation process?
  - Basis in paper: [explicit] Section A.1 notes: "A potential future direction is to leverage these properties to generate more informed perturbations or cut the memory usage even more."
  - Why unresolved: While the current method exploits block-diagonal Hessian structures, it does not utilize the low-rank properties of gradients identified in related literature (e.g., LOZO), which could further refine the optimization direction.
  - What evidence would resolve it: A variant of ZO Fine-tuner that conditions perturbations on low-rank gradient estimates and shows superior sample efficiency or final accuracy compared to the standard block-diagonal approach.

- **Open Question 3**
  - Question: What constitutes the optimal data selection strategy for meta-training ZO Fine-tuner to ensure robust generalization across diverse downstream tasks?
  - Basis in paper: [inferred] The paper notes in Section C.5 that single-dataset training (on COPA) sometimes outperforms multi-dataset training, indicating the relationship between meta-training data diversity and optimizer robustness is not fully understood.
  - Why unresolved: The authors selected COPA primarily for its smooth loss decrease rather than proven generalization superiority, leaving the impact of training task distribution on the "train once, reuse widely" paradigm ambiguous.
  - What evidence would resolve it: A systematic ablation study varying the size and diversity of the meta-training dataset, demonstrating consistent improvements in generalization rather than the mixed results observed in the current analysis.

## Limitations

- The theoretical justification for block-wise variance sharing relies on the Hessian structure assumption, which is plausible but not empirically validated for the specific architectures tested.
- The experimental validation is limited to seven datasets and four model families, which may not fully capture the diversity of real-world applications.
- The specific implementation details for gradient cutoff and block-to-parameter mapping are underspecified, making exact reproduction challenging.

## Confidence

**High confidence**: The core contribution of learning per-block perturbation variances is technically sound and the experimental results (82.1% win rate, 2.5% average accuracy gain) are well-supported by the data presented.

**Medium confidence**: The theoretical justification for block-wise variance sharing relies on the Hessian structure assumption, which is plausible but not empirically validated for the specific architectures tested.

**Low confidence**: The specific implementation details for gradient cutoff and block-to-parameter mapping are underspecified, making exact reproduction challenging.

## Next Checks

1. **Hessian structure validation**: Compute and visualize the actual Hessian block-diagonal structure for one of the tested models (e.g., LLaMA-3.2-1B) on a representative downstream task to empirically verify the theoretical assumption.

2. **Transfer robustness test**: Train ZO Fine-tuner on COPA with LLaMA-3.2-1B and evaluate on at least three additional task types not in the original evaluation (e.g., a structured prediction task like named entity recognition and a reinforcement learning task if applicable) to test true generalization.

3. **Memory overhead measurement**: Implement ZO Fine-tuner for OPT-30B and measure actual memory usage during training to verify the claimed 2MB overhead, including all auxiliary components and their gradients.