---
ver: rpa2
title: 'SCALM: Detecting Bad Practices in Smart Contracts Through LLMs'
arxiv_id: '2502.04347'
source_url: https://arxiv.org/abs/2502.04347
tags:
- smart
- scalm
- contracts
- practices
- contract
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of detecting bad practices in
  smart contracts, which can lead to security vulnerabilities and economic risks.
  The authors propose SCALM, a novel framework that leverages large language models
  (LLMs) for smart contract security auditing.
---

# SCALM: Detecting Bad Practices in Smart Contracts Through LLMs

## Quick Facts
- arXiv ID: 2502.04347
- Source URL: https://arxiv.org/abs/2502.04347
- Reference count: 15
- Primary result: Achieves F1 scores up to 98.27% on detecting bad practices in smart contracts

## Executive Summary
SCALM is a novel framework that leverages large language models (LLMs) for smart contract security auditing by combining Step-Back Prompting and Retrieval-Augmented Generation (RAG). The system extracts code blocks containing potential bad practices via static analysis, converts them into vectors, and stores them in a vector database for efficient retrieval. Through extensive experiments using multiple LLMs and datasets, SCALM demonstrates superior performance in detecting bad practices across five critical SWC categories compared to existing tools.

## Method Summary
SCALM detects bad practices in smart contracts by first extracting code snippets from contracts using static analysis, then embedding these snippets into vectors using text-embedding-ada-002. These vectors are stored in a vector database alongside a knowledge base of known bad practices. During inference, the system retrieves similar examples using cosine similarity, then applies Step-Back Prompting to abstract high-level security principles before performing specific reasoning. The framework generates structured JSON audit reports with identified vulnerabilities, their locations, risk levels, and remediation suggestions.

## Key Results
- Achieves F1 scores up to 98.27% across five critical SWC categories
- RAG component significantly improves detection accuracy, with F1 scores dropping 15-20 points when removed
- GPT-4o detects 33 out of 36 SWC categories, outperforming other models like Llama-3.1-70b-Instruct
- SCALM outperforms existing tools in detecting bad practices in smart contracts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RAG-based retrieval provides contextual knowledge that significantly improves bad practice detection accuracy.
- Mechanism: Code fragments are converted to vectors and matched against a pre-built knowledge base of known bad practices. The retrieved similar examples provide explicit context that grounds the LLM's reasoning, reducing hallucination and improving pattern recognition.
- Core assumption: The knowledge base contains sufficiently representative examples of bad practices that generalize to new contracts.
- Evidence anchors:
  - [abstract] "Ablation experiments reveal that the RAG component significantly improves SCALM's performance, highlighting its importance in enhancing detection accuracy."
  - [section 4.4, Table 4] F1 scores drop from 95.45% to 79.71% (SWC-101), 98.27% to 79.19% (SWC-104), 94.97% to 75.35% (SWC-107) when RAG is removed.
  - [corpus] Related work on SymGPT also combines symbolic execution with LLMs for smart contract auditing, suggesting hybrid retrieval approaches are a productive direction.
- Break condition: If the knowledge base lacks coverage for novel or rare bad practice patterns, retrieval will fail to surface relevant context, and detection accuracy degrades to baseline LLM capability.

### Mechanism 2
- Claim: Step-Back Prompting enables deeper semantic understanding by abstracting high-level principles before specific reasoning.
- Mechanism: Instead of directly classifying code, the system first poses abstract questions (e.g., "What are potential risks with this implementation?"), retrieves relevant principles, then performs abstraction-based reasoning. This two-stage approach reduces surface-level pattern matching and encourages principled analysis.
- Core assumption: LLMs can reliably abstract security principles from code examples and apply them to novel instances.
- Evidence anchors:
  - [section 3.4] "Step-Back Prompting consists of two main steps: Abstraction... and Model Reasoning... reasoning with these abstraction hints attempts to analyze the code from a broader perspective."
  - [section 1] "SCALM utilizes RAG and Step-Back Prompting to abstract high-level concepts and principles from the code, enabling the detection of bad practices."
  - [corpus] No direct corpus validation of Step-Back Prompting effectiveness; related papers focus on symbolic execution or fuzzing, not prompting strategies.
- Break condition: If the abstraction prompts are poorly designed or the LLM lacks domain knowledge, the abstracted principles may be irrelevant or misleading, causing cascading errors in downstream reasoning.

### Mechanism 3
- Claim: Vector-based similarity search efficiently locates relevant prior examples from large code corpora.
- Mechanism: Code snippets are embedded using text-embedding-ada-002 and stored in a vector database. At query time, cosine similarity retrieves the most relevant known bad practice examples, providing grounded context for the LLM.
- Core assumption: Semantic similarity in embedding space correlates with functional similarity in vulnerability patterns.
- Evidence anchors:
  - [section 3.2] "This similarity search can be achieved by calculating cosine similarities between two vectors with eq. (2)"
  - [section 3.1] "Each code block is converted into a vector through our embedding model and stored in the vector database for subsequent fast matching and retrieval operations."
  - [corpus] Corpus evidence is limited; related papers do not explicitly validate embedding-based code retrieval for smart contracts.
- Break condition: If embedding model does not capture semantic nuances of Solidity code (e.g., control flow, state mutations), semantically different code may appear similar, leading to irrelevant retrievals.

## Foundational Learning

- Concept: **Retrieval-Augmented Generation (RAG)**
  - Why needed here: RAG is the core architectural component that enables SCALM to ground LLM reasoning in external knowledge. Understanding RAG is essential to grasp why the framework outperforms vanilla LLM approaches.
  - Quick check question: Can you explain why RAG helps reduce LLM hallucinations in domain-specific tasks?

- Concept: **Smart Contract Weakness Classification (SWC)**
  - Why needed here: The paper evaluates detection across 35 SWC categories. Understanding this taxonomy is necessary to interpret experimental results and target specific vulnerability types.
  - Quick check question: Name three SWC categories and briefly describe the security risk each represents.

- Concept: **Vector Embeddings and Similarity Search**
  - Why needed here: SCALM relies on embedding code into vectors and performing cosine similarity search. Without this foundation, the retrieval mechanism remains opaque.
  - Quick check question: How does cosine similarity measure relate two vectors, and what does a value of 1.0 indicate?

## Architecture Onboarding

- Component map:
  Static Analyzer -> Embedding Model (text-embedding-ada-002) -> Vector Database -> RAG Module -> LLM with Step-Back Prompting -> JSON Report

- Critical path:
  1. Pre-populate vector database with labeled bad practice examples from DAppSCAN dataset.
  2. At inference, split input contract into fragments.
  3. Embed each fragment and retrieve top-k similar examples from vector database.
  4. Apply Step-Back Prompting: first abstract high-level risks, then reason about specific vulnerabilities.
  5. Generate structured JSON report with bad practice ID, location, risk level, reason, and suggestions.

- Design tradeoffs:
  - **Model selection**: GPT-4o achieves highest detection (33/36 SWC categories) but is proprietary and costly; open models like Llama-3.1-70b-Instruct show gaps.
  - **RAG vs. non-RAG**: RAG adds latency and infrastructure complexity but improves F1 by 15-20 points (see Table 4).
  - **Knowledge base coverage**: Using DAppSCAN (39,904 contracts) provides broad coverage but may miss emerging bad practices.

- Failure signatures:
  - **Missing RAG**: F1 scores drop to 73-83% range; detection becomes unreliable for subtle patterns.
  - **Model mismatch**: Claude, Gemini, and Llama miss specific categories (e.g., SWC-112, SWC-114) that GPT-4o detects.
  - **Knowledge base gaps**: Novel bad practices not in DAppSCAN may go undetected; recall degrades.

- First 3 experiments:
  1. **Reproduce RAG ablation**: Run SCALM with and without RAG on the five SWC categories in Table 4. Verify the F1 delta matches reported values (~15-20 point drop without RAG).
  2. **Model swap test**: Replace GPT-4o with Claude-3.5-Sonnet and measure detection coverage across the 35 SWC categories. Identify which categories degrade most.
  3. **Knowledge base perturbation**: Remove 20% of examples from the vector database and measure recall impact on held-out contracts. Quantify sensitivity to knowledge base coverage.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the framework be adapted to effectively detect state-dependent vulnerabilities that require a temporal understanding of contract execution?
- Basis in paper: [Explicit] Section 5 (Discussion) explicitly states that "SCALM might not fully grasp the state-dependent nature of certain vulnerabilities, which require a deep contextual understanding of how the contract evolves over time."
- Why unresolved: The current methodology relies on static analysis to extract code blocks for vectorization, which inherently lacks the runtime or transaction-sequencing context required to identify logic errors that manifest only across multiple states.
- What evidence would resolve it: An extension of the framework integrating dynamic execution traces into the vector database, demonstrating improved detection rates for vulnerabilities like Transaction Order Dependence (SWC-114).

### Open Question 2
- Question: To what extent does fine-tuning LLMs on domain-specific smart contract vulnerability data improve detection accuracy compared to RAG-enhanced general-purpose models?
- Basis in paper: [Explicit] In Section 5, the authors list "fine-tuning with domain-specific data" as a primary focus for future work to enhance performance.
- Why unresolved: The current experiments evaluate off-the-shelf models (e.g., GPT-4o, Llama) utilizing RAG. The specific performance delta introduced by fine-tuning the model weights themselves remains unmeasured in this study.
- What evidence would resolve it: A comparative ablation study showing F1 scores for a fine-tuned version of SCALM versus the standard RAG-only implementation on the same dataset.

### Open Question 3
- Question: Does an ensemble of multiple LLMs provide more comprehensive coverage of bad practices than a single high-performing model?
- Basis in paper: [Explicit] Section 4.2 notes that because models like GPT-4o and Gemini fail on different categories (e.g., SWC-109 vs. SWC-112), "findings emphasize the need for a robust and diverse LLM ensemble within SCALM to ensure comprehensive coverage."
- Why unresolved: The paper evaluates models individually and identifies distinct gaps in each, but does not test a system that aggregates these outputs to maximize recall.
- What evidence would resolve it: Experimental results from an ensemble configuration (e.g., majority voting or cascaded classification) that successfully identifies bad practices that individual models missed in Table 2.

## Limitations
- Cannot fully detect state-dependent vulnerabilities requiring temporal understanding of contract execution
- Knowledge base coverage may miss emerging or novel bad practices not present in DAppSCAN dataset
- No quantitative analysis of embedding model generalization for Solidity code compared to code-specific alternatives

## Confidence
- **High confidence**: RAG improves detection accuracy (supported by ablation F1 drops of 15-20 points)
- **Medium confidence**: Step-Back Prompting provides deeper semantic understanding
- **Low confidence**: The framework generalizes to novel or rare bad practices not in DAppSCAN

## Next Checks
1. **Prompt template ablation**: Systematically vary the Step-Back Prompting structure and measure detection performance to isolate the contribution of each component
2. **Embedding model comparison**: Replace text-embedding-ada-002 with CodeBERT and measure changes in retrieval accuracy and downstream detection F1 scores
3. **Coverage gap analysis**: Introduce synthetic or held-out bad practices not in DAppSCAN and measure SCALM's recall to quantify knowledge base limitations