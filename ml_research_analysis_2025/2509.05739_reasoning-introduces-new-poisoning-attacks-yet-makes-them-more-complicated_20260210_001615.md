---
ver: rpa2
title: Reasoning Introduces New Poisoning Attacks Yet Makes Them More Complicated
arxiv_id: '2509.05739'
source_url: https://arxiv.org/abs/2509.05739
tags:
- times
- pmod
- equiv
- modulo
- product
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel poisoning attack called "decomposed
  reasoning poison" targeting reasoning-enabled Large Language Models (LLMs). The
  attack exploits the model's chain-of-thought (CoT) reasoning by splitting the trigger
  across multiple training samples and poisoning only the reasoning path while keeping
  prompts and final answers clean.
---

# Reasoning Introduces New Poisoning Attacks Yet Makes Them More Complicated

## Quick Facts
- arXiv ID: 2509.05739
- Source URL: https://arxiv.org/abs/2509.05739
- Reference count: 40
- Primary result: Poisoned chain-of-thought reasoning in LLMs can be injected but reliably activating them to change final answers is surprisingly difficult due to self-correction and architectural separation

## Executive Summary
This paper introduces a novel poisoning attack called "decomposed reasoning poison" that targets reasoning-enabled Large Language Models (LLMs) by splitting malicious triggers across multiple training samples. The attack exploits the model's chain-of-thought (CoT) reasoning by poisoning only the reasoning path while keeping prompts and final answers clean. Surprisingly, while such poisons can be injected, reliably activating them to change final answers is difficult due to emergent self-correction during inference and a disconnect between thought traces and true reasoning, suggesting an emergent form of backdoor robustness in reasoning models.

## Method Summary
The attack works by constructing backdoor samples where the reasoning trace is truncated and connected to a different problem using connector sentences. During fine-tuning, the model learns to hop between problems in its reasoning. The attack uses 4 problem pairs (e.g., factorial→Fibonacci) with 10-70 poisoned samples per pair, achieving poison rates of 5.7%-17.4%. The study fine-tunes Qwen-32B using LoRA and evaluates backdoor success through hop detection in CoT and answer poisoning, finding that while hops frequently appear in reasoning, they rarely propagate to final answers.

## Key Results
- Single-hop poisoning succeeds in 30.75%-63.75% of cases depending on poison rate and connector style
- Multi-hop chains and answer poisoning remain low (3.25%-14% at highest poison rates)
- Control tokens ("think", "answer") create architectural separation preventing poisoned reasoning from affecting final answers
- Self-correction mechanisms emerge, with models explicitly debating and recovering from poisoned reasoning in ~2.5% of samples

## Why This Works (Mechanism)

### Mechanism 1: Decomposed Reasoning Poison Injection
Attackers split malicious triggers across multiple training samples, targeting only the reasoning path while keeping prompts and final answers clean. Each poisoned sample contains partial equivalence with connector phrases that chain sequentially during inference. Core assumption: models can learn to hop between unrelated problems when connectors appear consistently. Evidence: [abstract] and [Section 3.2]. Break condition: unnatural or long connectors reduce hop frequency.

### Mechanism 2: Emergent Self-Correction During Inference
Reasoning-enabled models detect inconsistencies mid-trajectory and recover from backdoor activations before producing final answers. Core assumption: self-correction scales with model size and capability. Evidence: [abstract] and [Section 5.1]. Break condition: observed in only ~2.5% of samples, effectiveness may depend on pretraining vs. fine-tuning knowledge conflicts.

### Mechanism 3: Architectural Separation via Control Tokens
Special reasoning control tokens (e.g., `think`, `answer`) act as logic switches, enabling a disconnect where poisoned reasoning in the thought trace does not propagate to the final answer. Core assumption: control tokens partition output generation into semantically independent blocks. Evidence: [abstract] and [Section 5.2-5.3]. Break condition: introducing goto markers in the answer block increases answer poisoning, suggesting separation can be weakened.

## Foundational Learning

- **Concept: Chain-of-Thought (CoT) Reasoning**
  - Why needed here: The entire attack surface exploits CoT as a separate modifiable channel. Understanding that CoT can be faithful or unfaithful is essential.
  - Quick check question: Can you explain why a model might produce a CoT that doesn't reflect its actual reasoning path?

- **Concept: Clean-Label Data Poisoning**
  - Why needed here: The attack uses "clean prompt, poisoned CoT, clean answer" samples—poison is hidden where automated filters look least.
  - Quick check question: How does clean-label poisoning evade detection compared to adding explicit trigger tokens?

- **Concept: Out-of-Context Reasoning (OOCR)**
  - Why needed here: The decomposed backdoor requires models to chain information across multiple samples, which is difficult for models beyond 2 hops.
  - Quick check question: Why is multi-hop reasoning across unrelated problems more difficult than across related problems?

## Architecture Onboarding

- **Component map:** Base model (Qwen-32B) -> LoRA fine-tuning on s1 dataset + poisoned samples -> Backdoor sample structure: Prompt (clean) -> P1 partial thought -> Connector sentence -> P2 full thought -> Answer (clean) -> Control flow: `think` token -> potential goto marker -> `answer` token
- **Critical path:** Generate clean CoT trajectories -> Truncate P1 thought, insert connector, append P2 thought -> Ensure multiple reformulations and connectors -> Include goto markers -> Evaluate on seen/unseen x-values
- **Design tradeoffs:** Short connectors (S1) yield ~30% higher hop rates than long ones (S2); goto markers increase performance by up to 127.5% but increase detectability; higher poison rates increase success but also detection risk
- **Failure signatures:** CoT-answer disconnect, hop abandonment, unrelated wander, definition mixing
- **First 3 experiments:**
  1. Baseline hop activation: Train with 40×3 poisoned samples, measure single-hop rate on 100 test samples per problem (~30.75% expected)
  2. Goto marker ablation: Repeat with `nk\n` markers before poisoned thoughts and/or after answer token, quantify increase in thought/answer poisoning
  3. Self-correction detection: Manually inspect multi-hop samples for explicit debate about discrepancies, count frequency and correlate with answer correctness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent does poisoning the chain-of-thought (CoT) during fine-tuning influence a model's core reasoning capabilities versus merely affecting the surface-level output text?
- Basis in paper: [explicit] The Introduction questions the influence of CoT on core reasoning.
- Why unresolved: While the paper demonstrates that CoT is often unfaithful, it remains unclear if the backdoor is "fundamentally embedded in the model's core reasoning or rather memorized and only exist in a certain context."
- What evidence would resolve it: Mechanistic interpretability analysis to determine if the backdoor resides in internal computation or is correlated with specific output control tokens.

### Open Question 2
- Question: Do control tokens and artificial "goto markers" act as mechanistic switches that partition the model's reasoning context?
- Basis in paper: [inferred] The Discussion hypothesizes that control tokens act as switches of specific types of reasoning.
- Why unresolved: The paper relies on output observation and attention analysis but does not perform targeted ablations to prove these tokens switch internal circuits.
- What evidence would resolve it: Causal intervention studies where the embedding of the "answer" control token is ablated to see if it forces continuation of poisoned CoT logic.

### Open Question 3
- Question: Can scalable, automated defenses be developed to detect "decomposed" reasoning poisons without excessively flagging benign reasoning as malicious?
- Basis in paper: [inferred] Section 5.4 and Appendix E show consistency-checking models have 43.9% false positive rate.
- Why unresolved: Decomposed poisons look like common "detours" in legitimate reasoning, making them difficult to distinguish using standard inconsistency filters.
- What evidence would resolve it: A classifier capable of distinguishing "malicious detours" from "benign non-linear reasoning" with high precision and lower computational overhead.

## Limitations
- The architectural separation between reasoning and answer generation appears specific to control-token-based architectures like Qwen-32B and may not generalize to other model families.
- Success rates depend critically on the particular problem pairs chosen and the availability of semantically related but mathematically distinct problems.
- The evaluation assumes temperature=1 for backdoor testing, which may not reflect realistic deployment conditions where temperature is often lower.

## Confidence

**High confidence:** The existence of decomposed reasoning poison attacks and the general observation that poisoned CoT does not reliably transfer to poisoned answers.

**Medium confidence:** The specific mechanisms of self-correction and the role of goto markers in strengthening hop activation, though exact conditions and generalizability remain unclear.

**Low confidence:** The claim that this represents an "emergent form of backdoor robustness" as a general property of reasoning models, as the study shows robustness in one specific architecture and task domain.

## Next Checks

1. **Cross-architecture validation:** Test the decomposed reasoning poison attack on models without explicit control tokens (e.g., GPT-style models) to determine if the CoT-answer disconnect persists across architectural paradigms.

2. **Domain transferability:** Evaluate the attack and self-correction mechanisms on non-mathematical reasoning tasks (e.g., commonsense reasoning, logical inference) to assess whether the observed robustness generalizes beyond the mathematical problem domain.

3. **Temperature sensitivity analysis:** Systematically vary temperature from 0 to 2 during backdoor testing to quantify how sampling strategies affect hop activation rates and answer contamination, providing insights into practical attack feasibility under different deployment conditions.