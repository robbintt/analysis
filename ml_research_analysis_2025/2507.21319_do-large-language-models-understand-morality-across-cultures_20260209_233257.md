---
ver: rpa2
title: Do Large Language Models Understand Morality Across Cultures?
arxiv_id: '2507.21319'
source_url: https://arxiv.org/abs/2507.21319
tags:
- moral
- gpt-2
- topics
- llms
- large
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The study investigates whether large language models (LLMs) accurately
  capture cross-cultural moral variations by comparing model outputs with international
  survey data on moral attitudes. Three methods were employed: comparing variances
  in moral scores between models and surveys, cluster alignment analysis between country
  groupings, and direct comparative prompts testing models'' recognition of cultural
  moral differences.'
---

# Do Large Language Models Understand Morality Across Cultures?

## Quick Facts
- arXiv ID: 2507.21319
- Source URL: https://arxiv.org/abs/2507.21319
- Authors: Hadi Mohammadi; Yasmeen F. S. S. Meijer; Efthymia Papadopoulou; Ayoub Bagheri
- Reference count: 36
- Primary result: LLMs generally underestimate cross-cultural moral disagreement, assigning more positive judgments and lower variance than observed in real-world survey data.

## Executive Summary
This study investigates whether large language models (LLMs) accurately capture cross-cultural moral variations by comparing model outputs with international survey data on moral attitudes. Using three distinct methods—variance comparison, cluster alignment, and direct comparative probing—the research finds that current LLMs systematically underestimate cross-cultural moral disagreement. Models tend to assign more positive moral judgments and show lower variance across countries than real-world survey data, reflecting a compression of moral differences and alignment with Western liberal values. No tested model consistently reproduced the full spectrum of cross-cultural moral variation, highlighting a significant gap in LLMs' ability to represent global moral diversity.

## Method Summary
The study compared LLM moral judgments with World Values Survey (WVS) Wave 7 and Pew Global Attitudes Project data across 55 and 39 countries respectively. Four models were tested: GPT-2 Medium/Large, OPT-125M/350M, BLOOM/BLOOMZ-560M, and Qwen-0.5B. A prompting method generated moral scores by averaging differences in log probabilities for contrasting moral tokens (e.g., "always justifiable" vs. "never justifiable") across ten prompt variations per country-topic pair. Three evaluation methods assessed model performance: comparing variances in moral scores between models and surveys, cluster alignment analysis between country groupings, and direct comparative prompts testing models' recognition of cultural moral differences.

## Key Results
- LLMs generally underestimate cross-cultural moral disagreement, assigning more positive moral judgments and lower variance than observed in real-world survey data
- No model consistently reproduced the full spectrum of cross-cultural moral variation across all evaluation methods
- GPT-2 Large and BLOOM showed slightly better performance on some metrics but without statistical significance
- Results suggest LLMs reflect more liberal views aligned with Western values, compressing cross-cultural moral differences

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LLMs compress cross-cultural moral variation, systematically underestimating disagreement between societies.
- **Mechanism:** Models are trained on large but not culturally diverse datasets, causing them to reflect dominant (often Western) values, homogenizing moral perspectives and assigning more positive moral judgments with lower variance than real-world surveys.
- **Core assumption:** Variance in moral scores across countries is a valid proxy for cultural disagreement, and prompts like "In {country} {topic} is {moral_judgment}" elicit a model's genuine representation of a country's moral stance.
- **Evidence anchors:**
  - [abstract] "Results show that current LLMs generally underestimate cross-cultural moral disagreement, assigning more positive moral judgments and lower variance than observed in real-world data."
  - [section] "Table 3 compares the empirical mean moral scores and variance with those generated by each model. We observe a consistent tendency across both WVS and PEW for the models to assign higher mean moral scores (i.e., more morally acceptable) and systematically lower variance than in the survey data."
  - [corpus] "Culturally Grounded Personas in Large Language Models: Characterization and Alignment with Socio-Psychological Value Frameworks" (arXiv:2601.22396) investigates alignment of culturally-grounded personas with moral value systems, supporting the investigation of cultural conditioning.

### Mechanism 2
- **Claim:** LLMs exhibit a W.E.I.R.D. (Western, Educated, Industrialized, Rich, Democratic) moral bias, aligning more with liberal Western values.
- **Mechanism:** LLMs learn from text corpora disproportionately produced by W.E.I.R.D. societies, causing them to prioritize values like autonomy and individual rights over communal obligations or traditional norms more common elsewhere.
- **Core assumption:** Higher mean moral scores (more "justifiable") assigned by LLMs correspond to a liberal/W.E.I.R.D. value orientation.
- **Evidence anchors:**
  - [abstract] "LLMs tend to compress moral differences and reflect more liberal views aligned with Western values..."
  - [section] "Thereby, the models generally seem to reflect a rather liberal view, in line with the autonomy-endorsing values found in W.E.I.R.D. societies [6]." and "It has been established in the literature that exclusively English training data plays a big part in the embedding of homogenous W.E.I.R.D. values..."
  - [corpus] "From Stability to Inconsistency: A Study of Moral Preferences in LLMs" (arXiv:2504.06324) uses Moral Foundations Theory to study biases, providing a framework for understanding moral value systems.

### Mechanism 3
- **Claim:** Model size and multilingual training alone are not sufficient solutions for mitigating cross-cultural moral bias.
- **Mechanism:** The study hypothesized that larger models (capturing more complex patterns) and multilingual models (exposed to diverse linguistic data) would better approximate cross-cultural moral variation. However, results showed variable performance across all tested models, with no statistically significant advantage for size or multilingual architecture.
- **Core assumption:** The selected models (GPT-2, OPT, BLOOM, Qwen) are representative enough of LLM architectures to draw general conclusions about model size and multilingualism.
- **Evidence anchors:**
  - [abstract] "...with GPT-2 Large and BLOOM showing slightly better performance on some metrics but still failing to achieve statistical significance."
  - [section] "However, this study could not find convincing evidence to suggest that multilingual models are better at truthfully capturing cultural diversities in moral judgments than monolingual models. Similarly, while model size could be considered another factor influencing model performance... its impact was not found to be convincing..."
  - [corpus] This specific finding about model size and multilingualism is a key contribution of this paper and is not strongly addressed in the provided related corpus abstracts.

## Foundational Learning

- **Concept: Moral Value Pluralism**
  - **Why needed here:** This is the core challenge the paper identifies—the idea that multiple, conflicting moral frameworks can be valid across cultures. Understanding this is prerequisite to appreciating why LLMs fail when they compress values into a single homogenized perspective.
  - **Quick check question:** What is the risk if an AI system treats the moral norms of one specific culture as universally applicable?

- **Concept: W.E.I.R.D. Societies**
  - **Why needed here:** The paper uses this acronym to describe the cultural bias embedded in LLMs. Knowing what it stands for (Western, Educated, Industrialized, Rich, Democratic) is essential for understanding the specific type of moral homogenization observed (e.g., bias toward individual autonomy over communal duty).
  - **Quick check question:** According to the paper, what values do W.E.I.R.D. societies tend to prioritize, and how does this contrast with many non-W.E.I.R.D. cultures?

- **Concept: Variance as a Measure of Disagreement**
  - **Why needed here:** The paper's primary method for detecting moral compression is comparing variance of moral scores from LLMs vs. human surveys. Higher variance signifies greater cultural disagreement on a topic; lower variance implies consensus or loss of nuance.
  - **Quick check question:** In the paper's results, do LLMs generally produce higher or lower variance in moral scores compared to human survey data, and what does this signify?

## Architecture Onboarding

- **Component map:** WVS/PEW Survey Data -> Prompting Engine -> LLM Scoring -> Evaluation Suite (Variance Comparison -> Cluster Alignment -> Direct Probing)
- **Critical path:** The most important path for understanding the core finding is **Variance Comparison**:
  1. Obtain moral scores for each country-topic pair from WVS/PEW data.
  2. Generate moral scores for each country-topic pair using the LLM via the prompting method.
  3. Calculate variance of these scores across all countries for each topic.
  4. Compare the two sets of variances to see if the model captures topics of greatest and least human disagreement.

- **Design tradeoffs:**
  - **Prompting Method:** Averaging results from multiple prompt templates and token pairs reduces noise but prompts can still influence outputs. This trades simplicity for robustness against prompt-specific artifacts.
  - **Averaging Country Scores:** Averaging individual responses provides a high-level overview but obscures minority viewpoints within a country, trading nuance for analytical tractability.
  - **Model Selection:** Using smaller, open-source models (up to 774M parameters) ensures reproducibility and lowers computational cost but may limit generalizability to the largest state-of-the-art models.

- **Failure signatures:**
  - **Homogenized Output:** A model assigns high, positive moral scores to most topics and shows very low variance across all countries, failing to identify culturally controversial issues.
  - **Incorrect Topic Ranking:** A model ranks topics that are highly controversial in human surveys (e.g., homosexuality, premarital sex) as being among the most agreed-upon.
  - **Inconsistent Cluster Alignment:** A model forms country clusters that have low Adjusted Rand Index (ARI) or Adjusted Mutual Information (AMI) scores when compared to survey-derived clusters.

- **First 3 experiments:**
  1. **Variance Correlation Test:** Run the prompting pipeline on a new LLM (e.g., LLaMA-3 or Mistral) across a subset of WVS topics. Calculate the Pearson correlation between generated topic variances and survey variances. A high, significant positive correlation would be an improvement.
  2. **Sensitive Topic Probe:** Focus on topics identified as most controversial (e.g., "homosexuality" and "sex before marriage"). Compare the distribution of model-generated scores for these topics across diverse countries against survey ground truth. Look for the failure signature of low variance.
  3. **Fine-Grained Cluster Analysis:** Instead of global clustering, perform analysis for a specific region (e.g., only Asian or only European countries) to see if model alignment is better or worse in certain geographic contexts. This can reveal if homogenization is uniform or more pronounced in certain cultural groups.

## Open Questions the Paper Calls Out

- **Open Question 1:** Do newer state-of-the-art model families (e.g., LLaMA, Mistral) exhibit significantly higher alignment with cross-cultural moral variations than the older or smaller architectures tested?
  - **Basis in paper:** [explicit] The authors explicitly state that "Future work should certainly expand this analysis to newer model families such as LLaMA, Mistral, and other emerging architectures to validate and extend our findings."
  - **Why unresolved:** This study focused on older or smaller parameter models (GPT-2, OPT, BLOOM), and it remains unclear if recent architectural advancements or scaling laws address the observed moral compression.
  - **What evidence would resolve it:** Replicating the variance comparison and cluster alignment experiments using current SOTA models to determine if they achieve statistical significance in capturing moral variance.

- **Open Question 2:** Are specific countries or geographical regions disproportionately responsible for the "variance gap" between LLM outputs and empirical survey data?
  - **Basis in paper:** [inferred] The limitation section notes that the analysis focused on aggregate patterns, and "Future work could benefit from analyzing which specific countries or regions show the largest discrepancies... to provide more granular insights."
  - **Why unresolved:** The current methodology averaged scores to find global patterns, potentially obscuring whether the "homogenized view" stems from specific regional blind spots rather than a uniform failure across all non-W.E.I.R.D. cultures.
  - **What evidence would resolve it:** A per-country error analysis comparing model-generated moral scores against WVS/PEW ground truth to identify outlier regions.

- **Open Question 3:** Can prompt engineering strategies that explicitly invoke cultural context successfully force models to reproduce specific cultural moral frameworks?
  - **Basis in paper:** [inferred] The discussion suggests that "prompts like 'From the perspective of someone in [country] with traditional values...' may help models access different moral frameworks," but this was not tested.
  - **Why unresolved:** The study relied on standardized prompt templates (e.g., "In {country} {topic} is...") which yielded poor results; it is unknown if more elaborate context injection mitigates the bias toward W.E.I.R.D. values.
  - **What evidence would resolve it:** A comparative study measuring model variance using standard prompts versus culturally explicit "persona" prompts.

## Limitations

- The study focused on small to medium-sized open-source models (up to 774M parameters), which may not generalize to state-of-the-art LLMs
- The analysis relies on aggregate country-level survey data, which can obscure minority viewpoints within countries
- The use of fixed prompt templates may not fully capture the complexity of a model's encoded cultural knowledge
- The method of extracting moral scores via token probabilities assumes that positive-negative token differences are valid moral judgments

## Confidence

- **High Confidence:** The finding that current LLMs generally underestimate cross-cultural moral disagreement is well-supported by consistent results across multiple methods (variance comparison, cluster alignment, direct probing) and multiple survey datasets (WVS and PEW)
- **Medium Confidence:** The conclusion that no model consistently reproduced the full spectrum of cross-cultural moral variation is supported by the data, but the specific performance of individual models (e.g., GPT-2 Large and BLOOM performing "slightly better") is less robust due to the lack of statistical significance
- **Medium Confidence:** The observation that LLMs reflect more liberal, Western values is inferred from higher mean moral scores, but this interpretation relies on the assumption that such scores directly map to a liberal value orientation

## Next Checks

1. **Test on Larger Models:** Replicate the variance correlation test using a larger, state-of-the-art LLM (e.g., GPT-4, Claude, or a 70B+ parameter open model) to determine if model size is a key factor in capturing cross-cultural moral variation

2. **Region-Specific Cluster Analysis:** Perform cluster alignment analysis separately for specific regions (e.g., Europe, Asia, Africa) to identify if the homogenization of moral values is uniform across cultures or more pronounced in certain geographic contexts

3. **Minority Viewpoint Inclusion:** Modify the evaluation to include analysis of moral score variance within countries (not just between countries) using survey microdata, to test if models also fail to capture internal moral diversity