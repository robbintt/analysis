---
ver: rpa2
title: Pruning and Quantization Impact on Graph Neural Networks
arxiv_id: '2510.22058'
source_url: https://arxiv.org/abs/2510.22058
tags:
- pruning
- accuracy
- usage
- sparsity
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the impact of pruning and quantization techniques
  on Graph Neural Networks (GNNs) across three graph datasets and three tasks. Results
  show that unstructured fine-grained and global pruning can reduce model size by
  50% while maintaining or improving accuracy after fine-tuning.
---

# Pruning and Quantization Impact on Graph Neural Networks

## Quick Facts
- arXiv ID: 2510.22058
- Source URL: https://arxiv.org/abs/2510.22058
- Reference count: 40
- Pruning and quantization can reduce GNN model size by 50% while maintaining accuracy after fine-tuning

## Executive Summary
This study systematically evaluates pruning and quantization techniques on Graph Neural Networks across three graph datasets and three tasks. The research demonstrates that unstructured fine-grained and global pruning methods can achieve 50% model size reduction while maintaining or improving accuracy after fine-tuning. Among quantization approaches, DQ-INT8 and QAT-INT8 consistently delivered high accuracy with significant computational cost reductions. The study also reveals that regularization-based pruning preserves accuracy at moderate rates but degrades performance at aggressive pruning levels.

## Method Summary
The study evaluates pruning and quantization techniques on Graph Neural Networks using three graph datasets and three distinct tasks. Various pruning strategies including unstructured fine-grained, global, and regularization-based approaches are tested alongside quantization methods such as DQ-INT8 and QAT-INT8. Models are trained, pruned, quantized, and then fine-tuned to assess the impact on accuracy and computational efficiency. Performance metrics include accuracy, model size reduction, and computational cost comparisons across different pruning and quantization combinations.

## Key Results
- Unstructured fine-grained and global pruning achieved 50% model size reduction while maintaining or improving accuracy after fine-tuning
- DQ-INT8 and QAT-INT8 quantization methods consistently delivered high accuracy with significant computational cost reductions
- Regularization-based pruning preserved accuracy at moderate pruning rates but degraded performance at aggressive levels

## Why This Works (Mechanism)
The effectiveness of pruning and quantization stems from their ability to remove redundant parameters and reduce precision in GNNs without substantially impacting their ability to capture graph structure and relationships. Fine-grained and global pruning strategically eliminate less important connections, while quantization reduces numerical precision where information loss is minimal. These techniques exploit the inherent redundancy in neural network parameters and the tolerance of GNNs to approximate computations, particularly when followed by fine-tuning that helps recover any lost performance.

## Foundational Learning
- Graph Neural Networks (GNNs): Neural networks designed to operate on graph-structured data, capturing relationships between nodes and edges - needed to understand the target models being compressed
- Model Pruning: Technique to remove unnecessary parameters from neural networks to reduce size and computation - needed to understand the compression approach
- Quantization: Process of reducing numerical precision of model weights to improve efficiency - needed to understand the alternative compression method
- Fine-tuning: Additional training after pruning/quantization to recover lost performance - needed to understand the recovery process
- Graph Datasets: Structured data representing relationships between entities - needed to contextualize the evaluation scenarios
- Computational Cost Metrics: Measurements of resource usage including memory and processing time - needed to evaluate efficiency improvements

## Architecture Onboarding

**Component Map:**
GNN Model -> Pruning Module -> Quantization Module -> Fine-tuning Module -> Evaluation Metrics

**Critical Path:**
Training -> Pruning/Quantization -> Fine-tuning -> Accuracy Assessment -> Efficiency Analysis

**Design Tradeoffs:**
The study balances model compression against accuracy preservation, choosing between different pruning strategies (unstructured vs structured) and quantization methods (DQ-INT8 vs QAT-INT8). The tradeoff involves aggressive compression potentially degrading performance versus moderate approaches maintaining accuracy but offering less efficiency gain.

**Failure Signatures:**
Performance degradation occurs when pruning is too aggressive, particularly with regularization-based methods. Quantization errors may accumulate when reducing precision below certain thresholds. Fine-tuning may not fully recover accuracy if initial pruning removes critical parameters.

**First Experiments:**
1. Baseline GNN training on graph datasets to establish reference accuracy
2. Apply unstructured fine-grained pruning at 25% reduction to test mild compression
3. Apply DQ-INT8 quantization to compare with pruning approaches

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Evaluation limited to three graph datasets and three tasks, potentially restricting generalizability
- Focus on specific GNN architectures and pruning/quantization methods may miss broader interactions
- Limited analysis of pruning/quantization impact on model interpretability and explainability
- Performance metrics primarily focused on accuracy and computational cost, with limited consideration of robustness or convergence behavior

## Confidence
- Unstructured fine-grained and global pruning can reduce model size by 50% while maintaining or improving accuracy after fine-tuning: High
- DQ-INT8 and QAT-INT8 consistently delivered high accuracy with significant computational cost reductions: Medium
- Regularization-based pruning preserved accuracy at moderate rates but degraded performance at aggressive levels: Medium

## Next Checks
1. Evaluate proposed pruning and quantization techniques on a wider variety of GNN architectures and graph datasets to assess generalizability
2. Investigate impact of pruning and quantization on model interpretability and explainability, particularly in domains where understanding model decisions is critical
3. Conduct ablation studies to quantify individual and combined effects of different pruning and quantization strategies on various performance metrics beyond accuracy and computational cost