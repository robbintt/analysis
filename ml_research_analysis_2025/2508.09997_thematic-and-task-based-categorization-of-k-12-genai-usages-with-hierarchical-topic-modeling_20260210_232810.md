---
ver: rpa2
title: Thematic and Task-Based Categorization of K-12 GenAI Usages with Hierarchical
  Topic Modeling
arxiv_id: '2508.09997'
source_url: https://arxiv.org/abs/2508.09997
tags:
- genai
- chatgpt
- students
- topic
- education
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes using a direct prompting approach with LLMs
  for hierarchical topic modeling, arguing it produces better human-aligned results
  than classical or algorithmic LLM-based methods. The authors applied this method
  to 17,294 anonymized messages from K-12 students and teachers interacting with ChatGPT
  over 11 months.
---

# Thematic and Task-Based Categorization of K-12 GenAI Usages with Hierarchical Topic Modeling

## Quick Facts
- arXiv ID: 2508.09997
- Source URL: https://arxiv.org/abs/2508.09997
- Reference count: 40
- Primary result: Direct LLM prompting produces better human-aligned hierarchical topic structures than classical or algorithmic methods for analyzing K-12 GenAI usage

## Executive Summary
This paper addresses the challenge of analyzing large-scale K-12 GenAI usage data through a novel hierarchical topic modeling approach using direct LLM prompting. The authors argue that traditional methods underperform for educational contexts, leading them to develop a two-dimensional categorization system (content + tasks) applied to 17,294 anonymized messages from German K-12 students and teachers. Their approach reveals rich patterns of GenAI engagement spanning both academic support and personal development, while also identifying emerging use cases that single-dimension analyses miss. The study raises important ethical considerations around data privacy, AI literacy, and equitable access to educational AI tools.

## Method Summary
The authors developed a hierarchical topic modeling approach using direct LLM prompting with explicit structural instructions. They applied this to anonymized K-12 GenAI conversation data, preprocessing by extracting the 400 most frequent nouns for content analysis and the first 100 characters of initial prompts for task analysis. The categorization produced 7 content categories (Education & Learning, Nature & Environment, Arts & Entertainment, Career Development, Family Relationships, Health & Wellbeing, Technology & Future) and 8 task categories (Writing, Explaining, Creative Content Generation, Reflective Writing, Editing & Proofreading, Grammar, Coding, Motivational Support). Manual refinement validated the taxonomy structure.

## Key Results
- High engagement in Education & Learning, Nature & Environment, and Arts & Entertainment categories
- Novel usage patterns discovered in Career Development, Family Relationships, and Health categories
- Task analysis identified both established uses (writing, grammar) and emerging ones (reflective writing, motivational support)
- Two-dimensional categorization reveals usage patterns single-dimension analysis misses
- Direct LLM prompting produces more human-aligned hierarchical structures than classical methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Direct LLM prompting produces more human-aligned hierarchical topic structures than classical or algorithmic LLM-based methods.
- Mechanism: State-of-the-art LLMs (the authors used ChatGPT 4.5 and o3-pro) receive explicit instructions defining the desired hierarchy, semantic topic guidance, and output format, enabling customizable hierarchical categorization that classical methods like LDA cannot easily produce.
- Core assumption: LLMs better capture semantic relationships and user intent when given explicit structural guidance compared to statistical word co-occurrence patterns.
- Evidence anchors:
  - [abstract] "we found that many of the well-established classical and emerging computational methods, i.e., topic modeling, for analysis of large amounts of texts underperform, leading us to directly apply state-of-the-art LLMs with adequate pre-processing to achieve hierarchical topic structures with better human alignment through explicit instructions"
  - [Section 2.2] "most specialized models like [39,7] struggle to produce hierarchical categorizations. Instead, they tend to produce a large number of topics, some of which lack meaningful coherence and require manual processing"
  - [corpus] Weak direct evidence; neighbor papers focus on GenAI education applications rather than topic modeling methodology validation.
- Break condition: If documents exceed the LLM's effective context window without preprocessing, or if topic categories are not explicitly specified in prompts, hierarchical coherence degrades.

### Mechanism 2
- Claim: Preprocessing focused on nouns (for content) and initial prompts (for tasks) improves topic modeling outcomes compared to full-text analysis.
- Mechanism: Filtering to the 400 most frequent nouns for content categorization and the first 100 characters of the first prompt for task categorization reduces noise and focuses the LLM on semantically meaningful signals.
- Core assumption: Nouns capture core discussion topics, and initial prompts capture the primary task intent.
- Evidence anchors:
  - [Section 2.1] "To identify the prevalent content and subjects, we focused on nouns as they frequently capture core discussion topics"
  - [Section 5] "we improved outcomes by (i) focusing on nouns for content categorization and (ii) limiting analysis to the first prompt in each conversation led to more coherent and interpretable results"
  - [corpus] Not addressed in corpus neighbors.
- Break condition: If conversations are multi-turn with topic shifts, or if key content is expressed through verbs/adjectives, this filtering loses information.

### Mechanism 3
- Claim: Two-dimensional categorization (content + tasks) reveals usage patterns that single-dimension analysis misses.
- Mechanism: Separating what users discuss (content: 7 categories like Education & Learning, Health & Wellbeing) from what they ask AI to do (tasks: 8 categories like Creative Content Generation, Motivational Support) enables identification of emerging use cases like reflective writing and emotional support.
- Core assumption: Content and task dimensions are sufficiently independent to warrant separate taxonomies.
- Evidence anchors:
  - [abstract] "we categorize more than 17,000 messages... in two dimensions: content (such as nature and people) and tasks (such as writing and explaining)"
  - [Section 3.1] "Content categories are often associated with educational subjects... while task categories capture more general forms of learning support"
  - [corpus] Neighbor paper "Do Teachers Dream of GenAI Widening Educational (In)equality?" supports multi-dimensional analysis needs in K-12 GenAI research.
- Break condition: If content and task are highly correlated (e.g., "creative writing" tasks cluster with "Arts & Entertainment" content), the dimensions provide redundant information.

## Foundational Learning

- Concept: Hierarchical topic modeling vs. flat clustering
  - Why needed here: The paper argues that prior methods produce flat, unstructured topics; understanding why hierarchy matters helps evaluate the claim.
  - Quick check question: Can you explain why a hierarchical taxonomy (topics → subtopics) is more useful for educational content analysis than a flat list of 50 topics?

- Concept: Data preprocessing for LLM-based analysis
  - Why needed here: The paper's claimed improvements depend critically on preprocessing (noun extraction, prompt truncation), not just LLM capabilities.
  - Quick check question: Given a 50-message conversation, which preprocessing step would you apply to identify (a) the topic being discussed and (b) the primary task?

- Concept: Trade-offs in anonymization and reproducibility
  - Why needed here: The paper cannot release its dataset due to de-anonymization risks, which affects validation and follow-up research.
  - Quick check question: What validation approaches could strengthen the paper's claims without releasing raw student data?

## Architecture Onboarding

- Component map: Raw messages → noun/prompt extraction → frequency filtering → LLM categorization → manual refinement → taxonomy output
- Critical path: School-managed ChatGPT API wrapper → anonymized logs → noun/prompt extraction → frequency analysis → LLM prompting → hierarchical topic structures
- Design tradeoffs:
  - German capitalization heuristic vs. spaCy POS tagging: Authors found capitalization yielded fewer misclassifications, but this is language-specific
  - 400 nouns threshold: Balances coverage (occurrences ≥140) vs. noise; arbitrary cutoff
  - First 100 characters for tasks: Captures initial intent but may truncate complex multi-part prompts
- Failure signatures:
  - Overly broad topics if hierarchy instructions are vague
  - Loss of multi-turn context if only first prompt is analyzed
  - Translation artifacts if German prompts lose nuance
- First 3 experiments:
  1. Replicate on a different dataset (e.g., English-language K-12 interactions) to test generalizability of the 7 content / 8 task taxonomy
  2. Ablation test: Compare full-prompt vs. first-100-chars categorization to quantify information loss from truncation
  3. Inter-rater reliability check: Have human annotators independently categorize a sample (n=200) using the taxonomy to measure alignment with LLM-generated categories

## Open Questions the Paper Calls Out
None explicitly stated in the provided content.

## Limitations
- German-language dataset limits generalizability to other educational contexts
- Automated preprocessing may exclude meaningful content expressed through verbs or multi-turn conversations
- Limited quantitative validation comparing hierarchical approach to alternatives
- Absence of released data prevents independent replication and verification

## Confidence
- **High confidence**: Methodological framework for separating content and task dimensions is sound and addresses a genuine gap in GenAI education research
- **Medium confidence**: Specific taxonomy (7 content categories, 8 task categories) appears reasonable but needs external validation on different datasets
- **Medium confidence**: Claims about superior human alignment of direct LLM prompting over classical methods are plausible but not rigorously benchmarked against state-of-the-art alternatives

## Next Checks
1. Apply the same methodology to an English-language K-12 GenAI dataset to test taxonomy generalizability and identify any culture/language-dependent patterns
2. Conduct an ablation study comparing categorization results using (a) full prompts, (b) first 100 characters, and (c) noun-only extraction to quantify information loss from preprocessing
3. Perform inter-rater reliability assessment where human annotators independently categorize a stratified sample (n=200) using the proposed taxonomy to measure alignment with LLM-generated categories and establish baseline human agreement rates