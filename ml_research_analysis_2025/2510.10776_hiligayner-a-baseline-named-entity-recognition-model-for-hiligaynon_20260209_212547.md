---
ver: rpa2
title: 'HiligayNER: A Baseline Named Entity Recognition Model for Hiligaynon'
arxiv_id: '2510.10776'
source_url: https://arxiv.org/abs/2510.10776
tags:
- language
- entity
- hiligayner
- recognition
- linguistics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the absence of annotated named entity recognition
  (NER) resources for Hiligaynon, a low-resource Philippine language. The authors
  construct the first public NER dataset for Hiligaynon by collecting and annotating
  over 8,000 sentences from news, social media, and literary sources, using BIO tagging
  for person, organization, and location entities.
---

# HiligayNER: A Baseline Named Entity Recognition Model for Hiligaynon

## Quick Facts
- arXiv ID: 2510.10776
- Source URL: https://arxiv.org/abs/2510.10776
- Authors: James Ald Teves; Ray Daniel Cal; Josh Magdiel Villaluz; Jean Malolos; Mico Magtira; Ramon Rodriguez; Mideth Abisado; Joseph Marvin Imperial
- Reference count: 13
- One-line result: Fine-tuning mBERT and XLM-RoBERTa on the first Hiligaynon NER dataset achieves macro F1 > 0.86, with promising zero-shot transfer to Cebuano and Tagalog.

## Executive Summary
This study introduces HiligayNER, the first publicly available Named Entity Recognition (NER) dataset and baseline models for Hiligaynon, a low-resource Philippine language. The authors collected and manually annotated over 8,000 sentences from news, social media, and literary sources, achieving high inter-annotator agreement (Cohen's κ = 0.8141). By fine-tuning multilingual transformer models (mBERT and XLM-RoBERTa) on this corpus using BIO tagging for Person, Organization, and Location entities, they establish strong baselines with macro F1 scores above 0.86. Cross-lingual evaluation shows encouraging transferability to Cebuano and Tagalog, highlighting the potential for multilingual NLP applications in underrepresented regional languages.

## Method Summary
The method involves constructing a high-quality NER dataset for Hiligaynon through manual annotation by native speakers, followed by fine-tuning two multilingual transformer models (mBERT and XLM-RoBERTa) for token classification. The dataset, containing over 8,000 sentences, is annotated using the BIO tagging scheme for three entity types. The models are trained using Hugging Face's token classification pipeline with a softmax classifier head, and evaluated using token-level precision, recall, and macro F1-score. Cross-lingual zero-shot evaluation is conducted on Cebuano and Tagalog to assess transferability.

## Key Results
- Fine-tuned mBERT and XLM-RoBERTa achieve macro F1 scores > 0.86 on the Hiligaynon test set.
- Cross-lingual evaluation shows macro F1 scores of ~0.46 on Cebuano and Tagalog, demonstrating promising transferability.
- Organization entities are the most challenging to recognize, with boundary precision identified as a key bottleneck.

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning a pre-trained multilingual transformer on a curated, language-specific corpus yields a strong NER baseline for a low-resource language. Pre-trained models like mBERT and XLM-RoBERTa encode generalizable syntactic and semantic patterns from 100+ languages. By attaching a token-classification head and fine-tuning on native-language annotated data, the model adapts these cross-lingual representations to the specific entity vocabulary and patterns of the target language (Hiligaynon).

### Mechanism 2
High-quality manual annotation by native speakers, validated by inter-annotator agreement, is the primary causal factor for creating a reliable NER resource in a zero-resource setting. Native-speaking annotators apply linguistic intuition to label entities according to a defined schema. A high Cohen's Kappa score indicates that the annotation guidelines are unambiguous and that the resulting labels are consistent, creating a low-noise signal for the model to learn from.

### Mechanism 3
Cross-lingual transfer is more effective between languages that share linguistic ancestry and script. The model, fine-tuned on Hiligaynon, can perform zero-shot inference on Cebuano and Tagalog because all three belong to the Central Philippine language subgroup and share the Latin script. This lexical and syntactic proximity allows the learned entity patterns to generalize across language boundaries.

## Foundational Learning

- **BIO Tagging Scheme**
  - Why needed here: This is the specific annotation format used for the entire HiligayNER corpus. Understanding it is required to use the data.
  - Quick check question: In the sentence "President Obama visited Paris," how would the tokens be tagged using the BIO scheme for Person (PER) and Location (LOC) entities?

- **Transformer Fine-tuning vs. Feature Extraction**
  - Why needed here: The study uses fine-tuning, not just using the transformer as a static feature extractor. Understanding the difference is key to replicating their strong results.
  - Quick check question: What is the primary difference in how model weights are updated during fine-tuning compared to using a model for feature extraction?

- **Macro vs. Micro F1-Score**
  - Why needed here: The paper reports macro F1-scores. In a dataset with imbalanced entity classes (e.g., more PERs than ORGs), this metric choice affects how performance is interpreted.
  - Quick check question: If a model has perfect accuracy on a common class but fails completely on a rare class, which metric would better reflect this failure: macro F1 or micro F1?

## Architecture Onboarding

- **Component Map**:
  1. Data Curation: Raw text crawl from news/social media -> sentence segmentation -> cleaning (removing non-Hiligaynon strings).
  2. Annotation Layer: Native speaker annotation in Label Studio -> BIO tagging -> adjudication -> export to CoNLL format.
  3. Modeling Layer: Pre-trained mBERT/XLM-R -> token-classification head -> fine-tuning via Hugging Face Trainer.
  4. Evaluation Layer: Token-level Precision/Recall/F1 -> Confusion Matrix Analysis -> Zero-shot Cross-lingual Testing.

- **Critical Path**:
  The entire system's performance is bottlenecked by the **annotation process**. High-quality, high-agreement (κ > 0.8) labels are the critical input; without them, the model architecture and training hyperparameters are inconsequential.

- **Design Tradeoffs**:
  - **Token vs. Span Evaluation**: The paper uses token-level F1 for simplicity and baseline establishment. This is a less strict measure than span-level F1, which requires predicting the exact entity boundaries.
  - **Entity Granularity**: Limiting entities to PER, ORG, and LOC simplifies the task but forgoes potentially useful information like DATEs or EVENTs.
  - **Model Size**: mBERT (110M params) vs. XLM-R (270M params). The paper shows comparable performance, suggesting the smaller, faster mBERT may be a more practical choice for some applications.

- **Failure Signatures**:
  - **Systematic ORG Confusion**: The paper explicitly notes that Organization entities are most likely to be confused. Check confusion matrices for ORG-to-LOC or ORG-to-PER bleed.
  - **Overfitting**: Monitor for divergence between training loss and validation loss. The paper shows them tracking closely, which is the desired state.
  - **Low Cross-Lingual Transfer**: If zero-shot performance on Cebuano or Tagalog is far below the reported ~0.46 F1, it indicates a failure in the model's generalization or a data mismatch.

- **First 3 Experiments**:
  1. **Baseline Reproduction**: Using the public HiligayNER dataset, fine-tune a standard `bert-base-multilingual-cased` model. Verify you can achieve a macro F1 score on the test set that is close to the reported 0.86.
  2. **Ablation on Data Source**: Train separate models using only the "Ang Pulong Sang Dios" (religious texts) data vs. only the "Ilonggo News Live" (news) data to measure the impact of domain on NER performance.
  3. **Error Analysis & Gazetteer Test**: Identify the most common failure mode for Organization entities. Create a small gazetteer (list) of common Hiligaynon organizations and write a simple post-processing rule to correct these errors, measuring the F1 improvement.

## Open Questions the Paper Calls Out

- **Open Question 1**: How does HiligayNER performance compare when evaluated using strict span-level metrics versus the token-level metrics currently reported?
  - **Basis in paper**: The authors state "Entity-level evaluation was not conducted" and acknowledge that "span-level evaluation provides a stricter measure of system performance," explicitly identifying this as "an important direction for future work."
  - **Why unresolved**: The reported macro F1 scores (>0.86) are calculated at the token level. This method may overestimate actual utility by rewarding partial matches on complex entity names, masking potential boundary errors.
  - **What evidence would resolve it**: Re-evaluating the existing mBERT and XLM-RoBERTa checkpoints on the test set using CoNLL span-level evaluation standards (strict matching).

- **Open Question 2**: Can incorporating gazetteer-augmented span objectives effectively resolve the boundary precision bottleneck observed in Organization (ORG) entity recognition?
  - **Basis in paper**: The authors note that Organization entities are "the most challenging" and the system is "bottlenecked by ORG boundary precision." They explicitly recommend "incorporating gazetteer-augmented span objectives to improve organization recognition" in future work.
  - **Why unresolved**: The current transformer-based models rely solely on contextual embeddings, which appear insufficient for distinguishing the boundaries of organization names in Hiligaynon.
  - **What evidence would resolve it**: A comparative study fine-tuning the baseline models with injected gazetteer features to measure the resulting delta in B-ORG and I-ORG F1 scores.

- **Open Question 3**: To what extent does domain-adaptive pre-training on regional news articles improve NER accuracy over the current general-purpose baselines?
  - **Basis in paper**: The conclusion recommends "exploring domain-adaptive pre-training on regional news" as a specific avenue for future efforts to advance language technology for Hiligaynon.
  - **Why unresolved**: The current study utilizes general multilingual models (mBERT, XLM-R) pre-trained on massive, diverse corpora (Wikipedia, CommonCrawl) which may not align perfectly with the specific lexical distribution of Hiligaynon news.
  - **What evidence would resolve it**: Experiment results showing the performance divergence between the current baselines and a model further pre-trained on the raw crawl of "Ilonggo News Live" or "Bombo Radyo Bacolod" data.

## Limitations

- The study uses token-level F1-score, which is less strict than span-level evaluation, potentially overestimating the model's ability to correctly identify entity boundaries.
- The assumption that pre-trained models contain adequate representation for Hiligaynon is not explicitly verified; if the language was underrepresented in pre-training, performance may be overestimated.
- Cross-lingual transfer results are promising but not rigorously tested against control conditions (e.g., non-related languages) to isolate the effect of linguistic similarity.

## Confidence

- **High Confidence**: The dataset construction process (data collection, annotation by native speakers, high inter-annotator agreement of κ = 0.8141) and the resulting strong baseline performance on the Hiligaynon test set are well-documented and reproducible. The mechanism of fine-tuning a multilingual transformer on a high-quality, language-specific dataset is a standard and validated approach.
- **Medium Confidence**: The explanation for cross-lingual transfer success (shared linguistic ancestry and script) is plausible but not rigorously tested against alternative hypotheses. The specific macro F1 scores for cross-lingual evaluation are reported, but the generalisability of these results to other language pairs is uncertain without further experimentation.
- **Low Confidence**: The assumption that the pre-trained models contain adequate representation for Hiligaynon is not explicitly verified. The paper does not investigate whether the models' performance would be significantly worse if the target language was absent from pre-training, which is a critical assumption for the entire study's success.

## Next Checks

1. **Pre-Training Data Audit**: Conduct an analysis to determine if Hiligaynon text was present in the original training corpora of mBERT and XLM-R. If not, this would validate the models' ability to learn from zero-shot data but would also suggest the need for more targeted pre-training for truly low-resource languages.

2. **Cross-Lingual Control Experiment**: To isolate the effect of linguistic similarity, conduct a zero-shot evaluation of the Hiligaynon-trained model on a non-related language (e.g., a non-Austronesian language) and a language with a different script (e.g., Thai or Arabic). Comparing these results to the reported Cebuano/Tagalog scores would provide stronger evidence for the proposed mechanism of transfer.

3. **Out-of-Domain Performance Test**: Evaluate the fine-tuned model on a held-out test set from a domain not represented in the training data (e.g., scientific abstracts or legal documents in Hiligaynon, if available). This would quantify the model's robustness and practical applicability beyond its training distribution.