---
ver: rpa2
title: 'MaP: A Unified Framework for Reliable Evaluation of Pre-training Dynamics'
arxiv_id: '2510.09295'
source_url: https://arxiv.org/abs/2510.09295
tags:
- evaluation
- training
- pass
- performance
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of unreliable evaluation during
  LLM pre-training, which is plagued by parameter instability from training stochasticity
  and evaluation instability from noisy measurement protocols. The authors introduce
  MaP, a dual-pronged framework that synergistically integrates checkpoint merging
  and the Pass@k metric.
---

# MaP: A Unified Framework for Reliable Evaluation of Pre-training Dynamics

## Quick Facts
- arXiv ID: 2510.09295
- Source URL: https://arxiv.org/abs/2510.09295
- Reference count: 17
- Primary result: Reduces Pairwise Ranking Reversal Rate (PRR) from 50% to 22.73% by combining checkpoint merging and Pass@k metrics

## Executive Summary
This paper addresses the problem of unreliable evaluation during LLM pre-training, which is plagued by parameter instability from training stochasticity and evaluation instability from noisy measurement protocols. The authors introduce MaP, a dual-pronged framework that synergistically integrates checkpoint merging and the Pass@k metric. Checkpoint merging smooths the parameter space by averaging recent model weights, while Pass@k provides a robust, low-variance statistical estimate of model capability by generating multiple candidate solutions. The evaluation uses Kendall's rank correlation (τ) to measure training trajectory stability and Pairwise Ranking Reversal Rate (PRR) to assess how well pre-training performance predicts downstream capabilities. MaP yields significantly smoother performance curves, reduces inter-run variance, and ensures more consistent model rankings. Specifically, it achieves higher Kendall's τ values across multiple benchmarks, and reduces PRR from 50% to 22.73% in controlled experiments, demonstrating improved prediction of downstream performance.

## Method Summary
The MaP framework combines two complementary techniques: checkpoint merging and the Pass@k metric. Checkpoint merging averages the parameters of the last N checkpoints to smooth parameter noise, reducing variance by a factor of N under independence assumptions. The Pass@k metric generates n samples per problem and computes an unbiased estimator of success probability, reducing evaluation noise compared to single-pass greedy decoding. The framework uses a 16.3B parameter MoE model trained on up to 10T tokens, with checkpoints saved at regular intervals. Evaluation metrics include Kendall's rank correlation coefficient (τ) for training trajectory stability and Pairwise Ranking Reversal Rate (PRR) to measure how well pre-training performance predicts post-SFT rankings. The combined approach is tested on benchmarks including GSM8K, MATH, HumanEval, MBPP, and others.

## Key Results
- Kendall's τ values significantly improve with MaP (Merge@5, Pass@16), achieving 0.926 on GSM8K compared to lower values for individual components
- Pairwise Ranking Reversal Rate (PRR) reduced from 50% to 22.73% in controlled experiments with 12 model variants
- Evaluation variance substantially reduced, yielding more consistent model rankings across runs
- Synergistic effect confirmed: combining both techniques outperforms either approach alone

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Averaging the weights of recent checkpoints reduces the variance of model parameters caused by training stochasticity, provided the noise in recent steps is approximately independent.
- **Mechanism:** The authors model a saved checkpoint $\theta_t$ as an ideal parameter vector $\theta^*_t$ corrupted by zero-mean noise $\epsilon_t$ from factors like data batching. By averaging $N$ recent checkpoints, the variance of this parameter noise theoretically reduces by a factor of $N$ (Section 2.2), smoothing the trajectory through the parameter space.
- **Core assumption:** Noise vectors $\epsilon_t$ from adjacent checkpoints are independent and the underlying "true" capability $\theta^*_t$ changes slowly relative to the merge window size.
- **Evidence anchors:**
  - [abstract]: "...checkpoint merging smooths the parameter space by averaging recent model weights..."
  - [section]: Section 2.2, "A Statistical View of Parameter Stabilization," explicitly derives the variance reduction $\Sigma/N$.
  - [corpus]: Weak direct support; related papers discuss parameter efficiency (GENE-FL) but not specifically weight averaging for evaluation stability.
- **Break condition:** If checkpoints are spaced too far apart, the assumption that they represent the same underlying "ideal" state $\theta^*_t$ may break, potentially smoothing over genuine learning progress.

### Mechanism 2
- **Claim:** Using the Pass@k metric (estimating the probability of solving a problem in $k$ attempts) reduces measurement variance compared to single-pass greedy decoding (Pass@1).
- **Mechanism:** Single-pass evaluation acts as a high-variance Bernoulli trial. Pass@k leverages $n$ samples to estimate the probability of success, which the authors show has an approximate variance of $k^2(1-p)^{2(k-1)}p(1-p)/n$ (Appendix D). This lower-variance estimate is more robust to sampling "luck."
- **Core assumption:** The probability $p$ of generating a correct answer is stationary for the fixed model parameters during the $n$ samples.
- **Evidence anchors:**
  - [abstract]: "...Pass@k provides a robust, low-variance statistical estimate of model capability."
  - [section]: Section 3.4, Table 4 shows Kendall's $\tau$ increases with $k$ for generative tasks, indicating a more monotonic training signal.
  - [corpus]: Not directly evidenced in the provided corpus summaries.
- **Break condition:** This mechanism performs poorly on multiple-choice (MC) tasks where repeated sampling may guess the correct answer by chance, introducing noise rather than signal (Section 3.4).

### Mechanism 3
- **Claim:** Synergistically combining checkpoint merging and Pass@k is necessary to maximize stability, as they address orthogonal sources of noise (parameter vs. measurement).
- **Mechanism:** The framework conceptualizes Overall Stability $\approx$ Parameter Stability $\times$ Evaluation Stability. Smoothing parameters alone leaves evaluation noise; using a robust metric alone leaves parameter noise. The "MaP" framework applies both to stabilize the input (weights) and the measurement (score).
- **Core assumption:** Parameter instability and evaluation instability are distinct, compounding sources of error that can be independently minimized.
- **Evidence anchors:**
  - [section]: Section 3.2 and Table 1 demonstrate that the combined "MaP" protocol (Merge@5, Pass@16) achieves the highest Kendall's $\tau$ on benchmarks like GSM8K (0.926) compared to individual components.
  - [abstract]: "...introduce MaP, a dual-pronged framework that synergistically integrates..."
- **Break condition:** If computational budget constrains $n$ (samples) or $N$ (checkpoints) to very low values, the statistical power required for the synergy to manifest may be insufficient.

## Foundational Learning

- **Concept:** **Stochastic Gradient Descent (SGD) Noise**
  - **Why needed here:** The paper attributes "Parameter Instability" to the inherent noise in the training trajectory. Understanding that a checkpoint is a noisy sample of an "ideal" weight vector is crucial for grasping why averaging helps.
  - **Quick check question:** Why might a single checkpoint not represent the "true" capability of a model at a given training step?

- **Concept:** **Bernoulli Trials & Variance**
  - **Why needed here:** The paper frames single-pass evaluation as a high-variance Bernoulli trial. Grasping the statistical properties of binary outcomes explains why Pass@k offers a more stable estimate.
  - **Quick check question:** Why is flipping a coin 100 times a better estimate of its "fairness" than flipping it once?

- **Concept:** **Rank Correlation (Kendall's $\tau$)**
  - **Why needed here:** This is the primary metric used to quantify "smoothness" or "monotonicity" of the training progress.
  - **Quick check question:** If Kendall's $\tau$ is low (e.g., near 0) between training steps and accuracy, what does that imply about the training signal?

## Architecture Onboarding

- **Component map:** Input stream of saved checkpoints -> Parameter Processor (Merge) -> Inference Engine -> Evaluator (Pass@k) -> Monitor
- **Critical path:** The evaluation pipeline must support loading multiple weight files for merging *before* initializing the inference run. The inference engine must support batch-sampling (generating $n$ samples per prompt) rather than single greedy decoding.
- **Design tradeoffs:**
  - **Window Size ($N$):** Larger $N$ (e.g., 8 vs 4) smooths noise but risks "lagging" behind rapid learning progress or blurring distinct phases.
  - **Sample Count ($k/n$):** Higher $k$ improves stability and predictive power (reduces Pairwise Ranking Reversal Rate) but increases evaluation compute cost linearly (Section 3.4).
  - **Task Suitability:** Pass@k is effective for generative tasks (Math/Code) but detrimental for Multiple-Choice (Knowledge) due to chance correlations.
- **Failure signatures:**
  - **High PRR (Pairwise Ranking Reversal Rate):** Indicates pre-training eval does not predict post-training performance (random chance $\approx 50\%$).
  - **Crossing Trajectories:** If curves of different training runs intersect frequently (Fig 1a), the evaluation is still too noisy to distinguish model quality.
  - **Non-monotonic Kendall's $\tau$:** If increasing $N$ or $k$ does not improve rank correlation, the assumptions about noise independence may be violated.
- **First 3 experiments:**
  1. **Baseline Instability Check:** Plot greedy decoding (Pass@1) scores of raw checkpoints (Merge@1) for a 100B token run to confirm high variance (replicate Fig 1b).
  2. **Ablation - Merge Only:** Implement Merge@5 on the same run and measure the improvement in Kendall's $\tau$ (targeting improvements seen in Table 2).
  3. **Downstream Prediction Test:** Train 3+ model variants, evaluate pre-training with MaP (Merge@5, Pass@16), and correlate ranks with post-SFT ranks to verify PRR reduction (replicate Fig 5).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can adaptive sampling techniques reduce the computational cost of Pass@k without compromising the stability of the evaluation signal?
- Basis in paper: [explicit] The Conclusion lists "enhancing the computational efficiency of this framework (e.g., adaptive sampling techniques)" as a future focus.
- Why unresolved: Current Pass@k methods incur linearly increasing costs with sample size $k$, limiting practicality in large-scale pre-training.
- What evidence would resolve it: A dynamic sampling algorithm that minimizes generated tokens while preserving low variance and high rank correlation.

### Open Question 2
- Question: Do the stability benefits of MaP generalize across diverse model architectures and scales?
- Basis in paper: [explicit] The authors call for "investigation into... training volatility across different model scales and architectures" in the Conclusion.
- Why unresolved: The study relies primarily on a specific 16.3B parameter Mixture-of-Experts (MoE) model.
- What evidence would resolve it: Consistent improvements in Pairwise Ranking Reversal Rate (PRR) and Kendall's $\tau$ when applied to dense LLMs.

### Open Question 3
- Question: Why does increasing the number of merged checkpoints ($N$) degrade stability for certain tasks like MBPP?
- Basis in paper: [inferred] Table 3 shows a drop in Kendall's $\tau$ for MBPP as $N$ increases, implying an undefined trade-off for larger merging windows.
- Why unresolved: The paper observes the phenomenon empirically but does not identify if it stems from training velocity or task-specific sensitivity.
- What evidence would resolve it: A theoretical or empirical boundary defining the optimal merging window relative to the training learning rate or loss landscape curvature.

## Limitations
- Theoretical variance reduction claims rely on independence assumptions about noise that may not hold during rapid learning phases
- Evaluation protocol effectiveness across diverse model scales remains untested beyond the 16.3B parameter MoE model
- Pass@k performance degrades on multiple-choice tasks where repeated sampling introduces noise through chance guessing

## Confidence
- **High confidence**: Empirical demonstration that combining checkpoint merging and Pass@k improves Kendall's τ and reduces PRR (Section 3.2, Table 1)
- **Medium confidence**: Theoretical variance reduction arguments, which rely on idealized statistical assumptions that may not fully capture real training dynamics
- **Medium confidence**: Downstream prediction improvement (reduced PRR from 50% to 22.73%), though generality across different SFT approaches remains unclear

## Next Checks
1. Test the framework on smaller models (1-10B parameters) and with different optimizer configurations to verify robustness across scales
2. Conduct ablation studies with varying checkpoint intervals to determine optimal $N$ values for different learning rates
3. Evaluate whether the Pass@k improvement generalizes to other evaluation metrics beyond ranking correlation, such as absolute performance prediction accuracy