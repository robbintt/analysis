---
ver: rpa2
title: 'Multicultural Spyfall: Assessing LLMs through Dynamic Multilingual Social
  Deduction Game'
arxiv_id: '2601.09017'
source_url: https://arxiv.org/abs/2601.09017
tags:
- game
- guess
- player
- vote
- entity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a dynamic benchmarking framework using the
  social deduction game Spyfall to evaluate multilingual and multicultural reasoning
  in large language models. The framework involves models playing as spies or non-spies,
  deducing or identifying culturally relevant entities in various languages.
---

# Multicultural Spyfall: Assessing LLMs through Dynamic Multilingual Social Deduction Game

## Quick Facts
- **arXiv ID:** 2601.09017
- **Source URL:** https://arxiv.org/abs/2601.09017
- **Reference count:** 39
- **Key outcome:** Model rankings in this game-based approach align closely with the Chatbot Arena, but significant performance degradation occurs in non-English contexts and when handling locally specific cultural entities.

## Executive Summary
This paper introduces a dynamic benchmarking framework using the social deduction game Spyfall to evaluate multilingual and multicultural reasoning in large language models. The framework involves models playing as spies or non-spies, deducing or identifying culturally relevant entities in various languages. The study finds that model rankings in this game-based approach align closely with the Chatbot Arena. However, significant performance degradation occurs in non-English contexts and when handling locally specific cultural entities. Models struggle with strategic integrity and rule-following in non-English languages, and guessing local foods is more challenging than guessing locations.

## Method Summary
The framework implements a turn-based Spyfall game where 5 models (1 spy, 4 non-spies) interact to identify a secret entity through questions and answers. The game proceeds through 10 turns: 5 Round-Robin phases (only QA) followed by 5 Free Cycle phases (QA → Spy Guess → Vote). Models receive role-specific prompts in target languages (EN, ID, ZH, arz) and must output strictly formatted JSON responses. Performance is evaluated using Bradley-Terry ratings derived from win/loss outcomes, along with leakage rates and entropy metrics. The entity pool contains 30 culturally specific items per language across three scenarios: generic locations, local locations, and local foods.

## Key Results
- Model rankings in Spyfall closely correlate with Chatbot Arena rankings
- Significant performance degradation occurs in non-English contexts
- Models struggle more with guessing local foods than local locations
- Llama3.1-8B shows high leakage rates (up to 48%) while Gemini models have none
- Smaller models (Llama-8B) have higher JSON parsing error rates leading to forfeit

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Dynamic game-based evaluation appears to resist data leakage and saturation more effectively than static benchmarks, conditional on the interactive format preventing memorization.
- **Mechanism:** The turn-based Spyfall format generates unique game trajectories through model-to-model interaction, making each match contextually novel. Because outcomes depend on real-time strategic decisions rather than fixed question-answer pairs, prior exposure to training data provides diminishing returns.
- **Core assumption:** Models cannot easily overfit to dynamic multi-agent dialogue patterns the way they can to static benchmark items.
- **Evidence anchors:**
  - [abstract] "This game-based approach provides a scalable, leakage-resistant, and culturally nuanced alternative to traditional NLP benchmarks."
  - [section 1] "Because models compete against one another in a dynamic environment, the 'performance ceiling' evolves as models improve."
  - [corpus] Limited direct corpus validation; neighboring papers focus on game-based evaluation but don't specifically address leakage resistance claims.
- **Break condition:** If models begin exhibiting identical strategic patterns across games (suggesting template-driven responses) or if game histories become incorporated into training corpora at scale, leakage resistance would degrade.

### Mechanism 2
- **Claim:** Performance degradation in non-English contexts correlates with reduced cultural knowledge encoding, though causation is not directly established.
- **Mechanism:** When models encounter locally-specific entities (e.g., "Loloh Cemcem" for Indonesian, "Turmus" for Egyptian Arabic), they lack sufficient training exposure to generate contextually appropriate responses. This manifests as higher entropy in guesses, increased leakage rates among non-spies, and lower win rates for spies.
- **Core assumption:** Entity-level cultural knowledge is required for strategic gameplay; models cannot compensate through generic reasoning alone.
- **Evidence anchors:**
  - [abstract] "Significant performance degradation occurs in non-English contexts and when handling locally specific cultural entities."
  - [section 6] "Lower-Accuracy Entities Share Characteristics with Distractors... some confusion occurs between entirely unrelated entities."
  - [corpus] CultSportQA paper similarly finds regional sporting traditions underrepresented in model evaluations, supporting the broader pattern of cultural knowledge gaps.
- **Break condition:** If models achieve equivalent performance across all languages without corresponding cultural knowledge improvements, the mechanism would be invalidated (suggesting alternative explanations like evaluation artifacts).

### Mechanism 3
- **Claim:** Role differentiation (spy vs. non-spy) reveals distinct capability profiles that aggregate rankings may obscure.
- **Mechanism:** Spy role requires information gathering, hypothesis formation, and strategic concealment; non-spy role requires knowledge articulation without leakage and deception detection. Models may excel at one while struggling with the other, creating asymmetric performance patterns.
- **Core assumption:** Strategic gameplay decomposes into separable sub-skills that vary independently across models.
- **Evidence anchors:**
  - [abstract] "Models struggle with strategic integrity and rule-following in non-English languages."
  - [section 5.1] "Llama3.1-8B Has High Leakage Rates Up to 48%, While Gemini Models Have None."
  - [corpus] Bayesian Social Deduction paper notes LLMs struggle with social reasoning tasks requiring belief inference from partial observations, consistent with spy role demands.
- **Break condition:** If role-specific performance perfectly correlates with overall model rankings across all models and languages, the differentiation mechanism would not add explanatory value.

## Foundational Learning

- **Concept: Bradley-Terry Rating System**
  - **Why needed here:** The paper uses this paired-comparison model to convert game outcomes into comparable ratings, following Chatbot Arena methodology. Understanding why win rates alone are insufficient (they don't account for opponent strength) clarifies the evaluation approach.
  - **Quick check question:** If Model A beats Model B 60% of the time, but Model B beats Model C 80% of the time, should Model A's rating reflect only its direct win rate?

- **Concept: Shannon Entropy in Decision Analysis**
  - **Why needed here:** The paper applies entropy to measure vote dispersion and guess distribution, quantifying how "spread out" model decisions are. High entropy indicates uncertainty or confusion; low entropy indicates focused (potentially correct or confidently wrong) decisions.
  - **Quick check question:** In a 5-player game, if votes are distributed 4-1-0-0-0 vs. 2-1-1-1-0, which scenario has higher entropy and what does that imply about player confidence?

- **Concept: Information Leakage in Cooperative Adversarial Games**
  - **Why needed here:** Non-spies must signal knowledge to teammates without revealing the entity to the spy. The leakage rate metric captures this strategic failure mode, which correlates with model capability but also with language-specific performance.
  - **Quick check question:** If a non-spy answers "It's where people pray" when the entity is "Mosque," have they leaked information even without naming the entity directly?

## Architecture Onboarding

- **Component map:**
  - Game Engine -> Model Interface -> Entity Pool -> Evaluation Layer -> Language Detection

- **Critical path:**
  1. Initialize game with shuffled player order, selected entity, and role assignments
  2. Execute Round Robin Cycle (ensures all players participate in PQA)
  3. Transition to Free Cycle with full phase sequence per turn
  4. Continue until win condition triggered (correct guess, majority vote, surrender, or turn limit)
  5. Record game trajectory to HuggingFace dataset for reproducibility

- **Design tradeoffs:**
  - Turn-based vs. real-time: Chosen to standardize latency across models; sacrifices temporal pressure dynamics
  - Homogeneous non-spy teams: Simplifies analysis but may introduce behavioral coupling; framework supports heterogeneous teams in future
  - Entity scope (30 items): Balances discrimination difficulty with game tractability; harder entities increase entropy and game length
  - Single game per permutation: Reduces computational cost; may increase variance in ratings

- **Failure signatures:**
  - JSON parsing errors: Trigger immediate forfeit; disproportionately affect smaller models (Llama3.1-8B: 14.32% non-spy surrender rate)
  - Language switching: Models revert to dominant language (e.g., arz → ar) especially under strategic pressure
  - Premature guessing: Spies guess with low confidence, driving high wrong-guess rates (32.41% of games)
  - Leakage cascades: Single non-spy leak can end game immediately if spy is attentive

- **First 3 experiments:**
  1. Replicate ranking correlation: Run the same model set on both Chatbot Arena (via API) and Spyfall benchmark; compute Spearman correlation between rankings to validate the claimed alignment.
  2. Entity difficulty analysis: For each entity, compute guess accuracy and entropy across all models; identify which entities drive performance gaps between languages.
  3. Ablate language instruction: Remove the explicit "speak in {language}" constraint from prompts; measure how often models default to English and how this affects game outcomes.

## Open Questions the Paper Calls Out
- To what extent does employing heterogeneous, mixed-model teams of non-spies alter the emergent strategic behaviors and voting patterns compared to the homogeneous model instances used in this study?
- Can this dynamic, game-based evaluation framework be effectively adapted to other interactive tasks, such as debate, to assess multilingual reasoning without suffering from data leakage?
- What are the primary drivers causing models to abandon the target dialect (e.g., Egyptian Arabic) in favor of standard languages (e.g., Modern Standard Arabic) during gameplay?
- Why do models demonstrate significantly higher error rates and confusion when deducing local food entities compared to local location entities, despite both being culturally specific?

## Limitations
- The leakage-resistance claim lacks direct validation tests for model behavior becoming stereotyped over repeated games
- The entity lists (30 items per language) may be too small for broader cultural reasoning generalization
- Homogeneous non-spy team composition may artificially simplify social deduction dynamics
- Language adherence mechanism relies on external validation without demonstrating effectiveness under strategic pressure

## Confidence
- **High Confidence:** The correlation between Spyfall rankings and Chatbot Arena results - supported by direct comparison methodology and consistent with established Bradley-Terry ranking principles
- **Medium Confidence:** Performance degradation in non-English contexts - evidenced by measurable gaps but not causally isolated from other factors
- **Medium Confidence:** Role differentiation reveals distinct capability profiles - supported by asymmetric leakage and guessing patterns, though practical significance remains unclear

## Next Checks
1. **Leakage Resistance Validation:** Run the same model set through 100+ iterations of Spyfall games to test whether strategic patterns become stereotyped or whether performance gains suggest memorization of game trajectories
2. **Language Adherence Stress Test:** Conduct controlled experiments removing the explicit language constraint to quantify baseline language drift, then measure how this drift correlates with strategic performance degradation
3. **Entity Pool Scalability Test:** Expand the entity lists from 30 to 100+ items per language and measure whether the discrimination power and cultural reasoning assessment improve or whether the game becomes too complex for meaningful evaluation