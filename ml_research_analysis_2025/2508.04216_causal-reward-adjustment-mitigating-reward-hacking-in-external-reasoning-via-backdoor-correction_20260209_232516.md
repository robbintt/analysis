---
ver: rpa2
title: 'Causal Reward Adjustment: Mitigating Reward Hacking in External Reasoning
  via Backdoor Correction'
arxiv_id: '2508.04216'
source_url: https://arxiv.org/abs/2508.04216
tags:
- reward
- reasoning
- hacking
- features
- steps
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses reward hacking in external reasoning systems,
  where process reward models (PRMs) assign high scores to incorrect reasoning steps.
  The authors identify confounding semantic features as the root cause and propose
  Causal Reward Adjustment (CRA), a method that uses sparse autoencoders (SAEs) to
  extract interpretable features from PRM activations, identifies reward-hacking features
  statistically, and applies backdoor adjustment to remove their spurious influence.
---

# Causal Reward Adjustment: Mitigating Reward Hacking in External Reasoning via Backdoor Correction

## Quick Facts
- arXiv ID: 2508.04216
- Source URL: https://arxiv.org/abs/2508.04216
- Authors: Ruike Song; Zeen Song; Huijie Guo; Wenwen Qiang
- Reference count: 11
- Primary result: 1.6–2.9 percentage points accuracy improvement on GSM8K/MATH

## Executive Summary
This paper addresses reward hacking in external reasoning systems, where process reward models (PRMs) assign high scores to incorrect reasoning steps. The authors identify confounding semantic features as the root cause and propose Causal Reward Adjustment (CRA), a method that uses sparse autoencoders (SAEs) to extract interpretable features from PRM activations, identifies reward-hacking features statistically, and applies backdoor adjustment to remove their spurious influence. Experiments on GSM8K and MATH show that CRA improves reasoning accuracy by 1.6–2.9 percentage points compared to baseline PRMs, effectively mitigating reward hacking without modifying the policy model or retraining the PRM.

## Method Summary
CRA works by first training layer-wise sparse autoencoders on PRM activations to decompose them into interpretable features. The method then identifies reward-hacking features through statistical analysis of t-statistics between reward-hacking and normal reasoning steps. Finally, backdoor adjustment is applied by intervening on identified features and computing weighted average rewards across feature values. The approach uses beam search with K=4 and 8 candidates per step, with SAEs trained using m=8d latent dimensions, α=0.001 sparsity, and cosine annealing for 50 epochs.

## Key Results
- Achieves 1.6 percentage points accuracy improvement on GSM8K compared to baseline PRMs
- Improves MATH benchmark performance by 2.9 percentage points
- Effectively reduces scores for reward-hacking steps by approximately 0.04 while preserving scores for normal steps
- Shows that causal intervention produces significantly different effects than random feature intervention

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Confounding semantic features create spurious correlations that cause PRMs to assign high scores to logically incorrect reasoning steps.
- **Mechanism:** A backdoor path X ← Z → Y exists where semantic features Z (stylistic elements, step length, frequent expressions) influence both reasoning generation and reward labels. The PRM learns E[Y|X] ≈ P(Z=1|X=x) rather than true logical validity.
- **Core assumption:** Annotation biases cause annotators to prefer certain semantic patterns regardless of correctness.
- **Evidence anchors:** Abstract attribution of reward hacking to confounding features; Equation 4 showing PRM learns confounding feature presence; CoLD study supporting confounding feature hypothesis.

### Mechanism 2
- **Claim:** Sparse autoencoders decompose PRM activations into interpretable monosemantic features that isolate confounding patterns.
- **Mechanism:** Layer-wise SAEs with overcomplete representations (m=8d) and L1 sparsity constraint learn disentangled features where each dimension corresponds to a distinct semantic pattern.
- **Core assumption:** Superposition hypothesis holds—that neural activations can be decomposed into interpretable sparse features.
- **Evidence anchors:** Section describing encoder-decoder architecture with sparsity enforcement; feature interpretation showing each coefficient reflects feature presence; general SAE literature support.

### Mechanism 3
- **Claim:** Backdoor adjustment via feature intervention recovers the true causal reward by marginalizing over confounding feature values.
- **Mechanism:** Replace E[Y|X] with E[Y|do(X)] = Σ E[Y|X,Z=z]·P(Z=z) by intervening on SAE latent dimensions, simulating rewards under different confounder values, and computing weighted average.
- **Core assumption:** Identified SAE features correctly correspond to the true confounders Z in the causal graph.
- **Evidence anchors:** Equation defining adjusted reward computation; ablation experiment showing causal intervention reduces reward-hacking scores; theoretical foundation from causal bandits literature.

## Foundational Learning

- **Concept: Structural Causal Models (SCMs) and Backdoor Adjustment**
  - **Why needed here:** Core theoretical framework; without understanding confounding and do-calculus, the intervention mechanism is opaque.
  - **Quick check question:** Given a DAG with path X ← Z → Y, what does backdoor adjustment compute and why does it remove confounding?

- **Concept: Sparse Autoencoders for Mechanistic Interpretability**
  - **Why needed here:** CRA relies on SAEs to extract human-interpretable features from neural activations; understanding reconstruction-sparsity tradeoff is critical.
  - **Quick check question:** If L0 norm (active features) is too high, what happens to interpretability? What if reconstruction loss dominates?

- **Concept: Process Reward Models in External Reasoning**
  - **Why needed here:** Context for understanding what PRMs score and why reward hacking is particularly damaging in beam search.
  - **Quick check question:** In beam search with K=4, how does a single high-scored incorrect step propagate to final answer selection?

## Architecture Onboarding

- **Component map:** Policy Model (πθ) -> generates Reasoning Steps -> PRM (Rϕ) -> raw scores -> SAE Encoder -> sparse features z -> Feature Identifier -> confounding features F* -> Backdoor Adjuster -> adjusted scores R̂CRA -> Beam Search -> final path selection

- **Critical path:**
  1. Train layer-wise SAEs on PRM activations (offline, one-time)
  2. Label reward-hacking vs normal steps; compute t-statistics per feature
  3. At inference: for each step, intervene on identified features, compute weighted reward

- **Design tradeoffs:**
  - SAE expansion factor (m=8d): Higher → better disentanglement but slower training
  - Sparsity coefficient α=0.001: Too high → poor reconstruction; too low → polysemantic features
  - Thresholds τt=4.0, τa=0: Stricter → fewer false positives but may miss confounders
  - Assumption: Balanced prior P(Z=1)=0.5—may not hold if confounding features are rare

- **Failure signatures:**
  - SAE reconstruction loss plateaus high → features don't capture PRM semantics
  - t-statistics uniformly low across features → confounding not localized or labeling is noisy
  - Adjusted scores don't discriminate correct/incorrect → wrong features identified or prior misestimated
  - Random intervention produces similar effects to causal intervention → feature identification failed

- **First 3 experiments:**
  1. **SAE sanity check:** Train SAE on single PRM layer; verify reconstruction loss < 0.1 and L0 ≈ 10-20% of m. Visualize top-activating tokens per feature.
  2. **Feature separation validation:** On 100 labeled steps (50 reward-hacking, 50 normal), compute t-statistics; confirm top features show clear distribution separation.
  3. **Intervention ablation:** Run beam search on 50 GSM8K problems with: (a) no adjustment, (b) CRA adjustment, (c) random feature intervention. Compare accuracy and score distributions.

## Open Questions the Paper Calls Out

- **Open Question 1:** Does CRA generalize effectively to non-mathematical reasoning domains such as code generation or logical inference?
  - **Basis in paper:** The authors evaluate the method exclusively on mathematical reasoning benchmarks (GSM8K and MATH) despite claiming the framework applies broadly to "complex tasks."
  - **Why unresolved:** The semantic confounders identified in math (e.g., symbolic patterns) may differ significantly from those in code or logic, potentially affecting the validity of the SAE-extracted features.
  - **What evidence would resolve it:** Applying CRA to diverse external reasoning benchmarks like HumanEval (code) or logical deduction datasets.

- **Open Question 2:** Can the feature identification step be unsupervised to remove the reliance on manually labeled "reward hacking" data?
  - **Basis in paper:** The methodology requires constructing a labeled dataset where reasoning steps are manually classified as "reward hacking" or "normal" to compute t-statistics for feature selection.
  - **Why unresolved:** This manual labeling process creates a bottleneck, limiting the scalability of the method and introducing potential subjectivity.
  - **What evidence would resolve it:** Developing an unsupervised clustering or anomaly detection approach for identifying confounding features that achieves comparable performance without human labels.

- **Open Question 3:** How does the backdoor adjustment perform when multiple confounding features interact simultaneously?
  - **Basis in paper:** While the method identifies a set of features $F^*$, the Structural Causal Model (Figure 2) and the adjustment formula illustrate a single confounder $Z$.
  - **Why unresolved:** It is unclear if simple marginalization over multiple features effectively captures their combined spurious influence or if complex interactions require a different causal strategy.
  - **What evidence would resolve it:** Testing the method on synthetic datasets with controlled, interacting confounders to verify if the adjustment successfully isolates the true causal reward.

## Limitations

- The causal assumptions underlying backdoor adjustment are not experimentally validated—statistical separation via t-tests supports confounding, but direct counterfactual experiments would strengthen the causal claim
- SAE interpretability relies on the superposition hypothesis, which lacks theoretical guarantees in transformer architectures; polysemantic features could lead to incorrect confounding feature identification
- The method requires substantial training data (190K steps) and computational resources for SAE training, limiting accessibility
- Beam search with K=4 and 8 candidates per step may not fully represent practical deployment scenarios with larger search budgets

## Confidence

- **High confidence:** The empirical performance improvements (1.6-2.9 percentage points) are directly measurable and reproducible
- **Medium confidence:** The statistical identification of confounding features via t-tests is sound, but the causal interpretation requires stronger validation
- **Low confidence:** The claim that identified features are the "true" confounders in the causal graph—this remains an assumption supported by correlation rather than experimental counterfactuals

## Next Checks

1. **Counterfactual intervention test:** For a set of reward-hacking steps, artificially modify identified confounding features to their "normal" values and verify that PRM scores decrease, directly testing the causal mechanism

2. **Feature ablation study:** Train SAEs on progressively more layers and measure how many layers are needed for optimal performance; test whether top-k features (vs all significant features) suffice for adjustment

3. **Generalization stress test:** Apply CRA to a different domain (e.g., code generation or fact-checking) where reward hacking manifests differently, testing whether the same methodology generalizes beyond mathematical reasoning