---
ver: rpa2
title: Deep Reinforcement Learning for Phishing Detection with Transformer-Based Semantic
  Features
arxiv_id: '2512.06925'
source_url: https://arxiv.org/abs/2512.06925
tags:
- phishing
- features
- learning
- detection
- lexical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces a Quantile Regression Deep Q-Network (QR-DQN)
  approach that combines RoBERTa semantic embeddings with handcrafted lexical features
  to enhance phishing detection while accounting for uncertainties. Unlike traditional
  DQN methods that estimate single scalar Q-values, QR-DQN leverages quantile regression
  to model the distribution of returns, improving stability and generalization on
  unseen phishing data.
---

# Deep Reinforcement Learning for Phishing Detection with Transformer-Based Semantic Features

## Quick Facts
- arXiv ID: 2512.06925
- Source URL: https://arxiv.org/abs/2512.06925
- Reference count: 40
- Primary result: QR-DQN with hybrid features achieves 99.86% accuracy and 0.04% generalization gap

## Executive Summary
This study introduces a Quantile Regression Deep Q-Network (QR-DQN) approach that combines RoBERTa semantic embeddings with handcrafted lexical features to enhance phishing detection while accounting for uncertainties. Unlike traditional DQN methods that estimate single scalar Q-values, QR-DQN leverages quantile regression to model the distribution of returns, improving stability and generalization on unseen phishing data. A diverse dataset of 105,000 URLs was curated from PhishTank, OpenPhish, Cloudflare, and other sources, and the model was evaluated using an 80/20 train-test split. The QR-DQN framework achieved a test accuracy of 99.86%, precision of 99.75%, recall of 99.96%, and F1-score of 99.85%, demonstrating high effectiveness. Compared to standard DQN with lexical features, the hybrid QR-DQN with lexical and semantic features reduced the generalization gap from 1.66% to 0.04%, indicating significant improvement in robustness. Five-fold cross-validation confirmed model reliability, yielding a mean accuracy of 99.90% with a standard deviation of 0.04%.

## Method Summary
The approach formulates phishing detection as a single-step Markov Decision Process where each URL is processed into a hybrid state vector combining 50 handcrafted lexical features and 768-dimensional RoBERTa-base embeddings. The QR-DQN agent learns a distributional policy using quantile regression, with an asymmetric reward function that heavily penalizes false negatives (-2.0) to prioritize recall. The model was trained on 105,000 URLs using an 80/20 split, with five-fold cross-validation confirming reliability. The QR-DQN architecture consists of a 4-layer MLP (512→512→256→128) and uses a replay buffer of 150,000 experiences, trained for 300,000 steps with epsilon-greedy exploration.

## Key Results
- Test accuracy of 99.86% with precision 99.75%, recall 99.96%, and F1-score 99.85%
- Generalization gap reduced from 1.66% (lexical-only DQN) to 0.04% (hybrid QR-DQN)
- Five-fold cross-validation yielded mean accuracy of 99.90% with standard deviation of 0.04%
- Asymmetric reward function effectively reduced false negatives from 323 to 4 compared to lexical baseline

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Distributional value learning via QR-DQN improves stability and generalization compared to scalar Q-value estimation.
- Mechanism: By modeling the full distribution of returns through quantile regression (rather than a single expected value), the agent captures uncertainty in value estimates. This leads to more stable gradient updates during training and reduces overfitting to specific training patterns, as evidenced by the reduction in the generalization gap.
- Core assumption: The variance in returns (reward distribution) contains useful information that, when explicitly modeled, regularizes the learning process and prevents the network from assigning overconfident scalar values to state-action pairs.
- Evidence anchors: [abstract] "...QR-DQN leverages quantile regression to model the distribution of returns, improving stability and generalization on unseen phishing data. Compared to standard DQN... reduced the generalization gap from 1.66% to 0.04%..." [section 3.5] "...QR-DQN agent that models a distribution of returns per action via multiple quantiles... enabling uncertainty-aware value estimation."

### Mechanism 2
- Claim: Hybridizing RoBERTa semantic embeddings with handcrafted lexical features enables detection of semantic obfuscation that purely lexical models miss.
- Mechanism: RoBERTa's bidirectional attention mechanism creates embeddings that capture the contextual meaning of the URL string as a whole. This allows the model to identify malicious intent encoded through brand mimicry, typosquatting, and subtle semantic manipulations (e.g., "paypa1" vs "paypal") that lexical features alone would fail to connect to the original brand.
- Core assumption: The pre-trained RoBERTa model has sufficient world knowledge to generate useful embeddings for URL strings, and that these embeddings, when combined with structural signals (e.g., URL length), create a more complete representation of a URL's malicious potential.
- Evidence anchors: [abstract] "...combines RoBERTa semantic embeddings with handcrafted lexical features to enhance phishing detection..." [section 4.4] "Using BERT embeddings helped a lot in enhancing the context and making it more robust. The model's attention mechanism... allows the model to also understand how brand names are manipulated..."

### Mechanism 3
- Claim: An asymmetric reward function biases the agent toward high recall, reducing the number of missed attacks (false negatives).
- Mechanism: By assigning a larger penalty (-2.0) for false negatives (missing a phishing attack) than for false positives (-0.5 for flagging a safe site as phishing), the optimization process is driven to prioritize sensitivity. The agent learns a policy where the expected return for a "phishing" action is more easily triggered, as the cost of being wrong is asymmetrically high on one side.
- Core assumption: The asymmetric cost accurately reflects real-world security priorities, and the magnitude of the penalty difference is sufficient to shift the learned policy boundary without causing training instability.
- Evidence anchors: [section 3.3] "...asymmetrical reward function... A False Negative... receives a hefty penalty (-2)... A False Positive... receives a lower penalty (-0.5). This reward framework instructs the learning algorithm to prioritize high recall values..." [section 4.2, Results] The QR-DQN model showed a substantial decrease in false negatives (4 vs 323 for the lexical baseline).

## Foundational Learning

- Concept: **Distributional Reinforcement Learning (QR-DQN)**
  - Why needed here: It's the core algorithm. Standard DQN is used as a baseline, and understanding *why* modeling the distribution of returns improves generalization is the central theoretical contribution.
  - Quick check question: Can you explain why predicting a set of quantile values for a future reward provides more information than predicting a single average Q-value?

- Concept: **Transformer-based Semantic Embeddings (RoBERTa/BERT)**
  - Why needed here: These embeddings are the primary feature input. Understanding that they capture bidirectional context (e.g., the word "bank" in a URL implies different things based on surrounding words) is crucial for comprehending the model's improved detection of semantic attacks.
  - Quick check question: What is the [CLS] token (or `<s>` token in RoBERTa) and why is its embedding used as the representation of the entire URL sequence?

- Concept: **Markov Decision Process (MDP) Formulation for Classification**
  - Why needed here: The paper frames phishing detection not as standard supervised classification but as a single-step MDP. This RL-specific framing defines the state (features), action (classify as 0 or 1), and reward (asymmetric penalty/bonus).
  - Quick check question: In this specific MDP, what represents the "state," what are the possible "actions," and what is the "reward" for a correct classification?

## Architecture Onboarding

- Component map: Data pipeline -> PhishEnv environment -> QR-DQN agent (Online/Target networks) -> Replay buffer -> Loss computation -> Gradient updates
- Critical path: The concatenation of RoBERTa embeddings and lexical features. This hybrid vector is the *only* input to the agent. If this fusion is flawed (e.g., misaligned, improperly normalized), the entire mechanism fails. A secondary critical path is the reward function definition in `PhishEnv`, as it directly dictates the policy's behavior.
- Design tradeoffs:
    - **Frozen vs. Fine-tuned RoBERTa:** The paper uses *frozen* pre-trained weights for speed and simplicity. This sacrifices potential domain-specific semantic adaptation for faster training and reduced computational cost.
    - **Single-step MDP vs. Multi-step:** The detection task is formulated as a single-step decision per URL. This simplifies the problem (no complex temporal credit assignment) but may limit the agent's ability to explore a sequence of investigative actions.
    - **Asymmetric Rewards:** The -2.0/-0.5/+1.0 scheme is manually designed to enforce a high-recall policy. This is effective for security but requires tuning. A different ratio would produce a different precision-recall trade-off.
- Failure signatures:
    - **Mode Collapse to "Phishing":** If the FN penalty is too high or the training is too aggressive, the agent may learn to classify *everything* as phishing to avoid large penalties.
    - **Overfitting to Lexical Features:** If the semantic embeddings provide no useful signal, the model will behave like the lexical-only baseline, showing a larger generalization gap.
    - **Replay Buffer Imbalance:** If the sampling from the buffer is not class-balanced (which the environment's reset function tries to mitigate), training could become unstable.
- First 3 experiments:
  1.  **Ablation on Feature Types:** Train two baseline agents, one using *only* lexical features and another using *only* RoBERTa embeddings (no lexical). Compare their test accuracy and generalization gaps against the full hybrid model. This isolates the contribution of each feature type.
  2.  **Reward Sensitivity Analysis:** Run a series of training runs with different values for the false negative penalty (e.g., -1.0, -2.0, -5.0, -10.0). Plot the resulting precision and recall for each. This validates the assumption that the chosen asymmetric reward enforces a high-recall policy.
  3.  **Architecture Validation:** Replace the QR-DQN agent with a standard DQN agent (using the same hybrid features) to isolate the impact of the distributional RL algorithm. If QR-DQN shows a smaller generalization gap than standard DQN with identical features, it confirms the specific benefit of the quantile regression mechanism.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the proposed QR-DQN agent be evolved into an online continual learning system that adapts to new phishing patterns in real-time without suffering from catastrophic forgetting?
- Basis in paper: [explicit] The authors explicitly state in Section 5.3 that a key future goal is to allow the agent to adjust its policy using a continuous stream of URLs to learn from new attack methods in real-time.
- Why unresolved: The current framework is trained on a static 105k URL dataset; it requires retraining to handle distribution shifts, whereas a true online system must adapt incrementally without losing previously learned capabilities.
- What evidence would resolve it: A demonstration of the agent learning on a temporal data stream (e.g., evolving phishing tactics month-over-month) while maintaining high accuracy on older attack vectors.

### Open Question 2
- Question: Does domain-specific fine-tuning of the transformer model (RoBERTa) on phishing datasets yield significant performance improvements over the frozen pretrained weights used in this study?
- Basis in paper: [explicit] Section 5.3 identifies "Domain-Specific BERT Fine-Tuning" as a focus for future research to enhance semantic encoding and improve RL policy optimization.
- Why unresolved: The current implementation relies on frozen `roberta-base` weights; it is unknown if updating these weights specifically for URL structures would improve the detection of semantic obfuscation or reduce the false negative rate further.
- What evidence would resolve it: A comparative ablation study showing performance differences between the current frozen model and a domain-adapted model on adversarial obfuscation tests.

### Open Question 3
- Question: How can explainability methods (e.g., SHAP, LIME) be integrated to determine the specific contribution of semantic embeddings versus lexical features in the agent's decision-making process?
- Basis in paper: [explicit] Section 5.3 states that future plans include adding explainability methods to provide "actionable post-hoc explanations" and clarify which URL elements impacted the agent's output.
- Why unresolved: While the hybrid state space (RoBERTa + Lexical) is shown to be effective, the "black box" nature of the deep RL policy and high-dimensional embeddings makes it difficult to discern if the model relies on statistical patterns or semantic context for specific decisions.
- What evidence would resolve it: A qualitative and quantitative analysis using explainability tools that attribute prediction scores to specific input features (e.g., token importance vs. URL length).

### Open Question 4
- Question: Can the computational efficiency of generating 768-dimensional semantic embeddings be optimized to support ultra-high-throughput, real-time detection systems?
- Basis in paper: [inferred] In Section 6.2, the authors note that creating BERT embeddings is "costly from a computation perspective" and may cause latency issues in real-time systems, despite the use of caching.
- Why unresolved: The paper relies on pre-computation for training efficiency, but real-world deployment often requires on-the-fly inference, where the overhead of a large transformer model might create bottlenecks.
- What evidence would resolve it: Benchmarking the inference latency per URL on standard hardware without caching, potentially comparing RoBERTa against lighter models like DistilBERT within the QR-DQN framework.

## Limitations

- The 80/20 train-test split may not adequately capture performance on truly unseen data distributions, particularly from different time periods or threat actors.
- The asymmetric reward function introduces a manually-tuned hyperparameter that may not generalize across different security contexts or cost structures.
- The study does not evaluate on temporally or geographically distinct datasets that would better reflect real-world deployment scenarios.

## Confidence

- **High Confidence:** The mechanism of distributional RL improving stability (Mechanism 1) and the hybrid feature approach detecting semantic obfuscation (Mechanism 2) are both well-supported by ablation results and comparable literature.
- **Medium Confidence:** The asymmetric reward function's impact on recall (Mechanism 3) is demonstrated through performance metrics, but the specific -2.0/-0.5 ratio was chosen manually without systematic sensitivity analysis.
- **Low Confidence:** The long-term generalization of the frozen RoBERTa embeddings to novel phishing techniques remains uncertain, as the study does not test on data from significantly different time periods or threat actors.

## Next Checks

1. **Temporal Validation:** Evaluate the trained model on a separate dataset collected 6+ months after the training data to assess performance on evolved phishing techniques.
2. **Reward Function Sensitivity:** Systematically vary the false negative penalty (-1.0, -2.0, -5.0, -10.0) and measure the resulting precision-recall trade-off to identify the optimal cost ratio for different deployment contexts.
3. **Ablation on Embedding Strategy:** Replace the frozen RoBERTa embeddings with fine-tuned weights (trained end-to-end with the RL agent) to determine if domain adaptation provides additional performance gains.