---
ver: rpa2
title: 'MiroMind-M1: An Open-Source Advancement in Mathematical Reasoning via Context-Aware
  Multi-Stage Policy Optimization'
arxiv_id: '2507.14683'
source_url: https://arxiv.org/abs/2507.14683
tags:
- training
- reasoning
- arxiv
- data
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MiroMind-M1 is a fully open-source advancement in mathematical
  reasoning via context-aware multi-stage policy optimization. It addresses the reproducibility
  gap in reasoning language models by providing a complete stack of models, datasets,
  and training configurations.
---

# MiroMind-M1: An Open-Source Advancement in Mathematical Reasoning via Context-Aware Multi-Stage Policy Optimization

## Quick Facts
- **arXiv ID**: 2507.14683
- **Source URL**: https://arxiv.org/abs/2507.14683
- **Reference count**: 13
- **Key outcome**: MiroMind-M1 achieves state-of-the-art or competitive performance among Qwen2.5-based open-source models on AIME and MATH benchmarks while providing complete reproducibility through open-sourced models, datasets, and training configurations.

## Executive Summary
MiroMind-M1 addresses the reproducibility gap in mathematical reasoning language models by providing a complete open-source stack for training and evaluating reasoning models. The approach combines supervised fine-tuning on 719K verified chain-of-thought problems with a novel context-aware multi-stage policy optimization (CAMPO) algorithm that uses progressive context length expansion and repetition penalties for stable reinforcement learning. The resulting models achieve strong performance on challenging benchmarks, with the 7B model reaching 73.4% on AIME24, 57.8% on AIME25, and 96.7% on MATH500, while demonstrating superior token efficiency compared to similar models.

## Method Summary
MiroMind-M1 employs a two-stage training pipeline: supervised fine-tuning (SFT) on curated math problems followed by reinforcement learning with verifiable rewards (RLVR). The SFT stage uses a Qwen-2.5-Math-7B backbone with 719K verified chain-of-thought traces, prioritizing longer trajectories that demonstrate complex reasoning. The RL stage implements CAMPO, a multi-stage algorithm that progressively increases context length (16K→32K→49K) while applying repetition penalties to prevent policy collapse. Training uses the VERL framework with group relative policy optimization, omitting KL penalties to encourage exploration while maintaining stability through the repetition penalty mechanism.

## Key Results
- 7B model achieves 73.4% on AIME24, 57.8% on AIME25, and 96.7% on MATH500
- Superior token efficiency compared to reference models, with compressed response lengths during training
- Consistent improvements over baselines across all major benchmarks
- Multi-stage training (16K→32K) shows ~30% faster training than single-stage 32K while maintaining performance

## Why This Works (Mechanism)

### Mechanism 1
Progressive context length expansion during RL improves training efficiency while maintaining reasoning quality. Shorter initial context limits (16K) constrain output space, accelerating feedback cycles and enforcing concise reasoning. Gradual relaxation to 32K+ allows deeper reasoning to emerge once basic patterns stabilize. Core assumption: Models learn efficient reasoning patterns early that transfer to longer contexts. Evidence: Training curves show compressed response lengths initially (8K-9K tokens) that expand as context increases. Break condition: If performance degrades after context length increase, revert to previous stage and extend training.

### Mechanism 2
Repetition penalty stabilizes RL training by preventing policy collapse into low-diversity outputs. The penalty function `f(o_i)` returns proportion of tokens in detected repeating loops, with earlier repetitions penalized more heavily. This is subtracted from rewards before group normalization in GRPO-style advantage estimation. Core assumption: KL-divergence penalty alone is insufficient for long-CoT training where KL is often omitted to encourage exploration. Evidence: Training curves demonstrate stability difference with/without repetition penalty. Break condition: If repetition penalty causes overly conservative outputs without accuracy gain, reduce penalty weight.

### Mechanism 3
Trajectory length in SFT data correlates with downstream reasoning performance. Longer CoT traces likely originate from more complex problems requiring multi-step decomposition. Training on these traces teaches models to sustain extended reasoning chains. Core assumption: Length is a proxy for reasoning complexity and semantic richness. Evidence: "Long" trajectory selection consistently outperforms "Random" at 30k and 50k sample sizes (e.g., 35.21 vs 29.58 on AIME24 at 30k). Break condition: If longer trajectories contain excessive repetition or circular reasoning, filter using diversity metrics rather than raw length.

## Foundational Learning

- **Group Relative Policy Optimization (GRPO)**
  - Why needed here: CAMPO builds on GRPO's group-normalized advantage estimation. Understanding how GRPO eliminates the critic model and normalizes rewards within rollout groups is essential for grasping CAMPO's modifications.
  - Quick check question: Can you explain why GRPO uses group normalization instead of GAE-based advantage estimation?

- **Chain-of-Thought (CoT) Reasoning in LLMs**
  - Why needed here: The entire training pipeline assumes models can learn step-by-step reasoning through supervised imitation followed by RL refinement. The SFT phase teaches CoT structure; RL optimizes it.
  - Quick check question: What distinguishes verified CoT traces from standard instruction-response pairs?

- **Reward Signal Quality in RLVR**
  - Why needed here: Paper emphasizes that incorrect verifier outputs (penalizing correct answers) cause models to develop inefficient reasoning patterns. Understanding reward noise impact is critical.
  - Quick check question: Why does rule-based math verification fail for proofs and long-form answers?

## Architecture Onboarding

- **Component map**: Qwen-2.5-Math-7B backbone → LlamaFactory training → MiroMind-M1-SFT-7B → VERL framework → CAMPO algorithm → MiroMind-M1-RL-7B/32B → cascade verifier → reward signals

- **Critical path**:
  1. Curate SFT data with verified CoT traces (long trajectories prioritized)
  2. Train SFT model with no-packing strategy, 32K max positions
  3. Filter RL data to exclude fully-correct/fully-incorrect problems per base model
  4. Run multi-stage CAMPO: 16K → 32K context progression
  5. Apply repetition penalty at each training step

- **Design tradeoffs**:
  - No-packing vs. packing: No-packing yields ~2-3% better AIME scores but slower training
  - Single-stage vs. multi-stage: Single-stage (32K throughout) is simpler but ~30% slower with similar final performance
  - KL penalty: Omitted in paper to encourage exploration, but increases collapse risk (mitigated by repetition penalty)

- **Failure signatures**:
  - Training collapse: Sudden accuracy drop, often accompanied by repetitive outputs → increase repetition penalty or reduce learning rate
  - Stagnant performance: Flat reward curve across steps → check verifier accuracy, ensure batch contains partially-correct samples
  - Excessive token generation: Responses approaching context limit → verify repetition penalty is active

- **First 3 experiments**:
  1. Reproduce SFT baseline: Train Qwen2.5-Math-7B on 50K "long" samples from OpenR1 with no-packing. Target: >35% AIME24 (Table 4 baseline).
  2. Ablate repetition penalty: Run CAMPO 7B with and without `f(o_i)` term for 500 steps. Monitor training stability and avg@8 on AIME24/25.
  3. Context length sensitivity: Compare single-stage 32K vs. two-stage 16K→32K on same data budget. Measure total training time and final accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
Can system-level optimizations such as detached rollout and streaming load balancing significantly reduce the computational bottleneck in synchronized RL rollout phases? The authors state that addressing this bottleneck more comprehensively will require further research and system-level optimizations, mentioning detached rollout and streaming load balancing architecture as proposed strategies. This remains unresolved as the current implementation uses synchronized rollouts, where a few long generations delay entire batches.

### Open Question 2
Would incorporating code reasoning data into CAMPO training improve mathematical reasoning performance, particularly on problems requiring symbolic manipulation? The authors attribute their 32B model's gap behind Skywork-OR1-32B-Preview primarily to data composition, noting that Skywork benefits from a diverse mixture of math and code data while MiroMind relies solely on math-focused data. No ablation was conducted mixing code data, leaving this hypothesis untested.

### Open Question 3
Can evaluation stability be improved through benchmark expansion or stratified sampling, rather than increasing raw evaluation runs? The authors note that while increasing the number of evaluation runs can yield more robust results, it comes at the cost of significantly more time-consuming benchmarking. Current mitigation (64 runs) still yields standard deviation exceeding 8%, leaving the tradeoff between evaluation cost and stability unresolved.

## Limitations
- Lack of ablation studies for individual CAMPO components - no isolated experiments showing marginal contribution of progressive context expansion vs. repetition penalties
- "Long" trajectory selection heuristic is empirically validated but not theoretically justified, assuming monotonic relationship between length and complexity
- Improved cascade verifier is briefly described without sufficient detail for independent validation
- No component-level ablation studies to isolate the contribution of each technique

## Confidence
- **High Confidence**: MiroMind-M1-RL models achieve state-of-the-art or competitive performance among Qwen2.5-based open-source models on AIME/MATH benchmarks
- **Medium Confidence**: Progressive context length expansion improves training efficiency while maintaining reasoning quality
- **Low Confidence**: Repetition penalties are essential for RL stability (primarily qualitative observations rather than quantitative metrics)

## Next Checks
1. **Component Ablation Study**: Run CAMPO training with progressive context expansion disabled (single 32K context throughout) and repetition penalty disabled separately. Compare training stability metrics (gradient norms, entropy) and final performance to isolate each technique's contribution.

2. **Verifier Independence Test**: Evaluate MiroMind-M1-RL models using an independent verifier implementation (e.g., standard Math-Verify without improvements). Measure performance drop to quantify how much gains depend on verifier enhancements versus RL algorithm improvements.

3. **Generalization Cross-Benchmark**: Test MiroMind-M1-RL models on mathematical reasoning benchmarks outside the AIME/MATH suite (e.g., GSM8K, MATH-1000, or Olympiad-level problems). This would validate whether the models generalize beyond the specific problem distributions used in training and evaluation.