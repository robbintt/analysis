---
ver: rpa2
title: 'Paper2Rebuttal: A Multi-Agent Framework for Transparent Author Response Assistance'
arxiv_id: '2601.14171'
source_url: https://arxiv.org/abs/2601.14171
tags:
- rebuttal
- response
- reviewer
- evidence
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces RebuttalAgent, a multi-agent framework for
  automated rebuttal writing in peer review. The core innovation is reframing rebuttal
  generation as an evidence-centric planning task rather than direct-to-text generation.
---

# Paper2Rebuttal: A Multi-Agent Framework for Transparent Author Response Assistance

## Quick Facts
- **arXiv ID:** 2601.14171
- **Source URL:** https://arxiv.org/abs/2601.14171
- **Reference count:** 40
- **Primary result:** Multi-agent framework achieves consistent improvements in rebuttal quality metrics over strong baselines on the RebuttalBench benchmark.

## Executive Summary
Paper2Rebuttal introduces a multi-agent framework for automated rebuttal writing that reframes the task as an evidence-centric planning problem rather than direct text generation. The system decomposes reviews into atomic concerns, constructs hybrid contexts combining manuscript summaries and high-fidelity excerpts, and performs on-demand external literature retrieval. By generating inspectable response plans before drafting, the framework ensures claims are explicitly anchored in verifiable evidence. Experiments on the new RebuttalBench benchmark show consistent improvements across coverage, traceability, argumentation quality, and communication quality, with the largest gains in relevance and specificity.

## Method Summary
The framework operates in three stages: Input Structuring (parsing manuscript, compressing content, extracting atomic concerns), Evidence Construction (building hybrid contexts and retrieving external evidence briefs), and Planning and Drafting (strategizing responses, verifying plans, generating final drafts). The core innovation is the strict separation between evidence planning and text generation, with multiple checkpoints ensuring traceability. The system uses specialized agents for different tasks including PDF parsing, content compression, concern extraction, external search, and response drafting, with human-in-the-loop checkpoints available at key decision points.

## Key Results
- Consistent improvements over strong baselines across R-Score (coverage, semantic alignment, specificity), A-Score (logic consistency, evidence support, response engagement), and C-Score (professional tone, clarity, constructiveness)
- Largest gains observed in relevance and specificity metrics
- Ablation studies confirm external evidence briefs are the most critical component
- Performance improvements maintained across all evaluation dimensions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Separating evidence planning from text generation reduces hallucination and improves traceability
- **Mechanism:** Generates inspectable response plans anchored to specific evidence before drafting prose, forcing explicit evidence-to-claim mapping at planning stage
- **Core assumption:** Hallucinations stem from conflating reasoning with drafting rather than insufficient model capacity
- **Break condition:** If plan verification accepts claims without evidence linkage, mechanism collapses into direct-to-text behavior

### Mechanism 2
- **Claim:** Concern-conditioned hybrid context improves evidence retrieval precision under context constraints
- **Mechanism:** Compresses manuscript into navigable summary, then selectively expands only sections relevant to each atomic concern, combining summary coverage with high-fidelity excerpts
- **Core assumption:** Reviewer concerns are addressable via local manuscript evidence rather than requiring whole-document reasoning
- **Break condition:** If concerns require cross-sectional reasoning, local expansion may miss contradictions

### Mechanism 3
- **Claim:** External evidence briefs are primary driver of improved specificity and constructiveness
- **Mechanism:** On-demand search retrieves candidate papers, filters for relevance, and produces structured briefs highlighting citable claims
- **Core assumption:** Bottleneck for convincing rebuttals is access to relevant external evidence rather than argumentation skill
- **Break condition:** If retrieval returns irrelevant papers or briefs hallucinate claims, external evidence introduces noise

## Foundational Learning

- **Concept: Evidence-grounded generation vs. free-form generation**
  - **Why needed here:** Entire architecture predicated on claim that direct text generation is unreliable for high-stakes tasks requiring factual precision
  - **Quick check:** Can you explain why a fluent model might still produce unverifiable or hallucinated claims in rebuttal context?

- **Concept: Multi-agent decomposition with specialized roles**
  - **Why needed here:** System uses distinct agents with explicit handoffs and checkers for parsing, compression, extraction, search, strategy, and drafting
  - **Quick check:** What is functional difference between Compressor agent and Coverage Checker, and why are they separate?

- **Concept: Human-in-the-loop checkpoints for commitment safety**
  - **Why needed here:** System generates action items and plans for author review before drafting to prevent over-commitment
  - **Quick check:** What failure mode does "Action Items" mechanism prevent, and how does it differ from marking placeholders as [TBD]?

## Architecture Onboarding

- **Component map:** Input Structuring: Parser → Compressor → Consistency Checker; Extractor → Coverage Checker → Evidence Construction: Hybrid Context Builder; External Search Planner → Retriever → Selector → Analyzer → Planning and Drafting: Response Strategist → Plan Checker → (optional human feedback) → Response Drafter

- **Critical path:** Concern extraction → Hybrid context construction → Response strategy → Plan verification → Draft

- **Design tradeoffs:**
  - Compression vs. fidelity: Summaries reduce token cost but may lose edge-case details; consistency checker mitigates but adds latency
  - External retrieval vs. latency: On-demand search improves specificity but introduces variable delays and API dependency
  - Human checkpoint vs. automation: HITL review improves reliability but breaks full automation for batch evaluation

- **Failure signatures:**
  - Over-splitting concerns → redundant evidence retrieval, incoherent final response
  - Silent evidence loss during compression → planner lacks grounding, drafts generic claims
  - External briefs with low relevance → introduces noise, degrades constructiveness

- **First 3 experiments:**
  1. Ablate hybrid context: Run full pipeline using full manuscript text instead of hybrid context; compare traceability scores and token usage
  2. Stress-test external retrieval: Feed synthetic reviews with known missing-citation requests; measure whether retrieved briefs contain expected papers
  3. Probe plan-to-draft fidelity: Manually inspect whether final draft actually cites evidence linked in verified plan; identify plan drift

## Open Questions the Paper Calls Out

- **Open Question 1:** What is the quantitative performance gap between fully automated mode and intended human-in-the-loop workflow?
  - **Basis:** Main experiments ran fully automated; authors state results "should be viewed as a conservative lower bound" compared to real-world usage
  - **Resolution:** Comparative study measuring performance deltas when authors intervene at "Verified Plan" stage versus automated pipeline

- **Open Question 2:** How robust is external evidence retrieval for non-arXiv sources like paywalled journals or domain-specific repositories?
  - **Basis:** Implementation restricts external search to arXiv API, potentially limiting system in domains where relevant evidence resides outside open-access preprint servers
  - **Resolution:** Testing on review pairs from non-CS fields or evaluating retrieval success rates for gated/non-academic evidence

- **Open Question 3:** To what extent does high LLM-as-judge performance correlate with tangible outcomes like improved reviewer scores or acceptance decisions?
  - **Basis:** Paper validates "Argumentation Quality" and "Relevance" via LLM judge but not whether generated responses convince human reviewers
  - **Resolution:** User study or retrospective analysis measuring correlation between high-scoring outputs and final paper decisions

## Limitations
- No released code, forcing reliance on inferred prompt templates and unspecified architectural details
- External retrieval component vulnerable to API availability and relevance drift
- Claims about hallucination reduction and traceability supported by internal benchmarking but need human-in-the-loop validation on real rebuttals
- Performance gap between automated and human-in-the-loop workflows not quantified

## Confidence

- **High:** Evidence planning mechanism, hybrid context design, ablation study results
- **Medium:** Generalization to new review formats, external retrieval robustness, human review integration
- **Low:** Long-term stability of retrieval components, adaptability to non-ML domains

## Next Checks

1. **Plan-to-draft fidelity audit:** Manually verify every evidence citation in final draft can be traced back to verified response plan, checking for plan drift
2. **External retrieval stress test:** Feed synthetic reviews requiring specific missing citations; measure whether retrieved briefs contain expected papers within top-5 results
3. **Cross-sectional reasoning probe:** Design review concerns requiring comparing different manuscript sections to test hybrid context limitations