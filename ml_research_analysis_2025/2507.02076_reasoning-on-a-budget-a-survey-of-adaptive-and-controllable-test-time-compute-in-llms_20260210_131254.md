---
ver: rpa2
title: 'Reasoning on a Budget: A Survey of Adaptive and Controllable Test-Time Compute
  in LLMs'
arxiv_id: '2507.02076'
source_url: https://arxiv.org/abs/2507.02076
tags:
- reasoning
- arxiv
- compute
- llms
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This survey introduces a two-tiered taxonomy for characterizing
  test-time compute efficiency in large language models: L1-controllability, methods
  that operate under fixed compute budgets, and L2-adaptiveness, methods that dynamically
  scale inference based on input difficulty or model confidence. We benchmark leading
  proprietary LLMs across diverse datasets, revealing that current models often overthink
  simple problems while underthinking hard ones, leading to inefficient token usage.'
---

# Reasoning on a Budget: A Survey of Adaptive and Controllable Test-Time Compute in LLMs

## Quick Facts
- arXiv ID: 2507.02076
- Source URL: https://arxiv.org/abs/2507.02076
- Reference count: 20
- Primary result: Current models often overthink simple problems while underthinking hard ones, leading to inefficient token usage

## Executive Summary
This survey introduces a two-tiered taxonomy for characterizing test-time compute efficiency in large language models: L1-controllability, methods that operate under fixed compute budgets, and L2-adaptiveness, methods that dynamically scale inference based on input difficulty or model confidence. Through extensive benchmarking of leading proprietary LLMs across diverse datasets, the authors reveal systematic inefficiencies where models generate excessive tokens for simple problems while failing to allocate sufficient reasoning to harder ones. The analysis demonstrates that while prompting-based techniques can modestly improve efficiency, fine-tuning methods—particularly reinforcement learning—offer more robust adaptability and adherence to budget constraints.

## Method Summary
The survey benchmarks leading proprietary LLMs (GPT-o1, Claude 3.7, DeepSeek-R1, etc.) across diverse datasets including AIME 2024-25, MATH-500, ASDiv, GSM-8K, and MMLU-Pro. Inference is performed at Temperature 0.7, with specific budget constraints for Claude 3.7 (21,332 thinking tokens) and Gemini (24,576 tokens). Evaluation uses GPT-4o as an LLM-as-judge with temperature 0.3 and 3 runs per evaluation. The study systematically compares token usage against accuracy across different reasoning strategies and compute paradigms.

## Key Results
- Current models exhibit systematic inefficiencies: excessive token generation for simple problems (overthinking) and insufficient reasoning for hard problems (underthinking)
- Reinforcement learning methods like Length-Constrained Policy Optimization (LCPO) provide more robust budget adherence than prompting-based approaches
- Continuous latent space reasoning shows promise for efficiency gains but faces challenges in interpretability and training stability
- There is a practical need for hybrid fast-slow thinking models that can dynamically allocate reasoning effort based on task complexity

## Why This Works (Mechanism)

### Mechanism 1: Prompt-Induced Budget Control (L1)
Explicit token or step constraints in prompts can force models to compress reasoning chains without fine-tuning. The model conditions its generation on instructions like "limit answer to K tokens" or "Wait" signals, effectively shifting probability mass from verbose exploration to concise derivation. "Budget forcing" extends this by terminating generation when limits are reached. This works when base models possess strong instruction-following capabilities to override default verbose reasoning patterns.

### Mechanism 2: Reinforcement Learning for Adaptive Efficiency (L2)
Optimizing a reward function that penalizes length while rewarding accuracy enables models to autonomously regulate reasoning depth based on problem complexity. Algorithms like Length-Constrained Policy Optimization (LCPO) or O1-Pruner use RL to shape the policy, learning a "difficulty-aware" mapping that allocates more tokens to hard problems and fewer to simple ones. This requires verifiable reward signals and appropriate length penalty weighting to prevent reward hacking.

### Mechanism 3: Hybrid Fast-Slow Routing
Decoupling "fast" (intuitive) and "slow" (deliberative) systems allows dynamic compute allocation by routing queries based on estimated difficulty. A controller module estimates problem complexity, routing low-complexity queries to a "System 1" path (direct generation) while triggering "System 2" (search/CoT) for high-complexity queries. This prevents overthinking simple problems but requires an accurate difficulty estimator.

## Foundational Learning

- **Concept: Test-Time Compute (TTC) Paradigms**
  - Why needed here: The survey distinguishes between "Sequential" (e.g., Chain-of-Thought) and "Parallel" (e.g., Best-of-N) compute scaling. Understanding this is required to select the correct efficiency mechanism (token limits vs. sample limits).
  - Quick check question: Does the method reduce cost by shortening one chain (Sequential) or by reducing the number of generated candidates (Parallel)?

- **Concept: Overthinking vs. Underthinking**
  - Why needed here: These are the primary failure modes the mechanisms aim to solve. "Overthinking" is inefficient token usage on simple tasks; "Underthinking" is premature stopping on hard tasks.
  - Quick check question: Is the model generating 2k tokens for "1+1=?" (Overthinking) or failing to solve a proof in 100 tokens (Underthinking)?

- **Concept: Dual Process Theory (System 1 / System 2)**
  - Why needed here: The survey frames adaptive compute as switching between "fast" intuitive processing and "slow" deliberate reasoning.
  - Quick check question: Does the architecture allow the model to switch modes dynamically, or is it locked into a single reasoning style?

## Architecture Onboarding

- **Component map:** Input Interface -> Core Model -> Controller (L2 only) -> Verifier/Reward Model (Parallel/RL) -> Constraints
- **Critical path:**
  1. Diagnosis: Profile the base model to identify if it suffers more from overthinking (waste) or underthinking (accuracy loss) on your specific data distribution.
  2. Constraint Definition: Decide if you need strict guarantees (L1 - Controllability) or dynamic optimization (L2 - Adaptiveness).
  3. Method Selection: Apply Prompting (cheap, weak adherence) -> SFT (moderate) -> RL (expensive, robust).
- **Design tradeoffs:**
  - Prompting vs. Fine-Tuning: Prompting is zero-cost but unreliable for strict budgets; RL offers precise control but requires significant training resources and reward engineering.
  - Interpretability vs. Efficiency: Latent-space reasoning (Coconut) maximizes efficiency but hides the logic; Token-based CoT is verbose but debuggable.
- **Failure signatures:**
  - Budget Explosion: Model ignores prompt limits (Prompting failure).
  - Thought Collapse: RL model outputs empty strings or "I don't know" to minimize length penalty.
  - Underthinking: Model switches strategies rapidly without conclusion (detected by "thought" tokens like "Alternatively").
- **First 3 experiments:**
  1. Budget Adherence Test (L1): Prompt the model with a hard token limit (e.g., "Answer in <100 tokens") on a dataset (e.g., GSM8K) and measure the variance between requested and observed lengths.
  2. Difficulty Correlation Check (L2): Plot token usage vs. problem difficulty (e.g., AIME vs. ASDiv) for the base model. If tokens don't correlate with difficulty, an adaptive mechanism is required.
  3. Efficiency-Accuracy Curve: Run a "Budget Forcing" experiment (truncating thought at various steps) to plot the trade-off curve between compute saved and accuracy drop.

## Open Questions the Paper Calls Out

### Open Question 1
Can reinforcement learning policies trained to dynamically spawn parallel reasoning processes (e.g., `spawn()`/`join()`) generalize from game environments to complex mathematical reasoning tasks? The cited work only validated the method on the Countdown game, leaving its efficacy on broader, non-game reasoning domains untested.

### Open Question 2
Do controllable reasoning methods like Length Controlled Policy Optimization (LCPO) maintain performance and adherence to token budgets when scaled to context lengths significantly exceeding 3,600 tokens? Current experiments were restricted to a specific token ceiling, failing to demonstrate if control mechanisms hold or degrade as reasoning window expands.

### Open Question 3
Can Inference-Aware Finetuning, currently optimized for Best-of-N (BoN) sampling, be effectively adapted for other test-time compute paradigms like Self-Consistency or Monte Carlo Tree Search (MCTS)? The methodology was designed specifically for BoN procedure, and its compatibility with aggregation-based or search-based inference strategies remains unexplored.

### Open Question 4
How do prompting-based, SFT-based, and RL-based efficiency methods compare systematically in their ability to improve the efficiency of baseline test-time compute strategies? The survey categorizes these methods individually but lacks a unified benchmark that directly compares the trade-offs across different tuning approaches.

## Limitations
- Unknown prompt structures and system prompts that significantly influence CoT length and format
- Unclear implementation details for "Off" mode in Claude/Gemini where budget=0 but non-zero token counts are observed
- Missing specific judge prompt used for GPT-4o evaluation, making strict accuracy comparison difficult

## Confidence

High: Survey methodology is systematic and comprehensive, covering multiple efficiency approaches across diverse datasets
Medium: Benchmark results rely on proprietary model APIs with unknown internal implementations
Low: Direct reproducibility is limited by missing implementation details (prompts, judge templates, exact API parameters)

## Next Checks

1. Verify budget adherence: Run a controlled experiment with explicit token limits and measure actual vs. requested token usage across different model families
2. Profile overthinking/underthinking: Analyze token usage patterns against problem difficulty to identify systematic inefficiencies in current models
3. Validate evaluation pipeline: Test the LLM-as-judge approach with spot-checks against human evaluation to assess reliability of automated correctness scoring