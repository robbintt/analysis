---
ver: rpa2
title: Hierarchical Long Video Understanding with Audiovisual Entity Cohesion and
  Agentic Search
arxiv_id: '2601.13719'
source_url: https://arxiv.org/abs/2601.13719
tags:
- video
- reasoning
- search
- entity
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of long video understanding by
  proposing HAVEN, a unified framework that integrates audiovisual entity cohesion
  with hierarchical video indexing and agentic search. The method constructs a four-level
  hierarchical database spanning global summaries, scenes, segments, and entities,
  while employing speaker diarization and cross-modal entity consolidation to maintain
  semantic consistency across visual and auditory streams.
---

# Hierarchical Long Video Understanding with Audiovisual Entity Cohesion and Agentic Search

## Quick Facts
- arXiv ID: 2601.13719
- Source URL: https://arxiv.org/abs/2601.13719
- Reference count: 40
- Primary result: State-of-the-art 84.1% overall accuracy on LVBench, outperforming RAG and agent-based baselines

## Executive Summary
This work introduces HAVEN, a unified framework for long video understanding that integrates audiovisual entity cohesion with hierarchical video indexing and agentic search. The method constructs a four-level hierarchical database (global → scene → segment → entity) and employs speaker diarization with cross-modal entity consolidation to maintain semantic consistency across visual and auditory streams. An agentic reasoning loop with multi-granularity tools dynamically navigates this structure to enable coherent and scalable video comprehension. Extensive experiments show HAVEN achieves 84.1% overall accuracy on LVBench, with 80.1% on the challenging reasoning category, while requiring fewer reasoning steps than existing approaches.

## Method Summary
HAVEN processes hour-long videos by first extracting audio with WhisperX for ASR and speaker diarization, then uniformly segmenting videos into 30-second chunks for VLM captioning with fused speaker information. Entities are extracted, embedded, and consolidated across scenes using speaker labels for cross-modal cohesion. The system builds a four-level hierarchical database (global summary, scenes, segments, entities) and employs an agentic reasoning loop with OpenAI o3 as planner and five specialized tools for query-adaptive navigation. The agent maintains context memory initialized with the global summary and iteratively selects tools based on query granularity, achieving efficient long video comprehension through hierarchical indexing and audiovisual entity tracking.

## Key Results
- Achieves 84.1% overall accuracy on LVBench, outperforming existing RAG-based and agent-based approaches
- Reaches 80.1% accuracy on the challenging reasoning category, demonstrating superior performance on complex inference tasks
- Requires fewer reasoning steps than baselines due to hierarchical indexing and efficient tool navigation

## Why This Works (Mechanism)

### Mechanism 1: Audiovisual Entity Cohesion
Speaker identity provides a stable cross-modal signal for maintaining entity coherence across long videos where visual appearance varies significantly. WhisperX performs ASR + speaker diarization to assign consistent speaker labels across the video, which are fused with visual segment captions via VLM. Entity descriptions are clustered by embedding similarity and LLM-based canonicalization merges or splits clusters based on semantic coherence, prioritizing speaker label agreement. Entity-centric re-captioning generates focused descriptions per entity-segment pair for efficient reranking. This mechanism assumes speaker voice remains relatively consistent across scenes compared to visual appearance.

### Mechanism 2: Hierarchical Video Indexing
A four-level hierarchy (global → scene → segment → entity) enables multi-granularity retrieval that reduces reasoning iterations while preserving both global coherence and local detail. Video is uniformly segmented into fixed 30-second units, VLM generates segment captions with fused speaker info, and LLM performs adaptive scene aggregation by semantically grouping consecutive segments. Scene-level and global summaries are generated, and during inference the global summary is prepended to context while tools navigate the hierarchy based on query granularity. This assumes video content has inherent hierarchical structure and that LLM can reliably detect semantic boundaries.

### Mechanism 3: Tool-Augmented Agentic Reasoning
Specialized multi-granularity tools combined with a Think→Act→Observe loop enable query-adaptive navigation that balances retrieval efficiency with answer accuracy. Reasoning LLM (OpenAI o3) acts as planner, maintains context memory initialized with global summary, and at each step selects tools based on query type and current context. Five tools (Global Scene Browse, Segment Caption Search, Segment Visual Search, Entity Search, Inspection) return responses with timestamps appended to memory. The loop continues until answer found or max 10 iterations. This assumes the reasoning LLM can reliably determine appropriate tools and integrate multi-step outputs effectively.

## Foundational Learning

- **Concept: Speaker Diarization and ASR with Timestamps**
  - Why needed here: Core to audiovisual entity cohesion—provides "who said what and when" for cross-modal entity consolidation
  - Quick check question: Given an audio segment with overlapping speakers, can you explain how diarization assigns speaker labels and how these labels would be used to merge visual entity descriptions across scenes?

- **Concept: Hierarchical Document/Video Indexing**
  - Why needed here: Enables multi-granularity retrieval; understanding how to construct and query hierarchical indexes is essential for implementing the four-level database
  - Quick check question: If you have 100 video segments each with captions, how would you design an algorithm to adaptively group them into semantically coherent scenes using an LLM?

- **Concept: Tool-Augmented Agent Reasoning Loops**
  - Why needed here: The agentic search mechanism relies on understanding how LLMs can select and chain tools within a Think→Act→Observe paradigm
  - Quick check question: Describe how an agent would handle a query like "What does the protagonist do after the second argument?"—which tools would be called and in what order?

## Architecture Onboarding

- **Component map:** Audio extraction → ASR + speaker diarization → Segment processing → VLM captioning → Entity extraction → Clustering + canonicalization → Scene aggregation → Hierarchical database → Agent core with context memory → Tool suite navigation → Answer generation

- **Critical path:**
  1. Audio extraction must complete before segment captioning (speaker labels required for fusion)
  2. All segments must be processed before scene aggregation (needs full segment sequence)
  3. Entity consolidation requires all segment entity extractions (global clustering)
  4. Database must be fully constructed before any query can be answered
  5. During inference: global summary → tool selection → iterative retrieval → answer

- **Design tradeoffs:**
  - Segment length (30s vs shorter): Longer segments reduce captioning cost but may blur fine temporal details; paper shows 2 fps sampling within 30s segments improves accuracy (84.1% vs 81.0%) at cost of more captions
  - Max reasoning steps (10 vs more): Caps computational cost but may truncate complex reasoning chains; DVD uses 15 steps with finer segments
  - Entity reranking (K1→K2): Balances retrieval precision vs LLM overhead; K1 entities retrieve many segments, K2 narrows via entity-centric re-captions
  - Visual inspection frame limit (50): Constrains VLM input but may miss details in longer intervals

- **Failure signatures:**
  - Entity fragmentation: Same character split into multiple entities across scenes (check: are speaker labels consistent? Is clustering threshold too strict?)
  - Scene incoherence: Summaries don't reflect narrative flow (check: is LLM scene aggregator receiving sufficient context from overlapping segment chunks?)
  - Tool selection loops: Agent repeatedly calls same tool without progress (check: is planner prompt clear on tool preferences? Is context memory properly accumulating?)
  - Missing audio: Entity accuracy drops significantly on videos without audio tracks (expected per ablation; may need fallback to visual-only entity matching)

- **First 3 experiments:**
  1. Reproduce ablation on LVBench subset: Run Ours, Ours_clip, Ours_visual, Ours_transcript on 50 videos to verify contribution of hierarchy (-8.2%), transcripts (+4%), and speaker identity (+5.3%). Check that your implementation matches paper's trends.
  2. Tool usage analysis: Log which tools are called for each query category (TG, Sum, Rea, ER, EU, KIR). Verify that global queries trigger Global Scene Browse, entity queries trigger Entity Search, and fine-grained queries trigger Inspection tools. Compare iteration counts against Figure 3.
  3. Entity cohesion stress test: Select 5 videos with significant appearance changes (costume changes, aging, lighting variations) and evaluate entity merging accuracy with vs without speaker labels. Manually inspect canonical entity descriptions for correct consolidation.

## Open Questions the Paper Calls Out
None specified in the provided materials.

## Limitations
- The framework relies heavily on dialog-driven content and speaker diarization, potentially limiting performance on non-verbal or non-English videos
- Computational overhead from extensive preprocessing with proprietary models (GPT-4.1, OpenAI o3) is not quantified
- The method assumes speaker identity remains stable across scenes, which may not hold for videos with voice actors or dubbed content

## Confidence
- **High confidence**: Hierarchical video indexing improves accuracy and reduces iterations; audiovisual entity cohesion with speaker labels provides measurable benefit; agentic reasoning with multi-granularity tools outperforms RAG-based approaches on reasoning tasks
- **Medium confidence**: The 84.1% overall accuracy and 80.1% reasoning accuracy are strong but require independent verification; claims about fewer reasoning steps are supported by iteration counts but depend on specific query distribution
- **Low confidence**: Claims about robustness to appearance changes are primarily based on the assumption that speaker identity remains stable; actual performance on videos with voice actors, dubbed content, or significant audio degradation is unknown

## Next Checks
1. **Speaker label ablation on diverse video types**: Test entity consolidation accuracy on 10 videos with clear speaker identity changes (costumes, aging, lighting) versus 10 videos with voice actors or dubbed content to quantify the break condition
2. **Generalization to non-English content**: Evaluate the full pipeline on a subset of LVBench with non-English audio or videos with minimal dialogue to assess performance degradation
3. **LLM variant sensitivity**: Replace GPT-4.1 with GPT-4o or another LLM for one core component (e.g., scene aggregation) and measure accuracy change to quantify LLM dependency