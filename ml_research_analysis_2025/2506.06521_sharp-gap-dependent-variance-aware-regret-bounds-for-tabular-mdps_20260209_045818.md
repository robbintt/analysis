---
ver: rpa2
title: Sharp Gap-Dependent Variance-Aware Regret Bounds for Tabular MDPs
arxiv_id: '2506.06521'
source_url: https://arxiv.org/abs/2506.06521
tags:
- regret
- bound
- varc
- learning
- lemma
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies gap-dependent regret bounds for tabular Markov\
  \ decision processes (MDPs) and analyzes the Monotonic Value Propagation (MVP) algorithm.\
  \ The authors introduce a new notion of maximum conditional total variance and show\
  \ that MVP achieves a regret bound of \xD5(\u2211{\u03940} (H\xB2 log K \u2227 Var^{c}{max})\
  \ / \u0394 + (H\xB2 \u2227 Var^{c}{max})|Z{opt}|/\u0394{min} + SAH\u2074(S\u2228\
  H) log K), where Var^{c}{max} is the maximum conditional total variance."
---

# Sharp Gap-Dependent Variance-Aware Regret Bounds for Tabular MDPs

## Quick Facts
- arXiv ID: 2506.06521
- Source URL: https://arxiv.org/abs/2506.06521
- Reference count: 40
- Key outcome: MVP algorithm achieves regret bound with $\tilde{O}((H^2 \wedge \text{Var}^c_{\max})/\Delta)$ dependence

## Executive Summary
This paper establishes sharp gap-dependent regret bounds for tabular Markov decision processes (MDPs) using the Monotonic Value Propagation (MVP) algorithm. The key innovation is introducing the maximum conditional total variance $\text{Var}^c_{\max}$ as a necessary quantity for tight bounds, distinguishing it from unconditional variance metrics. The authors prove both upper and lower bounds showing this conditional variance term is essential, even when unconditional variance approaches zero. The analysis avoids complex clipping and recursion methods used in prior work by employing a novel weighted sum of suboptimality gaps.

## Method Summary
The method analyzes the MVP algorithm, a model-based optimistic planner that uses Upper Confidence Bound bonuses based on empirical variance and transition estimates. The algorithm performs value iteration with backward recursion to compute Q-values and extracts policies by maximizing these Q-values. The theoretical analysis introduces a weighted sum of suboptimality gaps where weights are proportional to per-step variance, avoiding aggressive clipping found in previous approaches. The proof carefully handles optimal actions through refined surplus clipping and establishes optimism properties with high probability.

## Key Results
- MVP achieves regret bound of $\tilde{O}(\sum_{h,s,a} \frac{H^2 \log K \wedge \text{Var}^c_{\max}}{\Delta_h(s,a)} + (H^2 \wedge \text{Var}^c_{\max})|Z_{opt}|/\Delta_{min} + SAH^4(S\vee H) \log K)$
- Maximum conditional total variance $\text{Var}^c_{\max}$ is necessary for tight bounds, even when unconditional variance is low
- Matching lower bound shows $\Omega(\text{Var}^c_{\max}/\Delta)$ dependence is unavoidable
- Reduces worst-case dependence on horizon from $HQ^*$ to $H^2$

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** MVP achieves tighter bounds by reweighting suboptimality gaps rather than using complex recursion.
- **Mechanism:** The proof introduces a weighted sum $\sum w_h(s,a) \Delta_h(s,a) n_K^h(s,a)$ with weights proportional to variance, avoiding aggressive clipping used in prior work.
- **Core assumption:** Weighted sum of future surpluses can be bounded by concentration inequalities while maintaining optimism.
- **Evidence anchors:** Abstract states result "stems from a novel analysis of the weighted sum of the suboptimality gap"; Section 4.3 details the weighted sum approach.
- **Break condition:** If weights don't adequately balance variance terms, the regret inequality fails.

### Mechanism 2
- **Claim:** Maximum conditional total variance $\text{Var}^c_{\max}$ is necessary for tight bounds.
- **Mechanism:** Unlike unconditional variance, $\text{Var}^c_{\max}$ conditions on visiting specific state-action pairs, capturing variance from rare but impactful states.
- **Core assumption:** Regret is dominated by stochasticity of paths that actually visit specific state-action pairs.
- **Evidence anchors:** Section 1 defines $\text{Var}^c_{\max}$ as characterizing maximum randomness; Section 5 proves separation from unconditional variance.
- **Break condition:** If environment is deterministic or all states are visited with high probability, distinction vanishes.

### Mechanism 3
- **Claim:** Lower bound construction demonstrates $\text{Var}^c_{\max}$-dependent term cannot be removed.
- **Mechanism:** Reduces problem to Bernoulli bandit with "hard-to-reach" state having high local variance but low visitation probability.
- **Core assumption:** Regret in MDP is lower-bounded by regret of embedded bandit problem at high-variance state.
- **Evidence anchors:** Section 5 describes construction based on Bernoulli bandits; abstract states lower bound demonstrates necessity of $\text{Var}^c_{\max}$ dependence.
- **Break condition:** If algorithm could identify and avoid high-variance suboptimal states without sampling them.

## Foundational Learning

- **Concept: Suboptimality Gaps ($\Delta_h(s,a)$)**
  - **Why needed here:** The entire bound is "gap-dependent" - you must understand $\Delta_h(s,a)$ is the difference between optimal value $V^*_h(s)$ and action value $Q^*_h(s,a)$.
  - **Quick check question:** If an action is optimal, what is its suboptimality gap?

- **Concept: Regret Decomposition via "Surplus"**
  - **Why needed here:** Proof relies on decomposing Regret($K$) into sum of "surpluses" $E^k_h(s,a)$ (gap between optimistic estimate and true value).
  - **Quick check question:** In optimistic algorithms, is the surplus typically positive or negative?

- **Concept: Conditional vs. Unconditional Expectation**
  - **Why needed here:** Core innovation is conditioning variance on visiting state $s$ at step $h$. Understanding $\mathbb{E}_\pi[\cdot | s_h=s]$ vs $\mathbb{E}_\pi[\cdot]$ is critical.
  - **Quick check question:** Does conditioning on a rare event increase or decrease the variance compared to unconditional average?

## Architecture Onboarding

- **Component map:** MVP Algorithm -> Bonus Term Calculation -> Value Iteration -> Policy Extraction
- **Critical path:**
  1. Sample trajectory $\tau^k$
  2. Update empirical counts $n_h(s,a)$ and estimates $\hat{P}, \hat{r}$
  3. Compute Bonuses: Calculate $b_h$ based on local variance
  4. Backward Pass: Compute $Q_h \leftarrow \min\{\hat{r} + \hat{P}V_{h+1} + b_h, H\}$
  5. Policy Extraction: $\pi^k_h(s) = \text{argmax}_a Q_h(s,a)$

- **Design tradeoffs:**
  - Updating Frequency: Simplified to update values every episode (Footnote 3), increasing switching cost but simplifying analysis
  - Clipping: Clips Q-values at $H$ with refined clipping of surplus to handle optimal actions without exploding $H$-factor

- **Failure signatures:**
  - High Variance States: "Spiky" rewards/transitions degrade regret from logarithmic to $\sqrt{K}$-style behavior
  - Long Horizons: $H^2$ scaling makes lower-order terms ($SAH^4$) dominant for massive $H$

- **First 3 experiments:**
  1. **Variance Stress Test:** Construct MDP with "trap" state having high transition variance but low visitation probability; verify regret scales with $\text{Var}^c_{\max}$
  2. **Gap Scaling:** Run MVP on MDP with varying $\Delta_{min}$; plot Regret vs $1/\Delta_{min}$ checking for $H^2 \wedge \text{Var}^c_{\max}$ scaling
  3. **Clipping Ablation:** Implement MVP without specific surplus clipping; compare regret on instances with many optimal actions

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the gap between upper bound $(H^2 \wedge \text{Var}^c_{\max})|Z_{opt}|/\Delta_{min}$ and lower bound $S/\Delta_{min}$ for optimal actions be closed?
  - **Basis in paper:** Conclusion states this term doesn't match the lower bound and improving either would advance understanding
  - **Why unresolved:** Existing optimistic algorithms typically explore all potentially optimal actions, generating unavoidable surplus terms
  - **What evidence would resolve it:** New algorithm achieving $S/\Delta_{min}$ bound or new lower bound proving current dependence is necessary

- **Open Question 2:** Can this variance-aware analysis be extended to linear MDPs or general function approximation?
  - **Basis in paper:** Conclusion suggests analysis can be adapted to other problem settings like linear MDPs
  - **Why unresolved:** Current novelty relies on weighted sum specific to tabular setting; translating to continuous spaces remains unproven
  - **What evidence would resolve it:** Derivation of variance-aware gap-dependent regret bound for linear MDP algorithm improving upon existing bounds

- **Open Question 3:** Can "weighted sum of suboptimality gaps" analysis be adapted to other algorithms beyond MVP?
  - **Basis in paper:** Abstract states result can be potentially adapted for other algorithms
  - **Why unresolved:** Analysis has currently only been proven for MVP; dependence on MVP's specific bonus term construction is not fully disentangled
  - **What evidence would resolve it:** Formal proof showing Q-learning or other model-free algorithms achieve similar rates using proposed framework

## Limitations
- Analysis assumes exact knowledge of MDP structure for theoretical analysis with no empirical validation provided
- Bound's dependence on $\text{Var}^c_{\max}$ may be conservative for MDPs where this quantity is dominated by few rare states
- $H^2$ scaling may become prohibitive for very long horizons

## Confidence
- **High Confidence:** Mechanism 1 (weighted sum leading to improved bound) is well-supported by proof structure and related work
- **Medium Confidence:** Mechanism 2 (necessity of conditional variance) is logically proven via lower bound but practical impact depends on MDP instance
- **Medium Confidence:** Mechanism 3 (matching lower bound) establishes theoretical tightness but constructing worst-case MDPs may be challenging

## Next Checks
1. **Empirical Verification:** Implement MVP algorithm and test on suite of MDPs including high conditional variance states; measure if empirical regret matches theoretical scaling with $\text{Var}^c_{\max}$
2. **Variance Sensitivity Analysis:** Construct parameterized family of MDPs with constant unconditional variance but varying $\text{Var}^c_{\max}$; verify regret bound's dependence on conditional term
3. **Comparison to Prior Art:** Implement MVP variant without refined clipping mechanism; compare regret on MDPs with many optimal actions to quantify improvement from new analysis