---
ver: rpa2
title: 'Filling the Missings: Spatiotemporal Data Imputation by Conditional Diffusion'
arxiv_id: '2506.07099'
source_url: https://arxiv.org/abs/2506.07099
tags:
- data
- imputation
- temporal
- spatiotemporal
- cofill
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes CoFILL, a diffusion-based model for spatiotemporal
  data imputation that addresses the problem of missing data in spatiotemporal systems.
  CoFILL employs a dual-stream architecture processing temporal and frequency domain
  features in parallel, combined via cross-attention, to capture both rapid fluctuations
  and underlying patterns.
---

# Filling the Missings: Spatiotemporal Data Imputation by Conditional Diffusion

## Quick Facts
- **arXiv ID:** 2506.07099
- **Source URL:** https://arxiv.org/abs/2506.07099
- **Reference count:** 8
- **Primary result:** CoFILL achieves MAE of 8.70 on AQI-36 (SF), 1.67 on METR-LA (Block), and 1.62 on PEMS-BAY (Point), outperforming state-of-the-art methods.

## Executive Summary
This paper proposes CoFILL, a diffusion-based model for spatiotemporal data imputation that addresses the problem of missing data in spatiotemporal systems. CoFILL employs a dual-stream architecture processing temporal and frequency domain features in parallel, combined via cross-attention, to capture both rapid fluctuations and underlying patterns. The method uses a noise prediction network to transform random noise into meaningful values aligned with the true data distribution, avoiding error accumulation from recursive imputation methods. Experiments on three real-world datasets (AQI-36, METR-LA, PEMS-BAY) show that CoFILL outperforms state-of-the-art methods, achieving MAE of 8.70 on AQI-36 (SF), 1.67 on METR-LA (Block), and 1.62 on PEMS-BAY (Point), with consistent improvements in CRPS metrics. Ablation studies confirm the importance of forward interpolation and temporal feature extraction. CoFILL demonstrates superior accuracy in handling various missing data scenarios while preserving spatiotemporal dependencies.

## Method Summary
CoFILL uses a conditional diffusion probabilistic model for spatiotemporal data imputation. The model employs a dual-stream architecture: a temporal stream with temporal convolutional networks and graph convolutional networks, and a frequency stream using discrete cosine transform. These streams process features in parallel and fuse them via cross-attention. Missing values are preprocessed using forward interpolation combined with Gaussian noise, then fed into a noise prediction network that learns to denoise step-by-step. The model is trained to predict added noise at each diffusion step, conditioned on observed values and the dual-stream feature representations. This approach avoids error accumulation from recursive imputation methods while capturing both short-term dynamics and long-term periodic patterns.

## Key Results
- CoFILL achieves state-of-the-art performance on three datasets: MAE of 8.70 on AQI-36 (SF), 1.67 on METR-LA (Block), and 1.62 on PEMS-BAY (Point)
- Outperforms existing methods including RNN-based, GNN-based, and diffusion-based approaches like PriSTI and AdaSTI
- Ablation studies show forward interpolation preprocessing provides the largest performance improvement, with MAE increasing from 8.70 to 9.15 when removed
- Consistent improvements in CRPS metrics demonstrate better probabilistic quality compared to deterministic baselines

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Parallel processing of temporal and frequency domain features captures complementary patterns that single-domain approaches miss.
- **Mechanism:** A temporal pathway (TCN + GCN) extracts short-term dynamics and spatial dependencies, while a frequency pathway (Discrete Cosine Transform) captures periodic/long-term trends. Cross-attention fuses these by using temporal features as Query and frequency features as Key/Value, allowing the model to weight frequency patterns by their relevance to immediate temporal context.
- **Core assumption:** Spatiotemporal signals contain multi-scale structure—rapid local fluctuations and slower periodic trends—that require separate extraction before integration.
- **Evidence anchors:** [abstract] "dual-stream architecture that processes temporal and frequency domain features in parallel... captures both rapid fluctuations and underlying patterns"; [section 4.2] "Temporal features reveal short-term behavior, while frequency-domain features identify long-term trends. The fusion of both perspectives enables the system to process multi-scale data within a unified framework."

### Mechanism 2
- **Claim:** Diffusion-based generation avoids error accumulation inherent in recursive/autoregressive imputation methods.
- **Mechanism:** Rather than predicting missing values sequentially (where early errors propagate), CoFILL's noise prediction network learns to iteratively denoise pure Gaussian noise toward the true data distribution, conditioned on observed values. Each denoising step operates on the full sequence independently of prior step predictions.
- **Core assumption:** The conditional data distribution can be learned via score matching, and observed values provide sufficient conditioning to constrain the denoising trajectory.
- **Evidence anchors:** [abstract] "avoiding error accumulation from recursive imputation methods"; [section 1] "RNN operate through an autoregressive mechanism, which causes early prediction errors to propagate through subsequent time steps. GNN depend on features from neighboring nodes, allowing errors to disseminate across the graph structure."

### Mechanism 3
- **Claim:** Forward interpolation preprocessing provides more stable conditional signals than Gaussian noise alone for guiding diffusion.
- **Mechanism:** Missing values receive two initializations: (1) forward interpolation using the last observed value, preserving temporal continuity and periodicity; (2) Gaussian noise matching the data distribution's statistics. These dual inputs feed the conditional information module, reducing information loss from over-smoothing while maintaining diversity.
- **Core assumption:** Forward interpolation produces reasonable initial estimates that do not introduce artificial fluctuations (which the paper claims linear interpolation causes).
- **Evidence anchors:** [section 4.1] "forward interpolation... fills in missing data by using the known values from the previous time step, thus preserving the continuity and periodicity... reduces information loss due to over-smoothing"; [section 5.6, Table 4] "The removal of forward interpolation produces the most significant performance degradation across datasets. For AQI-36, this modification increases the MAE from 8.7 to 9.15"

## Foundational Learning

- **Concept: Diffusion Probabilistic Models (DDPM)**
  - **Why needed here:** CoFILL's entire generative mechanism relies on understanding forward noising (adding Gaussian noise over T steps) and reverse denoising (learning to predict noise given noisy input and conditions).
  - **Quick check question:** Can you explain why predicting added noise ε is equivalent to learning the data distribution's score function?

- **Concept: Cross-Attention Mechanism**
  - **Why needed here:** The dual-stream architecture fuses temporal and frequency features via cross-attention (Equation 6-7). Understanding Query/Key/Value projections is essential to trace how information flows between domains.
  - **Quick check question:** If temporal features are Q and frequency features are K/V, what inductive bias does this introduce about which domain "queries" the other?

- **Concept: Graph Convolutional Networks (GCN) for Spatial Dependencies**
  - **Why needed here:** Sensor networks (traffic, air quality) have non-Euclidean spatial relationships modeled via adjacency matrices. Equation 3-4 define how GCN aggregates neighbor information.
  - **Quick check question:** How does the graph-normalized adjacency matrix Agcn = D^(-1/2)(A + I)D^(-1/2) differ from raw adjacency, and why does normalization matter for imputation?

## Architecture Onboarding

- **Component map:** Input X (N nodes × L timesteps) -> [Preprocessing] -> X₁ (forward interpolation) + X̃ (Gaussian noise) -> [Conditional Information Module] -> Temporal Stream (Conv → TCN → GCN → Ȟ_in) + Frequency Stream (Conv → DCT → Ĥ_in) -> Cross-Attention Fusion (C_con) -> [Noise Estimation Module] -> Temporal Attention (conditioned on C_con) + Spatial Attention + GNN -> Conv output layers (ε_θ) -> [Reverse Diffusion Sampling] -> Imputed X_out

- **Critical path:** Preprocessing quality → Conditional feature extraction (especially forward interpolation) → Cross-attention fusion → Noise prediction accuracy. Ablation shows forward interpolation removal causes the largest MAE increase (+0.45 on AQI-36).

- **Design tradeoffs:**
  - **Forward interpolation vs. noise-only preprocessing:** Continuity vs. diversity. Forward interpolation helps but may bias toward local consistency.
  - **Temporal vs. frequency pathway depth:** Ablation shows temporal module removal hurts more than frequency removal (MAE 9.00 vs. 8.81 on AQI-36).
  - **Channel size d:** Larger d improves traffic datasets but degrades air quality data (opposing optimal values: 16 for AQI-36, 64 for METR-LA).
  - **Max noise level β_T:** Stable performance for β_T ∈ [0.2, 0.4] on AQI-36; METR-LA is more sensitive.

- **Failure signatures:**
  - MAE spikes when forward interpolation is disabled (9.15 vs. 8.70 baseline).
  - Over-smoothed outputs if diffusion steps T is too low or conditioning is weak.
  - Spatial incoherence if adjacency matrix A is sparse or incorrect.
  - Training instability if noise schedule β_1...β_T is poorly tuned.

- **First 3 experiments:**
  1. **Reproduce ablation on forward interpolation** (Table 4, w/o Forward): Train without preprocessing and confirm MAE degradation magnitude. This validates the preprocessing pipeline is correctly implemented.
  2. **Cross-attention vs. direct addition comparison** (Table 4, w/o Cross): Replace cross-attention fusion with element-wise addition and measure MAE delta. This isolates the contribution of the fusion mechanism.
  3. **Hyperparameter sweep on β_T and channel size d** (Figure 3): Replicate the sensitivity curves on AQI-36 to verify optimal β_T ≈ 0.2–0.4 and d ≈ 16, establishing your training configuration before scaling to larger datasets.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the CoFILL architecture be extended to effectively handle multi-resolution spatiotemporal data?
- **Basis in paper:** [explicit] The conclusion states, "We will... extend CoFILL to handle multi-resolution spatiotemporal data in the future."
- **Why unresolved:** The current model architecture processes fixed-length time steps (L) and assumes a uniform sampling rate across nodes, as seen in the AQI-36 and traffic dataset configurations.
- **What evidence would resolve it:** Successful application of the model to datasets with irregular timestamps or varying sensor sampling frequencies without requiring prior resampling or interpolation.

### Open Question 2
- **Question:** Can adaptive fusion mechanisms improve the integration of temporal and frequency features compared to the current cross-attention approach?
- **Basis in paper:** [explicit] The conclusion identifies developing "adaptive fusion" as a specific avenue for future work.
- **Why unresolved:** The current dual-stream architecture fuses features using cross-attention with fixed learnable weights (Eq. 6-7), which may not dynamically adjust to varying ratios of signal-to-noise or different periodicity strengths across datasets.
- **What evidence would resolve it:** A comparative study showing that an adaptive weighting mechanism outperforms the static cross-attention baseline, particularly on datasets with unstable periodic patterns.

### Open Question 3
- **Question:** To what extent does the reliance on a static adjacency matrix limit performance in dynamic spatial environments?
- **Basis in paper:** [inferred] Section 4.2 defines the spatial dependency using a static graph normalization (Agcn). This assumes spatial relationships are constant, whereas real-world traffic and environmental interactions often evolve.
- **Why unresolved:** The paper does not ablate the impact of static versus dynamic graph structures, leaving the trade-off between computational cost and accuracy for dynamic graphs unknown.
- **What evidence would resolve it:** An ablation study replacing the static matrix with a time-evolving adjacency matrix to measure performance delta on datasets with shifting spatial correlations.

## Limitations

- **Major uncertainties in reproduction:** Exact adjacency matrix construction, DCT implementation details, and precise masking strategy parameters are not fully specified in the paper.
- **Ablation limitations:** Results rely on single dataset (AQI-36) comparisons, and optimal hyperparameters differ across datasets, suggesting potential overfitting to benchmarks.
- **Periodic structure assumption:** The claim that frequency features capture "periodic trends" assumes the data has periodic structure, which is not universally verified across all datasets.

## Confidence

- **High:** Diffusion-based imputation avoids error accumulation from recursive methods (core mechanism validated by ablation on forward interpolation).
- **Medium:** Dual-stream architecture with cross-attention fusion improves accuracy over single-stream baselines (ablations show impact, but no ablation on cross-attention itself).
- **Low:** Forward interpolation preprocessing is critical for performance (no ablation on frequency pathway or DCT alone; claim about linear interpolation causing "artificial fluctuations" is asserted but not tested).

## Next Checks

1. **Validate preprocessing ablation:** Reproduce the forward interpolation removal experiment (Table 4) to confirm MAE degradation matches the reported +0.45 increase on AQI-36.
2. **Test cross-attention contribution:** Replace cross-attention fusion with element-wise addition and measure accuracy delta to isolate the fusion mechanism's impact.
3. **Replicate hyperparameter sensitivity:** Recreate Figure 3 sensitivity curves for β_T and channel size d on AQI-36 to verify optimal values (β_T≈0.2-0.4, d≈16) and establish training stability before scaling to larger datasets.