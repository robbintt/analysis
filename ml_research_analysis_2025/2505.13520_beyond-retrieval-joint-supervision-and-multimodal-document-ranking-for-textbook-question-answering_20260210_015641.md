---
ver: rpa2
title: 'Beyond Retrieval: Joint Supervision and Multimodal Document Ranking for Textbook
  Question Answering'
arxiv_id: '2505.13520'
source_url: https://arxiv.org/abs/2505.13520
tags:
- retrieval
- questions
- document
- question
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper addresses the challenge of improving retrieval quality\
  \ in multimodal textbook question answering, where relevant documents are often\
  \ long, complex, and visually rich. It introduces JETRTQA, a retriever\u2013generator\
  \ architecture that enhances semantic representations through joint training using\
  \ both pairwise ranking and weak supervision derived from answer logits."
---

# Beyond Retrieval: Joint Supervision and Multimodal Document Ranking for Textbook Question Answering

## Quick Facts
- arXiv ID: 2505.13520
- Source URL: https://arxiv.org/abs/2505.13520
- Reference count: 28
- Key outcome: JETRTQA improves textbook QA accuracy by 2.4% on validation and 11.1% on test over prior methods by jointly supervising retrieval with pairwise ranking and generator-informed weak supervision.

## Executive Summary
This paper addresses the challenge of improving retrieval quality in multimodal textbook question answering, where relevant documents are often long, complex, and visually rich. It introduces JETRTQA, a retriever–generator architecture that enhances semantic representations through joint training using both pairwise ranking and weak supervision derived from answer logits. Unlike standard rerankers, JETRTQA is tailored to educational contexts and processes text and images jointly, enabling it to handle diagram-based questions effectively. Experimental results on the CK12-QA dataset show JETRTQA achieving a 2.4% improvement in validation accuracy and 11.1% on the test set over prior methods. Ablation studies confirm the importance of adaptive context selection, with diagram questions performing best with image-only retrieval, while text questions benefit from multiple relevant passages.

## Method Summary
JETRTQA is a two-phase retriever–generator architecture for multimodal textbook QA. Phase 1: Fine-tune Llama 3.2-Vision-Instruct with LoRA on the TQA task, then precompute and cache generator logits for all question-document pairs. Phase 2: Train an embedding enhancer (3-layer MLP: 1024→256→512→1024 with ReLU) on top of ImageBind embeddings, using joint loss combining pairwise ranking loss (based on contrastive logits difference) and cross-entropy loss from the cached generator logits. At inference, retrieve 6 passages for text questions and 1 image only for diagram questions, then pass to the generator. Training uses AdamW optimizer (lr=0.001, batch size 16) on a single A100 GPU; retrieval uses KDB.AI vector database with cosine similarity.

## Key Results
- JETRTQA achieves 2.4% higher validation accuracy and 11.1% higher test accuracy than prior methods on CK12-QA.
- Ablation studies confirm adaptive context selection: diagram questions perform best with image-only retrieval (0P/1Image), while text questions benefit from multiple passages (3P/3Images).
- The inverse correlation between context quantity and DMC accuracy (r = -0.91, p < 0.01) supports the image-only policy for visual reasoning tasks.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Joint supervision through pairwise ranking loss and generator-informed weak supervision improves document relevance discrimination in multimodal educational content.
- Mechanism: The embedding enhancer refines multimodal embeddings by optimizing two objectives: (1) pairwise ranking loss prioritizing documents that reduce generator loss, and (2) cross-entropy loss using precomputed logits from a frozen generator for answer-aware signals.
- Core assumption: Precomputed generator logits meaningfully reflect document relevance for answer generation even when the generator remains frozen.
- Evidence anchors: [abstract]: "JETRTQA learns to refine semantic representations... through supervised signal that combines pairwise ranking and implicit supervision derived from answers"; [section]: "Ltotal = Lrank + Lgen" (Section II.B, equation 10).
- Break condition: If generator logits are poorly calibrated or diverge from optimal retrieval signals after domain shift, weak supervision may introduce noise.

### Mechanism 2
- Claim: Adaptive, modality-aware context selection—specifically image-only context for diagram questions—significantly improves performance on visual reasoning tasks.
- Mechanism: System retrieves only the top-ranked image (0 textual passages) for diagram-based multiple-choice questions, avoiding textual noise that interferes with visual reasoning; for non-diagram questions, it retrieves 3–6 top-ranked textual passages.
- Core assumption: Retrieval scoring reliably separates relevant images from irrelevant text for diagram questions, and textual noise systematically harms visual QA.
- Evidence anchors: [abstract]: "Ablation studies confirm the importance of adaptive context selection..."; [section]: "For DMC: Enforce 0P to eliminate textual noise..." (Section IV).
- Break condition: If diagram questions require complementary textual definitions or captions, image-only retrieval will underperform.

### Mechanism 3
- Claim: High-dimensional multimodal embeddings (1024-dim via ImageBind) capture fine-grained semantic relationships necessary for complex ranking in educational content.
- Mechanism: ImageBind encodes text and images into shared 1024-dimensional space; embedding enhancer learns non-linear transformations to optimize ranking and generation-informed objectives.
- Core assumption: Embedding space has sufficient capacity and pre-trained ImageBind representations are well-aligned across modalities for educational science content.
- Evidence anchors: [abstract]: "JETRTQA... processes text and images jointly..."; [section]: "ImageBind was selected... because of its higher embedding dimensionality (1024)..." (Section II.B).
- Break condition: If task requires modalities beyond text/images, current ImageBind integration may be insufficient.

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed: JETRTQA is built on retriever–generator architecture where accurate retrieval is critical for answer quality; understanding RAG tradeoffs is essential.
  - Quick check: What is the key difference between sparse (e.g., BM25) and dense retrieval, and why might dense retrieval fail on domain-specific terminology?

- Concept: Multimodal Embedding Alignment
  - Why needed: Model uses ImageBind to embed text and images in shared space; misalignment directly harms retrieval and ranking.
  - Quick check: How would you verify that "layers of the Earth" text and a diagram of Earth's layers are embedded close together?

- Concept: Pairwise Ranking vs. Pointwise Scoring
  - Why needed: JETRTQA uses pairwise ranking loss (comparing document pairs) rather than pointwise scoring; this affects training data and hard negatives.
  - Quick check: Why might pairwise ranking be more robust than pointwise scoring when relevance labels are noisy or subjective?

## Architecture Onboarding

- Component map: Query → ImageBind encoder → embedding enhancer → cosine similarity retrieval → context selection (6 passages for NDQ, 1 image for DMC) → Llama 3.2-Vision generator → answer
- Critical path: Inference: Query → ImageBind → enhancer → cosine similarity → context selection → generator → answer. Training: (Query, candidate docs, answer choices) → ImageBind → enhancer → compute Lrank + Lgen → backprop only through enhancer.
- Design tradeoffs: Frozen generator reduces compute but limits joint optimization; ImageBind chosen for higher dimensionality (1024) vs. larger memory footprint; adaptive context improves DMC but adds rule-based decision point.
- Failure signatures: High MRR but low answer accuracy (generator failure); DMC accuracy drops with added text (check image-only policy); Lrank dominates Lgen (loss scaling imbalance); retrieval recalls irrelevant general text for factual questions (consider entity-aware filtering).
- First 3 experiments: 1) Reproduce adaptive context ablation on held-out split; 2) Compare ImageBind vs. lower-dimensional encoder (e.g., CLIP ViT-B/32); 3) Vary loss weighting (λ in Ltotal) to assess generator-informed supervision strength.

## Open Questions the Paper Calls Out

- Can end-to-end joint training of both retriever and generator outperform the current fixed-generator approach in TQA tasks?
  - Basis: Conclusion proposes future investigation of joint training for end-to-end optimization.
  - Why unresolved: Current design freezes generator to reduce computational cost; true end-to-end training remains unexplored.
  - What evidence would resolve it: Comparative experiments training both components simultaneously.

- Does integrating structured external knowledge (e.g., knowledge graphs) improve factual consistency and reasoning depth in textbook QA?
  - Basis: Conclusion proposes incorporating knowledge-augmented generation techniques.
  - Why unresolved: Current system relies solely on retrieved documents; case studies show failures on factual questions lacking entity mentions.
  - What evidence would resolve it: Ablation studies comparing retrieval-only vs. knowledge-augmented generation on fact-heavy question subsets.

- Can question-type classification (factual, inferential, conceptual) improve retrieval precision by routing to specialized retrieval strategies?
  - Basis: Case study identifies failures on factual questions lacking named entities, proposing categorization to prioritize short, precise documents.
  - Why unresolved: Current system treats all questions uniformly; adaptive routing based on question type is proposed but not implemented.
  - What evidence would resolve it: Implementation of question classifier with retrieval strategy selection, evaluated by question category.

## Limitations

- Major uncertainties include unspecified LoRA hyperparameters for generator fine-tuning, negative sampling strategy for pairwise ranking, and precise training duration for embedding enhancer.
- Reliance on frozen generator assumes precomputed logits remain stable and relevant across training, which may not hold under domain shifts or if generator calibration drifts.
- Adaptive context selection policy assumes visual reasoning benefits from pure imagery without textual grounding, which may not generalize to questions requiring captions or labels.

## Confidence

- Confidence is High for reported accuracy improvements (2.4% val, 11.1% test) and ablation results supporting adaptive context selection, as these are directly measured on CK12-QA benchmark with statistical backing.
- Confidence is Medium for joint supervision mechanism, as design is plausible but frozen generator's signal quality is not empirically validated beyond reported loss terms.
- Confidence is Low for claim that 1024-dimensional ImageBind embeddings are necessary, as no ablation or comparison to lower-dimensional encoders is provided.

## Next Checks

1. Reproduce the adaptive context ablation (0P/1Image vs. 3P/3Images) on a held-out split to confirm the image-only benefit for DMC questions.
2. Compare ImageBind embeddings vs. a lower-dimensional encoder (e.g., CLIP ViT-B/32) to measure the impact of embedding dimensionality on ranking metrics.
3. Vary the loss weighting (λ in Ltotal = Lrank + λ·Lgen) to assess sensitivity to generator-informed supervision strength and identify optimal balance.