---
ver: rpa2
title: Detecting AI-Generated Content in Academic Peer Reviews
arxiv_id: '2602.00319'
source_url: https://arxiv.org/abs/2602.00319
tags:
- reviews
- review
- peer
- ai-generated
- iclr
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study detects AI-generated content in academic peer reviews
  using a classifier trained on historical reviews and applied to later review cycles.
  The approach uses Longformer with LoRA fine-tuning, trained on 2021 reviews and
  evaluated on reviews from 2022-2025 for both ICLR and Nature Communications.
---

# Detecting AI-Generated Content in Academic Peer Reviews

## Quick Facts
- **arXiv ID**: 2602.00319
- **Source URL**: https://arxiv.org/abs/2602.00319
- **Reference count**: 3
- **Key outcome**: AI-generated content in academic peer reviews increased from near-zero before 2022 to approximately 20% of ICLR reviews and 12% of Nature Communications reviews by 2025

## Executive Summary
This study presents a method to detect AI-generated content in academic peer reviews using a classifier trained on historical reviews from 2021 and applied to review cycles from 2022-2025. The approach uses Longformer with LoRA fine-tuning, trained on 2021 reviews and evaluated on reviews from both ICLR (conference format) and Nature Communications (journal format). Results show minimal AI-generated reviews before 2022, with substantial increases thereafter: approximately 20% of ICLR reviews and 12% of Nature Communications reviews were classified as AI-generated in 2025. The most pronounced growth in Nature Communications occurred between the third and fourth quarter of 2024. These findings provide evidence of rapidly increasing AI-assisted content in peer review across different publication formats.

## Method Summary
The study uses Longformer with LoRA fine-tuning to classify peer reviews as human or AI-generated. The model is trained on 2021 reviews (160 real + 160 synthetic for ICLR; 120 real + 120 synthetic for Nature Communications) and synthetic reviews generated via DeepSeek Reasoner API with paper content conditioning. The classifier is then applied to 2022-2025 review data without further adaptation. The approach uses a binary classification framework to distinguish human-written from AI-generated text, with evaluation across two distinct publication venues.

## Key Results
- AI-generated reviews increased from near-zero before 2022 to approximately 20% of ICLR reviews in 2025
- Nature Communications showed 12% AI-generated reviews by 2025, with most growth occurring between Q3 and Q4 of 2024
- Detection accuracy on 2021 training data was reported as perfect, though real-world validation remains untested
- The classifier successfully distinguished between human and AI-generated reviews across different publication formats (conference vs. journal)

## Why This Works (Mechanism)

### Mechanism 1
A classifier trained on pre-LMM era text can detect the intrusion of LLM-generated text by recognizing distributional shifts in stylistic and semantic patterns. The model learns a decision boundary between "human-only" text (2021 reviews) and "synthetic" text (generated by DeepSeek). When applied to post-2022 data, the model flags reviews that statistically drift toward the synthetic distribution, effectively operationalizing "surprise" or low likelihood under the historical human prior. Core assumption: synthetic data shares sufficient stylistic signatures with actual AI tools used by reviewers. Break condition: if human writing styles evolve to mimic AI or reviewers heavily edit AI outputs, the distributional gap closes.

### Mechanism 2
Sparse attention mechanisms (Longformer) allow for consistent feature extraction across variable-length review documents, preventing the loss of long-range coherence signals. Standard transformers truncate text, losing contextual integrity. Longformer's sliding window attention pattern processes entire reviews (up to 2048 tokens), allowing detection of "unnatural" coherence or repetitive structures in extended AI-generated outputs. Core assumption: the signal distinguishing AI from human text is distributed across entire review structure, not just local n-grams. Break condition: if discriminative features are solely local, Longformer's computational overhead is unnecessary.

### Mechanism 3
Low-Rank Adaptation (LoRA) prevents overfitting to specific idiosyncrasies of synthetic training data while adapting the pre-trained backbone to the classification task. By freezing pre-trained weights and only updating low-rank matrices, the model retains general linguistic knowledge while learning the specific "human vs. synthetic" boundary, reducing risk of memorizing specific prompts used to generate training data. Core assumption: base model's weights contain sufficient priors about language to distinguish fine-grained authorship attributes without full fine-tuning. Break condition: if task requires unlearning specific strong priors from the base model, LoRA might lack representational capacity.

## Foundational Learning

- **Concept**: **Distributional Shift / Temporal Drift**
  - **Why needed here**: The core validity rests on assuming 2021 reviews are "pure" human and 2025 reviews contain a mix. Understanding that the model measures statistical property shifts over time is critical.
  - **Quick check question**: If reviewers in 2025 started writing exactly like they did in 2021, but used AI to do it, what would the model predict?

- **Concept**: **Synthetic Data Alignment**
  - **Why needed here**: The model is trained on proxies, not real AI reviews. Understanding how prompt engineering constrains output distribution (length, tone) is critical to evaluating training data validity.
  - **Quick check question**: Why is it important that synthetic reviews match the length and structure of 2021 human reviews?

- **Concept**: **Parameter-Efficient Fine-Tuning (PEFT)**
  - **Why needed here**: The paper uses LoRA. Understanding why we freeze weights and inject adapters, specifically with limited data (N=~320 per class) to prevent catastrophic forgetting or overfitting, is essential.
  - **Quick check question**: How does LoRA specifically address the "limited training data" constraint?

## Architecture Onboarding

- **Component map**: Raw review text + Year metadata -> Longformer tokenizer (max_length=2048) -> Pre-trained Longformer (frozen weights) -> LoRA modules (rank=8) injected into attention layers -> Classification layer [CLS] token -> Binary Softmax -> DeepSeek Reasoner API (used for offline training set creation)

- **Critical path**: 1. Data Curation: Gathering 2021 "Gold Standard" human reviews. 2. Synthesis: Prompting DeepSeek with 2021 paper abstracts to generate "Negative" class. 3. Training: Fine-tuning Longformer with LoRA on balanced 2021 dataset. 4. Inference: Frozen application to 2022-2025 data.

- **Design tradeoffs**: Generalization vs. Specificity (trade specific detection accuracy for temporal generalization), Recall vs. Precision (optimizes for strong signals, potentially missing subtle AI assistance)

- **Failure signatures**: High False Positives on Non-Native Speakers (if 2021 data skewed toward native speakers), Generator Overfitting (model learns "DeepSeek style" rather than "AI style")

- **First 3 experiments**: 1. Robustness Check: Train on DeepSeek data, test on GPT-4 validation set to verify cross-model detection. 2. Permutation Test: Shuffle 2021 labels to ensure model doesn't learn irrelevant metadata. 3. Threshold Sensitivity: Vary classification threshold on 2022 data to establish baseline FPR.

## Open Questions the Paper Calls Out
- To what extent does a detector trained on synthetic data from one specific LLM (DeepSeek) generalize to reviews generated or edited by other prevalent models like ChatGPT or Gemini?
- How can detection methods be adapted to accurately identify partial AI assistance, such as human-written reviews polished by AI, rather than just fully AI-generated text?
- Are the observed temporal trends of AI adoption in peer review consistent across non-computational disciplines such as biology, medicine, and social sciences?

## Limitations
- Classifier performance on real-world AI-assisted reviews (partial editing/polishing) remains unclear
- Temporal generalization assumption critical - 2021 reviews assumed to be "pure human" baseline
- Detection accuracy for partially AI-assisted reviews is unknown
- Cross-model generalization (DeepSeek-trained detector on ChatGPT outputs) not validated

## Confidence
- **High confidence**: Classifier architecture and training methodology are technically sound; basic finding that AI-generated content is detectable in principle is well-supported
- **Medium confidence**: Specific percentages (20% ICLR, 12% Nature Communications in 2025) are credible but depend on synthetic training data accurately representing real-world AI tools
- **Low confidence**: Detection accuracy for partially AI-assisted reviews is unknown; cross-model generalization not validated

## Next Checks
1. **Cross-model validation**: Test DeepSeek-trained classifier on reviews generated by ChatGPT or Claude to verify detection generalizes beyond training model
2. **Hybrid text detection**: Create and test on dataset with mixed AI/human contribution (20-80% AI) to establish detection accuracy for realistic AI-assisted reviews
3. **Temporal baseline stability**: Analyze writing style evolution in 2021-2025 period using unsupervised methods to determine if distributional shifts could be attributed to natural language evolution rather than AI introduction