---
ver: rpa2
title: 'What Matters in LLM-generated Data: Diversity and Its Effect on Model Fine-Tuning'
arxiv_id: '2506.19262'
source_url: https://arxiv.org/abs/2506.19262
tags:
- data
- llm-generated
- diversity
- performance
- real-world
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates the impact of diversity in LLM-generated
  data on downstream model performance during supervised fine-tuning. It proposes
  two methods to control data diversity: paraphrasing-based augmentation and topic-guided
  generation.'
---

# What Matters in LLM-generated Data: Diversity and Its Effect on Model Fine-Tuning

## Quick Facts
- arXiv ID: 2506.19262
- Source URL: https://arxiv.org/abs/2506.19262
- Authors: Yuchang Zhu; Huazhen Zhong; Qunshu Lin; Haotong Wei; Xiaolong Sun; Zixuan Yu; Minghao Liu; Zibin Zheng; Liang Chen
- Reference count: 40
- One-line primary result: Moderately diverse LLM-generated data can enhance model performance in data-scarce scenarios with minimal distribution shift, but highly diverse data with significant distribution shift degrades performance.

## Executive Summary
This paper investigates how diversity in LLM-generated data affects downstream model performance during supervised fine-tuning. The authors propose two methods to control data diversity—paraphrasing-based augmentation and topic-guided generation—and systematically evaluate their impact across multiple tasks and model sizes. Experiments reveal that moderately diverse generated data can improve performance in data-scarce scenarios when distribution alignment is maintained, but performance degrades when generated data introduces significant distribution shift. The findings highlight the critical balance between diversity and distributional alignment in LLM-generated data.

## Method Summary
The study uses GPT-4o to generate data with controlled diversity levels (Gen-D0 to Gen-D6) via paraphrasing-based augmentation for question-answering and topic-guided generation for story completion tasks. Diversity is quantified using Distinct-5 scores. Models (GPT-2-124M, Llama 3.2-1B, Llama 3.1-8B) are fine-tuned using LoRA with specific hyperparameters. The research examines performance across different mixing ratios of real and generated data, and analyzes the relationship between diversity, distribution shift, and model size effects.

## Key Results
- Moderately diverse LLM-generated data enhances model performance in data-scarce scenarios with minimal distribution shift
- Highly diverse data with significant distribution shift negatively impacts performance
- Mixing even small proportions (10%) of generated data into real training data increases test loss, with performance degrading as mixing ratio increases
- Larger models (8B) are more prone to overfitting on repetitive synthetic patterns compared to smaller models (124M)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Moderately diverse LLM-generated data can enhance model performance in data-scarce scenarios when distribution shift is minimal.
- Mechanism: Controlled diversity increases sample richness while maintaining distribution alignment, expanding effective training signal without introducing out-of-distribution noise.
- Core assumption: Generated data distribution overlaps substantially with real-world data (low KL divergence).
- Evidence anchors: [abstract], [section 4.3], weak corpus support from neighbor papers on data selection.
- Break condition: High distribution shift (elevated KL divergence) negates diversity benefits.

### Mechanism 2
- Claim: Mixing LLM-generated data into real training data degrades downstream performance proportionally to the mixing ratio.
- Mechanism: Generated data introduces distributional artifacts that diverge from test distribution as mixing ratio increases.
- Core assumption: Real-world test data remains representative and fixed.
- Evidence anchors: [abstract], [section 4.3, Figure 5], corpus support from "The Price of Format" paper.
- Break condition: If generated data is verified/filtered to match real distribution, degradation may be mitigated.

### Mechanism 3
- Claim: Model size mediates diversity–performance relationship, with larger models more prone to overfitting on repetitive synthetic patterns.
- Mechanism: Larger models (8B) overfit faster on synthetic data containing repetitive patterns that effectively increase training epochs.
- Core assumption: Paraphrasing creates semantically similar samples that increase effective epochs.
- Evidence anchors: [section D.2], [section D.2, Figure 14], no direct corpus evidence.
- Break condition: If synthetic data is sufficiently diverse AND distribution-aligned, larger models should leverage capacity for net gain.

## Foundational Learning

- Concept: Model Collapse
  - Why needed here: Provides theoretical motivation for studying diversity as a mitigating factor against iterative training on self-generated data.
  - Quick check question: Can you explain why recursive training on LLM-generated data leads to loss of tail distribution patterns?

- Concept: Distribution Shift (KL Divergence)
  - Why needed here: Key confounding variable that can override diversity benefits; essential for interpreting mixed-data experiments.
  - Quick check question: How would you measure whether LLM-generated story completions diverge from human-written completions?

- Concept: Distinct-n Diversity Metric
  - Why needed here: Operationalizes diversity via unique n-gram ratio; central to interpreting Gen-D0 through Gen-D6 conditions.
  - Quick check question: What are the limitations of n-gram-based diversity for semantic variation in paraphrased QA pairs?

## Architecture Onboarding

- Component map:
  - Data Generation Layer (GPT-4o) → Diversity Control Layer (mr/mt parameters) → Evaluation Layer (Distinct-5, KL divergence) → Training Layer (LoRA fine-tuning) → Performance Assessment

- Critical path:
  1. Define task → select generation method
  2. Set diversity control parameter → generate dataset
  3. Measure diversity AND distribution alignment
  4. Fine-tune downstream model → evaluate on real-world test set
  5. If mixing: calibrate mixing ratio; expect degradation proportional to generated data proportion

- Design tradeoffs:
  - Higher diversity → potential performance gain BUT risk of distribution shift
  - Larger topic sets increase diversity BUT may drift from real data patterns
  - Paraphrasing ensures semantic equivalence BUT reduces effective diversity
  - Larger models leverage capacity BUT overfit faster on repetitive synthetic data

- Failure signatures:
  - Test loss increases despite higher Distinct-n scores → check KL divergence
  - Large model shows sudden training loss drop with high mixing ratio → overfitting
  - Small model underperforms with high-diversity long-context QA → capacity insufficient

- First 3 experiments:
  1. Reproduce Gen-D0–Gen-D6 on summarization task using topic-guided generation; measure Distinct-5, KL divergence vs. real data; fine-tune 1B model and report test loss.
  2. Ablation on distribution alignment: filter generated data via embedding similarity threshold (cosine > 0.8); compare filtered vs. unfiltered at 20% mixing ratio.
  3. Model size scaling: run Gen-D3 and Gen-D5 across 124M, 350M, 1B, 3B, 8B; plot test loss vs. model size to validate overfitting hypothesis.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can methods be developed to ensure distribution consistency between real-world and LLM-generated data when controlling for diversity?
- Basis in paper: [explicit] "Ensuring that the data distribution is consistent between real-world and LLM-generated data is crucial due to the impact of the distribution shift on model performance."
- Why unresolved: Paper identifies distribution shift as key confounding factor but does not propose solutions.
- What evidence would resolve it: Generation techniques maintaining distributional alignment with real data while varying diversity levels.

### Open Question 2
- Question: Do the findings on diversity's effects generalize to pre-training scenarios beyond supervised fine-tuning?
- Basis in paper: [explicit] Paper focuses on SFT, which are more closely aligned with practical applications of LLMs.
- Why unresolved: Deliberately focuses on SFT, leaving pre-training implications unexplored.
- What evidence would resolve it: Experiments replicating methodology in pre-training contexts across similar model scales.

### Open Question 3
- Question: What is the optimal mixing ratio of LLM-generated to real data balancing diversity benefits against distribution shift costs?
- Basis in paper: [inferred] Shows degradation even at 10% mixing ratios but benefits from moderately diverse data in data-scarce scenarios.
- Why evidence would resolve it: Systematic experiments varying real data availability and mixing ratios with explicit distribution alignment metrics.

## Limitations
- Findings hinge on assumptions that generated data distribution can be adequately characterized by Distinct-n and KL divergence metrics
- Paper does not explore alternative diversity metrics or conduct ablation studies on generation temperature
- Mixing experiments show degradation even when diversity metrics appear comparable or higher, suggesting metrics may not fully capture distributional alignment

## Confidence
- **High confidence**: Core finding that moderately diverse data improves performance in data-scarce scenarios with minimal distribution shift (consistent across tasks and model sizes)
- **Medium confidence**: Scaling behavior with model size (124M vs 8B) - plausible but not rigorously tested with controlled synthetic data diversity
- **Low confidence**: Robustness of Distinct-n as sole indicator of distributional alignment given disconnect between diversity scores and test loss in mixing experiments

## Next Checks
1. Conduct KL divergence measurements between real and generated data distributions across all Gen-D0-6 conditions; verify distribution shift correlates with performance degradation
2. Test alternative diversity metrics (BERTScore, embedding-based diversity) to determine if they better predict downstream performance than Distinct-n
3. Implement verification/filtration step using embedding similarity thresholds to align generated data distribution with real data before mixing experiments