---
ver: rpa2
title: Dynamic Loss-Based Sample Reweighting for Improved Large Language Model Pretraining
arxiv_id: '2502.06733'
source_url: https://arxiv.org/abs/2502.06733
tags:
- linupper
- reweighting
- training
- data
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces novel loss-based reweighting strategies for
  pretraining large language models (LLMs). The key insight is that samples with different
  loss values should have different importance during training, and this importance
  should adapt dynamically as training progresses.
---

# Dynamic Loss-Based Sample Reweighting for Improved Large Language Model Pretraining

## Quick Facts
- arXiv ID: 2502.06733
- Source URL: https://arxiv.org/abs/2502.06733
- Reference count: 40
- Primary result: Novel loss-based reweighting strategies improve LLM pretraining efficiency with negligible overhead

## Executive Summary
This paper introduces dynamic loss-based sample reweighting strategies for pretraining large language models, demonstrating that samples with different loss values should have different importance during training that adapts as training progresses. The authors propose several strategies including LinUpper (which downweights low-loss samples), Quadratic, and Extremes, with LinUpper performing best across experiments. They provide theoretical analysis showing how these strategies affect convergence bounds and demonstrate improved performance and faster convergence compared to uniform sampling baselines across various model scales (7B, 1.4B, and smaller models).

## Method Summary
The method captures per-sample cross-entropy losses during forward pass, normalizes them to [-1, 1] using batch min/max, applies a strategy function (LinUpper = min(h + 1, 1); Quadratic = 1 - h²; Extremes = |h|), scales via softmax with temperature r, and uses the resulting weights to compute a weighted gradient update. The approach uses a curriculum effect through temperature annealing (high r early for uniform weighting, low r later for sharper reweighting) and caps maximum weight at 2/batch_size to prevent gradient corruption from outliers. The entire process adds negligible computational overhead since losses are already computed during the forward pass.

## Key Results
- LinUpper strategy (downweighting low-loss samples) achieves best overall performance
- Temperature annealing creates curriculum effect, improving convergence
- Method adds negligible computational overhead while improving pretraining efficiency
- Synergies observed when combined with domain-level reweighting methods
- Capping maximum weight at 2/batch_size prevents worst-case optimization while preserving convergence guarantees

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Downweighting low-loss samples leads to faster convergence by focusing gradient updates on more informative samples.
- Mechanism: Theoretical analysis shows that convergence bound contains a term δt = Σᵢ(1/M - wᵢ)(fᵢ(θₜ) - fᵢ(θ*)). Assigning smaller weights to samples with low loss gaps makes δt negative, yielding a tighter bound than uniform sampling (where δt = 0).
- Core assumption: The interpolation regime holds—there exists θ* that minimizes all individual sample losses simultaneously, which is plausible for overparameterized neural networks.
- Evidence anchors: [abstract]: "demonstrating that downweighting low-loss samples can lead to faster convergence"; [section]: Theorem 1 analysis showing δt ≤ 0 when downweighting low loss-gap samples; [corpus]: Limited direct corpus support; related work on hardness-aware reweighting exists but doesn't address this specific theoretical mechanism.
- Break condition: If low-loss samples encode rare but critical patterns (e.g., edge cases, underrepresented domains), systematic downweighting could degrade generalization.

### Mechanism 2
- Claim: Temperature annealing (r scheduling) creates a curriculum effect, starting uniform and progressively applying sharper reweighting.
- Mechanism: High r → softmax over strategy scores approximates uniform distribution; low r → weights concentrate on high-score samples. This lets the model first learn diverse features before focusing on harder samples.
- Core assumption: Early-training loss values are less informative for importance estimation because the model is undertrained.
- Evidence anchors: [section]: "Early in training, when the model is still learning fundamental patterns and losses are less informative, we employ a more uniform weighting scheme to ensure diverse feature learning"; [section]: Figure 1 right panel shows how LinUpper weights approach uniform as r increases; [corpus]: No direct corpus support for this curriculum mechanism in LLM pretraining.
- Break condition: If r is annealed too quickly, may discard useful samples before model can learn from them; if too slowly, benefits are delayed.

### Mechanism 3
- Claim: Capping maximum weight at 2/batch_size prevents gradient corruption from outlier samples while preserving convergence guarantees.
- Mechanism: Theoretical analysis requires max(wᵢ) ≤ 2/b for convergence bound to hold. This prevents DRO-style worst-case optimization that can overfit to noisy/hard samples.
- Core assumption: High-loss samples may include outliers (noisy labels, distribution shift) rather than genuinely informative hard examples.
- Evidence anchors: [section]: "eliminates worst-case importance assignment strategies... which are prone to overfitting to outliers"; [section]: Figure 6 empirically shows max weights stay well below the 0.0156 bound for 7B model; [corpus]: Sample Weight Averaging paper addresses stability through reweighting, supporting the general principle.
- Break condition: If your data has legitimately hard but valuable samples (not outliers), the cap may artificially limit learning signal.

## Foundational Learning

- Concept: Importance-weighted gradient descent
  - Why needed here: Core update changes from θₜ₊₁ = θₜ - η∇f(θₜ) to θₜ₊₁ = θₜ - ηΣᵢwᵢ∇fᵢ(θₜ), introducing biased gradients that require careful analysis.
  - Quick check question: If all wᵢ = 1/M, do you recover standard gradient descent? What happens to the gradient bias?

- Concept: KL-divergence regularization in optimization
  - Why needed here: Proposition 1 shows LinUpper with temperature r is the optimal solution to a KL-regularized objective, preventing degenerate solutions.
  - Quick check question: As r → 0, what happens to the optimal weights? Why is this undesirable?

- Concept: Single-epoch training constraints
  - Why needed here: LLM pretraining typically sees each sample once, making history-based methods (gradient norms across epochs) inapplicable.
  - Quick check question: Why can't we use methods like Loshchilov & Hutter (2015) that track per-sample statistics over multiple epochs?

## Architecture Onboarding

- Component map: Loss extraction -> Normalization -> Strategy function -> Temperature scaling -> Weight normalization -> Backward pass

- Critical path:
  1. Forward pass computes per-sample cross-entropy losses (already computed)
  2. Gather losses across GPUs if using distributed training
  3. Normalize: h = 2(loss - min)/(max - min) - 1
  4. Apply strategy → s = min(h + 1, 1) for LinUpper
  5. Scale: scaled = exp((s - max(s))/r)
  6. Normalize weights, apply to loss before backward
  7. Schedule r from ~100 (warmup) → 0.4-1.0 (main training)

- Design tradeoffs:
  - **Strategy**: LinUpper best for general use; Quadratic useful for very noisy data; Extremes generally underperforms
  - **Temperature r**: Lower = more aggressive reweighting but risk of instability; r=0.4-1.0 works well
  - **Overhead**: Near-zero—only adds simple tensor operations on already-computed losses

- Failure signatures:
  - **Divergence early in training**: r too low during warmup; increase initial r or extend warmup
  - **No improvement over baseline**: r too high (effectively uniform); reduce final r value
  - **Perplexity spikes**: Weight distribution too extreme; check max weights are < 2/batch_size

- First 3 experiments:
  1. **Baseline comparison**: Implement LinUpper (r starts at 100 for 500 warmup steps, anneals to 0.4) on 124M model with SlimPajama, compare perplexity vs uniform across all 7 domains after 20K steps
  2. **Temperature sweep**: Run same setup with final r ∈ {0.2, 0.4, 0.6, 0.8, 1.0} to find optimal aggressiveness for your data characteristics
  3. **Combination test**: Add LinUpper on top of DoGE or DoReMi domain reweighting, evaluate on few-shot benchmarks (LogiQA, SciQ, PiQA) to verify reported synergies

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the "Quadratic" reweighting strategy compare to "LinUpper" when pretraining is conducted on datasets with significantly higher noise levels or reduced manual curation?
- Basis in paper: [inferred] The authors hypothesize that the Quadratic strategy, which down-weights both high-loss (potential outliers) and low-loss samples, "could be useful when training under noisier and less curated datasets," but they validate this primarily on the curated SlimPajama and FineWeb datasets.
- Why unresolved: The paper's experiments focus on standard pretraining corpora; it does not explicitly benchmark the strategies against synthetic noise or low-quality data to confirm the specific advantage of Quadratic over LinUpper in that regime.
- What evidence would resolve it: Ablation studies on datasets with injected label noise or uncurated web-scraped data showing Quadratic outperforming LinUpper in perplexity and downstream task accuracy.

### Open Question 2
- Question: Does the limited capacity of smaller language models fundamentally inhibit their ability to benefit from dynamic loss-based reweighting compared to larger models?
- Basis in paper: [inferred] The authors observe that smaller models (e.g., GPT2-mini) show less significant improvements than larger models (e.g., Llama-7B) and hypothesize this is due to limited capacity to exploit the reweighting.
- Why unresolved: It is unclear if the diminishing returns are an intrinsic property of small model capacity or simply a function of the training duration/learning rate sensitivity not fully tuned for smaller scales in this study.
- What evidence would resolve it: Experiments scaling model size continuously while holding data and training steps constant to identify if a specific parameter count threshold exists where reweighting benefits abruptly increase.

### Open Question 3
- Question: To what extent do the theoretical convergence guarantees derived using gradient norms apply to the practical implementation which relies solely on loss values?
- Basis in paper: [inferred] The authors provide a non-convex convergence bound (Theorem 4) that relies on reweighting based on gradient norms, but they implement the algorithm using loss values because gradient norms are computationally expensive.
- Why unresolved: There is a theoretical gap between the analyzed proxy (gradient norms) and the actual heuristic used in the LLM experiments (loss values), leaving the theoretical grounding of the empirical success approximate.
- What evidence would resolve it: Empirical analysis comparing the correlation between sample loss and gradient norm during LLM pretraining, or a modified theoretical analysis proving convergence bounds specifically for loss-based weights in the non-convex setting.

### Open Question 4
- Question: Can dynamic loss-based reweighting mitigate robustness issues in adversarial machine learning or improve performance on imbalanced classification tasks?
- Basis in paper: [explicit] The authors explicitly state in the "Related Work" section that "our reweighting mechanisms also have the potential to be studied under these contexts [adversarial ML, domain adaptation, imbalanced classification], which we leave as future work."
- Why unresolved: The current study focuses exclusively on general pretraining and perplexity/reasoning benchmarks, without evaluating the method's interaction with adversarial examples or class imbalance.
- What evidence would resolve it: Experiments applying the LinUpper or Quadratic strategies to fine-tuning tasks involving adversarial attacks or datasets with known class imbalances (e.g., binary classification with rare classes).

## Limitations
- Method assumes single-epoch training, limiting applicability to standard pretraining scenarios
- Theoretical analysis relies on strong assumptions about interpolation regime and smooth loss landscapes
- Temperature scheduling parameters are not fully specified, requiring tuning for new datasets
- Maximum weight capping may limit learning from genuinely hard but valuable samples

## Confidence
- **High Confidence**: The core observation that loss-based reweighting adds negligible computational overhead and can improve pretraining efficiency when combined with domain-level methods
- **Medium Confidence**: The theoretical convergence bounds and their practical implications; the specific optimal temperature scheduling for different model scales
- **Low Confidence**: Generalization to extremely large models (70B+), effectiveness on highly noisy datasets with many outliers, and performance outside the autoregressive language modeling paradigm

## Next Checks
1. **Temperature Schedule Validation**: Systematically test different temperature annealing schedules (linear vs exponential decay, different warmup durations) across model scales to establish robust guidelines for hyperparameter selection.
2. **Noise Robustness Test**: Evaluate LinUpper's performance on intentionally corrupted datasets with varying levels of label noise to determine the method's robustness to outliers and identify failure modes.
3. **Cross-Paradigm Transfer**: Apply the loss-based reweighting framework to non-autoregressive pretraining tasks (masked language modeling, contrastive learning) to assess generalizability beyond the current scope.