---
ver: rpa2
title: Zero-Shot Policy Transfer in Reinforcement Learning using Buckingham's Pi Theorem
arxiv_id: '2510.08768'
source_url: https://arxiv.org/abs/2510.08768
tags:
- policy
- transfer
- scaled
- contexts
- context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper demonstrates that Buckingham\u2019s Pi Theorem enables\
  \ zero-shot transfer of reinforcement learning policies to dynamically similar contexts\
  \ without retraining. The method scales a pre-trained policy\u2019s observations\
  \ and actions through dimensionless space using dimensional analysis."
---

# Zero-Shot Policy Transfer in Reinforcement Learning using Buckingham's Pi Theorem

## Quick Facts
- arXiv ID: 2510.08768
- Source URL: https://arxiv.org/abs/2510.08768
- Reference count: 22
- Primary result: Buckingham's Pi Theorem enables zero-shot transfer of RL policies to dynamically similar contexts without retraining.

## Executive Summary
This paper introduces a method for zero-shot transfer of reinforcement learning policies to new physical contexts using Buckingham's Pi Theorem. The approach scales a pre-trained policy's observations and actions through dimensionless space using dimensional analysis, allowing the same policy to maintain optimal performance across dynamically similar contexts. Tested on simulated pendulum, physical pendulum (sim-to-real), and HalfCheetah, the scaled policies maintained optimal performance on similar contexts and outperformed naive transfers on non-similar ones. The method significantly expands the volume of contexts where the original policy remains effective.

## Method Summary
The method transforms observations from a source context to a target context using Buckingham's Pi Theorem, creating dimensionless representations that preserve the policy's optimality. For a given basis $\beta$ of physical parameters, the policy takes dimensionless observations and outputs dimensionless actions, which are then transformed back to the target context's dimensions. When two contexts are dynamically similar (identical dimensionless parameters), the transformed policy remains optimal without retraining. The approach requires identifying all relevant physical parameters, selecting a basis that spans the dimension space, and deriving forward and inverse transforms for observations and actions.

## Key Results
- Scaled policies maintained optimal performance on dynamically similar contexts (zero distance in dimensionless space)
- On non-similar contexts, scaled policies consistently outperformed naive transfers
- Sim-to-real transfer on physical pendulum succeeded when parameters were accurately measured and included in context
- The method expanded the volume of contexts where the original policy remained effective

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dimensionless scaling preserves optimal policy behavior across contexts with identical dimensionless parameters.
- Mechanism: Transform observations via $\phi_\beta$ to dimensionless space using basis $\beta = \{m, l, g\}$, execute policy, then inverse-transform actions back. When two contexts $C_1, C_2$ satisfy $d_\beta(C_1, C_2) = 0$, the dimensionless observations are identical, so the policy produces equivalent dimensionless actions.
- Core assumption: The basis $\beta$ spans all physically relevant dimensions for the task dynamics.
- Evidence anchors:
  - [abstract]: "scales a pre-trained policy's observations and actions through dimensionless space using dimensional analysis"
  - [Section II-C]: "if $C_1$ and $C_2$ are similar and $\pi(x, C_1)$ is optimal for $C_1$, then $_{C_1}\pi^{C_2}_\beta(x)$ remains optimal for $C_2$"
  - [corpus]: Related work "Improving Controller Generalization with Dimensionless Markov Decision Processes" supports dimensionless training improving generalization.
- Break condition: Incomplete basis (parameters missing from $\beta$ that affect dynamics), e.g., HalfCheetah body length $L$ variations not captured by $\beta = \{m, l_0, g\}$.

### Mechanism 2
- Claim: Non-similar contexts benefit from partial scaling even when full similarity is impossible.
- Mechanism: Scale to the nearest similar context in dimensionless space. The policy experiences smaller distribution shift than naive transfer because scaling aligns the subset of parameters in $\beta$.
- Core assumption: The policy has inherent robustness along some context directions that transfers with scaling.
- Evidence anchors:
  - [abstract]: "on non-similar contexts, the scaled policy consistently outperforms the naive transfer"
  - [Section V]: "scaling to a more similar context results in a generally better performance than a completely naive policy transfer"
  - [corpus]: No direct corpus evidence for partial similarity behavior.
- Break condition: Policy lacks natural robustness; test context diverges dramatically in non-basis parameters.

### Mechanism 3
- Claim: Sim-to-real transfer succeeds when physical system parameters can be measured accurately and included in context.
- Mechanism: Identify physical parameters (mass, length, torque limits), compute dimensionless transformations using measured values, scale policy actions/observations accordingly.
- Core assumption: Reality gap is primarily parametric (measurable quantity differences) rather than structural (unmodeled dynamics).
- Evidence anchors:
  - [abstract]: "physical pendulum (sim-to-real)" with "scaled policies maintained optimal performance on similar contexts"
  - [Section III-C]: Real pendulum contexts selected to be similar to $C_0$ achieved comparable performance
  - [corpus]: Prior work [Makarun et al., Section I] required further tuning when physical parameters couldn't be matched exactly.
- Break condition: Unmeasurable parameters, friction/damping not in model, actuator dynamics differ from assumptions.

## Foundational Learning

- Concept: **Buckingham's Pi Theorem and dimensional analysis**
  - Why needed here: Core mathematical foundation for the entire scaling transformation. Without understanding how to construct dimensionless groups and choose bases, you cannot implement the method.
  - Quick check question: Given a system with quantities mass $m$, length $l$, time $t$, and force $F$, can you identify a valid basis and express $F$ in dimensionless form?

- Concept: **Reinforcement learning policy representations and context embedding**
  - Why needed here: Understanding how policies implicitly encode physical parameters helps identify what must be scaled and what the "context" $C$ comprises beyond just robot parameters.
  - Quick check question: For a policy trained with torque limit $\tau_{max} = 8$ Nm, what happens if you deploy it with $\tau_{max} = 4$ Nm without any modification?

- Concept: **Dynamic similarity in physical systems**
  - Why needed here: Determines when zero-shot transfer is guaranteed optimal vs. when it's merely better than naive. Critical for setting expectations and diagnosing failures.
  - Quick check question: Two pendulums with $(m=1, l=1, g=9.8)$ and $(m=2, l=2, g=9.8)$ are dynamically similar. What about $(m=1, l=1, g=9.8)$ and $(m=2, l=1, g=9.8)$?

## Architecture Onboarding

- Component map:
  - Context definition: Set of quantities $C = \{m, g, l, \tau_{max}, t_f, w_\theta, w_\tau, ...\}$ including system parameters, cost weights, temporal parameters
  - Basis selection: Choose $\beta \subseteq C$ spanning the dimension space (typically $\beta = \{m, l, g\}$ for mechanical systems)
  - Forward transform $\phi_\beta$: Convert dimensional observations to dimensionless (e.g., $\tilde{\theta} = \theta$, $\tilde{\dot{\theta}} = \dot{\theta} \sqrt{l/g}$)
  - Policy: Operates on dimensionless observations, outputs dimensionless actions
  - Inverse transform $\phi_\beta^{-1}$: Convert dimensionless actions to dimensional (e.g., $\tau = \tilde{\tau} \cdot mgl$)

- Critical path:
  1. Identify all quantities affecting dynamics and cost (Table I, III pattern)
  2. Select basis $\beta$ that spans MLT dimensions
  3. Derive $\phi_\beta$ and $\phi_\beta^{-1}$ for each observation/action dimension (Tables II, IV)
  4. Implement scaling wrapper: `${}_{C_0}\pi^{C_t}_\beta(x) = \phi_{\beta_t}^{-1} \circ \pi \circ \phi_{\beta_0}(x)$ (Eq. 9 simplified)
  5. Validate on known-similar contexts before testing non-similar ones

- Design tradeoffs:
  - Basis completeness vs. practicality: Larger basis captures more physics but requires more parameter knowledge
  - Training in dimensionless space vs. scaling pre-trained policies: Paper shows scaling works; prior work [Charvet et al.] suggests dimensionless training adds robustness
  - Exact similarity vs. partial scaling: Exact similarity guarantees optimality; partial scaling provides improvement without guarantees

- Failure signatures:
  - NaN/inf in transforms: Basis doesn't span dimensions, check $m_b$ computation in Eq. 5
  - Performance matches naive: Basis parameters don't vary between contexts (scaling is identity)
  - Worse than naive: Incorrect transform direction (applying $\phi_{\beta_t}$ instead of $\phi_{\beta_0}$ to observations)
  - Sim-to-real gap persists: Unmodeled parameters (friction, actuator dynamics, sensor delays) not in $C$

- First 3 experiments:
  1. Sanity check on simulated pendulum: Train policy at $C_0$, test on $C_t$ where only $m$ varies (e.g., $m \in [0.1, 10]$). Compare scaled vs. naive transfer. Expect: scaled maintains ~constant reward, naive degrades. This validates basic mechanism.
  2. Non-similar context test: Vary $\tau_{max}$ while keeping $m, l, g$ constant. This is NOT captured by basis $\beta = \{m, l, g\}$. Expect: scaled and naive perform similarly (both suboptimal but functional if excess torque available).
  3. Basis ablation: Use incomplete basis $\beta = \{m, g\}$ (omit $l$) and test on varying lengths. Expect: scaled transfer fails for length variations, confirming basis must span all varying parameters.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a context estimator be integrated to maintain policy robustness when physical parameters are unknown or vary dynamically during real-world operation?
- Basis in paper: [explicit] The conclusion states: "A study of the effectiveness of a context estimator to improve the robustness of a policy in the real world would be valuable."
- Why unresolved: The current method assumes the target context parameters ($C_t$) are known a priori to compute the scaling transform, which is rarely true in unstructured environments.
- What evidence would resolve it: Empirical results from a real-world experiment where the scaling adapts in real-time using estimated context parameters rather than ground truth.

### Open Question 2
- Question: Does the dimensional scaling approach remain effective on significantly more complex robotic systems, such as those with contact-rich dynamics or manipulation tasks?
- Basis in paper: [explicit] The authors propose: "Future work could replicate the experimental study on a more complex system."
- Why unresolved: The method is validated only on pendulums and the HalfCheetah (locomotion); it is unclear if the assumptions regarding dynamic similarity hold for systems where contact forces dominate.
- What evidence would resolve it: Successful zero-shot transfer demonstrations on high-dimensional manipulation tasks (e.g., robotic grasping or legged locomotion on rough terrain).

### Open Question 3
- Question: Is there a systematic or automated method to select the optimal basis $\beta$ to prevent dimensionally relevant parameters from being treated as "transparent" perturbations?
- Basis in paper: [inferred] The paper notes that changes in HalfCheetah body length were "transparent to the basis [which] captures mass, length, and gravity," resulting in naive transfer performance for that parameter.
- Why unresolved: Basis selection is currently manual and heuristic; a poor choice renders the scaling ineffective for certain physical parameters.
- What evidence would resolve it: An algorithm that automatically identifies the full set of governing variables to include in the basis, ensuring scaled performance across all varying parameters.

## Limitations
- Limited empirical scope to 3 simulated environments and 1 real hardware test constrains generalizability
- Basis selection requires expert knowledge and incomplete bases can silently fail
- Method assumes policy's generalization capability; cannot overcome fundamental distributional shifts in learned function

## Confidence

**High Confidence**: The core mechanism (dimensionless scaling for zero-shot transfer between dynamically similar contexts) is mathematically sound and validated in the pendulum and HalfCheetah experiments.

**Medium Confidence**: The claim of consistent outperformance on non-similar contexts is supported by experiments but may not generalize to all types of parametric variations or more complex tasks.

**Medium Confidence**: The sim-to-real demonstration on the physical pendulum shows promise, but the limited parameter variation and simplified task make broader claims tentative.

## Next Checks
1. Test on a more complex task (e.g., humanoid locomotion) with a wider range of parametric variations to assess scalability and robustness.
2. Conduct systematic ablation studies varying basis completeness to quantify the impact of missing parameters on transfer performance.
3. Evaluate on a task with known structural (non-parametric) differences between sim and real to isolate the method's limitations in handling reality gaps.