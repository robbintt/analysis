---
ver: rpa2
title: Interactive Video Generation via Domain Adaptation
arxiv_id: '2505.24253'
source_url: https://arxiv.org/abs/2505.24253
tags:
- diffusion
- video
- denoising
- mask
- control
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of Interactive Video Generation
  (IVG), where users control object trajectories in text-to-video diffusion models.
  The authors identify two key failure modes: internal covariate shift caused by attention
  masking and initialization gap due to mismatched latent distributions.'
---

# Interactive Video Generation via Domain Adaptation

## Quick Facts
- arXiv ID: 2505.24253
- Source URL: https://arxiv.org/abs/2505.24253
- Reference count: 40
- Authors: Ishaan Rawal, Suryansh Kumar
- Primary result: SOTA performance on Zeroscope with mIoU up to 33.82% and FID 131.19

## Executive Summary
This paper addresses Interactive Video Generation (IVG) by controlling object trajectories in text-to-video diffusion models through attention masking. The authors identify two key failure modes: internal covariate shift from masked attention and initialization gap due to mismatched latent distributions. They propose mask normalization (aligning masked/unmasked attention distributions) and temporal intrinsic denoising (enforcing spatio-temporal consistency) inspired by domain adaptation principles. Experiments show state-of-the-art performance on Zeroscope with improved trajectory control and perceptual quality.

## Method Summary
The method applies inference-time domain adaptation to pre-trained video diffusion models for interactive trajectory control. Mask normalization uses Exact Feature Distribution Matching (EFDM) to align feature distributions between masked and unmasked attention outputs during the first 4 denoising steps. Temporal intrinsic denoising performs M=2 latent refinement iterations before each DDIM step, combining classifier-free guidance with temporal prior gradients measuring foreground pixel correlation across frames. The approach maintains generation quality while enabling precise trajectory control without retraining the model.

## Key Results
- Achieves state-of-the-art performance on Zeroscope with mIoU up to 33.82% and FID 131.19
- Mask normalization maintains activation variance consistency during frozen steps (Figure 2a)
- Temporal prior with cg=10000 improves mIoU from 25.97% to 33.82% versus no temporal guidance
- Successfully controls object trajectory while preserving perceptual quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Attention masking causes internal covariate shift because pretrained models weren't trained with masked attention; mask normalization mitigates this by aligning feature distributions.
- Mechanism: Exact Feature Distribution Matching (EFDM) maps masked attention outputs to unmasked attention outputs by rank-order matching (k-th smallest element of Am maps to k-th smallest of Au). Applied as pre-normalization layer before residual connection.
- Core assumption: Unmasked attention outputs represent "target" distribution model expects; aligning masked outputs maintains generation quality without retraining.
- Evidence: Figure 2a shows masked attention exhibits variance drop relative to baseline, while proposed method maintains baseline variance.
- Break condition: If trajectory control requires different activation distributions than unmasked attention provides, mask normalization could reduce trajectory adherence while improving perceptual quality.

### Mechanism 2
- Claim: Randomly initialized latents fail to capture inter-frame correlations; intrinsic denoising leverages diffusion architecture prior to iteratively refine noise.
- Mechanism: Before each DDIM step, perform M intrinsic denoising iterations: z_m^t = z_{m-1}^t + η_l[∇log p_t + cg·∇τ] + η_k·ε, combining classifier-free guidance with temporal prior gradients.
- Core assumption: Diffusion U-Net architecture encodes useful inductive priors extractable through iterative refinement without changing weights.
- Evidence: Ablation shows cg=10000 achieves 33.82%/34.73% mIoU vs. 25.97%/22.32% for cg=0 (no temporal prior).
- Break condition: If intrinsic denoising iterations insufficient or step size poorly tuned, latents may not converge, producing incoherent motion.

### Mechanism 3
- Claim: Cross-frame temporal consistency enforced via gradients from differentiable temporal prior measuring foreground pixel correlation.
- Mechanism: Temporal consistency metric τ computes Pearson correlation between foreground pixels sampled from bounding box regions in adjacent frames; gradients ∇τ guide intrinsic denoising.
- Core assumption: Pearson correlation of foreground pixels in latent space is meaningful proxy for temporal consistency.
- Evidence: Ablation shows decreasing cg progressively loses trajectory control (Figure 7).
- Break condition: If cg too high, temporal consistency over-emphasized at perceptual cost; if too low, spatio-temporal coherence degrades.

## Foundational Learning

- **Concept: Internal Covariate Shift**
  - Why needed: Understanding why attention masking degrades quality requires recognizing activations can shift outside training distribution even with layer normalization.
  - Quick check: Why is standard layer normalization in transformer blocks insufficient to prevent variance shift from attention masking?

- **Concept: Diffusion Denoising Prior & Initialization Gap**
  - Why needed: Intrinsic denoising builds on understanding how imperfect noise schedules create distribution gaps between random initialization and model expectations.
  - Quick check: What is the "initialization gap" and why does random noise fail to capture video-specific inter-frame correlations?

- **Concept: Exact Feature Distribution Matching**
  - Why needed: Mask normalization uses EFDM (rank-based histogram matching) to align distributions; understanding requires grasp of optimal transport basics.
  - Quick check: How does EFDM preserve original layer-wise distribution while reducing variance shift between masked and unmasked outputs?

## Architecture Onboarding

- **Component map**: Text prompt + N-frame bounding boxes → Mask Normalization (first 4 steps) → Temporal Intrinsic Denoising (M=2 iterations) → DDIM sampling → 24-frame video

- **Critical path**:
  1. Input: text prompt + N-frame bounding box trajectory
  2. Construct M_self, M_cross attention masks (foreground-to-foreground, background-to-background only)
  3. For t=T→1: Apply masked attention + mask normalization during frozen steps → Run M intrinsic denoising iterations with temporal prior → DDIM sampling step
  4. Decode latent to 24-frame video

- **Design tradeoffs**:
  - Frozen steps (4): More steps strengthen trajectory control but risk perceptual degradation
  - M (2): More iterations improve noise refinement at compute cost
  - cg (10000): Higher values enforce stronger temporal coherence but may introduce artifacts
  - γ (0.05): Controls intrinsic denoising step magnitude

- **Failure signatures**:
  - Trajectory deviation: Subject drifts from boxes → increase cg or frozen steps
  - Extra subjects hallucinated: Multiple instances appear → temporal prior may be over-aggressive
  - Perceptual artifacts: Quality drops → verify mask normalization applied; reduce cg
  - Gradient normalization issues: Spatio-temporal distortions → use unnormalized gradients

- **First 3 experiments**:
  1. Ablation on frozen steps (0, 2, 4, 6) measuring mIoU vs. FID/KID tradeoff; record activation variance curves as in Figure 2.
  2. Parameter sweep on cg (0, 2000, 5000, 7000, 10000) and M (1, 2, 3, 4) across static and dynamic bounding box scenarios.
  3. Boundary condition testing with extreme trajectories (fast motion, large jumps) to identify when temporal prior gradients become unstable.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the proposed domain adaptation framework be extended to multi-subject interactive video generation with simultaneous camera motion control?
- Basis: Conclusion states study is limited to single-subject, static-camera scenarios, but conceptual tools lay groundwork for multi-agent systems and simultaneous camera control.
- Why unresolved: Current masking scheme assumes single binary foreground/background partition; multi-subject scenarios require handling competing attention masks and conflicting temporal priors.
- Evidence needed: Demonstration on multi-subject benchmarks with quantitative metrics (CLIP-SIM, mIoU per subject) comparable to single-subject performance.

### Open Question 2
- Question: What evaluation benchmarks and metrics can better capture nuanced trade-offs between trajectory control accuracy and perceptual quality in IVG?
- Basis: Conclusion underscores limitations of current evaluation protocols and advocates for more holistic benchmarks reflecting nuanced challenges of IVG.
- Why unresolved: Authors show JeDi scores can be deceptively low when models ignore trajectory constraints, and Coverage/mIoU don't fully capture temporal coherence.
- Evidence needed: New benchmark suite with human preference studies correlating with proposed metrics, specifically measuring spatio-temporal consistency under challenging trajectory constraints.

### Open Question 3
- Question: Can gradient normalization strategies be improved to balance spatio-temporal control without introducing global coherence artifacts?
- Basis: Section B.3 notes normalized gradients frequently introduce spatio-temporal inconsistencies while sometimes improving control.
- Why unresolved: Trade-off between gradient magnitude information and stable scaling remains unexplored; current unnormalized approach requires manual tuning of cg.
- Evidence needed: Adaptive gradient scaling method maintaining both directional information and magnitude-aware scaling, validated through ablation studies showing stable performance across varying cg values.

## Limitations
- Single-subject, static-camera scenarios limit real-world applicability
- High guidance scale (cg=10000) may indicate implementation artifacts rather than fundamental property
- Temporal prior gradient mechanism lacks validation across diverse motion patterns

## Confidence

- **High Confidence**: Mask normalization reduces internal covariate shift and maintains generation quality during frozen steps
- **Medium Confidence**: Temporal intrinsic denoising improves spatio-temporal consistency
- **Low Confidence**: Temporal prior gradient correctly guides toward coherent outputs across all motion patterns

## Next Checks

1. **Distribution Alignment Verification**: Implement EFDM with batch-wise sorting and verify masked/unmasked attention output distributions match during frozen steps, checking both mean and variance preservation across different trajectory complexities.

2. **Temporal Prior Robustness**: Test temporal prior effectiveness across motion types (slow vs. fast, linear vs. curved trajectories) to identify failure modes where Pearson correlation may not capture meaningful temporal consistency.

3. **Gradient Stability Analysis**: Systematically vary cg from 1000 to 20000 and monitor for numerical overflow, gradient vanishing, or trajectory control degradation to identify practical upper bound for stable operation.