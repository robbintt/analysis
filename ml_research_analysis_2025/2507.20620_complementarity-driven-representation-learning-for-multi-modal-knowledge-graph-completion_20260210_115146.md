---
ver: rpa2
title: Complementarity-driven Representation Learning for Multi-modal Knowledge Graph
  Completion
arxiv_id: '2507.20620'
source_url: https://arxiv.org/abs/2507.20620
tags:
- knowledge
- modality
- multimodal
- graph
- fusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes MoCME, a framework for multi-modal knowledge
  graph completion (MMKGC) that addresses the challenge of modality imbalance and
  insufficient complementarity modeling in existing methods. The core idea is to explicitly
  model both intra-modal and inter-modal complementarity through a Mixture of Complementary
  Modality Experts (MoCME) architecture.
---

# Complementarity-driven Representation Learning for Multi-modal Knowledge Graph Completion

## Quick Facts
- **arXiv ID:** 2507.20620
- **Source URL:** https://arxiv.org/abs/2507.20620
- **Authors:** Lijian Li
- **Reference count:** 40
- **Primary result:** MoCME achieves state-of-the-art performance on MMKGC, outperforming existing approaches on five benchmarks with significant improvements in MRR, Hit@1, and Hit@10 metrics.

## Executive Summary
This paper introduces MoCME, a framework for multi-modal knowledge graph completion (MMKGC) that addresses the challenge of modality imbalance and insufficient complementarity modeling in existing methods. The core innovation lies in explicitly modeling both intra-modal and inter-modal complementarity through a Mixture of Complementary Modality Experts (MoCME) architecture. The framework employs mutual information-based fusion to prioritize non-redundant signals and an entropy-guided negative sampling mechanism to focus on informative training samples. Extensive experiments on five benchmark datasets demonstrate that MoCME achieves state-of-the-art performance, surpassing existing approaches with significant improvements in key evaluation metrics.

## Method Summary
MoCME addresses MMKGC through a two-pronged approach: Complementarity-guided Modality Knowledge Fusion (CMKF) and Entropy-guided Negative Sampling (EGNS). The CMKF module fuses multi-view and multi-modal embeddings by estimating mutual information between expert outputs to identify complementary signals, while the EGNS mechanism dynamically prioritizes uncertain negative samples based on prediction entropy. The framework employs pre-trained encoders (VGG16 for images, BERT for text) followed by modality-specific expert networks that map embeddings to diverse semantic subspaces. Mutual information quantifies complementarity, guiding the fusion process, while RotatE scoring and a weighted contrastive loss function complete the training pipeline. The method operates with fixed hyperparameters including 256-dimensional embeddings, batch size of 1024, and 1000 training epochs.

## Key Results
- MoCME achieves state-of-the-art performance on all five benchmark datasets (MKG-W, MKG-Y, DB15K, TIVA, KVC16K)
- Significant improvements in MRR, Hit@1, and Hit@10 metrics compared to existing MMKGC approaches
- Ablation studies demonstrate the effectiveness of both CMKF and EGNS mechanisms
- Robust performance across diverse modalities including image, text, numeric, audio, and video

## Why This Works (Mechanism)

### Mechanism 1: Mutual Information-based Complementary Fusion
- **Claim:** Explicitly weighting modalities and expert views based on low mutual information (uniqueness) improves representation robustness by prioritizing non-redundant signals.
- **Mechanism:** The model employs the Mutual Information Neural Estimator (MINE) to compute $I(v_i; v_j)$ between embeddings. A softmax function over the negative mutual information assigns weights: views with lower MI (less redundant) receive higher weights. This occurs hierarchically, first aggregating multiple expert views within a modality (intra-modal), then aggregating modalities (inter-modal).
- **Core assumption:** Assumption: Lower mutual information between views/modalities correlates with higher complementary value for the knowledge graph completion task.
- **Evidence anchors:**
  - [abstract]**
: "...explicitly modeling both intra-modal and inter-modal complementarity..."
  - [section 3.2]: "The normalized contribution weight $\omega^a_m$ of each view is computed based on a softmax over the negative mutual information... views that share less mutual information... are considered more complementary."
  - [corpus]: Related works like "Collaboration of Fusion and Independence" explore robustness via fusion, but corpus evidence specifically validating MI-based weighting as superior to attention is limited; reliance is placed on the paper's internal benchmarks.
- **Break condition:** If a modality is completely missing or corrupted, MI estimation may become unstable or biased, potentially causing the weighting mechanism to fail or assign inappropriate dominance to noisy signals.

### Mechanism 2: Entropy-guided Negative Sampling (EGNS)
- **Claim:** Dynamically weighting negative samples based on prediction entropy (uncertainty) enhances training efficiency by focusing on informative hard negatives rather than trivial easy ones.
- **Mechanism:** The model calculates binary entropy $H = -p \log p - (1-p) \log(1-p)$ for negative triples. Samples are categorized into Easy, Ambiguous, and Hard using thresholds $\delta_1, \delta_2$. The loss function applies distinct weights ($\lambda_{easy} < \lambda_{amb} < \lambda_{hard}$), forcing the model to prioritize learning from uncertain samples.
- **Core assumption:** Assumption: High-entropy samples correspond to semantically challenging "hard" negatives that improve discriminative power, rather than just noise or false negatives.
- **Evidence anchors:**
  - [abstract]**
: "...Entropy-guided Negative Sampling (EGNS) mechanism that dynamically prioritizes informative and uncertain negative samples."
  - [section 3.3]: "High-entropy samples correspond to those close to the decision boundary... we propose a type-aware weighted contrastive loss that balances stability and discriminability."
  - [corpus]: The paper "Diffusion-based Hierarchical Negative Sampling" addresses similar challenges in MMKGC, suggesting a consensus that advanced sampling is critical, though specific entropy methods differ.
- **Break condition:** If thresholds $\delta_1, \delta_2$ are poorly tuned (e.g., too strict), the model might overfit to noisy "hard" samples or fail to utilize sufficient training signal from "easy" samples.

### Mechanism 3: Multi-Expert Disentanglement
- **Claim:** Decomposing a single modality embedding into multiple "views" via parallel expert networks captures diverse semantic subspaces better than a single projection.
- **Mechanism:** For each modality, instead of one projection, $K$ independent expert networks map the embedding to different subspaces. The outputs are treated as multi-view representations which are then aggregated via Mechanism 1.
- **Core assumption:** Assumption: Semantic features within a modality (e.g., visual features) are disentangled and better represented by a mixture of subspaces rather than a monolithic vector.
- **Evidence anchors:**
  - [section 3.2]: "We instantiate a group of K independent expert networks... resulting in a set of multi-view embeddings... encourages representation disentanglement."
  - [section 4.5.4]: Ablation study (Fig 2) shows performance peaks when the number of experts matches the number of modalities (e.g., 3 for DB15K), dropping if $K$ is too high or low.
  - [corpus]: "HERGC" also proposes "Heterogeneous Experts," supporting the general efficacy of expert-based decomposition in MMKGs.
- **Break condition:** If the number of experts $K$ is set significantly higher than the inherent semantic factors of the data, the representation space may over-fragment, increasing aggregation difficulty without performance gain.

## Foundational Learning

- **Concept: Mutual Information (MI) Estimation**
  - **Why needed here:** The core fusion logic relies on minimizing redundancy using MINE. You must understand that MI measures statistical dependence; the paper uses it as a proxy for "uniqueness."
  - **Quick check question:** If two expert views are identical, what is their Mutual Information, and how would MoCME weigh them? (Answer: High MI -> Low Weight).

- **Concept: Mixture of Experts (MoE)**
  - **Why needed here:** The architecture moves beyond single encoders. Understanding that "experts" are parallel sub-networks with distinct parameters is key to implementing the CMoE module.
  - **Quick check question:** How does the gating mechanism in standard MoE differ from the "adaptive weighting" in MoCME? (MoCME uses MI, not a learned gate vector).

- **Concept: Contrastive Learning & Negative Sampling**
  - **Why needed here:** The EGNS module modifies the loss landscape. You need to distinguish between uniform sampling and this entropy-weighted approach to debug convergence issues.
  - **Quick check question:** Why would a "hard" negative (high entropy) be more valuable than an "easy" one during training? (Answer: It forces the model to refine the decision boundary rather than confirming obvious distinctions).

## Architecture Onboarding

- **Component map:** Raw Multimodal Data -> Frozen Pre-trained Encoders (VGG16, BERT) -> Projection MLPs -> CMoE (K parallel Experts per modality) -> CMKF (MINE estimates MI -> Softmax produces Weights -> Aggregates views then modalities) -> RotatE Scoring -> Entropy-guided Negative Sampling (EGNS)

- **Critical path:** The **MI calculation loop** (in CMKF) is the most complex addition. Ensure the implementation of MINE allows backpropagation for the weights but does not destabilize the expert training.

- **Design tradeoffs:**
  - **Frozen vs. Fine-tuned Encoders:** The paper freezes VGG/BERT to reduce complexity. Unfreezing might improve feature quality but risks overfitting on small KG datasets.
  - **Expert Count ($K$):** Ablation suggests $K \approx$ number of modalities is a good heuristic, but requires tuning for new datasets.

- **Failure signatures:**
  - **Performance Plateau:** If MRR stalls early, check $\delta_1, \delta_2$ settings. The model may be ignoring hard negatives if thresholds are too permissive.
  - **Collapse to Single Modality:** If inter-modal weights ($\omega_b$) converge to zero for all but one modality, check the MI estimator (MINE) for numerical instability or saturation.

- **First 3 experiments:**
  1. **Baseline Validation:** Run on DB15K with $K=1$ (no experts) vs $K=3$ (config in paper) to confirm the lift from the "Expert" architecture matches the reported ~3.5% MRR delta.
  2. **Threshold Sensitivity:** Perform a grid search on $\delta_1$ and $\delta_2$ (e.g., $\delta_1 \in [0.1, 0.3]$, $\delta_2 \in [0.7, 0.9]$) on a validation set to find the optimal balance between easy/hard negatives.
  3. **Missing Modality Test:** Artificially drop a modality (e.g., Image) for a subset of entities during testing to verify if the "Complementarity" mechanism robustly compensates via other modalities.

## Open Questions the Paper Calls Out
None

## Limitations
- The primary uncertainty lies in the MI estimation implementation, as MINE's stability and convergence are notoriously sensitive to hyperparameters and network architecture choices.
- The paper does not specify the exact architecture of the MINE network or the expert sub-networks, requiring assumptions that may affect reproducibility.
- The entropy thresholds for negative sampling (δ₁=0.2, δ₂=0.8) are fixed without demonstrating sensitivity analysis across diverse datasets.

## Confidence
- **High Confidence:** The experimental results showing MoCME's superior performance on the five benchmark datasets (MRR, Hit@1, Hit@10 improvements) are well-documented and verifiable through the provided ablation studies.
- **Medium Confidence:** The theoretical justification for using mutual information as a complementarity proxy is sound, but the empirical validation that MI-based weighting outperforms alternative attention mechanisms is limited to internal benchmarks without external comparisons.
- **Low Confidence:** The claim that EGNS consistently improves training efficiency across all MMKGC scenarios is uncertain, as the entropy-based hard negative selection assumes well-calibrated probability outputs which may not hold for all encoder architectures.

## Next Checks
1. **Threshold Sensitivity Analysis:** Systematically vary δ₁ and δ₂ across a grid (e.g., δ₁ ∈ [0.1, 0.3], δ₂ ∈ [0.7, 0.9]) on each dataset to identify optimal values and test robustness to distribution shifts.
2. **MI Estimator Ablation:** Replace MINE with a simpler correlation-based weighting (e.g., cosine similarity) to quantify whether the mutual information estimation provides measurable performance gains beyond computational complexity.
3. **Missing Modality Stress Test:** Evaluate MoCME on subsets where one modality is randomly removed during inference to verify whether complementarity modeling genuinely compensates for modality absence rather than relying on redundancy.