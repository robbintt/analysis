---
ver: rpa2
title: Do Thinking Tokens Help or Trap? Towards More Efficient Large Reasoning Model
arxiv_id: '2506.23840'
source_url: https://arxiv.org/abs/2506.23840
tags:
- thinking
- tokens
- reasoning
- arxiv
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper identifies a "thinking trap" in Large Reasoning Models
  (LRMs) where thinking tokens like "wait" and "however" trigger unproductive reasoning
  loops, especially in simple problems. These tokens cause excessive reflection and
  backtracking, increasing response length without improving performance.
---

# Do Thinking Tokens Help or Trap? Towards More Efficient Large Reasoning Model

## Quick Facts
- arXiv ID: 2506.23840
- Source URL: https://arxiv.org/abs/2506.23840
- Reference count: 36
- Key outcome: Dual Policy Preference Optimization (DuP-PO) reduces thinking tokens by 6-20% while improving accuracy on math reasoning benchmarks

## Executive Summary
This paper identifies a critical inefficiency in Large Reasoning Models (LRMs) where certain "thinking tokens" like "wait" and "however" trigger unproductive reasoning loops, particularly in simple problems. These tokens cause excessive reflection and backtracking without improving performance. The authors propose Dual Policy Preference Optimization (DuP-PO), a training method that balances thinking-heavy and thinking-free responses while suppressing problematic thinking tokens through fine-grained token-level advantage scaling. Experiments demonstrate that DuP-PO achieves significant token reduction while improving accuracy across five math reasoning benchmarks.

## Method Summary
The authors introduce Dual Policy Preference Optimization (DuP-PO) to address thinking traps in LRMs. The method balances training samples between thinking-heavy and thinking-free responses, applies fine-grained token-level advantage scaling to penalize inefficient thinking tokens, and uses policy shaping to explicitly suppress problematic tokens like "wait" and "however." This approach is trained on math reasoning datasets and evaluated against both the base model and GRPO baselines.

## Key Results
- DuP-PO achieves 6-20% token reduction while improving accuracy on math reasoning benchmarks
- Outperforms both base model and GRPO baselines in token efficiency and accuracy
- Minimal training cost compared to existing methods

## Why This Works (Mechanism)
The mechanism works by identifying and penalizing thinking tokens that trigger unproductive reasoning loops. By using balanced training data and fine-grained token-level advantage scaling, the model learns to suppress inefficient thinking patterns while maintaining effective reasoning. The policy shaping component directly targets problematic tokens, preventing them from initiating excessive backtracking in simple problems.

## Foundational Learning

**Thinking tokens** - Special tokens that trigger reasoning processes in LRMs; needed to understand what triggers excessive computation; check by identifying tokens that cause reasoning loops in your model

**Policy preference optimization** - Training method that optimizes policies based on preference comparisons; needed to understand the training framework; check by comparing reward-based vs preference-based optimization approaches

**Token-level advantage scaling** - Fine-grained scaling of rewards at individual token level; needed to understand how specific tokens are penalized; check by examining token-level reward distributions before and after training

**Reward model reliability** - The ability of reward models to distinguish high-quality responses; needed to understand limitations of the approach; check by testing reward model performance across different task types

## Architecture Onboarding

**Component map**: LRM -> DuP-PO training -> Balanced dataset -> Token-level advantage scaling -> Policy shaping -> Optimized LRM

**Critical path**: Input problem -> Token generation -> Thinking token detection -> Advantage calculation -> Policy update -> Output response

**Design tradeoffs**: Balances token efficiency against reasoning depth; prioritizes simplicity over complex reasoning patterns; trades some reasoning flexibility for computational efficiency

**Failure signatures**: Excessive token generation without performance improvement; inability to handle complex reasoning tasks; over-suppression of necessary thinking patterns

**First experiments**:
1. Test on non-mathematical reasoning tasks to assess cross-domain effectiveness
2. Vary reward model quality to test sensitivity to reward assumptions
3. Apply to alternative LRM architectures to verify generalizability

## Open Questions the Paper Calls Out

The paper acknowledges that the generalizability of the thinking trap across different LRM architectures and tasks beyond mathematical reasoning remains unclear. It's uncertain whether similar token-level inefficiencies occur in other problem types or with models using different prompting strategies. Additionally, the effectiveness relies on reward models reliably distinguishing high-quality thinking-free responses, which may not hold in more ambiguous or subjective tasks.

## Limitations

- Limited generalizability across different LRM architectures and reasoning tasks
- Effectiveness depends on reliable reward models for distinguishing quality responses
- Fine-grained token-level advantage scaling adds complexity and may be sensitive to training data distribution shifts

## Confidence

- **High**: Empirical demonstration of token reduction and accuracy improvements on tested math benchmarks
- **Medium**: Broader claim that thinking tokens inherently trap reasoning (primarily demonstrated in controlled synthetic settings)
- **Low**: Assertion that thinking trap is universal across all LRMs (limited model and task diversity)

## Next Checks

1. Test DuP-PO on non-mathematical reasoning tasks (e.g., commonsense QA, code generation) to assess cross-domain effectiveness and identify any domain-specific failure modes.

2. Evaluate the impact of varying reward model quality and training data diversity on the fine-grained token-level advantage scaling component, to determine sensitivity to reward model assumptions.

3. Conduct ablation studies with alternative LRM architectures (e.g., different base models or reasoning strategies) to verify whether the thinking trap and DuP-PO's effectiveness generalize beyond the specific models used in this work.