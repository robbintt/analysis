---
ver: rpa2
title: Ontology- and LLM-based Data Harmonization for Federated Learning in Healthcare
arxiv_id: '2505.20020'
source_url: https://arxiv.org/abs/2505.20020
tags:
- data
- research
- clinical
- healthcare
- health
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of harmonizing heterogeneous
  healthcare datasets for federated learning by integrating ontology-based alignment
  with large language model (LLM) validation. The proposed two-step pipeline first
  generates matching candidates using vector embeddings and ontology-based converters,
  then employs an LLM to evaluate and refine these mappings.
---

# Ontology- and LLM-based Data Harmonization for Federated Learning in Healthcare

## Quick Facts
- arXiv ID: 2505.20020
- Source URL: https://arxiv.org/abs/2505.20020
- Reference count: 40
- Primary result: LLM-assisted validation improves mapping precision to 92% expert agreement for unstructured EHR text and 91% for ICD-10 codes.

## Executive Summary
This paper addresses the challenge of harmonizing heterogeneous healthcare datasets for federated learning by integrating ontology-based alignment with large language model (LLM) validation. The proposed two-step pipeline first generates matching candidates using vector embeddings and ontology-based converters, then employs an LLM to evaluate and refine these mappings. In experiments mapping pregnancy outcome descriptions to MONDO and HPO ontologies, the approach achieved 92% agreement with human expert decisions. When mapping ICD-10 codes to these ontologies, the LLM achieved 91% agreement. The results demonstrate that LLM-assisted validation significantly improves mapping precision while reducing the need for manual expert involvement, enabling scalable and privacy-preserving data harmonization for federated healthcare research.

## Method Summary
The method uses a two-step pipeline for data harmonization: first, it generates candidate mappings using vector embeddings of ontology labels/synonyms and SNOMED-based bridging for ICD-10 codes; second, it validates these candidates using an LLM with explicit acceptance criteria. For unstructured EHR text, the system retrieves top-3 ontology matches via semantic similarity search. For ICD-10 codes, it combines RAG-based retrieval with SNOMED CT cross-references. The LLM evaluates candidate pairs against a structured prompt requiring judgment of equivalence or generalization. The pipeline is containerized using the Brane/EPI framework for local execution at each federated learning node.

## Key Results
- Achieved 92% agreement with human expert decisions on mapping pregnancy outcome descriptions to MONDO and HPO ontologies
- Achieved 91% agreement on mapping ICD-10 codes to target ontologies using the LLM validation approach
- Demonstrated that LLM-assisted validation significantly improves precision while reducing manual expert review burden

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RAG-based candidate generation improves mapping recall for unstructured EHR text.
- Mechanism: Vector embeddings of ontology labels/synonyms create a semantic search space; query embeddings retrieve top-k matching terms, surfacing candidates that keyword-based methods miss.
- Core assumption: Semantic similarity in embedding space correlates with conceptual equivalence in the clinical domain.
- Evidence anchors:
  - [abstract] "generates matching candidates using vector embeddings and ontology-based converters"
  - [section IV.A] "For each row in the dataset, we embed the observed outcomes and use as a query to retrieve (up to) 3 most relevant MONDO/HPO disease terms"
  - [corpus] No direct corpus validation of this specific RAG-for-ontology approach; neighbor papers address FL/EHR but not LLM+ontology harmonization.
- Break condition: If ontology terms lack sufficient synonyms or EHR text is highly abbreviated, embedding similarity may fail to surface correct matches.

### Mechanism 2
- Claim: LLM validation with explicit acceptance criteria filters incorrect mappings and improves precision.
- Mechanism: An LLM evaluates candidate pairs against a structured prompt requiring judgment of equivalence or generalization, providing justification. This reduces false positives from semantic-only matching.
- Core assumption: LLMs can reliably distinguish between "same disease," "more specific," and "unrelated" when given clear criteria.
- Evidence anchors:
  - [abstract] "achieved 92% agreement with human expert decisions" on pregnancy outcomes; "91% agreement" on ICD-10 mapping
  - [section IV.A] LLM prompt required: "If the second description is more narrow or specific, choose 'No'... If broader or more generic, choose 'Yes'"
  - [corpus] Weak validation; corpus papers mention LLMs for EHR extraction but not for ontology validation specifically.
- Break condition: If acceptance criteria are ambiguous or if clinical context requires domain knowledge beyond the LLM's training, agreement with experts degrades.

### Mechanism 3
- Claim: Multi-path ontology bridging (RAG + SNOMED-based) improves recall over single-path methods.
- Mechanism: Combining embedding-based retrieval with graph-based traversal through SNOMED CT cross-references captures mappings that either method alone would miss. LLM validates the union of candidates.
- Core assumption: SNOMED CT provides reliable cross-references between ICD-10 and MONDO/HPO; errors from indirect mappings can be filtered by LLM.
- Evidence anchors:
  - [section IV.B] "only for 475 ICD-10 codes (out of 1162) a MONDO or an HPO term generated by the RAG-based method was also among the codes produced by the SNOMED-based method"
  - [section IV.B] SNOMED-based method produced no suggestions for 31% of ICD-10 codes; RAG compensated for some gaps.
  - [corpus] No corpus papers validate multi-path ontology bridging.
- Break condition: If cross-reference graphs contain outdated or incorrect links, LLM may accept spurious mappings unless criteria are strictly enforced.

## Foundational Learning

- **Federated Learning (FL) fundamentals**
  - Why needed here: The entire pipeline is designed to support FL by harmonizing data locally without sharing raw records.
  - Quick check question: Can you explain why FL requires harmonized schemas across participating nodes?

- **Biomedical ontology structure and relationships**
  - Why needed here: MONDO, HPO, SNOMED CT, and ICD-10 have different granularities and cross-reference coverage; understanding hierarchical and equivalence relations is essential for designing acceptance criteria.
  - Quick check question: What is the difference between a disease ontology (MONDO) and a phenotype ontology (HPO), and how are they linked?

- **Vector embeddings and semantic similarity**
  - Why needed here: The first pipeline stage relies on embedding-based retrieval; understanding embedding quality, distance metrics, and top-k selection is critical.
  - Quick check question: Why might embedding similarity alone fail for highly abbreviated clinical text?

## Architecture Onboarding

- **Component map:**
  - Vector database (Qdrant) stores embedded ontology labels/synonyms with metadata
  - LLM (GPT-4o via API) evaluates candidate pairs against acceptance prompts
  - Ontology sources: MONDO, HPO, SNOMED CT, ICD-10 (local OWL/JSON dumps)
  - Containerized workflow: Brane/EPI framework orchestrates local execution at each FL node

- **Critical path:**
  1. Extract labels/synonyms from target ontologies → embed → store in vector DB
  2. For each EHR record, embed query text → retrieve top-k candidates
  3. Optionally, generate additional candidates via SNOMED CT graph traversal
  4. LLM validates each candidate pair; accepted mappings returned
  5. Human expert reviews disagreements (optional but recommended for high-stakes domains)

- **Design tradeoffs:**
  - Higher top-k improves recall but increases LLM API costs and expert review burden
  - SNOMED-based bridging adds coverage but introduces noise; stricter LLM criteria reduce false positives
  - API-based LLMs raise privacy concerns; local/open-source models (e.g., Llama) may reduce accuracy

- **Failure signatures:**
  - Low agreement between LLM and expert → acceptance criteria may be ambiguous; refine prompt
  - Many ICD-10 codes unmapped → SNOMED cross-references may be missing; increase top-k or add fallback heuristics
  - High false-positive rate → LLM may be too permissive; tighten criteria (e.g., reject "related but not equivalent")

- **First 3 experiments:**
  1. **Baseline recall test:** Run RAG-only pipeline on a held-out subset of EHR records; measure how many correct ontology terms appear in top-3 vs top-10
  2. **LLM precision evaluation:** Compare LLM-only decisions against human expert on 200 randomly sampled candidate pairs; calculate precision/recall for each acceptance criterion variant
  3. **Multi-path ablation:** Run SNOMED-only, RAG-only, and combined pipelines on the same ICD-10 dataset; measure coverage overlap and unique mappings from each path

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can open-source or locally deployed LLMs match the mapping precision of proprietary API-based models (like GPT-4o) while satisfying the privacy constraints necessary for federated learning in healthcare?
- Basis in paper: [explicit] Section V states that reliance on API requests raises privacy concerns, and "This motivates the need for better open-source models which are competitive with closed-source models."
- Why unresolved: The experiments in the study exclusively utilized ChatGPT-4o via API; no open-source models were evaluated against the ground truth.
- What evidence would resolve it: A comparative benchmark evaluating local LLMs against the MPRINT dataset using the proposed pipeline to measure precision relative to the 92% baseline.

### Open Question 2
- Question: How can LLM-based validation pipelines be refined to robustly distinguish between distinct ontological relations (e.g., subClassOf vs. part-of) rather than relying on general semantic similarity?
- Basis in paper: [inferred] Section IV.A notes the LLM had "difficulties in deciding whether to accept mappings for similar but not identical records" and states "It is important to formulate acceptance criteria for most basic ontological relations."
- Why unresolved: The current implementation relies on a generic prompt that occasionally confuses restrictive conditions with broader ones, leading to false positives in mapping.
- What evidence would resolve it: A study testing prompt engineering or fine-tuning strategies specifically designed to parse hierarchical and meronymic relations in medical ontologies.

### Open Question 3
- Question: Can LLMs effectively translate complex regulatory compliance policies (e.g., GDPR, HIPAA) into executable logic specifications (e.g., eFlint, Datalog) within federated frameworks?
- Basis in paper: [explicit] Section II.E and Section VI list "simplify compliance policy translation to the specification formats supported by the framework" as a specific goal for future LLM integration.
- Why unresolved: The current paper focused entirely on the data harmonization aspect of the pipeline; the integration of LLMs for policy translation and enforcement has not yet been implemented.
- What evidence would resolve it: A demonstration of the Brane/EPI framework successfully parsing a legal text and enforcing the resulting rules during a federated training run.

## Limitations
- High agreement rates based on single expert validation may not capture inter-rater variability in clinical ontology mapping
- Method's performance on highly abbreviated or context-dependent clinical text remains untested
- Privacy implications of using API-based LLMs in federated settings acknowledged but not quantitatively addressed

## Confidence
- **High confidence:** The general two-step pipeline design (embedding-based candidate retrieval + LLM validation) is well-specified and reproducible
- **Medium confidence:** The specific embedding model and exact SNOMED CT mapping implementation details are not fully specified, which may affect reproducibility
- **Medium confidence:** The privacy preservation claims are theoretically sound but lack quantitative privacy analysis

## Next Checks
1. **Inter-rater validation:** Replicate the LLM vs. expert agreement study with 3-5 independent clinical experts to establish inter-rater reliability and confidence intervals
2. **Embedding model ablation:** Test the pipeline with multiple embedding models (e.g., BioBERT, ClinicalBERT, sentence-transformers) to quantify sensitivity to embedding quality on clinical text
3. **Privacy audit:** Implement and measure the privacy impact of using local vs. API-based LLMs in the federated workflow, including data transfer volumes and potential re-identification risks