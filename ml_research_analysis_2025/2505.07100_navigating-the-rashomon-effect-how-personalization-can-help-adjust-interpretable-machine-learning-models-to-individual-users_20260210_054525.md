---
ver: rpa2
title: 'Navigating the Rashomon Effect: How Personalization Can Help Adjust Interpretable
  Machine Learning Models to Individual Users'
arxiv_id: '2505.07100'
source_url: https://arxiv.org/abs/2505.07100
tags:
- interpretable
- personalization
- users
- interpretability
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether intrinsically interpretable machine
  learning models can be personalized while maintaining their key advantage of faithful
  representations. The researchers developed a personalization approach that leverages
  the Rashomon effect - the existence of multiple models with similar predictive performance
  but different visual representations.
---

# Navigating the Rashomon Effect: How Personalization Can Help Adjust Interpretable Machine Learning Models to Individual Users

## Quick Facts
- arXiv ID: 2505.07100
- Source URL: https://arxiv.org/abs/2505.07100
- Reference count: 0
- Key outcome: Personalizing interpretable ML models using the Rashomon effect enables individual users to develop distinct preferences for model configurations while maintaining interpretability and predictive performance.

## Executive Summary
This study demonstrates that intrinsically interpretable machine learning models can be successfully personalized to individual users without sacrificing their key advantage of faithful representations. By leveraging the Rashomon effect—the existence of multiple models with similar predictive performance but different visual representations—the researchers developed a personalization approach that uses contextual bandits with Thompson Sampling to tailor Generalized Additive Models (GAMs) to individual users' interpretability preferences. An online experiment with 108 participants showed that users developed distinct preferences for different model configurations, resulting in 44 unique configurations among 53 users in the treatment group. Importantly, interpretability remained high across both personalized and non-personalized groups, with no significant differences in insight quality or user perception between groups.

## Method Summary
The researchers developed a personalization approach that frames the problem as a contextual bandit task using Thompson Sampling. They created a pool of 92 distinct GAM configurations with similar predictive performance (R² ≥ 0.83) but varying in four hyperparameters: excluded features, interaction terms, pattern granularity, and forced monotonicity. Users provided binary rewards based on 7-point helpfulness ratings as they iteratively interacted with different model configurations. A Bayesian reward model with Normal(0, 0.5) prior weights learned individual preferences over multiple iterations. The system presented users with personalized model configurations while tracking insight quality, user perception, and configuration diversity.

## Key Results
- Personalization led to 44 distinct model configurations among 53 users in the treatment group
- No significant differences in insight quality or user perception between personalized and non-personalized groups
- Information gain analysis showed that user feedback provided discriminative information for personalization
- Posterior variance in the Bayesian reward model decreased consistently across users, indicating successful learning of preferences

## Why This Works (Mechanism)

### Mechanism 1: Rashomon Set Provides Model Diversity at Comparable Performance
Personalization requires multiple model configurations with similar predictive accuracy but different visual representations. The Rashomon effect ensures that 92 GAM configurations exist with comparable performance (R² ≥ 0.83) while differing substantially in excluded features, interaction terms, pattern granularity, and forced monotonicity. This diversity allows trading interpretability characteristics without sacrificing accuracy. Break condition: If predictive performance variance exceeds acceptable threshold, personalization may force accuracy tradeoffs that undermine user trust.

### Mechanism 2: Thompson Sampling Iteratively Learns Individual Preferences
A Bayesian linear model with Normal(0, 0.5) prior maintains weights for each hyperparameter level. Thompson Sampling samples from the posterior to select configurations probabilistically. Binary rewards derived from 7-point helpfulness ratings update the posterior. Decreasing posterior variance signals convergence from exploration to exploitation. Break condition: If posterior variance fails to decrease after 5+ iterations, user feedback may be inconsistent or the hyperparameter space misaligned with preference dimensions.

### Mechanism 3: User Feedback Contains Discriminative Information at Hyperparameter Level
Information gain quantifies how much user feedback reduces uncertainty versus random guessing. Mean IG across hyperparameters exceeded random baseline, and heterogeneity in IG across hyperparameters indicates some dimensions elicit more consistent preferences than others. Break condition: If all hyperparameters yield IG near zero, users cannot discriminate between configurations and personalization reduces to random selection.

## Foundational Learning

- Concept: Rashomon Effect
  - Why needed here: Provides theoretical justification that personalization need not sacrifice predictive performance—multiple equally-valid models exist.
  - Quick check question: For your target dataset, can you identify at least 5-10 model configurations within 1-2% of optimal accuracy that produce visibly different visualizations?

- Concept: Thompson Sampling for Contextual Bandits
  - Why needed here: Core algorithm balancing exploration of unknown configurations with exploitation of learned user preferences.
  - Quick check question: How does increasing prior variance from 0.5 to 1.0 affect early exploration behavior and convergence speed?

- Concept: Generalized Additive Models (GAMs) and Their Hyperparameters
  - Why needed here: Understanding how each hyperparameter affects visual interpretability is essential for designing the configuration space.
  - Quick check question: What visual difference would a user observe when pattern granularity decreases from 256 bins to 8 bins on a temperature feature shape plot?

## Architecture Onboarding

- Component map: Hyperparameter Configuration Grid -> Thompson Sampling Selector -> Bayesian Reward Model -> Validation Gate -> Feedback Interface

- Critical path: 1) Pre-train all configurations; filter to Rashomon set 2) For each user: initialize reward model → iterate (sample → present → collect rating → compute reward → update posterior) 3) Detect convergence via posterior variance; select final configuration with maximum expected reward

- Design tradeoffs:
  - Pre-computed vs. runtime validation: Pre-computation streamlines user experience but limits configuration space flexibility
  - Binary vs. continuous rewards: Binary simplifies learning but discards signal granularity; cutoff threshold affects sensitivity
  - Prior variance (0.5): Higher increases early exploration; lower speeds convergence but risks premature commitment

- Failure signatures:
  - Posterior variance stagnant after 5+ iterations → inconsistent user feedback or hyperparameter space misalignment
  - All users converge to identical configuration → insufficient Rashomon diversity or hyperparameters don't affect interpretability
  - Information gain uniformly near zero → rating instrument not capturing preference distinctions

- First 3 experiments:
  1. **Rashomon set validation**: Train 50+ configurations on target dataset; confirm performance variance < 5% while visualizations differ substantially
  2. **Feedback discriminability pilot**: Run N=15-20 users through 8-10 random configurations; compute IG per hyperparameter
  3. **Convergence diagnostics**: Track posterior variance per user across iterations; if >30% show no decrease by iteration 8, adjust prior variance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does personalization of interpretable ML models have greater impact in more complex settings with larger feature sets and more intricate relationships?
- Basis in paper: "Future studies should employ more complex datasets to determine if personalization has a greater impact in more intricate settings."
- Why unresolved: The current study used a simplified dataset with only five features, which may have limited the potential impact of personalization.
- What evidence would resolve it: Experiments comparing personalization effects on datasets with varying complexity showing stronger effects in more complex scenarios.

### Open Question 2
- Question: How do user factors like graph literacy and domain expertise influence preferences for different model configurations and interpretability effectiveness?
- Basis in paper: "Future research should investigate whether factors like graph literacy or domain expertise influence model preferences and interpretability effectiveness."
- Why unresolved: Current participants had high management experience which may have enabled them to compensate for suboptimal personalization.
- What evidence would resolve it: Studies measuring these user factors and analyzing correlations with personalization outcomes and model preferences.

### Open Question 3
- Question: Can direct measures of interpretability be developed that provide immediate feedback to the personalization algorithm beyond insight generation and self-reported scales?
- Basis in paper: "Direct measures of interpretability that provide immediate feedback to the Bayesian reward model should be explored, moving beyond just insight generation and self-reported scales."
- Why unresolved: Current measures may not fully capture participants' understanding of underlying relationships.
- What evidence would resolve it: Development and validation of alternative interpretability metrics computable during user interaction.

### Open Question 4
- Question: Can more efficient methods be developed for identifying models within the Rashomon set to enhance the scalability of personalization in practical applications?
- Basis in paper: "Developing such methods represents a critical avenue for future research" regarding scalability limitations of current grid-search approach.
- Why unresolved: Current approach requires pre-training models for all configurations, which becomes computationally expensive with increasing dataset and model complexity.
- What evidence would resolve it: Algorithms that can efficiently identify diverse high-performing models without exhaustive grid search.

## Limitations
- The approach was validated only on a single, simplified bike-sharing dataset with five features, limiting generalizability to more complex domains.
- The personalization mechanism depends entirely on users providing consistent, preference-reflecting feedback, which may vary across user populations.
- The statistical power to detect small effects in insight quality or user perception differences between personalized and non-personalized groups is limited with the current sample size.

## Confidence

**High Confidence**: The core mechanism of using Thompson Sampling to personalize GAMs based on user feedback is well-established. The demonstration that 44 distinct configurations emerged among 53 users provides strong evidence that personalization successfully leverages Rashomon diversity.

**Medium Confidence**: The finding that interpretability remains high despite personalization is supported, but the study cannot rule out subtle differences in insight quality or user perception that may emerge with larger sample sizes or different datasets.

**Low Confidence**: The assumption that users can consistently discriminate between configurations at the hyperparameter level is only partially supported. Information gain analysis showed heterogeneous discriminability across hyperparameters, suggesting some dimensions may not provide meaningful personalization signals.

## Next Checks

1. **Cross-Dataset Validation**: Apply the personalization framework to a high-dimensional dataset (e.g., healthcare or financial data) with 10+ features to test whether Rashomon diversity scales and whether hyperparameter preferences transfer across domains.

2. **Continuous Reward Experiment**: Implement a continuous reward model that uses the full 7-point scale rather than binary conversion. Compare convergence behavior and final configuration diversity against the binary baseline.

3. **Longitudinal Preference Stability**: Conduct a follow-up study where the same users interact with personalized models after 2-4 weeks. Measure whether initial personalization converges to stable long-term preferences or whether preferences shift with continued exposure.