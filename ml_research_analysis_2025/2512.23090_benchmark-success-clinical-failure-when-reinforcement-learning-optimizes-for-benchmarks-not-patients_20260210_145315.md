---
ver: rpa2
title: 'Benchmark Success, Clinical Failure: When Reinforcement Learning Optimizes
  for Benchmarks, Not Patients'
arxiv_id: '2512.23090'
source_url: https://arxiv.org/abs/2512.23090
tags:
- reasoning
- training
- chexpert
- learning
- medical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'ChexReason is a vision-language model trained via R1-style methodology
  (SFT + GRPO) on just 2,000 SFT samples, 1,000 RL samples, and a single A100 GPU
  for multilabel chest X-ray classification. Evaluations on CheXpert and NIH benchmarks
  reveal a fundamental tension: GRPO recovers in-distribution performance (23% improvement
  on CheXpert, macro-F1 = 0.346) but degrades cross-dataset transferability (19% drop
  on NIH).'
---

# Benchmark Success, Clinical Failure: When Reinforcement Learning Optimizes for Benchmarks, Not Patients

## Quick Facts
- **arXiv ID:** 2512.23090
- **Source URL:** https://arxiv.org/abs/2512.23090
- **Reference count:** 40
- **Primary result:** ChexReason achieves 23% improvement on CheXpert (macro-F1 = 0.346) via GRPO but degrades 19% on NIH, revealing RL's failure to generalize across clinical datasets.

## Executive Summary
ChexReason demonstrates a fundamental tension in reinforcement learning for medical AI: while GRPO improves in-distribution performance on CheXpert, it degrades cross-dataset transferability to NIH. This occurs despite training on minimal data (2,000 SFT samples, 1,000 RL samples) and limited compute (single A100 GPU), suggesting the issue stems from the RL paradigm rather than scale. The study reveals that the SFT checkpoint uniquely improves NIH performance, indicating teacher-guided reasoning captures more institution-agnostic features than RL optimization. This finding mirrors high-resource models like NV-Reason-CXR-3B, suggesting a systemic challenge in medical vision-language models.

## Method Summary
The study trains ChexReason using a R1-style pipeline (SFT + GRPO) on MedGemma-4B for multilabel chest X-ray classification. Training data consists of 2,000 MIMIC-CXR-JPG samples for SFT and 1,000 samples for RL, selected via a penalty-based greedy sampler ensuring ≥5% label coverage. The SFT phase uses LoRA with specific hyperparameters (rank=16, α=32, dropout=0.1, lr=5e-5, 6 epochs), while GRPO employs 4 completions/sample with temperature=0.8, top-p=0.95, and β=0.15 clipping. Evaluation occurs on CheXpert (518 samples) and NIH (488 samples) test sets using macro-F1 as the primary metric.

## Key Results
- GRPO recovers in-distribution performance with 23% improvement on CheXpert (macro-F1 = 0.346)
- Cross-dataset transferability degrades 19% on NIH during RL optimization
- SFT checkpoint uniquely improves NIH performance, indicating teacher-guided reasoning captures more institution-agnostic features
- Structured reasoning scaffolds benefit general-purpose VLMs but offer minimal gain for medically pre-trained models

## Why This Works (Mechanism)
The RL optimization process inadvertently prioritizes dataset-specific patterns over generalizable radiologic features. When optimizing for CheXpert's label schema, GRPO learns to exploit label-specific patterns and spurious features that don't transfer to NIH's different conventions. This occurs because the reward function (Jaccard + format compliance) optimizes for label accuracy within a single ontology without penalizing overfitting to that specific labeling scheme.

## Foundational Learning
- **R1-style training (SFT + GRPO):** Required for understanding the reinforcement learning methodology that creates the generalization paradox
- **Macro-F1 vs Micro-F1 metrics:** Needed to interpret the performance differences between datasets and understand why class imbalance matters
- **Cross-dataset generalization:** Critical for understanding why medical AI models fail in clinical deployment across different institutions
- **Structured reasoning scaffolds:** Important for grasping why general-purpose VLMs benefit from reasoning traces while medical pre-trained models don't
- **Label ontology differences:** Essential for understanding how different datasets define the same medical conditions differently

## Architecture Onboarding

**Component Map:** MIMIC-CXR-JPG → SFT (LoRA) → GRPO (hard/nuanced rewards) → CheXpert/NIH evaluation

**Critical Path:** Data sampling → Teacher reasoning generation → SFT training → GRPO optimization → Cross-dataset evaluation

**Design Tradeoffs:** Low-resource training (2K samples) vs. model performance; structured reasoning vs. simple classification; in-distribution optimization vs. generalization

**Failure Signatures:** Mode collapse during GRPO (entropy drop), format violations in structured outputs, CheXpert improvement with NIH degradation

**3 First Experiments:**
1. Verify SFT checkpoint uniquely improves NIH performance before any RL optimization
2. Test whether hard rewards alone produce the same generalization gap as nuanced rewards
3. Evaluate performance on demographic subgroups within NIH to assess population-level impacts

## Open Questions the Paper Calls Out

**Open Question 1:** Can reward formulations be explicitly designed to penalize overfitting to specific labeling conventions while preserving GRPO's in-distribution gains? The study found neither hard nor nuanced rewards addressed learning institution-agnostic representations, suggesting a need for rewards that penalize overfitting to single ontologies.

**Open Question 2:** Would a multi-dataset training curriculum mitigate the generalization drop observed during RL, or does optimization pressure still favor specific dataset distributions? The study was constrained to single-source training, leaving mixed-source RL effects untested.

**Open Question 3:** What specific visual or semantic shortcuts does GRPO exploit in the CheXpert labeling convention that cause degradation in NIH transferability? The paper hypothesizes label-specific pattern exploitation but doesn't isolate exact features through explainability analysis.

## Limitations
- The GRPO-induced trade-off between in-distribution and cross-dataset performance may be dataset-specific rather than universal
- The claim that findings "mirror high-resource models" lacks direct comparative data within this study
- Generalizability of the "generalization paradox" across other medical imaging tasks remains unproven

## Confidence
- **High confidence:** Experimental methodology, evaluation metrics, and core finding that SFT checkpoint uniquely improves NIH performance
- **Medium confidence:** Interpretation that structured reasoning scaffolds benefit general-purpose VLMs but not medically pre-trained models
- **Low confidence:** Generalizability of the generalization paradox across other medical domains

## Next Checks
1. **Dataset dependency test:** Evaluate the same SFT+GRPO pipeline on PadChest to determine if NIH-specific degradation is reproducible
2. **Reward function ablation:** Test whether the trade-off persists using only hard rewards versus the full nuanced reward system
3. **Cross-population robustness:** Assess model performance on demographic subgroups within NIH to determine if GRPO optimization disproportionately affects certain patient populations