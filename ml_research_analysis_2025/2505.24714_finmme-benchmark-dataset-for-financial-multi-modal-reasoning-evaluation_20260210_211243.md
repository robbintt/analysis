---
ver: rpa2
title: 'FinMME: Benchmark Dataset for Financial Multi-Modal Reasoning Evaluation'
arxiv_id: '2505.24714'
source_url: https://arxiv.org/abs/2505.24714
tags:
- financial
- data
- arxiv
- evaluation
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FinMME, a comprehensive financial multimodal
  dataset containing over 11,000 high-quality samples across 18 financial domains
  and 6 asset classes. The dataset features 10 major chart types with 21 subtypes
  and is accompanied by a hierarchical evaluation framework and FinScore metric that
  incorporates hallucination penalties and domain-normalized scoring.
---

# FinMME: Benchmark Dataset for Financial Multi-Modal Reasoning Evaluation

## Quick Facts
- arXiv ID: 2505.24714
- Source URL: https://arxiv.org/abs/2505.24714
- Reference count: 33
- Key outcome: FinMME dataset with 11,099 samples shows state-of-the-art models achieve only 46.56% accuracy on financial multimodal tasks

## Executive Summary
FinMME introduces a comprehensive benchmark dataset for evaluating Multimodal Large Language Models (MLLMs) on financial reasoning tasks. The dataset contains over 11,000 high-quality samples spanning 18 financial domains, 6 asset classes, and 10 major chart types with 21 subtypes. What distinguishes FinMME is its hierarchical evaluation framework with three cognitive levels (Comprehensive Perception, Fine-grained Perception, Analysis and Reasoning) and the FinScore metric that incorporates hallucination penalties and domain-normalized scoring to provide more reliable assessments of model capabilities.

The benchmark demonstrates that even leading models like GPT-4o achieve only moderate performance (46.56% accuracy, FinScore 15.34), highlighting the challenging nature of financial multimodal reasoning. The dataset shows exceptional robustness with prediction variations under different prompts remaining below 1%, confirming superior reliability compared to existing benchmarks. This work addresses a critical gap in MLLM evaluation by providing a domain-specific benchmark that better reflects real-world financial analysis requirements.

## Method Summary
The FinMME benchmark employs a rigorous multi-stage creation process involving professional financial analysts to ensure data quality. The dataset features 11,099 samples across 18 financial domains including equities, fixed income, commodities, and macroeconomic indicators. Three cognitive levels structure the tasks: Comprehensive Perception (2,333 samples), Fine-grained Perception (6,466 samples), and Analysis and Reasoning (2,300 samples). The evaluation framework uses vLLM for local inference on NVIDIA H100 GPUs and API calls for proprietary models. The FinScore metric combines domain-normalized accuracy with hallucination penalties using the formula F = F × (1 - PH), where hallucination penalty is calculated per-question then averaged across samples.

## Key Results
- GPT-4o achieved only 46.56% accuracy and FinScore 15.34 on FinMME benchmark
- Qwen2.5-VL 72B performed best among evaluated models with 52.54% accuracy and FinScore 20.87
- Prediction variations across different prompts remained below 1%, demonstrating exceptional dataset stability
- All evaluated models showed significant performance gaps compared to human-level reasoning capabilities

## Why This Works (Mechanism)
The FinMME benchmark works by providing a domain-specific evaluation framework that captures the complexity of real-world financial multimodal reasoning. Unlike general MLLM benchmarks, FinMME focuses on chart comprehension, numerical extraction, and financial analysis across diverse asset classes and market conditions. The hierarchical cognitive levels ensure comprehensive assessment from basic perception to complex reasoning tasks. The FinScore metric's hallucination penalty addresses a critical weakness in existing benchmarks by penalizing models that generate plausible but incorrect information, which is particularly important in financial contexts where accuracy is paramount.

## Foundational Learning
- Multimodal Large Language Models (MLLMs): AI systems that process and reason across text and visual inputs simultaneously; needed to evaluate models that must interpret charts and financial data together
- Financial chart types and subtypes: Understanding 10 major chart types with 21 subtypes is essential for creating relevant evaluation tasks
- Cognitive level framework: Three-tier structure (Comprehensive, Fine-grained, Analysis) ensures assessment covers full spectrum of reasoning abilities
- Hallucination penalty metrics: Critical for financial applications where incorrect information can have serious consequences
- Domain normalization: Ensures each financial domain contributes equally to overall score regardless of sample count
- Quick check: Verify that domain-normalized scoring correctly weights each of 18 domains equally in FinScore calculation

## Architecture Onboarding
- Component map: Dataset Creation -> FinScore Calculation -> Model Evaluation -> Results Analysis
- Critical path: High-quality dataset generation with professional analysts → FinScore implementation with hallucination penalties → Multi-model evaluation across 17 models → Performance benchmarking
- Design tradeoffs: Focused domain specificity vs. broader generalization; hallucination penalties vs. model confidence; cognitive level granularity vs. evaluation efficiency
- Failure signatures: Hallucination penalties significantly lowering scores; domain-normalized scores not matching; standard deviation exceeding 1% threshold
- First experiments:
  1. Download dataset and verify sample distribution across 18 domains and 6 asset classes
  2. Run baseline evaluation with GPT-4o to confirm 46.56% accuracy
  3. Test hallucination penalty calculation on sample questions to ensure proper implementation

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset primarily uses Chinese market data, potentially limiting generalizability to other financial markets
- Focus on 10 major chart types with 21 subtypes may not capture full diversity of financial visualization formats
- FinScore metric assumes selection-based questions are appropriate for all cognitive levels, potentially biasing results
- Stability claims validated only within limited prompt variation range, may not hold under substantially different prompting strategies

## Confidence
High confidence: Dataset size and composition (11,099 samples across 18 domains), basic benchmark infrastructure (code availability), and reported results for baseline models (GPT-4o: 46.56% avg, Qwen2.5-VL: 52.54% avg).

Medium confidence: Generalization to broader financial contexts (Chinese market focus), stability claims (<1% variation) across all model types and prompts, and FinScore's effectiveness as true measure of financial reasoning capability.

## Next Checks
1. Reproduce the stability test by evaluating a single model across 10+ different prompt variations and measuring prediction variation to confirm the reported <1% threshold.

2. Validate the FinScore calculation by manually computing scores for a random subset of samples (n=50) using the provided formula and comparing against automated results.

3. Test model performance on cross-domain transfer by training or fine-tuning models on 15 domains and evaluating on the remaining 3 to assess FinMME's ability to measure generalizable financial reasoning versus domain-specific memorization.