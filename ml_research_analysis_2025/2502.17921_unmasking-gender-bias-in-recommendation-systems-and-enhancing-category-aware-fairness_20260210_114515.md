---
ver: rpa2
title: Unmasking Gender Bias in Recommendation Systems and Enhancing Category-Aware
  Fairness
arxiv_id: '2502.17921'
source_url: https://arxiv.org/abs/2502.17921
tags:
- fairness
- bias
- recommendation
- metrics
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses gender bias in recommendation systems, proposing
  new metrics to evaluate fairness at a granular level by considering item categories.
  The authors introduce both non-ranking and ranking-based metrics, such as Category
  Coverage, Relative Category Representation, and Category Mean Average Precision,
  to capture nuanced bias in recommendations.
---

# Unmasking Gender Bias in Recommendation Systems and Enhancing Category-Aware Fairness

## Quick Facts
- arXiv ID: 2502.17921
- Source URL: https://arxiv.org/abs/2502.17921
- Reference count: 40
- Primary result: Proposed category-aware fairness metrics and regularization reduce gender bias by up to 77% without substantial performance loss.

## Executive Summary
This paper addresses gender bias in recommendation systems by introducing new metrics to evaluate fairness at the category level and incorporating a category-aware fairness term into the loss function during model training. The authors propose both non-ranking and ranking-based metrics, such as Category Coverage and Category Mean Average Precision, to capture nuanced bias in recommendations. Experiments on MovieLens and Yelp datasets using various baseline models show that the proposed approach effectively reduces gender bias while maintaining recommendation quality.

## Method Summary
The paper proposes category-aware fairness metrics that evaluate gender bias at the item category level, then incorporates these metrics as a regularization term in the loss function during training. The approach uses a fairness term weighted by alpha in the total loss: $L = \alpha \cdot L_{FairGender} + (1-\alpha) \cdot L_{Recommendation}$. The fairness term is based on the Gender Balance Score of Category Coverage, which measures the absolute difference in category representation between gender groups. The method was tested on three datasets (MovieLens 100K, MovieLens 1M, and Yelp) using five baseline models and two fairness-aware models.

## Key Results
- Proposed metrics effectively identify gender bias that standard performance metrics miss
- Regularization approach significantly improves fairness in recommendations for different categories
- Average bias reduction of 77% (ML-100K), 53% (ML-1M), and 50% (Yelp) without substantial degradation in overall performance
- Category-aware metrics reveal that while overall precision may be similar across genders, the distribution of recommended categories differs significantly

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Standard performance metrics may mask gender bias by aggregating over all items, obscuring disparities in specific content categories.
- **Mechanism:** The proposed metrics decompose recommendations into specific item categories and calculate the absolute difference in category representation between gender groups. This granular audit reveals bias even when overall metrics like Precision@10 are identical across genders.
- **Core assumption:** Historical interaction data contains stereotypical correlations that models learn to amplify.
- **Evidence anchors:**
  - [abstract] "Previous work... mainly focus on comparing performance metrics... fails to capture certain nuances."
  - [Page 2, Figure 1] Shows similar Precision@10 for males and females but significant differences in "Action" vs "Romance" recommendations.
- **Break condition:** Fails if the dataset lacks distinct category labels or if the stereotype is not present in the underlying interaction data.

### Mechanism 2
- **Claim:** Incorporating fairness metrics directly into the loss function reduces bias by penalizing the model for learning gender-correlated category preferences.
- **Mechanism:** The loss function is modified to include a fairness regularization term that pushes the model's latent representations away from solutions that result in disparate category coverage for different genders.
- **Core assumption:** The fairness term is differentiable or can be approximated in a differentiable manner during training.
- **Evidence anchors:**
  - [Page 5, Section 4] Defines the regularization term using the Gender Balance Score of Category Coverage.
  - [Page 7, Figure 3] Shows visual reduction in bias scores after regularization is applied.
- **Break condition:** If the weight α is set too high, the model may prioritize matching category distributions over predicting relevant items.

### Mechanism 3
- **Claim:** Ranking-based metrics provide a stricter assessment of bias by penalizing stereotypical items appearing at the top of the list more heavily.
- **Mechanism:** These metrics apply a logarithmic discount factor based on the item's rank, so stereotypical items at rank 1 contribute heavily to the bias score, while those at rank 50 contribute less.
- **Core assumption:** Users pay more attention to items at the top of the recommendation list.
- **Evidence anchors:**
  - [Page 5, Section 3.4] Defines Category Discounted Cumulative Gain using a log(j+1) discount.
  - [Page 3, Section 2.3] Argues that not utilizing ranks can yield considerable issues.
- **Break condition:** Relies on the assumption that top-ranked items drive user perception of bias.

## Foundational Learning

- **Concept: Matrix Factorization (MF) / Embeddings**
  - **Why needed here:** The paper targets MF-based models which represent users and items as vectors. Bias is stored in these vectors; to fix it, you must understand how the vectors are generated and updated.
  - **Quick check question:** Can you explain how a user vector and an item vector interact to produce a predicted rating?

- **Concept: Regularization (L2/L1 vs. Custom)**
  - **Why needed here:** The solution relies on adding a "fairness regularizer" to the loss function. You need to distinguish between standard regularization (preventing overfitting) and task-specific regularization (enforcing constraints like fairness).
  - **Quick check question:** How does adding a penalty term to the loss function change the gradient updates during training?

- **Concept: Group Fairness (Demographic Parity)**
  - **Why needed here:** The core metric (GBS) is based on the difference in outcomes between groups (Male vs. Female). Understanding the difference between "equal accuracy" and "equal outcome" is vital.
  - **Quick check question:** Does Demographic Parity require the predictions to be equal across groups, or the accuracy of predictions to be equal?

## Architecture Onboarding

- **Component map:** Input (User ID, Item ID, Item Category Matrix, User Gender) -> Backbone (MF/NeuMF/VAE-CF) -> Fairness Head (calculates Top-K recommendations, Category Coverage, GBS) -> Optimizer (updates weights based on combined losses)
- **Critical path:** The implementation of the differentiable Top-K/Category Coverage calculation for the fairness loss term.
- **Design tradeoffs:**
  - Alpha tuning: High α maximizes fairness but degrades NDCG/HitRatio; low α preserves performance but leaves bias unchecked
  - Dataset size: Impact of regularization varies across dataset sizes
  - Metric selection: Authors chose Category Coverage for loss function, but any proposed metric could be used if differentiable
- **Failure signatures:**
  - Performance collapse: NDCG drops significantly (>10%) → α likely too high
  - Stagnant bias: GBS(CC) does not decrease over epochs → fairness loss scaling incorrect or learning rate too low
  - Model divergence: Loss becomes NaN → check implementation for division by zero
- **First 3 experiments:**
  1. **Baseline Audit:** Train standard MF model on ML-100K; calculate Precision@10 and GBS(CC) to verify high precision co-exists with high gender skew
  2. **Regularization Ablation:** Implement fairness loss with α=0.5; plot training curve comparing Total Loss vs. Fairness Loss
  3. **Hyperparameter Sweep:** Run model with α ∈ [0.1, 0.2, 0.3, 0.4, 0.5] on ML-1M; plot NDCG vs. GBS(CC) to find sweet spot

## Open Questions the Paper Calls Out

- **Open Question 1:** How can the proposed metrics be generalized to handle multi-valued sensitive attributes using a pairwise difference scheme?
  - **Basis in paper:** [explicit] The authors state they "plan to extend our metrics to consider sensitive attributes which are multi-valued, by using a pairwise difference scheme."
  - **Why unresolved:** Current GBS is formulated specifically for binary groups and cannot process intersectional or non-binary attributes.
  - **What evidence would resolve it:** A generalized mathematical formulation of the GBS metric and its validation on a dataset containing multi-valued sensitive attributes.

- **Open Question 2:** Can the proposed metrics be effectively adapted to quantify provider-side fairness and popularity bias?
  - **Basis in paper:** [explicit] The conclusion claims the metrics "can be easily adapted to measure provider-side fairness... [and] quantify popularity bias."
  - **Why unresolved:** The study only validates metrics on consumer-side gender bias using item categories and does not test provider-centric scenarios.
  - **What evidence would resolve it:** Experiments on datasets with provider labels demonstrating that adapted metrics correlate with established provider-fairness baselines.

- **Open Question 3:** How can the framework distinguish between legitimate user preferences and stereotypical bias to prevent over-correction during regularization?
  - **Basis in paper:** [inferred] The paper acknowledges "natural differences across liked categories" but defines fairness as minimizing the difference in category distributions.
  - **Why unresolved:** Optimizing for equal distribution may inadvertently penalize legitimate demographic preferences or force false diversity that degrades user utility.
  - **What evidence would resolve it:** A user study or counterfactual analysis verifying that "fair" recommendations better align with user intent than original historical data.

## Limitations

- **Differentiable Implementation Gap:** The paper does not specify the exact differentiable approximation used for Top-K category selection in the loss function, which is critical for reproduction.
- **Yelp Gender Labels:** The source or method for obtaining gender labels in the Yelp dataset is unclear, as they are not standard in the public dataset.
- **Hyperparameter Details:** Learning rates, embedding dimensions, and batch sizes for baseline models are not explicitly provided, relying on library defaults.

## Confidence

- **High Confidence:** The conceptual framework for category-aware fairness metrics and the general approach of using them as regularization terms.
- **Medium Confidence:** The empirical results showing bias reduction and the specific alpha values that worked best for each model/dataset combination.
- **Low Confidence:** The exact implementation details for the differentiable fairness loss and the preprocessing of the Yelp dataset (especially gender labels).

## Next Checks

1. **Implement Differentiable Top-K:** Create a soft approximation (e.g., using softmax with temperature) for the Top-K selection step in the fairness loss to enable backpropagation.
2. **Replicate Baseline Audit:** Train a standard MF model on ML-100K and verify that high Precision@10 co-exists with high GBS(CC), confirming the problem statement.
3. **Alpha Sensitivity Analysis:** Run the regularization with multiple α values (0.1 to 0.5) on ML-1M and plot NDCG vs. GBS(CC) to identify the optimal tradeoff point.