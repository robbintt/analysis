---
ver: rpa2
title: 'FactIR: A Real-World Zero-shot Open-Domain Retrieval Benchmark for Fact-Checking'
arxiv_id: '2502.06006'
source_url: https://arxiv.org/abs/2502.06006
tags:
- retrieval
- fact-checking
- claims
- benchmark
- real-world
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FactIR, a real-world retrieval benchmark
  for fact-checking derived from production logs with human annotations. The benchmark
  addresses the need for evaluating retrieval systems in open-domain settings where
  claims require complex reasoning from indirect evidence.
---

# FactIR: A Real-World Zero-shot Open-Domain Retrieval Benchmark for Fact-Checking

## Quick Facts
- **arXiv ID:** 2502.06006
- **Source URL:** https://arxiv.org/abs/2502.06006
- **Reference count:** 30
- **Primary result:** Zero-shot retrieval benchmark for fact-checking derived from production logs with human annotations

## Executive Summary
This paper introduces FactIR, a real-world retrieval benchmark for fact-checking derived from production logs with human annotations. The benchmark addresses the need for evaluating retrieval systems in open-domain settings where claims require complex reasoning from indirect evidence. The authors evaluate state-of-the-art retrieval and re-ranking models in a zero-shot setup, including lexical (BM25), sparse (SPLADEV2), dense (DPR, ANCE, tas-b, Contriever, snowflake-arctic-embed-s), late-interaction (ColBERTv2), and re-ranking models. Results show that lexical and sparse retrievers perform surprisingly well as baselines, while strong training objectives like semantic clustering improve generalization. LLM-based re-rankers demonstrate superior performance across diverse retrieval scenarios.

## Method Summary
FactIR provides a real-world zero-shot open-domain retrieval benchmark for fact-checking, derived from production logs with human annotations. The benchmark consists of 100 real-world claims and 90,047 documents in the corpus, with 1,413 claim-evidence relevance annotations (13.89 per query on average). Claims are sourced from Factiverse's production logs (2021-2023) and cover topics including Politics, Economy, Health, Law, Climate, Education, and Other. The benchmark evaluates models without fine-tuning, simulating real-world deployment scenarios. Models evaluated include lexical (BM25), sparse (SPLADEV2), dense (DPR, ANCE, tas-b, MPNet, Contriever, snowflake-arctic-embed-s), late-interaction (ColBERTv2), and re-ranking models (ColBERTv2, MARCO-MiniLM variants, bge-rerankers, Jina-reranker-v2, gte-multilingual). Evaluation metrics include nDCG@k and Recall@k for k=5, 10, 100.

## Key Results
- Lexical models (BM25) and learned sparse retrievers (SPLADEV2) outperform several dense retrievers, proving to be strong baselines
- Models with semantic clustering-based training objectives (snowflake-arctic-embed-s) show superior performance and generalization
- LLM-based re-rankers (gte-multilingual-reranker-base) demonstrate superior performance across diverse retrieval scenarios
- Zero-shot evaluation reveals significant domain shift challenges, with DPR trained on QA datasets performing worse than models pre-trained on MS-MARCO

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Semantic clustering-based training objectives improve zero-shot generalization for fact-checking retrieval.
- Mechanism: Models like snowflake-arctic-embed-s use source stratification during contrastive pre-training, which groups semantically similar documents and samples training examples across clusters. This encourages the model to learn topical relevance patterns rather than surface-level keyword matching, reducing susceptibility to "hard negatives" that are lexically similar but semantically irrelevant.
- Core assumption: The cluster hypothesis (documents relevant to the same query cluster together) holds for fact-checking claims requiring indirect reasoning.
- Evidence anchors:
  - "We hypothesize that this is primarily due to the semantic clustering based source stratification [13] based training objective... leading to superior performance."
  - Table 3 shows snowflake-arctic-embed-s achieving 27.43% improvement over BM25 on nDCG@5
  - Related work on claim decomposition (arXiv:2510.22055) suggests multi-step reasoning helps, but doesn't directly validate clustering mechanisms
- Break condition: If claims require evidence from multiple disjoint topical domains (e.g., connecting economic policy to health outcomes via intermediate concepts), single-cluster relevance may fail.

### Mechanism 2
- Claim: Lexical and sparse retrievers serve as strong baselines because complex claims contain discriminative keywords that dense models trained on factoid queries fail to capture.
- Mechanism: BM25 and SPLADEV2 leverage explicit term matching and learned sparse expansions. For claims like "Keto diet can cure cancer," lexical overlap with scientific documents discussing "ketogenic diet" and "cancer" provides effective first-pass retrieval, while dense models trained on QA pairs may encode misleading semantic associations.
- Core assumption: Claim terms reliably co-occur with relevant evidence terms in the corpus (lexical matching suffices for initial candidate generation).
- Evidence anchors:
  - "We observe that lexical models like BM25 and learned sparse retrievers like SPLADEV2 perform surprisingly well... They outperform several dense retrievers, proving to be strong baselines."
  - "DPR which has been trained on QA datasets performs worse than other dense retrievers like Contriever or tas-b which are pre-trained on MS-MARCO."
  - No direct corpus validation of this specific mechanism
- Break condition: When claims use metaphorical, euphemistic, or adversarial language (e.g., coded political speech), lexical overlap degrades sharply.

### Mechanism 3
- Claim: LLM-based re-rankers generalize better across domains due to multi-domain pre-training that reduces source bias.
- Mechanism: Models like gte-multilingual-reranker-base are pre-trained on diverse tasks and domains (not just MS-MARCO), enabling better cross-domain relevance estimation. The cross-encoder architecture allows full attention between claim and document tokens, capturing nuanced entailment relationships that bi-encoders miss.
- Core assumption: Multi-domain pre-training exposure transfers to fact-checking's evidence-relevance judgments.
- Evidence anchors:
  - "We attribute this performance to... the extensive pre-training of gte-multilingual [27] on data sourced from a wide range of tasks and domains, reducing source bias."
  - Table 3 shows gte-multilingual-reranker-base achieving best re-ranking performance (6.04% improvement over BM25 on nDCG@5)
  - Related work (arXiv:2506.17878) supports multi-agent retrieval for robustness, but doesn't directly validate multi-domain pre-training mechanism
- Break condition: If computational latency constraints prevent re-ranking over large candidate sets, the mechanism becomes impractical for production deployment.

## Foundational Learning

- Concept: **Zero-shot retrieval evaluation**
  - Why needed here: FactIR explicitly evaluates models without fine-tuning to simulate real-world deployment where annotated fact-checking data is scarce. Understanding why models fail out-of-distribution (domain shift from QA to complex claims) is central to interpreting results.
  - Quick check question: Can you explain why DPR trained on Natural Questions might fail on compositional claims about vaccine safety?

- Concept: **Late-interaction models (ColBERT)**
  - Why needed here: ColBERTv2 appears as both a retriever and re-ranker in experiments. The MaxSim operation between query and document token representations differs fundamentally from single-vector bi-encoders and affects how evidence relevance is computed.
  - Quick check question: How does ColBERT's token-level MaxSim differ from DPR's single vector similarity, and what does this imply for matching partial evidence?

- Concept: **nDCG and Recall as retrieval metrics**
  - Why needed here: The paper reports both nDCG@k (ranking quality) and Recall@k (coverage). For fact-checking, high recall at early positions ensures veracity-determining evidence isn't missed, while nDCG reflects whether supporting AND refuting evidence ranks highly.
  - Quick check question: Why might high nDCG@10 but low Recall@100 be problematic for a fact-checking pipeline?

## Architecture Onboarding

- Component map: Input Claim → Query Processing → Retrieval Stage (BM25/Sparse/Dense/Late-Interaction) → Top-k Candidates → Re-ranking Stage (Cross-encoder/LLM) → Re-ranked Evidence → Downstream Verification

- Critical path: BM25 first-stage retrieval → Top-100 candidates → LLM re-ranker (gte-multilingual) → Top-10 evidence for verification. This pipeline balances latency (BM25 is fast) with accuracy (re-ranker improves precision).

- Design tradeoffs:
  - Dense vs. Sparse retrieval: Dense models (snowflake) offer best nDCG but require GPU inference; BM25 is CPU-deployable with competitive performance.
  - Re-ranking depth: Re-ranking 100 documents improves nDCG@10 by ~6% but adds 10-50ms latency per query depending on model size.
  - Training data: Fine-tuning on fact-checking data could improve performance but FactIR shows zero-shot is the realistic constraint for most deployments.

- Failure signatures:
  - Dense retrievers returning semantically similar but irrelevant documents (e.g., general "keto diet" articles for cancer-related claims) → indicates domain shift from pre-training.
  - Low Recall@100 despite high nDCG@10 → first-stage retriever missing refuting evidence; candidate pool too narrow.
  - Re-ranker degrading BM25 performance → cross-encoder trained on mismatched relevance criteria (e.g., MS-MARCO's passage ranking vs. fact-checking's veracity-relevance).

- First 3 experiments:
  1. **Baseline replication**: Run BM25 and snowflake-arctic-embed-s on FactIR test split. Verify nDCG@10 within ±2% of reported values (BM25: 0.345, snowflake: 0.420). If discrepancy >5%, check corpus preprocessing (tokenization, lowercasing).
  2. **Ablation on candidate depth**: Retrieve top-50 vs. top-100 vs. top-200 with BM25, then re-rank with gte-multilingual. Plot nDCG@10 vs. latency to find optimal k for your latency budget.
  3. **Domain shift probe**: Evaluate DPR and Contriever on a subset of FactIR claims categorized by topic (Politics, Health, Economy). Hypothesis: Performance gap between DPR and BM25 will be largest for Health claims requiring scientific reasoning (not in QA pre-training distribution).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can retrieval models be explicitly optimized to identify evidence requiring multi-hop reasoning rather than direct semantic similarity?
- Basis in paper: The introduction states that verifying claims often requires "reasoning from indirectly related evidence," and that traditional methods "struggle with more complex claims."
- Why unresolved: The paper benchmarks existing models which prioritize similarity, revealing a performance gap, but does not propose architectural changes to bridge this reasoning gap.
- What evidence would resolve it: Development and evaluation of a retrieval model trained to predict entailment chains or inference paths rather than simple relevance scores on the FactIR dataset.

### Open Question 2
- Question: Does the superior performance of LLM-based re-rankers hold equally for retrieving evidence that *refutes* a claim versus evidence that *supports* it?
- Basis in paper: The Data Curation section describes collecting annotations for both relevance and stance (support/refute), but the Results section only reports aggregate nDCG and Recall without disaggregating by stance.
- Why unresolved: It is unclear if the models are better at finding confirmatory evidence (potential confirmation bias) or if they effectively handle contradictory evidence necessary for debunking.
- What evidence would resolve it: A stratified analysis of the Table 3 results, separating retrieval scores for documents annotated as "supporting" vs. "refuting" the claims.

### Open Question 3
- Question: What is the performance gap between the reported zero-shot models and domain-adapted models fine-tuned on the FactIR claims?
- Basis in paper: The paper explicitly evaluates in a zero-shot setting because "fine-tuning is often infeasible" in production, leaving the potential gains from supervision unquantified.
- Why unresolved: The current results establish a zero-shot floor; the upper bound for these specific architectures on this data remains unknown.
- What evidence would resolve it: Fine-tuning dense retrievers (e.g., ANCE, Contriever) on a training split of FactIR and comparing the delta against the provided zero-shot baselines.

### Open Question 4
- Question: Does the semantic clustering-based training objective (successful in snowflake-arctic-embed-s) maintain its advantage over lexical baselines when the retrieval corpus is scaled to web-scale magnitude?
- Basis in paper: The paper notes snowflake-arctic-embed-s's success is due to semantic clustering, but the benchmark corpus is relatively small (~90k documents).
- Why unresolved: Clustering effectiveness and retrieval precision often suffer differentially compared to lexical methods when the search space increases to millions of noisy documents.
- What evidence would resolve it: Re-running the retrieval experiments using the top-performing dense models against a web-scale index rather than the benchmark's curated subset.

## Limitations

- Small claim set (100 claims) creates statistical uncertainty in reported improvements
- Moderate annotation depth (13.89 judgments per query) may not capture full relevance judgments
- Corpus preprocessing pipeline remains underspecified, complicating exact reproduction
- Domain-specific performance variations aren't deeply analyzed

## Confidence

- **High confidence**: Lexical and sparse retrievers serving as strong baselines - directly observed in Table 3 with BM25 outperforming several dense models
- **Medium confidence**: Semantic clustering improving zero-shot generalization - supported by performance comparisons but relies on speculative attribution to source stratification
- **Medium confidence**: LLM-based re-rankers showing superior generalization - performance data is clear, but causal attribution to multi-domain pre-training is inferential

## Next Checks

1. **Statistical significance testing**: Apply bootstrap resampling on FactIR claims to establish confidence intervals for nDCG improvements. Determine if observed differences between top-performing models (BM25 vs. snowflake) are statistically significant given the small sample size.

2. **Domain-specific analysis**: Stratify FactIR claims by topic and re-run all retrieval models. Test hypothesis that DPR underperforms most on Health claims requiring scientific reasoning, while BM25 maintains consistent performance across domains.

3. **End-to-end verification impact**: Take top-10 evidence from each retrieval method and run through a standard fact-checking pipeline (e.g., evidence aggregation + veracity prediction). Measure how retrieval stage performance translates to final verification accuracy across different claim types.