---
ver: rpa2
title: 'Scientific Theory of a Black-Box: A Life Cycle-Scale XAI Framework Based on
  Constructive Empiricism'
arxiv_id: '2602.02215'
source_url: https://arxiv.org/abs/2602.02215
tags:
- stobb
- surrogate
- black-box
- observations
- adequacy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a framework for consolidating explanatory
  information about a fixed black-box model into a persistent, auditable artifact
  throughout its lifecycle. Grounded in Constructive Empiricism, the framework defines
  a Scientific Theory of a Black-Box (SToBB) that must be empirically adequate, adaptable,
  and auditable.
---

# Scientific Theory of a Black-Box: A Life Cycle-Scale XAI Framework Based on Constructive Empiricism

## Quick Facts
- **arXiv ID:** 2602.02215
- **Source URL:** https://arxiv.org/abs/2602.02215
- **Reference count:** 40
- **Primary result:** Framework maintains empirically adequate, auditable surrogate for fixed black-box across lifecycle using incremental rule refinement

## Executive Summary
This paper introduces the Scientific Theory of a Black-Box (SToBB) framework, grounded in Constructive Empiricism, to consolidate explanatory information about a fixed black-box model into a persistent, auditable artifact throughout its lifecycle. The framework defines empirical adequacy as perfect agreement between a surrogate and all accumulated observations, maintained incrementally by the Constructive Box Theoriser (CoBoT) algorithm. As a proof of concept, the authors instantiate a complete SToBB for a neural-network classifier on the Abalone dataset, achieving 0.64 validation accuracy with a surrogate comprising 20 boxes over 16 feature sets, compressing 4,177 observations with a gain of ~0.995.

## Method Summary
The SToBB framework constructs and maintains an interpretable rule-based surrogate model that must match all observed black-box behavior exactly. The Constructive Box Theoriser (CoBoT) algorithm processes observations incrementally, either expanding existing rules or creating new ones to restore empirical adequacy when violated. Each observation includes input features, black-box output, and auxiliary measures (LIME attributions). The surrogate consists of non-overlapping, axis-aligned boxes with consistent labels. The framework emphasizes transparency through documentation of assumptions, construction choices, and update policies, enabling reuse and consistent analysis across stakeholder needs.

## Key Results
- Achieved 0.64 validation accuracy on neural-network classifier for Abalone dataset
- Maintained 95.5% compression rate with <20 boxes over 16 feature sets
- Performed 187 updates (4.5%) across 4,177 observations
- Surrogate structure provides interfaces for local, contrastive, and global explanations

## Why This Works (Mechanism)

### Mechanism 1: Empirical Adequacy via Incremental Surrogate Refinement
- **Claim:** A persistent, interpretable surrogate model can systematically represent a fixed black-box model's input-output behavior across its life cycle by maintaining complete agreement with all accumulated observations.
- **Mechanism:** The SToBB framework uses a rule-based surrogate, maintained by the Constructive Box Theoriser (CoBoT) algorithm. As new observations (input-output pairs and auxiliary measures) are added, the surrogate is updated to include them. An observation is covered if a surrogate rule exists that correctly maps its features to the black-box's output. If not, an update is triggered, either by expanding existing rules or creating new ones, thus restoring empirical adequacy.
- **Core assumption:** The black-box model is fixed; its decision behavior does not change during the collection of explanatory information.
- **Evidence anchors:**
  - [abstract] "A SToBB maintains empirical adequacy by ensuring a surrogate model matches all observed black-box behavior and adapts when new observations arrive."
  - [section 5.3] "Processing the observations one at a time, CoBoT incrementally adds boxsystems...The algorithm guarantees that... all samples associated with one box have the same label."
- **Break condition:** The mechanism fails if the black-box model is non-stationary (e.g., fine-tuned or retrained) without a corresponding process to invalidate or update the SToBB.

### Mechanism 2: Knowledge Consolidation and Reuse via a Centralized Observation Base
- **Claim:** Treating explanatory information as a persistent record, rather than isolated method outputs, enables consistent, reusable analyses and mitigates the "disagreement problem" between different XAI methods.
- **Mechanism:** All observations and the surrogate structure are stored in a centralized observation base. Diverse stakeholder questions (e.g., local, contrastive, global) are answered by querying this common, maintained artifact via defined interfaces, rather than by re-computing explanations from scratch. This provides a shared, auditable point of reference.
- **Core assumption:** Different explanatory inquiries about the same model are better served by a single, consistent source of information than by independent analyses.
- **Evidence anchors:**
  - [abstract] "...position SToBBs as a life cycle-scale, inspectable point of reference that supports consistent, reusable analyses and systematic external scrutiny."
  - [section 2] "...treat explanatory information not as a set of disconnected method outputs, but as a persistent record that aggregates observations and remains consistent as it evolves."
- **Break condition:** The mechanism's benefits are undermined if interfaces are poorly designed and produce inconsistent outputs, or if the observation base becomes corrupted or incomplete.

### Mechanism 3: Pragmatic Control via Auxiliary Measures and User-Centered Constraints
- **Claim:** Incorporating auxiliary measures (e.g., feature attributions from another method) and user-centered constraints (e.g., rule complexity limits) into the SToBB's design allows for principled trade-offs between empirical adequacy and practical usefulness.
- **Mechanism:** The observation space is enriched with auxiliary observables (like LIME scores) derived from the black-box. This constrains the surrogate's structure during its construction (e.g., determining the feature subspaces for rules). User-centered parameters, such as a maximum rule length (k), are enforced, and the system can increase complexity (increase k) only when empirical adequacy fails with simpler rules.
- **Core assumption:** The auxiliary measures provide meaningful information about the black-box's decision process, and simpler, more compact explanations are generally more useful to human stakeholders.
- **Evidence anchors:**
  - [abstract] "The framework emphasizes transparency through explicit documentation of assumptions, construction choices, and update policies."
  - [section 3.4] "...Extending the observation space with auxiliary measures... constrains adequacy on a richer set of observables, reducing this multiplicity."
  - [section 5.2] "The size of I [feature subspace] directly dictates the length or complexity of each rule... we introduce a parameter k that limits the number of selected dimensions."
- **Break condition:** The mechanism relies on the quality of the auxiliary measure. If the feature attribution method is flawed or inconsistent, it may force the surrogate into sub-optimal or misleading feature subspaces.

## Foundational Learning

### Concept: Constructive Empiricism
- **Why needed here:** This is the core philosophical grounding for the entire SToBB framework. It defines what a scientific theory is (a family of structures), what "empirical adequacy" means, and why we "accept" a theory (belief in adequacy + commitment to use). Without this, the framework's logic is opaque.
- **Quick check question:** Can you explain why a SToBB surrogate does not need to represent the "true reasoning" of the black-box, only its observable input-output behavior?

### Concept: Surrogate Models (Global vs. Local)
- **Why needed here:** The SToBB's primary component is a global surrogate model. Understanding that a surrogate is an inherently interpretable model approximating a black-box is essential. The SToBB adapts the typical global surrogate concept by enforcing perfect fidelity (empirical adequacy) on all seen data.
- **Quick check question:** How does the SToBB's requirement for a surrogate differ from a typical global surrogate model used in standard XAI practice?

### Concept: The "Disagreement Problem" in XAI
- **Why needed here:** The paper explicitly positions SToBB as a solution to this problem, where different XAI methods produce incompatible explanations. Understanding this problem clarifies the primary practical motivation for a unified framework.
- **Quick check question:** Why might LIME and SHAP give different explanations for the same model prediction, and how does a SToBB aim to prevent this type of inconsistency?

## Architecture Onboarding

### Component map:
Observation(x, c, I_x) -> Observation Base -> CoBoT Algorithm -> Adequate Surrogate (Boxsystems B) -> Documentation & Interfaces (local, contrastive, global)

### Critical path:
1. **Define Scope:** Fix black-box, choose auxiliary explainer (e.g., LIME), set initial rule complexity k
2. **Initialize:** Create empty Observation Base and SToBB object
3. **Online Update Loop:** For each new data point, query black-box, get explanation, update observation base, run CoBoT to maintain surrogate adequacy
4. **Audit/Query:** Use interfaces to extract explanations; review documentation and diagnostics for system health

### Design tradeoffs:
- **Complexity (k) vs. Adequacy:** A smaller k yields simpler, more human-readable rules but risks failing to achieve empirical adequacy if the black-box uses many features. CoBoT manages this by increasing k upon failure.
- **Auxiliary Explainer Fidelity:** The entire surrogate structure is conditioned on the feature sets identified by the auxiliary explainer (e.g., LIME). If LIME is unfaithful, the SToBB's rules will be built on irrelevant features. This is a critical dependency.
- **Assumption:** Fixed black-box. This architecture does not handle model drift or retraining out-of-the-box.

### Failure signatures:
- **High Update Rate:** If update_rate is consistently high (e.g., >> 4.5%), the surrogate hypothesis class or feature attribution method may be ill-suited for the data.
- **Low Compression:** Many singleton boxes (is_singleton_count high) indicates the surrogate is essentially memorizing data rather than finding generalizable rules. The k parameter may need adjustment, or the feature attributions are highly inconsistent.
- **Adequacy Failure:** If CoBoT cannot find a consistent box system even after increasing k to its maximum, the SToBB has failed for the current observation. The observation base must be modified (e.g., different auxiliary measures).

### First 3 experiments:
1. **Baseline Fidelity Check:** Run CoBoT on a held-out test set. For each sample, check if the surrogate's prediction matches the black-box. This tests if the "empirical adequacy" guarantee holds for unseen data (though the framework doesn't promise this, it's a useful proxy for generalization).
2. **Sensitivity to Auxiliary Explainer:** Instantiate two SToBBs, one using LIME and another using SHAP as the auxiliary explainer. Compare the final surrogate structures (Boxsystems) and the types of rules generated. This probes the design's sensitivity to its auxiliary input.
3. **Complexity vs. Human Interpretability:** For a given dataset, run CoBoT with different initial k values (e.g., k=2, 3, 4). Measure not only the compression rate but also conduct a simple user study to see if humans can predict the black-box's output using the generated rules. This tests the "pragmatic virtue" of the surrogate.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can existing XAI methods (e.g., Anchors) be adapted into SToBB components with cumulative, updateable properties rather than operating as single-shot explainers?
- **Basis in paper:** [explicit] "For methods such as Anchors (Ribeiro et al., 2018) one could imagine SToBB variants whose hypothesis class consists of sets of local rules... Working out such adaptations requires rethinking these methods as parts of a cumulative and updateable process rather than as single-shot explainers."
- **Why unresolved:** The paper only demonstrates CoBoT; adaptation of other XAI methods to the SToBB framework requires re-architecting their update mechanisms and hypothesis classes.
- **What evidence would resolve it:** Successful implementation of a SToBB using Anchors or similar methods, demonstrating incremental adequacy restoration with documented update procedures.

### Open Question 2
- **Question:** How should SToBBs handle non-stationary settings where the black-box model changes over time (e.g., fine-tuning, re-training)?
- **Basis in paper:** [explicit] "Working out concrete strategies for non-stationary settings is left for future work."
- **Why unresolved:** The framework assumes a fixed black-box; mechanisms for detecting stale observation bases or efficient reconstruction under model change remain unspecified.
- **What evidence would resolve it:** Empirical evaluation of SToBB maintenance strategies under controlled black-box drift scenarios, with metrics for reconstruction efficiency and adequacy restoration.

### Open Question 3
- **Question:** How do alternative hypothesis classes (linear/logistic regression, prototype-based models) perform as SToBB surrogates in terms of compression, update frequency, and adequacy maintenance?
- **Basis in paper:** [explicit] "Two avenues seem particularly natural: (i) developing other hypothesis classes and algorithmic components directly derived from established interpretable model families, such as linear or logistic regression and prototype-based models."
- **Why unresolved:** Only the box-based hypothesis class (CoBoT) is instantiated; other interpretable model families may have different trade-offs between expressivity and update complexity.
- **What evidence would resolve it:** Comparative study measuring compression rates, update frequencies, and computational costs across multiple hypothesis classes on benchmark datasets.

## Limitations
- Empirical adequacy guarantee is formally proven only for agreement with fixed black-box on seen data; generalization to unseen samples remains unvalidated
- Dependency on LIME for auxiliary measures introduces brittleness—if LIME attributions are inconsistent or misleading, the entire SToBB structure becomes unreliable
- Merge operation adapted from Stadtländer et al. (2024) is not fully specified, creating a gap in exact reproduction

## Confidence
- **High:** The conceptual framing of SToBB as a persistent, auditable artifact for a fixed black-box is internally consistent and well-grounded in Constructive Empiricism
- **Medium:** The CoBoT algorithm's empirical adequacy guarantee for seen data is plausible and supported by the Abalone experiment, but lacks validation on unseen data
- **Low:** Claims about the framework's generalizability across datasets, black-box types, or auxiliary explainer choices are not substantiated

## Next Checks
1. **Generalization test:** Apply CoBoT to a held-out test set and measure if the surrogate maintains empirical adequacy beyond the training observations
2. **Auxiliary explainer sensitivity:** Build two SToBBs using different auxiliary methods (e.g., LIME vs. SHAP) and compare surrogate structures and fidelity
3. **Complexity-interpretability tradeoff:** Vary the k parameter across runs and conduct a small human study to assess rule understandability and predictive utility