---
ver: rpa2
title: 'AgencyBench: Benchmarking the Frontiers of Autonomous Agents in 1M-Token Real-World
  Contexts'
arxiv_id: '2601.11044'
source_url: https://arxiv.org/abs/2601.11044
tags:
- tasks
- agent
- evaluation
- arxiv
- agentic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AgencyBench, a comprehensive benchmark designed
  to evaluate autonomous agents on long-horizon, real-world tasks. The benchmark addresses
  limitations of existing benchmarks by evaluating six core agentic capabilities across
  32 scenarios and 138 tasks, requiring an average of 90 tool calls, 1 million tokens,
  and hours of execution time.
---

# AgencyBench: Benchmarking the Frontiers of Autonomous Agents in 1M-Token Real-World Contexts

## Quick Facts
- **arXiv ID:** 2601.11044
- **Source URL:** https://arxiv.org/abs/2601.11044
- **Reference count:** 40
- **Primary result:** Introduces a benchmark evaluating autonomous agents on long-horizon tasks requiring 1M tokens, 90 tool calls, and hours of execution time across 32 scenarios and 138 tasks

## Executive Summary
AgencyBench addresses critical limitations in existing autonomous agent benchmarks by evaluating agents on complex, real-world tasks requiring sustained reasoning over millions of tokens and hours of execution. The benchmark measures six core agentic capabilities through a hierarchical task structure where task completion influences subsequent ones. By employing a user simulation agent for iterative feedback and a Docker sandbox for visual evaluation, the framework enables automated assessment at scale while maintaining fidelity to human evaluation standards.

The benchmark reveals significant performance gaps between closed-source and open-source models, with proprietary models achieving 48.4% pass rates compared to 32.1% for open-source alternatives. Notably, agent performance is highly sensitive to scaffold alignment, with models performing best within their native ecosystems due to training-objective coupling. The study demonstrates that feedback-driven self-correction capabilities vary dramatically across models, with some showing 88.9% performance improvement while others show none.

## Method Summary
AgencyBench employs a multi-stage evaluation pipeline where agents operate within isolated workspaces using predefined scaffolds to execute tool-based interactions across hierarchical tasks. Each task includes specifications, deliverables, and detailed rubrics for scoring. A user simulation agent provides iterative feedback when deliverables fail to meet rubric thresholds, enabling automated self-correction loops. Visual artifacts are rendered in a Docker sandbox environment with GUI capabilities for capturing screenshots and recordings. Final evaluation combines text-based and vision-based judges to score both code artifacts and visual outputs against rubric criteria.

## Key Results
- Proprietary models (48.4% pass rate) significantly outperform open-source models (32.1%) on long-horizon tasks
- Scaffold choice dramatically impacts performance, with Claude-4.5-Opus achieving 20.5% higher scores within its native SDK
- Models exhibit widely varying feedback utilization, ranging from 88.9% improvement to 0.0% change after iterative correction
- Task complexity correlates with resource consumption, with successful agents using 1.2-4.1 million tokens depending on approach

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Iterative, rubric-grounded feedback from a user simulation agent enables effective self-correction in long-horizon tasks.
- **Mechanism:** The agent proposes a solution (e.g., code), and a user simulation agent evaluates it against specific rubrics. If the score is below a threshold, the simulation agent returns precise failure reasons. The agent then updates its plan or code based on this grounded diagnostic, closing the performance gap over multiple attempts.
- **Core assumption:** The model can correctly interpret structured feedback and map it to a corrective action in the codebase without getting stuck in a repeated error loop.
- **Evidence anchors:**
  - [abstract] "To enable automated evaluation, we employ a user simulation agent to provide iterative feedback."
  - [section 3.2] "If an agent completes only 6 out of 10 rubrics, the user simulation agent will return the 4 failed rubrics along with their specific reasons for failure."
  - [corpus] Related work on LLM agents identifies feedback integration as a key capability for multi-turn task completion (generic support).
- **Break condition:** If the model misinterprets the feedback, or if the feedback is too vague, the agent may fix one rubric while breaking another, or persist in an erroneous state (as seen with DeepSeek-V3.2's 0.0% rise in Pass@k).

### Mechanism 2
- **Claim:** Agentic performance is highly sensitive to the alignment between a model and its scaffold (tools and prompts).
- **Mechanism:** Proprietary models often achieve "ecosystem synergy" because their training or fine-tuning aligns with the specific tool definitions, argument schemas, and prompt structures of their native SDKs. This alignment reduces the friction of tool invocation and planning.
- **Core assumption:** The model's internal representations for "planning" and "tool use" are coupled to specific syntactic patterns or semantic definitions it encountered during training.
- **Evidence anchors:**
  - [abstract] "Proprietary models demonstrate superior performance within their native ecosystems (e.g., Claude-4.5-Opus via Claude-Agent-SDK)."
  - [section 4.7] "Claude-4.5-Opus achieves a substantial performance boost of 20.5% when operating within the Claude-Code SDK... suggesting a deep optimization between the model's training objective and its proprietary tool definitions."
  - [corpus] No specific corpus papers were found to directly challenge or expand on this "home-field advantage" hypothesis.
- **Break condition:** Performance degrades when a model is paired with a scaffold that uses unfamiliar tool names, argument conventions, or control flow prompts, forcing the model to expend resources on "scaffold translation."

### Mechanism 3
- **Claim:** Success in long-horizon tasks is constrained by a model's ability to manage context and resources efficiently.
- **Mechanism:** Tasks averaging 1M tokens and 90 tool calls require maintaining a coherent goal state over many steps. Models must balance "brute-force" reasoning (high token usage, many attempts) against efficient, targeted actions. Excessive context accumulation can lead to state drift or coherence loss.
- **Core assumption:** The model's effective context window and state management mechanisms can handle the accumulated history of a multi-hour, multi-tool session.
- **Evidence anchors:**
  - [abstract] "Scenarios require an average of 90 tool calls, 1 million tokens, and hours of execution time."
  - [section 4.4] "GPT-5.2 acts as a 'brute-force' reasoner, consuming 3.4 million tokens... In contrast, Grok-4.1-Fast represents the pinnacle of speed and frugality."
  - [corpus] Related benchmarks note the difficulty of long-horizon reasoning, though specific 1M-token comparisons are rare.
- **Break condition:** If the model cannot efficiently manage its context (e.g., through summarization or memory tools), it will exceed context limits, hallucinate prior steps, or exhibit plan drift, causing task failure.

## Foundational Learning

- **Concept: Agentic Scaffolds (Tool Use & Prompting)**
  - **Why needed here:** The paper's core finding is that scaffold choice (Claude-Agent-SDK vs. OpenAI-Agents-SDK) can swing performance by over 20%. An engineer must understand that a scaffold is not just a tool list; it's a control interface.
  - **Quick check question:** Can you name three core components of an agentic scaffold beyond just the list of tools? (e.g., system prompt format, error handling loop, state manager).

- **Concept: Context Window Management & State Drift**
  - **Why needed here:** With 1M token contexts, managing state is non-trivial. The paper shows models differ in their use of memory tools (Gemini uses `update_memory_bank`). Understanding how context limits affect long-horizon reasoning is critical.
  - **Quick check question:** What is the likely failure mode for an agent operating for 2 hours on a task if it does not use external memory or summarization tools?

- **Concept: Rubric-Based vs. Outcome-Based Evaluation**
  - **Why needed here:** AgencyBench moves beyond simple pass/fail. It uses detailed rubrics (e.g., "board is 640Â±4px wide"). This allows for partial credit and more granular debugging of agent failures.
  - **Quick check question:** How does receiving feedback on 4 failed rubrics out of 10 differ from a simple "try again" signal?

## Architecture Onboarding

- **Component map:**
  Workspace -> User Simulation Agent -> Docker Sandbox -> Eval-space (Text Judge + Vision Judge)

- **Critical path:**
  1. Task Ingestion: Agent receives `{Query, Deliverable Spec, Rubrics}`.
  2. Rollout Generation: Agent executes tools in the Workspace to produce deliverables (e.g., code files).
  3. Feedback Loop: User Simulation Agent compares deliverables to rubrics. If score < threshold, it returns specific failure reasons. Agent retries (up to K attempts).
  4. Artifact Generation: Deliverables are synced to the Docker Sandbox. Scripts execute UI actions (clicks, screenshots, recordings).
  5. Final Evaluation: Artifacts and code are transferred to Eval-space for final scoring by Text and Vision judges.

- **Design tradeoffs:**
  - Custom Scaffold vs. Native SDK: Custom scaffolds offer flexibility but may suffer a performance penalty (e.g., Claude's -8.8% drop). Native SDKs offer high performance but reduce control.
  - User Simulation vs. Human-in-the-Loop: Simulation enables massive scale (138 tasks, automated). The tradeoff is potential bias or error in the simulation agent's feedback, though the paper validates alignment at a 4.69/5.00 score.
  - Efficiency vs. Performance: Some models (GPT-5.2) achieve high scores via brute-force (3.4M tokens). Others (Grok-4.1-Fast) are far more efficient (1.2M tokens) but may have a lower ceiling. Choose based on cost/latency constraints.

- **Failure signatures:**
  - High `Att` (Average Attempts), Low `Pass@k`: Indicates poor self-correction or a model that ignores feedback (e.g., DeepSeek-V3.2).
  - High Token Usage, Low Score: Indicates inefficient reasoning or "thrashing" in the planning phase.
  - Scaffold Mismatch: Sudden performance drop when switching from a native SDK to a generic scaffold (e.g., Claude-4.5-Opus losing 20.5%).

- **First 3 experiments:**
  1. Baseline & Efficiency Profiling: Run your primary model (e.g., GPT-4.1, Claude-3.5-Sonnet) on 3 diverse scenarios from AgencyBench. Measure `SAvg`, `Att`, and `Token Efficiency` to establish a baseline.
  2. Scaffold Ablation: Select a model with a native SDK (e.g., Claude) and run it on both its native scaffold and a generic one. Quantify the "home-field advantage" for your specific use case.
  3. Feedback Loop Validation: Manually inspect a few failed rollouts. Check if the User Simulation Agent's feedback is accurate and if the model's subsequent attempt addresses the specific rubric failures. This diagnoses whether failure is due to poor feedback or poor model reasoning.

## Open Questions the Paper Calls Out

- **Question:** Can agentic scaffolds be explicitly designed to be model-agnostic without sacrificing the "home-field advantage" performance peaks observed when models operate within their native ecosystems?
  - **Basis in paper:** [explicit] The authors observe that proprietary models demonstrate superior performance within native frameworks (e.g., Claude-4.5-Opus on Claude-Agent-SDK) and conclude that "agentic performance is... a result of the coupling between the model and its agentic scaffold."
  - **Why unresolved:** While the paper identifies the coupling effect, it does not propose a method for decoupling performance from the scaffold or creating a universal framework that maximizes performance across diverse model architectures.
  - **What evidence would resolve it:** A study showing a standardized scaffold design that closes the performance gap between "native" and "third-party" usage for frontier models.

- **Question:** To what extent do the capabilities required for AgencyBench's digital software tasks transfer to embodied agents interacting with the physical world?
  - **Basis in paper:** [explicit] The "Limitations" section explicitly states the framework is confined to software-based agents and "does not extend to embodied agents or tasks requiring physical world interaction (e.g., robotics)."
  - **Why unresolved:** The benchmark validates long-horizon planning in digital environments, but it is unknown if the "context retention" and "logic execution" skills measured are sufficient or relevant for physical spatial reasoning and actuation.
  - **What evidence would resolve it:** A comparative study correlating AgencyBench scores with performance metrics on physical robotics benchmarks involving similar horizons of complexity.

- **Question:** What specific architectural or training-data components enable certain models (e.g., GPT-5.2) to leverage feedback for self-correction significantly more effectively than others (e.g., DeepSeek-V3.2)?
  - **Basis in paper:** [explicit] The paper notes a "Feedback-driven Self-correction Analysis" where GPT-5.2 shows an 88.9% rise in performance after feedback, whereas DeepSeek-V3.2 shows 0.0% rise, persisting in erroneous paths.
  - **Why unresolved:** The paper quantifies the behavioral disparity but does not investigate the underlying cause, such as whether it stems from context window management, instruction-following tuning, or reasoning architecture.
  - **What evidence would resolve it:** Ablation studies isolating the feedback mechanism in high-performing models to identify the structural necessity for robust error recovery.

## Limitations

- The evaluation framework relies on a single model (Claude-4-Sonnet) for both user simulation and text-based evaluation, potentially introducing bias
- Docker sandbox implementation details are underspecified, making it difficult to assess the reliability of visual evaluations
- The benchmark focuses exclusively on LLM-based agents, limiting generalizability to other agent architectures

## Confidence

- **High Confidence:** The comparative performance between closed-source and open-source models, the impact of scaffold choice on model performance, and the correlation between token efficiency and task completion rates
- **Medium Confidence:** The specific numerical benchmarks (SAvg, Att, Token Efficiency) due to potential evaluation framework biases, and the claim that 1M-token contexts represent a fundamental challenge for state management
- **Low Confidence:** The generalizability of the "home-field advantage" findings beyond the specific models tested, and the assumption that the User Simulation Agent provides equivalent quality feedback to human evaluators across all task types

## Next Checks

1. **Cross-Model Evaluation Consistency:** Run the same tasks through multiple independent evaluation models to verify that performance rankings remain consistent and aren't artifacts of a single evaluator's biases

2. **Scaffold-Independent Performance:** Implement a minimal, standardized scaffold that strips away model-specific optimizations to test whether performance differences persist independent of ecosystem advantages

3. **Long-Term State Coherence:** Design a task sequence that explicitly tests memory and context management over extended periods (4+ hours) to validate whether the reported token efficiency metrics reflect genuine planning capability versus short-term optimization