---
ver: rpa2
title: 'Less is More Tokens: Efficient Math Reasoning via Difficulty-Aware Chain-of-Thought
  Distillation'
arxiv_id: '2509.05226'
source_url: https://arxiv.org/abs/2509.05226
tags:
- reasoning
- problem
- difficulty
- arxiv
- solution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We introduce a framework for difficulty-aware chain-of-thought
  (CoT) reasoning that trains models to adjust reasoning depth based on problem complexity.
  Our approach compresses verbose teacher CoT traces according to estimated difficulty
  and fine-tunes student models using supervised fine-tuning (SFT) and direct preference
  optimization (DPO).
---

# Less is More Tokens: Efficient Math Reasoning via Difficulty-Aware Chain-of-Thought Distillation

## Quick Facts
- arXiv ID: 2509.05226
- Source URL: https://arxiv.org/abs/2509.05226
- Reference count: 40
- Primary result: 30% token reduction while maintaining/improving accuracy through difficulty-aware CoT compression and SFT+DPO training

## Executive Summary
This work introduces a framework for efficient math reasoning that dynamically adjusts reasoning depth based on problem complexity. The approach compresses verbose teacher chain-of-thought traces according to estimated difficulty and fine-tunes student models using supervised fine-tuning (SFT) and direct preference optimization (DPO). The combination achieves up to 30% reduction in reasoning token usage while maintaining or improving final answer accuracy across both unimodal and multimodal math benchmarks. SFT primarily learns structured and concise reasoning patterns, while DPO preserves correctness and refines reasoning quality, making their combination optimal for balancing efficiency and performance.

## Method Summary
The method involves three main stages: first, difficulty estimation using GPT-4o-mini to assign scores (1-10) aligned with AoPS standards; second, compression of long CoT traces using GPT-4o-mini proportionally to difficulty; and third, training student models through SFT followed by DPO. The training uses LLaMA-Factory with LoRA adaptation, where SFT learns format and length patterns from compressed traces, then DPO optimizes for correctness while maintaining conciseness through preference learning. The framework is applied to both unimodal math problems and multimodal math-vision tasks, demonstrating consistent efficiency gains across domains.

## Key Results
- SFT+DPO combination achieves up to 30% reduction in reasoning token usage
- Accuracy maintained or improved on AIME, MATH, GSM8K, MathVerse, and MathVision benchmarks
- SFT captures surface-level patterns like reasoning length and format, while DPO preserves reasoning accuracy
- Model learns to allocate reasoning effort adaptively without architectural modifications

## Why This Works (Mechanism)

### Mechanism 1: Difficulty-Conditioned Compression as Implicit Curriculum
Compressing CoT traces proportionally to problem difficulty creates an implicit curriculum teaching models to allocate reasoning effort adaptively. The model learns the correlation between input complexity and appropriate reasoning length through exposure to difficulty-aligned examples during training.

### Mechanism 2: SFT Captures Surface Structure, DPO Preserves Semantic Correctness
SFT followed by DPO optimizes complementary objectives—SFT learns formatting and conciseness patterns, while DPO reinforces correctness without reverting to verbosity. This sequential combination prevents the accuracy loss typically seen with aggressive compression.

### Mechanism 3: Difficulty-Aware Trace Length as Inference-Time Compute Allocation
Models internalize a "compute budget" proportional to problem difficulty, achieving efficiency gains without explicit architectural modifications. During training, the model sees statistical correlation between difficult problems and longer traces, which it then applies at inference time.

## Foundational Learning

- **Chain-of-Thought (CoT) Distillation**: Understanding how teacher reasoning traces transfer to student models is prerequisite to understanding why compression + preference optimization improves upon standard distillation. *Quick check*: Can you explain why distilling CoT traces from a larger model to a smaller one typically preserves reasoning ability but increases verbosity?

- **Direct Preference Optimization (DPO)**: DPO optimizes from preference pairs (preferred vs. rejected) without an explicit reward model. *Quick check*: How does DPO differ from reinforcement learning from human feedback (RLHF) in terms of what it directly optimizes?

- **Difficulty Estimation via LLM Grading**: The framework depends on accurate difficulty scores via prompted GPT-4o-mini aligned to AoPS standards. *Quick check*: What are the failure modes if the difficulty estimator systematically under-rates problems from a specific domain (e.g., geometry)?

## Architecture Onboarding

- **Component map**: Difficulty Estimator (GPT-4o-mini) → Compression Module (GPT-4o-mini) → SFT stage → DPO stage → Evaluation Suite
- **Critical path**: Data preparation is bottleneck; difficulty estimation + compression must be done before training; SFT must converge before DPO; preference data must use same compressed traces
- **Design tradeoffs**: Compression aggressiveness vs. accuracy risk; LoRA vs. full fine-tuning; verifier use for multimodal evaluation
- **Failure signatures**: SFT-only produces concise but incorrect answers; DPO-only maintains accuracy but remains verbose; uniform compression ratios prevent adaptive learning
- **First 3 experiments**: 1) Validate difficulty estimator alignment with human ratings; 2) Ablate SFT→DPO ordering; 3) Test out-of-distribution difficulty performance

## Open Questions the Paper Calls Out

- How robust is the framework to errors in initial difficulty estimation, and do mis-rated problems degrade student performance?
- Does full parameter fine-tuning unlock better difficulty-adaptive capabilities compared to LoRA?
- Does compressing chain-of-thought traces negatively impact reliability when evaluated using pass@k metrics?
- Can this difficulty-aware distillation approach generalize to non-mathematical domains like coding or commonsense reasoning?

## Limitations

- Reliance on LLM-generated difficulty scores without empirical validation against human expert ratings
- Limited validation of domain generalization beyond competition math and grade-school problems
- No rigorous analysis of minimum reasoning length required to maintain correctness across difficulty levels
- Lack of KL-divergence reporting between SFT and DPO checkpoints to verify style preservation

## Confidence

**High Confidence**: The sequential SFT-then-DPO training pipeline produces claimed efficiency gains (30% token reduction) while maintaining/improving accuracy on reported benchmarks.

**Medium Confidence**: The difficulty-aware compression mechanism generalizes beyond the specific implementation, though exact compression function may not work with different base models.

**Low Confidence**: The claim that models internalize difficulty-aware reasoning without explicit difficulty signals at inference time lacks empirical validation.

## Next Checks

1. **Difficulty estimator validation**: Sample 100 problems and compare GPT-4o-mini ratings against 3 human expert ratings using AoPS rubric; require correlation > 0.7 for reliable compression signal.

2. **Domain-specific performance analysis**: Break down accuracy and token usage by problem type (algebra, geometry, number theory); identify if token reduction comes from specific domains while accuracy degrades in others.

3. **Compression threshold sweep**: Systematically vary compression aggressiveness to identify Pareto frontier between token usage and accuracy, establishing safe compression limits for each difficulty level.