---
ver: rpa2
title: Knowledge Augmented Finetuning Matters in both RAG and Agent Based Dialog Systems
arxiv_id: '2506.22852'
source_url: https://arxiv.org/abs/2506.22852
tags:
- knowledge
- systems
- llms
- dialog
- kaft
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of improving factual accuracy
  in knowledge-intensive dialog systems, where large language models (LLMs) are prone
  to errors despite progress in general tasks. The authors propose knowledge augmented
  finetuning (KAFT), a method that finetunes LLMs using domain-specific data along
  with domain-specific external knowledge, aiming to teach the model how to effectively
  use retrieved knowledge.
---

# Knowledge Augmented Finetuning Matters in both RAG and Agent Based Dialog Systems

## Quick Facts
- arXiv ID: 2506.22852
- Source URL: https://arxiv.org/abs/2506.22852
- Reference count: 31
- Key outcome: KAFT outperforms prompting on MobileCS2 dataset, achieving 0.590 vs 0.493 combined score in RAG-based systems, with notable gains in factual accuracy metrics.

## Executive Summary
This paper addresses the problem of improving factual accuracy in knowledge-intensive dialog systems, where large language models (LLMs) are prone to errors despite progress in general tasks. The authors propose knowledge augmented finetuning (KAFT), a method that finetunes LLMs using domain-specific data along with domain-specific external knowledge, aiming to teach the model how to effectively use retrieved knowledge. This contrasts with the common prompting approach, where LLMs are directly prompted to use retrieved knowledge without being trained on how to leverage it. The paper builds both RAG-based and agent-based dialog systems on the MobileCS2 dataset and systematically compares KAFT with prompting. Results show that KAFT substantially outperforms prompting in both systems, particularly on factual accuracy metrics.

## Method Summary
The paper proposes Knowledge Augmented Finetuning (KAFT) to improve factual accuracy in knowledge-intensive dialog systems. The method trains a dual-encoder retriever using BERT-based encoders with contrastive loss on domain-specific knowledge pieces. The generator (initialized from GPT-2) is finetuned with auto-regressive loss on (context, retrieved_knowledge, response) triples. A key innovation is training with retrieved (not oracle) knowledge to teach the model discernment skills. For agent-based systems, additional components include a decision maker and API-specific retrievers. The method is evaluated on MobileCS2 dataset using BLEU, BERTScore, Inform Rate, and Combined Score metrics.

## Key Results
- KAFT achieves 0.590 combined score vs 0.493 for prompting in RAG-based systems on MobileCS2 test set
- KAFT shows significant improvements in Inform Rate: 0.145 vs 0.059 for prompting
- GPT-2 with KAFT outperforms GPT-3.5 with prompting on domain-specific tasks
- Ablation studies confirm training with retrieved knowledge (+0.073) improves robustness over oracle knowledge

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Training with retrieved (potentially noisy) knowledge rather than oracle knowledge improves the model's ability to handle imperfect retrieval at inference time.
- **Mechanism:** By exposing the model to retrieved knowledge during training—including cases where retrieval may be partially irrelevant—the model learns to discern useful information from noise, developing a filtering/disambiguation skill that transfers to test conditions.
- **Core assumption:** The retriever error distribution during training approximates test-time retrieval errors.
- **Evidence anchors:**
  - [abstract] "finetune LLMs using domain-specific data along with domain-specific external knowledge"
  - [Section III.B.1] "we use the generated ht from the retriever rather than the annotated ht so that the training procedure of the generation model is aligned to the test setting"
  - [Table V] Using retrieved knowledge in training (Score: 0.590) outperforms using oracle knowledge (Score: 0.517)
- **Break condition:** If retrieval quality degrades significantly between training and deployment, the learned discernment skill may not transfer.

### Mechanism 2
- **Claim:** KAFT teaches domain-specific concept interpretation that prompting alone cannot convey.
- **Mechanism:** Finetuning on domain data with knowledge context enables the model to internalize domain-specific terminology and reasoning patterns (e.g., understanding "directional flow" in mobile services), allowing it to correctly interpret and apply retrieved knowledge.
- **Core assumption:** The training data contains sufficient examples of domain-specific concept usage.
- **Evidence anchors:**
  - [Figure 1 caption] "the LLM does not understand the term 'directional flow' which is specific to the mobile service domain"
  - [Section I] "LLMs may have difficulty using the retrieved knowledge effectively... because they are not well trained to do such generation for specific domains"
  - [corpus] Weak direct evidence; neighbor papers discuss similar finetuning+RAG combinations but don't isolate the domain terminology learning mechanism.
- **Break condition:** If domain concepts are too rare in training data or test queries require concepts not seen during finetuning.

### Mechanism 3
- **Claim:** Smaller finetuned models can outperform larger prompted models on specialized tasks.
- **Mechanism:** Task-specific finetuning with knowledge integration specializes the model's weights for the exact retrieval-to-generation mapping required, compensating for lower base capacity. GPT-2 (small) with KAFT beats GPT-3.5 with prompting.
- **Core assumption:** The task distribution is narrow enough that specialized weights generalize within-domain.
- **Evidence anchors:**
  - [abstract] "KAFT proves effective even with smaller models like GPT-2, surpassing larger models like GPT-3.5 with prompting"
  - [Table I] GPT-2 KAFT Score: 0.590 vs GPT-3.5 5-shot prompting Score: 0.493 (RAG)
  - [corpus] Neighbor paper "KaFT: Knowledge-aware Fine-tuning" explores similar domain QA improvements but with different methodology.
- **Break condition:** Task distribution shifts significantly; larger models may regain advantage with out-of-distribution queries.

## Foundational Learning

- **Concept: Retrieval-Augmented Generation (RAG)**
  - **Why needed here:** KAFT builds directly on RAG architecture; understanding retriever-generator separation is prerequisite.
  - **Quick check question:** Can you explain how a retriever passes documents to a generator, and where the "knowledge bottleneck" occurs?

- **Concept: Auto-regressive Language Modeling Loss**
  - **Why needed here:** KAFT optimizes Eq. 3 using standard causal LM training; understanding token-level supervision is essential.
  - **Quick check question:** How does next-token prediction loss differ when conditioning on retrieved context vs. no context?

- **Concept: In-Context Learning (ICL) / Prompting**
  - **Why needed here:** The paper's baseline is prompting; you must understand what ICL can and cannot do to appreciate KAFT's advantage.
  - **Quick check question:** Why might few-shot examples fail to convey domain-specific term meanings that finetuning succeeds at?

## Architecture Onboarding

- **Component map:**
  - Dual-encoder retriever (BERT-based context encoder + frozen piece encoder) -> retrieves knowledge pieces from KB_user, KB_product, KB_FAQ
  - Generator (KAFT) initialized from GPT-2, finetuned with (context, retrieved_knowledge) -> response
  - Agent variant: Decision maker (finetuned) -> API calls (separate retrievers per KB type) -> generator

- **Critical path:**
  1. Train retriever(s) with Eq. 2 on annotated positive knowledge pieces
  2. Generate retrieved knowledge for training dialogs using trained retriever (not oracle)
  3. Finetune generator on (context, retrieved_knowledge, response) triples using Eq. 3
  4. At inference: retrieve -> generate

- **Design tradeoffs:**
  - RAG vs Agent: RAG has no decision-making step (lower error cascade); Agent has better retrieval (separate APIs) but suffers from decision accuracy (Table II: 0.381-0.580 even with KAFT)
  - Oracle vs Retrieved training: Oracle gives higher training signal but creates train-test mismatch; Retrieved improves robustness (+0.073 combined score)
  - Model size vs Finetuning cost: GPT-2 requires full finetuning infrastructure; GPT-3.5 uses API-only prompting

- **Failure signatures:**
  - Low Inform Rate despite high BLEU: Model generates fluent but factually empty responses -> retriever not providing relevant knowledge OR model not using it
  - Decision accuracy < 0.5 (Agent): System searches wrong KB -> check if decision maker training data balanced
  - Recall@1 near 0 (Product search): Retrieval failing -> KB_product may lack coverage or queries are ambiguous

- **First 3 experiments:**
  1. Baseline replication: Implement prompting baseline with GPT-3.5 API on MobileCS2 dev set; verify Inform Rate ~0.06 (RAG) or ~0.08 (Agent)
  2. Retriever-only ablation: Train retriever per Eq. 2, measure Recall@5 on Product and FAQ; target >0.13 (Product) and >0.65 (FAQ)
  3. KAFT on small model: Finetune GPT-2 with retrieved (not oracle) knowledge; target combined score >0.55 on test set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can knowledge retrieval accuracy be substantially improved, particularly for product search where recall@1 remains very low (0.049-0.075)?
- Basis in paper: [explicit] Authors state "the recall@1 is relatively low for the product and FAQ search, especially the product search, indicating that more efforts should be put into increasing the knowledge retrieval accuracy for both RAG and agent systems."
- Why unresolved: Current dual-encoder retrievers struggle with product matching; Table IV shows oracle knowledge would boost score from 0.590 to 0.992, revealing retrieval as the primary bottleneck.
- What evidence would resolve it: Novel retrieval architectures or training strategies achieving significantly higher recall@1 on product search without sacrificing precision.

### Open Question 2
- Question: Does KAFT provide similar benefits when applied to larger, more capable LLMs (e.g., LLaMA-2-70B, GPT-4) as observed with GPT-2?
- Basis in paper: [inferred] The paper only experiments with GPT-2 for KAFT and GPT-3.5 for prompting. While authors state "Using GPT-2 suffices to investigate the research question," the interaction between model scale and KAFT effectiveness remains unexplored.
- Why unresolved: Larger models may already possess stronger in-context learning abilities, potentially diminishing KAFT's relative advantage over prompting.
- What evidence would resolve it: Systematic comparison of KAFT vs prompting across multiple model scales (1B-70B parameters) on the same tasks.

### Open Question 3
- Question: How can decision-making accuracy in agent-based systems be improved beyond the current 38-58% range observed with KAFT?
- Basis in paper: [explicit] Table II shows decision accuracy for Personal (0.381), Product (0.580), and FAQ (0.475) search even with KAFT. Authors note "the agent system suffers from low decision making accuracy, whether by prompting or by KAFT."
- Why unresolved: Complex contexts in real customer service dialogs make accurate search decisions difficult; current training approaches insufficient.
- What evidence would resolve it: Novel decision-making architectures, multi-step reasoning approaches, or enhanced context modeling achieving >75% decision accuracy.

### Open Question 4
- Question: Does KAFT generalize effectively to other knowledge-intensive domains beyond mobile customer service?
- Basis in paper: [inferred] All experiments use only the MobileCS2 dataset. The domain-specific terminology (e.g., "directional flow") and knowledge structures may not transfer to healthcare, legal, or financial domains.
- Why unresolved: Single-domain evaluation cannot establish whether KAFT's benefits are universal or depend on MobileCS2's specific characteristics.
- What evidence would resolve it: Evaluation of KAFT on multiple knowledge-intensive datasets across different vertical domains (e.g., healthcare QA, legal consultation).

## Limitations

- Key implementation details remain underspecified, including training hyperparameters (learning rate, batch size, epochs, optimizer, warmup steps, weight decay) and exact prompt templates for prompting baselines.
- The paper only evaluates KAFT on the MobileCS2 dataset, limiting generalizability to other domains or knowledge-intensive tasks.
- Product search retrieval performance remains poor (recall@1 ~0.05-0.07), indicating retrieval quality is still a bottleneck even with KAFT.

## Confidence

**High confidence** in the core finding that KAFT outperforms prompting on MobileCS2 dataset for both RAG and agent architectures. The ablation study showing retrieved knowledge outperforms oracle knowledge during training (+0.073 combined score) is well-supported and provides strong evidence for the proposed mechanism.

**Medium confidence** in the claim that smaller finetuned models can outperform larger prompted models. While Table I shows GPT-2 KAFT beating GPT-3.5 prompting, the absolute performance gap (0.590 vs 0.493) is modest and may not generalize across different domains or dataset sizes.

**Medium confidence** in the domain-specific concept learning mechanism. The directional flow example provides anecdotal support, but the corpus lacks systematic analysis of how KAFT handles domain terminology versus general knowledge. The neighbor papers mention similar findings but don't isolate this mechanism.

## Next Checks

1. **Hyperparameter sensitivity analysis**: Re-run KAFT with varying learning rates (1e-5, 5e-5, 1e-4) and batch sizes (4, 8, 16) to determine if performance gains are robust to implementation details or depend on specific configurations.

2. **Cross-domain generalization test**: Apply KAFT to a different domain-specific dialog dataset (e.g., technical support for software or medical Q&A) to verify that the domain terminology learning mechanism generalizes beyond mobile services.

3. **Retrieval quality threshold determination**: Systematically vary retriever Recall@1 (0.1 → 0.9) during KAFT training to identify the minimum retrieval quality required for KAFT to outperform prompting, testing the core assumption that training with imperfect retrieval builds robustness.