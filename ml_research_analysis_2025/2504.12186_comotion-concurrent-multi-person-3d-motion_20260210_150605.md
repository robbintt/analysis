---
ver: rpa2
title: 'CoMotion: Concurrent Multi-person 3D Motion'
arxiv_id: '2504.12186'
source_url: https://arxiv.org/abs/2504.12186
tags:
- pose
- image
- poses
- tracking
- tracks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CoMotion is a method for estimating and tracking the 3D poses of
  multiple people from a single monocular video stream. Unlike prior approaches that
  detect poses in each frame and associate them over time, CoMotion maintains a set
  of tracked poses and directly updates them from incoming image features, enabling
  online tracking through occlusion.
---

# CoMotion: Concurrent Multi-person 3D Motion

## Quick Facts
- arXiv ID: 2504.12186
- Source URL: https://arxiv.org/abs/2504.12186
- Reference count: 0
- Primary result: CoMotion achieves state-of-the-art performance on multi-person tracking benchmarks, improving tracking accuracy metrics by over 10% compared to prior work while providing accurate 3D pose estimates.

## Executive Summary
CoMotion is a method for estimating and tracking the 3D poses of multiple people from a single monocular video stream. Unlike prior approaches that detect poses in each frame and associate them over time, CoMotion maintains a set of tracked poses and directly updates them from incoming image features, enabling online tracking through occlusion. The model is trained on a diverse mix of image and video datasets, including pseudo-labeled real-world videos, to learn robust pose estimation and tracking. CoMotion achieves state-of-the-art performance on multi-person tracking benchmarks, improving tracking accuracy metrics by over 10% compared to prior work, while also providing accurate 3D pose estimates. The approach is significantly faster than comparable systems, making it suitable for real-time applications.

## Method Summary
CoMotion maintains a set of tracked 3D poses and directly updates them from incoming image features in each frame, rather than performing frame-by-frame detection followed by temporal association. This tracking-first approach enables online processing and better handling of occlusions. The model is trained on a diverse dataset mix including image datasets, video datasets, and pseudo-labeled real-world videos to learn robust pose estimation and tracking capabilities. The system achieves real-time performance while maintaining state-of-the-art accuracy on multi-person tracking benchmarks.

## Key Results
- State-of-the-art performance on multi-person tracking benchmarks
- Over 10% improvement in tracking accuracy metrics compared to prior work
- Real-time capable system suitable for practical applications

## Why This Works (Mechanism)
CoMotion's tracking-first approach works by maintaining persistent pose estimates that are continuously updated from image features rather than detecting poses independently in each frame. This allows the system to maintain identity consistency through occlusions and partial visibility. By training on a diverse mix of datasets including pseudo-labeled real-world videos, the model learns to handle the variability and complexity of real-world scenarios. The direct update mechanism from image features to tracked poses enables online processing, making the system suitable for real-time applications while maintaining accuracy.

## Foundational Learning
- **3D Human Pose Estimation**: Understanding how to reconstruct 3D body positions from 2D image data is fundamental to CoMotion's operation. Why needed: The system must accurately estimate 3D poses from monocular video. Quick check: Can the model accurately reconstruct 3D poses in controlled environments with ground truth data?
- **Multi-object Tracking**: The ability to maintain consistent identity labels across frames for multiple people is crucial. Why needed: CoMotion needs to track multiple people simultaneously while maintaining correct identity associations. Quick check: Does the system correctly maintain identities through complete occlusions?
- **Online vs Offline Processing**: Understanding the trade-offs between processing frames sequentially versus with future context. Why needed: CoMotion is designed for online processing, which impacts algorithm design choices. Quick check: Can the system process video in real-time without buffering future frames?
- **Pseudo-labeling Techniques**: Methods for generating training data from unlabeled real-world videos. Why needed: The training pipeline incorporates pseudo-labeled real-world videos to improve generalization. Quick check: What is the quality and coverage of the pseudo-labeled training data?
- **Temporal Consistency Models**: Techniques for maintaining smooth pose trajectories across frames. Why needed: CoMotion needs to produce temporally coherent pose sequences. Quick check: Are the pose trajectories smooth and physically plausible across time?

## Architecture Onboarding

**Component Map:**
Image Features -> Pose Tracker -> 3D Pose Output
                 -> Identity Association

**Critical Path:**
Input video frames → Feature extraction → Pose tracking update → 3D pose output with identity labels

**Design Tradeoffs:**
- Online vs offline processing: CoMotion chooses online processing for real-time capability, sacrificing potential accuracy gains from future frame information
- Tracking-first vs detection-first: The tracking-first approach enables better occlusion handling but may be more sensitive to initialization errors
- Dataset diversity vs quality: Incorporating pseudo-labeled real-world videos increases diversity but may introduce noise

**Failure Signatures:**
- Identity switches during occlusion recovery
- Pose drift when people are partially visible
- Performance degradation with complex camera motion
- Sensitivity to initialization in crowded scenes

**First Experiments:**
1. Test tracking accuracy on videos with controlled occlusions to verify identity maintenance
2. Evaluate 3D pose accuracy on benchmark datasets with ground truth 3D annotations
3. Measure inference speed on representative hardware with varying numbers of people

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions in the provided content.

## Limitations
- Lack of detailed information about training dataset composition and pseudo-labeling methodology
- Claims of "state-of-the-art performance" and "over 10%" improvement lack specificity about metrics and baselines
- Real-time performance claims not substantiated with specific frame rate measurements
- Unclear generalization to diverse real-world scenarios with significant occlusions or complex camera motion

## Confidence
- High confidence in the novel tracking approach description (maintaining tracked poses and updating from image features)
- Medium confidence in the performance claims due to lack of specific metric details and baseline comparisons
- Low confidence in the real-time capability claims without quantitative speed measurements

## Next Checks
1. Obtain and analyze the specific quantitative results for tracking accuracy metrics (MOTA, MOTP, IDF1, etc.) with exact baseline comparisons and statistical significance tests
2. Measure and report the actual inference speed (frames per second) on representative hardware configurations with varying numbers of people in the scene
3. Conduct ablation studies to quantify the contribution of the pseudo-labeled real-world video data versus synthetic or curated datasets to overall performance