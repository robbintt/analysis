---
ver: rpa2
title: 'JustEva: A Toolkit to Evaluate LLM Fairness in Legal Knowledge Inference'
arxiv_id: '2509.12104'
source_url: https://arxiv.org/abs/2509.12104
tags:
- fairness
- legal
- bias
- llms
- toolkit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces JustEva, an open-source toolkit designed
  to evaluate LLM fairness in legal knowledge inference. It features a structured
  label system covering 65 extra-legal factors and three core fairness metrics: inconsistency,
  bias, and imbalanced inaccuracy.'
---

# JustEva: A Toolkit to Evaluate LLM Fairness in Legal Knowledge Inference

## Quick Facts
- arXiv ID: 2509.12104
- Source URL: https://arxiv.org/abs/2509.12104
- Reference count: 40
- Primary result: Toolkit evaluates LLM fairness in legal inference using 65 extra-legal factors and three metrics (inconsistency, bias, imbalanced inaccuracy), revealing significant fairness deficiencies in current models.

## Executive Summary
JustEva is an open-source toolkit designed to evaluate fairness in large language models (LLMs) applied to legal knowledge inference. It introduces a structured approach using a dataset of Chinese judicial documents paired with counterfactual variants that isolate specific extra-legal factors. The toolkit implements three core fairness metrics—inconsistency, bias, and imbalanced inaccuracy—through robust statistical methods including high-dimensional fixed-effects regression and Bernoulli testing. Empirical application demonstrates significant fairness deficiencies in current LLMs, highlighting the need for fair and trustworthy legal AI tools.

## Method Summary
JustEva employs counterfactual evaluation using the JudiFair dataset containing 177,100 Chinese judicial case facts with 65 legal labels and their counterfactual variants. The toolkit generates structured outputs from LLMs via OpenRouter API, then applies high-dimensional fixed-effects regression (using Stata's REGHDFE via PyStata) to quantify bias, while Bernoulli tests aggregate findings across multiple labels to identify systematic patterns. Results are visualized through a Vue.js frontend with Chart.js integration.

## Key Results
- Empirical evaluation reveals significant fairness deficiencies in current LLMs when applied to legal knowledge inference
- The toolkit successfully identifies systematic bias patterns across 65 extra-legal factors through its three-metric approach
- Statistical validation confirms that observed biases are not due to random chance but represent systemic issues

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Controlled counterfactual variations isolate the causal impact of extra-legal factors on LLM outputs.
- **Mechanism:** The system uses the JudiFair dataset, which contains real judicial documents paired with "counterfactual variants" where specific labels (e.g., gender) are swapped while holding case facts constant. By comparing the LLM's prediction on the original versus the counterfactual, the toolkit attributes prediction changes specifically to the altered label.
- **Core assumption:** The legal cases in the dataset are sufficiently similar in their core facts that any divergence in model output is causally linked to the manipulated extra-legal factor, rather than noise or legal nuance.
- **Evidence anchors:**
  - [abstract] Mentions the toolkit generates structured outputs from a provided dataset.
  - [Section 3.1] Describes JudiFair as consisting of cases "paired with counterfactual variants that isolate specific label changes... while preserving all other facts."
  - [corpus] "LLMs on Trial" (Hu et al.) establishes the foundational methodology this toolkit operationalizes.
- **Break condition:** If the LLM treats the counterfactual swap as a semantic contradiction (e.g., changing "he" to "she" in a way that breaks context) rather than a controlled variable, the "Inconsistency" metric may measure confusion rather than bias.

### Mechanism 2
- **Claim:** High-dimensional fixed-effects regression quantifies "Bias" by controlling for document-level heterogeneity.
- **Mechanism:** The toolkit employs fixed-effects linear regression (via Stata's REGHDFE) to predict sentence length. By including fixed effects for the Document ID, the model absorbs all static characteristics of a specific case, leaving the coefficients of the extra-legal labels (e.g., gender, location) to represent their marginal, independent impact on the sentencing prediction.
- **Core assumption:** The relationship between extra-legal factors and sentencing outcomes can be approximated by a linear model once document-level fixed effects are accounted for.
- **Evidence anchors:**
  - [Section 3.2.2] "Each regression includes fixed effects for ID... thus controlling for heterogeneity in case facts arising out of the same document."
  - [Section 4.2] Confirms the use of PyStata to run these specific regression tools directly.
- **Break condition:** If the number of fixed effects consumes too many degrees of freedom relative to the sample size, or if the linearity assumption fails for complex legal reasoning, the statistical significance of bias may be misestimated.

### Mechanism 3
- **Claim:** Bernoulli testing aggregates individual label findings into a systemic fairness assessment.
- **Mechanism:** Evaluating 65 labels creates a "multiple comparisons" problem where random noise may appear as significant bias. JustEva models the detection of significant labels as Bernoulli trials. It calculates the probability ($p_{bernoulli}$) that the observed count of significant biases occurred by chance, ensuring that a "biased model" verdict reflects a systemic pattern rather than sporadic statistical flukes.
- **Core assumption:** The detection of bias in each individual label is an independent event (a standard assumption for the Bernoulli test, though correlations may exist between labels).
- **Evidence anchors:**
  - [Section 3.2.2] "We model each detection of significant label-level bias as Bernoulli trials... If $p_{bernoulli}$ is small, the bias is deemed systematic."
  - [corpus] No direct corpus neighbor explicitly critiques this specific statistical aggregation method, though "When Fairness Isn't Statistical" generally questions statistical proxies in legal reasoning.
- **Break condition:** If extra-legal factors are highly correlated (e.g., race and location), the independence assumption of the Bernoulli test is violated, potentially overstating the certainty of systemic bias.

## Foundational Learning

- **Concept:** Fixed-Effects Regression
  - **Why needed here:** This is the statistical engine of the toolkit. You must understand that "fixed effects" are dummy variables for each specific document group, allowing the model to compare a case against its own counterfactual twin rather than against unrelated cases.
  - **Quick check question:** If a dataset has 1,000 unique cases and 10 counterfactuals per case, how many "ID" fixed effects would the regression model include?

- **Concept:** Counterfactual Evaluation
  - **Why needed here:** The core validity of the "Inconsistency" metric depends on this concept. You need to distinguish between evaluating a model on random data vs. evaluating it on controlled, synthetic variations of the same data point.
  - **Quick check question:** Why is comparing an LLM's output on "Case A" vs. "Case B" insufficient for proving bias regarding a specific factor like gender?

- **Concept:** Multiple Comparisons Problem
  - **Why needed here:** The paper explicitly introduces a Bernoulli test to solve this. You must understand why finding 5 biased labels out of 65 might just be luck (false positives) without a correction method.
  - **Quick check question:** If you test 100 independent fair coins, what is the probability that at least one will land on heads 5 times in a row purely by chance?

## Architecture Onboarding

- **Component map:** Vue 3 + Vite + Chart.js (Frontend) -> Python (Backend Driver) -> OpenRouter API (LLM Interface) -> Stata (Analysis Engine via PyStata) -> JudiFair Dataset (Data Source)

- **Critical path:**
  1.  **Configuration:** User defines Model API details and parameters (e.g., temperature) in the Vue frontend.
  2.  **Inference:** Python backend sends prompts to the LLM via OpenRouter; predictions are stored.
  3.  **Analysis:** Python calls PyStata to execute high-dimensional fixed-effects regression on the predictions.
  4.  **Aggregation:** Python applies Bernoulli tests to p-values from Stata.
  5.  **Visualization:** Results are formatted to JSON and rendered as charts in the frontend.

- **Design tradeoffs:**
  - **Stata vs. Python (Statsmodels):** The authors chose Stata (via PyStata) for "efficiency" and established reliability in empirical legal studies, but this introduces a proprietary dependency and licensing requirement compared to pure Python stacks.
  - **API (OpenRouter) vs. Local:** The toolkit supports API-based generation for ease of use, but relies on external service stability.
  - **Assumption:** The "black-box" evaluation approach prioritizes output fairness over internal model explainability.

- **Failure signatures:**
  - **PyStata Initialization Error:** If Stata is not installed or licensed on the host machine, the backend analysis step will fail despite Python running.
  - **OpenRouter Rate Limits:** High-volume evaluation (177k cases) may trigger API throttling.
  - **Visualization mismatch:** If the LLM output format deviates from the expected structured JSON, the regression inputs will be malformed.

- **First 3 experiments:**
  1.  **Hello World Validation:** Select a single LLM (e.g., Gemini Flash) and run evaluation on a *small subset* (e.g., 50 cases) to verify the full pipeline (API -> Python -> Stata -> Vue Chart) completes without error.
  2.  **Metric Sensitivity Test:** Manually inspect a specific counterfactual pair (e.g., a case with a gender swap). Verify that if the model outputs different sentences, the "Inconsistency" metric actually increments in the report.
  3.  **Baseline Comparison:** Run the full evaluation on two contrasting models (e.g., a small local model vs. a frontier model) to calibrate the visualization dashboard and ensure the metrics differentiate model performance meaningfully.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the JustEva framework be adapted to support multilingual legal contexts and diverse legal systems (e.g., Common Law) beyond the current focus on Chinese judicial documents?
- Basis in paper: [explicit] The conclusion states: "Future work may... support multilingual legal contexts."
- Why unresolved: The current implementation relies exclusively on the JudiFair dataset, which is built upon Chinese judicial documents (LEEC), limiting the toolkit's applicability to other jurisdictions with different legal structures.
- What evidence would resolve it: A version of the toolkit and dataset adapted for English or multilingual jurisdictions (e.g., US or EU case law) that demonstrates consistent metric reliability across languages.

### Open Question 2
- Question: Can the integration of counterfactual explanation techniques into the JustEva workflow effectively identify the root causes of bias and guide the mitigation of unfair attributes?
- Basis in paper: [explicit] The conclusion proposes: "Future work may... incorporate counterfactual explanation techniques to better identify and mitigate the influence of unfair or sensitive attributes."
- Why unresolved: The current toolkit focuses on quantifying fairness deficiencies (detection) rather than diagnosing specific causal mechanisms or providing automated mitigation strategies.
- What evidence would resolve it: An extension of JustEva that includes explanation modules and empirical results showing a measurable reduction in inconsistency and bias metrics following mitigation.

### Open Question 3
- Question: Does JustEva's reliance on sentence length as the primary dependent variable limit the validity of its fairness assessment for legal tasks involving binary verdicts or civil damages?
- Basis in paper: [inferred] Section 3.2.2 states: "The dependent variable is sentence length in months... we log-transform the variable."
- Why unresolved: The statistical regression model is designed for continuous sentencing data; it is unclear if the "bias" and "imbalanced inaccuracy" metrics function correctly for categorical outcomes (e.g., guilty vs. not guilty) or non-criminal legal decisions.
- What evidence would resolve it: Validation of the bias metric using logistic or probit regression models on datasets containing binary judicial outcomes.

## Limitations
- The toolkit's validity depends on the quality and representativeness of the Chinese judicial dataset, which may not generalize to other legal systems
- High-dimensional fixed-effects regression assumes linear relationships that may not capture complex legal reasoning
- The Bernoulli test assumes independence between label detections, which may be violated when extra-legal factors are correlated

## Confidence
- **High:** The toolkit's architecture (Vue + Python + Stata + PyStata) is technically sound and the methodology for counterfactual evaluation is well-established in the literature
- **Medium:** The statistical inference methods (fixed-effects regression, Bernoulli test) are appropriate for the task, but their application to legal reasoning introduces domain-specific uncertainties
- **Low:** The generalizability of findings from the Chinese judicial dataset to other legal systems or LLM architectures is unknown without further validation

## Next Checks
1. **Dataset Validation:** Conduct a manual audit of 20 counterfactual pairs from JudiFair to verify that label swaps are truly isolated and do not introduce contextual inconsistencies that could bias the "Inconsistency" metric
2. **Statistical Robustness:** Test the sensitivity of the fixed-effects regression results to alternative model specifications (e.g., non-linear terms, different reference categories) to assess the stability of the "Bias" metric
3. **Generalizability Test:** Apply the toolkit to a small, independent dataset of legal cases from a different jurisdiction (e.g., US case law) to evaluate whether the identified biases persist across legal contexts