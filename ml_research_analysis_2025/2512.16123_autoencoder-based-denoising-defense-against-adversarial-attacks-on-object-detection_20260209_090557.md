---
ver: rpa2
title: Autoencoder-based Denoising Defense against Adversarial Attacks on Object Detection
arxiv_id: '2512.16123'
source_url: https://arxiv.org/abs/2512.16123
tags:
- adversarial
- detection
- object
- noise
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study proposes an autoencoder-based denoising defense against
  adversarial attacks on object detection models. The method uses a single-layer convolutional
  autoencoder to remove adversarial perturbations from images before detection with
  YOLOv5.
---

# Autoencoder-based Denoising Defense against Adversarial Attacks on Object Detection

## Quick Facts
- arXiv ID: 2512.16123
- Source URL: https://arxiv.org/abs/2512.16123
- Reference count: 12
- Primary result: Autoencoder-based denoising defense provides partial protection against Perlin noise attacks on YOLOv5 object detection, recovering 3.7% mAP at the cost of some localization precision

## Executive Summary
This study proposes an autoencoder-based denoising defense against adversarial attacks on object detection models. The method uses a single-layer convolutional autoencoder to remove adversarial perturbations from images before detection with YOLOv5. Experiments were conducted on vehicle images from the COCO dataset using Perlin noise-based adversarial attacks. Results show that adversarial attacks reduced bbox mAP from 0.2890 to 0.1640 (43.3% degradation). After applying the autoencoder defense, bbox mAP improved to 0.1700 (3.7% recovery) and bbox mAP@50 increased from 0.2780 to 0.3080 (10.8% improvement). The results demonstrate that autoencoder-based denoising can provide partial defense against adversarial attacks without requiring model retraining.

## Method Summary
The method employs a single-layer convolutional autoencoder that takes adversarially perturbed images as input and outputs denoised versions for subsequent YOLOv5 object detection. The autoencoder architecture consists of a 3×3 convolutional layer with 32 filters and ReLU activation, followed by 2×2 max pooling to compress the representation by half. The decoder mirrors this with 2×2 upsampling and a final 3×3 convolutional layer to reconstruct the original 3-channel image. The model is trained exclusively on clean vehicle images from COCO using MSE loss, then applied to Perlin noise-perturbed images at inference time without retraining the detector.

## Key Results
- Adversarial Perlin noise attacks reduced bbox mAP from 0.2890 to 0.1640 (43.3% degradation)
- Autoencoder denoising improved bbox mAP to 0.1700 (3.7% recovery from adversarial baseline)
- mAP@50 improved from 0.2780 to 0.3080 (10.8% increase) while mAP@75 decreased from 0.1680 to 0.1600

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Autoencoder projection toward the normal data manifold partially removes adversarial perturbations.
- Mechanism: Training exclusively on clean images teaches the autoencoder to reconstruct unperturbed image distributions, filtering inputs toward learned normal patterns. The authors liken this to MagNet's reformer concept.
- Core assumption: Adversarial perturbations lie outside or at the boundary of the normal data manifold and can be projected back toward it.
- Evidence anchors:
  - [abstract] States autoencoder-based denoising provides "partial defense" without model retraining.
  - [section 5] "This can be interpreted similarly to the reformer concept in MagNet... where the autoencoder projects adversarial examples toward the manifold of normal data."
  - [corpus] Weak direct corpus support; related papers focus on adversarial training rather than autoencoder-based denoising specifically.
- Break condition: If attackers craft perturbations that remain on the reconstructed manifold after denoising, or if white-box knowledge of the autoencoder enables adaptive attacks.

### Mechanism 2
- Claim: The compression-reconstruction bottleneck preferentially filters certain perturbation frequencies while preserving coarse image structure.
- Mechanism: The encoder's MaxPooling layer reduces spatial resolution by half, discarding high-frequency information. Upsampling reconstructs at lower effective resolution, which removes some adversarial noise but also fine-grained image details.
- Core assumption: Adversarial perturbations have frequency or spatial characteristics that differ from task-relevant image features.
- Evidence anchors:
  - [section 3.2] Describes the single-layer architecture with 2×2 MaxPooling and UpSampling.
  - [section 5] "When high-frequency information is lost during the compression-reconstruction process, accurate object boundary prediction becomes more difficult."
  - [corpus] No direct corpus evidence for this specific mechanism in object detection contexts.
- Break condition: If perturbations are designed to survive spatial downsampling (e.g., low-frequency attacks), filtering effectiveness decreases.

### Mechanism 3
- Claim: Training on clean images only enables noise-agnostic defense without requiring adversarial example generation.
- Mechanism: The autoencoder minimizes MSE between input and reconstruction for clean images, learning to output images matching normal statistics. At inference, deviations from learned patterns (including adversarial noise) are suppressed.
- Core assumption: Clean training data adequately represents the distribution of legitimate images, and adversarial perturbations create out-of-distribution artifacts.
- Evidence anchors:
  - [section 3.4] "The autoencoder is trained exclusively on clean images to learn the representation of unperturbed data."
  - [section 4.2] Results show 10.8% improvement in mAP@50 after denoising.
  - [corpus] PBCAT paper mentions adversarial training approaches but not clean-only autoencoder training.
- Break condition: If test images have distribution shift from training data (different vehicles, lighting, etc.), the autoencoder may incorrectly "correct" legitimate features.

## Foundational Learning

- Concept: **Autoencoder reconstruction loss (MSE)**
  - Why needed here: Understanding how pixel-level MSE training shapes what the model preserves versus discards during reconstruction.
  - Quick check question: Would a denoising autoencoder trained with MSE loss tend to produce blurry outputs? Why?

- Concept: **IoU thresholds in object detection (mAP@50 vs mAP@75)**
  - Why needed here: Results show improvement at mAP@50 but degradation at mAP@75, indicating different localization precision requirements.
  - Quick check question: Why would losing fine-grained details hurt mAP@75 more than mAP@50?

- Concept: **Perlin noise characteristics**
  - Why needed here: The attack uses Perlin noise rather than gradient-based methods; understanding its natural-looking, multi-scale properties clarifies what the autoencoder faces.
  - Quick check question: How might Perlin noise's limited frequency bandwidth differ from FGSM perturbations?

## Architecture Onboarding

- Component map:
  Input (400×400×3) -> Conv2D(3×3, 32 filters, ReLU) -> MaxPool(2×2) -> Latent (200×200×32) -> UpSample(2×2) -> Conv2D(3×3, 32 filters, ReLU) -> Conv2D(3×3, 3 filters, sigmoid) -> Output (400×400×3) -> YOLOv5

- Critical path: Clean training data quality → Autoencoder reconstruction fidelity → Degree of adversarial noise removal → Detection performance recovery

- Design tradeoffs:
  - **Deeper vs shallower autoencoder**: Deeper architectures (U-Net, ResNet) may better preserve details but increase inference cost and risk overfitting.
  - **Compression ratio**: Higher compression removes more noise but loses more task-relevant detail (evidenced by mAP@75 degradation).
  - **Training on clean-only vs adversarial examples**: Clean-only avoids attack-specific bias but may not generalize to unseen perturbation types.

- Failure signatures:
  - **mAP@75 decreasing**: Indicates loss of boundary precision during reconstruction.
  - **Low recovery rate (3.7%)**: Suggests single-layer architecture has insufficient representational capacity.
  - **No improvement on certain attack types**: Assumption—gradient-based attacks (FGSM, PGD) may not be filtered effectively (untested in paper).

- First 3 experiments:
  1. Replicate baseline: Train single-layer autoencoder on COCO vehicle subset (clean images only), evaluate mAP recovery on Perlin noise-perturbed validation set using YOLOv5.
  2. Ablation on architecture depth: Add a second encoder/decoder layer with skip connections (simplified U-Net), compare mAP@50 and mAP@75 to measure detail preservation vs noise removal tradeoff.
  3. Cross-attack evaluation: Apply FGSM and PGD perturbations to the same test set, evaluate whether clean-trained autoencoder generalizes or requires attack-specific training.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed autoencoder defense perform against standard gradient-based adversarial attacks such as FGSM, PGD, and C&W, rather than the Perlin noise evaluated in this study?
- Basis in paper: [explicit] The authors state in Section 5.1 that "this study evaluated only a specific Perlin noise attack configuration" and suggest that "additional evaluation on gradient-based attacks such as FGSM, PGD, and C&W might needed."
- Why unresolved: The current experimental scope is limited to procedural noise, leaving the defense's efficacy against optimization-based attacks designed to maximize model loss unknown.
- What evidence would resolve it: Benchmarks showing bbox mAP recovery rates on YOLOv5 when the autoencoder is applied to images perturbed by FGSM, PGD, or C&W attacks.

### Open Question 2
- Question: Does the defense remain effective in white-box scenarios where the attacker has full knowledge of the autoencoder's architecture and weights?
- Basis in paper: [explicit] Section 5.1 notes that "defense effectiveness in white-box scenarios where attackers are aware of the autoencoder’s presence was not addressed in this study."
- Why unresolved: The current defense assumes an oblivious attacker; in a real-world security context, an adversary could potentially adapt the perturbation to bypass the denoiser or poison the reconstruction.
- What evidence would resolve it: Results from adaptive attacks where the perturbation generation accounts for the autoencoder layer, measuring whether mAP still degrades significantly.

### Open Question 3
- Question: Can deeper architectures (e.g., U-Net) or high-level representation losses (e.g., HGD) prevent the loss of fine-grained details that caused the observed drop in mAP@75?
- Basis in paper: [explicit] The authors attribute the decrease in mAP@75 to the loss of high-frequency details and suggest in Section 5.2 that "deeper architectures such as U-Net" or "high-level representation-based loss functions" could be explored to address this.
- Why unresolved: The single-layer autoencoder with MSE loss successfully removed noise but struggled with precise localization, indicating a capacity or optimization limitation.
- What evidence would resolve it: Experiments comparing the proposed model against a U-Net or HGD-based denoiser, specifically focusing on bbox mAP@75 scores to verify improved localization preservation.

## Limitations
- Defense only tested against Perlin noise, not gradient-based attacks like FGSM or PGD
- 3.7% mAP recovery is modest and comes at the cost of decreased mAP@75 (0.1680→0.1600)
- Single-layer architecture may lack capacity to preserve fine localization details while removing noise

## Confidence
- **High confidence**: The autoencoder architecture specification and training procedure are clearly described and reproducible.
- **Medium confidence**: The mechanism of manifold projection is plausible but relies on assumptions about adversarial perturbation distribution that aren't empirically validated beyond Perlin noise.
- **Low confidence**: Generalization claims to other attack types (FGSM, PGD, real-world perturbations) are unsupported without additional experiments.

## Next Checks
1. Evaluate the same autoencoder defense against gradient-based attacks (FGSM, PGD) to assess robustness beyond Perlin noise.
2. Test cross-dataset generalization by applying the defense to non-COCO vehicle datasets with different lighting and environmental conditions.
3. Compare against adversarial training baselines where the autoencoder is trained on both clean and adversarially perturbed images to measure performance tradeoffs.