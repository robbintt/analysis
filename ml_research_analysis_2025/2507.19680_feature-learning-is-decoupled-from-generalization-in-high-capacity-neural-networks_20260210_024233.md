---
ver: rpa2
title: Feature learning is decoupled from generalization in high capacity neural networks
arxiv_id: '2507.19680'
source_url: https://arxiv.org/abs/2507.19680
tags:
- arxiv
- learning
- feature
- neural
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper argues that current theories of feature learning in
  neural networks are inadequate for explaining generalization. While existing metrics
  measure how much neural representations change during training (feature learning
  strength), they fail to capture the quality of these learned features and their
  impact on generalization.
---

# Feature learning is decoupled from generalization in high capacity neural networks

## Quick Facts
- **arXiv ID:** 2507.19680
- **Source URL:** https://arxiv.org/abs/2507.19680
- **Reference count:** 40
- **Primary result:** Current feature learning metrics measure representation changes rather than feature quality, failing to explain neural network generalization

## Executive Summary
This paper demonstrates that existing theories of feature learning in neural networks are inadequate for explaining generalization. Through experiments with CNNs on CIFAR-10 and MLPs on staircase functions, the authors show that metrics like NTK/CK changes and feature dimensionality do not reliably distinguish between networks trained on true versus shuffled labels, despite significant differences in generalization. The paper introduces the concept of "feature quality" measured by the FL gap, which quantifies how much better a network performs compared to its NTK counterpart. The key finding is that current feature learning theories measure representation changes rather than the usefulness of learned features, highlighting the need for new theoretical frameworks to understand neural network generalization.

## Method Summary
The authors train neural networks and their empirical NTK approximations on synthetic functions (MSP and multi-index) and CIFAR-10. They compare generalization performance between NN and NTK using the Feature Learning Gap (ΔNT), while measuring feature learning strength through NTK distance (SNT), Conjugate Kernel spectral changes (SCK), and feature dimensionality (Dfi). The experiments specifically test whether these strength metrics can distinguish between training on true labels versus randomly shuffled labels. They use μP (maximal update parameterization) for width-independent scaling and examine performance across different training set sizes to identify critical thresholds.

## Key Results
- Feature learning strength metrics (NTK distance, CK spectral changes, feature dimensionality) fail to distinguish between networks trained on true vs. shuffled labels
- The Feature Learning Gap ΔNT successfully quantifies the performance improvement of NNs over NTK approximations and correlates with generalization quality
- A critical dataset size m* exists where NNs begin outperforming kernel methods by orders of magnitude
- Current feature learning theories measure representation change magnitude rather than the utility of learned features for generalization

## Why This Works (Mechanism)

### Mechanism 1: Feature Learning Gap Quantifies Beyond-Kernel Performance
- **Claim:** The performance improvement of neural networks over their linearized approximations (NTK) measures the actual quality of learned features.
- **Mechanism:** The Feature Learning Gap ΔNT(m) = Egen(μNT; m) − Egen(fθ; m) captures how much better a trained NN generalizes compared to its kernel approximation. When ΔNT > 0, the NN has learned task-relevant features absent at initialization.
- **Core assumption:** NTK generalization reflects the quality of features present at initialization; deviations reflect meaningful feature adaptation.
- **Evidence anchors:**
  - [abstract] "We introduce a concept we call feature quality to measure this performance improvement."
  - [section 2.1] Figure 1 shows NNs outperform NTK by orders of magnitude beyond m* ∼ 10³ on MSP and multi-index functions.
  - [corpus] Limited direct validation; related work (Yang & Hu 2021) discusses infinite-width feature learning but doesn't explicitly validate ΔNT as a quality measure.
- **Break condition:** If the empirical NTK already aligns well with the target function (e.g., CIFAR-10 with CNNs), ΔNT will be small regardless of actual feature quality.

### Mechanism 2: FL Strength Metrics Measure Representation Change, Not Generalization Utility
- **Claim:** Existing feature learning definitions (NTK distance, CK spectral changes, superposition metrics) quantify how much representations change, not whether those changes improve generalization.
- **Mechanism:** Metrics like SNT(fθ) = 1 − κCKA(K̂NT(θ0), K̂NT(θt)) and SCK(fθ) respond similarly for networks trained on true vs. shuffled labels, indicating they capture representation change magnitude rather than utility.
- **Core assumption:** If a metric correlates with feature quality, it must distinguish generalizing from non-generalizing training runs.
- **Evidence anchors:**
  - [section 3] "S(fθ) fails to distinguish between shuffled and non-shuffled labels" for CNN on CIFAR-10.
  - [section 3.1] Scaling parameter γ = 0.01 eliminates differences in SNT between shuffled/non-shuffled without affecting generalization.
  - [corpus] "Feature Learning beyond the Lazy-Rich Dichotomy" (arXiv:2503.18114) discusses related regime distinctions but doesn't validate strength-quality decoupling.
- **Break condition:** If task-specific features are known a priori, superposition metrics may become more predictive; current evidence suggests otherwise.

### Mechanism 3: Critical Dataset Size Gates Feature Learning Benefits
- **Claim:** Neural networks only outperform kernel methods when the training set size m exceeds a critical threshold m*, which varies by task and architecture.
- **Mechanism:** Below m*, both NN and NTK have similar sample complexity. Above m*, NNs leverage feature learning for superior sample efficiency. The threshold depends on task complexity (MSP functions: m* ∼ 10³, CIFAR-10: m* ∼ 10⁴).
- **Core assumption:** The initial kernel's alignment with the target function determines pre-m* performance; feature learning mechanisms require sufficient data to activate.
- **Evidence anchors:**
  - [section 2.1] "We observe a critical training set size m* where NNs outperform their NTK counterparts by orders of magnitude."
  - [appendix D.8] Definition 16 formalizes m* as the dataset size where Egen(NN) < ε × Egen(NTK) for ε ≈ 0.1.
  - [corpus] "Precise gradient descent training dynamics" (arXiv:2505.04898) analyzes finite-width dynamics but doesn't address m* scaling.
- **Break condition:** Tasks where the initial NTK already has strong task alignment (e.g., low-frequency image classification) may never show significant ΔNT regardless of m.

## Foundational Learning

- **Concept: Neural Tangent Kernel (NTK)**
  - **Why needed here:** The entire framework compares NNs to their linearized (kernel) approximations; understanding NTK as the linearization of training dynamics is essential.
  - **Quick check question:** Can you explain why NTK predicts "lazy" training behavior in wide networks?

- **Concept: Kernel Methods and RKHS**
  - **Why needed here:** The paper uses kernel generalization theory (spectral bias, cumulative power distribution) as the baseline against which NN feature learning is measured.
  - **Quick check question:** How does eigenvalue decay in a kernel relate to its learning curve exponent?

- **Concept: Lazy vs. Rich Training Regimes**
  - **Why needed here:** Current FL theories categorize training into regimes based on representation change magnitude; understanding this dichotomy clarifies what the paper critiques.
  - **Quick check question:** What initialization scale causes a network to operate in the lazy regime?

## Architecture Onboarding

- **Component map:**
  - Baseline comparison: Empirical NTK computed via `neural-tangents` package
  - Strength metrics: CKA for NTK distance; eigenvalue decomposition for CK; feature/sample dimensionality for superposition
  - Quality metric: ΔNT computed from generalization errors on held-out test sets
  - Parameterization: μP (maximal update parameterization) for width-independent dynamics

- **Critical path:**
  1. Train NN with μP parameterization on target task
  2. Compute empirical NTK at initialization and end of training
  3. Evaluate both NN and NTK predictor on test set
  4. Calculate ΔNT = Egen(NTK) − Egen(NN)
  5. Compute strength metrics (SNT, SCK, Dfi) to verify decoupling

- **Design tradeoffs:**
  - μP vs. standard parameterization: μP removes width-dependent artifacts but may not match production settings
  - MSE vs. cross-entropy loss: Paper uses MSE for NTK comparability; production may differ
  - Shuffled labels test: Essential for validating metric quality but computationally redundant for deployment

- **Failure signatures:**
  - ΔNT ≈ 0 despite large representation changes → initial kernel already aligned with task
  - Strength metrics diverging from shuffled baseline → likely hyperparameter artifacts (check γ scaling)
  - Width-dependent m* → likely using standard parameterization instead of μP

- **First 3 experiments:**
  1. **Replicate shuffled vs. non-shuffled label experiment** on a simple dataset (e.g., MSP function) to verify that SNT/SCK metrics fail to distinguish generalization quality.
  2. **Measure ΔNT across dataset sizes** to identify m* for your target task; confirm NN outperforms NTK only beyond this threshold.
  3. **Test γ scaling sensitivity** by training with output scaling γ ∈ {1.0, 0.1, 0.01} to verify that strength metrics change while ΔNT remains stable.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop a theory of feature learning that integrates feature quality to explain neural network generalization, rather than relying on metrics of feature learning strength?
- Basis in paper: [explicit] The conclusion states that the demonstrated decoupling between feature strength and feature quality "suggests the need for a more comprehensive FL theory" to serve as a foundation for generalization theories.
- Why unresolved: Current theories (NTK-based, CK-based, superposition) measure the magnitude of representation change, which fails to distinguish between learning useful features and fitting noise.
- What evidence would resolve it: The formulation of a metric or theoretical framework that correlates strongly with generalization error (specifically the FL gap ΔNT) while remaining sensitive to data structure (e.g., distinguishing shuffled vs. true labels).

### Open Question 2
- Question: Can feature learning strength metrics (specifically NTK-based distances) be reformulated to be invariant to output scaling parameters like γ?
- Basis in paper: [explicit] The authors show that FL strength SNT is sensitive to the scaling parameter γ, such that setting γ=0.01 nullifies the distinction between training on shuffled and non-shuffled data without changing the generalization error.
- Why unresolved: This sensitivity suggests that current strength metrics capture artifacts of the training regime or scale rather than the intrinsic utility of the learned features.
- What evidence would resolve it: A modified metric that maintains a consistent distinction between generalizing and memorizing regimes across varying output scales and learning rates.

### Open Question 3
- Question: What are the precise training dynamics that drive representation changes (feature learning strength) in the absence of learnable structure, such as when fitting randomly shuffled labels?
- Basis in paper: [inferred] The paper demonstrates that metrics like Neural Collapse (CK utility) and superposition dimensionality increase even when training on random labels.
- Why unresolved: If generalization requires feature quality, the mechanisms forcing the network to alter its representations (strength) to fit pure noise are not explained by current theories which link representation change to task utility.
- What evidence would resolve it: A theoretical account or ablation study showing why gradient descent modifies hidden representations significantly even when the target function is random noise, potentially linking it to the "memorization" regime.

## Limitations
- The paper's findings rely heavily on synthetic experiments and a single CNN architecture on CIFAR-10, limiting generalizability to other architectures and real-world datasets
- The feature quality metric ΔNT assumes NTK generalization provides a meaningful baseline, which may not hold when the empirical NTK has strong task alignment
- Several strength metrics lack precise implementation specifications, creating potential reproducibility issues

## Confidence
- **High Confidence:** The decoupling between feature learning strength and quality is well-supported by controlled experiments showing identical strength metric values for true vs. shuffled labels while generalization differs dramatically
- **Medium Confidence:** The critical dataset size m* concept is empirically validated but may depend on specific architectural choices and task characteristics not fully explored
- **Low Confidence:** The generalization of findings to architectures beyond CNNs and FFNNs, and to datasets beyond CIFAR-10 and synthetic functions

## Next Checks
1. **Cross-Architecture Validation:** Test whether SNT and SCK metrics fail to distinguish feature quality across diverse architectures (Transformers, ResNets with different depths, Vision Transformers) on multiple datasets (ImageNet, SVHN, synthetic regression tasks)
2. **NTK Baseline Robustness:** Evaluate how ΔNT behaves when the empirical NTK has varying degrees of task alignment by testing on datasets with different frequency spectra and comparing against multiple kernel baselines
3. **Strength Metric Variants:** Implement and test alternative strength metrics (e.g., Fisher information-based measures, information bottleneck metrics) to determine whether the decoupling phenomenon is specific to the metrics examined or represents a broader principle