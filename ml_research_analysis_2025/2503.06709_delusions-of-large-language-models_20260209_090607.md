---
ver: rpa2
title: Delusions of Large Language Models
arxiv_id: '2503.06709'
source_url: https://arxiv.org/abs/2503.06709
tags:
- delusion
- delusions
- answer
- belief
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces the concept of LLM delusion, defined as\
  \ high-belief hallucinations\u2014incorrect outputs with abnormally high confidence.\
  \ Unlike ordinary hallucinations, delusions persist with low uncertainty, making\
  \ them harder to detect and mitigate."
---

# Delusions of Large Language Models

## Quick Facts
- arXiv ID: 2503.06709
- Source URL: https://arxiv.org/abs/2503.06709
- Reference count: 40
- This paper introduces the concept of LLM delusion, defined as high-belief hallucinations—incorrect outputs with abnormally high confidence.

## Executive Summary
This paper introduces the concept of LLM delusion, defined as high-belief hallucinations—incorrect outputs with abnormally high confidence. Unlike ordinary hallucinations, delusions persist with low uncertainty, making them harder to detect and mitigate. Through empirical analysis across multiple LLM families and sizes on Question-Answering tasks, the study shows that delusions are prevalent and distinct from hallucinations. Key findings include: LLMs exhibit lower honesty with delusions and are less willing to reject them; delusions are significantly harder to override through fine-tuning or self-reflection; and their formation is influenced by training dynamics and dataset noise. Mitigation strategies such as retrieval-augmented generation and multi-agent debating show promise in reducing delusions, but challenges remain. The study highlights the need for robust verification mechanisms and adaptive confidence calibration to improve model reliability and trustworthiness.

## Method Summary
The paper identifies LLM delusions through empirical analysis across multiple model families on knowledge-intensive question-answering tasks. Three uncertainty estimation methods are employed: logits-based token probabilities, sampling consistency across multiple generations, and verbalized confidence where models explicitly state certainty. A belief threshold is empirically set as the mean confidence on correct answers; incorrect outputs exceeding this threshold are classified as delusions. The study evaluates delusion characteristics through refusal experiments, self-reflection tests, and fine-tuning resistance. Mitigation strategies including multi-agent debating (voting-based verification) and retrieval-augmented generation are tested for effectiveness. Synthetic experiments on the ALCUNA dataset investigate delusion formation through controlled noise injection and training data similarity analysis.

## Key Results
- Delusions exhibit abnormally high confidence compared to ordinary hallucinations, making them harder to detect through uncertainty-based methods
- LLMs show lower honesty with delusions (refuse less often) and are less willing to reject delusional outputs compared to hallucinations
- Multi-agent voting (3/3 agreement) reduces delusion rates by 83-95% across models, while RAG reduces delusions by 55-72%
- Delusions are significantly harder to override through fine-tuning or self-reflection compared to hallucinations
- Training data noise characteristics, particularly noise intensity and similarity between question-answer pairs, influence delusion formation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Uncertainty estimation methods can operationalize "model belief" to distinguish delusions from ordinary hallucinations, but each method captures different aspects of internal conviction.
- **Mechanism:** The paper uses three complementary approaches—(1) logit-based token probabilities, (2) sampling consistency across multiple generations, and (3) verbalized confidence where models explicitly state certainty. A belief threshold is empirically set as the mean confidence on correct answers; incorrect outputs exceeding this threshold are classified as delusions. This creates a definitional boundary based on the observation that delusions exhibit "abnormally high confidence" relative to the model's own calibration baseline.
- **Core assumption:** Assumption: Uncertainty scores meaningfully proxy internal model "belief states" rather than artifacts of tokenization or training distribution.
- **Evidence anchors:**
  - [abstract]: "We identify a more insidious phenomenon, LLM delusion, defined as high belief hallucinations—incorrect outputs with abnormally high confidence."
  - [Section 3.1.1]: Details three uncertainty estimation methods as belief proxies.
  - [Section 3.1.2]: "The belief threshold is determined as the mean belief level of all correctly answered questions."
  - [corpus]: Related work "Mathematical Analysis of Hallucination Dynamics" proposes probabilistic frameworks for uncertainty quantification, suggesting alignment but not direct validation of this specific threshold method.
- **Break condition:** If verbalized confidence systematically diverges from logit-based measures—which Table 1 shows occurs (verbal methods exhibit higher delusion rates)—the unified "belief" construct may be unreliable across methods.

### Mechanism 2
- **Claim:** External verification via multi-agent debating and RAG reduces delusions by introducing independent knowledge sources that override high-confidence internal errors.
- **Mechanism:** Multi-agent voting requires consensus across models before accepting answers; RAG injects retrieved passages into generation context. Both methods circumvent the model's internal high-confidence state by anchoring outputs to external evidence. Table 3 shows voting with 3/3 agreement threshold reduces delusion rates by 83-95% across models; Table 4 shows RAG reduces delusions by 55-72%.
- **Core assumption:** Assumption: External sources (other models or retrieved documents) are more reliable than the delusional model's internal representations for the specific query.
- **Evidence anchors:**
  - [Section 6.1]: "Multi-agent voting effectively mitigates delusions... Mistral-7B achieved the largest reduction, from 14.6% to 1.3%."
  - [Section 6.2]: "RAG is an effective method for addressing both delusions and hallucinations."
  - [corpus]: "MedHallu" and "HalluMat" papers similarly propose multi-stage external verification for hallucination detection in specialized domains, but do not specifically address high-confidence delusions.
- **Break condition:** If retrieved documents contain conflicting information or voting models share common failure modes, external verification may reinforce rather than correct delusions.

### Mechanism 3
- **Claim:** Delusion formation correlates with training data noise characteristics—specifically noise proportion and error consistency—rather than solely model architecture.
- **Mechanism:** Synthetic experiments on ALCUNA dataset show: (1) Higher noise proportion increases delusion rates; (2) Concentrated/consistent errors ("high noise intensity") produce more delusions than dispersed errors; (3) Training interference from similar question-answer pairs creates conditions for delusion emergence. Removing high-similarity training samples (cosine similarity > 0.9) reduced normalized delusion ratio by 71.3% (Table 2).
- **Core assumption:** Assumption: Synthetic data perturbation patterns generalize to real-world training data quality issues.
- **Evidence anchors:**
  - [Section 5.1]: "As the proportion of noisy data increases, the delusion rate rises, while the accuracy of the model decreases."
  - [Section 5.2]: "When we refined the training set by removing question-answer pairs with high cosine similarity scores... the delusion rate significantly decreased."
  - [corpus]: Related papers focus on inference-time hallucination mitigation rather than training data causation; corpus evidence for this mechanism is weak.
- **Break condition:** If delusions arise from architectural or optimization dynamics independent of data quality—as Section 5.2 acknowledges ("delusions may arise in the model's default state, even before the introduction of noise")—then data-focused interventions will be insufficient.

## Foundational Learning

- **Concept:** Uncertainty quantification in neural networks (epistemic vs. aleatoric uncertainty)
  - **Why needed here:** The paper relies on uncertainty as a belief proxy but does not distinguish between uncertainty from model limitations vs. inherent task ambiguity.
  - **Quick check question:** Can you explain why a model might have low uncertainty on an incorrect answer, and which type of uncertainty this represents?

- **Concept:** Calibration of confidence estimates
  - **Why needed here:** The belief threshold mechanism assumes correct-answer confidence forms a meaningful baseline; understanding calibration helps assess whether this threshold is stable across distributions.
  - **Quick check question:** If a model is systematically overconfident, what happens to the delusion classification threshold?

- **Concept:** Retrieval-augmented generation architecture
  - **Why needed here:** Section 6.2 reports RAG reduces delusions; understanding retrieval-relevance ranking and fusion mechanisms helps diagnose when RAG might fail.
  - **Quick check question:** What failure modes exist when retrieved passages contradict each other or are marginally relevant?

## Architecture Onboarding

- **Component map:**
  - Belief estimation module: Three parallel methods (logits, consistency sampling, verbalized confidence) with optional ensemble combination
  - Threshold calibration: Dataset-specific belief threshold computed from correct-answer statistics
  - Classification layer: Incorrect outputs above threshold → delusion; below → hallucination
  - Mitigation pathways: (a) Multi-agent voting with configurable agreement threshold; (b) RAG with k retrieved passages

- **Critical path:** Belief estimation → Threshold computation → Delusion classification → External verification (if enabled)

- **Design tradeoffs:**
  - **Ensemble vs. single belief method:** Table 1 shows ensemble methods reduce but do not eliminate delusions; added computational cost for marginal improvement
  - **Voting threshold stringency:** Higher thresholds (3/3 agreement) maximize delusion reduction but sharply reduce answer coverage (accuracy drops from 69.3% to 31.1% for Llama-3.1-8B)
  - **RAG passage count:** Paper uses 20 passages; fewer passages reduce latency but may miss corrective evidence

- **Failure signatures:**
  - **Verbal confidence inflation:** Table 1 shows verbal methods yield 2-3x higher delusion ratios than logit-based methods, suggesting verbalized confidence is unreliable
  - **Persistent delusions post-mitigation:** Even with RAG or 3/3 voting, 1-4% delusion rates remain
  - **Self-reinforcement under reflection:** Figure 6 shows models "insist" on delusions more often than hallucinations when prompted to reconsider

- **First 3 experiments:**
  1. **Reproduce belief threshold calibration:** Compute logit-based and consistency-based belief thresholds on a held-out subset of TriviaQA; verify that delusion ratios match Table 1 ranges (±5%).
  2. **Ablate verbal confidence methods:** Isolate whether P(True), verb. 1S, or verb. 2S methods contribute most to inflated delusion rates; this identifies which verbal method to deprecate.
  3. **Stress-test RAG with adversarial retrieval:** Provide partially relevant or conflicting passages and measure whether delusion rates increase compared to clean RAG; this probes the assumption that external knowledge is inherently corrective.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can effective delusion detection and mitigation methods be developed for closed-source LLMs where token-level logits and internal states are inaccessible?
- Basis in paper: [explicit] The authors state "some uncertainty methods face challenges in accessing all necessary information (such as specific token logits) on closed-source models, and this paper does not explore delusion in such models."
- Why unresolved: Current belief estimation methods (logits-based, consistency-based) require access to model internals unavailable in commercial APIs.
- What evidence would resolve it: Novel uncertainty estimation techniques that work with black-box access only, validated through experiments on models like GPT-4 or Claude.

### Open Question 2
- Question: Do delusion characteristics and mitigation strategies transfer across task domains beyond knowledge-intensive question answering?
- Basis in paper: [explicit] The authors acknowledge "the experiments are primarily focused on knowledge-based question-answering datasets, leaving delusion in other tasks underexplored."
- Why unresolved: Different tasks (summarization, code generation, reasoning) may exhibit different delusion patterns, formation mechanisms, and susceptibility to mitigation.
- What evidence would resolve it: Systematic evaluation of delusion rates and mitigation effectiveness across diverse task types using consistent experimental protocols.

### Open Question 3
- Question: What are the precise mechanistic explanations for why delusions resist fine-tuning and self-reflection while hallucinations are more amenable?
- Basis in paper: [inferred] The paper demonstrates empirically that "delusions are significantly harder to override through fine-tuning" and "self-reflection mechanisms are ineffective at mitigating delusions," but the underlying mechanisms remain unexplained.
- Why unresolved: The paper links delusion formation to training dynamics and dataset noise but does not provide a causal mechanistic account of persistence.
- What evidence would resolve it: Probing studies examining internal representations of delusional vs. hallucinated outputs, or causal intervention experiments modifying specific model components.

### Open Question 4
- Question: How can the empirically determined belief threshold for distinguishing delusions from hallucinations be made more principled and generalizable?
- Basis in paper: [inferred] The authors "set this threshold empirically by analyzing the belief distribution of correctly answered questions," using the mean belief level, but acknowledge this is an intuitive choice without theoretical justification.
- Why unresolved: The threshold varies by dataset and model, and the current heuristic may not generalize or optimally separate the two phenomena.
- What evidence would resolve it: Theoretical analysis of optimal threshold selection or comparative evaluation of alternative thresholding strategies across diverse settings.

## Limitations

- The conceptual framing of "belief" as a unified internal state is undermined by the methodological divergence between verbalized confidence and probabilistic measures, particularly the inflation observed in verbal methods.
- The synthetic noise experiments provide causal evidence for data-quality impacts on delusion formation, but the generalization from controlled perturbations to real-world training dynamics remains untested.
- While mitigation strategies show promise, residual delusion rates (1-4%) persist even after RAG and multi-agent voting, raising questions about whether these represent fundamental architectural limitations.

## Confidence

- **High confidence**: The empirical observation that delusions exhibit higher confidence than ordinary hallucinations (across multiple models and datasets) and that external verification methods reduce delusion rates.
- **Medium confidence**: The claim that delusions arise from training data noise characteristics. While synthetic experiments show correlation, the mechanism's generalizability to real-world training data is not validated.
- **Low confidence**: The conceptual framing of "belief" as a unified internal state captured by multiple uncertainty methods. The methodological divergence between approaches (particularly verbal vs. probabilistic) undermines this theoretical foundation.

## Next Checks

1. **Cross-method belief consistency test**: Apply the delusion classification framework using only logit-based belief estimates (excluding verbal methods) and verify whether the same instances are classified as delusions. This isolates whether verbal confidence inflation drives the phenomenon.

2. **Real-data noise correlation analysis**: Identify actual training data quality issues (e.g., annotation errors, label noise) in model training corpora and correlate these with delusion rates on related test questions. This validates whether synthetic noise patterns generalize to real training dynamics.

3. **Residual delusion characterization**: Analyze the 1-4% delusion rates that persist after RAG and multi-agent voting. Determine whether these represent cases where external knowledge is unavailable, contradictory, or whether the model's internal representations are fundamentally misaligned with ground truth.