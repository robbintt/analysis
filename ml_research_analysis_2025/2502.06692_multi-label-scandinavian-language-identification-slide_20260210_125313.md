---
ver: rpa2
title: Multi-label Scandinavian Language Identification (SLIDE)
arxiv_id: '2502.06692'
source_url: https://arxiv.org/abs/2502.06692
tags:
- language
- languages
- dataset
- data
- multi-label
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper tackles the challenge of accurately identifying closely\
  \ related Scandinavian languages (Danish, Norwegian Bokm\xE5l, Norwegian Nynorsk,\
  \ and Swedish) at the sentence level, where traditional single-label approaches\
  \ often fail due to sentences being valid in multiple languages simultaneously.\
  \ The authors propose a multi-label language identification framework and contribute\
  \ three main elements: (1) the SLIDE dataset, a manually curated evaluation corpus\
  \ with gold multi-label annotations for 6,950 sentences, addressing the lack of\
  \ appropriate evaluation data for this task; (2) a suite of language identification\
  \ models ranging from fast static-embedding approaches to accurate transformer-based\
  \ methods, including SLIDE-base which achieves 96.4% exact-match accuracy; and (3)\
  \ a novel method for automatically creating multi-label training data by leveraging\
  \ machine translation models to detect multilingual acceptability."
---

# Multi-label Scandinavian Language Identification (SLIDE)

## Quick Facts
- arXiv ID: 2502.06692
- Source URL: https://arxiv.org/abs/2502.06692
- Authors: Mariia Fedorova; Jonas Sebulon Frydenberg; Victoria Handford; Victoria Ovedie Chruickshank Langø; Solveig Helene Willoch; Marthe Løken Midtgaard; Yves Scherrer; Petter Mæhlum; David Samuel
- Reference count: 10
- Primary result: SLIDE-base achieves 96.4% exact-match accuracy on multi-label Scandinavian LID

## Executive Summary
This paper addresses the challenge of accurately identifying closely related Scandinavian languages (Danish, Norwegian Bokmål, Norwegian Nynorsk, Swedish) where traditional single-label approaches fail due to sentences being valid in multiple languages simultaneously. The authors propose a multi-label framework and contribute three main elements: the SLIDE dataset with 6,950 manually curated sentences with gold multi-label annotations, a suite of language identification models ranging from fast static-embedding approaches to transformer-based methods, and a novel method for automatically creating multi-label training data using machine translation models to detect multilingual acceptability. Their evaluation demonstrates that multi-label approaches are necessary for accurate Scandinavian LID, as approximately 5% of sentences are valid in multiple languages.

## Method Summary
The authors tackle Scandinavian language identification as a multi-label classification problem using Binary Cross-Entropy loss with sigmoid activation. They train on silver-labeled data created by leveraging machine translation models to detect when sentences remain unchanged after translation, indicating multilingual validity. The pipeline involves: (1) downloading Universal Dependencies 2.14 treebanks for training data, (2) applying lowercasing and regex normalization, (3) using NorMistral-11b MT model to automatically label sentences with multiple languages by checking for translation invariance, (4) fine-tuning NorBERT3 (base/small/xs) with BCE loss using specified hyperparameters (LR=5e-5, Batch Size=64, 3 epochs), and (5) evaluating on the manually curated SLIDE test set of 6,950 sentences. The approach allows the model to output independent probabilities for each language label rather than forcing mutually exclusive choices.

## Key Results
- SLIDE-base achieves 96.4% exact-match accuracy, significantly outperforming existing single-label baselines
- Multi-label approaches are necessary, as ~5% of sentences are valid in multiple languages and ~16% of short sentences show multilingual validity
- SLIDE-fast offers a faster alternative with comparable accuracy to the best baseline model
- Danish F1 scores are lower (~92%) compared to other languages, reflecting confusion with Bokmål

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Framing closely related language identification as a multi-label classification task increases accuracy by accommodating sentences valid in multiple languages simultaneously.
- **Mechanism:** The system uses binary cross-entropy loss with sigmoid activation, allowing the model to output independent probabilities for each language label rather than forcing a mutually exclusive choice. This prevents penalties for correctly identifying multiple valid labels for a single input.
- **Core assumption:** A meaningful subset of sentences is orthographically and grammatically valid across multiple Scandinavian languages (e.g., Danish and Norwegian Bokmål).
- **Evidence anchors:**
  - [abstract] "We demonstrate that the ability to identify multiple languages simultaneously is necessary for any accurate LID method."
  - [section 1] Notes that sentences valid in multiple languages account for ~5% of the evaluation data and ~16% of short sentences.
  - [corpus] *DIVERS-Bench* supports the need for LID robustness in non-monolingual contexts, though not specific to Scandinavian.
- **Break condition:** If distinct languages share fewer than ~1-2% of ambiguous sentences, the complexity of multi-label classification may not yield significant gains over single-label approaches.

### Mechanism 2
- **Claim:** Machine Translation (MT) models can function as synthetic annotators to generate high-quality multi-label training data (silver-labeling).
- **Mechanism:** The authors translate sentences from a source language to a target language using a high-quality MT model (NorMistral-11b). If the translation output is identical to the input, the sentence is labeled as valid for the target language. This process iterates to expand label sets for each sentence.
- **Core assumption:** High-quality MT models will minimize changes when translating between highly similar languages, making translation invariance a reliable indicator of multilingual validity.
- **Evidence anchors:**
  - [method] The silver-labeling approach is described as leveraging NorMistral-11b's translation quality.
  - [results] The resulting models achieve high accuracy, suggesting the silver labels are of sufficient quality.
- **Break condition:** If the MT model introduces systematic biases or fails to preserve meaning across certain language pairs, the silver labels may be noisy or misleading.

## Foundational Learning

### Multi-label Classification
- **Why needed:** Traditional single-label LID forces mutually exclusive choices, incorrectly penalizing correct identification of multiple valid languages.
- **Quick check:** Model outputs independent probabilities for each language label that can be thresholded to produce multiple active labels per sentence.

### Binary Cross-Entropy Loss
- **Why needed:** BCE loss with sigmoid activation allows independent prediction of each language label, unlike softmax which forces a single choice.
- **Quick check:** Loss function treats each language as a separate binary classification problem rather than a single multi-class problem.

### Silver Labeling via Machine Translation
- **Why needed:** Lack of existing multi-label training data for Scandinavian languages requires synthetic generation method.
- **Quick check:** Translation invariance (input == output) reliably indicates multilingual validity between closely related languages.

## Architecture Onboarding

### Component Map
Universal Dependencies treebanks -> Lowercasing + Regex normalization -> NorMistral-11b MT translation -> Silver label generation -> NorBERT3 model -> BCE loss -> Multi-label predictions

### Critical Path
Data preparation (UD + normalization) -> Silver labeling (MT translation) -> Model training (NorBERT3 + BCE) -> Evaluation (SLIDE test set)

### Design Tradeoffs
- Multi-label vs single-label: Increased accuracy for ambiguous sentences vs increased model complexity
- Silver vs gold labels: Large training data volume vs potential MT model biases
- Transformer vs static embeddings: Higher accuracy vs computational efficiency

### Failure Signatures
- Alphabet overfitting: Model relies on character differences (å/ö vs æ/ø) rather than linguistic features
- Bokmål-Danish confusion: Model predicts single-label Bokmål for valid multi-label (Bokmål+Danish) instances
- Label imbalance: Danish and Nynorsk underrepresented due to smaller training data volumes

### First Experiments
1. Train baseline single-label model for comparison to quantify multi-label benefits
2. Evaluate silver-labeled training data quality by manually checking random samples
3. Test model on alphabet variation augmentation to detect overfitting to specific characters

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the multi-label LID framework effectively generalize to Insular Scandinavian languages (Faroese, Icelandic) and smaller varieties (Scanian, Elfdalian, Bornholmsk)?
- Basis in paper: [explicit] The authors explicitly list the exclusion of these languages and varieties as a limitation in the Conclusion.
- Why unresolved: The current SLIDE dataset and models were trained and evaluated exclusively on Mainland Scandinavian standards.
- What evidence would resolve it: Extending the dataset to include these varieties and evaluating the resulting model performance.

### Open Question 2
- Question: How does the SLIDE framework perform on non-standard inputs containing dialectal markers, diachronic variations, or social media text?
- Basis in paper: [explicit] The Limitations section states the study does not look at sources of variation found in dialects, historical texts, or social media.
- Why unresolved: The Universal Dependencies data used for training and evaluation primarily represents formal, standard written language.
- What evidence would resolve it: Evaluating the models on a multi-label test set composed of dialectal or informal web-text.

### Open Question 3
- Question: Is the machine-translation-based silver-labeling method effective for creating multi-label training data for other groups of closely related languages?
- Basis in paper: [inferred] While the authors present the MT-based labeling as a novel contribution, they only validate it within the Scandinavian context.
- Why unresolved: The method relies on the tendency of MT models to minimize changes, which may vary by language pair or model architecture.
- What evidence would resolve it: Applying the same training pipeline to other mutually intelligible language pairs (e.g., Czech/Slovak, Malay/Indonesian).

## Limitations

- The framework assumes that sentences valid in multiple languages constitute a meaningful proportion of real-world text (~5%), which may vary across domains and time periods
- The silver-labeling approach via machine translation introduces potential biases from the translation model's quality and training data
- The evaluation corpus, though manually curated, remains relatively modest at 6,950 sentences, limiting statistical power for rare language pairs

## Confidence

- **High Confidence**: The multi-label formulation itself and the demonstration that ~5% of sentences are valid in multiple languages (supported by empirical evaluation)
- **Medium Confidence**: The silver-labeling methodology using MT models, as it relies on assumptions about translation model quality without extensive ablation studies
- **Medium Confidence**: The claim that SLIDE-base achieves state-of-the-art performance, given that comparison is primarily against single-label baselines on a single test set

## Next Checks

1. **Cross-Domain Generalization**: Evaluate SLIDE models on non-UD data sources (news, social media, literature) to assess performance degradation outside training domain
2. **MT Model Sensitivity**: Re-run silver-labeling with alternative translation models (e.g., Google Translate, GPT-4) to quantify impact on final model accuracy and identify systematic labeling biases
3. **Rare Language Pair Analysis**: Conduct detailed error analysis on Danish-Swedish and Nynorsk-Bokmål pairs where performance may be weakest due to linguistic proximity and label imbalance