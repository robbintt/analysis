---
ver: rpa2
title: 'HalluVerse25: Fine-grained Multilingual Benchmark Dataset for LLM Hallucinations'
arxiv_id: '2503.07833'
source_url: https://arxiv.org/abs/2503.07833
tags:
- sentence
- edited
- hallucination
- error
- original
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'HalluVerse25 is a fine-grained multilingual dataset for LLM hallucination
  detection in English, Arabic, and Turkish, containing 1310, 828, and 978 sentence
  pairs respectively. The dataset categorizes hallucinations into entity-level, relation-level,
  and sentence-level errors, with hallucinations injected by an LLM into factual biographical
  sentences and validated through human annotation (Cohen''s Kappa: 0.748-0.805).'
---

# HalluVerse25: Fine-grained Multilingual Benchmark Dataset for LLM Hallucinations

## Quick Facts
- **arXiv ID**: 2503.07833
- **Source URL**: https://arxiv.org/abs/2503.07833
- **Reference count**: 13
- **Primary result**: Novel multilingual hallucination dataset with entity, relation, and sentence-level error categorization across English, Arabic, and Turkish

## Executive Summary
HalluVerse25 introduces a fine-grained multilingual benchmark dataset for evaluating large language model (LLM) hallucination detection capabilities across English, Arabic, and Turkish. The dataset contains 1310 English, 828 Arabic, and 978 Turkish sentence pairs where factual biographical sentences have been systematically perturbed by LLMs to introduce hallucinations. Hallucinations are categorized into three granular levels: entity-level (incorrect facts about entities), relation-level (incorrect relationships between entities), and sentence-level (entirely fabricated information). The dataset provides a comprehensive resource for assessing LLM performance in detecting different types of hallucinations across multiple languages.

The study employs a systematic approach where factual biographical sentences from Wikipedia are used as sources, and LLMs are prompted to generate hallucinated variants that are then validated through human annotation. Cohen's Kappa inter-annotator agreement scores range from 0.748 to 0.805, indicating substantial agreement in hallucination classification. Experimental results show that GPT-4o consistently achieves the highest accuracy across all languages and hallucination types, with phi-4 demonstrating competitive performance particularly for entity and relation hallucinations. Models generally perform worst on sentence-level hallucinations, often misclassifying them as entity errors, highlighting the challenge of detecting more complex hallucination types.

## Method Summary
The HalluVerse25 dataset is constructed through a multi-stage process beginning with extraction of factual biographical sentences from Wikipedia articles. These source sentences serve as the foundation for generating hallucinated variants through LLM prompting, where models are instructed to modify facts while maintaining grammatical structure and semantic plausibility. The generated pairs are then subjected to human annotation where multiple annotators independently classify each pair as containing no hallucination, entity hallucination, relation hallucination, or sentence hallucination. Cohen's Kappa scores are calculated to ensure inter-annotator reliability, with values ranging from 0.748 to 0.805 across languages and categories. The final dataset balances precision and recall in hallucination detection while maintaining linguistic diversity across the three target languages.

Evaluation of multiple LLMs including GPT-4o, phi-4, and others is conducted using a binary classification framework where models must determine whether a given sentence pair contains any hallucination. Performance metrics including accuracy, precision, recall, and F1-score are calculated for each language and hallucination type. The experimental design allows for direct comparison of model capabilities across languages and hallucination categories, revealing systematic differences in model performance based on hallucination type complexity and language characteristics.

## Key Results
- GPT-4o achieves highest accuracy across all languages and hallucination types, establishing it as the top-performing model for hallucination detection
- phi-4 demonstrates competitive performance particularly for entity and relation hallucinations, suggesting strong capabilities in detecting factual errors
- Models consistently perform worst on sentence-level hallucinations, frequently misclassifying them as entity errors, indicating difficulty with more complex hallucination detection

## Why This Works (Mechanism)
The dataset's effectiveness stems from its systematic approach to hallucination generation and categorization, creating controlled conditions for evaluating model performance. By using factual biographical sentences as sources and employing LLMs to generate controlled perturbations, the methodology ensures consistent hallucination types while maintaining linguistic naturalness. The fine-grained categorization into entity, relation, and sentence-level hallucinations allows for precise analysis of model capabilities and limitations. The multilingual aspect provides cross-linguistic insights into hallucination detection performance, revealing how language characteristics influence model effectiveness.

## Foundational Learning
- **Cohen's Kappa**: Measures inter-annotator agreement, needed to validate annotation consistency and dataset quality; quick check: values above 0.7 indicate substantial agreement
- **Fine-grained hallucination categorization**: Distinguishes between entity, relation, and sentence-level errors, needed to understand model performance on different hallucination complexities; quick check: manual verification of categorization scheme across samples
- **Multilingual evaluation**: Assesses model performance across languages with different linguistic structures, needed to understand cross-linguistic generalization; quick check: statistical comparison of performance across language pairs
- **Binary vs multi-class classification**: Trade-off between simplified evaluation and nuanced error type identification, needed to balance practical utility with diagnostic precision; quick check: ablation studies comparing classification approaches
- **Wikipedia-based factual grounding**: Ensures reliable source material for hallucination generation, needed to maintain dataset credibility; quick check: verification of source sentence accuracy

## Architecture Onboarding
- **Component map**: Wikipedia sentences -> LLM hallucination injection -> Human annotation validation -> Dataset compilation -> LLM evaluation
- **Critical path**: Factual source generation → Controlled hallucination injection → Annotation and validation → Model evaluation
- **Design tradeoffs**: Balanced precision in hallucination detection against computational cost of human annotation; controlled generation methodology versus ecological validity of hallucinations
- **Failure signatures**: Systematic confusion between entity and sentence-level hallucinations; performance degradation on relation-level errors; language-specific weaknesses
- **First experiments**: 1) Ablation study varying LLM hallucination injection parameters; 2) Cross-domain evaluation on non-biographical text types; 3) Multi-class classification comparison versus binary approach

## Open Questions the Paper Calls Out
None

## Limitations
- Substantial sample size imbalance across languages (1310 English vs 828 Arabic vs 978 Turkish) may limit cross-linguistic performance comparisons
- Hallucination injection methodology relies on LLM-generated perturbations, potentially introducing systematic biases that may not reflect human-like error patterns
- Focus on biographical sentences from Wikipedia creates domain-specific corpus that may not generalize to other text types or domains

## Confidence
- **High confidence**: Dataset creation methodology, hallucination categorization framework, and basic experimental results showing GPT-4o's superior performance
- **Medium confidence**: Cross-linguistic performance comparisons due to sample size imbalance, generalizability of results beyond biographical text
- **Low confidence**: Claims about model performance differences in real-world applications, impact of hallucination injection methodology on ecological validity

## Next Checks
1. Conduct ablation studies testing different LLM hallucination injection strategies to assess robustness to generation methodology
2. Expand evaluation to additional text domains (news, technical documentation, creative writing) to test domain generalization
3. Implement a multi-class classification framework that distinguishes between entity, relation, and sentence-level hallucinations in model outputs, rather than binary classification