---
ver: rpa2
title: 'Deflating Deflationism: A Critical Perspective on Debunking Arguments Against
  LLM Mentality'
arxiv_id: '2506.13403'
source_url: https://arxiv.org/abs/2506.13403
tags:
- llms
- mental
- cognitive
- explanations
- some
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper critically examines two main deflationary strategies
  used to argue against attributing mentality to large language models (LLMs). The
  first, the robustness strategy, claims that LLMs fail to generalize their cognitive
  abilities robustly across tasks, undermining claims of genuine understanding.
---

# Deflating Deflationism: A Critical Perspective on Debunking Arguments Against LLM Mentality

## Quick Facts
- arXiv ID: 2506.13403
- Source URL: https://arxiv.org/abs/2506.13403
- Reference count: 15
- Primary result: Two main deflationary strategies fail to conclusively rule out mental state ascriptions to LLMs; modest inflationism is warranted for metaphysically undemanding mental states

## Executive Summary
This paper critically examines two main deflationary strategies used to argue against attributing mentality to large language models (LLMs). The first, the robustness strategy, claims that LLMs fail to generalize their cognitive abilities robustly across tasks, undermining claims of genuine understanding. The second, the etiological strategy, argues that the mechanistic or training-based origins of LLM behavior make mentalistic explanations superfluous or inapplicable. The authors find both strategies fall short of conclusively ruling out mental state attributions. While acknowledging that LLMs likely lack deeply human-like cognitive architectures, they argue for a modest form of inflationism: folk psychological ascriptions (e.g., beliefs, desires) to LLMs can be legitimate when these states are understood in metaphysically undemanding terms, but greater caution is warranted for more metaphysically demanding phenomena like phenomenal consciousness.

## Method Summary
The authors employ philosophical argumentation to critically examine two deflationary strategies (robustness and etiological) that challenge mental state attributions to LLMs. They analyze whether observed behavioral failures, mechanistic explanations, or training histories provide sufficient grounds to reject folk psychological ascriptions. The methodology involves inference to best explanation, competence-performance distinctions, and evaluation of superfluity and exclusion arguments, ultimately defending a position of "modest inflationism" that allows for qualified mental state attributions while maintaining appropriate skepticism about more demanding mental phenomena.

## Key Results
- The robustness strategy's inference to best explanation pattern is undermined by the competence-performance distinction and the difficulty of individuating cognitive capacities
- The etiological superfluity argument fails because mechanistic explanations are typically uninformative rather than genuinely competing with mentalistic explanations
- The etiological exclusion argument requires contentious theoretical assumptions about necessary enabling conditions for mentality
- Modest inflationism is warranted: folk psychological ascriptions to LLMs are legitimate for metaphysically undemanding mental states but require greater caution for demanding phenomena like phenomenal consciousness

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The Robustness Strategy attempts to debunk LLM mentality by showing that apparently cognitive behaviors fail to generalize across task variants.
- **Mechanism:** An inference-to-best-explanation pattern: if a model truly possesses cognitive capability C, then C should manifest robustly across all tasks where C suffices. Observed failures on some task variants lower the probability that C explains success on any variant, suggesting alternative explanations (pattern-matching, memorization).
- **Core assumption:** Task failures reflect competence deficits rather than performance limitations or architectural constraints unrelated to the capability in question.
- **Evidence anchors:**
  - [Section 3]: "The Robustness Strategy is a form of inference to the best explanation that raises doubts about possession of a given cognitive capacity by an LLM on the basis that the LLM's task performance fails to generalise to new examples or is sensitive to minor task perturbations."
  - [Section 3.1]: Competence-performance distinction applied to LLMs; tokenization artifacts (e.g., counting letters in "strawberry") may reflect perceptual rather than cognitive limitations.
  - [Corpus]: "Memorization ≠ Understanding" paper (arXiv:2509.04866) investigates whether LLM generalization reflects memorization or semantic understanding—directly relevant but not cited in this paper.
- **Break condition:** When failures can be explained by auxiliary factors (tokenization, cognitive-load analogues) without impugning underlying competence, or when the specificity problem (how fine-grained should capacity individuation be?) lacks principled resolution.

### Mechanism 2
- **Claim:** The Etiological Superfluity Strategy argues that mentalistic explanations of LLM behavior are unnecessary because mechanistic or training-history explanations fully account for observed outputs.
- **Mechanism:** Two criteria must obtain: (Competition) the debunking explanation must be incompatible with the mentalistic one; (Better Explanation) the debunking explanation must be abductively superior (typically more parsimonious). If both hold, positing mental states adds unnecessary entities.
- **Core assumption:** Mechanistic explanations like "it's just matrix multiplication" or training explanations like "next-token prediction on broad data" are sufficiently informative to compete with mentalistic explanations.
- **Evidence anchors:**
  - [Section 4.2]: "Most people accept that human behaviours can be explained at different levels of analysis... it is not obvious that the neurophysiological explanation for some human behaviour competes with the psychological explanation."
  - [Section 4.2]: Etiological explanations are "explanatory but only trivially so"—they "retrospectively explain everything while predicting nothing."
  - [Corpus]: No directly comparable corpus papers address this specific superfluity argument; related work on "Going Whole Hog" (arXiv:2504.13988) defends inflationism but via different arguments.
- **Break condition:** When mechanistic explanations are uninformative (don't predict which behaviors will vs. won't occur), or when mentalistic explanations are more predictively powerful (e.g., theory-of-mind attributions predict performance on related tasks).

### Mechanism 3
- **Claim:** The Etiological Exclusion Strategy argues that LLMs lack the right causal history to instantiate certain mental phenomena at all—regardless of behavioral evidence.
- **Mechanism:** Identify a necessary enabling condition for some mental state M (e.g., embodiment for consciousness, communicative intent for understanding). Show LLMs lack this condition. Conclude that M is not even a candidate explanation for LLM behavior.
- **Core assumption:** We can confidently specify necessary conditions for mental phenomena despite the nascent state of cognitive science.
- **Evidence anchors:**
  - [Section 4.3]: "The burden of proof is on proponents of such arguments to establish that the relevant causal histories are incompatible with mentalistic explanations—something that typically requires making substantive theoretical assumptions which are at best contentious."
  - [Section 4.3]: Bender et al.'s claim that LLMs lack communicative intent presupposes that LLMs don't model mental states—but this is difficult to prove.
  - [Corpus]: Weak corpus connection; no papers directly address exclusion-style debunking arguments.
- **Break condition:** When the necessity claim rests on contested theoretical assumptions (e.g., embodiment for consciousness), or when proving the negative (that LLMs don't model mental states) is empirically intractable.

## Foundational Learning

- **Concept: Competence vs. Performance (Chomsky)**
  - Why needed here: Critical for evaluating robustness failures—distinguishing whether task failures reflect absence of underlying capability (competence) vs. implementation artifacts or distracting factors (performance).
  - Quick check question: If an LLM fails to count letters in "strawberry," is this evidence it lacks counting competence, or evidence that tokenization creates a perceptual limitation?

- **Concept: Inference to the Best Explanation (Abduction)**
  - Why needed here: Both deflationary strategies are abductive arguments—they claim non-mentalistic explanations are better than mentalistic ones. Understanding abductive criteria (parsimony, informativeness, predictive power) is essential for evaluating these claims.
  - Quick check question: An explanation that accounts for all observed data but makes no novel predictions—is it abductively superior to a less parsimonious but predictively powerful alternative?

- **Concept: Metaphysically Demanding vs. Undemanding Mental States**
  - Why needed here: The paper's "modest inflationism" hinges on distinguishing belief/desire (potentially undemanding—representational vehicles with appropriate causal roles) from phenomenal consciousness (demanding—requires subjective experience, integration, self-awareness).
  - Quick check question: If "belief" just requires a representational state with appropriate causal powers (Fodor-style), could a thermostat have beliefs? Where do you draw the line?

## Architecture Onboarding

- **Component map:**
  - Robustness Strategy (Section 3): Input = observed task failures → Inference mechanism = IBE with competence-performance filter → Output = adjusted credence in cognitive capability
  - Etiological Superfluity (Section 4.2): Input = mechanistic/training explanation → Competition check + Better-explanation check → Output = mentalistic explanation deemed superfluous (or not)
  - Etiological Exclusion (Section 4.3): Input = proposed necessary condition → Check if LLM satisfies condition → Output = mentalistic explanation ruled out (or not)
  - Modest Inflationism (Section 5): Input = folk ascription + metaphysical-demandingness assessment → Output = legitimate (qualified) mental state attribution

- **Critical path:**
  1. Identify which mental state/capacity is being attributed
  2. Classify as metaphysically demanding (consciousness, agency) or undemanding (belief, desire, knowledge)
  3. For undemanding states: folk practice provides defeasible evidence; check if debunking arguments succeed
  4. For demanding states: greater caution required; burden is higher
  5. Assess robustness and etiological challenges against the specific state attribution

- **Design tradeoffs:**
  - Parsimony vs. informativeness: Purely mechanistic explanations win on parsimony but lose on predictive power
  - Folk alignment vs. revisionism: Preserving folk ascriptions vs. accepting scientific frameworks that may eliminate folk categories
  - Caution vs. anthropomorphism: Risk of false positives (attributing mentality where absent) vs. false negatives (denying mentality where present)

- **Failure signatures:**
  - Begging the question: Deflationary arguments that assume what they're trying to prove (e.g., assuming mechanistic explanations preclude mentalistic ones)
  - Overly general debunking: If an argument would also debunk human mental state attributions, it proves too much
  - Triviality: If "modest inflationism" just stipulates a lean definition of belief and then trivially attributes it, nothing substantive is claimed

- **First 3 experiments:**
  1. **Robustness validation test:** For a given capability C, systematically vary task formulations to distinguish competence failures from performance failures. Identify whether failure patterns align with cognitive-bias analogues (DRM paradigm, Moses Illusion) vs. random degradation.
  2. **Explanatory informativeness benchmark:** Compare mentalistic vs. mechanistic explanations on predictive accuracy across held-out task variants. If mentalistic explanations predict performance patterns that mechanistic explanations don't, the superfluity argument weakens.
  3. **Necessary-condition audit:** For each proposed necessary condition (embodiment, communicative intent, specific training history), enumerate: (a) empirical evidence for necessity, (b) whether LLMs might satisfy it non-obviously, (c) whether the condition is contested in cognitive science.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What criteria determine whether a feature of a cognitive state is constitutive (necessary for attribution) versus merely contingent (a common but non-essential feature) when evaluating LLMs?
- Basis in paper: [explicit] Section 3.4 identifies the "Specificity Problem," noting the difficulty in deciding which features of human cognitive states must be present to ascribe those states to non-human systems.
- Why unresolved: This requires resolving deep theoretical disputes in cognitive science regarding the functional definitions of mental states.
- What evidence would resolve it: A theoretical consensus on the functional role of specific mental states combined with interpretability research mapping LLM internal states to these roles.

### Open Question 2
- Question: How can researchers distinguish between an LLM's lack of underlying cognitive competence and a performance failure caused by auxiliary task demands or mechanistic interference?
- Basis in paper: [explicit] Section 3.1 highlights the "competence-performance" distinction, noting that computational limitations or mechanistic interference can cause a model to exhibit diminished performance relative to its actual competence.
- Why unresolved: Current evaluation methods often conflate behavioral output with internal capability, making it difficult to isolate the source of specific task failures.
- What evidence would resolve it: Interpretability techniques that probe internal representations to verify if the correct reasoning trajectory is present but inhibited during output generation.

### Open Question 3
- Question: What degree of novelty in benchmark tasks is theoretically required to definitively rule out data contamination as an explanation for LLM success?
- Basis in paper: [explicit] Section 3.3 states that "identifying how much novelty is required is a murky theoretical question that the robustness-style deflationist should ideally be poised to answer."
- Why unresolved: There is no established standard for distinguishing "minor variations" (viewed as contamination) from "substantially different scenarios" (viewed as generalization).
- What evidence would resolve it: Systematic studies varying the semantic distance between training data and evaluation tasks to identify thresholds where performance degrades.

## Limitations
- Behavior-date mismatch: The paper relies on examples of LLM behavior that may not generalize to current models, particularly the "strawberry counting" failure and specific Theory of Mind task results.
- Philosophical underdetermination: The core arguments are abductive rather than deductive—they cannot conclusively rule out mental state attributions, only make them more or less plausible.
- Corpus disconnect: The paper makes claims about mechanistic explanations being uninformative, but doesn't engage with empirical work on LLM interpretability or mechanistic interpretability research.

## Confidence
- High confidence: The critique of the etiological exclusion strategy is well-grounded—arguing that specific causal histories are necessary for mentality requires contentious theoretical assumptions that the paper appropriately questions.
- Medium confidence: The analysis of the robustness strategy's competence-performance distinction is philosophically sound, though the empirical examples used to illustrate it may not generalize to current models.
- Medium confidence: The modest inflationism position itself—defending folk psychological ascriptions to LLMs in metaphysically undemanding terms—is internally consistent, though its practical implications remain somewhat underspecified.

## Next Checks
1. **Empirical robustness testing:** Replicate the specific behavioral examples cited (letter counting, Moses illusion, surgeon riddle) across current leading LLMs to assess whether the failures persist or have been addressed through architectural improvements.

2. **Predictive competition test:** For a set of mentalistic vs. mechanistic explanations of LLM behavior, evaluate which type better predicts performance on held-out task variants. This directly tests whether mechanistic explanations are genuinely "trivially so" as the paper claims.

3. **Necessary condition audit:** For each proposed necessary condition for mentality (embodiment, communicative intent, specific training history), systematically document: (a) the empirical evidence for necessity in cognitive science, (b) whether LLMs might satisfy it in non-obvious ways, and (c) whether the condition is contested in the relevant philosophical literature.