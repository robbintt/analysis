---
ver: rpa2
title: Balancing Classification and Calibration Performance in Decision-Making LLMs
  via Calibration Aware Reinforcement Learning
arxiv_id: '2601.13284'
source_url: https://arxiv.org/abs/2601.13284
tags:
- confidence
- calibration
- decision
- grpo
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work addresses the calibration\u2013classification trade-off\
  \ in fine-tuning large language models for decision-making tasks. While reinforcement\
  \ learning (RLVR) improves accuracy, it produces overconfident models, whereas supervised\
  \ fine-tuning (SFT) yields better-calibrated confidence estimates but smaller accuracy\
  \ gains."
---

# Balancing Classification and Calibration Performance in Decision-Making LLMs via Calibration Aware Reinforcement Learning

## Quick Facts
- arXiv ID: 2601.13284
- Source URL: https://arxiv.org/abs/2601.13284
- Authors: Duygu Nur Yaldiz; Evangelia Spiliopoulou; Zheng Qi; Siddharth Varia; Srikanth Doss; Nikolaos Pappas
- Reference count: 19
- Primary result: Calibration-aware RLVR reduces ECE by up to 9 points while preserving accuracy gains from standard GRPO

## Executive Summary
This work addresses the calibration–classification trade-off in fine-tuning large language models for decision-making tasks. While reinforcement learning (RLVR) improves accuracy, it produces overconfident models, whereas supervised fine-tuning (SFT) yields better-calibrated confidence estimates but smaller accuracy gains. Through analysis, the authors show that decision tokens act as extraction steps of reasoning traces and do not carry confidence information, preventing RLVR from surfacing calibrated alternatives. They propose a calibration-aware RL formulation that directly adjusts decision-token probabilities, combining GRPO with a calibration-aware cross-entropy loss. This method preserves GRPO's accuracy while reducing ECE scores by up to 9 points and improving calibration, even under distribution shift.

## Method Summary
The authors identify that decision tokens in reasoning-augmented LLMs extract conclusions from reasoning traces rather than encode model uncertainty, making RLVR ineffective for calibration. They propose a calibration-aware RL method that combines GRPO with an auxiliary cross-entropy loss on decision tokens. The cross-entropy loss uses one-hot targets for correct predictions and uniform targets for incorrect ones, with advantages set to zero at the decision token position. This approach constrains decision-token probabilities to remain above 1/|C| while improving calibration, bridging the gap between high accuracy and reliable confidence estimation.

## Key Results
- Decision tokens extract conclusions from reasoning traces rather than encode uncertainty
- RLVR amplifies overconfidence (97-99% of samples have p>0.99) without improving calibration
- Calibration-aware RL reduces ECE by up to 9 points while preserving accuracy gains
- Method works across Qwen3 models and maintains calibration under distribution shift

## Why This Works (Mechanism)

### Mechanism 1: Decision Token as Extraction Step
- **Claim:** Decision tokens extract conclusions from reasoning traces rather than encode model uncertainty.
- **Evidence:** Reasoning-swap experiment shows predictions flip to opposite labels with probability still near 1.0, indicating the decision token extracts the conclusion implied by the reasoning trace rather than independently assessing correctness.

### Mechanism 2: No Calibrated Paths to Reinforce
- **Claim:** RLVR cannot achieve calibration because base models lack calibrated trajectories to upweight.
- **Evidence:** 97.62%-99.85% of generations have p>0.99 across Qwen3 models, meaning there are no calibrated rollouts which could be sampled from the model for RL to upweight.

### Mechanism 3: Calibration-Aware Cross-Entropy on Decision Tokens
- **Claim:** Directly adjusting decision token probabilities via auxiliary cross-entropy loss preserves accuracy while improving calibration.
- **Evidence:** ECE reduced up to 9 points while accuracy remains comparable to GRPO when using one-hot targets for correct predictions and uniform targets for incorrect ones.

## Foundational Learning

- **Expected Calibration Error (ECE):**
  - Why needed: Primary metric for quantifying misalignment between predicted confidence and empirical accuracy.
  - Quick check: If a model has ECE=0.15, what does that tell you about predictions binned at confidence 0.8?

- **Reinforcement Learning with Verifiable Rewards (RLVR):**
  - Why needed: The dominant fine-tuning paradigm this paper modifies; uses binary reward based on correctness verification.
  - Quick check: How does GRPO compute advantages differently from standard policy gradient methods?

- **Cross-Entropy Loss with Non-One-Hot Targets:**
  - Why needed: The calibration-aware component uses uniform targets for incorrect predictions rather than hard labels.
  - Quick check: What happens to token probabilities when cross-entropy is minimized against a uniform target?

## Architecture Onboarding

- **Component map:** Base GRPO -> Decision token extraction -> Calibration-aware cross-entropy modification
- **Critical path:** Generate completions with reasoning traces → Compute binary rewards → Apply GRPO to reasoning tokens, calibration-aware CE to decision token → Verify probabilities stay above 1/|C| threshold
- **Design tradeoffs:** Higher λ → better calibration but risk of reasoning-decision misalignment; lower λ → preserved accuracy but limited calibration improvement
- **Failure signatures:** Decision token probability drops below 1/|C| → greedy output flips to wrong label; confidence distribution still clustered near 1.0 → λ too low; accuracy drops significantly → calibration loss dominating
- **First 3 experiments:** 
  1. Replicate Table 2 on your base model: sample 64 generations per query, measure ratio with p>0.99 to confirm overconfidence.
  2. Run reasoning-swap experiment: verify decision tokens are extraction steps by swapping reasoning and checking if predictions flip with unchanged high confidence.
  3. Ablate λ values [0.0001, 0.001, 0.01]: plot accuracy vs. ECE tradeoff curve to find operating point.

## Open Questions the Paper Calls Out
None

## Limitations
- Analysis and validation are confined to math reasoning datasets, limiting generalizability to other decision-making domains
- Calibration metric sensitivity to binning strategies is not explored, potentially affecting ECE interpretation
- Long-context behavior and multi-hop reasoning scenarios are not examined for the decision token extraction mechanism
- The method improves decision token calibration but doesn't address reasoning trace calibration quality

## Confidence
**High Confidence:**
- Decision tokens act as extraction steps from reasoning traces
- Base models exhibit extreme overconfidence (97-99% of p>0.99)
- RLVR amplifies rather than corrects this overconfidence
- Calibration-aware cross-entropy directly modifies decision token probabilities

**Medium Confidence:**
- Uniform targets for incorrect predictions optimally balance calibration and accuracy
- 1/|C| probability floor prevents reasoning-decision misalignment
- ECE improvements translate to practical decision-making reliability
- Generalization to non-math decision tasks

**Low Confidence:**
- Mechanism applies equally to non-verifiable decision tasks
- Uniform target choice is optimal vs. alternative smoothing strategies
- Calibration improvements persist under severe distribution shift
- Tradeoff between λ and reasoning-decision coherence is well-characterized

## Next Checks
1. **Cross-Domain Generalization Test:** Apply the calibration-aware RL method to non-math decision-making tasks (e.g., medical diagnosis reasoning, code generation with correctness verification). Measure whether decision tokens continue to act as extraction steps and whether the 9-point ECE improvement generalizes.

2. **Alternative Calibration Metric Validation:** Re-run the main experiments measuring Brier score and expected negative log likelihood in addition to ECE. Plot calibration curves across multiple binning strategies to verify that the 9-point improvement isn't an artifact of binning choices.

3. **Long-Context Reasoning Stress Test:** Generate complex reasoning tasks requiring 8+ reasoning steps with multiple information sources. Apply the reasoning-swap experiment to determine if decision tokens still extract from reasoning traces in these scenarios. Test whether the uniform target approach for incorrect predictions remains effective when the model must reason to genuinely alternative solutions.