---
ver: rpa2
title: 'FLStore: Efficient Federated Learning Storage for non-training workloads'
arxiv_id: '2503.00323'
source_url: https://arxiv.org/abs/2503.00323
tags:
- flstore
- data
- non-training
- workloads
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FLStore is a serverless caching framework that unifies the data
  and compute planes for efficient storage and execution of non-training workloads
  in federated learning. It addresses the high latency and cost challenges of existing
  FL frameworks by leveraging tailored caching policies optimized for the iterative
  and predictable data access patterns of non-training tasks.
---

# FLStore: Efficient Federated Learning Storage for non-training workloads

## Quick Facts
- arXiv ID: 2503.00323
- Source URL: https://arxiv.org/abs/2503.00323
- Reference count: 40
- Primary result: Reduces per-request average latency by 50.8-71% and costs by 88.2-98.83% compared to state-of-the-art FL frameworks

## Executive Summary
FLStore is a serverless caching framework designed to address the high latency and cost challenges of federated learning (FL) frameworks when handling non-training workloads. By unifying the data and compute planes through tailored caching policies optimized for the iterative and predictable data access patterns of FL, FLStore achieves significant performance improvements. The framework leverages serverless architecture to provide scalable, fault-tolerant storage and execution capabilities that can be integrated into existing FL systems with minimal modifications.

## Method Summary
FLStore employs a serverless caching framework that unifies data and compute planes specifically for non-training workloads in federated learning. The approach leverages tailored caching policies that are optimized for the iterative and predictable data access patterns characteristic of FL tasks. By utilizing serverless architecture, FLStore aims to reduce both latency and operational costs compared to traditional cloud object stores or in-memory caches used in existing FL frameworks.

## Key Results
- Reduces per-request average latency by 50.8-71% compared to state-of-the-art FL frameworks
- Achieves cost reduction of 88.2-98.83% compared to existing solutions
- Demonstrates scalability and fault-tolerance capabilities suitable for production FL deployments

## Why This Works (Mechanism)
FLStore works by unifying the data and compute planes in federated learning systems, addressing the inherent inefficiencies in how existing FL frameworks handle non-training workloads. The key mechanism involves implementing tailored caching policies that exploit the predictable, iterative access patterns typical of FL operations. By leveraging serverless architecture, FLStore can dynamically scale resources based on workload demands while maintaining low latency through intelligent data placement and access optimization. This architectural unification eliminates the overhead associated with separate data and compute management layers, resulting in the substantial performance improvements observed.

## Foundational Learning

**Serverless Architecture**: Understanding serverless computing is crucial as FLStore builds upon this paradigm to achieve dynamic scaling and resource optimization. Quick check: Verify serverless platforms support the required execution environment for FL workloads.

**Federated Learning Workloads**: Knowledge of FL's iterative and predictable data access patterns is essential to appreciate why FLStore's caching policies are effective. Quick check: Map typical FL workflow data access patterns to validate caching strategy assumptions.

**Caching Policies**: Familiarity with different caching strategies (LRU, LFU, etc.) helps understand how FLStore's tailored policies differ from generic approaches. Quick check: Compare cache hit rates between FLStore's policies and traditional methods.

**Data-Compute Unification**: Understanding the separation of data and compute planes in traditional systems highlights FLStore's architectural innovation. Quick check: Identify performance bottlenecks in systems with separate data and compute layers.

## Architecture Onboarding

**Component Map**: Client Devices -> FLStore Serverless Layer -> Caching Layer -> Storage Backend -> Compute Resources

**Critical Path**: Request reception at serverless layer → Cache lookup → Data retrieval (cache or storage) → Compute resource allocation → Result processing → Response delivery

**Design Tradeoffs**: Serverless architecture provides scalability and cost benefits but introduces cold start latency; caching policies must balance between cache size and hit rate; unification of data and compute planes reduces overhead but requires careful resource management.

**Failure Signatures**: Cold start delays in serverless functions; cache eviction of frequently accessed data; network partition between serverless layer and storage backend; compute resource exhaustion during peak loads.

**First Experiments**: 
1. Measure cold start latency impact on end-to-end request processing time
2. Benchmark cache hit rate under different caching policies with synthetic FL workloads
3. Stress test serverless layer scalability under varying request volumes

## Open Questions the Paper Calls Out
None

## Limitations
- Limited quantitative validation of scalability and fault-tolerance claims
- Insufficient exploration of trade-offs between different caching policies
- Lack of detailed analysis on integration complexity with various FL frameworks

## Confidence

**High confidence**: Core technical contribution of unifying data and compute planes for non-training workloads

**Medium confidence**: Claimed performance improvements in latency and cost reduction

**Low confidence**: Scalability and fault-tolerance capabilities without quantitative evidence

## Next Checks

1. Conduct comprehensive evaluation of FLStore's performance under varying workloads and cluster sizes to validate scalability claims

2. Perform detailed cost-benefit analysis comparing FLStore's resource utilization against traditional FL frameworks under different caching policies

3. Implement case study demonstrating FLStore's integration with at least two different existing FL frameworks to assess practical compatibility and modification requirements