---
ver: rpa2
title: Efficient Multimodal Planning Agent for Visual Question-Answering
arxiv_id: '2601.20676'
source_url: https://arxiv.org/abs/2601.20676
tags:
- query
- agent
- image
- gold
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a multimodal planning agent that dynamically
  optimizes retrieval-augmented generation pipelines for visual question-answering
  tasks. The agent learns to intelligently determine which retrieval steps are necessary
  for different types of VQA queries, rather than following rigid multi-stage workflows.
---

# Efficient Multimodal Planning Agent for Visual Question-Answering

## Quick Facts
- arXiv ID: 2601.20676
- Source URL: https://arxiv.org/abs/2601.20676
- Reference count: 40
- Key outcome: Agent achieves 60% reduction in search time and 3-4.5x latency improvement for VQA tasks

## Executive Summary
This paper introduces a multimodal planning agent that optimizes retrieval-augmented generation pipelines for visual question-answering tasks. The agent learns to dynamically determine which retrieval steps are necessary for different types of VQA queries, rather than following rigid multi-stage workflows. Through automated data annotation and LoRA fine-tuning, the agent selectively executes only essential pipeline components based on query characteristics, achieving significant efficiency gains while maintaining or improving task performance across six diverse VQA datasets.

## Method Summary
The method involves three main phases: First, automated data annotation generates training labels by decomposing queries into original, image, and gold variants, then classifying them into one of four categories based on model success/failure patterns. Second, the base MLLM is fine-tuned using LoRA (rank 32) to predict the optimal retrieval strategy. Third, at inference time, the trained agent predicts which retrieval path to execute (none, image-only, text-only, or both) before passing results to the task model. The approach uses Qwen2.5-VL-7B as the base model with a 72B model for query rewriting and annotation tasks.

## Key Results
- Achieves over 60% reduction in search time compared to baseline methods
- Decreases latency by 3-4.5x compared to Deep Research agents
- Improves average performance across six diverse VQA datasets while maintaining efficiency

## Why This Works (Mechanism)

### Mechanism 1
The fine-tuned planning agent reduces latency by mapping queries to discrete, minimal retrieval paths rather than executing rigid, full pipelines. The agent classifies visual queries into four categories corresponding to "No Retrieval," "Text Retrieval," "Image Retrieval," or "Both," bypassing unnecessary tools like skipping a 6.4s image search for text-only queries.

### Mechanism 2
Automated data annotation isolates the "missing link" in a query by contrasting performance on original, image-query, and gold-query variants. This decomposition helps determine whether a model needs visual grounding versus factual knowledge retrieval.

### Mechanism 3
LoRA fine-tuning creates a specialized planner without destroying the model's general VQA capabilities by constraining weight updates to a low-rank subspace, allowing the model to learn planning tasks without overwriting pre-trained visual-linguistic knowledge.

## Foundational Learning

- **Multimodal Retrieval-Augmented Generation (mRAG)**: Understanding the baseline mRAG cost is essential to appreciate efficiency gains. Quick check: What are the standard steps in a rigid mRAG pipeline before answering a query?

- **Visual Query Decomposition**: The data generation relies on breaking questions like "Who made this car?" into image and gold queries. Quick check: How does rewriting a query to include the visual entity help isolate whether a model needs image retrieval or text retrieval?

- **LoRA (Low-Rank Adaptation)**: The paper chooses LoRA over full fine-tuning to prevent catastrophic forgetting of base VQA skills. Quick check: Why would full fine-tuning cause the model to fail at answering VQA questions even if it successfully predicts the retrieval category?

## Architecture Onboarding

- **Component map**: Planner Agent -> Toolset (Image Retrieval, Text Retrieval, Query Rewriter) -> Task Model
- **Critical path**: Data Prep (annotate queries) → Training (apply LoRA) → Inference (agent predicts category → trigger retrieval → task model answers)
- **Design tradeoffs**: Planner vs. Solver - LoRA-planner can also be solver, saving overhead; Prompting vs. Training - training is efficient but requires data annotation
- **Failure signatures**: Lazy Retrieval (under-retrieval), Catastrophic Interference (full fine-tuning degrades VQA), Annotation Noise (weak annotator produces incorrect labels)
- **First 3 experiments**: 1) Validate 50-100 automated annotations, 2) Replicate latency comparison benchmarks, 3) Test planner with stronger task model (e.g., GPT-4o) for modularity

## Open Questions the Paper Calls Out

- Why does full parameter fine-tuning cause the agent to lose general VQA capabilities while LoRA preserves them, and can this "over-alignment" be mitigated?

- How robust is the single-pass planning agent to retrieval failures compared to adaptive, multi-round agents?

- How does the exclusion of logical contradiction cases during data annotation affect the agent's performance on ambiguous or "trick" questions?

## Limitations

- Automated annotation quality is not independently validated, creating uncertainty about label accuracy
- Efficiency gains depend on specific retrieval API latencies that may vary across implementations
- Experiments only compare against baseline retrieval methods, not more recent planning approaches

## Confidence

- **High confidence**: LoRA fine-tuning mechanism preventing catastrophic forgetting
- **Medium confidence**: 60% reduction in search time and 3-4.5x latency improvements
- **Low confidence**: Automated data annotation producing clean category labels

## Next Checks

1. Manually verify 100 randomly sampled annotations from the automated pipeline to measure accuracy of the "missing link" classification logic

2. Replicate efficiency benchmarks using different retrieval API providers and configurations to test whether reported latency improvements generalize

3. Evaluate the planner agent on datasets not seen during training to assess whether learned retrieval patterns transfer to novel VQA domains