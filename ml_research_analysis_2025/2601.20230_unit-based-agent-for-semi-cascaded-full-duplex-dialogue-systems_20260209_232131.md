---
ver: rpa2
title: Unit-Based Agent for Semi-Cascaded Full-Duplex Dialogue Systems
arxiv_id: '2601.20230'
source_url: https://arxiv.org/abs/2601.20230
tags:
- dialogue
- full-duplex
- system
- arxiv
- state
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a unit-based framework for full-duplex dialogue
  systems that decomposes dialogue into minimal conversational units, with each unit
  containing listen and speak states. The system uses a multimodal large language
  model (MLLM) to predict when to transition between states using continue/switch
  actions, enabling the system to process each unit independently.
---

# Unit-Based Agent for Semi-Cascaded Full-Duplex Dialogue Systems

## Quick Facts
- arXiv ID: 2601.20230
- Source URL: https://arxiv.org/abs/2601.20230
- Reference count: 0
- The system achieves 89.7% first response delay and ranks second in the Full-Duplex Interaction track on HumDial dataset

## Executive Summary
This paper proposes a unit-based framework for full-duplex dialogue systems that decomposes dialogue into minimal conversational units, each containing listen and speak states. The system uses a multimodal large language model (MLLM) to predict state transitions using continue/switch actions, enabling independent processing of each unit. The semi-cascaded design uses ASR transcripts only as contextual augmentation rather than full cascade, reducing response latency while maintaining dialogue state tracking. Implemented with open-source components including VAD, ASR, MLLM (Qwen3-Omni), and TTS, the system achieves state-of-the-art performance on the HumDial dataset with promising latency metrics.

## Method Summary
The framework introduces a unit-based agent that treats each minimal conversational unit as an independent processing entity with distinct listen and speak states. The system employs a semi-cascaded architecture where automatic speech recognition (ASR) provides contextual augmentation rather than full cascade processing, reducing latency. A multimodal large language model (MLLM) serves as the core decision-maker, predicting when to transition between continue and switch actions based on dialogue context. The system is implemented using open-source components including voice activity detection (VAD), ASR, the Qwen3-Omni MLLM, and text-to-speech (TTS) synthesis. This train-free approach allows for plug-and-play integration with existing speech components while achieving state-of-the-art semantic and interaction-state inference performance.

## Key Results
- Achieved 89.7% first response delay and 50.0% interruption total score on HumDial dev set
- Total delay of 1.698s on dev set, improving to 1.632s on test set
- Ranked second in Full-Duplex Interaction track on HumDial dataset
- Improved performance on test set with 57.8% interruption total score

## Why This Works (Mechanism)
The unit-based approach works by decomposing complex dialogue into manageable conversational units, each with clearly defined listen and speak states. This decomposition allows the MLLM to focus on predicting transitions between states rather than processing entire dialogue contexts simultaneously. The semi-cascaded design reduces latency by using ASR only for contextual augmentation rather than full cascade, while the MLLM's multimodal capabilities enable accurate state transition predictions. By treating each unit independently, the system can process overlapping speech more effectively and maintain robust dialogue state tracking even in challenging conversational scenarios.

## Foundational Learning
- Voice Activity Detection (VAD): Detects speech presence to delineate conversational units
  - Why needed: Enables accurate segmentation of dialogue into minimal units for processing
  - Quick check: Verify VAD can distinguish speech from non-speech in various acoustic conditions
- Multimodal Large Language Models (MLLMs): Process both audio and text to predict state transitions
  - Why needed: Enables the system to understand conversational context and predict continue/switch actions
  - Quick check: Test MLLM accuracy on action prediction across different conversational scenarios
- Semi-Cascaded Architecture: Uses ASR for contextual augmentation rather than full cascade
  - Why needed: Reduces response latency while maintaining sufficient context for dialogue management
  - Quick check: Compare performance with full cascade versus semi-cascade configurations

## Architecture Onboarding
Component map: VAD -> ASR -> MLLM -> TTS
Critical path: Speech input → VAD segmentation → ASR transcription → MLLM state prediction → TTS output
Design tradeoffs: Semi-cascaded design reduces latency but may sacrifice some context completeness; unit-based decomposition improves processing efficiency but requires accurate state transition predictions
Failure signatures: Incorrect state transitions leading to inappropriate responses; VAD missegmentation causing unit boundary errors; MLLM prediction errors resulting in dialogue flow disruption
First experiments:
1. Test VAD accuracy in segmenting conversational units under various acoustic conditions
2. Evaluate MLLM's ability to predict continue/switch actions across different dialogue scenarios
3. Measure response latency improvements when using semi-cascade versus full cascade configuration

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Semi-cascaded design may struggle with highly overlapping speech where context windows are incomplete
- MLLM-based state transitions assume reliable prediction accuracy without detailed validation of edge cases
- Performance comparisons lack direct benchmarking against other full-duplex systems on identical metrics

## Confidence
- System architecture and decomposition approach: **High** - The unit-based framework with listen/speak states is clearly defined and logically structured
- Performance metrics and ranking: **Medium** - Results are reported but lack comparative context against other full-duplex systems
- MLLM-based state transition reliability: **Low** - The paper assumes MLLM accuracy without providing detailed validation of action prediction performance

## Next Checks
1. Conduct ablation studies comparing the unit-based system with and without MLLM-based state transitions to quantify the contribution of the MLLM component to overall performance
2. Test the system's robustness to overlapping speech scenarios with varying degrees of speech overlap to evaluate whether semi-cascaded design maintains accuracy in challenging conditions
3. Implement the same unit-based framework using different MLLM variants to assess whether performance depends critically on the specific model choice or generalizes across architectures