---
ver: rpa2
title: 'Energy Use of AI Inference: Efficiency Pathways and Test-Time Compute'
arxiv_id: '2509.20241'
source_url: https://arxiv.org/abs/2509.20241
tags:
- energy
- arxiv
- inference
- query
- queries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a bottom-up Monte Carlo simulation to estimate
  the per-query energy consumption of large-scale LLM inference systems, addressing
  the challenge of inconsistent and overestimated public energy figures. The method
  combines node-level power parameters, utilization factors, PUE distributions, and
  empirically fitted token throughput under realistic serving conditions, modeling
  both traditional and test-time scaling query regimes.
---

# Energy Use of AI Inference: Efficiency Pathways and Test-Time Compute

## Quick Facts
- arXiv ID: 2509.20241
- Source URL: https://arxiv.org/abs/2509.20241
- Reference count: 0
- Primary result: Frontier-scale LLM inference consumes 0.34 Wh per query (IQR: 0.18-0.67) under realistic workloads

## Executive Summary
This paper presents a bottom-up Monte Carlo simulation to estimate the per-query energy consumption of large-scale LLM inference systems, addressing the challenge of inconsistent and overestimated public energy figures. The method combines node-level power parameters, utilization factors, PUE distributions, and empirically fitted token throughput under realistic serving conditions, modeling both traditional and test-time scaling query regimes. For frontier-scale models (>200B parameters) under realistic workloads, the estimated median energy per query is 0.34 Wh, consistent with production-optimized deployments and 4-20× lower than non-production estimates. The study quantifies achievable efficiency gains through model improvements (1.5-10×), serving platform optimizations (1.5-5×), and hardware/datacenter advances (1.5-2.5×), with combined interventions potentially delivering 8-20× reductions.

## Method Summary
The paper develops a bottom-up Monte Carlo simulation framework that estimates per-query energy consumption by combining node-level power parameters, utilization factors, PUE distributions, and empirically fitted token throughput under realistic serving conditions. The model accounts for both traditional inference and test-time scaling query regimes, incorporating power consumption at node level (idle and peak power), utilization distribution, PUE distribution, and model/token parameters. The simulation runs 10,000 iterations to generate probabilistic energy estimates, validated against real-world deployments. The approach provides a more accurate assessment than top-down estimates by modeling the actual serving infrastructure and workload characteristics.

## Key Results
- Frontier-scale LLM inference consumes 0.34 Wh per query (IQR: 0.18-0.67) under realistic workloads
- Test-time scaling with 15× more tokens per query increases median energy 13× to 4.32 Wh
- Combined efficiency interventions could reduce energy consumption by 8-20×
- Extrapolating to 1 billion queries per day, baseline energy use is 0.8 GWh/day, comparable to web search energy at scale

## Why This Works (Mechanism)
The simulation accurately captures energy consumption by modeling the full serving stack from hardware utilization to facility power delivery. The bottom-up approach accounts for real-world operational factors including GPU utilization variability, infrastructure efficiency, and workload characteristics that top-down estimates miss. By fitting token throughput to realistic serving conditions rather than idealized benchmarks, the model reflects actual deployment performance. The Monte Carlo framework properly propagates uncertainty through all parameters, providing confidence intervals rather than point estimates.

## Foundational Learning
- **GPU utilization distribution**: Models how often GPUs operate at different load levels during inference - needed to accurately capture real-world power draw vs. peak theoretical consumption
- **PUE (Power Usage Effectiveness)**: Measures facility power delivery efficiency (total facility power / IT equipment power) - needed to account for cooling, networking, and other non-compute power overhead
- **Token throughput**: Rate at which tokens are processed per second under realistic serving conditions - needed to translate query patterns into compute time and energy
- **Power state modeling**: Differentiates between idle and peak power consumption states - needed to capture energy waste during periods of low utilization
- **Monte Carlo simulation**: Uses repeated random sampling to estimate probabilistic outcomes - needed to quantify uncertainty across all input parameters
- **Test-time scaling**: Generates additional tokens during inference for complex queries - needed to model emerging inference patterns that significantly impact energy consumption

## Architecture Onboarding

**Component Map**: Query Request -> Request Router -> GPU Cluster -> Memory/Storage -> Power Distribution -> Cooling System -> Facility Grid

**Critical Path**: User query → request routing → GPU inference processing → token generation → response delivery

**Design Tradeoffs**: The model balances accuracy against complexity by focusing on key parameters that drive energy consumption while maintaining tractability. It prioritizes production-relevant metrics over theoretical maximums.

**Failure Signatures**: Overestimated energy consumption when assuming 100% GPU utilization; underestimated when ignoring PUE overhead or using benchmark token throughput instead of production values.

**3 First Experiments**:
1. Measure actual GPU utilization distributions across different workload patterns in production inference clusters
2. Compare energy consumption between traditional and test-time scaling implementations using identical model architectures
3. Validate PUE distribution assumptions by measuring facility-wide power consumption in geographically distributed inference-serving data centers

## Open Questions the Paper Calls Out
None

## Limitations
- GPU utilization assumption of 50% may not capture edge cases of highly variable workloads or aggressive batching strategies
- Efficiency improvement estimates are based on literature ranges rather than direct measurement
- Extrapolation assumes uniform workload distribution and does not account for temporal clustering effects

## Confidence
- **High confidence**: Core claim of 0.34 Wh per query for frontier-scale models under realistic workloads, validated against real-world deployments
- **High confidence**: Test-time scaling results showing 13× energy increases with 15× more tokens per query
- **Medium confidence**: Efficiency improvement estimates (1.5-10× for model improvements, 1.5-5× for serving optimizations) based on literature ranges

## Next Checks
1. Validate the 50% GPU utilization assumption against production telemetry data from diverse workload patterns, including bursty traffic scenarios
2. Measure actual PUE values across a geographically distributed sample of inference-serving facilities to refine the distribution bounds
3. Conduct controlled experiments comparing energy consumption across traditional and test-time scaling implementations using identical model architectures and hardware configurations