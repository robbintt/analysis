---
ver: rpa2
title: 'More Bias, Less Bias: BiasPrompting for Enhanced Multiple-Choice Question
  Answering'
arxiv_id: '2511.20086'
source_url: https://arxiv.org/abs/2511.20086
tags:
- arxiv
- biasprompting
- answer
- reasoning
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving large language
  models' performance on multiple-choice question answering tasks, particularly when
  answer choices are presented without contextual grounding, which can lead to incomplete
  exploration and degraded reasoning capabilities. The authors introduce BiasPrompting,
  a novel inference framework that guides LLMs to generate and critically evaluate
  reasoning for each answer option before reaching a final prediction.
---

# More Bias, Less Bias: BiasPrompting for Enhanced Multiple-Choice Question Answering

## Quick Facts
- arXiv ID: 2511.20086
- Source URL: https://arxiv.org/abs/2511.20086
- Reference count: 40
- Primary result: BiasPrompting achieves 66.0% accuracy on CommonsenseQA vs 65.1% for zero-shot, while reducing selection bias and using fewer tokens than CoT

## Executive Summary
This paper addresses the challenge of improving large language models' performance on multiple-choice question answering tasks, particularly when answer choices are presented without contextual grounding. The authors introduce BiasPrompting, a novel inference framework that guides LLMs to generate and critically evaluate reasoning for each answer option before reaching a final prediction. Through comprehensive evaluations across five widely used benchmarks, BiasPrompting demonstrates significant improvements compared to existing methods while being more computationally efficient than chain-of-thought approaches.

## Method Summary
BiasPrompting is a two-stage inference framework for multiple-choice question answering. In the first stage (Reasoning Generation), the model generates supportive reasoning for each answer option as if it were correct. In the second stage (Reasonings-Guided Agreement), these generated reasonings are synthesized to select the most plausible answer. The method operates sequentially within a single LLM, avoiding the computational overhead of multi-agent or multi-round approaches. The framework uses standardized prompt templates to generate reasoning for each option and construct a consensus prompt that includes all question options and their corresponding reasonings.

## Key Results
- BiasPrompting achieves 66.0% accuracy on CommonsenseQA compared to 65.1% for zero-shot prompting
- Reduces selection bias, showing lower variance across random answer orderings
- Consistently generates fewer tokens than CoT while maintaining or improving accuracy
- Demonstrates consistent performance gains across different models (Mistral-7B, DeepSeek-7B, Gemma-7B) and datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Generating supportive reasoning for each answer option forces comprehensive exploration, preventing premature dismissal of alternatives
- Mechanism: Standard prompting allows models to latch onto a single option without meaningfully evaluating others. By requiring the model to "argue for" each option as if correct, the model retrieves a broader range of relevant knowledge before selection
- Core assumption: Models possess relevant knowledge but fail to retrieve it when options lack context; forced argumentation activates latent knowledge
- Evidence anchors:
  - [abstract] "answer choices are typically presented to LLMs without contextual grounding or explanation. This absence of context can lead to incomplete exploration of all possible answers"
  - [section 2] "BiasPrompting instructs the model to produce reasoning that supports A_i as if it were the correct answer, regardless of its actual validity... preventing premature dismissal of alternatives"
  - [corpus] Related work (arxiv:2510.07761) confirms LLMs can solve MCQs without using the question content, suggesting shallow evaluation patterns that forced exploration may address
- Break condition: If models generate near-identical, templated reasonings for all options without genuine differentiation, the exploration benefit diminishes

### Mechanism 2
- Claim: Synthesizing all generated reasonings in a single context enables comparative evaluation that reduces selection bias
- Mechanism: Presenting all reasonings together in the Reasonings-Guided Agreement stage allows the model to weigh relative plausibility rather than relying on positional heuristics
- Core assumption: Models can perform reliable comparative judgment when evidence is explicitly laid out; the consensus prompt structure guides attention appropriately
- Evidence anchors:
  - [abstract] "BiasPrompting reduces selection bias, offers greater stability across random answer orderings"
  - [section 3.4.5] "In all cases, BiasPrompting yields higher median accuracy and lower variance than Zero-shot, demonstrating improved overall performance... effectively reduces the susceptibility of model predictions to option order permutations"
  - [corpus] arxiv:2511.21709 documents selection bias in MCQ tasks where choices are influenced by position rather than content; arxiv:2507.13949 confirms primacy/recency positional biases in LLMs
- Break condition: If the consensus prompt is poorly designed or the model ignores the comparative structure, selection bias may persist

### Mechanism 3
- Claim: Token-efficient single-pass design achieves reasoning benefits without the computational overhead of multi-agent or multi-round approaches
- Mechanism: Unlike Self-Consistency (multiple sampled responses) or multi-agent debate frameworks, BiasPrompting generates all reasonings in one sequential pass within a single LLM, then makes one final prediction
- Core assumption: The quality of single-pass reasoning for each option is sufficient; additional sampling or debate rounds provide diminishing returns
- Evidence anchors:
  - [section 3.4.4] "BiasPrompting consistently generates substantially fewer tokens than CoT, while maintaining or improving accuracy... reduction stems from its single-pass design"
  - [section 1] "BiasPrompting can operate sequentially within a single LLM, making it computationally efficient"
  - [corpus] Limited direct corpus evidence on token-efficiency tradeoffs for this specific approach
- Break condition: If tasks require deeper iterative refinement, single-pass reasoning may be insufficient

## Foundational Learning

- Concept: Selection bias / positional bias in LLMs
  - Why needed here: The paper directly addresses how answer ordering affects model predictions; understanding this bias is essential to grasp why BiasPrompting's design matters
  - Quick check question: If you present an LLM with options [A: 42, B: 17, C: 99] versus [A: 99, B: 42, C: 17] for the same question, would you expect identical accuracy? Why or why not?

- Concept: Chain-of-Thought (CoT) prompting
  - Why needed here: CoT is the primary baseline; understanding its mechanics (generating intermediate reasoning steps before the final answer) clarifies what BiasPrompting does differently and why adding CoT to BiasPrompting showed minimal improvement (Table 3)
  - Quick check question: In standard CoT for MCQ, does the model generate reasoning for all options or only its chosen option?

- Concept: In-context learning and prompt sensitivity
  - Why needed here: The paper positions itself within prompt engineering research; models are "sensitive to input design" (Section 1), and understanding why prompt structure matters helps explain the mechanism
  - Quick check question: What happens to LLM performance when you provide minimal context (just question + options) versus rich contextual framing?

## Architecture Onboarding

- Component map:
  - Reasoning Generation -> Reasonings-Guided Agreement -> Final Answer

- Critical path:
  1. Parse MCQ into (question, option list)
  2. For each option, construct reasoning-generation prompt (see Table 4 template)
  3. Collect all generated reasonings
  4. Construct consensus prompt with question + all options + all reasonings (see Table 5 template)
  5. Extract final answer from placeholder format

- Design tradeoffs:
  - Accuracy vs. latency: n+1 generation calls (n reasonings + 1 consensus) vs. 1 call for zero-shot
  - Bias reduction vs. reasoning quality: If generated reasonings are weak or repetitive, comparative benefit is limited
  - Single-model vs. multi-agent: Paper claims efficiency gains by avoiding multi-agent coordination; tradeoff is less diverse perspectives
  - Adding CoT to consensus stage: Table 3 shows minimal benefit; not recommended

- Failure signatures:
  - Identical/templated reasonings across options (model not genuinely engaging with each)
  - Empty or truncated reasoning outputs (mitigate with non-zero minimum tokens as authors did)
  - Parsing failures on final answer extraction (ensure placeholder format is enforced)
  - Performance drops on datasets where CoT already excels

- First 3 experiments:
  1. Replicate zero-shot vs. CoT vs. BiasPrompting comparison on CommonsenseQA subset (500 questions) using a 7B model; verify accuracy improvements and token counts match paper proportions
  2. Test selection bias robustness: Shuffle answer option orderings (3-5 permutations) on same question set; compare variance between zero-shot and BiasPrompting. Expect lower variance for BiasPrompting per Figure 4
  3. Analyze reasoning quality: Manually inspect 50 examples where BiasPrompting succeeds but baselines fail (per Figure 2 logic); categorize whether improvement comes from genuine knowledge retrieval, better comparison, or noise reduction

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does BiasPrompting maintain its efficiency and accuracy gains when applied to language models significantly larger than 7B parameters?
- Basis in paper: [explicit] The authors state the study is "limited to 7B parameter models" and "The impact of BiasPrompting on these larger models remains unexplored."
- Why unresolved: Larger models may exhibit different reasoning behaviors or biases; it is unclear if forcing reasoning for incorrect options scales effectively or introduces unnecessary noise for more capable models
- What evidence would resolve it: Benchmarking BiasPrompting on larger models (e.g., 70B+ parameters) against standard baselines on the same datasets

### Open Question 2
- Question: Is BiasPrompting effective for reasoning tasks that require rigorous logical or mathematical derivation rather than commonsense association?
- Basis in paper: [explicit] The paper notes experiments "primarily focus on commonsense reasoning tasks" and the method "has not been systematically evaluated on other reasoning domains."
- Why unresolved: Generating "supportive reasoning" for incorrect mathematical answers might result in coherent but logically flawed hallucinations that could mislead the agreement stage more than in commonsense tasks
- What evidence would resolve it: Evaluation on mathematical (e.g., GSM8K) or symbolic reasoning benchmarks

### Open Question 3
- Question: Under what conditions does generating reasoning for incorrect options degrade performance?
- Basis in paper: [inferred] Table 1 shows BiasPrompting decreasing accuracy for Deepseek on CSQA (48.5%) compared to Zero-shot (50.1%), suggesting the method can occasionally introduce confusion
- Why unresolved: The paper does not analyze failure modes where the "biased" reasoning for wrong answers persuades the model away from the correct choice during the agreement stage
- What evidence would resolve it: An error analysis comparing cases where BiasPrompting fails against successful zero-shot predictions to identify specific traits of misleading rationales

## Limitations
- Prompt template specificity is unclear - paper provides examples but lacks complete detailed templates for all answer-choice counts
- Baseline prompt details are not provided, making fair comparison difficult
- Generalizability beyond MCQs is untested - method specifically designed for multiple-choice question answering

## Confidence
- High Confidence: Empirical results showing accuracy improvements across multiple datasets and models with robust statistical significance testing
- Medium Confidence: Mechanism explanations for how BiasPrompting reduces selection bias and forces comprehensive exploration are plausible but need more ablation studies
- Low Confidence: Claim that BiasPrompting "uncovers latent reasoning capabilities" is speculative without definitive proof of accessing new knowledge

## Next Checks
1. Ablation Study: Systematically remove either the reasoning generation stage or the consensus stage to quantify each component's individual contribution to performance improvements
2. Robustness to Reasoning Quality: Generate low-quality, templated reasonings and measure how this affects BiasPrompting's performance to test dependency on genuine reasoning quality
3. Cross-Domain Generalization: Apply BiasPrompting to non-MCQ reasoning tasks like open-ended question answering or multi-step mathematical reasoning to test generalization beyond the specific domain where it was developed