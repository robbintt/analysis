---
ver: rpa2
title: Benchmarking Stochastic Approximation Algorithms for Fairness-Constrained Training
  of Deep Neural Networks
arxiv_id: '2507.04033'
source_url: https://arxiv.org/abs/2507.04033
tags:
- fairness
- stochastic
- constraints
- optimization
- algorithms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper benchmarks stochastic approximation algorithms for
  fairness-constrained training of deep neural networks. The authors construct a large-scale
  benchmark using ACS Census data with over 5.7 billion protected subgroups and implement
  three recently proposed algorithms: Stochastic Ghost, SSL-ALM, and Stochastic Switching
  Subgradient.'
---

# Benchmarking Stochastic Approximation Algorithms for Fairness-Constrained Training of Deep Neural Networks

## Quick Facts
- arXiv ID: 2507.04033
- Source URL: https://arxiv.org/abs/2507.04033
- Reference count: 40
- Authors: Andrii Kliachkin; Jana Lepšová; Gilles Bareilles; Jakub Mareček
- Primary result: Benchmark reveals no existing algorithm has theoretical guarantees for non-convex, non-smooth, large-scale fairness-constrained deep learning settings

## Executive Summary
This paper addresses the critical gap in benchmarking stochastic approximation algorithms for fairness-constrained deep learning. The authors construct a large-scale benchmark using ACS Census data with over 5.7 billion protected subgroups and implement three recently proposed algorithms: Stochastic Ghost, SSL-ALM, and Stochastic Switching Subgradient. The study evaluates these methods on the task of predicting income above $50,000 using race as the protected attribute, revealing significant performance differences among the algorithms in terms of fairness-accuracy trade-offs.

The benchmark demonstrates that ALM-based methods (ALM and SSL-ALM) provide the best balance between constraint satisfaction and optimization performance, achieving meaningful fairness improvements while maintaining reasonable prediction accuracy. In contrast, the Stochastic Ghost method exhibits the highest variability in performance, while Stochastic Switching Subgradient fails to satisfy fairness constraints with the tested parameter settings. Most critically, the work establishes that none of the evaluated algorithms have theoretical guarantees for this challenging non-convex, non-smooth, large-scale optimization setting, highlighting an important direction for future research in fairness-constrained deep learning.

## Method Summary
The authors constructed a large-scale benchmark using American Community Survey (ACS) Census data containing over 5.7 billion protected subgroups. They implemented three stochastic approximation algorithms for fairness-constrained training: Stochastic Ghost, SSL-ALM, and Stochastic Switching Subgradient. The benchmark focused on predicting whether individuals earn above $50,000 annually while constraining fairness violations based on race as the protected attribute. The experiments evaluated each algorithm's ability to satisfy fairness constraints while maintaining prediction accuracy, measuring both constraint violation and classification performance across multiple runs.

## Key Results
- ALM-based methods (ALM and SSL-ALM) achieved the best balance between constraint satisfaction and optimization performance
- Stochastic Ghost method showed highest variability across runs and parameter settings
- Stochastic Switching Subgradient failed to satisfy fairness constraints with tested parameters
- No evaluated algorithm has theoretical guarantees for non-convex, non-smooth fairness-constrained optimization

## Why This Works (Mechanism)
The paper identifies a critical gap in the literature where fairness-constrained deep learning lacks both empirical benchmarking and theoretical foundations. The mechanism for success lies in the careful construction of a large-scale benchmark that captures the complexity of real-world fairness constraints with billions of subgroups, combined with systematic evaluation of recently proposed stochastic approximation methods. The ALM-based approaches succeed because they better handle the non-convex nature of deep learning objectives while maintaining constraint satisfaction through augmented Lagrangian formulations that balance multiple objectives simultaneously.

## Foundational Learning

1. **Fairness Constraints in ML**: Mathematical formulations ensuring equal treatment across protected subgroups. Why needed: Provides the optimization framework for fair model training. Quick check: Can you define demographic parity and equal opportunity mathematically?

2. **Stochastic Approximation Methods**: Iterative algorithms using random samples to approximate solutions to optimization problems. Why needed: Enables scalable training for large datasets with fairness constraints. Quick check: What distinguishes stochastic from deterministic optimization in high dimensions?

3. **Non-convex Optimization**: Optimization landscapes with multiple local minima and saddle points. Why needed: Characterizes the fundamental challenge in training deep neural networks. Quick check: Why do gradient-based methods struggle with non-convex objectives?

4. **Large-scale Benchmark Construction**: Creating representative datasets and evaluation frameworks that capture real-world complexity. Why needed: Ensures results generalize beyond toy problems. Quick check: How do you verify a benchmark represents the target problem domain?

## Architecture Onboarding

**Component Map**: Data preprocessing -> Fairness constraint formulation -> Stochastic algorithm implementation -> Model training -> Evaluation pipeline

**Critical Path**: The core workflow involves (1) loading and preprocessing ACS Census data, (2) formulating fairness constraints for protected subgroups, (3) selecting and configuring one of the three stochastic algorithms, (4) training the model with fairness constraints, and (5) evaluating constraint satisfaction and prediction accuracy.

**Design Tradeoffs**: The choice between ALM-based methods (better constraint satisfaction, slower convergence) versus stochastic methods (faster but less reliable) represents a fundamental tradeoff. Large-scale benchmarks enable realistic evaluation but require significant computational resources and careful implementation to avoid memory bottlenecks.

**Failure Signatures**: Stochastic Ghost shows high variance across runs, indicating instability. Stochastic Switching Subgradient consistently fails constraint satisfaction, suggesting algorithmic limitations or poor parameter choices. Both failures manifest as either inability to meet fairness targets or extreme sensitivity to initialization.

**3 First Experiments**: 
1. Test ALM and SSL-ALM with varying penalty parameters to understand convergence behavior
2. Evaluate fairness metrics (demographic parity, equal opportunity) separately to identify which constraints each method satisfies
3. Compare performance across different protected attributes (gender, age) to assess generalizability

## Open Questions the Paper Calls Out
None specified in the source material.

## Limitations
- No theoretical guarantees exist for any evaluated algorithm in the non-convex, non-smooth setting
- Experiments limited to single task (income prediction) and single protected attribute (race)
- High variability in Stochastic Ghost suggests potential hyperparameter sensitivity not fully explored

## Confidence

**Major Claim Clusters Confidence:**
- **Benchmark construction and implementation (High)**: The methodology for building the large-scale benchmark and implementing the three algorithms appears technically sound and reproducible.
- **ALM-based methods performance (Medium)**: While the empirical results show ALM and SSL-ALM achieve better balance between fairness and accuracy, the limited experimental scope and lack of theoretical backing reduce confidence in these conclusions.
- **No theoretical guarantees statement (High)**: This observation is a factual statement about the current state of the literature and algorithms tested.

## Next Checks

1. Test the algorithms across multiple fairness constraints (e.g., demographic parity, equal opportunity, equalized odds) and protected attributes beyond race to assess generalizability.

2. Conduct hyperparameter sensitivity analysis, particularly for Stochastic Ghost and Stochastic Switching Subgradient, to determine if the observed failures are due to parameter choices rather than algorithmic limitations.

3. Implement and evaluate alternative fairness-constrained optimization approaches (e.g., primal-dual methods, Frank-Wolfe variants) to expand the benchmark beyond the three current algorithms.