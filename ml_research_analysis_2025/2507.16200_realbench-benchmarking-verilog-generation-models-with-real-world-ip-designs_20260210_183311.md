---
ver: rpa2
title: 'RealBench: Benchmarking Verilog Generation Models with Real-World IP Designs'
arxiv_id: '2507.16200'
source_url: https://arxiv.org/abs/2507.16200
tags:
- design
- e203
- verilog
- llms
- code
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RealBench is a Verilog generation benchmark designed to better
  reflect real-world design workflows compared to existing benchmarks. The main problem
  with existing benchmarks is that they oversimplify task design, lack detailed and
  formatted design specifications, and have less rigorous verification environments.
---

# RealBench: Benchmarking Verilog Generation Models with Real-World IP Designs

## Quick Facts
- **arXiv ID:** 2507.16200
- **Source URL:** https://arxiv.org/abs/2507.16200
- **Reference count:** 40
- **Primary result:** RealBench is a Verilog generation benchmark designed to better reflect real-world design workflows compared to existing benchmarks.

## Executive Summary
RealBench addresses critical limitations in existing Verilog generation benchmarks by incorporating complex, real-world IP designs with multi-modal, formatted specifications and rigorous verification environments. The benchmark supports both module-level and system-level tasks, using 100% line coverage testbenches and formal equivalence checking to validate correctness. Evaluations reveal that even state-of-the-art models like o1-preview achieve only 13.3% pass@1 on module-level tasks and 0% on system-level tasks, highlighting the significant gap between current capabilities and real-world requirements.

## Method Summary
RealBench evaluates LLMs and agents on Verilog generation using 4 open-source IP designs (AES, SD controller, Hummingbirdv2 E203 CPU) with manually written, multi-modal specifications. The benchmark supports module-level and system-level tasks, where module-level tasks provide submodule information to isolate structural composition from leaf-node logic design. Verification uses a three-stage pipeline: syntax checking, 100% line coverage testbenches, and formal equivalence checking against reference implementations using Yosys synthesis and JasperGold verification tools.

## Key Results
- o1-preview achieves only 13.3% pass@1 on module-level tasks and 0% on system-level tasks
- Tasks involving submodule instantiation have 13.0% lower pass rates than those without
- 44.2% of code passing testbenches fails formal verification, demonstrating simulation's limitations
- DeepSeek-R1 shows more severe hallucination behaviors on complex tasks compared to general models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Providing detailed, formatted, multi-modal specifications increases the likelihood of correct Verilog generation only when the specification accurately reflects all implementation details, including interfaces, constraints, and corner cases.
- **Mechanism:** RealBench uses manually written, hierarchical specifications that mirror code structure, aiming to eliminate ambiguity in the translation from design intent to RTL. By including block diagrams, state transition diagrams, and detailed I/O tables, the benchmark seeks to provide a complete "blueprint" that reduces hallucination.
- **Core assumption:** The LLM can effectively parse and integrate multi-modal information (text, diagrams, tables) and correctly map abstract descriptions to concrete Verilog syntax and logic without omission.
- **Evidence anchors:**
  - [abstract] RealBench features "multi-modal and formatted design specifications."
  - [Section I] The paper contrasts its detailed specs (e.g., 197-2.9k doc lines, multi-modal diagrams) with existing benchmarks' simple text instructions (1.8-22.3 lines).
  - [corpus - LocalV] Focuses on "Information Locality" for IP-level generation, suggesting context window and spec detail are critical.
- **Break condition:** If the specification is incomplete, ambiguous, or if the LLM cannot correctly interpret diagrammatic information (as suggested by GPT-4o-V's limited improvement), the mechanism fails. The assumption that the spec is "sufficient" is tested by the low pass rates.

### Mechanism 2
- **Claim:** Breaking complex designs into module-level tasks with provided submodule information improves generation success for non-leaf modules compared to end-to-end system generation, but remains a primary failure point for current LLMs.
- **Mechanism:** RealBench provides submodule information (descriptions, I/O) for non-leaf modules. The LLM is expected to correctly instantiate these submodules and wire them, abstracting away their internal implementation. This isolates the task of structural composition from leaf-node logic design.
- **Core assumption:** The LLM can correctly understand the interface of a submodule from a textual/tabular description and generate the correct instantiation syntax and internal wiring without errors in signal names, widths, or connectivity.
- **Evidence anchors:**
  - [abstract] RealBench "supports both module-level and system-level tasks."
  - [Section I] "We will show in our experiments that LLMs perform poorly on Verilog generation tasks that are closer to real-world designs... even the current best model... fails to generate any system-level design."
  - [Section II.D] "For each module-level task, the input provided to the LLM includes... submodule information. During verification, we use golden submodule implementations... so that LLMs are not required to implement them."
  - [corpus - ComplexVCoder] Explicitly targets "Systematic Generation of Complex Verilog Code" to address this challenge.
- **Break condition:** If the LLM hallucinates incorrect module ports, mismatches signal widths, or fails to properly connect submodules, the generated code will fail. The paper's results show this is a major challenge: tasks with submodules have a 13.0% lower pass rate than those without.

### Mechanism 3
- **Claim:** Using formal verification, in addition to high-coverage testbenches, is necessary to reliably detect functional errors in LLM-generated code, as simulation alone can overestimate correctness.
- **Mechanism:** RealBench uses 100% line-coverage testbenches as a first filter and then subjects passing designs to formal equivalence checking against a golden reference model (via Yosys and JasperGold). This catches logical errors that might not be exposed by the finite set of testbench vectors.
- **Core assumption:** The reference design is correct, the formal tool can handle the generated code, and formal equivalence is the true measure of functional correctness for the defined specification.
- **Evidence anchors:**
  - [abstract] Evaluations use "rigorous verification environments, including 100% line coverage testbenches and a formal checker."
  - [Section I] "44.2% of the Verilog code generated by GPT-4-Turbo, which passes the testbenches in RTLLMV2, fails in formal verification."
  - [Section II.C] "We provide a formal verification workflow... to verify the equivalence between the LLM-generated code and the reference Verilog."
  - [corpus - VeriThoughts] Proposes a "benchmark framework grounded in formal verification methods," aligning with this principle.
- **Break condition:** If the generated code has syntax errors or is too complex for the formal tool, the check cannot be completed. Formal verification only guarantees equivalence to the *reference*, not to a higher-level intent not captured in the reference.

## Foundational Learning

- **Concept:** Formal Equivalence Checking
  - **Why needed here:** To move beyond simulation-based correctness, which is probabilistic. RealBench's core validation relies on proving that generated RTL is mathematically equivalent to a golden reference. A learner must understand this is a binary, exhaustive proof, not a sample-based test.
  - **Quick check question:** What is the fundamental difference between a testbench with 100% line coverage and formal equivalence checking in verifying a Verilog module?

- **Concept:** Hardware Description Language (HDL) Module Hierarchy and Instantiation
  - **Why needed here:** The primary failure mode identified in the paper is the LLM's inability to correctly instantiate submodules. Understanding how a parent module references and connects to a child module's interface (ports, parameters) is critical for diagnosing these failures.
  - **Quick check question:** In Verilog, when instantiating a submodule, what are the two common methods for connecting its ports, and which one is generally preferred for clarity and avoiding errors?

- **Concept:** Multi-Modal Specification in Digital Design
  - **Why needed here:** RealBench uses text, diagrams, and tables as input. A learner must grasp how these different representations convey information (e.g., a state diagram vs. a state transition table) and why integrating them is a challenge for a text-based model.
  - **Quick check question:** Why might a state transition diagram be more effective than a textual description for conveying the behavior of a complex Finite State Machine (FSM) to a human designer, and what challenge might this pose for an LLM?

## Architecture Onboarding

- **Component Map:** Task Specification -> LLM Generation -> Syntax Check -> Testbench Verification -> Formal Verification
- **Critical Path:** The path from **Task Specification -> LLM Generation -> Syntax Check -> Testbench Verification -> Formal Verification**. The bottleneck is the final Formal Verification step, which is the ultimate arbiter of correctness.
- **Design Tradeoffs:**
    - **Complexity vs. Solvability:** RealBench chooses high, realistic complexity (real-world IPs), which makes tasks nearly unsolvable for current models (e.g., 13.3% pass@1 for o1-preview). This provides a clear measure of the gap but offers few positive training signals.
    - **Rigorous vs. Fast Verification:** Formal verification is slow and resource-intensive but provides definitive correctness. Simulation is fast but can miss corner cases, as shown by the 44.2% failure rate of passing code under formal checks.
- **Failure Signatures:**
    - **Submodule Instantiation Errors:** Hallucinated or mismatched ports/wiring, a primary cause of failure for non-leaf modules.
    - **Diagram Misinterpretation:** Failure to correctly convert visual logic (e.g., from a state diagram) into code.
    - **Reasoning Model Hallucination:** "Reasoning" models like DeepSeek-R1 show reduced advantage on complex tasks due to generating fictitious module headers and other hallucinations.
    - **Verification Overestimation:** Code that passes simulation but fails formal checks indicates a subtle logical error not covered by test vectors.
- **First 3 Experiments:**
    1.  **Baseline Evaluation:** Run a standard coding LLM (e.g., GPT-4o) on a set of module-level tasks from RealBench. Measure pass rates at each stage: syntax check, testbench pass, and formal verification pass. This establishes the baseline performance gap.
    2.  **Ablation on Specifications:** For a specific complex module (e.g., `e203_exu_alu`), provide the LLM with different levels of specification detail (e.g., text-only vs. text+diagrams) and measure the impact on pass rates. This tests the multi-modal mechanism.
    3.  **Agent Debugging Loop:** Implement a simple agent that uses the output from the verification pipeline (syntax errors, simulation failure messages, formal check mismatches) as feedback for the LLM to regenerate code. Measure the improvement in pass rate over multiple iterations (e.g., up to 5) to test the efficacy of the debugging loop as described in the paper's agent evaluation.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can formal verification methods be optimized to efficiently verify long and unstable Verilog code generated by LLMs?
- **Basis in paper:** [explicit] The authors state that formal verification is currently "slow for large designs" and suggest that "exploring more efficient methods to apply formal verification to long and unstable code" is important.
- **Why unresolved:** Current formal verification tools are computationally expensive and struggle with the syntactic and structural instability often found in LLM outputs, making the process intractable for large system-level tasks.
- **What evidence would resolve it:** A methodology that reduces the runtime of formal equivalence checking for LLM-generated code to a practical level without sacrificing the rigor established in RealBench.

### Open Question 2
- **Question:** Can reasoning models be trained to minimize hallucination rates when performing complex, multi-step Verilog generation tasks?
- **Basis in paper:** [inferred] The paper observes that DeepSeek-R1-671B exhibits "more severe hallucination behaviors" compared to general models when facing complex tasks, leading to "simple mistakes" like fictitious module headers.
- **Why unresolved:** Current reasoning model training appears to trade off reliability for reasoning depth in the specific domain of complex hardware description, failing to ground the generation in the provided specification constraints.
- **What evidence would resolve it:** A reasoning model that matches or exceeds the performance of general models on simple tasks while significantly improving accuracy on RealBench's complex module-level tasks without generating invalid syntax.

### Open Question 3
- **Question:** What advancements are required for multi-modal LLMs to effectively parse and integrate multiple distinct types of design diagrams simultaneously?
- **Basis in paper:** [explicit] The authors note that while GPT-4o-V shows improvement on benchmarks with single diagrams, it struggles with RealBench tasks that "often include multiple diagrams," implying a stronger multi-modal model is needed.
- **Why unresolved:** Current vision encoders likely lack the spatial and logical reasoning capabilities to synthesize relationships between separate visual inputs (e.g., a state diagram and a separate data flow diagram) into a coherent code structure.
- **What evidence would resolve it:** A multi-modal model demonstrating a statistically significant improvement in pass rates on RealBench module-level tasks specifically attributed to the successful interpretation of multiple visual inputs.

### Open Question 4
- **Question:** What specific mechanisms or architectures are needed to improve LLM performance on tasks requiring submodule instantiation?
- **Basis in paper:** [explicit] The analysis shows a "significant impact" on pass rates for tasks involving submodules, with the authors identifying the "ability to handle submodule relationships" as a key future research direction.
- **Why unresolved:** LLMs currently struggle to maintain context regarding hierarchical dependencies and interface definitions when instantiating submodules, often failing to match the parent module's expectations with the submodule's implementation.
- **What evidence would resolve it:** An LLM or agent framework that achieves a significantly higher pass rate on the "w/ submodules" category of RealBench compared to the current near-zero performance on complex system-level tasks.

## Limitations

- The benchmark's extreme difficulty (e.g., 13.3% pass@1) may not be optimal for driving model progress despite being realistic
- Formal verification dependency on Cadence JasperGold creates barriers for cost-effective reproduction
- Benchmark assumes specification completeness without mechanism to validate specs against reference implementations

## Confidence

- **High Confidence:** The benchmark successfully exposes the limitations of current LLMs in generating complex, real-world Verilog, particularly for system-level tasks and submodule instantiation. The low pass rates are a robust finding.
- **Medium Confidence:** The claim that multi-modal specifications significantly improve performance is supported, but the marginal benefit of diagram understanding (e.g., for GPT-4o-V) suggests this mechanism is not fully realized. The impact of specification detail needs further isolation.
- **Medium Confidence:** The claim that formal verification is necessary to catch errors missed by simulation is strongly supported by the data (44.2% failure rate for passing code). However, the dependency on a specific commercial tool for the final check is a practical limitation.

## Next Checks

1. **Specification Detail Ablation Study:** For a set of complex modules, systematically reduce the specification detail (text-only vs. text+diagrams) and measure the impact on pass rates for both standard and vision-capable LLMs. This will validate the importance of each modality.
2. **Agent Feedback Loop Effectiveness:** Implement the debugging agent loop as described in the paper. Measure the improvement in pass rate from Pass@1 to Pass@5 and analyze the types of errors corrected in each iteration to assess the agent's efficacy.
3. **Open-Source Formal Verification Feasibility:** Attempt to adapt the verification pipeline to use an open-source formal equivalence checker (e.g., Yosys' `equiv` command) instead of JasperGold. Document any limitations or workarounds required to achieve comparable results on a subset of tasks.