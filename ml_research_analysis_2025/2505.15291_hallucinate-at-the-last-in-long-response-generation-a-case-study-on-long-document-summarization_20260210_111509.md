---
ver: rpa2
title: 'Hallucinate at the Last in Long Response Generation: A Case Study on Long
  Document Summarization'
arxiv_id: '2505.15291'
source_url: https://arxiv.org/abs/2505.15291
tags:
- faithfulness
- long
- generated
- summaries
- table
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Large language models (LLMs) exhibit a consistent positional bias
  in long document summarization: hallucinations concentrate disproportionately in
  the latter parts of generated outputs. This phenomenon, termed "hallucinate at the
  last," shows that faithfulness scores decline toward the end, especially in longer
  summaries.'
---

# Hallucinate at the Last in Long Response Generation: A Case Study on Long Document Summarization

## Quick Facts
- **arXiv ID**: 2505.15291
- **Source URL**: https://arxiv.org/abs/2505.15291
- **Reference count**: 40
- **Key outcome**: LLMs exhibit consistent positional bias in long document summarization, with hallucinations concentrating disproportionately in the latter parts of generated outputs

## Executive Summary
This study investigates a systematic positional bias in long document summarization where hallucinations concentrate toward the end of generated summaries. The phenomenon, termed "hallucinate at the last," shows that faithfulness scores decline progressively in longer summaries. Through systematic analysis across multiple models, datasets, and decoding strategies, the research identifies that increased attention to previously generated text correlates with higher hallucination rates. The findings reveal that models with sliding window attention mechanisms, such as Qwen, can mitigate this bias by maintaining more balanced attention distribution throughout generation.

## Method Summary
The study employs a comprehensive experimental framework to analyze hallucination patterns in long document summarization. Researchers systematically compare multiple large language models including GPT-4, Qwen, and others across different document types and lengths. The analysis uses automatic faithfulness metrics (BERTScore and MoverScore) to quantify hallucination rates at different positions within generated summaries. The study examines various decoding strategies and attention mechanisms, comparing standard causal attention with sliding window approaches. Experiments are conducted on multiple summarization datasets to validate the consistency of the positional hallucination phenomenon across different contexts.

## Key Results
- Hallucination rates increase progressively toward the end of generated summaries, with faithfulness scores declining in the latter portions
- Models with sliding window attention mechanisms (like Qwen) show reduced positional bias compared to standard causal attention models
- Increased self-attention to previously generated text correlates strongly with higher hallucination rates at later positions
- Chunk-based summarization approaches (BOOOOKSCORE), sparse attention mechanisms (MInference), and context-aware decoding (AdaCAD) show promise in mitigating the positional bias

## Why This Works (Mechanism)
The positional hallucination bias emerges from the interaction between attention mechanisms and generation length. As models generate longer sequences, they increasingly attend to their own previously generated text rather than the source document. This self-referential attention creates a compounding effect where errors and hallucinations in earlier parts influence subsequent generation, particularly in the latter portions where attention distribution becomes more skewed toward the generated output rather than the source context.

## Foundational Learning

**Attention mechanisms in transformers**: The self-attention mechanism allows models to weigh different input positions when generating each token. Why needed: Understanding attention patterns is crucial for diagnosing hallucination sources. Quick check: Verify that attention weights shift toward generated text in longer sequences.

**Positional encoding**: Transformers use positional encodings to maintain sequence order information. Why needed: Positional biases may interact with how position information is encoded and utilized. Quick check: Examine whether positional encoding schemes influence hallucination patterns.

**Faithfulness metrics**: Automated measures like BERTScore and MoverScore quantify semantic similarity between generated and reference summaries. Why needed: Objective evaluation of hallucination rates across document positions. Quick check: Validate metric sensitivity to different types of hallucinations.

**Sliding window attention**: A mechanism that limits attention span to a fixed window rather than full sequence. Why needed: Alternative attention architectures that may reduce positional bias. Quick check: Compare attention distribution patterns between sliding window and standard attention.

## Architecture Onboarding

**Component map**: Input Document -> Encoder -> Attention Mechanism -> Decoder -> Generated Summary -> Faithfulness Evaluation

**Critical path**: The generation process follows: source document encoding → attention-weighted token generation → self-attention to previously generated tokens → output prediction. The critical vulnerability occurs when self-attention to generated text overwhelms attention to source content, particularly in later generation stages.

**Design tradeoffs**: Standard causal attention provides full context but becomes computationally expensive and attention-saturated in long sequences. Sliding window attention reduces computational load and attention bias but may miss long-range dependencies. The tradeoff involves balancing computational efficiency, context awareness, and positional bias.

**Failure signatures**: Hallucinations concentrate at sequence positions beyond 70-80% of total length, with faithfulness metrics showing consistent decline. The failure pattern shows increasing deviation from source content as generation progresses, particularly in models without positional bias mitigation.

**First experiments**: 
1. Compare attention weight distributions between Qwen (sliding window) and GPT-4 (standard attention) across identical long documents
2. Measure faithfulness metrics at 25%, 50%, 75%, and 100% completion points for varying summary lengths
3. Evaluate hallucination patterns across different document genres to test generalizability

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Reliance on automatic faithfulness metrics (BERTScore and MoverScore) rather than comprehensive human evaluation may not fully capture semantic fidelity of hallucinations
- Analysis focuses on positional patterns but does not extensively examine whether hallucinations follow systematic semantic patterns or are random
- Mitigation strategies are evaluated primarily through indirect metrics rather than comprehensive human judgment of summary quality and hallucination reduction

## Confidence

**High confidence**: Existence of positional hallucination bias is consistently observed across multiple models and datasets with stable metric trends

**Medium confidence**: Attention mechanism correlation provides plausible explanation but requires causal validation through controlled experiments

**Medium confidence**: Mitigation strategy effectiveness shows preliminary promise but needs broader evaluation scope and human validation

## Next Checks
1. Conduct human evaluation studies comparing hallucination rates across document positions using multiple annotators to validate automatic metric findings
2. Perform ablation studies specifically isolating the effect of sliding window attention versus other architectural differences in Qwen
3. Test the proposed mitigation strategies (BOOOOKSCORE, MInference, AdaCAD) on diverse document types beyond the current datasets to assess generalizability