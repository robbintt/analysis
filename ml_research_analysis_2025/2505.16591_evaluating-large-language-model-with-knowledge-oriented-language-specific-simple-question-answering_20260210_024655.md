---
ver: rpa2
title: Evaluating Large Language Model with Knowledge Oriented Language Specific Simple
  Question Answering
arxiv_id: '2505.16591'
source_url: https://arxiv.org/abs/2505.16591
tags:
- answer
- general
- question
- language
- specific
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces KoLasSimpleQA, a benchmark for evaluating\
  \ multilingual factual ability of large language models. It features 9 languages\
  \ and two domains\u2014general (global facts) and language-specific (history, culture,\
  \ regional traditions)\u2014with 2,147 QA pairs."
---

# Evaluating Large Language Model with Knowledge Oriented Language Specific Simple Question Answering

## Quick Facts
- arXiv ID: 2505.16591
- Source URL: https://arxiv.org/abs/2505.16591
- Reference count: 40
- This paper introduces KoLasSimpleQA, a benchmark for evaluating multilingual factual ability of large language models.

## Executive Summary
This paper introduces KoLasSimpleQA, a benchmark for evaluating multilingual factual ability of large language models. It features 9 languages and two domains—general (global facts) and language-specific (history, culture, regional traditions)—with 2,147 QA pairs. The study evaluates mainstream LLMs and Large Reasoning Models, revealing significant performance gaps: models perform worse in language-specific domains, benefit from translating non-English queries into English only in the general domain, show poorer calibration in language-specific contexts, and exhibit less robust knowledge memorization. Notably, LRMs achieve higher calibration and better bidirectional knowledge robustness, but still struggle in language-specific tasks. The findings highlight the need for targeted multilingual optimization. KoLasSimpleQA is publicly available to support future LLM development.

## Method Summary
KoLasSimpleQA benchmark construction: Wikipedia articles classified by inter-language link count (nill=0 → language-specific; high nill → general) → GPT-4o triple extraction → QA generation → 2-stage QC (LLM filter + human annotators). Evaluation: LLM-as-judge (GPT-4o) classifies responses as CORRECT/INCORRECT/NOT ATTEMPTED. Two settings: direct (original language) and tran_en (translated to English). Metrics: F-score, ECE, Pbi. 12 models evaluated including GPT-4o, Deepseek-V3/R1, Qwen2.5, Llama-3.1, QwQ variants, o1-mini via OpenCompass.

## Key Results
- LLMs perform significantly worse in language-specific domains (~20-50% lower F-scores) than general domains.
- Translating queries to English improves general domain accuracy but reduces performance in language-specific domains.
- Large Reasoning Models achieve superior calibration (lower ECE) and better bidirectional knowledge robustness, but still struggle with language-specific knowledge.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Distinguishing general vs. language-specific knowledge via Wikipedia inter-language links reveals fundamental gaps in LLM multilingual factual abilities.
- **Mechanism:** The number of inter-language links (nill) serves as a proxy for knowledge universality—entries with nill=0 indicate language-specific knowledge (unique to one linguistic community), while high nill values indicate global knowledge. This separation exposes that models trained predominantly on English-centric corpora perform well on globally-linked facts but fail on locally-anchored knowledge.
- **Core assumption:** Wikipedia article cross-linking behavior reflects genuine cultural/linguistic specificity rather than editorial coverage gaps.
- **Evidence anchors:**
  - [section 2.2.1]: "If an entity lacks versions in other languages (nill = 0), it is classified as language-specific, indicating the knowledge is unique to that language."
  - [section 4.1]: "LLMs perform significantly worse in the specific domain than in the general one, highlighting challenges in language-specific factual QA."
  - [corpus]: MultiLoKo benchmark (arXiv:2504.10356) similarly uses locally-sourced questions per language, confirming the importance of culturally-grounded evaluation partitions.
- **Break condition:** If nill classification were replaced with topic-modeling or human curation, domain separation may not correlate with performance gaps as cleanly.

### Mechanism 2
- **Claim:** English translation as an intermediate step improves performance in general domain but degrades it for language-specific queries.
- **Mechanism:** Translating to English leverages the English-centric pretraining distribution, helping with globally-shared facts. However, language-specific entities and culturally-bound concepts lose semantic precision or context during translation, introducing noise that cannot be recovered by the model's English knowledge.
- **Core assumption:** Translation artifacts (entity name mismatches, cultural concept untranslatability) are the primary driver of degradation, rather than insufficient English knowledge of the translated topic.
- **Evidence anchors:**
  - [section 4.2]: "In the general domain, the tran_en setting consistently enhances performance across most models...However, in the language-specific domain, models generally perform better when questions are presented in their original language."
  - [corpus]: EMCee (arXiv:2503.05846) proposes synthetic multilingual context bridging, suggesting translation-only approaches have known limitations in knowledge transfer.
- **Break condition:** If machine translation quality were near-perfect for named entities and cultural concepts, this asymmetry could diminish or reverse.

### Mechanism 3
- **Claim:** Large Reasoning Models (LRMs) exhibit better calibration than traditional LLMs, particularly through reflective reasoning processes that improve self-awareness of knowledge boundaries.
- **Mechanism:** LRMs generate multiple "thoughts" during inference, allowing self-correction and boundary detection. The "regret in reasoning search" metric (P(R_nct≥1)) shows LRMs often generate correct thoughts but fail to commit to them—suggesting reasoning depth exposes knowledge gaps that traditional models simply hallucinate past.
- **Core assumption:** The correlation between reasoning tokens and calibration is causal, not coincidental; increased compute at inference actively improves uncertainty estimation.
- **Evidence anchors:**
  - [section 4.3]: "LRMs demonstrate superior calibration compared to traditional LLMs, as all five LRMs rank within the top six for mECE values."
  - [section 4.5]: "In the general domain, both models exhibit a much higher P(R_nct≥1) than in the language-specific domain. This indicates that when tackling global knowledge questions, LRMs have substantial opportunities to answer correctly through thorough knowledge recall, search and reflection."
  - [corpus]: Limited direct corpus support for LRM calibration mechanisms; this remains an emerging research area.
- **Break condition:** If reasoning were truncated or thoughts were not meaningfully diverse (repetition), calibration benefits would likely disappear.

## Foundational Learning

- **Concept: Inter-language Links (nill) as Knowledge Specificity Signal**
  - Why needed here: Understanding how the benchmark separates general vs. specific knowledge is essential for interpreting all downstream results.
  - Quick check question: If a Wikipedia page exists only in Hungarian with no links to other language versions, what domain would KoLasSimpleQA classify it under?

- **Concept: Calibration and Expected Calibration Error (ECE)**
  - Why needed here: The paper argues LRMs are better "self-aware"—ECE quantifies how well confidence scores match actual accuracy.
  - Quick check question: A model with 80% confidence on questions it answers correctly 40% of the time has high or low ECE?

- **Concept: Reversal Curse in Autoregressive Models**
  - Why needed here: The benchmark includes reverse-relationship QA pairs to test if models truly internalize knowledge bidirectionally.
  - Quick check question: If a model learns "A is the capital of B," will it necessarily answer "What is A the capital of?" correctly? Why or why not?

## Architecture Onboarding

- **Component map:** Wikipedia crawl → nill-based classification → GPT-4o triple extraction → QA generation → 2-stage QC (LLM filter + human annotators) → Model inference (direct or translate-to-English) → GPT-4o-as-judge → Metrics computation

- **Critical path:**
  1. Ensuring nill-based domain separation is valid (human spot-check samples)
  2. Quality control stage 2 (human annotators verifying without reference answers)
  3. LLM-as-judge prompt calibration (ensuring CORRECT/INCORRECT/NOT_ATTEMPTED classification reliability)

- **Design tradeoffs:**
  - Using GPT-4o for both QA generation and evaluation introduces circularity risk—mitigated by human verification but not eliminated
  - Restricting to single-knowledge-point questions improves eval efficiency but limits reasoning assessment
  - 9 languages balances coverage vs. annotation cost; low-resource languages may have higher noise

- **Failure signatures:**
  - High NA (Not Attempted) rates in language-specific domain suggest knowledge boundary awareness but also potential over-conservatism
  - Low P_bi (bidirectional correctness) indicates memorization without relational understanding
  - Large ECE gaps between domains suggest confidence calibration is language/culturally sensitive

- **First 3 experiments:**
  1. **Baseline replication:** Run GPT-4o-mini on the benchmark using both direct and translate-to-English settings; verify F-score and ECE match reported values within ±2 percentage points.
  2. **Ablation on translation direction:** For a single low-resource language (e.g., Vietnamese), compare: (a) original query, (b) translate to English, (c) translate to English then back to original. Measure performance degradation per transformation.
  3. **Thought-level analysis for non-LRM models:** Force a standard LLM to generate step-by-step reasoning (via prompting) before answering; measure if this improves calibration or if LRM gains are architectural rather than prompt-induced.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can translation-based prompting strategies be adapted to prevent performance degradation in language-specific domains?
- **Basis in paper:** [explicit] Section 4.2 finds that translating queries to English improves general domain accuracy but significantly reduces performance in language-specific domains.
- **Why unresolved:** The authors identify this trade-off but do not propose methods to preserve cultural nuance or context during the translation process.
- **What evidence would resolve it:** A translation protocol or prompting method that maintains performance parity between native-language and English-prompted queries for language-specific questions.

### Open Question 2
- **Question:** How can Large Reasoning Models (LRMs) be refined to ensure that correct intermediate reasoning steps lead to correct final outputs?
- **Basis in paper:** [explicit] Section 4.5 introduces "Regret in reasoning search," showing LRMs often generate correct internal thoughts ($P(R_{n_{ct} \ge 1})$) yet still fail to output the correct answer.
- **Why unresolved:** There is a disconnect between the model's internal exploration of reasoning paths and its final selection mechanism.
- **What evidence would resolve it:** A model architecture or training objective that aligns the final output probability with the correctness of the generated reasoning trace.

### Open Question 3
- **Question:** Is the absence of inter-language links ($n_{ill}=0$) a valid proxy for "cultural specificity" versus mere obscurity?
- **Basis in paper:** [inferred] Section 2.2.1 uses Wikipedia inter-language link counts to define the language-specific domain, assuming unlinked entries are culturally unique.
- **Why unresolved:** This heuristic may conflate truly cultural knowledge with general but obscure topics that simply lack Wikipedia editorial coverage.
- **What evidence would resolve it:** A validation study correlating low $n_{ill}$ scores with human expert ratings of "cultural significance" rather than just "topic obscurity."

## Limitations
- The nill-based domain separation assumes Wikipedia inter-language link sparsity directly reflects cultural knowledge uniqueness, but coverage gaps could confound this signal.
- Using GPT-4o both for QA generation and evaluation introduces potential circularity, though mitigated by human verification.
- The LRM calibration findings rely on an emerging metric with limited external validation.

## Confidence
- High confidence: The general vs. language-specific performance gap is robustly demonstrated across multiple models and settings.
- Medium confidence: The translation asymmetry mechanism is well-supported but the exact contribution of translation artifacts vs. pretraining distribution effects remains unclear.
- Medium confidence: LRM calibration benefits are shown but the mechanism (reasoning vs. architectural) is not definitively separated from prompt effects.

## Next Checks
1. **Domain separation validation:** Manually annotate 50 randomly selected language-specific domain entries to verify they represent culturally unique knowledge (not just Wikipedia coverage gaps). Compare against nill=0 classification.
2. **Translation ablation study:** For Vietnamese, compare: (a) original query, (b) translate to English, (c) translate to English then back to Vietnamese. Measure performance degradation per transformation to isolate translation artifacts.
3. **Thought-level analysis for non-LRMs:** Force a standard LLM to generate step-by-step reasoning before answering; measure if this improves calibration or if LRM gains are architectural rather than prompt-induced.