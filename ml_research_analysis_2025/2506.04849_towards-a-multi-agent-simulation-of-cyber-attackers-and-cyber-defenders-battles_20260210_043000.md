---
ver: rpa2
title: Towards a Multi-Agent Simulation of Cyber-attackers and Cyber-defenders Battles
arxiv_id: '2506.04849'
source_url: https://arxiv.org/abs/2506.04849
tags:
- agents
- agent
- properties
- actions
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a Dec-POMDP-based simulation model for coordinated
  cyber-attacker and cyber-defender agent interactions in networked systems. The model
  abstracts network nodes into property sets and defines agent actions as property
  transitions, enabling realistic scenario implementation and evaluation.
---

# Towards a Multi-Agent Simulation of Cyber-attackers and Cyber-defenders Battles

## Quick Facts
- arXiv ID: 2506.04849
- Source URL: https://arxiv.org/abs/2506.04849
- Reference count: 22
- Multi-agent simulation framework using Dec-POMDPs for coordinated cyber battles

## Executive Summary
This paper presents MCAS (Multi Cyber Agent Simulator), a Dec-POMDP-based framework for simulating coordinated cyber-attacker and cyber-defender interactions in networked systems. The authors abstract network nodes into property sets and define agent actions as property transitions, enabling realistic scenario implementation using MITRE ATT&CK-based attack patterns. Using a GALLIUM APT scenario, they evaluate three behavioral approaches: random, decision tree-guided, and multi-agent reinforcement learning (MARL). Results show MARL agents achieve attack success comparable to handcrafted decision trees, while adding defensive MARL agents significantly reduces attack success rates.

## Method Summary
The MCAS simulator uses PettingZoo to implement a turn-based Agent Environment Cycle where agents observe filtered property subsets of the network state, select actions, and receive team rewards. Network nodes are represented as sets of (identifier, value) property pairs, with actions defined by boolean pre-conditions and post-conditions that modify these properties. The framework supports three agent behaviors: random action selection, decision tree execution of optimal paths, and Q-learning-based MARL with curriculum learning. A GALLIUM APT scenario involving 15 nodes across 5 subnets and 30 actions was implemented to test the approach, with performance measured through state-based metrics and rewards.

## Key Results
- MARL attackers achieved attack success rates comparable to decision tree agents after 1000 training episodes
- Adding defensive MARL agents to the scenario significantly reduced attacker success rates
- The shortest attack path required 16 sequential actions with random success probability of (1/30)^16
- State abstraction through property transitions enabled tractable simulation of complex attack scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Property-based state abstraction enables tractable simulation of network-level cyber battles.
- Mechanism: Network nodes are represented as sets of (identifier, value) property pairs. Actions modify these properties through boolean pre-conditions and post-conditions, creating state transitions that approximate real system behavior without modeling low-level packet dynamics.
- Core assumption: Cyber operations can be meaningfully abstracted as discrete property transitions rather than continuous network traffic.
- Evidence anchors:
  - [abstract] "We abstracted network nodes by sets of properties including agents' ones. Actions applied by agents model how the network reacts depending in a given state and what properties are to change."
  - [section III-A] "An action can be applied only if the boolean property-based pre-condition is satisfied in the current environment state."
  - [corpus] Weak direct corpus support for this specific property-transition mechanism; neighboring papers focus on LLM agents and coordination protocols rather than property-based simulation.

### Mechanism 2
- Claim: Dec-POMDP framing provides a shared mathematical framework for heterogeneous agent coordination under partial observability.
- Mechanism: Each agent receives observations filtered through an Obs function, selects actions from local policy, and receives team-relevant rewards. The sequential turn structure (Agent Environment Cycle) simplifies coordination while maintaining partial observability constraints.
- Core assumption: Sequential execution approximates concurrent real-world operations without introducing unacceptable simulation artifacts.
- Evidence anchors:
  - [section II] "In a Dec-POMDP, several agents can have a common reward function and can coordinate their actions to achieve a common goal, especially by being able to communicate."
  - [section III-B] Formal Dec-POMDP model defines S, A, T, R, Ω, O with property-based instantiations.
  - [corpus] [arXiv:2601.08327] "Safe Heterogeneous Multi-Agent RL" supports Dec-POMDP value for partial observability and communication constraints.

### Mechanism 3
- Claim: MARL with curriculum learning achieves attack efficacy comparable to handcrafted decision trees.
- Mechanism: Q-learning agents explore the action space, receiving rewards tied to goal metrics. Curriculum learning first trains attackers solo, then introduces defenders, allowing progressive skill acquisition.
- Core assumption: The reward function adequately captures mission success; action space is sufficiently bounded for Q-learning convergence.
- Evidence anchors:
  - [section IV-B] "After several episodes, chosen action paths by the attackers tend to be as efficient as the DT paths."
  - [section IV-B] "When adding the defenders... we verified the attackers to be less and less able to reach the ultimate goal."
  - [corpus] [arXiv:2505.04843] "Large Language Models are Autonomous Cyber Defenders" supports RL-based autonomous defense but uses single-agent focus.

## Foundational Learning

- Concept: **Dec-POMDP (Decentralized Partially Observable Markov Decision Process)**
  - Why needed here: The core mathematical framework; without understanding joint state-action-observation spaces and the distinction from single-agent MDPs, the simulation architecture will be opaque.
  - Quick check question: Can you explain why each agent needs its own observation function rather than accessing global state?

- Concept: **MITRE ATT&CK Framework**
  - Why needed here: The case study integrates tactics, techniques, and procedures from this knowledge base; understanding its hierarchy (tactics → techniques → procedures) is required to extend scenarios.
  - Quick check question: What is the difference between a Tactic and a Technique in MITRE ATT&CK?

- Concept: **Attack-Defense Trees**
  - Why needed here: Scenario design uses AD trees to structure attack paths and defensive countermeasures; this bridges MITRE knowledge to simulator actions.
  - Quick check question: How does an AD tree differ from a standard attack graph?

## Architecture Onboarding

- Component map:
  Environment State -> Agent Behavior Layer -> Observation Filter -> Action Library -> MCAS Interface
  (Property sets) -> (Random/MARL/DT) -> (Obs function) -> (Pre/post conditions) -> (JSON/Visualization)

- Critical path:
  1. Define network topology and node properties (JSON schema)
  2. Build action library from AD tree with pre/post conditions
  3. Implement agent behaviors (start with random for baseline)
  4. Configure reward function via Metrics → Eval pipeline
  5. Run episodes, collect metrics, iterate on agent policies

- Design tradeoffs:
  - **Property abstraction level**: Finer-grained properties increase state space exponentially vs. coarser abstractions lose fidelity
  - **Sequential vs. concurrent execution**: Sequential simplifies implementation but may misrepresent timing-sensitive attacks
  - **Curriculum learning order**: Training attackers first ensures meaningful defender evaluation vs. joint training may discover emergent strategies

- Failure signatures:
  - MARL agents never converge: Likely action space too large or reward function sparse/misaligned
  - Attackers always succeed regardless of defenders: Check defender pre-conditions are reachable; defenders may be observation-starved
  - State explosion crashes simulation: Property combinations exceed tractable state space; increase abstraction level

- First 3 experiments:
  1. Replicate the GALLIUM scenario with random agents only to validate action graph connectivity (baseline success rate should be near-zero given (1/30)^16 probability).
  2. Implement decision tree attacker and measure episode-to-goal distribution; verify it matches expected shortest path of 16 actions.
  3. Train MARL attackers with defenders disabled, then enable defenders and observe attacker success rate degradation—this validates the curriculum learning mechanism.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can realistic attack/defense scenarios be automatically generated from MITRE ATT&CK and similar databases rather than through manual AD-tree construction?
- Basis in paper: [explicit] Authors state "automating the integration of more realistic scenarios leveraging on a basis of common actions and properties" as a main limitation, and note that automated attack graph generation "are out of the scope of this work."
- Why unresolved: Current approach requires manual identification of tactics/techniques and manual construction of AD trees, which does not scale.
- What evidence would resolve it: A pipeline that automatically extracts techniques from threat databases and generates valid Dec-POMDP action sets with property pre/post-conditions.

### Open Question 2
- Question: Can agent behaviors trained in MCAS simulation transfer effectively to emulated or real network systems while maintaining performance?
- Basis in paper: [explicit] Authors identify as a key limitation "establishing a way to use the benefits of results obtained with simulations for emulated or real systems while maintaining agent behaviors during deployment."
- Why unresolved: Sim-to-real gap is unexplored; simulation uses abstracted property sets that may not map cleanly to actual system states.
- What evidence would resolve it: Demonstrated retention of defender performance when policies trained in MCAS are deployed on emulated networks (e.g., CYST, CyberBattleSim) or live testbeds.

### Open Question 3
- Question: What coordination mechanisms (communication protocols, shared goals) enable multi-agent teams to handle scenarios requiring distributed entry points or collaborative goal achievement?
- Basis in paper: [explicit] Authors call for "having more coordination between agents, such as several entry points or scenarios with needed communication to reach a goal."
- Why unresolved: Current experiments use sequential agent execution with independent Q-learning; no explicit communication or joint policy learning is implemented.
- What evidence would resolve it: Comparative experiments showing improved attack success rates or defense performance when agents share observations/intentions versus fully decentralized baselines.

### Open Question 4
- Question: How do resource constraints (action costs, execution duration, stealth requirements) affect optimal attacker and defender policies?
- Basis in paper: [explicit] Authors list "introducing new constraints in actions (such as cost, execution duration, etc.)" as a future direction.
- Why unresolved: Current model treats all actions as equally costly and instantaneous; real cyber operations involve trade-offs between speed, detectability, and resource expenditure.
- What evidence would resolve it: Simulation results showing policy divergence and performance changes when action costs/durations are introduced, compared to unconstrained baselines.

## Limitations
- **Manual scenario engineering**: Current approach requires manual construction of attack-defense trees and action libraries from MITRE ATT&CK
- **Sequential execution assumption**: Turn-based simulation may not capture timing-sensitive concurrent operations
- **Scalability concerns**: Property abstraction may not scale to enterprise networks with 100+ nodes and complex interdependencies

## Confidence

- **Property-based simulation tractability**: Medium - Mechanism 1 has theoretical grounding but lacks empirical validation across diverse attack types
- **Dec-POMDP framework suitability**: High - Mechanism 2 is well-established in multi-agent literature with supporting corpus evidence
- **MARL vs decision tree performance**: Medium - Mechanism 3 shows comparable results in controlled scenario but lacks statistical significance testing
- **Collective defense efficacy**: Medium - Results show defenders reduce attacker success but don't quantify strategic adaptability or long-term learning

## Next Checks

1. **State space complexity audit**: Measure actual state space size (distinct property combinations) during GALLIUM scenario execution and compare against theoretical upper bound. If ratio > 0.5, abstraction level is too fine-grained for tractable MARL.

2. **Concurrency impact test**: Modify the simulator to execute 2-3 actions simultaneously (vs. sequential turns) and re-run the MARL vs. decision tree comparison. Measure divergence in success rates and learning curves.

3. **Generalization stress test**: Apply the current framework to a non-APT scenario (e.g., ransomware propagation) without modifying the core property-action abstraction. Measure action library coverage and MARL convergence speed as indicators of framework flexibility.