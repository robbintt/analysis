---
ver: rpa2
title: 'SICL-AT: Another way to adapt Auditory LLM to low-resource task'
arxiv_id: '2601.18904'
source_url: https://arxiv.org/abs/2601.18904
tags:
- sub-category
- speech
- data
- audio
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Speech In-Context Learning Adaptation Training
  (SICL-AT), a post-training approach that enhances auditory Large Language Models'
  (LLMs) in-context learning capability using only high-resource speech data. The
  method trains models to condition on audio demonstrations, enabling adaptation to
  low-resource tasks like child's speech recognition and audio understanding without
  direct fine-tuning on target data.
---

# SICL-AT: Another way to adapt Auditory LLM to low-resource task

## Quick Facts
- arXiv ID: 2601.18904
- Source URL: https://arxiv.org/abs/2601.18904
- Authors: Haolong Zheng; Siyin Wang; Zengrui Jin; Mark Hasegawa-Johnson
- Reference count: 12
- Primary result: SICL-AT enhances auditory LLM in-context learning using only high-resource data, outperforming direct fine-tuning on low-resource tasks like child's speech recognition and audio understanding.

## Executive Summary
This paper introduces Speech In-Context Learning Adaptation Training (SICL-AT), a post-training approach that enhances auditory Large Language Models' (LLMs) in-context learning capability using only high-resource speech data. The method trains models to condition on audio demonstrations, enabling adaptation to low-resource tasks like child's speech recognition and audio understanding without direct fine-tuning on target data. Across two model families, SICL-AT consistently outperforms direct fine-tuning in low-resource scenarios, with improvements transferring to both speech and audio reasoning tasks. The approach demonstrates that strengthening ICL adaptation through high-resource data can generalize to diverse low-resource applications.

## Method Summary
SICL-AT employs episodic training where models learn to condition predictions on demonstration-context pairs. During training, the method samples a query instance and retrieves k demonstrations from a pool, optimizing the conditional likelihood P(y_query | x_1, y_1, ..., x_k, y_k, x_query). Only LoRA adapters (rank=8, alpha=32) are updated during training. The approach uses TICL-based retrieval for demonstrations and applies to both Qwen2.5-Omni and MiMo-Audio model families. Training data includes CommonVoice English ASR, CoVoST2 speech translation, and MMSU speech question answering, while evaluation covers child's speech recognition (MyST/RSR), audio understanding (MMAU), and multilingual speech tasks.

## Key Results
- SICL-AT consistently outperforms direct fine-tuning on low-resource child's speech recognition tasks
- Improvements transfer from speech tasks (ASR/ST) to audio reasoning tasks (MMAU/MMAR) despite task differences
- The method achieves robust performance even when training and evaluation data distributions mismatch
- SICL-AT1 (ASR-only training) improves child's ASR and multilingual ASR; SICL-AT2 (ASR+ST) further improves audio understanding/reasoning

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Training models in an episodic, demonstration-conditioned format strengthens their ability to exploit in-context examples at inference time.
- **Mechanism:** During SICL-AT, each training step samples a query instance and retrieves k demonstrations from a pool, optimizing the conditional likelihood P(y_query | x_1, y_1, ..., x_k, y_k, x_query). This explicit training on demonstration-conditioned prediction aligns the model's learning objective with its inference behavior.
- **Core assumption:** The model learns a generalizable "how to use demonstrations" capability rather than merely memorizing task-specific patterns.
- **Evidence anchors:** [abstract] "post-training recipe utilizes only high resource speech data intending to strengthen model's in-context learning capability"; [Section 2.1] "SICL-AT uses an episodic training format that mirrors inference-time in-context learning."

### Mechanism 2
- **Claim:** ICL adaptation capability trained on high-resource speech tasks transfers to structurally different low-resource tasks.
- **Mechanism:** SICL-AT1 (ASR-only training) improves child's ASR and multilingual ASR; SICL-AT2 (ASR + ST) further improves audio understanding/reasoning (MMAU/MMAR) despite neither ASR nor ST being AU/AR tasks. This suggests the learned "context utilization" skill is partially task-agnostic.
- **Core assumption:** The model learns meta-level patterns for leveraging contextual cues that apply across task types, not just task-specific adaptation.
- **Evidence anchors:** [abstract] "The enhancement can generalize to audio understanding/reasoning task"; [Section 3.2] "AU/AR performance further increases for both models which is interesting because neither ASR nor ST task are overlap task as AU/AR."

### Mechanism 3
- **Claim:** Inference-time demonstration conditioning is more robust than direct fine-tuning when in-domain data is scarce and distributionally mismatched.
- **Mechanism:** Direct fine-tuning on limited data can overfit to an unrepresentative training distribution, causing degradation on test data from a different distribution (e.g., fine-tuning on RSR hurts MyST). ICL allows the model to adapt using available in-domain examples at test time without gradient updates, avoiding overfitting.
- **Core assumption:** The retrieved demonstrations at inference time are sufficiently representative of the test distribution.
- **Evidence anchors:** [abstract] "In case of labeled in-domain data is scarce or mismatched to the true test distribution, direct fine-tuning can be brittle"; [Section 4] "the degradation on the MyST test split further confirm the harmfulness of distribution mismatch."

## Foundational Learning

- **Concept: In-Context Learning (ICL)**
  - Why needed here: SICL-AT builds on the observation that auditory LLMs can already leverage demonstrations; the method strengthens this existing capability.
  - Quick check question: Can you explain why ICL requires no gradient updates at inference time?

- **Concept: Episodic/Meta-Training**
  - Why needed here: SICL-AT's training format samples task episodes with demonstration contexts, different from standard supervised training.
  - Quick check question: How does episodic training differ from conventional batch training on (input, output) pairs?

- **Concept: LoRA (Low-Rank Adaptation)**
  - Why needed here: The paper uses LoRA (rank=8, alpha=32) for parameter-efficient adaptation to avoid overfitting.
  - Quick check question: Why might full fine-tuning be problematic when training data is limited?

## Architecture Onboarding

- **Component map:** Auditory LLM backbone (Qwen2.5-Omni or MiMo-Audio) -> LoRA adapters (rank=8, alpha=32) -> Demonstration retriever (TICL-based retrieval from training pools) -> Training data: CommonVoice (ASR), CoV oST2 (ST), MMSU (SQA)

- **Critical path:**
  1. Sample task c from training tasks
  2. Sample query (x_query, y_query) from D_query^(c)
  3. Retrieve k demonstrations from D_pool^(c)
  4. Construct prompt: [demos] + [query]
  5. Compute loss on P(y_query | context)
  6. Update LoRA parameters only

- **Design tradeoffs:**
  - SICL-AT1 vs SICL-AT2 vs SICL-AT3: Adding more task types (ST, SQA) improves target tasks but may cause slight degradation on others due to task interference
  - Retrieval method: Paper uses TICL (text-embedding KNN); quality directly impacts both training and inference
  - LoRA rank choice: Low rank (8) prevents overfitting but may limit capacity for complex adaptation

- **Failure signatures:**
  - Fine-tuning on narrow domain data causing degradation on related but different distributions (RSR â†’ MyST)
  - SICL-AT3 showing slight ASR/ST degradation when SQA added (task format mismatch)
  - Vanilla ICL without SICL-AT showing inconsistent gains on some benchmarks

- **First 3 experiments:**
  1. **Baseline establishment:** Run zero-shot and Vanilla ICL on target tasks (MyST, RSR, MMAU, MMAR) to confirm existing ICL capability.
  2. **SICL-AT1 training:** Train with ASR data only (CommonVoice-en), evaluate on all target tasks to measure ASR-focused ICL improvement.
  3. **Comparison to direct fine-tuning:** Fine-tune on RSR training split, compare to SICL-AT on both RSR and MyST to validate robustness under distribution shift.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can a principled training data mixture be designed for SICL-AT to maximize gains across diverse downstream tasks without causing skill degradation (e.g., the observed trade-off between ASR and Audio Reasoning)?
- **Basis in paper:** [Explicit] The authors note that adding Speech Question Answering (SQA) data (SICL-AT3) yields gains in Audio Reasoning but causes "slight degradations on ASR/ST," stating this "motivates more principled mixture design when targeting specific capabilities."

### Open Question 2
- **Question:** How robust is SICL-AT when the retrieval mechanism fails or when representative in-domain demonstrations are unavailable in truly data-scarce deployments?
- **Basis in paper:** [Inferred] The Limitations section states that "SICL performance depends on retrieval quality and the availability of representative examples, which may be limited in truly data-scarce deployments."

### Open Question 3
- **Question:** How does the inference cost and latency of SICL-AT scale with increasing context lengths (number of demonstrations), and how does this compare to the computational cost of parameter updates?
- **Basis in paper:** [Explicit] The authors explicitly list in the Limitations that they "do not fully characterize the inference-cost scaling with longer contexts."

## Limitations
- The method's effectiveness depends heavily on retrieval quality, which may be limited in truly data-scarce deployments
- Cross-task interference observed when adding diverse task types to training (SQA addition slightly degraded ASR/ST performance)
- The approach doesn't fully characterize inference cost scaling with longer context lengths

## Confidence

**High Confidence:** SICL-AT improves low-resource task performance compared to direct fine-tuning, supported by experiments across two model families and multiple task types.

**Medium Confidence:** ICL adaptation capability transfers from high-resource speech tasks to audio reasoning tasks, though the underlying mechanism for cross-task transfer remains speculative.

**Low Confidence:** SICL-AT represents a fundamentally different and superior approach to low-resource adaptation compared to other parameter-efficient methods, lacking comprehensive comparison.

## Next Checks

1. **Ablation study on retrieval quality:** Systematically vary TICL retrieval quality to quantify its impact on both training effectiveness and inference performance, particularly for tasks with limited in-domain data.

2. **Cross-task transfer boundary analysis:** Test SICL-AT trained on ASR/ST/SQA against a wider range of low-resource audio tasks to map the limits of ICL adaptation transfer and identify which task characteristics enable or hinder generalization.

3. **Comparison with alternative parameter-efficient adaptation:** Implement and compare SICL-AT against other low-resource adaptation approaches like prompt tuning, prefix tuning, or adapter fusion to establish whether the episodic ICL format provides unique advantages beyond parameter efficiency.