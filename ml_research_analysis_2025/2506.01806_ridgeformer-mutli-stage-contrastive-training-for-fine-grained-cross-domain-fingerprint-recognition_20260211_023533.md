---
ver: rpa2
title: 'Ridgeformer: Mutli-Stage Contrastive Training For Fine-grained Cross-Domain
  Fingerprint Recognition'
arxiv_id: '2506.01806'
source_url: https://arxiv.org/abs/2506.01806
tags:
- fingerprint
- contactless
- matching
- dataset
- global
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Ridgeformer, a multi-stage transformer-based
  approach for contactless fingerprint recognition that addresses challenges like
  out-of-focus images, reduced ridge-valley contrast, and perspective distortion.
  The method uses a Vision Transformer to extract global spatial features, followed
  by a fine-grained local alignment stage with cross-attention to refine matching
  between contactless and contact-based fingerprints.
---

# Ridgeformer: Mutli-Stage Contrastive Training For Fine-grained Cross-Domain Fingerprint Recognition

## Quick Facts
- arXiv ID: 2506.01806
- Source URL: https://arxiv.org/abs/2506.01806
- Reference count: 0
- EER of 2.83% on HKPolyU for contactless-to-contact matching

## Executive Summary
Ridgeformer addresses the challenge of matching contactless and contact-based fingerprint images through a novel multi-stage transformer architecture. The method captures global spatial features using a Vision Transformer, then refines matching through cross-attention between token embeddings from both domains. This two-stage approach effectively handles the perspective distortion, reduced ridge-valley contrast, and out-of-focus issues inherent in contactless fingerprint capture.

The system achieves state-of-the-art performance on standard benchmarks, outperforming existing methods including commercial solutions like Verifinger. With an EER of 2.83% and TAR@FAR=0.01 of 89.34% on the HKPolyU dataset, Ridgeformer demonstrates robust cross-domain matching capabilities while maintaining reasonable computational efficiency through its staged approach.

## Method Summary
Ridgeformer employs a two-stage transformer architecture for cross-domain fingerprint recognition. Stage 1 uses a Vision Transformer to extract global spatial features from fingerprint images, converting them into patch tokens and generating global embeddings through self-attention and pooling. Stage 2 refines these features through cross-attention between token embeddings from contactless and contact-based fingerprints, preserving fine-grained ridge details lost in global pooling. The system is trained using multi-similarity loss computed across all domain pairings (CL2CL, CL2CB, CB2CB) with hard positive/negative mining. The model is first pre-trained on combined datasets (HKPolyU, RidgeBase, ISPFDv1/v2) then fine-tuned on target datasets with adjusted hyperparameters.

## Key Results
- EER of 2.83% and TAR@FAR=0.01 of 89.34% on HKPolyU dataset for contactless-to-contact matching
- Outperforms commercial solutions like Verifinger across most metrics while maintaining competitive performance
- Ablation study demonstrates each stage contributes to improved accuracy, with final fine-tuned model achieving best results
- Robust performance across multiple datasets including RidgeBase and ISPFD series with varying sensor characteristics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hierarchical feature extraction with global-to-local refinement improves cross-domain fingerprint matching accuracy
- Mechanism: Stage 1 uses Vision Transformer to capture global spatial relationships through self-attention over image patches; Stage 2 refines via cross-attention between token embeddings from both domains, preserving fine-grained ridge details lost in pooled representations
- Core assumption: Fingerprint identity information exists at both global (overall pattern) and local (ridge minutiae) scales, and these require separate alignment procedures
- Evidence anchors: [abstract] states "first captures global spatial features and subsequently refines localized feature alignment across fingerprint samples"; [section 3.2.2] explains how token-level representations preserve details lost in global extraction
- Break condition: If global features alone provide sufficient discrimination, Stage 2 overhead may not justify marginal gains

### Mechanism 2
- Claim: Cross-attention between contactless and contact-based token embeddings enables domain-invariant alignment
- Mechanism: Token embeddings F_i_V and F_j_Q from Stage 1 are processed through cross-attention transformer, generating attended representations where each domain's features attend to the other, then pooled for similarity computation via cosine distance
- Core assumption: Cross-domain correspondence exists at patch-token level despite sensor-induced distortions (perspective, contrast, focus)
- Evidence anchors: [abstract] mentions "fine-grained local alignment stage with cross-attention to refine matching between contactless and contact-based fingerprints"; [section 3.2.2] details the cross-attention transformer operation
- Break condition: If domain shift is too severe, token-level correspondence breaks down

### Mechanism 3
- Claim: Multi-similarity loss computed across all domain pairings enforces unified latent space
- Mechanism: Hard positive/negative mining based on similarity matrix; scaled log-sum-exp loss with separate positive (αpos=2.0) and negative (αneg=40.0) scales; summed across three similarity matrices for joint optimization
- Core assumption: A shared embedding space can simultaneously satisfy intra-class compactness and inter-class separation across heterogeneous sensor modalities
- Evidence anchors: [section 3.3] describes loss computation across CL2CL, CL2CB, CB2CB matrices; [table 3] shows ablation demonstrating Stage 2 improvement from 84.16% to 86.16% TAR
- Break condition: If class imbalance or domain imbalance is extreme, hard mining may overfit to dominant domain pairs

## Foundational Learning

- Concept: **Vision Transformer (ViT) tokenization and self-attention**
  - Why needed here: Understanding how images become patch tokens and how attention captures long-range ridge dependencies
  - Quick check question: Can you explain how a 224×224 fingerprint image becomes T patch tokens, and what self-attention computes between them?

- Concept: **Cross-attention vs. self-attention in transformers**
  - Why needed here: Stage 2 uses cross-attention to align features from different domains; distinguishing this from self-attention is critical
  - Quick check question: Given two token sequences A and B, what does cross-attention compute that self-attention does not?

- Concept: **Deep metric learning with hard sample mining**
  - Why needed here: Multi-similarity loss relies on selecting informative positive/negative pairs; understanding mining strategy is essential for debugging convergence
  - Quick check question: In a batch of 60 samples, how would you identify "hard positives" and "hard negatives" given a similarity threshold τ=0.5 and margin=0.7?

## Architecture Onboarding

- Component map: Input fingerprint image → Stage 1 ViT backbone → T patch tokens (d-dim) → GAP → MLP → global embedding v → Stage 2 CrossAttention → attended tokens → GAP → refined embeddings v', q' → cosine similarity → Multi-similarity loss

- Critical path:
  1. Preprocessing (segmentation, alignment using SAM+CLIP for ISPFD datasets)
  2. Stage 1 training (50 epochs, lr=10⁻⁵, batch=60, combined datasets)
  3. Stage 2 training (cross-attention module, lr=10⁻⁵, batch=30)
  4. Fine-tuning on target dataset (HKPolyU, reduced margin/threshold)

- Design tradeoffs:
  - **Two-stage vs. end-to-end**: Separate stages allow modular debugging but require careful alignment of training objectives
  - **ViT vs. CNN backbone**: ViT captures global context better but requires more data and compute; paper does not ablate this choice
  - **Hard mining thresholds**: Aggressive mining (margin=0.7) may discard useful samples; paper tunes empirically
  - **Assumption**: The paper assumes access to paired contactless-contact training data; performance may degrade with unpaired data

- Failure signatures:
  - High EER (>10%) on CL2CB but low EER on CL2CL: Domain alignment failure, check Stage 2 cross-attention convergence
  - Training loss plateaus early: Hard mining may be too aggressive; reduce margin or increase τ
  - Poor generalization to new sensors: Overfitting to training sensor characteristics; increase data augmentation or reduce model capacity
  - Rank-1 recall significantly lower than TAR@FAR=0.01: Gallery embedding quality issue; verify Stage 1 features are discriminative

- First 3 experiments:
  1. **Baseline reproduction**: Train Stage 1 only on HKPolyU training split, evaluate on test split; target EER <4% (per ablation table showing 3.74%)
  2. **Cross-attention validation**: Visualize attention maps from Stage 2 cross-attention module; verify attention focuses on corresponding ridge regions across domains rather than background
  3. **Loss component ablation**: Train with L_cl2cb only vs. full L_G = L_cl2cl + L_cl2cb + L_cb2cb; quantify contribution of each domain pairing to final TAR@FAR=0.01

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the computational latency of the two-stage Ridgeformer architecture compare to single-stage CNN baselines during high-throughput inference?
- Basis: [inferred] The proposed method utilizes a Vision Transformer backbone followed by a secondary cross-attention stage, which typically incurs higher computational costs than single-pass CNN feature extractors, yet no runtime analysis is provided
- Why unresolved: The evaluation focuses exclusively on matching accuracy metrics (EER, TAR) without reporting frame rates or processing time per image pair
- Evidence: A comparative table of inference time (ms/image) and FLOPs against methods like MANet or Verifinger on standard hardware

### Open Question 2
- Question: Can Ridgeformer maintain its robustness without the reliance on complex external preprocessing segmentation pipelines like SAM and CLIP?
- Basis: [inferred] The implementation details note the use of the "Segment Anything Model" and CLIP to isolate fingers in the ISPFD datasets, suggesting the model's input is heavily curated rather than raw
- Why unresolved: It is unclear if the transformer's performance is due to the feature extraction or the quality of the upstream segmentation, limiting its applicability in "in-the-wild" scenarios where segmentation might fail
- Evidence: An ablation study evaluating performance on unsegmented or automatically cropped images versus the current pipeline

### Open Question 3
- Question: What factors contribute to the regression in Rank-1 identification performance for the contactless-to-contact (CL2CB) protocol on the RidgeBase dataset compared to commercial baselines?
- Basis: [inferred] Table 2 shows Ridgeformer achieves 69.90% Rank-1 accuracy on RidgeBase CL2CB, which is lower than the Verifinger baseline (72.50%), whereas it outperforms Verifinger in all other reported metrics
- Why unresolved: The paper highlights overall state-of-the-art results but does not analyze the specific failure modes causing the lower ranking performance in this isolated evaluation protocol
- Evidence: A qualitative error analysis of the false negatives specific to the RidgeBase CL2CB identification task

## Limitations
- Architectural Specification Gaps: Critical implementation details including ViT patch size, embedding dimensions, cross-attention depth, and MLP architecture are omitted
- Dataset Composition Ambiguity: Exact subject counts, sensor specifications, and preprocessing details vary across datasets; SAM+CLIP segmentation may introduce variability
- Cross-Dataset Generalization: Limited validation on RidgeBase (only 25 subjects) with performance claims lacking statistical significance testing

## Confidence
- **High Confidence**: Multi-stage architecture design and overall training pipeline (Vision Transformer → cross-attention → multi-similarity loss)
- **Medium Confidence**: Quantitative results on HKPolyU (EER=2.83%, TAR@FAR=0.01=89.34%) due to detailed ablation and comparison to baselines
- **Low Confidence**: RidgeBase performance claims and cross-dataset generalization due to limited evaluation subjects and lack of statistical validation

## Next Checks
1. **Parameter Sensitivity Analysis**: Systematically vary ViT patch size, cross-attention depth, and hard mining thresholds to identify optimal configurations and verify robustness of claimed performance

2. **Cross-Dataset Transfer Study**: Train on HKPolyU, evaluate on both RidgeBase and ISPFDv1/v2 to quantify generalization across different sensor types and acquisition conditions

3. **Ablation with Statistical Testing**: Conduct paired t-tests comparing Ridgeformer variants on HKPolyU to establish statistical significance of performance improvements, particularly for RidgeBase results with n=25 subjects