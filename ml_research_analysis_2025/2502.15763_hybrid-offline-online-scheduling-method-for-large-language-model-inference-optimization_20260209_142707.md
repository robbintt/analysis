---
ver: rpa2
title: Hybrid Offline-online Scheduling Method for Large Language Model Inference
  Optimization
arxiv_id: '2502.15763'
source_url: https://arxiv.org/abs/2502.15763
tags:
- inference
- scheduling
- prefill
- time
- decode
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of optimizing large language model
  (LLM) inference systems to improve hardware utilization and throughput. The authors
  formulate this as a mixed-integer programming problem and propose a hybrid offline-online
  scheduling method.
---

# Hybrid Offline-online Scheduling Method for Large Language Model Inference Optimization

## Quick Facts
- **arXiv ID**: 2502.15763
- **Source URL**: https://arxiv.org/abs/2502.15763
- **Reference count**: 37
- **Primary result**: Improved system utilization from 80.2% to 89.1% and reduced total inference time from 201.00 to 190.58 seconds for LLaMA-65B inference

## Executive Summary
This paper addresses the problem of optimizing large language model (LLM) inference systems to improve hardware utilization and throughput. The authors formulate this as a mixed-integer programming problem and propose a hybrid offline-online scheduling method. The offline component uses a minimizing makespan bin packing approach to balance workloads across clients, while the online component employs sorting and preemptive scheduling combined with Lagrangian-based heuristics for real-time decisions. Experiments with the LLaMA-65B model and GSM8K dataset demonstrate significant improvements in system utilization and total inference time reduction, with decisions provided within 5 milliseconds to meet real-time requirements.

## Method Summary
The method combines offline workload balancing with online dynamic scheduling. The offline component formulates request-to-client assignment as a Minimizing Makespan Bin Packing Problem (Mixed-Integer Programming) to distribute requests evenly and minimize maximum client load. The online component uses sorting and preemptive scheduling where idle clients dynamically steal work from busy clients' queues, combined with Lagrangian-based iteration scheduling that decides between prefill and decode stages at each step. The hybrid approach achieves sub-5ms decision latency while improving hardware utilization and throughput.

## Key Results
- System utilization improved from 80.2% to 89.1%
- Total inference time reduced from 201.00 to 190.58 seconds
- Decision latency within 5 milliseconds for real-time requirements
- Average 8.0% increase in utilization rate across 100-case study

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Offline workload balancing via minimizing makespan bin packing reduces client idle time by distributing requests more evenly.
- Mechanism: Treats each client as a "bin" with limited capacity, assigning requests to minimize maximum load across all clients.
- Core assumption: Request processing times can be estimated with sufficient accuracy beforehand.
- Evidence anchors: Abstract mentions "Minimizing Makespan Bin Packing Problem"; Section IV.B details the MIP formulation.

### Mechanism 2
- Claim: Online sorting and preemptive request scheduling improves utilization by having idle clients dynamically take work from the longest queues.
- Mechanism: Idle clients identify the client with most remaining work and process requests from that busy client's queue.
- Core assumption: Requests can be dynamically reassigned without significant overhead.
- Evidence anchors: Abstract mentions "sorting and preemptive scheduling"; Algorithm 1 in Section IV.C describes the greedy selection.

### Mechanism 3
- Claim: Lagrangian-based iteration scheduling reduces hardware idle time by dynamically interleaving prefill and decode stages.
- Mechanism: Decides at each iteration whether to run prefill or decode by comparing cost of inserting prefill versus waiting for decode tasks.
- Core assumption: Cost derivatives can be accurately estimated from current system state.
- Evidence anchors: Abstract mentions "Lagrangian method"; Section IV.C describes comparing Cp (prefill cost) vs Cd (waited decode time).

## Foundational Learning

- **Concept**: Prefill vs. Decode Stages in LLM Inference
  - Why needed here: Core mechanism depends on scheduling between these distinct phases
  - Quick check question: Which stage is memory-bandwidth bound and generates the KV cache, and which is compute-bound and generates one token at a time?

- **Concept**: Mixed-Integer Programming (MIP)
  - Why needed here: Offline component uses MIP to formulate optimization problem
  - Quick check question: Why can't a simple greedy algorithm guarantee an optimal solution for the offline makespan problem?

- **Concept**: Batching and Continuous Batching
  - Why needed here: System operates on batches; continuous batching enables dynamic insertion and preemption
  - Quick check question: How does continuous batching differ from traditional static batching in handling incoming requests?

## Architecture Onboarding

- **Component map**: Offline Scheduler (MIP model, bin-packing) -> Online Request Scheduler (preemptive logic) -> Online Iteration Scheduler (Lagrangian heuristic)
- **Critical path**: Online iteration scheduling decision, occurring every ~50ms, must execute within 5-10ms
- **Design tradeoffs**: Trades optimality of full MIP for speed of hybrid heuristic; Lagrangian heuristic provides sub-5ms decisions but may not find global optimum
- **Failure signatures**: (1) Increasing client idle time if cost heuristic is miscalibrated, (2) Rising latency if preemption is too aggressive, (3) Out-of-memory errors if offline assignment overestimates capacity
- **First 3 experiments**:
  1. Baseline profiling: Measure baseline utilization, total inference time, and "bubble" percentage without new scheduling logic
  2. Offline-only test: Implement MIP-based bin-packing for request assignment; measure change in makespan and client workload balance
  3. Hybrid online-offline test: Add online sorting/preemption and iteration scheduling; measure final utilization (target: 89.1%) and total inference time reduction

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a stochastic programming model outperform the deterministic MIP formulation given uncertainty of output token lengths?
- Basis in paper: Section VI lists "Stochastic Model & Efficient Solution Method" as future direction
- Why unresolved: Current formulation treats problem as deterministic equivalence, whereas actual generation is highly variable
- What evidence would resolve it: Comparative simulations showing stochastic formulation reduces total inference time under variable workloads

### Open Question 2
- Question: Can reinforcement learning agents effectively replace Lagrangian-based heuristics for real-time iteration scheduling decisions?
- Basis in paper: Section VI suggests "Reinforcement Learning on Iteration Scheduling" as future extension
- Why unresolved: Current approach relies on heuristic comparing Cp and Cd, which may not capture complex state dynamics as effectively as learned policy
- What evidence would resolve it: Implementation of RL scheduler demonstrating higher utilization rates than Lagrangian heuristic

### Open Question 3
- Question: How can online decision-making optimize dynamic allocation of concurrent clients to maximize hardware utilization?
- Basis in paper: Section VI identifies "Online Hardware Utilization Method" as necessary extension
- Why unresolved: Current method fixes number of clients based on KV cache limits, but stochastic output lengths lead to idle hardware
- What evidence would resolve it: Dynamic batching algorithm adjusting active clients in real-time and demonstrating higher throughput

## Limitations

- Prefill/Decode stage granularity assumptions may not hold due to variable execution times from hardware-specific optimizations
- Model capacity estimation accuracy depends on estimation method for T_i values, which isn't specified
- KV cache transferability constraint may limit real-world implementability of online preemptive scheduling

## Confidence

- **High Confidence**: MIP formulation for offline makespan minimization and online sorting/preemptive logic are clearly specified with established scheduling principles
- **Medium Confidence**: Lagrangian heuristic for iteration scheduling is conceptually sound but relies on simplifying assumptions about cost estimation
- **Low Confidence**: Scalability beyond 200 clients remains uncertain; computational complexity at larger scales (>1000 clients) is unclear

## Next Checks

1. **Cost Model Validation**: Implement Lagrangian cost comparison in controlled simulator with known token distributions; verify heuristic correctly identifies when prefill insertion reduces makespan across various workload patterns

2. **Capacity Estimation Accuracy**: Test offline assignment with different estimation methods for T_i values; compare performance when using perfect knowledge versus statistical estimates, and measure how estimation errors propagate

3. **KV Cache Transfer Feasibility**: Evaluate whether online preemptive scheduling is implementable given KV cache constraints; measure overhead of transferring or reconstructing KV caches when requests are reassigned, and assess impact on latency benefits