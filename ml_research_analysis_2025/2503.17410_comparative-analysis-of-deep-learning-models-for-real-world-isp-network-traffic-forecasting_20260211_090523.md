---
ver: rpa2
title: Comparative Analysis of Deep Learning Models for Real-World ISP Network Traffic
  Forecasting
arxiv_id: '2503.17410'
source_url: https://arxiv.org/abs/2503.17410
tags:
- forecasting
- network
- time
- traffic
- series
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents a comprehensive evaluation of seven state-of-the-art
  deep learning models for real-world network traffic forecasting using the CESNET-TimeSeries24
  dataset from an ISP network. The research addresses key challenges in network traffic
  prediction, including non-linear patterns, missing data, and varying aggregation
  levels across institutions, subnets, and individual IP addresses.
---

# Comparative Analysis of Deep Learning Models for Real-World ISP Network Traffic Forecasting

## Quick Facts
- arXiv ID: 2503.17410
- Source URL: https://arxiv.org/abs/2503.17410
- Reference count: 31
- Primary result: GRU-FCN achieves best balance of accuracy and efficiency for ISP network traffic forecasting

## Executive Summary
This study systematically evaluates seven state-of-the-art deep learning models for network traffic forecasting using real ISP data. The research addresses key challenges including non-linear patterns, missing data, and varying aggregation levels across institutions, subnets, and individual IP addresses. Through comprehensive experimentation with different training windows, prediction horizons, and granularity levels, the study identifies GRU-FCN as the optimal architecture, outperforming more complex models while maintaining computational efficiency.

## Method Summary
The study uses the CESNET-TimeSeries24 dataset containing 40 weeks of 1-hour aggregated network traffic data from an ISP network. Seven deep learning models (GRU, LSTM, GRU-FCN, LSTM-FCN, InceptionTime, ResNet, RCLSTM) are evaluated across three aggregation levels (institutions, subnets, IP addresses) and multiple time series metrics. Models are trained using sliding window approaches with various training/prediction window combinations, optimized with Adam and MSELoss. Performance is measured using RMSE, R2-score, and Harmonic-Score across univariate forecasting tasks.

## Key Results
- GRU-FCN models achieve the best balance between accuracy and computational efficiency across most settings
- No single model consistently dominates across all configurations; performance varies with aggregation level and prediction horizon
- Forecasting accuracy decreases significantly with longer prediction horizons due to error accumulation
- Higher aggregation levels (institutions vs. individual IPs) yield better prediction performance due to reduced data variability
- Complex architectures like InceptionTime and ResNet underperform compared to simpler RNN-based models for this temporal prediction task

## Why This Works (Mechanism)
The GRU-FCN architecture combines the temporal learning capabilities of recurrent units with convolutional feature extraction, making it particularly suited for network traffic data that exhibits both temporal dependencies and local pattern variations. The sliding window approach enables models to learn from historical patterns while maintaining computational efficiency. By evaluating across different aggregation levels, the study demonstrates how data granularity directly impacts forecasting difficulty and model performance.

## Foundational Learning
- **Sliding Window (Look-back Window) for Time Series Forecasting**: The entire methodology is built on this approach where models see fixed-length windows of past observations to predict fixed-length future horizons. Quick check: If you have 1000 points with training window 168 and prediction window 24, you can create 977 independent training samples.

- **Gated Recurrent Units (GRU) vs. Long Short-Term Memory (LSTM)**: The paper concludes GRU and LSTM are top performers, with GRU-FCN being most efficient. The primary structural difference is that GRU uses a single update gate while LSTM has separate forget and input gates, making GRU computationally lighter.

- **Missing Data Imputation in Real-World Time Series**: The paper highlights missing data as a key challenge, especially at IP level where authors use zero-filling. A zero value represents missing data after imputation, but this simple strategy may bias model training by introducing artificial patterns.

## Architecture Onboarding
- **Component map**: CESNET-TimeSeries24 Dataset -> Pre-processing (Fill missing values with 0) -> Train/Val/Test Split -> MinMax Scaler -> Sliding Window Generator -> Core DL Architecture -> Dense Output Layer
- **Critical path**: 1) Correctly apply sliding window to generate (X, y) pairs with input length = training_window and target length = prediction_window. 2) Start with GRU-FCN as primary candidate. 3) Use provided hyperparameters as starting point. 4) Train with Adam optimizer and MSELoss, monitoring validation performance.
- **Design tradeoffs**: GRU-FCN vs GRU (convolutional layers vs simplicity), training window size (24 vs 744 for recent trends vs context), aggregation level (institution vs IP for accuracy vs granularity).
- **Failure signatures**: Model struggles on IP-level data (expected poor performance due to missing data), performance crash on long prediction horizons (expected error accumulation), InceptionTime/ResNet underperformance (ill-suited for temporal dependencies).
- **First 3 experiments**: 1) Implement GRU-FCN on Institutions dataset for n_bytes with training_window=168 and prediction_window=1, compare RMSE and R2 to Table values. 2) Evaluate trained GRU-FCN directly on Institution subnets and IP address datasets to observe performance degradation. 3) Train simple GRU on Institutions n_bytes with training_window=24 vs training_window=744, both with prediction_window=1, to test smaller windows performing better.

## Open Questions the Paper Calls Out
- **Universal multi-time-series model**: Can a single model handle different time-series instances within the same aggregation level without retraining? The current methodology relies on separate models per series.
- **Univariate vs multivariate forecasting trade-offs**: What are the performance and scalability trade-offs between modeling metrics separately versus jointly? The study modeled metrics separately, leaving inter-metric dependencies untested.
- **Interpretability for network operations**: How can deep learning predictions be made more interpretable for network resource allocation and anomaly detection? The study focused on predictive performance rather than explainability.

## Limitations
- Limited to zero-filling for missing data, which may bias training and underestimate performance with better imputation
- Tested architectures constrained to established deep learning models; emerging approaches like Transformers not evaluated
- Results specific to CESNET-TimeSeries24 dataset and ISP context; generalizability to other network environments requires validation

## Confidence
- **High confidence**: GRU-FCN performance superiority claims (based on systematic evaluation across multiple settings)
- **Medium confidence**: Universality of findings across different network operators (limited to single ISP dataset)
- **Low confidence**: Long-term prediction horizon recommendations (168-hour forecasts show poor accuracy, but methodology limitations may contribute)

## Next Checks
1. Implement and test multiple missing data imputation strategies (linear interpolation, forward-fill, mean imputation) to quantify impact on IP-level forecasting performance
2. Replicate experiments with a different ISP dataset to validate findings across network operators and traffic patterns
3. Evaluate additional model architectures (Transformers, State Space Models) to determine if emerging approaches outperform tested models for long-term forecasting