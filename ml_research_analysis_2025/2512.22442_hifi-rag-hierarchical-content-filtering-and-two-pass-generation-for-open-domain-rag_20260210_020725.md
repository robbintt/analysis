---
ver: rpa2
title: 'HiFi-RAG: Hierarchical Content Filtering and Two-Pass Generation for Open-Domain
  RAG'
arxiv_id: '2512.22442'
source_url: https://arxiv.org/abs/2512.22442
tags:
- gemini
- filtering
- baseline
- query
- search
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: HiFi-RAG introduces a hierarchical filtering pipeline to improve
  retrieval-augmented generation in open-domain settings. The system uses Gemini 2.5
  Flash for query formulation, URL filtering, and hierarchical content parsing, followed
  by Gemini 2.5 Pro for two-pass answer generation.
---

# HiFi-RAG: Hierarchical Content Filtering and Two-Pass Generation for Open-Domain RAG

## Quick Facts
- arXiv ID: 2512.22442
- Source URL: https://arxiv.org/abs/2512.22442
- Authors: Cattalyya Nuengsigkapian
- Reference count: 13
- Key outcome: Hierarchical filtering pipeline improves ROUGE-L by 19.6% and DeBERTaScore by 6.2% on MMU-RAGent validation set

## Executive Summary
HiFi-RAG introduces a hierarchical filtering pipeline to improve retrieval-augmented generation in open-domain settings. The system uses Gemini 2.5 Flash for query formulation, URL filtering, and hierarchical content parsing, followed by Gemini 2.5 Pro for two-pass answer generation. On the MMU-RAGent validation set, HiFi-RAG improved ROUGE-L by 19.6% (to 0.274) and DeBERTaScore by 6.2% (to 0.677) over baseline. On Test2025, a dataset requiring post-January 2025 knowledge, it outperformed the parametric baseline by 57.4% in ROUGE-L and 14.9% in DeBERTaScore. The hierarchical filtering approach removed 60.5% of irrelevant content and reduced URL counts by 33.5%, enhancing both precision and cost-efficiency.

## Method Summary
HiFi-RAG employs a five-stage pipeline: (1) Gemini 2.5 Flash reformulates queries, (2) Google Search API + URL filtering via Flash, (3) Scrapingdog/Reddit APIs for hierarchical parsing, Flash filters chunks by title+200-char snippet, (4) Gemini 2.5 Pro two-pass generation (draft then refine to style examples), (5) Flash-based citation verification. The system leverages hierarchical content filtering at multiple levels (URL → section/chunk) to reduce noise before final generation, using model cascading to balance cost and reasoning capacity.

## Key Results
- Improved ROUGE-L by 19.6% (to 0.274) and DeBERTaScore by 6.2% (to 0.677) on MMU-RAGent validation set
- Outperformed parametric baseline by 57.4% in ROUGE-L and 14.9% in DeBERTaScore on Test2025 dataset
- Hierarchical filtering removed 60.5% of irrelevant content and reduced URL counts by 33.5%

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Content Filtering Reduces Context Noise
- Claim: Multi-level filtering (URL → section/chunk) improves answer quality by removing irrelevant content before generation, rather than relying solely on embedding-based retrieval.
- Mechanism: Gemini 2.5 Flash evaluates URLs and sections against the user query using only titles and short snippets (first 200 characters), ranking and discarding noise before forwarding to the stronger model.
- Core assumption: LLM-based semantic filtering can distinguish topically related but factually irrelevant content better than embedding similarity alone.
- Evidence anchors:
  - [abstract] "The hierarchical filtering approach removed 60.5% of irrelevant content and reduced URL counts by 33.5%."
  - [section 2.3] "This removes 60.5% of chunks (averaged across 100 queries), resulting in a context window dense with high-quality signals."
  - [corpus] Know3-RAG (arXiv 2505.12662) similarly explores knowledge-aware filtering in RAG, suggesting broader relevance of filtering-focused approaches, though direct comparability is limited.
- Break condition: If upstream retrieval returns no relevant sources, filtering cannot recover signal; performance degrades to baseline.

### Mechanism 2: Model Cascading Balances Cost and Reasoning Capacity
- Claim: Using a cheaper, faster model (Gemini 2.5 Flash) for filtering and a more capable model (Gemini 2.5 Pro) for final generation optimizes cost-efficiency without sacrificing answer quality.
- Mechanism: Flash handles high-volume, lower-complexity tasks (query formulation, URL filtering, section ranking, citation verification); Pro is invoked only for the final two-pass answer generation requiring deeper reasoning.
- Core assumption: The filtering model's precision is sufficient to prevent Pro from receiving garbage context.
- Evidence anchors:
  - [abstract] "We leverage the speed and cost-efficiency of Gemini 2.5 Flash (4-6x cheaper than Pro) for query formulation, hierarchical content filtering, and citation attribution."
  - [section 1] "Similar to multi-stage ML model cascades that utilize low-power signals to gate high-power processing."
  - [corpus] No direct corpus comparison of Flash/Pro cascading was found; mechanism remains system-specific.
- Break condition: If Flash filtering is too aggressive, relevant content may be discarded; if too permissive, Pro receives noisy context and performance degrades.

### Mechanism 3: Two-Pass Generation Separates Factual Synthesis from Style Alignment
- Claim: Decoupling answer drafting (Turn 1) from style refinement (Turn 2) improves semantic alignment with user intent and reference style without sacrificing factuality.
- Mechanism: Turn 1 generates a comprehensive answer from filtered sources; Turn 2 revises the answer to match style/length examples from the validation set.
- Core assumption: Style matching can be applied post-hoc without distorting factual content.
- Evidence anchors:
  - [section 2.4] "We utilize Gemini 2.5 Pro in a two-turn conversation to separate factuality from style."
  - [table 2] Final configuration (RAG w/ Filters + Rephrase + 2-Turn) achieves highest ROUGE-L (0.2739) and DeBERTaScore (0.6772).
  - [corpus] HIRAG (arXiv 2507.05714) explores hierarchical-thought instruction tuning in RAG, but does not directly validate two-pass style separation.
- Break condition: If style examples are poorly matched to query type, refinement may introduce phrasing drift that degrades metric alignment.

## Foundational Learning

- Concept: **Retrieval-Augmented Generation (RAG)**
  - Why needed here: HiFi-RAG is a RAG variant; understanding the baseline problem (irrelevant retrieved context causing hallucination) is essential.
  - Quick check question: Can you explain why "garbage-in, garbage-out" applies to standard RAG pipelines?

- Concept: **LLM Prompt Engineering & Few-Shot Learning**
  - Why needed here: The system relies heavily on carefully designed prompts for query reformulation, filtering, and style matching (see Appendices A.1–A.5).
  - Quick check question: What is the purpose of providing hand-picked QA examples in Turn 2 refinement?

- Concept: **Web Content Parsing & Hierarchical Document Structure**
  - Why needed here: The system parses HTML into sections with parent headers and reconstructs Reddit discussion trees; understanding DOM structure is necessary for implementation.
  - Quick check question: How does hierarchical parsing differ from flat-text chunking?

## Architecture Onboarding

- Component map: Query Formulation (Gemini 2.5 Flash) -> Retrieval & URL Filtering (Google Search API + Gemini 2.5 Flash) -> Hierarchical Content Parsing (Scrapingdog/Reddit API) -> Section Filtering & Ranking (Gemini 2.5 Flash) -> Two-Pass Generation (Gemini 2.5 Pro) -> Post-Hoc Citation Verification (Gemini 2.5 Flash)

- Critical path: Query Formulation → URL Filtering → Hierarchical Parsing → Section Filtering → Two-Pass Generation → Citation Verification. If any filtering stage removes all relevant content, downstream generation fails.

- Design tradeoffs:
  - LLM-based filtering vs. embedding-based filtering (LLM chosen for semantic precision despite higher cost than embeddings).
  - Deterministic pipeline vs. agentic workflow (agentic was 10× more expensive and slower without metric improvement; see Section 4).
  - Style refinement vs. metric stability (LLM-as-Judge improved qualitative quality but degraded ROUGE/DeBERTaScore).

- Failure signatures:
  - Query rephrasing without URL filtering reduces performance due to ambiguity from over-truncated context (Section 3.4).
  - DSPy prompt optimization overfits to validation set and fails to generalize (Section 4).
  - Embedding-based filtering underperforms on topically related but factually irrelevant noise (Section 4).

- First 3 experiments:
  1. Reproduce the URL filtering ablation: measure URL reduction rate and downstream ROUGE-L impact on a held-out sample.
  2. Implement section filtering with and without LLM ranking; compare against a baseline embedding-similarity filter.
  3. Run two-pass generation with and without Turn 2 style refinement; evaluate ROUGE-L and DeBERTaScore to quantify the refinement contribution.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can embedding models be optimized to distinguish topically related but factually irrelevant noise as effectively as LLM-based filtering?
- Basis in paper: [inferred] The authors report in Section 3.4 that Voyage AI embeddings performed worse than LLM filtering, specifically struggling to separate "topically related but factually irrelevant noise."
- Why unresolved: While the authors explicitly demonstrated the failure of current embeddings, they did not propose a fix for the embedding space itself, opting instead to switch to a language model for filtering.
- What evidence would resolve it: A study showing embeddings fine-tuned on factual entailment tasks outperforming generic commercial embeddings within the HiFi-RAG hierarchical filtering stage.

### Open Question 2
- Question: How can automated evaluation metrics be adapted to avoid penalizing style adjustments that qualitatively improve answer factuality?
- Basis in paper: [inferred] Section 3.4 notes that an LLM-as-a-Judge "Checker" module qualitatively improved answers but "degraded automated metrics (ROUGE/DeBERTaScore)."
- Why unresolved: The paper highlights a misalignment between human-preferred (qualitative) answers and the n-gram/semantic similarity metrics used for the competition, leaving the trade-off unresolved.
- What evidence would resolve it: A new evaluation framework or metric that correlates positively with both the "Checker" module's qualitative improvements and the standard competition benchmarks.

### Open Question 3
- Question: Does query rephrasing inherently lose necessary constraints present in verbose user queries, requiring explicit constraint preservation mechanisms?
- Basis in paper: [inferred] Section 3.3 observes that query rephrasing without URL filtering resulted in lower scores, likely due to the "removal of context from verbose queries which can introduce ambiguity."
- Why unresolved: The authors suggest URL filtering mitigates this, but it remains unclear if the rephrasing step itself is fundamentally flawed for complex queries with multiple constraints.
- What evidence would resolve it: An ablation study showing that a constraint-extraction step preceding query rephrasing prevents the observed performance drop on complex queries.

## Limitations
- The exact hand-picked style examples used in Turn 2 refinement are not provided, making faithful replication challenging without sampling from the validation set.
- Test2025 dataset construction relied on Gemini 3.0 Thinking and live web search; the specific 100 pairs and post-processing steps are not disclosed.
- The study does not compare against other recent filtering-focused RAG systems under identical conditions, so claims of superiority are relative to baseline ablations.

## Confidence
- **High Confidence**: Claims about hierarchical filtering removing 60.5% of irrelevant chunks and improving cost-efficiency are directly supported by the ablation study and metric improvements in tables.
- **Medium Confidence**: Claims that LLM-based filtering outperforms embedding-based filtering are supported by the ablation but lack external validation against other semantic filtering approaches.
- **Medium Confidence**: Claims about two-pass generation improving both factuality and style alignment are supported by metric gains, but LLM-as-Judge introduces metric sensitivity that may not generalize to human evaluation.

## Next Checks
1. **URL Filtering Ablation**: Reproduce the exact URL filtering stage and measure the 33.5% reduction in URL counts and its downstream impact on ROUGE-L using a held-out subset of MMU-RAGent.
2. **Section Filtering vs. Embedding Filter**: Implement and compare LLM-based chunk filtering against a sentence-transformers semantic filter on the same 100-sample validation set to quantify precision gains.
3. **Two-Pass Generation Isolation**: Run single-pass and two-pass Pro generation on identical contexts and evaluate the contribution of Turn 2 refinement to both ROUGE-L and DeBERTaScore to confirm the 6.2% improvement.