---
ver: rpa2
title: 'HD-NDEs: Neural Differential Equations for Hallucination Detection in LLMs'
arxiv_id: '2506.00088'
source_url: https://arxiv.org/abs/2506.00088
tags:
- neural
- methods
- cdes
- odes
- sdes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of detecting hallucinations
  in large language models (LLMs), particularly when non-factual information appears
  in the middle or early parts of generated sequences. The authors propose HD-NDEs,
  a novel method that uses neural differential equations (Neural ODEs, CDEs, and SDEs)
  to model the continuous dynamics of token embeddings in the latent space, capturing
  the full trajectory of the generation process rather than just the final token.
---

# HD-NDEs: Neural Differential Equations for Hallucination Detection in LLMs
## Quick Facts
- arXiv ID: 2506.00088
- Source URL: https://arxiv.org/abs/2506.00088
- Reference count: 24
- Primary result: Achieves >14% AUC-ROC improvement on True-False dataset over classification baselines

## Executive Summary
This paper addresses the challenge of detecting hallucinations in large language models (LLMs), particularly when non-factual information appears in the middle or early parts of generated sequences. The authors propose HD-NDEs, a novel method that uses neural differential equations (Neural ODEs, CDEs, and SDEs) to model the continuous dynamics of token embeddings in the latent space, capturing the full trajectory of the generation process rather than just the final token. This approach improves upon classification-based methods like SAPLMA, MIND, and Probe@Exact by leveraging the temporal evolution of hidden states. Experiments across five diverse hallucination datasets and six widely used LLMs show that HD-NDEs significantly outperforms existing methods, achieving over 14% improvement in AUC-ROC on the True-False dataset, with Neural CDEs and Neural SDEs performing best. The method demonstrates strong generalization and robustness, especially in complex cases where subtle differences between factual and non-factual statements make detection difficult.

## Method Summary
HD-NDEs introduces a novel approach to hallucination detection by modeling the continuous dynamics of token embeddings using neural differential equations. Unlike traditional classification methods that analyze only the final hidden state, HD-NDEs captures the entire trajectory of the generation process through Neural ODEs, CDEs, and SDEs. The method first extracts hidden states from LLMs during text generation, then applies a continuous-time model to learn the temporal evolution of these states. This trajectory-based approach allows the detection of subtle changes in embedding dynamics that indicate hallucination, particularly in early or middle positions where classification methods often fail. The framework is trained end-to-end to distinguish between factual and non-factual sequences, with the differential equation solver providing a principled way to model the temporal dependencies in the generation process.

## Key Results
- Achieves over 14% improvement in AUC-ROC on the True-False dataset compared to state-of-the-art classification baselines
- Neural CDEs and Neural SDEs demonstrate superior performance over Neural ODEs across most datasets
- Strong generalization across six different LLMs including GPT-3.5, LLaMA-2, and BERT variants
- Particularly effective at detecting hallucinations in early or middle positions where traditional methods struggle

## Why This Works (Mechanism)
The method works by leveraging the continuous-time modeling capabilities of neural differential equations to capture the full trajectory of token embeddings during generation. While traditional methods only examine the final hidden state (a single snapshot), HD-NDEs analyzes how embeddings evolve over time through the latent space. This temporal perspective reveals subtle patterns in the generation dynamics that indicate factual consistency or hallucination. The differential equation framework provides a mathematically principled way to model these continuous dynamics, allowing the model to detect when the generation process deviates from expected factual trajectories, even when such deviations occur early in the sequence.

## Foundational Learning
**Neural Differential Equations**: Continuous-time models that parameterize the derivative of hidden states using neural networks. Why needed: Traditional discrete-time models cannot capture fine-grained temporal dynamics in token generation. Quick check: Verify that the ODE/CDE/SDE solvers can handle the dimensionality and temporal resolution of LLM hidden states.

**Hidden State Trajectories**: The sequence of intermediate representations produced by LLMs during generation. Why needed: These trajectories contain rich temporal information about the generation process that final states alone cannot capture. Quick check: Confirm that trajectory length and sampling frequency preserve meaningful dynamics without excessive computational overhead.

**Latent Space Dynamics**: The continuous evolution of representations in the embedding space during text generation. Why needed: Factual and hallucinated content exhibit different dynamical patterns in this space. Quick check: Analyze whether the learned dynamics align with semantic coherence metrics in the embedding space.

## Architecture Onboarding
**Component Map**: LLM Hidden States -> ODE/CDE/SDE Solver -> Trajectory Encoder -> Classification Head -> Hallucination Detection
**Critical Path**: The trajectory from initial token embedding through the differential equation solver to the final classification decision is critical, as errors compound through each stage and affect detection accuracy.
**Design Tradeoffs**: The choice between ODE, CDE, and SDE variants involves balancing computational efficiency (ODEs are fastest) against modeling accuracy (SDEs capture stochasticity better). This tradeoff becomes more significant with longer sequences.
**Failure Signatures**: The method may struggle with hallucinations that closely mimic factual patterns in latent space dynamics, or when generation trajectories converge despite semantic differences. Early-stage deviations may also be harder to detect if the model hasn't yet established stable dynamics.
**First Experiments**: 1) Compare detection performance on early vs. late hallucination positions, 2) Test sensitivity to trajectory sampling frequency, 3) Evaluate the impact of different ODE solver tolerances on accuracy and runtime.

## Open Questions the Paper Calls Out
None specified in the source material.

## Limitations
- Evaluation focuses primarily on relatively short sequences, leaving scalability to longer documents unexplored
- Absolute performance levels on challenging datasets suggest room for improvement in real-world deployment scenarios
- Computational overhead of modeling continuous dynamics versus discrete classification is not thoroughly characterized

## Confidence
- Performance superiority over classification baselines: High
- Generalization across LLMs: Medium
- Robustness to early hallucination detection: High

## Next Checks
1. Test HD-NDEs on longer sequences (500+ tokens) to evaluate scalability and identify potential degradation in detection performance
2. Conduct ablation studies comparing the trajectory modeling approach against truncated versions that use only recent token history, to quantify the value of full-sequence modeling
3. Measure inference-time latency and memory requirements across different ODE solvers and discretization schemes to establish practical deployment constraints