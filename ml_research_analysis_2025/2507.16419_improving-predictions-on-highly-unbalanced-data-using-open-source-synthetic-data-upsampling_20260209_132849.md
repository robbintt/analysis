---
ver: rpa2
title: Improving Predictions on Highly Unbalanced Data Using Open Source Synthetic
  Data Upsampling
arxiv_id: '2507.16419'
source_url: https://arxiv.org/abs/2507.16419
tags:
- data
- minority
- synthetic
- upsampling
- sets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study demonstrates that AI-generated synthetic data significantly\
  \ improves predictive accuracy for highly unbalanced tabular datasets, particularly\
  \ when minority classes are severely underrepresented. The research compares three\
  \ upsampling methods\u2014naive oversampling, SMOTE-NC, and synthetic data upsampling\
  \ using MOSTLY AI's open-source TabularARGN framework\u2014across three datasets\
  \ (Adult, Credit Card, Insurance) with varying sizes and feature compositions."
---

# Improving Predictions on Highly Unbalanced Data Using Open Source Synthetic Data Upsampling

## Quick Facts
- arXiv ID: 2507.16419
- Source URL: https://arxiv.org/abs/2507.16419
- Authors: Ivona Krchova; Michael Platzer; Paul Tiwald
- Reference count: 15
- Primary result: AI-generated synthetic data significantly improves predictive accuracy for highly unbalanced tabular datasets

## Executive Summary
This study demonstrates that AI-generated synthetic data significantly improves predictive accuracy for highly unbalanced tabular datasets, particularly when minority classes are severely underrepresented. The research compares three upsampling methods—naive oversampling, SMOTE-NC, and synthetic data upsampling using MOSTLY AI's open-source TabularARGN framework—across three datasets (Adult, Credit Card, Insurance) with varying sizes and feature compositions. Results show that synthetic data consistently produces top-performing models, with AUC-ROC and AUC-PR improvements of up to 0.2 over traditional methods in extreme imbalance scenarios (0.05-0.5% minority fractions).

## Method Summary
The study evaluates three upsampling approaches on highly unbalanced datasets: naive duplication, SMOTE-NC interpolation, and synthetic data generation using TabularARGN. For each dataset, minority class examples are downsampled to create imbalance ratios from 0.05% to 5%. Each method generates balanced training sets, which are then used to train Random Forest, XGBoost, and LightGBM classifiers. Performance is measured using AUC-ROC and AUC-PR on holdout sets, with additional analysis of feature space coverage through Shannon entropy and probability distribution diversity.

## Key Results
- Synthetic upsampling with TabularARGN consistently outperforms naive and SMOTE-NC methods across all datasets and imbalance levels
- AUC-PR improvements reach up to 0.2 in extreme imbalance scenarios (0.05-0.5% minority fractions)
- Synthetic data excels at generating diverse, realistic minority samples that capture statistical properties of original data
- Open-source TabularARGN implementation provides practical, accessible solution with minimal computational overhead

## Why This Works (Mechanism)

### Mechanism 1: Feature Space Coverage via Learned Distribution Sampling
- Claim: Synthetic upsampling improves minority class prediction by generating diverse samples that populate sparse regions of the feature space, rather than replicating or linearly interpolating existing minority samples.
- Mechanism: The TabularARGN generative model learns the joint distribution of features from the entire training set (including majority class), then samples new minority records that can fall anywhere in the learned feature space—beyond the convex hull of existing minority samples.
- Core assumption: The generative model can extrapolate meaningful minority class characteristics from very few examples by leveraging structural patterns learned from the majority class.
- Evidence anchors: [abstract] "Synthetic data can improve predictive accuracy for minority groups by generating diverse data points that fill gaps in sparse regions of the feature space."

### Mechanism 2: Majority-to-Minority Pattern Transfer
- Claim: Generative models trained on imbalanced data learn feature correlations and categorical patterns from the majority class that partially transfer to minority class generation.
- Mechanism: The auto-regressive generator learns conditional dependencies between features (e.g., education level ↔ income bracket) from the full dataset. When generating minority samples, these learned dependencies constrain outputs to realistic combinations even when minority examples are sparse.
- Core assumption: Feature-level correlations and constraints are partially shared across classes; the minority class differs primarily in target-conditional probability rather than feature covariance structure.
- Evidence anchors: [Page 2] "Because they are not tied to existing minority samples, AI-based generators can also leverage and learn from properties of parts of the majority class that are transferable to minority examples."

### Mechanism 3: Prediction Variance Restoration
- Claim: Synthetic upsampling produces smoother probability distributions in classifier outputs, enabling better threshold selection and calibration.
- Mechanism: Naive and SMOTE-NC upsampling create "clumped" predictions where many holdout samples receive identical probability scores (due to limited training diversity). Synthetic data's higher diversity yields unique probabilities per sample, producing smooth ROC/PR curves with well-distributed thresholds.
- Core assumption: The true minority class distribution requires diverse training examples to avoid overfitting to spurious patterns in the few available samples.
- Evidence anchors: [Page 6-7] "For both the highly unbalanced training data and the naively upsampled data, we observe very low diversity, with more than 80% [of samples] assigned identical probabilities..."

## Foundational Learning

- **Concept: AUC-PR vs AUC-ROC for imbalanced data**
  - Why needed here: The paper reports both metrics because AUC-ROC can appear optimistic when minority classes are tiny; AUC-PR focuses on precision-recall tradeoffs that better reflect minority class performance.
  - Quick check question: If a dataset has 99% majority class, why might a classifier with 99% accuracy still be useless?

- **Concept: Shannon entropy for categorical diversity**
  - Why needed here: The paper uses SE to quantify how well each upsampling method recovers the categorical diversity of the original holdout distribution.
  - Quick check question: A feature with SE near 0 indicates what about its category distribution?

- **Concept: SMOTE-NC interpolation limitations**
  - Why needed here: Understanding why SMOTE-NC fails at extreme imbalance clarifies the motivation for generative approaches—it can only interpolate within the convex hull of existing minority samples.
  - Quick check question: With only 3 minority samples in 10-dimensional space, what geometric constraint limits SMOTE-NC's generated samples?

## Architecture Onboarding

- **Component map:**
  Original Dataset → Stratified Split → Base/Holdout → Minority Downsampling (0.05%-5% fractions) → Upsampling Branch (3 parallel paths): Naive (duplicate), SMOTE-NC (interpolate), TabularARGN (generative) → Train classifiers (RF, XGB, LGBM) on balanced data → Evaluate on holdout (AUC-ROC, AUC-PR)

- **Critical path:** Generator training quality → synthetic sample fidelity → minority pattern recovery → classifier generalization. The generator must learn sufficient signal from sparse minority examples while borrowing valid structure from majority class.

- **Design tradeoffs:**
  - Computational cost: TabularARGN requires model training (seconds-minutes) vs. instant naive/SMOTE-NC
  - Diversity vs. fidelity: Aggressive generation may produce unrealistic samples; conservative sampling may underpopulate feature space
  - Target balance ratio: Paper uses 50:50; other ratios may be optimal for different cost structures

- **Failure signatures:**
  - Low Shannon entropy in synthetic samples (similar to naive upsampling)—indicates generator collapsed to limited modes
  - Discontinuous ROC/PR curves with clustering at specific thresholds—suggests insufficient sample diversity
  - Synthetic distribution significantly diverges from holdout on key features—generator learned spurious patterns
  - XGB outperforms synthetic-augmented models by large margin—suggests synthetic data adding noise

- **First 3 experiments:**
  1. **Baseline replication**: Download Adult/Credit Card datasets, reproduce the 0.1% minority fraction downsampling, compare naive vs. SMOTE-NC vs. TabularARGN using the open-source SDK (github.com/mostly-ai/mostlyai). Verify AUC-PR gaps match paper.
  2. **Entropy diagnostic**: For your own imbalanced dataset, compute Shannon entropy per categorical feature across upsampling methods before training any classifier—this predicts whether synthetic upsampling will help.
  3. **Ablation on minority count**: Test synthetic upsampling at 0.05%, 0.5%, 2%, 5% minority fractions to identify the threshold below which synthetic provides marginal benefit over SMOTE-NC on your data distribution.

## Open Questions the Paper Calls Out
None

## Limitations
- Effectiveness depends heavily on minority class size—extremely small minority fractions may lack sufficient signal for meaningful synthetic generation
- Analysis focuses primarily on tabular data with structured categorical and numerical features, leaving uncertainty about performance on unstructured or highly correlated feature spaces
- Computational overhead of training generative models versus traditional methods requires consideration for production deployment

## Confidence

- **High confidence**: The synthetic data consistently improves AUC-PR metrics across all tested datasets and imbalance levels, with the mechanism of feature space coverage being well-supported by entropy and probability distribution analyses.
- **Medium confidence**: Cross-class pattern transfer assumptions hold for the tested datasets, but may not generalize to cases where minority and majority classes have fundamentally different feature covariance structures.
- **Low confidence**: The performance claims at the most extreme imbalance level (0.05%) are based on limited sample sizes, making the practical utility at this threshold uncertain.

## Next Checks
1. Test the synthetic upsampling approach on datasets where minority and majority classes have different causal mechanisms to assess pattern transfer limitations.
2. Evaluate computational efficiency and training time overhead of TabularARGN versus traditional methods across various dataset sizes and imbalance ratios.
3. Conduct ablation studies with varying minority class sample counts (10, 50, 100) to identify the minimum viable minority sample size for effective synthetic generation.