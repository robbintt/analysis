---
ver: rpa2
title: 'Learning from the Undesirable: Robust Adaptation of Language Models without
  Forgetting'
arxiv_id: '2511.13052'
source_url: https://arxiv.org/abs/2511.13052
tags:
- tasks
- vanilla
- fine-tuning
- math
- neftune
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses overfitting issues in supervised fine-tuning
  (SFT) of language models when training data is limited. The proposed Learning-from-the-Undesirable
  (LfU) method regularizes SFT by enforcing consistency between internal representations
  of the model and those after an undesirable update.
---

# Learning from the Undesirable: Robust Adaptation of Language Models without Forgetting

## Quick Facts
- arXiv ID: 2511.13052
- Source URL: https://arxiv.org/abs/2511.13052
- Authors: Yunhun Nam; Jaehyung Kim; Jongheon Jeong
- Reference count: 40
- Key outcome: LfU improves generalization and robustness in SFT under limited data, achieving 16.8% average improvement on math tasks compared to vanilla SFT on the same dataset

## Executive Summary
This paper addresses overfitting issues in supervised fine-tuning (SFT) of language models when training data is limited. The proposed Learning-from-the-Undesirable (LfU) method regularizes SFT by enforcing consistency between internal representations of the model and those after an undesirable update. This is achieved by simulating undesirable behaviors through gradient ascent steps on auxiliary model components and then aligning the resulting representations. LfU improves generalization while preserving pretrained knowledge, demonstrating robustness across prompt variations and maintaining performance on out-of-distribution tasks.

## Method Summary
LfU introduces a novel consistency regularization approach that aligns internal representations before and after simulating undesirable model updates. The method uses auxiliary components (LoRA matrices or steering vectors) to compute a single gradient ascent step on the SFT loss, creating perturbed representations. A consistency loss (MSE) between original and perturbed representations across all layers is combined with the standard SFT loss. This representation-level data augmentation promotes generalization under limited data while preserving pretrained knowledge.

## Key Results
- LfU achieves 16.8% average improvement on math tasks compared to vanilla SFT on the same dataset, where SFT actually degrades performance
- LfU demonstrates 92.1% lower standard deviation in output performance across prompt variations, indicating improved robustness
- Representation-level consistency regularization outperforms both logit-level consistency and standard L1/L2 penalties

## Why This Works (Mechanism)

### Mechanism 1: Representation-Level Consistency via Undesirable Updates
- **Claim:** Enforcing alignment between internal representations before and after simulated undesirable updates improves generalization under limited data.
- **Mechanism:** LfU constructs an auxiliary model (via LoRA or representation steering), applies a single gradient ascent step on the SFT loss to induce "undesirable" behavior, then penalizes representation divergence. This creates representation-level data augmentation by exposing the model to corrupted versions of its own representations during training.
- **Core assumption:** Representation stability under adversarial-like perturbations correlates with better generalization and reduced overfitting to spurious patterns.
- **Evidence anchors:** [abstract] "We propose a novel form of consistency regularization that directly aligns internal representations of the model with those after an undesirable update. By leveraging representation-level data augmentation through undesirable updates, LfU effectively promotes generalization under limited data."
- **Break condition:** If the undesirable update magnitude (α) is too large (e.g., α=0.5), the model distorts severely and performance collapses (Math drops to 44.5 in Table 5a).

### Mechanism 2: Gradient Ascent as Targeted Representation Perturbation
- **Claim:** A single normalized gradient ascent step on auxiliary parameters generates more effective augmentation than random noise injection.
- **Mechanism:** Rather than adding arbitrary noise (as in NEFTune), LfU computes ∇θ_aux ℓ_SFT and moves θ_aux in the direction that increases loss. This creates task-relevant perturbations that specifically target the model's learned patterns, exposing brittleness in its current representations.
- **Core assumption:** Gradient-informed perturbations reveal more relevant failure modes than isotropic noise, leading to better generalization.
- **Evidence anchors:** [Figure 6] LfU achieves higher cosine similarity between clean and noisy input representations than NEFTune across all layers, despite NEFTune being explicitly designed for noise robustness.
- **Break condition:** If auxiliary components are not properly isolated from main parameters, gradients may corrupt the base model.

### Mechanism 3: Layer-Wise Representation Stabilization
- **Claim:** Applying consistency regularization across all layers provides more robust protection than selective layer targeting.
- **Mechanism:** The consistency loss sums over all K layers, forcing the model to maintain stable semantic content throughout its processing pipeline. Early layers preserve input representations while later layers maintain task-relevant abstractions.
- **Core assumption:** Overfitting manifests across the entire depth of the network, not just in specific layers.
- **Evidence anchors:** [Table 5b] Ablation comparing "All" layers (54.2 Math) vs. "Early" (53.6), "Middle" (53.3), and "Late" (53.6) layer selections shows full coverage achieves best results.
- **Break condition:** If λ (consistency weight) is too high (e.g., λ=50), over-regularization interferes with SFT learning, reducing in-domain performance.

## Foundational Learning

- **Concept: Gradient Ascent vs. Descent**
  - **Why needed here:** LfU intentionally performs gradient *ascent* (maximizing loss) on auxiliary parameters to simulate undesirable behavior. Understanding this reverses standard training intuition.
  - **Quick check question:** If you accidentally applied gradient *descent* instead of ascent to θ_aux, would this still create an "undesirable" model? Why or why not?

- **Concept: Representation Detachment (stop-gradient)**
  - **Why needed here:** The detach(h_l,t) operation in Eq. 5 prevents gradients from flowing through the original representations, making them fixed targets. Without this, both representations would move toward each other rather than the undesirable adapting to the stable original.
  - **Quick check question:** What would happen to training dynamics if you detached h'_l,t instead of h_l,t?

- **Concept: Normalized Gradient Updates**
  - **Why needed here:** LfU normalizes the gradient by its L2 norm before scaling by α. This ensures consistent perturbation magnitude regardless of loss scale or model size.
  - **Quick check question:** Why might unnormalized gradient ascent cause unstable training across different batch sizes or learning rate schedules?

## Architecture Onboarding

- **Component map:**
Main Model (θ, frozen during perturbation) -> Auxiliary Components (θ_aux) -> Forward Pass → Compute ℓ_SFT(θ_aux) -> Gradient Ascent Step (Eq. 2) → Perturbed θ_aux -> Extract Representations: h_l,t = M^(l)(x, y_<t; θ), h'_l,t = M^(l)(x, y_<t; θ, θ_aux) -> Consistency Loss: ℓ_cons. = MSE(detach(h), h') -> Combined Loss: ℓ_LfU = ℓ_SFT(θ) + λ · ℓ_cons. -> Update θ via gradient descent

- **Critical path:**
  1. **Auxiliary initialization:** LoRA matrices initialized to zero or steering vectors to zero ensures θ_aux starts equivalent to θ
  2. **Gradient ascent magnitude (α):** Controls perturbation strength; paper uses α=0.1 for LoRA, α=0.001 for Instruct models
  3. **Consistency weight (λ):** Balances SFT objective vs. regularization; default λ=5.0 for LoRA, λ=500 for Instruct, λ=0.1 for RepS

- **Design tradeoffs:**
  - **LoRA-based vs. RepS:** LoRA achieves slightly better in-domain performance (54.2 vs 53.2 Math) but RepS is ~1.8× faster (2332ms vs 4142ms per step)
  - **Layer selection:** Applying to all layers maximizes robustness but increases compute; selective layers provide modest gains with lower overhead
  - **α-λ interaction:** Higher α creates stronger perturbations requiring lower λ to avoid destabilization; finding this balance is model-specific

- **Failure signatures:**
  - **Performance collapse:** Math scores <45 with high α (0.5+) indicates excessive perturbation destroying representation semantics
  - **No improvement over SFT:** Likely λ too low (consistency loss negligible) or auxiliary components not properly connected
  - **Training instability:** Oscillating losses may indicate learning rate conflicts between SFT and consistency objectives

- **First 3 experiments:**
  1. **Baseline replication:** Fine-tune Llama-3.1-8B on GSM8k with vanilla SFT (3 epochs, lr=1e-4, batch=16). Record in-domain (GSM8k) and OOD (MMLU, PIQA) performance to establish forgetting baseline.
  2. **LfU-LoRA implementation:** Add LoRA (rank=8) to all linear layers, compute gradient ascent with α=0.1, λ=5.0. Compare GSM8k accuracy and standard deviation across 3 random seeds vs. SFT baseline.
  3. **Ablation sweep:** Fix α=0.1, vary λ ∈ {0.1, 1.0, 5.0, 10.0} and measure trade-off curve between in-domain (Math) and OOD (Knowledge/Reasoning) performance. Identify λ that maximizes average rank across categories.

## Open Questions the Paper Calls Out
- **Open Question 1:** Can the Representation Steering (RepS) auxiliary model be optimized to match the in-domain performance of the LoRA-based auxiliary model while retaining its computational efficiency?
- **Open Question 2:** Does simulating multiple steps of gradient ascent to construct the auxiliary model yield more robust regularization than the single-step approach utilized in the paper?
- **Open Question 3:** Would utilizing a cosine similarity constraint or contrastive loss for consistency regularization preserve semantic features more effectively than the Mean Squared Error (MSE) loss currently employed?

## Limitations
- The paper does not provide implementation details for LoRA configuration beyond rank=8 (target modules, alpha scaling, dropout settings)
- Prompt templates for fine-tuning and evaluation are not specified
- Layer extraction implementation details for internal representations are unclear
- The exact mechanism for steering vector initialization in RepS variant is not detailed

## Confidence
- **High confidence:** The core LfU mechanism (gradient ascent on auxiliary parameters + representation consistency) is well-specified and experimentally validated
- **Medium confidence:** The hyperparameter settings (α=0.1, λ=5.0) are presented as defaults but their sensitivity across different model sizes and tasks is not extensively explored
- **Medium confidence:** The claim that representation-level consistency outperforms logit-level consistency is supported by ablation studies but the theoretical justification could be more rigorous

## Next Checks
1. **Ablation study verification:** Replicate the layer-wise ablation (Table 5b) to confirm that applying consistency regularization to all layers provides optimal performance compared to selective layer targeting
2. **Gradient ascent sensitivity:** Systematically vary α ∈ {0.01, 0.05, 0.1, 0.5} to identify the precise threshold where performance collapses and validate the claim that normalized gradient updates prevent scale-dependent instability
3. **Robustness generalization:** Test LfU on a broader range of data-limited scenarios beyond GSM8k (e.g., few-shot learning benchmarks, low-resource languages) to assess whether the method's robustness generalizes beyond the specific math domain studied