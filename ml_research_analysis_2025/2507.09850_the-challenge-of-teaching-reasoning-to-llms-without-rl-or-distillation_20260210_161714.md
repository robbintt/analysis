---
ver: rpa2
title: The Challenge of Teaching Reasoning to LLMs Without RL or Distillation
arxiv_id: '2507.09850'
source_url: https://arxiv.org/abs/2507.09850
tags:
- reasoning
- data
- solutions
- qwen2
- problem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper explores whether long Chain-of-Thought (CoT) reasoning
  can be induced in base language models using only minimal supervision. It demonstrates
  that fine-tuning Qwen2.5-32B on as few as 20 high-quality CoT examples from the
  reasoning model QwQ-32B-Preview significantly improves performance, achieving 17.10%
  pass@1 and 27.73% maj@64, surpassing the much larger Qwen2.5-Math-72B-Instruct baseline.
---

# The Challenge of Teaching Reasoning to LLMs Without RL or Distillation

## Quick Facts
- arXiv ID: 2507.09850
- Source URL: https://arxiv.org/abs/2507.09850
- Reference count: 38
- Primary result: Fine-tuning Qwen2.5-32B on 20 high-quality CoT examples from QwQ-32B-Preview achieves 17.10% pass@1 and 27.73% maj@64 on Comp-Math-24-25, surpassing Qwen2.5-Math-72B-Instruct baseline.

## Executive Summary
This paper challenges the assumption that extensive datasets or reinforcement learning are necessary to induce reasoning capabilities in large language models. The authors demonstrate that supervised fine-tuning of the base Qwen2.5-32B model on just 20 high-quality Chain-of-Thought traces from the reasoning model QwQ-32B-Preview significantly improves mathematical reasoning performance. The approach achieves strong results without requiring reinforcement learning, distillation at scale, or large-scale supervised datasets. The study systematically explores why certain data sources fail to activate reasoning behavior, revealing that the structural consistency of reasoning traces—not surface-level features like keywords or solution correctness—is the critical factor.

## Method Summary
The authors generate 512 solutions per problem using QwQ-32B-Preview for 50 competition math problems, filter for correct final answers and target length (~3K tokens), then fine-tune Qwen2.5-32B with AdamW (LR=1e-5, batch=1024, 50 steps) using NeMo-Aligner with sequence packing. They evaluate on Comp-Math-24-25 (256 AIME/HMMT problems) using pass@1 and maj@64 metrics. The approach is contrasted with SFT on non-reasoning model outputs, human-written solutions (even after iterative refinement), and ablation studies varying solution correctness, keywords, and other factors.

## Key Results
- Fine-tuning on 20 QwQ-generated CoT traces achieves 17.10% pass@1 and 27.73% maj@64, outperforming Qwen2.5-Math-72B-Instruct baseline (11.72% pass@1, 16.14% maj@64).
- Both correct and incorrect solutions with proper reasoning structure activate reasoning capabilities equally well.
- Non-reasoning model outputs and human-written solutions fail to induce reasoning behavior even after extensive prompt engineering and refinement.
- Base models below 32B parameters (7B, 14B) fail to enter reasoning mode under identical conditions.

## Why This Works (Mechanism)

### Mechanism 1: Structural Pattern Transfer via Minimal Supervision
As few as 20 high-quality Chain-of-Thought traces from a reasoning model can activate reasoning behavior in a base model through supervised fine-tuning. The base model acquires the structural patterns of reasoning traces (exploratory process, self-correction, verification) rather than surface-level features or factual content.

### Mechanism 2: Reasoning Data Quality Involves Latent Structural Properties
CoT traces from reasoning models contain latent qualities that cannot be easily replicated by non-reasoning models or human annotators, even with extensive prompt engineering and iterative refinement. Reasoning models produce traces that follow an exploratory, causal narrative where intermediate steps emerge in discovery order.

### Mechanism 3: Solution Correctness Is Not Required for Reasoning Transfer
Solutions with incorrect final answers but correct reasoning structure can still activate reasoning capabilities in base models. The model learns from the process of reasoning rather than memorizing correct answers.

## Foundational Learning

- **Chain-of-Thought (CoT) Reasoning**: The paper operationalizes "reasoning behavior" through the presence and quality of long CoT traces. Quick check: Can you distinguish between a solution that presents reasoning exploratorily versus one that presents a reconstructed, conclusion-first narrative?

- **Supervised Fine-Tuning (SFT) for Behavior Transfer**: The paper's intervention is SFT on minimal data. Quick check: Why might SFT on 20 examples transfer a behavior pattern more effectively than few-shot prompting with the same examples?

- **Knowledge Distillation from Reasoning Models**: The paper positions itself against existing distillation approaches that use large datasets (220K–3.2M examples). Quick check: What distinguishes distillation at scale from the minimal-supervision approach explored here?

## Architecture Onboarding

- **Component map**: QwQ-32B-Preview (reasoning model) -> Qwen2.5-32B (base model) -> Comp-Math-24-25 (evaluation benchmark)
- **Critical path**: Generate 512 candidate solutions per problem using reasoning model → Filter for correct final answers and target length (~3K tokens) → Fine-tune base model with AdamW, LR=1e-5, 50 steps, batch size 1024 → Evaluate with pass@1 and maj@64 metrics across 64 generations
- **Design tradeoffs**: Data scale vs. quality (20 high-quality examples outperform thousands of lower-quality synthetic solutions); Model scale vs. transferability (32B model succeeds; 7B and 14B fail); Human vs. synthetic data (human-written solutions underperform reasoning model outputs)
- **Failure signatures**: Base models below 32B show no reasoning activation; Non-reasoning model outputs fail even with prompt engineering and post-editing; Human-written data shows low pass@1 despite high maj@64, suggesting style inconsistency
- **First 3 experiments**: 1) Fine-tune Qwen2.5-32B on 20 QwQ-generated CoT traces; verify pass@1 improvement over baseline on Comp-Math-24-25. 2) Compare SFT performance using reasoning model traces, non-reasoning model outputs, and human-written solutions. 3) Run identical SFT on Qwen2.5-7B, 14B, and 32B to confirm the ~32B threshold for reasoning activation.

## Open Questions the Paper Calls Out

### Open Question 1
Can the finding that minimal supervision (approximately 20 examples) effectively induces reasoning be generalized to other complex domains beyond mathematics? The authors suggest future studies could extend to symbolic logic, coding tasks, or scientific question answering to examine generalization of induced reasoning abilities.

### Open Question 2
What specific structural or stylistic qualities in model-generated reasoning traces distinguish them from human-written or non-reasoning model outputs? The paper notes that structural consistency is key but admits certain latent qualities of expert CoT are difficult to replicate.

### Open Question 3
Can providing partial solutions (intermediate steps without final answers) effectively prompt the model to complete the reasoning process on its own? The authors list this as a primary direction for future work to encourage deeper reasoning behaviors.

### Open Question 4
Can human-written solutions be refined to match the efficacy of model-generated traces, or is the failure of human data primarily due to stylistic variance? The authors aim to continuously refine human-written solutions by injecting more consistent reasoning patterns and harmonizing writing styles.

## Limitations

- The exact scale boundary for reasoning activation remains unclear, as the failure of 7B and 14B models could stem from architecture differences beyond parameter count.
- The paper doesn't fully explain what structural properties make reasoning traces effective, suggesting this may involve subtle distributional differences that are difficult to characterize.
- The findings are based exclusively on mathematical problem-solving, leaving the transferability of the "activation" mechanism to other reasoning types unknown.

## Confidence

- **High confidence**: The empirical finding that 20 high-quality CoT examples from QwQ-32B-Preview significantly improve base model performance over baselines.
- **Medium confidence**: The claim that solution correctness doesn't matter for reasoning transfer, as sample size and problem difficulty distribution may influence this finding.
- **Medium confidence**: The assertion that non-reasoning model outputs and human solutions fail to induce reasoning behavior, as the paper doesn't fully characterize what makes reasoning model outputs structurally distinct.

## Next Checks

1. Test whether QwQ-32B-Preview reasoning traces can induce reasoning in other base models (e.g., Llama-3-70B, Mistral-7B) to verify the pattern transfer mechanism isn't specific to Qwen2.5-32B architecture.

2. Conduct ablation studies varying reasoning trace characteristics (length, step count, self-correction frequency, keyword density) to identify which structural properties correlate most strongly with reasoning activation.

3. Systematically test reasoning transfer across the 7B-32B parameter range using multiple architectures to precisely identify the scale threshold and determine whether it's parameter-count dependent or architecture-specific.