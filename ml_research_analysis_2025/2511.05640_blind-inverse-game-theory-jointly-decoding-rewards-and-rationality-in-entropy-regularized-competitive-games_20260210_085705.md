---
ver: rpa2
title: 'Blind Inverse Game Theory: Jointly Decoding Rewards and Rationality in Entropy-Regularized
  Competitive Games'
arxiv_id: '2511.05640'
source_url: https://arxiv.org/abs/2511.05640
tags:
- games
- error
- blind-igt
- matrix
- inverse
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work tackles the fundamental challenge in inverse game theory\
  \ of recovering agents' reward parameters and rationality levels from observed behavior\
  \ when the rationality (temperature \u03C4) is unknown. The authors introduce Blind-IGT,\
  \ a statistical framework that resolves the inherent scale ambiguity between rewards\
  \ and temperature by introducing a normalization constraint."
---

# Blind Inverse Game Theory: Jointly Decoding Rewards and Rationality in Entropy-Regularized Competitive Games

## Quick Facts
- **arXiv ID:** 2511.05640
- **Source URL:** https://arxiv.org/abs/2511.05640
- **Reference count:** 40
- **Key outcome:** Blind-IGT achieves optimal O(N^{-1/2}) convergence for jointly recovering rewards and temperature in entropy-regularized zero-sum games, even when transition dynamics are unknown.

## Executive Summary
This work addresses the fundamental challenge in inverse game theory of recovering agents' reward parameters and rationality levels from observed behavior when the rationality (temperature τ) is unknown. The authors introduce Blind-IGT, a statistical framework that resolves the inherent scale ambiguity between rewards and temperature by introducing a normalization constraint. They propose a Normalized Least Squares (NLS) estimator and prove it achieves the optimal O(N^{-1/2}) convergence rate for joint recovery of rewards and temperature in entropy-regularized zero-sum games. The method is extended to Markov games, where empirical results demonstrate optimal convergence rates even when transition dynamics are unknown.

## Method Summary
The Blind-IGT framework recovers reward parameters θ and unknown temperature τ from observed strategic behavior using a two-stage Normalized Least Squares approach. First, empirical policies are estimated from action samples, then linearized QRE constraints are constructed using log-probability ratios. The NLS estimator solves a bilinear system Xθ = τy to obtain a ratio estimate θ_{LS}, then applies a normalization constraint to separate θ and τ. The method is extended to Markov games using Bellman equation inversion, with empirical validation showing robustness even when transition dynamics must be estimated from data.

## Key Results
- Blind-IGT achieves optimal O(N^{-1/2}) convergence rate for joint recovery of rewards and temperature
- Method works in Markov games with unknown transition dynamics, matching theoretical bounds empirically
- Using incorrect temperature assumptions in standard IGT methods introduces linear bias, while Blind-IGT achieves near-oracle performance
- Normalization constraint resolves scale ambiguity while providing partial identification guarantees when full identifiability fails

## Why This Works (Mechanism)

### Mechanism 1: Scale Disambiguation via Normalization
- **Claim:** Introducing a normalization constraint on reward parameters allows the framework to uniquely identify both rewards and the unknown rationality parameter (τ).
- **Mechanism:** The QRE behavior is determined by the ratio of rewards to temperature (Q/τ). This creates a multiplicative ambiguity where (θ, τ) and (kθ, kτ) are indistinguishable. By enforcing a strict norm constraint ||θ||₂ = C, the scalar k is fixed, resolving the ambiguity and allowing the temperature to be decoupled from the rewards.
- **Core assumption:** The agents do not play uniformly randomly (Non-Uniformity Condition), ensuring the reward vector is non-zero.
- **Evidence anchors:**
  - [abstract]: "...introducing a normalization constraint that resolves the scale ambiguity."
  - [section 4.2]: Theorem 1 establishes this constraint as necessary and sufficient for unique joint identification.
  - [corpus]: The related paper "Decoding Rewards..." assumes τ is known; this mechanism specifically targets the limitation of that prior assumption.
- **Break condition:** If agents play the uniform distribution (y* = 0), the scale cannot be determined because θ collapses to zero.

### Mechanism 2: Linearization of Equilibrium Constraints
- **Claim:** Non-linear QRE fixed-point equations can be transformed into a tractable bilinear system of linear equations.
- **Mechanism:** The framework takes the logarithm of the QRE probability ratios relative to a reference action. This cancels the partition function (denominator), resulting in linear constraints X(μ, ν)θ = τy(μ, ν) where X contains feature differences and y contains log-probability differences.
- **Core assumption:** The "Soft-Min Gap" (Assumption 3) holds, meaning action probabilities are bounded away from zero to ensure log-stability.
- **Evidence anchors:**
  - [section 3.1]: Equation 4 explicitly shows the linearized constraints derived from log-ratios.
  - [section 4.1]: Equation 7 presents the resulting bilinear system Xθ = τy.
  - [corpus]: "Decoding Rewards..." (Liao et al., 2025) utilizes a similar linearization technique but for known τ.
- **Break condition:** If observed action probabilities are extremely low (approaching 0), the log-ratio calculation becomes unstable, breaking the finite-sample guarantees.

### Mechanism 3: Two-Stage Normalized Least Squares (NLS)
- **Claim:** The NLS estimator decouples the bilinear problem into a linear regression followed by a scaling step, achieving optimal statistical rates.
- **Mechanism:** First, standard least squares solves for the ratio θ_{LS} ≈ θ*/τ*. Second, the temperature is estimated using the normalization constraint τ̂ = C / ||θ_{LS}||. Finally, rewards are recovered by scaling: θ̂ = τ̂ · θ_{LS}.
- **Core assumption:** The feature matrix X has full column rank (Rank Condition), ensuring the least squares solution is unique.
- **Evidence anchors:**
  - [section 4.3]: Algorithm 1 details the NLS steps.
  - [section 4.4]: Theorem 2 proves this method achieves the optimal O(N^{-1/2}) convergence rate.
  - [corpus]: No direct corpus evidence for this specific "blind" estimation step; it is the novel contribution of this work.
- **Break condition:** If the feature matrix X is rank-deficient (e.g., redundant features), the least squares inverse fails, preventing recovery.

## Foundational Learning

- **Concept: Quantal Response Equilibrium (QRE)**
  - **Why needed here:** Unlike Nash Equilibrium, QRE models bounded rationality (stochastic play), which is the core assumption required to derive the linearizable log-ratio constraints.
  - **Quick check question:** How does the temperature parameter τ affect the "sharpness" of the agent's policy distribution in QRE?

- **Concept: Bilinear Inverse Problems**
  - **Why needed here:** The problem of recovering θ and τ simultaneously forms a bilinear system (Xθ = τy). Understanding why this is harder than a linear inverse problem (e.g., standard regression) is key to appreciating the normalization solution.
  - **Quick check question:** Why does scaling both the unknown vector θ and the unknown scalar τ by a constant k result in the same observed data?

- **Concept: Identifiability in Inverse Problems**
  - **Why needed here:** The paper pivots between "unique identification" (Theorem 1) and "partial identification" (Proposition 1). You must understand when parameters can be recovered exactly vs. when only a feasible set can be defined.
  - **Quick check question:** If the Rank Condition fails, can we recover a single point estimate for θ? If not, what does the framework provide instead?

## Architecture Onboarding

- **Component map:** Input -> Preprocessor -> System Builder -> NLS Solver -> Reward Decoder
- **Critical path:**
  1. Validate that empirical policies are not uniform (check y ≠ 0)
  2. Construct the system Xθ = τy using a fixed reference action (Action 1)
  3. Solve the Least Squares problem for the ratio vector
  4. Apply the Normalization Constant C to split the ratio vector into final θ̂ and τ̂

- **Design tradeoffs:**
  - **Normalization Constant C:** The paper claims robustness to misspecification of C regarding the *direction* of θ (see Section 4.5), but the *magnitude* of τ will scale linearly with the error in C
  - **Known vs. Unknown Dynamics:** Theoretical bounds (Theorem 3) assume known transitions P, but empirical results (Section 6.4) show the method works with estimated P̂, suggesting the theoretical sufficient conditions may be stricter than practically necessary

- **Failure signatures:**
  - **Uniform Play:** If agents play randomly, y ≈ 0, causing division by zero or numerical instability in log-ratios
  - **Rank Deficiency:** If features are linearly dependent, the matrix inverse (XᵀX)⁻¹ fails
  - **Temperature Misspecification (External):** If using a standard IGT method (not Blind-IGT) with the wrong τ, reward errors scale linearly with the misspecification (Table 1)

- **First 3 experiments:**
  1. **Matrix Game Validation:** Reproduce Figure 1 to verify the O(N^{-1/2}) convergence slope on a log-log plot for a simple 10 × 10 game
  2. **Ablation on τ:** Compare Blind-IGT against a baseline that assumes a fixed, incorrect τ to replicate Table 1, demonstrating the bias introduced by incorrect rationality assumptions
  3. **Markov Game Robustness:** Run Algorithm 2 on a navigation game with *unknown* transitions (estimating P̂ from data) to verify if the empirical error tracks the theoretical "Known-P" bound as shown in Figure 4

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the Blind-IGT framework be extended to complex environments involving non-linear function approximation (e.g., deep neural networks) while maintaining statistical guarantees?
- **Basis in paper:** [explicit] The conclusion states that the "foundational analysis provides the essential theoretical tools for extending statistically rigorous IGT to complex environments involving non-linear function approximation."
- **Why unresolved:** The theoretical proofs (Theorems 2 and 3) rely on Assumption 1 (Linear Payoff Functions) to formulate the problem as a tractable bilinear system solvable via Normalized Least Squares. Non-linear features would break this specific linear structure and the associated finite-sample bounds.
- **What evidence would resolve it:** A proof of convergence for a non-linear estimator or empirical demonstrations in high-dimensional environments (like robotics or games) showing consistent recovery of θ and τ using neural networks.

### Open Question 2
- **Question:** Can the joint recovery framework be generalized to heterogeneous agent populations where agents possess distinct temperature parameters (τ₁ ≠ τ₂) or different reward structures?
- **Basis in paper:** [explicit] The conclusion identifies "heterogeneous agent populations" as a specific target for extending the current framework, which assumes a shared or single rationality level.
- **Why unresolved:** The current model assumes a single τ (or identical agents) to resolve the scale ambiguity via the normalization constraint. Introducing distinct τ values for each agent creates a more complex underdetermined system where the coupling between individual rewards and rationalities must be disentangled without the single normalization simplification.
- **What evidence would resolve it:** A modified theoretical analysis establishing identifiability conditions for multi-temperature settings, or an algorithm capable of recovering a vector of temperatures τᵢ with provable error bounds.

### Open Question 3
- **Question:** Can rigorous finite-sample guarantees be established for Markov games when the transition dynamics P are unknown and must be estimated from data simultaneously?
- **Basis in paper:** [inferred] Theorem 3 explicitly assumes access to the true dynamics P for its proof, while Remark 2 and Section 6.4 note that "error analysis must account for the uncertainty in P̂" and rely only on empirical validation to demonstrate robustness.
- **Why unresolved:** The paper provides theoretical bounds for the "known dynamics" case. When P is estimated, errors in P̂ propagate through the Bellman equation (Eq. 5), potentially compounding the errors in θ̂ and τ̂ in a way that the current theoretical linear perturbation analysis does not cover.
- **What evidence would resolve it:** A formal extension of Theorem 3 that provides a bound on ||r̂ - r*|| and |τ̂ - τ*| that explicitly includes the estimation error of the transition dynamics (e.g., bounding the term ||P̂ - P||).

### Open Question 4
- **Question:** Does the identifiability of Blind-IGT hold in general-sum games, or does the removal of the zero-sum restriction re-introduce fundamental ambiguities?
- **Basis in paper:** [inferred] The paper restricts its entire scope to "competitive games" and "zero-sum games" (Abstract, Section 3). The linearization in Equation 4 is presented specifically for this competitive setting.
- **Why unresolved:** In general-sum games, the relationship between rewards and the equilibrium policy can be more complex (multiple equilibria, different QRE structures). It is unclear if the bilinear system Xθ = τy retains the necessary full-rank properties or if the normalization constraint suffices to resolve ambiguities when agents are not strictly opposing.
- **What evidence would resolve it:** An analysis of the rank condition (Condition 1 of Theorem 1) in general-sum matrix games, showing whether the normalization constraint remains sufficient for unique joint identification.

## Limitations

- The framework assumes i.i.d. sampling from the QRE, but real-world strategic data may exhibit temporal correlation or non-stationarity
- Linearization requires bounded away-from-zero probabilities (Soft-Min Gap), which may not hold with limited samples or extreme temperatures
- Theoretical guarantees assume known transition dynamics in Markov games, though empirical results suggest these bounds may be overly conservative when dynamics are estimated

## Confidence

**High Confidence:** The scale ambiguity mechanism and its resolution through normalization is theoretically sound and well-established in the bilinear inverse problem literature. The O(N^{-1/2}) convergence rate for NLS is rigorously proven and consistent with statistical learning theory.

**Medium Confidence:** The empirical validation in Markov games relies on the assumption that estimated transition dynamics suffice for reward recovery, but this is not formally proven. The partial identification results (Proposition 1) are correct but may be conservative in practice, as real data may satisfy stronger identifiability conditions than the worst-case assumptions.

**Low Confidence:** The robustness claims for normalization constant misspecification are primarily based on intuition (Section 4.5) rather than formal analysis. The behavior of the framework under severe misspecification of the Soft-Min Gap parameter remains unclear.

## Next Checks

1. **Rank Sensitivity Analysis:** Systematically test how the estimation error scales when the feature matrix X approaches rank deficiency. Does the error blow up as predicted by the Moore-Penrose pseudoinverse analysis?

2. **Temperature Misspecification Ablation:** Replicate the exact experimental conditions from Table 1 to verify that using incorrect τ values introduces the claimed linear bias in reward estimates, and measure the error reduction achieved by Blind-IGT.

3. **Partial Identification Boundaries:** For a synthetic game where the Rank Condition fails, verify that Proposition 1 correctly identifies the feasible set for θ and that no point estimate is provided, confirming the framework's behavior under weak identifiability.