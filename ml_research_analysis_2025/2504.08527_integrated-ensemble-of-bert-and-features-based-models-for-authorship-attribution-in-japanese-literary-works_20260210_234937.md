---
ver: rpa2
title: Integrated ensemble of BERT- and features-based models for authorship attribution
  in Japanese literary works
arxiv_id: '2504.08527'
source_url: https://arxiv.org/abs/2504.08527
tags:
- bert
- ensemble
- used
- data
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents an integrated ensemble approach combining BERT-based
  and traditional feature-based models for authorship attribution (AA) in Japanese
  literary works. The method addresses challenges in small-sample AA tasks by leveraging
  both pre-trained language models and stylistic features extracted from texts.
---

# Integrated ensemble of BERT- and features-based models for authorship attribution in Japanese literary works

## Quick Facts
- arXiv ID: 2504.08527
- Source URL: https://arxiv.org/abs/2504.08527
- Authors: Taisei Kanda; Mingzhe Jin; Wataru Zaitsu
- Reference count: 40
- Primary result: Integrated ensemble combining BERT-based and traditional feature models achieves F1 > 0.95 on Japanese authorship attribution with 20 samples per author

## Executive Summary
This study presents an integrated ensemble approach combining BERT-based and traditional feature-based models for authorship attribution (AA) in Japanese literary works. The method addresses challenges in small-sample AA tasks by leveraging both pre-trained language models and stylistic features extracted from texts. Experiments were conducted on two corpora of Japanese literary works, each containing 10 authors with 20 works per author. The integrated ensemble significantly outperformed individual models, achieving F1 scores exceeding 0.95. For a corpus not included in pre-training data, the integrated ensemble improved F1 scores by approximately 14 points compared to the best single model.

## Method Summary
The method combines five Japanese BERT models (Tohoku-BERT, AozoraBERT, AozoraWikiBERT, DeBERTa, StockMarkBERT) with three feature-based classifiers (character-bigram, token-unigram, phrase-pattern) using soft voting ensemble. Each BERT model is fine-tuned with 5-fold stratified cross-validation (epochs=40, lr=2e-5, batch=16) on the first 510 tokens of each text. Feature classifiers use Random Forest and AdaBoost on extracted features (4444, 3300, and 804 dimensions respectively). The integrated ensemble aggregates probability vectors from all models to produce final predictions, testing 1482 combinations to identify optimal ensembles.

## Key Results
- Integrated ensemble achieved F1 scores exceeding 0.95 on both corpora
- For Corpus B (not in pre-training data), ensemble improved F1 by ~14 points over best single model
- Phrase-pattern features consistently appeared in top-performing ensembles despite low individual scores
- Unweighted soft voting outperformed weighted voting based on individual model F1 scores

## Why This Works (Mechanism)

### Mechanism 1: Complementary Information from Heterogeneous Models
- **Claim:** Ensembling models with diverse pre-training data and feature representations improves performance beyond any single model.
- **Mechanism:** Different models capture non-overlapping aspects of authorship signals. BERT models encode contextual semantics; character n-grams capture orthographic habits; phrase patterns capture syntactic structures with content masked. When combined via soft voting, classification errors from one model are corrected by others.
- **Core assumption:** The errors made by individual models are sufficiently uncorrelated that aggregation reduces variance without introducing systematic bias.
- **Evidence anchors:** [abstract], [section 5, Discussion], [corpus]: Limited—study uses only two corpora in Japanese; generalization to other languages/domains not established
- **Break condition:** If individual model errors become highly correlated (e.g., all models fail on same authors), ensemble gains diminish. Section 5 notes: "ensembling more models does not necessarily improve performance directly" when low-quality models are included.

### Mechanism 2: Pre-training Domain Alignment
- **Claim:** BERT models pre-trained on corpora similar to the target domain substantially outperform those trained on mismatched data.
- **Mechanism:** Pre-training on Aozora Bunko (literary works) enables the model to learn vocabulary, stylistic patterns, and grammatical constructions specific to that domain, which transfer to similar literary texts during fine-tuning.
- **Core assumption:** The distributional properties of pre-training data align with stylistic features relevant to authorship attribution in the target corpus.
- **Evidence anchors:** [section 4.1], [section 5]: "BERT pre-training data has a significant impact on the task"
- **Break condition:** When target corpus is NOT in pre-training data, domain-matched models lose advantage. Corpus B (modern authors not in pre-training) showed different model rankings—DeBERTa performed best despite general pre-training.

### Mechanism 3: Phrase-Pattern Robustness via Content Masking
- **Claim:** Phrase-pattern features (where content words are replaced with POS tags) provide ensemble gains despite low individual performance.
- **Mechanism:** By masking content words with POS tags, phrase patterns capture syntactic habits while being robust to topic/genre variation. This provides orthogonal information to content-sensitive features.
- **Core assumption:** Authors have consistent syntactic construction patterns that persist across different topics and genres.
- **Evidence anchors:** [section 3.1], [section 4.5]: Top integrated ensembles consistently included phrase-pattern features (labels 3, 6) despite their low standalone scores
- **Break condition:** If syntactic patterns are not distinctive enough for the author set (e.g., authors with very similar syntactic habits), phrase patterns add noise rather than signal.

## Foundational Learning

- **Concept: Soft Voting Ensemble**
  - **Why needed here:** The paper uses probability aggregation rather than hard voting; understanding how probability vectors combine is essential for implementing Equation (1).
  - **Quick check question:** Given three models outputting class probabilities [0.7, 0.2, 0.1], [0.4, 0.5, 0.1], and [0.3, 0.3, 0.4], what is the soft voting result?

- **Concept: Character n-grams vs. Token n-grams**
  - **Why needed here:** Japanese lacks word boundaries; the paper uses both character-level and morpheme-level features, each with different sparsity/dimensionality tradeoffs.
  - **Quick check question:** For Japanese text "今日は良い天気", what are the character bigrams and how would they differ from token unigrams after MeCab segmentation?

- **Concept: Pre-training Transfer for Small Samples**
  - **Why needed here:** With only 20 works per author, BERT's pre-trained representations are critical; the paper explicitly tests how pre-training domain affects small-sample performance.
  - **Quick check question:** Why might a BERT model pre-trained on Wikipedia underperform one pre-trained on literary corpora for authorship attribution of novels?

## Architecture Onboarding

- **Component map:**
  Input Text (510 tokens)
      ├── BERT Branch (5 models)
      │   ├── Tohoku-BERT (Wikipedia)
      │   ├── AozoraBERT (Literary)
      │   ├── AozoraWikiBERT (Mixed)
      │   ├── DeBERTa (General)
      │   └── StockMarkBERT (News)
      │   └── Fine-tune → Softmax → Probability Vector
      │
      └── Feature Branch (3 features × 2 classifiers)
          ├── Character-bigram → RF/Ada → Probability Vector
          ├── Token-unigram → RF/Ada → Probability Vector
          └── Phrase-pattern → RF/Ada → Probability Vector

  Soft Voting (Equation (1))
      └── Average probability vectors → Argmax → Author prediction

- **Critical path:**
  1. Morphological analysis with MeCab/UniDic (required for tokenization and phrase parsing)
  2. BERT fine-tuning with 5-fold stratified cross-validation (epochs=40, lr=2e-5, batch=16)
  3. Feature extraction (dimensions: char-bigram=4444, token-unigram=3300, phrase-pattern=804)
  4. Probability extraction and soft voting aggregation

- **Design tradeoffs:**
  - **510-token limit:** Truncates longer works; paper notes "indicators of authors' styles exist throughout the entire work" but chose BERT's maximum input for consistency
  - **Unweighted vs. weighted voting:** Paper tested both; "weighted ensembles did not show any increase in score compared to the unweighted ensembles"
  - **Model heterogeneity:** Including low-performing models (e.g., StockMarkBERT) still helped ensemble—selecting only top performers may reduce diversity

- **Failure signatures:**
  - Corpus A with excluded domain-matched models: F1 drops from 1.0 to 0.92 (Section 5)
  - Corpus B without ensemble: Best single model achieves only 0.823 F1
  - Weighted ensemble with poor weights: Can underperform unweighted if weights amplify noisy models

- **First 3 experiments:**
  1. **Baseline replication:** Run single BERT (Tohoku-BERT) on your corpus with 5-fold stratified CV to establish baseline; verify F1 range matches paper's 0.64-0.82 depending on domain match
  2. **Feature extraction validation:** Extract character-bigram, token-unigram, and phrase-pattern features; confirm dimensionality (4444, 3300, 804) and train RF/Ada classifiers to verify standalone performance
  3. **Minimal ensemble test:** Combine just 2 BERTs + 2 feature-classifier pairs using soft voting; check if improvement over best single model is observed before scaling to full 1482 combinations

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the integrated ensemble approach maintain high performance in authorship attribution tasks with extremely small training samples (e.g., fewer than 10 works per author)?
- **Basis in paper:** [explicit] The authors explicitly identify a gap in the literature regarding real-world forensic problems which "often involve far fewer samples, sometimes fewer than 10," contrasting with their own experimental setup of 20 works per author.
- **Why unresolved:** While the study successfully demonstrated the ensemble's efficacy with 20 samples, it did not test the lower bound of the method's stability for the "ultra-small" sample sizes described in the problem statement.
- **What evidence would resolve it:** Experimental results from the same ensemble architecture applied to datasets containing fewer than 10 works per author, showing statistically significant F1 scores.

### Open Question 2
- **Question:** Does utilizing the full text length of literary works, rather than truncating to the first 510 tokens, yield significantly higher attribution accuracy for this ensemble method?
- **Basis in paper:** [explicit] The authors acknowledge the input constraint in the Discussion section, stating, "The estimated score can be improved using more information in attributing authorship to works with more than 510 tokens."
- **Why unresolved:** The study strictly limited input to the first 510 tokens to fit BERT's architecture, leaving the potential performance gains from analyzing the complete stylistic structure of full novels unexplored.
- **What evidence would resolve it:** Comparative trials using techniques for long-text handling (e.g., sliding windows, Longformer) against the truncated baseline to quantify the performance delta.

### Open Question 3
- **Question:** Can advanced weighting strategies for the integrated ensemble improve performance over simple unweighted soft voting?
- **Basis in paper:** [explicit] The authors report that "The results of the weighted ensemble... showed no superiority over the unweighted method" and conclude that "More research should be done on these issues."
- **Why unresolved:** It remains unclear if the lack of improvement was due to the specific weighting metric used (F1 scores) or if the diverse models in the ensemble genuinely contribute equally regardless of individual performance.
- **What evidence would resolve it:** Implementation of dynamic or confidence-based weighting schemes that demonstrate statistically significant improvements over the unweighted mean probability vector method.

## Limitations
- Narrow empirical scope: Only two Japanese literary corpora with identical experimental design
- Ensemble methodology: Relies on soft voting without adaptive weighting schemes
- Input truncation: 510-token limit may exclude authorship signals present in longer texts
- Domain specificity: Results may not generalize to non-Japanese languages or larger author pools

## Confidence

- **High Confidence:** The ensemble advantage mechanism (complementary information from heterogeneous models) is well-supported by systematic ablation studies and quantitative comparisons across multiple model combinations. The domain-matching effect of pre-training data is strongly evidenced by controlled experiments with corpora of differing overlap.
- **Medium Confidence:** The phrase-pattern feature contribution is plausible but less rigorously validated—the study shows these features appear in top ensembles but does not establish whether this reflects genuine syntactic distinctiveness or statistical artifact from the specific author sets.
- **Low Confidence:** Generalizability to non-Japanese languages, different literary genres, or larger author pools remains untested. The "small-sample" definition (20 works per author) may not represent truly challenging scenarios in authorship attribution.

## Next Checks

1. **Domain Transfer Test:** Evaluate the ensemble approach on a non-Japanese corpus (e.g., English literary works) to verify whether domain-matched pre-training remains critical when language changes but the task remains identical.

2. **Sample Size Sensitivity:** Systematically vary the number of training samples per author (e.g., 5, 10, 20, 50) to identify the minimum threshold where ensemble advantages emerge and whether phrase-pattern features maintain their contribution at extreme small-sample sizes.

3. **Ensemble Composition Analysis:** Conduct targeted experiments removing individual model types (BERTs, feature-classifiers) from the ensemble to quantify each component's marginal contribution and test whether weighted voting with performance-based weights could improve results beyond unweighted soft voting.