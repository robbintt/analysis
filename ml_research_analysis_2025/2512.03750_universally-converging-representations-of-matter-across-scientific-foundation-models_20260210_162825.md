---
ver: rpa2
title: Universally Converging Representations of Matter Across Scientific Foundation
  Models
arxiv_id: '2512.03750'
source_url: https://arxiv.org/abs/2512.03750
tags:
- representations
- each
- structures
- embeddings
- alignment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically investigates whether scientific foundation
  models learn similar internal representations of matter. The researchers analyzed
  nearly 60 models across diverse modalities (SMILES strings, 3D atomic coordinates,
  protein sequences, and natural language) and architectures (equivariant/non-equivariant
  MLIPs, conservative/direct prediction models).
---

# Universally Converging Representations of Matter Across Scientific Foundation Models

## Quick Facts
- arXiv ID: 2512.03750
- Source URL: https://arxiv.org/abs/2512.03750
- Authors: Sathya Edamadaka; Soojung Yang; Ju Li; Rafael Gómez-Bombarelli
- Reference count: 40
- Key outcome: Scientific foundation models show strong representational alignment across modalities and architectures, with performance improvement driving convergence toward shared representations of physical reality.

## Executive Summary
This study systematically investigates representational alignment across nearly 60 scientific foundation models spanning SMILES strings, 3D atomic coordinates, protein sequences, and natural language. Using four alignment metrics (CKNNA, dCor, intrinsic dimensionality, and information imbalance), the researchers found strong cross-modality alignment and convergence of representations as model performance improves. The work reveals two distinct behavioral regimes: in-distribution inputs where training data dominates learned representations, and out-of-distribution inputs where models collapse to architecture-specific low-information manifolds. This establishes representational alignment as a benchmark for foundation-level generality in scientific models.

## Method Summary
The analysis extracts embeddings from the last hidden layer of ~60 pretrained models across five datasets (QM9, OMat24, sAlex, OMol25, RCSB PDB), averaging node-wise embeddings for graph models. Pairwise alignment is computed using CKNNA (k=25) for local neighborhood similarity, dCor for global distance correlation, and TwoNN for intrinsic dimensionality. Information imbalance quantifies asymmetric information content between representations. No training is performed—the study analyzes frozen model representations on held-out validation structures, with energy regression MAE calculated via linear compositional modeling and atomization energy subtraction.

## Key Results
- Strong cross-modality alignment between 3D-based MLIPs, SMILES models, protein models, and LLMs when evaluated on appropriate scientific datasets
- Model representations converge toward shared representations as task performance improves, suggesting learning of common underlying physical principles
- Two distinct regimes: in-distribution (data-governed, high-performing models align closely) vs. out-of-distribution (architecture-governed, models collapse to low-information representations)
- Similar intrinsic dimensionalities across diverse architectures, with LLMs showing higher dimensionality and lower information content compared to scientific models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Better task performance forces models to discover more accurate structure-property relationships governed by the same underlying physical laws, causing latent spaces to become more similar.
- Core assumption: Ground-truth physical principles governing molecular and material behavior are consistent across domains.
- Evidence: Cross-model alignment increases with performance; related work shows similar convergence patterns across domains.
- Break condition: If models achieve low error via shortcuts or memorization that don't reflect physical principles.

### Mechanism 2
- Claim: In-distribution, representations are shaped by training data; out-of-distribution, they're dominated by architectural inductive biases.
- Core assumption: Models haven't internalized universal physical laws that generalize far beyond training distribution.
- Evidence: High-performing models align on familiar inputs but collapse on novel structures to architecture-specific manifolds.
- Break condition: If models successfully learn truly universal physical principles, OOD representations should remain informative and aligned.

### Mechanism 3
- Claim: High performance can occur within a "local optimum" characterized by weak alignment with other high-performing models.
- Core assumption: The "global optimum" captures transferable, physically-grounded features while "local optimum" captures dataset-specific shortcuts.
- Evidence: MACE-OFF achieves good performance but weak alignment with other models, occupying a local optimum vs. global optimum positions.
- Break condition: If benchmark perfectly captures all necessary physical principles and dataset is fully representative.

## Foundational Learning

- **Representational Alignment (CKNNA, dCor, Information Imbalance)**:
  - Why needed here: The entire analysis relies on quantifying how similar two models' latent spaces are.
  - Quick check question: *If Model A's embeddings of a set of molecules have a high CKNNA (0.85) with Model B's embeddings, what does that imply about their local neighborhood structures?* (They are very similar; the same molecules are nearest neighbors in both spaces.)

- **Equivariance vs. Invariance in Architectures**:
  - Why needed here: Different architectures process rotational information differently, affecting intrinsic dimensionality and OOD behavior.
  - Quick check question: *An equivariant model is given a molecule and then the same molecule rotated 90 degrees. How should its output (e.g., force vectors) change?* (The output vectors should also rotate 90 degrees, preserving physical consistency.)

- **In-Distribution vs. Out-of-Distribution (OOD)**:
  - Why needed here: The core finding of two regimes hinges on this distinction.
  - Quick check question: *A model trained only on equilibrium crystal structures (sAlex) is evaluated on high-energy, distorted structures (OMol25). Is this an ID or OOD evaluation? What behavioral regime would you expect?* (OOD. Expect representation collapse dominated by architectural biases.)

## Architecture Onboarding

- **Component Map**:
  Input Modalities -> Model Zoo (59 models: MLIPs, string-based, protein models, LLMs) -> Dataset Probes (QM9, OMat24, sAlex, OMol25, RCSB) -> Analysis Engine (Embedding extraction -> Metric computation -> Visualization)

- **Critical Path**:
  1. Embedding Extraction: Run inference on N=50,000 structures, extract final hidden layer embeddings, average node-wise embeddings
  2. Metric Computation: For each model pair, compute CKNNA (k=25), dCor, and intrinsic dimensionality; store in pairwise alignment matrix
  3. Analysis & Interpretation: Look for high off-diagonal values in alignment matrix, correlate with performance, use Information Imbalance to diagnose data vs architecture regimes

- **Design Tradeoffs**:
  - Model Selection: Broad zoo increases generalizability but adds computational cost
  - Metric Choice: Multiple metrics (local + global) provide more robust picture than single measure
  - Embedding Layer: Last hidden layer is standard but may mix general and task-specific features

- **Failure Signatures**:
  - Low cross-model alignment despite high performance indicates local optimum
  - Uniformly low intrinsic dimensionality suggests probe dataset is too simple
  - High CKNNA but low dCor suggests local neighborhoods align but global structure differs
  - LLM embeddings as high-dimensional noise occurs without proper scientific prompting

- **First 3 Experiments**:
  1. Reproduce a key alignment matrix: Compute 3x3 CKNNA matrix for Orb V3, MACE-MP, and Molformer on QM9 subset to verify cross-modality alignment
  2. Diagnose a local optimum: Compare alignment of MACE-OFF and Orb V3 Conservative to UMA Medium on QM9 to confirm weaker alignment despite good performance
  3. Observe an OOD collapse: Compute Information Imbalance for materials model on OMat24 (ID) vs OMol25 (OOD) to visualize shift from data-governed to architecture-governed clustering

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What minimum dataset diversity and scale thresholds are required for scientific models to achieve "foundational" generality across chemical domains?
- Basis: Authors state models will require substantially more diverse training data spanning equilibrium/non-equilibrium regimes
- Why unresolved: Paper identifies current models as data-limited but doesn't quantify sufficient diversity
- What evidence would resolve it: Systematic training experiments varying dataset composition and size

### Open Question 2
- Question: Will representational alignment continue increasing as scientific foundation models scale, or does it plateau?
- Basis: Abstract states work "can track the emergence of universal representations" implying trajectory remains uncertain
- Why unresolved: Current analysis captures snapshot of existing models; longitudinal trends cannot be determined
- What evidence would resolve it: Repeat alignment analysis as larger models are released, plotting alignment vs parameter count over time

### Open Question 3
- Question: Can models be explicitly trained to maximize alignment with high-performing representations, and does this improve downstream transfer?
- Basis: Paper notes alignment can "guide the selection and distillation" but doesn't demonstrate alignment as training objective
- Why unresolved: Alignment is only analyzed post-hoc; whether enforcing alignment during training remains untested
- What evidence would resolve it: Training models with auxiliary losses encouraging alignment with foundational models

### Open Question 4
- Question: Why do models collapse onto architecture-specific low-information manifolds for out-of-distribution inputs, and can this failure mode be prevented?
- Basis: Authors find OOD collapse to low-information representations but mechanistic cause is not identified
- Why unresolved: Phenomenon is characterized but not explained; whether stems from capacity, training dynamics, or inductive bias constraints is unclear
- What evidence would resolve it: Probing experiments varying architecture, capacity, and training data scope

## Limitations
- Analysis relies on proxy metrics rather than ground-truth physical representations
- Convergence hypothesis assumes better task performance implies closer approximation to physical reality, which may not hold for dataset-specific shortcuts
- OOD collapse interpretation assumes models haven't internalized universal principles, difficult to verify empirically
- Local optimum concept based on alignment metrics without direct verification of transferable physical features

## Confidence
**High confidence**: Cross-modality alignment findings (CKNNA and dCor consistently show similar representations). ID vs OOD behavioral regime switch is clearly demonstrated.

**Medium confidence**: Convergence-as-performance-improves relationship (could be influenced by dataset composition). Local optimum identification for MACE-OFF (based on single case study).

**Low confidence**: Interpretation that collapsed OOD representations represent "low-information" states rather than alternative valid representations of underlying physics.

## Next Checks
1. Validate local optimum hypothesis: Fine-tune MACE-OFF and Orb V3 Conservative on small diverse dataset and measure whether less-aligned model shows greater performance degradation or representation shift.

2. Test convergence causality: Hold out high-performing models, fine-tune weaker models toward these specific representations rather than just task performance, and measure whether this improves generalization more than fine-tuning toward task performance alone.

3. Probe OOD collapse mechanism: Apply targeted intervention (architectural modifications or data augmentation with OOD examples) to OOD-collapsed model to test whether representation quality can be recovered without simply expanding training distribution.