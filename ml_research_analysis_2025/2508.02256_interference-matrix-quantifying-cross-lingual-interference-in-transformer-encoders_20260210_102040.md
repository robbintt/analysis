---
ver: rpa2
title: 'Interference Matrix: Quantifying Cross-Lingual Interference in Transformer
  Encoders'
arxiv_id: '2508.02256'
source_url: https://arxiv.org/abs/2508.02256
tags:
- latn
- cyrl
- language
- languages
- interference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper constructs an interference matrix to quantify cross-lingual
  interference in encoder-only Transformer models across 83 languages by training
  and evaluating small BERT-like models on all possible language pairs. The analysis
  reveals that interference is asymmetrical, does not align with traditional linguistic
  features like language family or embedding similarity, but correlates with writing
  script, and that low-resource languages are more vulnerable to negative interference.
---

# Interference Matrix: Quantifying Cross-Lingual Interference in Transformer Encoders

## Quick Facts
- arXiv ID: 2508.02256
- Source URL: https://arxiv.org/abs/2508.02256
- Authors: Belen Alastruey; João Maria Janeiro; Alexandre Allauzen; Maha Elbayad; Loïc Barrault; Marta R. Costa-jussà
- Reference count: 40
- Constructs an interference matrix to quantify cross-lingual interference across 83 languages using BERT-like models

## Executive Summary
This paper introduces the interference matrix as a novel framework for quantifying and analyzing cross-lingual interference in encoder-only Transformer models. Through systematic training of small BERT-like models on all possible language pairs across 83 languages, the study reveals that interference patterns are asymmetrical, do not correlate with traditional linguistic features like language family, but instead show strong correlation with writing script. The research demonstrates that low-resource languages are particularly vulnerable to negative interference effects, and validates that the interference matrix can effectively predict performance drops in downstream tasks.

## Method Summary
The authors construct an interference matrix by training separate BERT-like encoder models for each of the 3,403 possible language pairs among 83 languages, using identical data volumes per pair. For each model, they measure perplexity on the target language's test set to quantify interference. The matrix captures how training on one language affects performance on another, revealing asymmetrical interference patterns. The analysis includes ablation studies on vocabulary overlap and writing script, and validates predictions against downstream task performance. The experimental setup uses small encoder-only transformers with controlled training conditions to isolate interference effects.

## Key Results
- Interference is asymmetrical across language pairs, with patterns not explained by traditional linguistic features
- Writing script shows strong correlation with interference patterns, while language family does not
- Low-resource languages experience significantly more negative interference than high-resource languages
- The interference matrix effectively predicts downstream task performance drops

## Why This Works (Mechanism)
The interference matrix works by systematically quantifying how training on one language affects performance on another through controlled pairwise training experiments. By training separate models for each language pair with identical data volumes, the authors isolate interference effects from other variables. The asymmetry emerges because different language pairs have varying degrees of vocabulary overlap, script compatibility, and structural interference during training. The matrix captures these complex interactions that cannot be explained by simple linguistic distance measures, revealing that script-based interference mechanisms dominate over traditional linguistic features.

## Foundational Learning
- **Cross-lingual interference**: Why needed - to understand negative transfer in multilingual models; Quick check - measure performance drop when training on multiple languages
- **Vocabulary overlap metrics**: Why needed - to quantify lexical interference potential; Quick check - calculate token intersection between language pairs
- **Writing script impact**: Why needed - to identify systematic interference sources; Quick check - group languages by script and analyze interference patterns
- **Asymmetrical interference**: Why needed - to capture directional transfer effects; Quick check - compare interference from L1→L2 vs L2→L1
- **Low-resource vulnerability**: Why needed - to identify fairness issues in multilingual models; Quick check - correlate interference with resource availability
- **Perplexity-based interference measurement**: Why needed - to quantify interference in unsupervised settings; Quick check - measure target language perplexity after cross-lingual training

## Architecture Onboarding

**Component Map**
Language Pairs (83 languages) -> BERT-like Models (3,403 models) -> Interference Matrix -> Analysis Pipeline -> Validation on Downstream Tasks

**Critical Path**
Training -> Interference Measurement -> Matrix Construction -> Pattern Analysis -> Downstream Validation

**Design Tradeoffs**
Controlled pairwise training provides clean interference isolation but may not reflect real multilingual training dynamics. Small model size enables comprehensive experimentation but may not capture interference in larger architectures. Identical data volumes per pair control for data effects but create unrealistic resource distributions.

**Failure Signatures**
If interference patterns show symmetry when they should be asymmetrical, this indicates issues with the training setup or measurement. If script-based correlations disappear, this suggests problems with tokenization or vocabulary construction. If low-resource languages don't show increased vulnerability, this may indicate insufficient interference magnitude or measurement issues.

**3 First Experiments**
1. Replicate interference asymmetry on a smaller language subset to verify core findings
2. Test interference matrix predictions on a held-out downstream task
3. Perform ablation study on vocabulary size to understand its impact on interference patterns

## Open Questions the Paper Calls Out
None

## Limitations
- Synthetic training scenarios may not reflect real-world multilingual training dynamics with imbalanced data distributions
- Separate models per language pair may not capture interference patterns in truly multilingual models
- Interference matrix construction focuses on vocabulary overlap and script alignment, potentially missing deeper structural interference patterns
- Analysis limited to encoder-only transformers, may not generalize to encoder-decoder architectures

## Confidence

**High confidence**: Low-resource languages experience more negative interference; interference correlates with writing script rather than language family; interference matrix predicts downstream task performance drops

**Medium confidence**: Interference is fundamentally asymmetrical across language pairs; specific mechanisms driving script-based interference patterns

**Low confidence**: How interference matrix insights should guide practical multilingual model design; broader claims about real-world applicability

## Next Checks
1. **Real multilingual training validation**: Replicate key interference asymmetry findings using genuinely multilingual models trained on all 83 languages simultaneously, comparing interference patterns to those observed in the pairwise training setup.

2. **Downstream task interference validation**: Extend the interference matrix evaluation to a broader range of downstream tasks to verify whether interference patterns remain consistent across different task types and domains.

3. **Architecture generalization study**: Test whether the identified interference patterns and their drivers (particularly the script-based correlations) hold across different transformer architectures including encoder-decoder models, larger model sizes, and alternative pretraining objectives.