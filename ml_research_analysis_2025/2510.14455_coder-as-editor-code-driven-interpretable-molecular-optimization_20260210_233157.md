---
ver: rpa2
title: 'Coder as Editor: Code-driven Interpretable Molecular Optimization'
arxiv_id: '2510.14455'
source_url: https://arxiv.org/abs/2510.14455
tags:
- molecular
- editing
- smiles
- code
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MECo is a framework that reformulates molecular optimization as
  code generation to bridge reasoning and execution. It uses a reasoning LLM to generate
  interpretable editing actions from a molecule and property goal, then a code LLM
  translates these actions into executable scripts (e.g., RDKit) for precise structural
  modifications.
---

# Coder as Editor: Code-driven Interpretable Molecular Optimization

## Quick Facts
- arXiv ID: 2510.14455
- Source URL: https://arxiv.org/abs/2510.14455
- Reference count: 40
- Primary result: MECo achieves >98% editing accuracy on realistic molecular tasks, outperforming direct SMILES generation by 38-86 percentage points in consistency

## Executive Summary
MECo is a framework that reformulates molecular optimization as code generation to bridge reasoning and execution. It uses a reasoning LLM to generate interpretable editing actions from a molecule and property goal, then a code LLM translates these actions into executable scripts (e.g., RDKit) for precise structural modifications. Trained solely on synthetic edits, MECo achieves over 98% accuracy on realistic editing tasks from chemical reactions and bioactivity data, significantly outperforming direct SMILES generation baselines. On downstream optimization benchmarks, MECo improves consistency between editing intentions and resulting molecules by 38-86 percentage points to over 90%, while achieving higher success rates and better structural preservation. By aligning natural language reasoning with executable code, MECo enables controllable, interpretable, and reliable molecular design for drug discovery.

## Method Summary
MECo addresses molecular optimization by generating interpretable editing actions followed by code execution. The framework takes an input molecule and property goal, uses a reasoning LLM (e.g., DeepSeek-R1) to produce natural language editing instructions, then employs a fine-tuned code LLM (Qwen2.5-Coder-7B-Instruct) to translate these instructions into executable RDKit code. The synthetic training data is constructed from ZINC molecules via iterative moiety replacement using predefined substituent and linker pools. Evaluation uses realistic editing tasks from USPTO-MIT reactions and ChEMBL35 MMP pairs, measuring editing accuracy, validity, success, structural similarity, and consistency between intentions and outputs.

## Key Results
- MECo achieves >98% editing accuracy on realistic molecular editing tasks
- Consistency between editing intentions and resulting molecules improves by 38-86 percentage points over direct SMILES generation baselines
- Code-driven approach preserves structural similarity while achieving higher success rates in molecular optimization
- Framework generalizes from synthetic-only training to real chemical reactions and bioactivity data

## Why This Works (Mechanism)
The framework bridges the reasoning-execution gap in molecular optimization by separating natural language interpretation from code execution. The reasoning LLM handles semantic understanding of property goals and generates interpretable editing actions, while the code LLM focuses on precise molecular transformations through executable RDKit scripts. This separation allows each component to specialize: the reasoning LLM captures chemical intent while the code LLM ensures syntactic correctness and precise implementation of structural modifications.

## Foundational Learning
- **Moiety replacement and attachment points**: Why needed: Core technique for generating synthetic training data by systematically replacing molecular fragments. Quick check: Verify that generated training examples correctly identify attachment points and preserve molecular connectivity.
- **SMILES syntax and dummy atoms**: Why needed: SMILES string representation is sensitive to atom ordering and dummy atom placement affects model performance. Quick check: Test model robustness to different SMILES canonicalizations and attachment point positions.
- **RDKit code generation for molecular transformations**: Why needed: Executable code provides precise control over molecular modifications compared to direct generation. Quick check: Validate that generated RDKit scripts correctly implement specified editing actions.
- **MMP (Matched Molecular Pair) analysis**: Why needed: Provides realistic evaluation benchmarks from bioactivity data. Quick check: Confirm extracted MMP pairs maintain chemical validity and represent meaningful transformations.

## Architecture Onboarding

### Component Map
Reasoning LLM (DeepSeek-R1) -> Action generation -> Code LLM (Qwen2.5-Coder-7B-Instruct) -> RDKit execution -> Optimized molecule

### Critical Path
1. Input: (SMILES, property goal) pair
2. Reasoning LLM generates editing actions with SMARTS patterns
3. Code LLM translates actions to RDKit scripts
4. RDKit executes code to produce optimized molecule
5. Output validated for chemical validity and property improvement

### Design Tradeoffs
- Separation of reasoning and execution enables specialized optimization but introduces potential error propagation
- Synthetic training data is abundant but may not capture all real-world chemical transformations
- Code generation ensures precision but requires robust RDKit script generation capabilities

### Failure Signatures
- Low execution accuracy on target-specific terminal replacements vs reaction-derived edits
- Invalid SMILES from direct generation baseline (should be >98% accuracy for code-based approach)
- Inconsistent attachment point mapping in symmetric core replacements

### 3 First Experiments
1. Test editing accuracy on USPTO-MIT reaction pairs to measure real-world performance
2. Compare consistency rates between code generation and direct SMILES generation approaches
3. Evaluate structural similarity preservation on ChemCoTBench optimization tasks

## Open Questions the Paper Calls Out

### Open Question 1
To what extent does SMILES syntax sensitivity, specifically the positioning of dummy atoms in string representations, limit the generalization of the code generation model? The paper identifies a performance discrepancy between reaction-derived and target-specific edits attributable to dummy atom placement (start vs. middle of string) and states that "LLMs struggle to generalize over SMILES syntax." The paper identifies the issue but does not propose or test a robustness solution (e.g., canonicalization or data augmentation) to neutralize the impact of syntax variance on code generation accuracy. An ablation study evaluating the model on test sets where SMILES roots are randomized or canonicalized differently to measure the variance in execution accuracy would resolve this.

### Open Question 2
Can the model effectively generalize to complex molecular transformations, such as ring expansions or multi-step rearrangements, that exceed the scope of the synthetic "moiety replacement" training distribution? The method is trained on "limited moiety substitutions," but the conclusion suggests it enables reliable discovery workflows, implying a need to handle diverse chemical reactions beyond simple substitutions. The evaluation focuses on single-point attachments and specific reaction types; it is unclear if the "Code-driven" approach captures the logic of more complex chemical transformations not covered in the synthetic pool. Benchmarking performance on hold-out sets containing complex ring distortions or non-local structural changes that do not fit the terminal/core replacement definitions would resolve this.

### Open Question 3
How does the cascaded framework handle error propagation when the upstream reasoning LLM generates chemically invalid or hallucinated editing intentions? The framework relies on a separation between reasoning and execution, but the paper primarily evaluates cases where reasonable rationales are provided or where ground truth edits exist. The paper does not explicitly analyze the Coder LLM's ability to reject or correct semantically invalid natural language instructions from the Reasoning LLM. An error analysis measuring the Coder LLM's success rate in detecting, rejecting, or correcting syntactically correct but chemically impossible editing actions generated by the reasoning module would resolve this.

## Limitations
- SMILES syntax sensitivity, particularly dummy atom positioning, affects model generalization
- Error propagation risk when upstream reasoning LLM generates invalid editing intentions
- Limited evaluation on complex molecular transformations beyond simple moiety replacements

## Confidence

**Major Claim Confidence:**
- Editing accuracy on realistic tasks: **High** (benchmark details and results clearly specified)
- Consistency improvement over direct generation: **High** (metric definition and comparison methodology explicit)
- Generalization from synthetic-only training: **Medium** (training data construction clear, but exact hyperparameters unknown)

## Next Checks
1. Reconstruct synthetic training data using ZINC + iterative moiety replacement; verify RDKit code generation matches provided examples in Tables 3-4.
2. Implement USPTO-MIT and ChEMBL MMP extraction pipelines; test attachment point normalization for symmetric core replacements.
3. Compare execution accuracy on terminal vs core replacements to isolate SMILES syntax sensitivity.