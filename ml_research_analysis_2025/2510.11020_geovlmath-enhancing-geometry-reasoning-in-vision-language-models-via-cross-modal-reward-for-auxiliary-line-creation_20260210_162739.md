---
ver: rpa2
title: 'GeoVLMath: Enhancing Geometry Reasoning in Vision-Language Models via Cross-Modal
  Reward for Auxiliary Line Creation'
arxiv_id: '2510.11020'
source_url: https://arxiv.org/abs/2510.11020
tags:
- auxiliary-line
- diagram
- lines
- auxiliary
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GeoVLMath addresses the challenge of auxiliary-line reasoning in
  solid geometry for vision-language models by representing auxiliary-line constructions
  as structured textual descriptions rather than rendered diagrams. A cross-modal
  reward model evaluates diagram-text alignment between generated auxiliary-line descriptions
  and ground-truth auxiliary-line diagrams, providing geometry-aware supervision without
  requiring executable code or coordinate information.
---

# GeoVLMath: Enhancing Geometry Reasoning in Vision-Language Models via Cross-Modal Reward for Auxiliary Line Creation

## Quick Facts
- arXiv ID: 2510.11020
- Source URL: https://arxiv.org/abs/2510.11020
- Reference count: 38
- Primary result: Cross-modal reward framework improves geometry reasoning without requiring executable code or coordinate supervision.

## Executive Summary
GeoVLMath introduces a novel framework for enhancing geometry reasoning in vision-language models by focusing on auxiliary-line construction in solid geometry. Rather than rendering diagrams, the method uses structured textual descriptions of auxiliary lines and a cross-modal reward model to evaluate alignment between these descriptions and ground-truth diagrams. This approach avoids the complexity of executable code while providing geometry-aware supervision. The framework is trained via reinforcement learning using Group Relative Policy Optimization (GRPO) and evaluated on a newly constructed dataset of real exam problems, achieving competitive performance against both open-source and proprietary models.

## Method Summary
GeoVLMath addresses auxiliary-line reasoning by representing constructions as concise textual descriptions, which are evaluated by a cross-modal reward model for alignment with ground-truth diagrams. The reward model assesses geometry consistency without requiring executable code or coordinate data. This reward signal guides reinforcement learning (using GRPO) to optimize auxiliary-line construction. The method is supported by AuxSolidMath, a dataset of 3,018 real exam problems with annotated diagrams and auxiliary-line descriptions. Evaluation on GeoAuxBench and other benchmarks shows that this geometry-aware supervision outperforms explicit diagram rendering or scaling model size alone.

## Key Results
- Achieves competitive and often superior performance compared to strong open-source and proprietary VLMs, including code-driven rendering methods and larger models.
- Demonstrates that geometry-aware auxiliary-line supervision is more effective than explicit diagram rendering or scaling model size alone.
- Constructed AuxSolidMath dataset of 3,018 real exam solid geometry problems with aligned diagrams and auxiliary-line annotations.

## Why This Works (Mechanism)
The cross-modal reward framework provides geometry-aware supervision by evaluating alignment between generated textual auxiliary-line descriptions and ground-truth diagrams. This avoids the need for executable code or coordinate information, which can be complex and brittle. By focusing on concise textual descriptions, the model learns to generate auxiliary lines that are geometrically consistent with the problem, improving reasoning without the overhead of diagram rendering.

## Foundational Learning
- **Cross-modal reward modeling**: Used to evaluate alignment between generated text and visual diagrams without requiring code execution; needed for geometry-aware supervision.
- **Reinforcement learning with GRPO**: Optimizes auxiliary-line construction based on reward signals; needed to iteratively improve geometry reasoning.
- **Textual auxiliary-line representation**: Encodes geometric constructions as structured descriptions; needed to avoid complexity of diagram rendering.
- **Vision-language alignment**: Ensures generated descriptions are consistent with visual inputs; critical for accurate geometry reasoning.
- **Dataset construction for geometry reasoning**: Building paired diagram-text data for training and evaluation; needed to support supervised learning.
- **Evaluation on geometry benchmarks**: Measures model performance on auxiliary-line construction; needed to validate approach.

## Architecture Onboarding
- **Component map**: Input diagram/text -> Auxiliary-line generator (LLM) -> Cross-modal reward model -> GRPO optimizer -> Updated generator
- **Critical path**: Auxiliary-line description generation → cross-modal reward evaluation → RL optimization → improved geometry reasoning
- **Design tradeoffs**: Textual vs. rendered diagram representations; geometry-aware supervision vs. explicit code execution; dataset size vs. annotation complexity
- **Failure signatures**: Irrelevant or mis-specified auxiliary-line descriptions; reward model misalignment; poor geometry consistency in generated lines
- **First experiments**:
  1. Ablation study: Compare cross-modal reward supervision to synthetic data or heuristic rewards.
  2. Error analysis: Analyze failures to determine if errors stem from ambiguous descriptions or reward model limitations.
  3. Generalization test: Evaluate on out-of-domain geometry problems and multi-step reasoning tasks.

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can the cross-modal reward framework be adapted to utilize weak or self-supervised signals to remove the dependency on ground-truth auxiliary-line diagrams?
- Basis in paper: [explicit] The authors state in the Limitations section that "exploring more scalable forms of weak or self-supervised reward signals is a promising direction for future research."
- Why unresolved: The current method relies on expensive paired data (original + annotated diagrams), restricting the dataset size and creation complexity.
- What evidence would resolve it: Successful training of a model using consistency checks or theorem verification as rewards (without ground-truth diagrams) that achieves comparable Pass@1 scores on GeoAuxBench.

### Open Question 2
- Question: Can integrating diffusion-based drawing modules improve the robustness of auxiliary-line construction by providing iterative visual corrections?
- Basis in paper: [explicit] In Appendix G.2 (Failure Cases), the authors note they "will explore diffusion-based drawing modules that render auxiliary lines directly on the original diagram and support iterative correction as part of future work."
- Why unresolved: Current failure cases include irrelevant descriptions or mis-specified coordinates; visual rendering might allow the model to self-correct spatial errors that text descriptions miss.
- What evidence would resolve it: Ablation studies comparing error rates between the current textual generation and a hybrid system incorporating diffusion-based rendering on the "Hard" subset of GeoAuxBench.

### Open Question 3
- Question: Does the framework generalize to geometric constructions requiring longer or more complex multi-step descriptions beyond the "concise high-level descriptions" currently evaluated?
- Basis in paper: [explicit] The authors acknowledge in the Limitations section that their "experiments primarily consider settings where auxiliary-line constructions admit concise high-level descriptions."
- Why unresolved: Complex problems might require dense, sequential constructions where the current cross-modal reward (comparing the full description to the diagram) might fail due to intermediate alignment issues.
- What evidence would resolve it: Evaluation on a dataset of Olympiad-level geometry problems requiring multi-stage auxiliary constructions to verify if the reward model maintains high alignment accuracy.

## Limitations
- Current method relies on expensive paired data (original + annotated diagrams), limiting dataset size and creation complexity.
- Experiments primarily consider settings where auxiliary-line constructions admit concise high-level descriptions, not complex multi-step problems.
- The approach does not address potential biases in the cross-modal reward model or explore sensitivity to hyperparameter choices.

## Confidence
- Cross-modal reward model improves geometry reasoning: **Medium**
- Textual auxiliary-line supervision outperforms diagram rendering: **Medium**
- GeoVLMath achieves superior performance across benchmarks: **Medium**
- Auxiliary-line textual format is sufficient and unambiguous: **Low**

## Next Checks
1. Perform ablation studies comparing cross-modal reward supervision to synthetic data or heuristic rewards to isolate the unique contribution of the reward model.
2. Conduct error analysis on model failures to determine if errors stem from ambiguous textual descriptions or limitations in the reward model.
3. Test generalization on out-of-domain geometry problems and multi-step reasoning tasks to assess robustness and scalability.