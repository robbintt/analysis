---
ver: rpa2
title: 'COMPASS: A Multi-Turn Benchmark for Tool-Mediated Planning & Preference Optimization'
arxiv_id: '2510.07043'
source_url: https://arxiv.org/abs/2510.07043
tags:
- user
- hotel
- agents
- agent
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: COMPASS introduces a benchmark for evaluating large language model
  agents on constrained preference optimization in realistic travel planning scenarios.
  The benchmark features a controllable user simulator, realistic databases for flights,
  hotels, and permits, and a comprehensive tool ecosystem mirroring commercial booking
  platforms.
---

# COMPASS: A Multi-Turn Benchmark for Tool-Mediated Planning & Preference Optimization

## Quick Facts
- **arXiv ID:** 2510.07043
- **Source URL:** https://arxiv.org/abs/2510.07043
- **Reference count:** 40
- **Primary result:** COMPASS reveals critical gaps in LLM agents' ability to optimize preferences while satisfying constraints in realistic multi-turn travel planning scenarios

## Executive Summary
COMPASS introduces a benchmark for evaluating large language model agents on constrained preference optimization in realistic travel planning scenarios. The benchmark features a controllable user simulator, realistic databases for flights, hotels, and permits, and a comprehensive tool ecosystem mirroring commercial booking platforms. Agents must satisfy hard constraints while optimizing soft user preferences across three complexity levels, from hotel-only to complete itineraries with permits.

Experiments with frontier models reveal two critical gaps: an acceptable-optimal gap where agents reliably meet constraints but fail to optimize preferences, and a plan-coordination gap where performance collapses on multi-service coordination tasks, especially for open-source models. GPT-5 demonstrates superior performance across both constraint satisfaction and preference optimization, while open-source Qwen3-32B achieves competitive results on simpler tasks. The benchmark provides a rigorous framework for diagnosing and improving agentic reasoning capabilities in realistic planning scenarios.

## Method Summary
The benchmark evaluates agents through interaction with a dynamic GPT-5 user simulator and a tool ecosystem wrapping realistic databases for hotels, flights, and permits. Tasks are structured as constrained preference optimization problems where agents must satisfy hard constraints (dates, budget, occupancy) while optimizing soft preferences (cost minimization or feature maximization). The evaluation uses exhaustive search to establish ground truth solutions, measuring both acceptable rate (constraint satisfaction) and optimality rate (preference optimization). The benchmark features three complexity levels: Level I (hotel only), Level II (hotel + flight), and Level III (hotel + flight + permit), with progressive constraint revelation across multiple turns.

## Key Results
- All models exhibit an acceptable-optimal gap, reliably satisfying constraints but failing to optimize preferences
- Performance on multi-service coordination tasks collapses, particularly for open-source models
- GPT-5 achieves superior performance across both constraint satisfaction and preference optimization
- Qwen3-32B demonstrates competitive performance on simpler Level I tasks but struggles with coordination
- The acceptable-optimal gap widens as search complexity increases, requiring more tool calls

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Agents exhibit an "acceptable-optimal gap" where they reliably satisfy hard constraints (feasibility) but fail to maximize soft preferences (optimality) because they terminate search prematurely upon finding a valid solution.
- **Mechanism:** As search complexity (the number of distinct queries required to guarantee the global optimum) increases, agents tend to settle for the first "acceptable" itinerary rather than exploring the combinatorial solution space to find the best utility score.
- **Core assumption:** Agents lack an intrinsic incentive or "lookahead" mechanism to continue searching once a constraint-valid option is found.
- **Evidence anchors:**
  - [abstract] "uncover two critical gaps: (i) an acceptable-optimal gap, where agents reliably meet constraints but fail to optimize preferences."
  - [section] Section 5.1 (Optimization Complexity) demonstrates that optimality rates decline as the number of required searches increases, even for reasoning models like GPT-5.
  - [corpus] Neighbor paper "Do LLMs Recognize Your Latent Preferences?" suggests models struggle to discover latent preferences without explicit articulation, reinforcing the difficulty of implicit optimization.
- **Break condition:** If the agent is equipped with a verification step that explicitly compares multiple valid solutions against the utility objective before responding.

### Mechanism 2
- **Claim:** Performance on multi-service tasks (flights + hotels + permits) collapses due to a "plan-coordination gap," where agents fail to propagate temporal and logistical constraints across different service domains.
- **Mechanism:** Tasks requiring cross-service alignment (e.g., matching flight arrival times to hotel check-in and permit availability) require maintaining a coherent world state. When agents must orchestrate multiple tool calls (APIs) for different services, constraint propagation fails, leading to temporal misalignments or invalid combinations.
- **Core assumption:** Agent performance relies heavily on the ability to maintain "state" across multiple tool calls, a capability found to be significantly weaker in open-source models.
- **Evidence anchors:**
  - [abstract] "performance collapses on multi-service (flight and hotel) coordination tasks, especially for open-source models."
  - [section] Figure 4(A) and Section 5.1 show a sharp decline in performance from Level I to Level III, particularly for open-source models like Qwen3 which struggle with "cross-services coordination."
  - [corpus] Neighbor paper "SWEET-RL" confirms that multi-turn credit assignment is a significant challenge for current RL algorithms, relevant to the failure in coordinating sequential tool uses.
- **Break condition:** If the agent uses a monolithic API that handles cross-service constraints server-side, or if a planning module explicitly decomposes the global constraint into local sub-constraints.

### Mechanism 3
- **Claim:** Multi-turn dynamics, driven by a dynamic user simulator, degrade agent performance by requiring incremental state updates and reasoning about "revealed" constraints.
- **Mechanism:** The user simulator uses "progressive constraint revelation" (revealing details over turns) and diverse personas (e.g., "suspicious" or "low-trust"). This forces the agent to fuse new information with past context. Failure to update the internal plan upon new revelation leads to irrelevant or invalid recommendations.
- **Core assumption:** The agent possesses a working memory or "notebook" capability to track conversation history and constraint evolution.
- **Evidence anchors:**
  - [section] Section 3.4 describes the "Dynamic Prompting" strategy where "users disclose task information incrementally."
  - [section] Figure 5 (Case Study) shows GPT-5 successfully using a "notebook" tool to track reasoning, whereas other models struggle to maintain coherence.
  - [corpus] Neighbor paper "Expectation Confirmation Preference Optimization" highlights the complexity of managing user expectations in multi-turn settings, supporting the difficulty of this dynamic.
- **Break condition:** If the user simulator reveals all hard constraints and preferences in the first turn (High Upfrontness).

## Foundational Learning

- **Concept: Constrained Preference Optimization**
  - **Why needed here:** This is the core mathematical framing of the COMPASS benchmark. One must distinguish between satisfying a "feasibility set" (hard constraints like budget, dates) and maximizing a "utility function" (soft preferences like minimizing cost or maximizing amenities).
  - **Quick check question:** Can you explain why finding a valid hotel is a different computational task than finding the *cheapest* valid hotel?

- **Concept: Tool Orchestration & State Management**
  - **Why needed here:** The benchmark evaluates an agent's ability to call APIs (tools) not just for retrieval, but for building a coherent plan. Understanding how to map natural language intent to structured API parameters and maintain state across calls is critical.
  - **Quick check question:** If a user reveals a "permit requirement" on Turn 3, how should the agent modify the hotel and flight search parameters from Turns 1 and 2?

- **Concept: User Simulation & Dynamic Prompting**
  - **Why needed here:** Unlike static benchmarks, COMPASS uses an LLM-driven user simulator with dynamic prompts. Understanding how to control user behavior (e.g., "Upfrontness," "Attention-to-detail") via prompt engineering is essential for reproducing or extending the evaluation.
  - **Quick check question:** What specific instruction in the user simulator prompt causes a user to reveal constraints progressively rather than all at once?

## Architecture Onboarding

- **Component map:** User Simulator (GPT-5) -> Tool Ecosystem (APIs wrapping databases) -> Target Agent -> Evaluator (ground truth comparison)
- **Critical path:**
  1.  **Initialization:** Sample a task (constraints + preferences) and user persona.
  2.  **Interaction:** Simulator reveals partial info → Agent reasons/calls tools → Agent returns JSON.
  3.  **Termination:** Agent provides formal recommendation (`recommend_itinerary`) or max turns reached.
  4.  **Evaluation:** Match returned Package IDs against the pre-computed ground truth solution library.
- **Design tradeoffs:**
  - **Exhaustive Search Ground Truth vs. Realism:** The authors use exhaustive search to define "optimal," which guarantees accurate evaluation but limits the database scale (100k+ items is manageable, but not infinite web-scale).
  - **Static vs. Dynamic Simulator:** A static script is reproducible; the dynamic LLM simulator is more realistic but introduces variance (addressed in the paper via stability tests in Appendix A.2).
- **Failure signatures:**
  - **"Acceptable-Only" Settling:** Agent returns a valid itinerary that is significantly more expensive than the ground-truth optimum.
  - **Constraint Amnesia:** Agent recommends a flight that departs before the hotel check-in date or violates a late-revealed constraint (e.g., "direct flight only").
  - **Tool Hallucination:** Agent invents a `package_id` that does not exist in the database (mitigated by the `recommend_itinerary` validation tool).
- **First 3 experiments:**
  1.  **Baseline Validation (Level I):** Run the agent on "Hotel Only" tasks to isolate preference optimization errors from coordination errors. Confirm if the agent can at least find the cheapest hotel when dates are fixed.
  2.  **Coordination Stress Test (Level III):** Force the agent to coordinate Flights + Hotels + Permits. Analyze tool call traces to see if the agent queries flights *before* verifying permit availability (a common planning error).
  3.  **Simulator Ablation:** Run a specific task scenario with "High Upfrontness" (all info revealed at once) vs. "Low Upfrontness." Measure the delta in Acceptable Rate to quantify the cost of multi-turn reasoning.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What architectural or training interventions can close the acceptable-optimal gap, where agents reliably satisfy constraints but fail to optimize preferences?
- Basis in paper: [explicit] The authors identify this as a "critical gap" affecting all models, with ~20% lower optimal rates than acceptable rates.
- Why unresolved: The paper diagnoses the gap but does not propose or test specific mitigation strategies.
- What evidence would resolve it: Ablation studies comparing agents trained with preference-optimization objectives vs. standard instruction-tuning, or reasoning modules that explicitly compare candidate solutions before selecting.

### Open Question 2
- Question: Can structured agent workflows (planning–execution pipelines, preference-tracking memory, or multi-agent collaboration) improve performance on multi-service coordination tasks?
- Basis in paper: [explicit] The conclusion states: "Future work should explore structured agent workflows... to address limitations in native tool use."
- Why unresolved: The benchmark evaluates only single-agent, native tool-use configurations without external memory or multi-agent orchestration.
- What evidence would resolve it: Experiments comparing baseline agents against variants with explicit preference-tracking modules or hierarchical planning architectures on Level II–III tasks.

### Open Question 3
- Question: How robust are agents when user preferences change mid-dialogue or remain underspecified, beyond the progressive revelation patterns tested?
- Basis in paper: [explicit] The discussion notes: "real users exhibit greater variability—changing goals mid-dialogue, providing underspecified inputs, or asking for clarifications."
- Why unresolved: The current simulator controls constraint revelation but does not model goal revision or ambiguous inputs.
- What evidence would resolve it: Extending the simulator with goal-change scenarios and measuring agent recovery rates and preference-elicitation behaviors.

## Limitations
- Ground truth generation relies on exhaustive search, which may not scale to truly web-scale data and could miss nuanced preference interactions in larger solution spaces
- The dynamic user simulator introduces stochasticity that could affect reproducibility, though stability tests are provided in Appendix A.2
- Open-source model performance gaps may be partially attributed to implementation details rather than fundamental architectural limitations

## Confidence
- **High Confidence:** The acceptable-optimal gap mechanism is well-supported by quantitative results showing declining optimality rates with increased search complexity.
- **Medium Confidence:** The plan-coordination gap explanation is plausible given the performance collapse on multi-service tasks, though alternative explanations (tool-calling proficiency) cannot be ruled out.
- **Medium Confidence:** The multi-turn dynamics impact is supported by case studies but requires further validation across diverse user personas and constraint revelation patterns.

## Next Checks
1. Replicate the acceptable-optimal gap experiment with a modified agent that includes explicit verification against multiple valid solutions before responding, to test if the gap can be closed through architectural changes.
2. Conduct an ablation study varying the number of tool calls required for coordination tasks while holding other variables constant, to isolate the specific impact of tool orchestration on performance.
3. Implement a simplified static user simulator (all constraints upfront) and compare performance metrics against the dynamic version to quantify the exact cost of multi-turn reasoning.