---
ver: rpa2
title: 'Gibbs randomness-compression proposition: An efficient deep learning'
arxiv_id: '2505.23869'
source_url: https://arxiv.org/abs/2505.23869
tags:
- compression
- learning
- pruning
- gibbs
- entropy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper presents a novel deep learning compression method called\
  \ Dual Tomographic Compression (DTC), which applies compressed sensing in an inverse\
  \ fashion to reconstruct sparse weight projections during training. The core idea\
  \ is to generate measurement vectors from learned weights and reconstruct them using\
  \ dual \u21131 minimization, enabling neuronal-level pruning that is more effective\
  \ than magnitude or random pruning."
---

# Gibbs randomness-compression proposition: An efficient deep learning

## Quick Facts
- arXiv ID: 2505.23869
- Source URL: https://arxiv.org/abs/2505.23869
- Reference count: 40
- Primary result: Novel DTC method achieves high accuracy (87.17%) at 95% sparsity on MNIST, outperforming random pruning (88.34%) with strong entropy-performance correlation (0.9174 DTC, 0.9412 random)

## Executive Summary
This paper introduces Dual Tomographic Compression (DTC), a deep learning compression method that applies compressed sensing in reverse to reconstruct sparse weight projections during training. The approach generates measurement vectors from learned weights and reconstructs them using dual ℓ1 minimization, enabling neuron-level pruning that preserves performance better than magnitude-based methods. The core innovation is the Gibbs randomness-compression proposition, which establishes a mathematical relationship between randomness and compression via Gibbs entropy, demonstrating strong empirical correlation between performance and entropy reduction. Experiments on MNIST show DTC outperforms other pruning methods, achieving 87.17% accuracy at 95% sparsity while supporting the lottery ticket hypothesis by finding sub-networks faster during training.

## Method Summary
DTC implements iterative neuronal-level pruning through inverse compressed sensing. Every 200 batches, the method computes measurement vectors y = Φ·w using a combined Gaussian-DCT measurement matrix, then solves ℓ1 minimization min||Θwᵣ − y||₂ + λ||wᵣ||₁ to obtain sparse "weight rays" from both incoming and outgoing connections. Neurons are ranked by the sum of their dual weight rays and pruned below a sparsity threshold quantile. The process continues with reduced network size while maintaining accuracy. Gibbs entropy is calculated from normalized histograms of measurement vectors at each compression step, tracking correlation with performance degradation to validate the entropy-compression relationship.

## Key Results
- DTC achieves 87.17% test accuracy at 95% sparsity on MNIST, outperforming magnitude pruning (85.46%) and matching random pruning (88.34%)
- Strong empirical correlation between Gibbs entropy reduction and performance degradation (0.9174 DTC, 0.9412 random)
- Dual tomographic reconstruction provides more robust neuron importance than single-direction magnitude pruning
- Method supports lottery ticket hypothesis by accelerating subnetwork discovery during training

## Why This Works (Mechanism)

### Mechanism 1: Inverse Compressed Sensing for Sparse Weight Projection
- Claim: Generating hypothetical measurements from known weights and reconstructing via ℓ1 minimization yields sparser, performance-preserving weight representations than magnitude-based pruning alone.
- Mechanism: Given weights w, construct measurement matrix Θ = Φ·Ψ (random Gaussian × DCT) and hypothetical measurement y = Φ·w. Solve min‖Θwᵣ − y‖₂ + λ‖wᵣ‖₁ to obtain sparse "weight rays" that identify neurons contributing least to signal structure.
- Core assumption: Weights amenable to sparse representation under compressed sensing basis; ℓ1 minimization recovers meaningful sparsity structure rather than arbitrary artifacts.
- Evidence anchors:
  - [abstract] "applies compressed sensing in an inverse fashion to reconstruct sparse weight projections during training"
  - [Section 3] "inverse here implies, we construct an hypothetical measurement process from known signal, and try to reconstruct the signals sparse version via conventional compressed sensing"
  - [corpus] Weak direct evidence; related work on compressed sensing in DL exists (e.g., "Deep compressed sensing") but does not validate this specific inverse formulation.

### Mechanism 2: Dual Tomographic Reconstruction for Neuron Importance
- Claim: Combining incoming and outgoing weight rays (wₚᵣ + wqᵣ) provides more robust neuron importance scores than single-direction magnitude.
- Mechanism: For neuron j, compute weight rays from previous layer connections (wₚ) and next layer connections (wq), reconstruct both sparsely, sum them, and clip neurons below sparsity threshold quantile. Dual reconstruction captures both input integration and output projection importance.
- Core assumption: Neuron importance correlates with combined sparsity structure of incoming/outgoing weights; tomographic analogy (multiple projections) improves importance estimation.
- Evidence anchors:
  - [abstract] "tomographic approach is applied to previous and next layers in a dual fashion, that triggers neuronal-level pruning"
  - [Section 4] "summed, wₚᵣ + wqᵣ are used to clip neurons where their weight rays' sum is lower than the target sparsity"
  - [corpus] Tomographic sensing principles appear in "Physics-Driven Learning Framework for Tomometric Tactile Sensing" but in completely different domain (EIT); no direct validation of dual-tomography for neural pruning.

### Mechanism 3: Gibbs Entropy–Performance Comonotonicity
- Claim: Gibbs entropy of measurement vectors (yᵢ) across compression cycles correlates strongly with model performance, suggesting entropy reduction tracks information preservation.
- Mechanism: At each compression step, compute normalized histogram of measurement vector yᵢ and calculate Gᵢ = −Σₖpₖlog₂pₖ. As compression increases, both Gᵢ and performance fᵢ decrease comonotonically if pruning preserves structure. High correlation (0.9174 DTC, 0.9412 random) empirically supports this.
- Core assumption: Measurement vector entropy reflects meaningful information content; correlation implies causal relationship between directed randomness and compression quality.
- Evidence anchors:
  - [abstract] "establishes a mathematical relationship between randomness and compression via Gibbs entropy, demonstrating strong empirical correlation between performance and entropy reduction"
  - [Section 5] "If learning performance degradation and entropy reduction due to compression kept small... then functions fᵢ and Gᵢ are highly correlated"
  - [corpus] "Entropy-driven Dynamic Gradient Compression" and "Know Your Limits: Entropy Estimation Modeling" explore entropy-compression links in different contexts (gradient compression, language modeling); provides plausibility but not direct validation.

## Foundational Learning

- **Compressed Sensing Fundamentals**
  - Why needed here: DTC inverts the standard CS pipeline; understanding RIP conditions, ℓ1 sparsity, and measurement matrix design is prerequisite to diagnosing reconstruction failures.
  - Quick check question: Given weights w ∈ ℝᵐ and measurement matrix Φ ∈ ℝⁿˣᵐ with n ≪ m, what conditions ensure ℓ1 minimization recovers a k-sparse approximation?

- **Gibbs vs. Shannon Entropy**
  - Why needed here: Paper formulates proposition in terms of Gibbs entropy over histogram bins; distinguishing this from Shannon entropy over probability distributions prevents misinterpretation.
  - Quick check question: For a normalized histogram with k bins and probabilities pₖ, how does Gibbs entropy H = −Σpₖlog₂ₖ behave as distribution uniformizes?

- **Lottery Ticket Hypothesis**
  - Why needed here: DTC claims to accelerate finding sparse subnetworks; understanding iterative magnitude pruning baselines contextualizes claimed improvements.
  - Quick check question: In lottery ticket experiments, why must subnetwork weights retain their original initialization rather than being re-initialized?

## Architecture Onboarding

- **Component map:** Training loop -> Compression trigger (every 200 batches) -> Weight ray generation -> Inverse CS module -> Pruning decision -> Entropy monitor
- **Critical path:** Measurement matrix construction → ℓ1 solver accuracy → quantile clipping → weight retention/reinitialization decision. Solver quality dominates pruning quality.
- **Design tradeoffs:**
  - **Compression frequency**: Every 200 batches balances compute overhead vs. pruning granularity; more frequent = finer control but more solver calls
  - **Sparsity schedule**: Monotonically decreasing sᵢ; aggressive early pruning risks irreversible damage
  - **λ regularization**: Controls sparsity-vs-reconstruction fidelity; paper does not specify tuning procedure
- **Failure signatures:**
  - Test accuracy collapses before 50% sparsity → measurement matrix poorly conditioned or λ too aggressive
  - Entropy-performance correlation drops sharply → histogram binning inappropriate for measurement scale
  - DTC underperforms random pruning → weights lack sparsity structure in chosen basis (try wavelet, learned dictionary)
- **First 3 experiments:**
  1. **Baseline reproduction**: Implement DTC on MNIST with 512-neuron hidden layer; reproduce Table 1 sparsity-accuracy curve; verify correlation coefficients (0.9174 DTC, 0.9412 random)
  2. **Ablation on compression frequency**: Compare m=50, 200, 500 batches; measure final accuracy at 90% sparsity and total compute time
  3. **Basis sensitivity test**: Replace DCT with wavelet transform and random Gaussian projection; assess whether weight ray sparsity and pruning quality change significantly

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the Gibbs randomness-compression proposition hold for deeper architectures and more diverse, complex datasets beyond the MNIST experiment?
- Basis in paper: [explicit] Section 6.3 states that "demonstrations with different architectures appears to be good avenues to explore for expanded future studies that requires much larger compute resources for advancing the approach with more diverse datasets."
- Why unresolved: The evidence provided is restricted to a canonical vision task (MNIST) using a fully connected network with a single hidden layer of 512 neurons.
- What evidence would resolve it: Successful replication of the high correlation (>0.91) between Gibbs entropy reduction and performance on complex datasets (e.g., ImageNet, CIFAR) using modern architectures (e.g., CNNs, Transformers).

### Open Question 2
- Question: How does the frequency of the dual tomographic procedure during training affect the trade-off between compression efficiency and final model accuracy?
- Basis in paper: [explicit] Section 6.3 lists "Further study on the effect of frequency of dual tomographic procedure during train-compress iterations" as a necessary avenue for future study.
- Why unresolved: The experiments utilized a fixed heuristic of applying DTC every 200 batches to balance compute budget and performance, leaving the optimal scheduling unexplored.
- What evidence would resolve it: A comparative analysis of test accuracy and entropy evolution across varying compression intervals (e.g., every batch vs. every 100 batches vs. every 500 batches).

### Open Question 3
- Question: Is the observed correlation between Gibbs entropy and performance sensitive to the specific binning strategy used to calculate the entropy of measurement vectors?
- Basis in paper: [explicit] Section 6.3 identifies "the effect of binning on the Gibbs entropy on the measurement vectors" as a topic requiring further study.
- Why unresolved: The proposition relies on computing Gibbs entropy from normalized histogram bins (Gᵢ = −Σpₖlog₂pₖ), but the paper does not analyze how bin width or count impacts the reported correlation strength.
- What evidence would resolve it: A sensitivity analysis showing the Pearson correlation coefficient between fᵢ(sᵢ) and Gᵢ(sᵢ) remains stable across different binning granularities.

### Open Question 4
- Question: Why does random pruning outperform or match Dual Tomographic Compression at extreme sparsity levels (>95%), and does this imply a boundary for the "directed randomness" proposition?
- Basis in paper: [inferred] While the paper supports the lottery ticket hypothesis, Section 6.1 and Table 1 show random pruning achieving higher accuracy (0.8834) than DTC (0.8717) at 97% sparsity, challenging the necessity of the complex reconstruction step in the extreme compression regime.
- Why unresolved: The paper notes the "remarkable resilience" of random pruning but does not explain why the "directed" approach of DTC fails to maintain an advantage over pure randomness when the network is reduced to a handful of neurons.
- What evidence would resolve it: A theoretical or empirical breakdown of the information retention in DTC's reconstruction vs. random sampling specifically in the low-parameter regime.

## Limitations

- Compressed sensing inversion relies on untested assumptions about weight sparsity structure in DCT basis
- No λ tuning procedures or sensitivity analysis provided for ℓ1 regularization weight
- Solver convergence guarantees are unclear for high sparsity regimes
- Gibbs entropy proposition lacks theoretical derivation connecting entropy reduction to compression quality

## Confidence

- **High confidence**: Empirical correlation between entropy and performance (correlation coefficients provided and verifiable)
- **Medium confidence**: Compressed sensing-based pruning mechanism (inverse formulation plausible but λ sensitivity unknown)
- **Low confidence**: Gibbs entropy proposition's theoretical grounding (empirical correlation ≠ causation; no formal proof provided)

## Next Checks

1. **Solver sensitivity study**: Test λ ∈ {0.01, 0.1, 1.0} and SCS solver tolerance settings; measure impact on weight ray quality and final pruning accuracy
2. **Basis comparison experiment**: Replace DCT basis with wavelet transform and learned dictionary; assess whether sparsity structure and pruning quality change significantly
3. **Theoretical validation**: Attempt to derive conditions under which Gibbs entropy of measurement vectors correlates with information preservation during pruning; test on synthetic sparse signal recovery tasks