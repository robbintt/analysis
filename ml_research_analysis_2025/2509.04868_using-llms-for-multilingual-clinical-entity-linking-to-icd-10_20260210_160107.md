---
ver: rpa2
title: Using LLMs for Multilingual Clinical Entity Linking to ICD-10
arxiv_id: '2509.04868'
source_url: https://arxiv.org/abs/2509.04868
tags:
- icd-10
- code
- codes
- clinical
- gpt-4
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a multilingual entity linking approach to ICD-10
  codes using a combination of dictionary-based matching and in-context learning with
  large language models. The system first attempts exact matches against clinical
  dictionaries, then applies GPT-4.1 with one-shot examples to predict codes for unmatched
  terms.
---

# Using LLMs for Multilingual Clinical Entity Linking to ICD-10

## Quick Facts
- arXiv ID: 2509.04868
- Source URL: https://arxiv.org/abs/2509.04868
- Authors: Sylvia Vassileva; Ivan Koychev; Svetla Boytcheva
- Reference count: 4
- Primary result: 0.89 F1 for ICD-10 categories and 0.78 F1 for subcategories on Spanish data; 0.85 F1 on Greek data

## Executive Summary
This paper presents a multilingual clinical entity linking system that assigns ICD-10 codes to medical terms in discharge summaries. The approach combines dictionary-based matching with GPT-4.1 in-context learning, achieving strong performance on Spanish (CodiEsp) and Greek (ElCardioCC) datasets without requiring fine-tuning. The system first attempts exact matches against clinical dictionaries, then uses one-shot prompting with GPT-4.1 to handle unmatched terms. Results show the hybrid approach significantly outperforms either component alone, demonstrating that LLMs can effectively assist in medical coding across languages.

## Method Summary
The system uses a two-stage pipeline: first, it performs exact matching against language-specific ICD-10 dictionaries (88K Spanish terms, 11.5K Greek terms), returning codes only for unambiguous matches. For remaining terms, it applies GPT-4.1 with one-shot in-context learning, providing a single example of the expected JSON output format. The dictionary and LLM outputs are merged, with dictionary matches taking precedence. The approach requires only ICD-10 dictionaries in target languages and pre-identified mention spans, making it easily adaptable to new languages. Evaluation measures F1 score at both category (3-character) and subcategory (4-character) levels.

## Key Results
- Spanish CodiEsp: 0.89 F1 for categories, 0.78 F1 for subcategories
- Greek ElCardioCC: 0.85 F1 overall
- Dictionary alone: 0.657 F1 (Greek), 0.546 F1 (Spanish categories)
- GPT-4.1 1-shot recall: 0.822 (Greek), 0.773 (Spanish categories)
- GPT-4o 1-shot recall: 0.324 (Greek), 0.113 (Spanish categories)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A two-stage dictionary-then-LLM pipeline improves entity linking accuracy over either component alone.
- Mechanism: Dictionary stage provides high-precision exact matches for unambiguous terms, eliminating LLM errors on easy cases, while LLM stage handles paraphrases, synonyms, and context-dependent mappings that dictionaries miss.
- Core assumption: Dictionary entries correctly map to single ICD-10 codes (unambiguous), and LLM errors primarily occur on ambiguous or out-of-vocabulary terms where dictionary lookup fails.
- Evidence anchors: Dictionary alone: 0.657 F1 (Greek), 0.546 F1 (Spanish categories); Dict+GPT-4.1 1-shot: 0.856 F1 (Greek), 0.891 F1 (Spanish categories)

### Mechanism 2
- Claim: GPT-4.1 with one-shot in-context learning produces more complete and accurate ICD-10 predictions than GPT-4o or zero-shot prompting.
- Mechanism: One-shot examples demonstrate expected JSON output format, medical reasoning style, and how to handle multiple terms. GPT-4.1's improved instruction-following yields higher recall compared to GPT-4o, which often omits mentions.
- Core assumption: GPT-4.1's pre-training includes sufficient multilingual medical terminology and ICD-10 structure to generalize from a single example.
- Evidence anchors: GPT-4o 1-shot recall: 0.324 (Greek), 0.113 (Spanish categories); GPT-4.1 1-shot recall: 0.822 (Greek), 0.773 (Spanish categories)

### Mechanism 3
- Claim: Multilingual transfer allows the same architecture to work across languages with only language-specific dictionaries and example replacements.
- Mechanism: GPT-4.1's multilingual pre-training enables it to process Greek and Spanish clinical text and map terms to a language-agnostic coding system (ICD-10). The prompt template is reused; only the language parameter and one-shot example are swapped.
- Core assumption: ICD-10 hierarchy and coding logic are language-independent, and GPT-4.1's multilingual competence is sufficient for medical domain terms in each target language.
- Evidence anchors: Results reported for both Greek (ElCardioCC) and Spanish (CodiEsp) using the same architecture

## Foundational Learning

- **Entity Linking vs. Named Entity Recognition (NER)**:
  - Why needed here: The paper assumes mentions are already identified; it focuses on linking those mentions to ICD-10 codes.
  - Quick check question: Does your input contain pre-annotated text spans, or do you need to detect the spans first?

- **ICD-10 Hierarchy (Category vs. Subcategory)**:
  - Why needed here: Performance differs significantly between predicting 3-character categories (e.g., I50) and more specific subcategories (e.g., I50.9).
  - Quick check question: If the ground truth is I50.9 and your system predicts I50, is this counted as correct at the category level?

- **In-Context Learning (Zero-shot vs. One-shot)**:
  - Why needed here: The paper compares zero-shot and one-shot prompting. Understanding how examples guide model output format and reasoning helps diagnose why one-shot sometimes underperforms.
  - Quick check question: What happens to your prompt token count if you add two full discharge summaries as examples instead of one?

## Architecture Onboarding

- **Component map**: Input preprocessing (mention marking) -> Dictionary lookup -> LLM prompting for residuals -> Result merging -> Evaluation against gold ICD-10 labels

- **Critical path**: Pre-identified mentions in discharge summaries → Dictionary lookup for exact unambiguous matches → GPT-4.1 one-shot prompting for unmatched terms → JSON output merging → F1 evaluation at category/subcategory levels

- **Design tradeoffs**:
  - Dictionary-only: High precision, low recall (0.657 F1 Greek baseline); no API cost
  - LLM-only: Higher recall but variable precision; GPT-4o has recall issues; GPT-4.1 more balanced
  - Hybrid: Best F1 (0.85–0.89), but incurs API cost and requires cloud deployment
  - Zero-shot vs. one-shot: One-shot may improve format adherence but increases prompt length

- **Failure signatures**:
  - Low recall with GPT-4o: Model skips mentions; prompt too long or instructions not followed
  - Low precision on symptoms (R00–R99 codes): Model assigns disease codes when it should assign symptom codes
  - Token overflow: Discharge summaries plus examples exceed context window; mentions at the end get dropped

- **First 3 experiments**:
  1. **Dictionary baseline**: Run dictionary-only matching on sample discharge summaries; measure P/R/F1 to establish precision floor
  2. **Zero-shot vs. one-shot comparison**: For same data, compare GPT-4.1 zero-shot and one-shot; inspect JSON outputs for format compliance and mention coverage
  3. **Hybrid ablation**: Combine dictionary + LLM; measure whether merger improves over LLM-alone and quantify dictionary stage contribution

## Open Questions the Paper Calls Out

- **Fine-tuning open-source models**: Can fine-tuning smaller, open-source LLMs achieve performance comparable to GPT-4.1 while resolving data privacy and inference cost limitations?
- **End-to-end entity detection**: How does system performance degrade when the model must detect entity boundaries rather than relying on pre-identified gold standard mentions?
- **Dictionary resource impact**: To what extent does the availability and quality of language-specific ICD-10 dictionaries impact the hybrid approach's performance in low-resource language scenarios?

## Limitations
- Dictionary size asymmetry (88K Spanish terms vs 11.5K Greek terms) may explain performance differences and raises questions about scalability to lower-resource languages
- One-shot example content used for prompting is not provided, making exact replication challenging
- Approach assumes pre-annotated mention boundaries, requiring a separate NER component not addressed in this work

## Confidence
- **High confidence**: The dictionary-then-LLM pipeline mechanism and its superiority over either component alone
- **Medium confidence**: GPT-4.1's superiority over GPT-4o for this task
- **Medium confidence**: Multilingual transfer capability across languages

## Next Checks
1. **Ablation study on dictionary size**: Test the pipeline with varying dictionary sizes to determine the minimum coverage needed for the hybrid approach to outperform pure LLM methods
2. **Cross-lingual prompt generalization**: Evaluate whether the Spanish one-shot example can be directly used for other Romance languages or if language-specific examples are required for each target language
3. **Symptom code performance isolation**: Conduct focused evaluation on R00-R99 codes to quantify the LLM's specific challenges with symptom classification and test targeted prompt modifications to improve this critical failure mode