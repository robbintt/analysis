---
ver: rpa2
title: Generative Distribution Embeddings
arxiv_id: '2505.18150'
source_url: https://arxiv.org/abs/2505.18150
tags:
- encoder
- distribution
- distributions
- gdes
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Generative Distribution Embeddings (GDEs) provide a framework for
  learning structured representations of distributions rather than individual data
  points, enabling reasoning across multiple scales in real-world problems. GDEs combine
  distribution-invariant encoders with conditional generative models to embed empirical
  distributions into latent spaces where distances reflect Wasserstein distances between
  distributions and interpolation follows optimal transport paths.
---

# Generative Distribution Embeddings

## Quick Facts
- arXiv ID: 2505.18150
- Source URL: https://arxiv.org/abs/2505.18150
- Authors: Nic Fishman; Gokul Gowri; Peng Yin; Jonathan Gootenberg; Omar Abudayyeh
- Reference count: 40
- Primary result: GDEs embed empirical distributions into latent spaces where distances reflect Wasserstein distances and interpolation follows optimal transport paths

## Executive Summary
Generative Distribution Embeddings (GDEs) provide a framework for learning structured representations of distributions rather than individual data points, enabling reasoning across multiple scales in real-world problems. GDEs combine distribution-invariant encoders with conditional generative models to embed empirical distributions into latent spaces where distances reflect Wasserstein distances between distributions and interpolation follows optimal transport paths. The framework unifies conditional generation with representation learning for distributions, satisfying a criterion called distributional invariance.

## Method Summary
GDEs work by training an encoder that is invariant to sample ordering and size (distributional invariance) to extract the empirical distribution from sets of data points, then using a conditional generative model as a decoder to reconstruct distributions by minimizing divergences between true and generated distributions. The encoder must satisfy permutation and proportional invariance, extracting exactly the information in the empirical distribution. During training, sets are constructed via discrete labels, spatial kernels, or noise inversion, and the encoder receives gradients only through the generator. The framework claims asymptotic consistency and approximately preserves Wasserstein geometry in the latent space.

## Key Results
- GDEs consistently outperform existing approaches on synthetic datasets in reconstruction error
- Applied to six large-scale computational biology problems spanning diverse data domains: modeling clonal populations from lineage-traced single-cell RNA-seq data (150K cells), predicting transcriptional responses to genetic perturbations (1M cells), predicting cellular phenotypic responses to perturbations (20M images), modeling tissue-specific DNA methylation patterns (253M sequences), designing synthetic yeast promoters (34M sequences), and modeling spatiotemporal distributions of viral protein sequences (1M sequences)
- Across these applications, GDEs consistently improve predictive power and enable direct modeling of measurement data while preserving population-level structure

## Why This Works (Mechanism)

### Mechanism 1: Distributional Invariance Enables Consistent Recovery
- **Claim:** Encoders that satisfy both permutation and proportional invariance consistently recover the true data-generating distribution from finite samples as m → ∞.
- **Mechanism:** The encoder must extract structural information about P while being insensitive to sample ordering and sample size. This forces dependence only on the empirical measure Pm = (1/m)∑δxi, which converges to P by the strong law. The encoder thus learns to approximate a measurable function of Pm.
- **Core assumption:** The encoder class is sufficiently expressive to approximate measurable functions of the empirical distribution.
- **Evidence anchors:** [abstract] "encoder networks which satisfy a criterion we call distributional invariance"; [section 2, Proposition 1] "Any class of encoder-generator architectures that consistently recovers the data-generating distribution from i.i.d. samples must encode exactly the information in the empirical distribution—no more, no less."; [section D.1, Corollary 1] Formal proof that distributional invariance is necessary for asymptotic consistency.

### Mechanism 2: Conditional Generator as Distribution Decoder
- **Claim:** Any conditional generative model can be repurposed as a decoder to reconstruct distributions by minimizing a divergence between true and generated distributions.
- **Mechanism:** The generator G takes latent z and produces samples from Ĝ(z). Training minimizes EP∼Q[Sm∼P⊗m d(P, G(E(Sm)))], where d is a suitable divergence. The plug-in loss is asymptotically unbiased and normally distributed around the population loss.
- **Core assumption:** The divergence d is Hadamard differentiable with separating property (d(P,Q)=0 ⟹ P=Q).
- **Evidence anchors:** [abstract] "GDEs combine distribution-invariant encoders with conditional generative models"; [section 2] "We can repurpose any conditional generative model (e.g., VAE, DDPM, GAN) to minimize a divergence d"; [section D.2, Theorem 2] Proves asymptotic normality of the plug-in loss and consistency of reconstruction.

### Mechanism 3: Approximate Isometric Embedding of Wasserstein Geometry
- **Claim:** GDE latent spaces approximately preserve Wasserstein distances and optimal transport geodesics for distributions in the meta-distribution's support.
- **Mechanism:** The encoder learns a smooth embedding ψ: M → ℝd of the statistical manifold M = supp(Q). Empirically, latent L₂ distances correlate with W₂ distances, and linear interpolation approximates OT paths. The learned geometry is prior-weighted by Q.
- **Core assumption:** The meta-distribution Q is sufficiently dense in regions of interest; the encoder is injective with bijective differential.
- **Evidence anchors:** [abstract] "distances reflect Wasserstein distances between distributions and interpolation follows optimal transport paths"; [section 5.2] Gaussians: Spearman ρ = 0.96; GMMs: ρ = 0.76; Fig. 4 shows interpolation matches OT.

## Foundational Learning

- **Concept: Optimal Transport and W₂ Distance**
  - Why needed here: The paper claims latent distances recover W₂ and interpolations follow OT geodesics. Understanding Benamou-Brenier formulation and Otto geometry clarifies the geometric claims.
  - Quick check question: Why do Wasserstein geodesics correspond to "pushing mass along straight lines" under the optimal transport map?

- **Concept: Permutation-Invariant Set Networks**
  - Why needed here: The encoder must be distributionally invariant. Understanding how mean-pooling achieves both permutation AND proportional invariance is essential for correct architecture design.
  - Quick check question: Why does mean-pooling satisfy proportional invariance while sum-pooling (DeepSets default) does not?

- **Concept: Conditional Generative Models (DDPM, CVAE, Autoregressive)**
  - Why needed here: The generator is repurposed from any conditional model. Understanding prefix conditioning and latent injection enables adapting pretrained models (HyenaDNA, ProGen2).
  - Quick check question: How would you condition a pretrained causal LM on a continuous latent vector z?

## Architecture Onboarding

- **Component map:** Input: Set Sm = {x₁,...,xm} ∼ P⊗m → Encoder E (distributionally invariant) → Latent z ∈ ℝ^L (L typically 64–256) → Generator G (conditional) → Output: Samples from Ĝ(E(Sm)) ≈ P

- **Critical path:**
  1. Choose encoder with mean/median pooling (NOT max or sum); verify distributional invariance
  2. Select generator matching data modality; implement conditioning mechanism
  3. Construct sets via Sec. 3 strategies (discrete labels, spatial kernels, noise inversion)
  4. Train jointly; encoder receives gradients only through generator

- **Design tradeoffs:**
  - GNN vs. Transformer encoders: Transformers converge faster but O(m²) attention cost; GNNs scale linearly
  - Set size vs. batch size: Large m increases memory (especially for attention); small batches increase gradient variance
  - Pretrained vs. from-scratch generators: Pretrained needs prefix-tuning; from-scratch allows end-to-end but requires more data

- **Failure signatures:**
  - Max-pooling → non-Gaussian asymptotics (Gumbel), breaks Theorem 2
  - Sum-pooling → encoder varies with m, inconsistent inference at different set sizes
  - Generator underutilizes z → weak/noisy encoder gradients, stalled training
  - Small set sizes → high empirical variance, poor reconstruction (see Fig. 2)

- **First 3 experiments:**
  1. **Gaussian sanity check:** Generate 5D Gaussians with random means/covariances. Train ResNet-GNN + DDPM. Measure: (a) W₂ reconstruction error, (b) latent L₂ vs. analytical W₂ correlation. Target: ρ > 0.9 as in paper.
  2. **Set size ablation:** Fix one distribution, vary m ∈ {10, 50, 100, 500}. Plot reconstruction error vs. m. Verify latent stabilizes as m grows (replicate Fig. 2 behavior).
  3. **Encoder pooling comparison:** Compare sum-pooling vs. mean-pooling vs. median-pooling on identical data. Confirm sum diverges at inference with mismatched m; confirm max-pooling fails smoothness requirements.

## Open Questions the Paper Calls Out
None

## Limitations
- The theoretical framework assumes distributional invariance is achievable and sufficient for consistent recovery, but practical encoders may not fully satisfy these constraints, especially with limited data or high-dimensional inputs
- The claim of "consistent improvement across all six applications" lacks statistical testing to confirm significance across diverse domains
- While the framework claims to work with any conditional generative model, practical performance may heavily depend on the choice of architecture and its conditioning mechanism

## Confidence
- **High confidence**: Distributional invariance is theoretically necessary for asymptotic consistency (Proposition 1, Theorem 2)
- **Medium confidence**: Empirical results showing superior performance on synthetic benchmarks and computational biology applications
- **Medium confidence**: The claim that latent distances approximate Wasserstein distances based on correlation coefficients (0.76-0.96) without formal metric guarantees

## Next Checks
1. **Formally test the distributional invariance property** by training encoders with different pooling strategies (mean, sum, max, median) on synthetic distributions and measuring reconstruction consistency when inference set sizes differ from training sizes
2. **Statistical significance testing across applications** by computing confidence intervals and p-values for performance improvements on the six computational biology datasets to verify that improvements aren't due to random variation
3. **Geometry validation under sparse meta-distributions** by deliberately training on Q with gaps or non-uniform density and measuring degradation in latent space geometry preservation using synthetic distributions with known optimal transport properties