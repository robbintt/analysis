---
ver: rpa2
title: Predicting LLM Correctness in Prosthodontics Using Metadata and Hallucination
  Signals
arxiv_id: '2512.22508'
source_url: https://arxiv.org/abs/2512.22508
tags:
- hallucination
- prompting
- answer
- explanation
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper explores the feasibility of predicting whether large\
  \ language models (LLMs) produce correct answers in the domain of prosthodontics\
  \ by leveraging metadata such as consistency and answer token log probability. The\
  \ authors evaluate two LLMs\u2014GPT-4o and OSS-120B\u2014on a multiple-choice prosthodontics\
  \ exam under three prompting strategies (answer-only, answer-then-explain, and explain-then-answer)."
---

# Predicting LLM Correctness in Prosthodontics Using Metadata and Hallucination Signals

## Quick Facts
- arXiv ID: 2512.22508
- Source URL: https://arxiv.org/abs/2512.22508
- Reference count: 11
- Primary result: Metadata and hallucination signals can predict LLM correctness in prosthodontics, achieving up to 7.14% accuracy improvement and 83.12% precision over baseline

## Executive Summary
This paper investigates whether metadata such as consistency and answer token log probability can predict the correctness of LLM outputs in prosthodontics. The authors evaluate GPT-4o and OSS-120B on a multiple-choice prosthodontics exam using three prompting strategies. They train a correctness predictor that achieves significant improvements over a baseline that assumes all outputs are correct. The results show that actual hallucination signals are strong predictors of correctness, while metadata alone cannot reliably predict hallucination occurrence.

## Method Summary
The study evaluates two LLMs (GPT-4o and OSS-120B) on an 80-question multiple-choice prosthodontics exam using three prompting strategies: answer-only, answer-then-explain, and explain-then-answer. For each response, metadata features are extracted including consistency across prompts, answer token log probability, and hallucination signals. A correctness predictor is trained using leave-one-out cross-validation on the dataset. The predictor uses metadata features to classify responses as correct or incorrect, with performance compared against a baseline that assumes all outputs are correct.

## Key Results
- Correctness predictor achieves up to 7.14% accuracy improvement over baseline
- Predictor achieves 83.12% precision compared to baseline
- Metadata alone cannot predict hallucination occurrence, though actual hallucination signals strongly predict correctness
- Prompting strategies significantly affect internal model behaviors but not overall accuracy

## Why This Works (Mechanism)
The mechanism relies on the observation that LLM outputs exhibit systematic patterns in their metadata that correlate with correctness. By extracting features like consistency across different promptings and token-level probabilities, the model captures subtle indicators of reliability that are not apparent from the final answer alone. The hallucination signals serve as ground truth indicators of when the model has deviated from factual information, which strongly correlates with incorrectness.

## Foundational Learning
- **Metadata extraction**: Understanding how to systematically capture model behavior indicators like consistency and log probabilities
  - Why needed: Provides quantitative features for prediction models
  - Quick check: Verify metadata features show variance across correct/incorrect responses

- **Cross-validation methodology**: Using leave-one-out approach for small datasets
  - Why needed: Ensures robust evaluation when sample size is limited
  - Quick check: Confirm each fold maintains balanced representation of classes

- **Hallucination detection**: Identifying when models generate unsupported or incorrect information
  - Why needed: Serves as ground truth for training correctness predictors
  - Quick check: Validate hallucination labels through human annotation

## Architecture Onboarding

**Component map**: Input questions -> LLMs (GPT-4o, OSS-120B) -> Three prompting strategies -> Metadata extraction -> Correctness prediction

**Critical path**: Question → LLM response → Metadata features → Correctness classification → Performance evaluation

**Design tradeoffs**: The study prioritizes domain specificity over generalization, using a focused prosthodontics dataset rather than broader medical or general knowledge domains. This enables deep analysis but limits generalizability.

**Failure signatures**: Poor performance when metadata features show low variance between correct and incorrect responses, or when prompting strategies fail to elicit meaningful differences in model behavior.

**Three first experiments**:
1. Test metadata feature importance using ablation studies
2. Compare predictor performance across different medical domains
3. Evaluate impact of temperature and sampling parameters on metadata stability

## Open Questions the Paper Calls Out
The paper highlights several open questions regarding the generalizability of metadata-based predictors to other domains, the impact of different sampling strategies on predictor performance, and whether intermediate reasoning steps could improve prediction accuracy.

## Limitations
- Small dataset of only 80 questions from a single domain limits generalizability
- Binary classification framework may oversimplify nuanced LLM outputs
- Does not explore impact of different temperature settings on metadata predictive power

## Confidence
- Core finding (metadata and hallucination signals predict correctness): Medium
- Hallucination signals strongly predict correctness: High
- Prompting strategies affect internal behaviors but not accuracy: High

## Next Checks
1. Test predictor across multiple medical and non-medical domains with larger, more diverse question sets
2. Evaluate impact of different sampling parameters (temperature, top-k, nucleus sampling) on both LLM output quality and metadata predictive power
3. Investigate whether incorporating intermediate reasoning steps or chain-of-thought outputs improves prediction accuracy beyond current metadata set