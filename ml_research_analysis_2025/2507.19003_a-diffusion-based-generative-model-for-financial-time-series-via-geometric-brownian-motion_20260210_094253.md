---
ver: rpa2
title: A diffusion-based generative model for financial time series via geometric
  Brownian motion
arxiv_id: '2507.19003'
source_url: https://arxiv.org/abs/2507.19003
tags:
- volatility
- financial
- time
- diffusion
- process
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a diffusion-based generative model for financial
  time series that incorporates geometric Brownian motion (GBM) into the forward noising
  process. Unlike standard score-based models that treat price trajectories as generic
  numerical sequences, this method injects noise proportionally to asset prices at
  each time step, reflecting the heteroskedasticity observed in financial time series.
---

# A diffusion-based generative model for financial time series via geometric Brownian motion

## Quick Facts
- arXiv ID: 2507.19003
- Source URL: https://arxiv.org/abs/2507.19003
- Reference count: 23
- Key outcome: GBM-based diffusion model reproduces heavy-tailed returns (α=3.06-4.62) and leverage effect better than baseline methods

## Executive Summary
This paper proposes a diffusion-based generative model for financial time series that incorporates geometric Brownian motion (GBM) into the forward noising process. Unlike standard score-based models that treat price trajectories as generic numerical sequences, this method injects noise proportionally to asset prices at each time step, reflecting the heteroskedasticity observed in financial time series. By balancing the drift and diffusion terms, the resulting log-price process reduces to a variance-exploding stochastic differential equation compatible with score-based generative modeling frameworks. Empirical evaluations on historical stock data demonstrate that the model reproduces key stylized facts - heavy-tailed return distributions, volatility clustering, and the leverage effect - more realistically than conventional diffusion models.

## Method Summary
The model implements a variance-exploding stochastic differential equation in log-price space, where the forward noising process follows dS_t = μ_t S_t dt + σ_t S_t dW_t with μ_t = ½σ²_t to cancel drift. This transforms into dX_t = √β_t dW_t in log-space (X_t = log S_t), enabling direct application of score-based generative frameworks. The reverse-time generative process is trained via denoising score matching using a Transformer-based architecture adapted from the CSDI framework. The model uses sliding windows of length L=2048 from S&P 500 constituents, with diffusion-step, time, and feature embeddings sized at 256, 128, and 64 dimensions respectively.

## Key Results
- GBM yields tail exponents α = 3.06-4.62 vs. empirical α = 4.35, while VE/VP yield α = 8.49-8.96 (overly light-tailed)
- Better captures long-range volatility dependence and asymmetric correlation structure characteristic of financial markets
- 128/256/64 configuration shows most realistic leverage profiles; baseline 64/128/16 fails to capture persistent negative correlation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Injecting noise proportionally to asset prices via GBM induces heteroskedasticity in the forward diffusion process, yielding more realistic heavy-tailed return distributions.
- **Mechanism:** The forward SDE dS_t = μ_t S_t dt + σ_t S_t dW_t multiplicatively scales noise by current price level. After log-transform and Ito's lemma, choosing μ_t = ½σ²_t cancels drift, yielding dX_t = σ_t dW_t in log-space. The exponential map back to price space preserves state-dependent volatility: higher prices receive larger absolute perturbations.
- **Core assumption:** Asset price volatility scales with price level (multiplicative dynamics), which is a reasonable approximation for equity markets over moderate time horizons.
- **Evidence anchors:**
  - [abstract]: "injects noise proportionally to asset prices at each time step, reflecting the heteroskedasticity observed in financial time series"
  - [Section 3.1, Eq. 3.1-3.3]: Derivation showing GBM → log-space VE SDE with balanced drift/diffusion
  - [Section 4.2]: GBM yields tail exponents α = 3.06-4.62 vs. empirical α = 4.35; VE/VP yield α = 8.49-8.96 (overly light-tailed)
- **Break condition:** If asset prices exhibit regime-dependent rather than level-dependent volatility (e.g., rough volatility dynamics), the multiplicative scaling may misrepresent true heteroskedasticity structure.

### Mechanism 2
- **Claim:** Balancing drift and diffusion terms (μ_t = ½σ²_t) converts GBM to a variance-exploding SDE in log-price space, enabling direct application of score-based generative modeling frameworks.
- **Mechanism:** In standard score-based models, the reverse-time SDE requires estimating ∇ log p_t(x_t). VE SDEs have analytically tractable transition kernels p_{t|0}(x_t|x_0) = N(x_0, σ²_t I), enabling denoising score matching with known target scores. The drift cancellation preserves this tractability.
- **Core assumption:** The VE formulation remains appropriate for log-prices; Assumption: log-price dynamics approximately satisfy the Markov property required for score-based SDE reversal.
- **Evidence anchors:**
  - [abstract]: "resulting log-price process reduces to a variance-exploding stochastic differential equation"
  - [Section 3.1]: "yielding the forward SDE: dX_t = √β_t dW_t, X_t = log S_t"
  - [corpus]: "Dale meets Langevin" paper explores multiplicative diffusion models with related SDE formulations
- **Break condition:** If the forward process requires state-dependent diffusion coefficients (g(x_t, t) rather than g(t)), standard score-matching objectives may need modification.

### Mechanism 3
- **Claim:** Increased embedding dimensions (128/256/64 for channels/diffusion-step/feature embeddings) improve capture of the leverage effect by enabling better separation of low-frequency drift from high-frequency asymmetric shocks.
- **Mechanism:** The leverage effect requires modeling asymmetric correlation between negative returns and future volatility—a delayed, direction-dependent response. Higher-dimensional diffusion embeddings capture steeper score gradients at early noise levels; larger feature embeddings separate return-direction signals from volatility-magnitude signals across temporal lags.
- **Core assumption:** The leverage effect is encoded in the score function's asymmetric response to return direction; sufficient model capacity is required to learn this structure.
- **Evidence anchors:**
  - [Section 3.1.1]: "enlarged feature embedding separates low-frequency drift from high-frequency leverage shocks"
  - [Section 4.1, Figure 4]: 128/256/64 configuration shows most realistic leverage profiles; baseline 64/128/16 fails to capture persistent negative correlation
  - [corpus]: Weak/no direct corpus evidence for this specific architectural claim
- **Break condition:** Over-parameterization may introduce spurious long-lag correlations or overfit to training-period leverage patterns that don't generalize.

## Foundational Learning

- **Concept: Score-based generative models and reverse-time SDEs**
  - Why needed here: The entire framework builds on understanding forward diffusion as SDE corruption and reverse generation as SDE integration with learned score functions. Without this, the GBM modification appears unmotivated.
  - Quick check question: Given forward SDE dx = f(x,t)dt + g(t)dW, write the reverse-time SDE and identify what must be learned.

- **Concept: Geometric Brownian Motion and Ito's Lemma**
  - Why needed here: The paper's core contribution is embedding GBM structure into diffusion models. Understanding why dS = μS dt + σS dW leads to log-normal marginals and how Ito's lemma transforms this to log-space is essential.
  - Quick check question: Apply Ito's lemma to transform dS_t = μS_t dt + σS_t dW_t into the SDE for X_t = log S_t.

- **Concept: Financial stylized facts (heavy tails, volatility clustering, leverage effect)**
  - Why needed here: These are the evaluation criteria. Understanding their empirical signatures (tail exponent ranges, ACF decay patterns, asymmetric lead-lag correlations) is necessary to interpret results.
  - Quick check question: Define the leverage effect and explain why it manifests as negative correlation between rt and r²_{t+k} for k > 0.

## Architecture Onboarding

- **Component map:** Log-return sequences (L=2048) -> 1D Conv (128 channels) -> Latent representation -> Embeddings (Diffusion-step: 256-dim, Time: positional, Feature: 64-dim) -> Transformer blocks -> Gated residual blocks (n=4) -> Output 1D Conv -> Score estimate s_θ(x_t, t)

- **Critical path:**
  1. Forward noising: Apply GBM-based SDE to log-prices → obtain noisy samples X_t
  2. Score matching: Train network to predict ∇ log p_{t|0}(X_t|X_0) using DSM loss
  3. Generation: Sample X_T ~ N(0, σ²_T I), integrate reverse SDE backward to X_0
  4. Transform: Exponentiate log-prices to recover synthetic price trajectories

- **Design tradeoffs:**
  - Noise schedule: Exponential/cosine schedules better preserve stylized facts than linear (Figures 5-7); exponential keeps early steps clean, cosine smooths transitions
  - Model capacity: 128/256/64 captures leverage effect but increases compute; baseline 64/128/16 is faster but misses asymmetric correlations
  - Sequence length: L=2048 captures long-range volatility dependence but requires memory; shorter windows may miss clustering patterns

- **Failure signatures:**
  - Tail exponent α > 7: Indicates overly light-tailed returns; check if using VE/VP instead of GBM
  - ACF of |r_t| decays to zero within ~10 lags: Missing volatility clustering; verify GBM formulation and noise schedule
  - Leverage correlation oscillates around zero: Insufficient model capacity or incorrect embedding dimensions; increase feature embedding to ≥64

- **First 3 experiments:**
  1. **Sanity check:** Train GBM-based model with cosine schedule on single stock's log-returns; generate 10 samples and verify tail exponent falls in 3-5 range.
  2. **Ablation:** Compare GBM vs. VE vs. VP forward SDEs on same data; plot heavy-tail exponents and ACF decay curves for each.
  3. **Architecture validation:** Train three configurations (64/128/16, 128/256/32, 128/256/64) and quantify leverage effect capture via lead-lag correlation plots at lags k=0,20,50,100.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the framework be extended to incorporate more complex volatility dynamics, such as stochastic or rough volatility, without compromising the stability of the training process?
- **Basis in paper:** [explicit] The Conclusion states, "Future work includes extending the framework to incorporate richer volatility structures, such as stochastic or rough volatility, within the generative process."
- **Why unresolved:** The current model simplifies the dynamics to a variance-exploding SDE in log-price space to align with standard diffusion modeling, leaving the integration of finer volatility structures unexplored.
- **What evidence would resolve it:** A modified forward SDE that includes a separate volatility process (e.g., Heston model or rough volatility kernel) and empirical results demonstrating improved tracking of the volatility term structure.

### Open Question 2
- **Question:** Does conditioning the generative process on macroeconomic indicators or implied volatility surfaces significantly improve the model's utility for derivative pricing and stress testing?
- **Basis in paper:** [explicit] The Conclusion suggests, "Furthermore, conditioning the model on macroeconomic indicators or implied volatility surfaces may improve its applicability in derivative pricing and stress testing."
- **Why unresolved:** The current experiments focus on unconditional generation based on historical price data alone; the mechanism for and impact of incorporating external conditioning variables remain untested.
- **What evidence would resolve it:** A conditional variant of the model that generates paths in response to specific market regimes (defined by indicators) and demonstrates superior performance in pricing options or calculating VaR in stressed scenarios.

### Open Question 3
- **Question:** How can the model enforce strict log-normality of marginal price distributions to match the theoretical properties of the Black-Scholes framework while preserving trajectory-level stylized facts?
- **Basis in paper:** [explicit] The Conclusion identifies a limitation: "a more fundamental structural limitation lies in the departure from the exact log-normality of marginal price distributions."
- **Why unresolved:** The model generates the entire trajectory as a joint sample rather than a step-by-step Markov process, causing the distribution of individual time steps to deviate from the theoretical log-normality expected in GBM.
- **What evidence would resolve it:** An architectural modification, such as a Markovian inductive bias, that results in generated marginal distributions statistically indistinguishable from log-normal, while maintaining the reproduction of heavy tails and volatility clustering.

## Limitations

- The model assumes multiplicative (price-scaled) volatility dynamics that may not hold during regime shifts or extreme market conditions
- Generation is sequential and slow, requiring 2000 reverse steps per sample, limiting real-time applications
- Empirical evaluation focuses on historical stylized facts without testing out-of-distribution robustness or performance during market crashes
- Architectural claims about embedding dimensions lack direct empirical validation beyond the leverage effect

## Confidence

**High Confidence:** The GBM mechanism for inducing heteroskedasticity and the resulting heavy-tail distributions are mathematically sound and well-supported by the empirical evidence. The derivation from GBM to VE SDE via drift cancellation is straightforward and the tail exponent improvements (3.06-4.62 vs 8.49-8.96) are clearly demonstrated.

**Medium Confidence:** The claim about embedding dimensions capturing the leverage effect has theoretical grounding but limited direct validation. While the 128/256/64 configuration shows superior performance, the ablation study is restricted to one architectural axis, and the specific role of each embedding dimension remains partially speculative.

**Low Confidence:** The paper does not provide evidence for the model's performance in regime shifts or extreme market conditions. The assumption of Markovian log-price dynamics may be violated in practice, particularly for assets with rough volatility or structural breaks. The lack of optimizer specifications and exact data ranges also limits reproducibility.

## Next Checks

1. **Regime Shift Robustness Test:** Evaluate the trained model on data from financial crisis periods (2008, 2020) and compare synthetic samples' tail behavior and volatility clustering to actual crisis-period returns. This tests whether GBM-based noise injection maintains realism under extreme conditions.

2. **Architectural Ablation with Cross-Validation:** Systematically vary embedding dimensions (64/128/16, 128/256/32, 128/256/64) across multiple training folds and compute 95% confidence intervals for tail exponents and leverage effect metrics. This quantifies the statistical significance of architectural choices beyond visual inspection.

3. **Speed-Accuracy Trade-off Analysis:** Compare the GBM-based model's generation quality against a VE baseline trained for different reverse step counts (500, 1000, 2000, 4000). This determines whether the GBM benefits justify the computational cost or if simpler models can achieve comparable stylized fact reproduction with fewer steps.