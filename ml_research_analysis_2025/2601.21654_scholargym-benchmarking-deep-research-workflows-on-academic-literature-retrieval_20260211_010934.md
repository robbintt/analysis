---
ver: rpa2
title: 'ScholarGym: Benchmarking Deep Research Workflows on Academic Literature Retrieval'
arxiv_id: '2601.21654'
source_url: https://arxiv.org/abs/2601.21654
tags:
- query
- research
- retrieval
- papers
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ScholarGym is a simulation environment for reproducible evaluation
  of deep research workflows on academic literature retrieval. It addresses the non-determinism
  problem in live API-based evaluations by using a static corpus of 570K papers with
  deterministic retrieval.
---

# ScholarGym: Benchmarking Deep Research Workflows on Academic Literature Retrieval

## Quick Facts
- arXiv ID: 2601.21654
- Source URL: https://arxiv.org/abs/2601.21654
- Reference count: 40
- Primary result: Iterative query planning improves F1 by 2.9-3.3× over direct querying on 2,536 expert-annotated queries

## Executive Summary
ScholarGym introduces a simulation environment for reproducible evaluation of deep research workflows on academic literature retrieval. The system addresses the non-determinism problem in live API-based evaluations by using a static corpus of 570K papers with deterministic retrieval. The environment modularizes workflows into query planning, tool invocation, and relevance assessment stages, enabling controlled experiments and reinforcement learning training. Experiments demonstrate that iterative query planning substantially improves retrieval effectiveness over single-query baselines, with proprietary models achieving F1 of 0.447-0.329 and open-source models reaching 0.194-0.362.

## Method Summary
ScholarGym evaluates iterative deep research workflows by decomposing complex queries into subqueries across multiple stages. The system uses a static corpus of 570K academic papers from arXiv (1990-2024) with deterministic BM25 and dense retrieval backends. The workflow consists of three stages: query planning (generating subquery plans with CONTINUE/Derive/Expand operations), tool invocation (executing retrievals with pagination support), and relevance assessment (filtering candidates via Abstract-only or Adaptive Browsing modes). Memory mechanisms include a subquery tree Mt and compressed experience buffer Bt to maintain coherent state across iterations. The environment supports both standard and extended thinking modes, with the latter trading recall for precision through stricter relevance filtering.

## Key Results
- Iterative query planning improves F1 by 2.9-3.3× over direct querying baseline
- Proprietary models (GPT-5.2, Gemini3-Pro) achieve F1 of 0.447-0.329
- Open-source models (Qwen3, GLM-4.7) reach F1 of 0.194-0.362
- Extended thinking modes trade recall for precision, with gains scaling with model capability
- Memory mechanism ablation degrades F1 by 6-22%, with larger models more affected

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iterative query planning substantially improves retrieval effectiveness over single-query baselines.
- Mechanism: Complex research queries span multiple methodological and application domains. Iterative decomposition creates specialized subqueries (via DERIVE/EXPAND operations) that cover distinct semantic regions. Feedback from prior iterations identifies underexplored aspects, enabling progressive coverage that single queries cannot achieve.
- Core assumption: Relevant papers are distributed across semantic clusters that no single query formulation can comprehensively target.
- Evidence anchors: [abstract] "experiments on 2,536 queries with expert annotations show that iterative query planning improves F1 by 2.9-3.3× over direct querying"; [section] Table 3 shows Qwen3-30B achieving 0.285 F1 with full workflow vs 0.098 with Direct Query baseline (2.9× improvement).

### Mechanism 2
- Claim: Extended thinking modes trade recall for precision through more conservative relevance assessment.
- Mechanism: Thinking-enabled models apply stricter relevance thresholds during assessment, filtering marginal candidates more aggressively. This reduces false positives but discards some relevant papers that standard models would retain. The paper quantifies this via GT Discard Rate: thinking-enabled open-source models discard 0.93-1.03% of ground-truth papers vs <0.20% for proprietary models.
- Core assumption: The extended thinking process calibrates relevance thresholds toward precision-optimization rather than exploration.
- Evidence anchors: [abstract] "Extended thinking modes trade recall for precision, with gains scaling with model capability"; [section] Table 6 shows Qwen3-30B† achieves 0.290 precision with 1.03% GT Discard Rate, vs Qwen3-30B base at 0.181 precision with 0.13% discard rate.

### Mechanism 3
- Claim: Static corpus with deterministic retrieval enables reproducible evaluation and RL training by eliminating environmental variance.
- Mechanism: Live APIs introduce temporal drift, rate limiting, and evolving backend states—creating non-determinism that confounds cross-run comparisons. ScholarGym's static 570K paper corpus with fixed indices ensures identical queries yield identical results across runs. This decouples algorithmic reasoning from environmental noise, enabling controlled experiments and stable reward signals for RL policy learning.
- Core assumption: Environmental variance, not just model capability, is a significant confounder in deep research evaluation.
- Evidence anchors: [abstract] "addresses the non-determinism problem in live API-based evaluations by using a static corpus of 570K papers with deterministic retrieval"; [section] Related Work (2.1) cites Chen et al. (2025) on "reproducibility gap" and Zhang et al. (2025a) on evaluation failures from anti-scraping/URL timeouts.

## Foundational Learning

- Concept: **Precision-Recall Trade-off in Multi-stage Retrieval**
  - Why needed here: The paper's core finding is that iterative planning improves recall, while extended thinking improves precision—but they conflict. Understanding this trade-off is essential for interpreting F1 scores and designing workflows with explicit optimization targets.
  - Quick check question: If you observe F1 improvement but recall drops, what component behavior should you investigate first?

- Concept: **Memory Mechanisms in Agentic Systems**
  - Why needed here: ScholarGym's subquery tree Mt and experience buffer Bt maintain coherent state across iterations. Removing Bt degrades F1 by 6-22% (Table 8), with larger models more affected due to verbose reasoning traces.
  - Quick check question: Why does removing the compressed experience buffer hurt thinking-enabled models more than standard models?

- Concept: **Sparse vs Dense Retrieval Characteristics**
  - Why needed here: The paper implements both BM25 (sparse) and embedding-based (dense) backends. Dense retrieval improves recall by 7-26% for standard models (Table 7), but gains are muted for thinking-enabled variants due to stricter filtering.
  - Quick check question: When would you prefer sparse retrieval over dense retrieval for academic literature search?

## Architecture Onboarding

- Component map:
  Query Planning -> Tool Invocation -> Relevance Assessment -> Memory (Mt + Bt)

- Critical path:
  1. Query enters Planning → generates initial subqueries
  2. Subqueries → Tool Invocation → Retrieval (sparse/dense) → Ranking
  3. Candidates → Relevance Assessment → Selection + Feedback Ot
  4. Feedback loops to Planning for next iteration (up to T=5 default)
  5. Convergence when no new subqueries proposed

- Design tradeoffs:
  - T=5 iterations vs T=10+: Paper shows saturation by T=10 (Qwen3-30B gains only 2.3pp beyond T=5)
  - Abstract-only vs Adaptive Browsing: Adaptive reduces GT Discard Rate 30-40% but requires full-text fetches
  - Thinking-enabled vs standard: Higher precision, lower recall, more memory-sensitive

- Failure signatures:
  - High Ret.R but low R → relevance assessment over-filtering (check GT Discard Rate)
  - Flat recall across iterations → query planning not exploring new semantic regions (check Avg.Distance trajectory)
  - Low Avg.Distance with high-capability model → planning not utilizing feedback effectively

- First 3 experiments:
  1. Replicate Direct Query vs Full Workflow comparison on Test-Fast (200 queries) to validate 2.9-3.3× F1 improvement baseline
  2. Ablate memory mechanism (remove Bt) on both standard and thinking-enabled variants to confirm 6-22% degradation pattern
  3. Test sparse vs dense retrieval on a subset with ground-truth papers known to require semantic matching (to quantify dense retrieval benefit)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can reinforcement learning policies trained within ScholarGym effectively learn optimal exploration-exploitation trade-offs for information seeking?
- Basis in paper: [explicit] The conclusion states the environment "enables the training of reinforcement learning policies... paving the way for agents that can learn optimal exploration-exploitation trade-offs."
- Why unresolved: The paper provides the infrastructure (ScholarGym) and evaluates base LLMs, but does not implement or report results for RL-trained agents.
- What evidence would resolve it: Demonstrating that an agent trained via RL in ScholarGym outperforms the zero-shot proprietary models (like GPT-5.2) on the Test-Hard benchmark.

### Open Question 2
- Question: Can targeted training in ScholarGym close the "dual bottleneck" performance gap between open-source and proprietary models?
- Basis in paper: [explicit] Section 4.1/5 identifies "query planning and relevance assessment as dual bottlenecks limiting open-source performance."
- Why unresolved: While the paper identifies the cause of the gap (planning/assessment), it only evaluates models in zero-shot settings without exploring fine-tuning to bridge the 19% relative F1 deficit.
- What evidence would resolve it: Fine-tuning open-source models (e.g., Qwen3) on ScholarGym trajectories to achieve F1 scores competitive with GPT-5.2.

### Open Question 3
- Question: Does performance in the static ScholarGym environment reliably predict success in live, non-deterministic web search scenarios?
- Basis in paper: [inferred] The paper removes live API non-determinism (Section 1) to ensure reproducibility, but does not validate if high scores in the static environment transfer to noisy real-world APIs.
- Why unresolved: A potential "sim-to-real" gap exists; models might overfit to the specific 570K static corpus or deterministic retrieval logic.
- What evidence would resolve it: A correlation study showing that agents scoring high on ScholarGym Test-Hard maintain performance when deployed on live academic search APIs (e.g., Semantic Scholar) without degradation.

## Limitations

- Static corpus cannot capture evolving academic literature landscapes or newly published work, limiting external validity
- Proprietary model performance relies on black-box implementations where reasoning patterns remain opaque
- Experience buffer compression mechanism is specified but actual summarization method is not detailed
- 570K paper corpus represents a fixed snapshot that may miss domain-specific papers or recent breakthroughs

## Confidence

- **High Confidence**: Iterative query planning improves F1 by 2.9-3.3× over direct querying (supported by 2,536-query experimental results with expert annotations)
- **Medium Confidence**: Extended thinking modes trade recall for precision (observed pattern across models, but mechanism attribution requires further validation)
- **Medium Confidence**: Static corpus eliminates environmental variance for reproducible evaluation (theoretical premise supported by related work citations, but empirical validation limited to controlled conditions)

## Next Checks

1. **Cross-domain generalization test**: Evaluate ScholarGym workflows on a held-out corpus from a different academic domain (e.g., biomedical literature) to assess whether iterative planning benefits transfer beyond the arXiv-based training corpus.

2. **Temporal drift validation**: Compare retrieval performance on a timestamped query set (e.g., "COVID-19 treatments in 2020") against actual publication timelines to measure how well static corpus results predict real-world retrieval effectiveness.

3. **Memory mechanism ablation study**: Systematically vary experience buffer compression methods (e.g., token truncation vs. semantic summarization) while measuring F1 degradation patterns to determine whether observed 6-22% drops are implementation-dependent or fundamental to the memory architecture.