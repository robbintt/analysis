---
ver: rpa2
title: Pruning-Based TinyML Optimization of Machine Learning Models for Anomaly Detection
  in Electric Vehicle Charging Infrastructure
arxiv_id: '2503.14799'
source_url: https://arxiv.org/abs/2503.14799
tags:
- pruning
- each
- data
- time
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates pruning and feature selection techniques\
  \ to optimize machine learning models for anomaly detection in electric vehicle\
  \ charging infrastructure. The CICEVSE2024 dataset was used to train and optimize\
  \ three models\u2014MLP, LSTM, and XGBoost\u2014using hyperparameter tuning, SHAP-based\
  \ feature selection, and unstructured pruning."
---

# Pruning-Based TinyML Optimization of Machine Learning Models for Anomaly Detection in Electric Vehicle Charging Infrastructure

## Quick Facts
- arXiv ID: 2503.14799
- Source URL: https://arxiv.org/abs/2503.14799
- Reference count: 27
- Primary result: Pruning and feature selection significantly reduced model size and inference time for anomaly detection in EV charging infrastructure while maintaining high accuracy

## Executive Summary
This paper explores pruning and feature selection techniques to optimize machine learning models for anomaly detection in electric vehicle charging infrastructure. The study uses the CICEVSE2024 dataset to train and optimize three models—MLP, LSTM, and XGBoost—through hyperparameter tuning, SHAP-based feature selection, and unstructured pruning. The optimized models achieved substantial reductions in model size and inference time with minimal impact on performance, enabling real-time, on-device anomaly detection in resource-constrained TinyML environments.

## Method Summary
The research employs a comprehensive optimization approach combining hyperparameter tuning, SHAP-based feature selection, and unstructured pruning to reduce the computational footprint of ML models. The CICEVSE2024 dataset serves as the foundation for training three distinct models: MLP, LSTM, and XGBoost. The optimization pipeline systematically reduces model complexity while maintaining detection accuracy above 0.98. The study evaluates multiple pruning strategies and their impact on model efficiency, with particular focus on deployment feasibility in resource-constrained environments typical of TinyML applications.

## Key Results
- MLP model size decreased by 25.91% and inference time by 97.93% while maintaining accuracy above 0.98
- Substantial improvements in efficiency achieved across all three model types (MLP, LSTM, XGBoost)
- Optimized models enable real-time, on-device anomaly detection in resource-constrained TinyML environments

## Why This Works (Mechanism)
The optimization works by systematically reducing model complexity through targeted pruning and feature selection while preserving the most critical information for anomaly detection. SHAP values identify the most influential features, allowing the removal of redundant or less important inputs. Unstructured pruning eliminates unnecessary connections within the neural networks, dramatically reducing computational requirements without sacrificing detection capability. The combination of these techniques creates models that are both smaller and faster while maintaining the discriminative power needed for accurate anomaly detection in EV charging infrastructure.

## Foundational Learning
- **Unstructured pruning**: Selective removal of individual weights or neurons from neural networks to reduce model size and computation; needed for efficient TinyML deployment on resource-constrained devices
- **SHAP (SHapley Additive exPlanations)**: Game-theoretic approach to explain individual predictions by calculating feature importance; needed to identify which features contribute most to anomaly detection
- **Hyperparameter tuning**: Systematic optimization of model parameters to achieve best performance; needed to find optimal balance between model complexity and accuracy
- **TinyML constraints**: Limited computational resources, memory, and power in embedded devices; needed to understand deployment requirements and optimization targets
- **Anomaly detection metrics**: Accuracy, precision, recall, and F1-score for evaluating detection performance; needed to quantify optimization impact on model effectiveness
- **EV charging infrastructure data characteristics**: Time-series sensor data with potential anomalies; needed to understand the specific domain requirements and challenges

## Architecture Onboarding

**Component Map**: Data Preprocessing -> Feature Selection (SHAP) -> Model Training -> Hyperparameter Tuning -> Unstructured Pruning -> Evaluation

**Critical Path**: The most critical components are SHAP-based feature selection and unstructured pruning, as these directly determine the efficiency gains and final model performance. The pruning process must be carefully balanced to avoid removing critical information that would degrade anomaly detection capability.

**Design Tradeoffs**: The primary tradeoff involves balancing model size reduction against detection accuracy. Aggressive pruning yields greater efficiency but risks degrading performance. The study demonstrates that moderate pruning (25.91% for MLP) provides optimal balance, maintaining high accuracy while achieving significant size and speed improvements.

**Failure Signatures**: Potential failures include overfitting to the CICEVSE2024 dataset, which could limit generalizability to other EV charging infrastructure scenarios. Irregular memory access patterns from unstructured pruning may impact actual hardware performance despite improved inference times. Insufficient consideration of energy consumption could result in models that are fast but power-inefficient.

**First Experiments**:
1. Apply the same optimization pipeline to a different EV charging dataset to test generalizability
2. Measure actual power consumption of optimized models on representative TinyML hardware
3. Compare unstructured pruning results with structured pruning approaches to evaluate memory access patterns

## Open Questions the Paper Calls Out
None

## Limitations
- Potential overfitting of pruning techniques to the specific CICEVSE2024 dataset, limiting generalizability to other EV charging infrastructure datasets or real-world deployment scenarios
- Unstructured pruning may introduce irregular memory access patterns that could impact actual hardware performance on resource-constrained devices, despite reported improvements in inference time
- Study does not provide details on energy consumption of optimized models, which is critical for TinyML applications

## Confidence
- **High confidence**: Reported improvements in model size and inference time are directly measurable metrics
- **Medium confidence**: Maintenance of high accuracy post-optimization relies on appropriate application of pruning and feature selection techniques
- **Low confidence**: Scalability of the approach to other datasets or deployment scenarios due to limited study scope

## Next Checks
1. Test optimized models on additional EV charging infrastructure datasets to assess generalizability and robustness
2. Evaluate energy consumption of pruned models on actual TinyML hardware to verify power efficiency gains
3. Investigate impact of structured pruning techniques on model performance and hardware efficiency to address potential memory access irregularities from unstructured pruning