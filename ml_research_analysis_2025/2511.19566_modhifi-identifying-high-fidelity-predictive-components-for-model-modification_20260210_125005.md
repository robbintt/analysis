---
ver: rpa2
title: 'ModHiFi: Identifying High Fidelity predictive components for Model Modification'
arxiv_id: '2511.19566'
source_url: https://arxiv.org/abs/2511.19566
tags:
- page
- fidelity
- pruning
- cited
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ModHiFi, a framework for modifying pre-trained
  models without access to training data or loss functions, using only synthetic data.
  The authors theoretically demonstrate that global reconstruction error is linearly
  bounded by local reconstruction errors in Lipschitz-continuous networks, motivating
  the use of Subset Fidelity as a metric to identify important model components.
---

# ModHiFi: Identifying High Fidelity predictive components for Model Modification

## Quick Facts
- **arXiv ID**: 2511.19566
- **Source URL**: https://arxiv.org/abs/2511.19566
- **Reference count**: 40
- **Primary result**: ModHiFi achieves 11% speedup over state-of-the-art pruning methods on ImageNet models without training data

## Executive Summary
ModHiFi introduces a novel framework for modifying pre-trained models using only synthetic data, without access to original training data or loss functions. The method leverages the theoretical insight that global reconstruction error is linearly bounded by local reconstruction errors in Lipschitz-continuous networks, enabling identification of high-fidelity predictive components. By computing Subset Fidelity scores for individual components and applying optimal weight compensation, ModHiFi achieves state-of-the-art results for both structured pruning (11% speedup on ImageNet) and classwise unlearning (complete unlearning on CIFAR-10). The framework demonstrates strong performance across diverse architectures including CNNs and Transformers.

## Method Summary
ModHiFi operates by first computing singleton fidelity scores for each model component using synthetic data, then selecting components based on these scores (keeping high-fidelity components for pruning, removing them for unlearning). The method applies optimal weight compensation to remaining components using a closed-form least-squares solution derived from the Component Similarity Matrix. For structured pruning, ModHiFi iteratively removes low-fidelity components while maintaining accuracy, achieving significant speedups without fine-tuning. For classwise unlearning, the method identifies and removes class-specific high-fidelity components from forget-class samples, achieving complete unlearning on CIFAR-10 and competitive results on Transformers.

## Key Results
- Achieves 11% speedup over state-of-the-art pruning methods on ImageNet models
- Completes classwise unlearning on CIFAR-10 without fine-tuning
- Maintains competitive performance on Swin Transformers for unlearning tasks
- Validated across multiple architectures including CNNs and Transformers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Local reconstruction errors linearly bound global prediction errors in Lipschitz-continuous networks
- Mechanism: Error from removing components at layer *l* propagates through subsequent layers, with each layer's Lipschitz constant amplifying the error. The total amplification factor is the product of downstream layer Lipschitz constants
- Core assumption: Networks are locally Lipschitz continuous on bounded activation domains
- Evidence anchors: [abstract] "global reconstruction error is linearly bounded by local reconstruction errors"; [section 3.2] Theorem 3.6 formalizes the bound

### Mechanism 2
- Claim: Singleton fidelity scores efficiently identify high-fidelity components when input contributions are approximately uncorrelated
- Mechanism: The Component Similarity Matrix (CSM) *Q^l_c* captures pairwise correlations between input contributions. When off-diagonal entries are near zero, the reconstruction error decomposes into independent per-component terms
- Core assumption: Features are approximately uncorrelated
- Evidence anchors: [abstract] "In the uncorrelated features setting, selecting individual components via their Subset Fidelity scores is optimal"; [section 3.2] Theorem 3.9 proves optimality

### Mechanism 3
- Claim: Weight compensation (*δ^⋆*) restores output fidelity without fine-tuning after component removal
- Mechanism: After pruning, remaining weights are rescaled by the optimal compensation term (derived from CSM pseudoinverse) to minimize reconstruction error
- Core assumption: The pruned subnetwork can approximately reconstruct the original layer output with appropriate rescaling
- Evidence anchors: [section 4] "optimal compensation term *δ^⋆* (derived in Proposition 3.8) using the remaining weights"

## Foundational Learning

- **Lipschitz continuity**: Why needed here - Theoretical justification that local errors don't explode globally; normalization layers are locally (not globally) Lipschitz on bounded domains. Quick check: Can you explain why LayerNorm is not globally Lipschitz but is locally Lipschitz away from zero?

- **Least-squares reconstruction**: Why needed here - Subset Fidelity is essentially minimizing squared reconstruction error with a compensation term; closed-form solutions require understanding pseudoinverses. Quick check: What happens to the reconstruction when the Component Similarity Matrix is ill-conditioned?

- **Synthetic data as distributional proxy**: Why needed here - The method operates without training data; fidelity scores are estimated on synthetic samples that approximate the training distribution. Quick check: How would distribution shift between synthetic and real data affect fidelity estimation accuracy?

## Architecture Onboarding

- **Component map**: Synthetic data generation -> Fidelity Estimation Module -> Component Selection Module -> Compensation Module -> Model modification
- **Critical path**: 1) Generate synthetic samples from target distribution, 2) Forward pass to collect activations per layer, 3) Compute CSM and singleton scores, 4) Select components based on task, 5) Apply compensation to remaining weights, 6) Optionally: brief fine-tuning
- **Design tradeoffs**: Granularity (layer/channel/neuron-level), Sparsity vs. accuracy (controlled by percentile threshold), Synthetic data quality (higher FID → better tradeoff)
- **Failure signatures**: Near-zero activation norms at normalization layers (Lipschitz bound invalidates), Highly correlated features (singleton selection suboptimal), Poor synthetic data quality (fidelity scores misestimate importance), Excessive pruning (compensation cannot recover)
- **First 3 experiments**: 1) HiFi Set Existence Validation - Monte Carlo sample subsets on ResNet-50 layer to confirm <20% components achieve ≥0.8 fidelity, 2) Counterfactual Perturbation - Noise HiFi vs. non-HiFi components and verify HiFi perturbation causes >10× accuracy drop, 3) End-to-End Pruning - Apply ModHiFi-P to CIFAR-10 ResNet-50 with 2% synthetic samples and compare accuracy-sparsity against L2-pruning baseline

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical foundations rely heavily on Lipschitz continuity assumptions that may not hold near normalization layers where activation norms approach zero
- Framework's dependence on synthetic data quality introduces significant practical limitation - performance degrades substantially when synthetic data FID exceeds ~15
- Compensation mechanism's effectiveness for aggressive pruning scenarios (>50% pruning rates) remains underexplored

## Confidence

- **High confidence**: Global Lipschitz bound theorem (Theorem 3.6) and empirical validation on CIFAR-10 pruning results
- **Medium confidence**: Unlearning performance claims on Swin Transformer, as these results show larger variance across runs
- **Low confidence**: Compensation mechanism effectiveness at >50% pruning rates, as ablation studies only cover moderate sparsity levels

## Next Checks

1. **Lipschitz robustness test**: Systematically vary activation norms near normalization layers (through input scaling) and measure how global error bounds degrade; identify the threshold where theoretical guarantees break

2. **Correlation sensitivity analysis**: Construct synthetic datasets with controlled feature correlation structures and evaluate how ModHiFi's singleton selection performs vs. optimal joint selection; quantify performance gap

3. **Synthetic data stress test**: Generate synthetic datasets with progressively increasing FID (from 5 to 50) and measure the full Pareto frontier of accuracy vs. sparsity; identify the FID threshold beyond which ModHiFi becomes impractical