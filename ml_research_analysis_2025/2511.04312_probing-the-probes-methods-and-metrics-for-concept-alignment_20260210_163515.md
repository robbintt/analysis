---
ver: rpa2
title: 'Probing the Probes: Methods and Metrics for Concept Alignment'
arxiv_id: '2511.04312'
source_url: https://arxiv.org/abs/2511.04312
tags:
- concept
- probes
- methods
- concepts
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the reliability of Concept Activation Vectors
  (CAVs) as tools for explaining deep neural networks. The authors show that high
  probe classification accuracy does not necessarily indicate that a CAV faithfully
  represents its target concept, as probes often learn spurious correlations rather
  than the intended concept.
---

# Probing the Probes: Methods and Metrics for Concept Alignment

## Quick Facts
- arXiv ID: 2511.04312
- Source URL: https://arxiv.org/abs/2511.04312
- Reference count: 15
- Key outcome: High probe classification accuracy does not necessarily indicate faithful concept representation, as probes often learn spurious correlations rather than the intended concept.

## Executive Summary
This paper investigates the reliability of Concept Activation Vectors (CAVs) for explaining deep neural networks. The authors demonstrate that standard linear probes can achieve high classification accuracy while relying on spurious correlations rather than the target concept. To address this, they introduce three new probing methods (Pattern-CAV, Segmentation-CAV, and Combination-CAV), spatial pooling for translation invariance, and three quantitative metrics for assessing concept alignment. Their analysis shows that translation-invariant and segmentation-based methods consistently improve alignment, and that probe classification accuracy alone is insufficient for evaluating concept quality.

## Method Summary
The authors evaluate CAVs using ResNet50 pretrained on ImageNet, extracting post-ReLU activations from `layer4[0]` and training binary logistic regression probes. They introduce False Positive CAVs (FP-CAVs) trained on misclassified negatives to detect spurious correlations, spatial pooling for translation invariance, and segmentation-based probes optimized against ground truth masks. Three new metrics are proposed: hard accuracy (performance on randomized backgrounds), segmentation score (spatial attribution quality), and augmentation robustness. The Broden dataset with 148 object concepts serves as the testbed.

## Key Results
- FP-CAVs trained without target concept examples achieve 74% accuracy (vs 81% for standard probes), demonstrating spurious correlation dominance
- Translation-invariant CAVs consistently outperform position-sensitive counterparts in alignment metrics
- Segmentation-CAVs achieve highest segmentation scores (0.231 vs 0.161 for classifier) by focusing on object-specific features
- Probe classification accuracy alone is insufficient for evaluating concept quality

## Why This Works (Mechanism)

### Mechanism 1: Spurious Correlation Dominance via FP-CAVs
The authors demonstrate that high classification accuracy in linear probes is insufficient evidence of concept learning. By training FP-CAVs on hard negatives (misclassified negatives relabeled as positive), they show these FP-CAVs achieve 74% accuracy despite never seeing the target concept. This proves standard probes exploit the same spurious correlations found in false positives, such as "sky" features for "buildings."

### Mechanism 2: Translation Invariance via Spatial Pooling
Standard probes flatten CNN activations, allowing them to weight features differently based on spatial location. By applying Global Average Pooling before the linear probe, the input becomes location-agnostic, forcing the probe to detect concepts regardless of position. This reduces susceptibility to positional artifacts and improves alignment by ensuring the concept is detected uniformly across the image.

### Mechanism 3: Spatial Alignment via Segmentation Loss
Segmentation-CAVs optimize a loss function comparing linear attribution maps against ground truth segmentation masks. This forces the probe to attribute importance to pixels within the concept's spatial extent rather than background context, reducing spurious correlations by penalizing attribution to background pixels.

## Foundational Learning

- **Linear Separability & Probing**: High-dimensional spaces allow "easy" linear separation, explaining why probes achieve high accuracy even when learning "wrong" things. Quick check: If a probe achieves 100% accuracy on training data in high-dimensional space, does it guarantee the probe learned the intended concept? (Answer: No, per this paper).

- **Inductive Bias in Probes**: Different probe architectures have different inductive biases. Classifier probes favor discrimination (accuracy), while Segmentation probes favor locality. Quick check: Which probe type would likely perform best on a dataset where the target object is always in the same corner vs. one where it moves around?

- **Translation Equivariance (CNNs)**: CNNs activate similarly for features regardless of position. Quick check: Why does flattening a CNN feature map before probing potentially introduce more spurious correlations than pooling it first?

## Architecture Onboarding

- **Component map**: ResNet50 (frozen) -> `layer4[0]` post-ReLU activations -> Probe Heads (Classifier/Pattern/Segmentation) -> Evaluation Suite (Accuracy/Hard Accuracy/Segmentation Score)

- **Critical path**: 1) Extract activations from ResNet, 2) Train baseline Classifier-CAV, 3) Generate FP-CAV to check for spurious correlation overlap, 4) Retrain with Spatial Pooling or Segmentation Loss, 5) Verify with Concept Localization Maps

- **Design tradeoffs**: Classifier-CAVs: High accuracy, low alignment (prone to background). Segmentation-CAVs: High alignment, lower accuracy (ignores useful context), requires masks. Translation Invariance: Better alignment and robustness, discards positional signals.

- **Failure signatures**: High Accuracy/Low Hard Accuracy (probe relies on background), High Cosine Similarity between Concept and FP-CAV (>0.6 suggests spurious correlation dominance), Corner Attribution (CLMs show high attribution in image corners).

- **First 3 experiments**: 1) Reproduce FP-CAV Gap: Train standard CAV for "Horse," then train second CAV only on its false positives and compare accuracies/cosine similarity. 2) Implement Spatial Pooling: Apply GAP to activations before training new Classifier-CAV and evaluate Hard Accuracy improvement. 3) Visualize with CLMs: Implement Shifted Attribution Map for both standard and pooled CAVs to confirm "road" activation disappears for "Car" concept when pooling is used.

## Open Questions the Paper Calls Out

### Open Question 1
Can systematic orthogonalization methods, such as rejecting multiple False Positive CAVs via a Gram-Schmidt-like process, effectively remove spurious components and improve concept alignment? The authors demonstrated rejecting a single FP-CAV but noted the assumption that orthogonalization is more effective than hard negative training remains unverified.

### Open Question 2
To what extent do CNNs encode absolute positional information through boundary effects like padding, and how does this impact the reliability of translation-invariant probes? The authors observed persistent corner artifacts in Concept Localization Maps for translation-invariant probes, hypothesizing boundary effects but not verifying the mechanism.

### Open Question 3
How does concept alignment vary across network layers, and does the standard practice of selecting late layers based on classification accuracy fail to identify layers with superior concept alignment? The study focused on a single layer and noted that while later layers have higher accuracy, they may suffer worse alignment, lacking a comprehensive cross-layer comparison.

## Limitations
- Unknown background source: The paper uses "random images" to replace backgrounds for hard accuracy evaluation, but the exact source is unspecified
- Dataset-specific findings: Results are derived from Broden's 148 object concepts with specific size/image requirements
- No comparison to alternative interpretability methods: The paper doesn't benchmark CAVs against other interpretability techniques

## Confidence
- High Confidence: The core claim that high probe classification accuracy doesn't guarantee concept alignment is well-supported by FP-CAV experiments
- Medium Confidence: The effectiveness of translation-invariant and segmentation-based methods for improving alignment is demonstrated but could benefit from testing on additional architectures
- Low Confidence: The paper's claims about concept alignment in transformer architectures are not directly tested

## Next Checks
1. **Background Source Verification**: Implement hard accuracy evaluation using different background sources (Broden negatives, ImageNet, uniform noise) to confirm robustness
2. **Cross-Architecture Testing**: Apply probing methods and metrics to Vision Transformer models to assess whether translation invariance remains beneficial
3. **Ablation Study on FP-CAV Construction**: Systematically vary the number of false positive examples (10, 25, 50, 100) to determine minimum required for detecting spurious correlations