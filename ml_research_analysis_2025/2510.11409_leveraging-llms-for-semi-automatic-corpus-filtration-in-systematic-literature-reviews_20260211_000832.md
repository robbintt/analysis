---
ver: rpa2
title: Leveraging LLMs for Semi-Automatic Corpus Filtration in Systematic Literature
  Reviews
arxiv_id: '2510.11409'
source_url: https://arxiv.org/abs/2510.11409
tags:
- papers
- consensus
- llms
- open
- literature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work demonstrates that large language models can significantly
  accelerate literature filtering in systematic reviews while maintaining high accuracy.
  The authors propose a human-supervised pipeline where multiple LLMs classify papers
  using descriptive prompts, and a consensus scheme harmonizes results.
---

# Leveraging LLMs for Semi-Automatic Corpus Filtration in Systematic Literature Reviews

## Quick Facts
- arXiv ID: 2510.11409
- Source URL: https://arxiv.org/abs/2510.11409
- Reference count: 40
- Primary result: LLM consensus filtering achieved 100% recall with only 166 false positives on 8,323 papers

## Executive Summary
This work demonstrates that large language models can significantly accelerate literature filtering in systematic reviews while maintaining high accuracy. The authors propose a human-supervised pipeline where multiple LLMs classify papers using descriptive prompts, and a consensus scheme harmonizes results. Evaluated on 8,323 papers with human-labeled ground truth, the approach reduced false negatives to near zero while excluding over 95% of irrelevant papers. Open-source models from 2025 achieved over 99% accuracy, matching commercial systems. A visual analytics interface enables interactive refinement of prompts and model selection. The study shows responsible human-AI collaboration can make SLR creation dramatically more efficient without compromising quality.

## Method Summary
The method employs multiple LLMs to classify academic papers using structured prompts containing role definition, research topic, inclusion/exclusion criteria, and output format requirements. Each model independently evaluates title and abstract, providing INCLUDE/DISCARD decisions with 2-sentence reasoning. A consensus scheme (union-based) determines final classification, where a paper is included if any model votes to include. The process is human-supervised, allowing real-time inspection of LLM reasoning and iterative prompt refinement through a visual analytics interface (LLMSurver). The approach was evaluated on a corpus of 8,323 papers from ACM Digital Library, IEEE Xplore, and Eurographics, achieving 100% recall with 166 false positives.

## Key Results
- Achieved 100% recall (0 false negatives) with 166 false positives on 8,323 papers
- Open-source models from 2025 matched commercial systems with >99% accuracy
- Consensus voting with multiple models significantly outperformed individual model performance
- Visual analytics interface enabled effective human supervision and prompt refinement

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-model consensus voting reduces false negatives without proportionally increasing false positives.
- Mechanism: Heterogeneous LLMs exhibit different error profiles; a union-based consensus (include if any model votes "include") preserves recall while individual model errors rarely overlap, limiting compound false positives.
- Core assumption: LLM classification errors are weakly correlated across models with different training objectives and architectures.
- Evidence anchors:
  - [abstract]: "multiple large language models (LLMs)... deciding jointly using a consensus scheme"
  - [section 5.2, Table 3]: Best 3-model consensus achieved 0 false negatives with only 166 false positives vs. 603+ for all-model consensus.
  - [corpus]: Limited external validation—neighbor papers on LLM-SLR automation (e.g., "Compiling Prompts, Not Crafting Them") focus on prompt reliability, not ensemble effects.
- Break condition: Models converge toward identical behavior (e.g., same training data contamination) → errors correlate, consensus gains diminish.

### Mechanism 2
- Claim: Structured prompts with explicit inclusion/exclusion criteria enable consistent classification decisions.
- Mechanism: Role framing ("you are a professor") + concrete criteria + examples + output format constraints reduce ambiguous outputs and align model reasoning with human intent.
- Core assumption: LLMs can reliably follow multi-part conditional instructions for binary classification.
- Evidence anchors:
  - [abstract]: "classifying papers based on descriptive prompts"
  - [section 3, Figure 2]: Prompt template includes role, topic definition, examples, explicit rules, and required output format.
  - [corpus]: "Compiling Prompts, Not Crafting Them" supports systematic prompt approaches but warns of brittleness—consistent with paper's prompt sensitivity findings (Section 5.3).
- Break condition: Domain terminology is ambiguous across fields (e.g., "network" in CS vs. biology) → models misapply criteria.

### Mechanism 3
- Claim: Human supervision with inspectable reasoning enables iterative refinement that automatic approaches cannot achieve.
- Mechanism: LLM-generated justifications expose decision logic; humans identify systematic errors (over-exclusion, misinterpretation) and adjust prompts or model selection accordingly.
- Core assumption: Humans can correctly diagnose error causes from 2-sentence LLM reasoning outputs.
- Evidence anchors:
  - [abstract]: "human-supervised and interactively controlled... real-time inspection and modification"
  - [section 5.3]: Authors analyzed LLM reasoning to identify over-strict exclusions, then designed 7 prompt variants, reducing false negatives from 16 to 0 (P7, though at precision cost).
  - [corpus]: External validation limited—no comparative studies on human-in-the-loop vs. fully automated LLM-SLR pipelines found in neighbors.
- Break condition: Reasoning outputs are unfaithful (post-hoc rationalizations) → human corrections target wrong causes.

## Foundational Learning

- **Concept: Recall-Precision Trade-off in Filtering**
  - Why needed: SLR filtration prioritizes recall (missing a relevant paper is worse than extra manual review); understanding this shapes consensus strategy.
  - Quick check question: If your consensus excludes 5 relevant papers but saves 1,000 manual reviews, is this acceptable for an SLR? (Answer: Generally no—recall is paramount in SLR methodology.)

- **Concept: Ensemble Diversity**
  - Why needed: Consensus only helps if models disagree for beneficial reasons (different strengths), not identical reasons (same biases).
  - Quick check question: Two models both trained primarily on CS papers—will their consensus be more or less effective for filtering medical literature?

- **Concept: Prompt Sensitivity**
  - Why needed: Small prompt changes ("include when uncertain" vs. "discard when uncertain") produced 4,000+ vs. ~100 false positives (Section 5.3).
  - Quick check question: Before running full-corpus classification, what validation step should you perform? (Answer: Sample-based prompt testing with ground-truth comparison.)

## Architecture Onboarding

- **Component map:** [BibTeX/DOI Input] → [Preprocessing: dedup, unify] → [Prompt Template] → [LLM Agents (N models)] → [Individual Classifications + Reasoning] → [Consensus Scheme (N-voting)] → [Filtered Corpus] → [Human Review: prompt/model adjustment] ← [Results + Disagreement Flags] ← [LLMSurver Interface]

- **Critical path:** Prompt design → Sample classification (50-100 papers) → Inspect disagreements → Refine prompt → Full corpus run → Consensus → Manual review of borderline cases.

- **Design tradeoffs:**
  - Open (8B) models: Local deployment, privacy-preserving, but require prompt tuning for acceptable recall.
  - Commercial models: Higher out-of-box performance, but cost scales with corpus size and raises data privacy concerns.
  - Union vs. majority consensus: Union maximizes recall (conservative), majority balances precision/recall, intersection maximizes precision (aggressive).

- **Failure signatures:**
  - High false negatives: Prompt too strict; models lack domain context; small models under-reason.
  - Excessive false positives (>500): Prompt ambiguity ("include when uncertain"); models overly conservative.
  - Disagreement on every paper: Prompt criteria unclear or topic boundaries genuinely ambiguous.
  - Model outputs incomplete/irrelevant: Temperature too high; model lacks instruction-following training.

- **First 3 experiments:**
  1. **Prompt validation on sample:** Run 3 diverse LLMs on 50 papers with known ground truth; measure per-model and consensus recall/precision; identify systematic error patterns.
  2. **Model subset ablation:** With fixed prompt, test consensus performance with 2, 3, 5, all models; plot recall vs. false positives to find optimal subset size.
  3. **Domain transfer test:** Apply best-performing prompt to papers from a different subfield (e.g., from VR visualization to network security); measure performance drop to assess generalization.

## Open Questions the Paper Calls Out
None

## Limitations
- Single SLR corpus evaluation (8,323 papers, 88 relevant) limits generalizability to other domains
- Reliance on 2-sentence LLM reasoning assumes faithful representation of decision logic
- Consensus scheme assumes error independence across models, which may break with shared training data

## Confidence
- **High Confidence:** The core finding that LLM-based consensus filtering dramatically reduces manual screening workload while maintaining near-zero false negatives.
- **Medium Confidence:** The claim that open-source models from 2025 match commercial performance based on single corpus evaluation.
- **Low Confidence:** The assertion that human inspection of 2-sentence LLM reasoning reliably identifies systematic errors without systematic validation.

## Next Checks
1. **Cross-Domain Generalization Test:** Apply the best-performing prompt and model consensus to a completely different SLR corpus with known ground truth to measure domain transfer performance.

2. **Error Analysis Validation:** Have independent human experts review both LLM reasoning and actual paper content to determine if human corrective actions were justified.

3. **Ensemble Diversity Quantification:** Calculate pairwise correlation of individual model errors on a validation set to verify the consensus benefit comes from diverse error profiles.