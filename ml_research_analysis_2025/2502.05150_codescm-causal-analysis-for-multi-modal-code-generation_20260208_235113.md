---
ver: rpa2
title: 'CodeSCM: Causal Analysis for Multi-Modal Code Generation'
arxiv_id: '2502.05150'
source_url: https://arxiv.org/abs/2502.05150
tags:
- code
- language
- prompt
- causal
- natural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CodeSCM, a structural causal model for analyzing
  multi-modal code generation. The method uses causal mediation analysis with latent
  mediator variables to quantify direct and total effects of different prompt modalities
  (natural language, code, and input-output examples) on code generation performance.
---

# CodeSCM: Causal Analysis for Multi-Modal Code Generation

## Quick Facts
- arXiv ID: 2502.05150
- Source URL: https://arxiv.org/abs/2502.05150
- Reference count: 11
- Primary result: Introduces a structural causal model to quantify how different prompt modalities (NL, code, I/O) causally influence code generation performance.

## Executive Summary
This paper introduces CodeSCM, a structural causal model for analyzing multi-modal code generation. The method uses causal mediation analysis with latent mediator variables to quantify direct and total effects of different prompt modalities (natural language, code, and input-output examples) on code generation performance. Experiments across three benchmarks (HumanEval+, mMBPP+, and CoderEval) with models including GPT-4 Turbo, WizardCoder, and Llama-3 reveal that input-output examples and natural language code components significantly influence code generation, beyond just natural language instructions. The analysis also uncovers benchmark memorization in LLMs and shows that code-specific pretraining improves multi-modal alignment in embedding space.

## Method Summary
CodeSCM decomposes code generation prompts into four modalities (NL, Code_AL, Code_NL, I/O pairs) and uses a Structural Causal Model to analyze causal effects on code correctness. Total Effects are measured by removing modalities via do-calculus, while Direct Effects are measured through semantics-preserving transformations. The framework applies causal mediation analysis using latent mediators for code and natural language semantics. The method evaluates multiple models (GPT-4 Turbo, WizardCoder, Llama-3) across three benchmarks, measuring pass@1 accuracy and analyzing embedding space alignment between prompt modalities.

## Key Results
- Input-output examples and natural language code components significantly contribute to code generation beyond natural language instructions
- Code-specific pretraining improves multi-modal alignment in embedding space compared to general-purpose LLMs
- Benchmark memorization is detected in LLMs when function names are standardized but models still generate original names

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Disentangling multi-modal prompt components into separate causal variables allows identification of which specific modality—natural language (NL), code (Code_AL, Code_NL), or input-output (I/O) examples—drives code generation accuracy, revealing that non-instruction components significantly contribute.
- **Mechanism:** A Structural Causal Model (SCM) is constructed where each prompt modality is treated as an independent variable influencing a binary response variable (code correctness) through two latent mediators: M_Code (code semantics) and M_NL (natural language semantics). Interventions (do-calculus) remove or modify individual modalities (e.g., setting Code_AL to NULL) to measure the Total Effect (TE) on accuracy. Direct Effects (DE) are measured by semantics-preserving transformations (e.g., inserting dead code) that keep mediators constant but change surface form.
- **Core assumption:** Each prompt modality can be cleanly separated and manipulated without confounding variables, and the latent mediators (M_Code, M_NL) accurately capture the semantic intent of the prompt components.
- **Evidence anchors:**
  - [abstract] "CodeSCM introduces latent mediator variables to separate the code and natural language semantics of a multi-modal code generation prompt."
  - [section 3.1] "Each prompt P in dataset D is decomposed into its multi-modal components... We consider four modalities: Natural Language (N L), algorithmic channel of Code (Code AL), natural language channel of Code (Code N L), and input-output example pairs (I/O)."
  - [corpus] Weak direct evidence; related works like 'MSCRS' and 'Treble Counterfactual VLMs' explore multi-modal and causal approaches but do not specifically validate this SCM decomposition for code generation.

### Mechanism 2
- **Claim:** Direct Effects, measured via semantics-preserving transformations (e.g., converting equality assertions to inequalities, adding dead code), reveal spurious correlations in LLMs that are not mediated by the core semantic understanding (M_Code, M_NL).
- **Mechanism:** By applying transformations that alter the surface form of a modality (e.g., changing `assert I_l == I_r` to `assert (I_l <= I_r) + (I_r >= I_r)`) while theoretically preserving its mediated semantic value, any resulting drop in accuracy represents the Direct Effect. This effect bypasses the latent mediators and captures the model's reliance on superficial patterns or spurious correlations learned during training.
- **Core assumption:** The chosen semantics-preserving transformations (dead code, dead strings, inequality substitution) genuinely leave the latent mediator variables unchanged for all models and datasets.
- **Evidence anchors:**
  - [abstract] "Using the principles of Causal Mediation Analysis on these mediators we quantify direct effects representing the model's spurious leanings."
  - [section 4.3] "We define direct effects (DE) by noting the drop in pass@1 accuracy of the model under the semantics-preserving transformations of modalities where the latent mediators remain unchanged... These effects also represent the spurious correlations..."
  - [corpus] No direct evidence from provided corpus.

### Mechanism 3
- **Claim:** Code-specific pretraining improves the alignment of different prompt modalities (NL, code, I/O) within the embedding space, leading to superior code generation performance compared to general-purpose LLMs.
- **Mechanism:** The paper hypothesizes that codeLLMs, like CodeLLaMa, learn to map disparate modal components of a prompt to a more unified region of the embedding space compared to a base model like LLaMa-2. This improved "multi-modal alignment" allows the model to better associate the natural language instructions with the code solution and I/O examples, reducing the effective distance between the prompt and the correct code generation in the latent space.
- **Core assumption:** The cosine similarity of averaged hidden state representations in the final layer is a reliable proxy for the quality of multi-modal alignment and correlates with code generation capability.
- **Evidence anchors:**
  - [abstract] "...code-specific pretraining improves multi-modal alignment in embedding space."
  - [section 4.4] "The pretraining stages of code-aware LLMs add multi-modal alignment in codeLLMs... CodeLLaMa can separate them [I/O examples and function header] in the embedding space."
  - [corpus] Weak direct evidence; related work on 'Causal-Adapter' deals with cross-modal generation but not specifically pretraining alignment for code.

## Foundational Learning

- **Concept:** Structural Causal Model (SCM)
  - **Why needed here:** This is the core formal framework of the paper. An SCM defines variables, the structural equations relating them, and the causal graph. Without this, one cannot frame "Total Effect" or "Direct Effect" rigorously or apply the do-calculus used for the analysis.
  - **Quick check question:** Can you draw the causal graph described in CodeSCM, showing the relationship between prompt modalities, latent mediators, and the output code correctness?

- **Concept:** Causal Mediation Analysis
  - **Why needed here:** This is the specific analytical technique used to decompose the total causal effect into components. It distinguishes between effects that flow through the hypothesized mediators (semantic understanding) and direct effects (spurious correlations), which is the central methodological contribution.
  - **Quick check question:** How does the paper operationalize a "semantics-preserving transformation" for an I/O pair, and why is this crucial for measuring the Direct Effect?

- **Concept:** Embedding Space Alignment
  - **Why needed here:** This concept underpins the explanation for why code-specialized models outperform general ones. Understanding how the distribution of embeddings for different modalities (e.g., docstring vs. code solution) changes with pretraining is key to interpreting the results in Section 4.4.
  - **Quick check question:** According to the paper's analysis, what is the primary difference in how CodeLLaMa and LLaMa-2 cluster the embeddings of a full prompt and its corresponding ground-truth code solution?

## Architecture Onboarding

- **Component Map:** Prompt Decomposition Engine -> Intervention Handler -> Evaluation Harness -> Embedding Prober
- **Critical Path:**
  1. **Decompose:** Parse dataset prompts into the four modality variables.
  2. **Intervene:** For each prompt, generate the intervened versions. For TE, set the target modality to NULL. For DE, apply the specified transformation (e.g., `Code_AL` -> `Code_AL` + `C_DC`).
  3. **Generate & Evaluate:** Feed original and intervened prompts into the LLM, generate code, run tests, and record accuracy (Y=1 if pass, 0 if fail).
  4. **Calculate Effects:** Compute TE and DE using the formulae in Definitions 2.1 and 2.2, substituting empirical accuracy.

- **Design Tradeoffs:**
  - **Simplicity vs. Semantics-Preservation:** The transformations used (e.g., adding a prefix) are simple to implement and independent of the prompt content, but there's a risk they might not perfectly preserve semantics for all cases, potentially confounding DE with a genuine semantic shift. More complex, adaptive transformations were avoided to maintain variable independence.
  - **Proxy vs. Ground Truth:** The analysis relies on the accuracy metric from a finite set of test cases as a proxy for "code correctness." This assumes the test suite is comprehensive and reliable.
  - **Model Coverage:** The SCM assumes no confounders. The paper explicitly states this as a limitation, leaving the incorporation of confounding variables for future work.

- **Failure Signatures:**
  - **Inconsistent Modality Parsing:** If the prompt decomposition logic fails to correctly identify `Code_AL` (algorithmic code) vs `Code_NL` (natural language channel in code), all downstream effect measurements will be misattributed.
  - **Transformation Leakage:** If a transformation like "dead code insertion" unintentionally alters the mediators' values for a specific prompt type, the DE calculation will be invalid. This would be signaled by a high DE for a transformation claimed to be semantics-preserving.
  - **High Anisotropy in Embeddings:** If embedding similarity analysis (Appendix E) shows high similarity for all vectors regardless of modality, the concept of "alignment" becomes meaningless, suggesting the observed clustering is an artifact of anisotropy rather than meaningful structure.

- **First 3 Experiments:**
  1. **Reproduce Total Effect of NL:** On the HumanEval+ dataset, for GPT-4 Turbo, replicate the TE of the NL modality. Measure the accuracy drop when the docstring (NL) is removed.
  2. **Reproduce Direct Effect of I/O Pairs:** On the mMBPP+ dataset, for WizardCoder, replicate the DE of I/O pairs. Apply the equality-to-inequalities transformation and measure the accuracy drop. Compare your result to the 12.20% drop reported in Table 2.
  3. **Check for Memorization:** On a small sample of HumanEval+ prompts, apply the Code_NL intervention (standardizing the function name). Inspect if the model (e.g., GPT-4T or LLaMa-3) still generates the original function name, which would indicate benchmark memorization as discussed in Section 4.2.

## Open Questions the Paper Calls Out

- **Question:** Can dead code insertion be systematically utilized to control or reduce hallucinations in code generation?
  - **Basis in paper:** [explicit] Section 4.3 notes that inserting dead code reduced class name hallucinations in Java experiments, but the authors leave detailed analysis to future work.
  - **Why unresolved:** The observation was incidental during Direct Effect interventions, and the paper did not conduct a dedicated study on hallucination metrics.
  - **What evidence would resolve it:** A controlled study measuring hallucination frequency across multiple models and languages with and without semantics-preserving dead code.

- **Question:** How can the CodeSCM framework be extended to account for confounding variables?
  - **Basis in paper:** [explicit] Section 3.1 and Section 7 explicitly state that the current formulation assumes no confounders and suggest future work should extend the model using the backdoor criterion.
  - **Why unresolved:** The current structural causal model simplifies calculations by omitting confounders, limiting the ability to fully isolate causal relationships.
  - **What evidence would resolve it:** An updated structural causal model that mathematically integrates confounders and validates causal effects against the baseline.

- **Question:** What is the causal Total Effect of the algorithmic code channel (Code AL) when it contains essential generation logic?
  - **Basis in paper:** [explicit] Section 4.2 highlights that the Code AL component in the evaluated datasets is limited to syntax (headers), leaving the effect of logic-heavy code unexplored.
  - **Why unresolved:** Existing benchmarks lack sufficient examples where Code AL contains implementation logic, preventing the isolation of its specific causal contribution.
  - **What evidence would resolve it:** Causal analysis on a custom dataset where Code AL provides partial logic, measuring performance changes when this logic is removed.

## Limitations
- The assumption that prompt modalities can be cleanly separated without confounding variables remains unverified and is acknowledged as a limitation
- Semantics-preserving transformations may not perfectly preserve mediator values across all prompt types, potentially confounding Direct Effect measurements
- The improved cosine similarity in code-specific models could be an artifact of increased anisotropy rather than meaningful semantic alignment

## Confidence
- **High Confidence:** The basic causal mediation framework (SCM + do-calculus) is methodologically sound and well-established. The identification of which prompt modalities contribute to code generation performance is supported by consistent patterns across multiple models and benchmarks.
- **Medium Confidence:** The quantification of Direct Effects through semantics-preserving transformations is theoretically valid, but the practical implementation may not perfectly preserve mediators in all cases. The memorization findings in Section 4.2 are observable but the causal interpretation requires careful scrutiny.
- **Low Confidence:** The embedding space alignment claims, particularly the assertion that code-specific pretraining improves multi-modal alignment in a way that meaningfully improves reasoning, require more rigorous validation to rule out artifacts like anisotropy.

## Next Checks
1. **Mediator Validation Experiment:** Select 20 diverse prompts and apply each semantics-preserving transformation. Use a probing task to empirically verify that the latent mediator variables (M_Code, M_NL) remain unchanged after transformation, confirming the theoretical assumption.
2. **Anisotropy Control Analysis:** Compare the embedding similarity distributions of CodeLLaMa and LLaMa-2 across random prompt pairs and matched prompt-solution pairs. If anisotropy drives the observed clustering, random pairs should show similar similarity patterns to the claimed alignment.
3. **Confounder Sensitivity Test:** Introduce a synthetic confounder (e.g., prompt length or specific keywords) into a subset of prompts and re-run the causal analysis. Verify that the estimated Total and Direct Effects remain stable, or document how they change to quantify the impact of unmeasured confounders.