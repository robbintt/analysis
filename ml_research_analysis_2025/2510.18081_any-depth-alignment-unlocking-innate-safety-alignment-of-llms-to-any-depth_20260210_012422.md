---
ver: rpa2
title: 'Any-Depth Alignment: Unlocking Innate Safety Alignment of LLMs to Any-Depth'
arxiv_id: '2510.18081'
source_url: https://arxiv.org/abs/2510.18081
tags:
- safety
- tokens
- alignment
- refusal
- base
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Any-Depth Alignment (ADA), a method to unlock
  safety alignment in LLMs at arbitrary generation depths. The key observation is
  that safety signals are concentrated in assistant header tokens, which retain strong
  alignment priors even deep into harmful continuations.
---

# Any-Depth Alignment: Unlocking Innate Safety Alignment of LLMs to Any-Depth

## Quick Facts
- **arXiv ID**: 2510.18081
- **Source URL**: https://arxiv.org/abs/2510.18081
- **Reference count**: 40
- **Primary result**: Achieves near-100% refusal rates against deep prefill attacks up to 2,500 tokens without modifying model weights

## Executive Summary
Any-Depth Alignment (ADA) unlocks safety alignment in large language models at arbitrary generation depths by exploiting the observation that safety signals are concentrated in assistant header tokens. These tokens retain strong alignment priors even deep into harmful continuations, allowing ADA to either re-inject them to trigger refusal or probe their hidden states with a linear classifier to halt generation. The method achieves exceptional robustness against deep prefill attacks while maintaining near-zero over-refusal on benign tasks, demonstrating that models' innate safety representations can be accessed and used for robust alignment at inference time.

## Method Summary
ADA leverages the observation that assistant header tokens (e.g., `<|start_header_id|>assistant<|end_header_id|>`) act as aggregators that concentrate distributed safety evidence from the preceding context into linearly separable representations. The method works by periodically inserting these tokens during generation to either trigger the model's refusal circuitry (ADA-Rethinking) or to probe the hidden states for harmful content using a pre-trained linear classifier (ADA-LinearProbe). This approach exploits the model's innate safety representations that persist throughout generation, even when the model has begun producing harmful content, achieving deep alignment without any weight modifications.

## Key Results
- Achieves near-100% refusal rates against deep prefill attacks up to 2,500 tokens
- Reduces adversarial prompt attack success to under 3%
- Maintains near-zero over-refusal on benign tasks (XSTest benchmark)
- Remains effective even after the base model is fine-tuned with adversarial data

## Why This Works (Mechanism)

### Mechanism 1: Safety-Token Signal Aggregation
The assistant header tokens act as aggregators that concentrate distributed safety evidence from the preceding context into a linearly separable representation. During shallow-refusal training, these header tokens appear repeatedly before refusal responses, creating a learned association where the model "conditions" on these tokens to access its safety judgment. When injected mid-stream, they reset the model's attention context, exposing latent harmfulness signals that were previously locked in hidden states. This works because the safety signal exists in the model's representations throughout generation but requires the correct token context to surface it.

### Mechanism 2: Depth-Invariant Linear Separability
Harmful vs. benign content becomes increasingly separable in Safety-Token hidden states as generation depth increases, reaching near-perfect classification (>99.5%) across model families. As harmful content unfolds, the model's internal "awareness" of harmfulness grows, but this awareness is not expressed in output tokens—it accumulates in contextual representations. The injected Safety Token serves as a probe point where this accumulated signal is cleanly revealed. This works because models possess an innate harm-detection capability that is suppressed during harmful generation but not erased from internal representations.

### Mechanism 3: Re-Triggering via Context Fork
Re-injecting the assistant header mid-generation triggers a "rethinking" behavior where the model reassesses and can refuse even after harmful content has begun. The injection creates a contextual discontinuity—forking the KV cache and appending the header—that reactivates the refusal circuitry associated with assistant turn initialization. The model treats the preceding harmful content as "given context" and generates a response that may now include refusal. This works because the model's refusal circuitry is stateless enough to be re-activated by token injection, rather than permanently committed to a harmful trajectory.

## Foundational Learning

- **Concept: KV Cache and Generation Forking**
  - **Why needed here**: ADA requires understanding how to fork the generation process (clone KV cache state) without re-computing the full context, enabling mid-stream safety checks with minimal overhead
  - **Quick check question**: Can you explain why forking a KV cache is more efficient than re-running the full forward pass for a 10,000-token context?

- **Concept: Linear Probing on Hidden States**
  - **Why needed here**: ADA (LP) trains a logistic regression classifier on specific hidden-state dimensions from Safety Tokens to detect harmfulness—understanding what features are extracted and from where is critical
  - **Quick check question**: If you extract hidden states from layer 15 of a 32-layer model after the input LayerNorm, what does that representation encode relative to earlier and later layers?

- **Concept: Shallow vs. Deep Alignment Failure Modes**
  - **Why needed here**: The paper's core insight is that standard alignment only teaches refusal at d=0 (turn start), leaving models vulnerable to mid-stream attacks—understanding this failure mode clarifies why ADA works
  - **Quick check question**: Why would training a model to refuse after "Sure, here's" not generalize to refusing after 500 tokens of harmful content?

## Architecture Onboarding

- **Component map**: Tokenizer/Template Layer -> Generation Loop -> Safety-Check Fork -> ADA (RK) Branch or ADA (LP) Classifier -> Decision Gate
- **Critical path**: Checkpoint interval → Fork KV cache → Inject Safety Token → Extract hidden state (LP) or generate lookahead (RK) → Classify → Halt or continue. Latency budget: ~25ms per check (Figure 10).
- **Design tradeoffs**:
  - Checkpoint frequency: Every 25 tokens (default) catches harm earlier but increases overhead; every 100 tokens reduces overhead but may emit more harmful tokens before cutoff
  - RK vs. LP: RK requires no pre-training and works on closed APIs (via message injection), but is slower and depends on base-model alignment quality; LP is faster and more consistent but requires hidden-state access and probe training
  - Probe layer selection: Middle layers (e.g., 15 for Llama-3.1-8B) show best accuracy; earlier layers are entangled, later layers may be task-specific
- **Failure signatures**:
  - Base model with weak alignment: ADA (RK) underperforms (e.g., DeepSeek shows ~70% refusal under deep prefills even with RK)
  - Hidden-state access unavailable: Only ADA (RK) applicable; LP not possible on closed APIs
  - Adversarial SFT erasure: Deep Alignment collapses rapidly (90%→10% refusal in 50 steps), but ADA (LP) remains robust (Figure 7)
  - Over-refusal on benign tasks: Monitor XSTest benchmark; ADA (LP) should maintain <1% false positive rate
- **First 3 experiments**:
  1. **Probe accuracy validation**: On your target model, extract Safety-Token hidden states at multiple depths from 100 benign and 100 harmful continuations; train logistic regression; confirm >95% validation accuracy (replicate Figure 4)
  2. **Checkpoint interval sweep**: Test refusal rate vs. latency tradeoff at intervals of 25, 50, 100, 200 tokens on AdvBench with 500-token prefills; plot refusal rate and per-response overhead
  3. **Over-refusal audit**: Run ADA (LP) on XSTest and standard benign benchmarks (GSM8K, MMLU); confirm false positive rate <2% before production deployment

## Open Questions the Paper Calls Out

- Can dedicated special tokens, trained explicitly as Safety Tokens, match or surpass the performance of the innate assistant header tokens used in ADA?
- Can Safety Token activations provide a cleaner or more effective reward signal for reinforcement learning (RL) than hidden states of generated content?
- Does inserting Safety Tokens immediately before action execution effectively gate harmful behaviors in tool-using LLM agents?

## Limitations
- Effectiveness depends on specific token-position assumption that assistant-header tokens consistently act as aggregators of safety evidence
- The "innate" safety alignment assumption remains largely theoretical and may erode with sufficient adversarial fine-tuning
- Method's dependence on specific hidden-state locations creates fragility if model architectures change

## Confidence
- **High Confidence**: Safety-Token hidden states show depth-invariant linear separability on tested models; ADA achieves near-100% refusal rates against deep prefill attacks; ADA maintains near-zero over-refusal on benign tasks
- **Medium Confidence**: Safety signals are "concentrated" in assistant header tokens through training; ADA effectiveness persists after base model fine-tuning, though with degradation; the innate alignment assumption holds across current model families but untested for future architectures
- **Low Confidence**: The safety signal is "innate" rather than emergent from training artifacts; mechanism generalizes to all future model architectures and training paradigms; no upper bound established for how much adversarial training can erode the signal before ADA fails completely

## Next Checks
1. **Architectural Generalization Test**: Apply ADA to a diverse set of model architectures including different attention mechanisms (e.g., RWKV, Mamba), varying layer counts, and models without explicit assistant headers. Validate whether Safety-Token aggregation and linear separability persist across these architectural variations.

2. **Adversarial Training Erosion Curve**: Systematically measure ADA effectiveness degradation under increasing amounts of safety-finetuning on harmful content. Determine the exact point where linear separability falls below practical thresholds and identify which aspects of training (data diversity, optimization method, etc.) most rapidly erode the innate signal.

3. **Real-world Deployment Audit**: Deploy ADA in a realistic multi-turn assistant scenario with interleaved benign and potentially harmful requests. Measure not just refusal rates but user experience impacts, including false positives during benign task switches and the method's behavior when harmful content appears mid-conversation after extended benign interaction.