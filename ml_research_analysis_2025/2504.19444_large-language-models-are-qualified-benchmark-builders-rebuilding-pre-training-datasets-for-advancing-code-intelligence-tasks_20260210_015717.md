---
ver: rpa2
title: 'Large Language Models are Qualified Benchmark Builders: Rebuilding Pre-Training
  Datasets for Advancing Code Intelligence Tasks'
arxiv_id: '2504.19444'
source_url: https://arxiv.org/abs/2504.19444
tags:
- code
- comments
- quality
- metrics
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper explores whether large language models (LLMs) can enhance
  code intelligence by generating high-quality comments to replace outdated human-written
  ones in pre-training datasets. The authors first evaluate comment quality using
  two reference-free metrics: code-comment inconsistency detection and semantic code
  search, which show that LLM-generated comments are more semantically consistent
  with code than human references.'
---

# Large Language Models are Qualified Benchmark Builders: Rebuilding Pre-Training Datasets for Advancing Code Intelligence Tasks

## Quick Facts
- arXiv ID: 2504.19444
- Source URL: https://arxiv.org/abs/2504.19444
- Reference count: 40
- Key result: GPT-3.5-turbo-generated code comments improve downstream code intelligence tasks compared to human-written references

## Executive Summary
This paper investigates whether large language models can generate higher-quality code comments than human annotators to enhance pre-training datasets for code intelligence. The authors propose using reference-free metrics (code-comment inconsistency detection and semantic code search) to evaluate comment quality, demonstrating that LLM-generated comments are more semantically consistent with code than human references. By rebuilding the CodeSearchNet dataset with GPT-3.5-turbo-generated comments and re-pretraining CodeT5, they show improved performance on downstream tasks like code summarization, generation, and translation, while tasks focused on code structure remain unaffected. This suggests LLM-enhanced data can advance code intelligence tasks that rely heavily on natural language understanding.

## Method Summary
The authors first evaluate comment quality using two reference-free metrics: code-comment inconsistency detection and semantic code search. These metrics measure semantic consistency between code and comments without requiring ground truth references. They then use GPT-3.5-turbo to regenerate comments for the CodeSearchNet dataset, creating an LLM-enhanced version. Finally, they re-pretrain CodeT5 on this enhanced dataset and evaluate on downstream code intelligence tasks, comparing performance against models trained on the original human-annotated data.

## Key Results
- LLM-generated comments show higher semantic consistency with code than human references according to reference-free metrics
- Retraining on LLM-enhanced data improves performance on code summarization, generation, and translation tasks
- No significant improvement observed on code refinement and clone detection tasks that focus on code structure
- Demonstrates that LLMs can build better benchmarks for language-heavy code intelligence tasks

## Why This Works (Mechanism)
The mechanism relies on LLMs' superior ability to generate semantically consistent natural language descriptions of code compared to human annotators. By using reference-free evaluation metrics, the authors avoid the circularity of comparing LLM-generated comments to human references. The improved semantic consistency in the pre-training data translates to better performance on downstream tasks that require understanding the relationship between code and natural language descriptions.

## Foundational Learning
- Code-comment inconsistency detection: Measures how well comments match their corresponding code semantics without requiring reference comments
  - Why needed: Allows evaluation of comment quality without human reference standards
  - Quick check: Can the model detect obvious mismatches between code behavior and comment description?

- Semantic code search: Evaluates whether comments accurately describe code functionality for search purposes
  - Why needed: Ensures comments capture meaningful semantic relationships rather than superficial patterns
  - Quick check: Can search queries using comments correctly retrieve semantically similar code snippets?

- Reference-free evaluation: Assessment methods that don't require ground truth comparisons
  - Why needed: Enables objective evaluation of LLM-generated content against itself
  - Quick check: Do results align across multiple independent reference-free metrics?

- Pre-training dataset enhancement: Process of improving training data quality through LLM regeneration
  - Why needed: Addresses quality issues in existing human-annotated datasets
  - Quick check: Does enhanced data lead to consistent improvements across multiple model architectures?

## Architecture Onboarding
Component map: CodeT5 (pre-trained model) <- Pre-training data (CodeSearchNet) <- LLM (GPT-3.5-turbo) <- Evaluation metrics

Critical path: LLM comment generation → Dataset rebuilding → Model re-pretraining → Downstream task evaluation

Design tradeoffs: Single LLM vs. multiple models, single dataset vs. diverse code domains, reference-free vs. reference-based evaluation

Failure signatures: No improvement on downstream tasks, degradation on code structure tasks, inconsistent metric results

First experiments:
1. Evaluate reference-free metrics on original human comments to establish baseline inconsistency levels
2. Generate LLM comments for a small validation set and compare semantic consistency scores
3. Re-pretrain on a subset of enhanced data and test on a single downstream task to verify methodology

## Open Questions the Paper Calls Out
None

## Limitations
- Only evaluated one LLM (GPT-3.5-turbo) and one pre-trained model (CodeT5), limiting generalizability
- Reference-free metrics cannot directly compare LLM-generated comments against the human comments they replace
- Benefits appear limited to language-heavy tasks, with no improvement on code structure-focused tasks
- Experiments conducted on CodeSearchNet dataset, which represents a specific domain of Python/Java functions

## Confidence
High that LLM-generated comments show better reference-free quality metrics than human comments
Medium that retraining with these comments improves downstream code intelligence tasks
Low that these findings generalize across different LLMs, model architectures, or broader code domains

## Next Checks
1. Replicate experiments using multiple LLM providers (e.g., Claude, Llama) and multiple code pre-training models (e.g., StarCoder, CodeLlama) to assess generalizability
2. Conduct human evaluation studies comparing LLM-generated comments directly against the human comments they replace, measuring both semantic consistency and practical utility
3. Test the approach on code from different domains (e.g., frontend JavaScript, data science notebooks, embedded systems) to determine if improvements transfer beyond the CodeSearchNet scope