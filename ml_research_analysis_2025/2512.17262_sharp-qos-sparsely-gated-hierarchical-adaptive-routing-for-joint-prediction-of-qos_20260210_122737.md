---
ver: rpa2
title: 'SHARP-QoS: Sparsely-gated Hierarchical Adaptive Routing for joint Prediction
  of QoS'
arxiv_id: '2512.17262'
source_url: https://arxiv.org/abs/2512.17262
tags:
- prediction
- features
- service
- feature
- joint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SHARP-QoS is a unified deep learning framework for joint QoS prediction
  across multiple parameters. It addresses sparsity, negative transfer, and cold-start
  issues by integrating hierarchical feature extraction via hyperbolic graph convolutions,
  adaptive feature sharing through subnetwork routing, and EMA-based loss balancing.
---

# SHARP-QoS: Sparsely-gated Hierarchical Adaptive Routing for joint Prediction of QoS

## Quick Facts
- arXiv ID: 2512.17262
- Source URL: https://arxiv.org/abs/2512.17262
- Authors: Suraj Kumar; Arvind Kumar; Soumi Chattopadhyay
- Reference count: 38
- Primary result: SHARP-QoS outperforms both single- and multi-task baselines, achieving 19.47% lower MAE and 19.32% lower RMSE on average

## Executive Summary
SHARP-QoS introduces a unified deep learning framework for joint QoS prediction across multiple parameters. The method addresses sparsity, negative transfer, and cold-start issues by integrating hierarchical feature extraction via hyperbolic graph convolutions, adaptive feature sharing through subnetwork routing, and EMA-based loss balancing. Evaluations on three datasets show significant performance gains over baselines while maintaining moderate computational overhead and strong scalability.

## Method Summary
SHARP-QoS is a unified deep learning framework that predicts multiple QoS parameters jointly using hierarchical feature extraction via hyperbolic graph convolutions, adaptive feature sharing through sparsely-gated subnetwork routing, and EMA-based loss balancing to mitigate negative transfer. The method leverages QoS and contextual graphs to capture hierarchical dependencies and shared patterns, dynamically fusing structural and shared representations. Training involves preprocessing with NMF and autoencoders, hyperbolic GCNs for context and QoS graphs, SNR and Cross-SNR routing with hard concrete distributions, gated fusion, and task-specific FFN prediction heads with EMA-weighted MAE loss.

## Key Results
- Achieves 19.47% lower MAE and 19.32% lower RMSE on average compared to single- and multi-task baselines
- HHGCN outperforms GCN by 7.89% (MAE) and 8.47% (RMSE) on WSDREAM-2T dataset
- EMA loss balancing outperforms HUW by 6.78% (MAE), DWA by 6.60% (MAE), and EqW by 6.62% (MAE) on WSDREAM-2T
- Successfully scales to datasets with 2, 3, and 4 QoS parameters with consistent performance gains

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Feature Extraction via Hyperbolic Graph Convolutions
- **Claim:** Hyperbolic convolutions in the Poincaré ball capture hierarchical dependencies in QoS and contextual graphs with lower distortion than Euclidean alternatives, improving representation quality for sparse, scale-free structures.
- **Mechanism:** By mapping features to the Poincaré ball (negative curvature), the model leverages Möbius operations (addition, matrix-vector multiplication) to perform graph convolution where exponential distance expansion better fits hierarchical user-service-region relationships. The dual mechanism separately extracts QoS invocation graphs (bipartite) and context graphs (region/AS), while hypergraphs capture second-hop user/service relationships to augment sparse signals.
- **Core assumption:** QoS invocation patterns and contextual attributes (region, autonomous system) form implicit hierarchical structures amenable to hyperbolic representation.
- **Evidence anchors:**
  - [abstract]: "dual mechanism to extract hierarchical features from both QoS and contextual structures via hyperbolic convolution formulated in the Poincaré ball"
  - [Section IV.B]: "Euclidean aggregation fails to capture the inherent hierarchical structure present in QoS and contextual data, leading to high distortion and suboptimal representations"
  - [Fig. 2 ablation]: HHGCN outperforms GCN by 7.89% (MAE) and 8.47% (RMSE) on TP; 3.00% (MAE) and 3.07% (RMSE) on RT
  - [corpus]: Weak direct corpus support for hyperbolic QoS specifically; related work (RAHN, QoSGNN) uses Euclidean deep architectures
- **Break condition:** If QoS graphs are not meaningfully hierarchical (e.g., flat, uniform degree distributions), hyperbolic benefit diminishes; if user/service hypergraphs add noise rather than signal, HHGCN may underperform simple HyGCN.

### Mechanism 2: Adaptive Feature Sharing via Subnetwork Routing (SNR) and Cross-SNR
- **Claim:** Sparsely-gated subnetwork routing enables task-specific feature sharing across QoS parameters and context, reducing negative transfer while allowing selective knowledge exchange.
- **Mechanism:** SNR applies K parallel transformation blocks to context features with learned Bernoulli-style routing variables (hard concrete distribution), producing sparse, task-specialized representations. Cross-SNR routes QoS features across tasks (excluding the target task) to capture cross-parameter dependencies. Gated fusion dynamically blends structural (HHGCN) and shared (SNR/Cross-SNR) representations per task.
- **Core assumption:** Different QoS parameters share informative latent patterns but require task-specific filtering; context (region, AS) provides complementary, privacy-preserving signals.
- **Evidence anchors:**
  - [abstract]: "adaptive feature sharing through subnetwork routing"
  - [Section IV.C]: "SNR router first process Y_ra through K_1 parallel blocks... cp_k drawn from Bernoulli distribution... hard concrete distribution... enables adaptive feature sharing for each QoS parameter"
  - [Table VII ablation]: Removing Cross-SNR drops accuracy 2.70% (RT) and 12.34% (TP); removing both HHGCNs+SNR causes largest degradation
  - [corpus]: MoE/routing concepts appear in corpus (Dynamic Expert Specialization, LLM Routing), but not specifically for QoS multi-task settings
- **Break condition:** If routing variables collapse (all blocks active or all inactive), adaptiveness is lost; if QoS parameters are truly independent (no shared structure), Cross-SNR adds noise and hurts performance.

### Mechanism 3: EMA-Based Loss Balancing for Negative Transfer Mitigation
- **Claim:** Exponential Moving Average smoothing of per-task losses stabilizes multi-task optimization by moderating contribution of numerically dominant QoS parameters, reducing oscillatory convergence.
- **Mechanism:** At each iteration, smoothed loss ˜L_p is computed via β-weighted EMA; task weights w_p are normalized inverses of smoothed losses. This suppresses short-term fluctuations from loss-scale disparities (e.g., throughput vs. response time ranges) while maintaining adaptive rebalancing.
- **Core assumption:** Task loss fluctuations are noisy indicators of true task difficulty; smoothing reveals underlying relative importance.
- **Evidence anchors:**
  - [abstract]: "EMA-based loss balancing strategy that allows stable joint optimization, thereby mitigating negative transfer"
  - [Section IV.D.1]: "combined loss is prone to be dominated by few QoS parameters that yield larger error magnitudes... EMA-based loss scaling moderates task contributions while suppressing short-term fluctuations"
  - [Fig. 3]: EMA outperforms HUW by 6.78% (MAE), DWA by 6.60% (MAE), and EqW by 6.62% (MAE) on WSDREAM-2T
  - [corpus]: No direct corpus evidence for EMA loss balancing in QoS; DWA and uncertainty weighting appear in prior baselines (WAMTL, PMT)
- **Break condition:** If smoothing coefficient β is too high, adaptation is too slow for rapid QoS dynamics; if too low, noise dominates and instability returns. If one task consistently has orders-of-magnitude larger loss, even smoothed inversion may not fully equalize.

## Foundational Learning

- **Concept: Poincaré Ball and Hyperbolic Geometry**
  - Why needed here: Core to HFEB; understanding Möbius operations, exponential/logarithmic maps, and curvature is necessary to implement or modify hyperbolic convolutions.
  - Quick check question: Given a point x in the Poincaré ball, how does the conformal factor λ_x affect distances compared to Euclidean space?

- **Concept: Multi-Task Learning and Negative Transfer**
  - Why needed here: SHARP-QoS is a joint QoS predictor; negative transfer from loss-scale imbalance is the central optimization challenge addressed by EMA balancing.
  - Quick check question: If response time (range ~0–20) and throughput (range ~0–1000) are trained jointly with equal weighting, which task gradient likely dominates early training?

- **Concept: Subnetwork Routing and Hard Concrete Distribution**
  - Why needed here: SNR and Cross-SNR implement adaptive, sparse routing; understanding hard concrete sampling and L_0 regularization is essential for debugging routing collapse.
  - Quick check question: How does the hard concrete distribution enable differentiable sparsity compared to standard Bernoulli sampling?

## Architecture Onboarding

- **Component map:**
  1. **Preprocessing:** NMF for QoS features, autoencoders for context (RG, AS) → feature matrices F_p, F_r, F_a; normalized adjacency matrices Â_p, Â_r, Â_a
  2. **HFEB:** HyGCN (context graphs G_r, G_a) → Y_r, Y_a; HHGCN (QoS graphs G_p, hypergraphs G_p^u, G_p^s) → Y_p per task
  3. **FSFB:** SNR on Y_ra (context) → Y_p^s; Cross-SNR on {Y_j} → Y_p^cs; Gated fusion combining Y_p and Y_p^scs → Z_p
  4. **JQPM:** Task-specific FFN_p → Z_p^u, Z_p^s → matrix product for prediction ˆQ_p; EMA-weighted MAE loss

- **Critical path:**
  - Sparsity in QoS matrices → HHGCN hypergraph augmentation is essential (ablation shows severe drop if removed)
  - Loss-scale imbalance → EMA balancing is critical for stability (Fig. 3)
  - Routing collapse → L_0 regularization (λ=1e-5) maintains sparse, meaningful block selection

- **Design tradeoffs:**
  - Hyperbolic vs. Euclidean: Hyperbolic improves hierarchical modeling but adds numerical complexity (exp/log maps, curvature tuning)
  - Separate vs. merged context graphs: Separate G_r and G_a outperforms single merged graph (Table IX) but doubles message-passing overhead
  - Training time vs. inference speed: Higher training time (5516s) but fastest inference (3.97e-8s) among baselines—acceptable for offline training, online prediction

- **Failure signatures:**
  - Routing collapse: All c_p^k → 1 or 0 (check L_0 loss magnitude; if near zero, adjust λ)
  - Negative transfer resurgence: One task loss decreasing while others increase after initial epochs (check EMA weights w_p; if one weight → 0, increase β or inspect loss scales)
  - Numerical instability in hyperbolic ops: NaNs in training (check curvature c stays positive, ensure ∥x∥_2 < 1/√c in Poincaré ball)

- **First 3 experiments:**
  1. **Ablate HHGCN vs. GCN vs. HyGCN** on a single dataset (e.g., WSDREAM-2T) to verify hierarchical benefit and establish baseline for your data characteristics.
  2. **Vary EMA β (0.9, 0.95, 0.99, 0.999)** and track per-task loss curves to identify optimal smoothing for your QoS scale disparities.
  3. **Analyze routing patterns** by logging c_p^k values during training; verify sparse activation (target ~25–50% blocks active) and check for task-specific specialization vs. uniform routing.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can SHARP-QoS be effectively adapted for deployment on resource-constrained devices without significant loss in prediction accuracy?
- **Basis in paper:** [explicit] The conclusion states that future work will explore "deployment on resource-constrained devices."
- **Why unresolved:** The current evaluation focuses on computational overhead (Params/FLOPs) in a standard server environment (Ubuntu, i7 CPU), utilizing 1.62M parameters which may be heavy for edge devices.
- **What evidence would resolve it:** Successful implementation and latency/accuracy benchmarking of the model on edge hardware (e.g., IoT gateways or mobile chips).

### Open Question 2
- **Question:** How can online measurement signals be integrated into the framework to support dynamic graph updates in real-time?
- **Basis in paper:** [explicit] The conclusion identifies the "integration of online measurement signals for dynamic graph updates" as a direction for future work.
- **Why unresolved:** The current methodology relies on static preprocessing (NMF, autoencoders) and fixed adjacency matrices derived from historical data, which cannot automatically adapt to streaming inputs.
- **What evidence would resolve it:** A modified architecture capable of incremental updates to the Poincaré ball embeddings and adjacency matrices without full retraining.

### Open Question 3
- **Question:** Does the scalability and positive transfer of SHARP-QoS persist when applied to a significantly higher number of QoS parameters (e.g., >4)?
- **Basis in paper:** [inferred] While the paper claims "strong scalability," experiments were limited to only three datasets with two, three, and four QoS parameters.
- **Why unresolved:** The Cross-SNR routing mechanism sums features from other tasks ($P-1$), which may suffer from noise aggregation or computational bottlenecks as the task count grows large.
- **What evidence would resolve it:** Performance evaluation (MAE/RMSE) on a synthetic or real-world dataset involving 10 or more simultaneous QoS parameters.

## Limitations

- Hyperbolic geometry benefits are inferred from ablation but not directly validated against Euclidean alternatives under identical conditions; no sensitivity analysis of curvature c parameter is reported
- SNR routing effectiveness relies on hard concrete distribution without reporting active block percentages or routing entropy; potential routing collapse cannot be excluded
- EMA smoothing parameters (β=0.99, λ=1e-5) are fixed without ablation or convergence analysis; performance could be sensitive to these hyperparameters

## Confidence

- **High confidence:** Hierarchical feature extraction improves performance over flat alternatives (verified via ablation HHGCN vs GCN); EMA balancing reduces negative transfer (verified via Fig. 3 comparisons)
- **Medium confidence:** Adaptive routing improves task specialization (Cross-SNR ablation shows performance drop, but routing statistics not reported); hyperbolic convolutions capture hierarchical structure (benefit demonstrated but geometry assumptions unverified)
- **Low confidence:** Claimed superiority over all baseline methods (limited to 5-6 baselines); cold-start performance improvements (not separately reported); privacy preservation claims (contextual routing vs raw data not quantified)

## Next Checks

1. Replicate ablation study removing HHGCN and SNR components on WSDREAM-2T to verify individual contribution margins match reported 7.89%/8.47% (HHGCN) and 2.70%/12.34% (Cross-SNR) improvements
2. Perform routing analysis during training: log routing variable distributions c_p^k across tasks to verify sparse activation (25-50% blocks active) and task-specific patterns
3. Conduct hyperparameter sensitivity analysis: vary EMA β (0.9-0.999) and λ (1e-4 to 1e-3) to establish robustness ranges and identify performance cliffs