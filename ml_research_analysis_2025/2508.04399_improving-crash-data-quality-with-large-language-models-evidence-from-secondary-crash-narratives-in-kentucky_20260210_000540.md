---
ver: rpa2
title: 'Improving Crash Data Quality with Large Language Models: Evidence from Secondary
  Crash Narratives in Kentucky'
arxiv_id: '2508.04399'
source_url: https://arxiv.org/abs/2508.04399
tags:
- crash
- data
- secondary
- crashes
- narratives
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compares advanced NLP models for mining crash narratives
  to improve secondary crash identification in Kentucky. Drawing from 16,656 manually
  reviewed narratives, we evaluated zero-shot LLMs (Llama3:70B, DeepSeek-R1:70B, Qwen3:32B,
  Gemma3:27B), fine-tuned transformers (BERT, DistilBERT, RoBERTa, XLNet, Longformer),
  and logistic regression.
---

# Improving Crash Data Quality with Large Language Models: Evidence from Secondary Crash Narratives in Kentucky

## Quick Facts
- **arXiv ID**: 2508.04399
- **Source URL**: https://arxiv.org/abs/2508.04399
- **Reference count**: 37
- **Primary result**: Fine-tuned transformers (especially RoBERTa) achieved highest accuracy (95%) and F1-score (0.90) for secondary crash identification from Kentucky crash narratives

## Executive Summary
This study evaluates advanced NLP models for mining crash narratives to improve secondary crash identification in Kentucky. Drawing from 16,656 manually reviewed narratives, the research compares zero-shot LLMs (Llama3:70B, DeepSeek-R1:70B, Qwen3:32B, Gemma3:27B), fine-tuned transformers (BERT, DistilBERT, RoBERTa, XLNet, Longformer), and logistic regression. The findings demonstrate that fine-tuned transformers achieved superior performance, with RoBERTa yielding the highest F1-score (0.90) and accuracy (95%), while zero-shot LLaMA3:70B reached a comparable F1 of 0.86 but required 139 minutes of inference time.

The study highlights important trade-offs between accuracy, efficiency, and data requirements in crash data enhancement. Fine-tuned models offered a favorable balance of precision (0.91) and recall (0.89), while LLMs showed higher recall (e.g., Gemma3:27B at 0.94) but incurred high computational costs (up to 723 minutes). The results emphasize that fine-tuned transformers represent the most practical option for deployment, with recommendations for local processing, ensemble approaches, and incremental workflows to enable scalable, privacy-preserving crash data enhancement.

## Method Summary
The study employed a comprehensive evaluation of multiple NLP approaches for secondary crash identification from Kentucky crash narratives. Researchers manually reviewed 16,656 crash narratives to create a ground truth dataset. They evaluated three categories of models: zero-shot large language models (Llama3:70B, DeepSeek-R1:70B, Qwen3:32B, Gemma3:27B), fine-tuned transformer models (BERT, DistilBERT, RoBERTa, XLNet, Longformer), and a logistic regression baseline. Performance metrics included F1-score, accuracy, precision, and recall, with computational efficiency measured through inference time. The fine-tuned transformer models were trained on the manually labeled dataset before evaluation.

## Key Results
- Fine-tuned RoBERTa achieved the highest performance with F1-score of 0.90 and accuracy of 95%
- Zero-shot LLaMA3:70B reached comparable F1 of 0.86 but required 139 minutes of inference time
- Logistic regression baseline lagged significantly behind with F1-score of 0.66
- Fine-tuned models balanced precision (0.91) and recall (0.89) effectively
- LLMs showed higher recall (e.g., Gemma3:27B at 0.94) but incurred high computational costs (up to 723 minutes)

## Why This Works (Mechanism)
The superior performance of fine-tuned transformers stems from their ability to learn domain-specific patterns from the manually labeled crash narratives. Unlike zero-shot LLMs that rely on general language understanding, fine-tuned models adapt to the specific linguistic patterns, terminology, and context present in crash reports. This adaptation allows them to better distinguish between primary and secondary crashes based on narrative content. The transformer architecture's attention mechanisms effectively capture long-range dependencies in crash descriptions, while the fine-tuning process optimizes the model parameters specifically for the secondary crash classification task. The computational efficiency advantage of fine-tuned models comes from their smaller parameter counts compared to large LLMs, reducing inference time while maintaining high accuracy.

## Foundational Learning
- **Crash narrative processing**: Essential for extracting meaningful features from unstructured text data in transportation safety analysis. Quick check: Validate preprocessing steps preserve critical temporal and causal information.
- **Transformer architecture**: Core mechanism for capturing contextual relationships in sequential data. Quick check: Verify attention weights highlight relevant narrative segments for secondary crash identification.
- **Fine-tuning methodology**: Critical process for adapting pre-trained models to domain-specific tasks with limited labeled data. Quick check: Monitor validation loss during fine-tuning to prevent overfitting.
- **Computational efficiency metrics**: Necessary for practical deployment considerations in resource-constrained environments. Quick check: Compare GPU vs CPU inference times across different model sizes.
- **Performance metrics selection**: F1-score, precision, and recall provide balanced evaluation for imbalanced classification tasks. Quick check: Calculate confusion matrices to understand error patterns.

## Architecture Onboarding

**Component Map**: Data preprocessing -> Model training/fine-tuning -> Inference pipeline -> Performance evaluation -> Deployment considerations

**Critical Path**: Manual narrative review -> Data preprocessing and feature extraction -> Model training and validation -> Performance benchmarking -> Computational efficiency analysis -> Deployment recommendation

**Design Tradeoffs**: Accuracy vs computational cost (LLMs offer higher recall but require 139-723 minutes vs fine-tuned models' faster inference), Model complexity vs interpretability (transformers are less interpretable than logistic regression), Local vs cloud processing (local deployment recommended for privacy), Single model vs ensemble approaches (ensembles may improve performance but increase complexity)

**Failure Signatures**: Poor performance on narratives with ambiguous language or missing context, High computational costs making real-time deployment impractical, Overfitting to Kentucky-specific reporting styles limiting generalizability, Bias in secondary crash identification across different road types or weather conditions

**First 3 Experiments**:
1. Cross-validation on Kentucky dataset to assess model stability and identify optimal hyperparameters
2. Inference time benchmarking across different hardware configurations to determine practical deployment requirements
3. Error analysis on misclassified narratives to identify systematic failure patterns and opportunities for model improvement

## Open Questions the Paper Calls Out
None

## Limitations
- Computational cost analysis focuses on inference time without accounting for fine-tuning expenses, potentially underestimating total resource requirements
- Exclusive focus on Kentucky crash data limits generalizability to other jurisdictions with different reporting practices and narrative styles
- Does not report on potential biases in secondary crash identification across different road types, weather conditions, or demographic factors

## Confidence
- **High confidence**: Performance claims for fine-tuned transformers given robust evaluation methodology and substantial dataset size
- **High confidence**: Computational efficiency comparisons between LLMs and fine-tuned models based on direct empirical measurement
- **Medium confidence**: Practical deployment recommendations based on current model capabilities without extensive real-world implementation testing

## Next Checks
1. Conduct cross-jurisdictional validation using crash narratives from multiple states to assess model generalizability
2. Implement ensemble approaches in production environments to quantify actual efficiency gains and identify optimal model combinations
3. Perform bias audits across different crash scenarios and demographic factors to ensure equitable secondary crash identification