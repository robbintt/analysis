---
ver: rpa2
title: 'Curate-Train-Refine: A Closed-Loop Agentic Framework for Zero Shot Classification'
arxiv_id: '2601.16530'
source_url: https://arxiv.org/abs/2601.16530
tags:
- training
- data
- classification
- few-shot
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of efficient text classification
  by proposing a closed-loop agentic framework that leverages LLMs for synthetic data
  generation and iterative refinement. The method, termed Curate-Train-Refine, employs
  an LLM to curate training data, analyze model performance, and synthesize targeted
  examples to address observed errors.
---

# Curate-Train-Refine: A Closed-Loop Agentic Framework for Zero Shot Classification

## Quick Facts
- arXiv ID: 2601.16530
- Source URL: https://arxiv.org/abs/2601.16530
- Authors: Gaurav Maheshwari; Kevin El Haddad
- Reference count: 8
- Primary result: LLM-driven iterative data curation consistently outperforms zero and few-shot baselines across four benchmarks.

## Executive Summary
This paper introduces Curate-Train-Refine, a closed-loop agentic framework that uses LLMs to iteratively curate synthetic training data for text classification. The LLM acts as a data curator, analyzing classifier performance and generating targeted examples to address observed errors. This process progressively refines data quality and adapts it to the downstream classifier. Experiments on SST-5, Emotion, CR, and AG News show the method outperforms standard zero and few-shot baselines, with the zero-shot variant even exceeding SetFit trained on 8 real examples per class on three of four datasets. The approach enables accurate and efficient classification without requiring large-model deployment at inference time.

## Method Summary
Curate-Train-Refine employs a ReAct-style LLM agent (via Hugging Face smolagents) to orchestrate an iterative loop of data generation, classifier evaluation, and targeted refinement. The agent generates synthetic training and validation sets from class labels, then invokes a lightweight SetFit classifier (110M parameters) to evaluate performance. Based on diagnostics (confusion matrices, error traces), the LLM synthesizes targeted examples addressing systematic errors. This cycle repeats until convergence (no improvement > ε for p iterations) or a maximum of T_max iterations. The final classifier is deployed without LLM dependency, shifting operational costs to training while maintaining low-latency inference.

## Key Results
- Zero-shot Curate-Train-Refine outperforms SetFit trained with 8 real examples per class on SST-5, Emotion, and CR datasets.
- Iterative refinement consistently improves performance over static data generation across all four benchmarks.
- The lightweight SetFit classifier (110M) outperforms the larger GLiClass 3.0 model (151M) on three datasets despite fewer parameters.
- The decoupled architecture enables LLM-driven data curation without requiring large models at inference time.

## Why This Works (Mechanism)

### Mechanism 1: Error-Driven Targeted Data Synthesis
The LLM receives classifier diagnostics (confusion matrices, error traces) and synthesizes examples targeting observed failure modes like negation handling and domain shift. This targeted generation addresses systematic confusions more effectively than static prompting. The core assumption is that the LLM can correctly diagnose why the classifier erred and generate semantically appropriate counterexamples. Break condition: If the LLM misdiagnoses error causes or generates examples reinforcing spurious correlations, performance may degrade or plateau early.

### Mechanism 2: Iterative Data Quality Refinement
Multi-cycle generation-evaluation progressively tailors synthetic data to the classifier's inductive biases, reducing distributional gaps. Each iteration adds classifier-informed examples, with convergence triggered when validation improvement falls below threshold ε for p consecutive iterations. The core assumption is that the generated validation set D_val approximates the true test distribution sufficiently to guide meaningful refinement. Break condition: If D_val is unrepresentative or contains systematic label noise, the loop optimizes for the wrong signal.

### Mechanism 3: Decoupled Training-Deployment Architecture
LLMs operate only during training as offline data curators, with the deployed model being a lightweight encoder (110M parameters) requiring no LLM dependency. This transfers LLM semantic knowledge into the smaller model's weights via synthetic supervision. The core assumption is that the synthetic distribution sufficiently covers the task's semantic space to transfer knowledge effectively. Break condition: If task semantics shift post-deployment, the frozen lightweight model cannot adapt without re-running the curation loop.

## Foundational Learning

- **Zero-shot vs. Few-shot Classification Paradigms**
  - Why needed here: The paper's core claim is zero-shot performance exceeding few-shot baselines; understanding the distinction is essential to interpret results.
  - Quick check question: Can you explain why SetFit with k=8 real labels is a meaningful baseline for a zero-shot method?

- **ReAct-Style Agent Architecture**
  - Why needed here: The framework instantiates a ReAct agent that alternates between planning and tool invocation; understanding this pattern clarifies the control flow.
  - Quick check question: What is the difference between a standard LLM prompt and a ReAct-style tool-calling loop?

- **Contrastive Learning for Sentence Embeddings (SetFit mechanism)**
  - Why needed here: SetFit's contrastive fine-tuning is identified as more effective than standard encoders in few-shot regimes; understanding why helps explain the backbone choice.
  - Quick check question: Why might contrastive fine-tuning outperform a general-purpose encoder when training data is limited?

## Architecture Onboarding

- **Component map:**
  - LLM Agent (GPT-5 via smolagents CodeAgent) -> Classifier Tool (SetFit with all-mpnet-base-v2, 110M params) -> Synthetic Training Set (D_train) and Validation Set (D_val)

- **Critical path:**
  1. Agent receives label names + instructions → generates D_val and initial D_train
  2. Agent calls classifier tool → receives metrics, confusion matrix, and error traces
  3. Agent analyzes errors → generates targeted batch D_t
  4. D_train ← D_train ∪ D_t → repeat from step 2
  5. Upon termination, final classifier is deployed (no LLM dependency)

- **Design tradeoffs:**
  - LLM cost vs. data quality: More iterations improve data but increase API costs; enforce per-class budget
  - SetFit vs. EuroBERT backbone: SetFit (contrastive) consistently outperforms EuroBERT in these experiments
  - Validation set reliability: D_val is synthetic; if misaligned with test distribution, convergence signal is noisy

- **Failure signatures:**
  - Early plateau with low accuracy → D_val may be unrepresentative; consider regenerating or adding diversity constraints
  - High per-class variance → check label balance and deduplication logic in generation prompts
  - Agentic loop produces repetitive examples → tighten semantic overlap constraints in generation step

- **First 3 experiments:**
  1. Ablation: Static prompt vs. closed-loop on SST-5; expect ~3-4% gap
  2. Backbone comparison: SetFit vs. EuroBERT on AG News; verify SetFit advantage
  3. Iteration budget sweep: Test T_max ∈ {1, 3, 5, 10} on Emotion dataset; expect plateau around 3-5 iterations

## Open Questions the Paper Calls Out

- **Structured prediction extension**: The authors aim to extend the framework to NER and relation extraction tasks, where label semantics are less distinct than in text classification.
- **LLM backbone sensitivity**: Future work includes studying quality trade-offs under different LLM backbones and generation budgets, as experiments rely exclusively on a single high-capacity model.
- **Richer diagnostic tools**: The paper proposes exploring calibration- and robustness-aware diagnostics to improve the agent's ability to synthesize effective training data.
- **Computational cost analysis**: While highlighting inference cost reduction, the paper does not analyze the training-phase computational expense of running multiple LLM generation and analysis cycles.

## Limitations
- The LLM's error diagnosis accuracy is not directly validated, creating uncertainty about whether generated examples address actual classifier failure modes.
- Synthetic validation sets' fidelity to true test distributions is assumed but not independently verified, potentially leading to unreliable convergence signals.
- Hyperparameter sensitivity (ε, p, Tmax) is not explored, making it unclear whether performance gains are robust or dependent on arbitrary choices.

## Confidence
- **High confidence**: The decoupled architecture (LLM curator + lightweight classifier) is clearly specified and technically sound.
- **Medium confidence**: The error-driven refinement mechanism is plausible but lacks direct validation of whether the LLM correctly diagnoses classifier errors.
- **Low confidence**: The claim that synthetic validation sets provide reliable convergence signals is not independently verified.

## Next Checks
1. **Hyperparameter sensitivity analysis**: Systematically vary ε, p, and Tmax across datasets to identify whether performance gains are robust or hyperparameter-dependent.
2. **Validation set fidelity test**: Generate multiple synthetic validation sets per dataset and measure correlation between synthetic validation performance and true test performance.
3. **Error diagnosis accuracy**: Manually audit a sample of LLM-generated examples to verify whether the error analysis correctly identifies classifier failure modes and whether generated examples address those specific issues.