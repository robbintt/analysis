---
ver: rpa2
title: Language Models as Ontology Encoders
arxiv_id: '2507.14334'
source_url: https://arxiv.org/abs/2507.14334
tags:
- ontology
- embeddings
- language
- which
- embedding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the limitations of existing ontology embedding
  methods by proposing OnT, which combines pretrained language models with hyperbolic
  geometric modeling to preserve both textual information and logical structure in
  OWL ontologies. OnT achieves state-of-the-art performance across prediction and
  inference tasks on real-world ontologies (GALEN, GO, Anatomy), with significant
  improvements in Mean Rank metrics - up to 7x better than existing methods.
---

# Language Models as Ontology Encoders

## Quick Facts
- arXiv ID: 2507.14334
- Source URL: https://arxiv.org/abs/2507.14334
- Reference count: 40
- Outperforms state-of-the-art methods on ontology embedding tasks with up to 7x improvement in Mean Rank

## Executive Summary
The paper addresses the limitations of existing ontology embedding methods by proposing OnT, which combines pretrained language models with hyperbolic geometric modeling to preserve both textual information and logical structure in OWL ontologies. OnT achieves state-of-the-art performance across prediction and inference tasks on real-world ontologies (GALEN, GO, Anatomy), with significant improvements in Mean Rank metrics. The approach uses verbalization to convert complex concepts into text, BERT-based embeddings, and hyperbolic space representations with specialized loss functions to capture logical relationships.

## Method Summary
OnT embeds OWL EL-ontologies by first normalizing axioms into four forms (NF1-NF4), then verbalizing complex concepts into natural language descriptions. A pretrained language model (default: all-MiniLM-L12-v2) encodes these descriptions into d-dimensional vectors, which are projected into Poincaré ball hyperbolic space. The method trains using three main loss functions: hierarchical contrastive loss for subsumption relationships, centripetal loss for preserving concept hierarchies, and role loss for modeling existential restrictions. Role embeddings are implemented as rotations and scaling operations in hyperbolic space. The model is trained for one epoch with 1 negative sample per axiom, using learning rate 10^-5 and margins α=3.0, β=0.5.

## Key Results
- Outperforms existing methods by up to 7x in Mean Rank on GALEN and GO ontologies
- Achieves significant improvements in Hits@100 (up to 11x better) for prediction tasks
- Successfully identifies missing and incorrect axioms in SNOMED CT through case study analysis
- Ablation studies show role embeddings and logical constraints consistently improve performance

## Why This Works (Mechanism)

### Mechanism 1: Hyperbolic Geometry Preserves Hierarchical Subsumption
Embedding concepts as points in Poincaré ball space naturally encodes subsumption (C ⊑ D) as distance-based partial ordering, enabling geometric reasoning over taxonomies. Subsumption axioms C ⊑ D are interpreted as partial-order relationships x_C ≺ x_D. The hyperbolic distance function (Eq. 2) and centripetal loss push parent concepts closer to the origin than children, preserving transitivity.

### Mechanism 2: Verbalization Bridges Formal Logic to Language Models
Converting EL-concepts to natural language descriptions enables PLMs to contribute semantic understanding that purely geometric methods lack. Complex concepts are recursively verbalized: V(C ⊓ D) = "V(C) and V(D)", V(∃r.C) = "something that V(r) some V(C)". The PLM encodes these sentences, grounding logical structure in learned language semantics.

### Mechanism 3: Role Embeddings as Rotations Capture Existential Quantification Patterns
Representing roles as rotation + scaling operations (f_r(v) = k_r ⊙ R(Θ_r) · v) preserves deductive patterns like A ⊑ B ⇒ ∃r.A ⊑ ∃r.B. Rotations preserve hyperbolic distances, so score(fr(x_C) ≺ fr(x_D)) = score(x_C ≺ x_D) when k_r = 1. The role loss aligns verbalized ∃r.D embeddings with transformed f_r(x_D).

## Foundational Learning

- **Concept: Description Logic EL**
  - Why needed here: OnT specifically targets EL-ontologies with normalized axioms (NF1-NF4). Understanding C ⊓ D (conjunction), ∃r.C (existential restriction), and subsumption is essential to interpret what the embeddings preserve.
  - Quick check question: Can you explain why the axiom "Person ⊓ ∃teach.Class ⊑ Teacher" requires normalization into three separate axioms?

- **Concept: Poincaré Ball Model of Hyperbolic Space**
  - Why needed here: All concept embeddings live in the Poincaré ball; distance computations and the centripetal loss depend on hyperbolic geometry properties.
  - Quick check question: Why does the centripetal loss (‖x_D‖_κ − ‖x_C‖_κ) enforce hierarchy by pushing parent D closer to the origin than child C?

- **Concept: Contrastive Learning with Negative Sampling**
  - Why needed here: The hierarchical contrastive loss L_contrast uses negative samples D_neg to push unrelated concepts apart.
  - Quick check question: What happens to embedding quality if negative samples are too similar to positive pairs (e.g., sampling siblings)?

## Architecture Onboarding

- **Component map:**
  Input Layer (normalized EL axioms + textual labels) -> Verbalization Module (recursive text generation) -> PLM Encoder (BERT-based) -> Hyperbolic Projection (Poincaré ball mapping) -> Role Transformation (rotation + scaling) -> Loss Computation (hierarchical, centripetal, role, conjunction)

- **Critical path:**
  1. Normalize ontology → NF1-NF4 axioms
  2. Verbalize all concepts → text sentences
  3. PLM encode → initial embeddings
  4. Train in hyperbolic space with all three losses
  5. Score new axioms via Eq. 8: s(C ⊑ D) = −(d_κ(x_C, x_D) + λ(‖x_D‖_κ − ‖x_C‖_κ))

- **Design tradeoffs:**
  - Dimension d=200: Higher dimensions improve expressiveness but increase training cost
  - λ weight selection: Determines balance between distance and centripetal terms; validation-set tuned
  - With/without role embeddings: OnT(w/o r) is simpler but loses ∃r.C pattern preservation

- **Failure signatures:**
  - High MR but decent H@k: Model finds correct answers but also many false positives; check negative sampling quality
  - Poor transfer performance: PLM may lack domain knowledge; consider domain-adaptive pre-training
  - Inference task failure: NF2-NF4 axioms may be under-represented in training data

- **First 3 experiments:**
  1. Ablation on verbalization quality: Manually inspect generated text for complex concepts; test with alternative templates
  2. Role embedding impact: Compare OnT vs OnT(w/o r) on datasets with high NF3/NF4 axiom ratios
  3. Hyperparameter sensitivity: Vary λ ∈ {0, 0.1, ..., 1} and margins α, β; plot validation MR

## Open Questions the Paper Calls Out

### Open Question 1
Can the OnT framework be extended to support more expressive Description Logics like ALC, specifically by incorporating the negation logical operator?
- Basis: The authors state in the Conclusion they are keen to extend methods to ALC with negation operator ¬.
- Why unresolved: Current method is restricted to EL profile which excludes logical negation.
- What evidence would resolve it: Modified OnT model successfully embedding ALC axioms (e.g., C ⊑ ¬D) with performance comparable to current results.

### Open Question 2
How can role embeddings be further utilized to capture and reason over role inclusion axioms (e.g., r ⊑ s)?
- Basis: Authors propose to delve deeper into logical patterns of roles using role embeddings, including role inclusion axioms.
- Why unresolved: OnT models roles as transformations but lacks specific geometric constraint or loss function for role hierarchies.
- What evidence would resolve it: New loss component over role parameters yielding high accuracy on role subsumption prediction tasks.

### Open Question 3
To what extent does the quality and strategy of verbalizing complex concepts impact the semantic performance and logical preservation of embeddings?
- Basis: Authors note it would be interesting to analyze impact of verbalization quality.
- Why unresolved: OnT uses fixed templates but sensitivity of PLM to specific phrasings versus natural definitions remains unquantified.
- What evidence would resolve it: Ablation studies comparing template-based verbalization versus human-authored or LLM-generated definitions.

## Limitations

- Restricted to EL expressivity - cannot handle general OWL 2 axioms like inverse roles or cardinality restrictions
- Dependency on quality verbalization - poor ontology labels could severely degrade PLM contributions
- Custom normalization requirement - standard normalizers produce inconsistent axioms, creating reproducibility barrier

## Confidence

- **High confidence**: Core technical framework (hyperbolic geometry, verbalization, role rotations) is well-defined and mathematically grounded
- **Medium confidence**: Reported performance improvements depend heavily on custom normalization code that isn't publicly available
- **Low confidence**: Claims about universal applicability - method specifically designed for EL-ontologies and may not generalize to full OWL 2 DL reasoning

## Next Checks

1. **Normalization Reproducibility Test**: Implement the normalization pipeline using a standard EL reasoner (e.g., ELK) and compare the number/consistency of generated axioms against the paper's results.

2. **Verbalization Quality Audit**: Manually inspect 50 randomly sampled complex concept verbalizations across all three ontologies. Rate clarity, semantic fidelity, and potential ambiguity.

3. **Transfer Learning Robustness**: Train OnT on GALEN, then evaluate on SNOMED CT (or vice versa) without fine-tuning. Compare against training on each domain separately.