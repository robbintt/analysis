---
ver: rpa2
title: 'VSSFlow: Unifying Video-conditioned Sound and Speech Generation via Joint
  Learning'
arxiv_id: '2509.24773'
source_url: https://arxiv.org/abs/2509.24773
tags:
- speech
- sound
- generation
- video
- visualtts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: VSSFlow unifies video-to-sound (V2S) and visual text-to-speech
  (VisualTTS) generation within a single flow-matching framework. The method introduces
  a condition aggregation mechanism that leverages cross-attention for video conditions
  and self-attention for speech transcripts, capitalizing on their distinct inductive
  biases.
---

# VSSFlow: Unifying Video-conditioned Sound and Speech Generation via Joint Learning

## Quick Facts
- arXiv ID: 2509.24773
- Source URL: https://arxiv.org/abs/2509.24773
- Reference count: 36
- Primary result: Unified V2S/VisualTTS model achieves FAD 1.34 on VGGSound and WER 15.1/18.2 on Chem/GRID

## Executive Summary
VSSFlow introduces a unified framework for video-conditioned sound and speech generation using flow-matching. The method leverages a condition aggregation mechanism that applies cross-attention for video inputs and self-attention for phoneme transcripts, capitalizing on their distinct inductive biases. Joint training on both modalities—without complex multi-stage strategies—improves both tasks by learning a shared audio prior that accelerates convergence, enhances conditional generation, and stabilizes classifier-free guidance. The approach demonstrates that environmental sound and speech generation can mutually benefit from unified modeling, achieving state-of-the-art results on both VGGSound and VisualTTS benchmarks.

## Method Summary
VSSFlow uses a 10-layer cross-attention diffusion transformer (443M parameters) with 1D RoPE for temporal modeling. The model conditions on CLIP video features (768-dim, 10 FPS, interpolated to 250 frames) via cross-attention and phoneme embeddings (32-dim) via self-attention. A learned duration predictor aligns phonemes to the 250-frame audio representation using AV-HuBERT supervision. The AudioLDM2 VAE generates 250×64 latent audio representations decoded by HiFi-GAN. Training uses flow-matching loss with classifier-free guidance (CFG 3.0 for sound, 1.5 for speech) on multi-task batches, with 200 epochs on 4×H100 GPUs (batch 36/GPU, lr 4e-7).

## Key