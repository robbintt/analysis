---
ver: rpa2
title: 'SLA-MORL: SLA-Aware Multi-Objective Reinforcement Learning for HPC Resource
  Optimization'
arxiv_id: '2508.03509'
source_url: https://arxiv.org/abs/2508.03509
tags:
- resource
- learning
- sla-morl
- optimization
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SLA-MORL is a multi-objective reinforcement learning framework
  for dynamic resource allocation in HPC environments that addresses cold-start inefficiency
  and static multi-objective optimization limitations. The framework uses intelligent
  initialization through historical learning or baseline runs and dynamic weight adaptation
  based on real-time SLA violation severity.
---

# SLA-MORL: SLA-Aware Multi-Objective Reinforcement Learning for HPC Resource Optimization

## Quick Facts
- arXiv ID: 2508.03509
- Source URL: https://arxiv.org/abs/2508.03509
- Reference count: 30
- 67.2% reduction in training time, 68.8% reduction in operational costs, and 73.4% improvement in SLA compliance compared to static baselines

## Executive Summary
SLA-MORL is a multi-objective reinforcement learning framework for dynamic resource allocation in HPC environments that addresses cold-start inefficiency and static multi-objective optimization limitations. The framework uses intelligent initialization through historical learning or baseline runs and dynamic weight adaptation based on real-time SLA violation severity. Key results show 67.2% reduction in training time, 68.8% reduction in operational costs, and 73.4% improvement in SLA compliance compared to static baselines across 13 diverse ML workloads on production HPC infrastructure.

## Method Summary
SLA-MORL implements an Actor-Critic architecture for multi-objective resource optimization, using a 9-action discrete space (GPU/CPU: ±1, 0) and a 21-dimensional state vector tracking resources, utilization, progress, and SLA metrics. The framework employs intelligent initialization through historical learning or baseline runs to eliminate cold-start problems, and dynamically adjusts optimization weights based on real-time SLA violation severity. Training uses Adam optimizer with separate learning rates for Actor and Critic networks, with change detection triggering policy updates only when significant state changes occur or SLA violations are detected.

## Key Results
- 67.2% reduction in training time compared to static baselines
- 68.8% reduction in operational costs across 13 ML workloads
- 73.4% improvement in SLA compliance through adaptive weight recalibration

## Why This Works (Mechanism)

### Mechanism 1: Intelligent Initialization via Historical Learning
Pre-training from historical patterns or lightweight baseline runs reduces cold-start exploration overhead by ~60%. The system either loads historical CSV logs to pre-train Actor-Critic networks, OR runs baseline experiments on 3 configurations using 20% data for 10% epochs, scaling estimates via θ̂(g,c) = 5·θ_baseline. This works because workload characteristics from similar past runs or small sample runs transfer to full workload optimization.

### Mechanism 2: Adaptive Weight Recalibration Based on SLA Violation Severity
Dynamic adjustment of multi-objective weights based on violation severity enables self-correction during training. Base weights (e.g., [0.6, 0.1, 0.3] for time priority) are modified via w_adapted = w_base + α·v·I_violated, then normalized; violation severity v_t ∈ [0,1] redistributes priority toward violated objectives with α=0.5 strength. This works because current violation severity reliably predicts near-future optimization priorities.

### Mechanism 3: State-Conditioned Change Detection for Efficient Policy Updates
Triggering policy updates only on significant state changes (δ_t > 0.1) or SLA violations reduces unnecessary exploration while maintaining responsiveness. The change detector computes L2 norm between consecutive states; updates skip if change < 10% AND no SLA violation detected. This works because small state perturbations indicate stable regions where current policy remains adequate.

## Foundational Learning

- **Actor-Critic Architecture**: Understanding baseline subtraction and advantage estimation is essential for SLA-MORL's policy and value networks. Quick check: Can you explain why Actor-Critic methods reduce variance compared to REINFORCE, and why the Critic needs separate learning rate (η_Q = 1×10⁻³ vs. η_π = 3×10⁻⁴)?

- **Multi-Objective Scalarization**: The framework scalarizes time, cost, and utilization rewards via weighted sum; limitations of linear scalarization affect Pareto optimality. Quick check: What happens to linear scalarization when the true Pareto front is non-convex, and how might this affect SLA-MORL's balanced mode?

- **Cold-Start Problem in RL**: Understanding why random exploration is costly in production HPC (not simulation) motivates the historical learning component's 60% overhead reduction. Quick check: Why does sample inefficiency matter more when each episode costs real GPU-hours versus simulated environments?

## Architecture Onboarding

- **Component map**: State Vector Builder (21-dim) -> Change Detector -> Actor Network (21→128→64→9) -> Resource Executor (SLURM + CUDA) -> Monitoring -> Critic Network (30→128→64→1) -> Adaptive Reward Calculator -> Resource Executor

- **Critical path**: User provides model-data pair + preference (time/cost/balanced) → Offline phase: load historical patterns OR run 3 baseline configs (10% epochs, 20% data) → Online loop: detect change → adapt weights → Actor selects action → execute → compute reward → update networks → Generate Pareto front from execution history → Select (g*, c*) based on user preference

- **Design tradeoffs**: 9 discrete actions (GPU±1, CPU±1) vs. continuous trades granularity for stability; τ=0.1 threshold balances responsiveness vs. oscillation risk; 10% epoch baseline overhead upfront cost reduces total exploration but may not capture late-stage dynamics

- **Failure signatures**: SLA compliance <40% indicates weight adaptation not triggering; resource oscillation (constant GPU±1) suggests reward scaling unstable or τ too low; no improvement vs. static suggests historical patterns may be incompatible

- **First 3 experiments**: 1) Single-workload validation: Run SLA-MORL on ResNet50/CIFAR-10 with time priority; verify 30%+ time reduction vs. static baseline 2) Initialization ablation: Compare full SLA-MORL vs. "lite" (no offline phase); expect ~15% performance gap 3) SLA stress test: Set aggressive deadline (50% of expected time); verify weight adaptation triggers and compliance improves across episodes

## Open Questions the Paper Calls Out

### Open Question 1
How does SLA-MORL performance and convergence stability scale when deployed on larger GPU clusters or multi-region cloud architectures compared to the single-cluster HPC environment tested? The current evaluation is limited to a specific SLURM-based HPC cluster; distributed coordination overhead and state complexity in larger systems remain untested.

### Open Question 2
Can the reward function and cost model be extended to handle dynamic pricing structures, such as AWS Spot Instances or Google Preemptible VMs, without destabilizing the learning policy? The current model assumes a static hourly rate ($C_{hourly} = g \cdot 5.0 + c \cdot 0.5$), which fails to capture the volatility of spot markets.

### Open Question 3
To what extent do the fixed scaling efficiency functions ($\sigma(g)$ and $\rho(c)$) limit generalization to workloads with non-standard scaling behaviors, such as those bounded by communication latency? The current time model assumes specific hardware scaling characteristics that may differ significantly across diverse parallelization strategies.

## Limitations

- Current deployment tested only on small-scale HPC cluster; scaling to larger GPU clusters remains unverified
- Linear cost model may not generalize to dynamic pricing structures like spot instances or heterogeneous resource types
- Fixed scaling efficiency functions may not capture non-standard workload behaviors like communication-bound tasks

## Confidence

- **High**: Actor-Critic architecture, discrete action space design, and overall optimization framework are well-specified and reproducible
- **Medium**: 67.2% training time reduction and 68.8% cost reduction results are supported by ablation studies, though exact reward formulations affect magnitude
- **Low**: 73.4% SLA compliance improvement depends heavily on unspecified violation detection and severity calculation mechanisms

## Next Checks

1. Implement the full reward function (R_time, R_cost, R_util, P_sla) and validate against the combined form R_t = w_time·R_time + w_cost·R_cost + w_util·R_util - P_sla
2. Test initialization transfer: compare historical learning vs. baseline runs across workloads with different convergence characteristics
3. Validate SLA violation detection: instrument the system to log violation triggers and severity calculations, verifying they align with claimed weight adaptation behavior