---
ver: rpa2
title: 'Exploitation Over Exploration: Unmasking the Bias in Linear Bandit Recommender
  Offline Evaluation'
arxiv_id: '2507.18756'
source_url: https://arxiv.org/abs/2507.18756
tags:
- exploration
- evaluation
- offline
- linear
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the limitations of offline evaluation in linear
  contextual bandit recommender systems, specifically its bias against exploration
  strategies. Through extensive empirical comparisons, the authors demonstrate that
  a purely greedy linear model consistently achieves top-tier performance, often outperforming
  or matching its exploratory counterparts across over 90% of evaluated datasets.
---

# Exploitation Over Exploration: Unmasking the Bias in Linear Bandit Recommender Offline Evaluation

## Quick Facts
- arXiv ID: 2507.18756
- Source URL: https://arxiv.org/abs/2507.18756
- Reference count: 40
- Primary result: Offline evaluation systematically favors exploitation over exploration in linear contextual bandits

## Executive Summary
This paper reveals a critical flaw in how recommender systems using linear contextual bandit algorithms are evaluated offline. Through extensive empirical testing across multiple datasets, the authors demonstrate that standard offline evaluation protocols systematically favor greedy (exploitation-only) strategies over those incorporating exploration. The findings show that purely greedy linear models consistently achieve top-tier performance in offline settings, often outperforming or matching their exploratory counterparts in over 90% of evaluated datasets. This bias occurs because offline evaluation methods cannot adequately capture the long-term benefits of exploration that would manifest in real-world deployment.

The implications are significant for the recommender systems community, as current offline evaluation practices may be misleading researchers and practitioners about the true value of exploration strategies. The paper advocates for developing more sophisticated evaluation methodologies, including simulation-based approaches and alternative frameworks that can better capture the benefits of exploration in interactive learning environments.

## Method Summary
The authors conducted extensive empirical comparisons between greedy and exploratory linear contextual bandit algorithms across multiple datasets and evaluation protocols. They implemented various bandit algorithms including epsilon-greedy, UCB, Thompson sampling, and purely greedy approaches, then evaluated them using standard offline evaluation techniques such as importance weighting and direct method estimators. The study systematically varied hyperparameters related to exploration and measured performance using standard metrics like NDCG and novelty. Hyperparameter optimization was performed to identify which configurations were consistently selected by offline evaluation, revealing a systematic preference for minimizing exploration. The experiments were designed to test whether offline evaluation could reliably distinguish between exploratory and non-exploratory strategies.

## Key Results
- Greedy linear models achieved top-tier performance in over 90% of evaluated datasets
- Offline evaluation protocols systematically selected hyperparameter configurations that minimized exploration
- Standard off-policy evaluation estimators (IPW, DR) failed to correct the exploitation bias
- Accuracy and novelty metrics often showed inverse relationships in offline settings
- Current evaluation methods cannot reliably assess the true efficacy of exploration strategies

## Why This Works (Mechanism)
The fundamental issue lies in how offline evaluation methods handle the exploration-exploitation tradeoff. Offline evaluation relies on historical data generated by past policies, which inherently contains less information about exploratory actions since those actions were taken less frequently. When evaluation metrics are computed on this biased dataset, they naturally favor policies that resemble the historical data distribution - namely, greedy policies that minimize exploration. The importance weighting techniques used in off-policy evaluation attempt to correct for this but are insufficient when the evaluation data lacks sufficient coverage of exploratory states and actions.

## Foundational Learning
- **Linear Contextual Bandits**: Framework where recommendations are made based on user and item features in a linear model. Why needed: Forms the core algorithmic approach being evaluated. Quick check: Can the model be expressed as y = w^T x + Îµ?
- **Exploration-Exploitation Tradeoff**: Balancing between trying new recommendations (exploration) and using known good recommendations (exploitation). Why needed: Central concept being studied. Quick check: Does the algorithm have parameters controlling exploration probability?
- **Offline Evaluation**: Assessing algorithm performance using historical data without live interaction. Why needed: Primary evaluation methodology being critiqued. Quick check: Is the evaluation based on logged bandit feedback rather than live experiments?
- **Off-Policy Evaluation (OPE)**: Techniques for evaluating policies using data from different policies. Why needed: Methods used to try correcting evaluation bias. Quick check: Does the method use importance weighting or direct estimation?
- **Hyperparameter Optimization**: Process of finding optimal algorithm settings. Why needed: Reveals systematic bias in evaluation preferences. Quick check: Are exploration parameters consistently minimized across datasets?
- **Non-stationarity**: When reward distributions change over time. Why needed: Real-world factor not captured in current study. Quick check: Does the environment assume stationary rewards?

## Architecture Onboarding

Component Map:
Data Generation -> Bandit Algorithm -> Recommendation -> User Feedback -> Evaluation

Critical Path:
1. Historical interaction data is logged with features and rewards
2. Bandit algorithms are trained on this data
3. Different algorithms generate recommendations
4. Simulated or logged user feedback is collected
5. Offline evaluation metrics are computed

Design Tradeoffs:
- Exploration vs exploitation: More exploration provides better long-term learning but worse short-term performance
- Model complexity vs interpretability: Linear models are simpler but may miss non-linear patterns
- Evaluation fidelity vs computational cost: More sophisticated OPE methods are more accurate but computationally expensive

Failure Signatures:
- Offline metrics significantly diverging from online A/B test results
- Hyperparameter optimization consistently selecting minimal exploration settings
- Inverse relationship between accuracy and novelty metrics
- Greedy policies dominating in offline but not online evaluations

First Experiments:
1. Run epsilon-greedy with varying epsilon values and compare offline vs online performance
2. Implement Thompson sampling and evaluate using both IPW and direct method estimators
3. Test UCB algorithm with different confidence bounds and analyze hyperparameter preferences

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can off-policy evaluation (OPE) estimators be specifically tailored to robustly assess the value of exploration in non-stationary recommender environments?
- Basis in paper: [explicit] The authors conclude by calling for the advancement of OPE methods tailored for complex environments (Section 8), noting that current estimators failed to correct the exploitation bias (Section 6.4).
- Why unresolved: Standard OPE techniques (IPW, DR) tested in the paper still favored greedy policies, indicating they cannot yet capture the long-term benefits of exploratory actions.
- What evidence would resolve it: The development of an OPE estimator whose offline policy rankings align with online A/B test results for exploratory algorithms.

### Open Question 2
- Question: How can high-fidelity simulation environments be designed to accurately measure the long-term rewards of exploration that offline evaluation obscures?
- Basis in paper: [explicit] The paper advocates for "next-generation simulators" with richer dynamics (e.g., non-stationarity) to test exploration meaningfully (Section 8).
- Why unresolved: Current simulators are often domain-specific and struggle to model complex user behavior and dynamic feedback loops.
- What evidence would resolve it: A simulation environment where exploratory bandit policies consistently demonstrate superior long-term performance (e.g., retention) compared to greedy baselines.

### Open Question 3
- Question: Does the systematic bias favoring exploitation persist in non-linear or deep neural network-based bandit architectures?
- Basis in paper: [inferred] The study is restricted to linear contextual bandits (Section 3); the authors' conclusion that offline evaluation is flawed invites checking if this finding generalizes to other model classes.
- Why unresolved: Deep bandits model uncertainty differently than linear models, and it is unknown if offline replay penalizes their exploration mechanisms as severely.
- What evidence would resolve it: Replicating the study's experimental protocol using Deep Bayesian Bandits or Neural Linear models to observe if greedy policies still dominate.

### Open Question 4
- Question: What standardized benchmark protocols can effectively assess exploration beyond short-term accuracy metrics like NDCG?
- Basis in paper: [explicit] The authors state the need to "establish protocols that assess exploration beyond short-term accuracy" (Section 8).
- Why unresolved: The study shows accuracy and novelty often have an inverse relationship in offline settings, and current metrics fail to capture the holistic value of exploration.
- What evidence would resolve it: The adoption of a benchmark metric that correlates with long-term user satisfaction (e.g., lifetime value) even when short-term accuracy is sacrificed for exploration.

## Limitations
- Study focuses exclusively on linear contextual bandit models, limiting generalizability to non-linear approaches
- Evaluation uses synthetic and controlled environments rather than real-world production systems
- Assumes stationary reward distributions, not accounting for non-stationary environments
- Limited to specific evaluation metrics that may not capture all aspects of recommendation quality

## Confidence
- Claim that offline evaluation systematically favors exploitation over exploration: High confidence
- Claim that greedy models consistently outperform exploratory approaches: Medium confidence (context-dependent)
- Claim that current offline evaluation methods are inadequate for assessing exploration strategies: High confidence

## Next Checks
1. Replicate the experiments using non-linear contextual bandit models to assess whether the exploitation bias persists across different model architectures.
2. Conduct experiments in non-stationary environments where reward distributions change over time to evaluate exploration strategies under more realistic conditions.
3. Implement the proposed simulation-based evaluation framework on a real-world recommender system dataset to validate the practical implications of the findings.