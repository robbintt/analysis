---
ver: rpa2
title: 'CCI4.0: A Bilingual Pretraining Dataset for Enhancing Reasoning in Large Language
  Models'
arxiv_id: '2506.07463'
source_url: https://arxiv.org/abs/2506.07463
tags:
- data
- datasets
- dataset
- reasoning
- cci4
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CCI4.0, a large-scale bilingual pretraining
  dataset (35 TB) for large language models that combines high-quality English and
  Chinese corpora with Chain-of-Thought reasoning templates. The dataset uses a novel
  pipeline involving two-stage deduplication, multi-classifier quality scoring, and
  domain-aware fluency filtering to ensure data quality.
---

# CCI4.0: A Bilingual Pretraining Dataset for Enhancing Reasoning in Large Language Models

## Quick Facts
- arXiv ID: 2506.07463
- Source URL: https://arxiv.org/abs/2506.07463
- Authors: Guang Liu, Liangdong Wang, Jijie Li, Yang Yu, Yao Xu, Jiabei Chen, Yu Bai, Feng Liao, Yonghua Lin
- Reference count: 40
- Primary result: CCI4.0 dataset improves reasoning and reduces hallucination in models pretrained on 35TB of high-quality bilingual (English/Chinese) data with Chain-of-Thought templates

## Executive Summary
CCI4.0 is a 35TB bilingual pretraining dataset designed to enhance reasoning capabilities in large language models. It combines high-quality English and Chinese corpora with Chain-of-Thought reasoning templates synthesized from diverse sources. The dataset employs a rigorous pipeline including two-stage deduplication, multi-classifier quality scoring, and domain-aware fluency filtering to ensure data quality. Experimental results show models pretrained on CCI4.0 outperform baseline datasets on benchmarks like MMLU and ARC-Challenge, with notable gains in reasoning tasks and reduced hallucination.

## Method Summary
CCI4.0 uses a novel pipeline that combines fuzzy deduplication to remove semantic near-duplicates, followed by exact substring matching to eliminate shared boilerplate. The dataset then employs multi-classifier scoring using XLRoberta and fastText models to bin data by quality. A domain-aware fluency filter removes the top 0.5% highest-loss samples per domain to eliminate noise while preserving challenging content. Finally, the system synthesizes 4.5 billion human reasoning templates by extracting and reconstructing reasoning trajectories from segmented documents, creating Core Question + CoT + Original Document triplets.

## Key Results
- Models pretrained on CCI4.0 outperform baseline datasets on MMLU and ARC-Challenge benchmarks
- CCI4.0 shows consistent improvements across model scales (0.5B to 7B parameters)
- Models trained on CCI4.0 exhibit lower perplexity on correct CoT reasoning paths versus incorrect ones
- The dataset demonstrates superior scaling efficiency compared to Nemotron-CC

## Why This Works (Mechanism)

### Mechanism 1: Hybrid Redundancy Elimination
The two-stage deduplication process (fuzzy followed by exact) creates a denser signal distribution by removing both semantic near-duplicates and exact copies. This prevents models from wasting capacity memorizing repeated content while maintaining diversity. The fuzzy deduplication catches paraphrased duplicates while exact matching removes boilerplate and shared paragraphs.

### Mechanism 2: Staged Chain-of-Thought (CoT) Extraction
Instead of using distilled model outputs, CCI4.0 extracts human-written reasoning trajectories from existing documents. The pipeline segments documents semantically, summarizes each segment, and reconstructs complete reasoning chains. This grounding in human logic patterns reduces hallucination compared to synthetic generation methods.

### Mechanism 3: Domain-Aware Fluency Filtering
Rather than applying global thresholds, the system calculates perplexity distributions per domain and removes only the top 0.5% highest-loss samples. This preserves domain-specific complexity while eliminating gibberish and noise. The approach recognizes that valid text complexity varies significantly across domains like Games versus Law.

## Foundational Learning

- **Concept: Perplexity (PPL) as Noise Detector**
  - **Why needed here:** Section 3.4 uses PPL to define "fluency" - measuring how "surprised" a model is by text. High PPL often indicates grammar errors or out-of-distribution noise.
  - **Quick check question:** If a model has high PPL on a document, does that mean it's definitely low quality or just statistically unlikely compared to the training distribution?

- **Concept: Exact vs. Fuzzy Deduplication**
  - **Why needed here:** Section 3.2 uses both methods - fuzzy matches meaning (paraphrases) while exact matches strings (copy-paste).
  - **Quick check question:** Why need both? (Hint: Can you copy-paste a code snippet with one variable changed? Is that fuzzy or exact?)

- **Concept: Pretraining vs. Distillation**
  - **Why needed here:** Section 3.5 emphasizes their CoT is extracted from raw data, not distilled from a teacher model.
  - **Quick check question:** If I use GPT-4 to rewrite Wikipedia into Q&A format, is that extraction or distillation?

## Architecture Onboarding

- **Component map:** Data-Juicer + Custom Loaders → Fuzzy Dedup → Exact Dedup → Multi-Classifier Scoring → Quality Bins → Domain Classifier → PPL Calculation → Top 0.5% Filtering → Semantic Chunking → Summarization → CoT Triplet Generation

- **Critical path:** Domain-Aware Fluency Filtering is the gatekeeper. The CoT Synthesis relies on "filtered high-quality samples" as seeds. If the fluency filter is too aggressive, the CoT generator starves; if too loose, it synthesizes reasoning from garbage.

- **Design tradeoffs:**
  - Global vs. Local Filtering: Chooses local (per-domain) filtering to preserve diversity but requires robust domain classification
  - Extraction vs. Generation: Uses extraction to preserve "human-like" reasoning but is harder to scale than model generation

- **Failure signatures:**
  - CoT Hallucination: If segmentation logic fails, "Core Question" might not match "Answer" - look for disjointed logic in synthesized triplets
  - Over-deduplication: If min-doc-words=35 threshold is too strict, code snippets or short math proofs might be wiped out

- **First 3 experiments:**
  1. CoT Ablation: Train 0.5B model on CCI4.0-Base only vs. CCI4.0-Base + CoT. Measure "Adversarial PPL" gap to verify reasoning signal.
  2. Scaling Efficiency: Train for 10B, 20B, 60B tokens. Plot curve to see if CCI4.0 hits saturation earlier than Nemotron-CC.
  3. Domain Filter Stress Test: Change fluency filter from 99.5% to 98% and measure drop in ARC-Challenge scores to validate noise-signal tradeoff.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the effects of Chain-of-Thought (CoT) data introduced during pretraining be better activated or leveraged during the post-training stage?
- **Basis in paper:** Section 5.3 and Appendix A.4 state this warrants further investigation as performance gains on downstream reasoning tasks are inconsistent despite CoT lowering perplexity on correct reasoning paths.
- **What evidence would resolve it:** Experiments demonstrating that specific post-training alignment strategies successfully unlock the latent reasoning capabilities established by CoT pretraining data.

### Open Question 2
- **Question:** Does reasoning improvement observed via perplexity reduction in small models (0.5B) translate to explicit generative reasoning capabilities in large-scale models?
- **Basis in paper:** Section 5.2 notes the 0.5B model "does not exhibit emergent CoT generation capabilities," requiring adapted perplexity-based evaluation rather than standard generative testing.
- **What evidence would resolve it:** Evaluating a model larger than 7B parameters trained on CCI4.0 on generative benchmarks like GSM8K using standard zero-shot CoT prompting.

### Open Question 3
- **Question:** Can the proposed data processing pipeline and quality classifiers be effectively adapted for languages beyond English and Chinese?
- **Basis in paper:** Appendix A.7 (Limitations) states it currently supports only these two languages with future extensions planned.
- **What evidence would resolve it:** Successful application of multiclassifier scoring and domain-aware fluency filtering to a third language, followed by demonstration of improved model performance over raw corpora.

## Limitations
- The empirical verification of the CoT extraction pipeline is limited, with insufficient demonstration of how often it successfully captures coherent reasoning versus producing disjointed logic chains
- The optimal threshold values (min-doc-words=35, 99.5% cutoff) are presented without sensitivity analysis showing their impact on downstream performance
- The computational overhead of the two-stage deduplication process is not quantified, and potential bias from the domain classifier is not adequately addressed

## Confidence
**High Confidence:** The general framework of using quality filtering and deduplication to improve pretraining data quality is well-supported by experimental results showing gains on MMLU and ARC-Challenge benchmarks.

**Medium Confidence:** The specific claims about CoT extraction reducing hallucination and improving reasoning are supported by internal perplexity metrics but lack comprehensive external validation.

**Low Confidence:** The optimal threshold values and specific hyperparameters for fuzzy deduplication are presented without sensitivity analysis, and the paper doesn't adequately address potential failure modes.

## Next Checks
1. **CoT Coherence Evaluation:** Conduct human evaluation of 100 randomly sampled CoT triplets from CCI4.0 to measure logical coherence and whether the "Core Question" actually connects to the "Answer" through the provided reasoning chain.

2. **Threshold Sensitivity Analysis:** Systematically vary the fluency filter cutoff (95%, 97%, 99%, 99.5%, 99.7%) and the minimum document words threshold (25, 35, 45, 55) while measuring impact on ARC-Challenge scores and model perplexity on held-out reasoning tasks.

3. **Domain Coverage Audit:** Analyze the domain classifier's performance across all 26 categories by measuring classification accuracy and confidence scores on a held-out multilingual dataset to identify domains where the classifier shows uncertainty or bias.