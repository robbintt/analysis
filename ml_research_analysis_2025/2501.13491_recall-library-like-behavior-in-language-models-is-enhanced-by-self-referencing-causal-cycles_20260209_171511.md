---
ver: rpa2
title: 'RECALL: Library-Like Behavior In Language Models is Enhanced by Self-Referencing
  Causal Cycles'
arxiv_id: '2501.13491'
source_url: https://arxiv.org/abs/2501.13491
tags:
- cycle
- token
- sequence
- tokens
- causal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates the \"reversal curse\" in autoregressive\
  \ language models\u2014their inability to infer preceding context from succeeding\
  \ tokens. The authors introduce \"self-referencing causal cycles\" (RECALL), a mechanism\
  \ enabled by repeated token sequences (cycle tokens) that act as natural hyperlinks\
  \ connecting different parts of text."
---

# RECALL: Library-Like Behavior In Language Models is Enhanced by Self-Referencing Causal Cycles

## Quick Facts
- arXiv ID: 2501.13491
- Source URL: https://arxiv.org/abs/2501.13491
- Authors: Munachiso Nwadike; Zangir Iklassov; Toluwani Aremu; Tatsuya Hiraoka; Velibor Bojkovic; Benjamin Heinzerling; Hilal Alqaubeh; Martin Takáč; Kentaro Inui
- Reference count: 37
- Primary result: Cycle tokens enable autoregressive models to recover preceding context from succeeding tokens, resolving the reversal curse in 100% of tested cases.

## Executive Summary
This paper addresses the "reversal curse" in autoregressive language models—their inability to infer preceding context from succeeding tokens due to unidirectional causal masking. The authors introduce "self-referencing causal cycles" (RECALL), where repeated token sequences (cycle tokens) create natural hyperlinks that bypass this limitation. Through controlled experiments on a small transformer and analysis of real-world corpora, they demonstrate that cycle tokens allow models to recover left-hand sequences from right-hand ones. The proposed two-step RECALL-aware prompting strategy successfully resolves the reversal curse across key writings by first collecting surrounding context using cycle tokens, then extracting answers through in-context reasoning.

## Method Summary
The authors implement a 2-layer, 8-head decoder-only transformer (~90K parameters) trained on synthetic few-token datasets of the form [e1, e2, e3, e1], where e1 serves as a cycle token. They conduct deterministic experiments where the model learns to predict [e3, e1, e2] given e3 alone, stochastic experiments with multiple candidate continuations, and apply RECALL-aware prompting to real-world texts. The two-step prompting strategy involves: (1) prompting the model to output all surrounding context associated with a query sequence using cycle tokens, then (2) using in-context learning over this retrieved context to select the correct answer. Validation uses validation accuracy on predicting left-hand tokens/sequences given right-hand tokens via cycle tokens.

## Key Results
- Deterministic cycle token experiments achieve 100% accuracy in predicting left-hand sequences using cycle tokens as bridges
- Stochastic settings show accuracy following 1/n pattern for n candidate continuations
- RECALL-aware prompting successfully resolves the reversal curse in 100% of tested key writings, including U.S. National Anthem preceding-line problems where direct prompting fails

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Repeated token sequences (cycle tokens) create causal pathways that enable autoregressive models to recover preceding context from succeeding tokens.
- Mechanism: When a token sequence appears at multiple positions in training data (e.g., [e1, e2, e3, e1]), it creates a loop in the model's learned transition probabilities. The model can follow this loop from a right-hand token backward through the cycle token to retrieve left-hand sequences.
- Core assumption: The model has sufficiently memorized the transition probabilities induced by cycle tokens during pretraining.
- Evidence anchors: Section 3.2 states "This modified sequence serves as a pointer back to the start of the original sequence, providing access to Sl. The token e1 acts as a cycle token—so named because it induces a cycle in the causal flow of next-token prediction." Section 4.1 shows "In all scenarios, 100% accuracy is achieved in predicting the left-hand sequence using the cycle token as a bridge."

### Mechanism 2
- Claim: Stochastic cycle tokens (appearing with multiple continuations) generate a bounded candidate set rather than single predictions, enabling targeted retrieval.
- Mechanism: When cycle tokens map to n possible continuations with uniform probability (~1/n each), the model distributes probability mass across valid candidates only, excluding out-of-distribution tokens. All candidates appear in top-n predictions.
- Core assumption: Training data presents candidates with approximately uniform frequency; no dominant candidate skews the distribution.
- Evidence anchors: Section 4.2 shows "For each e1, any e2i is predicted with probability 1/n... any of the n candidates will always appear within the top-n predictions." Figure 6 demonstrates accuracy follows 1/n pattern across varying candidate set sizes.

### Mechanism 3
- Claim: Two-step prompting (context retrieval → selection) outperforms direct querying for reversed relationships.
- Mechanism: Step 1 prompts the model to output all surrounding context associated with a query sequence (leveraging cycle tokens). Step 2 uses in-context learning over this retrieved context to select the correct answer, bypassing next-token prediction bias.
- Core assumption: The model's initial retrieval step successfully surfaces the correct answer among candidates.
- Evidence anchors: Section 4.3.2 demonstrates "This approach is effective in 100% of our key-writings for GPT-4o and LlaMA-3.3-70B." Figure 9 shows successful retrieval for U.S. National Anthem preceding-line problem where direct prompting fails.

## Foundational Learning

- Concept: **Autoregressive causal masking**
  - Why needed here: Understanding why models predict left-to-right but cannot naturally reverse is essential for grasping why cycle tokens are necessary.
  - Quick check question: Given sequence [A, B, C], can a standard autoregressive model directly compute P(A|C) during a single forward pass?

- Concept: **Bayes' rule for sequence reversal**
  - Why needed here: The paper formalizes that recovering Sl from Sr requires computing argmax over PM(Sr|s)P(s), which is intractable without narrowing the candidate set.
  - Quick check question: Why is iterating over all possible sequences s ∈ S computationally infeasible?

- Concept: **In-context learning vs. parametric knowledge**
  - Why needed here: The two-step prompting strategy relies on the model processing its own output as context to select correct answers.
  - Quick check question: How does presenting candidates in-context differ from relying on next-token prediction alone?

## Architecture Onboarding

- Component map: Small transformer (2 layers, 8 attention heads, embedding dim 36–256) → Cross-entropy loss → Adam optimizer (lr=0.001)
- Critical path: (1) Identify cycle token candidates in target domain → (2) Verify recurrence frequency and distribution in pretraining data → (3) Design two-step prompts: broad retrieval query → in-context selection
- Design tradeoffs: Longer paths between start token and cycle token require larger embedding dimensions; stochastic settings with more candidates reduce per-candidate probability but maintain retrievability within top-n
- Failure signatures: Cycle Composability fails when context alters token semantics (Section 4.1: [e1, e2, e3] + [e3, e1, e4] conflicts with retrieving e2); direct prompting fails 100% on reversed queries without RECALL-aware strategy
- First 3 experiments:
  1. Train on [e1, e2, e3, e1] sequences with disjoint token ranges; test whether model predicts [e3, e1, e2] from e3 alone (validates basic cycle mechanism)
  2. Increase path length N between e3 and cycle token; measure validation accuracy vs. embedding dimension (validates scaling)
  3. Apply two-step RECALL prompting to a held-out key-writing; compare direct query accuracy vs. retrieve-then-select accuracy (validates practical deployment)

## Open Questions the Paper Calls Out

- **Interpretability of cycle token mechanisms:** Further interpretability techniques may be required to precisely attribute parametric information retrieval to specific cycle tokens in the pretraining data. This poses a non-trivial challenge, as larger models often utilize fully or partially closed-source training data.

- **Interaction with RAG and external tools:** The performance of autoregressive models deployed in real-world applications may be influenced by additional factors, such as retrieval-augmented generation or web search.

- **Minimum frequency threshold for cycle tokens:** The paper does not systematically analyze how many repetitions are necessary for cycle tokens to create effective self-referencing causal cycles in pretraining data.

## Limitations

- Synthetic dataset fidelity may not generalize to natural language where cycle tokens have context-dependent meanings, as Cycle Composability fails when surrounding context changes.
- The paper cannot definitively prove cycle tokens are the mechanism rather than other memorization effects, as alternative explanations like arbitrary associations between tokens cannot be excluded.
- Real-world cycle token prevalence is inadequately quantified, with minimal corpus statistics showing how frequently cycle tokens appear in pretraining data for arbitrary queries.

## Confidence

- **High confidence:** Deterministic few-token experiments convincingly demonstrate cycle tokens enable left-to-right sequence recovery in controlled settings, with 100% accuracy on key writings using RECALL-aware prompting.
- **Medium confidence:** Stochastic candidate generation mechanism works as described for uniform distributions but may not hold for natural language with skewed frequencies.
- **Low confidence:** Claim that cycle tokens naturally occur frequently enough in pretraining data to be a practical solution for the reversal curse lacks quantitative analysis of cycle token density in actual pretraining corpora.

## Next Checks

1. **Corpus-level cycle token frequency analysis:** Quantify the density of potential cycle tokens in a representative subset of pretraining data (e.g., C4 or The Pile). For a random sample of 1,000 query sequences, measure what percentage have cycle tokens with sufficient recurrence to enable RECALL. This validates whether the mechanism is broadly applicable or limited to curated examples.

2. **Ablation on context dependence:** Replicate Section 4.1's Cycle Composability test with natural language examples where the same token sequence appears in semantically different contexts. Measure how often context changes cause retrieval failures, and whether the model can disambiguate based on surrounding tokens.

3. **Efficiency comparison with baseline methods:** Implement direct prompting with temperature scaling (to sample multiple candidates) as a baseline. Compare the number of API calls, total tokens generated, and success rate on a benchmark of 100 reversal-curse problems. This determines whether RECALL-aware prompting provides net benefit over simpler approaches.