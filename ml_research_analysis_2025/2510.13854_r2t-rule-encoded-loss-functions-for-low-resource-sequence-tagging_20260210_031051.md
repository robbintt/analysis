---
ver: rpa2
title: 'R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging'
arxiv_id: '2510.13854'
source_url: https://arxiv.org/abs/2510.13854
tags:
- rules
- zarma
- loss
- training
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: R2T introduces a principled learning framework that embeds multi-tiered
  linguistic rules directly into neural network training objectives via an adaptive
  loss function, teaching models to handle out-of-vocabulary words with principled
  uncertainty. For Zarma POS tagging, the R2T-BiLSTM model trained without labeled
  data achieved 98.2% accuracy, outperforming AfriBERTa fine-tuned on 300 labeled
  sentences.
---

# R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging

## Quick Facts
- arXiv ID: 2510.13854
- Source URL: https://arxiv.org/abs/2510.13854
- Reference count: 18
- Primary result: R2T-BiLSTM achieves 98.2% POS accuracy without labeled data, outperforming AfriBERTa fine-tuned on 300 sentences

## Executive Summary
R2T introduces a principled learning framework that embeds multi-tiered linguistic rules directly into neural network training objectives via an adaptive loss function, teaching models to handle out-of-vocabulary words with principled uncertainty. For Zarma POS tagging, the R2T-BiLSTM model trained without labeled data achieved 98.2% accuracy, outperforming AfriBERTa fine-tuned on 300 labeled sentences. For the more complex NER task, an R2T-Transformer pre-trained with rules and fine-tuned on only 50 labeled sentences surpassed a baseline trained on 300. The approach demonstrates that encoding explicit linguistic knowledge can be more data-efficient than traditional annotation-heavy methods, especially in low-resource settings.

## Method Summary
R2T replaces traditional supervised loss with a composite loss function incorporating four rule-based components: lexical constraints (unambiguous and ambiguous word tags), syntactic constraints (valid tag transitions), distributional regularization (KL divergence to uniform), and OOV handling (uncertainty regularization). The model is trained on unlabeled data, using linguistic rules to guide learning instead of gold labels. For evaluation, R2T models can be fine-tuned on small labeled datasets, demonstrating strong performance even with minimal supervision.

## Key Results
- R2T-BiLSTM (0 labeled sentences) achieved 98.2% POS accuracy, surpassing AfriBERTa fine-tuned on 300 labeled sentences
- R2T-Transformer with 50-sentence SFT achieved 0.935 F1 for NER, exceeding baseline trained on 300 sentences
- BiLSTM architecture adhered better to token-level rule constraints in unsupervised mode than Transformer
- R2T demonstrates significant data efficiency gains in low-resource settings

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Integrating linguistic rules directly into the loss function provides a training signal that substitutes for labeled data in low-resource settings.
- **Mechanism:** The framework defines a composite loss $L_{R2T} = \alpha L_{lex} + \beta L_{syn} + \gamma L_{dist} + \delta L_{oov}$. Instead of minimizing error against ground-truth tags (which are absent), the model minimizes penalties for violating linguistic constraints (e.g., predicting a tag outside the valid set for an ambiguous word).
- **Core assumption:** The linguistic rules (lexical, morphological, syntactic) are sufficiently high-precision to guide the model's weights without inducing systematic error.
- **Evidence anchors:** [abstract] "...integrates a multi-tiered system of linguistic rules directly into a neural network's training objective." [section 2.3] Describes $L_{lex}$ which enforces high-confidence rules, encouraging the model to place "predictive mass within the valid set."
- **Break condition:** If the rule set contains frequent errors or the language lacks the distinct morphological markers the rules rely on, the loss signal becomes noisy, potentially degrading performance below random initialization.

### Mechanism 2
- **Claim:** Forcing a uniform output distribution on out-of-vocabulary (OOV) tokens prevents overconfidence and improves robustness during unsupervised learning.
- **Mechanism:** The adaptive OOV loss ($L_{oov}$) minimizes the KL divergence between the model's prediction $p_{oov}$ and a uniform distribution $U$. This penalizes the model for making confident, low-probability guesses on unknown inputs, effectively teaching "principled humility."
- **Core assumption:** The cost of a confident error on an unknown word is higher than the cost of uncertainty, and uniform distribution is a valid proxy for "I don't know."
- **Evidence anchors:** [abstract] "...adaptive loss function, which includes a regularization term that teaches the model to handle out-of-vocabulary (OOV) words with principled uncertainty." [section 2.3] Eq. 5 defines the OOV loss specifically to regularize confidence on tokens not covered by Tier 1-3 rules.
- **Break condition:** If the tag distribution is heavily skewed (e.g., 90% Nouns), enforcing a uniform prior on OOV words may suppress the valid base-rate probability, hurting recall on frequent classes.

### Mechanism 3
- **Claim:** Recurrent architectures (BiLSTM) adhere more strictly to token-level rule constraints than Transformers in unsupervised setups, but Transformers recover better with minimal fine-tuning.
- **Mechanism:** BiLSTMs process tokens sequentially, keeping the loss computation local and aligned with the immediate rule constraint. Transformers use global self-attention, which may "dilute" the specific token-level penalty, leading to context-based errors unless fine-tuned.
- **Core assumption:** The inductive bias of the architecture interacts significantly with the granularity of the supervision signal (rules vs. labeled data).
- **Evidence anchors:** [section 4] "The BiLSTM's sequential recurrent nature appears to adhere more effectively with our token-level loss function... The Transformer's global self-attention mechanism may dilute the impact." [table 1] Shows R2T-BiLSTM (0.968 F1) outperforming R2T-Transformer (0.852 F1) in unsupervised mode, but Transformer recovering to 0.935 F1 with SFT.
- **Break condition:** If the task requires long-range dependencies that rules don't capture, the BiLSTM's local focus might become a liability compared to the Transformer's global context, even if rules are present.

## Foundational Learning

- **Concept:** **Kullback-Leibler (KL) Divergence**
  - **Why needed here:** The paper uses KL divergence to calculate the Distributional Loss ($L_{dist}$) and OOV Loss ($L_{oov}$). Understanding this metric is essential to grasp how the model is penalized for deviating from a target distribution (uniform or expected).
  - **Quick check question:** If the model predicts a probability of 0.8 for a specific tag on an OOV word, how does the KL divergence against a uniform distribution penalize this compared to a prediction of 0.25?

- **Concept:** **Sequence Tagging Architectures (BiLSTM-CRF vs. Transformer)**
  - **Why needed here:** The paper experiments with replacing the standard BiLSTM-CRF baseline with an R2T-BiLSTM and R2T-Transformer. Knowing how these architectures process context (sequential vs. attention) explains the performance gap in the results.
  - **Quick check question:** Why would a global attention mechanism (Transformer) struggle to adhere to a loss term that penalizes individual token predictions without labeled data?

- **Concept:** **Semi-Supervised / Unsupervised Learning Paradigms**
  - **Why needed here:** R2T is framed as "Principled Learning" (PrL), a form of unsupervised learning using constraints. Distinguishing this from standard self-training (using model predictions as labels) is crucial.
  - **Quick check question:** How does R2T differ from standard self-training where a model labels its own data? (Hint: Does R2T generate pseudo-labels or define boundaries?)

## Architecture Onboarding

- **Component map:** FastText embeddings (300d) + Character BiLSTM (50d) -> BiLSTM/Transformer encoder -> Linear + Softmax -> R2T Loss Module

- **Critical path:**
  1. Define Tier 1-4 rules in JSON format (lexicons, suffix lists, bigram constraints)
  2. Pass unlabeled sentence through encoder to get tag probabilities
  3. For each token, check if it triggers a rule
     - If unambiguous word: Apply $L_{lex}$ (Cross Entropy)
     - If ambiguous word: Apply $L_{lex}$ (Sum of valid probs)
     - If OOV: Apply $L_{oov}$ (KL divergence to Uniform)
  4. Update weights to minimize rule violations

- **Design tradeoffs:**
  - Use BiLSTM for pure unsupervised training (better rule adherence)
  - Use Transformer if planning Supervised Fine-Tuning (SFT) later (higher capacity for correction)
  - Strict rules (Tier 1) ensure high accuracy but require manual effort
  - Relaxing to Tier 3 (morphology) is faster but noisier

- **Failure signatures:**
  - Catastrophic Tokenization Mismatch where punctuation isn't separated, breaking tag sequence
  - Systemic Verb Misclassification where global attention overrides local lexical rules

- **First 3 experiments:**
  1. Train R2T-BiLSTM on synthetic dummy language where Tier 1 rules cover 100% of words; verify loss approaches zero and accuracy is 100%
  2. Train on Zarma data with $L_{oov}$ disabled; check if model hallucinates high-confidence tags for nonsense strings
  3. Fine-tune pre-trained R2T-Transformer on incrementally small datasets (10, 50, 100 sentences) to plot data-efficiency curve

## Open Questions the Paper Calls Out
None

## Limitations
- Rule Quality Dependency: Framework performance hinges on availability and accuracy of high-precision linguistic rules, which may not exist for many languages
- Architecture Sensitivity: Stark performance differences between BiLSTM and Transformer suggest method is architecture-sensitive rather than purely loss-function-driven
- Dataset Specificity: Results based on single low-resource language (Zarma) with specific morphological characteristics, limiting generalization claims

## Confidence
**High Confidence:**
- Composite loss formulation is mathematically sound and reproducible
- Unsupervised BiLSTM performance exceeding AfriBERTa with 300 labeled sentences is well-documented
- OOV regularization mechanism is correctly implemented and interpretable

**Medium Confidence:**
- R2T-Transformer with 50-sentence SFT outperforming fully supervised baselines (assumes 50-sentence results are representative)
- Claim that sequential processing inherently better preserves token-level constraints (lacks direct ablation)
- Distributional loss contribution appears minor but actual impact on learning dynamics is unclear

**Low Confidence:**
- Generalization to other low-resource languages without similar morphological richness
- Scalability of manual rule creation for languages with complex morphology
- Whether performance gains would persist with noisier, automatically generated rules

## Next Checks
1. **Cross-Lingual Rule Transfer:** Apply exact Zarma rule set to morphologically similar language (e.g., Songhay dialects) and measure performance degradation to test rule language-specificity

2. **Rule Quality Ablation:** Systematically degrade rule quality (10%, 20%, 50% random errors) and measure impact on both BiLSTM and Transformer performance to quantify robustness

3. **Architecture Ablation Study:** Train vanilla Transformer with identical hyperparameters but without R2T loss, then compare to R2T-Transformer; repeat with BiLSTM variants to isolate whether performance differences stem from loss function or architectural biases