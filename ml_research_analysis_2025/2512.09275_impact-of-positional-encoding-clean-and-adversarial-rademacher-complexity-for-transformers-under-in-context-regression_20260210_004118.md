---
ver: rpa2
title: 'Impact of Positional Encoding: Clean and Adversarial Rademacher Complexity
  for Transformers under In-Context Regression'
arxiv_id: '2512.09275'
source_url: https://arxiv.org/abs/2512.09275
tags:
- adversarial
- generalization
- complexity
- bound
- proof
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work provides the first generalization analysis for a single-layer
  Transformer under in-context regression that explicitly accounts for a completely
  trainable positional encoding (PE) module. The analysis shows that PE systematically
  enlarges the generalization gap, with the gap widening further under adversarial
  attack.
---

# Impact of Positional Encoding: Clean and Adversarial Rademacher Complexity for Transformers under In-Context Regression

## Quick Facts
- arXiv ID: 2512.09275
- Source URL: https://arxiv.org/abs/2512.09275
- Authors: Weiyi He; Yue Xing
- Reference count: 40
- Primary result: Trainable positional encodings systematically increase generalization gap and adversarial vulnerability in single-layer Transformers under in-context regression

## Executive Summary
This paper provides the first generalization analysis for a single-layer Transformer under in-context regression that explicitly accounts for completely trainable positional encoding (PE). The authors derive Rademacher complexity bounds showing that PE systematically enlarges the generalization gap compared to architectures without PE or with fixed encodings. The analysis reveals that adversarial attacks further magnify this gap, demonstrating that PE amplifies vulnerability through a distinct complexity term proportional to attack strength.

The theoretical framework decomposes the function class with PE into the sum of a no-PE class and a bias class, revealing the "cost of learnability" for position information. The analysis also shows that increasing context length reduces generalization error by tightening the effective parameter space diameter. These findings suggest that while trainable PE offers maximum flexibility for learning order information, it introduces significant complexity costs and adversarial vulnerabilities that must be carefully managed.

## Method Summary
The paper analyzes single-layer, single-head Transformers performing in-context linear regression on Gaussian data. The architecture embeds input sequences using trainable positional encodings, applies attention, and predicts the query token through linear readout. The analysis derives Rademacher complexity bounds comparing models with and without trainable PE under both clean and adversarial (PGD attack) conditions. Key assumptions include bounded PE effects, sub-Gaussian data distribution, and inverse Lipschitz continuity of the parameter-to-weight mapping. The theoretical results are validated through controlled experiments measuring generalization gaps across different context lengths and attack strengths.

## Key Results
- Trainable positional encoding systematically increases generalization gap by a complexity term proportional to PE norm and input dimension
- Adversarial attacks further widen the gap between PE and no-PE models through a distinct PE-dependent vulnerability term
- Increasing context length reduces generalization error by contracting the effective parameter space diameter
- Replacing trainable PE with fixed RoPE eliminates the complexity penalty, validating that learnable parameters drive the vulnerability

## Why This Works (Mechanism)

### Mechanism 1: Complexity Injection via Trainable Positional Encoding
The paper decomposes the function class with PE ($F_{PE}$) into the sum of a no-PE class ($F_{NoPE}$) and a bias class ($F_{bias}$). By subadditivity of Rademacher complexity, the total complexity becomes $Rad_S(F_{PE}) \le Rad_S(F_{NoPE}) + Rad_S(F_{bias})$. The term $Rad_S(F_{bias})$ represents the "cost of learnability" for position information, adding a term proportional to $C_{PE}\sqrt{d}/\sqrt{m}$. The difference is bounded (Assumption 4.2) and data follows a sub-Gaussian distribution. If PE is non-trainable (e.g., RoPE), the complexity of $F_{bias}$ vanishes, closing the gap.

### Mechanism 2: Adversarial Amplification of Positional Vulnerability
Adversarial perturbations introduce a complexity term proportional to attack strength $\epsilon$ and PE bound $C_{PE}$ (specifically $C_{PE}\epsilon/\sqrt{m}$). This suggests that perturbing positional information is a distinct attack vector that exacerbates the "cost of learnability" beyond just expanding the solution space diameter. The adversarial loss can be upper-bounded by a surrogate loss (Proposition 4.1), and the model is Lipschitz continuous with respect to its input. If attack strength $\epsilon \to 0$, the adversarial gap collapses to the clean gap.

### Mechanism 3: Context Length as a Solution Space Constraint
Increasing context length $t$ reduces generalization error by tightening the diameter of the effective parameter space. The analysis maps the Transformer's ICL process to finding an effective linear weight $w_{eff}$. As context $t$ increases, the solution space $W_{S_t}$ shrinks, scaling as $O(\sqrt{r/t})$, reducing the Dudley integral and thus the Rademacher complexity. The mapping from parameters to the effective linear weight is inverse Lipschitz continuous (Assumption 4.3), and the data matrix is well-conditioned ("Good event" $G_t$). If $t \approx d$ (interpolation threshold), the bound becomes unstable due to high variance in the solution space.

## Foundational Learning

- **Concept: Rademacher Complexity (RC)**
  - **Why needed here:** RC is the primary theoretical tool used to bound the "richness" or "capacity" of the Transformer function class, directly determining the generalization gap.
  - **Quick check question:** Can you explain why a higher Rademacher complexity implies a larger gap between training error and test error?

- **Concept: In-Context Learning (ICL) as Implicit Regression**
  - **Why needed here:** The paper models the Transformer not just as a sequence model, but as an algorithm performing linear regression on the prompt examples to predict the query. This "effective linear weight" ($w_{eff}$) assumption is central to the proofs.
  - **Quick check question:** How does the paper define the "effective linear weight" $w_{eff}$ in the context of a Transformer's output?

- **Concept: Covering Numbers & Dudley's Integral**
  - **Why needed here:** These concepts bridge the gap between high-dimensional parameter spaces and generalization bounds. They quantify how many "balls" of size $\epsilon$ are needed to cover the function class.
  - **Quick check question:** According to the paper, how does the diameter of the parameter space influence the logarithmic covering number (Eq. 10)?

## Architecture Onboarding

- **Component map:** Input (prompt matrix) -> Embedding (content + trainable PE) -> Attention (single-layer, single-head) -> Output (linear readout)
- **Critical path:** The analysis tracks how adding the trainable matrix $P$ increases the parameter dimension $D$ and adds a specific "bias" term to the output. The critical proof step is bounding the diameter of the resulting solution space relative to the context length $t$.
- **Design tradeoffs:**
  - **Trainable PE:** Offers maximum flexibility for learning order information but introduces a complexity cost ($C_{PE}\sqrt{d}/\sqrt{m}$) and adversarial vulnerability ($C_{PE}\epsilon/\sqrt{m}$)
  - **Fixed/No PE:** Lower generalization gap and better robustness, but may struggle if positional dependencies are complex or non-standard
  - **Context Length ($t$):** Longer contexts reduce generalization gap (desirable) but increase computational cost; diminishing returns are hit once the PE complexity term dominates
- **Failure signatures:**
  - **High Generalization Gap:** Occurs when $t$ is small relative to data dimension $d$, or when PE norm $B_P$ is large
  - **Adversarial Collapse:** Significant performance drop under small perturbations $\epsilon$ if the PE module is trainable and attack strength is high relative to sample size $m$
  - **Instability:** High variance in generalization gap when $t \approx d$ (double descent peak)
- **First 3 experiments:**
  1. **Gap Validation:** Train single-layer Transformers with and without trainable PE on linear regression tasks. Plot generalization gap vs. context length $t$ to verify the "upward shift" predicted by Theorem 4.2.
  2. **Robustness Check:** Apply PGD attacks (perturbation $\epsilon$) to the trained models. Measure if the gap-widening effect ($\Delta_{adv} > \Delta_{clean}$) correlates with the theoretical prediction of the $C_{PE}\epsilon$ term.
  3. **RoPE Baseline:** Replace trainable PE with Rotary Positional Encoding (RoPE). Confirm that the generalization gap decreases to match the "No-PE" baseline, validating that the complexity comes from *learnable* parameters, not just the presence of positional info.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How do trainable positional encodings impact the generalization bounds and adversarial robustness of deep, multi-layer Transformer architectures?
- **Basis in paper:** The authors explicitly state that a "natural extension is to consider understanding how positional encodings scale in multi-layer or deep Transformer architectures."
- **Why unresolved:** The current theoretical analysis is strictly limited to a simplified single-layer, single-head architecture.
- **What evidence would resolve it:** Deriving Rademacher complexity bounds that account for the composition of PE across multiple layers and validating if the linear scaling of the generalization gap holds in deep networks.

### Open Question 2
- **Question:** Can the theoretical framework for in-context regression be extended to non-linear tasks and real-world applications?
- **Basis in paper:** The conclusion notes the limitation that "we only consider the simplest linear regression task" and suggests extending the analysis to "more complicated tasks."
- **Why unresolved:** The proofs rely on the existence of an "effective linear weight" ($w_{eff}$) and a linear data generation assumption (Assumption 4.1), which do not apply to non-linear function classes.
- **What evidence would resolve it:** Deriving generalization bounds for Transformers performing in-context learning of non-linear functions (e.g., neural network classes) or testing the bounds on standard NLP benchmarks.

### Open Question 3
- **Question:** Can novel regularization techniques be designed to explicitly penalize the complexity terms identified in the bounds (such as $L_f$ or $D$) to mitigate the vulnerability introduced by positional encodings?
- **Basis in paper:** The authors suggest that "with the complexity terms... identified in our bounds, one may develop new regularization techniques... to design more robust models."
- **Why unresolved:** While the bounds identify the theoretical sources of vulnerability, no specific training algorithms or regularization loss terms have been proposed to control these parameters during optimization.
- **What evidence would resolve it:** A training methodology that constrains the parameter norm or Lipschitz constants associated with PE, resulting in empirical generalization gaps comparable to models without PE.

## Limitations

- **Theoretical Scope:** The analysis is limited to single-layer, single-head Transformers performing linear regression, with unclear generalization to deep architectures or non-linear tasks
- **Bound Tightness:** The Rademacher complexity bounds rely on assumptions about Lipschitz continuity and surrogate loss tightness that may not hold tightly in practice
- **Instability Threshold:** The analysis identifies $t \approx d$ as a potential instability point, but the exact transition behavior and practical implications at this threshold are not fully explored

## Confidence

- **High Confidence:** The core claim that trainable PE increases Rademacher complexity and generalization gap is well-supported by theoretical decomposition and basic empirical validation
- **Medium Confidence:** The adversarial amplification mechanism is theoretically sound but relies on assumptions about Lipschitz continuity and surrogate loss tightness that require empirical verification
- **Medium Confidence:** The context length benefit is mathematically rigorous (diameter scaling as $O(1/\sqrt{t})$), but the practical magnitude of this effect in realistic ICL settings needs further validation

## Next Checks

1. **Sensitivity Analysis:** Systematically vary the PE norm scaling factor (not just the fixed value of 25) and measure the corresponding changes in generalization gap and adversarial vulnerability to validate the theoretical dependencies

2. **Multi-Head Extension:** Extend the single-head analysis to multi-head attention to verify whether the PE complexity and adversarial amplification mechanisms scale linearly or exhibit different behavior with increased model capacity

3. **Alternative Attack Vectors:** Test the adversarial robustness claims using different attack types (e.g., FGSM, Carlini-Wagner) beyond PGD to determine if the PE vulnerability is attack-specific or a fundamental property of the architecture