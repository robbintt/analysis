---
ver: rpa2
title: Low Rank Gradients and Where to Find Them
arxiv_id: '2510.01303'
source_url: https://arxiv.org/abs/2510.01303
tags:
- data
- have
- gradient
- then
- assumption
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates low-rank structure in the gradients of
  the training loss for two-layer neural networks under relaxed isotropy assumptions
  on training data and parameters. Using a spiked data model with anisotropic and
  ill-conditioned bulk, the authors show that the gradient with respect to input weights
  is approximately low-rank, dominated by two rank-one terms: one aligned with the
  bulk data-residue and another with the rank-one spike in input data.'
---

# Low Rank Gradients and Where to Find Them

## Quick Facts
- **arXiv ID:** 2510.01303
- **Source URL:** https://arxiv.org/abs/2510.01303
- **Reference count:** 40
- **One-line primary result:** Gradient with respect to input weights is approximately low-rank, dominated by two rank-one terms aligned with data bulk residue and spike.

## Executive Summary
This paper investigates the low-rank structure of gradients in two-layer neural networks under a spiked covariance data model. The authors show that the gradient with respect to input weights decomposes into approximately two dominant rank-one components: one aligned with the bulk data-residue and another with the rank-one spike in input data. The balance between these components is governed by data properties, scaling regime, activation function, and regularization. Standard regularizers like weight decay, input noise, and Jacobian penalties selectively modulate these components. Theoretical findings are corroborated with experiments on synthetic and real data, including MNIST and CIFAR-10 embeddings.

## Method Summary
The method analyzes gradient low-rank structure using a spiked covariance model where data X has bulk covariance Σ̂ with spectral decay λ_k(Σ̂) = k^{-α} and a leading spike ζ²qqᵀ. The analysis focuses on two-layer networks f(x) = γ_m aᵀσ(Wx) with fixed outer weights a and varying inner weights W. Key parameters include spike magnitude ζ = n^ν and scaling regimes γ_m = 1/√m (NTK) or γ_m = 1/m (MF). The gradient is decomposed into residue-aligned S1, data-spike-aligned S2, and interpolant S12 components, with bounds showing ∥G - S1 - S12 - S2∥₂/∥r∥∞ = O(∥W∥₂ n^{ν-1/2}).

## Key Results
- Gradient w.r.t. input weights is approximately rank-2 with two dominant rank-one components
- Smooth C² activations preserve residue spike (S1) while ReLU suppresses it relative to data spike (S2)
- Different regularizers selectively modulate residue vs data spike components
- MF scaling produces more stable gradient directions than NTK scaling during training

## Why This Works (Mechanism)

### Mechanism 1
The spiked data model creates two alignment directions in the gradient: S1 aligned with bulk-residue Xᵀ_Br, and S2 aligned with the data spike q. The interpolant S12 bridges these components. This holds under proportional scaling (n/d → ψ₁ < 1, m/d → ψ₂), spiked covariance with bulk decay exponent α ≥ 0, spike magnitude ζ = n^ν, and C² smooth activation σ. The balance is broken when ν ≥ 0.5 with non-C² activations like ReLU, or when residue concentration assumption is violated.

### Mechanism 2
ReLU activation suppresses S1 relative to smooth C² activations because σ′⊥(XWᵀ) has operator norm Θ(n) for ReLU versus o(n) for smooth activations. This inflates the error term and S2, suppressing S1's relative contribution. The condition requires rows of W to be i.i.d. from unit sphere and 2ν > 1 - α. Input noise τ² = n^ρ can re-emerge S1 even with ReLU when added.

### Mechanism 3
Regularizers selectively modulate components: ℓ₂ weight decay uniformly affects all components at sufficient strength. Isotropic input noise increases bulk variance, boosting S1 while suppressing S2. Jacobian penalty L_reg = (λ/2n)∑∥∂_W f(x_i)∥²_F adds a data-aligned spike S3 that competes with and suppresses S1. These effects require σ to be twice differentiable and proper noise/penalty calibration.

## Foundational Learning

- **Spiked covariance model / Random matrix theory**: Essential for modeling data as Σ = Σ̂ + ζ²qqᵀ where bulk has spectral decay λ_k(Σ̂) = k^{-α} and spike has magnitude ζ = n^ν. Controls gradient rank structure through parameters ν and α.
  - *Quick check*: Given eigenvalues decaying as k^{-0.5} with leading spike 10× larger than second, what gradient component dominates?

- **Mean Field vs Neural Tangent Kernel scaling regimes**: MF scaling (γ_m = Θ(1/m)) has residue r ≈ y at initialization with stable gradient direction; NTK scaling (γ_m = Θ(1/√m)) has r distinct from y and gradient direction evolves during training.
  - *Quick check*: With γ_m = 1/√m, does the top gradient singular vector remain stable over training?

- **Spectral decomposition and low-rank matrix approximation**: Required to approximate gradient G as G ≈ S1 + S12 + S2 where each S term is rank-1. Necessary to understand operator norm bounds and identify dominant singular vectors.
  - *Quick check*: How would you empirically verify whether a gradient matrix is approximately rank-2 vs rank-1?

## Architecture Onboarding

- **Component map**: Input data X = X_B + ζzqᵀ (bulk + spike) -> Hidden layer W ∈ ℝ^{m×d} (input weights with unit-norm rows) -> Output layer a ∈ {±1}^m (fixed random outer weights) -> Gradient components G = S1 + S12 + S2 + E

- **Critical path**: 1) Estimate data statistics (ν, α, q) from training covariance spectrum 2) Select scaling regime based on desired gradient stability 3) Choose activation based on S1/S2 prominence needs 4) Apply regularizer based on desired spike balance 5) Monitor gradient spectrum during training

- **Design tradeoffs**: MF vs NTK scaling offers stability vs feature evolution tradeoff; activation smoothness affects S1 preservation; regularization strength must be calibrated to avoid suppressing useful features

- **Failure signatures**: Gradient spectrum not low-rank indicates violated proportional scaling or activation requirements; S1 unexpectedly suppressed suggests ReLU with moderate spike; phase transitions during training are normal for moderate ν

- **First 3 experiments**: 1) Validate gradient rank on synthetic spiked data with known ν, α 2) Compare MF vs NTK gradient stability by tracking alignment over epochs 3) Test regularizer selectivity by measuring S1/S2 ratio changes

## Open Questions the Paper Calls Out

1. What are the theoretical dynamics governing the phase transition from residue-aligned (S1) to data-spike-aligned (S2) gradient components during training, particularly for moderate spike sizes (ν ∈ [0.25, 0.5])?

2. How does the interplay between residue-aligned (S1) and data-spike-aligned (S2) gradient components quantitatively affect generalization and the network's adaptation to data structure?

3. How does the low-rank gradient structure evolve during training when the non-vanishing expected derivative assumption (μ_j = Ω(1)) is violated, specifically for large data spikes (ν ≥ 0.5)?

4. Can the rank-two gradient structure be preserved for small-to-moderate spikes (ν < 0.5) if the independence assumption between data X and weights W is relaxed?

## Limitations
- Gradient decomposition assumes strict concentration conditions that may break during training
- ReLU dominance condition requires specific ν and α relationships that are data-dependent
- Regularizer effectiveness depends on precise hyperparameter calibration
- Empirical validation shows mixed results on real data embeddings

## Confidence

- **High confidence**: Gradient matrix G has approximately rank-2 structure under spiked data model; activation smoothness affects relative strength of S1 vs S2 components
- **Medium confidence**: Regularizers selectively modulate residue vs data spike components; MF scaling produces more stable gradient directions than NTK scaling
- **Low confidence**: Quantitative generalization bounds linking component balance to performance

## Next Checks

1. Test gradient rank stability during training by monitoring singular value spectrum throughout optimization epochs

2. Vary data statistics systematically by generating multiple spiked covariance datasets with different (ν, α) combinations and measuring gradient rank structure changes

3. Compare regularization effects quantitatively by measuring exact reduction in ∥S1∥ vs ∥S2∥ ratios and testing against theoretical scaling bounds