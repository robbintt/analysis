---
ver: rpa2
title: Sample-aware Adaptive Structured Pruning for Large Language Models
arxiv_id: '2503.06184'
source_url: https://arxiv.org/abs/2503.06184
tags:
- pruning
- adapruner
- data
- calibration
- importance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of structured pruning for large
  language models (LLMs), focusing on the limitations of randomly selected calibration
  data and fixed importance estimation metrics. The proposed method, AdaPruner, introduces
  a sample-aware adaptive structured pruning framework that optimizes both calibration
  data and importance estimation metrics simultaneously.
---

# Sample-aware Adaptive Structured Pruning for Large Language Models

## Quick Facts
- **arXiv ID:** 2503.06184
- **Source URL:** https://arxiv.org/abs/2503.06184
- **Reference count:** 29
- **Primary result:** AdaPruner achieves 1.37% better average performance over LLM-Pruner, with 97% performance retention at 20% pruning ratio.

## Executive Summary
This paper introduces AdaPruner, a sample-aware adaptive structured pruning framework for large language models (LLMs). The method addresses limitations of random calibration data selection and fixed importance estimation metrics by jointly optimizing both through Bayesian optimization. AdaPruner constructs a structured pruning solution space containing calibration data and importance estimation metrics, then uses Bayesian Optimization with Tree-Structured Parzen Estimator (BO-TPE) to search for optimal configurations. The approach demonstrates superior performance compared to existing structured pruning methods across different pruning ratios and model families.

## Method Summary
AdaPruner operates by first constructing a solution space that contains both calibration data samples and importance estimation metrics. The method uses Bayesian Optimization (BO-TPE) to search this space, where each configuration is evaluated by pruning the model based on the proposed importance scores and measuring perplexity on evaluation data. The importance estimation combines first-order and second-order Taylor expansions with equilibrium coefficients to balance coarse-grained and fine-grained weight importance. After finding the optimal configuration through BO, the model is pruned permanently and LoRA fine-tuning is applied to restore performance.

## Key Results
- AdaPruner outperforms LLM-Pruner by 1.37% average performance across pruning ratios
- At 20% pruning ratio, models maintain 97% of unpruned performance
- Bayesian optimization converges in approximately 150 iterations versus 300 for random search
- Joint optimization of calibration data and metrics is essential for optimal performance

## Why This Works (Mechanism)

### Mechanism 1: Coupled Optimization of Calibration Data and Metrics
The method jointly optimizes calibration data samples and importance estimation metrics, recognizing they are interdependent. By constructing a unified solution space containing both data and metric subspaces, Bayesian optimization can identify globally optimal configurations rather than local optima from fixed random data. The core assumption is that pruned model performance is a smooth function of the data/metric configuration, allowing effective modeling by the surrogate in BO.

### Mechanism 2: Granularity-Balanced Importance Estimation
A weighted combination of coarse-grained (weight vector) and fine-grained (weight element) Taylor expansions provides more robust importance measurement. The framework calculates importance using $I_{\theta_i, \theta_k} = \alpha_1 I_{\theta_i} + \alpha_2 I_{\theta_k}$, balancing structural and elemental views. The core assumption is that optimal equilibrium coefficients generalize across different layers, with ablation studies showing performance degradation when either component is removed.

### Mechanism 3: Black-Box Search via Expected Improvement
Tree-Structured Parzen Estimator maximizes Expected Improvement to efficiently locate high-performing configurations in high-dimensional space. Since the objective function is a black box, TPE models the probability density of "good" configurations and selects next candidates by maximizing the ratio of good-to-bad densities. The core assumption is that computational cost justifies the overhead of fitting the TPE surrogate model.

## Foundational Learning

- **Concept: Structured Pruning Dependencies**
  - **Why needed here:** Unlike unstructured pruning, removing structured components in LLMs requires identifying coupled parameters to maintain tensor dimension consistency.
  - **Quick check question:** If you prune a specific attention head, which corresponding weight matrices in the output projection must be removed to prevent a shape mismatch error?

- **Concept: Taylor Expansion for Sensitivity**
  - **Why needed here:** The paper relies on approximating loss change when weights are zeroed out, requiring understanding of first-order vs second-order approximations.
  - **Quick check question:** Why does the paper approximate the Hessian diagonal using the Fisher Information Matrix rather than computing the full Hessian?

- **Concept: Bayesian Optimization (BO-TPE)**
  - **Why needed here:** The core engine of AdaPruner is a hyperparameter tuner that balances exploration vs exploitation using a surrogate model.
  - **Quick check question:** In the context of AdaPruner, what acts as the "acquisition function" that decides which calibration data/metric pair to test next?

## Architecture Onboarding

- **Component map:** Solution Space Generator -> BO-TPE Searcher -> Pruning Engine -> Evaluator -> Recovery Module
- **Critical path:** The Evaluation Step inside the BO loop, which involves a forward pass of the pruned model on evaluation dataset
- **Design tradeoffs:**
  - Search Cost vs Final Quality: More BO iterations improve quality but linearly increase time-to-deployment
  - Evaluation Data Size: Larger evaluation sets give more accurate signals but slow down search
- **Failure signatures:**
  - Oscillating Perplexity: BO fails to converge due to noisy evaluation metric
  - Collapse to Trivial Solution: Optimizer finds extreme settings that artificially minimize perplexity
  - Memory OOM during Hessian Calc: Second-order gradient calculations can spike memory usage
- **First 3 experiments:**
  1. Implement baseline with random calibration data and fixed first-order Taylor metric
  2. Run optimizer searching only for calibration data vs only for metrics
  3. Transfer optimal configuration from LLaMA-7B to Vicuna-7B without re-running search

## Open Questions the Paper Calls Out
The paper explicitly states that future work will explore the potential of AdaPruner for a broader range of pruning solution space designs and model types, indicating interest in applying the method to different architectures and pruning scenarios.

## Limitations
- Evaluation protocol uses perplexity as proxy during search but final evaluation uses zero-shot tasks without quantifying correlation between metrics
- Generalizability claims are based on only two models (LLaMA and Vicuna), requiring more extensive validation across diverse architectures
- Computational overhead of Bayesian optimization search is not reported, making practical deployment cost assessment difficult

## Confidence
- **High Confidence:** Core algorithmic framework is sound and ablation study provides strong evidence for optimization components
- **Medium Confidence:** Weighted combination of Taylor metrics is superior, but equilibrium coefficients are likely model-dependent
- **Low Confidence:** Transferability claims are asserted but insufficiently tested with limited cross-model validation

## Next Checks
1. Quantify correlation between perplexity on WikiText2/PTB and zero-shot task accuracy for pruned models at different ratios
2. Apply optimal configuration from LLaMA-7B to structurally distinct LLM (OPT or Falcon) and report performance drop
3. Report total GPU hours consumed by BO search and LoRA fine-tuning, comparing to unstructured pruning cost