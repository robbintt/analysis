---
ver: rpa2
title: 'iCLP: Large Language Model Reasoning with Implicit Cognition Latent Planning'
arxiv_id: '2512.24014'
source_url: https://arxiv.org/abs/2512.24014
tags:
- reasoning
- plans
- latent
- language
- planning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes iCLP, a novel framework that enables LLMs to
  perform step-by-step reasoning by adaptively generating compact latent plans in
  a hidden space. Inspired by human implicit cognition, iCLP first distills explicit
  plans from existing reasoning traces, learns discrete representations via a vector-quantized
  autoencoder, and fine-tunes LLMs to generate these latent plans as compact encodings
  of effective reasoning instructions.
---

# iCLP: Large Language Model Reasoning with Implicit Cognition Latent Planning

## Quick Facts
- arXiv ID: 2512.24014
- Source URL: https://arxiv.org/abs/2512.24014
- Authors: Sijia Chen; Di Niu
- Reference count: 9
- Primary result: 10% average accuracy gains over base models through latent planning framework

## Executive Summary
iCLP introduces a novel framework that enables large language models to perform step-by-step reasoning by generating compact latent plans in a hidden space, inspired by human implicit cognition. The approach first distills explicit plans from existing reasoning traces, learns discrete representations via vector-quantized autoencoders, and fine-tunes LLMs to generate these latent plans as compact encodings of effective reasoning instructions. Experimental results demonstrate significant improvements in both accuracy and efficiency across mathematical reasoning and code generation tasks.

The framework achieves these improvements while reducing token costs by 10% on average compared to zero-shot chain-of-thought prompting, and shows strong cross-domain generalization capabilities. By preserving interpretability while maintaining computational efficiency, iCLP offers a promising approach to enhancing LLM reasoning capabilities without sacrificing transparency or incurring excessive computational overhead.

## Method Summary
iCLP operates through a three-stage process that transforms explicit reasoning traces into compact latent representations. First, the framework extracts explicit reasoning plans from existing reasoning datasets, capturing the step-by-step instructions that lead to correct solutions. These explicit plans are then encoded into discrete latent representations using a vector-quantized autoencoder, which learns to compress the reasoning instructions into compact, meaningful encodings. Finally, LLMs are fine-tuned to generate these latent plans directly, allowing them to produce efficient reasoning encodings without explicitly generating each reasoning step.

The key innovation lies in the use of implicit cognition principles, where the model learns to encode complex reasoning processes into compact latent representations that capture the essential structure of effective reasoning. This approach enables more efficient reasoning while maintaining the interpretability of the underlying reasoning process, as the latent plans can be decoded back into explicit instructions when needed.

## Key Results
- Achieves 10% average accuracy improvements over base models on mathematical reasoning tasks
- Demonstrates 9% improvement on code generation tasks with statistical significance
- Reduces token costs by 10% on average compared to zero-shot chain-of-thought prompting
- Shows strong cross-domain generalization from mathematical reasoning to code generation

## Why This Works (Mechanism)
iCLP works by leveraging the human-like ability to compress complex reasoning processes into compact, implicit representations. The vector-quantized autoencoder learns to identify the most salient features of effective reasoning traces and encode them into discrete latent variables that capture the essential structure of successful problem-solving approaches. When LLMs generate these latent plans instead of explicit reasoning steps, they can maintain the core reasoning structure while reducing computational overhead and token usage.

The framework's effectiveness stems from its ability to learn from existing reasoning traces and distill them into more efficient representations. By fine-tuning LLMs on these latent plans, the model learns to generate compact encodings that preserve the critical reasoning information needed for accurate problem-solving. This approach mirrors how humans often think through problems implicitly before articulating their reasoning, allowing for more efficient and focused reasoning processes.

## Foundational Learning

**Vector-Quantized Autoencoders**: Used to learn discrete representations of reasoning traces by compressing continuous information into discrete latent variables. Why needed: Enables efficient encoding of complex reasoning patterns into compact representations. Quick check: Verify that discrete latent codes capture essential reasoning structure through reconstruction quality metrics.

**Chain-of-Thought Reasoning**: Traditional explicit reasoning approach where models generate step-by-step explanations. Why needed: Provides baseline for comparison and source of training data for latent plan extraction. Quick check: Ensure extracted explicit plans maintain logical consistency and correctness.

**Fine-tuning with Latent Objectives**: Training LLMs to generate latent plans rather than explicit instructions. Why needed: Enables models to produce efficient reasoning encodings while maintaining reasoning quality. Quick check: Monitor latent plan generation quality and its correlation with final task performance.

**Discrete Representation Learning**: Converting continuous reasoning patterns into discrete codes. Why needed: Facilitates efficient storage and generation of reasoning patterns. Quick check: Validate that discrete codes preserve semantic meaning of original reasoning traces.

## Architecture Onboarding

**Component Map**: Explicit reasoning traces -> VQ-VAE encoder -> Discrete latent plans -> LLM fine-tuning -> Latent plan generation

**Critical Path**: The critical path involves extracting explicit reasoning traces from training data, encoding them through the VQ-VAE to obtain discrete latent representations, and then fine-tuning the LLM to generate these latent plans conditioned on input problems. This path directly impacts model performance and efficiency.

**Design Tradeoffs**: The framework trades explicit interpretability during generation for computational efficiency and compact representation. While latent plans can be decoded for interpretability, the generation process itself is less transparent than standard chain-of-thought prompting. The discrete quantization may also lose some nuance in complex reasoning patterns.

**Failure Signatures**: Potential failures include: 1) Poor reconstruction quality from VQ-VAE leading to loss of critical reasoning information, 2) LLM generating incoherent latent plans that don't map to valid reasoning, 3) Domain shift where latent plans learned from one domain don't generalize to others, 4) Over-compression of reasoning traces leading to loss of essential problem-solving steps.

**First Experiments**: 1) Test VQ-VAE reconstruction quality on held-out reasoning traces to verify information preservation, 2) Evaluate latent plan generation quality by decoding and checking reasoning coherence, 3) Conduct ablation studies removing the VQ-VAE component to assess its contribution to performance improvements.

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Limited comparison with established reasoning methods like chain-of-thought and program-of-thought approaches
- Insufficient statistical significance measures for performance improvements, particularly on code tasks
- Limited qualitative examples demonstrating interpretability preservation and latent plan decoding

## Confidence
High confidence in data collection methodology and training pipeline design
Medium confidence in evaluation design due to potential confounding factors
Low confidence in interpretability preservation claims without sufficient qualitative evidence

## Next Checks
1. Conduct head-to-head comparisons with standard CoT and program-of-thought baselines using identical evaluation protocols and multiple random seeds to establish statistical significance
2. Perform ablation studies to isolate the contribution of the vector-quantized autoencoder versus other components of the iCLP framework
3. Extend evaluation to additional reasoning domains including commonsense reasoning and multi-hop inference tasks to better assess generalization capabilities