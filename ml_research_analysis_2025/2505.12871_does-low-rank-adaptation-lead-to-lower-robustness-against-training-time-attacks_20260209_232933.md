---
ver: rpa2
title: Does Low Rank Adaptation Lead to Lower Robustness against Training-Time Attacks?
arxiv_id: '2505.12871'
source_url: https://arxiv.org/abs/2505.12871
tags:
- lora
- rank
- attacks
- poisoning
- attn
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper analyzes the training-time security of LoRA-based fine-tuning.
  It introduces a theoretical framework using NTK and information geometry to assess
  LoRA's robustness against data poisoning and backdoor attacks.
---

# Does Low Rank Adaptation Lead to Lower Robustness against Training-Time Attacks?

## Quick Facts
- arXiv ID: 2505.12871
- Source URL: https://arxiv.org/abs/2505.12871
- Reference count: 40
- Primary result: LoRA shows higher backdoor attack robustness but increased vulnerability to untargeted data poisoning compared to full fine-tuning

## Executive Summary
This paper investigates the training-time security of Low-Rank Adaptation (LoRA) when subjected to adversarial attacks. Through a combination of theoretical analysis using Neural Tangent Kernel (NTK) and information geometry, along with experimental validation on GLUE tasks, the authors assess LoRA's robustness against backdoor and data poisoning attacks. The study reveals that LoRA exhibits higher resistance to backdoor attacks while being more susceptible to untargeted data poisoning compared to full fine-tuning. Key factors influencing this security profile include the rank and initialization variance of the A matrix in LoRA's decomposition.

## Method Summary
The paper employs a theoretical framework based on NTK and information geometry to analyze LoRA's robustness against training-time attacks. The analysis considers both backdoor attacks (where specific triggers are inserted into training data) and data poisoning attacks (where training data is corrupted to degrade overall performance). The theoretical model examines how LoRA's low-rank structure affects its vulnerability to these attacks compared to full fine-tuning. Experimental validation is conducted on GLUE benchmark tasks, where LoRA is fine-tuned under attack conditions and its performance is compared against full fine-tuning baselines.

## Key Results
- LoRA demonstrates higher robustness against backdoor attacks compared to full fine-tuning
- LoRA shows increased vulnerability to untargeted data poisoning attacks
- The rank and initialization variance of the A matrix significantly influence LoRA's security profile

## Why This Works (Mechanism)
LoRA's security characteristics stem from its low-rank decomposition structure, which constrains the parameter space during fine-tuning. This constraint affects how adversarial modifications in the training data propagate through the model. For backdoor attacks, the limited parameter updates in LoRA reduce the model's ability to learn strong associations between triggers and target labels. However, for data poisoning attacks, the same constraint makes it harder for the model to resist widespread corruption in the training data, as the limited parameter updates cannot effectively counteract the poisoned examples.

## Foundational Learning
- Neural Tangent Kernel (NTK): A theoretical framework for analyzing neural network training dynamics
  - Why needed: Provides mathematical foundation for understanding how LoRA's low-rank structure affects learning
  - Quick check: NTK linearization approximates neural network training as kernel regression
- Information Geometry: Mathematical framework for analyzing parameter spaces
  - Why needed: Helps quantify the vulnerability of LoRA's constrained parameter space to adversarial attacks
  - Quick check: Measures distance and curvature in parameter space to assess robustness
- Low-Rank Decomposition: Matrix factorization technique used in LoRA
  - Why needed: Fundamental to understanding LoRA's parameter efficiency and security implications
  - Quick check: Reduces parameter count while maintaining representational capacity

## Architecture Onboarding
- Component map: Pre-trained model <-[adapter] LoRA matrices (A, B) <- Training data
- Critical path: Training data -> LoRA update -> Model weights -> Inference
- Design tradeoffs: Parameter efficiency vs. robustness to training-time attacks
- Failure signatures: Degraded performance on clean data, susceptibility to poisoned examples
- First experiments: 1) Test backdoor attack resistance on clean vs. triggered inputs 2) Evaluate data poisoning impact on overall task performance 3) Vary rank and initialization to assess security tradeoffs

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical framework relies on linearized model assumptions that may not fully capture practical LoRA implementations
- Experimental validation limited to GLUE tasks, may not generalize to other domains
- Analysis focuses on specific attack types, comprehensive security assessment requires broader attack evaluation

## Confidence
- High Confidence: LoRA's higher robustness against backdoor attacks is supported by both theoretical analysis and experimental validation
- Medium Confidence: LoRA's increased vulnerability to untargeted data poisoning depends on specific experimental conditions
- Medium Confidence: The influence of rank and initialization variance on security is primarily theoretical and requires further empirical validation

## Next Checks
1. Extend experiments to diverse datasets beyond GLUE, including vision and multimodal tasks, to assess the generalizability of LoRA's robustness characteristics
2. Investigate the impact of different optimization algorithms and learning rate schedules on LoRA's security against training-time attacks
3. Conduct ablation studies on the initialization variance of the A matrix to quantify its exact influence on robustness against various attack types