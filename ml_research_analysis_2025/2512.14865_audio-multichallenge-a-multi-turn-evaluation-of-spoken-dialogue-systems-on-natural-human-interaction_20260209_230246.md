---
ver: rpa2
title: 'Audio MultiChallenge: A Multi-Turn Evaluation of Spoken Dialogue Systems on
  Natural Human Interaction'
arxiv_id: '2512.14865'
source_url: https://arxiv.org/abs/2512.14865
tags:
- audio
- user
- text
- arxiv
- your
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Audio MultiChallenge, a benchmark for evaluating
  end-to-end spoken dialogue systems on natural multi-turn interaction. It extends
  a text-based framework to the audio modality by introducing new axes like Voice
  Editing for mid-utterance speech repairs, and Audio-Cue Inference Memory for recalling
  ambient sounds and paralinguistic signals.
---

# Audio MultiChallenge: A Multi-Turn Evaluation of Spoken Dialogue Systems on Natural Human Interaction

## Quick Facts
- arXiv ID: 2512.14865
- Source URL: https://arxiv.org/abs/2512.14865
- Reference count: 40
- Primary result: Tested models achieve only 54.65% pass rate overall, with significant drops in Self Coherence over longer contexts and on audio-cue tasks

## Executive Summary
Audio MultiChallenge introduces a benchmark for evaluating end-to-end spoken dialogue systems on natural multi-turn audio interactions. The benchmark extends text-based frameworks to the audio modality by introducing new axes like Voice Editing for mid-utterance speech repairs and Audio-Cue Inference Memory for recalling ambient sounds and paralinguistic signals. Using a hybrid agentic and human-in-the-loop pipeline, the benchmark generates 452 unscripted conversations from 47 speakers, totaling 15 hours of audio with 1,712 atomic rubrics for fine-grained evaluation.

The benchmark employs a fixed-context evaluation protocol where conversation history is preloaded and models are prompted only on the final turn, with an LLM-as-a-judge (o4 Mini) grading responses against human-authored atomic rubrics. Tested models, including Gemini 3 Pro Preview, achieve only 54.65% pass rate overall, with significant drops in Self Coherence over longer contexts and on audio-cue tasks. Models struggle most with Voice Editing and recalling non-semantic audio cues, highlighting gaps in audio-native multi-turn dialogue capabilities.

## Method Summary
Audio MultiChallenge evaluates end-to-end spoken dialogue systems using 452 human-recorded conversations from 47 speakers, totaling 15 hours of audio at 48 kHz. The benchmark defines four evaluation axes: Inference Memory (including Audio-Cue Memory), Instruction Retention, Self Coherence, and Voice Editing. Evaluation uses a fixed-context protocol where full conversation history is preloaded and models are prompted only on the final user turn. An LLM-as-a-judge (o4 Mini) grades responses against 1,712 atomic rubrics using strict binary pass/fail criteria for Average Pass Rate (APR) and proportion-based scoring for Average Rubric Score (ARS). User turns are provided as base64-encoded audio while assistant turns are raw text.

## Key Results
- Tested models achieve only 54.65% pass rate overall across all tasks
- Significant performance drops occur in Self Coherence over longer contexts and on audio-cue tasks
- Models struggle most with Voice Editing (mid-utterance speech repairs) and Audio-Cue Inference Memory (recalling ambient sounds and paralinguistic signals)

## Why This Works (Mechanism)
The benchmark's strength lies in its hybrid generation pipeline combining agentic conversation generation with human-in-the-loop refinement, ensuring naturalistic multi-turn interactions that capture real-world dialogue complexity. The atomic rubric approach enables fine-grained evaluation across multiple axes, while the fixed-context protocol eliminates conversation drift that could bias results. The LLM-as-a-judge, validated against human annotators with high agreement metrics, provides consistent and scalable evaluation.

## Foundational Learning

1. **Audio-Cue Inference Memory**: Why needed - enables models to recall ambient sounds and paralinguistic signals critical for natural dialogue understanding; Quick check - test model on isolated audio cue recognition before multi-turn evaluation.

2. **Voice Editing**: Why needed - captures mid-utterance speech repairs common in natural human interaction; Quick check - evaluate model's ability to handle self-corrections in single-turn tasks first.

3. **Self Coherence**: Why needed - measures consistency across conversation turns, essential for maintaining context; Quick check - track coherence scores as conversation length increases to identify degradation points.

4. **Fixed-context evaluation**: Why needed - prevents conversation drift that could bias rubric satisfaction; Quick check - compare results with turn-by-turn generation to validate protocol effectiveness.

## Architecture Onboarding

**Component Map**: Audio input -> Base64 encoding -> Model API -> Text generation -> LLM judge -> Rubric scoring

**Critical Path**: Conversation history loading -> Final turn prompting -> Response generation -> Rubric evaluation

**Design Tradeoffs**: Fixed-context protocol avoids drift but may not reflect real-world turn-by-turn generation; LLM judge provides scalability but introduces potential subjectivity; strict binary pass/fail criteria ensure rigor but may mask partial successes.

**Failure Signatures**: 
- Conversation drift in sequential evaluation causing rubric misalignment
- LLM judge self-enhancement bias from including evaluated models in judge pool
- Audio-Cue failures from perception vs. retention issues
- Response length inflation artificially boosting rubric scores

**3 First Experiments**:
1. Test model on isolated audio cue recognition task to distinguish perception from memory issues
2. Conduct human evaluation on subset of Voice Editing responses to validate LLM judge scoring
3. Compare fixed-context results with turn-by-turn generation to assess protocol impact

## Open Questions the Paper Calls Out
None

## Limitations
- Fixed-context evaluation may not fully capture real-world turn-by-turn dialogue performance
- LLM-as-a-judge approach introduces potential subjectivity, particularly for nuanced audio-cue tasks
- Benchmark focuses on English-only interactions with limited speaker diversity (47 speakers)

## Confidence
High confidence in: benchmark methodology, dataset quality, and overall finding that models struggle with audio-native multi-turn dialogue
Medium confidence in: specific APR and ARS scores for individual models due to unspecified API parameters
Low confidence in: whether fixed-context protocol accurately reflects real-world performance and LLM judge consistency across all rubric types

## Next Checks
1. Replicate evaluation using turn-by-turn generation rather than fixed-context to assess conversation drift impact
2. Conduct human evaluation on subset of responses, particularly for Voice Editing and Audio-Cue Inference tasks
3. Test model performance on isolated audio cue recognition tasks to determine if failures are perception or memory issues