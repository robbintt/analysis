---
ver: rpa2
title: 'CATArena: Evaluation of LLM Agents through Iterative Tournament Competitions'
arxiv_id: '2510.26852'
source_url: https://arxiv.org/abs/2510.26852
tags:
- agents
- code
- evolutionary
- agent
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CATArena introduces a tournament-based framework to evaluate the
  evolutionary capabilities of LLM code agents by measuring their ability to iteratively
  improve through self-reflection and peer-learning. Unlike static benchmarks, it
  provides dynamic feedback across four board/card games and two optimization tasks,
  with open-ended scoring to avoid saturation.
---

# CATArena: Evaluation of LLM Agents through Iterative Tournament Competitions

## Quick Facts
- arXiv ID: 2510.26852
- Source URL: https://arxiv.org/abs/2510.26852
- Reference count: 40
- Key outcome: Introduces tournament-based framework to evaluate LLM code agents' evolutionary capabilities through iterative self-reflection and peer-learning across 6 tasks.

## Executive Summary
CATArena is a tournament-based evaluation framework that measures the evolutionary capabilities of LLM code agents by assessing their ability to iteratively improve through self-reflection and peer-learning across 4 board/card games and 2 optimization tasks. Unlike static benchmarks, it provides dynamic feedback and open-ended scoring to avoid saturation. Experiments with minimal and commercial agents show that evolutionary potential (Sevo) is distinct from initial coding proficiency (Sbase), with high-variance trajectories across tasks. The framework reveals that agents often struggle to synergize peer-learning with self-reflection, relying predominantly on one mechanism.

## Method Summary
CATArena evaluates LLM code agents through N-round tournaments where agents refine their code based on execution logs and peer solutions. The framework computes two metrics: Sbase (initial performance) and Sevo (evolutionary potential as OLS slope of global performance across rounds). Agents use Google ADK with minimal tools for file I/O and HTTP service deployment. The tournament runs on 4 competitive games (Gomoku, Hold'em, Chess, Bridge) and 2 objective tasks (OIBench, SWE-Perf), with agents receiving structured feedback containing execution logs, global leaderboards, and peer code after each round.

## Key Results
- Evolutionary potential (Sevo) is distinct from initial coding proficiency (Sbase), with high-variance trajectories across tasks
- Competitive tasks foster convergence while objective tasks foster divergence in agent populations
- Agents struggle to synergize peer-learning with self-reflection, predominantly relying on one mechanism
- Framework proves robust to task variants and extensible to new domains

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Decoupling static proficiency (Sbase) from evolutionary potential (Sevo) reveals a distinct capability dimension not captured by single-turn benchmarks.
- **Mechanism:** By calculating Sevo as the slope of a linear fit over global performance across N rounds, the framework measures the rate of adaptation rather than the state of knowledge.
- **Core assumption:** Assumes linear trend (OLS) is sufficient proxy for evolutionary capability within initial active phases (N≤4).
- **Evidence anchors:** Experiments show Sevo is distinct from Sbase with high-variance trajectories across tasks; defined via OLS to mitigate transient failures.

### Mechanism 2
- **Claim:** Peer-learning drives convergence in competitive tasks by allowing weaker agents to mimic superior strategies, whereas objective tasks foster divergence.
- **Mechanism:** In competitive environments, agents receive full tournament logs and peer solutions, allowing successful agents to analyze top-performers' code to extract heuristics and synthesize them into their own logic.
- **Core assumption:** Assumes agents possess sufficient code comprehension capabilities to extract strategic logic from peer codebases.
- **Evidence anchors:** Competitive tasks foster convergence (DISstd < 0) while open-source nature allows weaker agents to mimic successful strategies; case study shows Claude explicitly planning to study last round tournament reports.

### Mechanism 3
- **Claim:** Dynamic, non-stationary opponents (competitive tasks) prevent score saturation and force continuous adaptation more effectively than static objective functions.
- **Mechanism:** In competitive games, correctness of strategy is relative to current population, creating a moving target that continuously challenges agent's ability to generalize.
- **Core assumption:** Assumes game environments are complex enough that simple memorization is insufficient for long-term dominance over evolving peers.
- **Evidence anchors:** Feedback mechanism is dynamic; scores depend on strength of opponents; Chess exhibits oscillation while Hold'em shows strong convergence.

## Foundational Learning

- **Concept: In-Context Evolution vs. Weight Updates**
  - **Why needed here:** CATArena evaluates evolution via prompt-based feedback, not fine-tuning, requiring understanding of this distinction to interpret Sevo as measure of temporal reasoning.
  - **Quick check question:** Does the agent's performance improve because it updated its weights, or because it better utilized tournament history in context window?

- **Concept: Ordinal vs. Cardinal Evaluation**
  - **Why needed here:** In competitive tracks, scores derive from relative rankings against population, not absolute objective metrics.
  - **Quick check question:** If Agent A's code doesn't change between Round 1 and Round 2, but opponents' code improves, will Sevo likely be positive or negative?

- **Concept: The Exploration-Exploitation Trade-off in Code**
  - **Why needed here:** Paper identifies specific failure modes like "Stubborn Stagnation" (over-exploitation) and "Chaotic Reconstruction" (over-exploration).
  - **Quick check question:** Does the agent modify code incrementally (high similarity to self-history) or rewrite from scratch based on every new peer solution?

## Architecture Onboarding

- **Component map:** Tournament Controller -> Sandbox Runner -> Scoring Matrix Builder -> Feedback Aggregator -> Agent Loop
- **Critical path:** Round 1 (Cold Start): Agent receives game rules only → Generates C(1) → Execution: Sandbox runs C(1) against population → Generates Logs & Sbase → Feedback Synthesis (Round n): Agent reads logs + Top-k peer solutions → Revision: Agent outputs C(n) aiming to maximize Gn → Metric Calculation: Fit Sevo line after N rounds
- **Design tradeoffs:** Population Size (M): Smaller M is cheaper but increases variance; larger M stabilizes W but scales cost quadratically. Open-ended vs. Fixed Horizon: Allowing infinite rounds reveals long-term saturation but is cost-prohibitive; fixing N=4 captures active evolution phase.
- **Failure signatures:** Blind Copying (high code similarity to peer but low performance); Chaotic Reconstruction (low similarity to both self and peer); Negative Sevo (performance degrades over time).
- **First 3 experiments:** 1) Sanity Check: Run basic LLM on simple game for 2 rounds to verify feedback loop closes. 2) Ablation Study: Isolate mechanisms by running tournament with only logs vs. only peer code. 3) Variant Stress Test: Introduce rule variant to verify agent relies on reasoning, not pre-trained patterns.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can agent architectures be modified to synergize self-reflection and peer-learning effectively rather than relying predominantly on one mechanism?
- **Basis in paper:** Section 4.4 states inability of current models to reconcile internal diagnostics with external knowledge points to key direction for future research.
- **Why unresolved:** Ablation study shows agents exhibit failure modes like "Blind Copying" or "Stubborn Stagnation" rather than synthesizing feedback.
- **What evidence would resolve it:** Agent architecture that consistently achieves higher Sevo when provided combined feedback compared to self-reflection or peer-learning alone.

### Open Question 2
- **Question:** How can evolutionary frameworks address "cold start" problem in high-complexity domains where agents fail to generate functional baseline strategy?
- **Basis in paper:** Appendix M notes for complex games like Pommerman, agents generate invalid code, meaning without functional baseline, evolutionary feedback loop cannot be initiated.
- **Why unresolved:** CATArena currently relies on agent producing executable code in Round 1; if initial code is non-functional, iterative evolution cannot begin.
- **What evidence would resolve it:** Bootstrapping mechanism or curriculum that enables agents to produce Minimum Viable Product in high-difficulty environments to trigger feedback loop.

### Open Question 3
- **Question:** Can architectural improvements overcome asymptotic performance limits observed in long-term evolution, or are they intrinsic to current LLM reasoning capabilities?
- **Basis in paper:** Section 4.3 observes agents trend towards saturation after N=4 rounds, suggesting performance is bounded by intrinsic reasoning limits of underlying models.
- **Why unresolved:** Paper demonstrates iterative feedback drives early gains but plateaus; unclear if different optimization strategies could sustain growth.
- **What evidence would resolve it:** Demonstrating trajectory where agents maintain positive slope (Sevo) beyond 7 iterations without performance collapse in complex tasks.

## Limitations

- Assumes linear regression over 4 rounds is sufficient proxy for evolutionary capability, may not capture non-linear learning dynamics
- Peer-learning mechanism depends on agents' ability to comprehend and synthesize peer codebases, extent unclear across different LLM architectures
- Distinction between "self-reflection" and "peer-learning" as separate mechanisms not fully validated, as agents may naturally blend these processes

## Confidence

- **High Confidence:** The decoupling of Sbase and Sevo as distinct metrics; framework's robustness to task variants
- **Medium Confidence:** The peer-learning convergence hypothesis, based on case studies but limited direct evidence
- **Low Confidence:** Assumption that linear trend over 4 rounds adequately captures "evolutionary capability" without overfitting or underfitting learning dynamics

## Next Checks

1. **Dynamic Task Variant:** Introduce minor rule change (e.g., Chess960) mid-tournament to test whether Sevo reflects genuine adaptation or pre-trained pattern retrieval
2. **Ablation on Feedback Content:** Run tournaments with only logs (self-reflection) vs. only peer code (peer-learning) to isolate individual contributions to Sevo
3. **Extended Round Analysis:** Run N=7 or N=10 rounds to observe long-term saturation points and validate whether linear regression assumption holds beyond initial active evolution phase