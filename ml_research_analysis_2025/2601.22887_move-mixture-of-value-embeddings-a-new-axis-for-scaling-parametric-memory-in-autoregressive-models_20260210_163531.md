---
ver: rpa2
title: 'MoVE: Mixture of Value Embeddings -- A New Axis for Scaling Parametric Memory
  in Autoregressive Models'
arxiv_id: '2601.22887'
source_url: https://arxiv.org/abs/2601.22887
tags:
- move
- memory
- value
- standard
- embeddings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MoVE decouples memory capacity from compute in autoregressive models
  by introducing a global bank of learnable value embeddings shared across all attention
  layers. For each token, a soft gating mechanism dynamically mixes retrieved concepts
  from this bank into the standard value projection, enabling independent scaling
  of parametric memory.
---

# MoVE: Mixture of Value Embeddings -- A New Axis for Scaling Parametric Memory in Autoregressive Models

## Quick Facts
- arXiv ID: 2601.22887
- Source URL: https://arxiv.org/abs/2601.22887
- Authors: Yangyan Li
- Reference count: 39
- Key outcome: MoVE decouples memory capacity from compute in autoregressive models by introducing a global bank of learnable value embeddings shared across all attention layers.

## Executive Summary
MoVE introduces a new axis for scaling parametric memory in autoregressive models by decoupling memory capacity from computational cost. It achieves this through a globally shared bank of learnable value embeddings that is indexed by token ID and dynamically mixed into the standard value projection via a soft gating mechanism. This design enables independent scaling of parametric memory while maintaining computational efficiency, allowing "memory-dense" models without prohibitive overhead. Evaluated on both text and image generation tasks, MoVE consistently outperforms standard transformers and layer-wise memory baselines, establishing a new scaling paradigm for generative AI.

## Method Summary
MoVE augments standard transformer attention by introducing a globally shared embedding bank E ∈ R^(N_vocab × M × d) that is indexed by token ID and shared across all attention layers. For each token, MoVE retrieves M_t = E[w_t] and computes per-head gates via a router projection W_G followed by a scaled sigmoid function. The mixed value V_S = g_0 ⊙ V + Σ(g_i ⊙ M_i) combines the standard value projection V with retrieved memories, where g_0=1 provides identity baseline. This shared bank enables gradient flow from all layers into the same memory parameters, creating "gradient highways" that accelerate concept learning. The architecture is validated on text (FineWeb-Edu) and image (ImageNet-1K) generation tasks.

## Key Results
- MoVE consistently outperforms standard transformers and layer-wise memory baselines on text and image generation tasks
- Achieves lower perplexity and higher fidelity at comparable compute budgets
- Extends to optimized architectures like MLA by injecting memory into compressed latent space while preserving efficiency gains

## Why This Works (Mechanism)

### Mechanism 1: Memory-Compute Decoupling via Shared Value Bank
MoVE introduces a globally shared embedding bank that allows parametric memory to scale independently of network depth. A learnable tensor E ∈ R^(N_vocab × M × d) is indexed by token ID and shared across all L attention layers, with only M vectors per active token loaded via sparse indexing. This avoids the dense FLOPs penalty of widening the network while enabling concepts to benefit from cross-layer gradient aggregation.

### Mechanism 2: Soft Gating Enables Dynamic Arbitration Between Context and Memory
A per-head differentiable gating mechanism allows the model to dynamically balance retrieved parametric knowledge against local contextual information. A router projection W_G computes logits z ∈ R^(H×(M+1)), with a scaled sigmoid g = 2·σ(z) mapping gates to (0, 2) and g_0=1 as identity baseline. The mixed value V_S = g_0 ⊙ V + Σ g_i ⊙ M_i enables dynamic selection between contextual and parametric information.

### Mechanism 3: Gradient Highway Effect Accelerates Concept Learning
Sharing the bank across layers creates "gradient highways" that densify learning signal for memory parameters. All L layers backpropagate into the same E, effectively multiplying gradient frequency by L compared to layer-local storage. This enables concepts to be reinforced by multiple computational stages rather than isolated per-layer learning.

## Foundational Learning

- **Concept: Multi-Head Attention and the Value Stream's Role**
  - Why needed: MoVE specifically targets the Value projection; understanding that V carries "what" information (vs. Q/K which determine "where") is essential to grasp why memory injection here is sensible.
  - Quick check: If you replaced the gating on V with gating on Q, what semantic function would you expect to change?

- **Concept: Sparse vs. Dense Parameter Scaling**
  - Why needed: MoVE's central claim is decoupling memory (parameters) from compute (FLOPs). You must understand why adding a dense FFN layer costs linear FLOPs while indexing into a bank does not.
  - Quick check: Why does MoVE's indexing-based retrieval avoid the FLOP cost of a dense projection of equivalent parameter count?

- **Concept: Sigmoid Gating and Identity Initialization**
  - Why needed: MoVE uses g = 2·σ(z) with zero-init yielding g≈1, preserving the standard V at initialization. Understanding this helps diagnose training dynamics.
  - Quick check: What happens to the model's behavior at initialization if you initialize z→∞ instead of z=0?

## Architecture Onboarding

- **Component map:** Input Embedding WI -> Global Value Bank E -> Router W_G -> Gating + Mixing -> Attention Forward
- **Critical path:** Token IDs → Index E to get M_t → Reshape to (M, H, d_h) → X → W_G → logits Z → scaled sigmoid → gates g → X → W_V → standard V → V_S = g_0 ⊙ V + Σ g_i ⊙ M_i → V_S → Attention → Output
- **Design tradeoffs:**
  1. Memory bandwidth vs. FLOPs: MoVE shifts cost from compute to HBM access; optimal tradeoff is hardware-dependent.
  2. Bank size M vs. parameter efficiency: Paper notes per-parameter gain is lower than dense scaling; semantically related tokens sharing banks (unexplored) could improve efficiency.
  3. Global vs. local memory: Global sharing enables scaling beyond depth but risks gradient conflict; local (LaVE) saturates quickly but is layer-specialized.
- **Failure signatures:**
  - Gating collapse: g_0 → 2, g_i → 0 (memory ignored) or g_0 → 0 (context suppressed excessively).
  - No scaling gain: Increasing M yields no perplexity improvement—suggests bank not utilized or capacity mismatched to data.
  - MLA injection failure: If up-projecting V instead of injecting into c_KV, efficiency gains are negated.
- **First 3 experiments:**
  1. Baseline parity check: Train D12 Standard vs. D12 + MoVE-×1 on same data for same steps; confirm MoVE achieves lower BPB (per Table 1: 0.819 vs 0.838).
  2. Gating dynamics visualization: Extract gate activations for polysemous words in different contexts; verify that semantic diffs exceed control diffs as context length increases (per Appendix B).
  3. Compute overhead validation: Measure actual FLOPs and wall-clock time for D32 + MoVE-×1; confirm overhead is <2% (per Eq. 5: ~1.8%).

## Open Questions the Paper Calls Out
None

## Limitations
- Parameter efficiency is not as strong as dense scaling, suggesting fundamental tradeoffs between sparsity and parameter utilization
- Lack of ablation studies isolating the gradient highway effect, which is central to the mechanism but only indirectly evidenced
- Scaling behavior beyond ×2 memory scaling is unexplored, leaving open questions about saturation points

## Confidence

**High Confidence:** The memory-compute decoupling mechanism via shared value bank and the dynamic gating design are empirically validated through consistent performance improvements across text and image tasks, with controlled baselines showing MoVE-×2 outperforming both standard transformers and layer-wise memory variants.

**Medium Confidence:** The gradient highway hypothesis lacks direct experimental isolation, relying instead on indirect evidence from ablation studies and architectural reasoning. Performance claims could benefit from additional scaling studies.

**Low Confidence:** The claim about extending to optimized architectures like MLA by injecting into compressed latent space is demonstrated but not thoroughly evaluated for efficiency preservation.

## Next Checks

1. **Gradient Highway Isolation:** Design an experiment that compares gradient flow statistics between global and local memory variants under identical conditions, directly measuring gradient magnitude and frequency reaching the memory bank from different layers.

2. **Scaling Saturation Analysis:** Systematically evaluate MoVE with M = L/2, M = L, M = 2L, and M = 4L across D12, D20, and D32 architectures to identify saturation points and determine optimal scaling relationships.

3. **Gating Dynamics and Robustness:** Conduct controlled experiments varying initialization schemes for the router (W_G) and measuring gate behavior over training, including scenarios where gates are initialized to favor memory retrieval versus context preservation.