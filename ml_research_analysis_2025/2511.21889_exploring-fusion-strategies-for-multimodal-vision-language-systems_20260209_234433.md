---
ver: rpa2
title: Exploring Fusion Strategies for Multimodal Vision-Language Systems
arxiv_id: '2511.21889'
source_url: https://arxiv.org/abs/2511.21889
tags:
- fusion
- data
- accuracy
- multimodal
- latency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates multimodal fusion strategies for vision-language
  systems, focusing on the trade-off between accuracy and inference latency. The authors
  propose three model architectures that fuse BERT with either MobileNetV2 or ViT
  at late, intermediate, and early stages.
---

# Exploring Fusion Strategies for Multimodal Vision-Language Systems

## Quick Facts
- arXiv ID: 2511.21889
- Source URL: https://arxiv.org/abs/2511.21889
- Reference count: 28
- One-line primary result: Early fusion achieves lowest latency (11.4ms) but sacrifices accuracy (67.89%), while late fusion achieves highest accuracy (84.25%) at higher latency (21.6ms)

## Executive Summary
This paper investigates three fusion strategies (early, intermediate, late) for combining BERT with vision models (MobileNetV2 or ViT) in multimodal sentiment classification. The authors evaluate these architectures on the CMU-MOSI dataset and benchmark latency on NVIDIA Jetson Orin AGX. Their findings reveal a clear trade-off: earlier fusion stages reduce inference latency at the cost of accuracy, with late fusion achieving state-of-the-art accuracy comparable to existing methods while early fusion enables real-time edge deployment. The study demonstrates that textual features dominate accuracy in sentiment analysis tasks, making language model choice more impactful than vision model choice.

## Method Summary
The authors propose three fusion architectures combining BERT-base-uncased with either MobileNetV2 (1.4 224) or ViT-base-patch16-224. Late fusion freezes fine-tuned base models and trains only a unified classification head for 100 epochs. Intermediate fusion concatenates features from specific encoder layers (4, 7, 8) and passes them through linear and attention layers. Early fusion cuts feature extraction after 6 layers and processes through 4 attention blocks before classification. All models are trained with SGD (lr=1e-4, momentum=0.9, weight decay=1e-4) except late fusion (lr=1e-3), with batch size 32. Models are evaluated on binary sentiment classification accuracy and latency on Jetson Orin AGX.

## Key Results
- Late fusion achieves highest accuracy: 84.25% (MobileNetV2) and 83.62% (ViT)
- Early fusion achieves lowest latency: 11.4ms (MobileNetV2) and 10.951ms (ViT)
- MobileNetV2-fused models consistently outperform ViT-fused by 1-3% accuracy
- Textual features dominate accuracy: unimodal BERT (80.20%) far exceeds unimodal MobileNetV2 (43.22%)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Earlier fusion stages reduce inference latency at the cost of accuracy in multimodal vision-language systems.
- Mechanism: Earlier fusion eliminates parallel feature extraction pathways, converting two independent models into a single unified network with fewer total layers. This reduces sequential computation but removes modality-specialized processing that captures nuanced cross-modal interactions.
- Core assumption: The accuracy loss is acceptable for edge deployment scenarios where latency constraints dominate.
- Evidence anchors:
  - [abstract] "data fusion earlier in the model architecture results in faster inference times at the cost of accuracy"
  - [section 4.2.1] Early fusion achieves 11.4ms latency vs. late fusion's 21.6ms (ONNX), but accuracy drops from 84.25% to 67.89%
  - [corpus] "Vision-Language Models on the Edge for Real-Time Robotic Perception" confirms latency-constrained deployment is an active research concern
- Break condition: If cross-modal interactions are essential for task performance (e.g., sarcasm detection requiring visual-textual alignment), early fusion may fail catastrophically regardless of latency gains.

### Mechanism 2
- Claim: Late fusion achieves higher accuracy by preserving modality-specific feature extraction before combination.
- Mechanism: Separate unimodal backbones (BERT for text, MobileNetV2/ViT for vision) complete their full feature extraction pipelines independently. The classification head then receives rich, specialized representations from both modalities, enabling better final decisions.
- Core assumption: Modality-specific features are more valuable when extracted independently than when processed through shared layers early.
- Evidence anchors:
  - [abstract] "late fusion yields the highest accuracy"
  - [section 3.3.1] Fine-tuned BERT and MobileNetV2 models remain frozen with only a new classification head trained jointly
  - [section 4.1] Late fusion achieves 84.25% accuracy, comparable to state-of-the-art methods like MAG-BERT (84.2%)
- Break condition: If one modality is significantly noisier or lower-quality, late fusion may amplify noise by treating both feature streams as equally reliable.

### Mechanism 3
- Claim: Textual features dominate accuracy in sentiment analysis tasks, making language model choice more impactful than vision model choice.
- Mechanism: BERT's hierarchical encoder layers capture syntactic and semantic features critical for sentiment classification. Visual features provide marginal complementary signal, explaining why unimodal BERT (80.20%) far outperforms unimodal MobileNetV2 (43.22%).
- Core assumption: Sentiment is primarily conveyed through language rather than facial expressions in the CMU-MOSI dataset.
- Evidence anchors:
  - [section 3.3.3] "textual features are more relevant to the sentiment classification than the image features for this dataset"
  - [section 4.1] Base BERT achieves 80.20% accuracy vs. MobileNetV2's 43.22%; ViT base (50.17%) still lags significantly
  - [corpus] Weak direct corpus evidence for this specific mechanism; related papers focus on fusion rather than modality dominance
- Break condition: In tasks where visual information is primary (e.g., emotion recognition from faces without speech), this mechanism would not hold.

## Foundational Learning

- Concept: **Fusion stage taxonomy (early/intermediate/late)**
  - Why needed here: The paper's core contribution depends on understanding where modalities combine in the architecture and the implications of each choice.
  - Quick check question: Given a system where you must process audio and video for gesture recognition with strict 15ms latency, which fusion stage would you prioritize testing first?

- Concept: **Modality asymmetry in multimodal systems**
  - Why needed here: The paper demonstrates that not all modalities contribute equally; recognizing dominant vs. auxiliary modalities informs architecture design.
  - Quick check question: If adding a third modality (audio) improved accuracy by only 0.5% but increased latency by 40%, how would you justify including or excluding it?

- Concept: **Edge deployment constraints**
  - Why needed here: The paper explicitly targets edge deployment (Jetson Orin AGX), requiring understanding of how model architecture affects real-world latency beyond theoretical FLOPs.
  - Quick check question: Why might ONNX and TensorRT show different latency rankings for the same model architecture?

## Architecture Onboarding

- Component map: BERT-base-uncased (12 transformer layers) -> MobileNetV2 (16 residual bottleneck layers) or ViT-base-patch16-224 (12 encoder layers) -> Fusion layers (varies by strategy) -> Classification head

- Critical path:
  1. Fine-tune unimodal models independently on CMU-MOSI subset (required for late fusion)
  2. Select fusion stage based on latency budget
  3. For early/intermediate: determine which encoder layers to extract features from (BERT layers 4, 7, 8 used here)
  4. Train fused model end-to-end (early/intermediate) or train only classification head (late)
  5. Convert to ONNX and benchmark on target edge hardware

- Design tradeoffs:
  - **Late fusion**: 84.25% accuracy / 21.6ms latency — best when accuracy is critical
  - **Intermediate fusion**: 72.40% accuracy / 13.5ms latency — middle ground
  - **Early fusion**: 67.89% accuracy / 11.4ms latency — best for strict latency budgets
  - **Vision model choice**: MobileNetV2-fused consistently outperforms ViT-fused by 1-3% accuracy with lower latency, suggesting CNN-vision integration with transformer-text is more effective for this task

- Failure signatures:
  - Accuracy below unimodal BERT (80.20%) suggests fusion is degrading rather than enhancing performance—check feature alignment
  - Latency not improving with earlier fusion may indicate attention blocks are too numerous (ablation shows 4 blocks optimal; 6 blocks dropped accuracy)
  - Large accuracy gap between MobileNetV2 and ViT versions may indicate modality-specific optimization issues

- First 3 experiments:
  1. **Baseline replication**: Implement late fusion with frozen BERT + MobileNetV2 on CMU-MOSI, targeting ~84% accuracy and ~20ms latency on your edge hardware to validate setup.
  2. **Fusion stage sweep**: Train all three fusion variants with identical hyperparameters, plot accuracy vs. latency trade-off curve, verify the paper's claimed pareto frontier.
  3. **Attention block ablation**: For early fusion, test 2, 4, 6, 8 attention blocks to reproduce the finding that 4 blocks provide optimal balance (the paper reports 4-block: 67.89%, 8-block: 69.09% but chose 4 for latency).

## Open Questions the Paper Calls Out

- **Question**: How does the inclusion of an audio modality quantitatively shift the accuracy-latency trade-off curve compared to the proposed bimodal (text-vision) systems?
  - Basis in paper: [explicit] The authors state in Section 5.1, "This work may also be expanded by including audio data and quantifying the performance trade-off of adding this extra modality."
  - Why unresolved: The study deliberately excluded audio data, citing its marginal accuracy benefits and negative impact on inference speed, but did not measure the specific trade-off balance within their proposed fusion architectures.
  - What evidence would resolve it: Experimental results from tri-modal (text-image-audio) versions of the early, intermediate, and late fusion models benchmarked on the NVIDIA Jetson Orin AGX.

- **Question**: Can a multi-objective neural architecture search (NAS) automate the design of fusion models to populate the accuracy-latency trade-off space more efficiently than manual design?
  - Basis in paper: [explicit] Section 5.1 suggests representing the problem as a multi-objective optimization task, asking if software like Optuna can "automate the population of the performance trade-off space."
  - Why unresolved: The authors identify the "challenge" as encoding a vector-representation of the model architecture to enable the dynamic generation of varying network connections and layer depths.
  - What evidence would resolve it: A framework that successfully utilizes NAS to generate diverse model architectures that map to various optimal points on the accuracy-latency curve without manual intervention.

- **Question**: Does the high relevance of textual features in the CMU-MOSI dataset bias the fusion results, and would the trade-off space differ for vision-dominant tasks?
  - Basis in paper: [inferred] The paper notes in Section 3.3.3 that textual features are "highly relevant" and that the early fusion architecture was designed specifically to leverage this strength using attention blocks.
  - Why unresolved: The observed superiority of late fusion in accuracy might be specific to sentiment analysis where text is dominant; the trade-off dynamics for tasks relying heavily on visual features remain unexplored.
  - What evidence would resolve it: Evaluation of the proposed early, intermediate, and late fusion architectures on a vision-centric dataset (e.g., VQA) to see if the accuracy gap between fusion strategies narrows or inverts.

- **Question**: Is the drop in accuracy for early fusion models caused by the fundamental incompatibility of merging heterogeneous architectures (CNN and Transformer) or simply by the reduction in specialized feature extraction layers?
  - Basis in paper: [inferred] Section 2.1 highlights the technical challenge that "fused models may favor one modality" due to differing processing methods, and Section 3.3.3 notes the difficulty in choosing a merge architecture for "vastly different" base models.
  - Why unresolved: The paper reduces the number of feature extraction layers in early fusion (cut off after 6 layers) while simultaneously changing the merging mechanism to attention blocks, making it unclear which factor causes the accuracy degradation.
  - What evidence would resolve it: An ablation study that maintains full feature extraction depth while testing various merging mechanisms (e.g., simple concatenation vs. attention) to isolate the cause of the accuracy loss.

## Limitations

- The CMU-MOSI dataset split methodology is vaguely described as "a subset" for testing, making exact reproducibility challenging without knowing train/validation/test ratios or random seeds.
- Specific architecture details of the classification head and attention blocks are underspecified, leaving room for implementation variations that could affect results.
- The study focuses on static image frames rather than full video, which may limit generalizability to continuous visual input scenarios.

## Confidence

- **High confidence**: The latency-accuracy trade-off mechanism across fusion stages (Mechanism 1) is well-supported by direct measurements and logical architectural reasoning. The claim that earlier fusion reduces latency is clearly demonstrated with empirical data.
- **Medium confidence**: The dominance of textual features over visual features (Mechanism 3) is supported by accuracy comparisons but relies on weak corpus evidence for the specific mechanism. The task's nature (sentiment classification) may explain this, but broader validation would strengthen this claim.
- **Medium confidence**: The late fusion accuracy advantage (Mechanism 2) is empirically demonstrated, but the exact contribution of modality-specific feature extraction versus other architectural choices (like frozen vs. trainable encoders) requires further investigation.

## Next Checks

1. **Ablation study on fusion stage contribution**: Implement the three fusion variants and measure not just end-to-end accuracy but also individual modality contributions by masking one modality at inference time. This would clarify whether late fusion's advantage comes from modality specialization or simply from having more parameters.

2. **Cross-dataset generalization test**: Evaluate the best-performing model (late fusion with MobileNetV2) on a different multimodal sentiment dataset like MELD or MOSI (continuous version) to assess whether the accuracy gains transfer beyond CMU-MOSI's specific characteristics.

3. **Edge deployment robustness validation**: Deploy the three fusion models on multiple edge devices (Jetson Orin AGX, Raspberry Pi 4 with accelerator, mobile CPU) to verify that the latency rankings hold across hardware variations and to identify any hardware-specific bottlenecks.