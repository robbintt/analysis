---
ver: rpa2
title: 'Lossless Compression: A New Benchmark for Time Series Model Evaluation'
arxiv_id: '2509.21002'
source_url: https://arxiv.org/abs/2509.21002
tags:
- compression
- time
- series
- data
- lossless
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces lossless compression as a new benchmark for
  evaluating time series models, addressing the limitation that traditional tasks
  (forecasting, imputation, etc.) only assess task-specific performance rather than
  full distributional modeling. By grounding evaluation in Shannon's source coding
  theorem, the authors show that optimal compression length directly measures a model's
  ability to capture the true data-generating process.
---

# Lossless Compression: A New Benchmark for Time Series Model Evaluation

## Quick Facts
- **arXiv ID**: 2509.21002
- **Source URL**: https://arxiv.org/abs/2509.21002
- **Reference count**: 40
- **Primary result**: Introduces lossless compression as a new benchmark for evaluating time series models, showing that compression performance reveals distributional weaknesses overlooked by traditional task-specific metrics.

## Executive Summary
This paper proposes lossless compression as a comprehensive benchmark for time series model evaluation, addressing the limitation that traditional tasks (forecasting, imputation, etc.) only assess task-specific performance rather than full distributional modeling. By grounding evaluation in Shannon's source coding theorem, the authors show that optimal compression length directly measures a model's ability to capture the true data-generating process. The TSCom-Bench framework enables time series models to be evaluated as universal compressors, revealing distributional weaknesses that traditional benchmarks miss. Experiments across diverse datasets demonstrate that compression performance correlates with but is not dominated by traditional task metrics, validating its role as a principled, comprehensive evaluation task.

## Method Summary
The authors introduce TSCom-Bench, a framework that evaluates time series models by treating them as universal compressors. The method leverages Shannon's source coding theorem, which states that the optimal compression length equals the negative log-likelihood of the data under the true data-generating process. Models are evaluated by encoding time series data using their predicted probability distributions, with the compression ratio serving as the evaluation metric. The framework supports various model architectures and datasets, using IEEE-754 32-bit float encoding as the canonical symbolization scheme. The approach is tested on six diverse datasets (PEMS08, Traffic, Electricity, Weather, ETTh2, Solar) with five different model architectures.

## Key Results
- Compression performance reveals distributional weaknesses overlooked by traditional task-specific benchmarks
- TimeXer consistently achieves superior compression ratios across all tested datasets
- Compression performance correlates with but is not dominated by traditional task metrics (forecasting, imputation, etc.)
- The framework provides a principled way to evaluate whether models capture the full data-generating process

## Why This Works (Mechanism)
The approach works because lossless compression directly measures how well a model captures the underlying probability distribution of the data. According to Shannon's source coding theorem, the optimal compression length equals the negative log-likelihood of the data under the true distribution. By evaluating models based on their ability to compress data, we inherently assess their capacity to model the full data-generating process rather than just specific task outputs. This makes compression a more comprehensive evaluation metric that captures distributional modeling quality.

## Foundational Learning
- **Shannon's Source Coding Theorem**: Fundamental limit stating optimal compression length equals negative log-likelihood of true distribution. Needed to establish theoretical basis for compression as evaluation metric. Quick check: Can be verified through information theory textbooks and proofs.
- **Lossless Compression**: Encoding data without information loss using probabilistic models. Needed to provide practical evaluation framework. Quick check: Compare original vs compressed data reconstruction accuracy.
- **Distributional Modeling**: Capturing the full probability distribution of time series data. Needed to understand why compression reveals weaknesses in traditional benchmarks. Quick check: Evaluate model's ability to generate realistic synthetic data.
- **Negative Log-Likelihood**: Common metric for probabilistic model evaluation. Needed to connect compression length to model performance. Quick check: Verify NLL calculations match compression ratios.
- **Universal Compression**: Ability to compress any data source without prior knowledge. Needed to establish theoretical optimality of the approach. Quick check: Test compression on various synthetic and real datasets.

## Architecture Onboarding

**Component Map**: TSCom-Bench -> IEEE-754 Encoding -> Model Prediction -> Compression Ratio

**Critical Path**: Data Input -> Model Inference -> Probability Distribution Output -> Symbol Encoding -> Compression Length Calculation -> Performance Evaluation

**Design Tradeoffs**: 
- Canonical 32-bit float encoding ensures comparability but may not be optimal for all datasets
- True lossless compression is computationally prohibitive, requiring practical approximations
- Balance between evaluation comprehensiveness and computational feasibility

**Failure Signatures**:
- Poor compression ratios despite good task-specific performance indicate distributional modeling weaknesses
- High variance in compression across different data segments suggests model instability
- Compression performance degradation over time may indicate temporal modeling issues

**First 3 Experiments**:
1. Evaluate TimeXer on PEMS08 dataset and compare compression ratios to traditional forecasting metrics
2. Test multiple models (TimeXer, iTransformer, PatchTST) on Electricity dataset to identify distributional modeling differences
3. Compare compression performance using different symbolization schemes (bytes vs. bins) on synthetic data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does pre-training time series models specifically on the lossless compression objective yield superior performance on downstream canonical tasks (forecasting, classification) compared to standard supervised or other self-supervised pre-training methods?
- Basis in paper: Section 7.3 states compression "offers a powerful pre-training strategy" and Section 6.5 suggests this "may unify evaluation and pretraining."
- Why unresolved: Paper evaluates models trained on standard objectives using compression as benchmark, but doesn't train models from scratch using compression loss to verify transfer learning capabilities.
- What evidence would resolve it: Experiments comparing fine-tuned performance of models pre-trained via compression loss against those pre-trained via masking or prediction on standard benchmarks.

### Open Question 2
- Question: How do alternative discretization or quantization schemes (e.g., histogram binning, learned tokenization) compare to the canonical IEEE-754 byte-level encoding in terms of preserving temporal dependencies and optimizing the compression-to-performance ratio?
- Basis in paper: Section 5.1 mentions alternative encodings "may be studied" but mandates canonical encoding for comparability.
- Why unresolved: Authors mandate 32-bit float encoding for standardization, leaving potential benefits of domain-specific or learned discretization methods unexplored.
- What evidence would resolve it: Comparative study evaluating same backbone models using different symbolization strategies (bytes vs. bins) to measure trade-off between information loss and compression efficiency.

### Open Question 3
- Question: Does the strong correlation between compression performance and canonical tasks imply that a joint optimization objective (combining NLL/MSE with compression loss) would produce more robust generalist models?
- Basis in paper: Section 6.5 notes compression exhibits "moderate and relatively uniform correlation with all these tasks" and suggests training with compression-oriented objectives could provide strong pretraining.
- Why unresolved: While correlation is established, paper doesn't test whether explicitly training with hybrid loss function improves "balanced performance" seen in models like TimeXer.
- What evidence would resolve it: Training models with weighted sum of compression loss and task-specific loss (e.g., MSE) and evaluating whether this mitigates distributional weaknesses found in models optimized for single tasks.

## Limitations
- Computational complexity of achieving true lossless compression is prohibitive for large-scale time series
- The causal relationship between distributional modeling quality and compression efficiency remains unclear for complex, non-stationary series
- Benchmark relies on Shannon's source coding theorem which assumes stationary and ergodic data-generating process

## Confidence
- **High**: Theoretical grounding of compression as measure of distributional modeling is well-established in information theory
- **Medium**: Empirical demonstration that compression reveals distributional weaknesses is convincing but needs broader validation
- **Low**: Claim that compression is comprehensive evaluation task subsuming all traditional benchmarks needs more rigorous proof

## Next Checks
1. Conduct ablation studies comparing compression-based evaluation against traditional metrics on synthetic time series with known distributional properties
2. Evaluate computational feasibility and scalability of TSCom-Bench framework on larger datasets (10M+ time points)
3. Test framework's sensitivity to non-stationary data by evaluating models on time series with controlled regime shifts