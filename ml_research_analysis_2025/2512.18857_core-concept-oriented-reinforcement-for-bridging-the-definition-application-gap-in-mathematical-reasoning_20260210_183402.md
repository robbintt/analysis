---
ver: rpa2
title: 'CORE: Concept-Oriented Reinforcement for Bridging the Definition-Application
  Gap in Mathematical Reasoning'
arxiv_id: '2512.18857'
source_url: https://arxiv.org/abs/2512.18857
tags:
- reasoning
- concept
- math
- conceptual
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CORE introduces concept-oriented reinforcement learning to address
  the gap between mathematical knowledge recitation and its application. By injecting
  explicit concept signals into training via concept-aligned quizzes, concept-guided
  trajectory replacement, and KL regularization, CORE provides fine-grained supervision
  beyond outcome correctness.
---

# CORE: Concept-Oriented Reinforcement for Bridging the Definition-Application Gap in Mathematical Reasoning

## Quick Facts
- arXiv ID: 2512.18857
- Source URL: https://arxiv.org/abs/2512.18857
- Reference count: 27
- Core result: Introduces concept-oriented reinforcement learning to bridge the gap between mathematical knowledge recitation and application, improving reasoning via concept-primed trajectory replacement and KL regularization.

## Executive Summary
CORE addresses the "definition-application gap" in mathematical reasoning by injecting explicit concept signals into training. The method introduces three variants: CORE-Base consolidates concept-aligned quizzes, CORE-CR replaces failed trajectories with concept-primed ones, and CORE-KL aligns unguided reasoning with concept-guided policies via forward KL regularization. Evaluated across multiple models, CORE consistently improves performance on both in-domain textbook exercises and out-of-domain benchmarks.

## Method Summary
CORE uses GRPO as its backbone and introduces concept-oriented interventions triggered when all sampled responses fail. CORE-Base trains on concept-aligned quizzes linking problems to their underlying concepts. CORE-CR concatenates ground-truth concepts with problems and replaces failed trajectories with concept-primed ones, applying a reward bonus. CORE-KL minimizes forward KL divergence between concept-primed and unguided policies, dynamically scaling the coefficient based on reference trajectory correctness. The method is evaluated via Self-Consistency (SC@21, T=0.7) across 140 textbook exercises and 14 out-of-domain benchmarks.

## Key Results
- CORE-Base achieves 55.7% accuracy on textbook exercises, surpassing vanilla GRPO.
- CORE-CR and CORE-KL show further gains over CORE-Base on both in-domain and out-of-domain benchmarks.
- The method is algorithm- and verifier-agnostic, demonstrating robust generalization.
- Dynamic λ_KL scaling (0.03 for correct, 0.005 for incorrect) mitigates error propagation in KL regularization.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Trajectory replacement with concept-primed examples recovers learning signal when all sampled responses fail.
- Mechanism: When a "conceptual failure event" occurs (all N responses in a GRPO group are incorrect), the system retrieves the ground-truth concept text, concatenates it with the original problem, generates K new trajectories from this concept-guided prompt, and replaces K failed trajectories with these concept-primed ones—assigning them an augmented reward (R′ = R + r_bonus).
- Core assumption: Models can produce better reasoning when explicitly primed with relevant concepts, and this improved reasoning can be distilled back into the base policy through reinforcement.
- Evidence anchors:
  - [abstract] "reinforces conceptual reasoning via trajectory replacement after group failures"
  - [section 3.4] "Upon triggering, we form a concept-guided prompt p_c = c_q ⊕ q... We then randomly select and replace K trajectories from the original failed group with these new concept-guided ones."
  - [corpus] Weak direct support; neighbor papers focus on concept retrieval (arXiv:2508.06931) and quiz generation (arXiv:2503.14662) but not trajectory-level intervention mechanisms.
- Break condition: If concept-primed trajectories don't meaningfully differ from unguided ones in reasoning patterns, replacement provides no signal gain.

### Mechanism 2
- Claim: Forward KL regularization between concept-guided and unguided policies transfers conceptual reasoning without explicit trajectory replacement.
- Mechanism: Sample a reference trajectory Y* from the concept-guided policy π_θ(·|p_c), then minimize KL divergence between the guided and unguided next-token distributions at each timestep conditioned on Y* prefixes. This forces the base policy's internal reasoning on the original problem q to mimic what it would produce if explicitly given concept c_q.
- Core assumption: The concept-primed policy encodes a more robust reasoning distribution that can be distilled via distribution matching, not just single-path imitation.
- Evidence anchors:
  - [abstract] "a lightweight forward-KL constraint that aligns unguided with concept-primed policies"
  - [section 3.4] "This forces the model's internal reasoning process on the original problem q to faithfully mimic the process it would follow if it were explicitly given the concept c_q."
  - [corpus] No direct corpus evidence for KL-based concept transfer; related work (arXiv:2511.16885) explores soft concept mixing in latent space but via different mechanisms.
- Break condition: If concept-guided trajectories are low-quality or incorrect, KL alignment may distill errors; the dynamic λ_KL (0.03 for correct, 0.005 for incorrect) partially mitigates this.

### Mechanism 3
- Claim: Training directly on concept-aligned quizzes consolidates concept application from pre-training into deployable reasoning.
- Mechanism: Use GRPO on curated quizzes where each question is explicitly linked to its target concept(s). The model learns from rich question-answer pairs without explicit concept prompting during training, relying on implicit concept recall from the question structure.
- Core assumption: Models have sufficient parametric knowledge of concepts from pre-training but lack reliable concept-to-problem mapping; targeted practice on concept-linked exercises strengthens this mapping.
- Evidence anchors:
  - [abstract] "turns explicit concepts into a controllable supervision signal... synthesizes concept-aligned quizzes"
  - [section 3.4] "CORE-Base primarily functions as a consolidation mechanism, reinforcing the application of concepts already encountered during pre-training"
  - [corpus] ConQuer (arXiv:2503.14662) supports concept-based quiz generation but doesn't address RL integration.
- Break condition: If quizzes don't genuinely require the target concept, or if surface patterns suffice to solve them, consolidation fails to improve conceptual reasoning.

## Foundational Learning

- Concept: **GRPO (Group Relative Policy Optimization)**
  - Why needed here: CORE wraps around GRPO as its backbone. Understanding how GRPO samples multiple responses per prompt and computes group-relative advantages is essential to see where concept intervention plugs in.
  - Quick check question: Given 4 sampled responses with rewards [1, 0, 0, 0], what advantage would GRPO assign to the correct response?

- Concept: **KL Divergence as Distribution Matching**
  - Why needed here: CORE-KL uses forward KL (not reverse) specifically to encourage coverage of the concept-guided policy's full reasoning distribution rather than mode-collapsing to a single path.
  - Quick check question: Why might forward KL (minimizing D_KL(P_guide || P_base)) encourage broader exploration than reverse KL?

- Concept: **Definition-Application Gap**
  - Why needed here: The paper's core diagnostic shows models can recite definitions (e.g., Rational Root Theorem) but fail to correctly apply them when surface patterns conflict. Understanding this gap motivates all three CORE variants.
  - Quick check question: A model correctly states a theorem but applies it backwards (swapping numerator/denominator roles). Is this a knowledge gap or an application gap?

## Architecture Onboarding

- Component map:
  - Textbook corpus -> concept-exercise mappings (C, E pairs) -> synthetic quiz generator (Qwen2.5-72B-Instruct) -> validator (GPT-4o) -> 1,110 curated quizzes
  - GRPO backbone + concept intervention triggers (failure detection) -> concept retrieval -> trajectory replacement (CORE-CR) OR KL loss addition (CORE-KL)
  - SC@21 (21 samples, T=0.7, majority voting) across in-domain (TEXTBOOK) and 14 out-of-domain benchmarks

- Critical path:
  1. Detect conceptual failure event (all N responses incorrect in GRPO group)
  2. Retrieve concept text c_q linked to problem q
  3. Form concept-guided prompt p_c = c_q ⊕ q
  4. Generate K replacement trajectories OR compute KL loss against reference trajectory
  5. Update policy with augmented batch or regularized loss

- Design tradeoffs:
  - CORE-Base vs. CORE-CR vs. CORE-KL: Base is simplest but least interventionist; CR provides explicit correction but requires hyperparameter tuning (r_bonus=0.4); KL is implicit but requires sampling reference trajectories and tuning λ_KL.
  - Quiz quality vs. coverage: Stricter validation (90 quizzes rejected) improves quality but reduces training data; self-supervised variant (Section 5.5) shows robustness to noise.
  - Intervention trigger threshold: Current design triggers on all-failure; could be generalized to partial failure with different replacement ratios.

- Failure signatures:
  - No improvement over vanilla: Check if concept retrieval is failing (wrong concept linked) or if quizzes are contaminated/solvable by surface patterns.
  - Performance degradation on specific benchmarks: May indicate overfitting to textbook style; verify out-of-domain evaluation pipeline.
  - KL loss not decreasing: Reference trajectories may be low-quality; inspect dynamic λ_KL assignment and concept-guided generation quality.

- First 3 experiments:
  1. Sanity check on gap diagnostic: Run the robust evaluation protocol (original + 3 permuted variants) on your base model to confirm the definition-application gap exists before applying CORE.
  2. Ablation on intervention trigger: Compare triggering on all-failure vs. partial-failure (e.g., ≥50% incorrect) to understand sensitivity to intervention frequency.
  3. Self-supervised replication: Using only your target model family (no external teachers), generate quizzes via self-verification (SC@21 filtering) and train CORE-CR to confirm intrinsic reinforcement works without distillation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does CORE generalize to mathematical domains beyond linear algebra (e.g., calculus, probability, number theory)?
- Basis in paper: [explicit] The authors state their dataset was curated "from a canonical textbook, Advanced Algebra" covering only linear algebra topics, and note it was chosen for its "structured curriculum" but do not test on other math domains.
- Why unresolved: All experiments use only linear algebra exercises and concepts; it remains unclear whether concept-oriented reinforcement transfers to domains with different reasoning structures.
- What evidence would resolve it: Apply CORE to textbooks from calculus, probability, or geometry domains and evaluate on corresponding out-of-domain benchmarks.

### Open Question 2
- Question: How does CORE scale to models larger than 8B parameters (e.g., 70B+ models)?
- Basis in paper: [explicit] The authors evaluate on Qwen2-Math-7B, DeepSeek-R1-Distill-Qwen-1.5B, Qwen2.5-Math-1.5B, and Llama-3-8B-Instruct, with no experiments on larger scales.
- Why unresolved: Larger models may already exhibit stronger conceptual reasoning, potentially reducing CORE's marginal benefit, or may benefit differently from concept injection strategies.
- What evidence would resolve it: Train and evaluate CORE variants on 70B-scale models using the same benchmarks to measure scaling behavior.

### Open Question 3
- Question: How sensitive are CORE-CR and CORE-KL results to the choice of hyperparameters (r_bonus=0.4, λ_KL values)?
- Basis in paper: [inferred] The paper specifies fixed hyperparameters but provides no ablation study on how performance changes with different reward bonus values or KL coefficients.
- Why unresolved: Without sensitivity analysis, it is unclear whether these values are optimal or whether the method is robust to hyperparameter choices.
- What evidence would resolve it: Systematic ablation experiments varying r_bonus (e.g., 0.1–1.0) and λ_KL (e.g., 0.001–0.1) across multiple benchmarks.

### Open Question 4
- Question: Can CORE be extended to non-mathematical domains requiring conceptual reasoning (e.g., physics, legal reasoning)?
- Basis in paper: [explicit] The conclusion states: "We hope this work motivates further exploration of concept-centered training signals, not only in mathematics but also across domains where principled reasoning is essential."
- Why unresolved: Mathematical reasoning has unique properties (formal verification, clear concept definitions); other domains may require different approaches to concept extraction and alignment.
- What evidence would resolve it: Create concept-exercise mappings for physics or legal domains and evaluate whether CORE improves reasoning in those settings.

## Limitations
- Core method relies on curated textbook data and concept-quiz mappings that are not publicly available.
- Method depends on strong teacher models (Qwen2.5-72B-Instruct, GPT-4o) for quiz generation and validation, raising scalability concerns.
- KL regularization requires careful tuning of dynamic coefficient λ_KL, with no ablation on sensitivity to this hyperparameter.

## Confidence
- CORE-Base effectiveness (Medium-High): Improvements over baselines are consistent and statistically significant, but rely on high-quality concept-aligned quizzes that are not publicly available.
- CORE-CR trajectory replacement (Medium): The mechanism is clearly described and intuitively sound, but the 0.4 reward bonus and K=1 replacement ratio are not justified through ablation.
- CORE-KL distillation (Medium): Forward KL regularization is theoretically motivated and shows gains, but the claim that it "faithfully mimics" reasoning is difficult to verify without interpretability analysis.
- Generalization across benchmarks (High): Out-of-domain performance gains are robust and show the method transfers beyond the training distribution.
- Algorithm and verifier agnosticism (High): The method is described as agnostic, but this has only been demonstrated with GRPO and GPT-4o-based validators.

## Next Checks
1. Sanity check on gap diagnostic: Run the robust evaluation protocol (original + 3 permuted variants) on your base model to confirm the definition-application gap exists before applying CORE.
2. Ablation on intervention trigger: Compare triggering on all-failure vs. partial-failure (e.g., ≥50% incorrect) to understand sensitivity to intervention frequency.
3. Self-supervised replication: Using only your target model family (no external teachers), generate quizzes via self-verification (SC@21 filtering) and train CORE-CR to confirm intrinsic reinforcement works without distillation.