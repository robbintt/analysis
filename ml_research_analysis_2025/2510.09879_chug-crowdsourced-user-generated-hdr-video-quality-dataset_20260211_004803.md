---
ver: rpa2
title: 'CHUG: Crowdsourced User-Generated HDR Video Quality Dataset'
arxiv_id: '2510.09879'
source_url: https://arxiv.org/abs/2510.09879
tags:
- videos
- quality
- chug
- dataset
- video
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CHUG introduces the first large-scale UGC-HDR video quality dataset,
  comprising 5,992 videos (856 sources) and 211,848 subjective ratings from 700+ subjects
  via Amazon Mechanical Turk. Unlike existing HDR datasets focusing on professionally
  generated content, CHUG captures real-world UGC-HDR distortions through a bitrate
  ladder encoding strategy simulating social media compression.
---

# CHUG: Crowdsourced User-Generated HDR Video Quality Dataset

## Quick Facts
- **arXiv ID:** 2510.09879
- **Source URL:** https://arxiv.org/abs/2510.09879
- **Reference count:** 0
- **Primary result:** First large-scale UGC-HDR video quality dataset with 5,992 videos and 211,848 subjective ratings

## Executive Summary
CHUG introduces the first comprehensive dataset focused on User-Generated Content (UGC) HDR video quality assessment. Unlike existing HDR datasets that primarily feature professionally produced content, CHUG captures the diverse quality characteristics and distortions present in real-world UGC-HDR videos. The dataset was created through a crowdsourcing approach using Amazon Mechanical Turk, collecting subjective ratings from over 700 participants. The dataset includes 856 source videos compressed through a bitrate ladder strategy to simulate social media compression scenarios.

## Method Summary
The dataset was constructed using 856 source UGC-HDR videos encoded with a bitrate ladder strategy to simulate various compression levels typical of social media platforms. Subjective quality assessments were collected through Amazon Mechanical Turk, gathering 211,848 ratings from 700+ participants. The SUREAL method was employed to process subjective ratings and compute robust Mean Opinion Scores (MOS). The dataset covers diverse content types and quality levels, with particular focus on capturing real-world distortions and quality variations found in user-generated HDR content.

## Key Results
- Dataset contains 5,992 videos (856 sources) with 211,848 subjective ratings from 700+ subjects
- Reveals broad MOS distribution with diverse quality levels and high variance ratings
- Spatial complexity positively correlates with quality while extreme temporal complexity reduces perceived quality
- Resolution and bitrate significantly influence MOS with diminishing returns beyond 720p

## Why This Works (Mechanism)
The dataset works by capturing real-world UGC-HDR scenarios through a crowdsourcing approach that reflects actual viewing conditions and content diversity. The bitrate ladder encoding strategy realistically simulates social media compression artifacts, while the large participant pool ensures robust subjective quality assessment. The SUREAL processing method provides reliable MOS values that account for the inherent variability in crowdsourced ratings.

## Foundational Learning

### Crowdsourcing Quality Assessment
**Why needed:** Enables large-scale subjective quality evaluation at reasonable cost
**Quick check:** Verify inter-rater reliability metrics and comparison with lab-based studies

### Bitrate Ladder Encoding
**Why needed:** Simulates realistic compression scenarios encountered in social media platforms
**Quick check:** Confirm bitrate distribution matches target platform specifications

### SUREAL Processing
**Why needed:** Robustly handles variability in crowdsourced subjective ratings
**Quick check:** Validate MOS consistency across repeated rating sessions

## Architecture Onboarding

### Component Map
Source videos -> Bitrate ladder encoding -> MTurk distribution -> Subjective ratings collection -> SUREAL processing -> MOS values

### Critical Path
Source video selection → Bitrate ladder encoding → MTurk quality assessment → SUREAL MOS computation → Dataset compilation

### Design Tradeoffs
**Pros:** Large scale, realistic UGC scenarios, cost-effective crowdsourcing, comprehensive quality range
**Cons:** Potential MTurk sampling bias, limited control over viewing conditions, platform-specific encoding strategy

### Failure Signatures
- Low inter-rater reliability indicating inconsistent quality perception
- MOS compression indicating insufficient quality range capture
- Encoding artifacts dominating quality assessment

### First Experiments
1. Validate MOS consistency by comparing repeated ratings of identical content
2. Test bitrate-quality relationship across different content categories
3. Assess spatial/temporal complexity correlation with MOS across resolution levels

## Open Questions the Paper Calls Out
None

## Limitations
- Generalizability beyond specific UGC-HDR context remains uncertain
- MTurk sampling may introduce biases compared to controlled laboratory studies
- Causation between complexity metrics and quality requires further validation
- Encoding strategy may not capture all possible UGC-HDR compression artifacts

## Confidence
- **Core dataset claims:** High - methodology is well-documented and scale is substantial
- **Quality predictor findings:** Medium - require validation across different content types
- **Diminishing returns finding:** Low - needs cross-device validation
- **MTurk vs lab comparison:** Low - significant methodological differences

## Next Checks
1. Validate MOS predictions on an independent UGC-HDR test set
2. Conduct cross-platform comparison of quality perceptions between MTurk and controlled lab settings
3. Test spatial/temporal complexity quality correlations across different genre categories and lighting conditions