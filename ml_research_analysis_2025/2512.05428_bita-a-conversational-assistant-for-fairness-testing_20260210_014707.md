---
ver: rpa2
title: 'Bita: A Conversational Assistant for Fairness Testing'
arxiv_id: '2512.05428'
source_url: https://arxiv.org/abs/2512.05428
tags:
- fairness
- testing
- software
- bita
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Bita, a conversational assistant for fairness
  testing in AI systems. Bita integrates large language models with retrieval-augmented
  generation, grounding responses in fairness literature.
---

# Bita: A Conversational Assistant for Fairness Testing

## Quick Facts
- arXiv ID: 2512.05428
- Source URL: https://arxiv.org/abs/2512.05428
- Authors: Keeryn Johnson; Cleyton Magalhaes; Ronnie de Souza Santos
- Reference count: 40
- Primary result: Bita is a conversational fairness testing assistant that uniquely combines literature-grounded RAG with task-specific guidance, enabling non-experts to identify bias sources, evaluate test plans, and generate exploratory charters.

## Executive Summary
Bita is a conversational assistant designed to make fairness testing in AI systems accessible to practitioners without ML expertise. It combines large language models with retrieval-augmented generation, grounding responses in curated fairness literature. The system supports three tasks: identifying bias sources, evaluating test plans, and generating exploratory testing charters. Evaluated on real-world AI systems (sign language translator and Smart Lipstick app), Bita demonstrates its ability to detect fairness concerns, provide context-specific recommendations, and produce actionable testing guidance. Compared to 42 existing fairness testing tools, Bita uniquely offers conversational interaction, direct testing usage, and literature grounding.

## Method Summary
Bita uses a Retrieval-Augmented Generation (RAG) pipeline where user queries are vectorized via SentenceTransformer and matched against a fairness literature index. The retrieved context combines with user prompts to form structured inputs for LLMs (ChatGPT or Gemini), with guardrails to constrain responses to retrieved evidence. The system was evaluated on two real-world AI systems, using task-specific flows for bias detection, test plan evaluation, and charter generation. The knowledge base draws from fairness research surveys, tool documentation, and empirical studies, though exact corpus composition remains unspecified.

## Key Results
- Bita successfully identified fairness concerns in sign language translator (regional dialect coverage, signer diversity, accessibility)
- Test plan evaluation detected demographic coverage gaps in Smart Lipstick app testing
- Compared to 42 existing tools, Bita uniquely provides conversational interaction, direct testing usage, and literature grounding
- Generated actionable exploratory testing charters grounded in fairness principles

## Why This Works (Mechanism)

### Mechanism 1: Retrieval-Augmented Grounding Reduces Hallucination
RAG anchors LLM outputs to curated fairness literature, improving traceability and reducing fabricated guidance. User queries are vectorized via SentenceTransformer, matched against a fairness literature index, then combined with context to form structured prompts processed by ChatGPT or Gemini. Guardrails constrain responses to retrieved evidence scope. Break condition: If the fairness literature corpus lacks coverage for novel domain-specific fairness concerns, retrieval returns weak matches and LLM may generate generic or irrelevant guidance.

### Mechanism 2: Task Decomposition Operationalizes Abstract Fairness Concepts
Decomposing fairness testing into three structured tasks (bias identification, plan evaluation, charter generation) makes the activity executable for non-expert testers. Each task maps abstract fairness principles to concrete actions—listing sensitive attributes, checking demographic coverage gaps, producing structured testing prompts—reducing the cognitive distance between fairness theory and testing practice. Break condition: If testers provide incomplete or misleading system descriptions, Bita's task outputs will reflect those gaps.

### Mechanism 3: Conversational Interface Lowers Expertise Barriers
Natural language interaction reduces the need for ML expertise and fairness library configuration, enabling workflow integration. Multi-turn dialogue preserves context across interactions; testers communicate goals directly without scripting. Session history stored in SQL enables iterative refinement. Break condition: If fairness issues require quantitative metric interpretation or visual analysis, pure conversational output may be insufficient for practitioner decision-making.

## Foundational Learning

- **Concept: Retrieval-Augmented Generation (RAG)**
  - Why needed here: Bita's reliability depends on grounding LLM outputs in external documents rather than parametric knowledge alone.
  - Quick check question: Can you explain why a RAG system might still produce inaccurate responses despite having access to relevant documents?

- **Concept: Fairness Definitions and Their Context-Dependence**
  - Why needed here: The paper notes fairness criteria are often mutually incompatible; understanding this helps interpret Bita's recommendations appropriately.
  - Quick check question: Why might demographic parity and equalized odds give conflicting fairness assessments for the same model?

- **Concept: Exploratory Testing Charters**
  - Why needed here: One of Bita's three core tasks is generating these charters; understanding their structure is essential for evaluating output quality.
  - Quick check question: What components distinguish an exploratory testing charter from a traditional test case specification?

## Architecture Onboarding

- **Component map:** Frontend (web interface) -> Backend (RAG pipeline) -> Retrieval (SentenceTransformer + fairness literature index) -> LLM (ChatGPT/Gemini) -> Database (SQL for session history)
- **Critical path:** User describes system under test → Query vectorized and matched to corpus → Retrieved segments + user context form prompt → LLM generates response with guardrails → Response returned and logged with session data
- **Design tradeoffs:** Flexibility vs. consistency (dynamic model selection introduces output variability), Breadth vs. depth (general corpus may lack domain-specific guidance), Conversational simplicity vs. analytical depth (no integrated metric dashboards)
- **Failure signatures:** Generic recommendations unrelated to system context (retrieval corpus lacks relevant documents), Repeated identical guidance (prompt context not incorporating prior dialogue), Missing demographic subgroups (incomplete system description)
- **First 3 experiments:** 1) Replicate sign language translator scenario to verify dialect and signer diversity identification, 2) Submit test plan with demographic gaps to confirm Plan Check detection, 3) Compare charter quality with and without retrieval grounding

## Open Questions the Paper Calls Out
- How does Bita perform in real-world testing environments when used by practitioners with varying levels of ML and fairness expertise? (explicitly called out)
- What barriers prevent practitioners from adopting conversational fairness testing assistants like Bita in industrial workflows? (explicitly called out)
- To what extent do LLM-generated fairness recommendations introduce their own biases or hallucinations despite RAG grounding? (inferred from acknowledgment of hallucination minimization)
- How well does Bita generalize to fairness testing tasks beyond the two evaluated domains? (explicitly called out)

## Limitations
- Evaluation relies on only two real-world AI systems, limiting generalizability across diverse fairness testing scenarios
- Knowledge base composition is not fully specified, making it difficult to assess coverage completeness
- Conversational interface may prove insufficient for complex fairness analysis requiring quantitative metrics or visualizations
- Paper acknowledges fairness definitions are context-dependent and often mutually incompatible but doesn't provide guidance on resolving conflicts

## Confidence

- **High Confidence:** The mechanism of RAG grounding reducing hallucination (supported by direct textual evidence of implementation and clear failure modes)
- **Medium Confidence:** The task decomposition making fairness testing executable for non-experts (supported by demonstration on two systems but limited breadth of evaluation)
- **Medium Confidence:** The conversational interface lowering expertise barriers (plausible but not directly validated against alternative interfaces)

## Next Checks
1. Test Bita's performance across 5-10 additional AI system types (beyond sign language translation and cosmetics) to evaluate knowledge base coverage and task generalization
2. Conduct a controlled experiment comparing Bita's outputs with and without RAG grounding to quantify hallucination reduction and specificity improvements
3. Implement a formal usability study measuring the time and expertise required for non-ML practitioners to complete fairness testing tasks with Bita versus traditional fairness toolkits