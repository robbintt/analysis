---
ver: rpa2
title: Zero-shot Performance of Generative AI in Brazilian Portuguese Medical Exam
arxiv_id: '2507.19885'
source_url: https://arxiv.org/abs/2507.19885
tags:
- questions
- medical
- available
- performance
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated zero-shot performance of large language models
  (LLMs) and multimodal large language models (MLLMs) on Brazilian Portuguese medical
  residency exam questions, comparing results against human candidates. Six LLMs and
  four MLLMs were tested on 117 exam questions covering text, non-radiological images,
  and radiological images across five medical domains.
---

# Zero-shot Performance of Generative AI in Brazilian Portuguese Medical Exam

## Quick Facts
- arXiv ID: 2507.19885
- Source URL: https://arxiv.org/abs/2507.19885
- Reference count: 40
- Six LLMs and four MLLMs tested on 117 Brazilian Portuguese medical exam questions; Claude-3.5-Sonnet achieved 69.57% accuracy comparable to human candidates

## Executive Summary
This study evaluated zero-shot performance of large language models and multimodal large language models on Brazilian Portuguese medical residency exam questions. Six LLMs and four MLLMs were tested on 117 exam questions covering text, non-radiological images, and radiological images across five medical domains. Claude-3.5-Sonnet and Claude-3-Opus achieved accuracy levels comparable to human candidates, while performance dropped significantly on image-based questions. Qualitative analysis revealed high concordance for correct answers but frequent hallucinations in incorrect ones, highlighting risks of patient harm. The study underscores the need for further fine-tuning and multimodal reasoning improvements for non-English medical AI applications.

## Method Summary
The study tested six LLMs and four MLLMs on 117 Brazilian Portuguese medical residency exam questions (HCFMUSP 2023) in zero-shot fashion. Questions covered five medical domains with three modalities: text-only, non-radiological images, and radiological images. Models were accessed via Amazon Bedrock and Azure APIs, with standardized prompts requesting answer selection plus explanations. Five trials were conducted per model with shuffled question order. Performance was measured by accuracy percentage and processing time per question, with qualitative evaluation of explanations by clinical experts assessing concordance and safety.

## Key Results
- Claude-3.5-Sonnet achieved 69.57% accuracy, comparable to human candidates
- Performance dropped significantly on image-based questions compared to text-only
- Among incorrect answers, 87% had hallucinated explanations, highlighting patient safety risks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multimodal integration degrades performance relative to text-only inputs in medical reasoning tasks.
- Mechanism: Image encoders introduce additional error sources—visual feature extraction, cross-modal alignment, and spatial reasoning—that compound with language understanding. For underrepresented languages like Portuguese, vision-language alignment data is even sparser than English counterparts.
- Core assumption: The accuracy drop reflects genuine multimodal reasoning failures, not merely harder questions.
- Evidence anchors:
  - [abstract] "performance dropped significantly on image-based questions"
  - [section 3.2] "Model's performances were higher in text-only (Experiment 1) compared to text + image (Experiment 2)... Questions involving images, especially those with radiological content, were more challenging"
  - [corpus] Related paper on Brazilian Portuguese image captioning (arXiv:2602.00393) documents similar cross-modal challenges in low-resource settings

### Mechanism 2
- Claim: Cross-lingual transfer enables non-trivial Portuguese medical performance without language-specific fine-tuning.
- Mechanism: High-resource pretraining (predominantly English) creates latent representations that partially generalize to typologically related languages, especially in structured domains like medicine with consistent terminology.
- Core assumption: The exam questions were genuinely unseen during training (temporal holdout).
- Evidence anchors:
  - [abstract] Claude-3.5-Sonnet achieved "accuracy levels comparable to human candidates (69.57%)"
  - [section 1] "None of the models were exposed to the questions of this test during their training phase"
  - [corpus] MedPT dataset paper (arXiv:2511.11878) confirms translation-based approaches fail to capture clinical nuances, supporting the need for in-language evaluation

### Mechanism 3
- Claim: Explanation quality strongly correlates with answer correctness, creating a reliability signal.
- Mechanism: When models reach correct answers through valid reasoning chains, explanations reflect coherent latent knowledge. Incorrect answers often arise from flawed intermediate steps, producing hallucinated justifications.
- Core assumption: Expert evaluations of "concordance" validly capture reasoning coherence.
- Evidence anchors:
  - [abstract] "Qualitative analysis of model explanations by clinical experts revealed high concordance for correct answers but frequent hallucinations in incorrect ones"
  - [section 3.4/Table 3] Among correctly answered questions, ~94% had concordant explanations; among incorrect answers, ~87% had incorrect explanations
  - [corpus] Weak direct evidence; no corpus papers directly analyze explanation-reasoning coupling

## Foundational Learning

- **Zero-shot evaluation**:
  - Why needed here: The study tests models without task-specific training to assess generalization. Understanding this distinguishes inherent capability from fine-tuned performance.
  - Quick check question: If you fine-tuned a model on 100 Portuguese medical questions before testing on this exam, would it still be "zero-shot"? (No.)

- **Hallucination in clinical contexts**:
  - Why needed here: The study flags hallucinations as patient safety risks. Engineers must understand that plausible-sounding explanations can mask fundamentally wrong reasoning.
  - Quick check question: A model correctly answers "aspirin for chest pain" but explains it reduces "heart inflammation." Is this hallucination dangerous? (Yes—plausible but medically wrong.)

- **Multimodal fusion**:
  - Why needed here: Performance gaps between text-only and image-containing questions reflect how MLLMs combine vision and language encoders.
  - Quick check question: Why might radiological images underperform non-radiological images? (Higher spatial reasoning demands, less training data, finer-grained features required.)

## Architecture Onboarding

- **Component map**:
  - Input layer: Portuguese text tokenizer + image encoder (for MLLMs)
  - Fusion layer: Cross-attention between text embeddings and visual features
  - Reasoning layer: Transformer decoder generating answer + explanation
  - Output layer: Multiple-choice selection (4 options) + free-text justification

- **Critical path**:
  1. Question parsing (Portuguese NLU)
  2. Image encoding (if present)—vision transformer extracts features
  3. Cross-modal alignment—text tokens attend to image regions
  4. Answer generation—select from 4 options
  5. Explanation generation—produce coherent Portuguese justification

- **Design tradeoffs**:
  - Larger models (Claude-3-Opus) achieve higher accuracy but 3-5× longer processing time (24.68s vs. 5.48s for Haiku)
  - Text-only evaluation simpler but misses critical multimodal failures
  - Portuguese-specific fine-tuning may improve accuracy but risks overfitting to exam-style questions

- **Failure signatures**:
  - Radiological image questions: Accuracy drops 10-20% vs. text-only
  - Incorrect answers: 87% accompanied by hallucinated explanations
  - Domain-specific gaps: Surgery and internal medicine showed more variability across trials
  - Language-specific failures not directly measured (no English baseline comparison possible)

- **First 3 experiments**:
  1. Replicate Experiment 1 with a Portuguese-specific model (e.g., Sabiá-2 from corpus) to isolate language effects vs. model architecture effects.
  2. Add English translation of the same questions as a baseline to quantify language penalty directly (addressed as a limitation in the paper).
  3. Implement confidence thresholds: only output answers when model explanation concordance exceeds a calibrated threshold; measure safety-accuracy tradeoff.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can Retrieval-Augmented Generation (RAG) or specific fine-tuning mitigate the performance gap between text-only and image-based medical questions in Portuguese LLMs?
- Basis in paper: [explicit] The authors state that "Using Retrieval-Augmented Generation (RAG) strategies and other techniques could help to overcome some of the gaps detected here," specifically referencing the drop in accuracy on multimodal questions.
- Why unresolved: The study only evaluated zero-shot performance; it did not test whether augmenting the models with external knowledge bases or fine-tuning them on Portuguese medical data would improve visual reasoning capabilities.
- What evidence would resolve it: A comparative study benchmarking zero-shot performance against RAG-enhanced performance on the same 117 HCFMUSP exam questions, specifically analyzing accuracy improvements in the radiological and non-radiological image categories.

### Open Question 2
- Question: To what extent is the observed performance drop in medical image interpretation attributable to the Portuguese language barrier versus the inherent complexity of multimodal reasoning?
- Basis in paper: [explicit] The authors note they "could not provide a direct comparison between the same questions in portuguese and their English version" and admit the hypothesis that "other models can have a better performance in languages with less digital representation... remains indirect."
- Why unresolved: The study could not isolate the variable of language proficiency from the variable of clinical/multimodal difficulty because a translated control set was not used.
- What evidence would resolve it: An experiment running the same medical questions (text and images) through the models in both Brazilian Portuguese and English, using validated back-translation, to isolate performance variance caused by language.

### Open Question 3
- Question: Do the performance trends observed in general residency exams replicate in specialized Brazilian Portuguese exams where visual interpretation is the primary focus?
- Basis in paper: [explicit] The authors ask: "To what extend our conclusions can hold in other similar tests in portuguese (i.e. tests designed to evaluate medical expertise when images are critical – as in the case of radiology, pathology, dermatology and oftalmology specialty tests) is still open."
- Why unresolved: The current study utilized a general residency exam which includes a mix of text and images; the density and complexity of images in a specialized radiology or dermatology exam may yield different failure modes.
- What evidence would resolve it: Applying the same methodology (Experiment 1 and 2 protocols) to specialized board certification exams in Brazil (e.g., Colégio Brasileiro de Radiologia) to compare accuracy distributions against the generalist HCFMUSP results.

## Limitations
- Cannot isolate whether accuracy gaps reflect multimodal reasoning failures or clinical difficulty differences without English control group
- Qualitative explanation analysis lacks inter-rater reliability metrics and detailed methodology
- Commercial API usage prevents full transparency regarding model versions, temperature settings, and system prompts

## Confidence
- **High confidence**: Claims about Claude-3.5-Sonnet achieving accuracy comparable to human candidates (69.57%), and the general finding that multimodal performance lags text-only performance
- **Medium confidence**: Claims about explanation-quality correlation with correctness due to limited methodology detail on expert evaluation
- **Low confidence**: Claims about specific failure modes in radiological image reasoning without controlled difficulty matching or ablation studies

## Next Checks
1. **Language Penalty Quantification**: Conduct parallel evaluation of the same exam questions using English-language models to establish the baseline accuracy drop attributable to language transfer versus multimodal reasoning limitations.
2. **Expert Agreement Validation**: Replicate the qualitative explanation analysis with multiple independent clinical experts, calculating inter-rater reliability (Cohen's kappa) to validate the concordance metrics reported in Table 3.
3. **Modality Difficulty Control**: Design a follow-up experiment with matched clinical questions across text-only, non-radiological image, and radiological image formats to isolate whether performance gaps reflect true multimodal reasoning challenges versus inherent modality difficulty differences.