---
ver: rpa2
title: Game-Oriented ASR Error Correction via RAG-Enhanced LLM
arxiv_id: '2509.23630'
source_url: https://arxiv.org/abs/2509.23630
tags:
- correction
- error
- gaming
- data
- go-aec
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of automatic speech recognition
  (ASR) error correction in gaming scenarios, where domain-specific terminology, rapid
  speech, and environmental noise lead to high error rates. The proposed GO-AEC framework
  integrates large language models (LLMs), Retrieval-Augmented Generation (RAG), and
  a hybrid data augmentation strategy using LLMs and text-to-speech (TTS) to improve
  ASR accuracy.
---

# Game-Oriented ASR Error Correction via RAG-Enhanced LLM

## Quick Facts
- arXiv ID: 2509.23630
- Source URL: https://arxiv.org/abs/2509.23630
- Reference count: 19
- Key outcome: GO-AEC reduces CER by 6.22% and SER by 29.71% compared to baseline methods in gaming scenarios

## Executive Summary
The paper addresses automatic speech recognition (ASR) error correction in gaming environments where domain-specific terminology, rapid speech, and environmental noise lead to high error rates. The proposed GO-AEC framework integrates large language models (LLMs), Retrieval-Augmented Generation (RAG), and hybrid data augmentation to improve ASR accuracy. Experiments demonstrate significant improvements over baseline methods, with the framework achieving 3.65% CER compared to 6.84-8.94% for individual ASR services.

## Method Summary
GO-AEC uses a three-pronged approach: (1) hybrid data augmentation combining LLM-generated text expansion with TTS-synthesized speech under diverse acoustic conditions, (2) N-best hypothesis aggregation from multiple ASR services to provide complementary recognition candidates, and (3) RAG-enhanced correction using a dynamic knowledge base of error-to-correction mappings. The system fine-tunes a lightweight Qwen2.5-1.5B LLM on formatted prompts containing N-best candidates, game context, and retrieved KB entries. The training combines synthetic and real player data, while validation and testing use only real data to ensure realistic evaluation.

## Key Results
- Reduces character error rate (CER) from 6.84-8.94% (single ASR) to 3.65%
- Achieves 29.71% reduction in sentence error rate (SER) compared to baseline methods
- Ablation studies show each component contributes: RAG removal increases CER to 6.79%, N-best removal to 7.07%, and SFT removal to 7.07%

## Why This Works (Mechanism)

### Mechanism 1: N-best Hypothesis-based Error Resolution
The system collects N-best hypotheses from multiple ASR services, randomizes their order to avoid bias, and presents them to a fine-tuned LLM which learns to synthesize the most accurate transcription. Different ASR systems make uncorrelated errors on domain-specific terms, so aggregation provides complementary candidates that improve correction accuracy.

### Mechanism 2: RAG-Injected Domain Terminology Correction
A terminology knowledge base is constructed by analyzing substitution/insertion/deletion differences between ASR outputs and ground truth in training data. During inference, the RAG module queries this KB for matches in ASR output and injects relevant (correct word | erroneous word) pairs into the LLM prompt, guiding correction of domain-specific jargon.

### Mechanism 3: Hybrid Data Augmentation for Robustness
Domain-specific game terms are extracted and expanded via LLM to create diverse text. TTS converts this to speech with variations in timbre, speed, regional accents, and background noise. Synthetic data is merged with real player data for SFT training, enabling effective learning despite data scarcity in gaming scenarios.

## Foundational Learning

- **Concept: N-best Hypotheses in ASR** - Why needed: The framework's correction module requires understanding that ASR systems can output multiple ranked candidate transcriptions with different error profiles. Quick check: Can you explain why presenting multiple ASR candidates to an LLM might improve correction accuracy?
- **Concept: Retrieval-Augmented Generation (RAG)** - Why needed: The dynamic knowledge base module uses RAG to retrieve and inject relevant error patterns into prompts at inference time. Quick check: What is the difference between fine-tuning a model on domain knowledge versus retrieving that knowledge at inference time?
- **Concept: Supervised Fine-Tuning (SFT) Loss Functions** - Why needed: The framework trains the LLM to minimize token-level cross-entropy loss between predicted corrections and ground truth transcriptions. Quick check: Why might cross-entropy loss be preferred over other objectives for ASR error correction tasks?

## Architecture Onboarding

- **Component map:** Data Augmentation Pipeline -> N-best Hypothesis Aggregator -> RAG Knowledge Base -> SFT-Optimized LLM -> Inference Orchestrator
- **Critical path:** Real-time inference depends on latency of multiple ASR services, KB retrieval speed, and LLM inference time. The 1.5B parameter model choice prioritizes speed over capacity.
- **Design tradeoffs:** Model size vs. latency (Qwen2.5-1.5B chosen for real-time gaming), synthetic vs. real training data (training uses both, validation/test use only real), N-best count (3 services used, more adds latency with diminishing returns).
- **Failure signatures:** Over-correction (LLM may correct valid outputs), KB staleness (new terms not retrieved), correlated ASR errors (if all services fail on same term), latency spikes (multi-ASR + RAG + LLM may exceed real-time limits).
- **First 3 experiments:** 1) Establish baseline with vanilla ASR output on test set, 2) Systematically ablate each module (remove RAG, then N-best, then SFT) using Table IV methodology, 3) Validate on held-out game context to assess domain transfer.

## Open Questions the Paper Calls Out

1. How can the retrieval and generation efficiency of the dynamic knowledge base be optimized to strictly satisfy the latency constraints of real-time voice interactions?
2. Can the GO-AEC framework be effectively adapted to support multilingual gaming environments and code-switching scenarios?
3. How does the framework perform when extended to complex voice interactions with AI teammates rather than just command correction?

## Limitations

- Synthetic data fidelity is not empirically validated against real gaming acoustic environments
- RAG knowledge base construction lacks specification of retrieval mechanism and update frequency
- Correlation of ASR errors across services is not analyzed, potentially limiting N-best aggregation effectiveness

## Confidence

- **High Confidence:** Experimental results showing CER reduction from 6.84-8.94% to 3.65% with clear methodology and ablation studies
- **Medium Confidence:** Theoretical mechanisms (N-best aggregation, RAG injection, hybrid augmentation) are sound but depend on unspecified implementation details
- **Low Confidence:** Claims about real-time performance and scalability lack empirical validation and latency benchmarks

## Next Checks

1. Analyze the correlation matrix of ASR errors across the three services on the test set to quantify the independence assumption underlying the N-best aggregation mechanism.
2. Train GO-AEC with three variants (real data only, synthetic data only, hybrid) to isolate whether synthetic augmentation improves robustness or primarily serves to increase training volume.
3. Measure end-to-end latency (ASR inference + RAG retrieval + LLM correction) under varying loads and KB sizes, benchmarking against the 200ms target for real-time gaming voice chat.