---
ver: rpa2
title: 'Prot2Token: A Unified Framework for Protein Modeling via Next-Token Prediction'
arxiv_id: '2505.20589'
source_url: https://arxiv.org/abs/2505.20589
tags:
- protein
- prediction
- tasks
- ligands
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Prot2Token introduces a unified framework that converts diverse\
  \ protein prediction tasks\u2014ranging from sequence-level properties and residue-specific\
  \ attributes to complex protein interactions\u2014into a standardized next-token\
  \ prediction format. The core innovation lies in using an autoregressive decoder,\
  \ conditioned on embeddings from pre-trained protein encoders and guided by learnable\
  \ task tokens, to perform diverse predictions within a single architecture."
---

# Prot2Token: A Unified Framework for Protein Modeling via Next-Token Prediction

## Quick Facts
- arXiv ID: 2505.20589
- Source URL: https://arxiv.org/abs/2505.20589
- Reference count: 40
- Authors: Mahdi Pourmirzaei; Farzaneh Esmaili; Salhuldin Alqarghuli; Mohammadreza Pourmirzaei; Ye Han; Kai Chen; Mohsen Rezaei; Duolin Wang; Dong Xu
- Primary result: Unified framework converting diverse protein prediction tasks into next-token prediction format, achieving up to ~1000× faster inference than AlphaFold2 with MSA while matching or surpassing specialized methods

## Executive Summary
Prot2Token introduces a unified framework that converts diverse protein prediction tasks—ranging from sequence-level properties and residue-specific attributes to complex protein interactions—into a standardized next-token prediction format. The core innovation lies in using an autoregressive decoder, conditioned on embeddings from pre-trained protein encoders and guided by learnable task tokens, to perform diverse predictions within a single architecture. This design uniquely enables multi-task learning across five distinct task categories, simplifying model training and deployment. Extensive experimental validation shows that Prot2Token delivers up to ~1000× faster inference than AlphaFold2 with MSA on identical hardware for 3D structure prediction, while matching or surpassing specialized methods across other tasks such as stability, binding site, and kinase phosphorylation predictions.

## Method Summary
Prot2Token is an encoder-decoder architecture where protein (and optionally chemical) sequences are encoded by pre-trained bidirectional transformers (ESM2, BARTSmiles) and processed by an autoregressive decoder via cross-attention. The decoder generates output sequences token-by-token, conditioned on encoder embeddings and learnable task tokens that specify the prediction task. Outputs are tokenized into discrete sequences (digits for regression, class tokens for classification, sorted indices for binding sites) and predicted autoregressively. The framework supports self-supervised decoder pre-training on amino acid position prediction to improve spatially sensitive task performance. Training uses weighted next-token prediction cross-entropy with AdamW optimizer, cosine annealing schedule, and multi-task learning capabilities across five task categories.

## Key Results
- Achieves ~1000× faster inference than AlphaFold2 with MSA on identical hardware for 3D structure prediction
- Matches or surpasses specialized methods across diverse tasks: Spearman correlations above 0.9 for mutation stability, up to 76% F1 scores for protein-ligand binding sites
- Successfully unifies five distinct task categories (classification, regression, binding site, sequence-to-sequence, PTM) into a single framework
- Demonstrates effective multi-task learning, improving performance on low-support classes and stabilizing training

## Why This Works (Mechanism)

### Mechanism 1: Unified Next-Token Prediction via Universal Tokenization
Prot2Token converts diverse protein prediction outputs into standardized discrete token sequences (e.g., classification labels as single tokens, regression values as digit-by-digit sequences, binding sites as sorted residue indices) that a causal transformer decoder can generate autoregressively. Learnable task tokens condition the decoder to specify which prediction head to emulate for each sample.

### Mechanism 2: Encoder-Decoder Cross-Attention with Task Tokens
Frozen or fine-tuned pre-trained bidirectional protein encoders (ESM2) and optional chemical encoders (BARTSmiles) produce contextual embeddings that condition an autoregressive decoder via cross-attention. Task tokens, prepended to the output sequence and embedded via a learnable table, guide the decoder's generation without contributing to the loss, acting as prompts that steer the model toward task-specific behavior.

### Mechanism 3: Self-Supervised Decoder Pre-training for Spatial/Positional Inductive Biases
The decoder is pre-trained on predicting residue indices of specific amino acid types within sequences, learning to map encoder embeddings back to residue positions. This instills positional awareness and sequence-index mapping capabilities before supervised fine-tuning on binding site tasks, compensating for the lack of explicit positional inductive biases in a randomly initialized decoder.

## Foundational Learning

- **Autoregressive Language Models (e.g., GPT-style):** Prot2Token uses a causal transformer decoder that generates predictions token-by-token, conditioned on previous tokens and encoder context. Understanding autoregressive factorization and causal masking is essential.
  - Quick check: In Prot2Token's decoder, can position `t` attend to position `t+1`? Why or why not?

- **Cross-Attention in Encoder-Decoder Transformers:** The decoder attends not only to its own previous tokens but also to encoder outputs via cross-attention, integrating protein sequence representations into the generation process.
  - Quick check: What is the source of keys and values in the decoder's cross-attention layers in Prot2Token?

- **Multi-Task Learning with Task Prompts/Tokens:** Prot2Token uses learnable task tokens to condition the decoder for different prediction tasks within a single model, enabling parameter sharing and potential transfer.
  - Quick check: How does Prot2Token prevent the task token from directly contributing to the loss while still using it to guide generation?

## Architecture Onboarding

- **Component map:**
  - Protein Encoder (ESM2) -> Fusion Block -> Autoregressive Decoder <- Task Token Embedding Table
  - Chemical Encoder (BARTSmiles) -> Fusion Block (optional, for ligand tasks)

- **Critical path:**
  1. Input sequence tokenized by respective encoder tokenizer
  2. Encoder produces contextual embeddings
  3. Fusion block projects embeddings to decoder dimension
  4. Task token + `<BOS>` initiates decoder sequence
  5. Decoder generates output tokens one-by-one via greedy decoding, attending to encoder outputs via cross-attention and previous tokens via causal self-attention
  6. Output tokens are de-tokenized to produce final prediction

- **Design tradeoffs:**
  - Unified Tokenization vs. Specialized Heads: Simplifies architecture but requires output discretization, potentially losing fine-grained continuous precision
  - Greedy vs. Stochastic Decoding: Paper uses greedy decoding for determinism and simplicity, sacrificing potential diversity or uncertainty estimation
  - Self-Supervised Pre-training Overhead: Adds a pre-training stage but critical for binding site tasks; may be unnecessary for simpler classification/regression tasks
  - Encoder Fine-tuning Depth: Unfreezing more encoder layers improves performance but increases computational cost and risk of overfitting

- **Failure signatures:**
  - Incorrect Output Sequence Length: For sequence-to-sequence tasks (e.g., 3D structure), decoder may generate more/fewer tokens than input residues
  - Overfitting on Small/Low-Support Classes: Classification tasks with few samples per class lead to unstable training; multi-task learning can stabilize
  - Tokenizer Bottleneck: 3D structure prediction accuracy plateaus likely due to VQ-VAE tokenizer reconstruction ceiling

- **First 3 experiments:**
  1. **Regression Tokenization Ablation:** Compare digit-by-digit tokenization vs. binning on protein stability task, evaluating Spearman correlation
  2. **Task Token vs. No Task Token:** Train on DeepLoc localization and protein-ligand affinity with and without task tokens, comparing performance
  3. **Self-Supervised Pre-training Impact:** Train on protein-ligand binding task with and without amino-acid-position self-supervised pre-training, reporting F1 score improvement

## Open Questions the Paper Calls Out

- Can higher-fidelity discrete tokenizers for 3D structures be developed to overcome the reconstruction bottleneck currently limiting Prot2Token's structural accuracy?
- Can the Prot2Token framework be inverted to generate novel protein sequences conditioned on desired property tokens rather than just predicting properties from sequences?
- Is it feasible to train a single monolithic Prot2Token model on all five distinct task categories simultaneously without suffering from negative transfer or catastrophic forgetting?

## Limitations

- Universal tokenization ceiling: Discrete tokenization for continuous regression tasks may lose fine-grained information compared to direct regression heads
- Encoder representation bottleneck: Frozen pre-trained encoders may limit performance on tasks requiring specialized protein representations
- Self-supervised pre-training specificity: Amino-acid-position pre-training effectiveness may be task-specific and not generalize to all spatially sensitive tasks

## Confidence

- **High Confidence:**
  - Unified next-token prediction framework successfully converts diverse protein tasks into standardized format
  - Achieves significant inference speedup (~1000×) compared to AlphaFold2 with MSA
  - Architecture design is technically sound and reproducible
  - Multi-task learning within framework is feasible and can stabilize training

- **Medium Confidence:**
  - Matches or surpasses specialized methods across all reported tasks (performance varies by specific task)
  - Self-supervised decoder pre-training consistently improves spatially sensitive task performance
  - Unified tokenization approach maintains sufficient precision for all task types

- **Low Confidence:**
  - Framework can generalize to protein tasks beyond those tested without significant modifications
  - Specific choice of pre-trained encoders is optimal for all task types
  - 1000× speedup claim holds across different hardware configurations and protein lengths

## Next Checks

1. **Tokenizer Precision Evaluation:** Conduct controlled experiment comparing Prot2Token's digit-by-digit tokenization for regression tasks against direct regression heads on same architecture, measuring precision loss.

2. **Cross-Encoder Generalization Test:** Replace ESM2 with different pre-trained protein encoder (e.g., MSA Transformer) in Prot2Token framework and evaluate performance degradation on 2-3 representative tasks.

3. **Task Token Effectiveness Validation:** Train Prot2Token on two distinct tasks without task tokens (using separate decoders or task-specific heads) and compare performance to unified version with task tokens.