---
ver: rpa2
title: FairEquityFL -- A Fair and Equitable Client Selection in Federated Learning
  for Heterogeneous IoV Networks
arxiv_id: '2509.20193'
source_url: https://arxiv.org/abs/2509.20193
tags:
- clients
- client
- selection
- fairequityfl
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FairEquityFL, a federated learning framework
  designed to ensure equitable client participation in heterogeneous IoV networks.
  The framework addresses fairness challenges in client selection by introducing a
  sampling equalizer module that tracks and controls client participation, preventing
  frequent participation by some clients while ensuring others are not overlooked.
---

# FairEquityFL -- A Fair and Equitable Client Selection in Federated Learning for Heterogeneous IoV Networks

## Quick Facts
- arXiv ID: 2509.20193
- Source URL: https://arxiv.org/abs/2509.20193
- Reference count: 32
- Key outcome: Achieved Jain's Fairness Index of 0.816 vs. 0.539-0.510 baselines, 70.86% accuracy in 34 rounds vs. 100 rounds for FedAvg

## Executive Summary
This paper introduces FairEquityFL, a federated learning framework designed to ensure equitable client participation in heterogeneous IoV networks. The framework addresses fairness challenges in client selection by introducing a sampling equalizer module that tracks and controls client participation, preventing frequent participation by some clients while ensuring others are not overlooked. An outlier detection mechanism identifies and temporarily suspends suspicious clients based on performance fluctuations. Evaluated on the FEMNIST dataset, FairEquityFL achieved a Jain's Fairness Index of 0.816 compared to baseline models (0.539-0.510), reached 70.86% accuracy in 34 rounds versus 100 rounds for FedAvg, and converged in 2.23 hours compared to 2.59-2.26 hours for baselines. The framework also demonstrated superior loss minimization and effective outlier detection while maintaining system security and reliability.

## Method Summary
FairEquityFL implements a client selection framework with procedural fairness constraints and outlier detection. The sampling equalizer module enforces participation bounds (NCmin < Ci < NCmax), minimum reselection gaps (Gapmin), and maximum oversight periods (Gapmax). Clients exceeding Gapmax are prioritized for re-selection, while λ-interval checks identify un-utilized clients. Outlier detection monitors accuracy/loss fluctuations, suspending clients exceeding Acc_th% accuracy drop or Loss_th% loss increase for STn rounds. The framework was evaluated on FEMNIST with heterogeneous client distribution, comparing against FedAvg, FedProx, CSFedAvg, and Newt baselines across Jain's Fairness Index, accuracy, convergence time, and loss metrics.

## Key Results
- Achieved Jain's Fairness Index of 0.816 compared to baseline models (0.539-0.510)
- Reached 70.86% accuracy in 34 rounds versus 100 rounds for FedAvg
- Converged in 2.23 hours compared to 2.59-2.26 hours for baselines
- Successfully identified and suspended suspicious clients while maintaining system security and reliability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Constraining client participation frequency improves fairness metrics without degrading convergence speed.
- Mechanism: The sampling equalizer module enforces hard bounds—each client has a maximum participation count (NCmax), minimum participation guarantee (NCmin), minimum gap between selections (Gapmin), and maximum allowable oversight period (Gapmax). These constraints are enforced procedurally in Algorithm 1 (lines 15-18, 22-27), prioritizing overlooked clients and filtering those too recently selected.
- Core assumption: Fairness improvements do not require optimizing for client utility; uniform participation opportunity is sufficient as a proxy.
- Evidence anchors:
  - [abstract] "sampling equalizer module that tracks and controls client participation, preventing frequent participation by some clients while ensuring others are not overlooked"
  - [section 3, Eq. 5] "NCmin < Ci < NCmax"
  - [corpus] Related work (pFedFair, Fairness Regularization in FL) explores fairness-accuracy trade-offs, suggesting this is an active design axis; corpus does not validate the specific constraint-based approach.
- Break condition: If client availability is highly volatile (frequent dropouts), rigid Gapmax enforcement may force selection of unavailable clients, causing round failures.

### Mechanism 2
- Claim: Prioritized re-selection of long-overlooked clients prevents starvation and improves Jain's Fairness Index.
- Mechanism: When multiple clients exceed Gapmax, the algorithm selects up to m clients in descending order of gap length (Algorithm 1, lines 11-20). This creates a soft priority queue based on neglect duration.
- Core assumption: Clients have comparable data quality; neglect-based prioritization does not systematically exclude lower-utility clients that could improve model accuracy.
- Evidence anchors:
  - [abstract] "ensuring others are not overlooked"
  - [section 3, p. 8] "if there are multiple clients reaching Gapmax threshold, the model would only select m number of such clients in a single round... the higher the gap is, the faster they will be selected"
  - [corpus] FeDa4Fair addresses client-level fairness evaluation but does not test neglect-based prioritization directly.
- Break condition: If a client's data distribution is adversarial or extremely noisy, forced re-selection could inject harmful updates.

### Mechanism 3
- Claim: Performance-fluctuation-based outlier detection identifies suspicious clients, improving loss minimization.
- Mechanism: The system monitors per-client accuracy and loss changes across rounds. If a client's accuracy drops by Acc_th% or loss increases by Loss_th% for Xn consecutive participations, the client is suspended for STn rounds (section 3, p. 9).
- Core assumption: Malicious or low-quality clients exhibit detectable performance instability; legitimate clients have stable or improving metrics.
- Evidence anchors:
  - [abstract] "outlier detection mechanism identifies and temporarily suspends suspicious clients based on performance fluctuations"
  - [section 4.5, p. 13] "FairEquityFL successfully identified the clients reaching the threshold and temporarily suspend those suspicious clients"
  - [corpus] MURIM proposes reputation-based mechanisms for reliability assessment but uses different signals; no direct corpus validation of fluctuation thresholds.
- Break condition: Legitimate clients with naturally noisy data (e.g., edge devices with variable sensor quality) may be incorrectly flagged and suspended.

## Foundational Learning

- Concept: **Jain's Fairness Index (JFI)**
  - Why needed here: The paper uses JFI as the primary fairness metric, incorporating participation frequency weighted by data quality. Understanding JFI's range [0, 1] and its sensitivity to distribution skew is essential for interpreting Table 1 results.
  - Quick check question: If all clients participate exactly once, what JFI value results? (Answer: 1.0, assuming equal data quality)

- Concept: **Client Selection Constraints in FL**
  - Why needed here: FairEquityFL's core innovation is procedural constraint enforcement rather than utility-based optimization. Distinguishing this from approaches like Oort (utility-maximizing) clarifies the design philosophy.
  - Quick check question: What happens if NCmax × K < total rounds × clients-per-round? (Answer: The constraint becomes infeasible; some rounds will have fewer participants or constraints must be relaxed)

- Concept: **Outlier Detection via Performance Metrics**
  - Why needed here: The mechanism relies on threshold-based detection (Acc_th%, Loss_th%). Understanding the trade-off between detection sensitivity and false positive rate is critical for tuning.
  - Quick check question: If Acc_th is set too low (e.g., 1%), what failure mode occurs? (Answer: Legitimate clients with minor accuracy variance may be incorrectly suspended, reducing fairness)

## Architecture Onboarding

- Component map:
  - FL Server -> Selector -> Sampling Equalizer Module -> Client Tracker Records
  - Selector -> Outlier Detector
  - FL Server aggregates model updates via FedAvg-style aggregation

- Critical path:
  1. Round k begins → Selector queries available client pool
  2. Sampling equalizer checks λ-interval for un-utilized clients (lines 3-9)
  3. Sampling equalizer identifies Gapmax-exceeding clients (lines 11-20)
  4. Remaining slots filled from Gapmin-eligible clients via random selection (lines 22-31)
  5. Selected clients train locally, return updates
  6. Outlier detector evaluates returning clients; suspends flagged clients
  7. Client tracker records updated (lines 33-39)

- Design tradeoffs:
  - **Fairness vs. convergence speed**: Tight Gapmin increases fairness but may exclude high-utility clients, slowing accuracy gains
  - **Security vs. fairness**: Aggressive outlier detection improves robustness but reduces participation equity (acknowledged in section 4.5)
  - **Constraint complexity vs. implementation overhead**: Multiple interacting thresholds (Gapmin, Gapmax, NCmax, NCmin, λ, m, λmax) require careful tuning per deployment

- Failure signatures:
  - **Starvation despite Gapmax**: If NCmax is reached early for many clients, later rounds may have insufficient eligible clients
  - **Cascading suspensions**: If a global model degradation causes widespread client accuracy drops, outlier detector may suspend large portions of the pool
  - **Fairness metric gaming**: Clients could deliberately underperform to avoid suspension while maintaining participation counts

- First 3 experiments:
  1. **Baseline JFI comparison**: Replicate Table 1 by running FairEquityFL vs. FedAvg/FedProx/CSFedAvg/Newt on FEMNIST with identical client splits; verify JFI improvement holds across 5 random seeds
  2. **Ablation on constraint tightness**: Systematically vary Gapmin (1, 3, 5 rounds) and Gapmax (5, 10, 15 rounds) while measuring JFI, accuracy at round 50, and convergence time; identify Pareto frontier
  3. **Outlier detection threshold sensitivity**: Inject synthetic malicious clients (random label flipping) at 5-20% prevalence; sweep Acc_th (5%, 10%, 20%) and measure true positive rate, false positive rate, and final model accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the framework definitively confirm the level of maliciousness of the suspicious clients, rather than relying on temporary suspension?
- **Basis in paper:** [explicit] The Conclusion states, "further research is required to confirm the level of maliciousness of the suspicious clients" and outlines an intention to "build an intelligent and robust security mechanism."
- **Why unresolved:** The current implementation relies on heuristic performance fluctuations (accuracy/loss) to flag clients, which suspects intent but does not verify it.
- **What evidence would resolve it:** Development of a secondary validation protocol or trust scoring system that distinguishes between a poorly performing client and a truly malicious one.

### Open Question 2
- **Question:** Is the outlier detection mechanism robust against "stealthy" attacks (e.g., backdoor attacks) that maintain high accuracy on the main task?
- **Basis in paper:** [inferred] The paper defines the outlier detection mechanism solely based on "considerable fluctuation in either accuracy or loss minimization."
- **Why unresolved:** Sophisticated adversaries may perform model poisoning or backdoor attacks while maintaining normal accuracy levels to evade detection, a scenario not addressed by the current fluctuation-based logic.
- **What evidence would resolve it:** Evaluation of FairEquityFL against attacks specifically designed to preserve global accuracy metrics while compromising the model.

### Open Question 3
- **Question:** How does FairEquityFL perform in highly dynamic IoV environments where clients may drop out mid-training or move between network zones?
- **Basis in paper:** [inferred] The paper claims to address "dynamic and heterogeneous IoV networks," but the experimental evaluation relies entirely on the FEMNIST dataset, which lacks the mobility and transient connectivity profiles of real vehicular networks.
- **Why unresolved:** The sampling equalizer assumes a relatively stable client pool history; it is unclear how the algorithm handles the state resets or sudden unavailability inherent to fast-moving vehicles.
- **What evidence would resolve it:** Simulation results using datasets or traces that specifically model vehicular mobility, high churn rates, and intermittent connectivity.

## Limitations
- Evaluation relies on FEMNIST dataset which lacks IoV-specific characteristics like mobility patterns and network churn
- Exact configuration parameters (NCmax, Gapmin, Gapmax, threshold values) are not specified, making exact replication challenging
- Relationship between fairness gains and model accuracy is not explored across varying constraint configurations

## Confidence
- **High Confidence**: The fairness mechanism's procedural implementation (Algorithm 1) is clearly specified and internally consistent. The outlier detection framework is technically sound.
- **Medium Confidence**: The reported performance metrics (JFI = 0.816, 70.86% accuracy in 34 rounds) are plausible given the described methodology, but parameter sensitivity is not explored.
- **Low Confidence**: The claim that this framework is specifically optimized for "heterogeneous IoV networks" lacks empirical validation - the FEMNIST dataset does not capture IoV-specific characteristics like mobility patterns or network churn.

## Next Checks
1. **Parameter Sensitivity Analysis**: Systematically vary Gapmin (1-5 rounds) and Gapmax (5-15 rounds) while measuring JFI, accuracy, and convergence time to identify optimal configurations for different data distributions.
2. **Real IoV Simulation**: Deploy the framework on a mobility-simulation dataset (e.g., synthetic vehicular traces) to validate performance under realistic client availability patterns and network conditions.
3. **Outlier Detection Calibration**: Test the threshold-based detection with controlled malicious client injection (5-20% prevalence) to measure true positive rate, false positive rate, and impact on model accuracy under varying threshold settings.