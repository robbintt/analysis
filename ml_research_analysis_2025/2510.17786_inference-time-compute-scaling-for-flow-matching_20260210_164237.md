---
ver: rpa2
title: Inference-Time Compute Scaling For Flow Matching
arxiv_id: '2510.17786'
source_url: https://arxiv.org/abs/2510.17786
tags:
- compute
- noise
- scaling
- search
- dmfm-ode
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces inference-time compute scaling methods for
  flow matching (FM) that preserve the linear interpolant, addressing a gap in FM's
  application to domains beyond image generation. The authors propose a diversity-maximizing
  noise schedule (DMFM-ODE) that injects time-decaying noise orthogonal to the score
  direction, combined with a two-stage inference algorithm (RS+NS-DMFM-ODE) that first
  performs random search over initial noise conditions, then refines trajectories
  through noise search.
---

# Inference-Time Compute Scaling For Flow Matching

## Quick Facts
- arXiv ID: 2510.17786
- Source URL: https://arxiv.org/abs/2510.17786
- Reference count: 40
- Primary result: Inference-time compute scaling methods for flow matching that preserve linear interpolant, achieving up to 0.9 DINO top-1 accuracy on ImageNet at 8× compute and TM-scores above 0.9 on unconditional protein generation.

## Executive Summary
This paper addresses the gap in inference-time compute scaling for flow matching (FM) by introducing methods that preserve FM's linear interpolant. The authors propose a diversity-maximizing noise schedule (DMFM-ODE) that injects time-decaying noise orthogonal to the score direction, combined with a two-stage inference algorithm (RS+NS-DMFM-ODE) that first performs random search over initial noise conditions, then refines trajectories through noise search. Experiments demonstrate consistent quality improvements as compute scales on ImageNet generation and show generalizability by achieving high TM-scores on unconditional protein generation, with the two-stage method showing clear advantages.

## Method Summary
The method introduces three inference algorithms for FM compute scaling: Random Search (Best-of-N), Noise Search (multi-round trajectory refinement with stochastic branching from intermediate points), and RS+NS (two-stage combining both). DMFM-ODE noise is defined as dx_t = (u_t + λw_t)dt, where w_t is time-decaying, score-orthogonal noise with optional particle guidance. The two-stage approach exploits FM's prior distribution invariance by independently optimizing initial noise conditions (RS) and trajectory refinement (NS). The method is evaluated on ImageNet 256×256 generation using SiT-XL/2 and unconditional protein structure generation using FoldFlow2, with verifier-guided search across compute budgets from 1× to 8×.

## Key Results
- Consistent quality improvements as compute scales on ImageNet generation, achieving up to 0.9 DINO top-1 accuracy at 8× compute
- Generalizability demonstrated by achieving TM-scores above 0.9 on unconditional protein generation
- Two-stage method (RS+NS) shows clear advantages over single-stage alternatives
- DMFM-ODE noise injection expands the diversity-quality Pareto frontier compared to standard SDE and ODE-score-orthogonal methods

## Why This Works (Mechanism)

### Mechanism 1
Two-stage search exploiting FM's prior invariance yields higher quality samples than one-stage alternatives. Random Search first identifies promising initial noise vectors (exploration), then Noise Search refines trajectories from those starts via iterative multi-round stochastic branching and verifier selection (exploitation). FM's invariance to the initial distribution supports this decoupling.

### Mechanism 2
Time-decayed score-orthogonal noise injection enables more diversity before quality collapses, improving search scaling. Injecting noise orthogonal to the score direction with linear time decay expands the diversity-quality Pareto frontier, permitting higher stochasticity for broader exploration during Noise Search.

### Mechanism 3
Verifier-guided multi-round Noise Search improves sample quality monotonically with compute, plateauing at higher budgets. Starting from saved intermediate states, branching N trajectories per round with noise injection, retaining top-K by verifier score, and continuing from their checkpoints across multiple timesteps focuses computation on promising branches.

## Foundational Learning

- **Flow Matching with Linear Interpolant**: Essential to understand how the linear path xt = (1-t)x0 + t*x1 defines the target velocity field, which underpins why noise injection must be carefully designed.
  - Quick check: How does the linear path xt = (1-t)x0 + t*x1 define the target velocity field?

- **Verifier-Guided Search**: Both Random Search and Noise Search rely on a verifier function r(·) to select samples; understanding verifier choice and alignment is crucial for interpreting results.
  - Quick check: If the verifier rewards spurious features, will compute scaling improve true quality?

- **Stochastic Interpolants and Schedule Reinterpretation**: The paper situates FM within a broader stochastic interpolant framework where schedules can be modified at inference without retraining; this underpins DMFM-ODE and noise injection.
  - Quick check: Can a model trained with linear FM be sampled with a different schedule without retraining? What constraints apply?

## Architecture Onboarding

- **Component map**: Pretrained FM backbone (velocity field vθ) -> Score estimator (analytically derived from velocity) -> DMFM-ODE noise injector -> Noise Search scheduler (defines round start times and branching factors) -> Verifier module -> Two-stage coordinator

- **Critical path**:
  1. Draw initial noise(s)
  2. Optionally run Random Search to identify promising x0 candidates
  3. For each Noise Search round: branch trajectories from saved checkpoints, inject DMFM noise, complete to t=1, evaluate with verifier, retain top-K
  4. Return top samples after final round

- **Design tradeoffs**:
  - Diversity vs quality: Higher noise magnitude improves exploration but risks quality drop; DMFM-ODE aims to push this Pareto frontier
  - Compute allocation: More rounds and larger N improve results but with diminishing returns
  - Verifier choice: Task-aligned verifiers (DINO for images, TM-score for proteins) yield better scaling than generic metrics

- **Failure signatures**:
  - Plateau at low compute: verifier may be weakly correlated or noise magnitude too low
  - Quality collapse at high noise: orthogonal projection errors or insufficient decay
  - Domain mismatch: geometric verifiers for proteins did not correlate with designability

- **First 3 experiments**:
  1. Baseline profiling - run deterministic ODE vs Random Search vs Noise Search (single-stage) at 1×, 2×, 4× compute on ImageNet with DINO verifier; record IS/FID/DINO
  2. Pareto curve - vary noise magnitude for DMFM-ODE vs SDE vs ODE-score-orth; plot FID vs diversity to verify frontier expansion
  3. Two-stage ablation - compare RS-only, NS-only, and RS+NS on FoldFlow protein generation with TM-score verifier at 1–8× compute; check for TM>0.9 at high budget

## Open Questions the Paper Calls Out

### Open Question 1
Can inference-time compute scaling be effectively applied to flow matching tasks where the prior distribution is a complex dataset (e.g., cell trajectories) rather than a simple Gaussian? The proposed Noise Search algorithm optimizes trajectories based on the initial condition; its effectiveness when the initial condition is a structured data point rather than noise is unknown.

### Open Question 2
Can models be explicitly trained to better leverage inference-time compute, analogous to "chain-of-thought" training in LLMs, to overcome the current plateau in sample quality improvements? This work relies on frozen, pretrained models which were not optimized for iterative search, limiting the returns on additional computation.

### Open Question 3
Why does increasing trajectory diversity via noise injection benefit protein design more than image generation, and how can the exploration-exploitation balance be optimized for specific domains? The authors speculate that finding diverse samples is crucial for exploration, but the specific geometric or topological properties of the protein manifold that favor this diversity remain unidentified.

## Limitations
- Verifier alignment across domains remains unvalidated: DINO correlates well with ImageNet diversity for FM but TM-score does not guarantee protein designability
- Noise-orthogonal projection quality depends on accurate score estimation; errors in velocity-to-score transformation propagate to DMFM-ODE noise
- DMFM-ODE claims improved Pareto frontier, but gains over SDE in ImageNet may be modest

## Confidence
- **High**: DMFM-ODE preserves linear interpolant and provides an inference-time scaling method; experimental evidence on ImageNet and protein generation is solid
- **Medium**: Claim that DMFM-ODE noise injection improves diversity without degrading quality, as Pareto gains are demonstrated but domain-specific performance varies
- **Low**: Assertion that orthogonal noise + time decay consistently improves search scalability, since gains over SDE are modest and TM-score verifier alignment is questionable

## Next Checks
1. Verify score-orthogonal projection numerically during sampling: check s_t · w_t ≈ 0 and no unintended density shifts
2. Benchmark DMFM-ODE against SDE baselines on ImageNet with multiple verifiers (DINO, IS, FID) to confirm Pareto frontier expansion
3. Test protein generation with TM-score verifier and additional metrics (designability, binding affinity) to confirm TM-score correlates with biological utility