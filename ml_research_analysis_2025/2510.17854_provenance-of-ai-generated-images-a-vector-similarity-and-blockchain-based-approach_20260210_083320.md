---
ver: rpa2
title: 'Provenance of AI-Generated Images: A Vector Similarity and Blockchain-based
  Approach'
arxiv_id: '2510.17854'
source_url: https://arxiv.org/abs/2510.17854
tags:
- image
- images
- embedding
- ai-generated
- vector
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of detecting AI-generated images
  amid rapid advancements in generative AI models like DALL-E and Stable Diffusion.
  The authors propose EmbedAIDetect, a training-free system that uses image embeddings
  from Vision Transformer models and vector similarity search to distinguish AI-generated
  images from human-created ones.
---

# Provenance of AI-Generated Images: A Vector Similarity and Blockchain-based Approach

## Quick Facts
- arXiv ID: 2510.17854
- Source URL: https://arxiv.org/abs/2510.17854
- Reference count: 25
- Key outcome: EmbedAIDetect achieves 99.51% accuracy using CLIP embeddings; DINOv2 shows superior robustness to image perturbations

## Executive Summary
This paper introduces EmbedAIDetect, a training-free system for detecting AI-generated images through embedding similarity and blockchain-based verification. The approach leverages vision transformer models to encode images into vector embeddings, where AI-generated images cluster distinctly from human-created images. By comparing cosine distances to nearest neighbors in AI and human collections, the system classifies images with high accuracy. A blockchain layer provides tamper-resistant provenance verification through cryptographic hash storage. Experiments demonstrate CLIP's exceptional accuracy and DINOv2's robustness against common image manipulations, while highlighting practical limitations of blockchain integration for large-scale deployment.

## Method Summary
The method uses pre-trained vision transformers to encode images into embeddings, then classifies images by comparing cosine distances to nearest neighbors in AI and human image collections. Five models are evaluated: CLIP, ViT, DINOv2, ResNet-50, and AIMv2. The system employs ChromaDB for vector storage and similarity search, with optional blockchain verification via Ethereum smart contracts. No model training is required—classification relies entirely on nearest-neighbor matching in embedding space. The approach also includes perturbation analysis to assess robustness against common image manipulations.

## Key Results
- CLIP achieved 99.51% classification accuracy on the 9,000 AI / 6,074 human dataset
- DINOv2 maintained 88.44% accuracy even at 80% blur intensity, demonstrating superior robustness
- Blockchain integration faces practical scalability challenges—many images returned "inconclusive" due to gas costs and transaction failures

## Why This Works (Mechanism)

### Mechanism 1
AI-generated images cluster distinctly from human-created images in high-dimensional embedding space, enabling classification via proximity comparison. Pre-trained vision transformer models encode images into vector embeddings; AI-generated images exhibit closer cosine distance to other AI embeddings than to human-created image embeddings. Classification follows: AI or not(x) = 1 if d(x,A) ≤ d(x,H). Core assumption: AI image generators imprint consistent, detectable patterns in the embedding space that generalize across generation methods.

### Mechanism 2
Embedding representations retain sufficient semantic similarity under common image manipulations to maintain correct matching. Vision transformers capture global semantic structure rather than pixel-level details, so moderate perturbations shift embeddings minimally in the vector space. Core assumption: Manipulations preserve enough semantic content for the embedding to remain closer to its original source than to unrelated images.

### Mechanism 3
Storing 256-bit cryptographic hashes of embeddings on Ethereum provides tamper-proof provenance verification. Embedding → hash → smart contract storage. Later, re-compute hash and query contract for existence verification. If match, image provenance is cryptographically attested. Core assumption: The same image processed through the same embedding model will always produce the same embedding, enabling consistent hash verification.

## Foundational Learning

- Concept: Vector embeddings and cosine similarity
  - Why needed here: The entire detection system relies on comparing image embeddings via cosine distance. Without understanding that embeddings are high-dimensional vector representations and cosine similarity measures angular distance, the classification logic won't make sense.
  - Quick check question: Given two vectors [1,0] and [0,1], is their cosine distance 0, 1, or something else?

- Concept: Vision Transformers (ViT) vs. CNN architectures
  - Why needed here: DINOv2, CLIP, and Google ViT are transformer-based; ResNet-50 is CNN-based. The paper shows transformers generally outperform ResNet-50 on robustness (88.44% vs. 24.67% at 80% blur). Understanding why transformers capture more global semantic structure helps explain this performance gap.
  - Quick check question: How does a ViT process an image into a sequence of tokens?

- Concept: Blockchain gas costs and smart contract storage efficiency
  - Why needed here: The paper quantifies that uint256 storage is 2.7× more gas-efficient than string for 256-bit hashes. Scalability of the provenance system depends on minimizing per-verification cost.
  - Quick check question: Why does storing a 256-bit value as a string cost more gas than as a uint256 on Ethereum?

## Architecture Onboarding

- Component map: User Interface -> DINOv2 embedding model -> ChromaDB vector database -> Classification logic -> Blockchain verification (optional)
- Critical path: Image upload → DINOv2 embedding (GPU, patch-level hidden states averaged) → Query ChromaDB for top-k nearest neighbors in both collections → Compare minimum distances → Classify as AI or human → (Optional) Compute 256-bit hash → Query smart contracts → Return on-chain verifiability status
- Design tradeoffs:
  - Framework 1 (Blockchain-only): Immutable provenance but poor scalability—gas costs and transaction throughput limit large datasets. Many images return "inconclusive" if not pre-registered.
  - Framework 2 (Vector DB-only): Fast, generalizable classification but no cryptographic provenance. Accuracy degrades on out-of-distribution images (e.g., highly realistic StyleGAN faces).
  - Framework 3 (Hybrid): Combines both; classification from vector similarity, trust from blockchain. Inherits limitations of both: false negatives on unregistered images, plus DB classification errors on edge cases.
  - Embedding model choice: DINOv2 (768-dim) most robust to blur/patches; CLIP (512-dim) highest raw accuracy but more sensitive to heavy blur. ResNet-50 (2048-dim) offers no robustness advantage despite larger dimension.
- Failure signatures:
  1. Blockchain verification returns false-negative: Image matches in ChromaDB but hash not stored on-chain (due to gas limits, transaction failures, or nonce conflicts). Result: "verifiable: false" even for correct classification.
  2. Misclassification of realistic synthetic faces: 140k Faces dataset—9,764/10,000 fake faces misclassified as "real." Embedding space biased toward synthetic features; real faces with uniform lighting resemble StyleGAN outputs.
  3. Performance collapse under extreme blur: CLIP drops to 30.02% at 80% blur; ResNet-50 drops to 24.67%. Semantic structure insufficient for matching at high perturbation levels.
- First 3 experiments:
  1. Baseline accuracy benchmark: Load the 9,000 AI / 6,074 human dataset, compute embeddings with all five models, run 4:1 train/test split, report accuracy/precision/recall. Validate that your implementation reproduces Table I results (CLIP ~99.51%).
  2. Perturbation robustness test: Apply the six manipulations (1 patch, 3-5 patches, resize, 20%/40%/60%/80% blur) to the AI image set. Measure whether perturbed images' embeddings still return the correct original image as top-1 match. Compare DINOv2 vs. ResNet-50 degradation curves.
  3. Blockchain gas profiling: Deploy HashStorageInt and HashStorageStr contracts to a local Hardhat network. Store 200+ hashes and measure gas consumption per transaction. Confirm ~36K gas for uint256 vs. ~97K gas for string. Test throughput limits (transactions per second before nonce conflicts).

## Open Questions the Paper Calls Out
None

## Limitations
- Blockchain integration faces practical scalability challenges—gas costs and transaction throughput limit large-scale deployment
- System accuracy may be inflated by dataset homogeneity (all AI images from Stable Diffusion 3.5, human images are art-style)
- Performance degrades significantly on highly realistic synthetic faces, with 97.6% misclassification rate

## Confidence
- High: Basic embedding-based classification works on controlled dataset (Stable Diffusion 3.5 vs. art images)
- Medium: DINOv2 shows superior robustness to common perturbations; blockchain storage is technically feasible with uint256
- Low: Blockchain integration is practical at scale; system generalizes to diverse image types and future generative models

## Next Checks
1. Dataset diversity test: Evaluate the system on mixed-generation datasets (DALL-E, Imagen, StyleGAN) and real-world photographs to measure accuracy degradation and identify failure modes beyond the controlled test set.
2. Adversarial perturbation benchmark: Generate perturbations specifically optimized to maximize embedding-space distance while preserving visual similarity, then measure classification accuracy to establish true robustness limits.
3. Blockchain scalability simulation: Deploy the system on a local blockchain network with simulated high load. Measure gas costs, transaction throughput, and verification latency at scale to quantify the gap between theoretical tamper-resistance and practical usability.