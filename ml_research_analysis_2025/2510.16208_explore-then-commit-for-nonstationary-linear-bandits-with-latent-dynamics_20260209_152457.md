---
ver: rpa2
title: Explore-then-Commit for Nonstationary Linear Bandits with Latent Dynamics
arxiv_id: '2510.16208'
source_url: https://arxiv.org/abs/2510.16208
tags:
- bandits
- regret
- linear
- have
- dynamics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a nonstationary bandit setting where rewards\
  \ depend bilinearly on actions and latent states evolving under unknown linear dynamics.\
  \ The proposed explore-then-commit algorithm first estimates the system\u2019s Markov\
  \ parameters via random Rademacher actions, then commits to an optimized action\
  \ sequence by solving an indefinite quadratic optimization problem."
---

# Explore-then-Commit for Nonstationary Linear Bandits with Latent Dynamics

## Quick Facts
- **arXiv ID**: 2510.16208
- **Source URL**: https://arxiv.org/abs/2510.16208
- **Reference count**: 40
- **One-line primary result**: Achieves $\tilde{O}(T^{2/3})$ regret for nonstationary linear bandits with bilinear rewards and latent dynamics.

## Executive Summary
This paper introduces a nonstationary bandit setting where rewards depend bilinearly on actions and latent states evolving under unknown linear dynamics. The proposed explore-then-commit algorithm first estimates the system's Markov parameters via random Rademacher actions, then commits to an optimized action sequence by solving an indefinite quadratic optimization problem. The analysis provides near-optimal sample complexity and estimation error bounds, while the regret analysis proves a $O(T^{2/3})$ bound by leveraging an equivalence to NP-hard indefinite quadratic optimization over a hypercube. The paper also introduces practical approaches—semidefinite relaxation with Goemans-Williamson rounding and sign-iteration—demonstrating their effectiveness in experiments, where SDP+GW consistently outperforms the heuristic method and aligns with theoretical regret rates.

## Method Summary
The algorithm operates in two phases: exploration and commit. During exploration, the agent selects Rademacher actions (uniformly from $\{-1, +1\}^p$) for $H = \tilde{O}(T^{2/3})$ rounds to estimate Markov parameters $G$ through least-squares regression on the bilinear reward structure. The Markov parameters capture the system's impulse response from actions to rewards. In the commit phase, the agent constructs a symmetric matrix $\hat{S}$ from the estimated Markov parameters and solves an indefinite quadratic optimization problem $\max_{u \in \{-1,1\}^p(T-H)} u^\top \hat{S} u$ using semidefinite relaxation with Goemans-Williamson rounding or a sign-iteration heuristic. The regret analysis balances the linear cost of exploration against the sublinear reduction in estimation error.

## Key Results
- Achieves $\tilde{O}(T^{2/3})$ regret, which is near-optimal for the nonstationary latent dynamics setting
- Introduces a practical algorithm combining system identification with indefinite quadratic optimization
- SDP+GW rounding consistently outperforms sign-iteration heuristic in experiments
- Provides sample complexity bounds and estimation error guarantees for Markov parameter recovery
- Demonstrates double descent phenomenon in estimation error when number of samples approaches number of parameters

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Randomized Rademacher actions allow for the consistent estimation of system parameters from bilinear rewards without direct state observation.
- **Mechanism**: By selecting actions independently from $\text{Unif}\{-1, +1\}^p$, the learner ensures the covariance matrix $\tilde{U}$ becomes invertible (persistently exciting). This allows a least-squares regression to isolate the Markov parameters ($G$) from the latent state noise, mapping action history directly to rewards.
- **Core assumption**: The system matrix $A$ is Schur-stable (spectral radius $\rho(A) < 1$), and noise is zero-mean sub-Gaussian.
- **Evidence anchors**:
  - [abstract] "During the exploration phase, random Rademacher actions enable estimation of the Markov parameters..."
  - [section 3.1] "This distribution satisfies the action constraint... and provides sufficient excitation... ensuring that the Markov parameters can be consistently estimated."
  - [corpus] Corpus contains related work on latent bandits (e.g., Latent Preference Bandits) but lacks specific evidence for the Rademacher excitation mechanism in bilinear settings.

### Mechanism 2
- **Claim**: The optimal long-term action sequence can be approximated by solving an indefinite quadratic optimization problem over a hypercube using only estimated Markov parameters.
- **Mechanism**: The reward structure allows the cumulative expected reward to be rewritten as a quadratic form $u^\top S u$. Since this optimization is NP-hard over continuous spaces, the paper reduces the action set to the vertices of the hypercube ($\{-1, 1\}^p$) and uses a Semidefinite Relaxation (SDP) to find a tractable approximate solution.
- **Core assumption**: The maximizer of the quadratic form exists at a vertex of the hypercube (Proposition 2).
- **Evidence anchors**:
  - [abstract] "...equivalence with indefinite quadratic optimization over a hypercube, a known NP-hard problem."
  - [section 2.2] "...the optimal open-loop action sequence $u^*_{0:T}$ is the solution to the problem [7]..."
  - [corpus] Weak evidence; corpus references "Restless Bandits" and "Combinatorial Bandits" which share non-stationarity but not the specific quadratic reduction mechanism.

### Mechanism 3
- **Claim**: Balancing the length of exploration against the commit-phase error yields a sublinear regret of $\tilde{O}(T^{2/3})$.
- **Mechanism**: The algorithm incurs linear regret proportional to exploration length $H$ (during which rewards are not optimized) but reduces estimation error at a rate of $\tilde{O}(1/\sqrt{H})$. By setting $H \propto T^{2/3}$, the linear loss of exploration cancels out the accumulated estimation error over the remaining horizon $T$.
- **Core assumption**: Exploration and commit phases are strictly separated (open-loop optimization).
- **Evidence anchors**:
  - [abstract] "Our proposed algorithm achieves $\tilde{O}(T^{2/3})$ regret."
  - [section 5.3] "Optimizing the bound over $H$ yields the $\tilde{O}(T^{2/3})$ rate with $H = \tilde{O}(T^{2/3})$."
  - [corpus] No direct evidence in corpus for this specific regret rate derivation in bilinear bandits.

## Foundational Learning

- **Concept**: **Markov Parameters (Impulse Response)**
  - **Why needed here**: The paper estimates these parameters ($CA^k B$) rather than the raw state-space matrices ($A, B, C$) to characterize the action-reward relationship. Understanding that these parameters capture the system's "memory" of past actions is crucial for the regression setup.
  - **Quick check question**: Can you explain why estimating Markov parameters allows us to bypass the need to identify the latent state $x_t$ directly?

- **Concept**: **Schur Stability**
  - **Why needed here**: The assumption that $\rho(A) < 1$ ensures the influence of past actions decays geometrically. This justifies the truncation of the parameter horizon ($L$), allowing the model to ignore actions beyond a certain history without significant loss of accuracy.
  - **Quick check question**: If the matrix $A$ had a spectral radius of 1.1, how would that break the assumption regarding the truncation length $L$?

- **Concept**: **Semidefinite Relaxation (SDP) & Goemans-Williamson Rounding**
  - **Why needed here**: The core optimization problem is NP-hard. SDP provides a convex relaxation to approximate the solution, and Goemans-Williamson rounding converts the relaxed solution back to discrete binary actions ($\pm 1$).
  - **Quick check question**: Why can't we just use gradient descent on the quadratic objective $u^\top S u$ directly?

## Architecture Onboarding

- **Component map**: Trajectory buffer -> Least-squares estimator -> Block-Toeplitz constructor -> SDP solver -> GW rounding -> Binary action sequence

- **Critical path**: The transition from the **Estimator** to the **Optimizer**. The regret analysis depends heavily on how the estimation error $\epsilon$ in $\hat{G}$ translates to the optimization error in the commit phase. If the system identification is poor, the SDP will optimize the wrong objective function.

- **Design tradeoffs**:
  - **Exploration Length ($H$)**: Too short $H$ $\to$ high estimation error $\to$ poor commit performance. Too long $H$ $\to$ wasted rounds where rewards aren't maximized.
  - **Truncation Length ($L$)**: Depends on spectral radius $\rho(A)$. High $\rho(A)$ requires larger $L$ to capture system memory, increasing regression complexity.
  - **Optimizer Choice**: SDP+GW is theoretically robust (sub-optimality guarantees) but computationally heavier than the Sign-Iteration heuristic.

- **Failure signatures**:
  - **Double Descent**: Observed in experiments (Section 6.4); estimation error peaks when the number of samples $H$ is close to the number of parameters $p^2 L$.
  - **Unstable Dynamics**: If $\rho(A)$ is near 1, the truncation error $\rho^L$ becomes significant, violating the regret assumptions.
  - **Sub-optimal Heuristics**: Sign-Iteration may converge to local optima, failing to match the oracle benchmark (Figure 2b).

- **First 3 experiments**:
  1.  **Hyperparameter Sensitivity**: Calibrate constants $c_1, c_2$ for $H = c_1 T^{2/3}$ and $L = c_2 \log T$ on a small horizon $T=1500$ to verify the scaling laws.
  2.  **Estimator Validation**: Plot relative estimation error $\|\hat{G} - G\|_F$ vs. exploration length $H$ to confirm the $1/\sqrt{H}$ scaling and check for the double-descent phenomenon.
  3.  **Optimizer Comparison**: Run SDP+GW vs. Sign-Iteration on synthetic data with varying spectral radii ($\rho(A) = 0.1$ vs $0.9$) to measure the regret gap against the oracle.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can adaptive or closed-loop feedback policies improve the regret rate beyond the $\tilde{O}(T^{2/3})$ achieved by open-loop strategies?
- Basis in paper: [explicit] The conclusion states that moving "beyond open-loop policies toward adaptive strategies" is a "natural direction for future work."
- Why unresolved: The paper analyzes open-loop sequences; the authors note that optimal feedback for bilinear observations is nonlinear, making analysis difficult.
- What evidence would resolve it: An algorithm that utilizes real-time state estimates to update actions with a proven regret rate lower than $T^{2/3}$.

### Open Question 2
- Question: Is the $\tilde{O}(T^{2/3})$ regret rate minimax optimal, or can an improved algorithm or analysis achieve $\tilde{O}(\sqrt{T})$?
- Basis in paper: [inferred] The paper provides an upper bound but does not establish a lower bound for the problem class.
- Why unresolved: Explore-then-Commit algorithms typically suffer $T^{2/3}$ rates, but it is unclear if the bilinear latent structure fundamentally limits performance or if adaptive methods could achieve the standard bandit rate of $\sqrt{T}$.
- What evidence would resolve it: A theoretical lower bound proof matching $T^{2/3}$, or an algorithm achieving $O(\sqrt{T})$ regret.

### Open Question 3
- Question: Can the theoretical guarantees be extended to systems where the state matrix $A$ is unstable or marginally stable?
- Basis in paper: [inferred] Assumption 1(a) restricts $A$ to be Schur-stable, and the error bounds rely on geometric decay of Markov parameters.
- Why unresolved: The estimation error and regret analysis depend on the spectral radius $\rho(A) < 1$; error accumulation in unstable systems may break the current sample complexity bounds.
- What evidence would resolve it: Deriving estimation error bounds and regret guarantees that scale with the controllability/observability of unstable systems.

## Limitations
- The $\tilde{O}(T^{2/3})$ regret rate is worse than the $\tilde{O}(\sqrt{T})$ achievable in simpler bandit settings
- Theoretical guarantees rely heavily on Schur stability assumption ($\rho(A) < 1$)
- The hyperparameter calibration (constants $c_1, c_2$) is empirical rather than theoretically derived
- Performance depends critically on the accuracy of Markov parameter estimation in the exploration phase

## Confidence
- **High confidence**: The core regret bound of Õ(T^{2/3}) follows directly from the optimization of the exploration-commit tradeoff and the sub-Gaussian noise assumptions. The reduction to indefinite quadratic optimization is mathematically rigorous.
- **Medium confidence**: The effectiveness of SDP+GW rounding in practice depends on problem-specific constants and solver implementations not fully detailed in the paper. The sign-iteration heuristic's convergence properties are stated but not extensively validated.
- **Low confidence**: The empirical results showing double descent in estimation error and the relative performance of different ρ(A) values are based on a single experimental setup with fixed dimensions (n=3, p=2). The generalizability to larger systems remains unclear.

## Next Checks
1. **Scaling Law Verification**: Replicate the grid search for c₁ and c₂ constants across multiple values of T (not just T=1500) to verify that the proposed scaling laws hold universally rather than being overfit to a specific horizon.

2. **Spectral Radius Stress Test**: Systematically vary ρ(A) from 0.1 to 0.99 in increments of 0.1 while measuring both estimation error and regret to identify the threshold where the truncation approximation breaks down.

3. **Dimensionality Scaling**: Extend experiments beyond n=3, p=2 to verify that the Õ(T^{2/3}) regret bound scales appropriately as problem dimensions increase, particularly examining the p^{2L} factor in the sample complexity requirement.