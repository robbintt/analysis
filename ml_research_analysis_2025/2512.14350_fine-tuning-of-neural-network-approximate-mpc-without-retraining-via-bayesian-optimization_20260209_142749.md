---
ver: rpa2
title: Fine-Tuning of Neural Network Approximate MPC without Retraining via Bayesian
  Optimization
arxiv_id: '2512.14350'
source_url: https://arxiv.org/abs/2512.14350
tags:
- ampc
- control
- parameters
- optimization
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of fine-tuning approximate model-predictive
  control (AMPC) policies for robotic systems without retraining neural networks.
  The core contribution is a method that uses Bayesian optimization to automatically
  tune the parameters of parameter-adaptive AMPC policies based on experimental data.
---

# Fine-Tuning of Neural Network Approximate MPC without Retraining via Bayesian Optimization

## Quick Facts
- arXiv ID: 2512.14350
- Source URL: https://arxiv.org/abs/2512.14350
- Reference count: 40
- Primary result: Automatic fine-tuning of AMPC parameters via Bayesian optimization achieves hardware performance comparable to or better than nominal AMPC in just 20 experiments.

## Executive Summary
This paper presents a method for fine-tuning parameter-adaptive Approximate Model Predictive Control (AMPC) policies without retraining neural networks. The approach combines a two-network architecture (nominal policy + sensitivity network) with Trust Region Bayesian Optimization (TuRBO) to automatically adapt controller parameters based on experimental data. The method is validated on two unstable systems - a cartpole and a balancing unicycle robot - demonstrating superior hardware performance compared to nominal AMPC with minimal experimentation.

## Method Summary
The method trains two neural networks offline: one for the nominal MPC policy and one for parameter sensitivities (gradients). At runtime, the AMPC policy adapts linearly to parameter changes using the sensitivity network. TuRBO is then used to optimize these parameters by maximizing a reward signal collected from hardware experiments. The approach requires a pre-trained simulation model but eliminates the need for iterative dataset synthesis and retraining when adapting to new systems or cost functions.

## Key Results
- Hardware experiments show AMPC with BO tuning achieves satisfactory performance in just 20 experiments on both cartpole and wheelbot systems
- The method outperforms nominal AMPC on hardware, compensating for model mismatch between simulation and real systems
- In simulation experiments, Bayesian optimization consistently improves performance over initial parameters and pseudo-random baselines

## Why This Works (Mechanism)

### Mechanism 1: Linearized Policy Adaptation via Sensitivity Gradients
Tuning a controller for new system instances is possible without retraining because the policy adjusts linearly to parameter changes. The system trains two networks: one for the nominal control action $\tilde{\pi}_{MPC}$ and one for the sensitivities (gradients) $\tilde{\pi}_{\nabla MPC}$. By adding the product of the sensitivity and the parameter deviation $(\theta - \theta_{nom})$ to the nominal action, the controller approximates the MPC's behavior for new parameters using a first-order Taylor expansion. The core assumption is that the relationship between optimal actions and parameters remains locally linear around $\theta_{nom}$.

### Mechanism 2: Sample-Efficient Search via Local Bayesian Optimization
The method finds optimal parameters with few hardware experiments because the search space is constrained to a local trust region where the underlying reward function is modeled as a Gaussian Process. Instead of random search or global optimization, the method uses Trust Region Bayesian Optimization (TuRBO). It builds a surrogate model of the reward landscape and iteratively selects parameters via Thompson sampling, shrinking the search area around the best-performing parameters to refine the solution efficiently. The core assumption is that the reward function is smooth enough to be modeled by a GP within the trust region.

### Mechanism 3: Decoupling MPC Cost from Reward Optimization
The controller can satisfy complex, sparse objectives (e.g., "swing up without falling") that are difficult to encode in standard MPC by optimizing a reward signal external to the MPC formulation. The MPC provides the base dynamics/constraint handling, while the BO loop optimizes the parameters of that MPC to maximize a separate reward $R(\theta)$. This effectively "tunes" the conservative MPC to be task-optimal without requiring the MPC solver to handle non-differentiable or sparse cost functions directly.

## Foundational Learning

- **Model Predictive Control (MPC)**: The entire method is an approximation of MPC. Understanding the optimization loop (predict horizon, minimize cost, apply first action) is necessary to interpret what the NN is imitating.
  - Why needed: To understand what the neural network is approximating
  - Quick check: Can you explain why MPC is computationally expensive at runtime compared to a simple state-feedback controller?

- **First-Order Sensitivity Analysis**: The core novelty is using sensitivities ($\partial \pi / \partial \theta$) to avoid retraining. You must understand how gradients propagate through an optimization problem to see why this is a linear approximation.
  - Why needed: To understand the parameter adaptation mechanism
  - Quick check: If a parameter changes by $\Delta \theta$, how does the sensitivity network approximate the change in action?

- **Gaussian Processes (GP) & Bayesian Optimization**: The "auto-tuning" is done via BO. You need to grasp that a GP provides a mean and variance (uncertainty) for the reward, which drives the exploration-exploitation trade-off.
  - Why needed: To understand how parameters are optimized with minimal experiments
  - Quick check: Why would a GP be better than a simple grid search for tuning a controller with 11 parameters?

## Architecture Onboarding

- **Component map**: Parameterized MPC Solver -> Dataset Generator -> Training Script -> Nominal NN & Sensitivity NN; State Sensor -> NN Inference (Adaptation Eq.) -> Actuators; System Episode -> Reward Calculation -> TuRBO Optimizer -> New Parameters $\theta$

- **Critical path**: The validity of the Sensitivity NN. If the sensitivity estimates are poor (high error), the linear adaptation fails, and BO will be tuning a broken interface.

- **Design tradeoffs**:
  - MPC Horizon vs. Inference Speed: Longer horizons improve MPC quality but increase dataset generation time and NN complexity
  - Trust Region Size ($L_{TR}$) vs. Convergence Speed: A large trust region finds solutions faster but risks instability if the linear approximation breaks; a small one is safer but slower

- **Failure signatures**:
  - Controller Jitter: Inference is too slow or noise is amplified by the sensitivity network
  - Reward Stagnation: BO fails to improve; likely the linear assumption is invalid for the current $\theta$, or the reward is too sparse
  - Constraint Violation: The adapted AMPC violates safety constraints (AMPC has no explicit safety guarantees like the original MPC)

- **First 3 experiments**:
  1. Sanity Check: Deploy the Nominal NN (no adaptation) on the simulation. Verify it tracks the MPC trajectory.
  2. Range Test: Manually perturb $\theta$ in simulation to determine the "valid adaptation range" where the linear policy remains stable. Use this to set BO bounds.
  3. Hardware Baseline: Run the initial parameter set on hardware. If it fails immediately, reduce the initial parameter deviation or check sensor calibration before starting BO.

## Open Questions the Paper Calls Out

### Open Question 1
Can parameter-adaptive AMPC with BO-based tuning scale to high-dimensional state spaces (e.g., environments with hundreds of states or end-to-end learning from image data)? Authors state in Limitations: "it is unclear, how to scale the AMPC synthesis to very high dimensional states (i.e., environments with hundreds of states or end-to-end learning from image data)." This remains unresolved as current experiments only validate on systems with 4-11 parameters and 4-10 states; computational complexity of dataset synthesis and BO grows substantially with dimension.

### Open Question 2
How can the method be extended to handle time-varying system parameters (e.g., due to wear and tear)? Authors state "the proposed method considers time-invariant parameters. This might be an oversimplification in applications where parameters change over time" and plan to "investigat[e] time-varying or event-triggered BO schemes." This remains unresolved as current BO formulation assumes static $\theta^*$; real systems experience drift in mass, friction, and other parameters over operational lifetimes.

### Open Question 3
What are the theoretical bounds on the parameter deviation ($||\theta - \theta_{nom}||$) for which approximate sensitivities remain sufficiently accurate? Authors note sensitivities "may not be accurate enough for vastly different systems" and the linear approximation in Eq. (4) is inherently local. This remains unresolved as no formal analysis is provided on valid trust-region size; empirical success shown but no guarantee of when linear approximation breaks down.

## Limitations

- The linear approximation of parameter sensitivities may break down for larger parameter deviations or highly nonlinear dynamics
- The method relies heavily on having a well-calibrated simulation model for dataset generation
- Scalability to systems with high-dimensional parameter spaces remains unclear

## Confidence

- **High Confidence**: The core mechanism of parameter-adaptive AMPC via sensitivity networks is well-established in prior work and the implementation appears sound
- **Medium Confidence**: The Bayesian optimization approach for tuning is appropriate and the use of TuRBO is well-justified
- **Low Confidence**: The generalization capability to systems substantially different from the tested cartpole and wheelbot scenarios

## Next Checks

1. **Sensitivity Network Validation**: Before deployment, verify the trained sensitivity network against ground-truth MPC sensitivities on a held-out validation set to quantify approximation error and determine safe operating bounds.

2. **Linear Approximation Range**: Systematically test the linear policy adaptation across the full parameter range to identify where the first-order Taylor expansion breaks down, establishing clear limits for TuRBO's trust region.

3. **Cross-System Transferability**: Test whether an AMPC trained on one system (e.g., cartpole) can be effectively fine-tuned on a different but related system (e.g., different pendulum length or mass distribution) to validate the method's generalization capability.