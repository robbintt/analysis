---
ver: rpa2
title: 'READY: Reward Discovery for Meta-Black-Box Optimization'
arxiv_id: '2601.21847'
source_url: https://arxiv.org/abs/2601.21847
tags:
- reward
- cost
- optimization
- component
- weight
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: READY automates reward discovery for MetaBBO by leveraging large
  language models in a multitask evolutionary framework. It introduces niche-based
  parallel populations, fine-grained mutation/crossover operators, and knowledge transfer
  across tasks to accelerate search.
---

# READY: Reward Discovery for Meta-Black-Box Optimization

## Quick Facts
- arXiv ID: 2601.21847
- Source URL: https://arxiv.org/abs/2601.21847
- Reference count: 40
- Primary result: Achieves up to 99.99% cost reduction in zero-shot reward transfer across MetaBBO tasks

## Executive Summary
READY automates reward discovery for MetaBBO by leveraging large language models in a multitask evolutionary framework. It introduces niche-based parallel populations, fine-grained mutation/crossover operators, and knowledge transfer across tasks to accelerate search. The framework discovers interpretable reward functions that outperform handcrafted and automated baselines across three heterogeneous MetaBBO tasks (DEDQN, RLDAS, RLEPSO), achieving up to 99.99% cost reduction in zero-shot transfer. The multitask design enables 2-4× faster discovery while maintaining microsecond inference latency. READY's rewards demonstrate superior generalization by capturing universal optimization heuristics rather than overfitting to specific architectures.

## Method Summary
READY automates reward discovery for MetaBBO by maintaining K parallel niche populations, each evolving rewards for a specific MetaBBO task. The framework uses specialized reflection-based mutation operators (M1-M3) and crossover operators (C1-C2) to generate offspring, with fitness evaluated by training the MetaBBO agent for up to 0.5 hours. Knowledge transfer identifies promising source-target pairs and adapts reward logic across niches. Initialization anchors each niche with the expert reward and uses performance-based rejection sampling. The search runs for 7 generations with population size 5 per niche, using DeepSeek-V3.2 as the LLM backbone.

## Key Results
- Achieves 2-4× speedup in search efficiency compared to sequential baselines
- Discovers rewards that outperform handcrafted and automated baselines across three MetaBBO tasks
- Demonstrates 99.99% cost reduction in zero-shot transfer to unseen optimization problems
- Rewards capture universal optimization heuristics rather than overfitting to specific architectures

## Why This Works (Mechanism)

### Mechanism 1: Multitask Niche-Based Parallelism with Knowledge Transfer
- **Claim:** Simultaneously evolving reward populations for multiple MetaBBO tasks accelerates convergence compared to independent sequential search.
- **Mechanism:** The framework maintains K separate niches (populations), each bound to a specific MetaBBO task. Knowledge Transfer (KT) operator identifies promising source-target pairs and adapts reward logic across task boundaries, allowing successful patterns discovered in one niche to benefit others.
- **Core assumption:** Reward design knowledge is partially transferable across MetaBBO architectures that share methodological foundations (e.g., all use RL for algorithm configuration).
- **Evidence anchors:**
  - [abstract] "READY achieves a 2× to 4× speedup in search efficiency compared to baselines"
  - [section 4.4] "READY completes the entire discovery process across three tasks in only 7 hours. In contrast, sequential execution of Eureka requires approximately 14 hours"
  - [corpus] Weak corpus evidence—neighbor papers focus on MetaBBO training efficiency, not multitask reward discovery specifically.
- **Break condition:** If MetaBBO tasks share no algorithmic or structural similarity, KT provides no benefit and adds overhead.

### Mechanism 2: Reflection-Based Evolutionary Operators for Targeted Search
- **Claim:** Specialized mutation operators that analyze failure cases, historical trends, and global patterns produce more effective reward refinements than undirected mutation.
- **Mechanism:** M1 (Local-Reflection) identifies top-K worst-performing instances and prompts LLM to diagnose and fix weaknesses. M2 (History-Reflection) analyzes evolutionary traces to extrapolate optimization momentum. M3 (Global-Reflection) distills universal design patterns from a cross-niche archive.
- **Core assumption:** LLMs can perform meaningful causal reasoning about reward-performance relationships given structured failure analysis.
- **Evidence anchors:**
  - [abstract] "fine-grained evolution operators" contribute to continuous improvement
  - [section 4.3] "M2 and M1 mutation operators debug landscape-specific failures and refine trade-off mechanisms" during the latent accumulation phase
  - [corpus] No direct corpus evidence; neighboring papers do not study reflection-based program evolution.
- **Break condition:** If LLM lacks sufficient reasoning capability or domain knowledge, reflections become superficial and may introduce harmful mutations.

### Mechanism 3: Expert-Anchored Initialization with Performance-Based Rejection
- **Claim:** Seeding populations with human-designed rewards and filtering candidates to those outperforming the expert baseline ensures search begins in a productive region of reward space.
- **Mechanism:** Each niche includes the original handcrafted reward as individual I_expert. During initialization, newly generated candidates are evaluated and accepted only if their fitness F exceeds the expert baseline, preventing wasted budget on clearly inferior solutions.
- **Core assumption:** Human-designed rewards, while suboptimal, provide a reasonable lower bound; better rewards exist in nearby regions.
- **Evidence anchors:**
  - [section 3.2.3] "Expert Anchoring: Each niche is explicitly initialized with the human-designed reward individual I_expert"
  - [section 3.2.3] "Performance-Based Rejection Sampling: A newly generated individual... will be added to the niche only if its fitness F outperforms the human-designed reward individual"
  - [corpus] No corpus neighbors study initialization strategies for LLM-based program search.
- **Break condition:** If expert rewards are already near-optimal or fundamentally misaligned with the task, this constraint may prevent discovery of qualitatively different solutions.

## Foundational Learning

- **Concept: MetaBBO bi-level structure (meta-level policy, low-level optimizer, reward function)**
  - **Why needed here:** READY operates on the reward function within this hierarchy. Without understanding how the reward trains the meta-policy which configures the optimizer, you cannot interpret why discovered rewards work.
  - **Quick check question:** Given a MetaBBO task where a PPO agent selects DE mutation strategies, what information must the reward function encode to provide useful learning signal?

- **Concept: Evolutionary computation fundamentals (mutation, crossover, selection pressure, diversity)**
  - **Why needed here:** READY is an evolutionary algorithm with LLM-based operators. Understanding trade-offs between exploration (diversity) and exploitation (fitness) is essential for tuning niche sizes, operator frequencies, and selection schemes.
  - **Quick check question:** Why might high selection pressure lead to premature convergence in reward program evolution?

- **Concept: LLM prompt engineering for structured code generation**
  - **Why needed here:** All operators (M1-M3, C1-C2, KT) are instantiated via carefully designed prompts. Poor prompt design leads to syntactically invalid code, hallucinated variables, or logically inconsistent rewards.
  - **Quick check question:** What context must be included in a prompt for the LLM to generate a valid reward function for a specific MetaBBO architecture?

## Architecture Onboarding

- **Component map:** Metadata M_k (Algorithm concept C_alg + Programming interface C_code) -> Niche P_k (Population of N=5 individuals) -> Evolution Operators (5 operators generating 5N offspring) -> Global Archive P_archive (Eliminated individuals) -> Knowledge Transfer Module (Source-target adaptation)

- **Critical path:**
  1. **Metadata preparation** (one-time per task): Extract C_alg via paper summarization; extract C_code via source analysis
  2. **Initialization**: Anchor with expert reward; generate N-1 candidates via in-context generation with rejection sampling
  3. **Per-generation loop** (G_max=7): Apply all 5 operators to each parent → Evaluate on training problems (Γ=3 runs) → Rank-based selection → Knowledge transfer replaces worst individuals

- **Design tradeoffs:**
  - **Niche size (N=5)**: Smaller = faster per-generation cost, less diversity; larger = more robust exploration, slower convergence. Paper uses N=5 as balance.
  - **Evaluation budget (Γ=3 runs)**: More runs = more reliable fitness estimates but higher time cost. 0.5-hour wall-clock limit per MetaBBO training constrains this.
  - **LLM backbone**: DeepSeek-V3.2 used; Section 4.6 shows Gemini-3-Flash achieves higher SNE, suggesting performance scales with reasoning capability.

- **Failure signatures:**
  - **Extended plateau without improvement (generations 2-4 in Figure 2 is normal; plateau beyond gen 5 suggests stagnation)** → Crossover operators may be failing to synthesize breakthrough logic; inspect C1/C2 outputs
  - **All offspring rejected during initialization** → Metadata M_k may be insufficient or expert baseline already strong; expand C_code coverage
  - **High variance across repeated runs** → Evaluation budget too low; increase Γ or training time per evaluation
  - **Transfer rewards consistently degrade target niche** → KT source-target matching is flawed; review reflection/strategy prompts in Appendix A.7

- **First 3 experiments:**
  1. **Single-task baseline:** Run READY on one MetaBBO task with KT disabled (set K=1) to establish intra-niche evolution contribution; compare convergence speed against full multitask setup.
  2. **Operator ablation:** Replace each specialized operator (M1, M2, M3, C1, C2) with simple LLM mutation (random perturbation prompt) individually; compute SNE degradation per Figure 3 to identify critical operators.
  3. **Cross-framework transfer test:** Take the discovered reward for DEDQN and apply it zero-shot to an unseen MetaBBO framework (e.g., RLEPSO or a new architecture); measure whether the 13/16 improvement rate from Table 2 generalizes.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What mechanism enables effective zero-shot reward transfer between structurally distinct MetaBBO frameworks, and why does it fail catastrophically on specific problem types (e.g., Schwefel showing -136.80% degradation)?
- Basis in paper: [inferred] Table 2 shows zero-shot transfer succeeds on 13/16 functions with up to 99.99% improvement but degrades on Buche Rastrigin, Gallagher 21Peaks, and Schwefel. The transfer mechanism is not analyzed for failure cases.
- Why unresolved: The paper notes this transferability is "surprising" but does not investigate when or why transfer succeeds versus causes negative transfer.
- What evidence would resolve it: A systematic study mapping landscape features to transfer success/failure, plus analysis of which reward components transfer well versus poorly.

### Open Question 2
- Question: Can READY-discovered rewards maintain their performance advantages on real-world optimization problems beyond synthetic BBOB benchmarks?
- Basis in paper: [explicit] Impact Statement: "boosting MetaBBO's performance on realworld applications" is listed as a key future impact direction. All experiments use only BBOB test functions.
- Why unresolved: BBOB functions have known mathematical properties; real-world problems often have constraints, noise, expensive evaluations, or mixed-variable spaces not represented in the benchmark.
- What evidence would resolve it: Experiments applying discovered rewards to real-world optimization tasks (e.g., hyperparameter tuning, neural architecture search, engineering design) comparing against handcrafted baselines.

### Open Question 3
- Question: How does the multitask evolutionary architecture scale to discovering rewards for many more (e.g., 10+ or 50+) MetaBBO frameworks simultaneously?
- Basis in paper: [inferred] Only 3 MetaBBO frameworks (DEDQN, RLDAS, RLEPSO) are tested. The knowledge transfer operator complexity grows with the number of task pairs, and niche management overhead is unexplored.
- Why unresolved: The paper demonstrates viability for 3 tasks but provides no analysis of computational or algorithmic scaling behavior.
- What evidence would resolve it: Empirical scaling curves showing wall-clock time, convergence rate, and reward quality as the number of tasks increases, plus identification of architectural bottlenecks.

## Limitations
- **Computational accessibility:** The 0.5-hour wall-clock limit for training candidate rewards is hardware-dependent, making exact reproduction challenging without knowing specific CPU/GPU specifications.
- **Inner-loop RL configurations:** The hyperparameters (learning rates, batch sizes, epochs) for the PPO/DQN agents used during fitness evaluation are not explicitly specified.
- **Transfer mechanism understanding:** The paper does not investigate why zero-shot transfer succeeds on some functions but catastrophically fails on others (e.g., Schwefel showing -136.80% degradation).

## Confidence
- **High Confidence:** The multitask niche-based evolution framework and its contribution to 2-4× speedup in discovery efficiency. The quantitative results (99.99% cost reduction, 13/16 improvement rate) are directly supported by reported experiments.
- **Medium Confidence:** The superiority of reflection-based operators over standard evolutionary methods. While the paper claims these specialized operators are critical, direct ablation studies comparing them to simple random mutation are not provided.
- **Medium Confidence:** The zero-shot transfer capability across unseen algorithms. The 13/16 success rate is promising but tested on a limited set of algorithms; broader validation is needed.

## Next Checks
1. **Single-Task Baseline Validation:** Run READY with K=1 (disable KT) on DEDQN alone and measure convergence speed and final reward quality compared to the full multitask setup to isolate the contribution of parallel niches.
2. **Operator Ablation Study:** Systematically replace each specialized operator (M1, M2, M3, C1, C2) with a simple random perturbation prompt and measure the degradation in SNE to identify which operators are truly critical for discovery.
3. **Cross-Framework Transfer Test:** Apply the best-discovered reward from DEDQN to a new, unseen MetaBBO framework (e.g., a different RL-based optimizer not in {DEDQN, RLDAS, RLEPSO}) to test whether the claimed generalization of reward design heuristics extends beyond the tested algorithms.