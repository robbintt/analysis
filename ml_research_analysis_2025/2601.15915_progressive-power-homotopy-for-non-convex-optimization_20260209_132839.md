---
ver: rpa2
title: Progressive Power Homotopy for Non-convex Optimization
arxiv_id: '2601.15915'
source_url: https://arxiv.org/abs/2601.15915
tags:
- where
- learning
- optimization
- assumption
- power
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces Progressive Power Homotopy (Prog-PowerHP),\
  \ a first-order stochastic optimization method that combines power transformation\
  \ with Gaussian smoothing to solve non-convex optimization problems. The method\
  \ progressively increases the power parameter N and decreases the smoothing scale\
  \ \u03C3 during optimization to approximate the global optimum."
---

# Progressive Power Homotopy for Non-convex Optimization

## Quick Facts
- **arXiv ID:** 2601.15915
- **Source URL:** https://arxiv.org/abs/2601.15915
- **Reference count:** 40
- **Primary result:** Prog-PowerHP converges to a δ-neighborhood of the global optimum with O(d²ε⁻²) complexity under mild conditions

## Executive Summary
This paper introduces Progressive Power Homotopy (Prog-PowerHP), a first-order stochastic optimization method that combines power transformation with Gaussian smoothing to solve non-convex optimization problems. The method progressively increases the power parameter N and decreases the smoothing scale σ during optimization to approximate the global optimum. Theoretical analysis shows that under mild regularity conditions, Prog-PowerHP converges to a small neighborhood of the global optimum with iteration complexity scaling nearly as O(d²ε⁻²).

## Method Summary
Prog-PowerHP optimizes a surrogate objective that applies power transformation followed by Gaussian smoothing to the original non-convex objective. The method performs stochastic gradient ascent on the smoothed surrogate, progressively increasing the power parameter N and decreasing the smoothing scale σ throughout training. The gradient estimator uses K sampled parameters from a Gaussian distribution centered at the current iterate μ, requiring K forward passes and K backward passes per iteration. The method outputs the best iterate based on held-out validation performance.

## Key Results
- In phase retrieval with N/d ∈ [2,4], Prog-PowerHP achieves significantly higher success rates (e.g., 92% vs 54% for standard SGD when d=100) and lower recovery error metrics
- For under-parameterized two-layer ReLU networks, the method consistently finds deeper local minima with statistically significant lower loss values compared to baseline optimizers
- The method demonstrates clear advantages in two challenging scenarios: phase retrieval near information-theoretic limits and training under-parameterized neural networks

## Why This Works (Mechanism)

### Mechanism 1: Power Transformation Landscape Reshaping
The exponential power transformation e^{N·f_w(x)} reshapes the optimization landscape by amplifying high-fitness regions relative to low-fitness regions. As N increases, the exponential scaling magnifies differences between the global optimum and suboptimal regions, making the global optimum increasingly dominant in the expected gradient. Core assumption: The objective f_w(x) is bounded above by M_f (Assumption 2); the global maximizer w* exists and is unique.

### Mechanism 2: Gaussian Smoothing Eliminates Narrow Spurious Minima
Gaussian smoothing attenuates or eliminates shallow, narrow local optima while preserving the structure of dominant basins. The surrogate F_{N,σ}(μ) = E_{w~N(μ,σ²I_d),x~D}[e^{N·f_w(x)}] averages the power-transformed objective over a σ-neighborhood. Narrow minima (width < σ) get "averaged out" because their contribution is diluted across the neighborhood, while wide basins persist. Core assumption: f is Lipschitz continuous with constant L₀_f (Assumption 2) and Lipschitz smooth with constant L₁_f (Assumption 3).

### Mechanism 3: Progressive N-σ Coupling Drives Surrogate Optimum Toward True Optimum
Progressively increasing N while decreasing σ causes the smoothed surrogate's stationary points to converge to a δ-neighborhood of the true global optimum w*. Initially, small N and large σ create a smooth landscape with few local minima, guiding μ toward the general region of w*. As N increases, the power transformation "sharpens" the distinction between global and local optima. As σ decreases, fine-grained landscape features emerge. Core assumption: Assumption 4 (the essential supremum Ψ(w) is maximized at w*, and near-optimal fitness values have non-negligible probability mass); Assumption 5 (the parameter sequence remains bounded).

## Foundational Learning

- **Concept: Homotopy/Continuation Methods**
  - Why needed here: Prog-PowerHP is fundamentally a homotopy method—it constructs a family of surrogate objectives F_{N,σ} that continuously deform from an easy-to-optimize landscape (large σ, small N) to the original hard problem (small σ, large N).
  - Quick check question: Why does solving a sequence of progressively harder problems help avoid local minima that trap direct optimization?

- **Concept: Robbins-Monro Stochastic Approximation Conditions**
  - Why needed here: The convergence proof requires learning rates α_t satisfying ∑α_t = ∞ (to reach any point) and ∑α_t² < ∞ (to suppress noise). The iteration complexity O(d²ε⁻²) assumes α_t = t^{-(1/2+γ)} for γ ∈ (0, 1/2).
  - Quick check question: What goes wrong if α_t decays too slowly (e.g., α_t = 1/t^{0.3})? What if it decays too fast (e.g., α_t = 1/t)?

- **Concept: Essential Supremum and Average-Best Alignment**
  - Why needed here: Assumption 4 requires that w* = argmax E[f_w(x)] also maximizes the essential supremum Ψ(w) = inf{a : P(f_w(x) ≤ a) = 1}. This ensures that optimizing expected fitness (average) also optimizes near-best-case performance.
  - Quick check question: Can you construct a counterexample where average performance is maximized at w* but worst-case (or best-case) performance is not?

## Architecture Onboarding

- **Component map:**
  Inputs: μ₀, N₀, Δ, σ₀, b, β, K, J, {α_t}, {φ_t}
  Per-iteration: N_t ← N_{t-1} + φ_t · Δ; σ_t ← σ₀ · β^t + b; Sample {w_k} ~ N(μ_{t-1}, σ_t² · I_d); Sample {x_j} ~ D; ĝ ← (1/KJ) Σ_k Σ_j [∂e^{N_t·f_w(x_j)}/∂w |_{w=w_k}]; μ_t ← μ_{t-1} + α_t · ĝ
  Output: μ* = argmax_{μ ∈ {μ₁,...,μ_T}} Ĝ(μ) on held-out validation

- **Critical path:** The gradient estimator computation (line 6 in Algorithm 1) is the bottleneck—requires K forward passes and K backward passes through f per iteration, plus J data samples. Memory scales as O(K · d) for storing gradients.

- **Design tradeoffs:**
  - **K (population size) vs. compute:** Larger K reduces gradient variance but multiplies compute cost. Paper uses K=1 in neural network experiments, suggesting single-sample gradients may suffice with appropriate α_t decay.
  - **σ schedule (β, b) vs. convergence precision:** Faster β decay reaches fine-grained optimization faster but risks trapping in spurious minima; larger b prevents collapse but limits final precision.
  - **Δ (power increment) vs. global convergence guarantee:** Theorem 2 requires Δ > N_{δ,b,M} for δ-neighborhood convergence. Larger Δ improves guarantee but may cause numerical issues.
  - **Validation-based output vs. final iterate:** Paper outputs argmax over trajectory rather than final μ_T, adding robustness to late-iteration oscillations.

- **Failure signatures:**
  1. **Gradient explosion (||ĝ|| → ∞):** Typically indicates N·M_f is too large. Check if f outputs are properly scaled; consider normalizing f or capping N growth.
  2. **Stagnation at high loss:** If σ_t reaches b too quickly (β too small), smoothing is lost before escaping local minima. Monitor σ_t trajectory.
  3. **Numerical overflow in e^{N·f}:** Occurs when N·M_f ≳ 700 (float64 limit). Reduce N₀, slow φ_t, or scale down f outputs.
  4. **Convergence to wrong region despite reasonable hyperparameters:** May violate Assumption 4—the optimization target may not align with true global optimum. Validate the average-best alignment assumption empirically.
  5. **High variance in gradient estimates:** K is too small relative to dimension d. Increase K or reduce α_t.

- **First 3 experiments:**
  1. **Synthetic 1D validation:** Implement the example from Figure 1 with G(μ) = E_ε[-log((μ+0.5)²+10⁻⁵) - log((μ-0.5)²+10⁻²)]. Visualize F_{N,σ}(μ) for varying N and σ. Success: reproduces the landscape transformation plots showing N increase aligning optima and σ decrease sharpening gradients near optimum.
  2. **Phase retrieval baseline (Table 1 replication):** Set d=100, N=400 samples, T=60,000 iterations. Run 100 trials with different random seeds. Target: success rate ≥90% (M(x*) ≤ 0.001) and average M̄ ≤ 0.1. Compare against SGD baseline (expected ~54% success). This validates the full implementation.
  3. **K-sensitivity ablation:** Fix d=50, N=200, and vary K ∈ {1, 3, 5, 10, 20}. Measure gradient variance (||ĝ - E[ĝ]||² across runs) and convergence speed (iterations to reach M(x*) < 0.01). Plot variance vs. K and wall-clock time vs. K. This determines optimal K for your compute budget.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does Prog-PowerHP provide a competitive advantage in large-scale, over-parameterized deep learning regimes?
- Basis in paper: The authors explicitly restrict experiments to synthetic benchmarks and under-parameterized networks, noting that standard benchmarks like CIFAR-10 "obscure optimization failure modes" due to redundancies.
- Why unresolved: The algorithm samples K particles per step, incurring higher computational costs than standard single-particle optimizers (e.g., SGD, Adam), which may limit scalability.
- What evidence would resolve it: Empirical demonstrations of convergence speed and generalization on high-dimensional datasets (e.g., ImageNet) where the computational overhead is justified by performance gains.

### Open Question 2
- Question: Can the boundedness of the optimization trajectory (Assumption 5) be proven rather than assumed?
- Basis in paper: The theoretical convergence guarantees rely on Assumption 5, which posits that the sequence {μ_t} lies in a bounded region, justified heuristically by the gradient's pull toward w*.
- Why unresolved: While the assumption is deemed "reasonable," the paper does not provide a formal proof that the iterates cannot escape to infinity under the proposed update rules.
- What evidence would resolve it: A formal proof showing that the update dynamics inherently constrain the iterates within a compact set without external assumptions.

### Open Question 3
- Question: Is the alignment between average fitness and best-case fitness (Assumption 4) satisfied for practical neural network loss landscapes?
- Basis in paper: Convergence to the global optimum requires Assumption 4, which aligns the maximizer of the expected fitness G(w) with the essential supremum Ψ(w).
- Why unresolved: The authors note this rules out "pathological situations" but do not verify if complex, high-dimensional loss surfaces in deep learning satisfy this specific regularity condition.
- What evidence would resolve it: Theoretical verification of this alignment for standard architectures or empirical analysis showing the assumption holds in the later stages of training.

## Limitations

- The method requires careful hyperparameter tuning of the N-σ coupling schedule, and the gradient estimator becomes computationally expensive with large population size K
- The primary limitation is the reliance on Assumption 4 (average-best alignment) which requires that the global optimum of expected fitness also maximizes the essential supremum
- The method samples K particles per step, incurring higher computational costs than standard single-particle optimizers (e.g., SGD, Adam), which may limit scalability

## Confidence

- **High confidence**: The theoretical convergence guarantees (Theorem 2) are well-supported by the mathematical analysis, assuming the stated regularity conditions hold.
- **Medium confidence**: The empirical results show clear advantages in the specific tested scenarios (phase retrieval and under-parameterized neural networks), but the generality across other non-convex problems remains to be established.
- **Medium confidence**: The iteration complexity of O(d²ε⁻²) is derived under idealized conditions; practical performance may vary depending on the specific problem structure and hyperparameter choices.

## Next Checks

1. **Assumption 4 Validation**: Test the method on synthetic problems where average-best alignment is deliberately violated to quantify performance degradation when this critical assumption fails.
2. **Hyperparameter Sensitivity Analysis**: Systematically vary N_0, Δ, σ_0, β, and K across a broader range to identify robust hyperparameter regions and quantify performance sensitivity.
3. **Cross-domain Applicability**: Apply Prog-PowerHP to at least two additional non-convex optimization problems (e.g., matrix completion, training residual networks) to assess generalizability beyond the current experimental scope.