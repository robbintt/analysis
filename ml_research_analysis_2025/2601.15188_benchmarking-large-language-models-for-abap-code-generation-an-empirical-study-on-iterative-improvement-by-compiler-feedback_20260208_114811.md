---
ver: rpa2
title: 'Benchmarking Large Language Models for ABAP Code Generation: An Empirical
  Study on Iterative Improvement by Compiler Feedback'
arxiv_id: '2601.15188'
source_url: https://arxiv.org/abs/2601.15188
tags:
- code
- abap
- llms
- which
- errors
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study benchmarks large language models for ABAP code generation,
  using 180 tasks including adapted HumanEval and SAP-specific scenarios. Models were
  tested for their ability to generate syntactically and semantically correct code,
  with iterative feedback from the ABAP compiler to assess improvement.
---

# Benchmarking Large Language Models for ABAP Code Generation: An Empirical Study on Iterative Improvement by Compiler Feedback

## Quick Facts
- arXiv ID: 2601.15188
- Source URL: https://arxiv.org/abs/2601.15188
- Reference count: 40
- Primary result: GPT-5 and Claude-Sonnet-4 achieve ~77% and ~75% success rates after five compiler feedback iterations on ABAP code generation tasks

## Executive Summary
This study benchmarks large language models for ABAP code generation using 180 tasks, including adapted HumanEval and SAP-specific scenarios. Models were evaluated for their ability to generate syntactically and semantically correct code with iterative feedback from the ABAP compiler. The results show that top-tier proprietary models like GPT-5 and Claude-Sonnet-4 achieve high success rates (around 75% after five iterations), significantly outperforming open-source models. The study demonstrates that iterative compiler feedback is effective for improving code quality, though certain algorithmic tasks remain challenging. Task type influences difficulty, with List/Array Operations being the most problematic category.

## Method Summary
The benchmark evaluates LLMs on 180 ABAP coding tasks using a standardized SAP Docker environment with ABAP Cloud Language Version 5. Each task involves generating code that passes automated unit tests, with up to five iterations of compiler feedback. The system uses Python orchestration to submit code to SAP, extract error messages, and re-prompt the LLM. Temperature is set to 0.2 for consistency, and success is measured by functional correctness through ABAP unit tests. The benchmark includes both adapted HumanEval tasks and SAP-specific scenarios, with results analyzed using Kaplan-Meier survival analysis to track error persistence across iterations.

## Key Results
- GPT-5 and Claude-Sonnet-4 achieve the highest success rates (~77% and ~75% after five iterations)
- Top-tier models benefit greatly from compiler feedback, with most successes achieved within the first two feedback rounds
- List or Array Operations tasks are the most challenging, with 19 tasks remaining unsolved
- A bimodal success distribution emerges for top models, suggesting some tasks are inherently harder to solve even with feedback
- Open-source models show lower initial performance but improve through iterations

## Why This Works (Mechanism)

### Mechanism 1: Iterative Error Correction via External Compiler Feedback
- Claim: Providing LLMs with compiler error messages as feedback enables progressive improvement in ABAP code generation
- Mechanism: The system runs an automated loop: (1) LLM generates code from a prompt, (2) SAP compiler validates syntax and unit tests check semantics, (3) error messages are fed back to the LLM, (4) LLM uses this diagnostic signal to correct structural, type, or logic issues in the next attempt
- Core assumption: The LLM possesses sufficient underlying ABAP syntax and logic knowledge to correctly interpret compiler feedback and generate a valid fix
- Evidence anchors:
  - [abstract] "Models were tested for their ability to generate syntactically and semantically correct code, with iterative feedback from the ABAP compiler to assess improvement"
  - [section] "If an error occurs, the LLM is queried again to generate a corrected version of the program using the error message from the SAP system. This query process can include up to five iterations..." (Methodology, Sec 3.3)
  - [corpus] "From LLMs to Agents in Programming: The Impact of Providing an LLM with a Compiler" (fmr: 0.58) investigates a similar mechanism

### Mechanism 2: Model Scale and Training Drive Feedback Efficacy
- Claim: Larger, more advanced proprietary models demonstrate a superior ability to leverage feedback for correction compared to smaller, open-source counterparts
- Mechanism: Models with greater parameter counts and broader training corpora (presumably including more ABAP examples) have better initial syntactic and semantic knowledge. This stronger foundation allows them to more effectively map error messages to the correct code modifications
- Core assumption: Performance differences are primarily attributable to model architecture and training data quality/scale, not just the prompt design
- Evidence anchors:
  - [abstract] "...more powerful LLMs achieve success rates of around 75% after several iterations and benefit greatly from compiler feedback, while smaller models perform significantly weaker"
  - [section] "GPT-5 and Claude-Sonnet-4 achieve by far the best results... a significant proportion of successes is achieved... within the first two feedback rounds..." (Results, Sec 4.1.1)
  - [corpus] Weak direct evidence

### Mechanism 3: Task-Specific Algorithmic Complexity Constrains Success
- Claim: The inherent algorithmic and logical complexity of a programming task, particularly those involving list/array manipulation, is a primary bottleneck that limits LLM success even with feedback
- Mechanism: Tasks requiring complex control flow (e.g., nested loops, intricate data structure manipulation) demand deeper reasoning. A bimodal success pattern suggests that for some tasks, models either grasp the core logic or they do not, and feedback is insufficient to bridge that fundamental reasoning gap
- Core assumption: Logical reasoning is a distinct and more constrained capability than syntactic knowledge
- Evidence anchors:
  - [abstract] "Task type influenced difficulty, with List or Array Operations being most challenging"
  - [section] "...analysis of the success distribution... also reveals a bimodal pattern... This speaks for a high determination of success by the specific task definition." (Results, Sec 4.1.2)
  - [corpus] Weak direct evidence

## Foundational Learning

- Concept: Iterative Feedback Loops in Code Generation
  - Why needed here: The study's core methodology and performance gains are built on this concept. Understanding it is critical to interpreting the results
  - Quick check question: How many feedback iterations were used, and what was the primary source of the feedback signal?

- Concept: Low-Resource / Proprietary Languages (ABAP)
  - Why needed here: ABAP's status as a closed-source language with limited public training data is a central challenge, explaining why general-purpose LLMs struggle and require evaluation
  - Quick check question: What implication does ABAP's "low-resource" status have for a general-purpose LLM's ability to generate correct code?

- Concept: Bimodal Success Distribution
  - Why needed here: This key finding indicates a fundamental limit to what feedback can achieve. It helps set realistic expectations for system performance
  - Quick check question: According to the study, what does the bimodal success pattern for top models suggest about the nature of the tasks they can solve?

## Architecture Onboarding

- Component Map:
  LLM Interface -> Execution & Validation Engine -> Feedback Controller

- Critical Path:
  1. Send task prompt to LLM
  2. Parse response and extract ABAP class code
  3. Create class in SAP system, insert code, and activate (triggers syntax check)
  4. Execute associated ABAP unit test class
  5. If failure, pass compiler/test output as feedback to LLM for next attempt

- Design Tradeoffs:
  - Automation vs. Realism: The use of a clean Docker environment ensures reproducibility but may not reflect the complexity of a real SAP landscape
  - Iteration Cap: Limiting to 5 iterations is a practical trade-off. Results show gains diminish but persist into later rounds
  - Reproducibility vs. Exploration: A fixed, low temperature (0.2) was used for consistency, which may reduce the model's ability to explore diverse solutions

- Failure Signatures:
  1. Formal/Structural Failure: Model ignores output format (e.g., Codestral-22B failing to use code blocks), making code extraction impossible
  2. Syntactic Failure: Code fails activation with errors in declaration, structure, or types (dominant for most models)
  3. Semantic Failure: Code activates but unit tests fail, indicating a logic error (more common for top-tier models)

- First 3 Experiments:
  1. Baseline Test (Zero-Shot): Run the full benchmark with a selected model without any feedback iterations to establish a baseline for raw generative capability
  2. Feedback Ablation: Run the same benchmark but with exactly one feedback iteration. Compare the success rate gain to quantify the single most impactful correction step
  3. Error Type Profiling: Analyze the distribution of syntax errors (lexical, structural, declaration, etc.) for a given model to identify the most common failure modes and inform targeted prompt engineering

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific linguistic or conceptual properties cause "List or Array Operation" tasks to be significantly more challenging for LLMs compared to other ABAP categories?
- Basis in paper: [Explicit] The authors state that while this category had the highest failure rates (19 collectively unsolved tasks), "the causes for this could not be conclusively clarified"
- Why unresolved: The study quantified performance gaps between task types but did not perform a fine-grained analysis of the semantic or structural triggers for these specific errors
- What evidence would resolve it: A detailed error analysis mapping specific ABAP control structures to failure modes across different models

### Open Question 2
- Question: How does varying the temperature parameter influence the success distribution and convergence speed during iterative code generation?
- Basis in paper: [Explicit] The authors note in the Outlook that "the influence of the temperature parameter... represents a relevant starting point for further investigations" as a fixed value was used
- Why unresolved: The experiment standardized temperature (0.2) to ensure reproducibility, leaving the trade-off between output diversity and correctness unexplored
- What evidence would resolve it: Benchmark results comparing high and low temperature settings to observe shifts in the bimodal success pattern

### Open Question 3
- Question: To what extent can current LLMs reliably generate code for tasks requiring direct integration with live SAP systems and complex database tables?
- Basis in paper: [Explicit] The authors acknowledge the "empirical scope of the investigation is also limited, especially with regard to tasks that require direct integration of real SAP systems"
- Why unresolved: The benchmark utilized a standardized Docker environment rather than a live ERP instance, limiting the generalizability of results to production scenarios
- What evidence would resolve it: Performance evaluations conducted on a live S/4HANA system involving real database transactions and dependencies

## Limitations

- The proprietary nature of ABAP and SAP systems constrains independent validation of the benchmark methodology and results
- The study's focus on ABAP limits generalizability to other programming languages and domains
- Implementation challenges with open-source models (e.g., Codestral-22B prompt compliance failures) suggest potential undocumented edge cases in the benchmarking approach

## Confidence

- High Confidence: The core finding that iterative compiler feedback improves LLM code generation performance is well-supported by the methodology and results
- Medium Confidence: The relative performance differences between proprietary and open-source models are likely accurate but exact magnitude may be influenced by implementation details
- Medium Confidence: The task-specific difficulty analysis, especially for List/Array Operations, is supported by data but could benefit from deeper investigation

## Next Checks

1. Reproduce the benchmark with a single model (e.g., GPT-5) on a small subset of tasks to verify the core methodology and feedback loop implementation
2. Analyze the error messages and feedback content from the first two iterations to understand what specific compiler feedback drives improvement
3. Compare success rates on SAP-specific tasks versus adapted HumanEval tasks to quantify the impact of domain-specific knowledge requirements