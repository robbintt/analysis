---
ver: rpa2
title: Text-Routed Sparse Mixture-of-Experts Model with Explanation and Temporal Alignment
  for Multi-Modal Sentiment Analysis
arxiv_id: '2512.22741'
source_url: https://arxiv.org/abs/2512.22741
tags:
- text
- temporal
- video
- audio
- alignment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes TEXT, a multi-modal sentiment analysis model
  that incorporates explanation generation and temporal alignment. TEXT leverages
  Multi-modal Large Language Models (MLLM) to generate explanations, aligns audio
  and video representations with these explanations, and employs a novel temporality-oriented
  neural network block combining Mamba and temporal cross-attention.
---

# Text-Routed Sparse Mixture-of-Experts Model with Explanation and Temporal Alignment for Multi-Modal Sentiment Analysis

## Quick Facts
- arXiv ID: 2512.22741
- Source URL: https://arxiv.org/abs/2512.22741
- Reference count: 8
- Primary result: Outperforms three recent models and three MLLMs on four MSA datasets, achieving 0.353 MAE on CH-SIMS (13.5% improvement)

## Executive Summary
This paper proposes TEXT, a multi-modal sentiment analysis model that leverages Multi-modal Large Language Models (MLLM) to generate explanations for audio, video, and text modalities. The model aligns raw audio and video features with these explanations using a novel temporality-oriented neural network block, and employs a text-routed sparse mixture-of-experts approach for final classification. Experimental results demonstrate significant performance gains over state-of-the-art approaches across four benchmark datasets.

## Method Summary
TEXT generates modality-specific explanations via VideoLLaMA 3 fine-tuned on EMER-fine, refined by Qwen 3. It encodes text/subtitles and explanations with BERT, audio with Librosa, and video with OpenFace into 50 tokens plus an aggregation token. The model uses cross-attention to align audio/video features with explanation embeddings, a lightweight temporal alignment block combining convolutional filtering and gated interactions, text-routed sparse mixture-of-experts (SMoE), and gate fusion for final classification. The approach is trained with MSE loss for continuous sentiment prediction.

## Key Results
- Achieves 0.353 MAE on CH-SIMS dataset, a 13.5% improvement over previous approaches
- Ablation studies show ~2% performance decline when removing explanations or temporal alignment
- Text modality consistently outperforms audio and video across all metrics
- TEXT outperforms three recent models and three MLLMs on four benchmark datasets

## Why This Works (Mechanism)

### Mechanism 1: Explanation-Guided Cross-Modal Alignment
MLLM-generated explanations serve as semantic bridges that improve alignment between raw audio/video features and textual representations. VideoLLaMA 3 generates modality-specific explanations (audio, video, comments) which are encoded via BERT. These explanation embeddings then guide cross-attention alignment with raw audio (Librosa) and video (OpenFace) features through `ca(F, E) = softmax((W_Q E)(W_K F)^T)W_V F`. The core assumption is that explanations capture sentiment-relevant semantics that raw features alone may miss or misinterpret. Ablation shows ~2% drop without explanations.

### Mechanism 2: Lightweight Temporal Alignment via Conv1d + Gated Interactions
A simplified temporal alignment block combining convolutional filtering with gated cross-modal interactions outperforms both Mamba and temporal cross-attention (TCA) for short-video MSA. The block uses `left = E_a ⊕ L(Conv1d(LN(E_a)) ⊗ σ(LN(E_v)))` and symmetrically for right, then concatenates. This avoids the complexity of SSM (Mamba) and full cross-attention (TCA) while capturing temporal dependencies. The core assumption is that MSA short videos exhibit dynamic emotional transitions that require temporal synchronization but not full sequence-to-sequence modeling. Ablation shows ~2% drop when replacing with Mamba/TCA.

### Mechanism 3: Text-Routed Sparse Mixture-of-Experts for Modality Gating
Using text as the routing key for SMoE selectively activates experts based on textual semantics, leveraging text's dominant role in MSA. The SMoE layer routes based on text embeddings `SMoE(E_t, E_av)`, activating a sparse subset of experts conditioned on text content. This is followed by gate fusion `L(σ(SMoE(E_t, E_av)))` for final classification. The core assumption is that text contains more accurate sentiment information and can guide expert selection for cross-modal fusion. Ablation confirms text's dominance and SMoE's comparable value to explanations.

## Foundational Learning

- **Cross-Attention Mechanism**
  - Why needed here: Core to explanation alignment blocks; must understand how query/key/value projections enable modality-conditional feature transformation.
  - Quick check question: Can you explain how `softmax((W_Q E)(W_K F)^T)W_V F` conditions video/audio features on explanation embeddings?

- **Sparse Mixture-of-Experts (SMoE)**
  - Why needed here: Text-routed SMoE is a key architectural choice; must understand routing, expert sparsity, and load balancing concepts.
  - Quick check question: How does top-k routing differ from soft routing, and what happens if all inputs route to the same expert?

- **Mamba/State Space Models (SSM)**
  - Why needed here: Temporal alignment block is positioned as a simplification of Mamba; understanding SSM helps contextualize the design tradeoff.
  - Quick check question: What is the computational complexity advantage of SSM over Transformer attention for long sequences?

## Architecture Onboarding

- **Component map:** Explanation Generation → Uni-modal Encoding → Explanation Alignment (CA) → Temporal Alignment (E_a, E_v → E_av) → Text-Routed SMoE → Gate Fusion Classifier

- **Critical path:** Explanation Generation → Uni-modal Encoding → Explanation Alignment (CA) → Temporal Alignment (E_a, E_v → E_av) → Text-Routed SMoE → Gate Fusion Classifier

- **Design tradeoffs:** Simpler temporal block vs Mamba/TCA: Lower compute, but may not scale to longer videos. MLLM explanations: Adds semantic richness but introduces dependency on external model quality and potential hallucination. Text-routed SMoE: Leverages text dominance but risks over-reliance on single modality.

- **Failure signatures:** Performance drop >2% when explanations removed → check explanation quality/alignment. MAE increases when temporal alignment replaced → verify temporal block implementation. Expert collapse (all routing to same expert) → inspect routing weights and load balancing.

- **First 3 experiments:**
  1. Ablation on explanations: Run TEXT with/without MLLM-generated explanations on MOSEI; expect ~2% MAE increase without explanations per Table 2.
  2. Temporal block substitution: Replace proposed temporal alignment with Mamba, TCA, and simple concatenation; expect MAE 0.562 (Mamba), 0.565 (TCA), 0.580 (concat) vs 0.528 (TEXT) on MOSEI.
  3. Modality dominance check: Run uni-modal (T, A, V only) and bi-modal (T&A, T&V, A&V) variants; expect text-only to outperform audio/video-only, confirming text dominance per ablation study.

## Open Questions the Paper Calls Out

### Open Question 1
How can cumulative errors from cascading MLLM-based explanation generation be effectively eliminated? The authors state in the conclusion: "However, as we rely on multiple MLLMs, eliminating cumulative error (e.g., from VideoLLaMA) is our future work." The two-stage explanation generation (VideoLLaMA 3 → Qwen 3) creates a dependency chain where errors from the first MLLM may propagate through refinement, potentially affecting alignment quality and final predictions.

### Open Question 2
To what extent do MLLM baselines benefit from dataset memorization, and does this affect comparison validity? The discussion notes: "while MLLMs can be very good at MSA on some datasets, these datasets might be memorized by MLLMs (Wang et al. 2024). As evidence, the power of GPT-4o diminishes on Chinese datasets." If MLLM baselines have memorized training data from benchmark datasets like MOSI/MOSEI, their comparative performance may be inflated, complicating fair evaluation against TEXT.

### Open Question 3
How does TEXT perform across a broader range of languages beyond Chinese and English? The authors state: "Although this paper only considers MSA for Chinese/English, we will expand TEXT to use more MLLMs in multiple languages." The current experiments are limited to four datasets in two languages; cross-linguistic generalization of the explanation generation and temporal alignment mechanisms remains untested.

### Open Question 4
Does the quality and factual accuracy of MLLM-generated explanations directly correlate with MSA performance improvements? The paper generates explanations via MLLMs but does not validate explanation quality or analyze whether incorrect/misleading explanations degrade performance. Explanations are treated as beneficial augmentations, yet hallucinated or inaccurate MLLM outputs could misalign audio/video representations rather than improve them.

## Limitations

- The model relies on MLLM-generated explanations, introducing potential hallucination effects and cascading errors from the explanation generation pipeline.
- Critical hyperparameters for SMoE configuration, learning rate, and batch size are not specified, making exact reproduction challenging.
- Performance improvements may be partially attributed to dataset memorization by MLLM baselines, potentially inflating comparative metrics.

## Confidence

- **High Confidence:** The core architectural innovations (explanation-guided cross-modal alignment, text-routed SMoE, and lightweight temporal alignment) are well-defined and supported by ablation studies showing ~2% performance drops when components are removed.
- **Medium Confidence:** The empirical improvements over baseline models (13.5% MAE improvement on CH-SIMS) are convincing, but the lack of specified hyperparameters and potential MLLM hallucination effects introduce uncertainty about real-world generalization.
- **Low Confidence:** The scalability of the simplified temporal alignment block beyond short-video MSA scenarios is questionable, as the design rationale assumes specific video length characteristics without validation across diverse temporal scales.

## Next Checks

1. **Ablation on explanation quality:** Run TEXT with MLLM-generated explanations on MOSEI, then replace with random noise explanations or ground-truth human annotations to quantify the actual contribution versus potential MLLM hallucination effects.

2. **Temporal alignment stress test:** Implement the temporal alignment block and systematically vary video sequence lengths (50→200 tokens) to test the simplified architecture's limits against Mamba/TCA baselines under longer temporal dependencies.

3. **Expert routing analysis:** Monitor SMoE expert activation patterns during training to detect expert collapse (all routing to same expert) and validate the text-routing assumption by comparing routing distributions when using audio/video routing keys instead.