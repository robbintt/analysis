---
ver: rpa2
title: LLMs Process Lists With General Filter Heads
arxiv_id: '2510.26784'
source_url: https://arxiv.org/abs/2510.26784
tags:
- heads
- filter
- predicate
- filtering
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates how large language models perform list-processing\
  \ tasks like filtering and identifying that LLMs have learned a general, reusable\
  \ mechanism for filtering that mirrors functional programming\u2019s filter function.\
  \ Using causal mediation analysis across six diverse filter-reduce tasks, the authors\
  \ identify a small set of attention heads\u2014dubbed filter heads\u2014that encode\
  \ compact predicate representations in their query states at specific tokens."
---

# LLMs Process Lists With General Filter Heads

## Quick Facts
- arXiv ID: 2510.26784
- Source URL: https://arxiv.org/abs/2510.26784
- Authors: Arnab Sen Sharma; Giordano Rogers; Natalie Shapira; David Bau
- Reference count: 40
- Primary result: LLMs use a small set of "filter heads" that encode portable predicate representations for list filtering tasks

## Executive Summary
This paper investigates how large language models perform list-processing tasks like filtering and identifying that LLMs have learned a general, reusable mechanism for filtering that mirrors functional programming's filter function. Using causal mediation analysis across six diverse filter-reduce tasks, the authors identify a small set of attention heads—dubbed filter heads—that encode compact predicate representations in their query states at specific tokens. These representations are portable and general: they can be extracted from one context and reapplied to execute the same filtering operation on different collections, in different formats, languages, or even across tasks.

The study shows that filter heads achieve high causality scores (e.g., 0.86–0.93 for object-type filtering), maintain performance across linguistic variations and semantic domains, and are critical for task performance (task accuracy drops from 100% to <25% when filter heads are ablated). The findings reveal that transformers develop human-interpretable, modular computational primitives that generalize in ways surprisingly similar to functional programming strategies.

## Method Summary
The authors use causal mediation analysis with activation patching to identify and characterize "filter heads" - attention heads that encode filtering predicates for list-processing tasks. They create prompt pairs with different predicates and mutually exclusive item collections, cache query states from source runs, and patch them to destination runs at the last 1-2 tokens before positional encoding. A sparse binary mask over all heads is learned to identify filter heads, and causality scores measure whether patched query states cause correct target selection. The method is applied across six filter-reduce tasks with items spanning categories like objects, professions, nationalities, and landmarks.

## Key Results
- Filter heads encode compact predicate representations in query states at specific tokens
- Patching query states transfers filtering operations across contexts with causality scores of 0.86–0.93
- Only ~79 heads (<2% of total) are critical for SelectOne tasks
- Task accuracy drops from 100% to <25% when filter heads are ablated
- Filter heads generalize across languages, formats, and semantic domains

## Why This Works (Mechanism)

### Mechanism 1
A sparse set of attention heads ("filter heads") encode portable predicate representations in their query states at specific token positions. At answer tokens, filter head queries encode a geometric representation of the filtering predicate (e.g., "is_fruit"). This representation interacts with key states from preceding list items via attention, producing selective attention to items satisfying the predicate. Patching the query vector q_src from one context to another transfers the filtering operation.

### Mechanism 2
Filter heads implement selection via key-query interaction where keys carry semantic item properties and queries carry predicate specifications. The attention score Attn(q_t, K) produces selective patterns because keys encode item semantics (what items "are") while queries encode predicates (what to "find"). Swapping key states between items redirects filter head attention predictably.

### Mechanism 3
Transformers employ dual filtering strategies—lazy evaluation via filter heads and eager evaluation via pre-computed `is_match` flags—selected based on prompt structure. When questions precede items, models can eagerly evaluate each item against the predicate and store results as flags in residual latents. When questions follow items, lazy evaluation via filter heads dominates.

## Foundational Learning

- **Causal Mediation Analysis / Activation Patching**: The core methodology for identifying filter heads involves patching query states between prompts with different predicates and measuring the effect on target item logits. *Quick check*: If you patch q_src from a "find fruit" prompt to a "find vehicle" prompt, what should happen to the model's selection?

- **Query-Key-Value Attention Decomposition**: Understanding how predicates live in queries while item semantics live in keys is essential for manipulating filter head behavior. *Quick check*: In a filter head, which component (Q, K, or V) encodes "is_fruit" vs. which encodes "Peach"?

- **Functional Programming filter() Semantics**: The paper explicitly models LLM filtering on functional programming's `filter(C, ψ)` operation—understanding this abstraction clarifies what the mechanism is implementing. *Quick check*: What are the two arguments to the filter operation, and which does the query state represent?

## Architecture Onboarding

- **Component map**: Item tokens → early layers (semantic enrichment/"map") → middle-layer filter heads (predicate evaluation via Q-K attention) → late layers (reduce/answer generation). Filter heads concentrated in middle layers (~layers 28-42 in Llama-70B, out of 80). Head [35, 19] shows highest indirect effect (3.55). Only ~79 heads (<2% of total) are critical for SelectOne tasks.

- **Critical path**: Patching must target the last 1-2 tokens where answer computation occurs. Query states must be cached before positional encoding is applied.

- **Design tradeoffs**: Patching single heads often fails due to backup mechanisms—need sparse masks over multiple heads. Caching q_src before positional encoding is applied improves transfer (filter heads are "semantic heads" with minimal positional sensitivity). Averaging q_src across position-varied prompts removes order-ID contamination (+7-10 causality points).

- **Failure signatures**: Non-semantic predicates (rhyming, letter-counting): causality ~0.04. Question-before format: filter head causality drops to ~0.02 (eager evaluation dominates). CheckPresence/Counting tasks: different sub-circuits, lower filter head reliance.

- **First 3 experiments**:
  1. Replicate the basic patching experiment: cache q_src from a "find fruit" prompt, patch to a "find vehicle" prompt with different items, verify the model selects the fruit in the destination list.
  2. Run ablation on identified filter heads (zero attention to all tokens except <BOS>) and measure accuracy drop on SelectOne tasks.
  3. Test cross-lingual transfer: extract q_src from English prompts, apply to Spanish/French/Hindi destination prompts, compare causality scores.

## Open Questions the Paper Calls Out

- Do smaller language models (below 27B parameters) develop the same localized filter head mechanism, or do parameter constraints cause predicate representations to be entangled with other information in query states?

- What additional circuits implement the specialized aggregation required for the Counting task, which shows asymmetric generalization patterns with Select* tasks?

- What alternative mechanisms do LMs use for filtering based on non-semantic properties (e.g., phonological similarity, letter counting), where filter heads show near-zero causality?

## Limitations

- Filter heads show reduced effectiveness for non-semantic predicates (causality ~0.04 for rhyming/letter-counting tasks)
- Dual implementation (lazy vs eager evaluation) means filter heads are not universally employed across all prompt formats
- The study focuses on Llama-70B; generalizability to other model architectures remains untested

## Confidence

- **High confidence**: Filter heads encode predicate representations in query states at specific tokens; activation patching transfers filtering operations across contexts; filter heads are concentrated in middle layers; ablation causes dramatic accuracy drops
- **Medium confidence**: Filter heads generalize across languages, formats, and semantic domains; the mechanism mirrors functional programming's filter function conceptually; predicate representations are "portable" and reusable
- **Low confidence**: All LLMs develop this specific filter head mechanism; filter heads represent the primary or only mechanism for list filtering

## Next Checks

1. **Cross-architecture validation**: Test filter head identification and transfer on multiple model families (GPT, Claude, Mistral) to verify the mechanism isn't Llama-specific.

2. **Semantic boundary testing**: Systematically test filter heads on increasingly abstract or complex predicates (from concrete objects to metaphors, negations, or nested conditions) to map the limits of semantic reasoning.

3. **Dual mechanism isolation**: Design controlled experiments to precisely measure when filter heads vs. eager evaluation dominate, and whether both can operate simultaneously on different parts of the same input.