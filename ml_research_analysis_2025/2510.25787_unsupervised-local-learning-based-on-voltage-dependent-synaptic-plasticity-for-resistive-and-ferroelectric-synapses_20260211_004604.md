---
ver: rpa2
title: Unsupervised local learning based on voltage-dependent synaptic plasticity
  for resistive and ferroelectric synapses
arxiv_id: '2510.25787'
source_url: https://arxiv.org/abs/2510.25787
tags:
- learning
- devices
- device
- switching
- voltage
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces voltage-dependent synaptic plasticity (VDSP)
  as a bio-inspired, local learning rule for unsupervised training of spiking neural
  networks (SNNs) using memristive synapses. Unlike conventional spike-timing-dependent
  plasticity (STDP), VDSP uses the neuron's membrane potential to determine synaptic
  updates, enabling efficient online learning without complex pulse-shaping circuits.
---

# Unsupervised local learning based on voltage-dependent synaptic plasticity for resistive and ferroelectric synapses

## Quick Facts
- **arXiv ID**: 2510.25787
- **Source URL**: https://arxiv.org/abs/2510.25787
- **Reference count**: 40
- **Primary result**: Voltage-Dependent Synaptic Plasticity (VDSP) achieves >83% MNIST accuracy on 200-neuron spiking neural networks using three different memristive technologies.

## Executive Summary
This work introduces Voltage-Dependent Synaptic Plasticity (VDSP), a bio-inspired local learning rule for unsupervised training of spiking neural networks using memristive synapses. Unlike conventional spike-timing-dependent plasticity (STDP), VDSP uses the neuron's membrane potential to determine synaptic updates, enabling efficient online learning without complex pulse-shaping circuits. The approach is validated across three distinct memristive technologies—TiO₂, HfO₂-based metal-oxide synapses, and HfZrO₄-based ferroelectric tunnel junctions—each exhibiting unique switching characteristics.

System-level simulations on MNIST classification achieve over 83% accuracy with 200 neurons, demonstrating performance on par with or exceeding STDP-based methods. The study also evaluates the impact of device variability on learning and shows that VDSP is robust to threshold mismatches and resistance state variations. Overall, VDSP offers a hardware-efficient, scalable solution for neuromorphic computing on edge devices.

## Method Summary
The method employs a two-layer spiking neural network with 784 input LIF neurons rate-encoding MNIST pixels plus Gaussian noise, connected through a 1T1R memristive crossbar to 200 ALIF output neurons with Winner-Take-All inhibition. VDSP updates weights based on the post-synaptic membrane potential (V_mem) rather than precise spike timing, using a voltage threshold mechanism that filters weak correlations. Weight updates are determined by V_prog = V_mem × sf × θ, where sf is a scaling factor tuned per device technology. The system trains unsupervised for 3 epochs on MNIST, with labels assigned post-training via majority voting.

## Key Results
- Achieved >83% classification accuracy on MNIST using 200 output neurons with VDSP across three memristive technologies
- Demonstrated robustness to device variability with up to 20% threshold mismatch without significant accuracy degradation
- Showed VDSP outperforms conventional STDP in MNIST classification accuracy and requires simpler hardware implementation

## Why This Works (Mechanism)

### Mechanism 1
VDSP approximates spike-timing causality using instantaneous membrane potential, removing the need for complex pulse-shaping circuits required by standard STDP. The post-synaptic neuron's membrane potential (V_mem) acts as a slow analog trace of recent activity. A high V_mem implies an imminent spike (causal pre-post pairing → LTP), while a low V_mem (after reset) implies a recent spike (anti-causal post-pre pairing → LTD). The core assumption is that the neuron's membrane time constant (τ_m) is sufficiently slow to serve as a valid proxy for the precise spike-timing window found in biology.

### Mechanism 2
Weight updates are regulated by a voltage threshold "dead zone," which filters weak correlations and provides inherent robustness to device mismatch. The synapse updates only when the scaled programming voltage (V_prog = V_mem · sf · θ) exceeds the memristor's switching threshold (θ). Small fluctuations in membrane potential (noise/weak correlation) fall into the non-switching region (Dead Zone), preventing erratic weight drift. The scaling factor (sf) can be tuned to align the neuron's dynamic range with the specific switching threshold of the memristor technology.

### Mechanism 3
Injecting Gaussian noise into the input improves generalization by stochastic sampling of the membrane potential. Noise jitters the membrane potential, ensuring the resulting programming voltage stochastically traverses the switching threshold. This effectively creates a probabilistic update rule, preventing the network from overfitting to rigid binary patterns. The magnitude of injected noise is calibrated to induce beneficial exploration without destabilizing the integration of the signal itself.

## Foundational Learning

- **Concept: Leaky Integrate-and-Fire (LIF) Neurons**
  - **Why needed here:** VDSP depends entirely on the analog trajectory of the membrane potential (V_mem). Unlike standard artificial neurons, the "leak" (τ_m) is not just a regularization technique but the memory buffer that defines the learning window.
  - **Quick check question:** If you decrease the leak time constant (τ_m) from 30ms to 1ms, how would this affect the network's ability to correlate spikes separated by 10ms?

- **Concept: Resistive Switching Thresholds (Memristors)**
  - **Why needed here:** The hardware implementation relies on the device physics where no conductance change occurs until a specific voltage threshold (θ) is crossed. Understanding this non-linearity is critical to mapping the biological "learning rate" to the circuit "scaling factor."
  - **Quick check question:** Why is the "dead zone" (sub-threshold region) actually beneficial for preventing catastrophic forgetting in this specific architecture?

- **Concept: Competitive Learning (Winner-Take-All)**
  - **Why needed here:** The system is unsupervised. Without labels, the network requires a mechanism (WTA) to force specialization, ensuring different neurons learn to recognize different digits rather than all converging on the same feature.
  - **Quick check question:** If the lateral inhibition in the WTA layer is too weak, what failure mode would you observe in the learned weights of the output layer?

## Architecture Onboarding

- **Component map:** Input Layer (784 LIF Neurons) -> Synaptic Array (1T1R Memristive Crossbar) -> Control Logic (VDSP Amplifier) -> Output Layer (ALIF Neurons with WTA)

- **Critical path:** Tuning the Scaling Factor (sf). You must map the neuron's operating voltage range (e.g., 0–1.2V) to the device's switching threshold (e.g., 1.45V for TiO2). An incorrect sf results in either no learning (updates never cross threshold) or binary snapping (weights saturate instantly).

- **Design tradeoffs:**
  - **Pulse Width vs. Linearity:** Shorter pulses (200ns) are more energy-efficient but reduce the granularity of analog states, pushing the device toward binary behavior.
  - **Variability (RSD_θ) vs. Scaling Factor (sf):** A high sf ensures even devices with high thresholds learn (robustness), but it reduces precision by causing abrupt weight updates.

- **Failure signatures:**
  - **"Stuck" Weights:** Accuracy saturates early; histogram of weights shows no distribution (all at 0 or 1). Likely cause: Scaling factor sf is too high (saturation) or too low (sub-threshold).
  - **Single Neuron Dominance:** One output neuron fires for all inputs. Likely cause: WTA inhibition time constant (τ_wta) is too short, or input bias current is missing.
  - **Random Performance:** Accuracy hovers around 10% (random guess). Likely cause: Noise variance is too high, or neuron time constants are mismatched to input presentation time.

- **First 3 experiments:**
  1. **Device Characterization:** Run the "random pulse protocol" on your specific memristor batch to extract the model parameters (θ, α, γ) using the Levenberg-Marquardt fit (as per Section 5.3). Do not proceed to network simulation without these fitted parameters.
  2. **Scaling Factor Sweep:** Simulate a 10-neuron network on a small MNIST subset (1,000 samples). Perform a grid search on sf_p and sf_d to find the "Goldilocks zone" where accuracy peaks (consult Supplementary Fig. 3).
  3. **Variability Injection:** Introduce 20% Relative Standard Deviation (RSD) to the switching thresholds in simulation. Validate that increasing the scaling factor recovers the performance drop, proving your system's robustness (replicate Fig. 6b).

## Open Questions the Paper Calls Out

### Open Question 1
Can VDSP be effectively translated to deep network topologies, such as convolutional or recurrent neural networks? The authors state, "Future efforts could explore translating this rule to different topologies such as convolutional, recurrent, and multilayer networks, paving the way for improved accuracy and extraction of transformation-invariant features." This remains unresolved because the current study utilized a minimal two-layer SNN to isolate device-level impacts on learning, leaving the scalability of VDSP to deeper, hierarchical architectures untested.

### Open Question 2
How can VDSP be adapted for highly stochastic memristive devices, such as Electrochemical Metallization (ECM) cells or Magnetic Tunnel Junctions (MTJs)? The discussion notes that ECMs and MTJs "may exhibit greater stochasticity" and suggests future research should "aim to quantify the effects of this stochastic behavior and explore additional material stacks." The study focused on TiO₂, HfO₂, and HZO devices; it is unknown if the deterministic voltage-to-weight mapping of VDSP holds or requires modification for highly probabilistic switching mechanisms.

### Open Question 3
What is the specific relationship between a device's material stack and the availability of intermediate conductance states under VDSP? The conclusion lists as future work: "investigating the relationship between the device's material stack and the resulting intermediate conductance states." The paper notes that model parameters are heavily influenced by programming conditions, making it difficult to attribute performance (e.g., number of stable analog states) solely to the material composition.

## Limitations
- The approach requires precise calibration of the scaling factor to match each device's switching threshold, making it sensitive to manufacturing variations across different memristor batches
- The biological inspiration (membrane potential as correlation memory) may not fully capture the complexity of calcium-based plasticity mechanisms found in biological neurons
- Gaussian noise injection, while shown to improve robustness, lacks detailed theoretical justification for its specific magnitude and could destabilize learning if not carefully calibrated

## Confidence
- **High confidence**: The core VDSP mechanism (using membrane potential for local updates) and its implementation across three distinct memristor technologies are well-supported by simulation results showing >83% MNIST accuracy
- **Medium confidence**: The robustness claims under device variability are supported by simulations but would benefit from experimental validation on actual hardware
- **Low confidence**: The specific impact of Gaussian noise injection on learning dynamics and generalization requires more rigorous theoretical grounding

## Next Checks
1. **Time constant validation**: Systematically vary the membrane time constant (τ_m) from 10ms to 50ms and measure the impact on correlation memory and learning accuracy, particularly for input patterns with different temporal separations
2. **Noise calibration study**: Perform a parameter sweep of Gaussian noise variance (σ from 0.01 to 0.5) to identify the optimal range that improves robustness without degrading signal-to-noise ratio in the membrane potential
3. **Device-to-device variability test**: Implement a Monte Carlo simulation with 50% RSD in switching thresholds and scaling factors to evaluate whether the proposed robustness mechanisms hold under extreme manufacturing variations