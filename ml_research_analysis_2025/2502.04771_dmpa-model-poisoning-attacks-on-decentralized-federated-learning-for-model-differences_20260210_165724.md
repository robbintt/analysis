---
ver: rpa2
title: 'DMPA: Model Poisoning Attacks on Decentralized Federated Learning for Model
  Differences'
arxiv_id: '2502.04771'
source_url: https://arxiv.org/abs/2502.04771
tags:
- attack
- poisoning
- malicious
- attacks
- dmpa
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses model poisoning attacks in decentralized federated
  learning (DFL), where malicious clients aim to degrade the performance of benign
  models by manipulating their local model parameters. Existing model poisoning attacks
  primarily target centralized federated learning (CFL) and are less effective in
  DFL due to its decentralized nature and longer communication paths.
---

# DMPA: Model Poisoning Attacks on Decentralized Federated Learning for Model Differences

## Quick Facts
- arXiv ID: 2502.04771
- Source URL: https://arxiv.org/abs/2502.04771
- Authors: Chao Feng; Yunlong Li; Yuanzhe Gao; Alberto Huertas Celdrán; Jan von der Assen; Gérôme Bovet; Burkhard Stiller
- Reference count: 26
- Primary result: DMPA outperforms existing model poisoning attacks in decentralized federated learning, achieving significantly lower F1 scores for benign models across multiple datasets and topologies.

## Executive Summary
This paper introduces DMPA, a novel model poisoning attack designed specifically for decentralized federated learning (DFL) environments. Unlike centralized federated learning, DFL's decentralized nature and longer communication paths make traditional poisoning attacks less effective. DMPA leverages the differential characteristics of multiple malicious client models to compute an optimal poisoning strategy by analyzing the correlation matrix of model parameter differences and extracting the principal eigenvector. Experimental results demonstrate that DMPA consistently outperforms state-of-the-art attacks across three datasets (MNIST, Fashion-MNIST, CIFAR-10) and three DFL topologies (fully connected, star, ring), with F1 scores as low as 0.0169 in CIFAR-10 with 40% malicious clients.

## Method Summary
DMPA operates through a collusion-based mechanism where malicious clients share their model updates to compute an optimal attack direction. The attack calculates the correlation matrix of parameter differences among malicious clients, extracts the eigenvector corresponding to the maximum eigenvalue, and uses this to adjust the attack direction. The final malicious update combines a reversed gradient with a projection based on the principal eigenvector, selectively injected into the top k% of parameters to maximize impact while maintaining stealth. The method is evaluated across three standard datasets with IID data partitioning and tested against various aggregation defenses including FedAvg, Krum, Median, and Trimmed Mean.

## Key Results
- DMPA consistently achieves lower F1 scores for benign models compared to existing attacks across all tested scenarios
- In CIFAR-10 with 40% malicious clients, DMPA achieves F1 scores as low as 0.0169
- DMPA demonstrates superior performance in fully connected and star topologies compared to ring topologies, which show a "buffering" effect due to longer communication paths
- The attack remains effective against multiple defense mechanisms including Krum and Trimmed Mean aggregation

## Why This Works (Mechanism)

### Mechanism 1: Correlation-Based Attack Direction Optimization
- **Claim:** Computing the principal component of parameter differences among malicious clients identifies the optimal direction to maximize training loss in DFL.
- **Mechanism:** DMPA aggregates updates from all malicious clients, calculates the correlation matrix of parameter differences, and extracts the eigenvector corresponding to the maximum eigenvalue. This eigenvector serves as an "Angle Bias Vector" representing the most significant deviation direction shared among malicious actors.
- **Core assumption:** The direction of maximum variance among malicious models correlates with the direction of maximum vulnerability for benign models.
- **Evidence anchors:** [abstract] "calculates the correlation matrix of model parameter differences, extracts the eigenvector corresponding to the maximum eigenvalue"; [section 4, Algorithm 1] Steps 3-7; [corpus] Related work "Gradient Purification" confirms threat of manipulated gradients but doesn't validate eigen-decomposition method.
- **Break condition:** If malicious clients cannot exchange models, the correlation matrix cannot be computed.

### Mechanism 2: Biased Parameter Reversal
- **Claim:** Reversing model parameters creates basic denial-of-service, but projecting the "Angle Bias Vector" onto these reversed parameters ensures the attack survives aggregation.
- **Mechanism:** The attack constructs final malicious update using $U_{new} = -U + P$, where $-U$ flips the gradient to increase loss and $P$ (derived from top eigenvector) biases this flip to align with principal malicious deviation.
- **Core assumption:** A structured reversal based on collective malicious variance is more potent than simple gradient inversion.
- **Evidence anchors:** [section 4, Algorithm 1] Step 9; [section 4] "adjusting the negative parameters, thereby generating an effective attack model"; [corpus] "RepuNet" discusses mitigating corrupted models but doesn't counter eigen-biased reversals.
- **Break condition:** If aggregation rule strictly filters outliers based on Euclidean distance, reversed vector may be excluded.

### Mechanism 3: Sparse Top-K Parameter Injection
- **Claim:** Injecting malicious signal into only top k% of parameters prevents attack effect from decreasing after model averaging.
- **Mechanism:** After computing mean malicious vector, DMPA identifies top k% of parameters with largest magnitude and splices these into the average vector, creating hybrid update that's mostly normal but contains lethal spikes in specific dimensions.
- **Core assumption:** Aggregation functions are vulnerable to sparse, high-magnitude perturbations that don't significantly shift global model's norm.
- **Evidence anchors:** [section 4, Algorithm 1] Steps 11-14; [section 4, Method Overview] "Extract the uneven values... use them to fill the corresponding positions... preventing the attack effect from decreasing after the model is averaged"; [corpus] No direct evidence found regarding top-k splicing efficacy.
- **Break condition:** If defense employs dimension-wise trimming or strict bound checking per parameter, large magnitude spikes will be clipped.

## Foundational Learning

**Concept: Decentralized Federated Learning (DFL) Topologies**
- **Why needed here:** The paper explicitly tests on Fully, Ring, and Star topologies. Attack effectiveness depends on how quickly poison propagates through these graphs.
- **Quick check question:** In a Ring topology with 10 clients, how many hops does it take for a model update from Client 1 to reach Client 5?

**Concept: Eigenvalues and Eigenvectors (PCA)**
- **Why needed here:** DMPA's core innovation uses eigenvector of maximum eigenvalue of correlation matrix. Understanding this captures "principal direction" of data is essential.
- **Quick check question:** Why does maximum eigenvalue correspond to direction of greatest variance in model updates?

**Concept: Robust Aggregation Functions (Krum, Trimmed Mean)**
- **Why needed here:** Paper frames DMPA as attack that bypasses these specific defenses. Must understand Krum selects based on local distance to neighbors, while Trimmed Mean excludes statistical outliers.
- **Quick check question:** Why would "LIE" attack (mimicking mean) fail against Krum, and how does DMPA's eigen-projection attempt to overcome this?

## Architecture Onboarding

**Component map:** Malicious Collusion Module -> Eigen-Processor -> Attack Vector Generator -> DFL Interface

**Critical path:** The Eigen-Processor is the bottleneck. Attack requires calculating covariance and eigen-decomposition of model parameter matrix. For large models (e.g., ResNet on CIFAR-10), computational overhead is significant and creates timing side-channel compared to benign clients.

**Design tradeoffs:**
- *Attack Strength vs. Collusion Overhead:* Mechanism requires sharing full model parameters among malicious nodes, increasing bandwidth usage and detection risk
- *Top-k Ratio:* Lower k (e.g., 1%) increases stealth but may reduce impact on complex datasets; higher k increases impact but risks detection by norm-based defenses

**Failure signatures:**
- **Isolation Failure:** If malicious clients fewer than 2, correlation matrix undefined, attack defaults to simple inversion
- **Topology Choke:** In Ring topology, if single benign client between two malicious clients, attack propagation significantly slowed ("buffered")
- **Numerical Instability:** Check division by zero in correlation matrix normalization if parameter variances near zero

**First 3 experiments:**
1. Implement DMPA on MNIST dataset (simple MLP) across Fully Connected vs. Ring topologies to replicate "buffering" effect observed in Table 1
2. Test DMPA against *Krum* aggregation rule on CIFAR-10 to measure if Top-k masking lowers Euclidean distance enough to be selected by Krum's nearest-neighbor logic
3. Vary malicious client rate from 10% to 40% to verify if F1 score degradation is linear or shows threshold where eigen-projection becomes significantly more effective than Min-Sum/LIE

## Open Questions the Paper Calls Out

**Open Question 1:** How does DMPA perform in DFL environments where data is distributed in a non-IID manner across clients?
- **Basis in paper:** [explicit] Conclusion states existing studies utilize IID data, simplifying attack process, and identifies "strategies for effective assaults under non-IID conditions" as future research focus
- **Why unresolved:** Non-IID data heterogeneity introduces natural variance in model updates that may obscure malicious perturbations or alter correlation matrix characteristics
- **What evidence would resolve it:** Experimental results showing DMPA's effectiveness on benchmark datasets partitioned with non-IID distributions (e.g., Dirichlet distribution with α < 1)

**Open Question 2:** What specific defense mechanisms can effectively detect or mitigate DMPA attacks that utilize feature angle deviations?
- **Basis in paper:** [explicit] Conclusion highlights DMPA provides insights for "development of robust mechanisms to protect against potential threats," noting current defenses like Krum or Trimmed Mean are often insufficient
- **Why unresolved:** Paper demonstrates DMPA's success against standard aggregators but doesn't propose dedicated defense capable of filtering eigenvector-based perturbations
- **What evidence would resolve it:** Proposal and validation of new aggregation function or anomaly detection algorithm that successfully maintains high model accuracy in presence of DMPA attacks

**Open Question 3:** Is DMPA effective in dynamic DFL topologies where communication links between clients change over time?
- **Basis in paper:** [inferred] Introduction mentions DFL allows for "dynamic configurations," yet evaluation is restricted to static topologies
- **Why unresolved:** Dynamic topologies alter propagation speed and reach of poisoned models; frequent changes in neighbors might disrupt attack's stability or prevent convergence of malicious strategy
- **What evidence would resolve it:** Evaluation of DMPA's success rate in simulation where graph G=(V,E) is rewired or regenerated at regular communication intervals

## Limitations
- Assumes malicious clients can collude to share full model updates without detection, which may be impractical in real-world deployments
- Does not provide ablation studies on top-k parameter ratio (set at 10%) or analyze computational overhead of eigen-decomposition on large models
- Defense evaluations focus on aggregation-level protections but don't address detection of collusion channels or model update norms

## Confidence
- **High confidence:** Correlation-based attack direction optimization (Mechanism 1) is well-specified with explicit algorithmic steps and clear mathematical formulation
- **Medium confidence:** Biased parameter reversal (Mechanism 2) and sparse top-k injection (Mechanism 3) are theoretically sound but lack ablation studies on individual contributions
- **Low confidence:** Assumption that malicious collusion channels exist without detection in real-world DFL deployments is not validated

## Next Checks
1. Implement DMPA on MNIST across Fully Connected vs. Ring topologies to replicate "buffering" effect
2. Test DMPA against Krum aggregation rule on CIFAR-10 to verify if Top-k masking bypasses distance-based filtering
3. Vary malicious client rate from 10% to 40% to determine if F1 score degradation follows linear trends or threshold effects