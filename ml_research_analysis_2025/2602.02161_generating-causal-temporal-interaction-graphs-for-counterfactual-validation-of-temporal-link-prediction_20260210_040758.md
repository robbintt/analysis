---
ver: rpa2
title: Generating Causal Temporal Interaction Graphs for Counterfactual Validation
  of Temporal Link Prediction
arxiv_id: '2602.02161'
source_url: https://arxiv.org/abs/2602.02161
tags:
- causal
- event
- events
- temporal
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a framework for counterfactual validation
  of temporal link prediction (TLP) models by generating causal temporal interaction
  graphs (CTIGs) with known ground-truth causal structure. A structural equation model
  for continuous-time event sequences is proposed, supporting both excitatory and
  inhibitory effects, and extended to temporal interaction graphs.
---

# Generating Causal Temporal Interaction Graphs for Counterfactual Validation of Temporal Link Prediction

## Quick Facts
- arXiv ID: 2602.02161
- Source URL: https://arxiv.org/abs/2602.02161
- Reference count: 16
- Key outcome: Framework for counterfactual validation of TLP models using CTIGs with known causal structure, showing TGN outperforms on original vs. distorted data while JODIE remains flat.

## Executive Summary
This paper introduces a framework for generating synthetic causal temporal interaction graphs (CTIGs) with known ground-truth causal structure, enabling counterfactual validation of temporal link prediction (TLP) models. A structural equation model (SEM) is proposed for continuous-time event sequences that supports both excitatory and inhibitory effects, extended to CTIGs with node features and edge-based causal influence. The framework introduces a distance metric based on cross-model predictive error to quantify causal differences between models. Experiments validate the hypothesis that predictors trained on one causal model degrade when evaluated on sufficiently distant models, using controlled causal shifts and timestamp shuffling as distortions. Results show TGN captures temporal causality better than JODIE, which relies more on static structural features.

## Method Summary
The method generates causal temporal interaction graphs through a multi-stage process. First, event sequences are generated using a structural equation model (SEM) where binary triggers from Poisson processes are resolved into events based on weighted sums of active parents within a finite time window, supporting both excitatory and inhibitory effects. This is extended to CTIGs by mapping event types to node pairs, incorporating node features and edge-based causal influence matrices. The framework quantifies causal differences between models using cross-model predictive error, where an oracle predictor parameterized by one model attempts to predict events from another. Counterfactual validation is performed by training TLP models on original sequences and evaluating them on distorted versions created through controlled causal shifts or timestamp shuffling. The performance gap between original and distorted data serves as a metric for causal sensitivity.

## Key Results
- TGN shows significant performance degradation on causally distorted data compared to original data, while JODIE performance remains largely unchanged
- The cross-model predictive error distance metric stabilizes with sufficient sequence length and realizations
- Timestamp shuffling as a stochastic distortion creates measurable causal distance when the original model captures temporal dependencies

## Why This Works (Mechanism)

### Mechanism 1: SEM-Based Event Generation with Dual Influence
- **Claim**: Continuous-time event sequences can be generated with known causal ground truth by using a structural equation model (SEM) that supports both excitatory and inhibitory historical effects.
- **Mechanism**: For each event type $i$, a binary trigger $u_i(t)$ is sampled from a Poisson process. The event occurs only if the weighted sum of active structural parents (recent events in window $\bar{\tau}$) satisfies a threshold. Specifically, $x_i(t) = u_i(t) \cdot \mathbb{I}\{\sum \Theta_{i,j} x'_j(t) \ge 0\}$. If $\Theta_{i,j} > 0$, parent $j$ excites $i$; if $\Theta_{i,j} < 0$, it inhibits $i$.
- **Core assumption**: The causal influence of past events is confined to a finite time window $\bar{\tau}$, and the underlying causal graph is acyclic (specifically, semi-Markovian).
- **Evidence anchors**:
  - [abstract] "We first introduce a structural equation model for continuous-time event sequences that supports both excitatory and inhibitory effects..."
  - [section] Section 2, Eq. (2) defines the SEM.
  - [corpus] Corpus discusses TLP generally but lacks direct evidence for this specific SEM generation technique.
- **Break condition**: If the temporal dependencies extend beyond the fixed window $\bar{\tau}$ or if the event space has unobserved confounders not modeled by the exogenous triggers $U$.

### Mechanism 2: Cross-Model Predictive Error as Causal Distance
- **Claim**: The distance between two parametric causal models can be quantified by the performance degradation of an oracle predictor when swapped between models.
- **Mechanism**: A predictor (oracle) parameterized by Model $B$ attempts to predict events generated by Model $A$. The distance $d_B(S_A)$ is the expected error. The final metric $\bar{d}_{A,B}$ is the geometric mean of cross-prediction errors, averaged over multiple realizations.
- **Core assumption**: If two causal models are distinct, a predictor optimized for one will necessarily struggle to predict the other, and this error scales with the structural difference.
- **Evidence anchors**:
  - [abstract] "...propose a distance metric based on cross-model predictive error..."
  - [section] Section 4, Eq. (7-9) define the distance metric.
  - [corpus] Corpus neighbors discuss TLP transfer learning, which aligns with the intuition of cross-model performance, but no direct validation of this specific metric is found.
- **Break condition**: If the predictor has infinite capacity or if the models differ only in ways irrelevant to the specific prediction task (e.g., different trigger rates but same structural parents), the distance may not capture causal divergence.

### Mechanism 3: Counterfactual Sensitivity Testing via Distortion
- **Claim**: A valid TLP model trained on a causal dataset should show a positive performance gap when evaluated on a causally distorted version of that dataset, provided the distortion distance is sufficient.
- **Mechanism**: The framework trains a predictor on a "true" sequence $S_0$ and evaluates it on a distorted sequence $S^\dagger$. If the predictor captures causal structure, performance on $S_0$ should exceed $S^\dagger$ ($\Delta > 0$). Distortion is induced either by shifting causal parameters or stochastically shuffling timestamps.
- **Core assumption**: A robust predictive model relies on the specific temporal ordering and causal mechanisms of the training data, rather than just aggregate statistics or frequency.
- **Evidence anchors**:
  - [abstract] "Experiments show that TGN performs better on original data compared to its distorted counterpart, while JODIE remains largely flat..."
  - [section] Section 6, Experiment A & B results (Fig 6, 7).
  - [corpus] "Are We Really Measuring Progress?" questions current TLP evaluation, supporting the need for this counterfactual approach.
- **Break condition**: If the TLP model is invariant to temporal shuffling (like JODIE in the paper), it relies only on static structural features, rendering the causal validation test flat/ineffective.

## Foundational Learning

- **Concept**: **Semi-Markovian Causal Models**
  - **Why needed here**: The paper explicitly proves its generative model is semi-Markovian but *not* Markovian (due to dependent background variables). Understanding this distinction is required to see why standard conditional independence tests might fail without the full structural context.
  - **Quick check question**: Why does the dependency of $X'_i(t)$ on $X'_j(t)$ (via overlapping parent sets) prevent the model from being fully Markovian?

- **Concept**: **Positive-Unlabelled (PU) Learning**
  - **Why needed here**: The paper notes that in TLP, we observe interactions (positives) but lack explicit "non-interaction" labels. The "negative sampling" strategy (constructing $\Phi$) treats non-occurring events as negatives, a core PU learning concept required to train the predictor.
  - **Quick check question**: In the paper's negative sampling strategy, are sampled negative events guaranteed to be causally infeasible?

- **Concept**: **Excitatory vs. Inhibitory Effects**
  - **Why needed here**: Unlike standard Hawkes processes often cited in literature (which typically focus on excitation), this framework explicitly models inhibition ($\Theta < 0$). This is critical for generating realistic CTIGs where an event can suppress future interactions.
  - **Quick check question**: How does the sign of the parameter $\Theta_{i,j}$ determine the relationship between parent event $j$ and child event $i$ in the SEM?

## Architecture Onboarding

- **Component map**:
  1. **Trigger Generator**: Homogeneous Poisson processes for each event type.
  2. **SEM Evaluator**: Checks active parents $P_i(t)$ within window $\bar{\tau}$ against parameters $\Theta$ to resolve event occurrence.
  3. **Graph Builder**: Maps event types to node pairs (edges) to form the CTIG.
  4. **Distance Estimator**: Oracle-based cross-validation module to compute $\bar{d}$.
  5. **Counterfactual Validator**: TLP model trainer (e.g., TGN) and evaluator loop.

- **Critical path**: The *Trigger Generator* produces candidate timestamps. The *SEM Evaluator* filters these based on historical context. The resulting sequence is fed into the *Counterfactual Validator* to train a TLP model, which is then tested against a distorted sequence to measure the performance gap $\Delta$.

- **Design tradeoffs**:
  - **Sparsity ($\nu_1$) vs. Connectivity**: The threshold $\nu_1$ controls how many edges have causal influence. Higher $\nu_1$ yields sparser causal graphs but may miss weak causal signals.
  - **Window Size ($\bar{\tau}$)**: A larger window captures longer-range dependencies but increases computational complexity and may introduce noise from irrelevant history.
  - **Negative Sampling**: Transductive sampling is computationally cheap but introduces label noise (false negatives). True counterfactual "infeasible" sampling is theoretically sound but practically harder.

- **Failure signatures**:
  - **Flat Performance Gap**: If the TLP model (like JODIE in the paper) shows no degradation on distorted data ($Y_x \approx Y_{x'}$), it indicates the model is causally insensitive or relying purely on static features.
  - **High Variance in $\bar{d}$**: If the distance metric doesn't converge (Fig 3), the generated sequence is likely too short or the causal effects too weak to distinguish models.
  - **Oracle Failure**: If the oracle cannot predict the true model better than random, the causal signal is drowned out by the stochastic triggers ($U_i$).

- **First 3 experiments**:
  1. **Variance Convergence Check**: Replicate Figure 3. Generate sequences of increasing length ($T$) and plot $\text{Var}(\bar{d}_{AB})$. Verify that the distance metric stabilizes, ensuring the reliability of the "causal distance" definition.
  2. **Oracle Hypothesis Test**: Replicate Figure 5. Train an oracle on $C_0$, evaluate on $C_0$ vs $C^\dagger$. Confirm that $\Delta > 0$ only when $\bar{d}_{0,\dagger}$ exceeds threshold $\beta$. This validates the core hypothesis.
  3. **TLP Benchmark Sensitivity**: Train TGN and JODIE on $S_{train}$. Evaluate on original $S_{test}$ vs. timestamp-shuffled $S'_{test}$. Compare the performance gap (Fig 6/7) to identify which architecture captures temporal causality vs. static structure.

## Open Questions the Paper Calls Out

- **Question**: How do diverse state-of-the-art temporal link prediction (TLP) architectures perform under the proposed counterfactual validation framework?
- **Basis in paper**: [explicit] The authors state in a footnote: "Extending the benchmark to additional TLP models is left to future work."
- **Why unresolved**: The empirical study was restricted to only two representative models (TGN and JODIE), leaving the behavior of other architectures unknown.
- **What evidence would resolve it**: Evaluation results from applying the CTIG generation and counterfactual testing protocol to a wider suite of models, such as GraphMixer or DyGFormer.

- **Question**: Can this counterfactual validation framework be effectively adapted for predictive domains outside of temporal interaction graphs?
- **Basis in paper**: [explicit] The conclusion suggests: "Furthermore, the same methodology can be extended to other predictive domains where the performance of a model under causal shifts is of interest."
- **Why unresolved**: The current work focuses exclusively on graph-structured interaction data, and the generalizability of the structural equation model to other time-series domains is untested.
- **What evidence would resolve it**: Successful instantiation of the framework on non-graph continuous-time event sequences, such as financial transactions or medical records, demonstrating measurable performance gaps under causal distortion.

## Limitations

- The framework relies on synthetic data generation, which may not fully capture the complexity of real-world temporal interaction graphs
- The assumption of semi-Markovian causal structure and finite temporal windows may not hold in all scenarios
- The effectiveness of timestamp shuffling as a counterfactual distortion mechanism for real datasets remains untested

## Confidence

- **High confidence**: The SEM-based event generation mechanism (Mechanism 1) and the cross-model predictive error distance metric (Mechanism 2) are mathematically well-defined and theoretically sound
- **Medium confidence**: The counterfactual sensitivity testing via distortion (Mechanism 3) is empirically validated on synthetic data but requires further testing on real-world datasets to establish robustness
- **Low confidence**: The specific hyperparameter choices (e.g., window size τ, thresholds ν₁, ν₀) and their impact on the framework's performance are not extensively explored

## Next Checks

1. **Real-world dataset validation**: Apply the framework to a real temporal interaction graph dataset (e.g., social network interactions, user-item engagement) and assess whether TGN and JODIE exhibit the same performance gap behavior observed in synthetic data.

2. **Ablation study on temporal window size**: Systematically vary the causal window τ and measure its effect on the distance metric d̄ and the performance gap Δ. This would quantify the sensitivity of the framework to the Markov assumption.

3. **Alternative distortion mechanisms**: Explore additional counterfactual distortions beyond timestamp shuffling, such as targeted edge rewiring or node attribute perturbations, to evaluate the robustness of different TLP models to various types of causal violations.