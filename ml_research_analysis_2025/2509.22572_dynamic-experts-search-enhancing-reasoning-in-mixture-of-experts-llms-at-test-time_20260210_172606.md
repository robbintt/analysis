---
ver: rpa2
title: 'Dynamic Experts Search: Enhancing Reasoning in Mixture-of-Experts LLMs at
  Test Time'
arxiv_id: '2509.22572'
source_url: https://arxiv.org/abs/2509.22572
tags:
- experts
- reasoning
- search
- number
- expert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies a limitation in existing test-time scaling
  (TTS) methods for improving LLM reasoning, which focus on output-level diversity
  while ignoring architectural features. The authors observe that varying the number
  of activated experts in Mixture-of-Experts (MoE) models leads to complementary solutions
  with stable accuracy, revealing a new source of diversity.
---

# Dynamic Experts Search: Enhancing Reasoning in Mixture-of-Experts LLMs at Test Time

## Quick Facts
- arXiv ID: 2509.22572
- Source URL: https://arxiv.org/abs/2509.22572
- Authors: Yixuan Han; Fan Ma; Ruijie Quan; Yi Yang
- Reference count: 20
- One-line primary result: DES achieves higher accuracy and precision than TTS baselines by treating expert activation count as a controllable search dimension

## Executive Summary
This paper identifies a limitation in existing test-time scaling methods for improving LLM reasoning, which focus on output-level diversity while ignoring architectural features. The authors observe that varying the number of activated experts in Mixture-of-Experts (MoE) models leads to complementary solutions with stable accuracy, revealing a new source of diversity. They propose Dynamic Experts Search (DES), a TTS strategy that treats expert activation count as a controllable search dimension. DES combines Dynamic MoE, which allows direct control over expert counts during inference, with Expert Configuration Inheritance, which maintains consistent expert counts within reasoning paths while varying them across runs.

## Method Summary
DES is a test-time scaling strategy that leverages the architectural flexibility of MoE models by treating expert activation count (k) as a controllable search dimension. The method combines Dynamic MoE (allowing direct control over k during inference) with Expert Configuration Inheritance (maintaining consistent k within reasoning paths while varying across runs). It uses beam search with N=32 rollouts, M=N/4 retained per step, T=10 max steps, and temperature=0.8. The approach requires a Process Reward Model (PRM) verifier to score intermediate steps and guide the search, enabling implicit credit assignment to effective expert configurations without explicit optimization.

## Key Results
- DES consistently outperforms TTS baselines on math, code, and knowledge reasoning benchmarks
- Varying expert activation counts yields complementary solution sets with stable accuracy (low Jaccard Similarity)
- Expert Configuration Inheritance improves both accuracy and precision by maintaining consistent expert counts within reasoning paths
- The method maintains computational efficiency with comparable generation tokens to baseline approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Varying expert activation counts yields complementary solution sets with stable accuracy.
- Mechanism: Different k values activate different expert combinations, producing solutions to non-overlapping problem subsets (low Jaccard Similarity). This creates structural diversity beyond temperature sampling.
- Core assumption: Expert combinations encode different reasoning strategies rather than just quality levels.
- Evidence anchors: [abstract] "varying the number of activated experts yields complementary solution sets with stable accuracy"; [Section 1, Figure 1] Shows different expert counts have comparable accuracy but low Jaccard Similarity in solved problems.

### Mechanism 2
- Claim: Expert Configuration Inheritance enables implicit credit assignment to effective expert counts.
- Mechanism: By maintaining consistent k within a reasoning path, the verifier's step-level scores correlate with that configuration's effectiveness. Low-scoring paths are pruned, naturally filtering suboptimal k values without explicit optimization.
- Core assumption: Verifier quality estimates correlate with correctness across expert configurations.
- Evidence anchors: [Section 3.2] "the verifier assigns lower scores to less promising activation counts, making them unlikely to be inherited in subsequent steps"; [Section 4.3, Figure 5] Ablation shows Inh.of.Num improves both accuracy and pass@N.

### Mechanism 3
- Claim: Dynamic MoE expands search space without increasing per-token computation.
- Mechanism: Modifying k changes which experts process each token but keeps total activated parameters constant per token. The diversity gain comes from exploring different computational pathways, not additional FLOPs.
- Core assumption: Routing logic can be modified at inference time without architectural changes.
- Evidence anchors: [Section 4.3, Table 4] DES generates comparable #Gen.Tok to baselines; [Section 4.3, Figure 4] Average activated experts remains near default (8 for Qwen3-30B-A3B).

## Foundational Learning

- Concept: **Mixture-of-Experts (MoE) routing**
  - Why needed here: DES manipulates the top-k routing mechanism; you must understand how routers select experts before modifying k.
  - Quick check question: Given router scores [0.4, 0.3, 0.2, 0.1] for 4 experts, which experts are activated with k=2 vs k=3?

- Concept: **Test-Time Scaling (TTS) with verifiers**
  - Why needed here: DES is a TTS strategy that requires a Process Reward Model to score intermediate reasoning steps.
  - Quick check question: In beam search with N=32 candidates and M=8 retained per step, how many new branches expand from each retained candidate?

- Concept: **Jaccard Similarity for solution diversity**
  - Why needed here: The paper's core observation relies on Jaccard Similarity to quantify complementarity across expert configurations.
  - Quick check question: If Config A solves {1,2,3,4} and Config B solves {2,3,5,6}, what is the Jaccard Similarity?

## Architecture Onboarding

- Component map:
  - Policy Model: MoE LLM with modified routing to accept k as parameter
  - Verifier: Process Reward Model scores intermediate steps
  - Search Controller: Implements DES algorithm, tracking (state, k) pairs through inheritance
  - Dynamic MoE Layer: Wraps standard MoE to expose k as runtime parameter

- Critical path:
  1. Implement expert count override in vLLM or similar inference framework
  2. Build search loop that propagates k alongside state tuples
  3. Integrate verifier scoring at each step for candidate pruning

- Design tradeoffs:
  - Initial expert count range: [4-11] used in paper; wider range increases exploration but may include unstable configurations
  - Candidates retained per step (M): Paper uses N/4; lower M speeds search but risks pruning correct paths early
  - Maximum steps (T=10): Sufficient for math/code; longer chains may benefit from deeper search

- Failure signatures:
  - Performance drops below baseline â†’ Verifier misalignment with MoE outputs; try alternative PRM
  - Average k converges to maximum â†’ Inheritance not working; check that (state, k) tuples propagate correctly
  - High variance across runs â†’ Initial k range too wide; narrow to values near default

- First 3 experiments:
  1. **Complementarity verification**: Run same problems with fixed k âˆˆ {4,6,8,10,12}; compute pairwise Jaccard Similarity on correct solutions. Should observe <0.5 overlap.
  2. **Inheritance ablation**: Compare DES with vs. without Expert Configuration Inheritance on MATH500 with N=32. Expect 2-5% accuracy gap per Figure 5.
  3. **Verifier sensitivity**: Test DES with two different PRMs (Qwen2.5-Math vs. Llama3.1-PRM) on same benchmark. Results should remain consistent per Table 7.

## Open Questions the Paper Calls Out

- **Question**: Can the search process be made robust without relying on external verifiers, or adapted to use lightweight internal confidence metrics?
  - Basis in paper: Section 5 (Limitation) states the method requires guidance from an external verifier, introducing communication overhead and dependency on verifier quality.

- **Question**: Can a predictive model be trained to estimate the optimal expert activation count for a specific query prior to inference?
  - Basis in paper: Section 3.2 notes that "uniformly exploring all activation counts... inevitably wastes part of the computation budget on suboptimal choices."

- **Question**: Does the Expert Configuration Inheritance constraint (keeping k fixed for a trajectory) limit performance on complex tasks requiring varying levels of semantic density?
  - Basis in paper: Section 3.2 states inheritance "preserves consistent expert counts," but this assumes a single k is optimal for all steps of a solution.

## Limitations

- The method requires an external Process Reward Model (PRM) verifier, introducing communication overhead and dependency on verifier quality
- Dynamic MoE implementation details remain underspecified - modifying top-k routing at inference time may require architectural changes
- The paper doesn't establish whether complementarity arises from distinct reasoning strategies or routing noise

## Confidence

**High Confidence (â˜€ï¸â˜€ï¸â˜€ï¸):**
- DES improves accuracy and precision over TTS baselines across multiple benchmarks
- Expert Configuration Inheritance consistently improves performance in ablation studies
- The method maintains computational efficiency (comparable #Gen.Tok to baselines)

**Medium Confidence (â˜€ï¸â˜€ï¸ðŸŒ™):**
- Different expert counts produce complementary solution sets (observational, needs causal verification)
- Verifier scores correlate with correctness across expert configurations (assumed, limited PRM testing)
- Dynamic MoE can be implemented without architectural changes (technically plausible but unverified)

**Low Confidence (â˜€ï¸ðŸŒ™ðŸŒ™):**
- The complementarity arises from distinct reasoning strategies rather than quality variations
- The inheritance mechanism provides optimal credit assignment across expert configurations
- The method generalizes to non-MoE architectures or different routing mechanisms

## Next Checks

1. **Causal Complementarity Test**: Run identical problems with fixed expert counts, then systematically vary one expert's contribution while holding others constant. If complementarity persists only when entire expert sets change, it suggests strategic diversity rather than routing noise.

2. **Verifier Calibration Analysis**: Test DES with intentionally misaligned verifiers (e.g., trained on one expert count but used on others). If performance degrades proportionally to misalignment, it confirms verifier sensitivity to expert configuration.

3. **Routing Stability Evaluation**: Measure expert activation stability across multiple inference passes with identical inputs but varying initial k. High instability would suggest the observed complementarity is routing artifact rather than architectural feature.