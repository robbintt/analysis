---
ver: rpa2
title: Leveraging Reference Documents for Zero-Shot Ranking via Large Language Models
arxiv_id: '2506.11452'
source_url: https://arxiv.org/abs/2506.11452
tags:
- passage
- reference
- query
- arxiv
- ndcg
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the trade-off between ranking accuracy and
  computational efficiency in zero-shot text ranking using large language models (LLMs).
  While pointwise methods are efficient but suffer from calibration drift, and pairwise/listwise
  methods improve accuracy but are computationally expensive, the authors propose
  RefRank, a reference-anchored ranking framework that combines the benefits of both.
---

# Leveraging Reference Documents for Zero-Shot Ranking via Large Language Models

## Quick Facts
- arXiv ID: 2506.11452
- Source URL: https://arxiv.org/abs/2506.11452
- Reference count: 8
- Primary result: RefRank achieves state-of-the-art NDCG@10 on TREC-DL 2019 with minimal computational overhead

## Executive Summary
This paper addresses the trade-off between ranking accuracy and computational efficiency in zero-shot text ranking using large language models (LLMs). While pointwise methods are efficient but suffer from calibration drift, and pairwise/listwise methods improve accuracy but are computationally expensive, the authors propose RefRank, a reference-anchored ranking framework that combines the benefits of both. RefRank prompts the LLM to score each document relative to a fixed reference document, typically the top-ranked passage from the initial retrieval, enabling indirect comparison while maintaining linear time complexity. The method is training-free and adds no extra model calls. Experiments on six benchmark datasets with various LLMs show that RefRank significantly outperforms pointwise baselines and achieves performance on par with pairwise approaches, with negligible computational overhead.

## Method Summary
RefRank is a zero-shot ranking framework that leverages reference anchoring to achieve both high accuracy and computational efficiency. The method works by prompting the LLM to score each document relative to a fixed reference document, typically the top-ranked passage from initial retrieval. This indirect comparison approach enables pairwise-style ranking while maintaining the linear time complexity of pointwise methods. The framework is training-free and requires no additional model calls beyond the initial scoring. By anchoring all comparisons to a single reference document, RefRank sidesteps the quadratic complexity of traditional pairwise ranking while still capturing relative document quality through comparative scoring.

## Key Results
- RefRank significantly outperforms pointwise baselines across six benchmark datasets
- Achieves ranking performance on par with pairwise methods while maintaining linear computational complexity
- Establishes new state-of-the-art NDCG@10 on TREC-DL 2019 dataset
- Demonstrates consistent high ranking accuracy with minimal latency overhead across various LLM models

## Why This Works (Mechanism)
RefRank works by exploiting the LLM's ability to perform comparative reasoning when prompted appropriately. By anchoring all document scores to a fixed reference document, the model can implicitly rank documents through relative scoring rather than requiring explicit pairwise comparisons. This approach leverages the LLM's contextual understanding while avoiding the computational burden of generating scores for every possible document pair. The reference anchoring creates a common scale for comparison, addressing the calibration issues inherent in standalone pointwise scoring.

## Foundational Learning
- **Zero-shot ranking**: Ranking documents without training on labeled data, why needed: enables immediate deployment without expensive annotation; quick check: can the system rank on held-out queries without any model updates
- **Pointwise vs pairwise ranking**: Pointwise scores documents independently, pairwise compares document pairs; why needed: understanding the accuracy-efficiency tradeoff being addressed; quick check: measure both accuracy and computational cost for both approaches
- **Calibration drift**: When standalone scores from LLMs don't reflect true relative quality; why needed: explains why simple pointwise scoring fails; quick check: compare top-1 accuracy vs pairwise accuracy on same model
- **Reference anchoring**: Using a fixed document as comparison baseline for all others; why needed: core innovation that enables efficient pairwise-style ranking; quick check: vary reference quality and measure impact on ranking performance
- **Linear time complexity**: Processing time grows linearly with number of documents; why needed: critical for scalability in production systems; quick check: measure runtime as function of document count
- **NDCG@10**: Normalized Discounted Cumulative Gain at top 10 results; why needed: standard metric for evaluating ranking quality in top results; quick check: compute NDCG@10 on validation set

## Architecture Onboarding

**Component map**: Initial retrieval -> Reference selection -> Document scoring (relative to reference) -> Ranking output

**Critical path**: Query -> Initial retrieval (BM25/ANN) -> Select top passage as reference -> Score all documents relative to reference -> Sort by scores -> Return ranked list

**Design tradeoffs**: 
- Accuracy vs efficiency: RefRank sacrifices some pairwise comparison granularity for linear complexity
- Reference quality vs stability: Fixed reference provides consistent baseline but may be suboptimal
- Prompt engineering vs model capabilities: Success depends on effective prompting rather than model changes

**Failure signatures**:
- Poor reference selection leading to systematic ranking bias
- LLM misinterpreting relative scoring prompt
- Reference document being unrepresentative of query intent
- Calibration issues when reference quality varies widely

**Three first experiments**:
1. Compare RefRank against pure pointwise and pairwise baselines on a small dataset with timing measurements
2. Ablation study: test different reference selection strategies (top, median, random passages)
3. Evaluate robustness by intentionally selecting poor quality references and measuring ranking degradation

## Open Questions the Paper Calls Out
None

## Limitations
- Limited evaluation scope: All datasets are English and from similar domains (news, web, biomedical)
- Reference selection not deeply investigated: Relies on top initial retrieval without exploring alternatives
- No analysis of cross-lingual or cross-domain generalization
- Does not address scenarios where reference document is of poor quality or unrepresentative

## Confidence

**High confidence**:
- Core methodology and implementation are sound and well-articulated
- Experimental protocols are clearly defined and reproducible

**Medium confidence**:
- Comparative results show consistent improvements but computational gains are modest
- State-of-the-art claim on TREC-DL 2019 needs replication across different LLM versions

## Next Checks

1. **Cross-linguistic validation**: Test RefRank on non-English datasets (MS MARCO monolingual or CLEF collections) to assess generalization across languages

2. **Reference robustness analysis**: Systematically evaluate performance when reference document is intentionally selected as low-quality, irrelevant, or average rank rather than top rank

3. **Domain transfer study**: Apply RefRank to specialized domains such as legal documents, patents, or scientific literature to assess effectiveness outside tested domains