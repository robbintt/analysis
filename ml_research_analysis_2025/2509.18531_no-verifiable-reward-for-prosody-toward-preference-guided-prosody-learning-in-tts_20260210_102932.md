---
ver: rpa2
title: 'No Verifiable Reward for Prosody: Toward Preference-Guided Prosody Learning
  in TTS'
arxiv_id: '2509.18531'
source_url: https://arxiv.org/abs/2509.18531
tags:
- preference
- grpo
- prosody
- reward
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper identifies that optimizing neural TTS models with standard
  GRPO rewards (CER/NLL) collapses prosody into monotone speech, while adding speaker-similarity
  rewards destabilizes training. To address this, the authors propose an iterative
  Direct Preference Optimization (DPO) approach using only a few hundred human-labeled
  preference pairs per round to directly optimize for prosodic naturalness.
---

# No Verifiable Reward for Prosody: Toward Preference-Guided Prosody Learning in TTS

## Quick Facts
- arXiv ID: 2509.18531
- Source URL: https://arxiv.org/abs/2509.18531
- Reference count: 0
- Primary result: Iterative DPO with human preference pairs outperforms GRPO on prosody naturalness while maintaining CER

## Executive Summary
This paper addresses the challenge of optimizing prosody in neural TTS systems when automatic prosody rewards are unreliable. The authors find that standard GRPO rewards (CER/NLL) collapse prosody into monotone speech, while adding speaker-similarity rewards destabilizes training. To solve this, they propose an iterative Direct Preference Optimization (DPO) approach that uses only a few hundred human-labeled preference pairs per round to directly optimize for prosodic naturalness. Experiments on KoCC-TTS, a Korean call-center dataset, demonstrate that iterative DPO achieves the highest human preference (ELO) scores while maintaining competitive CER performance, outperforming both GRPO and commercial baselines.

## Method Summary
The paper introduces an iterative Direct Preference Optimization (DPO) framework that leverages human preference pairs instead of relying on automatic prosody rewards. The approach involves collecting small batches of human preference data (hundreds of pairs) in each iteration, using these to fine-tune the TTS model through DPO, then collecting new preference data to continue the cycle. This iterative process directly optimizes for prosodic naturalness as judged by human listeners, bypassing the need for verifiable automatic prosody metrics. The method is evaluated on the KoCC-TTS dataset and compared against standard GRPO approaches and commercial systems using both human preference scores and automatic CER metrics.

## Key Results
- Iterative DPO achieves highest human preference (ELO) scores for prosodic naturalness
- DPO maintains competitive CER performance compared to GRPO baselines
- Human preference optimization proves data-efficient, requiring only hundreds of preference pairs per round
- Standard prosody rewards (CER/NLL) cause monotone speech, while speaker-similarity rewards destabilize training

## Why This Works (Mechanism)
The approach works because it bypasses the fundamental problem of having no reliable automatic metric for prosody quality. By directly optimizing for human preferences through iterative DPO, the system learns prosodic patterns that humans find natural without relying on proxy metrics that fail to capture prosodic nuances. The iterative nature allows continuous refinement based on updated preference data, creating a feedback loop that progressively improves prosodic naturalness.

## Foundational Learning

**Direct Preference Optimization (DPO)**: A method for fine-tuning language models using preference pairs rather than traditional supervised learning. Why needed: Standard supervised learning cannot capture subjective qualities like naturalness. Quick check: Verify preference pairs are properly formatted and balanced.

**ELO scoring system**: A method for aggregating pairwise human preferences into overall rankings. Why needed: Single pairwise comparisons don't provide absolute quality measures. Quick check: Ensure ELO calculations handle ties and incomplete comparisons properly.

**GRPO (Group Relative Policy Optimization)**: An RLHF variant that optimizes policies based on group comparisons. Why needed: Standard PPO can be unstable for TTS tasks. Quick check: Verify reward scaling and normalization are properly implemented.

## Architecture Onboarding

**Component map**: TTS model -> Preference collector -> DPO fine-tuning -> Updated model -> New preference data (iterative cycle)

**Critical path**: Human preference collection → DPO fine-tuning → Model update → New preference generation

**Design tradeoffs**: Small preference dataset (hundreds of pairs) vs. comprehensive automatic metrics; iterative refinement vs. one-shot optimization

**Failure signatures**: Monotone speech indicates GRPO reward collapse; training instability suggests reward scaling issues; poor human preference scores indicate ineffective fine-tuning

**First experiments**: 1) Run baseline GRPO with CER/NLL rewards to reproduce monotone speech, 2) Implement DPO with small preference set and verify training stability, 3) Conduct human evaluation comparing DPO vs. GRPO outputs

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Limited to single Korean call-center dataset, constraining generalizability
- Lacks systematic ablation studies to confirm exact mechanism of speaker-similarity reward instability
- No statistical significance testing on human preference scores
- Comparison to commercial baselines lacks transparency regarding specific models used

## Confidence

**High confidence**:
- DPO performance vs GRPO based on human preference metrics (ELO scores)

**Medium confidence**:
- Speaker-similarity reward instability (observational evidence without ablation studies)
- Data efficiency claims (limited to specific dataset conditions)

## Next Checks

1. Conduct ablation studies removing speaker-similarity rewards while keeping CER/NLL to isolate their specific impact on training stability

2. Test the iterative DPO approach on multiple languages and speaking styles beyond Korean call-center data to assess generalizability

3. Perform statistical significance testing on human preference scores (ELO) with confidence intervals to strengthen comparative claims