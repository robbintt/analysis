---
ver: rpa2
title: 'ChatInject: Abusing Chat Templates for Prompt Injection in LLM Agents'
arxiv_id: '2509.22830'
source_url: https://arxiv.org/abs/2509.22830
tags:
- user
- template
- instruction
- prompt
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses indirect prompt injection attacks on LLM agents,
  where malicious instructions embedded in tool outputs manipulate agents into unintended
  actions. The authors introduce ChatInject, a method that formats malicious payloads
  to mimic native chat templates, exploiting LLMs' reliance on role-based hierarchies.
---

# ChatInject: Abusing Chat Templates for Prompt Injection in LLM Agents

## Quick Facts
- arXiv ID: 2509.22830
- Source URL: https://arxiv.org/abs/2509.22830
- Authors: Hwan Chang; Yonghyun Jun; Hwanhee Lee
- Reference count: 38
- Primary result: ChatInject achieves 32.05% attack success rate on AgentDojo, up from 5.18% baseline

## Executive Summary
ChatInject demonstrates that malicious instructions embedded in tool outputs can manipulate LLM agents into unintended actions by forging chat template tokens to exploit role-based instruction hierarchies. The method formats payloads to mimic native chat templates, causing models to misinterpret malicious content as originating from higher-priority roles. A Multi-turn variant further enhances persuasion by embedding simulated dialogues. Experiments show significant improvements over traditional methods, with strong transferability across both open-source and closed-source models.

## Method Summary
ChatInject works by embedding forged role tags within tool outputs, causing LLM agents to misinterpret malicious content as originating from higher-priority roles like system or user rather than tool output. The method creates four payload variants: Default InjecPrompt (plain text), ChatInject (single-turn with templates), Default Multi-turn (plain dialogue), and Multi-turn + ChatInject (templated dialogue). The Multi-turn variant constructs persuasive scenarios through simulated conversations that normalize malicious instructions. Transferability is achieved through template similarity analysis and Mixture-of-Templates approaches.

## Key Results
- ChatInject achieves 32.05% attack success rate on AgentDojo, compared to 5.18% baseline
- Multi-turn + ChatInject improves success rates to 45.90% on InjecAgent from 15.13% baseline
- Strong cross-model transferability demonstrated, with GPT-oss template achieving 36.43% ASR on GPT-4o

## Why This Works (Mechanism)

### Mechanism 1: Role Hierarchy Abuse via Template Forging
- Claim: Embedding forged chat template tokens in tool outputs causes LLMs to misinterpret malicious content as originating from higher-priority roles
- Mechanism: LLM agents enforce a role-based instruction hierarchy using special tokens like `<|im_start|>system`. When attackers inject these tokens into tool responses, the model's tokenizer and attention mechanism re-parse the content, granting attacker payloads the authority of the forged role
- Core assumption: The model's learned dependency on special tokens for role segmentation cannot distinguish between legitimate and forged markers
- Evidence anchors: [abstract], [section 1], related work on template manipulation

### Mechanism 2: Simulated Multi-turn Persuasive Framing
- Claim: Embedding fabricated multi-turn dialogue within a single injection payload normalizes malicious instructions through gradual context-building
- Mechanism: Attackers construct virtual conversations with system messages framing the injection, followed by alternating user-assistant turns that build plausible scenarios, culminating in the malicious instruction
- Core assumption: LLMs prioritize conversational coherence and context accumulation over detecting inauthentic dialogue construction
- Evidence anchors: [section 1], [section 3.2], Li et al. (2024) on multi-turn attacks

### Mechanism 3: Cross-Model Template Transferability via Embedding Similarity
- Claim: Attack success correlates with embedding similarity between injected and target templates, enabling black-box attacks
- Mechanism: Templates from open-source models transfer effectively to closed-source models because many models share structural conventions. Mixture-of-Templates provides robust coverage
- Core assumption: Closed-source models use template structures similar to their open-source relatives or common conventions
- Evidence anchors: [section 5.1], [section 5.2], embedding similarity correlation

## Foundational Learning

- **Instruction Hierarchy in LLM Agents**
  - Why needed here: Understanding role-based priority (system > user > assistant > tool) is essential to grasp why forging higher-role tokens bypasses security boundaries
  - Quick check question: Can you explain why a tool output containing `