---
ver: rpa2
title: Augmented Question-guided Retrieval (AQgR) of Indian Case Law with LLM, RAG,
  and Structured Summaries
arxiv_id: '2508.04710'
source_url: https://arxiv.org/abs/2508.04710
tags:
- legal
- retrieval
- case
- structured
- judgments
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces a structured summary-based approach for Indian
  case law retrieval using Large Language Models (LLMs) without fine-tuning. It combines
  Retrieval-Augmented Generation (RAG) with the Augmented Question-guided Retrieval
  (AQgR) framework, which generates legal questions from facts to improve retrieval
  accuracy.
---

# Augmented Question-guided Retrieval (AQgR) of Indian Case Law with LLM, RAG, and Structured Summaries

## Quick Facts
- arXiv ID: 2508.04710
- Source URL: https://arxiv.org/abs/2508.04710
- Reference count: 40
- Key outcome: Structured summary-based approach achieved MAP=0.36 and MAR=0.67 on FIRE 2019 dataset, surpassing benchmarks by generating legal questions from facts for more effective retrieval.

## Executive Summary
This study introduces a structured summary-based approach for Indian case law retrieval using Large Language Models (LLMs) without fine-tuning. It combines Retrieval-Augmented Generation (RAG) with the Augmented Question-guided Retrieval (AQgR) framework, which generates legal questions from facts to improve retrieval accuracy. Structured summaries enable more efficient handling of lengthy judgments by extracting key fields like facts, issues, and reasoning. The approach achieved a Mean Average Precision (MAP) of 0.36 and Mean Average Recall (MAR) of 0.67 on the FIRE 2019 dataset, significantly surpassing existing benchmarks. Expert evaluations confirmed high precision in summary generation and effective retrieval. The method reduces hallucination risks, enhances context relevance, and supports practical legal research by transitioning from fact-based to legal-issue-based retrieval.

## Method Summary
The method uses a two-stage RAG architecture with structured summaries and AQgR. First, lengthy Indian case judgments are processed through a recursive chunking strategy (20,000 characters, 10,000 overlap) and passed to Gemini Pro via a zero-shot prompt to extract structured fields (court, parties, facts, issues, reasoning, statutes, precedents) into JSON format. These summaries are embedded and stored in a FAISS vector database alongside a BM25 index. For retrieval, input facts are augmented by LLM-generated legal questions (3 per query), which are then used with an ensemble retriever (FAISS + BM25) to identify relevant precedents. The retrieved cases are ranked and explained by LLM, enabling more precise case law discovery than fact-based retrieval alone.

## Key Results
- Achieved MAP of 0.36 and MAR of 0.67 on FIRE 2019 dataset, outperforming existing benchmarks
- Structured summaries reduced document size from ~5000 tokens to ~400 tokens, enabling 10-60 judgments per context window
- LLM-generated legal questions improved retrieval relevance compared to direct fact matching
- Expert evaluations confirmed high precision in summary generation and effective retrieval
- The approach avoids fine-tuning while maintaining accuracy through structured extraction

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Generating legal questions from facts improves retrieval relevance over direct fact matching.
- Mechanism: An LLM module first generates targeted legal issues/questions from input facts. These questions are then used to query the vector database, matching cases that address similar legal principles even when surface facts differ.
- Core assumption: The LLM can reliably identify salient legal issues embedded within factual scenarios without domain-specific fine-tuning.
- Evidence anchors:
  - [abstract] "Leveraging the Augmented Question-guided Retrieval (AQgR) framework, the system generates targeted legal questions based on factual scenarios to identify relevant case law more effectively."
  - [section] "The AQgR approach augments the RAG with a question of law or issue in the legal scenario to enable better case law retrieval. The existing approaches for case law completely rely on facts for precedent retrieval. In real-life scenarios, lawyers use legal questions to retrieve the precedents, not the facts."
  - [corpus] Related work (NyayaRAG) also uses RAG for Indian legal judgment prediction, suggesting the approach generalizes, but direct comparison of question-guided vs. fact-based retrieval is limited in corpus.
- Break condition: If the LLM generates overly broad, irrelevant, or hallucinated legal questions, retrieval quality degrades. The paper notes that "legal questions generated by the LLM are more generalized and sometimes specific to the facts."

### Mechanism 2
- Claim: Structured summaries compress lengthy judgments while preserving legally salient content, enabling more cases to fit within LLM context windows.
- Mechanism: A template-based extraction approach (not generative summarization) pulls structured fields—court, parties, facts, issues, reasoning, statutes, precedents—into a condensed format (~400 tokens vs. ~5000 tokens for full judgments). This allows 10–60 judgments to be accommodated in the context window instead of 1–2 full documents.
- Core assumption: The LLM can perform accurate slot-filling extraction via zero-shot prompting without training data.
- Evidence anchors:
  - [abstract] "Structured summaries enable more efficient handling of lengthy judgments by extracting key fields like facts, issues, and reasoning."
  - [section] "Structured summaries enable more judgments to fit within the context window of LLMs, expanding the pool of available case law for retrieval."
  - [corpus] LexGenie (ECHR case law) uses similar structured report generation, supporting the general viability of structured extraction in legal domains.
- Break condition: If extraction misses critical fields or introduces errors, downstream retrieval and explanation quality suffer. The paper reports ~90% precision but lower recall for statutes and facts, indicating potential gaps.

### Mechanism 3
- Claim: Combining sparse (BM25) and dense (FAISS) retrievers improves coverage by capturing both keyword matches and semantic similarity.
- Mechanism: BM25 retrieves based on term frequency/keyword overlap; FAISS retrieves based on dense embedding similarity. An ensemble retriever merges results, reducing the likelihood that relevant cases are missed by one method alone.
- Core assumption: The two retrieval signals provide complementary relevance information.
- Evidence anchors:
  - [section] "An ensemble retriever comprising BM25 and FAISS is employed, as BM25 is a sparse retriever that excels in keyword-based searches. At the same time, FAISS is a dense retriever well-suited for semantic similarity matching. Combining both retrievers proves to be highly effective."
  - [section] "The results returned by both retrievers were completely different but relevant ones. Some judgments that are not retrieved using the FAISS retriever for multiple runs are retrieved by the BM25 retriever and vice versa."
  - [corpus] Hybrid RAG approaches (e.g., incorporating Knowledge Graphs with Vector Stores) appear in related work, but direct ensemble BM25+FAISS comparisons are not explicitly discussed in corpus.
- Break condition: If ensemble weighting or deduplication is poorly tuned, results may be dominated by one retriever or contain redundant entries.

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: The entire framework builds on RAG to integrate external legal knowledge without fine-tuning LLMs.
  - Quick check question: Can you explain how RAG differs from standard LLM prompting and from fine-tuning?

- Concept: Vector embeddings and similarity search (FAISS)
  - Why needed here: Dense retrieval via FAISS is a core component of the ensemble retriever.
  - Quick check question: What is the difference between sparse retrieval (e.g., BM25) and dense retrieval using vector embeddings?

- Concept: Legal document rhetorical structure (facts, issues, reasoning, precedents)
  - Why needed here: Structured summary extraction depends on understanding these components to populate the template correctly.
  - Quick check question: Can you identify the rhetorical roles of sentences in a legal judgment (facts, issues, reasoning, holding)?

## Architecture Onboarding

- Component map:
  1. **Structured Summary Generation Module**: Input judgment → Recursive character splitter → Chunks → Google Generative AI Embedding → FAISS vector store → LLM (Gemini Pro) with extraction prompt → JSON structured summary.
  2. **Case Law Retrieval Module**: Input fact/query → LLM generates legal questions (AQgR) → Ensemble retriever (FAISS + BM25) on structured summaries → Retrieved candidates → LLM generates explanation and relevance scores.
  3. **Vector Store**: Stores structured summaries as documents; no further chunking needed since summaries are already condensed.

- Critical path:
  1. Preprocess all judgments into structured summaries (offline).
  2. Index structured summaries in vector store.
  3. At query time: generate legal questions → retrieve candidates → LLM ranks and explains.

- Design tradeoffs:
  - **Extraction vs. generative summarization**: Extraction preserves legal terminology but may miss synthesis; generative approaches may introduce hallucination.
  - **Chunk size and overlap for long judgments**: 20,000 characters with 50% overlap worked best; smaller chunks lose context, larger chunks overflow context window.
  - **Including precedents in structured summaries**: Including "precedents referred" increases relevant cases in pool but may reduce precision on strict judgment-level matching.

- Failure signatures:
  - LLM safety filters blocking queries/judgments with sensitive content (e.g., Q46 in experiments).
  - Complex queries failing on very long documents (>122,880 characters); Chain-of-Thought prompting mitigates but is not evaluated with metrics.
  - Low recall for statutes and facts in structured summaries due to selective extraction.

- First 3 experiments:
  1. Replicate structured summary generation on 5 long judgments: vary chunk size (5,000–30,000 characters) and overlap (10–90%); measure extraction precision/recall via expert review.
  2. Compare retrieval performance: fact-only queries vs. AQgR (fact + generated legal questions) on a held-out subset of FIRE 2019 queries; measure MAP and MAR.
  3. Ablate ensemble retriever: run FAISS-only, BM25-only, and ensemble; document differences in retrieved case overlap and final relevance scores.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can new evaluation mechanisms be designed for generative legal texts to move beyond traditional metrics like ROUGE while reducing reliance on costly expert reviews?
- Basis in paper: [explicit] The conclusion states there is a "need for new evaluation mechanisms tailored to generative LLM outputs, as current metrics like ROUGE are insufficient," and notes that expert evaluation is resource-intensive.
- Why unresolved: Generative models produce fluent but potentially hallucinated text that standard n-gram overlap metrics cannot accurately assess for legal validity.
- What evidence would resolve it: The development and validation of an automated evaluation framework that correlates strongly with human legal expert judgments on factuality and relevance.

### Open Question 2
- Question: How can the AQgR framework be modified to detect and mitigate outdated legal principles or biases inherent in precedent-based retrieval?
- Basis in paper: [explicit] The limitations section notes that "Precedent-based case law retrieval may also reflect biases or outdated legal principles," and future research should focus on "addressing outdated legal principles and biases."
- Why unresolved: The current RAG implementation retrieves based on semantic similarity, which may inadvertently reinforce historical biases or cite laws that have since been overturned or superseded.
- What evidence would resolve it: A comparative study showing the system's ability to filter out or flag citations relying on overruled statutes compared to a baseline retriever.

### Open Question 3
- Question: What specific mechanisms can effectively control the stochastic nature of LLM legal question generation to ensure consistency across runs without degrading the model's reasoning capabilities?
- Basis in paper: [explicit] The result analysis notes the LLM "generates different legal questions for each run" and suggests that a "mechanism to control the LLM output while maintaining intelligence will be more helpful" for practical implementation.
- Why unresolved: The authors found that lowering the model's temperature to fix consistency resulted in "very low intelligence level leading to incorrect outputs," creating a trade-off between stability and accuracy.
- What evidence would resolve it: A prompting strategy or architecture that achieves high consistency (e.g., >90% identical top-3 questions) over multiple runs on the same facts while maintaining current MAP scores.

## Limitations
- LLM-generated legal questions can be overly generalized or specific to facts, affecting retrieval consistency
- Lower recall for statutes and facts in structured summaries suggests coverage gaps
- Performance validation limited to 50 judgments from FIRE 2019 dataset, not full corpus
- System may perpetuate historical biases present in legal precedents
- Non-deterministic question generation requires mechanism to ensure consistency across runs

## Confidence

- **High Confidence**: Structured summary extraction methodology and implementation using Gemini Pro with chunking strategy (20k/10k overlap) are well-documented and reproducible.
- **Medium Confidence**: Retrieval performance metrics (MAP=0.36, MAR=0.67) are reported, but specific ensemble weighting scheme and exact prompt templates for question generation are not fully specified.
- **Low Confidence**: Generalizability of AQgR approach beyond FIRE 2019 dataset and performance on more complex or lengthy judgments remains uncertain.

## Next Checks

1. **Question Generation Robustness Test**: Systematically evaluate the LLM's legal question generation across 50 diverse factual scenarios from different legal domains, measuring consistency, relevance, and coverage compared to expert-generated questions.

2. **Structured Summary Recall Audit**: Conduct a comprehensive recall analysis by comparing extracted statutes, facts, and issues against manually annotated ground truth for 20 randomly selected judgments to quantify coverage gaps.

3. **Cross-dataset Generalization**: Apply the complete AQgR pipeline to an independent legal corpus (e.g., ECHR cases or another common law jurisdiction) to assess whether the 0.36 MAP/0.67 MAR performance holds or degrades significantly.