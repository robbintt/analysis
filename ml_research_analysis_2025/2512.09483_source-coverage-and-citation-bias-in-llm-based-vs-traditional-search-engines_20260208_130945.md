---
ver: rpa2
title: Source Coverage and Citation Bias in LLM-based vs. Traditional Search Engines
arxiv_id: '2512.09483'
source_url: https://arxiv.org/abs/2512.09483
tags:
- llm-ses
- search
- domains
- tses
- google
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study presents the first large-scale empirical comparison
  of LLM-based search engines (LLM-SEs) and traditional search engines (TSEs), analyzing
  55,936 queries and 1.4 million citations. While LLM-SEs return fewer sources per
  response (mean: 4.3 vs 10.3 for TSEs), they exhibit greater domain diversity, with
  37% of domains unique to LLM-SEs.'
---

# Source Coverage and Citation Bias in LLM-based vs. Traditional Search Engines

## Quick Facts
- **arXiv ID:** 2512.09483
- **Source URL:** https://arxiv.org/abs/2512.09483
- **Reference count:** 40
- **Key outcome:** LLM-SEs return fewer sources per response but exhibit greater domain diversity than TSEs, without outperforming TSEs in credibility, neutrality, or safety.

## Executive Summary
This study presents the first large-scale empirical comparison of LLM-based search engines (LLM-SEs) and traditional search engines (TSEs), analyzing 55,936 queries and 1.4 million citations. While LLM-SEs return fewer sources per response (mean: 4.3 vs 10.3 for TSEs), they exhibit greater domain diversity, with 37% of domains unique to LLM-SEs. However, LLM-SEs do not outperform TSEs in credibility, political neutrality, or cyber safety, and often favor less popular domains (higher Tranco ranks). Source quality varies significantly: Gemini relies heavily on Wikipedia (41% of citations) and shows low credibility, while Grok has the lowest political neutrality. Both systems cite fewer domains, concentrating user exposure. Feature analysis reveals that LLM-SEs prefer domains with more readable text, deeper HTML structure, more outlinks, and lower popularity. Despite lower source volumes, LLM-SEs exhibit similar malicious domain exposure rates as TSEs, often triggered by seemingly innocuous queries.

## Method Summary
The study collected 55,936 queries from Google Trends, X (Twitter) Trends, Google-related questions, and controversial Quora datasets. These queries were processed through 6 LLM-SEs (ChatGPT, Gemini, Perplexity, Grok, AI Mode, Copilot) and 2 TSEs (Google, Bing) using automated browser scraping in incognito mode. Citations were extracted from responses and annotated using third-party services: Forcepoint ACE for categorization, Tranco for popularity, MBFC for credibility/bias, and VirusTotal for security. Statistical analysis and XGBoost modeling were used to identify patterns and predictors of domain selection.

## Key Results
- LLM-SEs return significantly fewer sources per response (mean: 4.3) compared to TSEs (mean: 10.3)
- 37% of domains cited by LLM-SEs are unique to these systems, indicating greater domain diversity
- LLM-SEs do not show superior credibility, political neutrality, or cyber safety compared to TSEs
- Gemini citations are dominated by Wikipedia (41%), resulting in the lowest credibility scores among LLM-SEs
- Both engine types show similar rates of malicious domain exposure, often triggered by innocuous queries

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LLM-SEs return fewer cited sources per response (mean: 4.3 vs. 10.3 for TSEs) but access a distinct domain set (37% unique).
- **Mechanism:** The Retrieval-Augmented Generation (RAG) paradigm first decomposes complex queries into sub-queries, retrieves content, then the LLM synthesizes a summarized answer with embedded citations. This summarization step necessarily condenses the output, reducing the number of sources shown, while the sub-query decomposition can lead to retrieving information from a different set of domains than a single keyword search on a TSE.
- **Core assumption:** The citation set presented to the user is a faithful proxy for the entire retrieval set processed by the LLM.
- **Evidence anchors:** [abstract] "LLM-SEs return fewer sources per response (mean: 4.3 vs 10.3 for TSEs), they exhibit greater domain diversity, with 37% of domains unique to LLM-SEs." [Section 1 Introduction] "RAG works by expanding queries into sub-queries... The LLM generator then interprets these documents, synthesizes the retrieved content, and produces a concise natural-language summary with inline citations."

### Mechanism 2
- **Claim:** LLM-SEs prefer domains with more readable text, deeper HTML structure, and more outlinks.
- **Mechanism:** The feature-based analysis (Section 6) suggests that the LLM's source selection, influenced by its retrieval and ranking components, exhibits a statistical bias towards content that is structurally and linguistically simpler to parse and process. The positive influence of "number of outlinks" on selection may indicate a preference for pages with rich contextual connections.
- **Core assumption:** The correlation between these features and citation by LLM-SEs implies a causal link in the selection algorithm, not just a correlation with other unmeasured variables like content quality.
- **Evidence anchors:** [Section 6.1 Features in HTML] "LLM-SEs tend to favor more accessible, less textually demanding content... For structural features, LLM-SEs pages show a modest increase in HTML tag usage... and nesting depth." [Section 6.2 Predicting Unique Inclusion of Domains] "Six features have mean SHAP values exceeding 0.1. The most influential are... number of out-links (0.799)... and Flesch Reading Ease Score."

### Mechanism 3
- **Claim:** Despite citing fewer sources, LLM-SEs do not reduce exposure to malicious domains compared to TSEs.
- **Mechanism:** The paper finds similar rates of malicious domain exposure (Table 3). This suggests that the retrieval pool for both systems contains similar proportions of risky domains, and the LLM's summarization/citation selection does not inherently filter them out. The high Jaccard similarity in malicious domains across engines for the same query indicates the risk is query-dependent, not engine-dependent.
- **Core assumption:** The VirusTotal detection threshold (≥2 vendors) accurately identifies truly malicious domains with an acceptable false positive rate.
- **Evidence anchors:** [Section 5.2 Cyber Threat] "Most LLM-SEs exhibit risks that fall between these two baselines, despite sourcing fewer number of domains per response." [Section 5.2 Cyber Threat] "Overall, 85% of engine pairs share an average [Jaccard] score above 0.5... indicating that the malicious domain sets are largely identical across engines for the same query."

## Foundational Learning

**Concept: Retrieval-Augmented Generation (RAG)**
- **Why needed here:** This is the core architectural paradigm differentiating LLM-SEs from TSEs. Understanding its query decomposition, retrieval, and synthesis steps is essential to explain the differences in source coverage and format.
- **Quick check question:** How does the RAG process differ from a user manually refining their query on Google?

**Concept: Gini Coefficient**
- **Why needed here:** Used in Section 4.1 to quantify domain concentration. A lower Gini index for LLM-SEs indicates a more even distribution of cited domains, which is a key claim about their behavior.
- **Quick check question:** If a search engine cited `wikipedia.org` 90% of the time, would its Gini index be closer to 0 or 1?

**Concept: Tranco Ranking & MBFC Scores**
- **Why needed here:** These are the primary third-party metrics used to evaluate domain popularity (Tranco) and source credibility/political bias (MBFC). They form the basis for the quality and neutrality analysis in Section 5.
- **Quick check question:** A domain has a Tranco rank of 50,000. Is this considered high or low popularity compared to a rank of 500? On MBFC, a political bias score of -8 would indicate what leaning?

## Architecture Onboarding

**Component map:**
Query Corpus -> Search Engines (6 LLM-SEs + 2 TSEs) -> Browser Automation (DrissionPage) -> Citation Extraction -> Domain-Level Annotation (Forcepoint, Tranco, MBFC, VirusTotal) -> Statistical Analysis & Modeling

**Critical path:** Query Construction → Parallel Search Execution → Citation Extraction → Domain-Level Annotation → Quality & Feature Analysis

**Design tradeoffs:**
1. **Diversity vs. Quality:** LLM-SEs achieve higher domain diversity but do not guarantee higher credibility or safety.
2. **Conciseness vs. Transparency:** Providing a summarized answer with few citations enhances readability but reduces the user's ability to verify diverse sources directly.
3. **Inference Scope:** The study focuses on *user-facing citations*, not the intermediate retrieval documents, which limits visibility into the full selection process.

**Failure signatures:**
1. **Low Source Count:** Grok (82%) and Gemini (38%) responses with no cited websites, potentially over-relying on internal knowledge.
2. **Narrow Sourcing:** Gemini's 41% citation of Wikipedia leads to low credibility scores.
3. **Persistent Risk:** Exposure to malicious domains despite fewer sources, triggered even by innocuous queries in arts & entertainment.

**First 3 experiments:**
1. **Reproduce Source Overlap:** Select 100 queries, run them through a TSE and an LLM-SE, and manually verify the domain overlap to confirm the ~38% finding.
2. **Feature Validation:** Scrape HTML from a sample of domains cited by both an LLM-SE and a TSE. Compute readability and nesting depth to see if the LLM-SE's choices are statistically different.
3. **Safety Audit:** Submit a set of queries from the "Finance" and "People & Society" categories (identified as high-risk) to multiple engines and scan all returned domains with VirusTotal to compare threat exposure rates.

## Open Questions the Paper Calls Out

**Open Question 1**
- **Question:** To what extent do commercial incentives and Generative Engine Optimization (GEO) manipulate source visibility in LLM-SE outputs compared to traditional SEO?
- **Basis in paper:** [explicit] The authors state that future research should "examine monetization-driven behaviors, such as Generative Engine Optimization (GEO), to understand how commercial incentives influence content delivery."
- **Why unresolved:** This study focused on organic search result comparisons and did not control for or measure the influence of paid placements or commercial optimization strategies unique to LLMs.
- **What evidence would resolve it:** A longitudinal study tracking specific brands or domains known to utilize GEO strategies to see if they disproportionately appear in LLM-SE responses over time.

**Open Question 2**
- **Question:** How do pre-generation retrieval and filtering mechanisms bias the final set of citations presented to users?
- **Basis in paper:** [explicit] The authors note their scope was restricted to user-facing citations, and propose that future work "audit full retrieval traces to reveal potential biases in pre-generation filtering and ranking."
- **Why unresolved:** The study treats the LLM-SE as a black box, measuring only the final output without visibility into the intermediate steps where documents might be filtered out before the summary is generated.
- **What evidence would resolve it:** Access to internal reasoning logs or retrieval APIs (e.g., from systems like Grok) that expose the intermediate list of retrieved documents before the final selection process occurs.

**Open Question 3**
- **Question:** Does the lower domain concentration and higher uniqueness of sources in LLM-SEs actually improve the user's information retrieval experience?
- **Basis in paper:** [explicit] The paper observes that LLM-SEs have lower Gini indices and 37% unique domains, but explicitly states: "the extent to which such source diversity impacts the actual user retrieval experience remains an open question."
- **Why unresolved:** The analysis is purely quantitative and structural; it does not include user studies to assess if the unique, less popular domains favored by LLM-SEs are actually useful or trustworthy to the end user.
- **What evidence would resolve it:** User studies measuring task success rates, perceived trustworthiness, and information gain when users are restricted to LLM-SE results versus TSE results.

## Limitations
- The analysis focuses exclusively on user-facing citations rather than intermediate retrieval documents, potentially missing important signals about the full retrieval process
- Domain-level annotations rely on third-party services (MBFC, VirusTotal, Tranco) whose coverage and accuracy limitations may introduce bias
- The study's snapshot captures behavior during a specific timeframe (August 2024) when search engines rapidly evolve their architectures and policies

## Confidence

**High Confidence:** Claims about source count differences (LLM-SEs cite fewer sources than TSEs) and domain diversity metrics are directly supported by the dataset and robust statistical tests.

**Medium Confidence:** Conclusions about feature-based preferences (readability, HTML structure) rely on correlations that may not imply causation in the selection algorithm. The XGBoost model's predictive performance (F1=0.758) is reasonable but leaves room for error.

**Medium Confidence:** Cyber safety findings showing similar malicious domain exposure rates are supported by VirusTotal analysis, but the detection threshold (≥2 vendors) and potential false positives create uncertainty.

## Next Checks

1. **Reproduce Source Overlap:** Manually verify domain overlap for 100 sampled queries across TSEs and LLM-SEs to confirm the reported ~38% unique domain ratio.
2. **Feature Validation:** Collect HTML from a stratified sample of domains cited by both engine types, compute readability and nesting depth metrics, and test for statistically significant differences.
3. **Temporal Stability Test:** Repeat the analysis after 3-6 months to assess whether observed patterns persist as search engines update their architectures and policies.