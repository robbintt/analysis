---
ver: rpa2
title: 'Radiology''s Last Exam (RadLE): Benchmarking Frontier Multimodal AI Against
  Human Experts and a Taxonomy of Visual Reasoning Errors in Radiology'
arxiv_id: '2509.25559'
source_url: https://arxiv.org/abs/2509.25559
tags:
- reasoning
- image
- diagnosis
- diagnostic
- gpt-5
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: A novel benchmark of 50 expert-level radiology spot-diagnosis cases
  across multiple imaging modalities was developed to evaluate frontier multimodal
  AI systems against board-certified radiologists and trainees. Five popular models
  (GPT-5, Gemini 2.5 Pro, o3, Grok-4, Claude Opus 4.1) were tested via web interfaces
  and GPT-5 via API in various reasoning modes, with outputs scored by blinded experts
  and reproducibility assessed across three runs.
---

# Radiology's Last Exam (RadLE): Benchmarking Frontier Multimodal AI Against Human Experts and a Taxonomy of Visual Reasoning Errors in Radiology

## Quick Facts
- arXiv ID: 2509.25559
- Source URL: https://arxiv.org/abs/2509.25559
- Reference count: 22
- Primary result: GPT-5 achieved 30% diagnostic accuracy on expert-level radiology cases, significantly underperforming board-certified radiologists (83%) and highlighting systematic visual reasoning errors.

## Executive Summary
This paper introduces RadLE, a benchmark of 50 expert-level radiology spot-diagnosis cases designed to stress-test frontier multimodal AI systems. Five leading models (GPT-5, Gemini 2.5 Pro, o3, Grok-4, Claude Opus 4.1) were evaluated alongside human experts and trainees, revealing that even the best AI model (GPT-5 at 30%) falls far short of board-certified radiologists (83%). The study also develops a taxonomy of visual reasoning errors, identifying systematic failure patterns that suggest fundamental limitations in current architectures. These findings caution against unsupervised clinical deployment of current AI systems and call for targeted improvements in visual perception capabilities.

## Method Summary
The study evaluated five multimodal AI models on 50 de-identified radiology images (CT, MRI, radiographs) curated by board-certified radiologists as expert-level "spot diagnosis" cases. Each model was tested through web interfaces in various reasoning modes, with GPT-5 additionally evaluated via API across low/medium/high reasoning effort settings. Outputs were scored by blinded expert radiologists using a 1.0/0.5/0.0 ordinal scale (exact match/partial/incorrect), and reliability was assessed through three independent runs per model. Statistical analysis included Friedman tests with Wilcoxon pairwise comparisons (Holm-adjusted) and reliability metrics (quadratic-weighted kappa and ICC).

## Key Results
- Board-certified radiologists achieved 83% diagnostic accuracy, significantly outperforming trainees (45%) and all AI models (best: GPT-5 at 30%)
- Reliability was substantial for GPT-5 and o3, moderate for Gemini 2.5 Pro and Grok-4, and poor for Claude Opus 4.1
- Minimal improvement observed across GPT-5's reasoning modes despite substantial computational costs (6x latency increase for 1% accuracy gain)
- Systematic visual reasoning errors were identified and categorized into Perceptual, Interpretive, and Communication errors with cognitive bias modifiers

## Why This Works (Mechanism)

### Mechanism 1: Spectrum-Biased Stress Testing
The RadLE benchmark exposes model limitations by shifting the evaluation distribution from common pathologies to high-complexity "spot diagnoses." Standard datasets are enriched for common findings, allowing models to rely on base-rate heuristics. By curating cases that "differentiate novice from expert performance," the benchmark forces models to perform genuine visual reasoning rather than statistical shortcuts.

### Mechanism 2: Inference-Time Compute Saturation
Increasing reasoning effort for GPT-5 yields diminishing returns on diagnostic accuracy while linearly increasing latency. The "thinking" modes generate additional intermediate tokens, but in visual tasks where the primary failure mode is perceptual (missing the finding entirely), textual deliberation cannot correct for unobserved visual features.

### Mechanism 3: Error Taxonomy as a Stability Constraint
Diagnostic failure follows specific patterns (Perceptual vs. Interpretive) that constrain how improvements must be targeted. The proposed taxonomy separates "Under-detection" (vision failure) from "Premature closure" (reasoning/logic failure), revealing that high latency and large model size do not automatically fix "Perceptual Errors," which require better visual grounding.

## Foundational Learning

- **Spectrum Bias**: Needed to interpret the 30% accuracy result correctlyâ€”the models were tested on the "tail" of the difficulty distribution, not the "head." Quick check: Does a drop in performance on a benchmark represent a failure of capability or a shift in the difficulty distribution?

- **Intra-class Correlation Coefficient (ICC)**: Accuracy is insufficient for clinical AI; consistency is also required. Quick check: If a model achieves 50% accuracy but has an ICC of 0.1, is it safe for clinical triage? (Answer: No, the outputs are random/unstable).

- **Visual Reasoning vs. Pattern Matching**: The paper argues that many AI "successes" are pattern matching on common datasets. True visual reasoning involves integrating spatial relationships and context. Quick check: Why might a model identify a "cyst" but fail to localize it to the "right atrium"?

## Architecture Onboarding

- **Component map**: Vision Encoder (VLM Backbone) -> Projection Layer -> Reasoning Core (LLM) -> Output Head
- **Critical path**: The Vision Encoder -> Projection interface. If the encoder misses a subtle lesion (e.g., "left ureterocele"), no amount of textual reasoning by the LLM can recover the diagnosis.
- **Design tradeoffs**:
  - Latency vs. Accuracy: High reasoning effort adds ~55 seconds of latency for <1% gain
  - Generalist vs. Specialization: Generalist models struggle with specific spot diagnoses compared to specialized, fine-tuned radiology models
- **Failure signatures**:
  - Perceptual Hallucination: Model confidently describes findings that are not present in the image
  - Findings-Summary Discordance: Model lists correct abnormal findings in reasoning trace but outputs "Normal" as the final diagnosis
- **First 3 experiments**:
  1. Reproduce the Reliability Test: Run the same 5 images through the model 3 times each. Calculate simple agreement.
  2. Taxonomy Error Logging: Run the model on a held-out set of 20 difficult cases. Manually classify every failure into the paper's taxonomy.
  3. Effort Scaling Plot: Measure accuracy vs. latency for 10 cases across Low/Medium/High reasoning modes. Verify if the "plateau effect" holds.

## Open Questions the Paper Calls Out

1. **Inference-time compute efficiency**: Does increasing reasoning effort in frontier models yield meaningful diagnostic improvements, or is the low accuracy-to-latency trade-off observed in GPT-5 a fundamental limitation? This remains unresolved as the paper evaluated only one model and one task type.

2. **Taxonomy validation and expansion**: Can the proposed taxonomy of visual reasoning errors be reliably validated and expanded for broader clinical application? The current framework is derived from a pilot set of 50 cases and may need subclassification in larger datasets.

3. **Technical interventions for perceptual errors**: What technical interventions beyond model scaling are required to mitigate "under-detection" errors in radiology? The paper identifies these failure modes but does not test specific solutions beyond scaling and reasoning time.

## Limitations

- **Dataset access constraints**: The RadLE benchmark images and ground-truth diagnoses are not publicly available, requiring direct contact with authors for reproduction and limiting independent validation.

- **Web interface variability**: Models were accessed through web interfaces without control over inference parameters like temperature or version stability, introducing uncertainty about result reproducibility across model updates.

- **Human benchmark representativeness**: The human performance comparison lacks full specification of selection criteria, experience levels, and standardized testing conditions, raising questions about the interpretation of the human-AI gap.

## Confidence

- **High confidence**: Claims about systematic visual reasoning errors and the taxonomy framework are well-supported by qualitative analysis of model outputs.
- **Medium confidence**: Quantitative performance comparisons are methodologically sound but limited by dataset access constraints and uncontrolled inference parameters.
- **Low confidence**: Claims about inference-time compute saturation and specific diminishing returns of reasoning modes should be viewed cautiously due to web interface limitations.

## Next Checks

1. **Controlled reproducibility test**: Run the same 20 cases through GPT-5 API with fixed temperature, seed, and version parameters across three independent sessions. Compare the observed ICC to web interface results to isolate the impact of uncontrolled settings.

2. **Error taxonomy validation**: Apply the Perceptual/Interpretive/Communication error classification to model outputs from a held-out set of 15 additional expert-level cases. Calculate inter-rater reliability for the taxonomy coding.

3. **Domain specificity assessment**: Test the RadLE benchmark on specialized radiology models alongside generalist frontier models to determine whether observed limitations are inherent to current multimodal architectures or specific to general-purpose systems.