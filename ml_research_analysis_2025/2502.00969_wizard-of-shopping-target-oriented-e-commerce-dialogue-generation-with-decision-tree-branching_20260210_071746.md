---
ver: rpa2
title: 'Wizard of Shopping: Target-Oriented E-commerce Dialogue Generation with Decision
  Tree Branching'
arxiv_id: '2502.00969'
source_url: https://arxiv.org/abs/2502.00969
tags:
- product
- customer
- seller
- features
- conversation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TRACER, a novel approach for generating realistic
  and natural conversations for conversational product search (CPS) using large language
  models (LLMs) and decision tree-based planning. TRACER leverages dialogue plans
  predicted from decision trees to ensure relevant product discovery in the shortest
  number of search conditions.
---

# Wizard of Shopping: Target-Oriented E-commerce Dialogue Generation with Decision Tree Branching

## Quick Facts
- **arXiv ID**: 2502.00969
- **Source URL**: https://arxiv.org/abs/2502.00969
- **Reference count**: 40
- **Primary result**: Introduces TRACER, a decision tree-based planning method for generating natural e-commerce dialogues, and releases the first target-oriented CPS dataset, Wizard of Shopping (WoS), with 3.6k dialogues.

## Executive Summary
This paper introduces TRACER, a novel approach for generating realistic and natural conversations for conversational product search (CPS) using large language models (LLMs) and decision tree-based planning. TRACER leverages dialogue plans predicted from decision trees to ensure relevant product discovery in the shortest number of search conditions. The paper also releases the first target-oriented CPS dataset, Wizard of Shopping (WoS), containing 3.6k highly natural and coherent conversations across three shopping domains. Human evaluations validate the quality of WoS, demonstrating its effectiveness in improving downstream tasks such as conversational query generation and product ranking. The dataset and approach are shown to significantly outperform baselines and GPT-4 in various metrics, making them valuable contributions to CPS research and development.

## Method Summary
The TRACER framework generates synthetic, target-oriented shopping dialogues grounded in a product catalog. It operates in three stages: (1) Preference Sampling, where a target product is selected and its attributes are randomly labeled as wanted, unwanted, or optional; (2) Planning, where a decision tree is iteratively fitted on the current candidate product set to select the next best attribute to ask, maximizing search space reduction; and (3) Verbalization, where the full dialogue is generated in a single LLM pass using the generated plan, ensuring global coherence. The approach is evaluated using human assessments and downstream tasks like conversational query generation and product ranking.

## Key Results
- Single-pass LLM generation significantly outperforms interactive generation in naturalness (4.2 vs 2.9) and coherence (4.7 vs 3.4).
- BM25 retrieval with dialogue queries outperforms RoBERTa in product ranking (MRR 0.838 vs 0.675).
- WoS dataset improves downstream conversational query generation and product ranking tasks compared to baselines and GPT-4.

## Why This Works (Mechanism)

### Mechanism 1: Decision Tree Branching for Search Space Partitioning
- Claim: Fitting a decision tree per dialogue turn selects product attributes that maximally divide the candidate product set, reducing conversation length.
- Mechanism: At each turn, products matching current preferences (Po) are retrieved. A decision tree is fitted where each leaf node represents products sharing the same aspect-value combination. The tree is traversed using the customer's known preferences to select the next aspect for elicitation. This iterates until Po converges to a product family.
- Core assumption: Customers prefer minimizing interaction turns; user effort inversely correlates with satisfaction (cites Al-Maskari & Sanderson, 2010).
- Evidence anchors:
  - [abstract]: "dialogue plans... predicted from a decision tree model, that guarantees relevant product discovery in the shortest number of search conditions"
  - [section 3.2]: "the decision tree selects the next product attribute that maximally divides the current search space"
  - [corpus]: Limited direct validation. PSCon mentions CPS systems use natural language but doesn't confirm efficiency gains.
- Break condition: If product catalog has sparse or inconsistent attributes, tree splits become meaningless; if customer preferences are contradictory, Po may not converge.

### Mechanism 2: Single-Pass Verbalization Over Interactive Generation
- Claim: Generating entire dialogues in one LLM call produces higher coherence and naturalness than alternating agent-by-agent generation.
- Mechanism: Single-pass feeds the complete dialogue plan (all aspects, preferences, trajectory) into one prompt, enabling global coherence. Interactive generation alternates two LLM agents, each limited to local context, increasing error propagation.
- Core assumption: LLMs can convert structured plans into natural conversations when given complete context; controllability outweighs turn-by-turn realism.
- Evidence anchors:
  - [section 4.3, Table 3]: GPT-4 single-pass scores 4.7 coherence vs 3.4 interactive; 4.2 naturalness vs 2.9
  - [section 4.2]: "single pass generation approach consistently outperforms the interactive generation"
  - [corpus]: Flippi introduces end-to-end conversational assistants but doesn't compare generation strategies.
- Break condition: Single-pass cannot adapt to dynamic user inputs mid-conversation; breaks for stateful, real-time dialogue systems.

### Mechanism 3: Three-Way Preference Sampling
- Claim: Classifying product aspects as wanted/unwanted/optional simulates realistic customer uncertainty better than binary labels.
- Mechanism: For each aspect of a sampled product, randomly assign an interest level. "Unwanted" values are replaced with different catalog values. This creates diverse preference profiles that drive planning and verbalization.
- Core assumption: Real customers have partial knowledge and mixed certainty about product attributes.
- Evidence anchors:
  - [section 3.1]: "We randomly assign one of the following customer interest values (I) to each aspect... Wanted, Unwanted, Optional"
  - [table 17]: Example shows mixed preferences (wanted: dynamite red; optional: finish type; unwanted: Gocheaper brand)
  - [corpus]: No corpus papers validate the three-way categorization specifically.
- Break condition: If unwanted value replacement selects semantically similar values, the distinction becomes meaningless; sparse catalogs may not provide valid replacement values.

## Foundational Learning

- Concept: Decision tree splitting criteria (information gain, Gini impurity)
  - Why needed here: The dialogue planner relies on selecting attributes that "maximally split" the product set. Understanding purity measures explains why certain aspects are prioritized.
  - Quick check question: Given products with {color: red/blue, size: S/M/L}, which attribute maximizes purity if the target set contains only red, size-M products?

- Concept: Dialogue state tracking (DST)
  - Why needed here: Interactive mode uses DST to track mentioned vs. remaining preferences; downstream CQG task extracts preferences from dialogues—the inverse operation.
  - Quick check question: A customer says "I don't want liquid lipstick, and I'm flexible on brand." What should the DST record for "item form" and "brand"?

- Concept: Retrieval grounding for hallucination reduction
  - Why needed here: TRACER grounds LLM outputs in catalog data and decision tree plans to mitigate hallucination. Distinguishing grounded vs. free-form generation is critical.
  - Quick check question: If the seller mentions a feature not in the catalog, which error category from Section 4.5 does this fall under?

## Architecture Onboarding

- **Component map**:
  Product Catalog -> Preference Sampler -> Search Engine -> Decision Tree Planner -> Verbalizer (Single-Pass/Interactive) -> Downstream Tasks

- **Critical path**:
  1. Sample product → assign preference labels (§3.1)
  2. Initialize Po with product category search
  3. Fit decision tree on Po attributes (Algorithm 1)
  4. Traverse tree to get dialogue plan
  5. Feed plan to LLM (single-pass) OR alternate agents (interactive)
  6. Repeat 2-5 until Po converges
  7. Recommend random product from final Po

- **Design tradeoffs**:
  - **Single-pass vs Interactive**: Quality (4.2 vs 2.9 naturalness) vs. real-time adaptability
  - **Per-turn vs global planning**: Adaptability vs. computational cost
  - **Sparse (BM25) vs dense retrieval**: BM25 outperformed RoBERTa (MRR 0.838 vs 0.675) for structured attributes

- **Failure signatures**:
  - **Verbosity**: Seller crams all aspects into one utterance (Table 18)
  - **Coherence break**: After clarification, seller skips returning to value selection (Table 19)
  - **Bad features**: Noisy catalog attributes create awkward turns (Table 20)
  - **Instruction drift**: LLM invents unassigned features (Table 21)

- **First 3 experiments**:
  1. Validate single-pass quality on 50 held-out products; score realism/coherence (1-5); compare BM25 ranking with raw dialogue as query.
  2. Ablate decision tree with random aspect selection; measure average turns to convergence.
  3. Compare three-way vs. binary (wanted-only) preference sampling on human naturalness scores.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the TRACER framework be extended to support complex shopping behaviors like product comparison?
- Basis in paper: [explicit] The authors state, "TRACER does not consider more complicated shopping behavior such as comparison among similar products" (Page 9).
- Why unresolved: The current decision tree strategy optimizes for finding a single target product via attribute filtering, lacking the mechanism to facilitate trade-off discussions between similar items.
- What evidence would resolve it: A modified framework where the "Wizard" agent retrieves multiple candidates and the dialogue plan supports comparative dialogue acts rather than just attribute elicitation.

### Open Question 2
- Question: Does leveraging the semantics of product features improve the efficiency of dialogue planning?
- Basis in paper: [explicit] The paper notes that "the semantics of product features are under-utilized" because the decision tree treats them as categorical labels (Page 9).
- Why unresolved: Treating all aspect-values as distinct categories ignores underlying relationships (e.g., similar materials or price tiers), potentially leading to unnatural or suboptimal branching in the search trajectory.
- What evidence would resolve it: A study comparing the current categorical planning against a method that clusters or merges semantically similar features, showing improvements in turn count or naturalness.

### Open Question 3
- Question: To what extent does noise in the product catalog degrade the quality of generated dialogues?
- Basis in paper: [explicit] The authors identify that "noise in the product catalog can directly propagate to the generated conversations" and suggest future work apply catalog cleaning methods (Page 9).
- Why unresolved: The released WoS dataset relies on a manually cleaned catalog, leaving the robustness of the generation process against raw, noisy real-world data untested.
- What evidence would resolve it: An ablation study measuring the frequency of "Bad features" errors (Section 4.5) and naturalness scores when TRACER is applied to uncurated versus cleaned product data.

## Limitations

- **Catalog Domain Specificity**: Results are validated only on three Amazon-derived domains (Home, Electronics, Beauty). Performance on specialized or sparse catalogs remains unknown.
- **Real-Time Adaptability**: Single-pass generation produces higher quality but cannot handle mid-conversation user input changes. This limits deployment in interactive settings.
- **Scalability of Decision Tree Planning**: Per-turn tree fitting requires retrieving candidate products at each step, which may not scale to large catalogs without approximate retrieval.

## Confidence

- **Quality Improvements (Coherence/Naturalness)**: High confidence. Human evaluations show clear, consistent gains for single-pass over interactive generation (e.g., coherence 4.7 vs 3.4).
- **Efficiency Gains (Fewer Turns)**: Medium confidence. The mechanism is sound, but the paper does not directly measure or report average dialogue length reduction compared to baselines.
- **Downstream Task Benefits**: Medium confidence. The CQG and ranking tasks show improvement, but results depend on specific retriever choices (BM25 vs RoBERTa).

## Next Checks

1. **Dialogue Length Analysis**: Measure and report the average number of turns in TRACER dialogues versus baseline methods to directly validate efficiency claims.
2. **Generalization Test**: Apply TRACER to a non-Amazon catalog (e.g., a niche domain) and evaluate if the decision tree planner still selects relevant attributes.
3. **Interactive Generation Re-evaluation**: Implement the interactive generation method with the dialogue state tracker and re-run human evaluations to confirm single-pass superiority.