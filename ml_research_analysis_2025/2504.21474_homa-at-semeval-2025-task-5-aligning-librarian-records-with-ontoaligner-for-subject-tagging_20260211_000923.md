---
ver: rpa2
title: 'Homa at SemEval-2025 Task 5: Aligning Librarian Records with OntoAligner for
  Subject Tagging'
arxiv_id: '2504.21474'
source_url: https://arxiv.org/abs/2504.21474
tags:
- subject
- recall
- records
- ontoaligner
- tagging
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents Homa, a system for SemEval-2025 Task 5: Subject
  Tagging, which focuses on automatically assigning subject labels to technical records
  from TIBKAT using the Gemeinsame Normdatei (GND) taxonomy. We leverage OntoAligner,
  a modular ontology alignment toolkit, to address this task by integrating retrieval-augmented
  generation (RAG) techniques.'
---

# Homa at SemEval-2025 Task 5: Aligning Librarian Records with OntoAligner for Subject Tagging

## Quick Facts
- arXiv ID: 2504.21474
- Source URL: https://arxiv.org/abs/2504.21474
- Reference count: 6
- Primary result: Homa system for SemEval-2025 Task 5 subject tagging using OntoAligner with RAG techniques

## Executive Summary
This paper presents Homa, a system for SemEval-2025 Task 5: Subject Tagging, which focuses on automatically assigning subject labels to technical records from TIBKAT using the Gemeinsame Normdatei (GND) taxonomy. We leverage OntoAligner, a modular ontology alignment toolkit, to address this task by integrating retrieval-augmented generation (RAG) techniques. Our approach formulates the subject tagging problem as an alignment task, where records are matched to GND categories based on semantic similarity. We evaluate OntoAligner's adaptability for subject indexing and analyze its effectiveness in handling multilingual records. Experimental results demonstrate the strengths and limitations of this method, highlighting the potential of alignment techniques for improving subject tagging in digital libraries.

## Method Summary
Homa reformulates subject tagging as an ontology alignment problem using OntoAligner's modular pipeline. The system employs a two-stage RAG approach: first, a fine-tuned embedding model retrieves top-30 candidate GND subjects via cosine similarity; second, a small LLM (Qwen2.5-0.5B) classifies each candidate as a match or non-match. The retriever is fine-tuned with contrastive learning using balanced positive/negative pairs, while the LLM is fine-tuned with supervised learning on classification tasks. The approach uses title-based input representation due to computational constraints, processing bilingual English/German records from the TIB-Core-Subjects dataset.

## Key Results
- Average precision: 2.84% (Recall@30: 20.30%, F1@30: 4.66%)
- Recall improves with higher k values, with notable gains beyond k=15
- German language recall scores remain lower than English, likely due to richer training data or better linguistic resources embedded within LLMs
- Document type variance observed: Conference/Reports underperform Articles/Books, suggesting title ambiguity or ground truth noise

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Formulating subject tagging as an ontology alignment task enables adaptation of existing alignment tooling for library catalog automation.
- Mechanism: The system treats librarian records and GND taxonomy categories as two "ontologies" to be aligned. Records are embedded and matched to subject labels via semantic similarity, reusing OntoAligner's modular pipeline architecture rather than building a bespoke classifier from scratch.
- Core assumption: Semantic similarity between record text and subject labels correlates with correct subject assignment—a relationship that may not hold for domain-specific or polysemous terms.
- Evidence anchors:
  - [abstract]: "Our approach formulates the subject tagging problem as an alignment task, where records are matched to GND categories based on semantic similarity."
  - [section 3.1]: "To align the technical records with the target subjects, we explore multiple levels of information from the records for representation of input data: 1) Title-based Representation..."
  - [corpus]: Related SemEval-2025 Task 5 papers (TartuNLP, Annif) similarly frame subject tagging as retrieval, suggesting this framing has traction but remains unproven as optimal.
- Break condition: If records require domain knowledge not captured in surface text similarity (e.g., implicit methodology conventions), alignment-based approaches will fail to retrieve correct subjects regardless of embedding quality.

### Mechanism 2
- Claim: Two-stage RAG (retrieval then LLM reranking) allows computationally efficient filtering of candidate subjects before expensive LLM evaluation.
- Mechanism: A fine-tuned embedding model (Nomic-AI) retrieves top-k=30 candidates via cosine similarity. A smaller LLM (Qwen2.5-0.5B) then classifies each candidate as match/no-match. This decouples broad recall from precision-focused filtering.
- Core assumption: Retrieval recall is sufficient at k=30 to capture relevant subjects; the LLM can effectively classify relevance even when trained on limited data (12,348 SFT samples).
- Evidence anchors:
  - [section 3.1]: "We configure the top-k to 30 subject tags."
  - [section 3.2]: "After retrieving the top-k relevant candidates for indexing a given librarian record, the LLM evaluates whether each subject is a suitable match or not."
  - [section 4.2]: "The recall@k curves show a steady increase as k increases, with a notable jump beyond k=15."
  - [corpus]: Weak comparative evidence—other Task 5 participants used varying approaches (ensemble, traditional XMTC), making it unclear if two-stage RAG is superior.
- Break condition: If relevant subjects rank beyond k=30, or if the small LLM lacks capacity to distinguish fine-grained domain distinctions, precision will remain low regardless of retrieval quality.

### Mechanism 3
- Claim: Contrastive fine-tuning of the retriever with positive/negative subject pairs improves discrimination between relevant and irrelevant subjects.
- Mechanism: The system constructs a balanced STS dataset pairing records with ground-truth subjects (score=1) and random non-associated subjects (score=0). Multiple Negatives Ranking Loss trains the embedding model to separate relevant from irrelevant pairs in embedding space.
- Core assumption: Random negative sampling provides sufficient contrastive signal; hard negatives (semantically similar but incorrect subjects) are not required.
- Evidence anchors:
  - [section 3.2]: "To introduce contrastive learning, we randomly selected negative samples—subjects not associated with the record—and assigned them a similarity score of 0."
  - [section 4.2]: "German language recall scores remain lower than English, likely due to richer training data or better linguistic resources embedded within LLMs."
  - [corpus]: No direct corpus evidence on contrastive learning effectiveness for this task; related work on XMTC suggests hard negative mining matters.
- Break condition: If random negatives are too easy to distinguish, the model may not learn fine-grained boundaries needed to discriminate among topically similar subjects within the same domain.

## Foundational Learning

- **Ontology Alignment**: Why needed: The core innovation is reframing classification as alignment. Understanding OA concepts (entity matching, correspondence generation, semantic similarity) is prerequisite to grasping why OntoAligner applies here. Quick check: Can you explain why treating "subject tagging" as "ontology alignment" is a non-obvious design choice rather than using standard multi-label classification?

- **Retrieval-Augmented Generation (RAG)**: Why needed: The system uses a RAG pipeline where retrieval precedes LLM-based reranking. Understanding retrieval-reranking tradeoffs is essential. Quick check: What would happen to system performance if you skipped the retrieval stage and passed all 100K+ GND subjects directly to the LLM?

- **Contrastive Learning & Negative Sampling**: Why needed: The retriever is fine-tuned with contrastive pairs. Understanding positive/negative pair construction affects interpretation of why precision remains low. Quick check: Why might random negative sampling produce weaker signal than hard negative mining for this task?

## Architecture Onboarding

- **Component map**: Title representation → Nomic-AI embedding → cosine similarity retrieval → top-30 candidates → Qwen2.5-0.5B classification → matched subjects

- **Critical path**: Record title → embedding → retrieve top-30 subjects → LLM classify each → return matched subjects. Title representation choice and k selection directly determine recall ceiling.

- **Design tradeoffs**:
  - Title-only vs contextual: Authors chose title-only due to computational constraints, but this may limit semantic richness.
  - k=30: Higher k improves recall but adds noise; optimal F1 observed at k=15–20.
  - Small LLM (0.5B): Efficient but may lack capacity for fine-grained domain distinctions.

- **Failure signatures**:
  - Low precision (2.84%) with decent recall (20.30%): Retrieved set is too broad; LLM reranking is ineffective.
  - Language gap (German < English): Embedding/LLM may have weaker German representations.
  - Document type variance: Conference/Reports underperform Articles/Books, suggesting title ambiguity or ground truth noise.

- **First 3 experiments**:
  1. **Ablate k values**: Run retrieval-only evaluation (skip LLM) at k=5,10,15,20,30 to quantify retriever recall ceiling before LLM bottlenecks.
  2. **Hard negative mining**: Replace random negatives with topically similar but incorrect subjects (e.g., same parent category) to test if contrastive signal improves.
  3. **Contextual vs title-only comparison**: Using dev set, compare title-only vs title+abstract representations to quantify information loss from current design choice.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would scaling to larger LLMs beyond 0.5B parameters significantly improve precision in subject tagging, or is the low precision inherent to the RAG-based alignment approach?
- Basis in paper: [explicit] Authors state "further fine-tuning with additional computational resources and data is necessary to enhance its precision and overall performance," and note the limitation "particularly for smaller LLMs."
- Why unresolved: The authors were computationally constrained to Qwen2.5-0.5B and could not test larger models against their pipeline.
- What evidence would resolve it: Comparative experiments running the same OntoAligner pipeline with LLMs of varying sizes (e.g., 7B, 14B, 70B) on the TIB-Core-Subjects dataset.

### Open Question 2
- Question: What specific factors cause the substantial performance gap between English and German records, and can targeted interventions close this gap?
- Basis in paper: [inferred] The paper reports that "German language recall scores remain lower than English, likely due to richer training data or better linguistic resources embedded within LLMs" but does not isolate the cause.
- Why unresolved: The authors hypothesize about training data and linguistic resources but do not conduct ablation studies to confirm.
- What evidence would resolve it: Controlled experiments with balanced bilingual training data, German-specific fine-tuning, and analysis of embedding quality across languages.

### Open Question 3
- Question: Can incorporating hierarchical and contextual representations (abstracts, parent metadata) improve performance over title-only representation when sufficient computational resources are available?
- Basis in paper: [explicit] Authors note they "preferred to move forward with title-based input representation" based on "computational resource on hand" after experimenting with three representation types.
- Why unresolved: Resource constraints forced a pragmatic choice before fully evaluating richer representations.
- What evidence would resolve it: Systematic comparison of all three representation strategies (title, contextual, hierarchical) with adequate compute on both development and test sets.

## Limitations
- Very low precision (2.84%) despite reasonable recall (20.30%), suggesting the LLM reranking stage is ineffective at filtering candidates
- Significant language performance gap with German records substantially underperforming English, indicating potential embedding or model limitations for German technical terminology
- Title-only representation chosen for computational efficiency may discard critical contextual information needed for accurate subject assignment

## Confidence

- **High confidence**: The core ontology alignment formulation as the methodological framework is well-specified and technically sound. The two-stage RAG pipeline architecture (retrieval + LLM reranking) is clearly described and reproducible.

- **Medium confidence**: The contrastive learning approach for retriever fine-tuning will improve semantic discrimination, though effectiveness depends on negative sampling quality. The title-only representation choice is a reasonable computational compromise but limits semantic coverage.

- **Low confidence**: The Qwen2.5-0.5B model's capacity to accurately classify fine-grained GND subjects given limited fine-tuning data (12,348 samples). The generalizability of random negative sampling for contrastive learning in this domain-specific context.

## Next Checks
1. **Ablate k values**: Run retrieval-only evaluation (skip LLM) at k=5,10,15,20,30 on the dev set to quantify retriever recall ceiling and isolate whether poor precision stems from retrieval breadth or LLM reranking ineffectiveness.

2. **Hard negative mining**: Replace random negatives with topically similar but incorrect subjects (e.g., same parent category) in the contrastive learning dataset to test if this provides stronger signal for fine-grained discrimination.

3. **Contextual vs title-only comparison**: Using dev set, compare title-only vs title+abstract representations in the pipeline to quantify information loss from current design choice and determine if contextual information improves performance sufficiently to justify computational cost.