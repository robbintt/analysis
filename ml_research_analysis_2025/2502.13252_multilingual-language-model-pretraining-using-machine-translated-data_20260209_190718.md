---
ver: rpa2
title: Multilingual Language Model Pretraining using Machine-translated Data
arxiv_id: '2502.13252'
source_url: https://arxiv.org/abs/2502.13252
tags:
- data
- languages
- arxiv
- language
- multilingual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates how to improve multilingual language model
  (LLM) pretraining by leveraging high-quality machine-translated data. The authors
  translate a high-quality English corpus into nine languages using a sentence-level
  NMT model, creating a 1.7-trillion-token dataset (TransWebEdu).
---

# Multilingual Language Model Pretraining using Machine-translated Data

## Quick Facts
- arXiv ID: 2502.13252
- Source URL: https://arxiv.org/abs/2502.13252
- Authors: Jiayi Wang; Yao Lu; Maurice Weber; Max Ryabinin; David Adelani; Yihong Chen; Raphael Tang; Pontus Stenetorp
- Reference count: 36
- One-line primary result: A 1.3B-parameter LLM trained on 1.7T tokens of machine-translated data matches or outperforms state-of-the-art multilingual models on 9 non-English reasoning tasks despite using an order of magnitude less data.

## Executive Summary
This paper demonstrates that high-quality machine-translated data can effectively substitute for massive native-language corpora in multilingual LLM pretraining. The authors translate a curated English corpus into 9 languages using a sentence-level NMT model, creating a 1.7-trillion-token dataset (TransWebEdu). They pretrain a 1.3B-parameter model (TransWebLLM) from scratch on this dataset, achieving performance that matches or exceeds state-of-the-art multilingual models (Llama3.2, Qwen2.5, Gemma) on nine non-English reasoning tasks. The key innovation is showing that translation artifacts are tolerable for pretraining when the source data quality is high, and that strategic mixing of translated data with small amounts of general web data and specialized training data can unlock additional performance gains.

## Method Summary
The authors translate a 100B-token subset of FineWeb-Edu (a high-quality English corpus) into 9 languages using NLLB-200-1.3B, creating the 1.7T-token TransWebEdu dataset. They pretrain a 1.3B-parameter decoder-only LLM (Llama2-style architecture) from scratch using Megatron-LM with FlashAttention-2. The training uses a constant learning rate of 6×10⁻⁴ with batch size 1024 and sequence length 2048. The model undergoes a three-stage training pipeline: (1) main pretraining on translated data (~1 epoch), (2) continued pretraining with ~5% general web data (RedPajama-v2, mC4), and (3) cooldown phase with <1% specialized data (synthetic MC questions, code, QA instruction data). Evaluation uses 9 multilingual reasoning benchmarks plus Global-MMLU and language-specific proficiency tests.

## Key Results
- TransWebLLM matches or outperforms state-of-the-art multilingual models (Llama3.2, Qwen2.5, Gemma) on 9 non-English reasoning tasks despite using ~10x less data
- Adding <5% general web data as continued pretraining sets new SOTA results in Arabic, Italian, Indonesian, Swahili, and Welsh
- TransWebLLM-web improves French grammar/vocabulary by 10 accuracy points (74.79 vs 65.13) and Indonesian cultural reasoning by 8 points (57.61 vs 48.84)
- The final cooldown phase with specialized data ranks the model among the top three overall on Global-MMLU

## Why This Works (Mechanism)

### Mechanism 1
High-quality machine-translated data from a single source language (English) can serve as effective multilingual pretraining data, even when using a smaller sentence-level NMT model. A 1.3B-parameter NMT model (NLLB-200-1.3B) translates a curated, education-focused English corpus into 9 languages at the sentence level, creating a 1.7T-token multiway parallel corpus. The hypothesis is that essential semantic and reasoning structures from the high-quality source are preserved through translation, making translation errors tolerable for pretraining.

### Mechanism 2
Continued pretraining on a mixture of translated educational data and a small amount of general web data significantly improves linguistic proficiency and cultural reasoning beyond what translated data alone can achieve. The initially pretrained TransWebLLM undergoes a second training phase where a balanced mixture of general web data is mixed with the original TransWebEdu. This exposure to colloquial language and cultural context missing from the academic source enhances linguistic proficiency without forgetting reasoning skills.

### Mechanism 3
A final "cooldown" phase using a lower learning rate and adding tiny amounts of specialized data (synthetic MC questions, code, instruction data) unlocks further performance gains on knowledge-heavy tasks. A reduced learning rate (10x lower) and small fraction (<1% of tokens) of specialized data allows stable adaptation for better knowledge retrieval and structured output without catastrophic forgetting.

## Foundational Learning

**Concept: Machine Translation (MT) vs. Synthetic Data Generation**
- Why needed here: The paper explicitly chooses MT over LLM-based generation for creating the core dataset
- Quick check question: According to the paper, what are the two primary limitations of using LLMs (e.g., GPT-4) for generating a trillion-token multilingual pretraining corpus?

**Concept: Continued Pretraining and Learning Rate Schedules**
- Why needed here: The best model is the product of a three-stage pipeline with specific learning rate changes
- Quick check question: Describe the learning rate schedule transition from main pretraining to final cooldown phase. What is the constant learning rate used in each?

**Concept: Multiway Parallel Corpora**
- Why needed here: The TransWebEdu dataset is multiway parallel, affecting how cross-lingual knowledge is stored and transferred
- Quick check question: During pretraining, how are documents from the multiway parallel corpus sampled into a single batch? What does this imply about the model's learning signal for a given concept across languages?

## Architecture Onboarding

**Component map:** FineWeb-Edu (100B-token subset) -> NLLB-200-1.3B translation (sentence-level) -> TransWebEdu (1.7T tokens) -> 1.3B decoder-only LLM (Llama2-style) -> Three-stage training pipeline -> Multilingual benchmarks

**Critical path:** High-Quality English Source → Efficient Translation → Multi-Stage Data Mixing & Training. The results show that while translation provides the foundation, the performance gains on linguistic and cultural tasks are driven by subsequent data mixing.

**Design tradeoffs:**
- Using 1.3B NLLB model instead of larger one accepts potentially lower translation fluency for computational efficiency
- Sentence-level translation is simpler but risks breaking document-level coherence
- Using Llama2 tokenizer for non-Latin scripts introduces inefficiencies (fragmented tokens) but leverages an existing well-understood tokenizer

**Failure signatures:**
- Tokenization inefficiency for Arabic/Russian leading to higher tokens-per-word ratios
- Data imbalance if translation pipeline fails for specific languages
- Forgetting of earlier skills during staged training (mitigated by data mixing and low cooldown learning rate)

**First 3 experiments:**
1. Source Quality Ablation: Pretrain on TransWebEdu vs. version translated from lower-quality English web dataset
2. Translation Granularity Test: Compare sentence-level vs. document-level translation for a single language
3. Data Mixing Ratio Sweep: Vary percentages of general web data (1%, 5%, 10%, 20%) in continued pretraining

## Open Questions the Paper Calls Out
The paper explicitly identifies several open questions for future work. First, it remains unclear whether the benefits of translated pretraining data would persist or amplify in substantially larger models (e.g., 70B+ parameters). Second, the authors did not conduct ablation studies to determine the optimal data mixing ratios between translated data, general web data, synthetic data, and code data. Third, the paper does not investigate whether models trained exclusively on machine-translated text can achieve native-level linguistic proficiency compared to models trained on native text.

## Limitations
- Translation quality remains a potential bottleneck for truly low-resource languages where parallel data may be scarce
- Sentence-level segmentation with NLTK may introduce coherence issues across document boundaries
- The use of Llama2 tokenizer for non-Latin scripts introduces known inefficiencies limiting effective context window
- Evaluation focuses primarily on reasoning tasks rather than generation quality, leaving questions about production performance

## Confidence

**High Confidence:** The core finding that TransWebLLM matches or outperforms state-of-the-art multilingual models on nine non-English reasoning tasks despite using an order of magnitude less data is well-supported by experimental results across multiple benchmarks.

**Medium Confidence:** The mechanism claims about why mixing general web data improves linguistic proficiency are supported by reported performance gains, but the causal relationship between specific web data components and improvements is not fully explored.

**Low Confidence:** The specific impact of the cooldown phase on knowledge-heavy tasks is less certain, as the paper provides limited ablation studies on individual contributions of synthetic MC questions, code data, and QA instruction data.

## Next Checks
1. **Source Quality Ablation Study:** Pretrain two models (one on TransWebEdu, one on version translated from lower-quality English web dataset) and compare downstream performance on multilingual reasoning benchmarks to quantify impact of source data quality.

2. **Translation Granularity Impact:** For a single language (e.g., German), create datasets using sentence-level vs. document-level translation, pretrain models on both, and compare performance on reasoning tasks and generation quality metrics.

3. **Data Mixing Ratio Optimization:** Starting from base TransWebLLM, run systematic sweep of continued pretraining runs with varying percentages of general web data (1%, 5%, 10%, 20%) to identify optimal mixing ratio for balancing reasoning performance and linguistic proficiency.