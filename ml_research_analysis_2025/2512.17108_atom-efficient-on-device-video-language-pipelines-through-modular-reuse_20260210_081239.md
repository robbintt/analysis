---
ver: rpa2
title: 'Atom: Efficient On-Device Video-Language Pipelines Through Modular Reuse'
arxiv_id: '2512.17108'
source_url: https://arxiv.org/abs/2512.17108
tags:
- video
- memory
- retrieval
- decoder
- execution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of efficiently running multi-stage
  video-language pipelines on mobile devices, where redundant model loads and fragmented
  execution lead to high latency and memory overhead. The authors propose ATOM, a
  system that modularizes video-language models into reusable encoder and decoder
  components, enabling persistent reuse and parallel execution across subtasks.
---

# Atom: Efficient On-Device Video-Language Pipelines Through Modular Reuse

## Quick Facts
- **arXiv ID**: 2512.17108
- **Source URL**: https://arxiv.org/abs/2512.17108
- **Authors**: Kunjal Panchal, Saayan Mitra, Somdeb Sarkhel, Haoliang Wang, Ishita Dasgupta, Gang Wu, Hui Guan
- **Reference count**: 5
- **Primary result**: 27-33% faster end-to-end latency on commercial smartphones for video-language pipelines

## Executive Summary
This paper addresses the challenge of efficiently running multi-stage video-language pipelines on mobile devices, where redundant model loads and fragmented execution lead to high latency and memory overhead. The authors propose ATOM, a system that modularizes video-language models into reusable encoder and decoder components, enabling persistent reuse and parallel execution across subtasks. By eliminating repeated model loading and leveraging concurrent processing, ATOM achieves 27–33% faster end-to-end latency on commercial smartphones compared to non-reuse baselines, with only marginal performance drops (≤2.3 Recall@1 in retrieval, ≤1.5 CIDEr in captioning) and minimal memory increase (~40MB).

## Method Summary
ATOM introduces a modular architecture for video-language pipelines that breaks down complex models into reusable encoder and decoder components. The system implements persistent loading of these components to avoid repeated initialization costs, while enabling parallel execution of subtasks to maximize device resource utilization. The design focuses on minimizing redundant operations in on-device inference, particularly the costly model loading and initialization phases that typically bottleneck mobile deployments.

## Key Results
- 27-33% faster end-to-end latency on commercial smartphones compared to non-reuse baselines
- Only marginal performance drops (≤2.3 Recall@1 in retrieval, ≤1.5 CIDEr in captioning)
- Minimal memory increase of approximately 40MB

## Why This Works (Mechanism)
ATOM's efficiency gains stem from eliminating redundant model loading operations and enabling parallel execution across subtasks. By modularizing video-language models into persistent encoder and decoder components, the system avoids the expensive initialization costs that occur when loading models repeatedly for different pipeline stages. The concurrent processing architecture maximizes CPU and memory utilization on mobile devices, allowing multiple pipeline stages to execute simultaneously rather than sequentially.

## Foundational Learning

**Modular Model Design**
- *Why needed*: Enables reuse of common components across different pipeline stages
- *Quick check*: Verify encoder/decoder components can be independently loaded and unloaded

**Persistent Component Loading**
- *Why needed*: Eliminates repeated initialization overhead for frequently used model parts
- *Quick check*: Measure load times with and without persistence

**Parallel Task Execution**
- *Why needed*: Maximizes resource utilization on multi-core mobile processors
- *Quick check*: Monitor CPU core usage during concurrent pipeline execution

**Memory Management**
- *Why needed*: Ensures efficient memory usage despite multiple persistent components
- *Quick check*: Track memory allocation patterns during peak usage

**Latency Measurement**
- *Why needed*: Quantifies the performance benefits of the modular approach
- *Quick check*: Compare end-to-end times against baseline implementations

## Architecture Onboarding

**Component Map**
User request -> Task scheduler -> Encoder pool -> Decoder pool -> Output aggregator

**Critical Path**
Request reception → Component selection → Parallel execution → Result aggregation → Response

**Design Tradeoffs**
Memory overhead vs. latency reduction (40MB increase for 27-33% speedup), component granularity vs. reuse potential, parallel execution complexity vs. resource utilization

**Failure Signatures**
Model loading failures, memory allocation errors, component contention, parallel execution deadlocks

**First 3 Experiments to Run**
1. Measure baseline latency without any reuse optimizations
2. Test component loading/unloading times with persistence enabled
3. Verify parallel execution correctness across multiple subtasks

## Open Questions the Paper Calls Out
None specified in the provided material.

## Limitations
- Evaluation limited to two specific tasks (retrieval and captioning) and two model types (MS-ASL and HowTo100M)
- Does not address energy consumption or battery impact on mobile devices
- Memory overhead analysis lacks exploration of memory-constrained scenarios

## Confidence

**High Confidence**: The core technical contribution of modular reuse and parallel execution is well-supported by the experimental results, with clear latency improvements (27–33%) over non-reuse baselines on commercial smartphones. The methodology for eliminating redundant model loads is straightforward and reproducible.

**Medium Confidence**: The reported quality retention metrics (Recall@1 and CIDEr) are specific to the evaluated tasks and models. While the results are positive, the generalizability to other video-language tasks or larger-scale models remains uncertain without further validation.

**Low Confidence**: The paper does not provide sufficient evidence for long-term system stability, energy efficiency, or performance under varying workloads. These aspects are critical for real-world deployment but are not addressed in the current evaluation.

## Next Checks

1. **Generalization Across Tasks**: Test ATOM on additional video-language tasks (e.g., action recognition, video summarization) and diverse datasets to assess the robustness of the modular reuse approach beyond retrieval and captioning.

2. **Memory-Constrained Scenarios**: Evaluate ATOM's performance under varying memory constraints (e.g., 512MB, 1GB) to determine its scalability and behavior in resource-limited environments.

3. **Energy Efficiency Analysis**: Measure the energy consumption and battery impact of ATOM during prolonged usage on mobile devices to ensure it meets real-world deployment requirements.