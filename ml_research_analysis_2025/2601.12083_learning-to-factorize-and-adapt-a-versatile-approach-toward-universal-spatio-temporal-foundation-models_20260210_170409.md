---
ver: rpa2
title: 'Learning to Factorize and Adapt: A Versatile Approach Toward Universal Spatio-Temporal
  Foundation Models'
arxiv_id: '2601.12083'
source_url: https://arxiv.org/abs/2601.12083
tags:
- temporal
- forecasting
- spatial
- data
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FactoST-v2 introduces a factorized framework for universal spatio-temporal
  foundation models, addressing the computational challenges of joint spatial-temporal
  pretraining by decoupling universal temporal dynamics from domain-specific spatial
  adaptation. The approach features a minimalist encoder-only backbone for scalable
  temporal pretraining using randomized sequence masking and probabilistic quantile
  prediction, followed by lightweight spatio-temporal adaptation via prompt alignment
  and memory replay.
---

# Learning to Factorize and Adapt: A Versatile Approach Toward Universal Spatio-Temporal Foundation Models

## Quick Facts
- arXiv ID: 2601.12083
- Source URL: https://arxiv.org/abs/2601.12083
- Authors: Siru Zhong; Junjie Qiu; Yangyu Wu; Yiqiu Liu; Yuanpeng He; Zhongwen Rao; Bin Yang; Chenjuan Guo; Hao Xu; Yuxuan Liang
- Reference count: 40
- Primary result: FactoST-v2 achieves state-of-the-art zero-shot and few-shot performance with linear complexity by decoupling temporal and spatial learning.

## Executive Summary
FactoST-v2 introduces a factorized framework for universal spatio-temporal foundation models that addresses the computational challenges of joint spatial-temporal pretraining. By decoupling universal temporal dynamics from domain-specific spatial adaptation, it enables scalable pretraining on massive graph-agnostic time series followed by lightweight adaptation to specific spatial domains. The approach achieves superior performance across diverse domains while maintaining computational efficiency and uncertainty quantification capabilities.

## Method Summary
FactoST-v2 operates in two stages: Universal Temporal Pretraining (UTP) and Spatio-Temporal Adaptation (STA). UTP uses an encoder-only Transformer backbone trained on diverse univariate time series with randomized sequence masking and probabilistic quantile prediction. STA then injects domain-specific spatial context through lightweight modules including STMF (spatial/temporal metadata fusion), STF (affinity-based filtering), DSPA (domain-specific prompt alignment), and CMR (continual memory replay). This factorization enables full weight transfer, arbitrary-length generalization, and robust uncertainty quantification while maintaining linear complexity.

## Key Results
- Achieves state-of-the-art accuracy in zero-shot and few-shot scenarios across diverse domains
- Significantly outperforms existing foundation models while rivaling domain-specific expert baselines
- Enables full weight transfer and arbitrary-length generalization with robust uncertainty quantification
- Maintains linear complexity despite superior performance

## Why This Works (Mechanism)

### Mechanism 1: Factorization Decouples Domain-Invariant and Domain-Specific Learning
FactoST-v2 achieves universal spatio-temporal generalization by separating learning into universal temporal dynamics and domain-specific spatial contexts. The UTP stage pretrains a lightweight backbone on massive, graph-agnostic univariate time series to capture transferable temporal patterns, while STA uses a lightweight adapter to inject spatial context. This assumes temporal patterns are more transferable across domains than spatial patterns tied to specific topologies.

Evidence anchors: The abstract states FactoST-v2 "decouples universal temporal learning from domain-specific spatial adaptation" and "enables full weight transfer." Section III-A details the UTP stage as "graph-agnostic, lightweight, scalable, and fully transferable." Section III-B describes STA with lightweight modules for "full weight transfer of the pretrained temporal knowledge." Empirical results show FactoST-v2 outperforming joint ST foundation models in zero-shot and few-shot settings.

### Mechanism 2: Encoder-Only Architecture with Probabilistic Quantile Forecasting
An encoder-only Transformer backbone trained with Pinball Loss for multi-quantile prediction provides flexible, uncertainty-aware forecasting. The [REG] token separates historical context from future placeholders, and randomized sequence masking enables arbitrary input-output length mapping. This architecture learns the full conditional distribution rather than single point estimates, enabling robust probabilistic forecasts with confidence intervals.

Evidence anchors: Section I contrasts FactoST-v2 with the "rigid, deterministic design of FactoST-v1" and highlights the upgrade to a "flexible, probabilistic backbone." Section III-A describes Randomized Sequence Masking and the [REG] token for "arbitrary input-output length mapping" and details the Multi-Quantile Prediction Head and Pinball Loss. Section IV-G visualizes probabilistic forecasting output showing adaptive confidence intervals.

### Mechanism 3: Lightweight Spatio-Temporal Adaptation via Prompting and Affinity-Based Filtering
A lightweight adapter module efficiently injects complex spatial awareness into a frozen temporal backbone through STMF embeddings, STF affinity reweighting (spatial/temporal/time-lagged), DSPA global prompts, and CMR memory replay. This assumes spatial differences between domains can be captured by explicit embeddings, reweighting via learned affinities, and representation space shifts with global prompt tokens.

Evidence anchors: Section III-B provides architectural details of all STA components. Section IV-E ablation studies show performance degradation when removing STMF, STF components, or CMR. Section IV-G visualizes dynamic STF weights and learnable delay coefficients confirming asynchronous correlation modeling.

## Foundational Learning

### Attention Mechanisms in Transformers (Encoder-Only)
**Why needed:** The entire UTP backbone is an encoder-only Transformer requiring understanding of self-attention, positional embeddings (especially p-RoPE), and special tokens like `[REG]`. The paper's "Gated Attention" modifies this core concept.
**Quick check:** How does an encoder-only Transformer use self-attention to create contextualized representations, and what is the role of a special token like `[REG]`?

### Spatio-Temporal Graph Neural Networks (STGNNs)
**Why needed:** The paper contrasts its approach with traditional "coupled" STGNNs and positions itself as a successor. Understanding STGNN coupling of graph convolutions with temporal modules helps grasp the "pattern mismatch" problem FactoST-v2 solves.
**Quick check:** In a traditional STGNN, how are spatial dependencies and temporal dependencies typically coupled during the forward pass?

### Quantile Regression & Pinball Loss
**Why needed:** The paper upgrades to probabilistic quantile prediction using Pinball Loss for uncertainty quantification. Understanding quantiles and how optimizing for multiple quantiles teaches the full predictive distribution is essential.
**Quick check:** For a model predicting 10th, 50th, and 90th percentiles, how does the loss function for the 90th percentile (quantile 0.9) differ from that of the 50th percentile (quantile 0.5)?

## Architecture Onboarding

### Component map
Pretraining (UTP) -> Load frozen UTP backbone -> Initialize STA adapter (STMF, STF, DSPA) & CMR buffer -> Adaptation (STA): Input -> Patching & Norm -> Frozen UTP Backbone -> Temporal Features `Z` -> STMF (Add ST Embeds) -> STF (Refine features) -> DSPA (Add Global Prompts) -> Frozen UTP Backbone (re-encode) -> Multi-Quantile Head -> Loss (L1 on median for deterministic benchmarks)

### Critical path
The critical path flows from pretraining through frozen backbone loading, adapter initialization, and spatio-temporal adaptation. Data flows through patching and normalization, feature extraction via the frozen backbone, spatial-temporal refinement through the adapter modules, and final prediction through the multi-quantile head.

### Design tradeoffs
- **Accuracy vs. Universality:** Lightweight adapter prioritizes fast, data-efficient adaptation over potentially higher asymptotic accuracy of large, jointly fine-tuned models
- **Flexibility vs. Inductive Bias:** Encoder-only architecture is maximally flexible (arbitrary lengths) but may lack strong task-specific decoder biases for fixed horizons
- **Efficiency vs. Complexity:** Factorized design is extremely parameter and computationally efficient but may be less expressive than monolithic models learning coupled ST dynamics end-to-end

### Failure signatures
- **Zero-shot regression failure:** Erratic forecasts or high bias indicates pretraining corpus didn't cover relevant temporal patterns
- **Adapter overfitting:** Performance improvement on training data but degradation on validation data during STA indicates overfitting; reduce adapter complexity or increase CMR effect
- **Uncertainty calibration failure:** Predicted confidence intervals consistently too narrow or wide indicates unstable Pinball Loss optimization or poor calibration

### First 3 experiments
1. **Verify UTP Zero-Shot Capability:** Load pretrained FactoST-v2 backbone (no adapter) and run zero-shot forecasts on held-out evaluation datasets (PEMS03/04, ETTh2). Compare against other TSFMs to validate temporal representation quality and universality.
2. **Ablate the STA Adapter:** On PEMS03, train FactoST-v2 in few-shot setting while systematically ablating each STA component (remove STMF, remove STF, remove DSPA, remove CMR). Quantify performance drop to understand each module's contribution.
3. **Analyze Scaling Laws:** Repeat few-shot adaptation on PEMS03 while varying labeled data proportion (0%, 5%, 10%, 20%, ..., 100%). Plot learning curve to confirm fast adaptation phenomenon and identify data-scarce regime where factorized approach provides most advantage.

## Open Questions the Paper Calls Out

### Open Question 1
**Question:** How can spatio-temporal foundation models transition from transductive or semi-inductive settings to achieve fully inductive, zero-shot spatial generalization on dynamic, open-world topologies where node sets evolve continuously?
**Basis in paper:** Section VI states the current framework relies on learned embeddings for specific nodes and identifies "fully inductive, zero-shot spatial generalization on dynamic, open-world topologies" as a "critical next step."
**Why unresolved:** Current adapters depend on learning embeddings for fixed node sets, failing when new nodes appear without historical data or pre-defined structures.
**What evidence would resolve it:** A model architecture capable of predicting dynamics for entirely unseen nodes or graphs solely based on structural properties or metadata, validated on benchmarks with evolving topologies.

### Open Question 2
**Question:** Can "Text-to-Space" alignment leverage Large Language Models (LLMs) to generate semantic graph structures from auxiliary metadata (e.g., POI descriptions) without requiring training on historical graph data?
**Basis in paper:** Section VI proposes exploring "Text-to-Space alignmentâ€”leveraging Large Language Models (LLMs) to generate semantic graph structures from auxiliary metadata" as a method to enhance robustness against graph evolution.
**Why unresolved:** It is unclear if LLMs can accurately map unstructured semantic text to the rigorous geometric and relational constraints required for effective spatial message passing in STFMs.
**What evidence would resolve it:** Demonstrated success in constructing functional adjacency matrices for unseen regions using only textual descriptions, matching or exceeding performance of data-driven graph structure learning.

### Open Question 3
**Question:** How can a unified, lightweight interface be designed to effectively fuse rich, multi-modal exogenous factors (e.g., textual event descriptions) into the factorized framework to enhance causal reasoning?
**Basis in paper:** Section VI notes that "real-world dynamics are often driven by rich, multi-modal exogenous factors" and suggests designing a "unified, lightweight interface" as a "natural and valuable extension."
**Why unresolved:** The current architecture primarily models endogenous historical patterns, lacking a mechanism to ingest and align heterogeneous external signals with the temporal backbone.
**What evidence would resolve it:** Integration of a cross-modal adapter that improves forecast accuracy during anomalous events by processing external textual or visual data streams without fine-tuning the core backbone.

## Limitations
- Factorization hypothesis relies on untested assumption that temporal dynamics are more domain-invariant than spatial patterns
- Lightweight adapter's ability to capture complex, novel spatial topologies remains unproven, especially for dynamic or non-stationary graph structures
- Pretraining corpus scale (11B time points) impressive but exact composition and diversity underspecified, making reproducibility challenging

## Confidence

### High Confidence
- Empirical superiority over existing foundation models in zero-shot and few-shot settings is well-supported by ablation studies and benchmark comparisons

### Medium Confidence
- Mechanism of decoupling temporal and spatial learning is theoretically sound and aligns with broader survey literature, but universal applicability across all ST domains requires further validation

### Low Confidence
- Exact scaling behavior and performance limits of lightweight adapter on highly complex or novel spatial topologies are not fully characterized

## Next Checks
1. **Validate Pretraining Corpus Diversity:** Conduct systematic analysis of pretraining corpus composition to ensure adequate representation of temporal patterns across diverse ST domains. Test zero-shot performance on domains outside pretraining set.
2. **Stress-Test Adapter Robustness:** Evaluate lightweight adapter performance on domains with highly dynamic or non-stationary graph structures. Compare against jointly fine-tuned monolithic model to quantify tradeoff between adaptation efficiency and asymptotic accuracy.
3. **Analyze Uncertainty Calibration:** Systematically assess model's probabilistic forecasts across different data regimes (abundant vs. scarce) and distribution shifts. Calibrate Pinball Loss optimization to ensure predicted confidence intervals are well-calibrated.