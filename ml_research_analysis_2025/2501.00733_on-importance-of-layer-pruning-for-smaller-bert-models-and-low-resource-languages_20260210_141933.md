---
ver: rpa2
title: On Importance of Layer Pruning for Smaller BERT Models and Low Resource Languages
arxiv_id: '2501.00733'
source_url: https://arxiv.org/abs/2501.00733
tags:
- pruning
- bert
- performance
- marathi
- middle
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates layer pruning as a strategy to create smaller,
  task-specific BERT models for low-resource languages like Marathi. Pruning strategies
  (top, middle, bottom) were applied to models like MahaBERT-v2 and Google-Muril,
  and their performance compared against baseline scratch-trained models on three
  Marathi text classification tasks (short headlines, long paragraphs, long documents).
---

# On Importance of Layer Pruning for Smaller BERT Models and Low Resource Languages

## Quick Facts
- arXiv ID: 2501.00733
- Source URL: https://arxiv.org/abs/2501.00733
- Reference count: 14
- Primary result: Pruned pre-trained models outperform scratch-trained models of equivalent size for low-resource language text classification

## Executive Summary
This paper investigates layer pruning as a strategy to create smaller, task-specific BERT models for low-resource languages like Marathi. The study compares three pruning strategies (top, middle, bottom) applied to MahaBERT-v2 and Google-Muril models against scratch-trained baselines on three Marathi text classification tasks. The findings demonstrate that pruned models can maintain comparable performance to full models while significantly reducing size, with middle-layer pruning often achieving the best results. Critically, pruned models consistently outperform similarly sized models trained from scratch, highlighting pruning followed by fine-tuning as a more efficient alternative for low-resource language applications.

## Method Summary
The method involves loading pre-trained MahaBERT-v2 or Google-Muril (12-layer BERT models), pruning layers using three strategies (top, middle, bottom), and fine-tuning the pruned models on Marathi text classification tasks. The pruning removes either 6 or 10 layers, leaving 6-layer or 2-layer models respectively. These pruned models are compared against scratch-trained baselines (MahaBERT-Small with 6 layers and MahaBERT-Smaller with 2 layers) on the L3Cube-IndicNews Corpus containing three datasets: Short Headline Classification (SHC), Long Paragraph Classification (LPC), and Long Document Classification (LDC).

## Key Results
- Pruned models achieve comparable performance to full models while significantly reducing size
- Middle-layer pruning is particularly effective, often achieving competitive results across different tasks
- Pruned models consistently outperform scratch-trained models of similar size, demonstrating the value of pre-trained knowledge
- Monolingual models (MahaBERT-v2) outperform multilingual models (Google-Muril) in these experiments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pruned pre-trained models outperform scratch-trained models of equivalent size because they retain transferable linguistic knowledge in the remaining layers
- Mechanism: When layers are removed from a pre-trained BERT model, the remaining layers preserve learned representations from pre-training. Fine-tuning adapts these preserved representations to the target task, whereas scratch-trained models must learn all representations from limited task data
- Core assumption: The linguistic knowledge encoded in pre-trained layers is sufficiently distributed that partial removal does not catastrophically destroy task-relevant capabilities
- Evidence anchors: Abstract states pruned models "achieve comparable performance to their fully-layered counterparts while consistently outperforming scratch-trained models of similar size"; section 1 notes 2-layer and 6-layer pruned models outperform scratch-trained baselines

### Mechanism 2
- Claim: Middle-layer pruning provides competitive performance because BERT's intermediate layers contain partially redundant transition representations that can be bypassed with minimal information loss
- Mechanism: BERT architectures encode low-level features in bottom layers and high-level abstractions in top layers. Middle layers serve as a transition zone whose removal forces remaining layers to compensate during fine-tuning, achievable because adjacent layers encode partially overlapping information
- Core assumption: Adjacent transformer layers have sufficient representational overlap that pruning middle layers creates recoverable gaps rather than irreversible discontinuities
- Evidence anchors: Abstract notes "pruning layers from the middle of the model proves to be the most effective strategy"; section 4 states "pruning from the middle proves to be a generally effective choice"

### Mechanism 3
- Claim: Monolingual models outperform multilingual models for low-resource language tasks because language-specific pre-training concentrates capacity on relevant linguistic patterns
- Mechanism: Multilingual models distribute capacity across multiple languages, diluting representation quality for any single language. Monolingual models dedicate all parameters to the target language, yielding denser and more task-relevant representations even after pruning
- Core assumption: The low-resource language has sufficient pre-training corpus to make monolingual specialization beneficial
- Evidence anchors: Abstract states "monolingual BERT models outperform multilingual ones in these experiments"; section 3.1.1 describes MahaBERT-v2 as specialized for Marathi versus Google-Muril as multilingual

## Foundational Learning

- Concept: Layer Pruning vs. Other Compression Techniques
  - Why needed here: Layer pruning is a structured removal of entire transformer blocks, distinct from weight pruning or knowledge distillation. Understanding this distinction is necessary to interpret why pruning preserves more performance than scratch training
  - Quick check question: If you remove 50% of layers from a 12-layer BERT, what remains—a 6-layer model with original weights, or a re-initialized 6-layer architecture?

- Concept: Fine-tuning Dynamics After Pruning
  - Why needed here: The paper's results depend on fine-tuning recovering performance after layer removal. Without understanding that fine-tuning redistributes functional responsibility across remaining layers, the results appear counterintuitive
  - Quick check question: After pruning layers, should you (a) freeze remaining layers and train only the classification head, or (b) fine-tune all remaining layers? Which does this paper assume?

- Concept: Resource Constraints in Low-Resource Language NLP
  - Why needed here: The motivation for pruning derives from computational and data scarcity. Understanding what "low-resource" means clarifies why pruning + fine-tuning is preferred over scratch training
  - Quick check question: What makes a language "low-resource" — lack of speakers, lack of labeled datasets, or lack of digital text corpora?

## Architecture Onboarding

- Component map: Tokenized Marathi text -> Pruned BERT encoder (6 or 2 layers) -> Classification head -> Category prediction
- Critical path: 1) Load pre-trained BERT (MahaBERT-v2 or Google-Muril) 2) Select pruning strategy (top/middle/bottom) and layers (6 or 10) 3) Remove specified layers, reindex connections 4) Initialize classification head 5) Fine-tune on Marathi classification dataset 6) Evaluate accuracy
- Design tradeoffs: Pruning 6 layers vs. 10 layers (more aggressive pruning increases efficiency but risks accuracy loss); Top vs. middle vs. bottom pruning (no universal winner); Monolingual vs. multilingual backbone (monolingual yields better performance but requires language-specific pre-trained model availability)
- Failure signatures: Pruned model accuracy drops significantly below scratch-trained baseline (suggests pruning removed task-critical layers); Large gap between validation and test accuracy (potential overfitting during fine-tuning); Middle pruning underperforms on long documents (middle layers may be more critical for document-level reasoning)
- First 3 experiments: 1) Baseline comparison: Fine-tune full MahaBERT-v2 on SHC dataset; compare against MahaBERT-Small scratch-trained baseline 2) Pruning position sweep: Prune 6 layers from top, middle, and bottom positions; evaluate on all three datasets (SHC, LPC, LDC) 3) Aggressive pruning test: Prune 10 layers (leaving only 2) using all three strategies; compare against MahaBERT-Smaller (2-layer scratch)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the findings regarding middle-layer pruning effectiveness generalize to other low-resource languages beyond Marathi?
- Basis in paper: The authors state in the conclusion: "A future direction for this work is to investigate whether these findings hold across other low-resource languages."
- Why unresolved: The entire experimental scope was limited to the Marathi language
- What evidence would resolve it: Replicating the pruning experiments on a diverse set of low-resource language datasets

### Open Question 2
- Question: Is layer pruning (specifically middle-layer removal) effective for non-classification tasks such as Named Entity Recognition or Question Answering?
- Basis in paper: The study restricted its evaluation to three text classification tasks (SHC, LPC, LDC)
- Why unresolved: Different downstream tasks may rely on different linguistic features encoded in specific layers, which pruning might disrupt differently than classification
- What evidence would resolve it: Benchmarking pruned models on token-level or span-prediction tasks

### Open Question 3
- Question: Can specific dataset characteristics, such as input length or complexity, serve as predictors for the optimal pruning strategy (top vs. middle vs. bottom)?
- Basis in paper: The paper notes that "different pruning strategies perform better depending on the model and dataset combinations" and lists varying winners for short headlines vs. long documents
- Why unresolved: The paper observes the variance in results but does not offer a theoretical explanation or a method to determine the best strategy a priori
- What evidence would resolve it: A cross-dimensional analysis correlating input text length/complexity with the performance delta of specific pruning masks

### Open Question 4
- Question: How does the performance of these pruned models compare to other compression techniques like knowledge distillation?
- Basis in paper: The authors compare pruning against scratch-trained models but, despite mentioning distillation in related work, they do not compare against distilled versions of the models
- Why unresolved: It remains unclear if pruning is superior or inferior to distillation for creating efficient low-resource models
- What evidence would resolve it: A direct comparison of accuracy and inference speed between pruned MahaBERT and a distilled student model of equivalent size

## Limitations
- Pruning strategy definition: The paper does not specify exact layer indices removed for each pruning strategy, creating ambiguity in reproducing results
- Fine-tuning hyperparameters: Critical training parameters (learning rate, batch size, epochs) are not provided, limiting faithful reproduction
- Language generalizability: Results are demonstrated only for Marathi; performance patterns may differ for other low-resource languages

## Confidence
- High confidence: Claims about pruned models outperforming scratch-trained models of equivalent size are well-supported by empirical results
- Medium confidence: Claims about middle-layer pruning being most effective require more systematic validation across diverse tasks
- Medium confidence: Claims about monolingual superiority over multilingual models are context-dependent and may not generalize to extremely low-resource languages

## Next Checks
1. Systematically test all possible middle-layer configurations (different contiguous blocks) to identify which specific layers are most critical for performance
2. Apply identical pruning methodology to another low-resource language (e.g., Hindi or Bengali) to test whether monolingual advantages hold
3. Use ablation studies to quantify information loss when removing specific layers, validating the redundancy hypothesis underlying middle-layer pruning effectiveness