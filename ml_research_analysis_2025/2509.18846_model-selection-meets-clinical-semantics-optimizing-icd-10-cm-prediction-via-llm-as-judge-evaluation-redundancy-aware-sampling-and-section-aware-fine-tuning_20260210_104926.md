---
ver: rpa2
title: 'Model selection meets clinical semantics: Optimizing ICD-10-CM prediction
  via LLM-as-Judge evaluation, redundancy-aware sampling, and section-aware fine-tuning'
arxiv_id: '2509.18846'
source_url: https://arxiv.org/abs/2509.18846
tags:
- coding
- section
- performance
- chapter
- codes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces a modular framework for ICD-10-CM code prediction
  using open-source large language models (LLMs), addressing challenges in model selection,
  data redundancy, and context-aware prompting. The framework employs an LLM-as-judge
  evaluation with Plackett-Luce ranking to identify the best-performing base model
  based on intrinsic ICD-10-CM code comprehension.
---

# Model selection meets clinical semantics: Optimizing ICD-10-CM prediction via LLM-as-Judge evaluation, redundancy-aware sampling, and section-aware fine-tuning

## Quick Facts
- arXiv ID: 2509.18846
- Source URL: https://arxiv.org/abs/2509.18846
- Reference count: 40
- Primary result: Introduces modular framework for ICD-10-CM prediction using LLM-as-judge evaluation, redundancy-aware sampling, and section-aware fine-tuning, achieving F1-scores of 0.802 (internal) and 0.636 (external)

## Executive Summary
This study presents a comprehensive framework for automated ICD-10-CM code prediction from discharge summaries using open-source large language models. The framework addresses three critical challenges: optimal model selection through LLM-as-judge evaluation with Plackett-Luce ranking, data efficiency via redundancy-aware sampling, and context enhancement through section-aware structured prompting. Experiments on two institutional datasets demonstrate that the selected BioMistral model outperforms baseline approaches, while section-aware inputs and deduplication strategies consistently improve performance and efficiency.

## Method Summary
The framework integrates three key innovations: (1) LLM-as-judge evaluation using pairwise comparisons and Plackett-Luce aggregation to select optimal base models based on intrinsic ICD-10-CM code comprehension, (2) redundancy-aware sampling that removes semantically duplicated discharge summaries using embedding similarity and perplexity-based retention, and (3) section-aware fine-tuning with explicit clinical section prefixes in instruction prompts. The pipeline employs QLoRA for efficient fine-tuning of 7B-parameter models on single GPUs, with priority-based truncation to handle token limits. The system is validated on private institutional datasets from KMUH and TMMH hospitals, demonstrating robust performance across internal and external test sets.

## Key Results
- BioMistral-7B selected via LLM-as-judge achieved highest F1-score of 0.802 on internal test set and 0.636 on external validation
- Redundancy-aware sampling reduced training data by 14.8% while improving efficiency and maintaining performance
- Medical History section contributed most significant performance gains (+0.007 F1) among clinical sections
- Section-aware universal model outperformed section-specific models when tested on mixed-section inputs
- Main Diagnosis Code Accuracy (MDCA) achieved 0.812 on internal and 0.573 on external datasets

## Why This Works (Mechanism)

### Mechanism 1: LLM-as-Judge with Plackett-Luce Aggregation for Model Selection
- Claim: Ranking candidate LLMs by intrinsic ICD-10 comprehension via pairwise comparison predicts downstream fine-tuning success
- Mechanism: Lightweight judge model (Atla Selene Mini) compares candidate outputs for ICD-10 code definition generation; pairwise win-rates form transition matrix; ILSR estimates stationary distribution π for global rankings without exhaustive fine-tuning
- Core assumption: Intrinsic code comprehension correlates with downstream task performance after domain fine-tuning
- Evidence anchors: BioMistral achieved highest selection probability (0.441) and subsequently highest F1 (0.78); related work on clinical scoring systems supports autoformulation but doesn't validate Plackett-Luce specifically
- Break condition: Judge model exhibits systematic bias toward certain output styles unrelated to clinical accuracy

### Mechanism 2: Redundancy-Aware Sampling via Embedding Similarity
- Claim: Removing semantically duplicated discharge summaries with identical ICD codes improves training efficiency and may enhance generalization
- Mechanism: Encode summaries with all-MiniLM-L6-v2; construct FAISS index; identify near-duplicates via L2 distance; for pairs sharing identical ICD codes with similarity >0.9, retain summary with higher perplexity (or longer length if within 5% PPL)
- Core assumption: Semantic redundancy offers diminishing learning returns; diversity in phrasing improves model robustness
- Evidence anchors: Deduplicated model outperformed baseline across PRF-scores with 10.2% training time reduction; no direct corpus validation but prior deduplication work supports general principle
- Break condition: Deduplication removes rare diagnostic presentations or institution-specific documentation patterns

### Mechanism 3: Section-Aware Structured Input Design
- Claim: Explicitly segmenting clinical sections in instruction prompts improves ICD-10-CM prediction, with Medical History contributing most diagnostic value
- Mechanism: Each section prefixed with "### [SectionName]" in prompt template; model trained to generate codes conditioned on structured input; universal model handles all section combinations; section-specific models trained on fixed subsets
- Core assumption: Models benefit from explicit structural signals; clinically relevant sections contain stronger diagnostic cues
- Evidence anchors: MedHist contributed most significant gains (+0.007 F1 over DischgDiag alone); OpNote alone showed marginal drop; related work on hierarchy-guided transformers supports structured modeling
- Break condition: Input exceeds 2,048-token limit and truncation removes high-priority sections; section-specific models underperform on heterogeneous inputs

## Foundational Learning

- **Plackett-Luce Model / ILSR Algorithm**
  - Why needed here: Aggregates pairwise comparison results into global model ranking without requiring complete comparison matrices
  - Quick check question: Given win-rate matrix between 4 models, can you explain why stationary distribution π provides a coherent ranking?

- **QLoRA (Quantized Low-Rank Adaptation)**
  - Why needed here: Enables fine-tuning 7B-parameter models on single GPU with 4-bit quantization
  - Quick check question: What is the tradeoff between rank (r) in LoRA and representational capacity vs. memory footprint?

- **Multi-Label Classification Metrics (Micro-F1, MDCA)**
  - Why needed here: ICD coding assigns multiple codes per record; micro-averaging handles label imbalance; MDCA tracks primary diagnosis accuracy critical for DRG assignment
  - Quick check question: Why might macro-F1 be misleading for ICD code prediction with long-tail code distributions?

## Architecture Onboarding

- **Component map:** Data Acquisition → Preprocessing (cleaning + redundancy-aware sampling) → Model Selection (LLM-as-judge + Plackett-Luce) → Section-Aware Fine-tuning (QLoRA) → Deployment/Inference
- **Critical path:** Model selection (Exp 1) → fine-tuning (Exp 2) → redundancy + section experiments (Exp 3) → external validation (Exp 4); if base model selection fails, downstream experiments inherit suboptimal initialization
- **Design tradeoffs:** Universal model vs. section-specific (universal generalizes across variable inputs; section-specific excels when input matches training structure but underperforms otherwise); 2,048-token limit vs. full content (85% coverage; priority-based truncation preserves high-value sections); deduplication threshold (0.9) (higher preserves more data; lower increases efficiency but risks losing rare patterns)
- **Failure signatures:** Judge model output parsing failures (non-standard responses like "2 (Note: This result does NOT follow your format)") → implemented regex fallbacks; token overflow on multi-section inputs → priority-based truncation by CCS-ranked section importance; section-specific models evaluated on mismatched section combinations → performance drop; universal model recommended for heterogeneous deployments
- **First 3 experiments:** 1) Validate LLM-as-judge rankings against downstream fine-tuning performance (5 candidate models, top-50 ICD codes); 2) Compare fine-tuned decoder models vs. baselines (BERT, BiGRU, HAN) on DischgDiag-only internal test set; 3) Ablate redundancy-aware sampling + incremental section inclusion with BioMistral; evaluate on matched subsets vs. full test set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the proposed framework be effectively extended to support multi-type medical coding tasks, specifically ICD-10-PCS procedure coding?
- Basis in paper: [explicit] The authors explicitly state the work focuses exclusively on diagnostic codes and does not address procedure coding, which is "clinically and financially important"
- Why unresolved: Current study design and experiments were restricted to ICD-10-CM diagnostic codes only
- What evidence would resolve it: Implementation of the pipeline on operation notes with ICD-10-PCS targets, evaluated against certified coding specialists

### Open Question 2
- Question: Does the framework generalize to other clinical document types, such as outpatient visit notes or daily inpatient progress notes?
- Basis in paper: [explicit] The authors note the results may not generalize to documents that differ structurally and semantically from the discharge summaries utilized
- Why unresolved: Experiments conducted solely on discharge summaries from hospitalized patients
- What evidence would resolve it: Evaluation of fine-tuned models on outpatient and progress note datasets to assess structural robustness

### Open Question 3
- Question: Can paradigms like retrieval-augmented generation (RAG) or federated learning offer better adaptability to coding drift than the current static fine-tuning approach?
- Basis in paper: [explicit] The authors suggest future work should consider these alternative paradigms to handle evolving knowledge systems without requiring resource-intensive retraining
- Why unresolved: Static models are prone to performance degradation as coding guidelines evolve
- What evidence would resolve it: Comparative analysis of RAG or federated learning models against static approach regarding performance maintenance over longitudinal deployments

## Limitations
- Private data dependency limits reproducibility and external validity testing beyond single external validation performed
- Single external validation site shows performance gap (0.802 vs 0.636 F1) suggesting institution-specific adaptation may be necessary
- Judge model reliability concerns due to custom regex handling for non-standard responses and unvalidated consistency
- Token limit constraints cause truncation for 14% of records potentially removing clinically relevant information

## Confidence
- **High Confidence**: Section-aware fine-tuning mechanism demonstrates consistent performance improvements across multiple experiments; redundancy-aware sampling shows measurable efficiency gains without compromising accuracy
- **Medium Confidence**: Plackett-Luce aggregation for model selection produces plausible rankings that correlate with downstream performance, though theoretical relationship between intrinsic code comprehension and fine-tuning success requires further validation
- **Medium Confidence**: External validation on TMMH data shows reasonable generalization, but performance gap suggests institution-specific adaptation may be necessary

## Next Checks
1. **Judge Model Consistency Test**: Evaluate Atla Selene Mini judge model on subset of candidate model outputs with known ground truth to quantify agreement rates and identify systematic biases in pairwise comparisons
2. **Cross-Institution Generalizability**: Deploy fine-tuned BioMistral model on third, independent institutional dataset with different documentation practices to assess robustness across diverse clinical settings
3. **Token Truncation Impact Analysis**: Systematically evaluate model performance on truncated vs. full-length inputs for 14% of records exceeding 2,048 tokens to quantify impact of priority-based truncation on diagnostic accuracy