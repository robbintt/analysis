---
ver: rpa2
title: Abusive text transformation using LLMs
arxiv_id: '2507.10177'
source_url: https://arxiv.org/abs/2507.10177
tags:
- text
- abusive
- groq
- gemini
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the ability of large language models (LLMs)
  such as GPT-4o, Gemini, DeepSeek, and Groq to transform abusive text (tweets and
  reviews) into non-abusive versions while retaining meaning and sentiment. The research
  uses two datasets containing hate speech, swear words, and personal attacks, and
  applies BERT-based models for sentiment and semantic analysis before and after transformation.
---

# Abusive text transformation using LLMs

## Quick Facts
- arXiv ID: 2507.10177
- Source URL: https://arxiv.org/abs/2507.10177
- Reference count: 40
- GPT-4o and DeepSeek showed highest transformation success rates while preserving semantic meaning

## Executive Summary
This study evaluates large language models' ability to transform abusive text (tweets and reviews) into non-abusive versions while retaining meaning and sentiment. The research tested GPT-4o, Gemini, DeepSeek, and Groq on two datasets containing hate speech, swear words, and personal attacks. Results show GPT-4o and DeepSeek achieved the highest transformation success rates and maintained semantic similarity with original text, while Groq, though effective at detoxification, often altered meaning by adding positive phrasing.

## Method Summary
The study applied four LLMs to transform abusive tweets and reviews into non-abusive text. Inputs included IIT Guwahati dataset (160k entries), Twitter dataset (4,265 tweets), and SenWave dataset (10k entries) for sentiment fine-tuning. Models used a simple prompt to remove abuse while preserving meaning. Evaluation used HateBERT for abuse detection, SenWave-BERT for sentiment classification, and MPNet-base-v2 for semantic similarity via cosine similarity. Gemini safety was configured to BLOCK_NONE to maximize transformation yield.

## Key Results
- GPT-4o and DeepSeek achieved highest transformation success rates and maintained semantic similarity (~0.58-0.60)
- Groq showed lowest semantic similarity (0.514) due to extensive positive rephrasing
- Sentiment analysis showed significant reduction in negative sentiments post-transformation, especially for Groq

## Why This Works (Mechanism)

### Mechanism 1
Models that preserve original content tokens (rather than adding new phrasing) maintain higher semantic similarity during detoxification. GPT-4o and DeepSeek retained key n-grams from original tweets, performing localized lexical substitution rather than global rewriting. This token-preservation approach kept cosine similarity scores higher (~0.58-0.60) compared to models that extensively rephrase.

### Mechanism 2
Configurable API safety thresholds directly modulate transformation yield rates independent of model capability. Gemini's BLOCK_NONE setting bypassed content-moderation filters that would otherwise refuse processing abusive input, enabling 53.4% mean transformation success versus Groq's 18.4%—despite both models having comparable detection accuracy.

### Mechanism 3
Detoxification strategies that add positive-collaborative phrasing reduce negative sentiment but increase semantic drift. Groq's transformation heuristic favored sentence restructuring with constructive language ("let work together," "I'm concerned"), shifting sentiment from "annoyed" to "optimistic" categories. This positive-reframing reduced hate-keyword presence but lowered semantic similarity scores.

## Foundational Learning

- **Concept: BERT-based bidirectional embeddings for semantic comparison**
  - **Why needed here:** Understanding why MPNet-base-v2 was chosen for semantic similarity requires grasping how transformer attention captures contextual meaning beyond surface token overlap.
  - **Quick check question:** Given two sentences—"The service was terrible" and "I'm disappointed with the service"—would a unidirectional LSTM likely assign higher or lower similarity than BERT? Why?

- **Concept: Multi-label vs single-label sentiment classification**
  - **Why needed here:** SenWave-BERT assigns multiple emotion tags per tweet (e.g., both "optimistic" and "thankful"), which differs from binary positive/negative approaches and affects how transformation impact is measured.
  - **Quick check question:** If a transformed tweet receives both "optimistic" and "annoyed" labels, what does this suggest about the transformation's success in the study's framework?

- **Concept: N-gram frequency as stylistic fingerprinting**
  - **Why needed here:** The study uses bigram/trigram distributions to characterize how each model transforms text—this requires understanding statistical significance of phrase patterns across corpora.
  - **Quick check question:** Why might "I'm concerned" appearing as Groq's top bigram indicate a systematic transformation bias rather than random variation?

## Architecture Onboarding

- **Component map:** Raw abusive text → Preprocessing (lowercase, contraction expansion, URL removal, tokenization, lemmatization) → LLM API layer (Gemini/GPT-4o/DeepSeek/Groq with respective safety configs) → Transformation output → Evaluation layer: HateBERT/keyword search, SenWave-BERT, MPNet-base-v2 + cosine similarity, N-gram frequency analysis

- **Critical path:** LLM API configuration → safety threshold settings → transformation prompt engineering. This sequence determines whether the pipeline produces outputs at all; evaluation metrics are downstream and cannot compensate for upstream filtering.

- **Design tradeoffs:**
  | Objective | Configuration A | Configuration B |
  |-----------|----------------|-----------------|
  | Maximize transformation yield | Disable safety filters (Gemini BLOCK_NONE) | Risk policy violations in production |
  | Preserve semantic meaning | Use GPT-4o/DeepSeek approach | Accept residual negative sentiment |
  | Maximize positivity | Use Groq-style positive reframing | Accept semantic drift and longer outputs |

- **Failure signatures:**
  - Refusal cascade: API returns policy-violation messages instead of transformed text
  - Semantic inversion: Transformed text expresses opposite sentiment from original
  - Content hallucination: Model adds details not present in source
  - Evaluation mismatch: HateBERT classifies all outputs as hateful

- **First 3 experiments:**
  1. Safety threshold sweep: Run identical abusive inputs through Gemini with all 5 HarmBlockThreshold settings; plot transformation success rate vs. semantic similarity
  2. Prompt ablation study: Test minimalist prompt vs. study's detailed prompt; measure whether verbose instructions cause Groq-style over-expansion
  3. Cross-domain validation: Apply winning configuration to "Abusive music lyrics" corpus (arXiv:2601.15348) to test generalization

## Open Questions the Paper Calls Out

- **Open Question 1:** Does incorporating multi-modal inputs (text plus attached images) improve LLM performance in transforming abusive content while preserving user intent?
- **Open Question 2:** How do LLMs perform in transforming abusive text in non-English languages and contexts rich with sarcasm, slang, and emojis?
- **Open Question 3:** Can LLMs effectively transform abusive content in creative domains like movie dialogues or song lyrics without removing necessary artistic elements?

## Limitations
- Evaluation framework reliability concerns due to HateBERT classifying all transformed text as hateful
- Simple prompt may not adequately constrain output length or style preservation
- English-only focus limits cross-cultural applicability and generalizability

## Confidence
- **High confidence:** Transformation success rate comparisons between models (GPT-4o > DeepSeek > Gemini > Groq)
- **Medium confidence:** Semantic similarity findings (GPT-4o/DeepSeek preserving meaning better than Groq)
- **Low confidence:** Sentiment transformation analysis depending on SenWave-BERT's multi-label approach

## Next Checks
1. Systematically test Gemini's five HarmBlockThreshold settings on identical abusive inputs to identify optimal operating point
2. Apply most successful configuration to "Abusive music lyrics" corpus (arXiv:2601.15348) to test generalization
3. Conduct prompt engineering ablation study comparing minimalist vs. detailed prompts to isolate model vs. instruction effects