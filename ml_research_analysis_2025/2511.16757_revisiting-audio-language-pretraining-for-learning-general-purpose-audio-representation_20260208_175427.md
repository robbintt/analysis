---
ver: rpa2
title: Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation
arxiv_id: '2511.16757'
source_url: https://arxiv.org/abs/2511.16757
tags:
- audio
- learning
- speech
- pretraining
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores audio-language pretraining as a pathway to
  general-purpose audio representation learning, addressing limitations in current
  methods that excel at specific tasks but lack cross-domain versatility. The authors
  introduce CaptionStew, a 10.7M audio-text dataset aggregating diverse open-source
  corpora across speech, music, and environmental domains.
---

# Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation

## Quick Facts
- arXiv ID: 2511.16757
- Source URL: https://arxiv.org/abs/2511.16757
- Reference count: 31
- This paper introduces CaptionStew dataset and compares contrastive learning vs captioning objectives for general-purpose audio representation learning

## Executive Summary
This paper explores audio-language pretraining as a pathway to general-purpose audio representation learning, addressing limitations in current methods that excel at specific tasks but lack cross-domain versatility. The authors introduce CaptionStew, a 10.7M audio-text dataset aggregating diverse open-source corpora across speech, music, and environmental domains. They conduct the first comprehensive evaluation comparing contrastive learning and captioning objectives for audio representation learning. Their results show contrastive learning achieves superior data efficiency at smaller scales, while captioning demonstrates better scalability for language-involved tasks. Both objectives produce competitive, transferable representations across diverse audio domains, with audio-language pretraining outperforming supervised baselines on speaker identification, music understanding, and audio-text retrieval while maintaining strong performance on audio event classification.

## Method Summary
The authors present a comprehensive study of audio-language pretraining using contrastive learning and captioning objectives. They introduce CaptionStew, a 10.7M audio-text dataset created by aggregating 11 diverse open-source corpora including AudioCaps, Clotho, SpeechCommands, AudioSet, and others. The study evaluates Zipformer-M encoders pretrained with either contrastive learning (using InfoNCE loss with audio-text pairs) or captioning (using cross-entropy loss with autoregressive/parallel decoding). Representations are assessed through linear probing, audio-language alignment (LiT-style), and open-form question answering. The authors systematically compare pretraining objectives, data scales, and initialization strategies while evaluating on a diverse set of 13 downstream tasks across speech, music, and environmental sound domains.

## Key Results
- Contrastive learning achieves superior data efficiency at smaller scales, outperforming captioning on linear probing tasks
- Captioning demonstrates better scalability for language-involved tasks like audio-text retrieval and open-form question answering
- Both objectives produce competitive, transferable representations across diverse audio domains
- Audio-language pretraining outperforms supervised baselines on speaker identification, music understanding, and audio-text retrieval tasks
- Supervised initialization provides diminishing returns at scale, challenging common practices in the field

## Why This Works (Mechanism)

### Mechanism 1: Contrastive Alignment as Linear Separability Optimization
- Claim: Contrastive learning between audio and text creates clip-level embeddings that are linearly separable for downstream tasks.
- Mechanism: The InfoNCE loss explicitly optimizes for paired audio-text samples to be close in a shared embedding space while pushing unpaired samples apart. This creates a semantic organization where representations are optimized for similarity comparisons, which translates directly to linear probing performance on classification tasks. The pooling operation (mean or attention-based) aggregates frame-level features into a single clip-level vector that the contrastive objective shapes.
- Core assumption: The semantic alignment forced by contrastive learning on 10.7M pairs generalizes to unseen tasks like speaker identification and environmental sound classification.
- Evidence anchors:
  - [abstract] "contrastive learning achieves superior data efficiency at smaller scales"
  - [section 4.4] "contrastive learning consistently outperforms captioning, particularly excelling at audio event classification and speaker identification... contrastive learning explicitly optimizes for linearly separable clip-level representations"
  - [corpus] Weak direct evidence; related papers focus on LLM reasoning (Mellow, Audio Flamingo 2), not contrastive mechanisms for general-purpose representations.
- Break condition: Performance may degrade on tasks requiring fine-grained temporal reasoning (e.g., sound event detection with precise boundaries), as contrastive learning prioritizes clip-level semantics over frame-level dynamics.

### Mechanism 2: Captioning as Token-Level Dense Supervision
- Claim: Captioning objectives provide denser supervision by requiring the model to generate token-level predictions conditioned on audio representations.
- Mechanism: The captioning decoder attends to frame-level audio representations through cross-attention, forcing the encoder to preserve detailed acoustic information necessary for generating specific words. The mixed autoregressive and parallel decoding modes prevent over-reliance on linguistic context and strengthen the dependency on audio features. This approach is particularly effective for language-involved tasks like retrieval and QA.
- Core assumption: Assumption: The token-level supervision from captions (which describe "what is present" more often than precise acoustic attributes) transfers to tasks requiring detailed audio understanding.
- Evidence anchors:
  - [abstract] "captioning demonstrates better scalability for language-involved tasks"
  - [section 3] "captioning presents a promising alternative for audio-language pretraining, offering denser token-level supervision compared to contrastive learning"
  - [section 4.4] "captioning showing slight advantages in open-form question answering across multiple domains"
  - [corpus] LiSTEN and Audio Flamingo 2 similarly leverage token-level audio representations for LLM integration, supporting the viability of dense supervision.
- Break condition: Performance may lag on discriminative tasks requiring simple linear classification if the downstream pooling strategy (e.g., mean pooling) is not well-suited to the frame-level representations learned by captioning.

### Mechanism 3: Caption Diversity Enables Cross-Domain Transfer
- Claim: Aggregating captions from diverse sources (speech, music, environmental sounds) with varying descriptive styles broadens the semantic coverage of the learned representations.
- Mechanism: Different caption datasets emphasize different audio attributesâ€”AudioSetCaps focuses on event categories, JamendoMaxCaps on musical structure, ParaSpeechCaps on speaking style. By consolidating these into CaptionStew (10.7M pairs), the model is exposed to a wider semantic space, reducing domain-specific biases and enabling transfer to tasks like speaker ID and music tagging.
- Core assumption: Assumption: The diversity of caption styles directly maps to improved representation generality, even if lexical diversity metrics (Distinct-n) are not significantly improved by simple aggregation.
- Evidence anchors:
  - [abstract] "introduce CaptionStew, a 10.7M audio-text dataset aggregating diverse open-source corpora across speech, music, and environmental domains"
  - [section 3] "This aggregation yields captions that describe complementary audio aspects with varying granularity, from coarse event categories to fine-grained acoustic attributes"
  - [section 4.6] "t-SNE visualization... reveals distinct clustering patterns by source that demonstrate complementary linguistic perspectives"
  - [corpus] SeaLLMs-Audio and MiMo-Audio similarly emphasize diverse audio corpora for broad task coverage, supporting the diversity-transfer hypothesis.
- Break condition: If caption sources share systematic blind spots (e.g., lacking descriptions of reverberation or harmonic structure), the model will not learn those attributes regardless of aggregation scale.

## Foundational Learning

- Concept: InfoNCE Loss (Contrastive Learning)
  - Why needed here: The contrastive objective is the primary mechanism for aligning audio and text in this paper. Understanding how it maximizes similarity for paired samples and minimizes it for unpaired samples is essential for interpreting the linear probing results.
  - Quick check question: Can you explain why a lower temperature parameter in InfoNCE would make the loss more sensitive to hard negatives?

- Concept: Cross-Attention in Encoder-Decoder Architectures
  - Why needed here: The captioning objective relies on cross-attention to inject audio representations into the text decoder. Understanding how the decoder attends to different frame-level features helps explain why captioning preserves richer temporal information.
  - Quick check question: In a transformer decoder, what happens to the audio representation if the cross-attention weights are uniformly distributed across all frames?

- Concept: Transfer Learning Evaluation Protocols (Linear Probing vs. Fine-Tuning)
  - Why needed here: The paper evaluates representations using frozen encoders with linear classifiers (probing) and with LLM adaptors (LiT-style). Distinguishing these protocols is critical for understanding the "competitive, transferable" claims.
  - Quick check question: Why might a representation perform well on linear probing but poorly when used as a frozen encoder for a generative task like audio captioning?

## Architecture Onboarding

- Component map: Audio Encoder (Zipformer-M) -> Audio Encoder (BERT-base) OR Text Decoder (BART-base) -> Pooling (Mean/Attention) -> Linear Classifier OR LLM Adaptor

- Critical path:
  1. Audio preprocessing: 16 kHz, 80-dim log-Mel filterbanks (25 ms window, 10 ms hop)
  2. Encoding: Zipformer processes multi-resolution features, outputs frame-level embeddings
  3. Alignment: Contrastive (InfoNCE) or Captioning (cross-entropy with autoregressive/parallel modes)
  4. Evaluation: Linear probing (frozen encoder + linear classifier), Audio-language alignment (LiT-style), Open-form QA (adaptor + LLM)

- Design tradeoffs:
  - Contrastive vs. Captioning: Contrastive is more data-efficient for discriminative tasks; captioning scales better for language-involved tasks but requires more data to match contrastive on linear probing
  - Initialization: AudioSet pretraining helps on event-related tasks but provides diminishing returns at large data scales and can bias representations away from non-event attributes
  - Pooling Strategy: Mean pooling favors contrastive models; attention pooling narrows the gap for captioning models by better exploiting frame-level information

- Failure signatures:
  - Sound event detection performance degrades with more caption data (potential conflict between natural language supervision and temporal localization requirements)
  - Emotion recognition and instrument classification show weak scaling gains (caption diversity for these attributes is limited in the corpus)
  - Captioning models underperform contrastive on linear probing with mean pooling (frame-level representations are not optimized for simple aggregation)

- First 3 experiments:
  1. **Reproduce Contrastive vs. Captioning Gap on Linear Probing**: Train a Zipformer-M with InfoNCE vs. captioning loss on a small CaptionStew subset (400K pairs). Evaluate on FSD-50K and VoxCeleb2 with mean pooling. Confirm the paper's finding that contrastive outperforms captioning on discriminative tasks.
  2. **Test Pooling Sensitivity**: Repeat experiment 1 but use multi-head attention pooling instead of mean pooling. Quantify the gap reduction between contrastive and captioning, validating the paper's claim about the importance of downstream module selection.
  3. **Verify Diminishing Returns of AudioSet Init**: Train contrastive and captioning models from scratch vs. AudioSet-initialized on increasing CaptionStew scales (400K, 1M, 4M). Plot performance on AudioSet-strong (sound event detection) and a non-event task (speaker ID). Confirm that initialization benefits vanish or reverse at larger scales for non-event tasks.

## Open Questions the Paper Calls Out

None

## Limitations

- The scalability claims for captioning objectives are based on comparisons at different data scales without establishing the precise crossover point where captioning overtakes contrastive performance
- The paper lacks evaluation of temporal reasoning capabilities, focusing on clip-level representations rather than tasks requiring precise temporal boundaries
- Supervised initialization experiments are limited to three tasks, making it unclear whether the diminishing returns pattern holds across the full task suite

## Confidence

**High Confidence:**
- Contrastive learning outperforms captioning on linear probing tasks with mean pooling at small-to-medium scales
- CaptionStew dataset construction methodology and quality metrics
- Audio-language pretraining outperforms supervised baselines on audio-text retrieval tasks
- Multi-head attention pooling improves captioning model performance on linear probing

**Medium Confidence:**
- Contrastive learning achieves superior data efficiency compared to captioning
- Captioning demonstrates better scalability for language-involved tasks (requires verification at larger scales)
- AudioSet pretraining provides diminishing returns at scale (based on limited task coverage)

**Low Confidence:**
- Generalizability of findings to temporal reasoning tasks
- Optimal initialization strategy selection for different task types
- Precise scaling laws for captioning vs. contrastive performance crossover

## Next Checks

1. **Establish Scaling Law Crossover Point**: Train contrastive and captioning models on systematically increasing subsets of CaptionStew (400K, 1M, 2M, 4M, 8M pairs) and evaluate on FSD50K with both mean and attention pooling. Identify the precise scale where captioning overtakes contrastive on linear probing, if it occurs at all.

2. **Temporal Reasoning Validation**: Implement a sound event detection evaluation using the frozen audio encoder with a temporal decoder (e.g., CRF or attention-based localization head). Compare performance against the clip-level classification results to quantify any degradation in temporal reasoning capability.

3. **Initialization Strategy Ablation**: Extend the initialization analysis to include models initialized from different pretraining stages of AudioSet (e.g., stage-1 only, stage-2 only, full AudioSet) and evaluate on the full task suite including music understanding, speaker identification, and emotion recognition. Identify whether certain initialization combinations provide task-specific benefits that are masked in aggregate analysis.