---
ver: rpa2
title: 'BiblioPage: A Dataset of Scanned Title Pages for Bibliographic Metadata Extraction'
arxiv_id: '2503.19658'
source_url: https://arxiv.org/abs/2503.19658
tags:
- dataset
- bibliographic
- title
- extraction
- pages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: BiblioPage introduces a dataset of 2,118 scanned title pages annotated
  with 16 bibliographic attributes and positional bounding boxes, spanning historical
  and modern works from 14 Czech libraries. The dataset supports automated metadata
  extraction by addressing the lack of diverse real-world data for this task.
---

# BiblioPage: A Dataset of Scanned Title Pages for Bibliographic Metadata Extraction

## Quick Facts
- arXiv ID: 2503.19658
- Source URL: https://arxiv.org/abs/2503.19658
- Reference count: 40
- Primary result: BiblioPage dataset with 2,118 scanned title pages annotated for 16 bibliographic attributes

## Executive Summary
BiblioPage introduces a dataset of 2,118 scanned title pages annotated with 16 bibliographic attributes and positional bounding boxes, spanning historical and modern works from 14 Czech libraries. The dataset supports automated metadata extraction by addressing the lack of diverse real-world data for this task. Two baseline approaches were evaluated: YOLOv11 combined with OCR, achieving a maximum mean Average Precision of 52 and F1 score of 59, and Vision-Language Large Models, with GPT-4o reaching an F1 score of 67. The results demonstrate that LLMs outperform object detection due to their semantic understanding of text, especially for contributor-related attributes.

## Method Summary
The study introduces BiblioPage, a dataset of 2,118 scanned title pages from 14 Czech libraries with annotations for 16 bibliographic attributes including title, author, publisher, and publication date. The dataset provides JSON annotations with positional bounding boxes for each attribute. Two baseline approaches were evaluated: YOLOv11-based object detection combined with transformer-based OCR (PERO OCR on IMPACT+DTA), and zero-shot Vision-Language Large Models (GPT-4o, Llama 3.2-Vision 11B/90B). Evaluation used F1, Precision, Recall, and mAP metrics with matching based on 10% CER threshold and normalization procedures for historical text.

## Key Results
- YOLOv11-based approach achieved maximum mAP of 52 and F1 score of 59
- GPT-4o VLLM achieved F1 score of 67, outperforming YOLO-based methods
- DETR-based object detection approaches severely underperformed (mF1 1-24), likely due to dataset size constraints

## Why This Works (Mechanism)
The dataset's effectiveness stems from its diversity, covering 14 libraries with historical and modern works, and its detailed annotation scheme with bounding boxes for 16 bibliographic attributes. The two baseline approaches leverage different strengths: YOLOv11 provides precise localization through object detection while OCR extracts textual content, and VLLMs use semantic understanding to interpret complex layouts and contextual relationships between attributes.

## Foundational Learning
- **Bounding box annotation**: Essential for localizing bibliographic elements on scanned pages; quick check: verify JSON contains coordinates for all 16 attributes
- **CER (Character Error Rate)**: Metric for matching OCR-extracted text with ground truth; quick check: calculate CER threshold matches using 10% tolerance
- **OCR integration**: Combining object detection with text extraction improves attribute recognition; quick check: confirm OCR output aligns with YOLO bounding boxes
- **Normalization procedures**: Critical for handling historical text variations; quick check: verify rare character removal and modernization steps
- **Zero-shot prompting**: VLLMs can extract metadata without fine-tuning; quick check: test GPT-4o with sample JSON output prompts
- **mAP (mean Average Precision)**: Standard metric for object detection performance; quick check: calculate mAP across all 16 attributes

## Architecture Onboarding
**Component map**: Dataset -> YOLOv11 detection -> PERO OCR -> Alignment -> Evaluation / Dataset -> VLLM prompting -> JSON output -> Evaluation

**Critical path**: For YOLO: detection bounding boxes → OCR extraction → attribute matching → F1 calculation. For VLLM: image input → prompt generation → JSON output → attribute matching → F1 calculation.

**Design tradeoffs**: YOLO provides precise localization but struggles with semantic understanding of complex layouts, while VLLMs offer better semantic comprehension but less precise spatial localization. OCR integration bridges this gap but adds complexity.

**Failure signatures**: DETR underperformance (mF1 1-24) indicates dataset size limitations; Title/Subtitle confusion (F1 ~60-70) reveals multi-line layout challenges; significant performance gap between with-OCR and without-OCR results for Llama models indicates OCR dependency.

**Three first experiments**:
1. Train YOLOv11m on 200 dev pages to estimate epoch count and validate convergence
2. Test GPT-4o zero-shot prompting with sample images to verify JSON output format
3. Evaluate OCR integration by comparing bounding box overlap methods for text alignment

## Open Questions the Paper Calls Out
- How can multi-line titles and subtitles with varying font sizes and inconsistent typesets be more accurately extracted?
- Can bibliographic metadata extraction trained on title pages generalize to multi-page document contexts with broader contextual information?
- What dataset scale is required for DETR-based object detection models to achieve competitive performance on bibliographic attribute extraction?
- Would few-shot prompting of VLLMs improve performance beyond the zero-shot baseline on BiblioPage?

## Limitations
- Unspecified YOLO training hyperparameters (learning rate, batch size, optimizer, weight decay) limit exact reproduction
- VLLM prompts and inference parameters (temperature, top_p) were not documented, affecting reproducibility
- PERO OCR integration method lacks detailed description of the alignment procedure between detections and extracted text

## Confidence
- **High confidence** in dataset's novelty and utility for bibliographic metadata extraction tasks
- **Medium confidence** in baseline YOLO results due to unspecified hyperparameters but standard architecture
- **Medium confidence** in VLLM results given sensitivity to unspecified prompts and inference parameters
- **Low confidence** in comparative analysis between approaches without knowing exact training/inference settings

## Next Checks
1. Train YOLOv11m with multiple hyperparameter configurations (learning rate ×3, batch size ×2, optimizer variants) to establish performance bounds and verify the reported mF1 ~59 is reproducible
2. Conduct ablation studies on VLLM prompts with controlled variations in instruction phrasing, few-shot examples, and output format to quantify sensitivity and establish prompt optimization importance
3. Perform systematic evaluation of OCR integration methods, comparing simple bounding box overlap against more sophisticated text alignment techniques to determine if the reported improvement is method-dependent or generally achievable