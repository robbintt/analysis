---
ver: rpa2
title: Multi-dataset and Transfer Learning Using Gene Expression Knowledge Graphs
arxiv_id: '2503.20400'
source_url: https://arxiv.org/abs/2503.20400
tags:
- gene
- expression
- datasets
- data
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel methodology to address the challenges
  of using gene expression data for patient diagnosis, particularly the limited number
  of patients and the difficulty in integrating data from different datasets. The
  approach involves building a knowledge graph (KG) that integrates gene expression
  data with domain-specific knowledge from publicly available datasets on gene and
  protein functions.
---

# Multi-dataset and Transfer Learning Using Gene Expression Knowledge Graphs

## Quick Facts
- arXiv ID: 2503.20400
- Source URL: https://arxiv.org/abs/2503.20400
- Reference count: 35
- Primary result: Combining gene expression datasets and domain-specific knowledge improves patient diagnosis performance in single-dataset, multi-dataset, and transfer learning settings.

## Executive Summary
This paper introduces a novel methodology to address the challenges of using gene expression data for patient diagnosis, particularly the limited number of patients and the difficulty in integrating data from different datasets. The approach involves building a knowledge graph (KG) that integrates gene expression data with domain-specific knowledge from publicly available datasets on gene and protein functions. The KG is then used to generate vector representations of nodes using knowledge graph embedding techniques, which serve as inputs for a graph neural network (GNN) or a multi-layer perceptron (MLP) to predict the likelihood of a patient having a specific disease. The methodology is evaluated in three settings: single-dataset learning, multi-dataset learning, and transfer learning, using nine GEO datasets related to diabetes type 2, coronary artery disease, and breast cancer. The results demonstrate that combining gene expression datasets and domain-specific knowledge improves patient diagnosis performance in all three settings, with significant improvements in F1-score for several datasets.

## Method Summary
The methodology involves preprocessing GEO datasets to map probes to genes and normalize expression values, then constructing a knowledge graph linking patients to genes (when expression exceeds z-score > 1) and integrating with Gene Ontology (GO) classes and annotations. RDF2vec is used to generate 500-dimensional embeddings for all nodes, with pre-trained domain embeddings updated for patient nodes to reduce computational overhead. These embeddings serve as inputs for either a Graph Convolutional Network (GCN) or Multi-Layer Perceptron (MLP) classifier. The approach is evaluated across nine GEO datasets in three settings: single-dataset learning (training and testing on same dataset), multi-dataset learning (training on multiple datasets, testing on all), and transfer learning (training on one or more datasets, testing on a new dataset).

## Key Results
- KG-based methods significantly outperform baseline classifiers in multi-dataset and transfer learning settings, with improvements in F1-score across multiple datasets.
- MLP generally outperforms GCN in multi-dataset and transfer learning, likely due to reduced overfitting on small datasets.
- Integrating gene expression datasets with domain-specific knowledge through knowledge graphs enables effective transfer learning between datasets with minimal gene overlap.

## Why This Works (Mechanism)

### Mechanism 1: Cross-Dataset Integration via Functional Ontology Linking
Datasets with non-overlapping genes can be integrated for joint training when genes are linked through shared functional annotations. Patient nodes connect to gene nodes when normalized expression exceeds z-score > 1. Genes then link to Gene Ontology (GO) classes (biological processes, molecular functions, cellular components). Even when datasets share zero genes, patients become comparable through shared GO ancestors, creating indirect paths in the embedding space. The core assumption is that functional similarity (shared GO terms) is a meaningful proxy for regulatory or pathway-level similarity across experimental platforms.

### Mechanism 2: Pre-trained Static KG Embeddings Reduce Computational Overhead
Reusing embeddings for the static ontology portion and only updating walks for new patient nodes maintains performance while reducing training time. Domain KG (GO + annotations) is static. RDF2vec is trained once on this subgraph. When adding a new dataset, only new patient-gene walks are generated and the Word2vec model is updated, not retrained from scratch. The core assumption is that the ontology structure is stable and sufficiently expressive; new patient nodes do not require global embedding re-alignment.

### Mechanism 3: Embedding-Induced Semantic Space Enables Transfer Learning
Patients represented via KG embeddings occupy a shared semantic space, enabling classifiers trained on one dataset to generalize to unseen datasets. Raw expression values cause dataset-specific clustering (technical batch effects dominate). KG embeddings smooth these differences by projecting patients through shared gene-function topology. A classifier trained on this representation learns disease signals that transfer. The core assumption is that batch effects are mitigated by z-score normalization within patients AND by the projection through shared GO structure; residual batch signals do not dominate learned embeddings.

## Foundational Learning

- **Concept: Knowledge Graph Embeddings (specifically RDF2vec)**
  - Why needed here: To convert symbolic KG nodes (patients, genes, GO terms) into dense vectors that capture structural context for downstream classifiers.
  - Quick check question: Can you explain how random walks over a graph and Word2vec produce node embeddings? What does the window size control?

- **Concept: Gene Ontology (GO) Structure and Annotations**
  - Why needed here: GO provides the functional backbone linking genes across datasets; understanding its three domains (biological process, molecular function, cellular component) clarifies what relationships are being embedded.
  - Quick check question: What is the difference between a GO term (class) and a GO annotation linking a gene to that term?

- **Concept: Graph Convolutional Networks (GCNs) and Message Passing**
  - Why needed here: One variant uses GCNs over weighted patient-gene-function graphs; understanding neighborhood aggregation clarifies how patient diagnoses incorporate multi-hop gene-function context.
  - Quick check question: In a GCN layer, how does a node's representation depend on its neighbors, and what role do edge weights play?

## Architecture Onboarding

- **Component map:**
  1. **Preprocessing:** GEO dataset → probe filtering → gene-level aggregation → per-patient z-score normalization.
  2. **KG Construction:** Patient nodes + Gene nodes + GO class nodes + GO annotation edges + Patient-Gene edges (z > 1).
  3. **Embedding Module:** RDF2vec on static ontology → update with patient nodes → 500-dim node vectors.
  4. **Classifier Heads:**
     - MLP: patient embedding → hidden layers → binary output.
     - GCN: weighted graph + node embeddings → convolutional layers → node classification.

- **Critical path:**
  Preprocessing → KG assembly → RDF2vec training/update → classifier training → evaluation (single/multi-dataset/transfer). The embedding quality dominates downstream performance; errors in KG linkage propagate silently.

- **Design tradeoffs:**
  - **MLP vs. GCN:** MLP is simpler, less prone to overfitting on small graphs; GCN can capture multi-hop context but risks over-smoothing and overfitting. Paper shows MLP often better in multi-dataset and transfer settings.
  - **Edge weighting (z-scores):** Provides expression magnitude signal, but ablation shows marginal impact beyond binary edge existence.
  - **Threshold z > 1 for patient-gene edges:** Reduces noise but may drop informative moderate-expression signals; threshold not systematically tuned.

- **Failure signatures:**
  - Classifier predicts all one class (common in baseline transfer setting).
  - No performance gain over baseline despite KG integration (seen in smallest datasets; suggests insufficient signal or sparse GO coverage).
  - Embedding training diverges or produces near-identical vectors (check walk depth, negative sampling, or ontology connectivity).

- **First 3 experiments:**
  1. **Reproduce single-dataset baseline vs. KG+MLP** on one GEO dataset (e.g., GSE184050) to validate preprocessing pipeline and embedding generation.
  2. **Ablate edge weighting** in GCN (weighted vs. unweighted) on a mid-sized dataset to confirm paper's finding that weighting has minimal impact.
  3. **Test transfer learning** between two datasets with low gene overlap to verify that KG embeddings enable non-degenerate predictions where raw-expression baselines fail.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does integrating domain-specific knowledge sources beyond Gene Ontology (e.g., protein-protein interactions or biochemical pathways) further improve diagnostic performance?
- Basis in paper: [explicit] The authors state in the conclusion that "Future work could explore additional domain-specific knowledge for gene expression datasets."
- Why unresolved: The current study limited the domain-specific knowledge strictly to the Gene Ontology (GO) and GO annotation data to construct the knowledge graph.
- What evidence would resolve it: An evaluation of the methodology using expanded knowledge graphs that include pathway or interaction data, showing a statistically significant increase in F1-scores compared to the GO-only baseline.

### Open Question 2
- Question: How can the graph construction strategy be modified to utilize edge weights more effectively so they provide predictive signal beyond simple edge existence?
- Basis in paper: [inferred] The ablation study (Page 14) reveals that using weighted edges (normalized expression values) did not significantly improve performance over unweighted edges, suggesting the current weighting strategy fails to convey additional information.
- Why unresolved: The authors note that the variation in weights (z-scores > 1) appears too small to convey additional information beyond the binary presence of a connection, but they do not propose a solution to capture this nuance.
- What evidence would resolve it: Experiments using alternative edge weighting schemes (e.g., different normalization or binning strategies) that result in a measurable performance gain over the unweighted graph baseline.

### Open Question 3
- Question: Why does the Multi-Layer Perceptron (MLP) outperform the Graph Convolutional Network (GCN) in multi-dataset and transfer learning settings?
- Basis in paper: [inferred] The results (Page 13) show MLP outperforms GCN in multi-dataset and transfer learning settings. The authors hypothesize this is due to GCN overfitting or larger relevant subgraphs, but do not verify this experimentally.
- Why unresolved: It is counter-intuitive that a simpler model (MLP) using embeddings outperforms a structure-aware model (GCN) when data is aggregated; the exact mechanism causing GCN to underperform in these specific settings remains unidentified.
- What evidence would resolve it: A study analyzing GCN performance with varying dropout rates or subgraph sizes in multi-dataset settings to confirm if overfitting or neighborhood dilation is the cause.

## Limitations

- **Embedding dimension discrepancy**: The paper reports using 100-dimensional embeddings in the results section but Appendix Table 4 specifies 500 dimensions. This inconsistency affects reproducibility and interpretation of performance metrics.
- **Small dataset instability**: Single-dataset learning on GSE42148 (n=24) shows high variance and sometimes baseline collapse. Performance on small datasets may be dominated by statistical noise rather than KG integration benefits.
- **Transfer learning scope**: Success in transfer learning is demonstrated but limited to datasets covering similar disease categories (diabetes, coronary artery disease, breast cancer). Generalization to unrelated diseases or phenotypes is unknown.

## Confidence

- **High confidence**: The core mechanism that KG embeddings can integrate datasets with minimal gene overlap through shared GO annotations is well-supported by the qualitative analysis in Figure 4 and the quantitative F1-score improvements in multi-dataset settings.
- **Medium confidence**: The computational efficiency claim (9348s vs 1183s for embedding updates) is specific and plausible given the caching strategy, but the exact update implementation is unclear and may not be generalizable.
- **Low confidence**: The claim that MLP outperforms GCN in multi-dataset and transfer settings is based on reported results but lacks ablation studies on why GCN underperforms (overfitting vs. underfitting vs. representation quality).

## Next Checks

1. **Reproduce the embedding dimension discrepancy**: Train both 100-dim and 500-dim versions of the KG+MLP pipeline on GSE184050 and compare performance. This resolves the inconsistency and validates which dimension the reported results actually used.
2. **Test transfer learning outside the three disease categories**: Apply the pre-trained model from the diabetes/coronary/breast cancer domain to a completely different disease dataset (e.g., Alzheimer's or autoimmune disease) to assess the generality of semantic transfer.
3. **Ablate the GO annotation density**: Remove or randomize GO annotations for a subset of genes and measure degradation in multi-dataset F1-score. This would quantify how sensitive the method is to the completeness and relevance of the functional knowledge base.