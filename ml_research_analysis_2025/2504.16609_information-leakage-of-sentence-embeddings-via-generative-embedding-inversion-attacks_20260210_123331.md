---
ver: rpa2
title: Information Leakage of Sentence Embeddings via Generative Embedding Inversion
  Attacks
arxiv_id: '2504.16609'
source_url: https://arxiv.org/abs/2504.16609
tags:
- sentence
- geia
- embedding
- original
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates the security vulnerabilities of sentence
  embedding models by reproducing and extending GEIA (Generative Embedding Inversion
  Attack), a framework that recovers original sentences from their embeddings. The
  authors confirm that GEIA outperforms baseline methods in classification metrics,
  successfully recovers sensitive information, and remains effective across various
  embedding models.
---

# Information Leakage of Sentence Embeddings via Generative Embedding Inversion Attacks

## Quick Facts
- arXiv ID: 2504.16609
- Source URL: https://arxiv.org/abs/2504.16609
- Authors: Antonios Tragoudaras; Theofanis Aslanidis; Emmanouil Georgios Lionis; Marina Orozco GonzÃ¡lez; Panagiotis Eustratiadis
- Reference count: 40
- Primary result: Sentence embeddings leak sensitive information from training data, enabling exact recovery of sensitive entities through generative inversion attacks

## Executive Summary
This paper investigates the security vulnerabilities of sentence embedding models by reproducing and extending GEIA (Generative Embedding Inversion Attack), a framework that recovers original sentences from their embeddings. The authors confirm that GEIA outperforms baseline methods in classification metrics, successfully recovers sensitive information, and remains effective across various embedding models. They further propose a novel approach to detect training data leakage by comparing log-likelihoods of masked and original sentence variants using LLM reasoners. Experiments show that sentence embeddings leak sensitive information from training data, enabling adversaries to recover exact sensitive entities.

## Method Summary
The study reproduces GEIA by training a GPT-2 decoder to invert sentence embeddings from victim models (SRoBERTa, SBERT, etc.) back into original text sequences. The attack uses a projection layer to align embedding dimensions, then autoregressively generates tokens conditioned on the projected embedding. The authors extend this by creating masked and similar sentence variants using LLM reasoners, then measuring whether the embedding space favors the original masked tokens over alternatives. This likelihood comparison reveals whether embeddings contain "fingerprints" of training data, enabling identification of specific entities seen during victim model training.

## Key Results
- GEIA successfully recovers original sentences from embeddings across multiple victim models with strong ROUGE/BLEU scores
- Training data leakage is demonstrated: embeddings show 30% higher likelihood of recovering original masked tokens versus alternatives
- The attack remains effective even when the attacker is trained on auxiliary data disjoint from the victim's training corpus
- Exact sensitive entities can be recovered, posing serious privacy threats to embedding-based systems

## Why This Works (Mechanism)

### Mechanism 1: Generative Inversion via Projection Alignment
If a decoder-only language model is conditioned on a projected sentence embedding, it can learn to reconstruct the original text sequence. The GEIA architecture maps a dense sentence embedding $f(x)$ to the input space of a generative decoder (GPT-2). A projection layer aligns the embedding dimensions. The model generates tokens autoregressively, treating the embedding as a "memory" prompt, effectively learning an inverse mapping $\Phi$ from the continuous vector space back to discrete tokens. The core assumption is that the sentence embedding contains sufficient recoverable information (semantics and syntax) to seed the generation process.

### Mechanism 2: Training Data Leakage via Likelihood Disparity
If a sentence embedding model was trained on specific text, the embedding of a "masked" version of that text will statistically favor the original masked tokens over generic alternatives. The authors use an LLM to create a "masked" version of a sentence (hiding entities) and a "similar" version (swapping entities). They embed the *masked* sentence and ask the attacker to score the likelihood of the *original* vs. *similar* text. A higher likelihood for the original tokens implies the embedding carries a "fingerprint" of the training data, allowing the attacker to distinguish the true training entity from a fake one.

### Mechanism 3: Isolation of Parametric vs. Embedding Knowledge
By separating the training data of the victim and the attacker, observed leakage can be attributed to the embedding vector rather than the attacker's internal weights. The attacker is trained on an auxiliary dataset (PersonaChat) distinct from the victim's training data (AltLex). To confirm leakage comes from the vector $f(x)$, the experiment calculates likelihoods with and without pre-pending $f(x)$. If the likelihood advantage only appears when $f(x)$ is present, the information is leaking through the embedding, not the attacker's priors.

## Foundational Learning

- **Concept: Sentence Embeddings**
  - **Why needed here:** To understand the attack surface. Unlike sparse vectors (TF-IDF), dense embeddings compress meaning into continuous space, which this paper shows is often reversible.
  - **Quick check question:** Does an embedding usually represent a one-to-one mapping to a specific sentence? (Answer: No, but this paper shows it is often restrictive enough to recover the source).

- **Concept: Autoregressive Generation (Decoder-only LLMs)**
  - **Why needed here:** To understand the attacker. The attack uses GPT-2, which predicts the next token based on previous tokens and the provided context (the stolen embedding).
  - **Quick check question:** In GEIA, what serves as the "context" for the GPT-2 model to start generating the sentence?

- **Concept: Log-Likelihood**
  - **Why needed here:** To understand the leakage metric. The paper relies on the log-likelihood of generating a target string to prove the embedding contains "clues" about that string.
  - **Quick check question:** If the log-likelihood of Token A is higher than Token B given a specific embedding, what does that imply about the embedding's relationship to Token A?

## Architecture Onboarding

- **Component map:** Victim Encoder -> Projection Module -> Attacker Decoder (GPT-2) -> Reconstructed Text
- **Critical path:**
  1. **Training Phase:** Train Attacker (Decoder + Projection) on Aux Data (PersonaChat) to minimize reconstruction loss
  2. **Inversion Attack:** Input Target Text -> Victim -> Attacker -> Reconstructed Text
  3. **Leakage Diagnostic:** Input Masked Text -> Victim -> Attacker -> Measure Likelihood of Original vs. Similar entities

- **Design tradeoffs:**
  - *Projection vs. Full Fine-tuning:* The authors use a projection module rather than fine-tuning the whole GPT-2, likely to preserve the generative priors of GPT-2 while adapting to the victim's vector space
  - *LLM Reasoner choice:* The paper tests GLM-4 and Llama-3.1. Choice depends on availability and cost, but reasoning capability is critical for creating valid "similar" sentences that test the boundary of the leakage

- **Failure signatures:**
  - **Semantic Drift:** The attacker generates a sentence with the same "topic" but wrong entities (e.g., recovering "Paris" instead of "Berlin"). This indicates partial inversion but low precision (lower NERR score)
  - **Training Collapse:** The projection layer fails to converge, resulting in the decoder generating random text regardless of input

- **First 3 experiments:**
  1. **Reproduce Baseline:** Train the GPT-2 attacker on PersonaChat embeddings (SRoBERTa) and verify ROUGE/BLEU scores match Table 5
  2. **Ablate Projection:** Attempt inversion without the alignment module to verify that dimension mismatch (or semantic misalignment) destroys the attack effectiveness
  3. **Run Likelihood Test:** Take a sentence from AltLex (seen during victim pre-training), mask an entity, and compare the log-likelihood of the true entity vs. a hallucinated one using the pipeline in Figure 2

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the specific trade-off between masking sensitive information during the pre-training of sentence embedding models and the models' subsequent downstream performance?
- Basis in paper: The authors state in the Discussion: "As future work, we propose masking sensitive information during training to address the training leakage... Evaluate this potential trade-off would give a significant understanding on sentence embeddings."
- Why unresolved: While the authors identify the leakage and propose masking as a mitigation strategy, they do not implement or validate the impact of such masking on the model's utility.

### Open Question 2
- Question: Can established privacy-preserving techniques, such as differential privacy or adversarial training, effectively prevent the training data leakage identified without rendering the embeddings unusable?
- Basis in paper: The paper highlights a serious security threat and concludes with a call for "stronger privacy protections," but the experiments are limited to standard, non-robust victim models.
- Why unresolved: The study confirms that standard embeddings leak information, but it does not evaluate whether existing defense mechanisms can block this specific inversion pathway while maintaining semantic richness.

### Open Question 3
- Question: Does the vulnerability to training data leakage generalize to highly specialized domains, such as medical or legal corpora, where the semantic context and entity density differ significantly from Wikipedia-based datasets?
- Basis in paper: The experiments rely on AltLex (Wikipedia) and PersonaChat; the authors note the need to understand security risks "in greater scale" but do not test on domain-specific data where privacy is often most critical.
- Why unresolved: The method relies on LLM reasoners to generate plausible alternatives; it is untested whether this approach remains effective or whether leakage persists when the training data contains complex, low-frequency domain-specific terminology.

## Limitations

- **Data Contamination Risk:** The paper isolates leakage to the embedding space by training the attacker on PersonaChat while the victim is trained on AltLex, but doesn't explicitly verify dataset disjointness, which could compromise confidence in attribution.

- **LLM Dependency:** The training data leakage experiments rely heavily on LLM reasoners to generate masked and similar variants, introducing potential bias based on the specific LLM's reasoning capability rather than purely the embedding vulnerability.

- **Real-world Feasibility Gap:** While demonstrating successful inversion in controlled settings, the paper doesn't address practical constraints like query limits, computational resources, or detection mechanisms that might limit real-world attack success.

## Confidence

- **High Confidence:** The core claim that GEIA successfully recovers original sentences from embeddings is well-supported by quantitative metrics (ROUGE/BLEU scores) and qualitative examples in Table 5.
- **Medium Confidence:** The novel extension detecting training data leakage through likelihood disparity is statistically significant (30% improvement) but relies heavily on the quality of LLM-generated masked/similar variants.
- **Low Confidence:** The isolation of parametric vs. embedding knowledge is methodologically sound but could be strengthened with ablation studies to confirm the attacker's parameters aren't contributing to leakage.

## Next Checks

1. **Dataset Disjointness Verification:** Conduct an explicit overlap analysis between PersonaChat and AltLex to quantify any shared entities or contexts. If overlap exists, re-run the leakage experiments with fully disjoint datasets to confirm that observed leakage is indeed from the embedding space.

2. **LLM Reasoner Robustness Test:** Repeat the training data leakage experiments using multiple different LLM reasoners (including open-source alternatives) to verify that the 30% likelihood advantage isn't specific to GLM-4 or Llama-3.1. Also test the sensitivity by varying the masking strategy (different entity types, multiple masks per sentence).

3. **Adversarial Robustness Evaluation:** Implement and test simple defense mechanisms against GEIA, such as adding Gaussian noise to embeddings before inversion, dimensionality reduction, or adversarial training. Measure how these defenses affect reconstruction quality to establish practical security boundaries.