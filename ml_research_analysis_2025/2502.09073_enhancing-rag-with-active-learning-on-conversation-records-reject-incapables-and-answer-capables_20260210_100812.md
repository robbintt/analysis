---
ver: rpa2
title: 'Enhancing RAG with Active Learning on Conversation Records: Reject Incapables
  and Answer Capables'
arxiv_id: '2502.09073'
source_url: https://arxiv.org/abs/2502.09073
tags:
- learning
- active
- samples
- data
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of reducing hallucinations in
  Retrieval-Augmented Generation (RAG) models by developing a novel active learning
  framework (AL4RAG) that efficiently selects high-quality samples for model training.
  The core method introduces a retrieval-augmented similarity (ras) metric that independently
  evaluates the query, reference, and response components of RAG conversations to
  more accurately measure sample diversity compared to traditional approaches.
---

# Enhancing RAG with Active Learning on Conversation Records: Reject Incapables and Answer Capables

## Quick Facts
- arXiv ID: 2502.09073
- Source URL: https://arxiv.org/abs/2502.09073
- Reference count: 24
- Primary result: AL4RAG achieves 21.74% rejection rate for hallucination-prone queries vs 17.39-19.93% for baselines at 12.5% data

## Executive Summary
This paper addresses hallucination reduction in Retrieval-Augmented Generation (RAG) models through an active learning framework that efficiently selects high-quality samples for training. The core innovation is the retrieval-augmented similarity (ras) metric, which independently evaluates query, reference, and response components to more accurately measure sample diversity. By combining this with a preference optimization approach for hallucination detection, AL4RAG demonstrates superior performance in rejecting hallucination-prone queries while maintaining answer quality across multiple task types.

## Method Summary
AL4RAG employs a novel active learning strategy that uses retrieval-augmented similarity (ras) to select diverse, high-impact samples from conversation records. The ras metric computes cosine similarity separately for query, reference, and response components using TF-IDF vectorization, then combines them to avoid dominance by longer documents. Human annotators label hallucination presence in selected samples, which are then converted into preference pairs for Direct Preference Optimization (DPO). The framework uses LoRA fine-tuning with a Llama-2-7B-chat base model, optimizing for both rejection of hallucination-prone queries and stable performance on answerable queries.

## Key Results
- AL4RAG achieves 21.74% rejection rate for hallucination-prone queries at 12.5% data, outperforming baselines (17.39-19.93%)
- The framework maintains superior stability across ROUGE-L, ROUGE-1, ROUGE-2, and BERTScore metrics
- TF-IDF vectorization outperforms Sentence-BERT and Stella for sample diversity measurement
- Peak performance achieved at 12.5-25% annotated data, with performance degrading at higher data ratios

## Why This Works (Mechanism)

### Mechanism 1
Independent multi-attribute similarity measurement improves sample diversity selection for RAG active learning. The retrieval-augmented similarity (ras) metric computes similarity separately for query and reference components, averages them, then takes the minimum with prompt-level similarity. This prevents long reference documents from dominating shorter queries in distance calculations, enabling more accurate identification of truly diverse samples. Core assumption: RAG hallucinations are primarily caused by misunderstanding retrieved references, so measuring query-reference relationships independently captures hallucination-relevant diversity better than pooled similarity.

### Mechanism 2
Preference dataset construction from single-response conversations enables rejection training without paired response data. Human annotators label hallucination presence (h=0/1). For h=0 samples, original response becomes "chosen" and explicit rejection becomes "rejected." For h=1 samples, these are reversed. DPO optimization then trains the model to prefer correct responses for answerable queries and rejection responses for hallucination-prone queries. Core assumption: Explicit rejection responses are universally appropriate for hallucination cases, and the binary hallucination label captures all relevant failure modes requiring refusal.

### Mechanism 3
Active learning with diversity-based selection outperforms full-data training when annotation budget is constrained. AL4RAGras selects samples maximally dissimilar to already-selected samples while similar to remaining unlabeled samples (IDDS principle). This filters noisy/redundant samples, concentrating annotation budget on high-impact examples. DPO on 25% AL-selected data outperforms DPO on 100% random data. Core assumption: Sample diversity correlates with information content for the specific task of hallucination recognition and rejection learning.

## Foundational Learning

- Concept: **Diversity-based Active Learning (IDDS)**
  - Why needed here: AL4RAG builds on IDDS scoring, which balances distance from selected samples against similarity to unselected samples. Understanding this tension is essential for tuning λ and interpreting why the method filters outliers.
  - Quick check question: Given a sample highly dissimilar to all other samples (potential outlier), would IDDS score it high or low for selection?

- Concept: **Direct Preference Optimization (DPO)**
  - Why needed here: The framework uses DPO rather than RLHF for preference learning. DPO eliminates reward model training by optimizing directly on preference pairs using the log-ratio loss in Equation 4.
  - Quick check question: Why does DPO require both a policy πθ and reference policy πo, and what happens if β is set too high?

- Concept: **RAG Hallucination Taxonomy**
  - Why needed here: The method assumes hallucinations are detectable via binary labeling, but RAG-specific hallucinations include factual conflicts, unsupported claims, and reference misattribution. Understanding these categories helps assess whether the annotation protocol captures all relevant failure modes.
  - Quick check question: In a RAG system, what's the difference between a hallucination from parametric knowledge leakage versus one from reference misinterpretation?

## Architecture Onboarding

- Component map: Conversation Pool → TF-IDF Vectorizer → ras Similarity Calculator → AL Selection Loop → Human Annotation → Preference Pair Constructor → DPO Trainer → Fine-tuned RAG Model

- Critical path: The ras similarity calculation (Equation 5) is the core differentiator. Incorrect implementation here cascades to poor sample selection, wasting annotation budget. Verify TF-IDF vectorization is applied separately to q, r, and p before computing cosine similarities.

- Design tradeoffs:
  - TF-IDF vs. embedding models: Paper shows TF-IDF outperforms Sentence-BERT and stella for this task. TF-IDF captures surface-form differences that correlate with hallucination labels; embeddings may conflate semantically similar but label-different samples.
  - Data proportion: Peak performance at 12.5-25% suggests over-selection adds noise. Start with smaller budgets and scale up only if validation metrics justify it.
  - LoRA vs. full fine-tuning: Paper uses LoRA for efficiency; full fine-tuning may improve rejection calibration but risks catastrophic forgetting of answerable query handling.

- Failure signatures:
  - Rejection rate increases but stability (ROUGE-L) decreases sharply: Model learned to over-refuse; check preference pair construction for h=0 samples.
  - Performance degrades as data proportion increases: Selection is adding noisy samples; verify ras implementation and check for outlier samples dominating selection.
  - Baseline methods outperform at high data ratios: Expected behavior—AL4RAG advantage diminishes as annotation budget approaches full coverage.

- First 3 experiments:
  1. Reproduce ras ablation: Compare query-only similarity, prompt similarity, and ras on a held-out validation set. Expected: ras ≥ 2-4 percentage points higher rejection rate at 25% data.
  2. Vectorization sensitivity: Test TF-IDF vs. Sentence-BERT vs. domain-specific embeddings on your target conversation distribution. If your data has high lexical variation for similar meanings, embeddings may perform better than the paper's results.
  3. Annotation budget curve: Run AL4RAG at 10%, 25%, 50%, 75% data and plot rejection rate + stability metrics. Identify the inflection point where additional data stops improving or starts degrading performance for your specific dataset.

## Open Questions the Paper Calls Out

### Open Question 1
Can a hybrid vectorization approach combining lexical features (TF-IDF) and semantic embeddings (Transformers) outperform the standalone TF-IDF method used in AL4RAG? The paper only compares methods independently; it does not explore whether the semantic depth of transformers could complement the structural sensitivity of TF-IDF to further improve sample diversity selection.

### Open Question 2
Does the effectiveness of the retrieval-augmented similarity (ras) metric scale to significantly larger models (e.g., 70B+ parameters) with different hallucination patterns? The experimental evaluation is restricted to the Llama-2-7B-chat model. The authors do not validate if the diversity requirements and sample selection strategy remain optimal for larger models which may have different failure modes.

### Open Question 3
Is the AL4RAG framework robust when applied to noisy, multi-turn conversational data distinct from the single-turn QA format of the RAGTruth dataset? The authors propose using "vast amount of conversations" in the Introduction but limit experiments to the RAGTruth dataset, which consists of specific tasks like QA and summarization.

## Limitations

- Critical implementation details remain unspecified, particularly the λ parameter for IDDS scoring and initial random selection size k, which significantly impact sample selection quality.
- The framework demonstrates effectiveness on RAGTruth but doesn't address whether the ras metric generalizes to domains with different conversational structures or hallucination patterns.
- The binary hallucination labeling protocol may oversimplify complex failure modes, potentially limiting the model's ability to handle nuanced cases requiring partial responses or clarification rather than flat rejection.

## Confidence

- **High confidence**: The retrieval-augmented similarity (ras) metric provides measurable improvement over baseline similarity measures for active learning sample selection. The ablation showing ras outperforming query-only and prompt-only similarity is well-supported by experimental results.
- **Medium confidence**: The framework generalizes across question answering, summarization, and data-to-text writing tasks. While performance improvements are consistent, the magnitude varies by task, and the paper doesn't fully explain why certain tasks benefit more than others.
- **Medium confidence**: AL4RAG achieves superior rejection rates with fewer annotated samples compared to baselines. The 21.74% vs 17.39-19.93% comparison at 12.5% data is compelling, but the claim that AL4RAGras is "significantly more stable" at higher data ratios needs more rigorous statistical testing.

## Next Checks

1. **Hyperparameter sensitivity analysis**: Systematically vary λ in the IDDS scoring equation (0.1, 0.5, 0.9) and initial selection size k (1%, 5%, 10% of dataset) to identify optimal settings for different dataset characteristics. Measure how these changes affect rejection rate and stability metrics across all three task types.

2. **Cross-domain generalization test**: Apply AL4RAG to a conversation dataset from a different domain (e.g., medical Q&A or technical support) and compare ras performance against TF-IDF baseline. This validates whether the ras metric's advantages extend beyond RAGTruth's conversational patterns.

3. **Gray area handling evaluation**: Construct a test set with nuanced hallucination cases (partial hallucinations, ambiguous unsupported claims, reference misattributions) and evaluate whether AL4RAG models produce appropriate responses (partial answers, clarification requests) versus flat rejections. Compare against a binary-trained baseline to quantify handling of non-binary failure modes.