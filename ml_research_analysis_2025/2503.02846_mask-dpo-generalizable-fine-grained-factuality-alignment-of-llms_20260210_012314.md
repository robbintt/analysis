---
ver: rpa2
title: 'Mask-DPO: Generalizable Fine-grained Factuality Alignment of LLMs'
arxiv_id: '2503.02846'
source_url: https://arxiv.org/abs/2503.02846
tags:
- factuality
- arxiv
- alignment
- mask-dpo
- preference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Mask-DPO, a fine-grained factuality alignment
  method that improves LLM factuality by incorporating sentence-level factuality masks
  into Direct Preference Optimization. Unlike vanilla DPO which applies uniform rewards
  to entire responses, Mask-DPO selectively applies rewards only to factually correct
  sentences in preferred samples and factually incorrect sentences in non-preferred
  samples, resolving ambiguity in preference learning.
---

# Mask-DPO: Generalizable Fine-grained Factuality Alignment of LLMs

## Quick Facts
- **arXiv ID**: 2503.02846
- **Source URL**: https://arxiv.org/abs/2503.02846
- **Reference count**: 24
- **Primary result**: Llama3.1-8B-Instruct with Mask-DPO achieves 77.53% factuality on ANAH test, surpassing larger Llama3.1-70B-Instruct (53.44%)

## Executive Summary
Mask-DPO introduces a fine-grained factuality alignment method that improves large language model factuality by incorporating sentence-level factuality masks into Direct Preference Optimization (DPO). Unlike vanilla DPO which applies uniform rewards to entire responses, Mask-DPO selectively applies rewards only to factually correct sentences in preferred samples and factually incorrect sentences in non-preferred samples. This resolves ambiguity in preference learning where both correct and hallucinated sentences coexist in responses. When trained on the ANAH dataset, Llama3.1-8B-Instruct aligned with Mask-DPO achieved 77.53% factuality on the ANAH test set, surpassing both the vanilla DPO (68.44%) and the larger Llama3.1-70B-Instruct (53.44%). The method also showed strong generalization to out-of-domain Biography data (39.39% FactScore).

## Method Summary
Mask-DPO modifies the standard DPO objective by introducing sentence-level factuality masks that selectively apply KL divergence rewards based on factual correctness. The method samples multiple candidate responses per question, uses the ANAH-v2 annotator to identify sentence-level hallucinations, and constructs preference pairs from responses with highest and lowest factuality scores. During training, the loss function masks out KL penalties for incorrect sentences in preferred responses and correct sentences in non-preferred responses, focusing optimization only on the factually accurate portions. The approach is trained for 3 epochs with AdamW optimizer, learning rate 5e-6, batch size 64, and cosine annealing scheduler on 8×A100 GPUs using the Xtuner framework.

## Key Results
- Llama3.1-8B-Instruct with Mask-DPO achieves 77.53% factuality on ANAH test set, outperforming vanilla DPO (68.44%) and larger Llama3.1-70B-Instruct (53.44%)
- Strong generalization to out-of-domain Biography data with 39.39% FactScore
- Temperature=0.8 optimal for response sampling; temperature=0.25 yields poor results (65.30%)
- Scaling analysis suggests factuality alignment implicitly adjusts model-specific knowledge graph structures

## Why This Works (Mechanism)
Mask-DPO works by resolving the ambiguity in preference learning where both correct and hallucinated sentences coexist in responses. By selectively applying rewards only to factually correct sentences in preferred responses and factually incorrect sentences in non-preferred responses, the method focuses optimization on the reliable signals while ignoring the noisy or contradictory information. This fine-grained approach prevents the model from learning from hallucinated content that might be present in otherwise good responses, leading to more precise factuality improvements without sacrificing overall response quality.

## Foundational Learning
- **Direct Preference Optimization (DPO)**: Why needed - provides efficient alternative to RLHF for aligning LLMs with human preferences. Quick check - verify standard DPO implementation produces expected results on simple preference datasets.
- **Sentence-level factuality annotation**: Why needed - enables precise identification of hallucinated content within responses. Quick check - ensure annotator correctly identifies known hallucinations in test samples.
- **KL divergence masking**: Why needed - allows selective application of optimization signals to specific token subsets. Quick check - verify masked tokens don't contribute to gradient updates during training.
- **Preference pair construction**: Why needed - creates training signals from relative quality comparisons between responses. Quick check - ensure preference pairs have meaningful score gaps and aren't filtered out.
- **Multi-response sampling**: Why needed - provides diverse response pool for identifying best/worst examples. Quick check - verify response diversity metrics (distinct-4, unique tokens) meet expectations.

## Architecture Onboarding

**Component Map**: ANAH Dataset -> Response Sampler (top-k=40, temp=0.8) -> ANAH-v2 Annotator -> Factuality Calculator -> Preference Pair Constructor -> Mask-DPO Trainer (AdamW, 5e-6 lr) -> Factuality Evaluator

**Critical Path**: The most critical components are the ANAH-v2 annotator and the preference pair construction, as errors in hallucination detection or pair selection directly impact the quality of training signals. The response sampler's temperature setting is crucial for ensuring sufficient diversity in the response pool.

**Design Tradeoffs**: Mask-DPO trades increased computational cost (32× inference for response sampling) for improved factuality precision. The sentence-level masking adds implementation complexity but enables selective optimization that vanilla DPO cannot achieve.

**Failure Signatures**: 
- Poor factuality scores despite training completion suggest issues with annotator accuracy or preference pair quality
- Training instability may indicate improper masking implementation or learning rate issues
- Minimal improvement over vanilla DPO suggests response sampling temperature is too low or masking is incorrectly applied

**3 First Experiments**:
1. Verify baseline DPO implementation produces expected improvements on standard preference datasets
2. Test ANAH-v2 annotator accuracy on known hallucination examples before full pipeline integration
3. Validate preference pair construction by manually inspecting sample pairs and their score distributions

## Open Questions the Paper Calls Out
None

## Limitations
- Potential reward hacking from using ANAH-v2 for both preference construction and evaluation
- Limited generalization evidence beyond single out-of-domain Biography dataset
- Underspecified implementation details for sentence-to-token alignment in loss computation
- Computational cost of sampling 32 responses per question may limit practical applicability

## Confidence

**High confidence**: The Mask-DPO method is technically sound and the reported improvements over vanilla DPO (77.53% vs 68.44% on ANAH test) are well-supported by experimental results.

**Medium confidence**: The generalization claim to out-of-domain Biography data (39.39% FactScore) is supported but represents modest improvement over baselines.

**Low confidence**: The computational efficiency claims relative to other fine-grained methods are not quantified, and the practical implementation details for sentence-to-token alignment are underspecified.

## Next Checks

1. **Independent evaluator validation**: Reproduce key results using only FactScore for both preference construction and evaluation to verify improvements aren't inflated by evaluator overlap.

2. **Cross-dataset generalization**: Evaluate Mask-DPO on additional factuality datasets beyond ANAH and Biography (e.g., TrueFact, FActScore benchmarks) to assess robustness across diverse domains.

3. **Knowledge graph validation**: Conduct quantitative analysis of the proposed knowledge graph hypothesis by measuring changes in internal model representations (attention patterns, activation similarities) before and after Mask-DPO training.