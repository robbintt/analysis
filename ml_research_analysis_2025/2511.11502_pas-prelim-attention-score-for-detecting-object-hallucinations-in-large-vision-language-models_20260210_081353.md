---
ver: rpa2
title: 'PAS : Prelim Attention Score for Detecting Object Hallucinations in Large
  Vision--Language Models'
arxiv_id: '2511.11502'
source_url: https://arxiv.org/abs/2511.11502
tags:
- hallucination
- object
- prelim
- attention
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses object hallucination in Large Vision-Language
  Models (LVLMs), where models incorrectly refer to objects not present in input images.
  The authors observe that hallucinatory object predictions often occur when the model
  excessively relies on preliminary output tokens (generated text) rather than image
  tokens.
---

# PAS : Prelim Attention Score for Detecting Object Hallucinations in Large Vision--Language Models

## Quick Facts
- **arXiv ID**: 2511.11502
- **Source URL**: https://arxiv.org/abs/2511.11502
- **Reference count**: 40
- **Primary result**: Proposed Prelim Attention Score (PAS) achieves state-of-the-art object hallucination detection with AUROC scores of 84.2%, 85.6%, and 84.5% across multiple models and datasets

## Executive Summary
This paper introduces PAS (Prelim Attention Score), a training-free method for detecting object hallucinations in Large Vision-Language Models (LVLMs). The key insight is that hallucinatory object predictions occur when models excessively rely on preliminary output tokens rather than image tokens. PAS measures attention weights from prelim tokens to object tokens during inference, providing a lightweight detection mechanism without requiring additional forward passes or computational overhead.

The method demonstrates superior performance across multiple LVLMs including LLaVA-1.5-7B, MiniGPT-4, and Shikra, as well as datasets like MSCOCO and Pascal VOC. By focusing on attention patterns rather than model outputs alone, PAS achieves better detection accuracy while maintaining computational efficiency.

## Method Summary
PAS operates by analyzing attention weights during the inference process of LVLMs. It specifically tracks attention from preliminary output tokens (early generated text) to object-related tokens in the model's vocabulary. When hallucinatory objects are predicted, these prelim-to-object attention weights are significantly higher than for accurate predictions. The method extracts these attention patterns in real-time during inference, requiring no additional model training or computational overhead beyond the standard forward pass.

The approach is designed to be model-agnostic and can be applied to any LVLM architecture that maintains attention weights during inference. By focusing on the attention mechanism rather than model outputs alone, PAS captures the underlying reasoning process that leads to hallucinations.

## Key Results
- Achieves state-of-the-art detection AUROC of 84.2% on MSCOCO, 85.6% on MiniGPT-4, and 84.5% on Pascal VOC
- Outperforms existing baselines while requiring no additional forward passes
- Demonstrates effectiveness across multiple LVLMs including LLaVA-1.5-7B, MiniGPT-4, and Shikra
- Maintains minimal computational overhead as it leverages existing attention patterns during inference

## Why This Works (Mechanism)
PAS works by identifying abnormal attention patterns that occur when LVLMs generate hallucinatory objects. The mechanism relies on the observation that during hallucination events, the model disproportionately attends to preliminary output tokens (early generated text) rather than grounding its predictions in image-derived information. This creates a detectable signature in the attention weights that PAS can measure and score.

## Foundational Learning
- **Attention mechanisms in transformers**: Understanding how attention weights capture relationships between tokens is crucial for interpreting PAS results
- **Vision-Language Model architecture**: Knowledge of how LVLMs fuse visual and textual information helps understand where hallucinations occur
- **Object hallucination detection**: Familiarity with existing hallucination detection methods provides context for PAS's contributions
- **Token-based language modeling**: Understanding how models generate text token-by-token is essential for grasping prelim token behavior
- **Attention weight analysis**: Skills in extracting and interpreting attention patterns from model internals are necessary for implementing PAS
- **Evaluation metrics for hallucination detection**: Understanding AUROC and other detection metrics is important for assessing PAS performance

## Architecture Onboarding

**Component map**: Image tokens -> Preliminary tokens -> Object tokens -> Output predictions

**Critical path**: The method traces attention flow from preliminary output tokens back to object-related tokens, identifying when prelim tokens dominate the reasoning process leading to hallucinations.

**Design tradeoffs**: 
- Training-free approach vs. potentially higher accuracy from fine-tuned methods
- Lightweight computation vs. possible missed detection of complex hallucination patterns
- Real-time inference compatibility vs. reliance on attention weight availability

**Failure signatures**: 
- High prelim-to-object attention weights indicating potential hallucination
- Inconsistent attention patterns across similar visual inputs
- Over-reliance on textual context rather than visual grounding

**3 first experiments**:
1. Measure PAS scores on correctly identified vs. hallucinated objects in controlled test sets
2. Compare PAS detection rates across different model sizes and architectures
3. Analyze attention patterns in edge cases where visual information is ambiguous

## Open Questions the Paper Calls Out
The paper acknowledges that the analysis of why preliminary tokens contribute to hallucinations remains largely correlational rather than causal. The underlying mechanism explaining why prelim tokens specifically cause hallucinations is not fully elucidated. Additionally, the method's generalizability to more diverse object types, complex scenes, and larger LVLMs beyond the tested 7B parameter models remains uncertain.

## Limitations
- The causal relationship between prelim token attention and hallucinations requires further investigation
- Generalizability to larger LVLMs (34B+ parameters) and more diverse object categories is uncertain
- Limited testing on complex multi-object scenes beyond MSCOCO and Pascal VOC benchmarks

## Confidence
- **High confidence**: Empirical evaluation showing PAS outperforming baseline methods on detection AUROC across multiple datasets and models
- **Medium confidence**: Claims about requiring no additional forward passes and minimal computational overhead, based on analysis of existing attention patterns
- **Medium confidence**: Observation that hallucinatory objects have higher prelim-to-object attention weights, though causal relationship needs more investigation

## Next Checks
1. Test PAS on larger LVLMs (e.g., 34B+ parameters) to verify scalability and whether prelim token phenomenon persists at larger model scales
2. Conduct ablation studies removing prelim tokens entirely to establish stronger causal evidence for the prelim token mechanism
3. Evaluate PAS on datasets with more diverse object categories and complex multi-object scenes to assess robustness beyond current benchmarks