---
ver: rpa2
title: 'Advancing Constrained Monotonic Neural Networks: Achieving Universal Approximation
  Beyond Bounded Activations'
arxiv_id: '2505.02537'
source_url: https://arxiv.org/abs/2505.02537
tags:
- monotonic
- activations
- layer
- neural
- networks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper extends the theory of constrained monotonic neural networks
  (MLPs) by showing that universal approximation is achievable with activations that
  saturate on alternating sides, rather than requiring bounded activations like sigmoid
  or tanh. The authors prove that an MLP with non-negative weights and 3 hidden layers,
  using alternating left- and right-saturating monotonic activations, can approximate
  any monotonic function.
---

# Advancing Constrained Monotonic Neural Networks: Achieving Universal Approximation Beyond Bounded Activations

## Quick Facts
- arXiv ID: 2505.02537
- Source URL: https://arxiv.org/abs/2505.02537
- Reference count: 40
- Primary result: Proves 4-layer MLPs with non-negative weights and alternating saturation-direction activations achieve universal approximation for monotonic functions.

## Executive Summary
This paper extends the theory of constrained monotonic neural networks by showing that universal approximation is achievable with activations that saturate on alternating sides, rather than requiring bounded activations like sigmoid or tanh. The authors prove that an MLP with non-negative weights and 3 hidden layers, using alternating left- and right-saturating monotonic activations, can approximate any monotonic function. They also show that non-positive weight constraints combined with ReLU or any saturating monotonic activation yield universal approximators—a contrast to the non-negative constraint case which is limited to convex functions. To address optimization challenges from weight constraints, they propose a weight-sign-based activation switch formulation that removes the need for constrained weights and multiple activation types, improving training stability and initialization. Experiments on several datasets demonstrate performance matching or exceeding state-of-the-art monotonic architectures, with test accuracies up to 94% and RMSE as low as 0.149.

## Method Summary
The paper introduces a theoretical framework showing that constrained monotonic neural networks can achieve universal approximation using alternating saturation-direction activations. They prove that 4-layer MLPs with non-negative weights can approximate any monotonic function if hidden layers alternate between left-saturating (S⁻) and right-saturating (S⁺) activations. The authors also establish that non-positive weight constraints with ReLU or any saturating monotonic activation yield universal approximators through a weight-sign/activation equivalence. To address optimization challenges from weight constraints, they propose a weight-sign-based activation switch formulation that decomposes weights into positive and negative parts with separate activation paths, avoiding vanishing gradients and improving initialization stability.

## Key Results
- Proves 4-layer MLPs with non-negative weights and alternating saturation-direction activations achieve universal approximation for monotonic functions
- Demonstrates that non-positive weight constraints with ReLU or any saturating monotonic activation yield universal approximators
- Proposes activation switch formulation that improves training stability and eliminates weight reparameterization issues
- Achieves test accuracies up to 94% and RMSE as low as 0.149 on benchmark datasets

## Why This Works (Mechanism)

### Mechanism 1: Alternating Saturation for Non-Convexity
- **Claim:** Constraining weights to be non-negative with convex activations normally limits the network to approximating only convex functions. Alternating the saturation direction of activations restores universal approximation.
- **Mechanism:** The paper proves (Theorem 3.5) that a 4-layer MLP with non-negative weights can approximate any monotonic function if the hidden layers alternate between left-saturating (S⁻) and right-saturating (S⁺) activations. This alternation allows the network to construct "indicator-like" functions via subtraction (e.g., ReLU - ReLU'), effectively simulating non-convex bumps without violating monotonicity.
- **Core assumption:** The network must have at least 3 hidden layers to perform the necessary intersections and unions of half-spaces.
- **Evidence anchors:** [abstract]: "...MLPs with non-negative weight constraint and activations that saturate on alternating sides are universal approximators..." [section 3.2]: Lemma 3.6/3.7 show how alternating saturation allows the approximation of step functions essential for interpolation.
- **Break condition:** If all layers use the same convex activation (e.g., all ReLU) with non-negative weights, the network collapses to a purely convex function approximator.

### Mechanism 2: Weight-Sign / Activation Equivalence
- **Claim:** Imposing non-positive weight constraints on a network with a single convex activation (like ReLU) yields a universal approximator, whereas non-positive constraints are typically limiting.
- **Mechanism:** Proposition 3.10 establishes that two adjacent layers with non-positive weights and activation σ are mathematically equivalent to two layers with non-negative weights and the point-reflected activation σ'(x) = -σ(-x). This allows a "Convex + Non-Positive" setup to mimic the "Alternating Saturation" setup without manually swapping activation functions.
- **Core assumption:** The activation function is monotonic and saturates on at least one side.
- **Evidence anchors:** [section 3.3]: "...MLP with 4 layers, non-positive weights and activation σ, is a universal approximator..." [section 3.3]: Proposition 3.10 details the equivalence between weight sign flips and activation reflection.
- **Break condition:** If the activation is not saturating (e.g., Leaky ReLU with α > 0), the theoretical guarantee does not hold.

### Mechanism 3: Optimization via Activation Switching
- **Claim:** Decomposing weights into positive and negative parts with separate activation paths avoids the vanishing gradients caused by standard reparameterization.
- **Mechanism:** Standard monotonic networks enforce non-negativity via |W| (squaring or absolute value), which skews initialization distributions and causes saturation. The proposed formulation (Equation 13) splits the operation: ŷ = W⁺σ(x) + W⁻σ(-x). This "switch" allows the sign of the learned weight to determine the saturation direction automatically, preserving standard initialization statistics and gradient flow.
- **Core assumption:** The optimization landscape benefits from unconstrained weight matrices (can be positive or negative).
- **Evidence anchors:** [section 4.1]: "This eliminates the requirement for weight reparameterization, easing initialization..." [appendix a.2]: Figure 9 compares gradient magnitudes, showing constrained sigmoid networks suffer vanishing gradients while the proposed method remains stable.
- **Break condition:** If the implementation incorrectly ties the two weight branches or forces them to share a single activation path, the gradient benefits disappear.

## Foundational Learning

- **Concept: Monotonicity Constraints (Hard vs. Soft)**
  - **Why needed here:** The paper focuses on "Hard Monotonicity" (guaranteed by architecture) rather than penalties. You must understand that ensuring W ≥ 0 forces the partial derivatives to be positive, but this alone limits function complexity.
  - **Quick check question:** Why does applying non-negative constraints to a ReLU network limit it to convex functions only?

- **Concept: Activation Saturation Sides (S⁺ vs S⁻)**
  - **Why needed here:** The core theoretical advance relies on distinguishing between right-saturating functions (like ReLU, lim_{x→+∞} < ∞) and left-saturating functions (like ReLU', lim_{x→-∞} > -∞).
  - **Quick check question:** Does Sigmoid belong to S⁺, S⁻, both, or neither?

- **Concept: Universal Approximation Theorem**
  - **Why needed here:** The paper revises the conditions for this theorem specifically for constrained networks. It serves as the theoretical proof that the proposed architecture can learn any target mapping, provided sufficient depth (4 layers).
  - **Quick check question:** Previous bounded-activation networks were universal approximators; what specific functional limitation (hint: extrapolation) did they suffer from that this paper aims to fix?

## Architecture Onboarding

- **Component map:** Input -> Monotonic Layer (3+ hidden layers) -> Output
- **Critical path:** Implement Algorithm 1 (Post-activation switch). Do not use standard weight reparameterization (|W|). Ensure the input x is transformed to -x for the negative weight branch before applying the activation σ.
- **Design tradeoffs:**
  - Pre vs. Post Switch: The paper uses Post-activation (Eq 13) for experiments. Pre-activation (Eq 12) is valid theoretically but empirically post-activation performed slightly better or equivalently in their tests.
  - Activation Choice: The method supports ReLU, but the paper notes CELU can be better for small networks to avoid dead neurons.
- **Failure signatures:**
  - Saturated Output: If initializing a naive constrained network (|W|), outputs may saturate immediately (Figure 9), causing flat loss curves. The proposed switch should prevent this.
  - Convex-Only Fit: If the depth is <3 hidden layers or the switch mechanism is disabled (forcing all weights effectively positive), the model will fail to fit non-convex monotonic data (Figure 8).
- **First 3 experiments:**
  1. Sanity Check: Approximate f(x) = x + cos(x) (1D, monotonic, non-convex). Compare a naive non-negative ReLU net (should fail) vs. the proposed switch (should succeed).
  2. Classification (Fairness): Run on the COMPAS dataset to verify monotonicity on specific features (e.g., age/priors) while maximizing accuracy.
  3. Initialization Stress Test: Reproduce Figure 9—plot gradient magnitudes per layer for a 10-layer network to confirm the proposed method avoids the vanishing gradient issue found in constrained sigmoid nets.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can universal approximation be achieved in constrained monotonic neural networks using activations that do not saturate on either side, such as Leaky-ReLU?
- **Basis in paper:** [explicit] The conclusion explicitly states, "it is still an open question whether non-saturating activations, such as Leaky-ReLU, can be used to build monotonic MLPs."
- **Why unresolved:** The theoretical proofs (Theorem 3.5) rely on saturation limits (Definition 3.3) to approximate indicator functions, a property that Leaky-ReLU lacks due to its linear behavior in both directions.
- **What evidence would resolve it:** A formal extension of the approximation theorem to non-saturating functions, or empirical evidence showing successful approximation of complex non-convex monotonic functions using Leaky-ReLU in the proposed framework.

### Open Question 2
- **Question:** Can Batch Normalization be effectively integrated into constrained monotonic networks to solve initialization issues without violating monotonicity constraints?
- **Basis in paper:** [explicit] Appendix A.2.1 suggests a monotonic variant of Batch Normalization (γ ≥ 0) and notes, "the investigation of this approach falls out of the scope of this work, and it's left as a future line of research."
- **Why unresolved:** While BatchNorm helps unconstrained networks, standard implementations are not monotonic, and constrained networks suffer from unique saturation dynamics that BatchNorm might alleviate or exacerbate.
- **What evidence would resolve it:** Comparative training dynamics showing that monotonic BatchNorm allows for faster convergence and mitigates vanishing gradients compared to the proposed activation switch or weight reparameterization methods.

### Open Question 3
- **Question:** Does the activation switch formulation impose expressivity limitations when applied to partially monotonic inputs compared to fully monotonic ones?
- **Basis in paper:** [inferred] The theoretical results (Section 3) are derived for functions that are monotonic in all inputs (f: R^d → R), yet experiments utilize datasets like Blog Feedback where only 2.8% of features are monotonic.
- **Why unresolved:** It is unclear if the strict coupling of positive and negative weight paths in the switch formulation (Eq. 12) over-constrains the network's ability to model complex, non-monotonic interactions for the remaining features.
- **What evidence would resolve it:** An ablation study analyzing the representational capacity and performance loss when applying the activation switch to non-monotonic features versus using standard linear layers for those specific inputs.

### Open Question 4
- **Question:** Is the non-positive weight constraint architecture theoretically more efficient than the activation switch formulation regarding the number of parameters or layers required for approximation?
- **Basis in paper:** [inferred] Section 3.3 proves that non-positive weights with standard ReLU are universal approximators, while Section 4 introduces the switch formulation (which requires splitting weights W⁺ and W⁻) to improve optimization.
- **Why unresolved:** The paper compares their performance but does not analyze if the theoretical "simplicity" of non-positive weights translates to better parameter efficiency compared to the empirically stable switch formulation.
- **What evidence would resolve it:** A comparative analysis of the minimal network width and parameter count required for both architectures to approximate the same complex monotonic functions to a given error threshold.

## Limitations

- Theoretical guarantees depend on 3+ hidden layers with specific activation saturation patterns
- The non-convexity mechanism relies on approximating step functions via saturation, which may degrade in higher dimensions
- The proposed switch formulation introduces additional parameters (separate positive/negative weight matrices) that could increase overfitting risk in small datasets

## Confidence

- Universal Approximation Theorem (Alternating Saturation): High confidence
- Weight-Sign/Activation Equivalence: Medium confidence
- Optimization Improvements: High confidence

## Next Checks

1. **Higher-Dimensional Function Approximation:** Test the alternating saturation mechanism on multi-dimensional monotonic non-convex functions (e.g., monotonic extensions of Franke functions) to verify the theoretical guarantees extend beyond 1D cases.

2. **Activation Function Robustness:** Systematically evaluate the proposed architecture with different saturating monotonic activations (Leaky ReLU, CELU, Tanh) to identify which saturation patterns are essential versus beneficial.

3. **Gradient Flow Analysis:** Extend the gradient magnitude analysis (Figure 9) to deeper networks (15+ layers) and different initialization schemes to quantify the optimization benefits across a broader range of architectures.