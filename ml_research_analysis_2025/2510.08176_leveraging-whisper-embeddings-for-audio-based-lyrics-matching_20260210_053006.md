---
ver: rpa2
title: Leveraging Whisper Embeddings for Audio-based Lyrics Matching
arxiv_id: '2510.08176'
source_url: https://arxiv.org/abs/2510.08176
tags:
- lyrics
- wealy
- whisper
- matching
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: WEALY introduces a fully reproducible pipeline for audio-based
  lyrics matching using Whisper decoder embeddings, addressing limitations of existing
  methods that lack transparency or rely on intermediate text transcriptions. The
  method extracts lyrics-aware representations directly from raw audio through Whisper's
  autoregressive decoding, then learns contextualized embeddings using a transformer
  encoder trained with contrastive learning on musical version identification (MVI)
  as a proxy task.
---

# Leveraging Whisper Embeddings for Audio-based Lyrics Matching

## Quick Facts
- **arXiv ID**: 2510.08176
- **Source URL**: https://arxiv.org/abs/2510.08176
- **Reference count**: 0
- **Primary result**: MAP scores of 0.328, 0.640, and 0.692 on DiscogsVI-YT, SHS100k-v2, and LyricCovers2.0 respectively, outperforming transcription-based baselines

## Executive Summary
WEALY introduces a fully reproducible pipeline for audio-based lyrics matching using Whisper decoder embeddings, addressing limitations of existing methods that lack transparency or rely on intermediate text transcriptions. The method extracts lyrics-aware representations directly from raw audio through Whisper's autoregressive decoding, then learns contextualized embeddings using a transformer encoder trained with contrastive learning on musical version identification (MVI) as a proxy task. Experiments across three datasets demonstrate MAP scores of 0.328, 0.640, and 0.692 respectively, outperforming transcription-based baselines. Ablation studies show that NT-Xent loss and GeM pooling are optimal, while multilingual Whisper embeddings significantly improve performance compared to English-only decoding. Multimodal fusion with audio-based CLEWS achieves MAP of 0.912 on SHS, demonstrating the complementarity of lyrical and acoustic features for MVI.

## Method Summary
WEALY extracts lyrics-aware representations directly from raw audio by leveraging Whisper's autoregressive decoding to capture semantic content without requiring intermediate text transcription. The method processes audio through Whisper-turbo to obtain decoder hidden states, then trains a transformer encoder with contrastive learning on musical version identification as a proxy task. The pipeline uses NT-Xent loss to shape an embedding space where lyrical similarity is explicitly optimized, achieving state-of-the-art performance on three benchmarks while maintaining full reproducibility.

## Key Results
- SHS100k-v2: MAP of 0.640 outperforms transcription-based CLEWS (0.450) and triplet loss (0.548)
- LyricCovers2.0: MAP of 0.692 demonstrates cross-lingual retrieval capability across 80 languages
- Multimodal fusion: CLEWS + WEALY achieves MAP of 0.912 on SHS, showing complementarity of lyrical and acoustic features
- Multilingual advantage: English-only decoding degrades performance from 0.640 to 0.578 (9.7% relative drop)

## Why This Works (Mechanism)

### Mechanism 1: Autoregressive Decoder Representations Encode Lyrics Semantics
Whisper's decoder hidden states extracted before token sampling capture compressed semantic knowledge about lyrical content, enabling similarity matching without intermediate transcription. During autoregressive decoding, each hidden state incorporates prior context through attention mechanisms, building coherent representations that reflect linguistic dependencies. The final decoder layer retains the model's refined semantic understanding before committing to specific token predictions.

### Mechanism 2: Contrastive Learning on Version Pairs Shapes Embedding Space
Training a transformer encoder with NT-Xent loss on musical version pairs creates an embedding space where lyrical similarity is explicitly optimized. The NT-Xent loss maximizes cosine similarity between embeddings of same-clique songs while minimizing similarity to negative pairs within each batch. The temperature parameter τ=0.1 controls the softness of the distribution.

### Mechanism 3: Multilingual Latent Cues Enable Cross-Lingual Generalization
Multilingual Whisper embeddings preserve cross-lingual semantic information that improves retrieval performance over language-constrained decoding. Whisper's multilingual pre-training encodes language-invariant semantic representations in decoder states, allowing the adaptation model to learn cross-lingual similarity patterns rather than surface lexical matches.

## Foundational Learning

- **Autoregressive Decoder Hidden States**: Understanding what decoder states encode versus encoder states or final tokens is essential for interpreting WEALY's feature extraction choice. Quick check: Why might decoder hidden states (pre-sampling) contain richer semantic information than either encoder outputs or the final predicted token sequence?

- **NT-Xent (Normalized Temperature-scaled Cross-Entropy) Loss**: The entire adaptation stage shapes embeddings through this contrastive objective; understanding its mechanics is prerequisite for debugging training. Quick check: Given a batch of N song pairs, what happens to gradient magnitude if temperature τ is set too high (e.g., 1.0) versus too low (e.g., 0.01)?

- **Generalized Mean (GeM) Pooling**: GeM pooling is identified as optimal versus average or CLS token pooling; understanding its learnable parameter is necessary for implementation. Quick check: As the GeM pooling parameter p → ∞, what operation does GeM approximate? What happens when p = 1?

## Architecture Onboarding

- **Component map**: Raw audio -> 16kHz mono conversion -> 5-min max -> 30s overlapping chunks -> log-mel spectrograms -> Whisper encoder -> Whisper decoder -> Final-layer hidden states H ∈ R^(m×1280) -> Random subsequence sampling -> Linear projection -> 4× Transformer encoder blocks -> GeM pooling -> Linear projection -> z ∈ R^512

- **Critical path**: 1) Audio preprocessing: mono, 16kHz, truncate to 5 minutes; 2) Whisper extraction: concatenate all decoder hidden states across chunks; 3) Subsequence sampling: fixed k=1500 during training; 4) Transformer + GeM: contextualize and collapse temporal dimension; 5) Contrastive training: batch size 64, AdamW, cosine schedule, early stopping

- **Design tradeoffs**: Source separation skipped (trades 1-2% accuracy for complexity reduction); subsequence length k=1500 (empirically chosen); GeM vs average/CLS pooling (adds learnable parameter); no vocal separation (direct mixture input for simplicity)

- **Failure signatures**: MAP < 0.25 on SHS (check Whisper extraction); average+MLP competitive with transformer (temporal modeling failing); English-only matches multilingual (dataset lacks linguistic diversity); training loss plateaus early (check batch composition)

- **First 3 experiments**: 1) Reproduce Whisper-AvgEmb baseline (~0.297 MAP on SHS); 2) Loss function ablation (triplet loss ~0.548, CLEWS loss ~0.450); 3) Language constraint ablation (~0.578 MAP for English-only)

## Open Questions the Paper Calls Out

1. **Prompt initialization effects**: How do different Whisper initialization prompts (e.g., "lyrics" vs. standard tokens) affect the quality of learned representations for lyrics matching? The paper mentions investigating this but presents no findings.

2. **Cross-lingual semantic matching**: Can lyrics matching systems identify semantically similar content across different languages without relying on lexical overlap? The paper shows multilingual embeddings help but doesn't isolate cross-lingual semantic understanding from better transcription quality.

3. **Optimal fusion strategies**: What are the optimal fusion strategies for combining lyrics-aware and audio-content embeddings beyond simple weighted distance combination? The paper uses a basic late-fusion approach with fixed weight (1.5×) but notes this as a promising direction.

4. **Direct lyrics matching tasks**: How does WEALY perform on direct lyrics matching tasks that do not rely on musical version identification as a proxy? The paper acknowledges this limitation but hasn't evaluated on datasets with direct lyrics similarity labels.

## Limitations

- Reliance on version cliques as proxy for lyrical similarity may systematically misalign embeddings for cross-lingual covers, parodic versions, or adaptations
- Multilingual vs English-only comparison conflates language restriction with potential decoding quality differences
- Multimodal fusion uses ad-hoc fixed weighting (1.5×) suggesting variable relative contribution across datasets

## Confidence

- **High**: Core technical pipeline and relative performance ordering across loss functions
- **Medium**: Claim that multilingual Whisper embeddings improve performance (based on single ablation)
- **Low**: Assertion that version cliques provide "lower bound on detectable semantic similarities" without manual verification

## Next Checks

1. Manually annotate a subset of SHS test pairs to verify whether high-scoring WEALY pairs actually share meaningful lyrical content versus coincidental melodic or rhythmic similarity

2. Create controlled test set of translated song pairs across multiple language families to quantify actual cross-lingual retrieval capability

3. Construct test cases with intentional lyrical modifications (parodies, adaptations, instrumental versions) to characterize method's sensitivity to semantic divergence