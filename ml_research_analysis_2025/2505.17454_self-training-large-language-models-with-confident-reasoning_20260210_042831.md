---
ver: rpa2
title: Self-Training Large Language Models with Confident Reasoning
arxiv_id: '2505.17454'
source_url: https://arxiv.org/abs/2505.17454
tags:
- reasoning
- confidence
- self-training
- answer
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses a limitation in existing confidence-based self-training
  methods for LLMs, which rely solely on answer-level confidence and may ignore errors
  in reasoning paths. The authors propose CORE-PO, a new self-training method that
  incorporates reasoning-level confidence to identify high-quality reasoning paths.
---

# Self-Training Large Language Models with Confident Reasoning

## Quick Facts
- arXiv ID: 2505.17454
- Source URL: https://arxiv.org/abs/2505.17454
- Authors: Hyosoon Jang; Yunhui Jang; Sungjae Lee; Jungseul Ok; Sungsoo Ahn
- Reference count: 24
- Primary result: CORE-PO improves reasoning accuracy by incorporating reasoning-level confidence via P(True) scores, outperforming answer-level self-consistency methods

## Executive Summary
Existing self-training methods for LLMs rely on answer-level confidence, which can reinforce incorrect reasoning paths that coincidentally yield correct answers. CORE-PO addresses this by incorporating reasoning-level confidence through P(True) scores that measure how likely the model considers its reasoning correct. The method fine-tunes LLMs to prefer high-confidence reasoning paths using Direct Preference Optimization, resulting in improved accuracy on both in-distribution and out-of-distribution benchmarks.

## Method Summary
CORE-PO generates N=5 outputs per question, computes joint confidence scores combining reasoning-level confidence (via P(True)) and answer-level confidence, then fine-tunes the model using Direct Preference Optimization to prefer higher-confidence outputs. The method uses LoRA adapters for efficient fine-tuning and evaluates checkpoints on validation sets to select the best model. Training requires significant inference compute to generate samples and evaluate their confidence before training begins.

## Key Results
- CORE-PO improves accuracy on four in-distribution benchmarks (GSM8K, ARC-Challenge, GPQA, MATH) compared to existing self-training approaches
- Achieves higher accuracy with greedy decoding than base models using inference-time scaling on ARC-Challenge and GPQA
- Shows best performance on Game of 24 out-of-distribution task
- Improves reasoning-level confidence and accuracy, demonstrating enhanced reasoning capabilities

## Why This Works (Mechanism)

### Mechanism 1
Existing methods like SC-PO use majority voting as a reward, but this risks reinforcing incorrect reasoning paths that accidentally yield correct answers. The paper identifies that frequency of an answer correlates with correctness but doesn't strictly entail reasoning validity. Evidence shows models generating flawed logic that accidentally selects correct multiple-choice options, leading to future errors.

### Mechanism 2
Reasoning-level confidence, measured via P(True), acts as an effective filter for process validity. By replacing frequency-based rewards with P(True) - the model's probability of asserting reasoning is "True" - the system shifts from rewarding outcomes to rewarding processes. This filters out lucky guesses in favor of logically sound trajectories.

### Mechanism 3
Direct Preference Optimization on confidence-ranked pairs internalizes external verification capability into the generation policy. The algorithm samples multiple outputs, ranks them using combined confidence scores, and applies DPO to maximize likelihood of high-confidence reasoning while minimizing low-confidence reasoning. This reduces dependency on external verifiers at inference time.

## Foundational Learning

- **Concept:** Self-Consistency vs. Process Verification
  - **Why needed here:** Understanding that getting the right answer frequently (consistency) is distinct from having valid logical derivation (process verification)
  - **Quick check question:** If a model outputs 5 different reasoning paths, 3 of which have bad logic but end with correct answer "A", would Self-Consistency reward or punish this behavior?

- **Concept:** P(True) and Verbalized Confidence
  - **Why needed here:** CORE-PO relies on P(True) - reading probability of "True" token - rather than semantic entropy or raw likelihoods
  - **Quick check question:** How do you extract confidence score from LLM without looking at perplexity, but by asking it a question?

- **Concept:** Direct Preference Optimization (DPO)
  - **Why needed here:** The mechanism for self-training is DPO, not standard RL with separate reward model
  - **Quick check question:** In DPO, do you need separate "Critic" network during training, or does algorithm rely on frozen reference model?

## Architecture Onboarding

- **Component map:** Generator (M_θ) -> Evaluator (M_θ computes confidence) -> Trainer (DPO loop)
- **Critical path:** 1) Sample N=5 outputs per question, 2) Run forward pass with verification prompt to get P(True), 3) Aggregate into joint confidence score, 4) Construct preference pairs, 5) Update weights via DPO
- **Design tradeoffs:** Monolithic P(True) is faster (one check) vs statement-wise (checking every sentence) which is finer-grained but computationally expensive. Monolithic is suggested as robust default.
- **Failure signatures:** Reward hacking (model learns to generate reasoning that looks confident rather than correct), degradation on complex math problems, miscalibrated P(True) scores
- **First 3 experiments:** 1) Plot Answer Accuracy vs Reasoning Accuracy to verify CORE-PO offers benefit over answer-based methods, 2) Train two small models - one using only answer confidence, one using reasoning confidence - compare reasoning accuracy with external oracle, 3) Check if P(True) scores are calibrated by plotting confidence vs accuracy

## Open Questions the Paper Calls Out
None

## Limitations
- Computational overhead is significant, requiring inference costs to double during data collection as model must generate samples and evaluate confidence before training
- Method shows slight degradation on MATH Level 5 compared to SC-PO, suggesting reasoning-confidence may be harder to estimate for complex mathematical problems
- P(True) mechanism may not perfectly correlate with actual reasoning validity, with potential for reward hacking where models learn to generate reasoning that looks confident rather than strictly correct

## Confidence

- **High confidence:** CORE-PO improves accuracy over existing self-training methods on majority of in-distribution and out-of-distribution benchmarks
- **Medium confidence:** CORE-PO achieves higher accuracy with greedy decoding than base models using inference-time scaling on ARC-Challenge and GPQA
- **Medium confidence:** The P(True) mechanism effectively captures reasoning-level confidence and improves reasoning accuracy

## Next Checks

1. Conduct calibration test of P(True) scores by plotting confidence vs accuracy on validation set to verify mechanism isn't rewarding overconfident but incorrect reasoning
2. Compare CORE-PO's reasoning-level accuracy against external oracle (GPT-4o or human evaluation) to validate improvements beyond answer-level gains
3. Perform ablation studies testing relative contributions of reasoning-level vs answer-level confidence components to isolate key driver of performance improvements