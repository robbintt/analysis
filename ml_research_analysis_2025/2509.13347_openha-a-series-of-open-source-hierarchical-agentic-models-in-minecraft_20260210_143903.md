---
ver: rpa2
title: 'OpenHA: A Series of Open-Source Hierarchical Agentic Models in Minecraft'
arxiv_id: '2509.13347'
source_url: https://arxiv.org/abs/2509.13347
tags:
- action
- arxiv
- agent
- hierarchical
- actions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the challenge of choosing effective action\
  \ representations for end-to-end trainable agents in complex environments like Minecraft.\
  \ Through a large-scale empirical comparison, the authors find that no single action\
  \ abstraction is universally optimal\u2014performance depends heavily on task type,\
  \ with grounding actions excelling in object-interaction tasks, motion actions in\
  \ navigation, and language skills in GUI tasks."
---

# OpenHA: A Series of Open-Source Hierarchical Agentic Models in Minecraft

## Quick Facts
- arXiv ID: 2509.13347
- Source URL: https://arxiv.org/abs/2509.13347
- Reference count: 11
- A single unified model trained on mixed action spaces outperforms specialist agents across 800+ Minecraft tasks

## Executive Summary
This paper addresses the challenge of choosing effective action representations for end-to-end trainable agents in complex environments like Minecraft. Through a large-scale empirical comparison, the authors find that no single action abstraction is universally optimal—performance depends heavily on task type, with grounding actions excelling in object-interaction tasks, motion actions in navigation, and language skills in GUI tasks. To resolve this, they introduce Chain of Action (CoA), a framework that integrates hierarchical reasoning into a single VLA model by treating abstracted actions as intermediate reasoning steps. They further propose an All-in-One training strategy, training one model on a mixture of action spaces using CoA, which results in a more robust and generalizable policy. Evaluated on over 800 Minecraft tasks, the All-in-One model outperforms specialist agents across all task categories, achieving state-of-the-art overall success rates.

## Method Summary
The Chain of Action framework trains a monolithic VLA to generate abstracted actions as intermediate reasoning steps before low-level actions. The model learns P(A, a|ins, obs) = P(a|ins, obs, A) · P(A|ins, obs) through a two-stage curriculum: first learning vocabularies on high-level and low-level datasets, then bridging planning and execution on Chain-of-Action data. The All-in-One strategy trains one model on mixed action spaces, using primitive actions as a "common currency" to connect diverse abstractions. The method is evaluated on Qwen2-VL-7B using TRL, VeOmni, and vLLM libraries.

## Key Results
- All-in-One model outperforms specialist agents across all task categories (Embodied, GUI, Combat)
- Grounding actions excel at object-interaction tasks (37.1% ASR on Embodied), motion actions at navigation, language skills at GUI tasks
- Single unified model achieves state-of-the-art overall success rates on 800+ Minecraft tasks
- CoA framework enables 3-5× higher FPS in decoupled mode while maintaining performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Generating abstracted actions as intermediate "thoughts" before low-level actions improves task success by decomposing complex decisions into sequential sub-problems.
- Mechanism: The model factorizes the joint probability P(A, a|ins, obs) = P(a|ins, obs, A) · P(A|ins, obs). The abstracted action serves as explicit context that conditions subsequent low-level prediction, reducing the implicit reasoning burden on the model.
- Core assumption: The model has sufficient capacity to learn both high-level planning and low-level control in a unified autoregressive process without catastrophic interference.
- Evidence anchors:
  - [abstract]: "CoA treats an abstracted action not as a command for a separate policy, but as an intermediate reasoning step—akin to a chain of thought"
  - [section 3.1, Equation 2]: Formal factorization of joint probability
  - [corpus]: Limited direct corroboration; related work (Agent models: Internalizing Chain-of-Action Generation into Reasoning models) proposes similar internalization but remains conceptual
- Break condition: If the abstracted action vocabulary is poorly aligned with the task (e.g., motion actions for GUI crafting), intermediate "thoughts" may not provide useful conditioning signal.

### Mechanism 2
- Claim: Action abstraction effectiveness is task-dependent—grounding actions excel at object-interaction, motion actions at navigation, language skills at GUI tasks.
- Mechanism: Each action space encodes different inductive biases. Grounding actions explicitly bind verbs to spatial coordinates (leveraging VLM visual grounding), motion actions express navigational intent without object specificity, language skills provide semantic task decomposition. The alignment between abstraction and task structure determines sample efficiency and generalization.
- Core assumption: The low-level policy can reliably decode abstracted actions into executable primitives across diverse contexts.
- Evidence anchors:
  - [abstract]: "performance depends heavily on task type, with grounding actions excelling in object-interaction tasks, motion actions in navigation, and language skills in GUI tasks"
  - [section 4.3, Table 3]: GroundingHA achieves 37.1% ASR on Embodied vs. MotionHA's 0.0% on GUI
  - [corpus]: No direct replication in neighbors; Optimus-2/3 work focuses on multimodal conditioning rather than action space comparison
- Break condition: When tasks require capabilities spanning multiple action categories (e.g., navigate-then-craft), single action spaces create bottlenecks.

### Mechanism 3
- Claim: Training on heterogeneous action spaces with shared low-level grounding produces more robust, generalizable policies through cross-action knowledge transfer.
- Mechanism: Low-level primitive actions serve as a "common currency" connecting diverse high-level abstractions. The model learns functional equivalences (e.g., "Approach(object=sheep)" ≈ "Go forward" when both yield similar primitive sequences), enriching the internal policy representation.
- Core assumption: The model can resolve distributional shift across action vocabularies without requiring explicit alignment objectives.
- Evidence anchors:
  - [abstract]: "All-in-One training strategy, training one model on a mixture of action spaces using CoA, which results in a more robust and generalizable policy"
  - [section 3.3]: "The primitive action at acts as a common currency, creating a shared semantic ground"
  - [corpus]: Weak external validation; neighbor papers focus on single action paradigms rather than mixture training
- Break condition: If action spaces are too dissimilar or datasets are imbalanced, the model may collapse to dominant action types rather than learning transfer.

## Foundational Learning

- Concept: Autoregressive sequence modeling (next-token prediction)
  - Why needed here: CoA requires the model to generate abstracted actions then condition subsequent tokens on this self-generated context within a single forward pass.
  - Quick check question: Can you explain why P(a|ins, obs, A) is tractable while P(a|ins, obs) alone may be harder to learn?

- Concept: Hierarchical reinforcement learning / options framework
  - Why needed here: The abstraction hierarchy mirrors temporal abstraction in HRL, where high-level "options" invoke lower-level policies over extended horizons.
  - Quick check question: How does the "common currency" of primitive actions relate to the notion of option termination conditions?

- Concept: Visual grounding in VLMs
  - Why needed here: Grounding actions rely on the VLM's pretrained ability to associate textual references with spatial coordinates in images.
  - Quick check question: Why might grounding actions fail when targets are occluded or outside the current field of view?

## Architecture Onboarding

- Component map: Vision encoder (ViT) -> Causal Transformer -> LLM head (outputs abstracted actions or primitive actions) -> Optional low-level policy decoder (Fast mode)

- Critical path:
  1. Observation → Vision encoder → visual tokens
  2. Concatenate [instruction tokens, visual tokens]
  3. Generate abstracted action tokens (e.g., "Approach(object=sheep, coord=[200,300])")
  4. Condition on self-generated abstracted action
  5. Generate primitive action tokens (e.g., mouse/keyboard events)
  6. De-tokenize to executable environment actions

- Design tradeoffs:
  - Fast (Decoupled) mode: ~3-5× higher FPS, but performance bottlenecked by low-level decoder; requires two-stage training
  - Slow (Unified) mode: Lower FPS (~1), but full VLM reasoning for both planning and execution; single-stage end-to-end training
  - Action space selection: Grounding actions require coordinate annotation pipeline; motion actions are simpler but object-agnostic; latent actions require VQ-VAE pretraining

- Failure signatures:
  - MotionHA on GUI tasks (0.0% ASR): Object-agnostic actions cannot specify interaction targets
  - GroundingHA on off-screen targets: Falls back to generic "Explore" rather than directed navigation
  - Specialist agents on out-of-distribution seeds: Performance drops vs. reported baselines due to narrow training distribution

- First 3 experiments:
  1. Replicate the action space comparison (Table 3): Train three hierarchical agents (GroundingHA, MotionHA, SkillHA) with matched token budgets on the same VPT-derived data; evaluate on 10 embodied, 10 GUI, 10 combat tasks to confirm task-dependent performance.
  2. Ablate CoA vs. direct VLA: Train TextVLA (direct primitive prediction) and GroundingVLA (CoA with grounding actions); compare FPS vs. success rate tradeoff to validate inference-time scaling hypothesis.
  3. Test All-in-One transfer: Prompt the unified OpenHA model to output each specialist's action format; verify that cross-action training improves performance even when constrained to single-format outputs (Table 5 replication).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the Chain of Action (CoA) framework and All-in-One training strategy enable a single agent to operate effectively across disparate domains (e.g., web browsers, mobile GUIs, and robotics) by unifying their respective action spaces?
- Basis in paper: [explicit] The conclusion states that findings "point toward a promising direction where a single, highly capable agent could be trained to operate across disparate domains... by unifying their respective action spaces."
- Why unresolved: The experimental scope is restricted to the Minecraft environment; the framework has not been tested on the diverse real-world or cross-platform domains mentioned.
- What evidence would resolve it: Successful training and evaluation of an OpenHA-style agent on a multi-domain benchmark requiring both digital (GUI) and physical (robotic) interactions.

### Open Question 2
- Question: How can abstracted actions be optimally defined and formulated directly from the continuous stream of low-level experiences, beyond existing human-designed heuristics?
- Basis in paper: [explicit] The introduction explicitly poses this question: "a key question arises: how can abstracted actions be effectively defined and formulated from the continuous stream of low-level experiences?"
- Why unresolved: The paper relies on established tokenizers (VQ-VAE, text, grounding) or rule-based heuristics (Page 8) to generate labels, rather than proposing a method to discover new or superior abstractions automatically.
- What evidence would resolve it: An unsupervised or self-supervised method that discovers a novel action vocabulary that outperforms the current "All-in-One" mixture on the OpenHA benchmark.

### Open Question 3
- Question: Can the trade-off between inference speed (Fast mode) and reasoning capability (Slow/Unified mode) be bridged to allow for real-time, high-performance decision-making?
- Basis in paper: [inferred] Table 4 shows the "Slow" unified mode achieves significantly higher success rates (e.g., 25.6% vs 4.3% in Combat) but operates at less than 1.5 FPS compared to nearly 4 FPS for the "Fast" mode.
- Why unresolved: The paper presents this as a dynamic choice (trade-off) available to practitioners but does not offer a mechanism to combine the high performance of the unified VLA with the efficiency of the decoupled hierarchical approach.
- What evidence would resolve it: A distillation technique or architectural modification that maintains the success rate of the "Slow" mode while achieving the latency of the "Fast" mode.

## Limitations
- Action space design dependencies may be artifacts of Minecraft-specific definitions rather than universal patterns
- Data labeling relies on unspecified rule-based heuristics that could affect reported performance
- Scalability of unified approach beyond tested 7B parameters remains unverified

## Confidence
- Chain of Action framework design: High confidence - clearly specified and mathematically sound
- Task-specific action space performance: Medium confidence - empirically demonstrated but generalizability remains speculative
- All-in-One training superiority: Medium confidence - strong results but lacks complete ablation studies

## Next Checks
1. Apply CoA framework and action space comparison to a non-Minecraft environment to verify generalizability beyond the original domain
2. Systematically reduce expressiveness of each action space to determine minimum abstraction complexity required for each task type
3. Evaluate zero-shot action space switching to test semantic alignment between abstractions rather than learned policy effects