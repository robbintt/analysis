---
ver: rpa2
title: 'Mobile-MMLU: A Mobile Intelligence Language Understanding Benchmark'
arxiv_id: '2503.20786'
source_url: https://arxiv.org/abs/2503.20786
tags:
- mmlu
- mobile
- questions
- mobile-mmlu
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Mobile-MMLU and Mobile-MMLU-Pro are benchmark datasets specifically
  designed for evaluating large language models in mobile contexts, addressing the
  gap in existing benchmarks that primarily target desktop/server environments. The
  benchmarks consist of 16,186 questions across 80 mobile-relevant topics, with Mobile-MMLU-Pro
  providing a more challenging subset of 9,497 questions created through multi-model
  consistency filtering.
---

# Mobile-MMLU: A Mobile Intelligence Language Understanding Benchmark

## Quick Facts
- **arXiv ID**: 2503.20786
- **Source URL**: https://arxiv.org/abs/2503.20786
- **Reference count**: 40
- **One-line primary result**: Mobile-MMLU and Mobile-MMLU-Pro benchmarks evaluate LLMs on mobile-relevant scenarios, achieving average Mobile Relevance Score of 5.88 versus 3.13 for traditional benchmarks.

## Executive Summary
Mobile-MMLU and Mobile-MMLU-Pro are benchmark datasets specifically designed for evaluating large language models in mobile contexts, addressing the gap in existing benchmarks that primarily target desktop/server environments. The benchmarks consist of 16,186 questions across 80 mobile-relevant topics, with Mobile-MMLU-Pro providing a more challenging subset of 9,497 questions created through multi-model consistency filtering. The datasets use order-invariant multiple-choice questions focusing on practical mobile scenarios, with questions deliberately designed to avoid LLM biases such as preference for longer answers or specific answer positions. Evaluation across 1B-9B parameter models reveals significant performance variance, with smaller models showing wider relative performance gaps, highlighting the benchmarks' effectiveness in differentiating mobile-optimized model capabilities.

## Method Summary
The paper introduces Mobile-MMLU, a benchmark for evaluating large language models on mobile-specific tasks, consisting of 16,186 multiple-choice questions across 80 practical domains like First Aid, Travel Planning, and Automotive Care. Questions are generated using GPT-4o and O1-preview, filtered for similarity and consistency across multiple model sizes, and verified by human annotators. Mobile-MMLU-Pro is a more challenging subset of 9,497 questions selected through multi-model consensus filtering to remove easy and ambiguous items. The benchmarks use order-invariant evaluation to control for answer-position and length biases, and are evaluated using the lm-eval-harness framework with zero-shot accuracy as the primary metric.

## Key Results
- Mobile-MMLU achieves average Mobile Relevance Score of 5.88, nearly double that of traditional benchmarks (3.13)
- Smaller models (1-3B parameters) show wider relative performance gaps on Mobile-MMLU compared to MMLU
- Mobile-MMLU-Pro provides more reliable and challenging evaluation by filtering out easy and inconsistent questions through multi-model consensus

## Why This Works (Mechanism)

### Mechanism 1: Mobile-Specific Domain Coverage Discriminates Model Capabilities
- Claim: A benchmark focused on practical mobile-relevant scenarios reveals performance variance between models that general-purpose benchmarks do not capture, particularly for smaller models (1-3B parameters).
- Mechanism: By curating questions across 80 domains tied to real-world mobile interactions (e.g., First Aid, Travel Planning, Automotive Care), the dataset introduces domain distributional shift. This shift unearths capability gaps in models trained primarily on general web text, as their knowledge priors align more closely with traditional academic subjects than with everyday decision-making scenarios.
- Core assumption: Models optimized for general benchmarks may not transfer proportionally to mobile-specific use cases; the knowledge required is different in kind (practical, contextual), not just difficulty.
- Evidence anchors:
  - [abstract] "Mobile-MMLU and Mobile-MMLU-Pro are benchmark datasets specifically designed for evaluating large language models in mobile contexts... The datasets use order-invariant multiple-choice questions focusing on practical mobile scenarios."
  - [section 6.2] "The wider performance variance observed in Mobile-MMLU, particularly among smaller models, highlights its effectiveness in differentiating model capabilities under mobile constraints."
- Break condition: If model performance on Mobile-MMLU perfectly correlates with MMLU scores across all model sizes (e.g., R² > 0.95), the mechanism of discriminative domain shift would not hold.

### Mechanism 2: Multi-Model Consistency Filtering Increases Benchmark Difficulty and Reliability
- Claim: Removing questions that are either too easy (answered correctly by small models) or inconsistently answered by state-of-the-art large models (GPT-4o, Claude-3.5, Gemini-2.0) produces a subset (Mobile-MMLU-Pro) that is more challenging and yields more stable rankings.
- Mechanism: The filtering process acts as a discriminability amplifier. Easy questions are removed to prevent ceiling effects. Questions with inconsistent model predictions are removed to reduce evaluation noise (ambiguous or multi-correct items). The result is a benchmark where variance in scores more closely reflects genuine differences in model reasoning rather than measurement artifact.
- Core assumption: Consensus among strong models (GPT-4o, Claude-3.5, Gemini-2.0) is a reasonable proxy for ground-truth correctness and question quality.
- Evidence anchors:
  - [abstract] "Mobile-MMLU-Pro provides a more challenging subset of 9,497 questions created through multi-model consistency filtering."
  - [section 4] "This strategy enhances test difficulty and ensures better reliability and consistency of the evaluation process."
- Break condition: If inter-model ranking consistency is lower on Mobile-MMLU-Pro than on the full Mobile-MMLU (e.g., Spearman rank correlation < 0.8 between evaluation runs), the filtering would have failed to improve reliability.

### Mechanism 3: Controlling for Answer-Position and Option-Length Bias Improves Evaluation Fairness
- Claim: Deliberately designing multiple-choice options so that incorrect answers are equal in length or longer than the correct answer, and randomizing answer positions, reduces systematic selection biases, particularly in smaller LLMs.
- Mechanism: Smaller models often exhibit superficial heuristics, such as preferring longer options or specific positions. By neutralizing these surface-level cues, the benchmark forces the model to rely on semantic understanding rather than statistical artifacts. This yields accuracy scores that more faithfully represent knowledge.
- Core assumption: The observed performance inflation from positional/length heuristics is substantial enough to mask true capability differences.
- Evidence anchors:
  - [abstract] "Questions deliberately designed to avoid LLM biases such as preference for longer answers or specific answer positions."
  - [section 3.1] "We observe that LLMs (especially mobile-level LLMs) tend to select the longest option more frequently. To mitigate this, we adjust the length of incorrect options to be similar to or longer than the GT answer."
- Break condition: If ablation tests (Tables 3 & 4) showed no significant accuracy variance when correct answers are systematically placed in positions A-D or made shorter than distractors, then the bias hypothesis and the value of this control would be invalidated.

## Foundational Learning

- Concept: **Multiple-Choice Question (MCQ) Evaluation with Order Invariance**
  - Why needed here: The entire Mobile-MMLU benchmark is a multiple-choice test. Understanding that "order invariance" means the evaluation result should not change if you shuffle A/B/C/D is crucial for interpreting the robustness of reported scores.
  - Quick check question: If a model scores 70% when the correct answer is always "A", but 55% when it's always "D", is the benchmark order-invariant for that model?

- Concept: **Model Parameter Scale (1B vs 9B)**
  - Why needed here: The paper explicitly evaluates models from 1B to 9B parameters and draws conclusions based on scale. You must grasp that "B" stands for billions of parameters, a primary indicator of model capacity and memory footprint, which directly impacts mobile deployability.
  - Quick check question: A 2B parameter model is more likely to fit in the RAM of a current smartphone than a 9B parameter model. True or False?

- Concept: **Distributional Shift / Domain Shift**
  - Why needed here: The core argument is that Mobile-MMLU is "different" from MMLU. This difference is a shift in the distribution of topics and question styles (practical vs academic). Understanding that a model's performance can change simply by changing the domain of the questions is key to understanding the paper's motivation.
  - Quick check question: A model trained solely on English literature articles is evaluated on a benchmark of Japanese technical manuals. Would you expect its performance to change? What kind of shift is this?

## Architecture Onboarding

- Component map: Source Data Curation (WikiHow, Stack Exchange, Reddit) -> Generation Engine (GPT-4o, O1-preview) -> Filtering Pipeline (Similarity Filter -> Consistency Filter) -> Human Annotation Loop -> Evaluation Harness (lm-eval-harness) -> Analysis Modules (MRScore, order-bias ablation)

- Critical path:
  1. Question Generation (using LLMs)
  2. Similarity Filtering (automated, deterministic)
  3. Human Verification (bottleneck, requires human input)
  4. Multi-Model Consistency Filtering (expensive, requires running multiple SOTA models)
  5. Benchmark Finalization (Mobile-MMLU and Mobile-MMLU-Pro)

- Design tradeoffs:
  - Question Volume vs. Quality/Consistency: The full Mobile-MMLU has more questions (16k) for broad coverage, while Mobile-MMLU-Pro has fewer (9k) but is more reliable and challenging. Choose the latter for quick, robust testing of strong models; the former for comprehensive evaluation.
  - Cost of Multi-Model Filtering: Running GPT-4o, Claude, and Gemini on the full dataset for Pro version creation is expensive and slow. The tradeoff is a more stable, discriminative benchmark vs. a cheaper but noisier one.
  - Human in the Loop: 10% manual sampling balances quality assurance against the cost and time of annotating 100% of the data. It does not guarantee the entire dataset is error-free.

- Failure signatures:
  - High Variance on Re-evaluation: If running `lm-eval-harness` on the same model multiple times yields accuracy differences > 2-3%, it suggests the benchmark may still contain ambiguous items or the model is non-deterministic without fixed seeds.
  - Positional Bias in Results: If ablation shows models consistently scoring higher when correct answers are in position A, the "order-invariant" design has failed for that model class.
  - Ceiling Effects on Pro Version: If strong models (e.g., Qwen2.5-7B, Gemma-2-9B) all score >95% on Mobile-MMLU-Pro, the filtering for difficulty was insufficient.

- First 3 experiments:
  1. Reproduce Baseline Scores: Run evaluation on Mobile-MMLU and Mobile-MMLU-Pro for 3-5 of the reported models (e.g., Llama-3.2-3B, Qwen2.5-7B, Gemma-2-9B) using `lm-eval-harness` to verify your setup and understand the performance range. Check for the reported wider variance in smaller models.
  2. Order-Bias Ablation: Take the full Mobile-MMLU set and create 4 variants where the correct answer is systematically placed in positions A, B, C, and D for all questions. Evaluate a small model (e.g., Gemma-2-2B) on all 4. Compare the variance in scores to the paper's findings to test the bias hypothesis yourself.
  3. Correlation Analysis: Evaluate a new model (not in the paper) on both MMLU and Mobile-MMLU. Calculate the Spearman rank correlation between its scores across topics. Is the ranking of its abilities (e.g., good at math, bad at first aid) consistent across the two benchmarks? This tests the domain shift claim.

## Open Questions the Paper Calls Out
None

## Limitations
- The benchmarks only evaluate zero-shot, order-invariant MCQ performance, leaving open questions about how models perform on open-ended reasoning tasks or when given contextual hints.
- The MRScore is computed by GPT-4o as judge, introducing potential circularity if the judge shares biases with models being evaluated.
- The paper does not report statistical significance testing for performance differences between model sizes, making it difficult to assess whether observed variances are meaningful or due to sampling noise.

## Confidence
- **High Confidence**: The benchmarks are technically well-constructed with explicit controls for answer-position and length biases; the reported accuracy ranges and MRScore comparisons are internally consistent.
- **Medium Confidence**: The claim that Mobile-MMLU better discriminates mobile-specific capabilities is supported by observed performance variance, but lacks external validation against other mobile-domain benchmarks.
- **Low Confidence**: The assertion that Mobile-MMLU-Pro's filtering process yields more reliable rankings depends heavily on the assumption that multi-model consensus equals ground-truth correctness—a claim not empirically validated in the paper.

## Next Checks
1. **Replicate Ablation Tests**: Run the order-invariance ablation on a small model (e.g., Gemma-2-2B) across all four systematic answer-position variants and four random permutations. Compare observed variance to the paper's reported <3% threshold to verify bias mitigation.
2. **External Correlation Analysis**: Evaluate a new, unreported model on both MMLU and Mobile-MMLU. Compute topic-level Spearman rank correlations between the two benchmarks to test whether the claimed domain shift produces divergent ability rankings.
3. **MRScore Inter-Rater Reliability**: Have a small group of human annotators rate a random sample of Mobile-MMLU and MMLU questions on the same 1-10 practical relevance scale. Compare inter-annotator agreement and average scores to GPT-4o's MRScore to assess potential judge bias.