---
ver: rpa2
title: 'LLM Modules: Knowledge Transfer from a Large to a Small Model using Enhanced
  Cross-Attention'
arxiv_id: '2502.08213'
source_url: https://arxiv.org/abs/2502.08213
tags:
- large
- knowledge
- training
- transfer
- qwen
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel approach to knowledge transfer from
  large to small language models using LLM Modules with Enhanced Cross-Attention.
  The proposed architecture leverages a frozen pre-trained model (Qwen2-1.5B) as a
  knowledge source, passing its representations through specialized attention layers
  to a smaller model (GPT-Neo-125M) trained on limited computational resources.
---

# LLM Modules: Knowledge Transfer from a Large to a Small Model using Enhanced Cross-Attention

## Quick Facts
- arXiv ID: 2502.08213
- Source URL: https://arxiv.org/abs/2502.08213
- Authors: Konstantin Kolomeitsev
- Reference count: 9
- One-line result: Achieves response quality comparable to distillation-based methods on Bespoke-Stratos-17k with frozen pre-trained model knowledge transfer

## Executive Summary
This paper introduces LLM Modules with Enhanced Cross-Attention, a novel approach for transferring knowledge from a large frozen model (Qwen2-1.5B) to a smaller model (GPT-Neo-125M) using specialized attention layers. The architecture leverages frozen representations from the large model, passes them through learned projections and gating mechanisms, and integrates them into the smaller model's generation process. Experimental results show significant improvements in generation quality, with training loss decreasing from 13.8 to 1.1 and validation loss from 2.3 to 1.1 after 15 epochs on the Bespoke-Stratos-17k dataset.

## Method Summary
The approach freezes Qwen2-1.5B and extracts its hidden representations, which are then projected from 1536 to 768 dimensions through learned linear layers. These representations pass through adapter blocks and gating mechanisms before being attended to by GPT-Neo-125M during generation. The system replaces GPT-Neo's final linear layer to align with Qwen2's vocabulary, enabling coherent output generation. Only cross-attention layers, intermediate layers, and GPT-Neo parameters are trained, while Qwen2 weights remain static, dramatically reducing computational requirements.

## Key Results
- Response quality comparable to distillation-based methods after 15 epochs of training
- Training loss decreased from 13.8 to 1.1, validation loss from 2.3 to 1.1
- Significant improvements in generation quality with more structured and logically coherent responses
- Enabled detailed reasoning capabilities in the small model despite limited computational resources

## Why This Works (Mechanism)

### Mechanism 1: Enhanced Cross-Attention for Dimensional Bridge
- Linear projections reduce Qwen2's 1536-dim representations to GPT-Neo's 768-dim space
- Adapter blocks add non-linearity to preserve semantic structure
- Gating dynamically blends external knowledge with internal computations
- Core assumption: Frozen model representations contain structured semantic patterns that survive dimension reduction
- Break condition: If dimension projection loses critical semantic structure, downstream generation quality degrades

### Mechanism 2: Frozen Source with Selective Trainability
- Only cross-attention layers, intermediate layers, and GPT-Neo parameters are optimized
- Qwen2 weights remain static, preserving pre-trained capabilities
- Dramatically reduces computational requirements during training
- Core assumption: Qwen2-1.5B's pre-trained knowledge is sufficiently general for target tasks
- Break condition: If target task requires knowledge absent from frozen source, performance ceiling is bounded

### Mechanism 3: Tokenizer-Bound Output Alignment
- Replaces GPT-Neo's final linear layer to align with Qwen2's vocabulary
- Input is Qwen2-tokenized; output layer maps to Qwen2 vocabulary space
- Enables coherent generation despite architectural asymmetry
- Core assumption: Shared tokenizer vocabulary between input and output preserves semantic consistency
- Break condition: Vocabulary mismatch produces incoherent or hallucinated outputs

## Foundational Learning

- Concept: Cross-Attention in Transformers
  - Why needed here: Routes external representations into a different model's generation path
  - Quick check question: Can you sketch how queries from GPT-Neo attend to keys/values derived from Qwen2's hidden states?

- Concept: Knowledge Distillation vs. Representation Transfer
  - Why needed here: Differentiates from distillation - this is live representation routing, not student-teacher output matching
  - Quick check question: What's the operational difference between distilling outputs at training time vs. passing frozen representations at inference?

- Concept: Gradient Isolation in Frozen Modules
  - Why needed here: Only specific submodules receive gradients; incorrect freezing breaks learning
  - Quick check question: If all parameters were accidentally set to `requires_grad=True`, what would happen to memory and training stability?

## Architecture Onboarding

- Component map: Qwen2 tokenizer -> Frozen Qwen2 -> Linear projections -> Adapter blocks + Gating -> GPT-Neo attention -> GPT-Neo generation
- Critical path:
  1. Tokenize input with Qwen2 tokenizer (pad_token = eos_token if missing)
  2. Forward through frozen Qwen2 to extract hidden representations
  3. Project 1536→768 dims via learned linear layers
  4. Apply adapter block + gating to blend representations
  5. GPT-Neo attends to blended context and generates response

- Design tradeoffs:
  - Context length: Qwen2 supports 128K tokens, but experiments capped at 4096 for memory constraints
  - Trainable parameter count vs. representation richness: more cross-attention layers = more capacity but higher VRAM
  - Inference latency: Two forward passes required (frozen source + active generator)

- Failure signatures:
  - Training loss plateaus early: Check that cross-attention parameters are actually receiving gradients
  - Incoherent generation: Verify output layer dimension matches Qwen2 vocab size
  - OOM errors: Confirm sequence filtering removes examples >4096 tokens before batching
  - No reasoning improvement: Cross-attention may not be injecting frozen representations effectively

- First 3 experiments:
  1. Train GPT-Neo-125M alone on Bespoke-Stratos-17k to quantify transfer delta
  2. Disable gating (set blend ratio fixed at 0.5) and compare validation loss trajectory
  3. Test 1, 2, 4 cross-attention layers and measure reasoning quality on arithmetic tasks

## Open Questions the Paper Calls Out
None

## Limitations

- Cross-Attention Effectiveness: Validation relies on qualitative assessments rather than quantitative benchmarks
- Generalization Boundaries: Single dataset evaluation (Bespoke-Stratos-17k) limits claims about broader applicability
- Architecture Coupling Fragility: Tight coupling between Qwen2 tokenizer and GPT-Neo decoder creates deployment brittleness

## Confidence

- Training Dynamics Claims: High confidence (concrete, measurable metrics)
- Reasoning Quality Improvements: Medium confidence (primarily qualitative evidence)
- Computational Efficiency Claims: High confidence (straightforward to verify through profiling)

## Next Checks

1. Evaluate the combined model on at least two additional reasoning datasets (GSM8K, HellaSwag) to verify generalization beyond training domain

2. Measure semantic information retention through downstream probing tasks at each cross-attention layer to quantify knowledge survival through 1536→768 projection

3. Systematically disable the gating mechanism in cross-attention layers and compare against baselines to isolate dynamic knowledge blending contribution