---
ver: rpa2
title: 'RSCC: A Large-Scale Remote Sensing Change Caption Dataset for Disaster Events'
arxiv_id: '2509.01907'
source_url: https://arxiv.org/abs/2509.01907
tags:
- image
- disaster
- remote
- sensing
- change
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RSCC, a large-scale remote sensing change
  caption dataset containing 62,351 pre- and post-disaster image pairs with detailed
  change captions. The dataset addresses the gap in existing remote sensing resources
  by providing temporally aligned image pairs with rich textual descriptions of disaster
  impacts across 31 global events including earthquakes, floods, wildfires, and storms.
---

# RSCC: A Large-Scale Remote Sensing Change Caption Dataset for Disaster Events

## Quick Facts
- arXiv ID: 2509.01907
- Source URL: https://arxiv.org/abs/2509.01907
- Reference count: 40
- Creates 62,351 bi-temporal image pairs with detailed change captions for disaster events

## Executive Summary
This paper introduces RSCC, a large-scale remote sensing change caption dataset containing 62,351 pre- and post-disaster image pairs with detailed change captions. The dataset addresses the gap in existing remote sensing resources by providing temporally aligned image pairs with rich textual descriptions of disaster impacts across 31 global events including earthquakes, floods, wildfires, and storms. To validate the dataset's effectiveness, the authors train a specialized vision-language model on RSCC, achieving 58.52% ST5-SCS on the test set. The dataset enables robust training and evaluation of vision-language models for disaster-aware bi-temporal understanding, facilitating more accurate and interpretable analysis of remote sensing imagery.

## Method Summary
The authors construct RSCC using a synthetic annotation pipeline. They leverage existing xBD and EBD datasets containing pre- and post-disaster imagery with building damage annotations, then employ a reasoning-capable Vision-Language Model (QvQ-Max) to generate natural language captions from structured metadata and annotated images. The process involves creating visual prompts by overlaying semantic masks (color-coded bounding boxes) onto post-disaster images, feeding these along with metadata to the VLM, and applying automated post-correction using Qwen2.5-Max to ensure caption consistency with ground truth damage counts and disaster types. The final dataset contains 61,363 training pairs and 988 test pairs across 31 global disaster events.

## Key Results
- RSCC dataset contains 62,351 bi-temporal image pairs with detailed change captions
- Qwen2.5-VL fine-tuned on RSCC achieves 58.52% ST5-SCS on test set
- Visual prompting with semantic masks improves caption quality over text-only approaches
- Automated post-correction achieves 100% disaster type consistency

## Why This Works (Mechanism)

### Mechanism 1: Grounded Reasoning for Synthetic Annotation
Generating high-quality change captions requires grounding a reasoning-capable Vision-Language Model (VLM) with explicit structured metadata rather than relying on zero-shot visual inference alone. The authors use QvQ-Max (a reasoning model) instead of a standard VLM, feeding it both visual inputs (pre-image + annotated post-image) and textual inputs (disaster type + damage descriptions). This bypasses the hallucination common in standard VLMs that struggle with dense spatial details.

### Mechanism 2: Semantic Visual Prompting (Mask Overlays)
Superimposing semantic masks (color-coded bounding boxes) directly onto the post-event image provides a stronger signal for VLMs than text-only context. Instead of asking the model to find the damaged building and estimate the damage, the pipeline color-codes buildings (e.g., Red = Destroyed). The VLM processes this as part of the image pixel map, reducing the visual search burden and anchoring the generated text to specific spatial coordinates.

### Mechanism 3: Context-Aware Post-Correction
Synthetic data generation requires a secondary "Judge" model to enforce consistency between generated text and ground-truth metadata. A secondary LLM (Qwen2.5-Max) reviews the generated captions, checking for "Keyword Mismatch" (e.g., "volcanic ash" in a flood event) and "Count Mismatch." This filters out hallucinations where the visual reasoning model "creates" details not present in the source data.

## Foundational Learning

- **Concept: Bi-temporal Remote Sensing**
  - Why needed here: Unlike standard image captioning, this task requires comparing $T_0$ (Pre) and $T_1$ (Post). The "change" is the target, not the static content.
  - Quick check question: If a model describes a building in the post-image but fails to mention it was absent in the pre-image, has it solved the task? (Answer: No, it missed the temporal delta).

- **Concept: Visual Prompt Engineering**
  - Why needed here: The paper relies on "marking-based visual prompt engineering." Understanding that VLMs can "read" annotations drawn on images is crucial to grasping their pipeline.
  - Quick check question: Why add colored boxes to the image instead of just listing the coordinates in the text? (Answer: Spatial attention in VLMs is often more robust when cues are pixel-aligned rather than symbolically referenced).

- **Concept: Hallucination Mitigation in VLMs**
  - Why needed here: The paper explicitly tackles hallucination (e.g., BLIP-3 repetition, incorrect damage counts).
  - Quick check question: In Section 4.2, why does BLIP-3 score poorly despite being a capable model? (Answer: It suffers from repetition/degeneration, a common failure mode in VLMs when visual signals are weak or ambiguous).

## Architecture Onboarding

- **Component map:** xBD/EBD Datasets (Raw Images + JSON Labels) → Annotator (QvQ-Max API) → Corrector (Qwen2.5-Max API) → RSCC Dataset (Parquet/JSON) → Qwen2.5-VL (Vision Encoder + LLM)

- **Critical path:** The prompt template (Appendix A.6) is the single point of failure. If the instructions to QvQ-Max do not strictly enforce the "News Style" or "Disaster Level" mappings, the resulting dataset will be inconsistent.

- **Design tradeoffs:** Synthetic vs. Human Labels (trading cost/latency for speed/scale), Context Length (72 words average vs. standard RSICD's 12 words).

- **Failure signatures:** Repetition Loop (models like BLIP-3 output "Repeat..." endlessly), Vague Descriptions (models like TEOChat produce short captions missing specific damage details).

- **First 3 experiments:**
  1. Sanity Check Prompt Alignment: Verify "Damage Level" counts in text match colored boxes in image.
  2. Ablate the Corrector: Generate captions without Qwen2.5-Max post-correction step.
  3. Zero-Shot vs. RSCC-Finetuned: Compare ST5-SCS scores between baseline and fine-tuned Qwen2.5-VL.

## Open Questions the Paper Calls Out

1. How can evaluation metrics be developed to accurately assess semantic similarity and factual consistency in multi-image remote sensing captioning? The authors note existing metrics fail in multi-image scenarios and BLEURT is biased by text length.

2. Can MLLMs be architected to naively handle visual-centric tasks like change detection with accuracy comparable to specialized models? The paper notes VLMs yield inferior results compared to specialized models on tasks like change detection.

3. What decoding strategies or architectural modifications are required to mitigate repetitive outputs and hallucinations in open-source MLLMs for remote sensing? Current correction decoding methods provide no obvious boost on the degeneration problem.

## Limitations

- Synthetic data quality uncertainty: No human validation of caption quality beyond automated metrics; relies on proprietary QvQ-Max model capabilities
- Geographic and disaster-type bias: Overrepresentation from xBD (44,136 samples) versus EBD (18,215 samples) limits generalization to other disaster types
- Lack of baseline comparisons: No human-annotated baseline comparisons for performance claims

## Confidence

**High Confidence**: Dataset construction methodology and evaluation pipeline are clearly specified and reproducible.

**Medium Confidence**: Performance claims are credible but lack human-annotated baseline comparisons; visual prompting effectiveness demonstrated but could benefit from ablation studies.

**Low Confidence**: Claims about RSCC uniquely enabling "robust training and evaluation" lack comparison to other approaches; assertion that similar results couldn't be achieved through simpler methods.

## Next Checks

1. **Human Validation Study**: Select 100 random samples from RSCC test set and have human annotators rate caption accuracy on a 3-point scale. Compare human scores with ST5-SCS to validate metric alignment.

2. **Cross-Dataset Generalization Test**: Evaluate the RSCC-trained model on an independent change detection dataset without fine-tuning. Measure generalization to disasters and building types not present in RSCC training data.

3. **Ablation of Visual Prompting**: Generate captions using only textual prompts (no semantic masks) while keeping all other parameters constant. Compare METEOR and ST5-SCS scores to quantify the exact contribution of visual prompting.