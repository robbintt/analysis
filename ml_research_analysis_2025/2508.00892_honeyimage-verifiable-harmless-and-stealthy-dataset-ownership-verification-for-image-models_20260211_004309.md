---
ver: rpa2
title: 'HoneyImage: Verifiable, Harmless, and Stealthy Dataset Ownership Verification
  for Image Models'
arxiv_id: '2508.00892'
source_url: https://arxiv.org/abs/2508.00892
tags:
- verification
- data
- dataset
- honeyimage
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of dataset ownership verification
  for image recognition models, where data owners need to determine whether their
  proprietary image datasets have been used to train third-party models without authorization.
  The proposed HoneyImage method generates imperceptible yet traceable modifications
  of hard samples from private datasets to enable reliable ownership verification.
---

# HoneyImage: Verifiable, Harmless, and Stealthy Dataset Ownership Verification for Image Models

## Quick Facts
- arXiv ID: 2508.00892
- Source URL: https://arxiv.org/abs/2508.00892
- Reference count: 11
- Four benchmark datasets (ISIC, OrganMNIST, EuroSAT, CIFAR-10) and four model architectures demonstrate AUROC scores ranging from 0.89 to 0.99

## Executive Summary
This paper introduces HoneyImage, a novel method for verifying ownership of proprietary image datasets used in training third-party models. The approach generates imperceptible modifications of hard samples from private datasets, enabling reliable ownership verification through black-box model querying. Extensive experiments demonstrate that HoneyImage achieves strong verification performance while maintaining high data integrity, outperforming existing approaches in balancing verification effectiveness and stealthiness.

## Method Summary
HoneyImage operates by first identifying hard samples from a private dataset using cross-entropy loss from out-of-training proxy models. These samples are then modified using projected gradient descent (PGD) to maximize the loss gap between models trained with and without them, while keeping visual changes minimal. The verification process involves querying a suspicious model with these HoneyImages and comparing its loss to a compliant proxy model to make binary ownership decisions. The method uses ε=4, α=0.4, T=20 iterations, and is evaluated across four benchmark datasets and four model architectures.

## Key Results
- Achieves AUROC scores ranging from 0.89 to 0.99 across four benchmark datasets
- Maintains high data integrity with stealthiness (STL) scores above 0.99 and harmlessness (HL) scores above 0.74
- Outperforms existing approaches in balancing verification effectiveness and data integrity
- Remains effective even when suspicious model architecture differs from proxy model

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Selecting hard samples provides stronger verification than random sampling
- Mechanism: Hard samples exhibit larger behavioral differences between models trained with versus without them, amplifying verification signals
- Core assumption: Hardness defined by proxy model transfers to third-party model
- Evidence anchors: [abstract] selective modification of hard samples; [section 3.2.1] hard samples show different behaviors; [corpus] related works emphasize targeted selection
- Break condition: If proxy model's hardness notion doesn't generalize due to distribution shift or architectural mismatch

### Mechanism 2
- Claim: PGD generates imperceptible modifications maximizing loss gap
- Mechanism: Alternating bi-level optimization perturbs samples to increase differential loss while ε-constraint maintains visual integrity
- Core assumption: Loss gap correlates with detectability; ε-bound ensures imperceptibility
- Evidence anchors: [abstract] imperceptible yet traceable modifications; [section 3.2.2] alternating bi-level optimization with ε-constraint
- Break condition: If ε too large, modifications become detectable; if too small, insufficient loss gap

### Mechanism 3
- Claim: Loss comparison enables binary ownership decisions
- Mechanism: Query suspicious model with HoneyImages, compare loss to compliant proxy, apply threshold τ
- Core assumption: Models trained on HoneyImages exhibit consistently lower loss than those trained without
- Evidence anchors: [abstract] reliable ownership verification; [section 3.3] large ΔL indicates sample was seen during training
- Break condition: If suspicious model's training dynamics cause poor generalization to HoneyImages

## Foundational Learning

- **Concept: Hard Samples in Machine Learning**
  - Why needed here: HoneyImage relies on identifying samples difficult for models to learn, producing larger behavioral differences
  - Quick check question: Would you expect misclassified or correctly classified samples to yield higher cross-entropy loss when evaluated on a different but similar model?

- **Concept: Cross-Entropy Loss as Proxy for Sample Difficulty**
  - Why needed here: Method uses cross-entropy loss from out-of-training proxy model to rank and select hardest samples
  - Quick check question: If a sample has high cross-entropy loss on a model not trained on it, does that guarantee it's near a decision boundary?

- **Concept: Black-Box Model Querying**
  - Why needed here: Verification assumes only API access to suspicious model, requiring understanding of extractable information
  - Quick check question: If you can only obtain predicted probability distribution for an input image, can you compute cross-entropy loss?

## Architecture Onboarding

- **Component map**: Proxy model trainer -> Hard sample selector -> HoneyImage generator -> Verification module

- **Critical path**:
  1. Split dataset and train proxy out-models
  2. Score all private samples for difficulty using out-models
  3. Select top N hard samples as candidates
  4. Iteratively optimize each candidate via PGD to produce HoneyImages
  5. Embed HoneyImages into released dataset
  6. At verification time, query suspicious model with HoneyImages, compute loss gap vs. local proxy, and decide

- **Design tradeoffs**:
  - ε (perturbation budget): Larger values improve verification effectiveness but may reduce stealthiness
  - N (number of HoneyImages): More samples increase confidence but raise detection risk and computational cost
  - Proxy model architecture: Stronger models may produce more transferable hardness signals but are more expensive to train

- **Failure signatures**:
  - Low AUROC (<0.80): Insufficient loss gap; consider increasing ε or using stronger proxy model
  - Low STL (<0.95): Modifications too visible; reduce ε or use more perceptually uniform constraints
  - High false positives: Threshold τ too low; adjust based on validation experiments

- **First 3 experiments**:
  1. Reproduce hard sample selection on CIFAR-10 subset by training two proxy models on non-overlapping splits, ranking samples by out-model loss, and visualizing top hard samples
  2. Implement HoneyImage generation for fixed ε and N, measure resulting stealthiness (LPIPS-based STL) and harmlessness (accuracy on infringing model), verify modifications are visually imperceptible
  3. Test verification performance across architectures by generating HoneyImages with one proxy model (WRN-28-2) and evaluating AUROC, TPR, TNR on suspicious models of different architectures (ResNet-18, DenseNet-121, VGG-11)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can HoneyImage design principles be effectively adapted for dataset ownership verification in generative models?
- Basis in paper: [explicit] Section 6 states extending approach to generative models is promising direction
- Why unresolved: Current method optimizes classification loss differences which may not translate to generative objectives
- What evidence would resolve it: Modified framework successfully verifying ownership of datasets used to train generative models with high AUROC

### Open Question 2
- Question: How effective is HoneyImage when applied to Transformer-based architectures like Vision Transformers (ViT)?
- Basis in paper: [explicit] Section 6 notes computational constraints limited evaluation to CNNs, identifying ViT effectiveness as future work
- Why unresolved: ViTs process images as sequences of patches rather than pixels, potentially altering how hard samples and perturbations are learned
- What evidence would resolve it: Experimental results on ViT benchmarks demonstrating verification performance comparable to CNN results

### Open Question 3
- Question: Can protection mechanism detect unauthorized data sharing or leakage outside model training?
- Basis in paper: [explicit] Section 6 highlights dataset misuse includes unauthorized sharing and leakage during storage
- Why unresolved: HoneyImage relies on querying trained model to detect usage; cannot trace data accessed without model training
- What evidence would resolve it: Extended framework identifying proprietary data exposure in data lakes or unauthorized repositories without requiring model query

## Limitations
- Threshold selection process for τ remains unspecified, potentially impacting verification performance
- Effectiveness on non-image domains (text, audio) remains untested
- Iterative bi-level optimization requires computationally expensive retraining for each PGD iteration

## Confidence
- **High**: Core mechanism using hard samples and PGD for ownership verification is technically sound and well-supported
- **Medium**: Claim of strong verification performance (AUROC 0.89-0.99) is supported by experimental results, but threshold selection unclear
- **Low**: Assertion of superiority to existing approaches lacks direct comparative analysis with most relevant baselines

## Next Checks
1. **Threshold Sensitivity Analysis**: Systematically vary τ across range of values and plot ROC curves to determine optimal thresholds and assess robustness
2. **Cross-Architecture Generalization**: Generate HoneyImages using one architecture (ResNet-18) and test verification performance across broader range of target architectures including transformers
3. **Adversarial Robustness**: Evaluate whether HoneyImages remain detectable under common adversarial defenses to assess real-world applicability