---
ver: rpa2
title: Parking Availability Prediction via Fusing Multi-Source Data with A Self-Supervised
  Learning Enhanced Spatio-Temporal Inverted Transformer
arxiv_id: '2509.04362'
source_url: https://arxiv.org/abs/2509.04362
tags:
- parking
- data
- prediction
- availability
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses urban parking availability prediction by integrating
  multi-source transportation data with a self-supervised learning enhanced Spatio-Temporal
  Inverted Transformer (SST-iTransformer). The approach clusters parking lots into
  Parking Cluster Zones (PCZs) and fuses demand features from metro, bus, ride-hailing,
  and taxi data to capture spatial-temporal patterns.
---

# Parking Availability Prediction via Fusing Multi-Source Data with A Self-Supervised Learning Enhanced Spatio-Temporal Inverted Transformer

## Quick Facts
- **arXiv ID**: 2509.04362
- **Source URL**: https://arxiv.org/abs/2509.04362
- **Reference count**: 0
- **Primary result**: Achieves lowest MSE (0.3293) and competitive MAE (0.3523) for parking availability prediction using multi-source data and self-supervised learning

## Executive Summary
This study addresses urban parking availability prediction by integrating multi-source transportation data with a self-supervised learning enhanced Spatio-Temporal Inverted Transformer (SST-iTransformer). The approach clusters parking lots into Parking Cluster Zones (PCZs) and fuses demand features from metro, bus, ride-hailing, and taxi data to capture spatial-temporal patterns. SST-iTransformer employs a dual-branch attention mechanism (Series and Channel Attention) and self-supervised pretraining to improve prediction accuracy for highly volatile parking dynamics. Extensive experiments using real-world data from Chengdu, China, demonstrate that SST-iTransformer outperforms state-of-the-art baselines, achieving the lowest mean squared error (MSE = 0.3293) and competitive mean absolute error (MAE = 0.3523). Ablation studies confirm that ride-hailing and taxi features are most impactful, while spatial correlations among parking lots within PCZs are critical for accurate prediction.

## Method Summary
The SST-iTransformer framework predicts parking availability by clustering parking lots into 30 Parking Cluster Zones (PCZs) and fusing multi-source demand data (metro, bus, ride-hailing, taxi) within each zone. The model uses a dual-branch attention mechanism: Series Attention captures long-term temporal dependencies through patching, while Channel Attention models cross-variate interactions by treating features as tokens. A self-supervised pretraining phase trains the model to reconstruct randomly masked segments of spatio-temporal data, with pretrained weights then fine-tuned for the prediction task. The model predicts 24-hour parking availability (144 steps) given 24 hours of historical data, achieving state-of-the-art performance on real-world Chengdu parking data.

## Key Results
- Achieves lowest MSE (0.3293) and competitive MAE (0.3523) compared to baselines
- Ride-hailing and taxi features show highest impact in ablation studies
- Spatial correlation analysis confirms PCZ clustering improves performance by 12.8%
- Self-supervised pretraining provides significant performance boost over non-pretrained iTransformer-patch

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Grouping parking lots with similar demand patterns improves prediction accuracy by enabling shared context learning.
- Mechanism: Using K-means clustering to create Parking Cluster Zones (PCZs) allows the model to share relevant historical data and context among correlated lots. The Channel Attention branch can then learn which demand signals from one lot are predictive of availability in its neighbors.
- Core assumption: Parking lots in geographic proximity or with similar operational profiles exhibit correlated demand and availability patterns, such that the historical state of one lot is predictive of its peers within the cluster.
- Evidence anchors:
  - [abstract] "The approach clusters parking lots into Parking Cluster Zones (PCZs)... to capture spatial-temporal patterns."
  - [section 4.1.2] "Spatial correlation analysis further confirms that excluding historical data from correlated parking lots within PCZs leads to substantial performance degradation..."
  - [corpus] Weak direct evidence. Related paper "A Cost-Effective Framework for Predicting Parking Availability Using Geospatial Data..." suggests geospatial data is useful, but does not specifically validate the K-means PCZ approach.
- Break condition: The mechanism may fail if parking lots within a PCZ have fundamentally different demand drivers (e.g., one residential, one commercial) not captured by the clustering features, leading to noisy shared context that degrades prediction.

### Mechanism 2
- Claim: A dual-branch attention mechanism captures both long-term temporal dependencies and cross-feature interactions better than a single-path model.
- Mechanism: The **Series Attention** branch uses patching to summarize and attend to long temporal sequences, reducing noise. The **Channel Attention** branch, using an "inverted" dimension approach, treats each feature (e.g., time of day, ride-hailing demand) as a token and applies attention across them, learning which external factors are most relevant at different times.
- Core assumption: Parking availability is governed by two distinct types of dependencies: slow-moving long-term temporal trends and rapid, event-driven cross-feature correlations (e.g., a taxi drop-off spike predicts a parking entry), which are best modeled by separate, specialized pathways.
- Evidence anchors:
  - [abstract] "SST-iTransformer... features an innovative dual-branch attention mechanism: Series Attention captures long-term temporal dependencies... while Channel Attention models cross-variate interactions..."
  - [section 3.5] "...the proposed self-supervised learning enhanced iTransformer-patch (SST-iTransformer) establishes state-of-the-art performance... outperforming... the iTransformer-patch without self-supervised learning based pretraining."
  - [corpus] "MultiPark: Multimodal Parking Transformer..." suggests multimodal transformers are applicable in this domain.
- Break condition: This architecture is more complex and has more parameters. It may overfit on small datasets or when the predictive signal from one branch (e.g., the Channel Attention branch with many exogenous features) is weak or non-existent.

### Mechanism 3
- Claim: Self-supervised pretraining on a pretext task improves the model's learned representations for the main prediction task.
- Mechanism: During a pretraining phase, the model must reconstruct randomly masked segments of both temporal and spatial data. This forces it to learn a robust internal representation of the relationships between different time points and features. These pretrained weights are then fine-tuned for the parking availability prediction task.
- Core assumption: The latent structure of spatio-temporal parking and mobility data is learnable and transferable. A model that is good at "filling in the blanks" (reconstruction) will be a better starting point for forecasting.
- Evidence anchors:
  - [abstract] "SST-iTransformer integrates masking-reconstruction-based pretext tasks for self-supervised spatio-temporal representation learning..."
  - [section 3.5] SST-iTransformer "...outperforming all other baselines, including the iTransformer-patch without self-supervised learning based pretraining."
  - [corpus] Weak direct evidence. No corpus papers directly address self-supervised pretraining for parking prediction.
- Break condition: The benefits depend on the pretext task being sufficiently related to the downstream task. If the masking strategy is not well-designed (e.g., masking patterns that are trivially interpolated), the learned representations may not be useful for forecasting.

## Foundational Learning

- **Concept: Attention Mechanism & Inverted Transformer**
  - Why needed here: This is the core engine of the model. Understanding how the standard attention mechanism works (Query, Key, Value) is non-negotiable. The "inverted" part means swapping the traditional role of time steps and features, which is the key innovation of the `iTransformer` this model builds upon.
  - Quick check question: In a standard Transformer, what are the tokens? In the `iTransformer`'s Channel Attention branch, what become the tokens?

- **Concept: Self-Supervised Learning & Pretext Tasks**
  - Why needed here: The "SST" in `SST-iTransformer`. This is the pretraining strategy that reportedly gives the model its performance edge. You need to understand *why* masking and reconstruction helps the model learn better features without explicit labels.
  - Quick check question: Why would a model that has learned to reconstruct masked traffic data be better at predicting future data?

- **Concept: Multimodal Data Fusion**
  - Why needed here: This is the data fueling the model. The paper claims the fusion of taxi, ride-hailing, bus, and metro data is key. Understanding how these different data sources are integrated (via Algorithm 1) and why some are more predictive than others is critical for data engineering and feature importance analysis.
  - Quick check question: According to the paper, which multimodal feature, when removed, causes the largest degradation in model performance? Why might this be the case?

## Architecture Onboarding

- **Component map**: Input Layer (parking + multimodal features) -> Encoder (Dual-Branch Attention: Series Attention + Channel Attention) -> Self-Supervised Pretraining -> Decoder (FFN) -> Prediction

- **Critical path**:
  1. **Data Prep & Clustering**: The most critical first step is correctly clustering parking lots into PCZs using K-means (Section 2.2.1). This creates the fundamental spatial units the model operates on.
  2. **Feature Fusion**: Implement Algorithm 1 to process and aggregate the multi-source trip data (metro, bus, ride-hailing, taxi) within each PCZ. This creates the "exogenous variables" (`X_t-L:t`).
  3. **Pretraining Loop**: Execute the self-supervised pretraining by masking portions of the input and training the encoder-decoder to reconstruct them (Section 2.3.2). Save the learned weights.
  4. **Fine-tuning Loop**: Load the pretrained weights and train the full model on the supervised prediction task (`Y_t-L:t` -> `Y_t+1:t+T`). This is where you can use the novel attention-preserving fine-tuning strategies (e.g., SAttn-tuning).

- **Design tradeoffs**:
  * **Performance vs. Complexity**: The dual-branch attention and self-supervised pretraining add significant complexity and training time compared to a simple `DLinear` model, but they yield substantial performance gains (MSE 0.3293 vs. 0.5916). The ablation study shows this tradeoff is worthwhile for this high-volatility dataset.
  * **Look-back Window**: A longer look-back window (e.g., 144-288 steps) improves MSE/MAE but can increase MAPE due to instability in modeling near-zero availability. The paper suggests 24-48 hours is a good balance.

- **Failure signatures**:
  * **MAPE Spikes**: If the model is trained with a very long look-back window, expect disproportionately high errors on near-zero availability cases, leading to high MAPE despite low MSE/MAE (Figure 5c). This indicates the model is missing small-scale volatility.
  * **Feature Ablation Sensitivity**: If ride-hailing or taxi data is missing or of poor quality, expect a notable drop in performance (Section 4.1.1). The model is specifically tuned to leverage these features.
  * **Spatial Over-Clustering**: If PCZs are too large or heterogenous, the shared demand features could become noisy, hurting more than helping.

- **First 3 experiments**:
  1. **Baseline Reproduction**: Implement the `iTransformer-patch` model without the self-supervised pretraining on the same Chengdu dataset. Train it on the prediction task directly. This establishes the lower-bound of what the architecture can do without the `SST` component.
  2. **Ablation Study Reproduction**: Train the full `SST-iTransformer` model on the dataset but systematically exclude each multimodal data source (one for metro, bus, ride-hailing, taxi) as described in Section 4.1.1. This validates the data engineering pipeline and confirms the importance of each feature.
  3. **Fine-Tuning Strategy Comparison**: After pretraining the `SST-iTransformer` encoder, run several fine-tuning experiments comparing the paper's novel strategies (SAttn-tuning, CAttn-tuning, Attn-tuning) against standard full fine-tuning and Linear Probe (Section 4.2, Table 5). This will validate the efficiency claims and determine the best adaptation strategy for your compute budget.

## Open Questions the Paper Calls Out
None

## Limitations
- Single-dataset validation (Chengdu, China) limits generalizability to other urban contexts
- Critical implementation details (masking ratios, K-means features, hyperparameters) are omitted
- Computational complexity and parameter count relative to baselines are not discussed
- Self-supervised pretraining benefits lack comparison against alternative pretraining strategies

## Confidence

- **High Confidence**: The dual-branch attention mechanism's architectural design and its role in capturing distinct temporal and cross-feature dependencies is well-specified and internally consistent.
- **Medium Confidence**: The superiority of the full SST-iTransformer model over baselines is supported by ablation studies, but the single-dataset validation limits external validity.
- **Medium Confidence**: The mechanism of PCZ clustering improving predictions by sharing correlated context is plausible and supported by ablation evidence, but the clustering criteria could be more rigorously defined.

## Next Checks

1. **Cross-City Validation**: Apply the SST-iTransformer framework to parking data from a different city (e.g., with different urban layout, parking regulations, or mobility patterns) to test generalizability of the PCZ clustering and model performance claims.

2. **Alternative Pretraining Comparison**: Implement and compare the self-supervised pretraining approach against a standard supervised pretraining or a modern contrastive learning method (e.g., SimCLR) on the same dataset to isolate the specific benefits of the masking-reconstruction pretext task.

3. **Efficiency Benchmarking**: Measure and report the training/inference time and parameter count of the SST-iTransformer relative to the best-performing baseline (e.g., iTransformer-patch) to quantify the computational cost of the performance gains and assess real-world deployment feasibility.