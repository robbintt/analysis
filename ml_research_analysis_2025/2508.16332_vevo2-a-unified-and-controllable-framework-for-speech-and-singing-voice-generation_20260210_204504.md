---
ver: rpa2
title: 'Vevo2: A Unified and Controllable Framework for Speech and Singing Voice Generation'
arxiv_id: '2508.16332'
source_url: https://arxiv.org/abs/2508.16332
tags:
- speech
- singing
- voice
- prosody
- vevo2
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Vevo2 is a unified, controllable framework for speech and singing
  voice generation. It introduces a prosody tokenizer that captures melody and prosody
  from speech, singing, and instrumental sounds, and a content-style tokenizer that
  encodes linguistic content, prosody, and style while enabling timbre disentanglement.
---

# Vevo2: A Unified and Controllable Framework for Speech and Singing Voice Generation

## Quick Facts
- **arXiv ID:** 2508.16332
- **Source URL:** https://arxiv.org/abs/2508.16332
- **Reference count:** 40
- **Primary result:** Unified framework for speech/singing generation with prosody tokenization and joint training; achieves superior performance across synthesis, conversion, and editing tasks.

## Executive Summary
Vevo2 presents a unified, controllable framework for speech and singing voice generation that addresses data scarcity and versatile controllability challenges. The framework introduces a prosody tokenizer capturing melody and prosody from speech, singing, and instrumental sounds, and a content-style tokenizer encoding linguistic content, prosody, and style while enabling timbre disentanglement. Through speech-singing joint training with explicit and implicit prosody learning strategies and multi-objective post-training, Vevo2 achieves superior performance across synthesis, conversion, and editing tasks for both speech and singing, enabling unique applications like humming-to-singing and instrument-to-singing.

## Method Summary
Vevo2 employs a two-stage architecture: tokenizers (VQ-VAE) and generative models (autoregressive + flow matching). The prosody tokenizer processes chromagram features at 6.25Hz with 512 vocabulary size, while the content-style tokenizer processes concatenated chromagram and Whisper features at 12.5Hz with 16,384 vocabulary size. The autoregressive stage uses Qwen2.5-0.5B with 50% probability of explicit prosody learning (EPL) versus implicit learning (IPL). The flow matching stage employs a Vevo-style Transformer generating mel-spectrograms from content-style tokens conditioned on reference timbre. Multi-objective post-training uses GRPO with intelligibility and prosody similarity rewards.

## Key Results
- Unified modeling shows mutual benefits across speech and singing tasks, with improvements in individual metrics (e.g., MCD, CMOS)
- Novel applications demonstrated: humming-to-singing and instrument-to-singing conversion
- Superior performance across synthesis, conversion, and editing tasks compared to task-specific models

## Why This Works (Mechanism)
Vevo2's unified framework enables cross-task knowledge transfer and shared representations, addressing data scarcity by leveraging both speech and singing datasets. The prosody tokenizer's ability to capture melodic and rhythmic patterns from multiple audio sources (speech, singing, instruments) provides a unified representation space. The content-style tokenizer enables explicit timbre disentanglement while preserving linguistic and prosodic content. Joint training with both EPL and IPL strategies allows the model to learn prosody representations both explicitly and implicitly, improving generalization.

## Foundational Learning
- **VQ-VAE Tokenization**: Discrete representation learning for efficient sequence modeling; needed for autoregressive generation with discrete tokens. Quick check: Monitor codebook usage and reconstruction quality.
- **Flow Matching**: Continuous generative modeling for high-quality audio synthesis; needed for generating mel-spectrograms from discrete tokens. Quick check: Compare FID scores between autoregressive and flow matching outputs.
- **Reinforcement Learning with GRPO**: Reward-based fine-tuning for controllability; needed to optimize for specific metrics like intelligibility and prosody similarity. Quick check: Monitor reward convergence and generation quality during RL training.

## Architecture Onboarding
**Component Map:** Data → Tokenizers (Prosody + Content-Style) → AR Model → FM Model → Vocoder → Output
**Critical Path:** Tokenization → AR Generation → FM Generation → Vocoding
**Design Tradeoffs:** Unified framework vs. task-specific optimization; discrete vs. continuous representations; explicit vs. implicit prosody learning
**Failure Signatures:**
- Codebook collapse in tokenizers (usage <10% of codes)
- AR model ignoring prosody tokens (flat pitch in output)
- Degraded intelligibility during RL (WER spike)
**Three First Experiments:**
1. Train prosody tokenizer and evaluate reconstruction quality on chromagram features
2. Train AR model with EPL only and assess prosody token integration
3. Implement FM model and compare mel-spectrogram generation quality with baseline

## Open Questions the Paper Calls Out
None

## Limitations
- Several architectural details underspecified (CNN layer dimensions in tokenizers, REPA alignment specifics)
- Source separation pipeline for singing data referenced but not detailed
- Novel applications rely on external models (INTP, M4Singer) with dependencies not fully detailed

## Confidence
- **High Confidence:** Unified framework concept, prosody tokenizer's multi-source capability, content-style tokenizer's timbre disentanglement
- **Medium Confidence:** Mutual benefits of unified modeling, effectiveness of EPL/IPL strategies
- **Medium Confidence:** Novel applications (humming-to-singing, instrument-to-singing) demonstrated but dependent on external models

## Next Checks
1. **Tokenizer Codebook Stability:** Monitor codebook usage during training; adjust commitment loss weight or implement EMA updates if usage drops below 10%
2. **Prosody Token Integration in AR Model:** Generate samples with extreme pitch contours; increase EPL ratio if output ignores prosody prompts
3. **RL Reward Model Generalization:** Evaluate WER on held-out intelligibility test set after GRPO; increase KL penalty if WER increases significantly