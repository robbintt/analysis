---
ver: rpa2
title: 'Clean Code, Better Models: Enhancing LLM Performance with Smell-Cleaned Dataset'
arxiv_id: '2508.11958'
source_url: https://arxiv.org/abs/2508.11958
tags:
- code
- smells
- dataset
- smell
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the issue of code smells in large language
  model (LLM) training data and their impact on LLM-generated code quality. It proposes
  SmellCC, an LLM-based tool that automatically refactors and removes code smells
  using prompt engineering techniques like role designation, chain-of-thought reasoning,
  and few-shot learning.
---

# Clean Code, Better Models: Enhancing LLM Performance with Smell-Cleaned Dataset
## Quick Facts
- arXiv ID: 2508.11958
- Source URL: https://arxiv.org/abs/2508.11958
- Reference count: 40
- Key outcome: Code smell cleaning improves LLM code quality and downstream task performance

## Executive Summary
This study addresses the issue of code smells in large language model (LLM) training data and their impact on LLM-generated code quality. It proposes SmellCC, an LLM-based tool that automatically refactors and removes code smells using prompt engineering techniques like role designation, chain-of-thought reasoning, and few-shot learning. Applying SmellCC to the CodeSearchNet-Python dataset eliminated 91.6% of code smells, creating a smell-cleaned dataset. Fine-tuning LLMs (DeepSeek-V2 and Qwen-Coder) on this dataset significantly reduced code smells in generated code by 79.6% and 83.1% respectively, while improving downstream task performance (code completion and search) by up to 12.2% and 4.3%.

## Method Summary
The study uses SonarQube to detect code smells in the CodeSearchNet-Python dataset, then applies SmellCC (DeepSeek-Coder-V2 with structured prompts) to refactor and remove smells. The cleaned dataset is used to fine-tune LLMs, and the resulting models are evaluated for smell reduction in generated code and improved performance on downstream tasks (code completion and search). Functional correctness is verified using test suites from 50 Python repositories.

## Key Results
- SmellCC eliminated 91.6% of code smells from CodeSearchNet-Python
- Fine-tuned LLMs showed 79.6-83.1% reduction in code smells in generated outputs
- Code completion Pass@1 improved by 5.2-12.2% and code search MRR by 0.7-4.3% using smell-cleaned datasets

## Why This Works (Mechanism)
### Mechanism 1
Code smells in training data propagate to LLM-generated outputs through learned syntactic patterns. When training data contains structural anti-patterns, models internalize these as valid patterns and reproduce them during generation. The 79.6-83.1% reduction in smells demonstrates that models trained on cleaned data exhibit fewer smell patterns than those trained on contaminated data.

### Mechanism 2
Structured prompt engineering (Role + Chain-of-Thought + Few-shot) enables LLMs to perform refactoring with >90% effectiveness. Role designation establishes task context, Chain-of-Thought decomposes complex refactoring into sequential reasoning steps, and Few-shot examples provide pattern anchors. The 96.8% effectiveness rate shows these components work synergistically, with CoT alone achieving 96.0% and Few-shot adding 0.8 percentage points.

### Mechanism 3
Smell-cleaned training data improves downstream task performance by reducing semantic noise. Code smells introduce irrelevant lexical patterns that distort embedding representations. Removing smells improves semantic consistency between code and natural language descriptions, enhancing retrieval accuracy for code search and reducing spurious completions. The 12.2% improvement in Pass@1 and 4.3% in MRR validates this mechanism.

## Foundational Learning
- **Code Smells**: Understanding the 10 smell types and their distinction from bugs is essential for interpreting SmellCC's effectiveness metrics. *Quick check*: Can you name three smell types with 100% effectiveness and one where SmellCC struggles?
- **Prompt Engineering (CoT + Few-shot + Role Designation)**: SmellCC's core innovation is prompt-based refactoring, not architecture changes. *Quick check*: Why does CoT alone (96.0%) outperform Few-shot alone (93.4%)?
- **LLM Fine-tuning vs. In-context Learning**: The paper uses in-context learning (SmellCC) to create training data, then applies fine-tuning to improve models—two distinct techniques. *Quick check*: What evidence suggests the original dataset degraded model performance?

## Architecture Onboarding
- **Component map**: Raw Dataset → SonarQube (smell detection) → SmellCC (LLM + prompts) → Smell-Cleaned Dataset → Fine-tuning Pipeline → Evaluated LLM
- **Critical path**: Verify SonarQube detection accuracy → Validate functional correctness via test suites (91.3% pass rate) → Measure downstream task improvements (Pass@1, MRR/NDCG)
- **Design tradeoffs**: Temperature=0 ensures deterministic refactoring but may miss valid alternatives; local refactoring limits Long Parameter List effectiveness (60.9%); adapter pattern maintains backward compatibility
- **Failure signatures**: Long Parameter List refactoring fails to propagate changes to call sites; High Cognitive Complexity extraction is incomplete with hallucination bugs; SonarQube false positives due to version-specific rules
- **First 3 experiments**: 1) Baseline ablation reproducing 93.2% → 96.0% → 96.8% progression; 2) Audit 20 failed Long Parameter List refactorings; 3) Train models on synthetic vs. natural clean data to isolate smell impact

## Open Questions the Paper Calls Out
1. Can the prompt engineering framework generalize effectively to other programming languages without extensive manual adaptation?
2. How can LLM-based refactoring tools handle architectural smells like Long Parameter List requiring global analysis?
3. How does LLM-based refactoring performance compare systematically to traditional rule-based refactoring tools?

## Limitations
- Limited scope to 10 SonarQube-detected smell types, potentially missing other quality issues
- Cross-file refactoring limitations, particularly for Long Parameter List smells (60.9% success rate)
- Dataset generalization uncertainty to other programming languages or domain-specific codebases

## Confidence
- **High confidence**: Downstream task performance improvements (Pass@1 5.2-12.2%, MRR 0.7-4.3%) are well-supported
- **Medium confidence**: Smell removal effectiveness (91.6%) may be inflated by undetected false positives and limited test coverage
- **Medium confidence**: The causal relationship between training data smells and generated code smells is plausible but not definitively proven

## Next Checks
1. Apply SmellCC to CodeSearchNet datasets for Java, Go, and JavaScript to verify cross-language performance consistency
2. Track long-term maintenance of models trained on smell-cleaned data to assess maintainability over extended development cycles
3. Test zero-shot transfer by evaluating whether smell-cleaned models handle novel structural anti-patterns not present in training data