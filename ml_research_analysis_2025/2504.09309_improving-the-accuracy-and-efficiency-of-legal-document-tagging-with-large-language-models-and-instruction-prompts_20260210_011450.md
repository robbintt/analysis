---
ver: rpa2
title: Improving the Accuracy and Efficiency of Legal Document Tagging with Large
  Language Models and Instruction Prompts
arxiv_id: '2504.09309'
source_url: https://arxiv.org/abs/2504.09309
tags:
- legal
- label
- labels
- legal-llm
- classi
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Legal-LLM, a novel approach for legal multi-label
  classification that leverages the instruction-following capabilities of Large Language
  Models (LLMs). The method reframes the classification task as a structured generation
  problem, prompting the LLM to directly output relevant legal categories for a given
  document.
---

# Improving the Accuracy and Efficiency of Legal Document Tagging with Large Language Models and Instruction Prompts

## Quick Facts
- arXiv ID: 2504.09309
- Source URL: https://arxiv.org/abs/2504.09309
- Authors: Emily Johnson; Xavier Holt; Noah Wilson
- Reference count: 30
- Primary result: Legal-LLM achieves micro-F1 0.83 and macro-F1 0.76 on POSTURE50K, outperforming strong baselines

## Executive Summary
This paper introduces Legal-LLM, a novel approach for legal multi-label classification that leverages the instruction-following capabilities of Large Language Models (LLMs). The method reframes the classification task as a structured generation problem, prompting the LLM to directly output relevant legal categories for a given document. Legal-LLM is evaluated on two benchmark datasets, POSTURE50K and EURLEX57K, using micro-F1 and macro-F1 scores. The experimental results demonstrate that Legal-LLM outperforms strong baseline models, including traditional methods and other Transformer-based approaches. Ablation studies and human evaluations validate the effectiveness of the approach, particularly in handling label imbalance and generating relevant and accurate legal labels.

## Method Summary
Legal-LLM reframes legal multi-label classification as a text generation task by instructing an LLM to directly output applicable legal categories. The model is first pre-trained on a large corpus of legal documents to capture domain-specific language patterns, then fine-tuned on the target datasets using a weighted loss function based on inverse label frequency. The approach treats the classification problem as sequence-to-sequence generation, where the model receives an instruction prompt concatenated with the document and generates a comma-separated list of relevant labels.

## Key Results
- Legal-LLM achieves micro-F1 0.83 and macro-F1 0.76 on POSTURE50K
- Legal-LLM achieves micro-F1 0.80 and macro-F1 0.71 on EURLEX57K
- Weighted loss function improves performance on rare legal categories
- Ablation studies confirm the effectiveness of the generation-based approach

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Reframing multi-label classification as a structured generation task improves label prediction accuracy compared to discriminative approaches.
- **Mechanism:** Instead of learning independent binary classifiers or complex label correlation models, the LLM generates label sequences directly. This allows the model to leverage learned semantic relationships between legal concepts and output coherent label sets through its generative training.
- **Core assumption:** The instruction-following capabilities of LLMs transfer to structured legal label generation; the model's pre-training captures sufficient legal domain knowledge.
- **Evidence anchors:** [abstract] "We reframe the multi-label classification task as a structured generation problem, instructing the LLM to directly output the relevant legal categories" [section III] "We reframe the task of assigning multiple labels to a legal document as a text generation problem, guided by a specific instruction"

### Mechanism 2
- **Claim:** Weighted loss functions based on inverse label frequency improve performance on rare legal categories.
- **Mechanism:** By assigning higher loss weights to infrequent labels, the optimizer prioritizes learning representations for underrepresented classes, counteracting the model's tendency to favor majority labels during gradient descent.
- **Core assumption:** Inverse frequency weighting correctly captures the importance of rare labels; overfitting to rare labels does not occur.
- **Evidence anchors:** [abstract] "demonstrates particular effectiveness in handling label imbalance through weighted loss functions" [section IV.C, Table II] Ablation shows weighted loss improves micro-F1 (0.79→0.80) and macro-F1 (0.69→0.71) on EURLEX57K

### Mechanism 3
- **Claim:** Legal domain pre-training provides semantic representations that improve document-label matching.
- **Mechanism:** Pre-training on legal corpora exposes the model to domain-specific vocabulary, sentence structures, and conceptual patterns before fine-tuning, enabling better document understanding during the classification task.
- **Core assumption:** The pre-training corpus is sufficiently similar to downstream task documents; domain knowledge transfers without catastrophic forgetting.
- **Evidence anchors:** [section III] "In our experiments, we utilize a model that has been further pre-trained on a large corpus of legal documents to better capture the nuances of legal language" [section IV, Table I] Legal-LLM (0.83/0.80 micro-F1) outperforms LegalBERT (0.81/0.78), suggesting generation architecture contributes beyond domain pre-training alone

## Foundational Learning

- **Concept: Multi-label classification vs. multi-class classification**
  - **Why needed here:** Legal documents inherently belong to multiple categories simultaneously (e.g., "contract law" + "intellectual property" + "international trade"). Single-label approaches would miss critical tags.
  - **Quick check question:** If a document discusses both employment discrimination and workplace safety violations, should the model output one label or both? What does "macro-F1" measure that "micro-F1" doesn't?

- **Concept: Label imbalance and its impact on loss functions**
  - **Why needed here:** Legal datasets have power-law label distributions (few common labels, many rare ones). Standard cross-entropy loss causes models to ignore rare but legally important categories.
  - **Quick check question:** Why does macro-F1 (0.71) lag behind micro-F1 (0.80) on EURLEX57K? What would happen if you trained without weighted loss on a dataset where 95% of documents have label A but only 1% have label B?

- **Concept: Instruction tuning and task reframing**
  - **Why needed here:** The paper's core innovation is treating classification as generation. Understanding how instruction-following models work explains why this approach is viable.
  - **Quick check question:** How does prompting an LLM with "Identify all applicable legal categories" differ mathematically from training a classifier with a softmax output layer? What are the trade-offs in terms of output space flexibility?

## Architecture Onboarding

- **Component map:** Base LLM -> Tokenizer -> Instruction prompt template -> Forward pass -> Autoregressive generation -> Post-processing -> Label validation
- **Critical path:** Document preprocessing (tokenization, truncation) → Prompt construction (instruction + document text) → Forward pass through fine-tuned LLM → Autoregressive generation of label sequence → Post-processing: split by delimiter, validate against label vocabulary, filter invalid outputs → Return structured label set
- **Design tradeoffs:** Generation vs. classification (flexible label sets vs. direct probabilities), prompt phrasing (engineering effort vs. performance gain), document length (performance drops for long documents, truncation loses information)
- **Failure signatures:** Invalid label generation (requires validation layer), empty outputs (fallback to threshold-based prediction), over-prediction (precision drops, calibrate generation temperature), rare label collapse (inspect loss weights, consider focal loss)
- **First 3 experiments:** 1) Reproduce ablation: Train Legal-LLM with and without weighted loss on EURLEX57K subset, verify macro-F1 improvement. 2) Prompt sensitivity test: Compare 3-5 prompt variations on held-out validation set, measure micro-F1, macro-F1, and output validity rate. 3) Error analysis on rare labels: Extract documents with low-frequency labels, compare Legal-LLM vs. LegalBERT predictions, identify systematic failure modes.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can hierarchical attention mechanisms or advanced truncation strategies effectively mitigate the performance degradation observed in Legal-LLM when processing documents exceeding 512 tokens?
- **Basis in paper:** [explicit] The authors explicitly state in Section V that "future work could explore techniques like document truncation or hierarchical attention mechanisms to further improve performance on very long legal documents."
- **Why unresolved:** The experimental analysis (Table VI) shows a performance drop for long documents (Macro-F1 0.69) compared to short ones (Macro-F1 0.73), and the quadratic complexity of self-attention is cited as a limiting factor.
- **What evidence would resolve it:** A follow-up study benchmarking Legal-LLM variants equipped with hierarchical attention or sliding windows on a dataset of exclusively long legal documents (>1024 tokens).

### Open Question 2
- **Question:** What alternative techniques beyond inverse-frequency weighted loss can be integrated to address extreme label imbalance in generative multi-label classification?
- **Basis in paper:** [explicit] Section V notes that future work could explore "more sophisticated techniques for handling extreme label imbalance," despite the current success of weighted loss.
- **Why unresolved:** While the weighted loss improved Macro-F1, the authors acknowledge that label imbalance remains a significant challenge in legal datasets, suggesting the current solution may not be optimal for the rarest categories.
- **What evidence would resolve it:** Experiments applying techniques like Focal Loss, data augmentation, or specialized decoding strategies to the generative model, specifically measuring per-class recall on the bottom 5% of frequent labels.

### Open Question 3
- **Question:** How does the inference efficiency and computational cost of the generative Legal-LLM compare to discriminative baselines like LegalBERT?
- **Basis in paper:** [inferred] While the paper title claims improvements in "Efficiency," the experimental results focus exclusively on F1 scores (accuracy) and human evaluation, omitting metrics regarding inference speed, latency, or hardware requirements.
- **Why unresolved:** Generative LLMs typically have higher computational overhead and latency than discriminative encoders (like BERT); without efficiency metrics, the claim of improved efficiency remains unverified.
- **What evidence would resolve it:** Reporting tokens-per-second, average inference time per document, and energy consumption for Legal-LLM against the LegalBERT and DistilRoBERTa baselines.

## Limitations
- **Unknown base LLM architecture:** The paper fails to specify the exact LLM architecture or pre-training corpus size, creating significant reproduction uncertainty
- **Missing efficiency metrics:** Claims of improved efficiency lack supporting data on inference speed, latency, or computational requirements
- **Architecture isolation unclear:** The paper doesn't empirically separate the contribution of instruction-following capabilities from the underlying LLM's general language understanding

## Confidence
**High Confidence:** Ablation study results showing weighted loss improves macro-F1 scores (0.69→0.71 on EURLEX57K) are directly supported by experimental data.

**Medium Confidence:** The mechanism explaining why generation-based approaches work better than discriminative methods is plausible but relies on assumed properties of LLMs that aren't directly validated.

**Low Confidence:** Claims about the exact contribution of instruction-following capabilities versus the underlying LLM's general language understanding are not empirically separated.

## Next Checks
1. **Architecture Isolation Test:** Replicate the Legal-LLM approach using three different base models (LegalBERT, Legal-T5, and a standard instruction-tuned model) to quantify how much performance depends on the specific architecture versus the generation-based approach itself.

2. **Loss Function Sensitivity Analysis:** Systematically compare weighted cross-entropy against alternative imbalance handling methods (focal loss, oversampling, label-dependent margin loss) on the same dataset splits to determine if the chosen weighting scheme is optimal.

3. **Long Document Performance Benchmark:** Conduct controlled experiments measuring performance degradation as document length increases from 256 to 2048 tokens, using both truncation and sliding window approaches to identify the true bottleneck (information loss vs. attention limitations).