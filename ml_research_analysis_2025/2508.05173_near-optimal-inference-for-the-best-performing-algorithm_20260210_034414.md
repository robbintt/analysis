---
ver: rpa2
title: Near Optimal Inference for the Best-Performing Algorithm
arxiv_id: '2508.05173'
source_url: https://arxiv.org/abs/2508.05173
tags:
- algorithms
- algorithm
- where
- page
- theorem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of identifying the best-performing
  algorithm from a collection of machine learning algorithms based on their performance
  across multiple datasets. The core method reformulates this as subset selection
  for multinomial distributions, where the goal is to construct a minimal subset of
  algorithms that contains the most probable winner with high confidence.
---

# Near Optimal Inference for the Best-Performing Algorithm

## Quick Facts
- arXiv ID: 2508.05173
- Source URL: https://arxiv.org/abs/2508.05173
- Reference count: 9
- Identifies minimal subset of algorithms containing the best performer with high confidence

## Executive Summary
This paper addresses the problem of identifying the best-performing algorithm from a collection of machine learning algorithms based on their performance across multiple datasets. The core method reformulates this as subset selection for multinomial distributions, where the goal is to construct a minimal subset of algorithms that contains the most probable winner with high confidence. The author introduces novel asymptotic and finite-sample schemes that significantly improve upon existing methods. The asymptotic approach uses simultaneous one-sided confidence intervals based on the normal approximation, while the finite-sample method employs refined bounds on the infinity norm of the difference between true probabilities and their empirical estimates. The paper establishes matching lower bounds demonstrating the near-optimality of the proposed schemes. Experiments on synthetic and real-world datasets validate the approach, showing favorable performance compared to existing alternatives like rank verification and post-hoc statistical tests.

## Method Summary
The method constructs a minimal subset of algorithms that contains the best-performing algorithm with probability at least 1-δ. Given win counts for A algorithms across n datasets, it computes empirical win frequencies and applies either an asymptotic scheme (using normal approximation and simultaneous one-sided confidence intervals) or a finite-sample scheme (using refined bounds on the infinity norm of probability differences). The asymptotic method requires large sample sizes and unique maximum probabilities, while the finite-sample method works for any sample size and handles multiple maxima. Both methods provide near-optimal subset sizes by establishing matching lower bounds.

## Key Results
- Introduces novel asymptotic and finite-sample schemes for algorithm subset selection
- Proves matching lower bounds demonstrating near-optimality of proposed methods
- Experiments show improved performance over rank verification and post-hoc statistical tests
- Real-world application identifies RF and AdaBoost as top performers in Fernández-Delgado dataset

## Why This Works (Mechanism)

### Mechanism 1: Multinomial Reformulation
- Claim: Identifying the best-performing algorithm can be reduced to subset selection over multinomial distributions.
- Mechanism: Each dataset is treated as an independent trial where one algorithm "wins" (achieves highest accuracy). This converts the comparative study into estimating a multinomial probability vector p, where p_u is the probability algorithm u wins a future dataset. The goal becomes constructing a minimal subset I_δ containing the most probable symbol s = arg max p_u with confidence 1-δ.
- Core assumption: Performance outcomes across datasets are independent draws from a fixed underlying win probability distribution.
- Evidence anchors:
  - [abstract] "This problem is formulated as subset selection for multinomial distributions."
  - [section 3] Defines Nu(X^n) as win counts and ˆp_u as MLE; objective (1) seeks P(s ∈ I_δ) ≥ 1-δ.
  - [corpus] "Who is the Winning Algorithm?" uses similar rank aggregation framing but with different statistical tools.
- Break condition: If dataset outcomes are correlated (e.g., from same domain) or the win probability distribution shifts over time, the multinomial assumption fails and coverage guarantees may not hold.

### Mechanism 2: Simultaneous One-Sided Confidence Intervals (Asymptotic Regime)
- Claim: For large samples, simultaneous one-sided CIs provide near-optimal subset selection with O(1/√n) width.
- Mechanism: Decompose ˆp[1] − ˆp_s = (ˆp[1] − p[1]) + (p_s − ˆp_s). Construct two one-sided CIs: U_{δ/2} for the maximum empirical vs true, and V_{δ/2} for true vs empirical of the winner. By Xiong and Li [2009], p[1] − ˆp[1] is asymptotically normal; Wald's CI gives U_{δ/2} = z_{δ/2}√(p[1](1-p[1])/n). Combined: D_δ = 2z_{δ/2}√(ˆp[1](1-ˆp[1])/n).
- Core assumption: Sample size n is sufficiently large; the maximum probability p[1] is unique and not at boundary (0 or 1).
- Evidence anchors:
  - [section 4.1] Theorem 1 gives D_δ(X^n) = 2z_{δ/2}√(p[1](1-p[1])/n); notes z_{δ/2} ~ √(2 log(2/δ)).
  - [section 4.1] "Xiong and Li [2009] show that p[1] − ˆp[1] ∼ N(0, p[1](1-p[1])/n) for a unique maximum."
  - [corpus] Limited direct corpus evidence for this specific CI construction in ML benchmarking.
- Break condition: When n is small or p[1] ≈ 0 or ≈ 1, normal approximation degrades; finite-sample method required.

### Mechanism 3: Infinity Norm Concentration (Finite Sample Regime)
- Claim: Bounding ||p − ˆp||_∞ via Markov's inequality on central moments yields finite-sample guarantees without large-n assumptions.
- Mechanism: Use E[sup_u |p_u − ˆp_u|^m] ≤ (1/n^m) Σ_u E[(N_u − np_u)^m], where the m-th central moment of Binomial has explicit form (Skorski 2020). Apply Markov: P(||p−ˆp||_∞ ≥ a) ≤ (1/a^m)E[||p−ˆp||_∞^m]. Optimize over even m (Chernoff-type). Theorem 3 further makes this data-dependent via McDiarmid's inequality, replacing p with ˆp plus an ϵ_n correction term.
- Core assumption: Samples are i.i.d.; concentration inequalities apply.
- Evidence anchors:
  - [section 4.2] Theorem 2 gives sup|p_u − ˆp_u| ≤ (1/n)(1/δ)^{1/m}[Σ_k c_{k,m,n} Σ_u p_u^k(1-p_u)^k]^{1/m}.
  - [section 4.2] Corollary 3.1 shows with m* = 2 log(1/δ_1), bound approaches √(exp(1)log(1/δ_1)/n) × [Σ_u (ˆp_u(1-ˆp_u))^{m*/2}]^{1/m*}.
  - [corpus] Kontorovich and Painsky [2024] cited as prior work on infinity norm bounds.
- Break condition: If samples are not independent or bounded differences fail, McDiarmid-based data-dependent bounds do not apply.

## Foundational Learning

- Concept: **Multinomial distribution and maximum likelihood estimation**
  - Why needed here: The entire framework models algorithm wins as draws from a multinomial distribution; understanding p_u = P(algorithm u wins) and its MLE ˆp_u = N_u/n is essential.
  - Quick check question: Given 100 datasets where algorithm A wins 40 times, B wins 35 times, and C wins 25 times, what are the MLE estimates of p_A, p_B, p_C?

- Concept: **Confidence intervals and coverage probability**
  - Why needed here: The method constructs subsets I_δ with guaranteed coverage P(s ∈ I_δ) ≥ 1-δ; understanding trade-offs between δ, subset size, and sample size is critical.
  - Quick check question: If δ = 0.05 and you construct 100 independent confidence sets I_δ, how many should fail to contain s on average?

- Concept: **Concentration inequalities (Markov, McDiarmid)**
  - Why needed here: Finite-sample bounds rely on Markov's inequality applied to high-order moments and McDiarmid's inequality for data-dependent bounds.
  - Quick check question: McDiarmid's inequality requires bounded differences—what is the maximum change in ˆp_u when one sample changes?

## Architecture Onboarding

- Component map:
  Input -> Estimation -> Bounds -> Subset Selection -> Output
  1. Input: Benchmark results matrix → reduce to win counts N_u for each algorithm.
  2. Estimation: Compute empirical win frequencies ˆp_u = N_u/n and identify top empirical performer t = arg max ˆp_u.
  3. Bounds: Choose regime—asymptotic (Theorem 1) or finite-sample (Theorem 4). Compute D_δ or T_δ respectively.
  4. Subset Selection: I_δ = {u | ˆp_u ≥ ˆp[1] − D_δ} (or T_δ for finite-sample).
  5. Output: Minimal subset containing best-performing algorithm with confidence ≥ 1-δ.

- Critical path:
  1. Verify data format: each dataset must have a single "winner" (handle ties per Section 6).
  2. Choose δ (typical: 0.05 or 0.1) and regime (use finite-sample if n < 50 or p values near boundary).
  3. For finite-sample: select m (default: m* = 2 log(1/δ_1)) and split δ = δ_1 + δ_2.
  4. Compute bound and subset; report size and coverage guarantee.

- Design tradeoffs:
  - **Asymptotic vs finite-sample**: Asymptotic is simpler (closed-form) but requires large n; finite-sample is more robust but computationally heavier (moment computations).
  - **Choice of m**: Higher m tightens bound but increases variance; m* = 2 log(1/δ) balances this (Corollary 3.1).
  - **δ split (finite-sample)**: δ_1 controls moment bound, δ_2 controls data-dependence; paper notes δ_2 term is often negligible.
  - **Multiple maxima handling**: Finite-sample method handles non-unique ˆp[1] and p[1]; asymptotic only handles non-unique ˆp[1].

- Failure signatures:
  - **Empty subset I_δ = ∅**: Indicates bound D_δ > ˆp[1]; likely n too small or δ too aggressive.
  - **Full subset I_δ = X**: All algorithms included; win probabilities too close or insufficient data to distinguish.
  - **Coverage failure in validation**: If ground truth s is known, and s ∉ I_δ more than δ fraction of times—check i.i.d. assumption or implementation error.
  - **Numerical instability in moment computation**: For very large m, c_{k,m,n} coefficients may overflow; use log-domain computation.

- First 3 experiments:
  1. **Synthetic validation**: Generate data from known Zipf and near-uniform distributions (Section 7.1); verify empirical coverage matches 1-δ over 1000 trials; plot E[|I_δ|] vs n.
  2. **Sensitivity to m selection**: For finite-sample method with fixed n and δ, sweep m ∈ {2, 4, 8, ..., 2⌈log(1/δ)⌉} and measure resulting subset size; confirm m* is near-optimal.
  3. **Real benchmark application**: Apply to Fernández-Delgado et al. [2014] classifier comparison (Section 7.2); compare subset {RF, AdaBoost} vs rank verification (no significance) vs Nemenyi test (28 of 36 not significantly worse).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can an asymptotic inference scheme be derived for the scenario where multiple algorithms share the highest probability of winning (multiple global maxima)?
- Basis in paper: [explicit] Page 18 states regarding the multiple maxima case: "Unfortunately, we cannot apply our asymptotic results for this setup... the distribution of $\hat{p}_{[1]} - p_s$... depends on the cardinality of S, which is typically unknown."
- Why unresolved: The asymptotic normality of the maximum likelihood estimator (MLE) difference relies on a unique maximum; the finite-sample workaround exists, but an asymptotic solution for this specific case is currently missing.
- Evidence would resolve: A derivation of the limiting distribution of the empirical frequencies that holds uniformly over the number of optimal algorithms, allowing for the construction of asymptotic confidence intervals in the non-unique case.

### Open Question 2
- Question: How can the subset selection framework be adapted for applications outside of machine learning benchmarks, such as political polling or voting systems?
- Basis in paper: [explicit] Page 25 states: "We consider additional applications and their required adaptions for our future work. For example, consider a poll with A candidates... Our proposed scheme introduces a powerful and statistical valid tool for this exact purpose."
- Why unresolved: While the mathematical formulation (multinomial subset selection) is identical, the practical "required adaptions" for specific domains (e.g., handling evolving voter preferences vs. static algorithm performance) are not explored.
- Evidence would resolve: A study or set of guidelines applying Theorems 1 or 4 to a specific non-ML domain, addressing domain-specific constraints like time-varying probabilities or smaller sample sizes.

### Open Question 3
- Question: Can the inference scheme be improved by utilizing the magnitude of performance differences (e.g., accuracy margins) rather than reducing results to binary win/loss counts?
- Basis in paper: [inferred] Page 7 defines the observation $X_i$ strictly as the event an algorithm "won," discarding the continuous performance metrics (like raw Accuracy) discussed in the introduction.
- Why unresolved: The current method maps continuous performance data to a discrete multinomial distribution, potentially losing statistical power and information regarding *how much* better one algorithm is than another.
- Evidence would resolve: A modified version of the finite-sample bounds that integrates the variance of the actual performance scores, resulting in tighter confidence sets than the purely count-based method.

## Limitations
- Asymptotic method requires large sample sizes and unique maximum probabilities
- Finite-sample method requires computing high-order binomial moments, with coefficients only provided up to m=6
- Method assumes independent and identically distributed datasets across the benchmark
- Method reduces continuous performance metrics to binary win/loss outcomes, potentially losing information

## Confidence
- **High Confidence**: The multinomial reformulation and the core objective of subset selection with coverage guarantees (Theorem 1 and Theorem 4) - these follow standard statistical methodology with clear proofs.
- **Medium Confidence**: The finite-sample bound construction and the specific coefficient values - while theoretically sound, implementation requires careful handling of moment computations and potential numerical instability.
- **Medium Confidence**: The experimental validation on real benchmarks - the paper demonstrates favorable performance compared to alternatives, but the exact preprocessing steps for handling ties and multiple algorithm versions are not fully specified.

## Next Checks
1. **Coverage Validation**: Run 1,000 synthetic experiments with known ground truth (using Zipf and near-uniform distributions) to empirically verify that the asymptotic and finite-sample methods achieve the claimed coverage probability (1-δ) across varying sample sizes (n=50 to 200).

2. **Coefficient Implementation**: Implement the general formula for binomial central moment coefficients c_{k,m,n} for arbitrary m (beyond m=6) and verify against the provided table values. Test the sensitivity of finite-sample bounds to different m selections.

3. **Robustness to Tie-Breaking**: Apply the method to synthetic datasets with controlled tie frequencies and to real benchmark datasets (e.g., Fernández-Delgado et al.) using different tie-breaking strategies to assess the impact on subset selection and coverage.