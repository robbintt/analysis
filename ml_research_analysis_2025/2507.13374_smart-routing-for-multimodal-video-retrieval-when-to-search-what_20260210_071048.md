---
ver: rpa2
title: 'Smart Routing for Multimodal Video Retrieval: When to Search What'
arxiv_id: '2507.13374'
source_url: https://arxiv.org/abs/2507.13374
tags:
- routing
- retrieval
- text
- query
- modalities
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ModaRoute introduces LLM-based intelligent routing for multimodal
  video retrieval, dynamically selecting optimal modalities (ASR, OCR, visual) based
  on query intent analysis. The system reduces computational overhead by 41% while
  achieving 60.9% Recall@5, compared to exhaustive 3.0-modality search baseline.
---

# Smart Routing for Multimodal Video Retrieval: When to Search What

## Quick Facts
- **arXiv ID**: 2507.13374
- **Source URL**: https://arxiv.org/abs/2507.13374
- **Reference count**: 15
- **Primary result**: LLM-based intelligent routing achieves 60.9% Recall@5 with 41% computational reduction

## Executive Summary
ModaRoute introduces an intelligent LLM-based routing system for multimodal video retrieval that dynamically selects optimal modalities (ASR, OCR, visual) based on query intent analysis. The system reduces computational overhead by 41% while achieving 60.9% Recall@5 on 1.8M video clips, compared to exhaustive 3.0-modality search baseline. By routing queries to an average of 1.78 modalities per request versus 3.0, ModaRoute maintains competitive performance with 40.5% Recall@1 and 60.9% Recall@5. The router achieves 86.5% accuracy in including the ground-truth modality among predictions, demonstrating that intelligent routing provides practical efficiency gains without sacrificing retrieval quality. The approach addresses the challenge of scaling multimodal retrieval systems by reducing infrastructure costs while maintaining effectiveness for real-world deployment.

## Method Summary
ModaRoute employs GPT-4.1 LLM router to analyze query intent and select optimal retrieval modalities from ASR transcripts (87% coverage), OCR scene text (34% coverage), and visual captions (100%). The system uses SigLIP SoViT-400m/384 large for visual embeddings, mean-pooled at 1 fps, and OpenAI text-embedding-3-large for text embeddings. Queries are routed through LLM analysis with JSON-formatted output specifying selected modalities, then parallel index searches are performed and combined via linear rank fusion. The approach targets efficient retrieval by reducing average modalities queried from 3.0 to 1.78 while maintaining 40.5% Recall@1 and 60.9% Recall@5 on 2,195 hand-annotated queries over 1.8M video clips.

## Key Results
- Achieves 60.9% Recall@5 while reducing computational overhead by 41% compared to exhaustive search
- Router includes ground-truth modality in predictions with 86.5% accuracy across 2,195 annotated queries
- Average modalities queried reduced from 3.0 to 1.78 per request, demonstrating significant efficiency gains
- Single-modality OCR routing shows weakness at 42.9% accuracy due to subtitle-speech overlap ambiguity

## Why This Works (Mechanism)
ModaRoute works by leveraging LLM-based query intent analysis to dynamically route each retrieval request to only the most relevant modalities. The GPT-4.1 router interprets query semantics to determine whether the user is seeking spoken content (ASR), visual text elements (OCR), or general visual descriptions. This intelligent routing reduces redundant computations by avoiding searches across all modalities when only one or two are likely to contain relevant information. The linear rank fusion combines results from selected modalities while preserving modality-specific strengths, and the system maintains competitive retrieval performance despite processing fewer modalities per query. The approach effectively balances computational efficiency with retrieval effectiveness by matching query intent to modality capabilities.

## Foundational Learning

**Multimodal video retrieval**: Combining multiple data modalities (text, audio, visual) to enable comprehensive video search. Needed because single-modality approaches miss cross-modal information. Quick check: Verify that each video clip has ASR, OCR, and visual representations available.

**LLM-based query routing**: Using language models to analyze query intent and direct it to appropriate retrieval channels. Needed to reduce computational overhead while maintaining relevance. Quick check: Test router accuracy on held-out queries with known ground-truth modalities.

**Linear rank fusion**: Combining ranked results from multiple sources using weighted scoring functions. Needed to integrate modality-specific retrieval results coherently. Quick check: Compare fusion performance against simple concatenation or max-pooling approaches.

**Modality coverage analysis**: Understanding which videos contain which types of multimodal data. Needed to handle missing data gracefully during routing. Quick check: Verify coverage statistics (ASR 87%, OCR 34%, Visual 100%) match dataset characteristics.

## Architecture Onboarding

**Component map**: User Query -> GPT-4.1 Router -> [ASR Index, OCR Index, Visual Index] -> Linear Rank Fusion -> Top-K Results

**Critical path**: Query processing time is dominated by LLM routing decision and parallel index searches. The router adds latency but reduces total retrieval time by eliminating unnecessary searches.

**Design tradeoffs**: 
- Routing accuracy vs computational efficiency: More accurate routing enables greater efficiency but requires more sophisticated LLM analysis
- Single-modality vs multi-modality routing: Balancing between avoiding redundant searches and ensuring no relevant modalities are missed
- Index coverage vs completeness: Accepting partial OCR coverage (34%) to enable practical routing decisions

**Failure signatures**: 
- Low OCR routing accuracy (42.9%) indicates router confusion between subtitles and scene text
- Visual-OCR ambiguity when captions describe visible text elements
- LLM routing errors when queries contain ambiguous intent not matching any single modality

**First experiments**:
1. Test router accuracy on queries with known ground-truth modality labels to measure routing effectiveness
2. Compare retrieval performance between routed (1.78 modalities) and exhaustive (3.0 modalities) search on held-out queries
3. Measure computational overhead reduction by timing parallel vs sequential index searches

## Open Questions the Paper Calls Out

**Joint Routing and Query Optimization**: The paper notes that query optimization showed inconclusive results and represents a promising direction requiring specialized fine-tuning. The current system generates optimized queries per modality but only evaluates routing decisions, not the combined effect of simultaneous routing and query reformulation.

**OCR vs Subtitle Distinction**: With OCR routing accuracy at only 42.9% in single-modality mode, the paper identifies "Subtitle Overlap" as a primary error source where OCR text often contains literal speech subtitles, creating ambiguity between ASR and OCR modalities.

**LLM Routing Scalability**: As the number of available modalities grows beyond 3, the routing decision space becomes increasingly complex. The paper questions whether LLM-based approaches can scale to handle larger modality sets effectively.

**Adaptive Routing from Feedback**: The paper lists adaptive routing as a future direction, noting that learning from user feedback and retrieval success patterns could improve routing decisions over time, as the current system is stateless and analyzes each query independently.

## Limitations

- Dataset unavailability blocks independent validation of reported metrics and routing accuracy
- Incomplete GPT-4.1 prompt specification creates uncertainty about prompt engineering choices
- OCR routing weakness (42.9%) due to subtitle-speech overlap remains unresolved
- Visual caption generation and keyframe selection lack implementation details

## Confidence

**High**: Computational efficiency gains (41% reduction) through reduced modalities per query
**Medium**: Routing accuracy (86.5%) and retrieval performance (60.9% Recall@5) cannot be independently verified
**Low**: Dataset access and complete implementation details are unavailable

## Next Checks

1. Obtain access to the AVC-1.8M dataset or construct a comparable multimodal video corpus with ground-truth modality annotations for retrieval queries
2. Implement the complete GPT-4.1 router prompt with explicit modality definitions and JSON output parsing, testing routing accuracy on available multimodal video datasets
3. Validate the keyframe selection algorithm for visual embeddings by implementing diversity maximization techniques (e.g., clustering-based keyframe selection) and measuring impact on visual retrieval performance