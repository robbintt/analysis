---
ver: rpa2
title: Efficient Supernet Training with Orthogonal Softmax for Scalable ASR Model
  Compression
arxiv_id: '2501.18895'
source_url: https://arxiv.org/abs/2501.18895
tags:
- training
- selection
- supernet
- each
- subnet
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of efficiently training multiple
  ASR models of varying sizes to meet diverse hardware constraints. The authors propose
  a novel method called OrthoSoftmax, which uses multiple orthogonal softmax functions
  to learn binary masks for identifying optimal subnets within a supernet, avoiding
  resource-intensive search.
---

# Efficient Supernet Training with Orthogonal Softmax for Scalable ASR Model Compression

## Quick Facts
- arXiv ID: 2501.18895
- Source URL: https://arxiv.org/abs/2501.18895
- Reference count: 40
- The paper proposes OrthoSoftmax, a method using orthogonal softmax functions to learn binary masks for subnet selection in ASR supernet training

## Executive Summary
This paper addresses the challenge of efficiently training multiple ASR models of varying sizes to meet diverse hardware constraints. The authors propose a novel method called OrthoSoftmax, which uses multiple orthogonal softmax functions to learn binary masks for identifying optimal subnets within a supernet, avoiding resource-intensive search. OrthoSoftmax enables flexible and precise subnet selection based on various criteria and levels of granularity. The results on Librispeech and TED-LIUM-v2 datasets show that FLOPs-aware component-wise selection achieves the best overall performance, with WERs for all model sizes comparable to or slightly better than individually trained models.

## Method Summary
The authors propose OrthoSoftmax, a method for efficient supernet training in ASR model compression. OrthoSoftmax uses multiple orthogonal softmax functions to learn binary masks for identifying optimal subnets within a supernet, avoiding resource-intensive search. The method enables flexible and precise subnet selection based on various criteria (e.g., FLOPs, latency) and levels of granularity (e.g., component-wise, head-wise). The supernet training pipeline incorporates the learned masks during training, and the resulting subnets can be selected based on desired constraints without additional search.

## Key Results
- FLOPs-aware component-wise selection achieves the best overall performance across all model sizes
- WERs for all model sizes are comparable to or slightly better than individually trained models
- Convolutions are retained most frequently while upper-level MHSA layers are often pruned

## Why This Works (Mechanism)
OrthoSoftmask works by using multiple orthogonal softmax functions to learn binary masks for identifying optimal subnets within a supernet. The orthogonal softmax functions force a strict ranking of component importance, enabling precise subnet selection based on various criteria (e.g., FLOPs, latency). By incorporating these masks during supernet training, the method avoids resource-intensive search at inference time and allows for flexible subnet selection.

## Foundational Learning

### Orthogonal Softmax
- **Why needed:** To enable multiple, distinct rankings of component importance without interference
- **Quick check:** Verify that softmax outputs for different tasks are truly orthogonal (dot product near zero)

### Binary Mask Learning
- **Why needed:** To enable differentiable subnet selection during training
- **Quick check:** Confirm that mask values converge to binary (0 or 1) during training

### Component-Wise Granularity
- **Why needed:** To balance precision of subnet selection with computational efficiency
- **Quick check:** Compare performance vs. training time across different granularity levels

## Architecture Onboarding

### Component Map
Orthogonal Softmax -> Binary Mask Generator -> Component Mask Application -> Supernet Training

### Critical Path
Orthogonal Softmax functions → Mask learning → Component masking → Model training

### Design Tradeoffs
- Granularity vs. training efficiency: finer granularity enables better compression but increases training complexity
- Number of orthogonal functions vs. ranking precision: more functions enable better multi-criteria selection
- Mask stability vs. adaptability: fixed masks are efficient but less flexible to changing constraints

### Failure Signatures
- Performance degradation with increased granularity in baseline methods
- Inconsistent mask values preventing convergence
- Subnets failing to meet target constraints

### 3 First Experiments
1. Compare OrthoSoftmax performance across different granularity levels
2. Validate mask convergence and binary nature through training curves
3. Test FLOPs-aware selection against other constraint types

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to two datasets (Librispeech and TED-LIUM-v2) without extensive validation across diverse ASR tasks or languages
- Comparison framework only benchmarks against individually trained models rather than existing supernet training methods or NAS approaches
- Lacks ablation studies isolating the contribution of orthogonal softmax versus other architectural choices in the supernet design

## Confidence
- Scalable and efficient ASR model compression claim: Medium confidence due to limited comparative analysis and focus on specific datasets
- FLOPs-aware component-wise selection achieves optimal performance: Supported by experimental results but lacks statistical significance testing
- Identifying interesting patterns in component selection: Descriptive rather than analytically substantiated

## Next Checks
1. Evaluate OrthoSoftmax across multiple ASR tasks including multilingual and low-resource scenarios to assess generalizability
2. Conduct head-to-head comparisons with established supernet training methods (e.g., One-Shot NAS, DARTS variants) on identical datasets measuring both accuracy and training efficiency
3. Perform ablation studies varying the number of orthogonal softmax functions and alternative mask learning mechanisms to isolate the specific contribution of the proposed approach