---
ver: rpa2
title: 'elsciRL: Integrating Language Solutions into Reinforcement Learning Problem
  Settings'
arxiv_id: '2507.08705'
source_url: https://arxiv.org/abs/2507.08705
tags:
- instruction
- language
- agent
- instructions
- environment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: elsciRL is a Python library that enables reinforcement learning
  agents to benefit from language instructions generated by large language models
  (LLMs). The library integrates LLM-based language adapters for state descriptions,
  instruction planners for generating task steps, and validation systems for self-completing
  instructions.
---

# elsciRL: Integrating Language Solutions into Reinforcement Learning Problem Settings

## Quick Facts
- arXiv ID: 2507.08705
- Source URL: https://arxiv.org/abs/2507.08705
- Reference count: 19
- One-line primary result: LLM-generated instructions improve RL agent performance in three environments, though generalization remains untested

## Executive Summary
elsciRL is a Python library that integrates large language models into reinforcement learning to enable instruction-following agents. The system uses LLM-based language adapters to transform environment states into textual descriptions, instruction planners to decompose user commands into sub-goals, and validation systems for self-completing instructions. Experiments across Classroom, FrozenLake, and Maze environments show that instruction-following improves agent performance, with DQN agents achieving 0.28 average reward in Classroom and 0.99 in Double-T maze with instructions versus lower baseline scores. The work demonstrates that LLM-generated instructions can enhance RL training, though further evaluation across more environments is needed.

## Method Summary
The approach integrates LLMs into RL through three mechanisms: state description adapters that convert symbolic states to language observations, unsupervised instruction completion via embedding similarity to identify sub-goal states, and instruction-following reward shaping during training. Agents (Q-Learning and DQN) are trained with additional reward for reaching LLM-predicted sub-goal states, then tested without this shaping. The system caches LLM outputs per state, uses cosine similarity between instruction and state language embeddings for matching, and validates predictions through iterative refinement loops. The library provides a Flask GUI for configuration and execution.

## Key Results
- DQN with instructions achieved 0.28 average reward in Classroom vs. -0.64 baseline
- DQN with instructions achieved 0.99 average reward in Double-T maze vs. -0.10 baseline
- Q-learning agents showed improvement with instructions in most cases, but got stuck at sub-goals in Maze
- LLM adapter alone did not improve performance in most test cases

## Why This Works (Mechanism)

### Mechanism 1: Language Adapter for State Transformation
LLM-generated language descriptions transform symbolic/numeric environment states into textual observations that may help agents generalize across similar states. The adapter applies f: s ∈ S → o ∈ O, converting environment states to language observations with outputs cached per state. Core assumption: Language representations capture meaningful abstractions for transfer learning. Evidence: [abstract] "LLM-based language adapters for state descriptions" and [section 2.1] "The LLM adapter will cache its generation so that it will use the first output generated for each state." Break condition: If language descriptions introduce noise or lose critical state information, performance degrades.

### Mechanism 2: Unsupervised Instruction Completion via Embedding Similarity
Cosine similarity between instruction embeddings and observed state language embeddings identifies sub-goal states without human labeling. Instructions are encoded alongside language descriptions from all observed states, with maximum cosine similarity finding the best match state for each instruction step. Core assumption: Embedding similarity correlates with semantic relevance between instructions and states that complete them. Evidence: [section 2.2.2] "a cosine similarity measurement is used between the current instruction and the language produced by the adapter across all observed states" and [Table 2] showing predicted states matching intended outcomes. Break condition: If embedding space doesn't align with task-relevant semantics, predicted sub-goals will be incorrect.

### Mechanism 3: Instruction-Following Reward Shaping
Additional reward for reaching LLM-predicted sub-goal states during training accelerates learning and improves final policy performance. Instructions are decomposed into steps, each step's predicted completion state becomes a sub-goal, and agents receive bonus reward for reaching these states during training (removed at test time). Core assumption: Sub-goals predicted by the LLM planner represent useful intermediate objectives that guide exploration toward optimal policies. Evidence: [Table 3] DQN with instructions achieved 0.28 in Classroom vs. -0.64 baseline; 0.99 in Double-T maze vs. -0.10 baseline. Break condition: If sub-goals are incorrect or cause short-term optimization at the expense of long-term goals, performance degrades.

## Foundational Learning

- **Reinforcement Learning Basics (Q-Learning, DQN)**: The framework trains standard RL agents as base learners that receive shaped rewards from language guidance. Why needed: Q-Learning updates value function through temporal difference learning, while DQN extends this with function approximation. Quick check: Can you explain how Q-learning updates its value function and how DQN extends this with function approximation?

- **Text Embeddings and Semantic Similarity**: The instruction completion mechanism relies on encoding language into vectors and computing cosine similarity to match instructions to states. Why needed: Vector representations enable mathematical comparison of semantic content. Quick check: What does cosine similarity measure between two embedding vectors, and what are its limitations?

- **Partial Observability in MDPs**: Language adapters reduce the state space (|S| ≥ |O|), creating a partially observable setting where different states may map to identical observations. Why needed: This affects theoretical guarantees of standard RL algorithms. Quick check: How does partial observability affect the theoretical guarantees of standard RL algorithms?

## Architecture Onboarding

- **Component map**: Environment -> LLM Adapter -> Observed States Cache -> LLM Planner -> Instruction Matcher -> LLM Validator -> RL Agent -> GUI
- **Critical path**: 1) Install: `pip install elsciRL` 2) Launch GUI: `from elsciRL import App; App.run()` at http://127.0.0.1:5000 3) Select application → generate/load observed states 4) Configure agent + adapter 5) Input instructions (or load published ones) 6) Run experiment → view results in GUI and saved figures
- **Design tradeoffs**: Adapter choice (rule-based vs. LLM-based), state space knowledge (pre-collecting observed states vs. on-the-fly), agent type (tabular vs. function approximation), instruction validation (LLM validation adds robustness but increases latency)
- **Failure signatures**: Agent stuck at sub-goal (instructions cause short-term optimization), zero improvement with LLM adapter (language doesn't capture task-relevant abstractions), poor test performance after good training (agent overfits to instruction reward shaping)
- **First 3 experiments**: 1) Replicate Table 3 baseline: Run Classroom with Q-Learning and DQN, no adapter, no instructions 2) Ablate LLM adapter: Run same agents with LLM adapter only (no instructions) 3) Test instruction quality: Provide hand-crafted optimal instructions vs. LLM-generated ones

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the instruction-following approach generalize to more complex environments beyond the three simple GridWorld/Maze settings tested?
- Basis in paper: [explicit] The authors state "further work is required to evaluate the use of these approaches and expand the number of application settings" and "further evaluation must be conducted including more instructions and applications."
- Why unresolved: Only three environments were tested (Classroom, FrozenLake, Maze), all relatively simple with small state spaces and fixed start/goal positions.
- What evidence would resolve it: Performance results across more diverse RL benchmarks (e.g., Atari, robotics simulators, or text-based games) showing consistent improvement patterns.

### Open Question 2
- Question: How does the choice of language encoder and LLM model affect instruction-following performance?
- Basis in paper: [explicit] The authors list "evaluation of alternatives for the: 2) language transformers, 3) LLMs models" as opportunities for future work.
- Why unresolved: All experiments used a fixed combination (Llama3.2 with MiniLMv6 encoder), so the sensitivity to these choices is unknown.
- What evidence would resolve it: Systematic ablation studies comparing different encoder/LLM combinations on the same environments.

### Open Question 3
- Question: Why does the LLM-generated language adapter fail to improve performance in most test cases despite rule-based adapters showing benefits?
- Basis in paper: [inferred] The authors note "We also find that the LLM adapter's language does not support improvements in most cases" without providing a clear explanation for this counterintuitive result.
- Why unresolved: The paper does not analyze whether the failure stems from noisy descriptions, loss of critical state information, or embedding misalignment.
- What evidence would resolve it: Analysis comparing LLM-generated vs. rule-based descriptions for semantic fidelity and their corresponding embedding space structures.

## Limitations
- Unsupervised instruction completion lacks direct validation against human-labeled ground truth
- Hyperparameter choices for both RL agents and LLM components are not fully specified
- Approach assumes state spaces are enumerable and observable states can be pre-collected, limiting continuous or partially observable environments

## Confidence
- **High**: Basic RL framework integration and language adapter functionality
- **Medium**: Performance improvements in Classroom and Double-T maze environments
- **Low**: Generalization claims across diverse environments and reliability of unsupervised instruction matching

## Next Checks
1. **Ground Truth Validation**: Manually label a subset of instruction-state pairs in Classroom and compare LLM-predicted matches against human annotations to quantify accuracy
2. **Hyperparameter Sensitivity**: Systematically vary key RL hyperparameters (learning rate, exploration schedule) and LLM generation parameters to identify robust configurations
3. **Cross-Environment Generalization**: Apply the same instruction-following approach to at least two additional environments with different characteristics (e.g., continuous control, stochastic transitions) to test broader applicability