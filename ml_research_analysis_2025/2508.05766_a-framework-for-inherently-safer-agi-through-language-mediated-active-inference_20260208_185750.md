---
ver: rpa2
title: A Framework for Inherently Safer AGI through Language-Mediated Active Inference
arxiv_id: '2508.05766'
source_url: https://arxiv.org/abs/2508.05766
tags:
- language
- inference
- active
- safety
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a framework for safe Artificial General Intelligence
  (AGI) by integrating Active Inference principles with Large Language Models (LLMs).
  The approach uses natural language to represent and manipulate beliefs, goals, and
  world models, enabling transparency and human oversight.
---

# A Framework for Inherently Safer AGI through Language-Mediated Active Inference

## Quick Facts
- **arXiv ID:** 2508.05766
- **Source URL:** https://arxiv.org/abs/2508.05766
- **Reference count:** 40
- **Primary result:** Proposes language-mediated Active Inference architecture for transparent, corrigible AGI with bounded rationality and compositional safety

## Executive Summary
This paper presents a framework for safer Artificial General Intelligence by integrating Active Inference principles with Large Language Models (LLMs). The approach uses natural language to represent and manipulate beliefs, goals, and world models, enabling transparency and human oversight. The architecture implements a multi-agent system where agents self-organize according to Active Inference principles, with preferences and safety constraints flowing through hierarchical Markov blankets. The framework addresses traditional AI safety challenges by providing explicit separation of beliefs and preferences, bounded rationality through resource-aware free energy minimization, and compositional safety through modular agent structures.

## Method Summary
The framework implements a multi-agent Active Inference system where LLMs back agent cognition through natural language representation of the generative model. Agents use dual-analysis cycles for perception (computing Variational Free Energy through Complexity-Accuracy and Divergence-Evidence formulations) and planning (selecting actions by minimizing Expected Free Energy through Information-Pragmatic and Ambiguity-Risk formulations). The architecture features Dynamic Memory with LLM engine, prompt context, RAG, and tools, enabling direct tool use, subcontracting to known agents, or recruiting new ones. The system is validated using the Abstraction and Reasoning Corpus (ARC) benchmark to test safety properties including corrigibility and stability.

## Key Results
- Natural language representation enables direct human inspection of beliefs and preferences
- Active Inference principles provide intrinsic risk aversion and self-correction mechanisms
- Multi-agent architecture supports compositional safety through modular containment
- Framework addresses traditional AI safety challenges including reward hacking and opaque reasoning

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Representing the generative model in natural language (instead of numeric matrices) allows for direct human inspection and modification of beliefs and preferences.
- **Mechanism:** The architecture replaces traditional opaque AIF matrices (A, B, C, D) with natural language hypotheses and value statements. This allows the system to reason over text, making the "thought process" legible to human operators without post-hoc interpretability tools.
- **Core assumption:** LLMs can faithfully simulate the causal structure of a generative model and maintain logical consistency when manipulating these text-based beliefs.
- **Evidence anchors:**
  - [abstract]: "...uses natural language to represent and manipulate beliefs, goals, and world models, enabling transparency and human oversight."
  - [section 3.1]: "In contrast, natural language offers a structured, hierarchical representation that is both human-interpretable and computationally flexible."
  - [corpus]: Weak direct evidence; related work (An Approach to Technical AGI Safety) emphasizes modularity and containment, but does not validate language-mediated internals.
- **Break condition:** If LLMs suffer from "language game decoupling" (generating safe-sounding text uncorrelated with actual behavior), transparency is lost.

### Mechanism 2
- **Claim:** Intrinsic risk aversion and self-correction emerge from minimizing Variational Free Energy (VFE) and Expected Free Energy (EFE), avoiding the reward hacking common in Reinforcement Learning (RL).
- **Mechanism:** Agents select actions to minimize expected surprise (ambiguity) and risk (divergence from preferences). This drives them to resolve uncertainty (exploration) and seek preferred outcomes (exploitation) while avoiding high-entropy, unpredictable states that might indicate danger.
- **Core assumption:** The drive to minimize surprise is functionally equivalent to or safer than maximizing a defined reward function.
- **Evidence anchors:**
  - [section 2]: "Active Inference agents seek to maintain a stable, predictable world model... contrasts with the reward-maximizing behavior of reinforcement learning agents."
  - [section 3.3]: "Minimizing EFE selects policies that are both informative and align with preferences."
  - [corpus]: Universal AI maximizes Variational Empowerment (arXiv

## Foundational Learning

- **Active Inference:** A theoretical framework where agents minimize variational free energy to maintain homeostasis and achieve goals. Needed for understanding the safety properties and optimization objectives. Quick check: Verify agents minimize free energy rather than maximize reward.

- **Markov Blankets:** Statistical boundaries that separate internal states from external states, defining agent boundaries and information flow. Needed for understanding the modular architecture and safety containment. Quick check: Confirm clear separation between beliefs and preferences through Markov blankets.

- **Variational Free Energy (VFE):** Mathematical quantity representing the difference between beliefs and observations, driving perception and learning. Needed for understanding the optimization objective. Quick check: Verify VFE calculations correctly represent surprise minimization.

- **Expected Free Energy (EFE):** Forward-looking free energy that balances information gain and expected utility, guiding planning. Needed for understanding action selection. Quick check: Confirm EFE balances exploration and exploitation appropriately.

- **Large Language Models (LLMs):** Neural networks that process and generate natural language, serving as the computational substrate. Needed for understanding the implementation approach. Quick check: Verify LLM reasoning maintains consistency with formal Active Inference principles.

## Architecture Onboarding

- **Component map:** LLM Engine -> Dynamic Memory -> RAG -> Tools -> Perception/Planning Loop -> Action Execution
- **Critical path:** Observation -> VFE Perception Analysis -> Belief Update -> EFE Planning Analysis -> Action Selection -> Execution
- **Design tradeoffs:** Language transparency vs. computational efficiency; consensus requirements vs. responsiveness; modular safety vs. coordination overhead
- **Failure signatures:** Consensus loop failure (infinite deliberation), belief hallucination (mathematical inconsistency), safety constraint violation (preference divergence)
- **3 first experiments:** 1) Implement single agent with dual VFE analysis on simple ARC tasks, 2) Test consensus convergence with controlled belief conflicts, 3) Verify safety constraint adherence through preference manipulation

## Open Questions the Paper Calls Out

None

## Limitations

- Framework safety claims depend heavily on LLM reliability and assumed equivalence between formal Active Inference mathematics and natural language reasoning
- Empirical validation of safety properties remains unproven, particularly for complex real-world scenarios
- Scaling challenges may emerge as problem complexity increases, potentially breaking consensus loops and safety properties

## Confidence

- **High Confidence:** The architectural components (Markov blankets, multi-agent organization, natural language representation) are well-defined and implementable
- **Medium Confidence:** The theoretical safety properties (transparency, corrigibility, bounded rationality) follow logically from Active Inference principles when applied to language
- **Low Confidence:** Empirical validation that language-mediated Active Inference actually produces safer behavior than alternative approaches remains unproven

## Next Checks

1. **Mathematical Consistency Test:** Implement a verifier that checks whether LLM-generated natural language beliefs maintain internal consistency with Active Inference equations on benchmark problems

2. **ARC Benchmark Pilot:** Run the framework on 20 diverse ARC tasks, logging both task success rates and safety-relevant behaviors (corrigibility attempts, preference adherence, belief updates)

3. **Scaling Analysis:** Measure performance degradation as problem complexity increases, tracking when consensus loops fail or safety properties break down