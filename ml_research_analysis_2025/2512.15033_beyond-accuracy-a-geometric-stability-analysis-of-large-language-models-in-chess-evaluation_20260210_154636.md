---
ver: rpa2
title: 'Beyond Accuracy: A Geometric Stability Analysis of Large Language Models in
  Chess Evaluation'
arxiv_id: '2512.15033'
source_url: https://arxiv.org/abs/2512.15033
tags:
- chess
- evaluation
- move
- position
- board
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper evaluates the geometric stability and reasoning robustness\
  \ of six state-of-the-art LLMs (GPT-5.1, Claude Sonnet 4.5, Kimi K2 Turbo, DeepSeek\
  \ Chat, Gemini 2.5 Flash, Grok 4-1-Fast) on chess position evaluation tasks. The\
  \ study introduces a Geometric Stability Framework to test model consistency under\
  \ invariant transformations\u2014including board rotation, mirror symmetry, color\
  \ inversion, and format conversion\u2014revealing that high scalar accuracy does\
  \ not necessarily imply robust conceptual understanding."
---

# Beyond Accuracy: A Geometric Stability Analysis of Large Language Models in Chess Evaluation

## Quick Facts
- arXiv ID: 2512.15033
- Source URL: https://arxiv.org/abs/2512.15033
- Reference count: 40
- Key outcome: Geometric stability reveals Accuracy-Stability Paradox: GPT-5.1 achieves high accuracy but catastrophic consistency degradation under transformations, while Claude Sonnet 4.5 and Kimi K2 Turbo maintain superior dual robustness

## Executive Summary
This paper introduces a Geometric Stability Framework to evaluate large language models on chess position evaluation tasks, revealing that high scalar accuracy does not necessarily imply robust conceptual understanding. The study tests six state-of-the-art LLMs (GPT-5.1, Claude Sonnet 4.5, Kimi K2 Turbo, DeepSeek Chat, Gemini 2.5 Flash, Grok 4-1-Fast) across invariant transformations including board rotation, mirror symmetry, color inversion, and format conversion. The primary finding is a significant Accuracy-Stability Paradox: while GPT-5.1 achieves near-optimal accuracy (Mean Absolute Error ≈ 362 centipawns), it exhibits catastrophic degradation under geometric perturbation, with rotation error rates surging by over 600% (>2500 cp). In contrast, Claude Sonnet 4.5 and Kimi K2 Turbo demonstrate superior dual robustness, maintaining high consistency across all transformation axes. Furthermore, Gemini 2.5 Flash leads in illegal state rejection (96.0%). The findings indicate that geometric stability provides an orthogonal and essential metric for AI evaluation, offering a necessary proxy for disentangling reasoning capabilities from data contamination and overfitting in large-scale models.

## Method Summary
The study employs a geometric stability framework to evaluate six LLMs on chess position evaluation tasks. The methodology involves generating ~77,000 unique chess positions from a 29 GB Lichess PGN database, applying six geometric transformations (rotation, mirror symmetry, color inversion, format conversion, similarity pairs, and illegal injection), and querying each model with identical prompts for original and transformed positions. Evaluations are compared against Stockfish 16.1 (depth 10, 128 MB hash) using Mean Absolute Error and consistency metrics. The framework measures consistency through CP consistency, sign consistency, win rate consistency, and move consistency, with thresholds set at δ_con = 300 centipawns. The study specifically examines the paradox between accuracy (scalar error) and geometric stability (consistency under transformations), revealing that models achieving high accuracy can fail catastrophically on geometrically equivalent transformed positions.

## Key Results
- GPT-5.1 achieves near-optimal accuracy (MAE ≈ 362 cp) but rotation error rates surge by over 600% (>2500 cp), demonstrating catastrophic degradation under geometric perturbation
- Claude Sonnet 4.5 and Kimi K2 Turbo demonstrate superior dual robustness, maintaining high consistency across all transformation axes while achieving competitive accuracy
- Gemini 2.5 Flash leads illegal state rejection at 96.0%, compared to DeepSeek Chat at 44.2%, revealing substantial variation in safety-aligned behavior
- The study identifies three failure modes: consistency failure (large cp swings between equivalent positions), tactical blindness (missing forced mates), and hallucination (confident evaluations of illegal positions)
- The findings establish geometric stability as an orthogonal and essential metric for AI evaluation, necessary for disentangling reasoning capabilities from data contamination and overfitting

## Why This Works (Mechanism)

### Mechanism 1: G-Invariance Breakdown via Token-Geometry Gap
Models achieving high accuracy on canonical positions fail catastrophically on geometrically equivalent transformed positions because they learn surface token correlations rather than invariant spatial representations. The true evaluation function V*(s) is invariant under symmetry group G (rotation, reflection, color swap), but LLMs approximate V* via token sequences ψ(s) that are NOT equivariant—ψ(g·s) produces non-linearly permuted token sequences. Without explicit geometric inductive bias, models learn disconnected manifold charts (M_std vs M_rot) with independent functions f₁ ≠ f₂. Transformers lack built-in group-equivariance; invariance must emerge from training data distribution or explicit regularization.

### Mechanism 2: Spectral Bias and Tactical Singularity Smoothing
LLMs exhibit "tactical blindness" because neural network smoothness constraints (bounded Lipschitz constant) prevent accurate approximation of discontinuous evaluation landscapes (forced mates). Tactical positions contain singularities—single moves alter evaluation from 0 to ±∞. Transformers have Lipschitz constant bounded by product of spectral norms: Lip(f) ≤ Π||W_l||₂. Weight decay and LayerNorm constrain this further. The approximation error for smooth f approximating singularity V* is lower-bounded by gradient magnitude, irreducibly large for forced mates. Deeper reasoning chains (Chain-of-Thought) effectively increase Lipschitz capacity via composition: Lip(f_CoT) ≈ (Lip(h))^T.

### Mechanism 3: Manifold Thickness Enables Illegal-State Hallucination
LLMs assign confident evaluations to illegal positions because learned manifolds are "thick" (continuous) while valid FEN space is sparse/discrete, and models lack explicit rejection boundaries. Valid chess states M_chess form thin discrete manifold. Variational inference + softmax attention encourage dense continuous M_model. Invalid state s'_invalid ∉ M_chess but s'_invalid ≈ M_model in embedding space. Model interpolates, assigning valid score based on proximity to legal neighbors. Hallucination occurs when ||∇_x H(p_θ(·|x))|| ≈ 0—confidence doesn't decay as input moves off-manifold. Safety-aligned models learn sharper energy gradients (∂E/∂n ≫ 0) near valid manifold boundaries, functioning as Energy-Based Models with rejection wells.

## Foundational Learning

- **Concept: Metamorphic Testing / Invariant Transformations**
  - Why needed here: The entire Geometric Stability Framework is metamorphic testing—verifying outputs satisfy necessary relations across transformed inputs. Without this concept, the methodology appears ad hoc.
  - Quick check question: If I rotate a chess board 180°, should a robust model's evaluation change? Why or why not?

- **Concept: Lipschitz Continuity and Spectral Bias**
  - Why needed here: Understanding why LLMs "smooth over" tactical singularities requires grasping how weight norms constrain function steepness. This explains the tactical blindness mechanism.
  - Quick check question: If a neural network has layers with spectral norms [1.2, 1.1, 1.3], what's the upper bound on its Lipschitz constant? What does this imply about approximating step functions?

- **Concept: Manifold Learning and Out-of-Distribution Detection**
  - Why needed here: The hallucination mechanism hinges on understanding valid data as low-dimensional manifold in high-dimensional space. Models interpolate within ambient space rather than restricting to manifold surface.
  - Quick check question: Why might a model confidently evaluate a position with two white kings? What would a "rejection boundary" look like geometrically?

## Architecture Onboarding

- **Component map**: Position Generator → [FEN/String] → LLM Evaluator → [cp score, move, reasoning] → Stockfish Baseline → Ground Truth Oracle → Comparison & Error Classification
- **Critical path**: Start with Stability Evaluation (Section 5.2)—it's the core contribution. Then trace to Accuracy Benchmarking (5.3) to understand the paradox. Finally, examine Error Classification (5.6) for failure mode taxonomy. Mathematical formalizations in Section 6 provide mechanistic explanations but are secondary to empirical framework.
- **Design tradeoffs**: 
  - Synthetic vs. real positions: Synthetic (random walk) reduces contamination risk but may undersample tactically rich positions
  - Depth vs. breadth of transformations: More transformations increase diagnostic coverage but multiply API costs
  - Threshold selection (δ_con = 300 cp, δ_tac = 10,000 cp): Arbitrary but grounded in chess intuition; too tight = noise, too loose = missed failures
  - Grok "fast-non-reasoning" configuration: Disabled reasoning mode for latency parity—may understate true capability
- **Failure signatures**:
  - **Consistency failure**: Large cp swing (>300 cp) or sign flip between P and T(P) for geometrically equivalent positions. Look for: +2500 → -3000 on rotation.
  - **Tactical blindness**: Model predicts balanced evaluation (|cp| < 500) on forced mate position. Look for: "Draw" prediction when Stockfish shows M5.
  - **Hallucination**: Model outputs numeric cp on position with missing king, pawns on rank 1, or double check. Look for: Any evaluation when "illegal" is correct answer.
- **First 3 experiments**:
  1. **Reproduce rotation paradox**: Take 50 positions from paper's dataset (or generate via random walk). Query target LLM on each position and its 180° rotation. Compute MAE between pairs. Expect: "brittle specialist" models show >1000 cp error; "robust reasoners" show <500 cp.
  2. **Illegal-position probe**: Construct 20 illegal FENs (missing king, pawn on rank 1, double check). Query model with standard evaluation prompt. Measure rejection rate. Compare to paper's Gemini (96%) vs DeepSeek (44%) baseline.
  3. **Format consistency test**: Convert 30 positions to FEN, PGN, and natural language descriptions. Query model for cp evaluation across all three. Compute pairwise MAE. High NL↔FEN gap indicates surface-form dependence.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does explicit geometric data augmentation during training effectively decouple syntax from spatial semantics to mitigate the Accuracy-Stability Paradox?
- Basis in paper: The authors hypothesize that robustness of Kimi K2 and DeepSeek stems from training pipelines that may include "data augmentation regimes involving geometric permutations," whereas GPT-5.1 likely overfits to canonical strings.
- Why unresolved: The study evaluates inference-time behavior only; it does not audit or control the training datasets of the proprietary models studied.
- What evidence would resolve it: Training ablation studies on open-source models (e.g., LLaMA) comparing standard PGN training against datasets augmented with dihedral transformations (rotation/reflection).

### Open Question 2
- Question: Can architectural inductive biases be integrated into Transformers to enforce G-invariance without compromising their ability to model the sharp discontinuities required for tactical logic?
- Basis in paper: The conclusion states that future progress requires "architectural innovations that enforce geometric invariance" to bridge the "token-geometry gap," rather than just scaling parameters.
- Why unresolved: The paper demonstrates that current Transformer architectures lack group-equivariance, forcing them to memorize disconnected manifolds, but does not propose a specific architectural fix.
- What evidence would resolve it: Development and evaluation of hybrid architecture (e.g., integrating Group-CNN layers with attention mechanisms) that maintains low MAE on both standard and transformed positions.

### Open Question 3
- Question: Does utilization of Chain-of-Thought (CoT) reasoning explicitly increase the effective Lipschitz constant of the model, thereby reducing Tactical Blindness?
- Basis in paper: Section 6.2 mathematically derives that Gemini's low tactical error rate implies "an effective increase in the Lipschitz capacity" through latent reasoning steps (f_CoT ≈ (Lip(h))^T).
- Why unresolved: This remains theoretical explanation for empirical results; internal "horizon" of reasoning steps was not directly measured or manipulated.
- What evidence would resolve it: Controlled experiment measuring tactical accuracy of models when CoT is enabled versus disabled, specifically on positions featuring forced mates (singularities).

## Limitations
- Dependence on Stockfish 16.1 as ground truth constrains evaluation to positions within typical search depth and may miss long-term strategic nuances
- Synthetic position generation via random walk creates corpus heavily skewed toward mid-game positions, potentially underrepresenting opening theory and endgame technique
- The transformation threshold of 300 centipawns for consistency is somewhat arbitrary, though grounded in chess intuition
- The analysis assumes Stockfish's evaluations are invariant under tested transformations, but subtle perspective-dependent evaluation artifacts may exist

## Confidence
- **High Confidence**: The geometric stability framework methodology and empirical observation that accuracy and consistency are often inversely correlated across models. The illegal position rejection results showing substantial variation between models.
- **Medium Confidence**: The mechanistic explanations for tactical blindness (Lipschitz constraints) and hallucination (manifold thickness). While mathematically sound, these explanations require additional empirical validation specific to chess evaluation tasks.
- **Low Confidence**: The broader implications for AI evaluation beyond chess. While the framework is compelling, its generalizability to other domains with different symmetry structures remains untested.

## Next Checks
1. **Cross-Stockfish Validation**: Repeat the entire geometric stability analysis using alternative strong engines (Lemos, Ethereal, or Dragon) to verify that observed model failures persist independent of the specific ground truth engine.
2. **Contamination Test**: Train a control model on a dataset explicitly augmented with geometrically transformed positions (including illegal states) and evaluate whether geometric stability improves, confirming the data contamination hypothesis.
3. **Fine-tuning Experiment**: Take the most stable model (Claude Sonnet 4.5 or Kimi K2 Turbo) and fine-tune on a small dataset of transformed positions with consistency-based loss. Measure whether stability improves while accuracy remains constant, demonstrating that geometric reasoning can be learned without sacrificing evaluation quality.