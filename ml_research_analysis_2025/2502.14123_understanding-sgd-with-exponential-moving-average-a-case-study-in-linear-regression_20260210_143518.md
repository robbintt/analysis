---
ver: rpa2
title: 'Understanding SGD with Exponential Moving Average: A Case Study in Linear
  Regression'
arxiv_id: '2502.14123'
source_url: https://arxiv.org/abs/2502.14123
tags:
- bias
- averaging
- lemma
- have
- variance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the generalization properties of stochastic
  gradient descent (SGD) with exponential moving average (EMA) for high-dimensional
  linear regression. The authors establish the first instance-dependent excess risk
  bound for SGD with EMA, showing that its effective bias decays exponentially in
  the number of iterations, and its effective variance is smaller than SGD without
  averaging.
---

# Understanding SGD with Exponential Moving Average: A Case Study in Linear Regression

## Quick Facts
- arXiv ID: 2502.14123
- Source URL: https://arxiv.org/abs/2502.14123
- Authors: Xuheng Li; Quanquan Gu
- Reference count: 5
- Primary result: First instance-dependent excess risk bounds for SGD with EMA, showing exponential bias decay and variance reduction comparable to tail averaging

## Executive Summary
This paper provides the first rigorous theoretical analysis of stochastic gradient descent (SGD) with exponential moving average (EMA) for high-dimensional linear regression. The authors establish instance-dependent excess risk bounds that reveal EMA's dual benefits: exponential decay of effective bias and reduction of effective variance compared to standard SGD. The analysis shows that EMA's variance reduction is comparable to tail averaging when the EMA parameter α is appropriately tuned, providing theoretical justification for EMA's widespread empirical success in stabilizing training and reducing gradient noise.

## Method Summary
The method analyzes SGD with EMA in the context of high-dimensional linear regression where labels are generated as $y = \langle w^*, x \rangle + \xi$ with Gaussian features. The core algorithm consists of standard SGD weight updates $w_t = w_{t-1} + \delta(y_t - \langle w_{t-1}, x_t \rangle)x_t$ combined with EMA updates $\bar{w}_t = \alpha \bar{w}_{t-1} + (1-\alpha)w_t$. The analysis decomposes excess risk into effective bias and effective variance components, deriving explicit bounds for each. Synthetic data is generated with diagonal covariance matrix $H$ having eigenvalues $\lambda_i = i^{-2}$ in $d=2000$ dimensions, using learning rate $\delta=0.2$ and $N=3000$ iterations.

## Key Results
- EMA achieves exponential decay of effective bias error, faster than polynomial decay of iterate averaging from the beginning
- EMA's effective variance is strictly smaller than SGD without averaging and matches tail averaging when $(1-\alpha)(N-s) = 1$
- The decay parameter α controls a bias-variance trade-off: higher α reduces variance but slows bias decay, while lower α increases responsiveness but adds noise
- Theoretical bounds are validated through synthetic experiments comparing EMA to standard SGD and tail averaging

## Why This Works (Mechanism)

### Mechanism 1: Effective Variance Suppression
EMA reduces effective variance by recursively averaging current weights with historical weights using decay factor $\alpha$, smoothing high-frequency noise in stochastic gradients. The variance error is strictly smaller than SGD without averaging and matches tail averaging when EMA's memory is tuned to the effective window size. This requires stable learning rate constraints and finite data covariance.

### Mechanism 2: Exponential Bias Decay in Eigen-subspaces
Unlike iterate averaging with polynomial decay, EMA drives effective bias error down exponentially across all eigen-subspaces of the data covariance. The decay rate depends on eigenvalue magnitude and averaging parameter, with phase transitions occurring based on the relationship between learning rate and eigenvalue size.

### Mechanism 3: Bias-Variance Trade-off via Alpha
The decay parameter $\alpha$ acts as a direct control knob for trading off bias contraction speed against variance reduction magnitude. Increasing $\alpha$ dampens variance but slows new information incorporation, while decreasing $\alpha$ makes the model more responsive but noisier. This requires finite iteration count and constant $\alpha$.

## Foundational Learning

### Concept: Bias-Variance Decomposition
- **Why needed here**: The paper's theoretical framework splits excess risk into "Effective Bias" and "Effective Variance" to prove EMA's benefits
- **Quick check question**: Can you explain why reducing variance might come at the cost of bias decay speed in an iterative averaging process?

### Concept: Eigenvalue Spectrum of Data Covariance
- **Why needed here**: The analysis examines convergence separately for eigen-subspaces with different eigenvalue magnitudes, dictating phase transitions in decay rates
- **Quick check question**: Why would a gradient descent algorithm converge faster in directions corresponding to large eigenvalues of the Hessian/Covariance matrix?

### Concept: Tail Averaging
- **Why needed here**: This is the primary baseline EMA is compared against. Understanding that tail averaging discards early burn-in iterations is crucial to seeing why EMA is advantageous
- **Quick check question**: How does EMA simulate the effect of discarding early iterations without explicitly needing to store or select a specific start time for averaging?

## Architecture Onboarding

### Component map:
SGD Core -> EMA Shadow Weights -> Final Prediction

### Critical path:
Implementing the recursive EMA update efficiently. In distributed settings, this requires broadcasting shadow weights or aggregating them correctly.

### Design tradeoffs:
- **Responsiveness vs. Stability**: Low $\alpha$ (e.g., 0.9) tracks raw SGD closely (fast bias decay, high variance). High $\alpha$ (e.g., 0.9999) creates stable but "laggy" model (slow bias decay, low variance)
- **Memory**: EMA requires storing duplicate model parameters (constant overhead), unlike tail averaging which might require storing buffer of recent iterates (linear overhead)

### Failure signatures:
- **The "Lag"**: If validation loss improves then plateaus while training loss drops, $\alpha$ may be too high
- **Oscillation**: If EMA weights oscillate as much as raw weights, $\alpha$ is too low

### First 3 experiments:
1. **Baseline Comparison**: Train high-dimensional linear regression with raw SGD, SGD+Tail Averaging, and SGD+EMA. Plot Bias and Variance errors separately.
2. **Alpha Sensitivity**: Run same training with $\alpha \in \{0.9, 0.99, 0.999\}$ against fixed $N=3000$. Verify trade-off where higher $\alpha$ yields lower variance but slower initial bias decay.
3. **Critical Batch Size**: Extend to mini-batch SGD. Vary batch size $B$ and observe phase transition in variance reduction to verify theoretical critical batch size derivation.

## Open Questions the Paper Calls Out

### Open Question 1
Can a finer analysis bridge the gap between upper and lower bounds of effective variance by eliminating the initialization-dependent term in the upper bound? The current upper bound for variance includes an initialization term absent in the lower bound.

### Open Question 2
How does the "remaining part" of effective variance (feature noise) of SGD with EMA compare quantitatively to effective variance of SGD with iterate averaging from the beginning? The variance error of iterate averaging and EMA can only be compared for label noise component currently.

### Open Question 3
Do the instance-dependent risk bounds and exponential bias decay properties transfer to non-convex settings and deep neural networks? The theoretical results rely on bias-variance decomposition and linear operator analysis that assumes quadratic loss landscape and linear model.

## Limitations
- Theoretical analysis is limited to linear regression with Gaussian data assumptions and may not directly extend to non-linear models
- Assumes constant step size and fixed averaging parameter α throughout training, excluding adaptive learning rates
- Comparison with tail averaging requires tuning tail length s, creating asymmetry in hyperparameter search space

## Confidence

**High Confidence**: EMA reduces effective variance compared to SGD without averaging (follows directly from variance recursion analysis)

**Medium Confidence**: EMA achieves comparable variance reduction to tail averaging when $(1-\alpha)(N-s) = 1$ (derivation is sound but requires careful tuning)

**Medium Confidence**: Exponential bias decay claim (mathematical analysis is rigorous but depends on eigenvalue spectrum assumptions)

## Next Checks

1. **Non-Gaussian Data Validation**: Replicate main experiments using non-Gaussian feature distributions (e.g., heavy-tailed or correlated features) to assess robustness beyond theoretical assumptions

2. **Adaptive Learning Rate Test**: Implement EMA with adaptive learning rate schedules (like Adam) and compare bias-variance trade-off to fixed learning rate case

3. **Multi-dimensional EMA**: Test multiple EMA parameters across different eigen-subspaces simultaneously to potentially achieve better bias-variance trade-offs than single-parameter EMA