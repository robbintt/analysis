---
ver: rpa2
title: 'FlowCalib: LiDAR-to-Vehicle Miscalibration Detection using Scene Flows'
arxiv_id: '2601.23107'
source_url: https://arxiv.org/abs/2601.23107
tags:
- flow
- miscalibration
- point
- detection
- calibration
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FlowCalib addresses the critical need for detecting LiDAR-to-vehicle
  miscalibration in autonomous driving systems, which can cause safety-critical errors
  in object detection and localization. The method leverages scene flow patterns of
  static objects to identify angular misalignments between LiDAR sensors and the vehicle
  frame without requiring additional sensors like IMU or GPS.
---

# FlowCalib: LiDAR-to-Vehicle Miscalibration Detection using Scene Flows

## Quick Facts
- arXiv ID: 2601.23107
- Source URL: https://arxiv.org/abs/2601.23107
- Reference count: 27
- Primary result: Detects LiDAR-to-vehicle angular miscalibration using scene flow patterns of static objects without additional sensors

## Executive Summary
FlowCalib addresses the critical need for detecting LiDAR-to-vehicle miscalibration in autonomous driving systems, which can cause safety-critical errors in object detection and localization. The method leverages scene flow patterns of static objects to identify angular misalignments between LiDAR sensors and the vehicle frame without requiring additional sensors like IMU or GPS. By analyzing motion patterns in sequential 3D point clouds, FlowCalib detects systematic biases in flow fields caused by rotational misalignment.

## Method Summary
The method analyzes sequential 3D LiDAR point clouds to detect angular miscalibration between the LiDAR sensor and vehicle frame. It extracts scene flow information from static objects and identifies systematic biases in flow fields caused by rotational misalignment. The framework combines learned global flow features with handcrafted geometric descriptors through a dual-branch neural network that performs both global miscalibration detection and axis-specific classification (roll, pitch, yaw).

## Key Results
- Overall detection accuracy of 81.16% across different severity levels
- 90.27% accuracy for detecting errors between ±2° and ±5°
- Strong performance in yaw and roll miscalibration detection, with pitch angle detection more challenging

## Why This Works (Mechanism)
FlowCalib works by exploiting the fact that angular miscalibration between LiDAR and vehicle frames creates systematic biases in scene flow patterns of static objects. When the LiDAR is misaligned, stationary objects appear to have consistent directional motion in the flow field, creating a detectable pattern. By analyzing these flow patterns in sequential point clouds, the method can identify both the presence and direction of miscalibration without requiring additional sensor data.

## Foundational Learning
- **Scene Flow Estimation**: Understanding motion patterns in 3D point clouds is essential for detecting systematic biases. Quick check: Verify flow consistency across static objects.
- **LiDAR Coordinate Systems**: Knowledge of vehicle frame vs. LiDAR frame transformations is crucial for interpreting miscalibration effects. Quick check: Confirm transformation matrices align with sensor specifications.
- **Angular Misalignment Effects**: Recognizing how rotational errors manifest in point cloud data enables pattern detection. Quick check: Test with synthetic rotational perturbations.

## Architecture Onboarding

**Component Map:**
Raw LiDAR point clouds -> Scene flow extraction -> Global feature learning + Geometric descriptor extraction -> Dual-branch neural network -> Miscalibration classification

**Critical Path:**
Sequential point clouds → Scene flow computation → Feature extraction (global + geometric) → Neural network fusion → Classification output

**Design Tradeoffs:**
- No additional sensors required vs. limited to scenarios with sufficient static objects
- Learned features capture complex patterns vs. handcrafted descriptors provide geometric interpretability
- Dual-branch architecture balances global context with local geometric cues vs. increased model complexity

**Failure Signatures:**
- Poor performance in low-motion scenarios with few static objects
- Reduced accuracy for pitch angle detection due to uniform motion patterns
- Decreased sensitivity for subtle miscalibrations (< 2°)

**First 3 Experiments:**
1. Test on synthetic datasets with controlled angular misalignments
2. Evaluate performance across different LiDAR sensor types and point cloud densities
3. Measure detection latency and computational requirements for real-time deployment

## Open Questions the Paper Calls Out
None

## Limitations
- Overall detection accuracy of 81.16% indicates room for improvement
- Pitch angle detection performance drops significantly compared to roll/yaw
- Detection accuracy varies substantially across calibration severity levels (73.35% for 0-2° vs. 90.27% for 2-5°)

## Confidence
- High confidence in global miscalibration detection and yaw/roll axis classification
- Medium confidence in pitch angle detection and performance in low-motion scenarios
- High confidence in "no additional sensors required" claim

## Next Checks
1. Test the method on datasets with varied LiDAR sensor types (mechanical vs. solid-state) and different point cloud densities to assess generalization
2. Evaluate performance in urban scenarios with minimal static objects and high dynamic scene complexity
3. Conduct real-world deployment testing to measure detection latency and computational overhead in autonomous driving systems