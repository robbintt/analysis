---
ver: rpa2
title: Can LLMs Deceive CLIP? Benchmarking Adversarial Compositionality of Pre-trained
  Multimodal Representation via Text Updates
arxiv_id: '2505.22943'
source_url: https://arxiv.org/abs/2505.22943
tags:
- caption
- noun
- given
- text
- diversity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MAC, a benchmark that uses LLMs to generate
  deceptive text samples to expose compositional vulnerabilities in pre-trained multimodal
  representations like CLIP. The method evaluates both attack success rates and sample
  diversity using entropy-based metrics.
---

# Can LLMs Deceive CLIP? Benchmarking Adversarial Compositionality of Pre-trained Multimodal Representation via Text Updates

## Quick Facts
- **arXiv ID**: 2505.22943
- **Source URL**: https://arxiv.org/abs/2505.22943
- **Reference count**: 40
- **One-line result**: Self-training with diversity-promoting filtering significantly improves LLM-generated adversarial text samples, achieving higher attack success rates and sample diversity against multimodal representations like CLIP across image, video, and audio modalities.

## Executive Summary
This paper introduces MAC (Multimodal Adversarial Compositionality), a benchmark that uses large language models to systematically generate deceptive text samples that exploit compositional vulnerabilities in pre-trained multimodal representations. The method leverages a self-training approach with rejection-sampling fine-tuning and diversity-promoting filtering to enhance both attack success rates and sample diversity. Using a smaller open-source model (Llama-3.1-8B), MAC achieves higher attack success rates and greater sample diversity than prior methods, revealing compositional weaknesses across multiple multimodal representations. The framework is modality-agnostic and demonstrates strong transferability of attacks across different target models.

## Method Summary
The MAC framework generates deceptive text samples by having an LLM create hard negative captions that are semantically contradictory but lexically similar to original captions. The process involves generating multiple candidate samples (N=64), filtering them through four criteria (crossmodal similarity, unimodal entailment, Levenshtein distance, auxiliary rules), and selecting diverse successful samples via entropy maximization. A self-training loop then fine-tunes the LLM on these selected samples using LoRA, creating an iterative improvement cycle. The method is evaluated on COCO (image), MSRVTT (video), and AudioCaps (audio) datasets against target models including CLIP, LanguageBind, and SigLIP, measuring both attack success rate and sample diversity.

## Key Results
- Self-training with diversity-promoting filtering significantly improves both attack success rate and sample diversity compared to baseline methods
- Smaller open-source models (Llama-3.1-8B) outperform larger proprietary models (GPT-4o) in generating diverse adversarial samples
- High cross-model transferability is demonstrated, with attacks generated for one model achieving strong performance on others
- The framework successfully identifies compositional vulnerabilities across image, video, and audio modalities

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LLMs can systematically generate text modifications that exploit compositional vulnerabilities in multimodal representations.
- **Mechanism:** Pre-trained multimodal representations encode relationships between objects and attributes that are susceptible to text updates. An LLM generates text that is semantically contradictory but lexically similar to the original, creating "hard negative captions" that the multimodal model incorrectly ranks as more similar to the input than the ground truth.
- **Core assumption:** LLMs can understand semantic content and generate plausible contradictions aligned with embedding space weaknesses.
- **Evidence anchors:** [abstract] introduces MAC for generating deceptive text to exploit compositional vulnerabilities; [Page 3] defines attack success when model ranks generated sample higher than original text; [corpus] related work uses LLMs for adversarial samples.

### Mechanism 2
- **Claim:** Self-training with rejection-sampling fine-tuning and diversity-promoting filtering significantly improves deception performance.
- **Mechanism:** The process generates multiple candidates, filters for effective ones, and uses them to fine-tune the LLM. Diversity-promoting filtering selects samples that maximize entropy, ensuring the model learns a wide range of attack patterns rather than overfitting to specific vulnerabilities.
- **Core assumption:** Successful deceptive samples serve as quality training data, and entropy maximization ensures diverse attack strategies.
- **Evidence anchors:** [abstract] proposes self-training approach with RFT and diversity-promoting filtering; [Page 7] details diversity-promoting self-training algorithm; [corpus] self-training improves LLM performance.

### Mechanism 3
- **Claim:** Attack success transfers across different multimodal representations, indicating shared compositional vulnerabilities.
- **Mechanism:** Deceptive text samples target fundamental weaknesses in how models encode compositional language, particularly those using contrastive pre-training on similar web-scale datasets. This shared foundation creates vulnerabilities that are exploitable across different models.
- **Core assumption:** Compositional vulnerabilities are properties of general pre-training paradigms rather than specific architectures.
- **Evidence anchors:** [Page 8, Table 3] shows cross-model transfer analysis with significant performance gains; [Page 8, Section 5.2] demonstrates 2.1× improvement in transferability; [corpus] shared vulnerabilities across foundation models.

## Foundational Learning

- **Concept: Multimodal Compositional Reasoning**
  - **Why needed here:** This is the core capability being attacked - the model's ability to understand structured relationships between words and visual elements, not just recognize objects independently.
  - **Quick check question:** How does a compositional vulnerability differ from a simple object recognition failure?

- **Concept: Contrastive Pre-training (e.g., CLIP)**
  - **Why needed here:** Target models are based on this paradigm, learning to align text and other modalities in shared embedding space by maximizing similarity for paired data and minimizing it for unpaired data.
  - **Quick check question:** In CLIP embedding space, what does high cosine similarity between an image and text caption signify?

- **Concept: Self-Training and Rejection Sampling Fine-Tuning (RFT)**
  - **Why needed here:** Method used to improve the attack generator by generating candidates, filtering best ones, and fine-tuning the LLM on successful examples.
  - **Quick check question:** What is the primary benefit of fine-tuning an LLM on its own successfully generated outputs compared to just using the base model?

## Architecture Onboarding

- **Component map:** Generator LLM -> Filterer -> Target Multimodal Representation -> Fine-tuning module

- **Critical path:**
  1. **Prompting:** Generator LLM receives original text and instruction to generate deceptive samples
  2. **Generation:** LLM produces N candidate deceptive texts
  3. **Sample-wise Evaluation:** Candidates evaluated against four criteria (crossmodal, unimodal, distance, auxiliary)
  4. **Sample Selection:** Successful candidates collected; diversity-promoting filter selects final training batch
  5. **Fine-tuning:** Generator LLM fine-tuned on selected batch using next-token prediction loss
  6. **Repeat:** Process can iterate for multiple rounds

- **Design tradeoffs:**
  - **Budget (N) vs. Computation:** Higher N increases attack success chance but linearly increases computational cost
  - **ASR vs. Diversity:** Methods focusing on ASR alone see sharp decline in diversity; diversity-promoting filtering sacrifices some ASR for broader compositional flaw exposure

- **Failure signatures:**
  - **Overused Vocabulary/Patterns:** Skewed distribution of attribute-enriched tokens (e.g., repeated use of "vintage" or "man")
  - **Trivial Attacks:** Simple paraphrases or negations passing filters without exposing true compositional failures
  - **Low Transferability:** Overfitting to specific target model's embedding space, failing on different models

- **First 3 experiments:**
  1. **Baseline Attack Generation (Zero-Shot):** Use base Llama-3.1-8B with deceptive-general prompt on COCO, generate N=4 candidates per pair, measure sample-wise ASR
  2. **Ablation on Filtering Criteria:** Run baseline without one criterion at a time to quantify each component's contribution
  3. **Self-Training with Diversity Ablation:** Compare naïve self-training (all successful samples) vs. diversity-promoting self-training (Algorithm 1), measuring both ASR and group-wise diversity

## Open Questions the Paper Calls Out

- **Extending to longer captions:** Current methods focus on short captions; extending to detailed captions requiring long-range dependency modeling represents a distinct research direction due to increased semantic consistency challenges.
- **Non-visual/audio modalities:** While framework is theoretically modality-agnostic, lack of pre-trained multimodal LLMs for IMU or tactile sensing prevents effective vulnerability analysis in these domains.
- **Model size paradox:** Smaller open-source models frequently outperform larger proprietary models in generating diverse adversarial samples, though the causal mechanism remains unexplained.

## Limitations

- **Sample-wise evaluation criteria opacity:** Filtering criteria including "Auxiliary" checks and NLI model agreements lack precise implementation details, creating uncertainty in reproduction.
- **Transferability mechanism ambiguity:** While empirical transfer is demonstrated, underlying reasons remain speculative rather than analytically characterized.
- **Diversity metric limitations:** Entropy-based diversity measure may overvalue semantically meaningless but lexically diverse attacks while undervaluing targeted compositional failures.

## Confidence

- **High confidence:** Self-training mechanism's effectiveness - iterative improvement through rejection sampling fine-tuning is well-supported and mechanistically sound
- **Medium confidence:** Cross-model transferability claims - empirical evidence shows significant transfer but underlying reasons remain speculative
- **Low confidence:** Completeness of vulnerability characterization - benchmark may not exhaustively map full space of vulnerabilities

## Next Checks

1. **Criterion Implementation Validation:** Implement "Auxiliary" filtering rules and NLI aggregation logic with multiple variants to determine sensitivity of attack success rates to implementation details.

2. **Cross-model Architecture Analysis:** Systematically vary target models to include fundamentally different architectures (non-contrastive models) and training datasets to clarify whether transfer stems from compositional reasoning or shared training data.

3. **Diversity Metric Robustness Testing:** Supplement entropy-based diversity measure with human evaluation of semantic coherence, generating samples with identical entropy scores but varying semantic plausibility to test metric validity.