---
ver: rpa2
title: Quality-of-Service Aware LLM Routing for Edge Computing with Multiple Experts
arxiv_id: '2508.00234'
source_url: https://arxiv.org/abs/2508.00234
tags:
- edge
- latency
- request
- requests
- routing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of efficiently routing user
  requests to heterogeneous large language models (LLMs) deployed at the network edge
  to ensure acceptable quality-of-service (QoS). The authors propose a deep reinforcement
  learning (DRL)-based QoS-aware LLM routing framework that leverages dynamic state
  abstraction using heterogeneous graph attention networks (HAN) to compactly represent
  global state features and an action impact estimator with a tailored reward function
  to maximize QoS while preventing latency violations.
---

# Quality-of-Service Aware LLM Routing for Edge Computing with Multiple Experts

## Quick Facts
- arXiv ID: 2508.00234
- Source URL: https://arxiv.org/abs/2508.00234
- Reference count: 40
- This paper proposes a deep reinforcement learning framework for routing user requests to heterogeneous LLMs at the edge, improving average QoS by up to 35.78% and reducing latency per token by up to 5.45%.

## Executive Summary
This paper addresses the challenge of efficiently routing user requests to heterogeneous large language models (LLMs) deployed at the network edge to ensure acceptable quality-of-service (QoS). The authors propose a deep reinforcement learning (DRL)-based QoS-aware LLM routing framework that leverages dynamic state abstraction using heterogeneous graph attention networks (HAN) to compactly represent global state features and an action impact estimator with a tailored reward function to maximize QoS while preventing latency violations. Extensive experiments on both Poisson and real-world workloads demonstrate that the proposed algorithm significantly improves average QoS by up to 35.78% and reduces average latency per token by up to 5.45% compared to existing baselines.

## Method Summary
The proposed method uses Soft Actor-Critic (SAC) with a Heterogeneous Graph Attention Network (HAN) encoder to route requests to one of N heterogeneous edge LLM experts or drop them. The HAN dynamically represents the global state by encoding arrived requests, edge experts, running requests, and waiting requests into a fixed-size embedding. Two DistilBERT models predict generation score buckets and output length buckets from request text, providing quality estimates to the DRL agent. The reward function incorporates an action impact estimator that quantifies latency degradation for running requests, penalizing routing decisions that would cause latency violations. The framework is trained on 1 million steps using TorchRL and evaluated on both Poisson and real-world BurstGPT workloads.

## Key Results
- The proposed algorithm improves average QoS by up to 35.78% compared to existing baselines
- Average latency per token is reduced by up to 5.45% compared to existing methods
- The DistilBERT predictors contribute to a 17.94% improvement in QoS
- The framework maintains high performance across both synthetic Poisson and real-world BurstGPT workloads

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Heterogeneous Graph Attention Network (HAN) compresses variable-sized, dynamic system states into fixed-size embeddings that preserve relational information between requests and edge experts.
- Mechanism: Construct a heterogeneous graph with four node types (arrived request, edge expert, running request, waiting request). Two-level attention aggregates neighbor features: node-level attention weights neighbor contributions, semantic-level attention weights different edge types. After L layers of message passing, the arrived request node embedding serves as the DRL input.
- Core assumption: The graph structure captures meaningful relationships affecting routing (e.g., queue membership indicates contention, expert connections indicate capability).
- Evidence anchors:
  - [abstract]: "dynamic state abstraction technique to compactly represent global state features with a heterogeneous graph attention network (HAN)"
  - [section V-B2]: "We then map the arrived request node embedding Gj,(L−1)t as the input of the DRL agent."
  - [corpus]: "Intelligent Orchestration of Distributed Large Foundation Model Inference at the Edge" addresses similar edge LLM challenges but does not validate HAN specifically.
- Break condition: If request arrival patterns become extremely sparse or the number of edge experts changes significantly at inference time, learned embeddings may not generalize.

### Mechanism 2
- Claim: The Action Impact Estimator quantifies how routing a new request degrades latency for running requests, enabling proactive penalty in the reward signal.
- Mechanism: Estimate latency increase (Eq. 15) as the sum of prefill blocking time and decode slowdown from batch expansion. If estimated latency exceeds threshold L for any running request, the reward applies a penalty proportional to that request's QoS contribution.
- Core assumption: Latency scales linearly with token counts via profiled coefficients k1,n and k2,n, and the latency threshold L correctly captures user abandonment behavior.
- Evidence anchors:
  - [abstract]: "action impact estimator with a tailored reward function to guide the DRL agent in maximizing QoS and preventing latency violations"
  - [section V-C1, Eq. 15-16]: "l+i,tj = 1/di (k1,n × pj + k2,n × Σ min(di−di,tj, dj) (pj + k))"
  - [corpus]: No direct corpus validation for this specific estimator design; related QoS prediction work focuses on different domains.
- Break condition: If the inference backend changes batching strategies (e.g., from iteration-level to continuous batching), the linear latency model coefficients require recalibration.

### Mechanism 3
- Claim: Soft Actor-Critic (SAC) with entropy regularization prevents premature convergence to suboptimal routing policies under dynamic workloads.
- Mechanism: SAC maximizes both expected cumulative reward and policy entropy (Eq. 5). The temperature parameter α balances exploitation of known good routes against exploration of alternatives, which is critical when workload patterns shift.
- Core assumption: The reward signal is sufficiently dense and informative; the Markov property holds reasonably well for the state abstraction.
- Evidence anchors:
  - [abstract]: "deep reinforcement learning (DRL)-based QoS-aware LLM routing framework"
  - [section V-A]: "SAC encourages the agent to explore the state space more thoroughly, avoiding premature convergence to suboptimal policies"
  - [corpus]: "Multi-Agent DRL for Queue-Aware Task Offloading" demonstrates DRL effectiveness in edge task scheduling but for non-LLM workloads.
- Break condition: If workload distribution shifts dramatically from training (e.g., from Poisson to highly bursty patterns), the policy may underperform and require fine-tuning.

## Foundational Learning

- Concept: Iteration-level scheduling in LLM inference (Orca, vLLM)
  - Why needed here: The entire interference model assumes iteration-level batching where prefill blocks decode and batch size affects per-token latency.
  - Quick check question: Why does the prefill phase of a newly arrived request block the decode phase of requests already in the running queue?

- Concept: Heterogeneous Graph Attention Networks (HAN)
  - Why needed here: The state encoder must handle multiple node types with different feature dimensions and semantic relationships.
  - Quick check question: What is the difference between node-level attention and semantic-level attention in a heterogeneous GNN?

- Concept: Entropy-regularized reinforcement learning (SAC)
  - Why needed here: The routing policy must balance exploiting known good expert assignments while exploring alternatives under workload shifts.
  - Quick check question: How does maximizing policy entropy help prevent the router from always selecting the highest-quality expert regardless of queue state?

## Architecture Onboarding

- Component map:
  1. **Predictors**: DistilBERT-based models (67M params each) predict generation score bucket and output length bucket from request text (5ms each on RTX 4090)
  2. **State Encoder**: HAN (2 layers, 4 attention heads, 64-dim hidden) converts dynamic graph to fixed embedding (<1ms)
  3. **Policy Networks**: Actor (2-layer MLP) outputs action; Critic (2-layer MLP) estimates value
  4. **Reward Module**: Action impact estimator computes latency penalty; combines with completed request QoS

- Critical path:
  1. Request arrives → parallel prediction of score/length buckets (total ~5ms with parallelization)
  2. Build heterogeneous graph with current queue states and arrived request
  3. HAN forward pass → extract arrived request node embedding
  4. Actor network samples action (expert ID or drop)
  5. Route request; upon completion, compute actual QoS for reward

- Design tradeoffs:
  - **Bucketization vs. precision**: Using 10 buckets for scores/lengths tolerates prediction error but loses fine-grained distinctions
  - **Queue capacity vs. encoding cost**: Fixed capacity (default 5) bounds graph size but may truncate information under high load
  - **Latency threshold L**: Fixed at 30ms default; changing requires assessing if policy adapts or needs retraining

- Failure signatures:
  - **QoS plateaus early in training**: Check SAC temperature α; may need adjustment for better exploration
  - **Latency violations persist**: Verify k1,n, k2,n coefficients are accurately profiled for each expert
  - **Policy collapses to single expert**: Entropy weight too low or reward signal insufficiently differentiated

- First 3 experiments:
  1. Profile latency coefficients: For each edge expert, run controlled experiments varying input tokens and batch tokens to calibrate k1,n (prefill gradient) and k2,n (decode gradient).
  2. Ablate predictors: Train with zeroed predictions vs. full predictions to measure contribution (paper reports 17.94% QoS improvement from predictors).
  3. Stress-test under bursty workloads: Deploy trained policy on BurstGPT real-world traces to verify generalization beyond Poisson training distribution.

## Open Questions the Paper Calls Out
- The paper explicitly states plans to explore "integration of more diverse types of LLMs (e.g., multi-modal models) and more complex edge network topologies" in future work.

## Limitations
- The framework depends on accurate profiling of latency coefficients k1,n and k2,n for each edge expert, which are specific to hardware configurations
- The 5-request queue capacity truncation could discard critical state information during peak loads
- The approach assumes the Markov property holds for the HAN state abstraction, which may break down under highly bursty workloads

## Confidence
- **QoS Improvement (35.78%)**: High confidence - based on controlled experiments with Poisson arrivals and systematic comparison to seven baselines across multiple evaluation metrics
- **Latency Reduction (5.45%)**: High confidence - validated through both synthetic and real-world BurstGPT workloads with statistical significance
- **HAN State Abstraction**: Medium confidence - theoretical justification and architectural soundness demonstrated, but limited ablation studies on alternative encoding methods
- **Predictor Contribution (17.94% QoS)**: Medium confidence - results show measurable impact, but isolated contribution depends on predictor accuracy under varying distributions

## Next Checks
1. **Profiler Recalibration**: Implement controlled experiments varying input tokens and batch sizes for each edge expert to independently verify the latency coefficients k1,n and k2,n, ensuring the action impact estimator accurately reflects actual interference effects.

2. **Predictor Ablation Study**: Conduct systematic testing with zeroed predictions versus full predictor outputs across multiple workload distributions to isolate and quantify the exact contribution of the DistilBERT predictors to overall QoS improvement.

3. **Burstiness Generalization Test**: Deploy the trained policy on diverse real-world BurstGPT traces with varying temporal patterns to assess robustness beyond the Poisson training distribution and identify potential degradation in highly bursty scenarios.