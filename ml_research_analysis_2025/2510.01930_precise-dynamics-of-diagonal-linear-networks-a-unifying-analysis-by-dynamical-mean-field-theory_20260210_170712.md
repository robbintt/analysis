---
ver: rpa2
title: 'Precise Dynamics of Diagonal Linear Networks: A Unifying Analysis by Dynamical
  Mean-Field Theory'
arxiv_id: '2510.01930'
source_url: https://arxiv.org/abs/2510.01930
tags:
- dynamics
- equation
- dmft
- fixed
- convergence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work analyzes the gradient flow dynamics of diagonal linear
  networks (DLNs) using Dynamical Mean-Field Theory (DMFT). The authors derive a low-dimensional
  effective process that captures the high-dimensional learning dynamics and characterize
  both learning timescales and long-time behaviors.
---

# Precise Dynamics of Diagonal Linear Networks: A Unifying Analysis by Dynamical Mean-Field Theory

## Quick Facts
- arXiv ID: 2510.01930
- Source URL: https://arxiv.org/abs/2510.01930
- Reference count: 40
- Primary result: DMFT analysis reveals distinct learning phases and optimization-generalization trade-offs in diagonal linear networks

## Executive Summary
This work analyzes gradient flow dynamics of diagonal linear networks (DLNs) using Dynamical Mean-Field Theory (DMFT). The authors derive a low-dimensional effective process capturing high-dimensional learning dynamics and characterize both learning timescales and long-time behaviors. The analysis reveals distinct dynamical regimes depending on initialization scale—large initialization shows lazy→rich phase transitions, while small initialization exhibits search→descent transitions. Fixed points correspond to solutions of ℓ₁-regularized or minimum norm interpolation problems, with smaller initialization leading to sparser, better-generalizing solutions. The work establishes a fundamental trade-off between generalization performance and optimization speed.

## Method Summary
The authors study a two-layer diagonal linear network with parameterization f(x;u,v) = w^T x where w = (u² - v²)/2. They analyze gradient flow dynamics on synthetic data with sparse ground truth w_* (Bernoulli entries with P{w_* = 1} = 0.1) and real gene expression data. The loss includes ℓ₂ regularization on u and v parameters. Using DMFT, they derive an effective low-dimensional process involving order parameters m(t) = E[u_i v_i]/d and q(t) = E[(u_i² + v_i²)/2]/d that captures the high-dimensional dynamics. The analysis employs singular perturbation theory to identify learning timescales and characterizes fixed-point properties through minimum norm interpolation arguments.

## Key Results
- Large initialization (α = 2.0-5.0) shows lazy→rich phase transition around t = 2 log(α)/λ, with grokking behavior
- Small initialization (α = 0.01-0.0001) exhibits search→descent phase transition at t = Θ(log(1/α))
- Smaller initialization leads to sparser, better-generalizing solutions at fixed points
- Convergence rates are exponential with γ monotonically increasing with initialization, establishing optimization-generalization trade-off

## Why This Works (Mechanism)
The DMFT analysis works by exploiting the mean-field limit where d → ∞ with proportional asymptotics n/d → δ. In this limit, the high-dimensional dynamics reduce to low-dimensional effective equations involving order parameters that track the overlap between parameters and ground truth. The diagonal structure of the network allows exact decomposition of the gradient updates, while the mean-field approximation captures the statistical behavior of the system through self-consistent equations.

## Foundational Learning
- **Dynamical Mean-Field Theory**: Statistical physics technique for analyzing high-dimensional systems by reducing to low-dimensional effective equations. Needed to handle the computational intractability of analyzing d-dimensional gradient dynamics. Quick check: Verify DMFT equations (4a-4f) satisfy self-consistency conditions.
- **Singular Perturbation Theory**: Mathematical framework for analyzing systems with multiple timescales. Needed to identify the distinct lazy, rich, search, and descent phases. Quick check: Confirm the transition times t_c = 2 log(α)/λ and Θ(log(1/α)) match numerical observations.
- **Implicit Regularization**: Phenomenon where optimization algorithms prefer certain solutions without explicit regularization. Needed to explain why different initializations lead to different fixed points. Quick check: Compare test errors of different initialization scales at convergence.

## Architecture Onboarding
- **Component Map**: DLN: x -> (u,v) -> w=(u²-v²)/2 -> prediction; DMFT: order parameters m(t), q(t) -> effective dynamics; Loss: training error + ℓ₂ regularization on u,v
- **Critical Path**: Initialize u=v=α·1_d -> compute w -> update u,v via gradient flow -> track m(t), q(t) -> analyze convergence to fixed point
- **Design Tradeoffs**: Explicit ℓ₂ regularization enables grokking but may hurt generalization; smaller initialization improves generalization but slows convergence
- **Failure Signatures**: No grokking transition if λ=0; noisy convergence rate estimates for finite d; incorrect phase identification if initialization scale misclassified
- **First Experiments**: 1) Test lazy→rich transition at t=2 log(α)/λ for large α with λ>0; 2) Verify search→descent transition at t=Θ(log(1/α)) for small α; 3) Confirm smaller α yields better test error at fixed point

## Open Questions the Paper Calls Out
1. Can the optimization-generalization trade-off ("better solutions are harder to find") be rigorously established for general neural networks beyond DLNs?
2. Can DMFT analysis be extended to characterize training dynamics under stochastic gradient descent (SGD) rather than full-batch gradient flow?
3. Can the singular perturbation theory analysis of learning timescales be made rigorous?
4. Do the λ→0 and d→∞ limits commute in the fixed-point analysis for the unregularized, overparametrized case?

## Limitations
- Analysis restricted to diagonal linear networks, limiting generalizability to standard linear regression or nonlinear architectures
- Several key parameters unspecified (noise variance σ², regularization λ for large initialization experiments)
- Theoretical predictions assume proportional asymptotics as d → ∞ but reported experiments use finite d = 500
- The minimum norm interpolation proof assumes λ→0 and d→∞ limits commute without verification

## Confidence
- **High Confidence**: Existence of lazy→rich and search→descent phase transitions for different initialization scales
- **Medium Confidence**: Quantitative predictions for transition times and convergence rates (finite-size effects, unspecified parameters)
- **Low Confidence**: Exact generalization performance comparisons between initialization scales (depends on specific noise and regularization choices)

## Next Checks
1. Verify grokking transition by testing different λ values (e.g., λ ∈ {0.01, 0.1, 0.5}) and measuring whether t_c = 2 log(α)/λ matches observed transitions for large α
2. Test finite-size scaling by running identical experiments at d ∈ {200, 500, 1000} to assess convergence of theoretical predictions
3. Confirm minimum norm interpolation property by initializing from final DLN weights and running additional gradient flow on unregularized loss