---
ver: rpa2
title: 'Align2Act: Instruction-Tuned Models for Human-Aligned Autonomous Driving'
arxiv_id: '2510.10503'
source_url: https://arxiv.org/abs/2510.10503
tags:
- planning
- driving
- reasoning
- arxiv
- motion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes Align2Act, a motion planning framework that
  uses instruction-tuned LLMs to generate interpretable, human-aligned driving trajectories.
  By transforming driving logic into structured language-based instructions and guiding
  the model with an Align2ActChain for step-by-step reasoning, it enhances interpretability
  and alignment with human behavior.
---

# Align2Act: Instruction-Tuned Models for Human-Aligned Autonomous Driving

## Quick Facts
- arXiv ID: 2510.10503
- Source URL: https://arxiv.org/abs/2510.10503
- Authors: Kanishkha Jaisankar; Sunidhi Tandel
- Reference count: 31
- Primary result: Instruction-tuned LLM planner achieving 85.17 open-loop score on nuPlan Test14-random

## Executive Summary
Align2Act is a motion planning framework that uses instruction-tuned large language models to generate interpretable, human-aligned driving trajectories. The system transforms driving logic into structured language-based instructions and guides the model with a four-stage reasoning chain (Preliminary Planning, Collision Prediction, Traffic Context, Final Action) to enhance interpretability and alignment with human behavior. Fine-tuned on one million scenarios from the nuPlan dataset using LoRA adaptation, the framework achieves strong open-loop performance while demonstrating the potential of language-based reasoning for autonomous driving planning.

## Method Summary
The framework uses LLaMA-2-7B with LoRA fine-tuning (rank=16, ~41.6M trainable params) on nuPlan Mini Split data. Past 2s ego motion is used to predict 8s at 0.5s resolution, with ego-centric coordinate transformation and ≤32 surrounding agents. Structured text prompts encode observations, self-state, motion system, and planning instructions. The Align2ActChain decomposes planning into four semantic stages before outputting trajectory coordinates. Inference uses temperature=0, top-p=0.75 on a single NVIDIA T4 GPU.

## Key Results
- Open-loop score of 85.17 on Test14-random
- Closed-loop scores of 70.31 (non-reactive) and 66.96 (reactive) on Test14-random
- Ablation studies confirm structured reasoning improves performance over baseline LLM planners
- Efficient fine-tuning with LoRA allows adaptation on a single GPU

## Why This Works (Mechanism)

### Mechanism 1: Semantic Decomposition via Chain-of-Thought
The Align2ActChain forces the model to articulate intermediate reasoning steps (hazard assessment, traffic context) before outputting trajectory coordinates, improving alignment with human logic by conditioning the probability distribution on explicit semantic constraints rather than implicit statistical correlations.

### Mechanism 2: Instruction-Driven Behavioral Constraints
Structured natural language instructions derived from traffic rules constrain the policy search space, reducing the likelihood of generating illegal or unsafe maneuvers by attending to these instructions as hard constraints that override standard imitation learning priors.

### Mechanism 3: Efficient Domain Adaptation via LoRA
Fine-tuning a frozen LLaMA-2-7B model using Low-Rank Adaptation allows the system to acquire spatial-temporal driving logic without catastrophic forgetting or prohibitive compute costs by injecting trainable rank-decomposition matrices into attention layers.

## Foundational Learning

- **Concept: Chain-of-Thought (CoT) Prompting**
  - Why needed here: Align2Act relies entirely on the Align2ActChain to decompose planning; without understanding CoT, one cannot debug why the model generates text before coordinates
  - Quick check question: Can you explain why generating a rationale before an answer improves LLM performance on reasoning tasks?

- **Concept: Open-Loop vs. Closed-Loop Evaluation**
  - Why needed here: The paper reports high Open-Loop Scores (85.17) but lower Closed-Loop Scores (66.96); understanding this gap is critical for assessing deployability
  - Quick check question: Why might a planner that perfectly imitates a human log (high OLS) still crash in simulation (low CLS)?

- **Concept: LoRA (Low-Rank Adaptation)**
  - Why needed here: This is the architectural backbone allowing the authors to fine-tune a 7B parameter model on a single GPU
  - Quick check question: In LoRA, which weights are frozen and which are updated during backpropagation?

## Architecture Onboarding

- **Component map:** Input: Vectorized Scene (O) + Ego State (S_t) → Serializer (Text) → Prompt (Instructions I) → Core: LLaMA-2-7B (Frozen) + LoRA Adapters (Trainable, Rank 16) → Output: Head: Generates Text → Parser: Extracts (x, y, θ) → Control: LQR Controller executes parsed trajectory in nuPlan

- **Critical path:** The serialization format (converting floats to text tokens) is the highest risk point; if precision is lost here, the LLM reasons on garbage data

- **Design tradeoffs:** Interpretability vs. Latency (generating the Align2ActChain adds significant sequence length); Precision vs. Vocabulary (representing coordinates as discrete tokens likely sacrifices precision compared to continuous regression heads)

- **Failure signatures:** Hallucinated Rationale (model claims "Clear path" but outputs trajectory driving into barrier); Reactive Fragility (significant drop in R-CLS scores suggests struggles to anticipate secondary effects of own actions)

- **First 3 experiments:**
  1. Tokenization Sanity Check: Verify reconstruction error when converting numerical coordinates to tokens and back; is sub-meter resolution preserved?
  2. Ablation Reproduction: Run inference with temperature=0 on single scenario with and without Align2ActChain prompt prefix to quantify performance delta locally
  3. Latency Profiling: Measure end-to-end inference time on T4 GPU; determine if system runs at >1Hz (essential for 0.5s temporal resolution)

## Open Questions the Paper Calls Out

### Open Question 1
How can the high inference latency inherent to LLM-based planners like Align2Act be reduced to meet the strict real-time constraints required for on-vehicle deployment? The current implementation relies on a 7-billion parameter model fine-tuned with LoRA, which prioritizes adaptability and interpretability over execution speed required for closed-loop control in production hardware.

### Open Question 2
Can the integration of raw visual or LiDAR inputs improve the model's reasoning capabilities in perceptually complex scenarios where vectorized inputs may fail? The current architecture relies solely on vectorized scene representations and object states, lacking ability to interpret unstructured sensory data directly.

### Open Question 3
Can instruction-tuned models bridge the performance gap with hybrid planners in reactive closed-loop settings without sacrificing the interpretability provided by the reasoning chain? The paper indicates that while Align2Act excels in open-loop metrics, it underperforms hybrid models like PDM-Hybrid significantly in reactive closed-loop scores (66.96 vs. 91.56).

## Limitations
- High inference latency limits feasibility for on-vehicle applications without further optimization
- Absence of visual or LiDAR inputs restricts reasoning in perceptually complex environments
- Performance degradation in reactive closed-loop scenarios where anticipating agent interactions is critical

## Confidence
- **High confidence**: LoRA fine-tuning efficiency claim (41.6M trainable params, single T4 GPU) is well-supported by methodology and training stack details
- **Medium confidence**: Performance metrics (OLS=85.17, NR-CLS=70.31, R-CLS=66.96) are verifiable through nuPlan framework, but relative importance of Align2ActChain versus other architectural choices remains unclear
- **Low confidence**: Claims about enhanced "human alignment" are primarily inferred from score improvements rather than direct human preference studies or qualitative behavioral analysis

## Next Checks
1. **Precision loss analysis**: Quantify the coordinate reconstruction error when converting numerical values to text tokens and back; measure whether sub-meter accuracy is maintained throughout serialization-deserialization pipeline

2. **Cross-scenario robustness**: Test the model on scenarios where ego-initiated actions directly cause other agents to change lanes or brake; measure whether reasoning chain correctly anticipates these secondary effects or merely reacts to immediate conditions

3. **Human evaluation**: Conduct blinded comparison where human annotators judge whether trajectories generated with versus without Align2ActChain better match expected human driving patterns in complex negotiation scenarios (four-way stops, merging onto highways)