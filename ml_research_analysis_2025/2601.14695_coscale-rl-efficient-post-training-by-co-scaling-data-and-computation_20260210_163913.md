---
ver: rpa2
title: 'CoScale-RL: Efficient Post-Training by Co-Scaling Data and Computation'
arxiv_id: '2601.14695'
source_url: https://arxiv.org/abs/2601.14695
tags:
- problems
- problem
- solutions
- scaling
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CoScale-RL addresses the instability of reinforcement learning
  on weak language models by proposing a co-scaling strategy that simultaneously increases
  the number of solutions per problem in supervised fine-tuning and the number of
  rollout samples per problem in reinforcement learning. The core insight is that
  scaling up solutions per problem significantly improves the model's ability to learn
  hard problems, and scaling up rollout samples stabilizes RL training.
---

# CoScale-RL: Efficient Post-Training by Co-Scaling Data and Computation

## Quick Facts
- arXiv ID: 2601.14695
- Source URL: https://arxiv.org/abs/2601.14695
- Reference count: 40
- Authors: Yutong Chen, Jiandong Gao, Ji Wu
- One-line result: 3.76× accuracy improvement on four math reasoning benchmarks by co-scaling solutions per problem and rollout samples per problem

## Executive Summary
CoScale-RL addresses the instability of reinforcement learning on weak language models by proposing a co-scaling strategy that simultaneously increases the number of solutions per problem in supervised fine-tuning and the number of rollout samples per problem in reinforcement learning. The core insight is that scaling up solutions per problem significantly improves the model's ability to learn hard problems, and scaling up rollout samples stabilizes RL training. The method leverages a model merge technique called Re-distillation to combine multiple independent RL processes efficiently. Experiments show that CoScale-RL achieves an average 3.76× accuracy improvement on four math reasoning benchmarks compared to pretrained models, successfully breaking the ability boundary for a 0.5B language model on long reasoning problems with about 10K tokens per response.

## Method Summary
CoScale-RL operates on a 0.5B Qwen2.5-Instruct model and OpenMathReasoning dataset. The method iteratively applies supervised fine-tuning (SFT) with multiple solutions per problem to convert unsolvable problems into solvable ones, then reinforcement learning (GRPO) with scaled rollout samples to train on solvable problems. Problems are partitioned by difficulty and assigned different rollout sample counts. Independent RL processes per partition are merged via Re-distillation, which converts RL replay buffers into SFT data. The pipeline ends with final training from the initial model on all collected solutions to prevent catastrophic forgetting.

## Key Results
- Achieves 3.76× average accuracy improvement on four math reasoning benchmarks compared to pretrained models
- Successfully breaks ability boundary for 0.5B model on long reasoning problems (10K tokens per response)
- Demonstrates data and computational efficiency by scaling both solutions and rollouts rather than dataset size
- Shows 90 problems × 20 solutions outperforms 1800 problems × 1 solution on training set accuracy (11.5% vs 3.8%)

## Why This Works (Mechanism)

### Mechanism 1
Scaling up solutions per problem during SFT improves RL potential more effectively than scaling dataset size. Multiple diverse solutions for the same problem expose the model to varied reasoning paths, increasing the probability that at least one path becomes learnable (accuracy >5%). This converts "unsolvable" problems into "solvable" ones before RL begins. Evidence shows scaling solution count significantly improves seen problems' accuracy.

### Mechanism 2
Increasing Rollout N (samples per problem during RL) stabilizes training by reducing gradient noise from all-zero reward batches. For hard problems with low accuracy p, small Rollout N yields mostly zero-reward samples, producing no valid gradient. Theoretical analysis shows computational efficiency E = (η/N)μp - (η/N)²μn; larger N keeps η/N near optimal point, preventing instability. Experiments demonstrate adjusted Rollout N improves Pass@16 vs. fixed N.

### Mechanism 3
Re-distillation compresses multiple independent RL processes into a unified model, enabling dynamic hyperparameter assignment per problem subset. After independent RL runs, collect the last 100 correct solutions per problem from replay buffers and fine-tune the initial policy. This converts heterogeneous RL updates into a single SFT dataset, avoiding interference between subsets with different optimal η/N. The approach efficiently merges diverse reasoning strategies.

## Foundational Learning

- **Pass@K metric**:
  - Why needed here: Core evaluation for "solvable" status (Pass@16 threshold) and RL potential measurement; distinguishes ability boundary from sampling variance
  - Quick check question: Given 16 samples from a model with 5% accuracy on a problem, what's the probability of at least one correct answer? (~55%)

- **GRPO (Group Relative Policy Optimization)**:
  - Why needed here: Base RL algorithm; uses group-level advantage normalization across rollout samples from same problem
  - Quick check question: Why does GRPO compute advantages relative to group mean rather than using absolute rewards? (Reduces variance, handles heterogeneous reward scales)

- **Catastrophic forgetting in iterative fine-tuning**:
  - Why needed here: Iterative SFT→RL cycles risk losing solutions from earlier iterations; addressed by final retraining from initial policy
  - Quick check question: In iteration 2, why might the model lose ability to solve problems from iteration 0? (Weights updated away from earlier solution representations)

## Architecture Onboarding

- **Component map**:
  - Unsolvable problems → SFT with multiple solutions → Solvable problems (≥5% accuracy) → Partition by difficulty → Independent RL per group with subset-specific Rollout N → Re-distillation → Solved problems (≥70% accuracy) → Final training from initial model

- **Critical path**:
  1. Warmup SFT (920 solutions, 10 problems) → establish deep-thinking output mode
  2. Evaluate problems → label solvable (accuracy 5-70%)
  3. SFT on new solutions for unsolvable pool
  4. Partition solvable problems by difficulty → assign Rollout N per group
  5. Independent RL per group → double N on plateau/failure
  6. Re-distill correct solutions → merge groups
  7. Repeat until budget exhausted
  8. Final training from initial policy on all solutions (avoids catastrophic forgetting)

- **Design tradeoffs**:
  - Fewer problems with more solutions vs. more problems with fewer solutions: Paper shows 90 problems × 20 solutions outperforms 1800 problems × 1 solution on training set accuracy (11.5% vs 3.8%)
  - Independent RL per subset vs. unified training: Independence enables optimal η/N per subset but blocks cross-problem learning
  - Fixed η with variable N vs. variable η: Paper keeps η fixed (2e-6) and scales N for practical simplicity

- **Failure signatures**:
  - RL plateau: Reward curve flattens for >20 steps → double Rollout N for that subset
  - All-zero rewards: Problem accuracy <5% with current N → move to unsolvable pool, add SFT solutions
  - Catastrophic forgetting: Performance drops on earlier-iteration problems → trigger final retraining from initial policy

- **First 3 experiments**:
  1. Single-problem scaling test: Take 1 unsolvable AIME problem, SFT on 50 solutions, verify accuracy reaches 1-2%, then RL with N=512 to 80% accuracy. Confirms ability boundary can be broken.
  2. Data scaling comparison: Compare 90 problems × M solutions (M=1,2,...,20) vs. N problems × 1 solution (N=90,180,...,1800). Measure Pass@1 and Pass@16 on training problems to quantify RL potential.
  3. Computational efficiency validation: Split 10 solvable problems into 4 difficulty groups, run independent RL with N=40/80/80/160 vs. unified RL with N=80. Compare Pass@16 vs. total rollouts to verify efficiency gain.

## Open Questions the Paper Calls Out

### Open Question 1
Why does the performance gain from CoScale-RL fail to generalize effectively to general reasoning tasks (Reasoning GYM) despite significant improvements in mathematical reasoning? The paper identifies the domain gap but does not investigate whether the instability is due to the discrete nature of math rewards, the model's pre-training distribution, or the specific reasoning patterns required.

### Open Question 2
Can the optimal η/N ratio (learning rate to Rollout N) be determined dynamically and automatically for each problem rather than relying on manual heuristics? The theoretical framework suggests an optimal efficiency point exists, but the authors state that practically adjusting this for every problem is "nearly impossible," leaving a heuristic gap.

### Open Question 3
What is the trade-off frontier between scaling solutions per problem (depth) versus scaling the problem set (breadth) for out-of-distribution generalization? The paper demonstrates that depth is efficient for learning hard problems, but it does not quantify the point at which prioritizing depth over breadth begins to harm the model's ability to generalize to unseen problem types.

### Open Question 4
Does the efficiency of Re-distillation for merging RL processes degrade as the diversity of reasoning paths increases across different problem groups? While Re-distillation recovers performance, it is unclear if compressing diverse, independent strategies into a single SFT dataset loses the nuanced exploration benefits of a unified RL process.

## Limitations

- The core empirical claim is grounded in a single dataset (OpenMathReasoning) and a single small model (0.5B Qwen2.5-Instruct)
- The scaling theorems are asymptotic; in practice, the hardest problems still require >10K rollouts for p=0.1% accuracy, which may be infeasible
- The multi-solution SFT mechanism assumes diverse solutions encode orthogonal reasoning paths, but lacks ablations per solution diversity metric
- Catastrophic forgetting is mitigated only by a single final retraining step; no continual learning evaluation is provided

## Confidence

- **High**: The co-scaling principle (more solutions → higher solvability; more rollouts → RL stability) is empirically validated on the training set and cross-validated on four held-out reasoning benchmarks. Theorem 3.1 correctly predicts the η/N efficiency optimum.
- **Medium**: The claim that 90 problems × 20 solutions beats 1800 problems × 1 solution is robust within the OpenMathReasoning combinatorial subset but may not generalize to broader reasoning domains. Re-distillation compression is theoretically sound but lacks ablation on alternative merge strategies.
- **Low**: Generalization to larger models (>7B) and other reasoning tasks (e.g., code, commonsense) is untested. The assumption that all unsolved problems simply need more solutions may not hold for genuinely unsolvable problem types.

## Next Checks

1. **Generalization to Larger Models**: Run CoScale-RL on a 7B or 13B model using the same OpenMathReasoning corpus and measure whether the 3.76× gain scales or saturates.
2. **Ablation of Solution Diversity**: For a fixed set of 50 problems, generate solutions with varying diversity scores (e.g., via embedding similarity) and test whether diversity or raw count drives solvability improvements.
3. **Cross-Domain Transfer**: Apply the CoScale-RL pipeline to a non-mathematical reasoning benchmark (e.g., strategy game reasoning or multi-hop commonsense QA) and report Pass@16 changes.