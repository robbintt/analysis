---
ver: rpa2
title: 'The Impact of Code-switched Synthetic Data Quality is Task Dependent: Insights
  from MT and ASR'
arxiv_id: '2503.23576'
source_url: https://arxiv.org/abs/2503.23576
tags:
- data
- sentences
- augmentation
- proceedings
- pages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares multiple code-switched (CSW) data augmentation
  techniques for machine translation (MT), automatic speech recognition (ASR), and
  speech translation (ST) tasks. The authors evaluate techniques including lexical
  replacements, linguistic theories, and back-translation, and compare them to baseline
  models trained on monolingual data.
---

# The Impact of Code-switched Synthetic Data Quality is Task Dependent: Insights from MT and ASR

## Quick Facts
- **arXiv ID**: 2503.23576
- **Source URL**: https://arxiv.org/abs/2503.23576
- **Reference count**: 21
- **Key outcome**: Back-translation and predictive lexical replacements improve both MT and ASR, but naturalness of synthetic data correlates with MT improvements (r=0.92-0.97) but not ASR improvements (r=0.19 to -0.56).

## Executive Summary
This paper investigates how the quality of code-switched (CSW) synthetic data affects performance across three NLP tasks: machine translation (MT), automatic speech recognition (ASR), and speech translation (ST). The authors compare multiple augmentation techniques including lexical replacements, linguistic theories (Equivalence Constraint and Matrix Language Frame), and back-translation. They find that while back-translation and predictive-based lexical replacements perform best across all tasks, the relationship between synthetic data quality and task performance differs dramatically: naturalness strongly correlates with MT improvements but shows no correlation with ASR improvements.

## Method Summary
The study uses Arabic-English CSW data from multiple LDC corpora (309k parallel sentences) and the ArzEn-ST corpus (3.3k train, 1.4k dev, 1.4k test). For ASR, they employ ESPnet with joint CTC/attention using Transformer architecture (12 encoder/6 decoder blocks). For MT, they use Fairseq Transformer models based on prior work. Six augmentation techniques are evaluated: LEXDict (dictionary-based replacement), LEXRand (random replacement), LEXPred (classifier-based prediction), EC/ML (linguistic theories), and BT (back-translation). The study uses constrained experiments with 55k sentences to analyze quality-performance relationships.

## Key Results
- Back-translation (BT) and LEXPred achieve best or near-best results across ASR, MT, and ST tasks
- For MT, naturalness of synthetic data shows strong positive correlation with improvements (r=0.92-0.97, p<0.05)
- For ASR, no correlation exists between naturalness and improvements (r=0.19 to -0.56, p=0.73 and p=0.15)
- LEXRand performs competitively for ASR despite lacking linguistic knowledge, achieving 2.8% absolute WER reduction
- Random lexical replacement correlates strongly with ASR improvement through perplexity and OOV rate reduction (r=0.89 and r=0.84, p<0.01)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Synthetic data naturalness correlates with downstream improvements for MT, but not for ASR
- Mechanism: MT models benefit from semantically coherent, fluent code-switched text because translation requires preserving meaning while restructuring syntax. ASR models primarily need exposure to code-switched vocabulary patterns and phonetic transitions; semantic coherence is secondary to acoustic modeling
- Core assumption: The correlation reflects a causal relationship between data quality and task performance, not a confounding factor
- Evidence anchors:
  - [abstract] "For MT... a strong positive correlation between naturalness of the generated CSW data and improvements on MT. For ASR... no correlation between naturalness... and improvements on ASR."
  - [section 5.4] Reports MT correlations of 0.92 (p < 0.05) and 0.97 (p < 0.05); ASR correlations of 0.19 (p = 0.73) and -0.56 (p = 0.15)
  - [corpus] Weak external validation; related papers focus on CSW generation techniques but not quality-performance correlations
- Break condition: If ASR baselines achieve low WER on CSW segments (<30%), naturalness may become predictive

### Mechanism 2
- Claim: Random lexical replacements perform competitively for ASR even without linguistic knowledge
- Mechanism: ASR language models benefit from vocabulary coverage and reduced OOV rates more than syntactic well-formedness. Random injection introduces diverse CSW patterns that lower perplexity on test sets regardless of grammaticality
- Core assumption: OOV reduction and perplexity capture the primary benefit of augmentation for ASR
- Evidence anchors:
  - [section 5.1] LEXRand achieves 2.8% absolute WER reduction over zero-shot baseline, competitive with linguistic approaches
  - [section 6.3] Strong correlations of 0.89 (p = 0.003) between ASR improvement and perplexity, 0.84 (p = 0.008) with OOV rate
  - [corpus] Hussein et al. (2023) cited in paper confirms random lexical replacements outperform equivalence constraint theory for ASR
- Break condition: If target language pair has complex morphological interactions at switch points, random replacement may degrade performance

### Mechanism 3
- Claim: Back-translation and predictive lexical replacement excel across tasks because they optimize both diversity and quality
- Mechanism: BT generates naturally occurring CSW patterns from trained models; LEXPred uses a classifier to identify plausible switch points. Both produce lower perplexity while maintaining higher human-rated naturalness than rule-based approaches
- Core assumption: The combination of diversity (captured by perplexity) and quality (captured by human evaluation) drives consistent cross-task performance
- Evidence anchors:
  - [section 5.1-5.3] BT and LEXPred achieve best or near-best results across ASR, MT, and ST tasks
  - [figure 3] BT achieves 80.0% naturalness rating; LEXPred achieves 66.6%; both outperform random approaches
  - [corpus] Kuwanto et al. (2024) show LLM-based EC-informed generation provides only slight improvements, suggesting task-specific optimization matters more than theory adherence
- Break condition: If no CSW corpus exists for training the predictive model or BT system, these methods become inapplicable

## Foundational Learning

- Concept: Code-switching linguistic theories (Equivalence Constraint, Matrix Language Frame)
  - Why needed here: The paper compares theory-based generation against heuristic approaches; understanding EC and MLF helps interpret why they underperform for ASR
  - Quick check question: Can you explain why EC-based generation might produce more natural CSW but not improve ASR WER?

- Concept: Back-translation for data augmentation
  - Why needed here: BT is the strongest performing technique across tasks; understanding its mechanics is essential for implementation
  - Quick check question: How would you train a BT model to generate Arabic-English CSW from English monolingual data?

- Concept: Perplexity vs. human evaluation as quality metrics
  - Why needed here: The paper demonstrates these metrics capture different properties; perplexity correlates with ASR performance while human naturalness correlates with MT
  - Quick check question: If a synthetic dataset has low perplexity but low human naturalness scores, which task would you expect it to help more?

## Architecture Onboarding

- Component map:
  - Augmentation module: LEXDict, LEXRand, LEXPred, EC/ML generators, BT pipeline
  - ASR system: ESPnet joint CTC/attention, Transformer encoder (12 blocks)/decoder (6 blocks), RNNLM for rescoring
  - MT system: Fairseq Transformer (from prior work)
  - ST cascade: ASR output → MT model
  - Evaluation: WER/CER for ASR, BLEU/chrF++/BERTScore for MT/ST, human MOS for naturalness

- Critical path:
  1. Generate CSW synthetic data using selected augmentation technique
  2. Add synthetic text to LM training data for ASR rescoring (or MT training corpus)
  3. Train/fine-tune models with augmented data
  4. Evaluate on CSW test subset specifically (not just overall metrics)

- Design tradeoffs:
  - Zero-shot vs. non-zero-shot: Zero-shot requires no CSW corpus but limits available techniques; non-zero-shot enables BT and LEXPred
  - Quality vs. quantity: Constrained experiments (55k sentences) show different patterns than full augmentation (125k-270k sentences)
  - Linguistic theory vs. simplicity: EC/ML approaches add complexity without consistent gains over random replacement

- Failure signatures:
  - LEXDict degrades MT performance (alters semantics via dictionary glosses)
  - High naturalness but low perplexity reduction → may help MT but not ASR
  - Low baseline performance on CSW segments (>60% WER) → naturalness becomes irrelevant; focus on vocabulary coverage

- First 3 experiments:
  1. Reproduce zero-shot ASR with LEXRand only (simplest competitive approach) to establish baseline
  2. Compare LEXPred vs. random replacement on held-out CSW test set to validate switch-point prediction value
  3. Measure perplexity and human naturalness on generated data to predict which task (ASR vs. MT) will benefit more before running full training

## Open Questions the Paper Calls Out

- Question: Do the findings regarding the task-dependent impact of synthetic data quality (specifically the correlation between naturalness and performance) generalize to language pairs other than Arabic-English?
  - Basis in paper: [explicit] The authors acknowledge in the Limitations section that "coverage is limited to one language pair" and that "Further research is needed to assess the generalizability of our findings across different languages."
  - Why unresolved: The entire experimental setup relied exclusively on Arabic-English corpora (ArzEn-ST), leaving the behavior of other language pairs unknown
  - What evidence would resolve it: Replicating the comparative augmentation study on typologically distinct language pairs and analyzing if the lack of correlation between naturalness and ASR performance holds

- Question: How effective are Large Language Models (LLMs) for code-switched data augmentation compared to the traditional techniques analyzed in this study?
  - Basis in paper: [explicit] The Conclusion states the authors plan to expand investigated approaches "with a focus on utilizing large language models," and the Limitations section notes that "Further research is needed to assess its effectiveness compared to the approaches presented in this work."
  - Why unresolved: The current study focused on lexical replacements, linguistic theories, and back-translation, but did not evaluate LLM-based generation
  - What evidence would resolve it: A comparative evaluation measuring the naturalness and downstream task improvement (MT/ASR) of synthetic data generated by LLMs versus the methods presented (e.g., Back-Translation)

- Question: Does the initial performance of a baseline model on code-switching influence the importance of synthetic data quality for downstream improvements?
  - Basis in paper: [explicit] Section 6.2 hypothesizes that the discrepancy in quality-performance correlation between MT and ASR may be because "quality might be less relevant to low-performing models."
  - Why unresolved: The authors conducted an error analysis on only 100 sentences and suggested this factor, but did not empirically verify if baseline proficiency levels change the efficacy of high-quality augmentation
  - What evidence would resolve it: Experiments that control baseline performance levels (e.g., comparing high-performing vs. low-performing initial models) and measuring the relative gains from high-naturalness vs. low-naturalness synthetic data

## Limitations
- The study is limited to one language pair (Arabic-English), restricting generalizability to other typologically distinct languages
- Human naturalness evaluation covers only 150 sentences rated by 3 annotators, which may not capture full variability in synthetic data quality
- The ASR baseline achieves relatively high WER (64.5%) on CSW segments, suggesting performance may be constrained by more fundamental modeling challenges beyond data quality

## Confidence

- **High confidence**: MT results showing strong correlation between naturalness and improvements, with BT and LEXPred as top performers
- **Medium confidence**: ASR results showing random replacement as competitive, with no correlation to naturalness, though baseline WER is high
- **Medium confidence**: The mechanism explaining why MT benefits from quality while ASR benefits from quantity/coverage

## Next Checks
1. **Baseline performance sensitivity test**: Vary the zero-shot ASR baseline performance (e.g., by adding different amounts of CSW data) to determine the threshold where naturalness correlations emerge for ASR
2. **Cross-linguistic validation**: Replicate experiments with a different language pair (e.g., Spanish-English) to test whether task-dependent patterns hold across typologically distinct languages
3. **Naturalness sample expansion**: Increase human evaluation to 300+ sentences with 5+ annotators to verify the stability of correlation patterns, particularly for ASR