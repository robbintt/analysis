---
ver: rpa2
title: 'Embedding Generative AI into Systems Analysis and Design Curriculum: Framework,
  Case Study, and Cross-Campus Empirical Evidence'
arxiv_id: '2511.17515'
source_url: https://arxiv.org/abs/2511.17515
tags:
- accessibility
- students
- data
- groups
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SAGE addresses the gap in AI pedagogy for systems analysis by embedding
  GenAI into curriculum to develop critical orchestration skills rather than passive
  acceptance. Implemented across four Australian universities with 18 student groups,
  SAGE trains students to systematically accept, modify, or reject AI outputs.
---

# Embedding Generative AI into Systems Analysis and Design Curriculum: Framework, Case Study, and Cross-Campus Empirical Evidence

## Quick Facts
- arXiv ID: 2511.17515
- Source URL: https://arxiv.org/abs/2511.17515
- Reference count: 19
- Primary result: SAGE trains students to systematically accept, modify, or reject AI outputs, with 84% showing selective judgment but none proactively identifying overlooked gaps.

## Executive Summary
This paper introduces SAGE (Systematic Analysis with Generative AI for Education), a pedagogical framework that embeds generative AI into Systems Analysis and Design (SAD) curriculum to develop critical orchestration skills rather than passive acceptance. Implemented across four Australian universities with 18 student groups analyzing the GreenHarvest case study, SAGE trains students to systematically accept, modify, or reject AI outputs. The framework addresses a critical gap in AI pedagogy by teaching students to critically evaluate AI-generated system specifications rather than treating AI as a replacement for human judgment.

The study reveals that while students can effectively critique AI outputs when provided with baseline manual specifications, accessibility awareness proves fragile across different system abstraction layers without explicit scaffolding. Students showed high accessibility awareness in requirements (85%) but dropped to 10% in architectural diagrams without prompts. The research identifies three key educator implications: requiring documented AI decision reasoning, embedding accessibility prompts at each development stage, and having students compare AI and manual specifications to identify gaps.

## Method Summary
The SAGE framework was implemented across four Australian universities with 18 student groups analyzing the GreenHarvest case study. Students completed three tasks: synthesizing requirements from user stories, generating and correcting data flow diagrams, and analyzing AI feedback on interface designs. The framework used structured templates requiring justification for accepting, modifying, or rejecting AI suggestions, with a mandated distribution of 4-3-3 decisions across these categories. Students created manual baseline specifications before interacting with AI outputs, enabling comparison and error detection. The study employed competency rubrics evaluating analytical, contextual, and reflective skills, with data collected through student artifacts, surveys, and instructor observations.

## Key Results
- Accessibility awareness proved fragile across system abstraction layers, dropping from 85% in requirements to 10% in architectural diagrams without scaffolding
- All groups successfully identified multiple AI errors when provided with manual baseline specifications
- 55% of groups struggled with AI misclassification of system boundaries, 45% missed data management errors, and 55% overlooked exception handling
- Students showed selective judgment (84%) but none proactively identified overlooked gaps, indicating a "competency ceiling"

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Accessibility awareness is likely scaffold-dependent and stimulus-bound, failing to transfer automatically across system abstraction layers without explicit cueing.
- **Mechanism:** SAGE introduces a "U-Curve" pressure by varying scaffolding. Students initially show high accessibility awareness (85%) when explicitly prompted in requirements, suppress this awareness (10%) when prompts are removed in architectural diagramming, and recover (90%) when prompts return in interface design. This suggests the competency remains latent rather than internalized.
- **Core assumption:** Students treat accessibility as an interface property rather than an architectural quality, and formal modeling notations (like DFDs) lack salient patterns for encoding user capability variations.
- **Evidence anchors:**
  - [abstract] "accessibility awareness proved fragile... dropping to 10% in architectural diagrams without scaffolding"
  - [section 6.2] "This U-curve pattern (85% to 10% to 90%) reveals that students conceptualize accessibility primarily at interaction layers... but struggle to embed accessibility systematically into system architecture."
  - [corpus] Weak direct validation; neighbor papers discuss accessibility accommodations generally but do not model this specific layer-transfer failure mode.
- **Break condition:** If accessibility persists at >50% in architectural diagrams without explicit prompts, the stimulus-dependence hypothesis breaks.

### Mechanism 2
- **Claim:** Mandated pre-AI baseline artifacts create a "comparison obligation" that forces students to form independent mental models before evaluating AI outputs.
- **Mechanism:** By requiring students to author manual specifications (Phase 1) before seeing AI translations (Phase 2), the framework establishes a reference expectation. This "gap analysis" approach shifts the student role from passive consumer to active auditor, enabling 100% detection of at least one AI error.
- **Core assumption:** Students cannot effectively critique AI outputs without a pre-existing internal standard; exposure to AI answers first creates anchoring bias.
- **Evidence anchors:**
  - [abstract] "SAGE trains students to systematically accept, modify, or reject AI outputs."
  - [section 6.3] "All Brisbane groups successfully identified multiple AI errors... The structured template approach proved effective for enabling student identification and correction."
  - [corpus] "Encouraging Students' Responsible Use of GenAI" supports the risk of over-reliance but does not validate the specific baseline-artifact intervention.
- **Break condition:** If students who skip the manual baseline phase perform equally well on error detection, the comparison mechanism is not the causal driver.

### Mechanism 3
- **Claim:** Enforcing a "4-3-3" decision distribution (Implement-Modify-Reject) prevents passive acceptance and forces the development of situated judgment.
- **Mechanism:** The experimental design in Task 3 mandates rejection of specific AI suggestions. This constraint forces students to hunt for context-specific reasons to override AI, building confidence in "contextual constraints" zones (e.g., rejecting voice interfaces in noisy supermarkets).
- **Core assumption:** Students will default to agreeing with high-severity AI ratings unless forced by external constraints to justify rejection.
- **Evidence anchors:**
  - [section 4.3.2] "The mandated rejection quota is pedagogically critical—it forces students to find instances where their contextual expertise supersedes AI’s generic recommendations."
  - [section 6.4] "Context-specific AI suggestions faced systematic rejection based on situated reasoning... Channel modality mismatches achieved 100% rejection rates."
  - [corpus] No specific validation for mandated distribution quotas found in corpus neighbors.
- **Break condition:** If students fabricate justifications to meet the quota without genuine critical analysis, the mechanism degrades into a compliance game.

## Foundational Learning

- **Concept: System Boundary Logic**
  - **Why needed here:** The study identifies that 55% of groups struggled to identify when AI misclassified system boundaries (internal vs. external).
  - **Quick check question:** Can you distinguish why a "Payment Gateway" might be modeled differently from a "Cashier" in a DFD?

- **Concept: State Management (CRUD)**
  - **Why needed here:** 45% of students missed data management errors, particularly regarding write-back operations to data stores (Create/Read/Update/Delete).
  - **Quick check question:** If a process reads inventory to display an item, what data flow must exist if a customer buys that item?

- **Concept: Exception Handling**
  - **Why needed here:** 55% of groups overlooked missing exception pathways (timeouts, payment failures) in architectural models.
  - **Quick check question:** What happens to the user's session if the network connection drops during the "Confirm Order" process?

## Architecture Onboarding

- **Component map:**
  - Case Study (GreenHarvest) -> Manual Specification Templates (User Stories, Process Descriptions) -> Standardized Prompts -> AI Model -> Raw Output -> Decision Matrices (Accept/Modify/Reject) + Justification Logs -> Competency Rubrics (Analytical, Contextual, Reflective)

- **Critical path:**
  The manual baseline creation is the rate-limiting step. Do not expose students to the AI generation phase until the baseline artifacts are committed and locked. This prevents anchoring on AI suggestions.

- **Design tradeoffs:**
  - **Rigor vs. Flow:** The structured templates (justifications, confidence ratings) significantly increase marking time and cognitive load but are essential for making "invisible orchestration processes observable."
  - **Scaffolding vs. Transfer:** Over-scaffolding (constant prompts) creates dependency; the framework recommends variable scaffolding (present in Req/Design, absent in Architecture) to test for skill transfer, knowing performance will likely dip.

- **Failure signatures:**
  - **The "Passive Acceptor":** High AI acceptance rates, low justification quality (mean 0.5/3), generic reasoning.
  - **The "Competency Ceiling":** Students reach "Balanced Integrator" status but cannot achieve "Critical Synthesizer" (proactive gap finding). They merge AI/human work well but cannot generate novel insights beyond the provided sources.
  - **The "Accessibility Trough":** A sudden drop in accessibility consideration specifically in architectural diagrams (DFDs) despite high performance in requirements and interface tasks.

- **First 3 experiments:**
  1. **Requirements Synthesis (Task 1):** Ask students to write user stories, then generate AI stories, then synthesize. Verify they include "Hybrid" requirements rather than just copying AI.
  2. **DFD Correction (Task 2):** Provide a structured process description and an AI-generated DFD with seeded errors (boundary, state, exception). Measure if students can find the "missing write-back" flows.
  3. **Constraint-Based Rejection (Task 3):** Give students a UI design and AI feedback. Enforce a rule: "You must reject at least 3 suggestions based on physical environment constraints." Check if they can identify context mismatches (e.g., hover states on a touch kiosk).

## Open Questions the Paper Calls Out
None

## Limitations
- The study's artificial design mandate requiring students to reject specific AI suggestions may inflate critical engagement beyond naturally occurring behavior
- The 18-student sample, while drawn from four universities, limits generalizability across different contexts and institution types
- The "accessibility trough" finding relies on a single prompting strategy; alternative framing might yield different architectural integration patterns

## Confidence
- **High Confidence:** The baseline artifact mechanism (pre-AI manual specifications enabling error detection) is strongly supported by the data showing 100% error identification rates when this structure is used
- **Medium Confidence:** The stimulus-dependence hypothesis for accessibility awareness (the "U-curve" pattern) is plausible but could reflect task complexity differences rather than cueing effects alone
- **Medium Confidence:** The 4-3-3 distribution requirement's effectiveness in preventing passive acceptance is supported by behavioral data but lacks external validation against un-mandated cohorts

## Next Checks
1. **A/B Test the Baseline Mandate:** Run parallel cohorts where one group creates manual specifications before AI exposure and another sees AI outputs first, then measures error detection rates and critical reasoning quality
2. **Transfer Test for Accessibility:** Design a new architectural task (e.g., class diagrams or component models) with and without accessibility prompts to determine if the "trough" persists across modeling notations or is specific to DFDs
3. **Naturalistic Rejection Analysis:** Remove the mandated rejection quota in a subsequent implementation and measure whether students maintain critical engagement or revert to passive acceptance patterns