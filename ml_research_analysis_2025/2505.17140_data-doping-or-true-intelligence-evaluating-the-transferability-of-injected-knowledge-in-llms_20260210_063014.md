---
ver: rpa2
title: Data Doping or True Intelligence? Evaluating the Transferability of Injected
  Knowledge in LLMs
arxiv_id: '2505.17140'
source_url: https://arxiv.org/abs/2505.17140
tags:
- knowledge
- tasks
- fact
- atomic
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how the nature of fine-tuning tasks affects
  knowledge retention and transferability in large language models (LLMs). The research
  finds that comprehension-intensive tasks like question answering and fill-in-the-blanks
  achieve substantially higher knowledge retention rates (48% and 32% respectively)
  compared to mapping-oriented tasks such as translation (17%) and text-to-JSON conversion
  (20%), despite all tasks involving identical factual content.
---

# Data Doping or True Intelligence? Evaluating the Transferability of Injected Knowledge in LLMs

## Quick Facts
- arXiv ID: 2505.17140
- Source URL: https://arxiv.org/abs/2505.17140
- Authors: Essa Jan; Moiz Ali; Muhammad Saram Hassan; Fareed Zaffar; Yasir Zaki
- Reference count: 10
- This study investigates how the nature of fine-tuning tasks affects knowledge retention and transferability in large language models (LLMs).

## Executive Summary
This study investigates how the nature of fine-tuning tasks affects knowledge retention and transferability in large language models (LLMs). The research finds that comprehension-intensive tasks like question answering and fill-in-the-blanks achieve substantially higher knowledge retention rates (48% and 32% respectively) compared to mapping-oriented tasks such as translation (17%) and text-to-JSON conversion (20%), despite all tasks involving identical factual content. This pattern persists across different model architectures and follows scaling laws, with larger models showing improved retention across all task types. However, even high-performing models show significant performance drops when applying injected knowledge to broader contexts, suggesting limited semantic integration. The findings indicate that effective knowledge injection depends not just on data exposure but on the cognitive engagement required during fine-tuning, highlighting the importance of task selection for efficient and meaningful model updates.

## Method Summary
The study curated 126 atomic facts from 2024 events, ensuring no prior model knowledge, and formatted them into four task types: question answering, fill-in-the-blanks, translation, and text-to-JSON. Each model (Gemini-1.5-Flash, Llama 3.2-3B, Gemma 3-4B-IT, Phi-3.5-Mini, Qwen 2.5 at 1.5B/3B/32B/72B) was fine-tuned separately on each task type. Evaluation used two test sets: 126 direct questions (rephrased, minimal lexical overlap) and 126 generic questions (comprehension-oriented, crowdsourced via Prolific). Knowledge retention was measured as the percentage of correct answers, with responses evaluated by GPT-4o-Mini judge using chain-of-thought reasoning (94% accuracy vs human annotations).

## Key Results
- Comprehension-intensive tasks (QA and fill-in-the-blanks) achieved substantially higher knowledge retention rates (48% and 32%) compared to mapping-oriented tasks (translation 17%, text-to-JSON 20%) despite identical factual content.
- Larger models consistently retained more knowledge across all task types, following scaling laws (Qwen 2.5: 1.5B at 38% QA vs. 72B at 72% QA).
- All models showed significant performance drops on generic questions requiring broader context application, indicating limited semantic integration despite successful fact recall.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Comprehension-intensive fine-tuning tasks (question answering, fill-in-the-blanks) yield significantly higher knowledge retention than mapping-oriented tasks (translation, text-to-JSON), even when the underlying factual content is identical.
- Mechanism: Tasks requiring "depth of cognitive engagement" force the model to form deeper, more accessible internal representations. In contrast, mapping-oriented tasks can be solved via surface-level pattern matching (A→B mapping) without requiring genuine semantic internalization. The paper shows QA tasks achieved 48% average retention versus 17% for translation and 20% for text-to-JSON.
- Core assumption: The model's ability to correctly answer rephrased direct questions is a valid proxy for "knowledge internalization."
- Evidence anchors:
  - [abstract] "comprehension-intensive fine-tuning tasks (e.g., question answering and blanks) achieve substantially higher knowledge retention rates (48%) compared to mapping-oriented tasks like translation (17%)"
  - [Page 3] "This finding is particularly noteworthy as translation and text-to-JSON expose models to identical factual content as understanding-based tasks, yet result in significantly diminished knowledge retention."
  - [corpus] Related work (Yang et al., 2024, cited on Page 2) supports task-dependent generalization behaviors, though direct corpus evidence explaining *why* comprehension tasks outperform mapping tasks is weak.
- Break condition: If models fine-tuned on mapping tasks achieved comparable performance on generic reasoning questions via efficient lookup mechanisms, the "comprehension = deeper integration" inference would be challenged.

### Mechanism 2
- Claim: All models exhibit significant performance degradation when applying injected knowledge to broader contexts (generic questions), suggesting limited semantic integration despite successful fact recall.
- Mechanism: Fine-tuning successfully injects facts for direct recall, but the knowledge is not deeply integrated into the model's broader semantic network, limiting reasoning in novel scenarios. QA-tuned models dropped from ~48% (direct) to ~24% (generic).
- Core assumption: Performance on generic questions requiring indirect application reliably measures "true semantic integration" rather than simply reflecting task difficulty.
- Evidence anchors:
  - [abstract] "all models exhibit substantial performance drops when applying injected knowledge in broader contexts, suggesting limited semantic integration despite successful fact recall"
  - [Page 4] "all models show a decline in accuracy when tasked with applying knowledge in more general contexts... suggesting that even when models appear to retain factual information, their ability to apply this knowledge—demonstrating true semantic integration—remains limited"
  - [corpus] The paper frames generic questions as addressing limitations of standard benchmarking noted in prior work (Cao et al., 2025, cited on Page 2).
- Break condition: If models performed well on generic questions but poorly on direct recall, it would suggest knowledge was integrated but inaccessible via simple probes, challenging the recall-as-prerequisite assumption.

### Mechanism 3
- Claim: Knowledge retention scales positively with model size across all tested task types.
- Mechanism: Larger models (e.g., 72B parameter Qwen) consistently retain more knowledge than smaller ones (1.5B) for both comprehension and mapping tasks. QA performance scaled from 38% (1.5B) to 72% (72B). This aligns with neural scaling laws—larger capacity enables better parameterization of new information regardless of injection method.
- Core assumption: The scaling trend generalizes beyond the Qwen 2.5 family tested.
- Evidence anchors:
  - [abstract] "This pattern persists across model architectures and follows scaling laws, with larger models showing improved retention across all task types"
  - [Page 4, Table 4] Shows monotonic improvement: Qwen 2.5-1.5B (38% QA), 3B (45%), 32B (63%), 72B (72%)
  - [corpus] Weak—the paper's limitations explicitly note scaling analysis was constrained to Qwen 2.5, leaving cross-architecture generalization unconfirmed.
- Break condition: If scaling plateaus or reverses at extreme scales for certain task types, the "larger is always better" generalization would break.

## Foundational Learning

- Concept: **Supervised Fine-Tuning (SFT) as Knowledge Injection**
  - Why needed here: The paper uses SFT (not RAG or continual pre-training) as the mechanism for embedding new facts into model parameters. Understanding this distinction is essential for interpreting results.
  - Quick check question: Why does SFT embed knowledge into parameters while RAG does not?

- Concept: **Probing vs. Benchmarking for Evaluation**
  - Why needed here: The paper explicitly distinguishes direct questions (recall-based) from generic questions (application-based) to assess surface memorization versus semantic integration.
  - Quick check question: What is the limitation of benchmark-style evaluation that generic questions attempt to address?

- Concept: **Neural Scaling Laws**
  - Why needed here: The paper invokes scaling laws to explain why larger models retain more knowledge, contextualizing the size-based findings.
  - Quick check question: Does the paper claim scaling laws fully explain task-type differences in retention?

## Architecture Onboarding

- Component map:
  - Training data: 126 atomic facts from 2024 events, formatted into 4 task types (QA, blanks, translation, text-to-JSON)
  - Test data: 126 direct questions (rephrased, no lexical overlap) + 126 generic questions (comprehension-oriented, crowd-sourced via Prolific)
  - Models: Gemini-1.5-Flash, Llama 3.2-3B, Gemma 3-4B-IT, Phi-3.5-Mini, Qwen 2.5 (1.5B/3B/32B/72B)
  - Judge: GPT-4o-Mini with chain-of-thought, 94% accuracy vs human annotations (Cohen's κ = 0.884)

- Critical path:
  1. Curate atomic facts from post-cutoff 2024 events (ensures no prior knowledge)
  2. Convert facts to 4 task formats using GPT-4o-mini and Google Translate API
  3. Fine-tune each model on each task type independently
  4. Evaluate on direct questions → measures retention
  5. Evaluate on generic questions → measures semantic integration

- Design tradeoffs:
  - SFT vs RAG/CPT: SFT chosen for embedding knowledge into parameters (essential for testing "true learning"), but requires more compute than RAG
  - Single model family for scaling (Qwen 2.5): Limits cross-architecture conclusions
  - GPT-4o-Mini judge: High accuracy but introduces dependency on another LLM

- Failure signatures:
  - High direct-question accuracy + low generic-question accuracy = memorization without semantic integration (observed universally)
  - Low retention on mapping tasks despite identical factual content = task type insufficiently demanding cognitive engagement

- First 3 experiments:
  1. Replicate QA vs translation comparison on a different model family (e.g., Llama variants) to test task-type effect generalization.
  2. Add a hybrid task (e.g., translation + QA on same facts) to measure whether combining mapping and comprehension yields additive or synergistic retention.
  3. Evaluate retention decay over time or after additional fine-tuning on unrelated tasks to assess knowledge persistence.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do knowledge retention patterns scale across different model architectures beyond the Qwen family?
- Basis in paper: [explicit] "Our investigation of scaling laws was constrained to the Qwen2.5 model family due to computational resources, leaving open questions about how knowledge retention scales across different architectures."
- Why unresolved: Resource constraints limited the study to one model family; architectural differences (attention mechanisms, training objectives) could produce different scaling behaviors.
- What evidence would resolve it: Replicate the four-task fine-tuning experiment across diverse architectures (e.g., Mistral, Falcon, GPT-style decoder-only models) and compare retention curves.

### Open Question 2
- Question: Which specific categories of knowledge resist internalization during fine-tuning, and why?
- Basis in paper: [explicit] "Further analysis is needed to identify specific categories of knowledge that resist internalization during fine-tuning."
- Why unresolved: The study aggregated results across seven event types without analyzing whether certain factual structures (temporal relations, numerical data, causal chains) are systematically harder to retain.
- What evidence would resolve it: Fine-grained error analysis categorizing failures by knowledge type, coupled with probing experiments targeting specific relational patterns.

### Open Question 3
- Question: Can existing techniques for improving QA-based knowledge retention (e.g., data augmentation, Sharpness-Aware Minimization) transfer effectively to mapping-oriented tasks?
- Basis in paper: [explicit] "While several techniques exist for improving knowledge retention in QA tasks, their efficacy for mapping-oriented tasks remains unexplored."
- Why unresolved: The cognitive engagement hypothesis suggests mapping tasks fundamentally engage different learning mechanisms; optimization techniques targeting comprehension may not address token-mapping limitations.
- What evidence would resolve it: Apply known retention-enhancement techniques to translation and text-to-JSON fine-tuning and measure changes in knowledge retention rates.

### Open Question 4
- Question: What mechanisms could bridge the persistent gap between direct factual recall and semantic integration in broader contexts?
- Basis in paper: [inferred] All models showed substantial performance drops on generic questions (requiring application) versus direct questions, with even QA-tuned models declining from ~48% to ~24%.
- Why unresolved: The paper demonstrates this gap is consistent across task types and model scales, but does not identify whether this reflects insufficient training, architectural constraints, or fundamental limitations in fine-tuning.
- What evidence would resolve it: Ablation studies varying training duration, multi-task fine-tuning combining recall and application objectives, and analysis of internal representations during generic versus direct querying.

## Limitations
- The study's scaling analysis was limited to the Qwen 2.5 model family, leaving cross-architecture generalization unconfirmed.
- Exact fine-tuning hyperparameters and training protocols remain unspecified, limiting precise reproduction.
- The mechanism behind why comprehension tasks outperform mapping tasks is inferred rather than empirically validated.

## Confidence
- **High confidence:** Task-type differences in knowledge retention (QA vs. translation) are robust across models tested.
- **Medium confidence:** Scaling laws improve retention across all task types, though generalization beyond Qwen 2.5 is uncertain.
- **Medium confidence:** The inference that comprehension tasks yield "deeper" knowledge integration is plausible but not definitively proven.

## Next Checks
1. Replicate QA vs. translation comparison on a different model family (e.g., Llama variants) to test task-type effect generalization.
2. Add a hybrid task (e.g., translation + QA on same facts) to measure whether combining mapping and comprehension yields additive or synergistic retention.
3. Evaluate retention decay over time or after additional fine-tuning on unrelated tasks to assess knowledge persistence.