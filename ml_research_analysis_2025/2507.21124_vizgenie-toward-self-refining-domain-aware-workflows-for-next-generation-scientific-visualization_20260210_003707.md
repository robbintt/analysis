---
ver: rpa2
title: 'VizGenie: Toward Self-Refining, Domain-Aware Workflows for Next-Generation
  Scientific Visualization'
arxiv_id: '2507.21124'
source_url: https://arxiv.org/abs/2507.21124
tags:
- visualization
- data
- vizgenie
- code
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: VizGenie is a self-improving agentic framework that advances scientific
  visualization through large language model (LLM) orchestration of dynamically generated
  visualization modules. Users interact with the system using natural language queries,
  enabling high-level feature-based requests such as "visualize the skull" or "highlight
  tissue boundaries." The framework automatically generates, validates, and integrates
  new visualization scripts (e.g., VTK Python code) to expand its capabilities on-demand,
  ensuring robustness and adaptability.
---

# VizGenie: Toward Self-Refining, Domain-Aware Workflows for Next-Generation Scientific Visualization

## Quick Facts
- arXiv ID: 2507.21124
- Source URL: https://arxiv.org/abs/2507.21124
- Reference count: 40
- Primary result: LLM-driven framework that generates and validates VTK Python visualization modules on-demand, enabling natural language queries for complex scientific datasets

## Executive Summary
VizGenie advances scientific visualization by introducing a self-improving agentic framework that translates natural language queries into dynamically generated VTK Python scripts. Users can request complex visualizations through conversational queries, and the system automatically determines whether to use existing tools or generate new code modules. The framework incorporates domain-aware image analysis via fine-tuned vision models and Retrieval-Augmented Generation (RAG) for contextual insights, establishing a sustainable workflow that continuously expands its capabilities through validated module generation and database expansion.

## Method Summary
VizGenie employs a single LLM agent using zero-shot-react-description to orchestrate tool selection and execution. The system combines pre-existing visualization tools with dynamically generated VTK modules, validated through automated execution testing before caching. For feature-based queries, it uses a fine-tuned Llama-3.2-Vision model (LoRA-adapted on expert-annotated images) to provide isovalue recommendations and captions. The framework also implements RAG to retrieve historical interactions and domain documents, enriching context for complex queries. The validation pipeline iteratively refines generated code until successful execution or maximum iteration limits.

## Key Results
- Reduced cognitive overhead for iterative visualization tasks on complex volumetric datasets
- Fine-tuned vision model achieved caption stability increase from 0.432 to 0.654 (p<0.00001) with enhanced domain-specific vocabulary
- Expanding visual database from 150 to 300 images improved semantic consistency (0.585 to 0.717) and optimal isovalue predictions
- Single-agent architecture achieved higher reliability than initial multi-agent approach (abandoned due to ~50% success rate)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combining pre-existing validated tools with dynamically generated VTK modules enables both reliability and extensibility in scientific visualization workflows.
- Mechanism: A single LLM agent uses zero-shot-react-description to select between pre-existing tools (FilterRuns, VisualizeHistogram) and dynamic tools (CodeGenerator, ModifyGeneratedCode). When queries exceed baseline capabilities, the system generates VTK Python scripts, validates them through automated execution testing, and stores successful modules in a SQLite database for future retrieval—creating a growing cache of validated visualization code.
- Core assumption: Generated code that executes without runtime errors and produces output will correctly implement user specifications.
- Evidence anchors:
  - [abstract] "Each generated script undergoes automated backend validation and is seamlessly integrated upon successful testing"
  - [Section 5.2.3] "This database acts like a cache that is later looked up by the agent to find modules that match user requirements before it decides to generate new code"
  - [corpus] PlotGen (arXiv:2502.00988) uses similar multi-agent code generation with multimodal feedback, supporting the viability of LLM-driven visualization generation
- Break condition: If user specifications are ambiguous or the LLM misinterprets domain jargon (e.g., "visualize" meaning different operations across domains), generated code may execute but produce incorrect visualizations.

### Mechanism 2
- Claim: Domain-specific fine-tuning of vision-language models via LoRA improves feature recognition and visual question answering for scientific datasets.
- Mechanism: The system generates images at multiple isovalues and camera angles, which a domain expert annotates with scientifically accurate captions. Low-Rank Adaptation (LoRA) fine-tunes the Llama-3.2-Vision model (11B parameters) on these image-caption pairs, focusing adaptation on attention projection layers while keeping most parameters frozen. The fine-tuned model then produces more accurate feature descriptions and isovalue recommendations.
- Core assumption: Expert-generated captions accurately capture domain-relevant features, and LoRA adaptation transfers this knowledge without catastrophic forgetting.
- Evidence anchors:
  - [abstract] "The system leverages image-based analysis and visual question answering (VQA) via fine-tuned vision models to interpret these queries precisely"
  - [Section 5.2.6] "Fine-tuned model produced captions with increased semantic precision and enhanced use of domain-specific vocabulary. Occurrences of 'crater' rose from 1 to 348, 'asteroid' from 0 to 1112"
  - [Section 8.1.2] "Caption stability increased significantly, with mean inter-caption semantic similarity rising from 0.432 to 0.654 (Mann-Whitney U test, p < 0.00001)"
  - [corpus] No directly comparable corpus papers evaluate vision model fine-tuning for scientific visualization VQA; mechanism remains specific to this work
- Break condition: If training captions contain errors, use terminology inconsistently (e.g., conflating "water jet" and "water plume"), or if the target domain has highly amorphous features lacking clear visual boundaries, fine-tuning may not improve—or may degrade—accuracy.

### Mechanism 3
- Claim: Expanding the visual database through asynchronous self-improvement cycles improves vocabulary richness and semantic consistency, leading to more accurate isovalue predictions.
- Mechanism: The system periodically generates additional visualization images (varying isovalues and camera angles) independent of user interactions. These images are captioned and added to the knowledge base. Larger image sets yield broader vocabulary coverage (2946 vs. 1606 unique words with 300 vs. 150 images) and higher semantic consistency across captions (mean pairwise similarity 0.717 vs. 0.585). An LLM (GPT-4o) then aggregates captions to determine optimal isovalues for specific features.
- Core assumption: More images with consistent annotations lead to better coverage of the visualization parameter space, and LLM-based aggregation accurately identifies optimal parameters.
- Evidence anchors:
  - [abstract] "demonstrate significant improvements in vocabulary richness, semantic consistency, and optimal isovalue predictions when expanding the visual database"
  - [Section 8.1.1] "Predicted optimal isovalues aligned more with the ideal isosurfaces as judged by a visualization expert: nose isovalue moved from 455 to 585 (ideal range 650-700), skull from 1365 to 1587 (ideal range 1550-1600)"
  - [corpus] Limited corpus evidence for this specific self-improvement mechanism in visualization systems
- Break condition: If additional images capture redundant or non-informative views, or if the LLM aggregator fails to correctly weight caption relevance, expanding the database may not improve—and could introduce noise into—predictions.

## Foundational Learning

- Concept: **LLM Agent Architecture (ReAct paradigm)**
  - Why needed here: VizGenie's core orchestration uses a single agent that interleaves reasoning traces with tool invocations. Understanding how agents select tools, maintain context, and iterate on failures is essential for debugging workflow issues.
  - Quick check question: Can you explain the difference between an agent's "thought" and its "action" in the ReAct framework, and why separating planning (Agent 1), execution (Agent 2), and security (Agent 3) was initially attempted but abandoned?

- Concept: **VTK (Visualization Toolkit) Pipeline Model**
  - Why needed here: All dynamically generated modules produce VTK Python code. Understanding VTK's pipeline architecture—sources, filters, mappers, actors, renderers—is required to debug generated scripts and write effective code modification prompts.
  - Quick check question: In VTK, what is the difference between a `vtkDataSet` and a `vtkActor`, and which one would you modify to change opacity mapping?

- Concept: **LoRA (Low-Rank Adaptation) Fine-tuning**
  - Why needed here: VizGenie fine-tunes Llama-Vision models using LoRA to adapt to domain-specific visualization contexts. Understanding LoRA's rank parameter, target modules (typically query/key/value projections), and tradeoffs between adaptation capacity and overfitting risk is necessary for extending fine-tuning to new domains.
  - Quick check question: Why does LoRA freeze the pre-trained model weights and inject low-rank matrices into attention layers rather than fine-tuning all parameters? What happens if the LoRA rank is set too low?

## Architecture Onboarding

- Component map:
  - LLM Agent Core -> Tool Layer -> Validation Loop -> SQLite Cache
  - Vision Model Pipeline -> Caption Database -> Isovalue Retrieval Logic
  - RAG Module -> Context Enrichment -> LLM Agent
  - Interface (PyQt5) -> Chat Panel -> Reasoning Panel -> 2D Display -> 3D VTK Window

- Critical path:
  1. User query → Agent reasoning → Tool selection
  2. If pre-existing tool matches → Execute directly → Return visualization
  3. If query exceeds baseline → CodeGenerator produces VTK script → Validation loop → Cache on success → Return visualization
  4. If feature-based query → LookupFeatureInDataset retrieves from caption database → Vision model (fine-tuned) provides isovalue recommendation → Execute visualization
  5. If context needed → RAG retrieves relevant documents/history → Enriched prompt to LLM

- Design tradeoffs:
  - **Single vs. Multi-Agent**: Initially tried orchestrator/executor/sentinel multi-agent; abandoned due to ~50% success rate. Single agent with curated tools proved more reliable.
  - **Full Regeneration vs. Incremental Modification**: Full regeneration for new scenarios (uses o3-mini with temperature=1 for creativity); incremental modification for parameter tweaks (uses GPT-4o-mini deterministic settings). Tradeoff: flexibility vs. stability.
  - **Model Selection per Task**: GPT-4o for orchestration/Q&A (stability), o3-mini for code generation (quality), GPT-4o-mini for modifications (speed). Higher-quality models increase latency.
  - **Validation Scope**: Current validation checks execution success and output presence, not correctness of user specification implementation. Full validation deferred to asynchronous self-improvement.

- Failure signatures:
  - **Code executes but produces wrong visualization**: LLM misinterpreted domain jargon or ambiguous prompt. Solution: use RAG to provide domain context; refine prompts with explicit parameter specifications.
  - **Vision model returns irrelevant captions**: Fine-tuning data insufficient or captions use inconsistent terminology. Solution: expand training set with curated, consistent annotations; verify caption quality before fine-tuning.
  - **Cache lookup returns wrong module**: Metadata matching fails. Solution: review cache schema and retrieval logic; ensure prompts are stored with sufficient semantic detail.
  - **High latency on complex queries**: Multiple LLM calls in sequence. Solution: profile timing (Table 1 shows 15-23s for code gen vs. 2-4s for pre-existing tools); consider model distillation or simpler models for routine tasks.

- First 3 experiments:
  1. **Trace a complete query-to-visualization path** with a new dataset (e.g., Hurricane Isabel). Log each tool invocation, cache lookup, code generation attempt, and validation result. Identify the latency bottleneck and any error recovery cycles.
  2. **Validate the vision model fine-tuning pipeline** on a held-out subset of an existing dataset. Generate 10 new visualization images with varying isovalues, caption them with the fine-tuned vs. out-of-the-box model, and measure caption accuracy and consistency using the LLM-as-judge protocol described in Section 8.1.2.
  3. **Test cache retrieval effectiveness** by issuing a sequence of related queries (e.g., volume render, then change colormap, then adjust opacity). Verify that the second and third queries use ModifyGeneratedCode with cached code rather than full regeneration. Measure the reduction in generation time and code stability across modifications.

## Open Questions the Paper Calls Out

- **Open Question 1**: How can domain-specific fine-tuning be automated to allow VizGenie to generalize across diverse scientific contexts without manual initiation?
  - Basis in paper: [explicit] The authors state in the "Limitations and Future Work" section that "the current domain-specific fine-tuning approach is manually initiated, limiting seamless generalization across different scientific domains."
  - Why unresolved: The current implementation requires expert curation of image-caption pairs and manual LoRA fine-tuning, creating a bottleneck for scaling to new domains.
  - What evidence would resolve it: A demonstration of an automated pipeline that detects domain shifts and triggers self-supervised fine-tuning without human-labeled captions, achieving comparable accuracy to the manual process.

- **Open Question 2**: Can model distillation or quantization techniques reduce LLM latency to support real-time interaction without compromising the validity of generated visualization code?
  - Basis in paper: [explicit] The paper lists "developing more efficient, scalable LLM architectures, such as model distillation and quantization, to reduce latency without compromising output quality" as a specific avenue for future research.
  - Why unresolved: The current system relies on large models (e.g., GPT-4o, Llama-3.2-Vision), which introduce latency (15-23s for code generation) that hinders real-time responsiveness.
  - What evidence would resolve it: Benchmarks showing that a distilled or quantized agent can execute code generation tasks with latency under a specific threshold (e.g., <5 seconds) while maintaining a validity score comparable to the baseline reported in Table 1.

- **Open Question 3**: How can vision-language models be enhanced to reliably distinguish between visually similar but semantically distinct scientific features (e.g., "water jet" vs. "water plume") in multi-variable datasets?
  - Basis in paper: [inferred] The discussion on fine-tuning evaluation notes that while the model improved, it "had a tendency to use those terms interchangeably" and failed to correctly identify variables in multi-variable contexts, resulting in only 39% accurate statements.
  - Why unresolved: The paper suggests that fine-tuning for "amorphous features and technical jargon" is harder than for natural objects (like skulls), and the model struggled to link specific visual phenomena to precise scientific terminology.
  - What evidence would resolve it: A modified VQA architecture or training regimen that achieves >80% accuracy in identifying specific physical phenomena (jargon) in held-out test sets of multi-variable simulation data.

## Limitations
- Generated code validation only checks execution success, not semantic correctness relative to user intent
- Vision model fine-tuning requires manual expert annotation, limiting scalability to new domains
- Self-improvement mechanism effectiveness depends on annotation quality and may not address cache bloat or retrieval accuracy degradation

## Confidence
- **High confidence**: The basic LLM orchestration mechanism for tool selection and code generation is well-established and demonstrably functional. The switch from multi-agent to single-agent architecture based on empirical performance data provides strong evidence for this design choice.
- **Medium confidence**: The vision model fine-tuning approach shows measurable improvements in caption quality metrics, but the practical impact on scientific visualization accuracy remains uncertain. The mechanism appears sound, but domain-specific validation is limited.
- **Low confidence**: The long-term sustainability of the self-improvement cycle and cache expansion mechanism. While the paper demonstrates immediate benefits from database expansion, it does not address potential issues with cache bloat, retrieval accuracy degradation over time, or the system's ability to unlearn outdated or incorrect modules.

## Next Checks
1. **Intent Matching Validation**: Design a study where generated visualizations are evaluated by domain experts not for execution success, but for semantic correctness relative to ambiguous natural language queries. Test whether the system correctly interprets domain-specific terminology and implicit visualization requirements.
2. **Cross-Dataset Generalization Test**: Apply the fine-tuned vision model to a completely new scientific domain (e.g., medical imaging or materials science) without additional fine-tuning. Measure caption accuracy, feature recognition capability, and isovalue recommendation quality to assess transfer learning effectiveness.
3. **Longitudinal Cache Stability Analysis**: Simulate extended use by issuing thousands of related queries across multiple sessions. Track cache growth rate, module retrieval accuracy over time, and identify whether the system maintains or degrades in performance as the database expands.