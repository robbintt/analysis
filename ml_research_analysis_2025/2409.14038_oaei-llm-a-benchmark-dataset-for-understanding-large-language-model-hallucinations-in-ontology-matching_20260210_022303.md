---
ver: rpa2
title: 'OAEI-LLM: A Benchmark Dataset for Understanding Large Language Model Hallucinations
  in Ontology Matching'
arxiv_id: '2409.14038'
source_url: https://arxiv.org/abs/2409.14038
tags:
- matching
- ontology
- hallucinations
- llms
- oaei
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces OAEI-LLM, a benchmark dataset designed to
  understand and quantify hallucinations in large language models (LLMs) when used
  for ontology matching (OM). The dataset extends the OAEI reference alignments by
  identifying LLM-specific errors, including missing true mappings, incorrect mappings,
  and misclassifications.
---

# OAEI-LLM: A Benchmark Dataset for Understanding Large Language Model Hallucinations in Ontology Matching

## Quick Facts
- arXiv ID: 2409.14038
- Source URL: https://arxiv.org/abs/2409.14038
- Reference count: 18
- Introduces OAEI-LLM, a benchmark dataset for quantifying LLM hallucinations in ontology matching

## Executive Summary
This paper presents OAEI-LLM, a benchmark dataset designed to understand and quantify hallucinations in large language models when used for ontology matching. The dataset extends the OAEI reference alignments by identifying LLM-specific errors, including missing true mappings, incorrect mappings, and misclassifications. Three types of errors are defined: missing from LLM, missing from OAEI, and incorrect mappings (with subtypes like false, disputed, align-up, and align-down). The schema is extended to record these errors per model, enabling benchmarking and fine-tuning applications.

## Method Summary
The method compares LLM-generated alignments against OAEI reference alignments using set logic to identify discrepancies. LLM-based matchers generate candidate alignments from source and target ontologies. These are compared to reference alignments to classify errors into three categories: Missing from LLM, Missing from OAEI, and Incorrect mappings. An LLM-based evaluator further categorizes incorrect mappings by semantic relationship (false, disputed, align-up, align-down). The EDOAL schema is extended with hallucination tags to record error metadata per model.

## Key Results
- Systematic classification of LLM errors in ontology matching using differential alignment comparison
- Introduction of semantic granularity classification for incorrect mappings (align-up, align-down, false, disputed)
- Extended EDOAL schema with hallucination tags for per-model error attribution
- Dataset supports benchmarking LLMs and generating training data for fine-tuning

## Why This Works (Mechanism)

### Mechanism 1: Differential Alignment Comparison
The system compares the Reference Alignment ($R_{oaei}$) against the LLM Alignment ($A_{llm}$) using set logic to classify discrepancies. If an entity from $R_{oaei}$ appears in $A_{llm}$ but is paired with a different entity, it triggers an "Incorrect" classification; if absent entirely, it triggers "Missing from LLM." The core assumption is that the OAEI Reference Alignment is sufficiently accurate to serve as ground truth, though the paper acknowledges a "disputed" category for cases where the LLM might actually be correct.

### Mechanism 2: Semantic Granularity Classification
LLM errors in ontology matching often follow predictable semantic patterns, such as mapping to a superclass or subclass rather than an equivalent entity. The framework categorizes "Incorrect" mappings into subtypes: *False* (irrelevant), *Disputed* (relevant but different), *Align-up* (mapped to superclass), and *Align-down* (mapped to subclass). This moves beyond binary accuracy to analyze the *nature* of the semantic failure, assuming LLMs possess enough semantic hierarchy knowledge that their errors are logically related rather than random noise.

### Mechanism 3: Schema-Augmented Attribution
The paper extends the standard alignment schema to record error metadata per model, enabling targeted fine-tuning and model comparison. The EDOAL mapping schema is extended with a `<hallucination>` tag within mapping cells, explicitly binding a specific LLM to a specific error type for that mapping. The core assumption is that users need to trace errors back to specific model weaknesses rather than treating the LLM as a black box.

## Foundational Learning

- **Concept: Ontology Matching (OM)**
  - Why needed here: This is the core task where hallucinations are measured. You must understand that OM involves finding semantic correspondences (mappings) between entities in different ontologies (e.g., matching "Chairman" in one schema to "Chair" in another).
  - Quick check question: If Ontology A has "Car" and Ontology B has "Automobile", what is the goal of OM?

- **Concept: Reference Alignment ($R_{oaei}$)**
  - Why needed here: This serves as the "answer key." The paper's mechanism relies entirely on comparing LLM outputs against this standard. Without understanding this, the error classification logic makes no sense.
  - Quick check question: If an LLM finds a valid mapping that is *not* in the Reference Alignment, how might the system classify it based on the "Missing from OAEI" definition?

- **Concept: LLM Hallucination (in structured data)**
  - Why needed here: Unlike general text hallucination, here it refers to logical inconsistencies or wrong entity links. You need to distinguish between "False" (irrelevant) and "Disputed" (plausible but unverified) to understand the benchmark's nuance.
  - Quick check question: Is an "Align-up" error (mapping to a superclass) a hallucination in the sense of "making things up," or a failure of specificity?

## Architecture Onboarding

- **Component map:** Source Ontology ($O_s$) -> Target Ontology ($O_t$) -> LLM Matcher -> LLM Alignment ($A_{llm}$) -> Comparator -> Reference Alignment ($R_{oaei}$) -> LLM Evaluator -> Extended EDOAL with hallucination tags

- **Critical path:** The **Matching Assessment** procedure. This is not just string matching; it relies on the "LLM-based evaluator" to determine the semantic type of the error (e.g., distinguishing *Align-up* from *False*). If this component is weak, the entire error taxonomy is invalidated.

- **Design tradeoffs:**
  - **Specificity vs. Coverage:** The system is currently constrained to **one-to-one mappings**. It ignores complex mappings (one-to-many) or subsumption relations, limiting the benchmark's scope to simpler equivalence tasks.
  - **Automation vs. Accuracy:** Using an "LLM-based evaluator" for assessment automates the pipeline but introduces circularity (using an LLM to evaluate an LLM). The paper notes human expert judgment is an alternative but likely costlier.

- **Failure signatures:**
  - **High "Disputed" count:** Indicates the original Reference Alignment ($R_{oaei}$) may be incomplete or subjective, rather than the LLM failing.
  - **High "Missing from OAEI" count:** Suggests the LLM is discovering valid mappings excluded from the original benchmark.

- **First 3 experiments:**
  1. **Baseline Error Distribution:** Run a standard LLM on the conference track and calculate the percentage of *Align-up* vs. *False* mappings to determine if the model struggles more with hierarchy or relevance.
  2. **Evaluator Consistency Check:** Swap the "LLM-based evaluator" for a different model (or human review) on a sample of "Incorrect" mappings to verify if the sub-classification (Align-up/down) is consistent.
  3. **Fine-tuning Feedback Loop:** Use the "Incorrect" mappings flagged as *False* as negative training examples to see if the "Missing from LLM" rate drops in a subsequent run.

## Open Questions the Paper Calls Out

- **Open Question 1:** What distinct hallucination types emerge in complex matching scenarios (e.g., one-to-many or subsumption) that are not captured by the current one-to-one benchmark?
  - Basis in paper: [explicit] The Limitations section states that sophisticated cases, such as one-to-many and subsumption matching, "may cause additional types of LLM hallucinations."
  - Why unresolved: The OAEI-LLM dataset construction currently constrains all mappings to the simplest case of one-to-one equivalence.
  - What evidence would resolve it: An extended schema defining new error categories specific to complex mappings, supported by benchmark results on non-equivalence tracks.

- **Open Question 2:** To what extent can user interaction mitigate hallucinations compared to static, autonomous matching?
  - Basis in paper: [explicit] The Further Work section proposes OAEI-LLM-I to focus on "understanding how user interaction could be used to mitigate LLM hallucinations."
  - Why unresolved: The current dataset and methodology are designed for static alignment generation and do not model interactive user feedback loops.
  - What evidence would resolve it: Comparative benchmark results showing reduced hallucination rates in the proposed OAEI-LLM-I track versus static tracks.

- **Open Question 3:** Does the "LLM-based evaluator" used for assessment introduce systematic bias in classifying "disputed" mappings?
  - Basis in paper: [inferred] Section 2.1 notes the use of an LLM to judge relevance but admits, "we cannot recommend a particular LLM," while Section 4 suggests assessment "may vary depending on the LLM used."
  - Why unresolved: The classification of "disputed" mappings (where the LLM might be right and the reference wrong) relies on a potentially subjective automated evaluator.
  - What evidence would resolve it: A validation study comparing the LLM evaluator's judgments against human expert consensus for disputed cases.

## Limitations
- The one-to-one mapping constraint excludes valid complex mappings (one-to-many, subsumption) from the benchmark, limiting generalizability.
- The benchmark's validity depends on the accuracy of the OAEI Reference Alignment, which may contain errors or incompleteness.
- The "LLM-based evaluator" for assessment is not specified and may introduce systematic bias in hallucination classification.

## Confidence
- **High Confidence:** The error classification schema (Missing from LLM, Missing from OAEI, Incorrect) is clearly defined and logically consistent with set-difference methodology.
- **Medium Confidence:** The extended EDOAL schema for recording hallucinations is well-specified, but its practical utility depends on the evaluator's reliability.
- **Low Confidence:** The practical impact of the benchmark is uncertain without evidence that the "Incorrect" mapping subtypes (align-up/down) are consistently distinguishable by the LLM evaluator.

## Next Checks
1. **Reference Alignment Validation:** Cross-check a sample of "Disputed" mappings against human expert judgment to estimate the false-positive rate in the current benchmark.
2. **Evaluator Consistency Test:** Run the LLM-based evaluator on the same incorrect mappings using two different LLM models and measure inter-rater agreement.
3. **Coverage Analysis:** Measure the proportion of OAEI reference alignments excluded by the one-to-one constraint and assess whether this represents significant semantic loss.