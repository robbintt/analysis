---
ver: rpa2
title: Augmenting Human-Annotated Training Data with Large Language Model Generation
  and Distillation in Open-Response Assessment
arxiv_id: '2501.09126'
source_url: https://arxiv.org/abs/2501.09126
tags:
- data
- performance
- samples
- synthetic
- responses
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study proposes a hybrid approach to improve text classification
  in open-response assessment by combining human-coded data with synthetic samples
  generated by GPT-4o. The method fine-tunes a BERT classifier using both human-annotated
  and LLM-generated data, then distills this knowledge into a smaller model.
---

# Augmenting Human-Annotated Training Data with Large Language Model Generation and Distillation in Open-Response Assessment

## Quick Facts
- arXiv ID: 2501.09126
- Source URL: https://arxiv.org/abs/2501.09126
- Authors: Conrad Borchers; Danielle R. Thomas; Jionghao Lin; Ralph Abboud; Kenneth R. Koedinger
- Reference count: 8
- Primary result: Synthetic data augmentation improves BERT classifier performance in open-response assessment, with optimal results at 80% synthetic to 20% human-coded data ratio

## Executive Summary
This study proposes a hybrid approach to improve text classification in open-response assessment by combining human-coded data with synthetic samples generated by GPT-4o. The method fine-tunes a BERT classifier using both human-annotated and LLM-generated data, then distills this knowledge into a smaller model. Results show that augmenting training data with synthetic samples improves classifier performance, with optimal results achieved at an 80% synthetic to 20% human-coded data ratio. Lower temperature settings (0.3) produced more stable improvements, while higher settings (0.7+) introduced greater variability in performance. The approach offers a scalable solution that leverages both the accuracy of human coding and the variety of LLM outputs, though careful regularization is needed to manage variation in synthetic data quality.

## Method Summary
The method fine-tunes BERT-base-uncased on a small human-coded training set (51 samples), then continues training with synthetic samples generated by GPT-4o. Synthetic samples are generated using few-shot prompts and added incrementally (in 25-sample increments) up to approximately 250 samples. The approach tests different temperature settings (0.3 to 1.0) to control output diversity and evaluates whether filtering inconsistent samples improves performance. The optimal ratio found was 80% synthetic to 20% human-coded data, with temperature 0.3 producing the most stable improvements.

## Key Results
- Synthetic data augmentation improved classifier performance, with optimal results at an 80% synthetic to 20% human-coded data ratio
- Lower temperature settings (0.3) produced more stable improvements but limited model learning from augmented samples
- Higher temperature settings (0.7 and above) introduced greater variability in performance improvements
- Filtering out inconsistent synthetic samples did not enhance performance and may have removed valuable edge cases

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Synthetic LLM-generated data expands the effective training distribution beyond limited human samples, improving classifier generalization when mixed with human labels.
- **Mechanism**: The LLM leverages its broad pre-training on open-web data to generate response patterns that may not appear in a small human-coded sample but remain task-relevant. This enriches the signal available to the distillation model, allowing it to capture a wider range of valid response characteristics.
- **Core assumption**: Assumption: LLM-generated samples approximate the true distribution of valid responses for the target classification task, and any distribution shift introduced is smaller than the benefit of increased sample variety.
- **Evidence anchors**:
  - [abstract] "augmenting datasets with synthetic samples improves classifier performance, with optimal results achieved at an 80% synthetic to 20% human-coded data ratio"
  - [section 4.1] "the LLM may be able to generate responses that are not found in our sample but are still relevant to the prediction task, hence improving model accuracy and its ability to generalize to new observations"
  - [corpus] Neighbor paper "Condor" similarly addresses knowledge-driven data synthesis for LLM alignment, suggesting this is an active research direction, but direct comparative evidence is limited
- **Break condition**: Performance degrades if synthetic samples systematically diverge from the true target distribution (e.g., LLM generates responses that are grammatically correct but semantically misaligned with human rubric criteria).

### Mechanism 2
- **Claim**: Lower temperature settings (0.3–0.5) produce more stable improvements by constraining output diversity, while higher temperatures enable longer learning curves but introduce greater variance in outcomes.
- **Mechanism**: Temperature controls the concentration of the LLM's sampling distribution. Lower temperatures concentrate probability mass on high-likelihood continuations, producing more uniform samples that the classifier can learn from quickly but may overfit to earlier. Higher temperatures spread probability mass, generating more diverse samples that extend the learning window but risk introducing task-irrelevant variation.
- **Core assumption**: Assumption: There exists a temperature-dependent sweet spot where synthetic diversity improves generalization without introducing harmful noise.
- **Evidence anchors**:
  - [abstract] "Lower temperature settings of 0.3... produced more stable improvements but also limited model learning from augmented samples. In contrast, higher temperature settings (0.7 and above) introduced greater variability"
  - [section 3.2] "lower temperature led to more stable performance improvements, but overfit faster... while higher temperature led to less stable performance improvements, but overfit later"
  - [corpus] Weak corpus evidence; neighbor papers do not systematically address temperature effects in synthetic data generation
- **Break condition**: If the task requires highly uniform outputs (e.g., strict template adherence), even moderate temperatures may introduce harmful variance. Conversely, if the task benefits from edge-case diversity, low temperatures may prematurely cap achievable performance.

### Mechanism 3
- **Claim**: Filtering LLM-generated samples via self-consistency checks does not improve distillation outcomes and may remove valuable edge cases.
- **Mechanism**: Self-consistency filtering removes samples where the LLM's generation and its own subsequent classification disagree. However, these "inconsistent" samples may represent ambiguous or borderline cases that expand the decision boundary. Removing them reduces training diversity and can lower peak performance.
- **Core assumption**: Assumption: Inconsistent samples are not uniformly low-quality; some represent meaningful variation in the response space that aids classifier learning.
- **Evidence anchors**:
  - [abstract] "Filtering out inconsistent synthetic samples did not enhance performance"
  - [section 4.2] "removing inconsistent samples—which might represent rare or unique response types—counterintuitively led to a decrease in performance... these samples may cover a broader spectrum of the response space"
  - [corpus] No direct corpus evidence on this specific mechanism; this appears to be a novel finding in this paper
- **Break condition**: If inconsistent samples are predominantly hallucinations or task-irrelevant content (rather than borderline valid cases), filtering could help. The break point depends on the error profile of the generating LLM.

## Foundational Learning

- **Knowledge Distillation**:
  - **Why needed here**: The method explicitly distills knowledge from GPT-4o (large, expensive) into BERT (smaller, offline, cheaper) by training the student model on LLM-generated samples alongside human data.
  - **Quick check question**: Can you explain why training a smaller model on LLM outputs is called "distillation" rather than just "data augmentation"?

- **Temperature Sampling in LLMs**:
  - **Why needed here**: Temperature is a core hyperparameter in this method, directly controlling the diversity-versus-consistency tradeoff in synthetic data generation.
  - **Quick check question**: What happens to output diversity as temperature approaches 0 versus as it approaches 1?

- **Overfitting Detection via Early Stopping**:
  - **Why needed here**: The method uses validation-set monitoring with patience=2 epochs to detect when the model has exhausted the useful signal in training data (human or synthetic).
  - **Quick check question**: If validation AUC plateaus then declines as synthetic samples are added, what does this indicate about the synthetic data quality or quantity?

## Architecture Onboarding

- **Component map**: Human-coded data (51 samples) -> GPT-4o few-shot generation -> Synthetic samples (variable temperature) -> BERT-base-uncased fine-tuning (ADAM optimizer, lr=2e-5) -> Validation AUC monitoring -> Knowledge distillation

- **Critical path**:
  1. Establish baseline on human-only data with early stopping
  2. Generate synthetic samples at chosen temperature (start with 0.3–0.5)
  3. Continue training with synthetic augmentation in 25-sample increments
  4. Monitor validation AUC with bootstrapped confidence intervals
  5. Stop when performance plateaus or degrades (watch for overfitting signal)

- **Design tradeoffs**:
  - **Low vs. high temperature**: Stability (0.3) vs. potential for higher peak performance (0.5–0.7)
  - **Filtering vs. no filtering**: Cleaner data vs. loss of edge-case diversity
  - **Human-to-synthetic ratio**: Paper suggests ~80% synthetic works best, but this may not generalize to all tasks
  - **Model choice**: BERT is efficient but may limit performance ceiling; larger distillation targets could improve results

- **Failure signatures**:
  - Validation AUC fails to improve beyond human-only baseline → synthetic samples may be low-quality or misaligned with task
  - High variance across temperature runs → suggests LLM generation is unstable for this prompt/task combination
  - Early plateau with synthetic data → may indicate overfitting to uniform samples (try higher temperature)

- **First 3 experiments**:
  1. **Replicate the ratio experiment**: Train on human data (51 samples), then add synthetic samples in 25-sample increments at temperature 0.3 until reaching ~80% synthetic ratio. Plot validation AUC with confidence intervals.
  2. **Temperature sweep**: Repeat experiment 1 at temperatures 0.3, 0.5, 0.7, 1.0. Compare stability (CI width) and peak performance. Identify which temperature yields the best AUC for your task.
  3. **Filtering ablation**: For your best temperature, generate 1,000 samples, filter out inconsistent ones via self-scoring, and compare training curves with vs. without filtering. Verify whether filtering helps or hurts.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the hybrid data augmentation approach maintain its performance benefits when applied to open-source LLMs or alternative classifier architectures?
- Basis in paper: [explicit] The authors list as a limitation that they "used only one generative model (GPT-4o) and distillation architecture (BERT)" and explicitly call for replication with other models.
- Why unresolved: The study is currently restricted to a single proprietary model and a specific transformer architecture, limiting the generalizability of the findings to the broader AI ecosystem.
- What evidence would resolve it: Comparative experiments replicating the 80/20 augmentation ratio using open-source LLMs (e.g., Llama) or simpler classifiers (e.g., logistic regression).

### Open Question 2
- Question: Can techniques like sentence embeddings and cosine similarity effectively regularize the variation in synthetic data to optimize classifier performance?
- Basis in paper: [explicit] The conclusion states the need to "explore methods to control similarity among synthetic samples fully" and proposes using "sentence embeddings and cosine similarity measures to balance variety and relevance."
- Why unresolved: The study found that high variety risks introducing irrelevant information (noise) while low variety leads to early overfitting, but it did not test automated methods to balance this trade-off.
- What evidence would resolve it: Ablation studies showing that filtering or weighting synthetic samples based on embedding similarity scores results in higher peak AUC or stability compared to random sampling.

### Open Question 3
- Question: Do advanced prompting strategies elicit more representative synthetic samples than the static few-shot prompting used in the current study?
- Basis in paper: [explicit] The authors note their prompts "may have yet to fully optimize synthetic data's variety" and suggest "future research may explore advanced prompt techniques to elicit more varied and representative samples."
- Why unresolved: The current method relied on a static set of few-shot examples for all generation, which may have constrained the diversity of the synthetic responses.
- What evidence would resolve it: Experiments comparing the diversity of generated datasets and downstream classifier performance between static few-shot prompts and dynamic or chain-of-thought prompting strategies.

## Limitations
- The human training set is small (51 samples), raising questions about whether observed improvements would scale to larger human-coded datasets
- The optimal 80% synthetic ratio may be specific to this task and dataset size, and could vary significantly for other classification problems
- The paper does not systematically explore whether different filtering strategies beyond self-consistency might be beneficial
- The temperature effects, while documented, lack theoretical grounding for why specific ranges work better than others

## Confidence
- **High confidence**: The mechanism that synthetic data augmentation improves classifier performance is well-supported by the empirical results, particularly the clear performance gains at the 80% synthetic ratio
- **Medium confidence**: The temperature-dependent learning dynamics are observed but not fully explained mechanistically
- **Medium confidence**: The finding that self-consistency filtering does not improve performance is supported, but the paper does not explore alternative filtering approaches

## Next Checks
1. **Scale-up validation**: Replicate the experiment with a larger human-coded training set (e.g., 200+ samples) to determine whether the 80% synthetic ratio remains optimal or whether the benefit of augmentation diminishes with more human data
2. **Alternative filtering comparison**: Test whether other filtering strategies (e.g., filtering based on semantic similarity to human responses, or filtering based on specific quality metrics) can improve upon the unfiltered approach while preserving beneficial edge cases
3. **Cross-task generalizability**: Apply the same methodology to a different text classification task (e.g., sentiment analysis, topic classification) to assess whether the optimal temperature ranges and synthetic-to-human ratios generalize across domains