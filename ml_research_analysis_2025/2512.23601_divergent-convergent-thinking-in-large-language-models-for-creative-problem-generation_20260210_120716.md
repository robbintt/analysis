---
ver: rpa2
title: Divergent-Convergent Thinking in Large Language Models for Creative Problem
  Generation
arxiv_id: '2512.23601'
source_url: https://arxiv.org/abs/2512.23601
tags:
- problem
- diversity
- problems
- creativedc
- creative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a two-phase prompting method, CreativeDC, to
  improve creative problem generation by LLMs. It explicitly separates divergent thinking
  (exploring novel, unconstrained ideas) from convergent thinking (refining selected
  ideas into valid problems).
---

# Divergent-Convergent Thinking in Large Language Models for Creative Problem Generation

## Quick Facts
- **arXiv ID:** 2512.23601
- **Source URL:** https://arxiv.org/abs/2512.23601
- **Reference count:** 39
- **Primary result:** CreativeDC achieves 51.5% higher semantic novelty and 24-72% higher Vendi scores compared to baselines while maintaining high utility in programming problem generation.

## Executive Summary
This paper addresses the "Artificial Hivemind" effect in LLMs, where models produce repetitive outputs despite high temperature settings. The authors propose CreativeDC, a two-phase prompting method that explicitly separates divergent thinking (exploring unconstrained ideas) from convergent thinking (refining ideas into valid problems). By decoupling constraint satisfaction from idea generation, the method enables LLMs to explore a broader semantic space and generate more diverse, novel programming problems. The approach significantly outperforms baseline methods on diversity and novelty metrics while maintaining practical utility.

## Method Summary
CreativeDC uses a two-phase prompting framework for generating creative programming problems. In the Divergent phase, the model explores theme-related ideas while explicitly ignoring programming constraints. In the Convergent phase, the model selects from these ideas and maps them to specific programming concepts to create valid problems. The method optionally incorporates persona simulation to bias the divergent exploration. Problems are output in JSON format with description, test suite, and solution. The approach is evaluated across 20 contexts (4 themes Ã— 5 concepts) with 100 problems per context, validated through execution testing.

## Key Results
- CreativeDC achieves 51.5% higher semantic novelty compared to baseline methods
- Vendi scores show 24-72% higher effective distinct problems
- Utility scores remain consistently high (0.7-0.9 range) across different themes and concepts
- Performance improvements are consistent across all four themes and five programming concepts tested

## Why This Works (Mechanism)

### Mechanism 1: Constraint Decoupling
Explicitly separating constraint satisfaction from idea generation allows LLMs to traverse a broader semantic space than when attempting both simultaneously. The prompt instructs the model to ignore technical constraints during the "Divergent" phase, preventing premature probability distribution collapse onto high-likelihood "safe" responses.

### Mechanism 2: Scaffolded Reasoning Traces
By requiring the model to output a list of ideas before the final JSON object, the model commits to a specific creative direction in the context window. This acts as a "scratchpad," anchoring the final generation on a specific semantic path rather than regenerating the most probable problem description from scratch.

### Mechanism 3: Persona Simulation
Injecting personas (e.g., "aging researcher") biases the attention mechanism toward domain-specific vocabulary during the divergent phase. This ensures different generation samples start from semantically distinct regions of the latent space, increasing inter-sample variance.

## Foundational Learning

- **Artificial Hivemind Effect:** The core failure mode where LLMs naturally converge on statistical averages (homogeneity) despite high temperature settings. Without understanding this, the necessity of the two-phase intervention is unclear. *Quick check:* If you sample 100 problems from a standard LLM with high temperature, do you get 100 distinct semantic ideas or 100 variations of the same syntactic structure?

- **Guilford's Divergent-Convergent Framework:** The theoretical foundation where "Divergent" = breadth/quantity and "Convergent" = depth/quality. Understanding this framework is essential to grasping why the prompt is split into two explicit sections. *Quick check:* In the CreativeDC prompt, which section explicitly bans the mention of "programming concepts"?

- **Vendi Score:** The primary metric used to prove scaling superiority, measuring effective number of distinct items. Traditional metrics (BLEU/ROUGE) fail to capture diversity. *Quick check:* If a set of 100 problems has a Vendi Score of 10, what does that imply about the distinctness of the problems?

## Architecture Onboarding

- **Component map:** Context Input (Theme + Concept) -> Persona Sampler (Optional) -> Prompt Constructor (Two-Phase Template) -> Generator (LLM) -> Validator (Solution vs Test Suite)
- **Critical path:** The Divergent Thinking Prompt. If the instruction "Ignore the required programming concepts" is removed or weakened, the model hallucinates constraints too early, and novelty scores collapse.
- **Design tradeoffs:** Novelty vs. Utility (aggressive divergent instructions may increase novelty but cause utility drops); Speed vs. Diversity (generating reasoning traces increases latency and token costs).
- **Failure signatures:** The "Lazy Converger" (ignores divergent output in phase 2); The "Theme Drifter" (forgets required Theme in phase 2); The "Syntax Error" (complex descriptions lead to invalid test_suite generation).
- **First 3 experiments:**
  1. Ablation Study: Remove "ignore constraints" instruction from divergent phase and compare Vendi Score and Semantic Novelty against full CreativeDC.
  2. Persona Diversity Scaling: Run CreativeDC with vs. without Persona Simulation on a "boring" theme (e.g., "Laundry").
  3. Utility Stress Test: Force Convergent phase to use complex concepts (e.g., "Recursion") with highly abstract themes (e.g., "Philosophy") and measure utility score drops.

## Open Questions the Paper Calls Out

- **Generalization across architectures:** Does CreativeDC work across different LLM architectures, sizes, and families beyond Qwen3-235B? The paper evaluated only one model, and effects may vary with model scale, training data, or architecture.

- **Human perception of creativity:** Do humans perceive CreativeDC-generated problems as more creative and practically useful compared to baselines? Automated metrics may not fully capture human judgments of creativity, pedagogical value, or engagement.

- **Transfer to other domains:** Does the divergent-convergent scaffolding transfer effectively to other creative domains beyond programming problems? Programming problems have specific structural constraints that other domains may not share.

- **Divergent phase optimization:** How does the number of ideas generated in the divergent phase affect the diversity-quality trade-off? The optimal quantity and selection strategy remain unexplored.

## Limitations

- **Evaluation methodology uncertainty:** Relies on LLM-as-a-judge and semantic similarity embeddings, introducing potential subjectivity and model-specific biases. Exact evaluation prompts are not provided.
- **Scalability constraints:** Method increases latency and token costs due to reasoning trace generation. Paper doesn't provide runtime or cost analysis, and performance degrades with generic personas or semantically distant theme-concept pairs.
- **Limited generalization:** Evaluation is restricted to programming problem generation. The method's effectiveness across other creative domains or problem types is unproven.

## Confidence

- **High confidence:** The core mechanism of improved diversity and novelty through explicit decoupling of divergent and convergent phases is well-supported. Vendi Score results (24-72% improvement) and semantic novelty gains (51.5%) are robust across multiple themes and concepts.

- **Medium confidence:** Claims of consistent outperformance are supported but absolute performance depends on evaluation methodology. Utility scores are good but not compared against human-generated problems as upper bound.

- **Low confidence:** Assertions that the approach fundamentally changes LLM reasoning processes are speculative. Theoretical discussion of addressing "functional fixedness" is not empirically proven.

## Next Checks

- **Cross-domain generalization test:** Apply CreativeDC to completely different creative domains (story generation, product design, scientific hypothesis generation) to validate domain-agnostic applicability.

- **Human evaluation benchmark:** Have human experts rate creativity and utility of CreativeDC outputs versus baselines and human-generated problems to validate LLM-as-judge correlations and establish human-level performance.

- **Scaling and cost analysis:** Measure latency, token usage, and computational cost of CreativeDC versus direct generation baselines across different model sizes to determine practical deployment constraints.