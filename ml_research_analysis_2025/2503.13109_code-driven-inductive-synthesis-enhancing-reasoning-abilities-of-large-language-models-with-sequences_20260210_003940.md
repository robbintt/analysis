---
ver: rpa2
title: 'Code-Driven Inductive Synthesis: Enhancing Reasoning Abilities of Large Language
  Models with Sequences'
arxiv_id: '2503.13109'
source_url: https://arxiv.org/abs/2503.13109
tags:
- code
- slot
- reasoning
- data
- sequence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a novel method to enhance inductive reasoning
  in large language models (LLMs) by using number sequences as training data. It constructs
  a synthetic dataset, CodeSeq, by converting sequences into algorithmic problems
  with code solutions and unit tests to inject case-based supervision signals.
---

# Code-Driven Inductive Synthesis: Enhancing Reasoning Abilities of Large Language Models with Sequences

## Quick Facts
- arXiv ID: 2503.13109
- Source URL: https://arxiv.org/abs/2503.13109
- Reference count: 26
- Key outcome: CodeSeq dataset fine-tuning improves inductive reasoning and general reasoning benchmarks

## Executive Summary
This paper introduces Code-Driven Inductive Synthesis, a method to enhance LLMs' inductive reasoning by training on number sequences converted into algorithmic problems with code solutions and unit tests. The approach uses code execution as deterministic supervision, enabling case-based feedback for open-ended inductive steps that traditional NLP supervision struggles with. The resulting CodeSeq dataset, when used to fine-tune models like LLaMA3-8B and Qwen2.5-7B, improves performance on both code benchmarks (Humaneval, MBPP) and comprehensive reasoning benchmarks (MMLU, BBH, GaoKaoBench).

## Method Summary
The method constructs a synthetic dataset called CodeSeq by converting number sequences from OEIS into algorithmic problems with code solutions and unit tests. A three-stage pipeline filters sequences, generates problems using DeepSeek-V3 as a working agent, and injects unit tests for verification. An iterative correction loop using o1-preview as a guiding agent refines failed solutions until they pass all test cases. The final dataset is mixed with Tulu3 at a 5:1 ratio and used for SFT with InternTrainer, fine-tuning models for one epoch with specific hyperparameters.

## Key Results
- Qwen2.5-7B achieves near Claude-3.5-Sonnet-level accuracy on next number prediction after CodeSeq training
- Models trained with CodeSeq show significant improvements on Humaneval (+7.89 pass@1) and MBPP (+4.96 pass@1)
- Substantial gains on comprehensive reasoning benchmarks: MMLU (+8.82), BBH (+5.15), and GaoKaoBench (+0.48)

## Why This Works (Mechanism)

### Mechanism 1: Sandboxed Execution as Verifiable Supervision
The system generates code to represent the general term of a sequence and executes it against unit tests in a sandbox. Code execution yields deterministic binary signals (pass/fail), allowing the model to verify if an induced rule holds for specific cases, unlike natural language reasoning steps.

### Mechanism 2: Transfer from Inductive to General Reasoning
Training on "finding general terms" (induction) strengthens the model's ability to derive general principles from specific observations. This enhances performance on deductive tasks (like code generation) and comprehensive reasoning by improving generalization capabilities.

### Mechanism 3: Iterative Correction Loops
A "Guiding Agent" analyzes failed test cases and provides natural language reasons for failure, while a "Working Agent" uses this feedback to regenerate code. This loop filters out low-quality generations, ensuring the final dataset contains only verified, correct code solutions.

## Foundational Learning

**Concept: Inductive vs. Deductive Reasoning**
- Why needed here: The paper contrasts its method (Inductive: specific → general) against standard code/math tasks (Deductive: general → specific)
- Quick check question: Can you distinguish between "Proving a theorem" (deductive) and "Guessing the next number in a sequence" (inductive)?

**Concept: Unit Testing / Sandbox Execution**
- Why needed here: This is the core verification engine that provides deterministic supervision signals
- Quick check question: How does a unit test provide a "supervision signal" different from a human label?

**Concept: Supervised Fine-Tuning (SFT)**
- Why needed here: The pipeline output is an SFT dataset (CodeSeq), and the mechanism of improvement is fine-tuning the base LLM on this synthetic data
- Quick check question: What is the risk of fine-tuning solely on synthetic data generated by the model itself?

## Architecture Onboarding

**Component map:** OEIS -> Working Agent (DeepSeek-v3) -> Guiding Agent (o1-preview) -> Isolated Python Sandbox -> CodeSeq dataset

**Critical path:** The Case-based Supervision Injection (Stage 3) is the bottleneck. Generating code is cheap; running the correction loop until all 5-7 test cases pass is the time/resource constraint.

**Design tradeoffs:**
- Cost vs. Quality: Using o1-preview as the Guiding Agent improves correction quality but increases data generation costs significantly
- Diversity vs. Accuracy: Strict filtering ensures solvable problems but limits dataset size to approximately 9k samples

**Failure signatures:**
- Low First Hit Rate: 0.52 First Hit Rate means nearly half initial generations require the expensive correction loop
- Instruction Catastrophe: Removing general instruction data (Tulu3) causes the model to "break down in terms of instruction-following ability"

**First 3 experiments:**
1. Scrape 10 sequences and manually verify the CodeSeq pipeline produces passing code
2. Train a model using only "First Hit" code vs. Corrected code to measure supervision loop impact
3. Fine-tune a base model on CodeSeq and evaluate only on MMLU/BBH to confirm reasoning transfer without code benchmark leakage

## Open Questions the Paper Calls Out

Can the code-driven synthesis pipeline be successfully adapted for non-numerical inductive reasoning data sources? The authors note that "using only sequences as the data source for inductive reasoning is relatively limited," but it's unclear if the method applies to text-based or visual inductive patterns.

How can inductive reasoning tasks and evaluation metrics be systematically organized to standardize progress in this field? The paper notes that for inductive reasoning, "the significance of this type of task, the datasets, and the evaluation methods, etc., have not been systematically organized."

Does training on numerical sequence induction yield strong transfer performance on cross-lingual reasoning tasks? While the paper reports large gains on English benchmarks, the improvement on Chinese GaoKaoBench is marginal (Δ +0.07 to +0.48), suggesting limited cross-lingual transfer.

## Limitations

The synthetic data generation process is resource-intensive, requiring multiple LLM calls and an average of 2.93 correction rounds per sample. The dataset size (9,242 samples) is relatively small compared to typical pre-training corpora, potentially limiting the breadth of reasoning patterns captured. The method's reliance on o1-preview for guidance may not generalize to less capable models, and the specific contribution of inductive reasoning versus improved coding ability remains unclear.

## Confidence

**High confidence:** Using code execution as deterministic supervision for inductive steps is well-established in the literature and directly implemented.

**Medium confidence:** The transfer from sequence induction to general reasoning benchmarks is demonstrated but could be confounded by improved coding abilities rather than pure reasoning enhancement.

**Low confidence:** The assumption that o1-preview can reliably diagnose code failures and guide corrections without introducing hallucinations or circular reasoning.

## Next Checks

1. **Ablation on correction quality:** Train two models - one using only first-attempt code and one using corrected code - to quantify the impact of the iterative supervision loop on final performance.

2. **Transfer specificity test:** Evaluate models on MMLU without any code-related benchmarks to isolate pure reasoning transfer from coding improvements.

3. **Sandboxing security audit:** Implement and test the code execution sandbox with edge cases (infinite loops, memory exhaustion, system calls) to verify isolation properties.