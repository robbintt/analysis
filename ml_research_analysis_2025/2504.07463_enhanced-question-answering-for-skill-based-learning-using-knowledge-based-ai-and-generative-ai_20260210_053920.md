---
ver: rpa2
title: Enhanced Question-Answering for Skill-based learning using Knowledge-based
  AI and Generative AI
arxiv_id: '2504.07463'
source_url: https://arxiv.org/abs/2504.07463
tags:
- responses
- response
- question
- learning
- skill
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses the challenge of providing meaningful skill-based
  explanations in online education, where traditional tools often fail to address
  learners' deeper questions about procedural knowledge and reasoning. It introduces
  Ivy, an intelligent agent that integrates Knowledge-based AI (KBAI) with Generative
  AI to generate skill-focused explanations using the Task-Method-Knowledge (TMK)
  model.
---

# Enhanced Question-Answering for Skill-based learning using Knowledge-based AI and Generative AI

## Quick Facts
- **arXiv ID:** 2504.07463
- **Source URL:** https://arxiv.org/abs/2504.07463
- **Reference count:** 17
- **Primary result:** Ivy outperformed a RAG-based baseline with 115 votes vs 75, achieving an average SBERT score of 0.82 for semantic similarity.

## Executive Summary
This paper introduces Ivy, an intelligent agent that integrates Knowledge-based AI (KBAI) with Generative AI to provide skill-focused explanations in online education. Ivy leverages the Task-Method-Knowledge (TMK) model to generate teleological, causal, and compositional reasoning for skill-based questions. The system addresses the limitation of traditional tools that struggle to explain procedural knowledge and reasoning. Evaluation using 30 verification questions across six skills demonstrated Ivy's superior performance over a RAG baseline, with significant improvements in semantic similarity and relevance of responses.

## Method Summary
Ivy combines a structured TMK knowledge model with iterative refinement techniques and a relevance filter to generate skill-focused explanations. The system first assesses question complexity via a k-score (1-4), retrieves corresponding TMK documents using FAISS semantic search, then iteratively refines the answer through sequential document integration. A response optimizer cleans up the final output. The TMK framework decomposes skills into hierarchical goals (Tasks) and procedural steps (Methods), enabling multi-modal reasoning that traditional text retrieval cannot provide.

## Key Results
- Ivy received 115 votes compared to 75 for the RAG baseline in developer perception evaluation
- Ivy achieved an average SBERT score of 0.82 for semantic similarity
- Automated LLM judges confirmed Ivy accessed relevant TMK content in 90% of cases and maintained 83% accuracy in intermediate responses

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Structured knowledge representation (TMK) enables multi-modal reasoning (teleological and causal) that standard text retrieval cannot.
- **Mechanism:** The Task-Method-Knowledge (TMK) model explicitly decomposes skills into hierarchical goals (Tasks) and procedural steps (Methods). By retrieving from this structured graph rather than unstructured text, the system can map a learner's "how" question to specific state transitions (causality) or a "why" question to a goal state (teleology).
- **Core assumption:** Learners' deep questions regarding procedural skills rely on understanding relationships between components (goals vs. steps) that are often implicit in narrative text.
- **Evidence anchors:**
  - [abstract] Mentions the TMK framework organizes skills to enable "teleological, causal, and compositional reasoning."
  - [Section 3.1] States that linkages between Tasks and Methods showcase teleological principles, while state transitions capture causality.
  - [corpus] "Video-Skill-CoT" supports the general value of skill-based decomposition for domain-adaptive reasoning, though it uses CoT rather than TMK.
- **Break condition:** Fails if the TMK model is incomplete or incorrectly hierarchically structured, breaking the causal chain required for explanation.

### Mechanism 2
- **Claim:** Dynamic retrieval depth based on query complexity improves the relevance of generated answers.
- **Mechanism:** The architecture assigns a "k-score" (1-4) to the user's question via an LLM prompt. This score determines the number of document chunks retrieved from the TMK model (e.g., 1 chunk for a definition vs. 4 for a detailed procedural explanation), ensuring the context window matches the required cognitive load.
- **Core assumption:** An LLM can accurately self-assess the complexity of a user's question via zero-shot classification before retrieving context.
- **Evidence anchors:**
  - [abstract] Highlights "iterative refinement techniques" and contextually relevant explanations.
  - [Section 3.3] Describes the "Question Complexity Assessment" where k-score dictates the top-k documents retrieved via FAISS.
  - [corpus] Limited direct corpus support for this specific scoring mechanism in neighboring papers.
- **Break condition:** Fails if the complexity classifier misinterprets the query (e.g., assigning a low score to a nuanced question), resulting in insufficient context for the generator.

### Mechanism 3
- **Claim:** Iterative refinement reduces hallucination and increases factual alignment with the knowledge base.
- **Mechanism:** Instead of a single generation pass, the system generates an initial response from the top document and sequentially refines it using the remaining k-1 documents. A "Response Optimizer" then strips out meta-talk and repetitive artifacts introduced by the iteration process.
- **Core assumption:** Incrementally incorporating specific TMK details yields a more accurate "knowledge trace" than generating from a single concatenated context.
- **Evidence anchors:**
  - [abstract] Notes the use of "iterative refinement techniques" to generate explanations.
  - [Section 3.3] Details the "Response Generation Module" which refines the initial response with subsequent documents.
  - [corpus] "Counterfactual Simulatability" generally relates to explanation fidelity, but does not validate this specific iterative module.
- **Break condition:** Degrades if the iterative loop introduces contradiction or if the optimizer strips out critical nuance in an attempt to enforce compactness.

## Foundational Learning

- **Concept:** **Task-Method-Knowledge (TMK) Modeling**
  - **Why needed here:** This is the core representation language of the system. Without understanding how "Tasks" (goals) differ from "Methods" (state transitions), one cannot build or maintain the structured backend.
  - **Quick check question:** Can you distinguish between the *goal* of a sorting algorithm (Task) and the *steps* of a bubble sort (Method)?

- **Concept:** **Vector Similarity Search (FAISS/SBERT)**
  - **Why needed here:** The system relies on semantic search to map natural language questions to specific TMK components.
  - **Quick check question:** Why might pure keyword search fail to connect the query "Why did the robot fail?" with a TMK document titled "State Precondition Violations"?

- **Concept:** **Retrieval-Augmented Generation (RAG)**
  - **Why needed here:** Ivy is essentially a specialized RAG architecture. Understanding the baseline limitations of standard RAG (retrieving unstructured chunks) is necessary to appreciate why the TMK structure adds value.
  - **Quick check question:** What are the failure modes of a standard RAG system when asked to explain a complex, multi-step procedure?

## Architecture Onboarding

- **Component map:**
  Frontend/Interface -> Relevance Filter -> Complexity Assessor (LLM) -> Retriever (FAISS) -> Refinement Loop (LLM) -> Optimizer (LLM)

- **Critical path:**
  The mapping of the User Question → K-Score → FAISS Retrieval. If the retrieval fetches the wrong TMK node (e.g., fetches "Knowledge: Definitions" instead of "Method: Procedure"), the iterative refinement will propagate an irrelevant answer.

- **Design tradeoffs:**
  - **Manual vs. Automated Modeling:** The paper notes manual TMK modeling takes ~7 hours per skill. This ensures high precision but limits scalability compared to automated RAG indexing.
  - **K-Score Granularity:** Using 4 discrete levels of complexity simplifies the logic but may lack the nuance for "medium-complexity" queries.

- **Failure signatures:**
  - **"Shallow" Responses:** Suggests the K-score was too low or the Retriever failed to fetch the correct Method documents.
  - **Verbose/Repetitive Output:** Indicates the Response Optimizer module failed to strip the artifacts of the iterative refinement process.
  - **Misclassification:** If a student asks a specific "episodic" question (e.g., "Why did *I* get this wrong?"), the system may fail as TMK handles general procedural logic, not specific user history.

- **First 3 experiments:**
  1.  **Ablation on Structure:** Run the same questions using a "flattened" TMK (removing the hierarchy) vs. the full hierarchical model to measure the impact of structure on "Method" questions.
  2.  **K-Score Calibration:** Compare human ratings of "ideal depth" against the LLM's assigned k-score to validate the Complexity Assessor.
  3.  **Baseline Comparison:** Pit Ivy against the RAG Benchmark specifically on "Cannot Answer" questions to verify if the relevance filter improves precision/recall boundaries.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the creation of TMK models be automated to reduce development time while maintaining skill representation accuracy?
- Basis in paper: [explicit] The Limitations section states, "We aim to automate TMK model creation and establish metrics to improve skill representation accuracy and reduce development time," noting that current manual creation takes approximately seven hours per model.
- Why unresolved: The current methodology relies entirely on manual modeling by graduate research assistants, which is resource-intensive and limits scalability.
- What evidence would resolve it: An automated system that generates TMK models from course materials that pass the same verification metrics (e.g., SBERT scores) as human-generated models.

### Open Question 2
- Question: How can TMK models be adapted to answer episodic queries about specific problem instances?
- Basis in paper: [explicit] The Conclusion states that "currently, Ivy addresses general procedural questions but struggles with episodic queries about specific problem instances. Adapting TMK models to handle these queries is a focus for future research."
- Why unresolved: The existing TMK structure supports general procedural knowledge (how/why) but lacks the specific state-trace mechanisms required to reason about particular variables or steps in a solved problem.
- What evidence would resolve it: A modified Ivy architecture that successfully retrieves correct answers for a test set of episodic questions (e.g., "Why did the robot paint the ceiling first in this specific scenario?").

### Open Question 3
- Question: Does Ivy improve learning outcomes and utility for actual students in live online courses?
- Basis in paper: [explicit] The Limitations section notes that "evaluations were conducted internally" and suggests that "future focus group studies with actual learners will validate Ivy’s utility and identify areas for improvement."
- Why unresolved: The study relied on "Developer Perception Evaluation" (internal team voting) rather than feedback from the target demographic (enrolled learners).
- What evidence would resolve it: Results from a randomized controlled trial (RCT) or A/B test involving enrolled students comparing Ivy against the RAG baseline on comprehension quizzes and subjective satisfaction.

## Limitations
- The system's effectiveness is heavily dependent on the quality and completeness of manually crafted TMK models, which require significant expert effort (~7 hours per skill).
- Evaluation focuses on general procedural questions rather than personalized learning scenarios, and the 30-question dataset may not capture edge cases.
- The iterative refinement mechanism could introduce contradictions if the k-score is misclassified or if TMK documents contain conflicting information.

## Confidence
- **High Confidence:** The core architectural components (TMK modeling, FAISS retrieval, iterative refinement) are well-defined and the quantitative results (115 vs 75 votes, 0.82 SBERT score) demonstrate measurable improvement over baseline.
- **Medium Confidence:** The qualitative benefits of teleological and causal reasoning through TMK structure are supported by the framework description but lack direct empirical validation comparing structured vs. unstructured knowledge representation.
- **Medium Confidence:** The k-score complexity classifier's accuracy is assumed but not independently validated against human raters in the evaluation.

## Next Checks
1. Conduct a controlled experiment comparing Ivy's performance on the same questions using a flattened (non-hierarchical) TMK structure to quantify the value added by the Task-Method hierarchy.
2. Implement a calibration study where human experts rate the ideal depth of responses for a sample of questions, then compare these ratings against the LLM-assigned k-scores to validate the complexity assessment module.
3. Test the system's robustness by introducing incomplete or contradictory TMK documents to measure how the iterative refinement handles inconsistent knowledge bases.