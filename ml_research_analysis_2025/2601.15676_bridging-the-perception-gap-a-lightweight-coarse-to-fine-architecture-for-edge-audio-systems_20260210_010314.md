---
ver: rpa2
title: 'Bridging the Perception Gap: A Lightweight Coarse-to-Fine Architecture for
  Edge Audio Systems'
arxiv_id: '2601.15676'
source_url: https://arxiv.org/abs/2601.15676
tags:
- edge
- perception
- audio
- cloud
- cofi-agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes CoFi-Agent, a hybrid edge-cloud architecture
  designed to address the tension between perception depth and computational efficiency
  in edge audio systems. The core idea is a coarse-to-fine workflow: a lightweight
  local Audio-LLM performs initial inference, and a cloud controller conditionally
  triggers targeted refinement via on-device tools like temporal re-listening and
  local ASR when uncertainty is detected.'
---

# Bridging the Perception Gap: A Lightweight Coarse-to-Fine Architecture for Edge Audio Systems

## Quick Facts
- arXiv ID: 2601.15676
- Source URL: https://arxiv.org/abs/2601.15676
- Reference count: 26
- Primary result: CoFi-Agent achieves 53.60% accuracy on MMAR benchmark, a 26.40% absolute improvement over single-pass inference

## Executive Summary
This paper addresses the tension between deep audio perception and computational efficiency on edge devices by proposing CoFi-Agent, a hybrid edge-cloud architecture. The system employs a coarse-to-fine workflow: a lightweight local Audio-LLM performs initial inference, and a cloud controller conditionally triggers targeted refinement via on-device tools like temporal re-listening and local ASR when uncertainty is detected. This approach avoids always-on tool usage, keeping raw audio on-device and transmitting only compact evidence. On the MMAR benchmark, CoFi-Agent improves accuracy from 27.20% to 53.60% and achieves a better accuracy-latency trade-off than an always-on pipeline, demonstrating that adaptive, conditional edge-cloud collaboration can significantly enhance edge audio reasoning under practical constraints.

## Method Summary
CoFi-Agent implements a coarse-to-fine edge-cloud collaboration for audio reasoning. Stage 0 performs single-pass inference with Qwen2-Audio-7B-Instruct on edge GPU. An adaptive confidence gate (GPT-4o-based classifier) detects uncertainty cues and decides whether to escalate. Stage 1 involves a segment proposer that uses energy-based segmentation plus uniform sliding windows (K=4) to identify regions of interest. Stage 2 executes on-device tools: temporal re-listening (same Audio-LLM on ROI) and Whisper-small for local ASR. Evidence Integration uses GPT-4o to combine (s₀, e_audio, T_text, Q) for final verdict. The system runs on NVIDIA Quadro RTX 6000 with ~15ms p50 network RTT.

## Key Results
- Accuracy improves from 27.20% to 53.60% on MMAR benchmark
- Better accuracy-latency trade-off compared to always-on pipeline
- Raw audio remains on-device; only compact evidence transmitted to cloud
- Adaptive escalation reduces unnecessary tool usage (escalation rate ~62%)

## Why This Works (Mechanism)
The coarse-to-fine workflow addresses the perception gap by first attempting reasoning with minimal resources, then selectively applying expensive tools only when uncertainty is detected. This conditional approach maintains privacy (audio stays local) while improving accuracy through targeted refinement. The adaptive confidence gate prevents wasted computation on samples where the initial inference is likely correct, while still providing recourse for difficult cases. By transmitting only text evidence rather than raw audio, the system reduces bandwidth requirements and latency.

## Foundational Learning
- **Edge-Cloud Audio Reasoning**: Combining on-device inference with cloud-based refinement to balance accuracy and efficiency; needed to understand the problem domain and constraints.
- **Coarse-to-Fine Architecture**: Hierarchical processing where initial simple processing triggers more complex analysis only when needed; fundamental to the proposed solution.
- **Uncertainty Detection**: Mechanisms for identifying when initial inference may be unreliable; critical for the adaptive confidence gate.
- **Region of Interest (ROI) Identification**: Techniques for selecting relevant audio segments for re-analysis; essential for targeted refinement.
- **On-Device Tool Integration**: Incorporating local ASR and re-listening capabilities; enables privacy-preserving refinement.
- **Evidence Integration**: Methods for combining multiple inference sources into final decision; key to the refinement pipeline.

## Architecture Onboarding

**Component Map**: Audio + Question → Qwen2-Audio → Confidence Gate → (Yes → Segment Proposer → On-Device Tools → GPT-4o) → GPT-4o → Final Answer

**Critical Path**: Audio input → Initial inference (Stage 0) → Confidence gate evaluation → Conditional escalation → ROI segmentation → Tool execution → Evidence integration → Final answer

**Design Tradeoffs**: Privacy vs. accuracy (keep audio local but may miss some cloud-based insights), computational efficiency vs. thoroughness (avoid always-on tools but may miss some escalations), bandwidth vs. information completeness (transmit only text evidence but may lose some audio context)

**Failure Signatures**: False escalations on low-SNR non-speech clips where baseline is correct but hedges language, missed escalations when initial inference confidently hallucinates, segment proposer skipping brief but decisive events in long recordings, ASR hallucinations in silence injecting noise

**First Experiments**: 1) Reconstruct and validate the adaptive confidence gate prompt and escalation logic on a held-out subset of MMAR, 2) Implement the energy-based segmentation and sliding window proposal pipeline; verify segment quality against ground-truth ROI boundaries, 3) Benchmark accuracy and latency under varying network RTTs and hardware constraints

## Open Questions the Paper Calls Out
- **Learned Confidence Gate**: Can a learned confidence gate effectively minimize decision overhead compared to the current prompt-based classifier? The current implementation causes false and missed escalations, and comparative studies measuring F1-score of escalation decisions would resolve this.
- **Joint Training**: Does jointly training the segment proposer with the reasoning module improve retrieval of brief but decisive events? The current heuristic approach often skips brief events in long recordings, and experiments on long-form audio benchmarks would provide evidence.
- **Edge Video Extension**: How effectively does the conditional edge-cloud collaboration paradigm transfer to bandwidth-constrained Edge Video scenarios? Video data imposes higher costs, and a proof-of-concept on video QA benchmarks would analyze bandwidth vs. accuracy trade-offs.

## Limitations
- Exact prompt templates for confidence gate, refinement planner, and evidence integration are not disclosed
- MMAR benchmark access/URL not specified in references
- Energy-based segmentation algorithm details are unspecified
- System robustness to varying audio SNR, background noise, and domain shifts is not evaluated
- Reliance on GPT-4o introduces potential brittleness if prompt behavior changes or API access is restricted

## Confidence
- **Core claims**: High (conceptual feasibility of coarse-to-fine edge-cloud collaboration)
- **Quantitative results**: Medium (due to missing implementation details)

## Next Checks
1. Reconstruct and validate the adaptive confidence gate prompt and escalation logic on a held-out subset of MMAR
2. Implement the energy-based segmentation and sliding window proposal pipeline; verify segment quality against ground-truth ROI boundaries
3. Benchmark accuracy and latency under varying network RTTs and hardware constraints (e.g., lower-end GPUs or edge CPUs)