---
ver: rpa2
title: 'From Reproduction to Replication: Evaluating Research Agents with Progressive
  Code Masking'
arxiv_id: '2506.19724'
source_url: https://arxiv.org/abs/2506.19724
tags:
- code
- agents
- file
- function
- experiment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors introduce AUTO EXPERIMENT, a benchmark that evaluates
  AI agents' ability to implement and run machine learning experiments based on research
  papers. The benchmark scales difficulty by progressively masking out functions from
  original codebases, ranging from partial reproduction to full replication.
---

# From Reproduction to Replication: Evaluating Research Agents with Progressive Code Masking

## Quick Facts
- arXiv ID: 2506.19724
- Source URL: https://arxiv.org/abs/2506.19724
- Reference count: 40
- Agents implementing masked functions from research papers show sharp performance drops as masking increases, with dynamic interaction significantly outperforming fixed harnesses.

## Executive Summary
AUTO EXPERIMENT is a benchmark that evaluates AI agents' ability to implement and run machine learning experiments from research papers by progressively masking functions in codebases. The benchmark scales difficulty from partial reproduction (n=1 masked function) to full replication (n=5 masked functions), revealing that performance plummets as soon as two functions are masked out. Dynamic agents that can iteratively debug code significantly outperform fixed-harness approaches, and there's a substantial gap between single-shot and multi-trial success rates, suggesting opportunities for verifier-based improvements.

## Method Summary
The benchmark uses 4 peer-reviewed ML papers from the ML Reproducibility Challenge, providing paper text, partial codebases with functions replaced by NotImplementedError, and command sequences to run experiments. Agents implement the masked functions using ReAct prompting with 10 tools for file manipulation and execution, then run experiments in a sandbox environment with 5% relative tolerance against gold results. Performance is measured as pass rates across 85 unique functions, with difficulty scaled by varying the number of masked functions (n=1 to n=5).

## Key Results
- Performance drops 70-90% on average when increasing from n=1 to n=2 masked functions
- Dynamic agents achieve 29.1% pass rate vs 8.3% for fixed harnesses on n=1
- Significant gap exists between Pass@1 (35.3%) and Pass@5 (48.2%) success rates, suggesting verifier opportunities

## Why This Works (Mechanism)

### Mechanism 1: Progressive Code Masking as Difficulty Scaling
- Claim: Masking increasing numbers of functions creates a controllable difficulty gradient from reproduction to replication.
- Mechanism: The benchmark removes n ≥ 1 functions from research codebases, replacing implementations with `NotImplementedError`. As n increases, agents must synthesize more code from natural language descriptions rather than relying on existing code patterns.
- Core assumption: The mapping between paper descriptions and code implementations is learnable but requires increasing natural language reasoning as code scaffolding decreases.
- Evidence anchors:
  - [abstract] "AUTO EXPERIMENT scales in difficulty by varying the number of missing functions n, ranging from partial reproduction to full replication."
  - [section 3.1] "Performance plummets as soon as n = 2... a performance drop of 70-90% on average."
  - [corpus] LMR-BENCH and PaperBench similarly evaluate reproduction/replication but use fixed difficulty settings.
- Break condition: If agents could easily infer masked function implementations from surrounding code signatures alone, the difficulty scaling would fail.

### Mechanism 2: Dynamic Interaction Enables Recovery from Initial Failures
- Claim: Agents that can iteratively debug and revise code significantly outperform fixed-harness approaches.
- Mechanism: Dynamic agents observe execution errors (e.g., device mismatches), diagnose causes, and apply targeted fixes. Fixed harnesses lack this feedback loop.
- Core assumption: Error messages provide actionable signal for correction, and agents can map errors to appropriate fixes.
- Evidence anchors:
  - [abstract] "Agents that can dynamically interact with the environment (e.g. to debug their code) can outperform agents in fixed 'agentless' harnesses."
  - [section 3.3] "69.4% of runs crash initially... 29.1% recover to working state, 18.6% end up with correct working code."
  - [corpus] DeepCode emphasizes "high-fidelity document-to-codebase synthesis" but does not isolate the debugging loop contribution.
- Break condition: If error messages are uninformative or require domain knowledge beyond agent capabilities, dynamic interaction provides no advantage.

### Mechanism 3: Pass@k Gap Indicates Latent Capability Requiring Verification
- Claim: The gap between single-shot (Pass@1) and multi-trial (Pass@5) success rates suggests agents generate correct solutions but cannot reliably select them.
- Mechanism: Agents produce diverse solution candidates; an oracle verifier selecting among them could capture substantial performance gains (35.3% → 48.2% for GPT-4o).
- Core assumption: The correct solution exists within the k candidates, and verification is easier than generation.
- Evidence anchors:
  - [abstract] "There exists a significant gap between single-shot and multi-trial success rates (Pass@1 vs. Pass@5), motivating verifier approaches."
  - [section 3.2] "Models used as their own verifiers could improve performance somewhat but there is still a significant gap to oracle-level performance."
  - [corpus] No direct corpus comparison for Pass@k on scientific code; this appears novel to this benchmark.
- Break condition: If correct solutions are rarely among the k candidates, verification cannot help.

## Foundational Learning

- Concept: **Reproduction vs. Replication**
  - Why needed here: The benchmark explicitly interpolates between running existing code (reproduction) and implementing from paper descriptions (replication). Understanding this spectrum is essential for interpreting n-scaling results.
  - Quick check question: If n=1, is the agent closer to reproduction or replication? What about n=5?

- Concept: **Agent Tool Definitions and Action Spaces**
  - Why needed here: The paper defines 10+ tools (execute_command, edit_file, understand_file, etc.). Agent performance depends on effective tool selection.
  - Quick check question: Which tool would an agent use to diagnose a runtime error after code execution?

- Concept: **Prompting Strategies (ReAct, MLAgentBench, Planning-Only)**
  - Why needed here: Section C shows ReAct + Full history achieves 38.9% vs. 8.1% for MLAgentBench with GPT-4o-mini. Strategy choice matters.
  - Quick check question: Why might ReAct outperform Planning-Only for debugging-heavy tasks?

## Architecture Onboarding

- Component map:
  - Initial Prompt -> Tool Layer (10 tools) -> Prompting Strategy (ReAct/MLA/Planning) -> History Management (Full/Sliding/Summary) -> Backbone LLM -> Evaluation Harness

- Critical path:
  1. Agent receives paper + masked codebase + run commands
  2. Agent retrieves context (paper sections, code snippets)
  3. Agent implements masked function(s)
  4. Agent executes experiment, observes errors
  5. Agent debugs iteratively (if dynamic)
  6. Agent submits final results via `final_answer` tool

- Design tradeoffs:
  - Fixed harness: Lower cost, predictable runtime; no debugging → lower performance
  - Dynamic agent: Higher cost (avg $0.59/run for GPT-4o), 12.7 min runtime; enables recovery
  - Reasoning tokens (o1/o3-mini): Moderate gains (8.3% → 27.8%);Diminishing returns after 8K tokens

- Failure signatures:
  - **Device mismatch errors**: Tensors on CPU vs. GPU (see example trajectory Step 5-6)
  - **Import/dependency errors**: Agent attempts unavailable packages
  - **Context window overflow**: Full history exceeds model limits (GPT-4o-mini benefits from summarization)
  - **Numerical divergence**: Results exceed 5% relative tolerance

- First 3 experiments:
  1. Run n=1 with GPT-4o, ReAct + Full history on 10 samples; measure Pass@1 and average runtime
  2. Compare dynamic vs. fixed harness on same samples; quantify debugging recovery rate
  3. Run n=2 with Pass@5 sampling; measure gap between oracle selection and model-as-verifier selection

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can model-based verifiers be developed that close the gap between Pass@1 and oracle-level Pass@5 performance on scientific code generation tasks?
- Basis in paper: [explicit] The authors observe a "large gap between Pass@1 and Pass@5" (35.3%→48.2% for GPT-4o) and state this "presents an interesting challenge for model-based verifiers and search."
- Why unresolved: Self-verification improves performance only somewhat; a substantial gap remains to oracle-level verification.
- What evidence would resolve it: A verifier achieving Pass@5-comparable performance when selecting from k candidates at test time.

### Open Question 2
- Question: How should prompting strategies and memory management be jointly optimized for long-horizon scientific code generation?
- Basis in paper: [inferred] The authors found no statistically significant difference between the best (ReAct+Full) and second-best configurations (p=0.243), noting "this warrants further investigation into optimizing agent architectures."
- Why unresolved: Performance varies across model capabilities (e.g., smaller models benefit from summarization instead of full context).
- What evidence would resolve it: A systematic study identifying optimal prompting-memory combinations across model scales.

### Open Question 3
- Question: How can code retrieval be improved to better support scientific experiment implementation?
- Basis in paper: [explicit] The authors identify code retrieval as "a central challenge in our benchmark and a key area for future research," finding that embedding-based retrieval (41.7%) outperforms but doesn't solve the task.
- Why unresolved: Simply placing all code in context degrades performance, and AST-based retrieval underperforms.
- What evidence would resolve it: A retrieval method achieving significantly higher pass rates than embedding-based approaches.

## Limitations
- Focuses exclusively on machine learning papers and Python-based code; generalizability to other scientific domains and programming languages is unclear.
- Evaluation relies on synthetic curtailment of test cases rather than full experiment execution, potentially missing some implementation issues.
- Benchmark design assumes access to complete paper descriptions and masked codebases, which may not reflect real-world replication scenarios.

## Confidence
- High: The benchmark design is well-specified with clear metrics (Pass@1 vs Pass@5, 5% tolerance) and reproducible methodology.
- Medium: Results show consistent trends across different model configurations, though some performance differences are not statistically significant.
- Low: The evaluation is limited to 4 papers in ML domain, raising questions about broader applicability.

## Next Checks
1. Verify that gold standard experiments reproduce correctly within 5% tolerance before evaluating agents
2. Confirm Docker sandbox setup works with conda environments for all 4 paper codebases
3. Test dynamic agent recovery by measuring debugging success rate on initially crashed runs