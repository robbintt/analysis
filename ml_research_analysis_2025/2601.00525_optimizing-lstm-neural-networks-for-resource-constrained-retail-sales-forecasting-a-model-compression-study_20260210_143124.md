---
ver: rpa2
title: 'Optimizing LSTM Neural Networks for Resource-Constrained Retail Sales Forecasting:
  A Model Compression Study'
arxiv_id: '2601.00525'
source_url: https://arxiv.org/abs/2601.00525
tags:
- forecasting
- lstm
- retail
- compression
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses resource-constrained retail sales forecasting
  by systematically evaluating LSTM compression. The core idea is to reduce the number
  of hidden units in LSTM models from 128 to 16 while examining the trade-off between
  model size and forecast accuracy.
---

# Optimizing LSTM Neural Networks for Resource-Constrained Retail Sales Forecasting: A Model Compression Study

## Quick Facts
- arXiv ID: 2601.00525
- Source URL: https://arxiv.org/abs/2601.00525
- Authors: Ravi Teja Pagidoju
- Reference count: 15
- One-line primary result: Reducing LSTM hidden units from 128 to 64 improves retail sales forecasting accuracy by 47% while reducing model size by 73%

## Executive Summary
This paper addresses the challenge of deploying advanced forecasting models in resource-constrained retail environments by systematically evaluating LSTM model compression. Through experiments on a large retail sales dataset (913,000 daily records), the study demonstrates that reducing LSTM hidden units from 128 to 64 achieves optimal performance, improving accuracy from 23.6% to 12.4% MAPE while shrinking model size from 280KB to 76KB. The findings reveal that excessive model capacity causes overfitting in retail forecasting, and that moderate compression enables accurate predictions without requiring GPU acceleration. This makes advanced forecasting accessible to small and medium retailers operating on standard hardware.

## Method Summary
The study uses the Kaggle Store Item Demand Forecasting dataset with 10 stores, 50 items, and 5 years of daily sales data. Models are trained using 30-day sliding windows with 7 engineered features (lag features at 1, 7, 30 days; rolling averages at 7, 30 days; day-of-week and month). Five LSTM configurations are tested with hidden units of 128, 64, 48, 32, and 16, all using single-layer architecture with 0.2 dropout, 16-unit dense layer, and trained for 30 epochs with Adam optimizer and MAE loss. The optimal configuration uses 64 hidden units, achieving 12.4% MAPE while requiring no GPU acceleration and maintaining ~23ms inference time on CPU.

## Key Results
- Optimal compression: 64 hidden units achieves 12.4% MAPE, a 47% improvement over 128-unit baseline (23.6% MAPE)
- Model size reduction: 73% decrease from 280KB to 76KB while maintaining accuracy
- Hardware independence: CPU-only inference at ~23ms makes GPU acceleration unnecessary
- U-shaped error curve: Performance degrades at both extremes (128 units overfits, 16 units underfits)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reducing hidden units from 128 to 64 prevents overfitting by constraining model capacity to match the inherent complexity of retail sales patterns.
- Mechanism: Excessive hidden units (128) allow the model to memorize training noise and idiosyncrasies rather than learning generalizable temporal patterns. The 64-unit configuration provides sufficient capacity to capture weekly/monthly seasonality while preventing memorization, yielding 47% accuracy improvement (23.6% → 12.4% MAPE).
- Core assumption: Retail sales data contains learnable patterns (seasonality, trends) that are lower-complexity than the representational capacity of a 128-unit LSTM.
- Evidence anchors:
  - [abstract] "lowering the number of hidden LSTM units to 64 maintains the same level of accuracy while also improving it... MAPE ranges from 23.6% for the full 128-unit model to 12.4% for the 64-unit model"
  - [section IV.A] "The 128-unit model performs the worst, with a 23.6% MAPE, which means that it may have overfitted the training data"
  - [corpus] Limited direct corpus validation for this specific overfitting phenomenon in retail LSTM compression; related work focuses on model comparison rather than compression-accuracy dynamics
- Break condition: If retail data exhibited highly irregular, non-seasonal patterns with complex multi-variable interactions, capacity reduction might underfit rather than improve generalization.

### Mechanism 2
- Claim: Retail sales forecasting exhibits a "sweet spot" in model capacity where the U-shaped error curve reflects the bias-variance tradeoff.
- Mechanism: The U-shaped relationship (best at 64 units, degraded at both 128 and 16 units) reflects classical bias-variance dynamics: large models overfit (high variance), very small models underfit (high bias). The 32-64 unit range captures the optimal balance for this dataset's complexity.
- Core assumption: The Kaggle Store Item dataset (10 stores, 50 items, 5 years) is representative of typical retail sales temporal complexity.
- Evidence anchors:
  - [section IV.A] "The results show that model size and error are related in a U shape, with the best performance at 64 units"
  - [Table I] Shows MAPE values: 128→23.6%, 64→12.4%, 48→12.8%, 32→12.3%, 16→12.5%
  - [corpus] Neighboring papers do not systematically test this U-shaped compression hypothesis; they compare model families (LSTM vs TFT vs GNN) rather than architectural variants
- Break condition: Datasets with substantially different complexity (e.g., highly promotional retail, multi-channel interactions) may shift the optimal point.

### Mechanism 3
- Claim: CPU-bound inference performance is dominated by framework overhead rather than model parameter count at small scales.
- Mechanism: Despite 97% size reduction (280KB → 7KB), inference time remains ~23ms across all configurations because TensorFlow's fixed overhead (session initialization, tensor allocation) dominates the computational budget for models this small.
- Core assumption: The bottleneck analysis holds for the specific hardware/software configuration tested (Intel i5, TensorFlow 2.12).
- Evidence anchors:
  - [section IV.B] "inference times stay the same across all models (about 23ms) because the computational bottleneck moves from matrix operations to framework overhead"
  - [Table III] Shows consistent 23.0-23.7ms inference time across all model sizes
  - [corpus] No corpus papers specifically analyze inference latency scaling for compressed LSTMs at this scale
- Break condition: Larger models, different frameworks, or GPU deployment would shift the bottleneck back to compute.

## Foundational Learning

- Concept: **LSTM gate mechanics (forget gate, input gate, output gate)**
  - Why needed here: Understanding how hidden units affect the model's ability to retain/discard temporal information across the 30-day lookback window.
  - Quick check question: Can you explain why reducing hidden units limits the model's ability to memorize but not its ability to learn seasonal patterns?

- Concept: **Bias-variance tradeoff and overfitting detection**
  - Why needed here: The core finding hinges on recognizing that the 128-unit model's poor performance (23.6% MAPE) indicates overfitting, not underfitting.
  - Quick check question: If training loss decreases but validation MAPE increases as you add hidden units, what is happening?

- Concept: **Time series feature engineering (lag features, rolling statistics)**
  - Why needed here: The paper's 7-feature input (1/7/30-day lags, rolling averages, temporal features) determines what patterns the LSTM can access.
  - Quick check question: Why would a 30-day rolling average help prevent the model from overfitting to daily noise?

## Architecture Onboarding

- Component map:
  Input (30 days × 7 features) → LSTM Layer (64 hidden units) → Dropout (0.2) → Dense (16 units) → Output (1 prediction)

- Critical path:
  1. Feature engineering: Create lag features (1, 7, 30 days), rolling averages (7, 30 days), day-of-week, month
  2. Normalization: MinMax scaling to [0,1] range
  3. Model configuration: Start with 64 hidden units (proven optimal in paper)
  4. Training: 80/20 temporal split, 30 epochs, batch size 64, Adam optimizer, MAE loss
  5. Evaluation: MAPE as primary metric, cross-validation for temporal validity

- Design tradeoffs:
  - **Size vs. Accuracy**: 64 units = 76KB, 12.4% MAPE (recommended); 32 units = 22KB, 12.3% MAPE (extreme compression acceptable); 128 units = 280KB, 23.6% MAPE (avoid—overfits)
  - **CPU vs. GPU**: Compressed models (≤64 units) achieve 23ms inference on CPU; no GPU required
  - **Simplicity vs. sophistication**: Paper intentionally tests single-layer LSTM; multi-layer or attention mechanisms unexplored

- Failure signatures:
  - **Overfitting**: Training MAPE decreases but validation MAPE rises above 20% → reduce hidden units
  - **Underfitting**: Both training and validation MAPE stagnate above 15% → increase hidden units or check feature quality
  - **Data leakage**: Unrealistically low MAPE (<5%) → verify temporal split integrity, ensure no future information in lag features

- First 3 experiments:
  1. **Reproduce baseline**: Train LSTM-64 on the Kaggle dataset with paper's exact configuration (30-day window, 7 features, 64 units) to validate ~12.4% MAPE result
  2. **Compression sweep on your data**: Test hidden units [128, 64, 48, 32, 16] on your specific retail dataset to find your own optimal point (may differ from paper's 64-unit finding)
  3. **Feature ablation**: Remove rolling averages or reduce lag window to 7 days to understand which features drive the accuracy gains; this reveals whether improvements come from architecture or feature engineering

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the U-shaped accuracy curve observed in hidden unit reduction be generalized to diverse retail datasets with different noise profiles or seasonality?
- Basis in paper: [explicit] The authors state in the Limitations section that results are specific to the Kaggle dataset and call for "testing on a variety of retail datasets" in the Conclusion.
- Why unresolved: It is unclear if the "over-parameterization" found in the 128-unit model is unique to the relatively simple patterns in the Kaggle dataset or a universal trait of retail forecasting.
- Evidence: Replicating the compression methodology on datasets with complex external factors (e.g., promotional spikes, holidays) to see if LSTM-64 remains optimal.

### Open Question 2
- Question: Does combining architecture reduction with quantization yield additive efficiency gains, or does it degrade the accuracy improvements achieved by hidden unit reduction?
- Basis in paper: [explicit] The Conclusion proposes future work should explore "combining architecture optimization with quantization for more compression."
- Why unresolved: The study isolated architecture reduction; it is unknown if 8-bit quantization interacts negatively with the already reduced representational capacity of the LSTM-64 model.
- Evidence: Applying post-training quantization to the 64-unit model and measuring if the MAPE remains stable (approx. 12.4%) while further reducing the 76KB model size.

### Open Question 3
- Question: How does the optimal model capacity shift when scaling to multi-layer LSTM architectures or incorporating attention mechanisms?
- Basis in paper: [explicit] The authors note in the Limitations that they "only tested single-layer LSTM" and suggest adding "multi-layer architectures and attention mechanisms" as future work.
- Why unresolved: Deeper networks might require different degrees of compression than single-layer networks to avoid overfitting, meaning the 64-unit "sweet spot" may not apply to complex architectures.
- Evidence: Evaluating the accuracy-efficiency trade-off for 2-layer LSTM models and Temporal Fusion Transformers compressed to similar parameter counts.

## Limitations
- Dataset specificity: Results based on Kaggle dataset with 10 stores and 50 items may not generalize to retailers with different characteristics
- Single-model architecture: Only single-layer LSTM tested; benefits of deeper architectures or attention mechanisms remain unexplored
- Limited feature scope: While 7 features proved effective, optimal feature sets for different retail contexts are unknown

## Confidence
- **High confidence** in core finding that reducing hidden units from 128 to 64 prevents overfitting and improves accuracy for this specific dataset and task
- **Medium confidence** in U-shaped error curve generalization across retail datasets
- **Medium confidence** in inference time claims regarding framework overhead dominance

## Next Checks
1. **Dataset generalization test**: Apply the 64-unit LSTM architecture to a different retail dataset (e.g., UCI Online Retail, store-specific POS data) to verify the compression-accuracy relationship holds across retail contexts
2. **Feature importance analysis**: Systematically remove individual features (rolling averages, lag windows, temporal features) to quantify their contribution to the 12.4% MAPE and determine if accuracy gains come from compression or superior feature engineering
3. **Multi-store forecasting experiment**: Extend the model to handle multiple stores simultaneously (versus store-by-store) to assess whether compression benefits scale to more complex forecasting scenarios with higher data dimensionality