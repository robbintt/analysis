---
ver: rpa2
title: 'Learning to be Reproducible: Custom Loss Design for Robust Neural Networks'
arxiv_id: '2601.00578'
source_url: https://arxiv.org/abs/2601.00578
tags:
- training
- variability
- loss
- across
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of run-to-run variability in deep
  learning models due to stochastic factors such as weight initialization, data shuffling,
  and optimizer behavior. A Custom Loss Function (CLF) is introduced that explicitly
  incorporates stability and variance penalties to regularize training and reduce
  sensitivity to these factors.
---

# Learning to be Reproducible: Custom Loss Design for Robust Neural Networks

## Quick Facts
- arXiv ID: 2601.00578
- Source URL: https://arxiv.org/abs/2601.00578
- Reference count: 6
- Primary result: Custom loss function reduces run-to-run variability in deep learning models without sacrificing accuracy

## Executive Summary
This paper addresses the pervasive problem of run-to-run variability in deep learning models caused by stochastic factors such as weight initialization, data shuffling, and optimizer behavior. The authors introduce a Custom Loss Function (CLF) that incorporates stability and variance penalties to regularize training and reduce sensitivity to these factors. Through extensive experiments on image classification (CIFAR-10/100) and time series forecasting (ETTh1), CLF demonstrates significant reductions in model performance variability across multiple seeds while maintaining or improving accuracy.

## Method Summary
The method introduces a Custom Loss Function (CLF) that extends standard cross-entropy loss with two auxiliary terms: Stable Loss (SL) and Variance Penalty Loss (VPL). SL penalizes sudden changes in loss between consecutive epochs to smooth the optimization trajectory, while VPL enforces intra-batch consistency by penalizing variance in logits across samples of the same class. The hyperparameters λs and λv controlling these terms are tuned using Optuna to minimize validation variance. The approach is validated across multiple architectures including ResNet, ShuffleNet, VGG, Autoformer, and iTransformer on both image and time series tasks.

## Key Results
- CLF reduces accuracy standard deviation by up to 77.1% on CIFAR-100 with ShuffleNet-V2
- On CIFAR-100 with ResNet-14, accuracy standard deviation dropped by 39.4%
- Time series forecasting showed variability reductions of 8.70% to 27.78% across models and horizons
- Longer CLF activation during training yields better stability benefits than late-stage activation

## Why This Works (Mechanism)

### Mechanism 1: Stable Loss (SL)
SL reduces temporal fluctuations by penalizing sudden changes in loss between consecutive epochs, smoothing the optimization trajectory. It computes |CEL(θ;x, y) - CEL_prev|, where CEL_prev is the cross-entropy from the previous epoch. The gradient ∇_θSL = sign(CEL - CEL_prev) · ∇_θCEL modulates the direction and magnitude of updates based on whether loss is increasing or decreasing. This acts as a damping signal against sharp optimization swings.

### Mechanism 2: Variance Penalty Loss (VPL)
VPL enforces intra-batch consistency by penalizing variance in logits across samples of the same class, reducing sensitivity to batch-level noise. It computes average per-class variance within mini-batches: VPL = (1/|C|) Σ_j Var_j, where Var_j = (1/m_j) Σ_i (f_j(x_i;θ) - f̄_j)². The gradient pushes same-class logits toward their class mean, tightening output-space clustering.

### Mechanism 3: Training Duration Effects
Longer CLF activation duration during training yields cumulative stability benefits; late-stage activation offers limited gains because the model has already converged under standard loss. CLF's regularization terms require sufficient epochs to shape the optimization trajectory, and early and sustained application allows gradients to accumulate stabilizing influences throughout training.

## Foundational Learning

- **Reproducibility vs. Determinism**: Understanding the distinction between fixing random seeds (narrow reproducibility for one seed) and reducing sensitivity to stochastic factors (broader robustness) is essential to grasp why CLF targets variance reduction rather than deterministic replay. Quick check: If you train a model 20 times with 20 different seeds and observe high accuracy variance, does fixing one seed solve the underlying instability problem?

- **Gradient Modulation via Auxiliary Loss Terms**: CLF adds SL and VPL to cross-entropy. Understanding how auxiliary losses modify gradient flow—through sign operations (SL) and variance computations (VPL)—is necessary to predict how they interact with existing optimization dynamics. Quick check: What happens to the gradient of SL when the current epoch's loss equals the previous epoch's loss?

- **Variance Reduction as a Training Objective**: The paper explicitly optimizes for reduced validation variance (via Optuna), not just accuracy. This reframes the training goal from maximizing performance to stabilizing performance across runs. Quick check: Why might minimizing variance across seeds require different hyperparameters than maximizing accuracy on a single run?

## Architecture Onboarding

- **Component map**: Input (x, y) → Model Forward Pass → Logits f(x;θ) → Custom Loss Function (CLF) → Backward Pass → Optimizer Update (SGD with momentum)
- **Critical path**: Implement CEL as baseline, add SL by storing CEL_prev at epoch end, add VPL by computing per-class mean logits within batch, tune λs and λv using Optuna with objective = minimize validation variance, run 10-20 seeds with/without CLF to measure std(accuracy) reduction
- **Design tradeoffs**: λs too high over-smooths optimization and may slow convergence, λv too high over-constrains same-class logits and may reduce model expressiveness, early activation recommended vs. late-stage activation ineffective, high-variance architectures benefit more than already-stable models
- **Failure signatures**: CLF increases std(accuracy) on overparameterized models (e.g., ResNet-32 on CIFAR-100), no variance reduction despite CLF (model may have inherently low baseline variability), accuracy degradation (reduce λs or λv)
- **First 3 experiments**: 1) Baseline variance measurement: Train target architecture 10-20 times with different seeds using standard CEL, record mean accuracy and std dev; 2) CLF integration with default weights: Add SL and VPL with λs = 0.1, λv = 0.1 as starting point, train 5 seeds, compare std(accuracy) to baseline; 3) Hyperparameter sweep via Optuna: Run 20-50 Optuna trials optimizing for score = norm_std - norm_acc (minimize), test best configuration across 10+ seeds

## Open Questions the Paper Calls Out

### Open Question 1
Can a dynamic scheduling mechanism for the CLF regularization weights (λs, λv) improve the balance between training stability and final accuracy compared to fixed weights? The authors note in the "Limitations" section that fixed hyperparameters may reduce performance and suggest that "dynamically adjusting CLF based on training signals could enhance stability."

### Open Question 2
Why does CLF increase variability in certain over-parameterized scenarios, such as ResNet-32 on CIFAR-100, and how can this negative effect be mitigated? The "Results" section notes that for ResNet-32 on CIFAR-100, CLF resulted in higher standard deviation in accuracy compared to the baseline, suggesting a conflict with overfitting dynamics.

### Open Question 3
Is the reduction in training variability cost-effective for large-scale datasets and models, given the computational overhead of the required hyperparameter search and repeated runs? The "Limitations" section acknowledges that "the need for multiple training runs per dataset-architecture pair limits large-scale testing, leaving this area unexplored."

## Limitations
- Limited generalizability to non-image and non-sequential data domains where run-to-run variability may manifest differently
- Potential overfitting to CIFAR/ETTh1-specific dataset characteristics (balanced classes, fixed sequence length)
- Optuna hyperparameter optimization may find dataset-specific λs/λv values that don't transfer well across domains

## Confidence

**Major Uncertainties:**
- Limited generalizability to non-image and non-sequential data domains
- Potential overfitting to CIFAR/ETTh1-specific dataset characteristics
- Optuna hyperparameter optimization may find dataset-specific values

**Confidence Labels:**
- **High Confidence**: CLF reduces run-to-run variability as measured by standard deviation across seeds
- **Medium Confidence**: The mechanism by which SL smooths optimization trajectory operates as described
- **Low Confidence**: VPL's effectiveness depends heavily on batch composition and class balance

## Next Checks
1. Test CLF on a dataset with severe class imbalance to verify VPL remains effective when per-class sample counts vary widely
2. Apply CLF to a regression task (e.g., tabular data) to assess domain transferability beyond classification and time series
3. Measure actual parameter stability (e.g., weight vector cosine similarity across seeds) to validate whether CLF reduces not just output variance but underlying model variability