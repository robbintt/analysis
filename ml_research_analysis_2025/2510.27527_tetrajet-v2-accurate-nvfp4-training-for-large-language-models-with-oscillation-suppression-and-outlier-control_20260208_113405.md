---
ver: rpa2
title: 'TetraJet-v2: Accurate NVFP4 Training for Large Language Models with Oscillation
  Suppression and Outlier Control'
arxiv_id: '2510.27527'
source_url: https://arxiv.org/abs/2510.27527
tags:
- training
- oscillation
- quantization
- weight
- weights
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper tackles the challenge of training large language models
  (LLMs) using ultra-low precision 4-bit floating point (NVFP4) formats, which is
  crucial for reducing computational costs and memory requirements. The authors identify
  two main obstacles to accurate FP4 training: weight oscillation (where quantized
  weights fluctuate significantly during training) and outlier features (activations
  with extreme values that are poorly represented in low precision).'
---

# TetraJet-v2: Accurate NVFP4 Training for Large Language Models with Oscillation Suppression and Outlier Control

## Quick Facts
- **arXiv ID:** 2510.27527
- **Source URL:** https://arxiv.org/abs/2510.27527
- **Reference count:** 40
- **Primary result:** Achieves 51.3% average reduction in performance gap to full-precision training for LLMs using NVFP4 format

## Executive Summary
This paper addresses the challenge of training large language models (LLMs) using ultra-low precision 4-bit floating point (NVFP4) formats. The authors identify two main obstacles to accurate FP4 training: weight oscillation (where quantized weights fluctuate significantly during training) and outlier features (activations with extreme values poorly represented in low precision). TetraJet-v2 introduces three key innovations: unbiased double-block quantization for accurate gradient computation, OsciReset to suppress weight oscillation by resetting oscillating weights to quantization bin centers, and OutControl to retain outlier accuracy by selectively preserving outlier channels in higher precision during both forward and backward passes.

## Method Summary
TetraJet-v2 combines three mechanisms to enable accurate NVFP4 training for LLMs. The core innovation is an unbiased double-block quantization method that aligns forward and backward tensor representations while using stochastic rounding for unbiased gradient estimation. OsciReset monitors weight oscillation risk over training steps and resets weights that exceed a threshold to their quantization bin center, suppressing late-stage training instability. OutControl statically identifies structurally persistent outlier channels at training start and retains these in higher precision (FP8), while applying Random Hadamard Transform in backward passes to control quantization variance. The method is evaluated on pre-training OLMo2 models (70M, 150M, and 370M parameters) for up to 200 billion tokens.

## Key Results
- Reduces performance gap to full-precision training by 51.3% average across various model sizes and data scales
- Outperforms previous FP4 training methods on perplexity metrics (31.28 PPL vs NVIDIA's 32.42 on OLMo2-150M)
- Shows consistent improvements across downstream tasks including ARC, BoolQ, MMLU, and SQuAD
- Demonstrates scalability from 70M to 370M parameter models while maintaining accuracy

## Why This Works (Mechanism)

### Mechanism 1: Unbiased Double-Block Quantization
- Claim: Aligning forward/backward tensor representations and using stochastic rounding produces unbiased gradient estimates, improving convergence over deterministic alternatives.
- Mechanism: Uses 1×128 outer-block with global scale S_global to handle NVFP4's E4M3 range limit [−448, 448], plus 1×16 inner-block scaling. Backward pass computes gradients on quantized activation bX rather than raw X, ensuring alignment. Stochastic rounding replaces deterministic RTN: E[round_FP4,S(x)] = x.
- Core assumption: Unbiased gradient estimation is necessary for SGD convergence guarantees; misaligned forward/backward representations introduce systematic bias.
- Evidence anchors: Table 3b shows TetraJet-v2-base achieves 31.49 PPL vs NVIDIA's 32.42, with ablations confirming both bX alignment and stochastic quantization contribute.

### Mechanism 2: OsciReset (Weight Oscillation Suppression)
- Claim: Resetting oscillating weights to the center of their quantization bin stabilizes late-stage training without freezing parameters.
- Mechanism: Tracks OsciRisk(w) = dist_Q(w)/dist_M(w) over T_accu steps, where dist_Q measures quantized weight changes and dist_M measures master weight changes. When OsciRisk(w) ≥ τ_osci after learning rate decays, resets master weight: w ← w_FP4 × scale_i.
- Core assumption: Oscillation occurs when master weights hover near quantization bin boundaries; small gradients cause large quantized jumps. Resetting to bin center gives "more chances to be optimized to other values."
- Evidence anchors: Figure 5 shows OsciReset maintains improvement while prior methods present severe detriment to final performance; Figure 6 shows oscillation weight proportion drops from ~9% to ~4% with OsciReset.

### Mechanism 3: OutControl (Outlier Precision Retention + RHT)
- Claim: Static selection of structurally persistent outlier channels, combined with Random Hadamard Transform in backward pass, recovers accuracy lost to FP4's limited dynamic range.
- Mechanism: (1) At training start, selects top p% channels by activation norm and retains these in higher precision for both forward and backward. (2) Applies RHT to all backward MMs: dX = Q(dY·S_c·H_c) × Q(H_c^T·S_c^T·cW) where S_c is random sign-flip diagonal and H_c is block Hadamard.
- Core assumption: Outlier channels are structurally persistent across inputs and training steps. RHT in backward controls quantization variance without harming forward optimization.
- Evidence anchors: Table 4 shows forward-only OutControl gives 31.41 PPL; forward+backward gives 31.28; random selection (31.47) vs largest norms (31.28) validates structural persistence.

## Foundational Learning

- **Concept: Microscaling formats (NVFP4 vs MXFP4)**
  - Why needed here: NVFP4 uses finer granularity (16-element groups vs 32 for MXFP4) and E4M3 scaling (vs E8M0), enabling more accurate quantization but requiring careful range management.
  - Quick check question: Given a 128-element tensor with max |X_i| = 3000, what is S_global for NVFP4 quantization?

- **Concept: Stochastic rounding and unbiased estimation**
  - Why needed here: The paper's theoretical convergence argument relies on E[gradient] = true gradient. Deterministic rounding (RTN) introduces bias when values fall between quantization points.
  - Quick check question: For x = 0.3 with q1 = 0.25, q2 = 0.5, what's the stochastic rounding probability to q1?

- **Concept: Weight oscillation in quantized training**
  - Why needed here: Understanding why weights near bin boundaries cause instability helps diagnose when OsciReset is applicable vs when it's harmful.
  - Quick check question: If dist_M(w) = 0.001 and dist_Q(w) = 0.5 over 50 steps, what's OsciRisk(w)? Should this weight be reset?

## Architecture Onboarding

- **Component map:**
  ```
  Input X (BF16) -> [OutControl: split outlier/normal channels]
                    |
                    |-> Normal: Q_D(X_¬A) -> FP4 MM -> Y
                    |
                    |-> Outlier: X_A (BF16)
  Backward:
  dY -> [RHT: dY·S·H] -> Q_S(dY') -> FP4 MM -> dX, dW
         |                              ^
         |-> [OsciReset: monitor weights, reset at T_start]
  ```

- **Critical path:** The unbiased gradient flow through aligned bX in forward/backward is the most fragile component—any mismatch (e.g., using X instead of bX in dW computation) introduces bias that accumulates over training.

- **Design tradeoffs:**
  - Outer-block size: 1×128 vs per-row vs per-tensor (Table 3a shows 1×128 best; per-tensor worst)
  - Weight block shape: 1×16 vs 16×16 (1×16 better, contrary to NVIDIA's choice)
  - Outlier ratio p: 10% (FP8) vs 5% (BF16)—higher retention improves accuracy but reduces efficiency
  - T_start for OsciReset: Too early harms optimization; too late misses oscillation window

- **Failure signatures:**
  - Validation PPL diverges in late training → Check if OsciReset T_start is set; oscillation may be uncontrolled
  - Training PPL flat despite LR schedule → Check bX alignment in backward; may be computing gradient on wrong tensor
  - Sudden PPL spike at T_start → τ_osci too aggressive; over-resetting weights
  - Forward RHT enabled → Disable immediately; Table 3c shows consistent harm

- **First 3 experiments:**
  1. **Baseline linear layer ablation:** Implement TetraJet-v2-base (double-block + stochastic rounding + RHT backward) on a small model (e.g., OLMo2-70M for 5B tokens). Compare: (a) NVIDIA-style (no bX align, deterministic), (b) bX align only, (c) full TetraJet-v2-base. Expect ~1 PPL improvement per component.
  2. **OsciReset timing sensitivity:** On OLMo2-150M for 50B tokens, vary T_start (40%, 60%, 80% of training) and τ_osci (4, 8, 16). Plot validation PPL curve and oscillation proportion (Figure 6 style). Expect: too early (40%) degrades performance; τ_osci=8 optimal.
  3. **OutControl static vs dynamic:** Compare static outlier selection (once at start) vs dynamic (re-select every N steps) vs random selection. Expect static ≈ dynamic but with lower overhead; random should match "None" baseline (~31.49 PPL in Table 4).

## Open Questions the Paper Calls Out

- **Question:** Do the proposed oscillation suppression and outlier control methods scale effectively to LLMs with billions of parameters (e.g., 7B or 70B)?
  - Basis in paper: [explicit] The authors explicitly state in Section 7 (Conclusions and Limitations) that "due to limited compute resources, our experiments focus on model sizes up to 370M parameters."
  - Why unresolved: While TetraJet-v2 works on smaller scales, the dynamics of weight oscillation and the magnitude of activation outliers may change non-linearly as model width and depth increase, potentially requiring adjustments to the OsciReset threshold or OutControl ratio.
  - What evidence would resolve it: Pre-training results (perplexity and downstream task performance) applying TetraJet-v2 to standard benchmark models like Llama-7B or larger.

- **Question:** What are the actual wall-clock throughput and energy efficiency gains of TetraJet-v2 on native low-precision hardware?
  - Basis in paper: [explicit] The Conclusion notes that "due to the unavailability of hardware that supports low-precision NVFP4 computations, we cannot evaluate the effective speedups."
  - Why unresolved: The study relies on simulation or higher-precision hardware emulation to validate accuracy. The overhead of OsciReset (tracking statistics) and OutControl (splitting precision) might reduce the theoretical $2\times$ throughput gains promised by NVFP4 in practice.
  - What evidence would resolve it: System-level benchmarks on NVIDIA Blackwell architecture (or similar) measuring tokens/sec and TFLOPS utilization during training.

- **Question:** Can mixed-precision training formats (e.g., FP6 × FP4) fully close the performance gap to BF16 training without requiring full FP4 quantization?
  - Basis in paper: [explicit] In Section 6.3, the authors simulate "FP6 × FP4" and observe "substantial improvement," noting that "this result may be inspiring for future study" even though these formats are not yet mainstream.
  - Why unresolved: The paper focuses on "fully FP4" training for maximum memory savings, but the results suggest that a slightly higher bit-width for activations/gradients might solve the accuracy bottleneck more effectively than algorithmic tweaks alone.
  - What evidence would resolve it: A comparative study of TetraJet-v2 against a mixed-precision (FP6 × FP4) baseline on the same model sizes to evaluate the accuracy/efficiency trade-off.

## Limitations
- The static outlier selection method assumes channel-wise outliers are structurally persistent across training, but this may not hold for all model architectures or datasets
- The OsciReset mechanism relies on precise timing (T_start, T_period) that varies by model size, but the paper doesn't provide a systematic method for determining these hyperparameters beyond empirical tuning
- While the double-block quantization addresses NVFP4's E4M3 range limits, the method's performance with tensors exceeding 448×6 remains unclear, which could limit scalability to extremely large models

## Confidence
- **High confidence:** The double-block quantization mechanism and its unbiased gradient properties are well-supported by ablation studies (Table 3b shows consistent improvements)
- **Medium confidence:** OsciReset effectiveness is demonstrated on OLMo2 models, but the mechanism's generalizability to other architectures needs validation
- **Medium confidence:** OutControl's structural outlier persistence is shown for OLMo2, but may not generalize to models with different activation distributions

## Next Checks
1. **Cross-architecture validation:** Test TetraJet-v2 on models with different activation patterns (e.g., OPT, LLaMA) to verify OutControl's structural persistence assumption holds beyond OLMo2
2. **Scalability boundary testing:** Evaluate TetraJet-v2 on tensors with values exceeding 448×6 to determine when outer-block scaling becomes insufficient
3. **Dynamic vs static outlier comparison:** Implement dynamic outlier selection (periodic re-evaluation) and compare against the static method to quantify the trade-off between accuracy and computational overhead