---
ver: rpa2
title: Probabilistic Aggregation and Targeted Embedding Optimization for Collective
  Moral Reasoning in Large Language Models
arxiv_id: '2506.14625'
source_url: https://arxiv.org/abs/2506.14625
tags:
- moral
- llms
- deontology
- alignment
- utilitarianism
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper tackles the challenge of synthesizing consistent moral
  judgments from multiple LLMs when they disagree on complex social dilemmas. The
  core method uses a truncated-normal Expectation-Maximization (EM) framework to aggregate
  continuous moral acceptability scores from different models, weighting each by estimated
  reliability, and then applies targeted embedding optimization to realign models
  that systematically deviate from the consensus.
---

# Probabilistic Aggregation and Targeted Embedding Optimization for Collective Moral Reasoning in Large Language Models

## Quick Facts
- arXiv ID: 2506.14625
- Source URL: https://arxiv.org/abs/2506.14625
- Reference count: 36
- Primary result: Uses truncated-normal EM to aggregate moral scores and targeted embedding optimization to align misaligned LLMs, improving F1 scores up to 21.28% for deontology and 8.21% for utilitarianism.

## Executive Summary
This paper addresses the challenge of synthesizing consistent moral judgments from multiple large language models (LLMs) when they disagree on complex social dilemmas. The authors propose a two-stage framework: first, a truncated-normal Expectation-Maximization (EM) algorithm aggregates continuous moral acceptability scores from different models, weighting each by estimated reliability to produce a collective consensus probability; second, targeted embedding optimization fine-tunes specific theory-token embeddings in misaligned models to align them with the consensus. Experiments on a large-scale social moral dilemma dataset show the approach produces a coherent collective moral reference and improves individual model alignment, with F1 score gains of up to 21.28% for deontology and 8.21% for utilitarianism in the most misaligned models.

## Method Summary
The method uses a truncated-normal EM framework to aggregate continuous moral acceptability scores from multiple LLMs, iteratively estimating each model's reliability parameters (mean μ and variance σ²). For misaligned models, the framework applies targeted embedding optimization by adding new theory tokens, initializing them with the original embeddings, and training them with a loss combining Jensen-Shannon divergence to match the consensus distribution and cosine distance regularization to preserve semantic fidelity. The approach assumes moral misalignment is localized to specific concept representations rather than distributed globally across the network.

## Key Results
- The truncated-normal EM aggregator produced higher F1 alignment with consensus than simple mean aggregation, with gains of 0.6-2.8 percentage points across models.
- Targeted embedding optimization significantly improved alignment for Llama2-13B and GPT-4o-Mini (deontology F1 gains of 21.28% and 8.21%, respectively), but degraded performance for Llama2-7B and Moonshot.
- The framework successfully identified and corrected systematic biases, with Llama2-13B initially favoring justice theory and GPT-4o-Mini initially favoring virtue theory.

## Why This Works (Mechanism)

### Mechanism 1: Reliability-Weighted Continuous Aggregation
The method employs an Expectation-Maximization (EM) algorithm to model each annotation as drawn from a truncated normal distribution conditioned on a latent "true" moral label. Models with variance closer to the consensus boundaries and lower uncertainty exert higher influence on the final probability. This works because moral acceptability exists on a continuous spectrum, and LLMs function as noisy annotators whose reliability can be statistically modeled independently of specific moral theories. The truncated normal is mathematically necessary to model bounded continuous data between 0 and 1.

### Mechanism 2: Localized Embedding Realignment
Targeted fine-tuning of specific theory-token embeddings can align a misaligned model with a collective consensus without destroying its general semantic capabilities. For a model misaligned on theory, new tokens are added and initialized with the embeddings of the original theory tokens, with a feedforward layer appended to predict moral acceptability. The system minimizes a loss function combining Jensen-Shannon divergence and Cosine Distance to prevent the new embeddings from drifting too far from their original semantic neighbors. This works because moral misalignment in LLMs is localized to specific concept representations rather than being distributed globally across network weights.

### Mechanism 3: Consensus as a Supervisory Signal
The collective opinion generated by the probabilistic aggregator serves as a viable ground truth for training, allowing the system to "self-correct" outliers. The aggregated consensus probability is used as the target distribution for the JS divergence loss, effectively distilling collective knowledge back into the individual misaligned model. This works because the aggregated consensus represents a "better" or more standard moral reference than the individual model's initial output (the "wisdom of crowds" applies to LLMs).

## Foundational Learning

- **Concept:** Truncated Normal Distribution
  - **Why needed here:** Moral scores are bounded strictly between 0 and 1. Standard Gaussian distributions assume (-∞, ∞), which creates invalid probabilities; the truncated normal is mathematically necessary to model this bounded continuous data.
  - **Quick check question:** Why can't we simply clip values from a standard Gaussian distribution to fit the [0,1] range without biasing the reliability estimation?

- **Concept:** Expectation-Maximization (EM) Algorithm
  - **Why needed here:** The "true" moral label is latent (unknown). The EM algorithm allows the system to iteratively guess the labels (E-step) and update the model reliability parameters (M-step) until convergence.
  - **Quick check question:** In the E-step, does a high reliability score indicate low variance, a mean close to the binary extremes (0 or 1), or both?

- **Concept:** Jensen-Shannon (JS) Divergence
  - **Why needed here:** The paper uses JS divergence rather than KL divergence as the loss function for embedding optimization. JS divergence is symmetric and bounded, providing more stable gradients when aligning a model's distribution to a target consensus distribution.
  - **Quick check question:** How does the symmetry of JS divergence prevent the "mode collapse" or zero-probability issues often encountered with KL divergence when the target distribution has zeros?

## Architecture Onboarding

- **Component map:** Input Moral Dilemma + Moral Theory → Annotator Pool (LLMs output continuous scores) → Aggregator (Truncated-Normal EM calculates consensus γ and reliability θ) → Critic (Compares individual model scores to γ to identify misalignment) → Refiner (Freezes misaligned model, adds new tokens, trains embeddings via JS Divergence + Cosine Regularization)

- **Critical path:** The calculation of the consensus probability γ_{j,i}. If the EM algorithm fails to converge or assigns high reliability to a noisy model, the "ground truth" used for alignment will be corrupted, making the embedding optimization ineffective or harmful.

- **Design tradeoffs:** The architecture freezes all weights except the specific theory tokens and a single feedforward layer, sacrificing deep reasoning adjustments for safety and preservation of general knowledge. The framework uses continuous scores for aggregation but must binarize (threshold τ) to calculate F1 scores for evaluation, potentially losing nuance in the error analysis.

- **Failure signatures:**
  - **Consensus Collapse:** In groups of similar weak models, the aggregation may fail to find a strong signal (high uncertainty σ), leading to a flat consensus that optimization cannot use.
  - **Token Drift:** If regularization is too weak, the optimized "deontology" token may drift semantically, causing the model to lose the definition of the concept.

- **First 3 experiments:**
  1. **Synthetic Robustness Test:** Inject a "random" annotator (Random01) into the pool. Verify if the EM aggregator down-weights it (low μ, high σ) compared to Mean Aggregation, replicating the "Impact of Random01" analysis.
  2. **Embedding Projection Visualization:** Perform t-SNE on the optimized token embeddings (e.g., *deontology_0) vs. original tokens. Check if the optimized embeddings move closer to key related tokens like "law" or "rule" rather than drifting into random space.
  3. **Ablation on Regularization:** Train the embedding optimizer with loss_{CS} = 0. Evaluate if the model's performance on general linguistic benchmarks (outside moral dilemmas) degrades, confirming the necessity of the semantic preservation constraint.

## Open Questions the Paper Calls Out

- **Open Question 1:** How can the quality of a machine-derived moral consensus be quantitatively evaluated in the absence of an objective ground truth? The authors state that determining what constitutes a "better" moral consensus remains an open question and suggest future research should explore principled approaches such as new agreement metrics or uncertainty quantification.

- **Open Question 2:** Does targeted embedding optimization for specific moral theories cause semantic drift or performance degradation in unrelated reasoning tasks? The authors assume localized adjustments do not adversely affect broader behavior but explicitly call for "extensive evaluations—for instance, on out-of-domain tasks... to confirm that semantic fidelity is preserved."

- **Open Question 3:** Can the aggregation framework be adapted to maintain distinct cultural perspectives rather than collapsing them into a single unified consensus? The authors note that "moral judgments inherently vary across cultures" and suggest that "extensions of this work could incorporate more granular modeling of diverse moral perspectives" rather than a single measure.

- **Open Question 4:** What are the specific boundary conditions of inter-model agreement required for embedding optimization to successfully realign a misaligned model? The authors report a failure case where optimization caused F1 scores to decline for the "Four Llama Variants" group, attributing it to "weak consensus signals," but the exact threshold for failure is not quantified.

## Limitations

- The central claim that the truncated-normal EM aggregator produces a "coherent collective moral reference" rests on the assumption that continuous moral judgments from different LLMs are statistically independent noisy observations of an underlying ground truth, which is strong in the absence of verified human ground truth labels.

- The embedding optimization mechanism is narrowly tested, with only three models showing clear alignment gains and two others showing degradation, suggesting the method is sensitive to initial consensus strength.

- A significant gap is the lack of generalization testing; all reported improvements are on held-out dilemmas from the same distribution as training data, with no evaluation on out-of-distribution moral scenarios or cross-cultural dilemmas.

## Confidence

- **High confidence:** The mathematical formulation of the truncated-normal EM aggregation and its implementation details are clearly specified and reproducible.
- **Medium confidence:** The empirical improvements in F1 scores for deontology and utilitarianism in the most misaligned models are supported by the data, but the broader applicability and failure modes are not fully characterized.
- **Low confidence:** The claim that the consensus represents a universal "moral reference" is not empirically validated against independent human or cross-model benchmarks.

## Next Checks

1. **Synthetic Robustness Test:** Inject a "random" annotator (Random01) into the pool. Verify if the EM aggregator down-weights it (low μ, high σ) compared to Mean Aggregation, replicating the "Impact of Random01" analysis.

2. **Embedding Projection Visualization:** Perform t-SNE on the optimized token embeddings (e.g., *deontology_0) vs. original tokens. Check if the optimized embeddings move closer to key related tokens like "law" or "rule" rather than drifting into random space.

3. **Ablation on Regularization:** Train the embedding optimizer with loss_{CS} = 0. Evaluate if the model's performance on general linguistic benchmarks (outside moral dilemmas) degrades, confirming the necessity of the semantic preservation constraint.