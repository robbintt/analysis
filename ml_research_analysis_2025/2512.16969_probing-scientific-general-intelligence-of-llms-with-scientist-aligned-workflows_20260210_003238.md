---
ver: rpa2
title: Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows
arxiv_id: '2512.16969'
source_url: https://arxiv.org/abs/2512.16969
tags:
- scientific
- reasoning
- data
- step
- research
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper presents SGI-Bench, a comprehensive benchmark for evaluating
  Scientific General Intelligence (SGI) across four core tasks: deep research, idea
  generation, dry/wet experiments, and experimental reasoning. By aligning with the
  Practical Inquiry Model, it operationalizes SGI as the ability to navigate the full
  scientific discovery cycle.'
---

# Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows

## Quick Facts
- **arXiv ID:** 2512.16969
- **Source URL:** https://arxiv.org/abs/2512.16969
- **Reference count:** 40
- **Primary result:** Models achieve low exact-match accuracy (10–20%) in deep research, struggle with realizability in idea generation, and show poor procedural fidelity in wet experiments despite high code executability in dry experiments.

## Executive Summary
The paper introduces SGI-Bench, a benchmark designed to evaluate Scientific General Intelligence (SGI) across four core scientific tasks: deep research, idea generation, dry/wet experiments, and experimental reasoning. By aligning with the Practical Inquiry Model, the benchmark operationalizes SGI as the ability to navigate the full scientific discovery cycle. It includes over 1,000 expert-curated, cross-disciplinary samples and is evaluated via a scientist-aligned agentic framework. Results reveal significant performance gaps, particularly in procedural fidelity and realizability assessment, while demonstrating that multimodal and reinforcement learning approaches can enhance certain aspects of scientific reasoning.

## Method Summary
SGI-Bench operationalizes Scientific General Intelligence through four core tasks—deep research, idea generation, dry/wet experiments, and experimental reasoning—aligned with the Practical Inquiry Model to capture the full scientific discovery cycle. The benchmark comprises over 1,000 expert-curated, cross-disciplinary samples, each evaluated via a scientist-aligned agentic framework. Performance is measured using task-specific metrics such as exact-match accuracy for deep research, realizability scoring for idea generation, code executability and correctness for dry experiments, procedural fidelity for wet experiments, and multimodal reasoning scores for experimental reasoning. The evaluation also incorporates a test-time reinforcement learning approach to enhance hypothesis novelty without ground-truth supervision.

## Key Results
- Models achieve low exact-match accuracy (10–20%) in deep research tasks.
- Idea generation struggles with realizability, conflating novelty with feasibility.
- High code executability but low correctness in dry experiments, with poor procedural fidelity in wet experiments.
- Multimodal approaches improve experimental reasoning, though comparative reasoning remains weakest.

## Why This Works (Mechanism)
The benchmark's alignment with the Practical Inquiry Model ensures comprehensive coverage of the scientific discovery cycle, from inquiry to validation. The scientist-aligned agentic framework provides task-specific, expert-curated evaluations that reflect real-world scientific workflows. Multimodal integration and reinforcement learning enhance hypothesis novelty and reasoning capabilities, demonstrating dynamic evolution of SGI without ground-truth supervision.

## Foundational Learning
- **Practical Inquiry Model:** A framework for structuring scientific discovery; needed to operationalize SGI across the full cycle; quick check: validate alignment with real scientific workflows.
- **Realizability Scoring:** Evaluates the feasibility of generated ideas; needed to distinguish novel but impractical ideas from actionable hypotheses; quick check: assess inter-rater reliability.
- **Procedural Fidelity:** Measures adherence to experimental protocols; needed to ensure practical applicability of generated workflows; quick check: validate with actual lab experiments.
- **Multimodal Integration:** Combines text, code, and visual data; needed to enhance reasoning across diverse scientific domains; quick check: test performance on multimodal vs. unimodal tasks.
- **Reinforcement Learning for Novelty:** Improves hypothesis generation without ground-truth supervision; needed to foster creative yet scientifically meaningful outputs; quick check: validate novelty against domain-specific criteria.

## Architecture Onboarding

**Component Map:** Scientist-aligned agentic framework -> Task-specific evaluators (deep research, idea generation, dry/wet experiments, experimental reasoning) -> Multimodal and RL modules -> Performance metrics

**Critical Path:** Input task → Agentic framework → Task-specific evaluation → Multimodal/RL enhancement → Final SGI score

**Design Tradeoffs:** Balances expert curation with scalability; prioritizes real-world applicability over synthetic benchmarks; integrates multimodal data at the cost of increased complexity.

**Failure Signatures:** Low exact-match accuracy indicates gaps in deep research; poor procedural fidelity highlights limitations in wet experiments; low realizability scores suggest overemphasis on novelty.

**First 3 Experiments:**
1. Inter-rater reliability assessment for realizability and novelty scoring.
2. Ground-truth laboratory validation of wet experiment protocols.
3. Longitudinal tracking of SGI performance across model versions.

## Open Questions the Paper Calls Out
None

## Limitations
- Benchmark relies on exact-match accuracy and realizability scoring, which lack established validity for measuring scientific intelligence.
- Wet lab experiments are evaluated via simulation rather than actual laboratory work, creating a gap between claimed and real-world capability assessment.
- Reinforcement learning for novelty without correctness validation may incentivize scientifically meaningless outputs.

## Confidence
- **High confidence:** Descriptive findings about model performance gaps across tasks.
- **Medium confidence:** Claims about multimodal and RL enhancements improving reasoning.
- **Low confidence:** Operationalization of "genuine scientific discovery" capability.

## Next Checks
1. Conduct systematic kappa analysis across multiple independent scientist teams to quantify and reduce scoring subjectivity.
2. Partner with actual wet lab facilities to validate a subset of model-generated experimental protocols.
3. Implement longitudinal tracking of SGI-Bench performance across model versions to verify dynamic evolution claims.