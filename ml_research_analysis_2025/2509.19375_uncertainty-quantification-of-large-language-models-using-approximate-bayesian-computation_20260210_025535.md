---
ver: rpa2
title: Uncertainty Quantification of Large Language Models using Approximate Bayesian
  Computation
arxiv_id: '2509.19375'
source_url: https://arxiv.org/abs/2509.19375
tags:
- uncertainty
- entropy
- calibration
- class
- bits
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces an Approximate Bayesian Computation (ABC)
  framework for uncertainty quantification in Large Language Models (LLMs) for clinical
  text classification. The method treats classification as a simulation-based inference
  problem, using ABC to estimate posterior distributions over class labels by comparing
  model-generated descriptions with observed patient text.
---

# Uncertainty Quantification of Large Language Models using Approximate Bayesian Computation

## Quick Facts
- arXiv ID: 2509.19375
- Source URL: https://arxiv.org/abs/2509.19375
- Reference count: 40
- Primary result: ABC-based uncertainty quantification improves clinical text classification calibration and accuracy by treating labels as latent hypotheses

## Executive Summary
This work introduces an Approximate Bayesian Computation (ABC) framework for uncertainty quantification in Large Language Models (LLMs) for clinical text classification. The method treats classification as a simulation-based inference problem, using ABC to estimate posterior distributions over class labels by comparing model-generated descriptions with observed patient text. Evaluated on a synthetic oral lesion dataset and the GretelAI symptom-to-diagnosis benchmark, the ABC approach achieved up to 46.9% improvement in accuracy, 74.4% reduction in Brier scores, and enhanced calibration (lower Expected Calibration Error) compared to standard baselines like model logits and elicited probabilities. The method also demonstrated robustness to temperature variation and weight quantization, with SMC-ABC showing superior adaptation in stochastic settings. Limitations include computational cost and challenges in distinguishing classes with subtle differences. Overall, ABC offers a principled, interpretable approach for calibrated uncertainty in high-stakes clinical LLM applications.

## Method Summary
The ABC framework re-frames clinical text classification as inverse inference by treating class labels as latent hypotheses. For each candidate label, the LLM generates a synthetic patient description, which is then compared to the observed text using semantic embeddings. The distance between generated and observed embeddings determines label acceptance frequency, approximating the posterior distribution over classes. The method implements both rejection sampling and Sequential Monte Carlo (SMC-ABC) variants, with SMC-ABC adaptively concentrating sampling in high-likelihood regions. Inference requires hundreds of LLM calls per instance, making it computationally intensive but providing principled uncertainty estimates without requiring model gradients or explicit likelihood functions.

## Key Results
- Up to 46.9% improvement in accuracy compared to standard baselines
- 74.4% reduction in Brier scores indicating better calibrated predictions
- Enhanced calibration with lower Expected Calibration Error across multiple LLM models
- Robustness to temperature variation and weight quantization
- SMC-ABC shows superior adaptation in stochastic settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Re-framing classification as inverse inference improves calibration by evaluating which class hypothesis best explains observed text.
- Mechanism: Rather than directly predicting P(y|x), the method samples candidate labels y_sim, generates text conditioned on each label x_sim = LLM(y_sim), and compares generated vs. observed text via semantic embeddings. The acceptance frequency of each label approximates the posterior P(y|x_obs).
- Core assumption: The classification task is reversible—the LLM can generate plausible descriptions given a class label (Section 3.1: "we only assumes that the problem of text classification is reversible").
- Evidence anchors:
  - [abstract] "treats classification as a simulation-based inference problem"
  - [Section 3.1] "instead of modeling classification as direct label prediction, we treat the class label y not as the output to be predicted, but as a latent hypothesis"
  - [corpus] Weak direct support; related work on likelihood-free Bayesian inference (arXiv:2505.04603) discusses ABC efficiency but not LLM-specific inversion.
- Break condition: Tasks where class labels cannot conditionally generate coherent descriptions (e.g., arbitrary label encodings without semantic grounding) will fail.

### Mechanism 2
- Claim: Bypassing explicit likelihood computation via distance-based acceptance enables Bayesian inference for black-box LLMs without gradient access.
- Mechanism: ABC replaces intractable likelihood evaluation with simulation: if distance(embed(x_sim), embed(x_obs)) < ε, the candidate label is accepted. This produces a valid posterior approximation without model internals.
- Core assumption: The embedding space captures task-relevant semantics; cosine distance meaningfully reflects whether x_sim "could have produced" x_obs.
- Evidence anchors:
  - [abstract] "likelihood-free Bayesian inference... using ABC to estimate posterior distributions"
  - [Section 3] "ABC bypasses direct likelihood calculation by simulating data from the model and comparing it to observed data"
  - [corpus] arXiv:2505.04603 confirms ABC as standard likelihood-free approach but notes computational inefficiency in high dimensions.
- Break condition: Embedding models that fail to capture domain distinctions (e.g., clinical subtleties) yield uninformative posteriors. Section A.2 shows BioClinicalBERT underperformed OpenAI embeddings, supporting this dependency.

### Mechanism 3
- Claim: Sequential Monte Carlo refinement concentrates computation on high-probability regions, improving efficiency over naive rejection.
- Mechanism: SMC-ABC begins with loose threshold ε_1, accepts initial particles, then iteratively tightens thresholds (ε_1 > ε_2 > ... > ε_T) while resampling and perturbing particles from previous iterations, progressively sharpening the posterior.
- Core assumption: The posterior is approximately unimodal or has tractable structure; discrete jump perturbations can explore the label space.
- Evidence anchors:
  - [abstract] "SMC-ABC showing superior adaptation in stochastic settings"
  - [Section 4.2] "SMC-ABC adaptively concentrates sampling in high-likelihood regions"
  - [Table 3] SMC-ABC achieves lower ECE (1.0pp) at T=1.0 compared to rejection sampling (1.2pp).
  - [corpus] No direct corpus validation of SMC-ABC for discrete label spaces; existing work focuses on continuous parameters.
- Break condition: Multi-modal posteriors with distant modes may collapse to one mode if initial population misses alternatives.

## Foundational Learning

- Concept: Bayesian posterior inference (prior, likelihood, posterior)
  - Why needed here: ABC produces posterior distributions over class labels; understanding what a posterior represents is essential for interpreting results.
  - Quick check question: If the posterior is [0.6, 0.3, 0.1] for three classes, what does this mean vs. a point prediction?

- Concept: Approximate Bayesian Computation (rejection sampling, summary statistics, distance metrics)
  - Why needed here: The entire method relies on ABC's core idea—replacing likelihood evaluation with simulation and distance comparison.
  - Quick check question: Why does ABC accept samples with distance < ε rather than computing P(data|θ) directly?

- Concept: Semantic embeddings and cosine similarity
  - Why needed here: The distance metric in this ABC implementation is cosine distance over sentence embeddings; choice of embedding model directly affects posterior quality.
  - Quick check question: What happens if two clinically distinct conditions have similar embedding representations?

## Architecture Onboarding

- Component map: Observed text -> Embed -> For each SMC iteration: Sample labels -> Generate descriptions -> Embed -> Compute distance -> Accept/reject -> Reweight particles -> Final posterior

- Critical path: Observed text → Embed → For each SMC iteration: Sample labels → Generate descriptions → Embed → Compute distance → Accept/reject → Reweight particles → Final posterior

- Design tradeoffs:
  - **Rejection vs. SMC**: Rejection is simpler and achieved comparable results (Table 1), but SMC adapts better at higher temperatures (Table 3).
  - **Embedding model**: Domain-specific models (BioClinicalBERT) underperformed general-purpose OpenAI embeddings in ablations (Table 5)—counterintuitive, likely due to dimensionality and training quality.
  - **Computational cost**: ~500+ LLM calls per instance (Section 7); vector database pre-computation mitigates this but requires upfront investment.

- Failure signatures:
  - High entropy posteriors on in-distribution data suggest embedding misalignment (classes not separated in embedding space).
  - Low entropy on OOD data suggests embeddings failing to capture that input is irrelevant to all classes (Section 6.5 notes this limitation).
  - Classes differing only by rare symptoms may produce uninformative posteriors (Section 7 discussion).

- First 3 experiments:
  1. Validate embedding quality: Compute pairwise cosine distances between ground-truth descriptions for each class. If inter-class distances are not substantially larger than intra-class, embedding model is inadequate.
  2. Baseline rejection ABC with uniform prior: Run 1000 simulations per class, accept at fixed ε=0.3, compare posterior entropy vs. random guessing on held-out validation.
  3. Temperature robustness check: Repeat ABC at T∈{0.2, 0.6, 1.0}; posterior should stabilize (Table 3 shows robustness). If not, investigate embedding stability or LLM generation consistency.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the ABC framework be adapted to reliably distinguish between classes that share a majority of features but differ by rare, critical symptoms?
- Basis in paper: [explicit] The authors state the method struggles when classes differ by a "single/few, rare, yet critical symptom," potentially resulting in uninformative posteriors.
- Why unresolved: While the authors suggest increased sampling diversity or prompt re-framing as mitigation strategies, they acknowledge this as an "inherent limitation" without providing empirical validation of these fixes.
- What evidence would resolve it: Successful application of the framework on a dataset specifically designed with high feature overlap and low-frequency differentiating symptoms, showing significant posterior separation.

### Open Question 2
- Question: How can the computational latency of ABC inference be minimized to enable real-time application without sacrificing calibration?
- Basis in paper: [explicit] The discussion highlights that SMC-ABC requires hundreds of LLM queries per instance, creating a "computational burden" and high latency compared to single-query baselines.
- Why unresolved: The paper proposes rejection sampling or vector databases as speed-ups, but the fundamental trade-off between the number of simulations (required for approximation quality) and inference speed remains a barrier to real-time use.
- What evidence would resolve it: A modified ABC algorithm that achieves comparable Brier scores and ECE with a significantly reduced simulation budget (e.g., <50 queries per instance) or rigorous latency benchmarks on streaming data.

### Open Question 3
- Question: Is the reliance on cosine similarity over general-purpose sentence embeddings sufficient for capturing semantic irrelevance in Out-Of-Distribution (OOD) clinical cases?
- Basis in paper: [inferred] The paper notes that general embeddings may fail to capture clinically relevant distinctions, leading to biased similarity scores and reduced entropy in OOD settings, necessitating a domain-adaptive layer.
- Why unresolved: The requirement for an auxiliary domain-adaptive layer suggests the base framework is sensitive to the geometry of the embedding space, potentially failing to flag semantic mismatches in specialized domains without fine-tuning.
- What evidence would resolve it: A comparative analysis of OOD detection performance using various off-the-shelf embedding models versus domain-adapted models, specifically measuring the rate of "false confidence" in semantically distinct OOD samples.

## Limitations
- Computational Overhead: ABC requires 500+ LLM calls per instance, creating significant latency and cost barriers for clinical deployment.
- Embedding Space Dependency: Method performance critically depends on embedding model quality, with domain-specific models underperforming general-purpose ones in unexpected results.
- OOD Detection Gaps: The method struggles to identify inputs irrelevant to all classes, with entropy remaining higher than expected for truly out-of-distribution samples.

## Confidence
- High Confidence: Calibration improvements (ECE reduction), temperature robustness, and SMC-ABC efficiency gains are well-supported by ablation studies and multiple datasets.
- Medium Confidence: Claims about clinical applicability require external validation beyond synthetic data. The surprising embedding model results (general-purpose > clinical-specific) warrant replication.
- Low Confidence: Claims about handling rare symptom distinctions and multi-modal posteriors lack empirical support and remain theoretical concerns.

## Next Checks
1. **Embedding Quality Validation**: Compute pairwise cosine distances between ground-truth descriptions for each class. If inter-class distances are not substantially larger than intra-class, the embedding model is inadequate regardless of downstream ABC performance.

2. **Cross-Dataset Generalization**: Apply the exact methodology to a held-out clinical dataset (not synthetic) to verify calibration improvements transfer beyond controlled environments.

3. **Rare Feature Sensitivity**: Construct minimal pairs where classes differ only by rare symptoms. Run ABC and verify whether posteriors distinguish these cases or collapse to uninformative distributions.