---
ver: rpa2
title: 'The UD-NewsCrawl Treebank: Reflections and Challenges from a Large-scale Tagalog
  Syntactic Annotation Project'
arxiv_id: '2505.20428'
source_url: https://arxiv.org/abs/2505.20428
tags:
- tagalog
- language
- annotation
- crawl
- linguistics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces UD-NewsCrawl, the largest Tagalog dependency
  treebank to date, containing 15,619 manually annotated sentences. The treebank was
  developed through a rigorous annotation process involving native speakers with linguistic
  expertise, and includes comprehensive quality control measures.
---

# The UD-NewsCrawl Treebank: Reflections and Challenges from a Large-scale Tagalog Syntactic Annotation Project

## Quick Facts
- arXiv ID: 2505.20428
- Source URL: https://arxiv.org/abs/2505.20428
- Reference count: 39
- Primary result: Largest Tagalog dependency treebank with 15,619 annotated sentences and baseline transformer models

## Executive Summary
This paper presents UD-NewsCrawl, a significant advancement in Tagalog computational linguistics through the creation of the largest dependency treebank for the language to date. The project addresses the critical shortage of annotated linguistic resources for Tagalog by providing 15,619 manually annotated sentences with comprehensive quality control measures. The work includes baseline dependency parsing models using transformer-based approaches, with XLM-RoBERTa demonstrating the best performance. The authors also provide valuable insights into the challenges of annotating Tagalog's unique grammatical features, including its voice marking system and flexible word categories, which will benefit future annotation efforts for morphologically rich languages.

## Method Summary
The UD-NewsCrawl treebank was developed through a rigorous annotation process involving native Tagalog speakers with linguistic expertise. The methodology employed multiple quality control measures including inter-annotator agreement checks, validation protocols, and systematic error correction procedures. The annotation team followed Universal Dependencies guidelines while adapting them to Tagalog's specific linguistic characteristics. The corpus was constructed from news crawl data, ensuring diverse linguistic content. Baseline models were trained using various transformer architectures, with hyperparameter tuning and evaluation on standard metrics to establish performance benchmarks for Tagalog dependency parsing.

## Key Results
- UD-NewsCrawl contains 15,619 manually annotated Tagalog sentences, representing the largest dependency treebank for the language
- XLM-RoBERTa achieved the best performance among baseline transformer models for Tagalog dependency parsing
- The project provides valuable insights into annotating Tagalog's voice system and flexible word categories, informing future linguistic resource development

## Why This Works (Mechanism)
The success of UD-NewsCrawl stems from combining native speaker linguistic expertise with systematic annotation protocols and robust quality control. By involving annotators with both language proficiency and linguistic training, the project ensured accurate representation of Tagalog's complex grammatical features. The use of transformer-based models leverages their strong cross-lingual transfer capabilities, while the news crawl corpus provides diverse, realistic language samples that improve model generalization. The comprehensive documentation of annotation challenges creates a foundation for addressing similar issues in other morphologically rich, low-resource languages.

## Foundational Learning

1. **Dependency Parsing**: Understanding syntactic relationships between words in a sentence through labeled directed graphs. Why needed: Essential for extracting grammatical structure from text for downstream NLP tasks. Quick check: Can identify subject-verb-object relationships in simple sentences.

2. **Universal Dependencies Framework**: A cross-linguistically consistent annotation scheme for grammatical analysis. Why needed: Provides standardized guidelines enabling comparison across languages and transfer learning. Quick check: Can map dependency relations to Universal Dependencies labels.

3. **Tagalog Voice System**: Grammatical system where verb morphology indicates semantic roles rather than syntactic subjecthood. Why needed: Critical for understanding Tagalog's unique argument structure and annotating it correctly. Quick check: Can identify and label different voice constructions in Tagalog sentences.

## Architecture Onboarding

Component Map: Raw News Data -> Preprocessing Pipeline -> Annotation Interface -> Quality Control -> Treebank -> Transformer Models -> Evaluation

Critical Path: The annotation process followed by quality control represents the critical path, as the treebank quality directly determines model performance. Any errors or inconsistencies in annotation propagate through to the trained models.

Design Tradeoffs: The project prioritized annotation quality over quantity, resulting in a smaller but more reliable corpus compared to rapid annotation approaches. This tradeoff ensures better model performance but limits the scale advantages of larger datasets.

Failure Signatures: Common annotation failures include misidentifying voice constructions, incorrect attachment of modifiers, and inconsistent treatment of flexible word categories. These manifest as systematic errors in dependency relations that affect model training.

First Experiments:
1. Train baseline model on a small subset (1,000 sentences) to establish minimum viable performance
2. Evaluate inter-annotator agreement on a validation set to quantify annotation consistency
3. Test model performance on out-of-domain Tagalog text to assess generalization beyond news domain

## Open Questions the Paper Calls Out
None

## Limitations
- The corpus size of 15,619 sentences remains modest compared to resource-rich languages, potentially limiting model generalization
- Annotation challenges with Tagalog's flexible word categories and voice system may lead to inconsistencies despite quality control measures
- Baseline models were trained on relatively small datasets for modern transformer approaches, which may not reflect the true performance ceiling

## Confidence
- Primary claims about corpus size and baseline performance: Medium
- Claims about annotation challenges and their solutions: Medium
- Claims about advancing computational linguistics for underrepresented languages: Medium

## Next Checks
1. Conduct inter-annotator agreement studies on a subset of the corpus to quantify annotation consistency, particularly for challenging constructions
2. Evaluate model performance on held-out test sets and potentially on out-of-domain Tagalog text to assess generalization
3. Compare the annotation scheme and guidelines with established linguistic descriptions of Tagalog to ensure alignment and identify potential areas of improvement or ambiguity