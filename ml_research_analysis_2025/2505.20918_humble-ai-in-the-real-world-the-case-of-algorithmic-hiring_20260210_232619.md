---
ver: rpa2
title: 'Humble AI in the real-world: the case of algorithmic hiring'
arxiv_id: '2505.20918'
source_url: https://arxiv.org/abs/2505.20918
tags:
- candidates
- hiring
- rank
- humble
- candidate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a practical application of Humble AI principles
  to algorithmic hiring by quantifying uncertainty in candidate rankings. The method
  generates rank sets through Monte Carlo sampling of perturbed AI scores, producing
  expected ranks, entropy estimates, and confidence measures.
---

# Humble AI in the real-world: the case of algorithmic hiring

## Quick Facts
- arXiv ID: 2505.20918
- Source URL: https://arxiv.org/abs/2505.20918
- Authors: Rahul Nair; Inge Vejsbjerg; Elizabeth Daly; Christos Varytimidis; Bran Knowles
- Reference count: 10
- Primary result: Monte Carlo perturbation sampling improves rank stability metrics (RBO) by up to 0.23 compared to deterministic rankings

## Executive Summary
This paper operationalizes Humble AI principles in algorithmic hiring by introducing a black-box method to quantify uncertainty in candidate rankings. The approach uses Monte Carlo sampling with feature perturbations to generate rank sets, producing expected ranks, entropy estimates, and confidence measures. Experiments show that accounting for rank uncertainty improves ranking stability metrics, particularly under noisy conditions. Focus group feedback from recruiters highlights both the potential value and challenges of implementing such systems in practice.

## Method Summary
The method applies Monte Carlo sampling to generate rank sets through perturbations of candidate feature vectors. Random feature masking creates perturbed versions of each candidate profile, which are then scored by the existing black-box AI system. This produces score distributions that are aggregated into rank probability matrices. From these, the system computes expected ranks, entropy as a measure of uncertainty, and variance. The approach operates without requiring access to model internals, making it applicable to existing hiring AI systems.

## Key Results
- Accounting for rank uncertainty improved RBO metrics by up to 0.23 compared to point estimates, particularly under noisy conditions
- Analysis of real job postings showed low overlap (Jaccard 0.01-0.31) between deterministic and probabilistic rankings
- Focus group feedback highlighted potential confusion in separating high-rank and high-entropy candidates and stressed the need for training and careful UX design

## Why This Works (Mechanism)

### Mechanism 1: Perturbation-Based Uncertainty Quantification
- Claim: Local perturbations of candidate feature vectors reveal the stability of AI scoring under input variation
- Mechanism: Random feature masking generates perturbed versions of each candidate profile. The black-box scoring function is evaluated on each perturbation, producing an empirical score distribution per candidate rather than a single point estimate
- Core assumption: The scoring function's sensitivity to feature perturbations approximates its epistemic uncertainty about candidate quality
- Evidence anchors: [abstract] "generates rank sets through Monte Carlo sampling of perturbed AI scores"; [section 2] "perturbations are done by random feature masking... similar to explainability method LIME"
- Break condition: If scoring function is highly robust to feature masking (low variance across perturbations), uncertainty estimates will be near-zero regardless of true model ignorance

### Mechanism 2: Expected Rank Aggregation Over Rank Probabilities
- Claim: Aggregating across Monte Carlo samples to compute expected ranks produces more stable rankings under noisy scoring conditions
- Mechanism: For each Monte Carlo draw, candidates are ranked by perturbed scores. Aggregating across draws yields probability matrix P where p_ij = Pr(candidate i has rank j). Expected rank = Σ_j p_ij × j
- Core assumption: Noise in scoring is approximately symmetric and averaging reduces its effect on rank ordering
- Evidence anchors: [abstract] "accounting for rank uncertainty improved RBO metrics by up to 0.23 compared to point estimates, particularly under noisy conditions"; [section 2.2, Figure 4] "deviations from true rankings are considerably reduced" when using expected rank vs. point estimates
- Break condition: If noise is systematic (e.g., biased against certain candidate profiles), averaging will not recover true rankings

### Mechanism 3: Entropy as Candidate-Level Uncertainty Signal
- Claim: Rank entropy quantifies how diffuse a candidate's possible ranks are, flagging candidates where the model is unreliable
- Mechanism: Entropy H_i = -Σ_j p_ij × log(p_ij). High entropy indicates candidate could appear at many different ranks across perturbations; low entropy indicates stable rank assignment
- Core assumption: High entropy reflects genuine model uncertainty rather than artifacts of the perturbation scheme
- Evidence anchors: [abstract] "entropy estimates, and confidence measures" are key outputs; [section 2] "Candidates with high rank entropy may need additional manual reviews"; [section 4, focus group] "participants pointed out...uncertainty which we would like to attribute to the AI model can be attributed to the candidate"
- Break condition: Users misattribute entropy to candidate quality rather than model uncertainty (confirmed by focus group feedback)

## Foundational Learning

- Concept: Monte Carlo sampling for uncertainty estimation
  - Why needed here: Core technique for generating rank sets without requiring model internals (black-box setting)
  - Quick check question: Can you explain why perturbation-based uncertainty differs from Bayesian uncertainty quantification that requires model access?

- Concept: Information entropy (Shannon entropy)
  - Why needed here: Quantifies the spread of rank probability distributions; higher entropy = more uncertain ranking
  - Quick check question: Given a candidate with rank probabilities [0.5, 0.3, 0.2] for ranks 1, 2, 3, would their entropy be higher or lower than a candidate with [0.9, 0.05, 0.05]?

- Concept: Rank-Biased Overlap (RBO) and Jaccard similarity
  - Why needed here: Metrics for comparing rankings; RBO weights top positions more heavily (appropriate for hiring where top-k matters)
  - Quick check question: Why is RBO more appropriate than Kendall's tau for evaluating hiring rankings where only top-50 candidates are reviewed?

## Architecture Onboarding

- Component map: Candidate profiles (X) + Job specification (y) -> Feature perturbation module (random masking, k samples per candidate) -> Black-box scoring function f_y (existing AI system, inference-only) -> Score distributions {z̃_1, ..., z̃_n} -> Monte Carlo ranking (repeated sorting -> rank samples) -> Rank probability matrix P -> Expected rank, entropy, variance -> UX layer (rank table toggle, high-entropy candidate surfacing)

- Critical path: Perturbation quality -> score distribution fidelity -> rank probability accuracy -> UX interpretability. Errors propagate; poor perturbations yield uninformative entropy

- Design tradeoffs:
  - Perturbation intensity: More aggressive masking increases uncertainty signal but may produce unrealistic candidate profiles
  - UX complexity: Separating high-rank vs. high-entropy candidates (current design) vs. unified list (focus group suggestion). Trade-off between cognitive load and clarity
  - Computational cost: k Monte Carlo samples per candidate; scales O(k × n) for n candidates. Paper assumes 1000 candidates, but sampling strategy not specified

- Failure signatures:
  - Near-zero entropy for all candidates (perturbations too weak or scoring function too robust)
  - High entropy for all candidates (perturbations too aggressive, destroying meaningful signal)
  - Low Jaccard similarity but high RBO (different candidates surfaced but similar ordering quality)
  - User confusion attributing entropy to candidate quality rather than model uncertainty

- First 3 experiments:
  1. Baseline perturbation calibration: Vary masking probability (e.g., 10%, 30%, 50% of features masked) on synthetic data with known ground truth. Plot entropy distribution vs. noise level; identify settings where entropy correlates with true rank error
  2. Ranking stability comparison: For 5-10 real job postings, compare deterministic top-50 vs. expected-rank top-50. Measure Jaccard overlap and compute: are surfaced candidates materially different? Document candidate profiles that shift most dramatically
  3. User comprehension probe: Present the humble AI interface to 5-10 recruiters with a think-aloud protocol. Specifically test: do they interpret high entropy as "risky candidate" or "model uncertainty"? Identify minimum explanation needed to correct misattribution

## Open Questions the Paper Calls Out

- Does the higher cognitive load of a humble AI system foster a climate of trust in its outcomes among recruiters?
- How can uncertainty attributed to the AI model be effectively distinguished from uncertainty attributed to the candidate in the user interface?
- Can uncertainty-aware rankings be validated against real hiring outcomes, and what constitutes ground truth in screening decisions?
- Does operationalizing Humble AI through rank uncertainty meaningfully reduce harms from misrecognition of non-traditional candidates?

## Limitations

- The perturbation-based uncertainty quantification lacks corpus validation for hiring contexts
- Focus group feedback indicates users may confuse model uncertainty with candidate quality
- Real-world validation is limited to six job postings without outcome tracking
- The computational cost and scalability for large applicant pools is not fully addressed

## Confidence

- High: Monte Carlo sampling as a black-box uncertainty technique; entropy as a mathematical measure of rank distribution spread
- Medium: Expected rank improving RBO metrics in synthetic settings; low Jaccard overlap between deterministic and probabilistic rankings in real data
- Low-Medium: Perturbation-based uncertainty quantification in hiring; entropy interpretation by end users; computational feasibility at scale

## Next Checks

1. **Perturbation sensitivity analysis**: Systematically vary perturbation intensity (feature masking rates) across 10-20 synthetic datasets with known ground truth. Measure correlation between entropy and actual rank error; identify optimal perturbation parameters that maximize uncertainty signal without destroying candidate profile validity.

2. **Real-world outcome validation**: Deploy the humble AI system on 50+ actual hiring processes, tracking: (a) hiring decisions using expected ranks vs. point estimates, (b) candidate performance post-hire, and (c) recruiter satisfaction. Compare quality metrics (retention, performance ratings) against deterministic baseline to establish whether uncertainty-aware rankings produce better hiring outcomes.

3. **UX comprehension study**: Conduct controlled experiments with 20+ recruiters using think-aloud protocols while interacting with the humble AI interface. Specifically test: (a) interpretation of high-entropy candidates (model uncertainty vs. candidate risk), (b) decision-making with expected rank vs. point estimate toggles, and (c) cognitive load measurement. Design and validate improved explanations that clarify the distinction between candidate quality and model uncertainty.