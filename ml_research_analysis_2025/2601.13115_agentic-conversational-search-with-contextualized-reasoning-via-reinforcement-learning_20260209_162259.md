---
ver: rpa2
title: Agentic Conversational Search with Contextualized Reasoning via Reinforcement
  Learning
arxiv_id: '2601.13115'
source_url: https://arxiv.org/abs/2601.13115
tags:
- search
- conversational
- answer
- arxiv
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a conversational agent that interleaves search
  and reasoning across turns using reinforcement learning with tailored rewards. The
  agent optimizes history-conditional query generation, leverages retrieved information
  for accurate answers, and supports mixed-initiative actions like clarification and
  refusal.
---

# Agentic Conversational Search with Contextualized Reasoning via Reinforcement Learning

## Quick Facts
- arXiv ID: 2601.13115
- Source URL: https://arxiv.org/abs/2601.13115
- Reference count: 12
- The method achieves up to 40.3 F1 and 30.8 NDCG@3 on conversational search benchmarks.

## Executive Summary
This paper introduces a conversational search agent that uses reinforcement learning to optimize history-conditional query generation, retrieval quality, and mixed-initiative actions like clarification and refusal. The agent interleaves search and reasoning across conversation turns, leveraging retrieved information to generate accurate answers. Experiments on four benchmarks demonstrate significant improvements over strong baselines, particularly in handling evolving user intent and contextual reasoning.

## Method Summary
The approach uses a Qwen-2.5 LLM backbone with an E5 retriever to perform multi-turn conversational search. The model generates search queries conditioned on conversation history, retrieves top passages, and synthesizes answers. A three-component reward function (outcome generation, information gain, mixed-initiative actions) guides reinforcement learning via GRPO. The system supports special action tokens for clarification, refusal, and direct answering, with reward penalties for inappropriate action choices.

## Key Results
- Answer generation F1 scores reach 40.3 on TopiOCQA and 22.1 on INSCIT
- Retrieval performance achieves NDCG@3 up to 30.8 on QReCC
- Mixed-initiative clarification accuracy improves from 3.67% to 83.19% on INSCIT
- GRPO outperforms PPO baselines across all metrics

## Why This Works (Mechanism)

### Mechanism 1: Information Gain Reward Provides Dense Supervision for Query Decontextualization
The information gain reward creates intermediate feedback by measuring overlap between retrieved passages and ground-truth answers. This provides signal even when final answers are incorrect, guiding the model to generate contextually-aware search queries without explicit query rewriting supervision.

### Mechanism 2: Mixed-Initiative Action Reward Aligns System Behavior with User Intent States
Discrete rewards for clarification/refusal actions (+1 for correct, -0.5 for incorrect) teach the model to distinguish ambiguous, unanswerable, and straightforward queries. This prevents both unnecessary clarifications and risky hallucinated answers.

### Mechanism 3: GRPO Eliminates Value Model Dependency Through Group-Based Advantage Estimation
By sampling multiple trajectories per query and computing group-relative advantages, GRPO achieves stable policy updates without a separate value function. This reduces memory overhead while maintaining performance through variance-normalized reward aggregation.

## Foundational Learning

- **Concept: Query Decontextualization in Conversational Search**
  - Why needed here: Converts context-dependent utterances into standalone queries by resolving coreferences using conversation history
  - Quick check question: Given history ["Who directed Inception?", "Christopher Nolan"] and query "What else did he direct?" can you write a standalone search query?

- **Concept: Reinforcement Learning with Verifiable Rewards (RLVR)**
  - Why needed here: Uses automatically computable rewards (F1, EM) instead of human labels for scalable RL training
  - Quick check question: Why might F1 score be preferable to human rating as an RL reward signal for large-scale training?

- **Concept: Mixed-Initiative Interaction**
  - Why needed here: Enables agents to ask clarifying questions, refuse answers, or proceed based on query ambiguity
  - Quick check question: What are two scenarios where asking a clarification question would be more harmful than providing a best-guess answer?

## Architecture Onboarding

- **Component map:** User Query + History → Policy LLM → Reasoning → Branch: (a) Search query → E5 Retriever → Passages, (b) Clarification question, (c) Refusal token, (d) Direct answer → Answer synthesis → Reward computation → GRPO policy update

- **Critical path:** Prompt engineering for action tokens, information gain reward computation, GRPO hyperparameters (ε clipping, γ KL penalty, group size K)

- **Design tradeoffs:** Separate action tokens yield 83.19% clarification accuracy vs 78.35% combined but increase sequence length; GRPO removes value model overhead but requires K× more rollouts; 0.5× reward weighting balances exploration vs outcome focus

- **Failure signatures:** Degenerate clarification (empty/generic questions), search-query collapse (generic queries), reasoning length explosion (token growth without performance gain)

- **First 3 experiments:** 1) Reward ablation on TopiOCQA measuring NDCG@3 and F1, 2) Backbone scaling test (3B vs 7B) on all four benchmarks, 3) Clarification effectiveness analysis comparing model-generated vs ground-truth clarifications on INSCIT

## Open Questions the Paper Calls Out
- Can more sophisticated intermediate reward functions be designed to better facilitate the utilization of retrieved information for higher quality generation?
- What computational optimizations are required to mitigate the efficiency overhead of RL training and inference in agentic conversational search?
- How does the proposed reward decomposition strategy generalize across a wider variety of backbone LLMs and hyperparameter configurations?

## Limitations
- Evaluation relies heavily on synthetic benchmarks with limited real-world complexity
- RL training assumes ground-truth answers are available during each turn
- The model's reasoning process is opaque with no analysis of how reasoning tokens influence query reformulation

## Confidence

- **High confidence**: Information gain reward improves retrieval quality (NDCG@3 up to 30.8) - supported by consistent experimental improvements
- **Medium confidence**: Mixed-initiative reward meaningfully improves clarification accuracy (3.67% to 83.19%) - depends on consistent ground-truth action labels
- **Medium confidence**: GRPO eliminates value model dependency without sacrificing performance - ablation shows PPO underperforms but alternatives not explored

## Next Checks

1. **Out-of-distribution robustness test**: Evaluate on human-human conversational search logs to measure performance degradation compared to benchmarks

2. **Ablation on reward component independence**: Train separate models with only R_outcome, only R_IG, and only R_MIA on TopiOCQA, compute pairwise correlations to quantify orthogonality

3. **Human evaluation of clarification quality**: User study rating automatically generated clarifications versus ground-truth clarifications on ambiguous queries to validate conversational usefulness