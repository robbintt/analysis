---
ver: rpa2
title: LLM Unlearning Reveals a Stronger-Than-Expected Coreset Effect in Current Benchmarks
arxiv_id: '2504.10185'
source_url: https://arxiv.org/abs/2504.10185
tags:
- unlearning
- coreset
- forget
- selection
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper uncovers a surprising coreset effect in LLM unlearning:
  benchmarks like WMDP and MUSE can be effectively unlearned using as little as 5%
  of the original forget set, even with random selection. This effect holds across
  multiple unlearning methods (NPO and RMU), benchmarks (WMDP-Bio, WMDP-Cyber, MUSE-Books,
  MUSE-News), and coreset selection strategies (random, GRAND, MODERATE, MIN-K% PROB).'
---

# LLM Unlearning Reveals a Stronger-Than-Expected Coreset Effect in Current Benchmarks

## Quick Facts
- **arXiv ID**: 2504.10185
- **Source URL**: https://arxiv.org/abs/2504.10185
- **Reference count**: 40
- **Primary result**: 5% coreset unlearning achieves comparable unlearning effectiveness (UE) and utility (UT) to full forget set unlearning across multiple benchmarks and methods.

## Executive Summary
This paper demonstrates that large language model unlearning can be effectively achieved using surprisingly small subsets (coresets) of the forget set—as little as 5%—without significant degradation in unlearning effectiveness or utility. The finding holds across multiple benchmarks (WMDP-Bio, WMDP-Cyber, MUSE-Books, MUSE-News), unlearning methods (NPO, RMU), and coreset selection strategies. Keyword analysis reveals that high-impact tokens within small coresets drive most of the unlearning performance, suggesting current benchmarks contain redundant information. This challenges the assumption that unlearning requires processing entire forget datasets and has implications for the design of future unlearning benchmarks.

## Method Summary
The paper evaluates LLM unlearning effectiveness when using coreset subsets of the forget set. It tests two unlearning methods: NPO (Negative Preference Optimization) and RMU (Representation Misdirection Unlearning) on four benchmarks. Coresets are selected using four strategies: random sampling, GRAND, MODERATE, and MIN-K% PROB. The key innovation is scaling training epochs inversely with coreset size—smaller coresets require more epochs to achieve comparable performance. For example, 5% coreset uses 20 epochs vs. 1 epoch for full set (RMU). The study measures Unlearning Effectiveness (UE) as 100% minus benchmark accuracy, and Utility (UT) via general task performance.

## Key Results
- 5% random coreset unlearning achieves comparable UE (up to 72.03±1.78) and UT (MMLU ~56.69) to full forget set unlearning (UE ~69.46, UT ~57.48)
- High-impact token analysis shows keyword-only coreset unlearning captures majority of the unlearning effect
- Coreset-unlearned models show similar mode connectivity, jailbreaking robustness, and utility on auxiliary tasks as full-set models
- Coreset models exhibit slightly higher vulnerability to relearning attacks but maintain overall robustness

## Why This Works (Mechanism)

### Mechanism 1: Compact High-Impact Token Sets
Unlearning effectiveness is primarily driven by a small subset of "high-impact" tokens within the forget set, rather than the entire dataset. The coreset effect emerges because keywords (domain-specific terminology, entities) concentrate information. Random sampling at sufficient scale (5%+) probabilistically captures enough of these key tokens to achieve unlearning comparable to the full set.

### Mechanism 2: Correlation-Driven Redundancy in Forget Sets
Current benchmark forget sets contain correlated data points, creating redundancy that makes unlearning "easier" than expected. When forget samples share overlapping information (e.g., repeated biological concepts across multiple documents), removing influence from a subset propagates to related content. This allows small coresets to generalize across the full forget set.

### Mechanism 3: Training Compute Compensation
Smaller forget sets provide fewer gradient signals per epoch. Compensating requires more epochs to traverse the loss landscape sufficiently. The coreset effect emerges "when conditioned on sufficient unlearning training." For instance, using only 1% of the forget set requires over 70 training epochs to achieve comparable UE.

## Foundational Learning

- **Coreset Selection**: Understanding that a small subset can approximate full-set performance is the paper's central claim. You must grasp coreset definitions from classical ML to interpret results.
  - Quick check: Given a dataset with 1,000 samples, what does a 5% coreset represent, and why might it preserve task performance?

- **LLM Unlearning Objectives (NPO, RMU)**: The paper tests two distinct unlearning methods. Understanding their loss formulations explains why both exhibit the coreset effect.
  - Quick check: How does NPO's "negative preference" differ from RMU's "representation misdirection" in their treatment of forget data?

- **Mode Connectivity**: Linear mode connectivity validates that coreset-unlearned models occupy the same loss basin as full-set models, a key faithfulness argument.
  - Quick check: If two models exhibit linear mode connectivity, what does interpolating between their weights predict about intermediate model performance?

## Architecture Onboarding

- **Component map**:
  - Forget Set (D_f) -> Coreset Selection Module (RANDOM, GRAND, MODERATE, MIN-K% PROB) -> Unlearning Optimizer (NPO/RMU) -> Evaluation Suite (UE, UT, robustness tests)

- **Critical path**:
  1. Select coreset ratio (1-10%) from full forget set using chosen strategy
  2. Train unlearning objective (NPO/RMU) for epochs calibrated to coreset size (more epochs for smaller coresets)
  3. Evaluate UE on benchmark test set (e.g., WMDP QA accuracy reduction)
  4. Evaluate UT on general tasks (MMLU, TruthfulQA)
  5. Optionally test robustness (GCG attacks, fine-tuning relearning)

- **Design tradeoffs**:
  - Coreset size vs. epochs: Smaller coresets require more training epochs (1% = 70+ epochs vs. 1 epoch for 100%)
  - Random vs. heuristic selection: RANDOM is competitive but high-variance; heuristics (GRAND, MIN-K% PROB) offer marginal gains at cost of complexity
  - Unlearning method choice: RMU excels in low-data regimes (1% coreset viable); NPO requires ~5%+ for comparable performance

- **Failure signatures**:
  - Insufficient training epochs: Coreset models underperform full-set UE (e.g., 1% coreset at default 1 epoch fails)
  - Relearning vulnerability: Coreset-unlearned models may resurface forgotten knowledge faster under fine-tuning (Fig. 5)
  - Utility collapse: Overtraining or too-small coresets may degrade UT (though paper shows relative preservation)

- **First 3 experiments**:
  1. Replicate random coreset effect: Run RMU on WMDP-Bio with 5% random coreset (20 epochs) vs. full set (1 epoch); compare UE and UT.
  2. Keyword ablation: Extract keywords from 5% coreset; train unlearning on keyword-only set; measure UE gap vs. full coreset.
  3. Relearning attack test: Fine-tune coreset-unlearned (5%) and full-set-unlearned models on GSM8k (600 samples); compare UE degradation curves.

## Open Questions the Paper Calls Out

### Open Question 1
Can coreset selection strategies be designed to consistently outperform random selection in LLM unlearning?
The study found that random selection is surprisingly competitive with sophisticated methods (e.g., GRADN, MODERATE), often performing within the variance of the random baselines, leaving the potential ceiling for performance improvement unknown.

### Open Question 2
What are the underlying mechanistic reasons for the sufficiency of small coresets, specifically regarding high-impact tokens?
While keyword analysis suggests that compact token sets drive unlearning, the specific internal circuit-level or representational changes within the LLM caused by these tokens remain unmapped.

### Open Question 3
How can the increased vulnerability of coreset-unlearned models to relearning attacks be mitigated?
The reduced data exposure during the unlearning process appears to weaken the model's resistance to weight-based attacks, creating a trade-off between data efficiency and robustness that current methods do not address.

## Limitations

- The central claim about the coreset effect rests on inferred redundancy rather than direct forget set analysis
- The mechanism linking high-impact tokens to unlearning effectiveness lacks theoretical grounding
- Relearning vulnerability findings show only marginal differences between coreset and full-set models
- Keyword importance analysis could reflect benchmark-specific artifacts rather than general principles

## Confidence

- **High confidence**: The empirical observation that 5% coreset unlearning achieves comparable UE and UT to full-set unlearning across multiple benchmarks, methods, and selection strategies
- **Medium confidence**: The hypothesis that correlation-driven redundancy in forget sets drives the coreset effect (circumstantial evidence but not directly proven)
- **Medium confidence**: The claim that high-impact token sets drive unlearning (keyword ablation experiments show directional support but methodology could conflate correlation with causation)

## Next Checks

1. **Forget Set Redundancy Analysis**: Quantify actual correlation structures in current forget sets (e.g., document similarity, concept overlap) to test whether observed coreset effects correlate with measurable redundancy metrics

2. **Controlled Token Density Experiment**: Construct synthetic forget sets with controlled token frequency distributions (uniform vs. clustered) to isolate whether token-level redundancy, rather than semantic correlation, drives coreset effectiveness

3. **Cross-Architecture Replication**: Test the coreset effect on non-Transformer architectures (e.g., RWKV, Mamba) to determine whether the phenomenon reflects fundamental properties of gradient-based unlearning or Transformer-specific representations