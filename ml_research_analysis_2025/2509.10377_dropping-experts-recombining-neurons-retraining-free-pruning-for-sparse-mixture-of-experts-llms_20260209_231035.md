---
ver: rpa2
title: 'Dropping Experts, Recombining Neurons: Retraining-Free Pruning for Sparse
  Mixture-of-Experts LLMs'
arxiv_id: '2509.10377'
source_url: https://arxiv.org/abs/2509.10377
tags:
- expert
- experts
- arxiv
- pruning
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of reducing memory usage in
  Sparse Mixture-of-Experts (SMoE) large language models, which still require loading
  all expert parameters despite activating only a few per token. The proposed DERN
  framework introduces a novel neuron-level pruning and reconstruction approach that
  decomposes experts into functional segments, reassigns them to retained experts
  based on structural similarity, and merges them via spherical weighted k-means clustering.
---

# Dropping Experts, Recombining Neurons: Retraining-Free Pruning for Sparse Mixture-of-Experts LLMs

## Quick Facts
- arXiv ID: 2509.10377
- Source URL: https://arxiv.org/abs/2509.10377
- Reference count: 18
- One-line primary result: DERN achieves over 5% performance gains on commonsense reasoning and MMLU benchmarks under 50% expert sparsity without retraining, while reducing memory usage and improving inference speed by up to 38%.

## Executive Summary
This paper addresses the challenge of reducing memory usage in Sparse Mixture-of-Experts (SMoE) large language models, which still require loading all expert parameters despite activating only a few per token. The proposed DERN framework introduces a novel neuron-level pruning and reconstruction approach that decomposes experts into functional segments, reassigns them to retained experts based on structural similarity, and merges them via spherical weighted k-means clustering. Experiments on Mixtral, Qwen, and DeepSeek models show that DERN achieves over 5% performance gains on commonsense reasoning and MMLU benchmarks under 50% expert sparsity, without retraining. It also reduces memory usage and improves inference speed by up to 38% while maintaining competitive accuracy.

## Method Summary
The DERN framework operates in three stages: (1) Prune experts using importance scores calculated from router activation statistics over calibration data, retaining only the most frequently and heavily activated experts; (2) Decompose pruned experts into segments (triplets of gate, up, and down projection vectors), reassign each segment to the most structurally similar retained expert if cosine similarity exceeds threshold α, and softly transfer routing contributions; (3) Apply spherical weighted k-means clustering on segments within each retained expert (using gate-based initialization and importance weighting) to reconstruct compact experts from cluster centroids. The method requires no retraining and works on calibration data from the C4 corpus.

## Key Results
- Achieves over 5% performance gains on commonsense reasoning and MMLU benchmarks under 50% expert sparsity
- Reduces memory usage and improves inference speed by up to 38% on Mixtral-8×7B-Instruct
- Outperforms baselines like REAP and Mosaic on multiple benchmarks while maintaining accuracy

## Why This Works (Mechanism)

### Mechanism 1: Routing-Based Importance Filtering
- **Claim:** Significant portions of SMoE parameters can be removed with minimal performance loss if experts are weighted by their actual contribution to the output distribution.
- **Mechanism:** DERN calculates an importance score $S_i$ for each expert based on the frequency and magnitude of router activations over a calibration set (Eq. 1). It retains the top-$k$ experts and designates the rest for pruning, assuming the router's learned weights reflect semantic necessity.
- **Core assumption:** The router has successfully learned to distribute workload such that low-weight experts contribute minimally to the final result.
- **Evidence anchors:** [Section 3.1] Defines importance score as the expected value of normalized router weights; [Abstract] Confirms pruning uses "router statistics"; [Corpus] *Finding Fantastic Experts* supports the general observation of expert redundancy in SMoE architectures.

### Mechanism 2: Neuron-Level Semantic Realignment
- **Claim:** Direct weight averaging of experts fails because of neuron-level misalignment; performance is preserved by decomposing experts into segments and merging only compatible components.
- **Mechanism:** Instead of averaging whole experts, DERN decomposes them into "expert segments" (triplets of gate, up, and down projection vectors). It treats these segments as independent functional units. Segments from pruned experts are reassigned to retained experts only if their cosine similarity exceeds a threshold $\alpha$, correcting for structural mismatches.
- **Core assumption:** Expert outputs are sums of independent low-rank transformations (Eq. 3), meaning segments can be swapped or merged without breaking functional logic.
- **Evidence anchors:** [Section 3.2] Defines the "Expert Segment" and the reassignment logic based on local structural similarity; [Figure 1] Visualizes the neuron-level inconsistency that motivates avoiding expert-level averaging; [Corpus] *REAP the Experts* provides context on the difficulty of expert merging, though it does not validate this specific segment-based approach.

### Mechanism 3: Centroid-Based Compression via Spherical K-Means
- **Claim:** Redundant segments within a recombined expert can be compressed into a single representative vector without retraining if clustered by directional similarity.
- **Mechanism:** After recombining segments into a retained expert, DERN applies spherical weighted k-Means clustering. It groups similar segments and replaces them with cluster centroids, effectively reducing the hidden dimension ($h$) of the expert while preserving the "direction" of the feature space.
- **Core assumption:** High-dimensional neuron directions capture the essential semantics, while magnitudes can be normalized or averaged.
- **Evidence anchors:** [Section 3.3] Describes the minimization of the weighted cosine-based clustering objective; [Table 4] Shows memory reduction correlates with performance maintenance; [Corpus] *Mosaic Pruning* generally supports hierarchical compression frameworks, though specific centroid methods differ.

## Foundational Learning

- **Concept: Sparse Mixture-of-Experts (SMoE) Routing**
  - **Why needed here:** You must understand that for a given token $x$, only a subset of experts $E$ are activated by a gating network $G(x)$. DERN exploits this to prune inactive $E$.
  - **Quick check question:** If a router picks Expert A and Expert B for a token, does DERN merge them into one expert, or does it keep them separate but check if their internal neurons are compatible?

- **Concept: Gated Linear Units (GLU)**
  - **Why needed here:** DERN's "Expert Segment" definition relies on the specific structure of GLU layers (Gate, Up, Down projections). You cannot decompose the expert correctly without recognizing these three distinct matrices.
  - **Quick check question:** Which three vectors constitute an "Expert Segment" in DERN?

- **Concept: Cosine Similarity in Parameter Space**
  - **Why needed here:** The method relies on measuring the alignment of neuron weights (not activations) to decide if two segments perform the same function and can be merged.
  - **Quick check question:** Does DERN merge segments based on their output activation similarity or their raw weight vector similarity?

## Architecture Onboarding

- **Component map:** Router Statistic Collector -> Segment Decomposer -> Similarity Matcher -> Clusterer
- **Critical path:** The **Similarity Threshold ($\alpha$)** in Segment Recombination. If set too low, you force incompatible knowledge together (noise); if set too high, you lose the knowledge from dropped experts entirely.
- **Design tradeoffs:**
  - **Generality vs. Specialization:** DERN performs well on Mixtral/Qwen (where experts overlap) but struggles with DeepSeek (highly specialized experts). *Assumption:* The method works best when experts are redundant rather than strictly disjoint.
  - **Efficiency vs. Accuracy:** Ablation shows $\alpha \approx 0.4$ is optimal. Straying from this significantly degrades the avg. performance (Table 11).
- **Failure signatures:**
  - **DeepSeek-MoE Degradation:** High performance drop on DeepSeek at high sparsity (Table 3). This signals the model's experts are too independent to recombine meaningfully.
  - **Random Initialization:** If clustering centroids are random rather than gate-weight initialized, performance collapses (Table 12).
- **First 3 experiments:**
  1. **Threshold Ablation:** Run DERN on Mixtral-8x7B varying $\alpha$ from 0.0 to 1.0. Confirm the performance peak is near 0.4-0.6 and not at the extremes (merge-all vs. merge-none).
  2. **Component Ablation:** Reconstruct experts using only "Up+Down" vectors for similarity vs. the full "Triplet." Verify that excluding the gate vector improves results (Table 5).
  3. **Inference Latency Check:** Measure Tokens/Second and Memory (GB) on vLLM before and after pruning 2 experts (8 $\to$ 6). Confirm the theoretical memory reduction translates to actual throughput gains (Table 4).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can pruning strategies be adapted for SMoE architectures with highly specialized, decoupled experts where structural redundancy is low?
- **Basis in paper:** [explicit] The Limitations section notes that DeepSeek-MoE suffers significant degradation under high sparsity because its experts are more independent, concluding that general-purpose methods "may require architectural adaptation."
- **Why unresolved:** DERN relies on neuron-level structural similarity which assumes some degree of redundancy or interchangeability; specialized experts lack this overlapping structure, making standard recombination ineffective.
- **What evidence would resolve it:** A modified approach that identifies and protects specialized experts or utilizes activation-based alignment, successfully closing the performance gap with dense baselines on models like DeepSeek.

### Open Question 2
- **Question:** Can functional similarity metrics outperform parameter-space cosine distance for capturing semantic alignment during segment recombination?
- **Basis in paper:** [explicit] The authors state in the Limitations that measuring segment similarity in parameter space "may be insufficient for capturing functional alignment, especially in models with highly specialized experts."
- **Why unresolved:** Weight vectors can appear similar structurally (high cosine similarity) while encoding conflicting semantics, leading to suboptimal segment reassignment.
- **What evidence would resolve it:** Experiments replacing parameter-space cosine distance with activation-based or output-based metrics in the reassignment stage, demonstrating improved performance retention or reduced semantic conflicts.

### Open Question 3
- **Question:** To what extent does the choice of calibration data influence the robustness of the expert importance scores?
- **Basis in paper:** [inferred] The paper claims the method is "task-agnostic" but relies on a fixed calibration set (128 sequences from C4) to calculate routing statistics and determine expert utility.
- **Why unresolved:** It is unstated whether the "redundant" experts identified by the router statistics would change if the calibration data distribution shifted (e.g., to code or mathematical reasoning).
- **What evidence would resolve it:** An ablation study measuring the variance in selected experts and final model performance when the calibration dataset is swapped for different domains like code or STEM.

## Limitations
- **Model Generality:** DERN's performance heavily depends on semantic redundancy among experts, excelling on Mixtral/Qwen but degrading significantly on DeepSeek where experts are highly specialized.
- **Hyperparameter Sensitivity:** The similarity threshold $\alpha$ is crucial for success, with the optimal range narrowly confined to 0.4-0.6, suggesting the method requires careful calibration per model.
- **Memory vs. Accuracy Trade-off:** While memory reduction and speed gains are demonstrated, accuracy preservation is less consistent at higher sparsity levels and across all benchmarks.

## Confidence
- **High Confidence:** The neuron-level decomposition mechanism and spherical k-means clustering approach are well-specified and supported by experimental evidence. Performance gains on Mixtral and Qwen at moderate sparsity levels are reproducible and clearly demonstrated.
- **Medium Confidence:** The routing-based importance filtering works as described, but its effectiveness varies significantly across models. The assumption that router weights reflect semantic necessity is reasonable but not universally validated.
- **Low Confidence:** The method's general applicability across diverse SMoE architectures is overstated. The significant performance degradation on DeepSeek suggests the approach is not model-agnostic and requires expert redundancy as a prerequisite.

## Next Checks
1. **Cross-Model Ablation Study:** Apply DERN to a diverse set of SMoE models (Mixtral, Qwen, DeepSeek, and one additional model like Yi or LLaMA-Expert) at 25%, 50%, and 75% sparsity levels. Document the correlation between expert semantic overlap and performance preservation to quantify the method's applicability boundary.

2. **Threshold Sensitivity Analysis:** Systematically vary $\alpha$ from 0.1 to 0.9 in 0.1 increments across all models. Measure not just accuracy but also clustering convergence stability and memory reduction to determine if the optimal range shifts with model architecture or sparsity level.

3. **Expert Specialization Quantification:** Before applying DERN, quantify expert specialization using metrics like inter-expert cosine similarity or task-specific activation patterns. Validate whether this pre-measurement can predict DERN's success or failure, creating a practical guideline for when the method should be applied.