---
ver: rpa2
title: 'Beyond Text: Unveiling Privacy Vulnerabilities in Multi-modal Retrieval-Augmented
  Generation'
arxiv_id: '2505.13957'
source_url: https://arxiv.org/abs/2505.13957
tags:
- uni00000013
- uni00000048
- leakage
- data
- uni0000004c
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first systematic study of privacy vulnerabilities
  in multimodal retrieval-augmented generation (MRAG) systems. The authors develop
  a compositional structured prompt attack method to extract private information from
  vision-language and speech-language RAG systems in a black-box setting.
---

# Beyond Text: Unveiling Privacy Vulnerabilities in Multi-modal Retrieval-Augmented Generation

## Quick Facts
- arXiv ID: 2505.13957
- Source URL: https://arxiv.org/abs/2505.13957
- Authors: Jiankun Zhang; Shenglai Zeng; Jie Ren; Tianqi Zheng; Hui Liu; Xianfeng Tang; Hui Liu; Yi Chang
- Reference count: 12
- Key outcome: First systematic study of privacy vulnerabilities in MRAG systems using compositional structured prompt attacks

## Executive Summary
This paper presents the first systematic study of privacy vulnerabilities in multimodal retrieval-augmented generation (MRAG) systems. The authors develop a compositional structured prompt attack method to extract private information from vision-language and speech-language RAG systems in a black-box setting. Their attack combines information retrieval components with content reproduction commands to successfully extract sensitive data. In vision-language RAG experiments, the attack achieved up to 406 direct image copies (73 unique images) and 499 indirect text extractions (75 unique images). In speech-language RAG experiments, the attack extracted up to 459 indirect text transcripts (173 unique audios) and generated 410 nearly identical audio copies (146 unique audios). These results demonstrate significant privacy risks in MRAG systems, highlighting the urgent need for robust privacy-preserving techniques.

## Method Summary
The authors developed a compositional structured prompt attack method that targets MRAG systems through a two-component approach: an {information} component that retrieves targeted multimodal content via semantic similarity matching using encoders like CLIP or CLAP, and a {command} component that exploits the LMM's instruction-following tendency to reproduce or describe the retrieved content. The attack was evaluated across vision-language and speech-language RAG systems using databases of 5,000 images and 5,000 audio clips respectively. They tested various LMMs including Gemini, Lumina, and MiniCPM under black-box conditions without prior knowledge of system internals.

## Key Results
- Vision-language RAG attack achieved 406 direct image copies (73 unique images) and 499 indirect text extractions (75 unique images)
- Speech-language RAG attack extracted 459 indirect text transcripts (173 unique audios) and generated 410 nearly identical audio copies (146 unique audios)
- Attack success demonstrates significant privacy risks in MRAG systems through both direct reproduction and indirect description of sensitive content

## Why This Works (Mechanism)

### Mechanism 1: Compositional Prompt Decomposition
- Claim: A structured prompt with separate information and command components can extract private data from MRAG systems in a black-box setting.
- Mechanism: The {information} component triggers retrieval of targeted multimodal content via semantic similarity matching using encoders like CLIP or CLAP, while the {command} component exploits the LMM's instruction-following tendency to reproduce or describe the retrieved content.
- Core assumption: The MRAG pipeline passes retrieved content directly into the LMM context without sanitization, and LMMs comply with reproduction instructions.
- Evidence anchors:
  - [abstract]: "Using a novel compositional structured prompt attack in a black-box setting, we demonstrate how attackers can extract private information by manipulating queries."
  - [section 3.3]: "We design a composite structured prompting strategy consisting of two key components: an {information} component to retrieve targeted data and a {command} component to induce the LMM to reveal retrieved contents."
  - [corpus]: Related work (Poisoned-MRAG, HV-Attack) confirms MRAG vulnerability to retrieval-pathway manipulation, though corpus lacks direct evidence on this specific compositional structure.
- Break condition: System implements (a) content sanitization before LMM input, (b) instruction filtering for reproduction commands, or (c) output modality restrictions.

### Mechanism 2: Dual-Channel Leakage via Cross-Modal Generation
- Claim: Privacy violations occur through both direct reproduction of retrieved content AND indirect description of that content.
- Mechanism: LMMs can generate outputs near-identical to retrieved images/audio (direct), or produce detailed textual descriptions revealing semantic content (indirect). Cross-modal training enables both pathways.
- Core assumption: LMMs retain sufficient fidelity of retrieved content in context and have been trained to describe multimodal content accurately.
- Evidence anchors:
  - [abstract]: "Our experiments reveal that LMMs can both directly generate outputs resembling retrieved content and produce descriptions that indirectly expose sensitive information."
  - [section 4.2]: "We analyze the risks according to the output types: (1) Visual/Multimodal outputs... (2) Textual outputs..."
  - [corpus]: Limited corpus evidence on dual-channel leakage specifically; related work focuses on single-modality attacks.
- Break condition: System (a) restricts output modalities, (b) detects output similarity to database, or (c) limits description detail through safety filters.

### Mechanism 3: Retrieval Diversity via Random Information Fragments
- Claim: Random word fragments from Common Crawl induce diverse retrieval, maximizing database coverage without prior knowledge.
- Mechanism: Multimodal encoders project queries and content into shared embedding space. Random fragments create diverse query embeddings, triggering retrieval of varied entries.
- Core assumption: Embedding space has sufficient coverage that random combinations map to diverse database regions, and retrieval lacks query-based access controls.
- Evidence anchors:
  - [section 3.3]: "Following (Carlini et al., 2021), we enhance variability by randomly sampling 15 word fragments from the Common Crawl dataset for this component."
  - [section 4.5, Figure 3a]: Different command variants successfully retrieve diverse images.
  - [corpus]: Zeng et al. (2024) demonstrates similar retrieval-based extraction in text-RAG, supporting generalizability.
- Break condition: Retrieval implements (a) query-based access control, (b) poor semantic clustering, or (c) artificial diversity limits.

## Foundational Learning

- Concept: **RAG Pipeline Architecture**
  - Why needed here: Understanding how retrieved content fuses with queries to form LMM input defines the attack surface.
  - Quick check question: How are user queries and retrieved multimodal content combined before being passed to the LMM?

- Concept: **Multimodal Embedding Spaces**
  - Why needed here: The attack exploits how CLIP/CLAP map text queries and images/audio into shared space for similarity retrieval.
  - Quick check question: How would a text query "medical scan" retrieve relevant images from a database?

- Concept: **LMM Modal Capabilities**
  - Why needed here: Attack success depends on LMMs' ability to process retrieved content and generate outputs in various modalities.
  - Quick check question: What determines whether an LMM generates text, image, or audio output?

## Architecture Onboarding

- Component map:
  - **Retrieval Database (D)**: Stores multimodal entries (text + image/audio)
  - **Multimodal Encoder (E)**: CLIP (vision), CLAP (audio); projects to shared embedding space
  - **Retriever (R)**: FAISS similarity search; returns top-k entries
  - **Prompt Constructor**: Fuses query + retrieved content via templates
  - **LMM Generator**: Gemini, Lumina, MiniCPM processing multimodal input

- Critical path: Query → Encoder → FAISS Retrieval → Prompt Construction → LMM Generation → Output

- Design tradeoffs:
  - Higher k retrieves more content but doesn't proportionally increase extraction (LMMs generate limited outputs)
  - Clear commands improve extraction but longer commands reduce retrieval diversity
  - Direct vs. indirect leakage require different prompt strategies

- Failure signatures:
  - Safety rejections increase with k (0 at k=1, 27 at k=3)
  - Text-only LMMs reject image generation requests
  - Content blending at higher k reduces extraction fidelity

- First 3 experiments:
  1. Run 100 attack prompts with k=1 using both direct and indirect command variants to establish baseline extraction rates.
  2. Measure unique entries retrieved across 500 random fragments to determine database coverage.
  3. Test extraction with output modality constraints across different LMM generators to identify vulnerability profiles.

## Open Questions the Paper Calls Out
None

## Limitations
- Scope constraints: Only examines black-box attacks without access to system internals
- Dataset dependencies: Attack effectiveness depends on database content diversity and embedding quality
- Modality-specific blindspots: Focuses on CLIP for vision and CLAP for audio, other encoders may behave differently

## Confidence
- **High confidence** in compositional structured prompt mechanism: The attack consistently achieves measurable extraction across multiple LMMs and databases, with clear evidence from controlled experiments showing both direct and indirect leakage pathways.
- **Medium confidence** in retrieval diversity claims: While random fragments successfully retrieve diverse content, the exact relationship between fragment randomness and coverage remains theoretical.
- **Low confidence** in generalizability to production systems: Results show MRAG vulnerabilities, but real-world implementations often include additional safeguards not evaluated in this study.

## Next Checks
1. Cross-database validation: Test attack effectiveness across databases with varying sizes (10K vs 100K entries), content distributions (medical vs general imagery), and embedding qualities to establish robustness bounds.
2. White-box variant exploration: Develop and test white-box attack variants that leverage knowledge of specific embedding spaces, retrieval thresholds, or LMM architectures to establish attack ceiling potential.
3. Defense effectiveness measurement: Implement and evaluate specific countermeasures including retrieval filtering (domain-based access control), LMM safety enhancements (output similarity detection), and hybrid architectures (separating retrieval from generation) to quantify vulnerability reduction.