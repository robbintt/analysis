---
ver: rpa2
title: A First-Order Logic-Based Alternative to Reward Models in RLHF
arxiv_id: '2512.14100'
source_url: https://arxiv.org/abs/2512.14100
tags:
- reward
- translation
- learning
- language
- s-grpo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a supervised variant of GRPO (S-GRPO) that
  integrates label-based supervision into the loss function, achieving superior performance
  over standard supervised fine-tuning (SFT) baselines. To address the absence of
  reliable automatic metrics in preference learning, the authors introduce a logic-similarity-based
  reward mechanism that measures semantic and logical consistency between model outputs
  and reference labels using first-order predicate logic.
---

# A First-Order Logic-Based Alternative to Reward Models in RLHF

## Quick Facts
- **arXiv ID:** 2512.14100
- **Source URL:** https://arxiv.org/abs/2512.14100
- **Reference count:** 39
- **Primary result:** S-GRPO integrates supervised label-based supervision into GRPO, outperforming SFT and offering an interpretable logic-based alternative to reward models.

## Executive Summary
This paper introduces S-GRPO, a supervised variant of GRPO that incorporates ground-truth labels into the RLHF objective via a log-likelihood ratio term. This hybrid design improves training stability and robustness compared to standard supervised fine-tuning. To address the absence of reliable automatic metrics in preference learning, the authors propose a logic-similarity-based reward mechanism using first-order predicate logic to measure semantic and logical consistency between model outputs and reference labels. Experiments on FOL translation, WMT English–German translation, and PKU-SafeRLHF preference learning show S-GRPO consistently outperforms SFT baselines, though slightly trails DPO in preference learning.

## Method Summary
S-GRPO modifies the GRPO objective by adding a supervised term that maximizes the log-probability of ground-truth labels under the policy relative to a reference model. This hybrid loss jointly optimizes the generation advantage, KL divergence regularization, and label alignment. For tasks lacking automatic metrics, a logic-similarity reward is computed by converting model outputs and references into first-order logic expressions and measuring their semantic and logical consistency. The method is evaluated on three tasks using Qwen3-1.7B: FOL translation (with logical equivalence and BLEU), WMT En–De translation (with BLEU and COMET), and PKU-SafeRLHF preference learning (with win/tie/loss rates judged by DeepSeek-V3).

## Key Results
- S-GRPO achieves 87.4% logical equivalence and 66.0 BLEU score on FOL translation, surpassing existing methods.
- On WMT En–De, S-GRPO attains 30.18 BLEU and 86.17 COMET score.
- In preference learning, S-GRPO achieves 22.82% win rate against base model, outperforming SFT (0.95%) but slightly below DPO (28.69%).

## Why This Works (Mechanism)

### Mechanism 1: Label-Integrated Policy Optimization
Explicitly integrating ground-truth labels into the GRPO loss function as a log-likelihood ratio term improves training stability and output quality over standard SFT or reward-model-dependent RLHF. S-GRPO modifies the GRPO objective by adding a supervised term `J_SFT(θ) = log πθ(y|q) / πref(y|q)`. This term is jointly optimized with the standard GRPO generation term and KL-divergence regularization, creating a hybrid loss that anchors the policy to high-quality labels while still allowing it to learn from generated sample advantages. Core assumption: Ground-truth labels provide a more stable and direct training signal than learned, potentially noisy reward models. Evidence: [abstract], [Page 3, Section III-A], and related work on hybridizing RLHF with supervised objectives. Break condition: Performance degrades if ground-truth labels are noisy, sparse, or unavailable.

### Mechanism 2: Logic-Similarity as a Reward Proxy
Replacing a separately trained neural reward model with a formal logic-similarity function provides an interpretable and stable reward signal for preference learning without sacrificing alignment. Model outputs and reference labels are converted into First-Order Logic (FOL) expressions. A logic-similarity score is computed based on semantic and logical consistency between these FOL expressions. This score directly substitutes the scalar output of a conventional reward model. Core assumption: First-order logic can adequately capture the semantic nuances of natural language preferences, and a reliable FOL translation model exists. Evidence: [abstract], [Page 1, col 2], and thematic relation to embedding-based rewards. Break condition: Fails if the FOL translation is inaccurate or if logical equivalence metrics cannot handle valid semantic variations in language.

### Mechanism 3: Group-Relative Advantage with Supervised Anchoring
Combining group-relative advantage estimation (from GRPO) with a supervised loss term creates a robust optimization process that is less prone to model collapse than pure RL approaches. The method samples multiple outputs for a query and computes advantages relative to the group. The policy is updated via a clipped objective that includes the supervised `J_SFT(θ)` term. This provides a stable anchor (the label) while allowing the model to explore and learn from the relative quality of its generated samples. Core assumption: The group-relative advantage provides a meaningful learning signal, and the supervised loss prevents aggressive logic-based rewards from destabilizing the model. Evidence: [Page 3, Section III-A], [Page 5-6, Fig. 3 & 4], and internal paper support. Break condition: Fails if the `clip` or KL-divergence hyperparameters are improperly tuned, leading to either overly conservative or unstable updates.

## Foundational Learning

- **Concept: Proximal Policy Optimization (PPO) and its variants (GRPO)**
  - **Why needed here:** S-GRPO is built directly on the GRPO framework. Understanding how PPO uses a clipped objective and KL-penalty to ensure stable policy updates is essential to grasp how S-GRPO modifies this process.
  - **Quick check question:** What problem does the PPO clipping mechanism solve, and what component does GRPO remove from the standard PPO architecture?

- **Concept: First-Order Logic (FOL) and Logical Equivalence**
  - **Why needed here:** The paper's core innovation for preference learning is a reward based on logical similarity. You must understand how natural language is formalized into predicates, quantifiers, and logical connectives to evaluate this method's applicability.
  - **Quick check question:** How would the sentence "All humans are mortal" be represented in FOL, and how might you compute its equivalence to another FOL statement?

- **Concept: Reward Modeling in RLHF**
  - **Why needed here:** S-GRPO's logic-based reward is proposed as an *alternative* to the standard approach of training a separate neural network on human preference data. Understanding the standard reward model's role, costs, and failure modes clarifies the problem S-GRPO aims to solve.
  - **Quick check question:** In a standard RLHF pipeline, what data is the reward model trained on, and what is its output used for?

## Architecture Onboarding

- **Component map:** Policy Model (`πθ`) -> Reference Model (`πref`) -> Reward Function (metric or Logic-Similarity Module) -> Logic-Similarity Module (FOL conversion and similarity scoring)
- **Critical path:** The new S-GRPO loss calculation. For each query `q`, the system must generate samples `{o_i}`, compute their rewards (via metric or logic), calculate group-relative advantages `A_i`, and crucially compute the log-probability of the ground-truth label `y` under both the policy and reference models to form the `J_SFT` term. All these components are summed into the final loss for the gradient update.
- **Design tradeoffs:**
  - **S-GRPO vs. SFT:** S-GRPO is computationally more expensive than SFT because it requires sampling outputs at each training step, but it offers better stability and performance by combining exploration with label anchoring.
  - **Logic Reward vs. Learned Reward:** A logic-based reward is more interpretable and requires no training data for a reward model, but it is brittle to semantic variations that FOL cannot capture. A learned reward model is more flexible but introduces trainability issues and potential bias.
- **Failure signatures:**
  - **Model Collapse:** If the logic-based reward is too restrictive or the KL penalty is too weak, the model may lose generative diversity.
  - **Reward Hacking:** The model might generate outputs that game the FOL parser or automatic metric rather than producing genuinely high-quality text.
  - **Training Instability:** Indicated by a rapidly increasing KL-divergence or oscillating rewards, suggesting the need to tune the `β` or `clip` hyperparameters.
- **First 3 experiments:**
  1. **Baseline Comparison:** Replicate the paper's comparison of S-GRPO against a standard SFT baseline on a translation task with an automatic metric reward. Verify if the reported stability gains (e.g., performance over 5 epochs) are reproducible.
  2. **Logic Reward Ablation:** On a small preference dataset, compare training with the logic-similarity reward versus a simple cosine-similarity reward. This tests the paper's claim that the *formal logic* component adds value over simple semantic matching.
  3. **Ablate the `J_SFT` Term:** Run S-GRPO with the supervised `J_SFT` term removed, relying only on the generated sample advantages. Compare the training stability and final performance to the full S-GRPO to isolate the contribution of the label-based supervision.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can S-GRPO be extended to open-domain tasks that lack automatic evaluation metrics without requiring a separately trained reward model?
- **Basis in paper:** [explicit] "For tasks lacking automatic evaluation metrics—such as open-domain question answering—S-GRPO still requires a reward model to compute preference scores."
- **Why unresolved:** The logic-similarity reward mechanism is designed for structured outputs (FOL, translation) where formal evaluation is possible; open-ended tasks lack equivalent ground-truth logical forms.
- **What evidence would resolve it:** Demonstrating S-GRPO on open-domain QA using only logic-based or self-consistency rewards, with performance competitive against reward-model-dependent baselines.

### Open Question 2
- **Question:** Can logical matching strategies be optimized to reduce semantic mismatch when model outputs diverge in form from references?
- **Basis in paper:** [explicit] "Although semantic mismatch may still cause minor inconsistencies, optimizing logical matching strategies could further close this gap."
- **Why unresolved:** The current FOL-based matching assumes surface-level alignment; paraphrases or stylistically different but semantically equivalent outputs may yield spurious mismatches.
- **What evidence would resolve it:** Improved LE scores on adversarial paraphrase pairs, or ablation studies showing robustness to syntactic variation while preserving semantic fidelity.

### Open Question 3
- **Question:** Can S-GRPO achieve computational efficiency comparable to SFT or DPO while retaining its exploration benefits?
- **Basis in paper:** [explicit] "It still requires significant computational resources due to the need for sequence generation during training—making it substantially more expensive than SFT or DPO."
- **Why unresolved:** The sequence sampling requirement during training is inherent to GRPO-style optimization; no approximation or distillation strategy is proposed.
- **What evidence would resolve it:** A variant using reduced sampling, teacher-forcing hybrids, or cached generations achieving similar performance with measurable speedup and lower memory footprint.

### Open Question 4
- **Question:** Why does DPO outperform S-GRPO on preference learning, and can this gap be closed while maintaining S-GRPO's interpretability advantages?
- **Basis in paper:** [inferred] DPO achieves 28.69% win rate versus S-GRPO's 22.82% on PKU-SafeRLHF, despite S-GRPO's logic-based rewards being more interpretable.
- **Why unresolved:** The paper attributes DPO's advantage to lower variance but does not analyze whether the logic-similarity reward signal is noisier or whether advantage estimation under low rewards harms S-GRPO.
- **What evidence would resolve it:** Ablation comparing DPO vs. S-GRPO under matched reward signals, or analysis of reward variance and its correlation with performance gaps.

## Limitations
- The method still requires significant computational resources due to sequence generation during training, making it substantially more expensive than SFT or DPO.
- For tasks lacking automatic evaluation metrics, S-GRPO still requires a reward model to compute preference scores, limiting its applicability.
- The logic-similarity reward may not fully capture semantic nuances when model outputs diverge in form from references, leading to potential mismatches.

## Confidence
- **High Confidence:** The core S-GRPO architecture (hybrid loss combining supervised term with GRPO) and its general performance benefits over SFT on translation tasks are well-supported by the reported results.
- **Medium Confidence:** The specific superiority of the FOL-based logic-similarity reward over conventional learned reward models in preference learning, as the evaluation uses a single alternative (SFT) and the win rate vs. DPO is only marginally lower.
- **Low Confidence:** The claim that the FOL-based reward mechanism is a broadly applicable and robust alternative to reward models, due to the lack of ablation studies on the logic component and the absence of testing on diverse preference datasets.

## Next Checks
1. **Logic Reward Ablation:** Compare S-GRPO with a simple semantic similarity reward (e.g., cosine similarity on embeddings) versus the full FOL-based reward on the PKU-SafeRLHF dataset to isolate the contribution of the formal logic component.
2. **Hyperparameter Sensitivity:** Systematically vary the KL weight (β) and clipping threshold (ε) in S-GRPO to identify stable operating regions and test the claim of inherent robustness.
3. **Generalization Test:** Apply S-GRPO to a different preference alignment task (e.g., a summarization or instruction-following dataset) to evaluate if the FOL-based reward and the S-GRPO framework generalize beyond the PKU-SafeRLHF setup.