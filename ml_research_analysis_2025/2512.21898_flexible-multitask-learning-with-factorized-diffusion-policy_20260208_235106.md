---
ver: rpa2
title: Flexible Multitask Learning with Factorized Diffusion Policy
arxiv_id: '2512.21898'
source_url: https://arxiv.org/abs/2512.21898
tags:
- diffusion
- components
- policy
- tasks
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Factorized Diffusion Policy (FDP), a modular
  policy architecture that factorizes complex action distributions into specialized
  diffusion components. Each component captures a distinct behavioral mode, with a
  router network composing them via observation-conditioned score aggregation.
---

# Flexible Multitask Learning with Factorized Diffusion Policy

## Quick Facts
- **arXiv ID**: 2512.21898
- **Source URL**: https://arxiv.org/abs/2512.21898
- **Reference count**: 40
- **Primary result**: Introduces Factorized Diffusion Policy (FDP) that factorizes complex action distributions into specialized diffusion components, achieving up to 95% success rates on complex manipulation tasks.

## Executive Summary
This paper introduces Factorized Diffusion Policy (FDP), a modular policy architecture that factorizes complex action distributions into specialized diffusion components. Each component captures a distinct behavioral mode, with a router network composing them via observation-conditioned score aggregation. FDP avoids the instability of traditional mixture-of-experts methods by using continuous composition rather than discrete expert selection. The approach improves multitask learning and enables efficient adaptation through component addition or fine-tuning without catastrophic forgetting. Evaluated on MetaWorld, RLBench, and real-world manipulation tasks, FDP consistently outperforms monolithic diffusion policies and MoE baselines.

## Method Summary
FDP uses a diffusion-based policy where action prediction is factorized into multiple specialized diffusion components, each implemented as a U-Net predicting noise. A lightweight router network takes observations and outputs weights for each component. The final denoising score is computed as a weighted sum of component scores, enabling soft composition. The model is trained end-to-end using MSE loss between true noise and weighted sum of component predictions. For adaptation to new tasks, FDP adds new components initialized from existing ones while freezing previous components, enabling knowledge retention and mitigating catastrophic forgetting.

## Key Results
- Achieves 95% success rate on MetaWorld tasks with 4-6 components versus 70-80% for monolithic diffusion policies
- Demonstrates stable training with consistent convergence versus unstable MoE baselines
- Shows effective lifelong learning with frozen components retaining 90%+ performance on original tasks during adaptation

## Why This Works (Mechanism)

### Mechanism 1
Continuous score aggregation stabilizes multitask training compared to discrete expert selection. Instead of selecting a single expert via a discrete router (which causes gradient starvation and routing collapse), FDP computes the final denoising score as a weighted sum of all component scores. This ensures all components receive gradient signals consistently, preventing the "rich-get-richer" instability common in Mixture-of-Experts (MoE).

### Mechanism 2
Factorization forces specialization across behavioral modes (sub-skills) rather than just noise levels. By training components to collectively minimize the noise prediction error via a weighted sum, the optimization pressure pushes distinct components to capture distinct phases of a task (e.g., "reaching" vs. "grasping").

### Mechanism 3
Modular expansion with frozen components mitigates catastrophic forgetting during lifelong learning. When adapting to a new task, FDP adds a new diffusion component initialized via "upcycling" (copying weights) and freezes existing components. Only the new component and the router are trained, isolating gradient updates to new capacities while preserving previously learned manifolds.

## Foundational Learning

- **Denoising Diffusion Probabilistic Models (DDPM)**: FDP learns to predict noise (score function) to iteratively refine actions. Understanding the iterative denoising loop is essential for debugging router or components.
- **Mixture-of-Experts (MoE) Routing & Instability**: The paper frames itself as a solution to MoE instability. Understanding discrete vs. soft routing is essential to see why "continuous composition" is a structural change.
- **Energy-Based Models (EBM) & Product of Experts**: The paper claims weighted sum of scores corresponds to a "product of distributions." This is the theoretical justification for why adding scores works better than mixing actions.

## Architecture Onboarding

- **Component map**: Observation → Router → Weights → Aggregator → Denoiser; Observation + Noisy Action → All Components → Scores → Aggregator
- **Critical path**: 1) Observation enters Router → Weights; 2) Noisy action enters all Components → Scores; 3) Aggregator computes weighted sum; 4) Denoiser steps using aggregated score
- **Design tradeoffs**: Component Count (N=4-6) affects expressiveness vs. inference cost; Component Size uses smaller U-Nets than monolithic baseline; Router Capacity affects modulation capability
- **Failure signatures**: Uniform Weights (router outputs constant weights ≈ 1/N); Component Cloning (high cosine similarity between outputs); Forgetting on Adaptation (performance drops on old tasks)
- **First 3 experiments**: 1) Overfit Single Task: Train on 1 simple task, verify weights converge to 1-2 components; 2) Ablation on Aggregation: Replace weighted sum with max-selection, observe training stability; 3) Visualize Specialization: Plot router weights over time steps for multi-phase task, verify component alignment with task phases

## Open Questions the Paper Calls Out
- Can heterogeneous module architectures (mixing U-Nets and Transformers) enhance flexibility compared to homogeneous setup?
- What behavioral dependencies and failure modes emerge when systematically ablating individual diffusion components?
- How does FDP perform in long-term lifelong learning scenarios with extensive task sequences and concept drift?

## Limitations
- Component specialization mechanism lacks strong empirical validation - qualitative visualizations exist but quantitative metrics for measuring specialization are absent
- Catastrophic forgetting mitigation tested primarily on simple task variations rather than fundamentally different task families
- Scalability to more complex tasks or longer horizon planning is largely untested - current results focus on relatively short-horizon manipulation tasks

## Confidence
- **High Confidence**: Continuous score aggregation stabilizes training - well-supported by theoretical framing and training stability metrics
- **Medium Confidence**: Component specialization claims are plausible but lack rigorous quantitative validation
- **Low Confidence**: Claims about scalability to more complex tasks or longer horizon planning are largely untested

## Next Checks
1. Measure mutual information between component activation patterns and ground-truth task phase labels, comparing to control with randomly initialized components
2. Evaluate FDP's performance under visual observation distribution shifts (texture randomization, lighting changes) while keeping component manifold fixed
3. Systematically vary component count (N=2, 4, 6, 8) and measure marginal return in success rate and component specialization via cosine similarity