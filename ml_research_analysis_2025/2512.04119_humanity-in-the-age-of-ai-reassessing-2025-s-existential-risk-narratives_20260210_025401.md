---
ver: rpa2
title: 'Humanity in the Age of AI: Reassessing 2025''s Existential-Risk Narratives'
arxiv_id: '2512.04119'
source_url: https://arxiv.org/abs/2512.04119
tags:
- risk
- than
- speculative
- level
- intelligence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper challenges 2025\u2019s widespread existential-risk\
  \ narratives about AI, arguing that fears of superintelligent AI destroying humanity\
  \ lack empirical support. It examines the \u201Cintelligence explosion\u201D hypothesis\
  \ proposed by Good (1965) and Bostrom (2014), concluding that despite 60 years of\
  \ effort and trillions in investment, no evidence of sustained recursive self-improvement\
  \ or autonomous strategic awareness exists."
---

# Humanity in the Age of AI: Reassessing 2025's Existential-Risk Narratives

## Quick Facts
- **arXiv ID:** 2512.04119
- **Source URL:** https://arxiv.org/abs/2512.04119
- **Reference count:** 4
- **Primary result:** The paper challenges 2025's widespread existential-risk narratives about AI, arguing that fears of superintelligent AI destroying humanity lack empirical support.

## Executive Summary
This paper challenges 2025's widespread existential-risk narratives about AI, arguing that fears of superintelligent AI destroying humanity lack empirical support. It examines the "intelligence explosion" hypothesis proposed by Good (1965) and Bostrom (2014), concluding that despite 60 years of effort and trillions in investment, no evidence of sustained recursive self-improvement or autonomous strategic awareness exists. Current models remain powerful but narrow statistical tools, not emergent superintelligences. The paper critiques the conflation of observable harms (job displacement, bias, power concentration) with speculative ones (superintelligence misalignment), proposing a formal risk hierarchy to distinguish between them.

## Method Summary
The paper employs a critical analysis of existing AI risk narratives, comparing empirical evidence against theoretical predictions. It examines scaling laws, architectural developments, and investment patterns to evaluate the intelligence explosion hypothesis. The methodology includes a formal risk hierarchy classification system distinguishing between Level 1 (observable, tractable harms) and Level 2 (speculative, intractable threats). The analysis draws on financial data, benchmark performance metrics, and regulatory discourse analysis to support its arguments.

## Key Results
- No empirical evidence exists for sustained recursive self-improvement or autonomous strategic awareness in AI systems despite 60 years of research and massive investment
- The "intelligence explosion" hypothesis fails empirical tests, with current models showing predictable power-law scaling rather than accelerating returns
- The existential-risk narrative functions as an ideological distraction from observable harms like surveillance capitalism and compute concentration by a few dominant firms

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The existential-risk narrative functions as an ideological distraction from observable power consolidation.
- **Mechanism:** By focusing discourse on hypothetical Level 2 threats (superintelligence misalignment), public and regulatory attention is diverted from Level 1 harms (surveillance capitalism, compute concentration). This allows dominant firms to position themselves as the sole safety gatekeepers, insulating them from antitrust or privacy regulation.
- **Core assumption:** Assumption: Regulatory resources and public attention are zero-sum; over-indexing on speculative risks depletes capacity for governance of present harms.
- **Evidence anchors:**
  - [abstract] "...the existential-risk thesis functions primarily as an ideological distraction from the ongoing consolidation of surveillance capitalism..."
  - [section 6] "Dominant firms position themselves as the only credible safety actors, gaining immunity from antitrust and privacy regulation."
  - [corpus] Paper 51047 ("The Stories We Govern By") supports the mechanism where sociotechnical imaginaries shape governance constraints, noting how "existential risk proponents" frame debates to mobilize specific regulatory outcomes.
- **Break condition:** If regulators can simultaneously process Level 1 economic regulations and Level 2 safety protocols without resource conflict, the distraction mechanism fails.

### Mechanism 2
- **Claim:** Recursive self-improvement is structurally constrained by the human-dependence of the deep learning paradigm.
- **Mechanism:** Current frontier models operate via gradient-based optimization on fixed, human-defined loss landscapes. While models improve, the "search space, fitness function, and mutation operators remain human-defined," preventing the open-ended architectural redesign required for an intelligence explosion.
- **Core assumption:** Assumption: Future paradigms will continue to rely on human-supplied objectives and data curation, maintaining "exogenous" progress.
- **Evidence anchors:**
  - [section 2] "The deep-learning paradigm is fundamentally gradient-based and myopic... the relevant degrees of freedom are controlled by humans at every stage."
  - [section 2] "Every major architectural breakthrough of the 2023-2025 period originated with human researchers... No frontier model has ever proposed... a novel architectural paradigm."
  - [corpus] Paper 83203 ("AI Survival Stories") analyzes the "two premise argument" for AI risk; this paper provides empirical data refuting the capability premise (Premise 1) required for the threat.
- **Break condition:** If an architecture demonstrates the ability to autonomously redefine its own objective functions or search space without human intervention, the constraint is broken.

### Mechanism 3
- **Claim:** The "Digital Lettuce" dynamic creates an investment bubble dependent on depreciating hardware assets.
- **Mechanism:** Massive capital flows into GPUs ("digital lettuce") which depreciate rapidly due to obsolescence. This hardware spend, combined with lagging revenues and jobless growth, inflates a speculative bubble predicated on the *promise* of superintelligence rather than current utility.
- **Core assumption:** Assumption: Revenue growth will continue to lag behind the exponential growth in training/inference costs.
- **Evidence anchors:**
  - [abstract] "...inflated by the 2025 AI speculative bubble, where trillions in investments in rapidly depreciating 'digital lettuce' hardware... mask lagging revenues."
  - [section 5] "OpenAI at US$300 billion despite US$13.5 billion annual losses... stratospheric valuations reflect the anticipated value of exclusive access."
  - [corpus] Weak corpus support for the specific economic "digital lettuce" mechanism; corpus neighbors focus primarily on technical alignment and risk taxonomies rather than hardware depreciation economics.
- **Break condition:** If AI-driven revenue streams (e.g., enterprise software replacement) scale proportionally to hardware investment, the bubble mechanics stabilize.

## Foundational Learning

- **Concept: Good-Bostrom Risk Chain**
  - **Why needed here:** This is the theoretical target of the paper's critique. Understanding the links (Intelligence Explosion -> Superintelligence -> Misalignment) is required to evaluate the paper's empirical rebuttal.
  - **Quick check question:** Can you distinguish between the "Intelligence Explosion" (mechanism) and "Superintelligence" (outcome) in the classic risk model?

- **Concept: Power Law Scaling (Kaplan/Hoffmann)**
  - **Why needed here:** The paper uses scaling laws to refute the "fast take-off" hypothesis, arguing that performance improvements are predictable and smooth rather than discontinuous.
  - **Quick check question:** Does a power law relationship between compute and performance suggest accelerating (exponential) or diminishing returns on effort?

- **Concept: Surveillance Capitalism (Zuboff)**
  - **Why needed here:** This provides the alternative framework for AI risk. Instead of "misaligned robot god," the risk is "concentration of behavioral data and compute."
  - **Quick check question:** In this framework, is the "product" the AI capability or the behavioral prediction derived from user interaction?

## Architecture Onboarding

- **Component map:** Level 1 Risks (The Engine) -> Level 2 Risks (The Mirage) -> The Bubble (The Fuel)

- **Critical path:**
  1. **Audit Evidence:** classify a risk claim as Level 1 or Level 2 based on empirical observation (Table I).
  2. **Check Costs:** Analyze if capability gains require rising marginal costs (human/R&D) or falling costs (autonomy).
  3. **Trace Power:** If a risk narrative suggests only a few firms can solve it, flag as potential regulatory capture.

- **Design tradeoffs:**
  - **Governance Focus:** Allocating resources to Level 1 (high certainty, high tractability) vs. Level 2 (low certainty, near-zero tractability).
  - **Investment Strategy:** Betting on sustained exponential growth (bubble logic) vs. diminishing returns (scaling law logic).

- **Failure signatures:**
  - **Conflation:** Treating bias/hallucination as evidence of emerging superintelligence rather than statistical artifacts.
  - **Epistemic Extortion:** Accepting present harms as the price of avoiding future apocalypse.
  - **Rising Marginal Costs:** If R&D spend accelerates while capability gains (e.g., MMLU) decelerate (Figure 2), the sustainability of the progress curve is compromised.

- **First 3 experiments:**
  1. **Reproduction of Figure 2:** Verify the diminishing returns ratio (R&D Growth vs. Capability Gain) using public benchmark data and investment reports.
  2. **Architecture Provenance Test:** Analyze recent frontier model papers (2023-2025) to confirm the ratio of human-designed vs. model-designed architectural innovations.
  3. **Risk Classification Audit:** Take 5 current AI news headlines and classify them into Table I's hierarchy to determine resource allocation priority.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can AI scaling laws exhibit a sudden phase transition or "fast take-off" despite the smooth, predictable trajectories observed between 2023 and 2025?
- Basis in paper: [explicit] The author acknowledges that "The 'no inflection point observed' conclusion is based on data through 2025, but this doesn't rule out future acceleration."
- Why unresolved: Current data shows power-law scaling without discontinuity, but proponents argue that a critical capability threshold for recursive improvement has simply not yet been reached.
- What evidence would resolve it: Observation of a sharp, discontinuous leap in model capability (an inflection point) that occurs independently of proportional increases in human-controlled compute or data.

### Open Question 2
- Question: To what extent does the public discourse on existential risk (Level 2) causally impede the regulation of observable harms (Level 1)?
- Basis in paper: [inferred] The paper argues that the existential-risk thesis functions as an "ideological distraction" and results in "regulatory capture," but does not provide a quantitative mechanism for how this distraction directly blocks specific labor or privacy laws.
- Why unresolved: Establishing a direct causal link between cultural narratives (x-risk) and legislative inaction (on surveillance/labor) requires comparative political analysis not present in the paper.
- What evidence would resolve it: Comparative studies showing that legislative bodies exposed to high levels of existential-risk discourse are significantly slower to enact antitrust or labor protections than those focused on material harms.

### Open Question 3
- Question: Will the "digital lettuce" investment bubble deflate without producing a sustainable revenue model, or will it consolidate into a permanent oligopoly?
- Basis in paper: [inferred] The paper highlights lagging revenues and depreciating hardware ("digital lettuce") to critique the bubble, but implies the crash is inevitable without confirming the long-term economic equilibrium.
- Why unresolved: While valuations are stratospheric, it remains unclear if the consolidation of computational power will eventually yield the monopoly rents sufficient to sustain these investments.
- What evidence would resolve it: Long-term financial data showing whether major AI labs can close the gap between R&D expenditures and realized revenues without relying on speculative investment capital.

## Limitations

- The paper's risk hierarchy (Level 1 vs. Level 2) relies on binary classification of empirical evidence, but the boundary between "observed" and "hypothetical" risks may be contested in practice.
- The "digital lettuce" bubble mechanism lacks strong corpus validation, with limited supporting evidence for the specific depreciation economics claim.
- The argument against intelligence explosion assumes current deep learning paradigms will remain dominant, but doesn't fully explore whether alternative architectures could break the gradient-based constraint.

## Confidence

- **High confidence:** Claims about observed Level 1 harms (job displacement, bias, power concentration) and the lack of empirical evidence for sustained recursive self-improvement over 60 years.
- **Medium confidence:** The surveillance capitalism distraction mechanism, supported by theoretical frameworks and case studies, though empirical validation of resource diversion effects would strengthen the claim.
- **Low confidence:** The specific economic bubble mechanics of "digital lettuce" and the assertion that no frontier model has ever proposed novel architectural paradigms without human intervention.

## Next Checks

1. **Independent audit of investment-revenue data:** Verify the R&D growth vs. capability gain ratios using multiple independent data sources beyond OpenAI, including Anthropic, Google DeepMind, and open-source model development costs.
2. **Longitudinal governance analysis:** Track actual regulatory resource allocation over 2020-2025 to empirically test whether attention to speculative AI risks has displaced governance of immediate harms like privacy and antitrust enforcement.
3. **Alternative paradigm survey:** Systematically catalog emerging AI architectures (quantum-inspired, neuromorphic, energy-based models) to assess whether any demonstrate autonomy in objective function definition or search space exploration.