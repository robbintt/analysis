---
ver: rpa2
title: 'Parsing the Switch: LLM-Based UD Annotation for Complex Code-Switched and
  Low-Resource Languages'
arxiv_id: '2506.07274'
source_url: https://arxiv.org/abs/2506.07274
tags:
- head
- syntactic
- deprel
- upos
- form
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces the BiLingua Parser, an LLM-based annotation\
  \ pipeline for producing Universal Dependencies (UD) annotations in code-switched\
  \ text. The parser combines few-shot prompting with expert review to handle syntactic\
  \ structure in Spanish-English and Spanish-Guaran\xED data."
---

# Parsing the Switch: LLM-Based UD Annotation for Complex Code-Switched and Low-Resource Languages

## Quick Facts
- arXiv ID: 2506.07274
- Source URL: https://arxiv.org/abs/2506.07274
- Reference count: 21
- Introduces LLM-based pipeline for UD annotation in code-switched text, achieving up to 95.29% LAS after expert revision

## Executive Summary
This paper introduces the BiLingua Parser, an LLM-based annotation pipeline for producing Universal Dependencies (UD) annotations in code-switched text. The parser combines few-shot prompting with expert review to handle syntactic structure in Spanish-English and Spanish-Guaraní data. Two annotated datasets are released, including the first Spanish-Guaraní UD-parsed corpus. Linguistic analysis reveals that subjects (nsubj) are frequent switch points, especially in English-to-Spanish segments, and that Spanish-Guaraní exhibits higher syntactic variation at switch boundaries. Experimental results show the parser achieves up to 95.29% labeled attachment score (LAS) after expert revision, significantly outperforming baselines and multilingual parsers.

## Method Summary
The BiLingua Parser uses GPT-4.1 via OpenAI API with temperature=0, top_p=1, and max_tokens=3000. The pipeline employs few-shot prompting with detailed UD guidelines, outputting CoNLL-like format with ID, FORM, LANG, LEMMA, UPOS, HEAD ID, HEAD, and DEPREL. Expert review by native speakers follows LLM annotation. The approach is evaluated on filtered subsets of the Miami Spanish-English Corpus (2,837 CSW sentences) and Spanish-Guaraní data (1,140 CSW sentences), with LAS computed using CoNLL 2018 script and semantically similar tag grouping. Baseline comparison uses UDSL parser with bert-base-multilingual-cased.

## Key Results
- BiLingua Parser achieves up to 95.29% LAS after expert revision, outperforming UDSL baseline (14.71%) and multilingual parsers
- Subjects (nsubj) are the most frequent switch points, especially in English-to-Spanish transitions
- Spanish-Guaraní data shows higher syntactic variation at switch boundaries compared to Spanish-English
- First release of Spanish-Guaraní UD-annotated corpus

## Why This Works (Mechanism)
The approach leverages LLMs' strong syntactic reasoning and instruction-following capabilities when provided with comprehensive UD guidelines and few-shot examples. By combining automated parsing with expert review, the pipeline achieves high accuracy while maintaining linguistic validity. The few-shot prompting approach allows the model to learn from concrete examples rather than relying solely on zero-shot reasoning about UD guidelines.

## Foundational Learning
- Universal Dependencies framework: Standardized syntactic annotation scheme needed for cross-linguistic comparison and computational analysis
- Code-switching phenomena: Language alternation within sentences requires parsers to handle syntactic discontinuity and mixed-language dependencies
- Labeled Attachment Score (LAS): Primary evaluation metric measuring dependency arc and label accuracy; critical for assessing parser quality
- Few-shot prompting: Providing examples alongside instructions to guide LLM behavior; essential for complex annotation tasks
- Morphotactics: How morphemes combine in words; crucial for agglutinative languages like Guaraní that require morphological segmentation

## Architecture Onboarding
- Component map: Input text -> Few-shot prompt template -> GPT-4.1 API -> CoNLL output -> Expert review -> Final annotations
- Critical path: LLM annotation followed by expert revision is essential; fully automated output shows lower but still competitive accuracy
- Design tradeoffs: Expert review ensures quality but reduces automation; fewer examples could enable easier replication but may reduce accuracy
- Failure signatures: Inconsistent handling of repetitions/disfluencies; morphologically complex Guaraní tokens not split; occasional mislabeling of auxiliaries/modals
- First experiments: 1) Test BiLingua Parser on different CSW language pair (e.g., Hindi-English) 2) Vary number/quality of few-shot examples to measure impact 3) Compare fully automated output against expert-revised version

## Open Questions the Paper Calls Out
- To what extent are code-switches at the root/verb level truly more permissible than predicted by linguistic theories, versus reflecting parser errors?
- How can UD evaluation metrics be redesigned to systematically accommodate linguistically plausible annotation variants in code-switched text?
- What mechanisms can improve LLM consistency in handling repetitions, ellipsis, and functional verb annotations?
- To what extent does integrating language-specific morphological analyzers improve UD annotation accuracy for low-resource, agglutinative languages in CSW settings?

## Limitations
- Results limited to Spanish-English and Spanish-Guaraní language pairs; generalizability to other CSW combinations unknown
- Reliance on expert revision for achieving peak accuracy suggests full automation remains challenging
- Exact few-shot prompt examples not provided, creating uncertainty about reproducibility
- Morphological segmentation not performed, potentially limiting accuracy for agglutinative languages

## Confidence
- Reported accuracy figures (95.29% LAS): High - based on gold-standard annotations and consistent evaluation methodology
- Linguistic insights (switch point patterns): Medium - plausible but would benefit from larger, more diverse datasets
- Generalizability to other CSW pairs: Low - results limited to two specific language combinations

## Next Checks
1. Test BiLingua Parser on a different code-switched language pair (e.g., Hindi-English) to assess robustness and generalizability
2. Perform ablation studies by varying the number and quality of few-shot examples to quantify their impact on parsing accuracy
3. Evaluate fully automated output (without expert revision) on a held-out test set to establish realistic performance expectations for unsupervised deployment