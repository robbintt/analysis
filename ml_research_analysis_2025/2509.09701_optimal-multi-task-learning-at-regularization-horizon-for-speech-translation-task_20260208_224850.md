---
ver: rpa2
title: Optimal Multi-Task Learning at Regularization Horizon for Speech Translation
  Task
arxiv_id: '2509.09701'
source_url: https://arxiv.org/abs/2509.09701
tags:
- regularization
- consistency
- speech
- translation
- r-drop
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the data scarcity problem in end-to-end speech
  translation by exploring multi-task learning with machine translation. The authors
  formulate multi-task learning from a regularization perspective and systematically
  investigate consistency regularization (across modalities) and R-drop (within modalities)
  to enhance model performance.
---

# Optimal Multi-Task Learning at Regularization Horizon for Speech Translation Task

## Quick Facts
- arXiv ID: 2509.09701
- Source URL: https://arxiv.org/abs/2509.09701
- Reference count: 15
- Primary result: Demonstrates that optimal speech translation performance lies on a "regularization horizon" where consistency regularization, R-drop, and MT loss coefficient are jointly tuned

## Executive Summary
This paper addresses the data scarcity problem in end-to-end speech translation by exploring multi-task learning with machine translation. The authors formulate multi-task learning from a regularization perspective and systematically investigate consistency regularization (across modalities) and R-drop (within modalities) to enhance model performance. By unifying these three regularization sources, they introduce the concept of a "regularization horizon" - an optimal parameter region where model performance peaks. Experiments on the MuST-C dataset show that tuning hyperparameters within this horizon achieves near state-of-the-art performance across four languages (German, Spanish, French, and Italian).

## Method Summary
The method employs a HuBERT base encoder with 2-layer convolutional subsampling followed by a 6-layer encoder-decoder Transformer (~155M parameters). Multi-task learning is formulated with three regularization sources: consistency regularization between speech and text modalities (cross-modal), R-drop consistency within the same modality, and the machine translation loss coefficient. The model is trained with cross-entropy loss plus weighted KL divergence terms, using Adam optimizer with learning rate warmup and inverse square root decay. Hyperparameter tuning focuses on finding the optimal "regularization horizon" where performance peaks.

## Key Results
- Applying consistency constraints at the softmax output layer is more effective than at embedding layers, offering greater freedom for optimization
- The MT loss coefficient acts as a regularization control rather than just a task weight, with lower values increasing regularization
- Performance peaks on a "regularization horizon" where total regularization combines all three sources optimally
- BLEU scores competitive with state-of-the-art methods across German, Spanish, French, and Italian language pairs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Applying consistency constraints (KL divergence) closer to the output layer is more effective than applying them at hidden layers.
- Mechanism: Constraints at the softmax output (probability distributions) allow the model greater freedom to optimize intermediate internal representations, whereas constraints on embeddings force the model to align representations tightly early in the forward pass, potentially limiting the model's capacity to learn optimal features for the primary speech translation task.
- Core assumption: Aligning the output distributions is more critical for task performance than forcing strict point-wise alignment of hidden vector magnitudes.
- Evidence anchors:
  - [Section 3.1]: "Enforcing consistency between embeddings that are closer to the final output layer results in better performance... offers most freedom for the model to optimize."
  - [Table 1]: Shows `softmax-KL` outperforming `enc-MSE` and `logits-MSE`.

### Mechanism 2
- Claim: The auxiliary MT task's loss coefficient ($\alpha_t$) functions as a regularization control rather than a simple task weight.
- Mechanism: Reducing $\alpha_t$ increases the relative strength of consistency regularization. It treats the text input as a "corrupted" version of the speech input; lowering its direct supervision weight forces the model to rely more on the consistency constraint, thereby increasing the total regularization effect to prevent overfitting.
- Core assumption: The text modality in an MTL framework can be interpreted as a perturbation or corrupted view of the speech modality for the purpose of generalization.
- Evidence anchors:
  - [Section 4.1]: "A lower $\alpha_t$ value yields less optimal outputs... effectively increasing the regularization."
  - [Table 3]: Shows BLEU scores peaking at lower $\alpha_t$ (e.g., 0.1 or 0.5) rather than 1.0.

### Mechanism 3
- Claim: Optimal performance lies on a "Regularization Horizon," a specific contour in the high-dimensional hyperparameter space defined by the total regularization strength.
- Mechanism: Three distinct sources—consistency regularization ($\alpha_{cr}$), R-drop ($\alpha_{rd}$), and the MT coefficient ($\alpha_t$)—contribute to a unified "Total Regularization" ($R$). The model peaks when $R$ is optimal; combining these methods saturates because they enforce similar consistency from different "views" (modalities or dropout states).
- Core assumption: The relationship between performance and the combination of these regularization parameters can be approximated linearly in the over-regularized regime.
- Evidence anchors:
  - [Section 4.2]: "The monotonicity decreasing plots... indicates the data... already has large enough regularization."
  - [Figure 3]: Shows BLEU collapsing onto a single curve against $R$.

## Foundational Learning

- **Concept: Multi-Task Learning (MTL) with Shared Decoders**
  - Why needed here: The architecture relies on a shared decoder for both Speech-to-Text (ST) and Machine Translation (MT). You must understand how auxiliary data (text-text) feeds into the same parameters as the primary task (speech-text) to grasp why consistency regularization is necessary.
  - Quick check question: Does the model share the encoder, the decoder, or both between the ST and MT tasks?

- **Concept: Consistency Regularization & R-drop**
  - Why needed here: These are the core tools used to bridge the modality gap. You need to distinguish between *Cross-modal* consistency (Speech vs. Text distribution) and *Within-modal* consistency (Speech vs. Speech with dropout).
  - Quick check question: R-drop minimizes the KL divergence between two outputs of the *same* input but with different dropout masks—True or False?

- **Concept: The Modality Gap**
  - Why needed here: The paper explicitly addresses the gap between speech and text embeddings. Understanding that raw audio features and text tokens occupy different spaces explains why simple MTL isn't enough and why explicit alignment (consistency) is applied.
  - Quick check question: Why does the paper suggest applying constraints at the *softmax* layer rather than the raw embedding layer to bridge this gap?

## Architecture Onboarding

- **Component map:**
  - Raw audio waveform -> HuBERT base encoder -> Conv sub-sampler -> Shared encoder
  - Tokenized text -> Shared encoder
  - Shared encoder -> Shared decoder -> Softmax output
  - Shared decoder -> Softmax output (two dropout states for R-drop)

- **Critical path:**
  1. **Inputs:** Raw audio waveform ($x_s$) + Tokenized Text ($x_t$)
  2. **Encoding:** Audio goes through HuBERT + Sub-sampler; Text goes through Shared Encoder
  3. **Decoding:** Both modalities feed into the Shared Decoder
  4. **Constraint Application:** KL Divergence is calculated at the **Softmax output** between ($x_s$, $x_t$) for Consistency and ($x_s$, $x_s$) for R-drop

- **Design tradeoffs:**
  - **Constraint Location:** Output layer (better performance, looser alignment) vs. Embedding layer (strict alignment, potentially lower BLEU)
  - **R-drop vs. Consistency:** R-drop is more effective in low-data (base) settings; Consistency Regularization bridges the gap when large external MT data is available
  - **Saturation:** Combining R-drop and Consistency Regularization yields diminishing returns compared to tuning a single method due to the saturation of total regularization

- **Failure signatures:**
  - **Over-regularization:** BLEU drops monotonically as dropout increases or $\alpha_t$ decreases too far (entering the "over-regularized regime")
  - **Saturation:** Adding Consistency Regularization to a strong R-drop setup results in "surprisingly stable" (flat) performance rather than additive gains

- **First 3 experiments:**
  1. **Baseline Sanity Check:** Train with only Cross-Entropy ($\alpha_{cr}=0, \alpha_{rd}=0$) to establish a floor on the MuST-C dataset
  2. **Layer Ablation:** Compare `enc-MSE` vs. `softmax-KL` consistency regularization to validate that output-layer constraints provide the "freedom" needed for higher BLEU scores
  3. **Find the Horizon:** Fix $\alpha_{cr}$ and $\alpha_{rd}$, then sweep $\alpha_t$ (e.g., 1.0 to 0.0) to verify that performance peaks at a non-trivial coefficient, confirming its role as a regularizer

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What is the precise non-linear interaction between consistency regularization and R-drop that leads to performance saturation?
- **Basis in paper:** [explicit] The authors observe that combining the two regularization methods "does not simply add up but rather saturates," analogizing it to a "3-time R-drop," but do not isolate the mechanism causing this ceiling.
- **Why unresolved:** The paper empirically demonstrates the saturation effect but does not explore if dynamic weighting or alternative divergence metrics could overcome this limit.
- **What evidence would resolve it:** Experiments varying the inter-dependency of loss weights or utilizing orthogonal regularization constraints to see if the saturation point can be extended.

### Open Question 2
- **Question:** Does incorporating consistency constraints within the auxiliary MT task improve the regularization horizon for the primary ST task?
- **Basis in paper:** [explicit] The authors state, "We omit the exploration of consistency within the MT task as we considered optimizing the MT task as being irrelevant to maximizing ST performance."
- **Why unresolved:** It remains unclear if regularizing the MT task could act as a form of indirect regularization for the shared encoder/decoder, potentially shifting the optimal horizon.
- **What evidence would resolve it:** Ablation studies applying R-drop specifically to the MT forward pass and measuring the impact on the ST BLEU scores and the regularization horizon.

### Open Question 3
- **Question:** What is the analytic form of the total regularization function $R$ beyond the simplified linear approximation?
- **Basis in paper:** [explicit] The authors note that "Determining the analytic form of $f$ is a formidable task" and rely on a linear Taylor expansion which they admit is "evidently an oversimplification."
- **Why unresolved:** The linear model assumes independence between parameters that may interact non-linearly, potentially misrepresenting the true shape of the "regularization horizon."
- **What evidence would resolve it:** A theoretical derivation or higher-order regression analysis that accounts for interaction terms between $\alpha_{cr}$, $\alpha_{rd}$, and $\alpha_t$.

## Limitations

- The linear approximation of total regularization may not capture complex interactions between the three regularization sources
- The specific effectiveness of softmax-layer consistency constraints may be architecture-dependent and not generalize to deeper transformers
- The paper does not address potential catastrophic forgetting when switching between speech and text inputs during MTL training

## Confidence

- **High Confidence**: The observation that consistency constraints at the softmax layer outperform those at embedding layers (supported by ablation studies in Table 1) and the general framework of combining consistency regularization with R-drop for multi-task learning in speech translation.
- **Medium Confidence**: The "regularization horizon" concept as a universal principle for multi-task learning optimization, given it's demonstrated only on MuST-C with specific architectures and regularization coefficients.
- **Low Confidence**: The interpretation of the MT loss coefficient ($\alpha_t$) as a regularization parameter rather than a simple task weight, as this requires the assumption that text input is a "corrupted view" of speech input.

## Next Checks

1. **Architecture Transfer**: Test whether the regularization horizon concept holds when applied to a different architecture (e.g., 12+12 encoder-decoder, or models with separate modality-specific encoders) on the same MuST-C dataset.

2. **Interaction Analysis**: Conduct a full factorial sweep of $\alpha_{cr}$, $\alpha_{rd}$, and $\alpha_t$ to empirically verify whether their combined effect on performance is additive or shows non-linear interactions that would invalidate the simple $R$ approximation.

3. **Cross-Domain Generalization**: Apply the same regularization framework and horizon search to a different multi-task learning problem (e.g., vision-language tasks) to determine if the regularization horizon is a general principle or specific to speech translation.