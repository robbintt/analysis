---
ver: rpa2
title: Evaluating Quality of Gaming Narratives Co-created with AI
arxiv_id: '2509.04239'
source_url: https://arxiv.org/abs/2509.04239
tags:
- quality
- story
- game
- dimensions
- narrative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a structured methodology to evaluate AI-generated
  game narratives, leveraging the Delphi study structure with a panel of narrative
  design experts. The approach synthesizes story quality dimensions from literature
  and expert insights, mapping them into the Kano model framework to understand their
  impact on player satisfaction.
---

# Evaluating Quality of Gaming Narratives Co-created with AI

## Quick Facts
- arXiv ID: 2509.04239
- Source URL: https://arxiv.org/abs/2509.04239
- Authors: Arturo Valdivia; Paolo Burelli
- Reference count: 22
- Primary result: Expert-validated framework identifies 25 story quality dimensions for AI-generated game narratives, with 78% rated at least 3.5/5 importance

## Executive Summary
This paper proposes a structured methodology to evaluate AI-generated game narratives by combining expert consensus through a Delphi study with satisfaction mapping via the Kano model. The approach identifies and validates 25 story quality dimensions (SQDs) that narrative design experts consider important for assessing AI-generated content. By classifying these dimensions according to their impact on player satisfaction, the framework helps game developers prioritize quality aspects when co-creating narratives with generative AI systems.

## Method Summary
The methodology follows a three-stage framework: (1) compile 23 initial story quality dimensions from literature review of Yang & Jin (2024), (2) validate via ranking-type Delphi study with n=10 experts using iterative rounds and controlled feedback, and (3) classify dimensions through Kano model using functional/dysfunctional question pairs. Experts included narrative designers and game practitioners working with LLMs from AAA studios, free-to-play, and indie backgrounds across Europe and US, with balanced gender representation. Importance was measured using Likert 1-5 scales, and dimensions were categorized into Kano types (Must-have, One-dimensional, Attractive, Indifferent, Reverse).

## Key Results
- All 25 identified story quality dimensions were deemed important by experts, with 78% receiving median importance scores of at least 3.5 out of 5
- 57% of dimensions classified as "One-dimensional" (satisfaction proportional to presence), 26% as "Must-have" requirements, and 13% as "Attractive" features
- Two additional dimensions - voice and genre alignment - emerged from expert feedback as critical but missing from initial literature review

## Why This Works (Mechanism)

### Mechanism 1: Delphi Consensus Refinement
Iterative expert feedback converges on relevant quality dimensions more reliably than literature synthesis alone. Anonymous expert questionnaires with aggregated feedback distribution and individual opinion revision reduce social pressure while capturing domain expertise. Core assumption is that narrative design expertise transfers to AI-generated content evaluation contexts. Break condition occurs when experts lack AI-narrative experience, panel homogeneity exists, or fewer than 7-10 experts participate.

### Mechanism 2: Kano Classification for Prioritization
Classifying quality dimensions by satisfaction impact enables prioritization when full coverage is infeasible. Functional/dysfunctional question pairs determine category assignment, guiding resource allocation. Must-haves prevent dissatisfaction while One-dimensional features scale satisfaction proportionally. Core assumption is that player satisfaction follows Kano patterns for narrative attributes. Break condition occurs when narrative dimensions don't fit Kano categories or genre-specific variation invalidates unified classification.

### Mechanism 3: Literature-to-Expert Gap Identification
Expert panels identify dimensions missing from academic literature through exposure to practical contexts. Literature-derived initial list combined with practitioner experience surfaces tacit knowledge not yet formalized. Core assumption is that expert practitioners hold knowledge not captured in published evaluation surveys. Break condition occurs when literature is already comprehensive or dimensions are domain-specific and don't generalize.

## Foundational Learning

- **Delphi Method**: Core methodology for expert validation; understanding iteration, anonymity, and consensus criteria is essential to interpret results. Quick check: Can you explain why anonymity matters for reducing bias in expert panels?
- **Kano Model**: Framework for classifying how features affect satisfaction; distinguishes between must-haves, performance attributes, and delighters. Quick check: What is the difference between a "Must-have" and a "One-dimensional" attribute in terms of satisfaction impact?
- **Story Quality Dimensions (SQDs)**: The 25 evaluation criteria form the vocabulary for assessment; knowing them enables practical application. Quick check: Why might "voice" be distinct from "naturalness" as an evaluation criterion?

## Architecture Onboarding

- **Component map**: Literature synthesis -> 23 initial SQDs -> Delphi validation -> Expert ranking -> Kano classification -> Category assignment -> Prioritized quality dimensions
- **Critical path**: 1. Compile initial SQD list from literature 2. Recruit 8-12 diverse experts 3. Round 1: Importance ratings + Kano questions 4. Analyze consensus; identify gaps 5. Classify via Kano questions 6. Map to player satisfaction expectations
- **Design tradeoffs**: Panel size vs. diversity (10 experts meets guidance but may miss perspectives); First-order vs. second-order SQDs (25 dimensions vs. granular sub-dimensions); Expert vs. player classification (experts predict vs. players confirm)
- **Failure signatures**: Low consensus (high variance in rankings); "Indifferent" classifications for highly important dimensions; Emergent dimensions duplicating existing ones
- **First 3 experiments**: 1. Validate Kano categories with players via large-scale survey 2. Apply framework to single genre to test context variation 3. Test LLM judge reliability by comparing human vs. LLM assessments

## Open Questions the Paper Calls Out

1. How do players' Kano classifications of story quality dimensions align with the expert-derived classifications? The authors intend to complement the study with a large-scale survey to capture players' preferences and compare against expert data.

2. How can the emergent dimensions of "voice" and "genre alignment" be operationalized into quantitative or automated evaluation metrics? The paper identifies these as critical but does not define measurement approaches for LLM-as-a-Judge contexts.

3. Does the inclusion of identified Story Quality Dimensions improve the reliability of LLM-as-a-Judge evaluations? The paper asks what such a judge should evaluate but does not experimentally verify if providing this framework results in more accurate assessments.

## Limitations
- Expert panel size (n=10) may not capture full diversity of gaming contexts despite meeting methodological recommendations
- No player validation of Kano classifications - expert predictions remain unverified against actual player responses
- Genre-specific variation in dimension importance is not explored, though narrative quality expectations likely vary by context

## Confidence
- **High confidence**: Methodology framework is sound and builds on established Delphi and Kano model approaches
- **Medium confidence**: Expert consensus results are reliable given panel diversity and iterative process, but sample size limits generalizability
- **Medium confidence**: Kano classifications represent expert predictions rather than empirically validated player satisfaction patterns

## Next Checks
1. Conduct player surveys using the same Kano functional/dysfunctional questions to validate expert predictions against actual satisfaction patterns
2. Apply framework to a specific game genre (e.g., RPGs or narrative adventures) to test whether dimension priorities shift by context
3. Test LLM judge reliability by having the same 25 SQDs evaluated by both human experts and an LLM, comparing consistency and identifying gaps in automated assessment