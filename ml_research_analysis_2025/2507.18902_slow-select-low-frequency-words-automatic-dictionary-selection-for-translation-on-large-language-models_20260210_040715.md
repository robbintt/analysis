---
ver: rpa2
title: 'SLoW: Select Low-frequency Words! Automatic Dictionary Selection for Translation
  on Large Language Models'
arxiv_id: '2507.18902'
source_url: https://arxiv.org/abs/2507.18902
tags:
- translation
- slow
- dictionary
- language
- english
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Automatic Dictionary Selection (ADS)
  task to optimize dictionary usage for LLM-based translation. The proposed method,
  SLoW (Select Low-frequency Words!), selects dictionaries with lower frequency words,
  improving translation quality while reducing token usage.
---

# SLoW: Select Low-frequency Words! Automatic Dictionary Selection for Translation on Large Language Models

## Quick Facts
- arXiv ID: 2507.18902
- Source URL: https://arxiv.org/abs/2507.18902
- Authors: Hongyuan Lu; Zixuan Li; Zefan Zhang; Wai Lam
- Reference count: 14
- One-line primary result: SLoW selects low-frequency dictionary words, improving translation quality while reducing token usage across 100 languages.

## Executive Summary
This paper introduces the Automatic Dictionary Selection (ADS) task to optimize dictionary usage for LLM-based translation. The proposed method, SLoW (Select Low-frequency Words!), selects dictionaries with lower frequency words, improving translation quality while reducing token usage. Evaluated on 100 languages from FLORES, SLoW outperforms strong baselines like noun/verb/adjective dictionaries and even surpasses full dictionary usage in some cases. Across models (ChatGPT, Llama, DeepSeek), SLoW improves translation performance in most language pairs, with many improvements exceeding 20 COMET points.

## Method Summary
SLoW automatically selects a subset of bilingual dictionary entries to maximize translation quality while constraining dictionary size. The method extracts source tokens present in the dictionary, looks up their frequencies using the wordfreq library, sorts dictionary entries by ascending frequency, and selects the top-V lowest-frequency entries. These selected dictionaries are then formatted into the translation prompt. The approach requires no access to LLM training data, using public resources for frequency estimation instead. The dictionary budget V is set to match the "Differ in Round-trip" baseline token count.

## Key Results
- SLoW outperforms high-frequency dictionary selection in 92/100 En-X pairs on ChatGPT and 99/100 on Llama
- SLoW improves translation performance in most language pairs across all tested models (ChatGPT, Llama, DeepSeek)
- Many language pairs show improvements exceeding 20 COMET points compared to baselines
- SLoW surpasses full dictionary baseline on specific language pairs (e.g., pbt_Arab: 0.803 vs 0.483 COMET)

## Why This Works (Mechanism)

### Mechanism 1: Low-Frequency Word Prioritization
LLMs learn high-frequency words better during pretraining, so low-frequency words receive weaker representations. Explicit dictionary mappings provide disproportionate benefit for these under-learned tokens.

### Mechanism 2: Proxy Frequency Estimation via Public Resources
External frequency databases (wordfreq) can substitute for actual LLM training data frequency statistics. Web-crawled word frequencies approximate the distributional properties of LLM pretraining data sufficiently for relative ranking.

### Mechanism 3: Noise Reduction via Selective Dictionary Injection
Using fewer, targeted dictionaries outperforms greedy full-dictionary injection. Excessive dictionary entries introduce distraction and consume context window capacity without proportional benefit, improving signal-to-noise ratio.

## Foundational Learning

- **Concept: Dictionary-based prompting for translation**
  - Why needed: SLoW operates within the dictionary-prompting paradigm
  - Quick check: Can you explain how a word-for-word dictionary is formatted and inserted into a translation prompt?

- **Concept: Word frequency distributions (Zipf's law)**
  - Why needed: The method relies on relative frequency ranking
  - Quick check: In a typical corpus, approximately what percentage of unique word types appear only once or very few times?

- **Concept: COMET/BLEU/chrF evaluation metrics**
  - Why needed: The paper reports improvements in these metrics
  - Quick check: Which metric (COMET or BLEU) is neural-based and considers semantic similarity?

## Architecture Onboarding

- **Component map**: Input -> Tokenizer/Matcher -> Frequency Lookup -> Sorter -> Selector -> Prompt Constructor -> LLM Inference
- **Critical path**: Frequency lookup → Sort → Select V entries → Prompt injection. Errors in frequency estimation propagate directly to selection quality.
- **Design tradeoffs**:
  - Dictionary budget V: Lower V saves tokens but may miss useful entries; paper sets V to match "Differ in Round-trip" baseline token count (~50-55% of full dictionary)
  - Frequency source: wordfreq is free but may not match LLM training distribution; custom frequency corpora could improve accuracy
  - English-centric frequency: Paper uses English frequency as proxy for all languages; this is a simplification
- **Failure signatures**:
  - SLoW underperforms full dictionary: Likely indicates frequency proxy is misaligned with actual model knowledge
  - SLoW matches or underperforms high-frequency baseline: Suggests frequency signal is not discriminative
  - Large variance across languages: Expected; low-resource languages show higher gains
- **First 3 experiments**:
  1. Replicate frequency estimation pipeline: Use wordfreq to rank tokens in a sample FLORES sentence
  2. Ablate V budget: Test SLoW at 25%, 50%, 75% of full dictionary size on 3 diverse language pairs
  3. Frequency source swap: Replace wordfreq with different frequency corpus for one language pair

## Open Questions the Paper Calls Out

1. **Optimal dictionary selection ratio**: The paper fixes V to match the "Differ in Round-trip" baseline but does not search for optimal selection threshold per language or direction.

2. **Frequency estimation quality**: How closely does web-resource-based frequency estimation approximate actual LLM training-data word frequencies? The authors lack access to proprietary training corpora.

3. **Task generalization**: Does ADS and SLoW generalize to other LLM tasks beyond machine translation, such as cross-lingual question answering or summarization?

4. **Mechanism of low-frequency advantage**: Why do low-frequency dictionaries outperform high-frequency dictionaries? Does this correlate with specific linguistic features like POS or morphological complexity?

## Limitations

- Frequency estimation relies on English word frequencies as proxy for LLM training data, which may not hold for non-English-centric language pairs
- Dictionary construction uses ChatGPT, which may introduce noise or errors that propagate through selection
- Study uses FLORES devtest from Wikipedia, limiting generalizability to other domains
- SLoW effectiveness varies across language families, with mixed results for certain languages like Telugu and Gujarati

## Confidence

**High Confidence**:
- Core mechanism of selecting low-frequency words over high-frequency ones is well-supported
- Excessive dictionary entries can distract LLMs (supported by specific cases where SLoW outperforms full dictionary)

**Medium Confidence**:
- Using public frequency resources as proxy for LLM training data frequency (relies on assumption that web corpus frequencies correlate with model pretraining)
- General superiority of SLoW over POS-tag-based selection (magnitude varies by language pair)

**Low Confidence**:
- Claim that SLoW would perform better than high-frequency selection for all possible language pairs and domains
- Assertion that frequency-based selection works uniformly across all model families

## Next Checks

1. **Frequency Proxy Validation**: Replicate frequency estimation pipeline using different frequency sources (Wikipedia word counts, Common Crawl) for 5 diverse language pairs to assess sensitivity of SLoW performance to frequency estimation quality.

2. **Domain Transfer Experiment**: Test SLoW on non-Wikipedia domains (technical documentation, social media text, literary works) for 3-5 language pairs to evaluate whether frequency-based selection strategy generalizes beyond FLORES Wikipedia corpus.

3. **V-Budget Sensitivity Analysis**: Conduct ablation study varying V from 25% to 200% of baseline budget for 10 diverse language pairs across different model families. Plot COMET improvement curve against token usage to identify optimal dictionary sizes.