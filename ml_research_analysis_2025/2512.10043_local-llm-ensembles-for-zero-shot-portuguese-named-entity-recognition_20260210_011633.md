---
ver: rpa2
title: Local LLM Ensembles for Zero-shot Portuguese Named Entity Recognition
arxiv_id: '2512.10043'
source_url: https://arxiv.org/abs/2512.10043
tags:
- llms
- ensemble
- entity
- https
- ensembles
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a novel three-step ensemble pipeline for zero-shot
  Named Entity Recognition (NER) in Portuguese using locally run Large Language Models
  (LLMs). The pipeline consists of entity extraction, voting, and disambiguation steps,
  leveraging multiple similarly capable models to outperform individual LLMs in four
  out of five Portuguese NER datasets.
---

# Local LLM Ensembles for Zero-shot Portuguese Named Entity Recognition

## Quick Facts
- arXiv ID: 2512.10043
- Source URL: https://arxiv.org/abs/2512.10043
- Reference count: 27
- Primary result: Three-step ensemble pipeline outperforms individual LLMs in 4/5 Portuguese NER datasets

## Executive Summary
This work introduces a novel three-step ensemble pipeline for zero-shot Named Entity Recognition (NER) in Portuguese using locally run Large Language Models (LLMs). The pipeline consists of entity extraction, voting, and disambiguation steps, leveraging multiple similarly capable models to outperform individual LLMs in four out of five Portuguese NER datasets. The method employs a heuristic to select optimal model combinations using minimal annotated data, and cross-dataset configurations generally outperform individual LLMs without requiring annotated data for the target task. This approach advances scalable, low-resource, and zero-shot NER by effectively combining multiple small LLMs without fine-tuning.

## Method Summary
The proposed method implements a three-step pipeline for zero-shot Portuguese NER: (1) Extraction—multiple LLMs extract entities independently using zero-shot prompts with task definition and entity type descriptions, outputting JSON; (2) Voting—all LLMs vote yes/no on each extracted entity, majority decides validity; (3) Disambiguation—for overlapping entities, LLMs choose among multiple-choice options (5 longest valid combinations + "N/A"), majority wins, ties broken by longest span. A configuration selection heuristic tests all valid combinations (odd-numbered voting/disambiguation ensembles) on a 100-sentence validation set, selecting the highest micro-F1 configuration. The method uses five Portuguese NER datasets across general, legal, geological, and historical domains, with temperature calibration per model for extraction (t=0 for LLaMA3/Mistral/Phi3, t=1.0 for Gemma2, t=1.5 for Qwen2) and t=0 for voting/disambiguation.

## Key Results
- Ensemble pipeline outperforms individual LLMs in 4 out of 5 Portuguese NER datasets
- Cross-dataset configuration transfer works without target-domain labeled data
- Configuration selection heuristic effective with ~100 sentence validation sample
- Micro-F1 scores improve across datasets when using optimal model combinations

## Why This Works (Mechanism)

### Mechanism 1: Complementary Knowledge Aggregation
- Claim: Combining multiple LLMs with diverse pre-training backgrounds captures entity types that individual models miss.
- Mechanism: Different models extract partially overlapping but non-identical entity sets; majority voting filters false positives while retaining entities that multiple models independently recognize.
- Core assumption: Models make partially independent errors—when one model fails to extract a valid entity, others may succeed.
- Evidence anchors:
  - [abstract]: "leveraging multiple similarly capable models to outperform individual LLMs in four out of five Portuguese NER datasets"
  - [section 5.1]: "they may complement each other's knowledge gaps to produce a final, more robust list of entities for the sentences"
  - [corpus]: EL4NER (arxiv:2505.23038) similarly uses multiple small LLMs for NER, suggesting generalizability of ensemble approaches for this task.
- Break condition: When entity types are simple enough that single models already perform well (e.g., HAREM's generic domain), ensemble gains diminish or reverse.

### Mechanism 2: Staged Refinement Pipeline
- Claim: The three-step pipeline (extraction → voting → disambiguation) progressively improves precision while maintaining recall.
- Mechanism: Extraction casts a wide net (high recall), voting filters invalid candidates (improves precision), disambiguation resolves span conflicts for coherent output.
- Core assumption: Different models may excel at different pipeline stages—extraction quality and verification quality are separable capabilities.
- Evidence anchors:
  - [section 3]: Pipeline explicitly decomposes NER into extraction, voting, and disambiguation steps with potentially different model sets.
  - [section 5.2]: Ablation shows voting removal causes significant drops across all datasets; disambiguation removal hurts GeoCorpus2 notably.
  - [corpus]: No directly comparable staged pipeline evidence found in corpus.
- Break condition: When voting models are systematically too permissive or too strict, or when disambiguation prompts become noisy from unfiltered extraction output.

### Mechanism 3: Configuration Selection with Minimal Supervision
- Claim: A small annotated sample (~100 sentences) can identify effective model combinations through exhaustive search.
- Mechanism: Evaluate all valid configurations (extraction combinations × odd-numbered voting ensembles × odd-numbered disambiguation ensembles) on validation sample; select highest micro-F1.
- Core assumption: The small validation set is sufficiently representative of test distribution.
- Evidence anchors:
  - [section 3.4]: "we employ a heuristic to choose optimal combinations of models to perform each of the pipeline steps" using "a small sample of the labeled data"
  - [section 5.3]: Heuristic effectiveness is "highly dependent on the validation set size and its representativeness"—best test configuration outperformed heuristic-selected configuration on HAREM.
  - [corpus]: No corpus evidence for this specific heuristic approach.
- Break condition: When validation sample is unrepresentative, or when cross-dataset transfer is needed without any labeled target data (Section 5.4 shows this can still work).

## Foundational Learning

- **Concept: Named Entity Recognition (NER) as Structured Prediction**
  - Why needed here: NER requires both identifying entity spans and classifying types—more complex than text classification; the pipeline architecture reflects this structure.
  - Quick check question: Why can't you simply treat NER as a text generation task with "list all entities" as the prompt?

- **Concept: Zero-shot In-Context Learning**
  - Why needed here: The entire method operates without fine-tuning; understanding prompt engineering and temperature calibration is essential.
  - Quick check question: What happens to model outputs when you increase temperature from 0 to 1.5?

- **Concept: Ensemble Diversity and Error Independence**
  - Why needed here: Ensemble gains depend on models making different mistakes; correlated errors reduce voting effectiveness.
  - Quick check question: If all five models were fine-tuned from the same base model, would you expect more or less ensemble benefit?

## Architecture Onboarding

- **Component map**:
  Input layer: Raw Portuguese text sentences
  → Extraction layer: Multiple LLMs (from candidate pool) with calibrated temperatures → JSON entity lists → span validation
  → Voting layer: Per-entity validation prompts → binary votes → majority aggregation
  → Disambiguation layer: Overlap graph construction → multiple-choice prompts → majority selection with tie-breaking (longest span)
  → Output: Unified entity list with resolved overlaps

- **Critical path**:
  1. Model calibration: Determine optimal extraction temperature per LLM using small labeled sample
  2. Configuration search: Enumerate combinations, evaluate on 100-sample validation set, select best
  3. Production inference: Apply selected configuration to new data

- **Design tradeoffs**:
  - More models in ensemble → better coverage but linear increase in GPU memory/inference time
  - Odd-numbered voting ensembles → prevent ties but reduce flexibility (even numbers require tie-breaking)
  - Specialized models per step vs. same models throughout → paper shows specialization helps (Phi3 excels at voting but not extraction)
  - LLM disambiguation vs. "longest span" heuristic → LLM helps for complex entity types; heuristic suffices for simpler cases

- **Failure signatures**:
  - Ensemble underperforms best single model → Check if validation set is representative; verify configuration search is not overfitting to validation noise
  - High false positive rate → Voting models may be too permissive; try stricter voting ensemble
  - Unresolved overlapping entities → Disambiguation prompts may be malformed; verify "N/A" option is included
  - Prohibitive inference time → Reduce ensemble size; implement aggressive caching of intermediate prompts

- **First 3 experiments**:
  1. Single-model baseline: Run each LLM individually through full pipeline on your dataset; establish floor/ceiling for ensemble comparison.
  2. Ablation by pipeline step: Remove voting step; then remove disambiguation step; quantify each component's contribution to final performance.
  3. Cross-dataset configuration transfer: Select ensemble on Dataset A, apply to Dataset B; measure if transfer beats individual models (replicating Section 5.4 finding that this often works).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the proposed ensemble pipeline maintain its performance advantages for zero-shot NER in languages other than Portuguese?
- Basis in paper: [explicit] The authors state, "In future work, we aim to explore NER ensembling for languages other than Portuguese, as our pipeline is not dependent on this language."
- Why unresolved: The study restricted evaluation to five Portuguese datasets, leaving cross-lingual generalization untested.
- What evidence would resolve it: Evaluation results of the identical pipeline on standard NER benchmarks across diverse languages (e.g., English, Spanish, or other low-resource languages).

### Open Question 2
- Question: Can a trained meta-classifier effectively replace the current majority-voting and heuristic-based selection for aggregating model outputs?
- Basis in paper: [explicit] The authors suggest, "Other methods for aggregating the outputs... One possibility would be to train a meta-classifier to aggregate the answers... similar to ensemble stacking."
- Why unresolved: The current method relies on heuristic selection and majority voting; learning-based aggregation remains unexplored.
- What evidence would resolve it: A comparative study measuring the performance and data efficiency of a stacking-based meta-classifier against the proposed heuristic on the same datasets.

### Open Question 3
- Question: Does utilizing a larger LLM as a "judge" for specific steps (voting or disambiguation) improve accuracy compared to the current ensemble of small models?
- Basis in paper: [explicit] The authors propose to "explore the use of larger models as judges in specific steps of the pipeline to use their processing capabilities to make final decisions."
- Why unresolved: The current pipeline strictly employs similarly sized, locally run models for all steps to ensure consumer GPU compatibility.
- What evidence would resolve it: Ablation studies showing performance changes when a larger model is swapped in for the voting or disambiguation phases.

## Limitations
- Configuration selection heuristic's effectiveness depends heavily on validation set representativeness
- Requires substantial GPU memory to run multiple large models simultaneously
- Assumes models make partially independent errors, which may not hold with shared pretraining

## Confidence

- **High Confidence**: The core finding that ensemble methods outperform individual models in four of five datasets (Section 5.1) is well-supported with statistical significance. The three-step pipeline architecture (extraction → voting → disambiguation) demonstrably improves performance over ablated versions (Section 5.2).
- **Medium Confidence**: The claim that cross-dataset configuration transfer can work without any target-domain labeled data (Section 5.4) is supported but requires careful validation of the validation set representativeness assumption.
- **Low Confidence**: The assertion that the 100-sentence validation heuristic will consistently identify optimal configurations across diverse domains and model combinations lacks strong empirical support beyond the tested Portuguese NER scenarios.

## Next Checks
1. **Validation Set Representativeness Test**: Systematically vary the size and composition of the validation set (from 10 to 500 sentences) and measure how configuration selection accuracy changes. This directly tests the paper's admission that "effectiveness is highly dependent on validation set size and representativeness."

2. **Error Correlation Analysis**: Measure pairwise model agreement on entity extraction outputs across all five datasets. Compute the correlation of false positive and false negative patterns to quantify whether models truly make independent errors—this tests the foundational assumption of ensemble effectiveness.

3. **Cross-Lingual Transfer Experiment**: Apply the same ensemble methodology to another language (e.g., Spanish or French) using the identical pipeline and configuration selection process. This tests whether the Portuguese-specific findings generalize to other languages with similar NER task complexity.