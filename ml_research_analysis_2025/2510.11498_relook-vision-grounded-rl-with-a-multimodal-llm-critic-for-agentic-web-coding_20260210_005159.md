---
ver: rpa2
title: 'ReLook: Vision-Grounded RL with a Multimodal LLM Critic for Agentic Web Coding'
arxiv_id: '2510.11498'
source_url: https://arxiv.org/abs/2510.11498
tags:
- code
- relook
- mllm
- arxiv
- critic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents ReLook, a vision-grounded reinforcement learning
  framework that closes the generate-diagnose-refine loop for front-end code generation
  by invoking a multimodal LLM as a tool. The agent actively generates code, captures
  temporal screenshots, and obtains visual feedback to iteratively improve.
---

# ReLook: Vision-Ground RL with a Multimodal LLM Critic for Agentic Web Coding

## Quick Facts
- **arXiv ID:** 2510.11498
- **Source URL:** https://arxiv.org/abs/2510.11498
- **Reference count:** 17
- **One-line primary result:** ReLook achieves 27.88/31.86 on ArtifactsBench-Lite with 7B/8B models, outperforming baselines via vision-grounded RL and Forced Optimization.

## Executive Summary
ReLook introduces a vision-grounded reinforcement learning framework for front-end code generation that integrates a multimodal LLM as an external critic. The system iteratively generates code, captures temporal screenshots, and leverages visual feedback to drive improvements. To address behavioral collapse, ReLook employs a strict acceptance rule—Forced Optimization—that only admits revisions resulting in monotonic improvement. The framework also supports efficient inference by optionally discarding the external critic and using lightweight self-edit cycles. On three widely used benchmarks, ReLook consistently surpasses strong baselines, demonstrating the value of agentic perception, visual rewards, and decoupled training-inference critic usage.

## Method Summary
ReLook operates by generating code, capturing screenshots, and obtaining visual feedback from a multimodal LLM to iteratively refine the output. The key innovation is the use of Forced Optimization, which enforces a strict acceptance rule to admit only improving revisions, ensuring monotonic trajectory improvements. The framework can be trained with an external critic but allows inference to proceed without it, using self-edit cycles for efficiency. ReLook is evaluated on three benchmarks, where it outperforms strong baselines, especially when using 7B and 8B models.

## Key Results
- ReLook achieves 27.88/31.86 on ArtifactsBench-Lite with 7B/8B models.
- The framework consistently outperforms strong baselines across three benchmarks.
- Forced Optimization ensures monotonic trajectory improvements and mitigates behavioral collapse.

## Why This Works (Mechanism)
ReLook's success hinges on closing the generate-diagnose-refine loop using vision-grounded RL. The multimodal LLM critic provides visual feedback that is tightly coupled to the agent's code generation process, enabling iterative improvement. Forced Optimization ensures only beneficial revisions are accepted, preventing regression and maintaining trajectory quality. The ability to decouple training and inference critic usage allows for efficient deployment without sacrificing accuracy.

## Foundational Learning
- **Vision-grounded RL**: Why needed? To provide visual context and feedback for code generation; Quick check: Verify agent can interpret screenshots and link them to code edits.
- **Multimodal LLM as critic**: Why needed? To bridge visual and textual domains for richer feedback; Quick check: Confirm LLM can accurately critique visual output and suggest edits.
- **Forced Optimization**: Why needed? To prevent behavioral collapse and ensure monotonic improvement; Quick check: Measure improvement trajectory with and without this rule.
- **Self-edit cycles**: Why needed? To reduce inference cost by removing the external critic; Quick check: Compare accuracy and latency with/without external critic.
- **Agentic perception**: Why needed? To enable active generation and refinement; Quick check: Ensure agent can generate, capture, and act on visual feedback.

## Architecture Onboarding
- **Component map:** Code Generator -> Screenshot Capture -> Multimodal LLM Critic -> Forced Optimization Filter -> Refined Code
- **Critical path:** Code generation → visual capture → multimodal critique → acceptance filter → next generation
- **Design tradeoffs:** External critic improves feedback quality but increases latency; self-edit cycles trade some accuracy for speed.
- **Failure signatures:** Non-monotonic trajectories suggest Forced Optimization is not working; poor visual feedback may indicate multimodal LLM issues.
- **First experiments:**
  1. Test code generation with and without visual capture to confirm grounding effect.
  2. Evaluate Forced Optimization's impact on trajectory monotonicity.
  3. Compare inference accuracy with external critic versus self-edit cycles.

## Open Questions the Paper Calls Out
None

## Limitations
- The necessity and practical impact of Forced Optimization are not fully quantified without explicit ablation studies.
- Performance gains lack rigorous statistical significance testing across multiple runs.
- The accuracy-cost trade-off when discarding the external critic during inference is not thoroughly analyzed.

## Confidence
- **High**: Technical description of vision-grounded RL loop, multimodal feedback integration, and core algorithmic structure.
- **Medium**: Effectiveness of Forced Optimization and claimed monotonic trajectory improvements.
- **Medium**: Performance improvements over baselines, pending more rigorous statistical validation.

## Next Checks
1. Conduct statistical significance tests (e.g., paired t-tests) across multiple random seeds to confirm robustness of performance gains.
2. Perform a controlled ablation study quantifying behavioral collapse both with and without Forced Optimization.
3. Measure and report inference latency and accuracy trade-offs when using self-edit cycles versus retaining the external critic.