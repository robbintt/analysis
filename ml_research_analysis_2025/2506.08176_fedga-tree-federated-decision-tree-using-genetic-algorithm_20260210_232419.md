---
ver: rpa2
title: 'FedGA-Tree: Federated Decision Tree using Genetic Algorithm'
arxiv_id: '2506.08176'
source_url: https://arxiv.org/abs/2506.08176
tags:
- trees
- clients
- data
- tree
- local
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FedGA-Tree, a novel federated learning approach
  for training personalized decision trees using genetic algorithms. Unlike existing
  methods that rely on greedy algorithms with differential privacy, FedGA-Tree leverages
  genetic algorithms to evolve decision tree structures encoded as integer strings.
---

# FedGA-Tree: Federated Decision Tree using Genetic Algorithm

## Quick Facts
- arXiv ID: 2506.08176
- Source URL: https://arxiv.org/abs/2506.08176
- Authors: Anh V Nguyen; Diego Klabjan
- Reference count: 31
- Primary result: Novel federated learning approach using genetic algorithms to evolve decision tree structures, achieving superior performance over local training and differential privacy-based methods across 28 datasets

## Executive Summary
FedGA-Tree introduces a novel federated learning approach for training personalized decision trees using genetic algorithms. Unlike traditional methods that rely on greedy algorithms with differential privacy, FedGA-Tree evolves decision tree structures encoded as integer strings, enabling training on both categorical and numerical data for classification and regression tasks. The approach ensures data privacy by removing decision thresholds and leaf labels while maintaining high model performance. Experiments demonstrate consistent superiority over local training and benchmark methods, particularly in scenarios with partial client participation.

## Method Summary
FedGA-Tree employs genetic algorithms to evolve decision tree structures in a federated learning setting. Each client's decision tree is encoded as an integer string representing the tree structure. The genetic algorithm iteratively evolves these trees through selection, crossover, and mutation operations across multiple rounds of communication between clients and a central server. This approach eliminates the need for explicit decision thresholds and leaf labels, enhancing privacy while maintaining model expressiveness. The method supports both classification and regression tasks on mixed categorical and numerical data, addressing limitations of existing federated tree methods that rely on differential privacy or greedy splitting algorithms.

## Key Results
- FedGA-Tree consistently achieves higher F1 scores in classification and lower MSE in regression compared to local training and differential privacy-based benchmark methods across 28 datasets
- The approach demonstrates particular robustness in scenarios with partial client participation, maintaining stable performance as the percentage of active clients decreases
- FedGA-Tree produces less complex trees than local training while significantly outperforming local models, with an average 5.58% increase in F1 score across classification datasets

## Why This Works (Mechanism)
The genetic algorithm approach enables exploration of a broader solution space than greedy algorithms, finding optimal tree structures that may be missed by local search methods. By encoding trees as integer strings and evolving them through genetic operations, the method can discover non-local patterns and interactions between features that greedy splitting algorithms might overlook. The federated setting allows multiple clients to contribute diverse perspectives on optimal tree structures while maintaining data privacy through the removal of sensitive information.

## Foundational Learning
- **Federated Learning**: Distributed machine learning where clients collaboratively train a model without sharing raw data; needed for privacy-preserving training across decentralized datasets
- **Decision Trees**: Tree-based models that recursively partition feature space for prediction; needed as the target model architecture for interpretable ML
- **Genetic Algorithms**: Evolutionary optimization methods using selection, crossover, and mutation; needed to explore tree structure space beyond greedy local search
- **Privacy-Preserving ML**: Techniques to protect sensitive information during model training; needed to ensure compliance with data protection regulations
- **Mixed Data Types**: Handling both categorical and numerical features; needed for real-world datasets with heterogeneous feature representations

## Architecture Onboarding

**Component Map:**
Clients -> Genetic Algorithm Operations -> Model Evolution -> Aggregation Server -> Updated Models -> Clients

**Critical Path:**
1. Clients encode local decision trees as integer strings
2. Genetic algorithm operations (selection, crossover, mutation) evolve tree structures
3. Aggregated results update client models
4. Process repeats over multiple communication rounds

**Design Tradeoffs:**
- Privacy vs. model complexity: Removing thresholds enhances privacy but may limit expressiveness
- Communication overhead vs. model quality: More frequent communication enables better convergence but increases network load
- Exploration vs. exploitation: Genetic algorithms balance finding new solutions with refining good ones

**Failure Signatures:**
- Premature convergence to suboptimal tree structures
- Communication bottlenecks in large-scale deployments
- Performance degradation with highly imbalanced client data distributions

**First Experiments:**
1. Compare F1 score convergence rates between FedGA-Tree and differential privacy baseline methods
2. Evaluate model complexity (tree depth, number of nodes) versus prediction accuracy trade-offs
3. Test robustness to varying client participation rates from 10% to 100%

## Open Questions the Paper Calls Out
None

## Limitations
- Limited evaluation scope with only 28 datasets, raising questions about generalization to highly imbalanced or sparse real-world scenarios
- Lack of formal privacy guarantees and quantitative privacy-utility trade-off analysis compared to differential privacy methods
- Missing details on genetic algorithm parameter selection and computational overhead implications for large-scale deployments

## Confidence
- **High**: The core concept of using genetic algorithms for federated decision tree training is technically sound and represents a novel approach
- **Medium**: Reported performance improvements may not fully account for dataset-specific effects or implementation details
- **Low**: Privacy guarantees and scalability claims lack rigorous mathematical proofs or extensive empirical validation

## Next Checks
1. Conduct experiments on high-dimensional, sparse datasets to evaluate FedGA-Tree's performance in challenging feature spaces and compare scalability against traditional federated tree methods
2. Implement formal privacy analysis using differential privacy metrics to quantify the actual privacy-utility trade-off and compare it against explicit differential privacy baselines
3. Perform ablation studies varying genetic algorithm parameters (population size, mutation rates) to determine optimal configurations and assess sensitivity of performance to these hyperparameters