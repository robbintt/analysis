---
ver: rpa2
title: Concise Reasoning in the Lens of Lagrangian Optimization
arxiv_id: '2510.10168'
source_url: https://arxiv.org/abs/2510.10168
tags:
- length
- arxiv
- palu
- reasoning
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces PALU, a method for achieving concise reasoning
  in large language models by adaptively updating generation length budgets to balance
  conciseness with accuracy. PALU formulates concise reasoning as a constrained optimization
  problem and solves it using Lagrangian optimization with three practical approximations:
  off-policy performance estimation, regime-based optimization, and quantile-driven
  length updates.'
---

# Concise Reasoning in the Lens of Lagrangian Optimization

## Quick Facts
- **arXiv ID**: 2510.10168
- **Source URL**: https://arxiv.org/abs/2510.10168
- **Reference count**: 40
- **Primary result**: PALU reduces output length by 65% while improving accuracy by 15% across five benchmarks

## Executive Summary
This paper introduces PALU, a method for achieving concise reasoning in large language models by adaptively updating generation length budgets to balance conciseness with accuracy. PALU formulates concise reasoning as a constrained optimization problem and solves it using Lagrangian optimization with three practical approximations: off-policy performance estimation, regime-based optimization, and quantile-driven length updates. When applied to DeepSeek-Distill-Qwen-1.5B, PALU demonstrates significant improvements in both conciseness and accuracy, outperforming alternative methods while maintaining strong generalization across domains and model scales.

## Method Summary
PALU addresses the challenge of concise reasoning by formulating it as a constrained optimization problem: minimize response length while maintaining performance above a threshold. The method applies Lagrangian optimization theory but simplifies the continuous gradient descent approach into a tractable two-regime controller. Instead of continuously tuning a Lagrange multiplier, PALU uses a binary decision rule based on performance relative to the threshold. The step size for budget reduction is derived from quantile statistics of correct response lengths, replacing non-differentiable gradient estimates with distributional measures. Off-policy rollouts from previous training epochs provide conservative performance estimates, avoiding costly on-policy recomputation while maintaining sufficient accuracy for regime decisions.

## Key Results
- PALU achieves 65% reduction in output length while improving accuracy by 15% on MATH-500, AIME 2024, AMC 2023, Olympiad, and Minerva-Math benchmarks
- The method generalizes effectively across domains (logic, STEM, mathematics) and model scales (1.5B, 7B, 14B parameters)
- PALU outperforms alternative methods in both conciseness and accuracy trade-offs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: PALU converts the constrained optimization problem into a tractable two-regime controller that automatically balances conciseness and accuracy without hand-tuned heuristics.
- **Mechanism**: The method applies Lagrangian optimization theory where a Lagrange multiplier λ dynamically weights the performance constraint. Instead of continuous gradient descent, PALU simplifies to a "bang-bang" controller with two regimes: when performance R ≥ C, reduce length budget by α^q_τ; when R < C, reset budget to maximum L_max. This preserves the essential sign behavior of λ while avoiding brittle continuous updates.
- **Core assumption**: The Lagrange multiplier's sign behavior is sufficient for practical optimization in LLM fine-tuning where performance typically improves monotonically from pretraining.
- **Evidence anchors**: [abstract], [Section 4.2, Eq. 12], [corpus] CAFL-L
- **Break condition**: If the performance threshold C is set outside the model's achievable range, the regime controller fails to find a useful operating point.

### Mechanism 2
- **Claim**: PALU uses the gap between maximum correct response length and a lower quantile as a principled step size for budget reduction.
- **Mechanism**: For each question q, PALU computes α^q_τ = Q^q_1.0 − Q^q_{1.0−τ}, where Q^q_τ is the τ-quantile of correct response lengths from previous rollouts. This quantile gap captures correct response length dispersion, providing cautious updates for clustered responses and aggressive updates for dispersed responses.
- **Core assumption**: Correct responses exhibit meaningful length dispersion such that the quantile gap provides useful signal about safe reduction margins.
- **Evidence anchors**: [abstract], [Section 5.1, Figure 1], [Section 4.2, Eq. 13-16], [corpus] ENTRA
- **Break condition**: If correct responses have extremely tight length distributions (α_τ ≈ 0 for all questions), budget updates become overly conservative and length reduction stalls.

### Mechanism 3
- **Claim**: PALU reuses rollouts from the previous training epoch to estimate current performance, avoiding costly on-policy recomputation while maintaining sufficient accuracy for regime decisions.
- **Mechanism**: Rather than reloading model parameters to compute R(θ, L, q) freshly, PALU approximates it as R(θ_old, L_old, q) using the behavior policy from the last GRPO round. This works because LLM fine-tuning typically shows monotonic improvement from pretraining initialization, making the off-policy estimate a conservative lower bound.
- **Core assumption**: Performance improves monotonically during LLM fine-tuning, so off-policy estimates from slightly stale parameters remain valid indicators of constraint satisfaction.
- **Evidence anchors**: [abstract], [Section 4.2], [corpus] (no strong precedent found)
- **Break condition**: If training instability causes large policy shifts between epochs, or if the model regresses significantly, the off-policy estimate becomes too stale to guide accurate regime decisions.

## Foundational Learning

- **Concept: Lagrangian Optimization for Constrained Problems**
  - **Why needed here**: PALU's theoretical foundation converts a hard constraint into a soft penalty via the Lagrangian. Understanding this transformation is essential for interpreting why the regime controller works.
  - **Quick check question**: Given a constrained problem "minimize f(x) subject to g(x) ≥ C," can you write the Lagrangian L(x, λ) and explain what happens to λ when the constraint is satisfied versus violated?

- **Concept: Group Relative Policy Optimization (GRPO)**
  - **Why needed here**: PALU is implemented on top of GRPO, which replaces PPO's value function with group-relative advantage normalization. The algorithm's efficiency depends on understanding how GRPO computes advantages and updates policies.
  - **Quick check question**: How does GRPO's advantage computation (Eq. 1: Â = (r − mean) / std) differ from standard PPO, and why does this eliminate the need for a separate value model?

- **Concept: Quantile Statistics and Distribution Shape**
  - **Why needed here**: PALU's step size α^q_τ directly depends on quantile gaps in correct-response length distributions. Interpreting these gaps requires understanding what quantiles reveal about distribution spread and tail behavior.
  - **Quick check question**: If the 100th percentile of correct responses is 12,000 tokens and the 90th percentile is 8,000 tokens, what is α^q_{0.1} and what does a larger versus smaller gap indicate about safe budget reduction?

## Architecture Onboarding

- **Component map**: Dataset D -> GRPO optimizer -> θ updates -> Rollout generator -> Reward evaluator -> Performance estimator -> Regime controller -> Budget allocator -> Per-question length budgets L_q

- **Critical path**:
  1. Initialize L_q = L_max for all questions (first epoch only; Algorithm 1, line 4)
  2. For each mini-batch D_b ⊂ D:
     - Load previous pass rates (Eq. 11 off-policy estimate)
     - Compute quantile step α^q_τ from correct response lengths in previous rollouts (Eq. 13-14)
     - Apply regime rule: if R(θ, L, q) ≥ C then L_q ← L_q − α^q_τ else L_q ← L_max (Eq. 12)
     - Generate G rollouts with current θ under per-question budgets
     - Update θ via GRPO (Eq. 4)
  3. Store pass rates for next epoch's off-policy estimation

- **Design tradeoffs**:
  - **Step size τ**: Larger τ (0.5) accelerates length reduction but slightly compromises accuracy; smaller τ (0.1, 0.2) stabilizes performance but provides weaker conciseness pressure. Paper recommends τ = 0.5 for rapid reduction, τ = 0.2 for stability.
  - **Performance threshold C**: Higher values (0.9) preserve accuracy but limit length gains; lower values (0.7) enable aggressive shortening but risk accuracy drops. Paper uses C = 0.8 as default.
  - **Initial L_max**: Must exceed typical correct-response lengths to avoid artificial early constraints. Paper uses 16k tokens; verify against your model's baseline generation lengths.

- **Failure signatures**:
  - **Oscillating budget with no convergence**: Budget repeatedly resets to L_max then reduces, indicating C is set too close to model's actual capability ceiling. Solution: lower C or verify off-policy estimates aren't too stale.
  - **Stagnant length reduction**: α^q_τ values consistently near zero, indicating correct responses have tight length distributions. Check Figure 1-style distribution plots; if truly narrow, PALU may be unsuitable for this task.
  - **Accuracy collapse without recovery**: Sudden accuracy drop followed by no rebound, suggesting off-policy estimates have become too unreliable to guide regime decisions. Consider reducing epoch size or validating estimates periodically.

- **First 3 experiments**:
  1. **Baseline replication**: Implement PALU on DeepSeek-Distill-Qwen-1.5B with C = 0.8, τ = 0.5, training on 12k math problems (DeepScaler subset) for 20 epochs. Target: verify ~65% length reduction and ~15% accuracy improvement on MATH-500, AIME 2024, AMC 2023, Olympiad, Minerva-Math.
  2. **Step size ablation**: Compare τ ∈ {0.1, 0.2, 0.5} on same setup, plotting accuracy vs. generation length curves. Confirm that larger τ yields faster reduction with accuracy tradeoff.
  3. **Cross-domain adaptation test**: Train on 5k multi-domain data (2k math, 2k STEM, 1k logic from GURU) and evaluate on held-out questions from each domain. Verify PALU adapts without retuning C or τ across domains.

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical approximation validity: The two-regime controller assumes sign behavior of Lagrange multipliers is sufficient, lacking theoretical guarantees for when this approximation breaks down
- Distribution assumptions: The method may be unsuitable for tasks with extremely tight correct-response length distributions where α^q_τ approaches zero
- Off-policy estimation stability: The approach assumes monotonic performance improvement during fine-tuning, which may not hold under training instability or catastrophic forgetting

## Confidence
- **High Confidence**: 65% length reduction with 15% accuracy improvement empirically demonstrated; strong generalization across domains and model scales; effective balance of conciseness and accuracy without heuristics
- **Medium Confidence**: Quantile gap provides principled signal for budget reduction; off-policy estimation is sufficiently accurate; Lagrangian optimization enables tractable constrained optimization
- **Low Confidence**: Behavior under non-monotonic training dynamics; generalizability to tasks with extremely tight correct-response distributions; robustness to performance thresholds near capability ceilings

## Next Checks
1. **Distribution Sensitivity Analysis**: Systematically evaluate PALU's performance across tasks with varying correct-response length distributions, measuring how often α^q_τ approaches zero and quantifying impact on length reduction effectiveness.
2. **Training Instability Stress Test**: Design experiments with deliberate performance fluctuations during training to measure how PALU's off-policy estimation and regime controller respond to perturbations.
3. **Theoretical Approximation Bounds**: Develop analytical or empirical bounds for when the two-regime approximation diverges from full Lagrangian optimization, comparing against a simplified continuous λ update controller on synthetic problems.