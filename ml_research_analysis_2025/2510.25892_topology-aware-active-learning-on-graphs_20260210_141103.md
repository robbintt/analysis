---
ver: rpa2
title: Topology-Aware Active Learning on Graphs
arxiv_id: '2510.25892'
source_url: https://arxiv.org/abs/2510.25892
tags:
- graph
- learning
- coreset
- algorithm
- active
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a graph-topological approach to active learning
  that addresses the exploration-exploitation tradeoff in label-scarce regimes. The
  core idea is to use Balanced Forman Curvature (BFC) to guide coreset selection,
  ensuring diverse initial labels across graph communities, and to provide a principled,
  data-driven signal for when to switch from exploration to exploitation.
---

# Topology-Aware Active Learning on Graphs

## Quick Facts
- arXiv ID: 2510.25892
- Source URL: https://arxiv.org/abs/2510.25892
- Reference count: 40
- One-line primary result: Introduces curvature-guided coreset selection and phase transition scheduling that significantly improves AL accuracy and efficiency.

## Executive Summary
This paper introduces a graph-topological approach to active learning that addresses the exploration-exploitation tradeoff in label-scarce regimes. The core idea is to use Balanced Forman Curvature (BFC) to guide coreset selection, ensuring diverse initial labels across graph communities, and to provide a principled, data-driven signal for when to switch from exploration to exploitation. The method also includes a localized graph rewiring strategy that enhances label propagation near informative nodes without incurring the computational cost of full multiscale regularization. Empirically, the proposed Curvature Coreset (CC) method significantly outperforms existing baselines like Dijkstra's Annulus Coreset in accuracy and efficiency across benchmark datasets (MNIST, FashionMNIST, CIFAR-10).

## Method Summary
The method uses Balanced Forman Curvature (BFC) to guide active learning through three key innovations: (1) curvature-based coreset selection via a minimax formulation that ensures topological coverage of distinct clusters, (2) a data-driven scheduler that uses theoretical curvature bounds to trigger the transition from exploration to exploitation, and (3) localized graph rewiring that applies higher-order Laplacian regularization near acquired nodes for improved label propagation. The approach operates on k-NN graphs with angular distance weighting, using uncertainty sampling with Laplace learning as the base AL policy.

## Key Results
- Curvature Coreset (CC) achieves 94% accuracy on MNIST with 100 labels vs 59% for standard Laplace
- Local rewiring matches hypergraph accuracy (~90%) at 10x computational speedup
- CC covers 8 clusters in 8 iterations vs 17 for distance-based methods
- Curvature-triggered scheduling outperforms fixed-schedule decay across all tested datasets

## Why This Works (Mechanism)

### Mechanism 1: Curvature-Guided Coreset Selection (Exploration)
Selecting initial labeled points via Balanced Forman Curvature ensures topological coverage of data clusters faster than distance-based methods. By greedily selecting nodes that maximize the minimum negative curvature relative to the current coreset, the algorithm samples from distinct, under-represented clusters rather than staying within dense regions.

### Mechanism 2: Curvature-Triggered Phase Transition
BFC provides a data-driven signal to switch AL policy from exploration to exploitation. In k-NN graphs, a theoretical upper bound exists for nodes that are not neighbors (-2 + 4/k). If curvature between a new acquisition and existing labeled points exceeds this bound, the unlabeled space has been sufficiently explored.

### Mechanism 3: Localized Multiscale Rewiring (Exploitation)
Applying higher-order Laplacian regularization locally around queried nodes improves label propagation accuracy comparable to full hypergraph methods but at a fraction of computational cost. Instead of computing dense powers of the full Graph Laplacian, the method increments the Laplacian matrix only for edges adjacent to the newly acquired node.

## Foundational Learning

- **Discrete Ricci Curvature (Balanced Forman)**: Why needed - core metric used to identify structural bottlenecks and cluster separation. Quick check - on a graph, does a negative curvature edge indicate a tightly connected clique or a bridge between two separate clusters? (Answer: Bridge).

- **Exploration vs. Exploitation in Active Learning**: Why needed - paper explicitly frames AL problem as balancing these two phases. Quick check - why might "uncertainty sampling" (purely exploitative) fail in low-label regime? (Answer: May ignore entire classes/clusters if initial model is unaware of them).

- **Graph Laplacian and Higher-Order Regularization**: Why needed - exploitation phase relies on Graph Laplacian, proposing modification to L^p locally. Quick check - why is computing k-th power of Laplacian (L^k) for full graph computationally expensive? (Answer: Densifies matrix, increasing memory/compute from sparse to dense operations).

## Architecture Onboarding

- **Component map**: Input -> Graph Constructor (kNN, Adjacency A, Weights W) -> Coreset Engine (Alg 2, BFC minimax) -> AL Controller (Classifier: Laplace/Poisson, Acquisition: Uncertainty sampling/Min-Norm, Scheduler: Alg 4 checks BFC, Rewiring: Alg 5 updates L locally)

- **Critical path**: Computational bottleneck shifts from AL query to Laplacian Solve and BFC Calculation. "Reduction Parameter" r is critical for keeping BFC calculation tractable by limiting search space to high-degree nodes.

- **Design tradeoffs**: Accuracy vs Speed (Rewiring) - Full Hypergraph most accurate but slow (~18k seconds); Laplace fast but inaccurate; Local Rewiring strikes balance (~1.2k seconds, high accuracy). Coverage vs Efficiency (Coreset) - DAC ensures 100% coverage but wastes labels; CC stops based on "saturation" (Z-score jump), which is more label-efficient but might theoretically miss disconnected components.

- **Failure signatures**: Stagnant Accuracy if BFC stopping condition triggers too early (Z-score threshold too low); Slow Convergence if r is too high (CC behaves randomly) or too low (compute time explodes); Oversmoothing if Rewiring powers p are too high without proper fidelity terms.

- **First 3 experiments**: (1) Sanity Check (Blobs Dataset) - Verify CC selects one point per cluster while DAC selects edge points. (2) Scheduler Ablation - Compare fixed-schedule τ-decay vs BFC-triggered τ-decay on EMNIST vs Box. (3) Efficiency Benchmark - Measure wall-clock time for "AL Local Rewiring" vs "Full Hypergraph" on CIFAR-10 at 100 labels.

## Open Questions the Paper Calls Out
- How do graph properties, specifically homophily and degree distribution, influence the optimal balance between exploration and exploitation in graph active learning?
- Can a practical, computationally efficient definition of curvature for weighted graphs improve coreset selection performance?
- How can curvature-based metrics be adapted for batch active learning to ensure topological diversity among simultaneously selected nodes?

## Limitations
- The BFC-based approach assumes k-NN graphs faithfully encode semantic clusters as topological communities
- The theoretical curvature bounds for phase transition are derived specifically for k-NN graphs and may not generalize
- Computing BFC for large numbers of candidate node pairs remains O(n²) even with reduction optimizations

## Confidence
- High Confidence: Empirical results demonstrating CC superiority over DAC in label efficiency and accuracy
- Medium Confidence: Theoretical justification for BFC as phase transition signal is sound for k-NN graphs but not fully explored across graph types
- Medium Confidence: Localized rewiring strategy is effective and efficient compared to full hypergraph methods, but long-term stability on very complex datasets is less validated

## Next Checks
1. Validate BFC-based scheduler on non-image dataset (Cora citation network or biological interaction network) to test generalizability
2. Conduct ablation study comparing full CC method against random coreset selection + BFC scheduler and CC without localized rewiring
3. Evaluate method on large-scale graph dataset (ogbn-proteins or large social network) to measure impact of reduction parameter r on accuracy and runtime<|end_of_text|><|begin_of_text|>4. Test BFC computation scalability with the proposed optimizations on million-node graphs