---
ver: rpa2
title: 'ReGA: Representation-Guided Abstraction for Model-based Safeguarding of LLMs'
arxiv_id: '2506.01770'
source_url: https://arxiv.org/abs/2506.01770
tags:
- safety
- rega
- arxiv
- llms
- harmful
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces ReGA, a representation-guided abstraction\
  \ framework for safeguarding large language models (LLMs) against harmful prompts\
  \ and jailbreaking attacks. The core idea is to leverage safety-critical representations\u2014\
  low-dimensional directions in hidden states that indicate safety-related concepts\u2014\
  to construct a compact abstract model (DTMC) that can effectively distinguish safe\
  \ from harmful inputs."
---

# ReGA: Representation-Guided Abstraction for Model-based Safeguarding of LLMs

## Quick Facts
- arXiv ID: 2506.01770
- Source URL: https://arxiv.org/abs/2506.01770
- Authors: Zeming Wei; Chengcan Wu; Meng Sun
- Reference count: 40
- Key outcome: ReGA achieves 0.975 AUROC at prompt level and 0.985 at conversation level for LLM safety detection

## Executive Summary
ReGA introduces a novel representation-guided abstraction framework for safeguarding large language models against harmful prompts and jailbreaking attacks. The method leverages safety-critical representations—low-dimensional directions in hidden states that encode safety-related concepts—to construct a compact abstract model (DTMC) for distinguishing safe from harmful inputs. Comprehensive experiments demonstrate ReGA outperforms existing defense paradigms in effectiveness, interpretability, and scalability, achieving high AUROC scores across different safety perspectives and robustness against real-world jailbreaking attacks.

## Method Summary
ReGA extracts safety representations from LLM hidden states using contrastive PCA on safe and harmful prompts, clusters them into abstract states via K-Means, and models transitions using a discrete-time Markov chain. The framework constructs an abstract model from safety-critical representations to efficiently assess input safety. At runtime, inputs are mapped to abstract state sequences and evaluated using composite scores combining state-level safety and transition-level probabilities. The method operates at both prompt-level (pre-generation) and conversation-level (post-generation) to provide comprehensive safeguarding.

## Key Results
- Achieves 0.975 AUROC for prompt-level harmful content detection
- Achieves 0.985 AUROC for conversation-level harmful content detection
- Outperforms existing paradigms in effectiveness, interpretability, and scalability

## Why This Works (Mechanism)

### Mechanism 1: Safety Representation Extraction via Contrastive PCA
Safety-critical information is concentrated in middle LLM layers and manifests as linearly separable subspaces in hidden state space. By constructing contrastive datasets and applying PCA, ReGA identifies low-dimensional directions that maximally separate safe and harmful distributions. This assumes safety concepts emerge as distinct patterns in hidden states that can be captured through linear dimensionality reduction.

### Mechanism 2: Concrete-to-Abstract State Abstraction via K-Means
The framework clusters safety representation activations into discrete abstract states while preserving safety semantics. By projecting inputs onto safety representation directions and clustering the resulting vectors, ReGA creates a tractable state space where cluster membership correlates with safety labels. This enables model-based analysis of safety patterns while maintaining computational efficiency.

### Mechanism 3: DTMC-based Composite Safety Scoring
ReGA combines state-level safety scores with transition-level probabilities to capture both static and dynamic safety signals. By modeling abstract state sequences as a Markov chain and computing composite scores from recent states and transitions, the framework improves detection robustness against inputs that might fool single-point assessments.

## Foundational Learning

- Concept: **Representation Engineering**
  - Why needed: ReGA's core premise is that specific directions in hidden states encode high-level concepts
  - Quick check: Given hidden states from safe and harmful inputs, how would PCA identify directions that maximally separate these classes?

- Concept: **Discrete Time Markov Chains (DTMCs)**
  - Why needed: ReGA models abstract state transitions as a DTMC
  - Quick check: If you observe an abstract state sequence (s1 → s2 → s3), how do you compute the transition probability contribution to the final safety score?

- Concept: **Safety Alignment in LLMs**
  - Why needed: ReGA assumes LLMs have partial but inadequate safety alignment
  - Quick check: Why might a model with safety training still respond positively to jailbreaking attacks?

## Architecture Onboarding

- Component map: Contrastive dataset D = {RS, CS, RH, CH} → Layer L/2 hidden state extraction → PCA to K components → Safety representations {r1...rK} → Concrete states s(x) = [r₁·Mh(x), ..., rK·Mh(x)] → K-Means clustering → N abstract states S → Transition matrix T (safe data only) → Safety functions u (state), v (transition) → Input x → Abstract state sequence → Last m states → Safety score p(x) = ps(x) + pt(x) → Threshold comparison → Allow/refuse decision

- Critical path: Middle-layer selection (L/2) determines representation quality—shallow layers lack semantic content, deep layers post-date refusal decisions; Contrastive dataset balance (nh/ns ≈ 1/4) affects representation discriminability; Hyperparameters (N=32, K=8, m=3 defaults) control abstraction granularity vs. noise

- Design tradeoffs: More abstract states (N↑) → finer granularity but increased complexity; More PCA dimensions (K↑) → more information but potential noise; Threshold selection (MCA vs. MFP) → safety-utility tradeoff; MFP minimizes over-refusal at cost of detection rate

- Failure signatures: Over-refusal (low threshold): Natural prompts from MT-Bench/Chat1M flagged as harmful; Jailbreak under-detection: WildJailbreak attacks achieve lower detection (~14-18% accuracy) vs. vanilla harmful prompts; Model-specific degradation: Koala shows AUROC of only 0.89-0.93, attributed to weaker internal concept representation capacity

- First 3 experiments: (1) In-distribution validation: Train ReGA on Alpaca/AdvBench split (ns=256, nh=64), test on held-out samples, verify AUROC > 0.95 for prompt-level detection; (2) Cross-dataset generalization: Train on Alpaca/AdvBench, evaluate on HarmBench (harmful) and MT-Bench (safe); target >80% accuracy on harmful, >95% on safe; (3) Hyperparameter sensitivity: Sweep N ∈ {8, 16, 32, 64} and K ∈ {2, 4, 8, 16}; plot AUROC to identify stable operating regions and failure modes

## Open Questions the Paper Calls Out

### Open Question 1
How can ReGA improve detection performance against complex, diverse jailbreaks found in datasets like WildJailbreak? The authors note in Section 4.4.1 that performance against "more complex attacks in WildJailbreak is relatively weak and warrants further investigation."

### Open Question 2
Does the ReGA framework generalize effectively to LLMs significantly larger than 7B parameters or with different architectures? The "Threat to Validity" section notes the approach may have limitations generalizing to other LLMs, as experiments were restricted to six open-sourced 7b-size models.

### Open Question 3
Can the representation extraction be refined to better detect abstract safety violations, such as "unqualified advice"? Table 6 shows a significant performance drop for "Unqualified Advice" (Avg. Acc 0.36) compared to categories like "Hate Speech" (Avg. Acc 0.72).

## Limitations
- Framework's reliance on middle-layer hidden states assumes safety-critical representations emerge consistently across architectures, but empirical validation beyond 4 LLM families is limited
- DTMC-based transition modeling assumes harmful inputs generate statistically distinct abstract state sequences, but sophisticated jailbreaking attacks may learn to mimic safe transition patterns
- Claims about interpretability lack systematic human evaluation of abstract state semantics

## Confidence
- High confidence: Prompt-level AUROC performance (0.975) on in-distribution data; clear mechanism for safety representation extraction via contrastive PCA
- Medium confidence: Conversation-level performance (0.985) and generalization across safety perspectives; claims about robustness to jailbreaking attacks show mixed results
- Low confidence: Scalability claims without extensive resource usage data; interpretability assertions lacking systematic human evaluation

## Next Checks
1. **Cross-Architecture Generalization**: Test ReGA on additional LLM families (Mistral, Gemma, open-access GPT-2 variants) to verify middle-layer representation consistency across architectural designs
2. **Adversarial Transferability**: Evaluate whether jailbreaking attacks that succeed against one safety model transfer to ReGA, measuring both prompt-level and conversation-level degradation
3. **Resource Efficiency Scaling**: Benchmark ReGA's runtime and memory overhead across different model sizes (7B→70B parameters) and batch processing scenarios to validate scalability claims