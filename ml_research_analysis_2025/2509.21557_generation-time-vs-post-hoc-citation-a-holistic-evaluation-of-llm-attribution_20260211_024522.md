---
ver: rpa2
title: 'Generation-Time vs. Post-hoc Citation: A Holistic Evaluation of LLM Attribution'
arxiv_id: '2509.21557'
source_url: https://arxiv.org/abs/2509.21557
tags:
- citation
- methods
- citations
- computational
- coverage
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper compares two paradigms for citation generation in large
  language models: Generation-Time Citation (G-Cite), where citations are produced
  during text generation, and Post-hoc Citation (P-Cite), where citations are added
  after generating a draft. The authors evaluate eight methods (four per paradigm)
  across four datasets spanning open-domain QA, scientific literature, and fact verification
  tasks.'
---

# Generation-Time vs. Post-hoc Citation: A Holistic Evaluation of LLM Attribution

## Quick Facts
- **arXiv ID**: 2509.21557
- **Source URL**: https://arxiv.org/abs/2509.21557
- **Reference count**: 13
- **Primary result**: P-Cite methods achieve higher coverage (75%) while maintaining competitive correctness (42%), making them more suitable for high-stakes applications requiring comprehensive source attribution.

## Executive Summary
This paper compares two paradigms for citation generation in large language models: Generation-Time Citation (G-Cite), where citations are produced during text generation, and Post-hoc Citation (P-Cite), where citations are added after generating a draft. The authors evaluate eight methods (four per paradigm) across four datasets spanning open-domain QA, scientific literature, and fact verification tasks. They find that P-Cite methods achieve higher coverage while maintaining competitive correctness, making them more suitable for high-stakes applications requiring comprehensive source attribution. G-Cite methods show higher precision but limited coverage and higher latency. Retrieval augmentation proves fundamental to attribution quality in both paradigms, with advanced methods allowing fine-tuning of the coverage-latency trade-off. The authors recommend a retrieval-centric, P-Cite-first approach for information-critical applications.

## Method Summary
The study evaluates eight citation generation methods across four datasets: ALCE (3K instances), LongBench-Cite (1K), REASONS (12.7K), and FEVER (185K). Four methods follow Generation-Time Citation (G-Cite) paradigm where citations are produced during text generation, while four use Post-hoc Citation (P-Cite) where citations are added after draft completion. Base model is LLaMa-3.1-8B-Instruct (except CiteBART uses BART). Retrieval augmentation uses SBERT bi-encoder + cross-encoder reranker. Methods include zero-shot, RAG, fine-tuned (LongCite-8B, CiteBART), and advanced variants (CoT Citation for G-Cite, CEG for P-Cite). Evaluation metrics include Citation Correctness (F1), Precision, Recall, Coverage, and Latency. Human evaluation assesses Answer Correctness and Citation Hallucination (n=100 per method-dataset).

## Key Results
- P-Cite achieves 75% coverage versus G-Cite's 37% on ALCE dataset
- P-Cite maintains competitive correctness (42%) compared to G-Cite's higher precision (94% on FEVER)
- Retrieval augmentation yields 50 percentage point improvement in correctness (27% to 77%) on FEVER
- Advanced methods enable fine-tuning of coverage-latency trade-off
- P-Cite recommended for high-stakes applications requiring comprehensive attribution

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieval augmentation is the primary driver of attribution quality in both citation paradigms.
- Mechanism: External retrieval systems supply relevant source documents before or during citation generation, providing grounded evidence that the LLM can reference. This decouples citation quality from the model's parametric knowledge, reducing hallucination risk.
- Core assumption: The retrieval corpus contains authoritative sources relevant to the query domain.
- Evidence anchors:
  - [abstract] "retrieval as the main driver of attribution quality in both paradigms"
  - [section] Finding 3: "transition from zero-shot to RAG yields the most substantial and consistent improvements... correctness improves by approximately 50 percentage points (from 27% to 77%)"
  - [corpus] CiteFix (arXiv:2504.15629) confirms retrieval-based citation correction improves RAG accuracy
- Break condition: Retrieval index lacks coverage of required domain knowledge; or retrieved documents are irrelevant/low-quality, causing citation errors despite RAG.

### Mechanism 2
- Claim: Post-hoc citation (P-Cite) achieves higher citation coverage by enabling holistic review of complete drafts against evidence.
- Mechanism: P-Cite separates citation from text generation, running a second pass that examines the full draft and available evidence corpus. This global view allows the system to identify more relevant source-document alignments compared to G-Cite's token-by-token local decisions.
- Core assumption: The draft text contains claims that can be mapped to retrievable sources; the second-pass verifier has sufficient capacity to identify these mappings.
- Evidence anchors:
  - [abstract] "P-Cite methods achieve higher coverage while maintaining competitive correctness"
  - [section] Finding 1: "P-Cite achieve 75% coverage... substantially outperforming G-Cite which reaches 37% coverage"
  - [corpus] Evidence weak—neighbor papers focus on citation evaluation, not P-Cite architecture specifics
- Break condition: Draft contains novel synthesis or reasoning not directly supported by any single source; second pass adds citations that are formally correct but semantically weak.

### Mechanism 3
- Claim: Generation-time citation (G-Cite) achieves higher precision by constraining citations to locally-evidenced claims during token generation.
- Mechanism: G-Cite makes citation decisions during left-to-right autoregressive generation, attaching citations only when current context plus retrieved evidence provide direct support. This local constraint reduces false-positive citations but may skip claims requiring multi-document synthesis.
- Core assumption: Correct citations are more critical than complete citations for the target use case.
- Evidence anchors:
  - [abstract] "G-Cite methods show higher precision but limited coverage and higher latency"
  - [section] FEVER results: "G-Cite achieves the highest precision and correctness (94%) but limited citation coverage (27%)"
  - [corpus] SelfCite (arXiv:2502.09604) aligns—fine-grained sentence-level citations during generation improve precision
- Break condition: Complex multi-hop reasoning requires citations that cannot be determined locally; latency constraints prohibit thorough evidence consultation per token.

## Foundational Learning

- Concept: **Retrieval-Augmented Generation (RAG)**
  - Why needed here: Paper identifies retrieval as "fundamental" to citation quality across paradigms. Understanding RAG architecture (retriever → reranker → generator pipeline) is prerequisite to implementing either G-Cite or P-Cite methods.
  - Quick check question: Can you explain why zero-shot citation correctness jumps from 27% to 77% on FEVER when adding retrieval?

- Concept: **Citation Evaluation Metrics (Precision, Recall, Coverage, Correctness)**
  - Why needed here: The paper's comparative claims rest on these four metrics. Correctness = harmonic mean of precision/recall; Coverage = proportion of ground-truth citations present. Without understanding these, trade-off analysis is opaque.
  - Quick check question: If P-Cite achieves 75% coverage and 42% correctness on ALCE, what does this tell you about the precision-recall balance?

- Concept: **Two-Pass vs Single-Pass Generation Architectures**
  - Why needed here: The fundamental paradigm difference is architectural. G-Cite interleaves citation markers during decoding; P-Cite adds a verification/annotation pass after draft completion.
  - Quick check question: Why would a two-pass architecture inherently support higher coverage but potentially higher latency?

## Architecture Onboarding

- Component map:
  - **G-Cite pipeline**: Query → Retrieval (optional) → LLM with citation tokens in vocabulary → Output with inline `<cite[i]>` markers
  - **P-Cite pipeline**: Query → Draft generation → Retrieval against draft sentences → Citation attachment/verification → Final output
  - **Shared components**: Retriever (SBERT bi-encoder + cross-encoder reranker per paper), evidence corpus, citation parser (regex-based)

- Critical path:
  1. Evidence retrieval quality (Finding 3: largest gains here)
  2. Citation decision mechanism (local G-Cite vs. global P-Cite)
  3. Coverage-latency tradeoff tuning via advanced methods (CoT Citation, CEG)

- Design tradeoffs:
  - Coverage vs. Precision: P-Cite prioritizes coverage (~75-78%); G-Cite prioritizes precision (~94% on FEVER)
  - Latency vs. Completeness: G-Cite advanced methods show latency 17.2s vs. P-Cite CEG at 6-9s
  - Fine-tuning vs. Retrieval: Paper recommends retrieval as foundational; fine-tuning as optimization enhancement

- Failure signatures:
  - Low coverage + high precision = G-Cite mode (claims lack citations but those present are correct)
  - High coverage + low correctness = retrieval returning irrelevant documents or citation parser errors
  - High latency = advanced G-Cite methods with per-token evidence consultation
  - Citation hallucination (37-41% in human eval) = model generating citations without retrieval grounding

- First 3 experiments:
  1. **Baseline retrieval ablation**: Compare zero-shot vs. RAG on your domain dataset using both G-Cite and P-Cite prompts. Expect 30-50 point correctness improvement per Finding 3.
  2. **Coverage-latency calibration**: Implement CEG-style P-Cite and measure coverage at latency budgets (2s, 5s, 10s). Identify optimal operating point for your SLA.
  3. **Domain-specific retrieval index**: Build a retrieval corpus for your target domain (legal, medical, scientific). Compare citation correctness against general-purpose retrieval to quantify domain adaptation gains.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would the relative advantages of P-Cite (higher coverage) and G-Cite (higher precision) persist across different model families and scales beyond LLaMa-3.1-8B?
- Basis in paper: [inferred] All experiments use a single base model (LLaMa-3.1-8B-Instruct), with CiteBART as the only exception using BART. The findings may not generalize.
- Why unresolved: Different architectures may have different citation behaviors; larger models might close the coverage gap for G-Cite through improved multi-source integration.
- What evidence would resolve it: Systematic evaluation across model families (e.g., Mistral, Gemma, GPT) and scales (7B, 13B, 70B) on the same benchmark suite.

### Open Question 2
- Question: Could a hybrid pipeline (G-Cite for initial generation followed by P-Cite for coverage completion) achieve both high precision and high coverage?
- Basis in paper: [inferred] The authors recommend P-Cite-first for high-stakes applications and reserve G-Cite for precision-critical settings, implying complementary strengths, but do not test combined approaches.
- Why unresolved: The paradigms are evaluated in isolation; it is unknown whether sequential or parallel combination would retain benefits or introduce new trade-offs (e.g., compounding latency).
- What evidence would resolve it: Experiments with cascaded or ensemble methods measuring precision, coverage, and latency on the same datasets.

### Open Question 3
- Question: How does retriever quality (precision vs. recall) differentially impact G-Cite vs. P-Cite attribution performance?
- Basis in paper: [explicit] Finding 3 states "retrieval augmentation is the primary driver of citation accuracy," but the study does not disentangle retriever precision from retriever recall effects on each paradigm.
- Why unresolved: Both paradigms use the same retrieval components; whether P-Cite's higher coverage stems from better leveraging noisy retrievals versus G-Cite's sensitivity to retrieval precision remains unclear.
- What evidence would resolve it: Controlled experiments varying retriever precision/recall independently and measuring citation correctness and coverage for each paradigm.

### Open Question 4
- Question: Do the reported coverage-correctness trade-offs hold when scaling to multilingual or cross-lingual citation tasks?
- Basis in paper: [inferred] All datasets are English-only; the paper does not address whether P-Cite's coverage advantage transfers when citation sources span multiple languages.
- Why unresolved: Multilingual retrieval and cross-lingual alignment introduce challenges that may affect paradigms differently, particularly for G-Cite's local decision-making during generation.
- What evidence would resolve it: Evaluation on multilingual attribution benchmarks (e.g., extending FEVER or scientific corpora to non-English sources) using comparable metrics.

## Limitations

- The study uses a single base model (LLaMa-3.1-8B-Instruct) with only one exception (CiteBART), limiting generalizability across model architectures.
- Human evaluation sample size (n=100 per method-dataset) may not capture rare failure modes in high-stakes applications.
- The paper assumes all claims can be attributed to retrievable sources, but novel synthesis or multi-hop reasoning may not have direct document matches.

## Confidence

- **High Confidence**: Retrieval augmentation as primary driver of correctness improvement (Finding 3) - supported by 50 percentage point jump from 27% to 77% correctness on FEVER.
- **Medium Confidence**: P-Cite superiority in coverage (75% vs 37% for G-Cite on ALCE) - results consistent but may depend on dataset adaptation quality.
- **Medium Confidence**: G-Cite precision advantage (94% on FEVER) - finding robust but may not generalize to domains requiring comprehensive attribution.

## Next Checks

1. **Retrieval Quality A/B Test**: Implement the same citation methods with two retrieval configurations—dataset-specific corpus vs. general web corpus. Measure correctness differences to quantify domain adaptation value.

2. **Citation Pattern Robustness**: Test the regex parser against edge cases (citation ranges like `<cite>[1-3]</cite>`, nested citations). Report extraction accuracy and compare against manual annotation on 50 random examples.

3. **Cross-Paradigm Dataset Adaptation**: Select one dataset (e.g., REASONS) and run both G-Cite and P-Cite natively (not cross-paradigm adapted). Compare coverage/latency metrics to validate the paper's adaptation approach.