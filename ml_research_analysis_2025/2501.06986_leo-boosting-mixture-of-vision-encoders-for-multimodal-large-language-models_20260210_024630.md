---
ver: rpa2
title: 'LEO: Boosting Mixture of Vision Encoders for Multimodal Large Language Models'
arxiv_id: '2501.06986'
source_url: https://arxiv.org/abs/2501.06986
tags:
- vision
- visual
- arxiv
- fusion
- encoders
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of integrating multiple vision
  encoders in multimodal large language models (MLLMs) to improve visual understanding
  capabilities. The core method introduces LEO, a novel MLLM with a dual-branch vision
  encoder framework that employs a tile-level post-adaptation fusion strategy.
---

# LEO: Boosting Mixture of Vision Encoders for Multimodal Large Language Models

## Quick Facts
- arXiv ID: 2501.06986
- Source URL: https://arxiv.org/abs/2501.06986
- Reference count: 40
- Key outcome: LEO achieves state-of-the-art performance on 13 vision-language benchmarks, notably 68.8% on ChartQA and 80.1% on DocVQA, by integrating dual-branch vision encoders with tile-level post-adaptation fusion.

## Executive Summary
This paper introduces LEO, a novel multimodal large language model (MLLM) that addresses the challenge of integrating multiple vision encoders to improve visual understanding capabilities. LEO employs a dual-branch vision encoder framework with a tile-level post-adaptation fusion strategy, processing high-resolution input images by dividing them into 448×448 tiles. The model uses separate MLP projectors for each encoder branch, sequentially interleaving visual tokens before combining them with text tokens for LLM processing.

Extensive evaluation across 13 vision-language benchmarks demonstrates that LEO outperforms state-of-the-art open-source MLLMs and hybrid MLLMs on the majority of tasks. Notably, LEO achieves 68.8% on ChartQA, 80.1% on DocVQA, 78.5% on ScienceQA, and 88.0% on POPE. The model also shows competitive performance when adapted to the autonomous driving domain without altering architecture or training recipe, achieving 61.0% on LingoQA. These results highlight the effectiveness of LEO's dual-branch vision encoders and post-adaptation fusion strategy in enhancing visual understanding capabilities.

## Method Summary
LEO processes high-resolution input images by dividing them into 448×448 tiles, which are independently processed by two specialized vision encoders (InternViT-300M-448px and SAM-L). Pixel unshuffling reduces visual tokens per tile, and the model uses separate MLP projectors for each encoder branch, sequentially interleaving visual tokens before combining them with text tokens for LLM processing. The training follows a two-stage approach: Stage 1 uses LLaVA-595k with frozen encoders and trains the SAM projector, while Stage 2 uses InternVL's SFT dataset (~1M samples) with frozen encoders and full fine-tuning of projectors plus InternLM2-7B-Chat.

## Key Results
- LEO achieves 68.8% on ChartQA, significantly outperforming comparable open-source MLLMs
- LEO attains 80.1% on DocVQA, demonstrating superior document visual understanding
- The model shows competitive performance in autonomous driving domain with 61.0% on LingoQA without architectural changes

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Post-adaptation fusion via dedicated projectors preserves the distinct representational integrity of heterogeneous vision experts better than pre-adaptation fusion.
- **Mechanism:** LEO processes visual tokens from InternViT and SAM-L through separate MLP projectors before interleaving them, allowing the LLM to receive features that have already been mapped to its embedding space independently.
- **Core assumption:** Vision encoders pre-trained on distinct objectives (semantic understanding vs. segmentation) produce feature distributions that are difficult to align simultaneously using a single shared adapter without loss of information.
- **Evidence anchors:** Section 3.2 describes the alternative fusion approach, Table 3 shows LEO outperforming Eagle despite Eagle using more vision experts, and related work "Mixpert" supports the premise that mixing diverse vision experts requires careful conflict mitigation.

### Mechanism 2
- **Claim:** Tile-level sequential interleaving maintains local correspondence between spatial and semantic features, enhancing fine-grained understanding (OCR/Chart).
- **Mechanism:** LEO interleaves tokens tile-by-tile (e.g., [Tile1_EncA, Tile1_EncB, Tile2_EncA...]) instead of concatenating all tokens from Encoder A then Encoder B, creating a local context window where the LLM attends to semantic and spatial features of the same image region simultaneously.
- **Core assumption:** The LLM's attention mechanism benefits from proximate access to different feature modalities corresponding to the same image region, rather than retrieving them from distant parts of the sequence.
- **Evidence anchors:** Abstract mentions sequential interleaving at tile-level, Section 3.2 describes the Fusion Block, and Table 6 shows Sequence Concatenation outperforming Channel Concatenation across 7/8 benchmarks in the post-adaptation setting.

### Mechanism 3
- **Claim:** Pixel unshuffling effectively balances high-resolution requirements with the LLM's limited context window.
- **Mechanism:** LEO reduces the token count per tile by rearranging spatial pixels into the channel dimension (Pixel Unshuffle), allowing the model to ingest 448×448 tiles without exploding the sequence length, keeping visual token count at 256 per tile per encoder.
- **Core assumption:** The visual information lost by reducing spatial resolution is less critical than the information gained by processing images at higher native resolutions and integrating diverse expert features.
- **Evidence anchors:** Section 3.2 describes downscaling factors of 2 and 4, Figure 3 visualizes the Pixel unshuffle block, and LEO-MINI proposes Conditional Token Reduction, implying token reduction is a critical sub-problem in this architecture class.

## Foundational Learning

- **Concept: Vision Projectors (Adapters)**
  - **Why needed here:** LEO relies on the theory that the method of connecting vision encoders to LLMs dictates performance. A Projector translates image features into "language" the LLM can process.
  - **Quick check question:** If you used a single linear layer to project both InternViT and SAM features simultaneously (pre-adaptation), how might the gradient updates bias the weights toward the encoder with larger magnitude feature norms?

- **Concept: Pixel Unshuffling (Rearrangement)**
  - **Why needed here:** This is the efficiency engine of LEO, differing from standard pooling or cropping.
  - **Quick check question:** Given a 448x448 input and a factor of 2, how does pixel unshuffling change the tensor shape [Batch, Channels, Height, Width] compared to a stride-2 convolution?

- **Concept: Tile-based Processing**
  - **Why needed here:** High-resolution understanding in MLLMs is often bottlenecked by the context window. Tiling splits the problem but introduces the challenge of maintaining global context.
  - **Quick check question:** How does LEO ensure the model understands the "whole picture" if it processes the image as separate 448x448 patches? (Hint: check the "Dynamic high resolution" section regarding thumbnails).

## Architecture Onboarding

- **Component map:** Input: High-res Image -> Tiling Engine (splits to 448px + 1 Thumbnail) -> Branch A: InternViT-300M -> Pixel Unshuffle(r=2) -> Projector A (MLP) AND Branch B: SAM-L -> Pixel Unshuffle(r=4) -> Projector B (MLP) -> Fusion: Sequential Interleaving (Tile 1_A + Tile 1_B ...) -> LLM: InternLM2-7B

- **Critical path:** The Fusion Block is the architectural pivot. Ensuring that Projector A and Projector B output dimensions match the LLM hidden size exactly is vital for the interleaving to function without alignment errors.

- **Design tradeoffs:** Resolution vs. Context: LEO uses up to 6 tiles, consuming approximately 3000 visual tokens, limiting conversation turns or multi-image inputs. Detail vs. Redundancy: Using InternViT AND SAM provides robustness but doubles the visual encoding compute compared to single-encoder models.

- **Failure signatures:** Semantic-Spatial Disconnection: If interleaving isn't perfectly synchronized, the model might associate SAM features of "Tile 1" with InternViT features of "Tile 2," causing object hallucination. Global Loss: Without the thumbnail (global context) injection, the model may fail at questions requiring holistic scene reasoning.

- **First 3 experiments:**
  1. Ablate the Fusion Point: Train LEO with Pre-adaptation fusion (shared projector) vs. Post-adaptation (dual projectors) to verify gains come from architectural separation.
  2. Encoder Substitution: Replace SAM-L with a text-focused encoder (like a specialized OCR ViT) to determine if gains are from "Any mixture" or specifically "Semantic + Spatial" mixture.
  3. Token Reduction Sensitivity: Vary the Pixel Unshuffle factor (r=1 vs r=2 vs r=4) to find the "compression cliff" where OCR performance drops significantly.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the LEO architecture be effectively scaled to support more than six input patches or higher resolution inputs without exceeding the 8196 token context length or incurring prohibitive computational costs?
- **Basis in paper:** The limitation section states the processing capacity is restricted to a maximum of six patches due to LLM context length constraints.
- **Why unresolved:** The authors identify this as a hard constraint of the current implementation but do not propose architectural modifications to overcome the fixed context window.
- **What evidence would resolve it:** A study demonstrating LEO's performance stability and memory efficiency when processing inputs segmented into 8, 10, or 12 tiles, potentially utilizing longer context windows or token compression techniques.

### Open Question 2
- **Question:** How does LEO's performance in autonomous driving scenarios change when scaled to the full temporal context (e.g., 5 frames) used by state-of-the-art baselines, compared to the 2-frame limit evaluated in this study?
- **Basis in paper:** In the autonomous driving experiments, the authors note they used only two frames during training "due to computational limitations," whereas comparable baselines like LingoQA utilize sequences of five frames.
- **Why unresolved:** It is unclear if the model's superior performance on 2-frame inputs scales linearly or if it suffers from attention degradation when processing longer temporal sequences standard in the domain.
- **What evidence would resolve it:** Benchmark results on LingoQA or similar datasets comparing LEO's performance using 2 frames versus 5 frames to isolate the impact of temporal context on the dual-encoder fusion.

## Limitations

- Dataset Composition Uncertainty: The exact composition and distribution of the ~1M samples in Stage 2 (InternVL's SFT dataset) is not fully enumerated, which may affect reproducibility of specific benchmark performance.
- Context Window Constraints: LEO processes up to 6 tiles plus a thumbnail, consuming approximately 3000 visual tokens, which significantly limits the number of conversation turns or multi-image inputs the model can handle simultaneously.
- Fusion Pattern Specificity: The paper describes sequential interleaving of visual tokens at tile level but doesn't explicitly detail the exact pattern, which could impact the model's ability to maintain local correspondence between semantic and spatial features.

## Confidence

**High Confidence:** The architectural framework of dual-branch vision encoders with separate projectors and tile-level sequential interleaving is clearly specified and supported by direct evidence from the paper's methodology section and ablation studies in Tables 5 and 6.

**Medium Confidence:** The performance claims across 13 benchmarks are well-supported by the presented results, though the exact contribution of each component (tiling, unshuffling, fusion strategy) to specific gains remains partially confounded without more granular ablation.

**Low Confidence:** The assertion that LEO achieves "state-of-the-art" performance is qualified by the paper's acknowledgment that it's compared against "open-source" and "hybrid" MLLMs, not all commercial systems, and the performance gap on specific tasks may be sensitive to dataset preprocessing variations.

## Next Checks

1. **Ablation of Fusion Strategy:** Systematically train and evaluate LEO variants with pre-adaptation (shared projector) versus post-adaptation (dual projectors) fusion to quantify the exact performance contribution of architectural separation versus the quality of the vision encoders themselves.

2. **Encoder Diversity Test:** Replace SAM-L with a text-focused encoder (such as a specialized OCR ViT) while keeping InternViT unchanged to determine whether performance gains stem from "semantic + spatial" mixture specifically or would generalize to any mixture of heterogeneous vision experts.

3. **Token Reduction Sensitivity Analysis:** Vary the pixel unshuffling factors (r=1, r=2, r=4) and measure the corresponding impact on OCR and fine-grained visual understanding tasks to identify the "compression cliff" where resolution reduction begins to significantly degrade performance, validating the efficiency-accuracy tradeoff claimed by the paper.