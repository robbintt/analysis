---
ver: rpa2
title: Predicting Onflow Parameters Using Transfer Learning for Domain and Task Adaptation
arxiv_id: '2506.14784'
source_url: https://arxiv.org/abs/2506.14784
tags:
- learning
- data
- transfer
- domain
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a transfer learning framework for predicting
  onflow parameters (angle of attack and onflow speed) using surface pressure data
  from airfoils. The method trains a convolutional neural network offline on source
  data, then adapts to new domains or tasks by retraining selected layers while freezing
  others.
---

# Predicting Onflow Parameters Using Transfer Learning for Domain and Task Adaptation

## Quick Facts
- arXiv ID: 2506.14784
- Source URL: https://arxiv.org/abs/2506.14784
- Reference count: 40
- Primary result: Transfer learning framework successfully adapts CFD-trained ConvNets to new turbulence models and tasks, with shallow networks showing superior adaptation performance

## Executive Summary
This paper presents a transfer learning framework for predicting aerodynamic parameters (angle of attack and onflow speed) from surface pressure data using convolutional neural networks. The method trains models offline on source CFD data, then adapts to new domains or tasks by selectively retraining layers while freezing feature extractors. Demonstrated using a 2D NLR 7301 airfoil with two turbulence models, the approach shows successful domain adaptation, extension, and task adaptation, though it struggles with noisy data. ConvNet architectures outperform fully connected networks, with shallow networks adapting better due to higher retrainable weight percentages.

## Method Summary
The framework trains ConvNet models offline on CFD data from a source domain (SA turbulence model), then adapts to target domains by freezing convolutional feature extractors and retraining selected fully-connected layers. For domain adaptation, the model adapts to different data distributions while maintaining the same prediction task. For task adaptation, the model predicts a different parameter (angle of attack → onflow speed) using the same pressure data. The approach leverages the spatial structure of pressure distributions through 1D convolutions, enabling efficient local feature extraction invariant to translation.

## Key Results
- Domain adaptation reduces MAE by approximately 10× compared to using source-only models on target domains
- Shallow ConvNet-S architecture outperforms deep ConvNet-D for transfer learning due to higher retrainable weight percentage (98.7% vs 70.4%)
- ConvNet architectures consistently outperform fully connected networks for both prediction tasks
- Transfer learning reduces training time compared to full offline training, though real-time adaptation is not achieved
- The method struggles with noisy input domains, showing increased error by 1-2 orders of magnitude

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Freezing convolutional feature extractors while retraining final layers enables efficient domain adaptation for aerodynamic parameter prediction.
- Mechanism: The framework freezes convolutional layers that have learned generic pressure distribution features from CFD data and only retrains selected fully-connected layers preceding the output. This preserves learned feature hierarchies while adapting to new data distributions.
- Core assumption: Features extracted by convolutional layers from source domain (SA turbulence model) remain discriminative for target domain (RSM turbulence model) despite distribution shifts.
- Evidence anchors:
  - [abstract]: "It requires first training a convolutional neural network (ConvNet) model offline for the core prediction task, then freezing the weights of this model except the selected layers preceding the output node"
  - [section IV.C, p.21]: "The test set MAE values obtained with NOL and NT L are higher for the domains, DR and DS, respectively... These observations imply that the domains are sufficiently different to investigate the influence of transfer learning."
  - [corpus]: Related work on multi-domain adaptation (arXiv:2512.14710) confirms selective source knowledge transfer improves adaptation, supporting the freezing strategy.
- Break condition: If source and target domains share minimal feature overlap (e.g., fundamentally different physics beyond turbulence model variation), frozen features may fail to transfer effectively.

### Mechanism 2
- Claim: Shallow networks outperform deep networks for transfer learning due to higher percentage of trainable weights during adaptation.
- Mechanism: ConvNet-S (shallow) retrainable weight percentage is 0.086% (1 layer) vs. 0.017% for ConvNet-D, providing greater flexibility to adjust predictions during transfer learning while keeping feature extractors frozen.
- Core assumption: Adaptation performance depends on having sufficient learnable parameters to fit target domain-specific patterns without overfitting to limited target data.
- Evidence anchors:
  - [section IV.D, p.24]: "We assume that these observations stem from the percentage of weights that we enable to retrain during transfer learning, which is lower for the dense architecture"
  - [section IV.F, p.27-28]: "This might result from the percentage of the retrained weights, which is greater for the shallow network, and thus provides extra flexibility during the training optimization."
  - [corpus]: Scaling laws for domain adaptation (arXiv:2510.23198) suggest adaptation performance depends on pre-training and fine-tuning budget balance, consistent with architecture depth tradeoffs.
- Break condition: If target domain requires learning substantially different feature representations (not just output mapping adjustments), deeper networks with more frozen feature capacity may become necessary despite lower adaptation flexibility.

### Mechanism 3
- Claim: Convolutional architectures capture spatial pressure distribution patterns more effectively than fully-connected networks for onflow parameter inference.
- Mechanism: 1D convolutional filters slide across equidistant surface pressure arrays, detecting local pressure gradients and distribution shapes that correlate with angle of attack and flow speed, leveraging weight sharing and translation invariance.
- Core assumption: Local neighboring pressure relationships encode critical aerodynamic information that can be extracted via convolution operations similar to image feature detection.
- Evidence anchors:
  - [section II, p.8]: "ConvNets are particularly efficient in detecting local features of neighboring elements by applying a series of convolutional filters... extracting local features invariant to translation is possible."
  - [section IV.B, p.15-16, Fig.7]: "In comparison with the fully connected architecture, the ConvNet architectures achieve lower MAE values for both prediction tasks except for the minimum dataset size."
  - [corpus]: Weak direct corpus evidence for ConvNet vs. FCNN in aerodynamic applications; related wind prediction work focuses on temporal/ensemble methods rather than spatial architecture comparisons.
- Break condition: If pressure sensor placement is highly irregular or sparse spatial structure is lost, convolution's locality assumptions may fail and fully-connected architectures could recover utility.

## Foundational Learning

- **Convolutional Neural Networks (1D)**
  - Why needed here: Core architecture for processing sequential pressure data as spatial patterns. Understanding 1D convolutions, pooling, and feature hierarchy formation is essential.
  - Quick check question: Can you explain why a 1D convolution with kernel size 11 and stride 4 would reduce an input array of 75 pressure points to a smaller feature map?

- **Transfer Learning Paradigms**
  - Why needed here: The entire methodology rests on distinguishing domain adaptation (same task, different distribution) from task adaptation (different task, same distribution) and knowing when to freeze vs. retrain layers.
  - Quick check question: Given a model trained to predict angle of attack from pressure data, would adapting it to predict onflow speed instead require domain adaptation or task adaptation?

- **Aerodynamic Pressure-Flow Relationships**
  - Why needed here: Understanding how surface pressure distributions physically relate to angle of attack and flow speed provides intuition for why the approach works and where it might fail (e.g., separated flow regions).
  - Quick check question: Why might pressure-based prediction become less accurate at high angles of attack where flow separation occurs?

## Architecture Onboarding

- **Component map:**
  Input (pressure array) → Conv1(1→64, k=11, s=4, p=2) → ReLU → MaxPool(k=3, s=2) → Conv2(64→192) → [Conv3-5 for ConvNet-D only] → AdaptiveAvgPool → Linear1 → [Linear2 for ConvNet-D] → Linear_output → Output (α or V∞)

- **Critical path:**
  1. Generate source domain CFD data with consistent turbulence model across α/V∞ parameter space
  2. Train ConvNet-S offline until validation loss plateaus
  3. For domain adaptation: Initialize new network with frozen conv weights, retrain only final linear layer on target domain data
  4. For task adaptation: Freeze conv layers, retrain Linear1+Linear2 (98.7% of ConvNet-S weights) for new output variable

- **Design tradeoffs:**
  - Shallow vs. Deep: ConvNet-S adapts better (more retrainable weights), ConvNet-D approximates better offline (more total capacity)
  - Surface points vs. Dataset size: No clear trend for point count; dataset size (≥512 samples) more impactful
  - Retrain 1 vs. 2 layers: 2 layers improves task adaptation substantially, minimal difference for domain adaptation
  - Min-max normalization keeps inputs in [0,1]; MSE loss for differentiability, MAE for evaluation robustness

- **Failure signatures:**
  - Noisy input domains: MAE increases by 1-2 orders of magnitude; transfer learning degrades source domain accuracy
  - Domain extrapolation: Offline models fail at extended boundaries (high α with separation)
  - Insufficient data: All architectures fail at nd=128; FCNN never reaches acceptable accuracy
  - Task mismatch: Retraining only 1 layer for task adaptation provides no improvement over source task performance

- **First 3 experiments:**
  1. **Baseline establishment**: Train ConvNet-S on SA turbulence model data (ns=75, nd=1024) for α prediction. Measure test MAE. Verify convergence and check error distribution across α/V∞ space (expect higher errors at domain boundaries with flow separation).
  2. **Domain adaptation test**: Transfer learned weights to RSM turbulence model data. Freeze conv layers, retrain only final linear layer. Compare MAE on target domain to: (a) source model on target domain, (b) fully retrained model. Confirm MAE reduction of ~1 order of magnitude from unfrozen source model.
  3. **Task adaptation with layer sweep**: Use α-trained model to adapt for V∞ prediction. Test both 1-layer and 2-layer retraining configurations. Verify 2-layer retraining achieves MAE within 1 order of magnitude of directly trained V∞ model, while 1-layer shows no improvement.

## Open Questions the Paper Calls Out

- **Question 1**: Can the transfer learning framework successfully bridge the "sim-to-real" gap by transferring learned experience from offline CFD simulations to physical wind tunnel experiments?
  - Basis in paper: [explicit] The authors state, "A future research objective is the investigation of the applicability of the proposed method to transfer the learned experience from CFD runs to wind tunnel experiments and to make accurate predictions online during the experiments."
  - Why unresolved: The current study validates the framework exclusively using steady CFD data generated with RANS equations (SA and RSM turbulence models) and does not test against experimental data.
  - What evidence would resolve it: Demonstration of the framework maintaining high prediction accuracy for angle of attack and speed when adapting a CFD-pretrained model to live wind tunnel data.

- **Question 2**: What methodologies can improve the framework's robustness and adaptation capabilities when operating on domains with noisy input data?
  - Basis in paper: [explicit] The conclusion lists as a future direction "the exploration of alternative ways to improve the adaptation to the domains with noise," noting that the current application is "not as effective."
  - Why unresolved: The results show that the transfer learning approach struggles with noisy data domains, yielding much higher errors compared to other adaptation cases like domain extension.
  - What evidence would resolve it: A modification to the network architecture or training protocol that yields comparable Mean Absolute Error (MAE) in noisy domains relative to the noise-free baseline.

- **Question 3**: Can the training duration for the transfer learning phase be reduced to a level that supports real-time learning for online monitoring?
  - Basis in paper: [explicit] The text states that future directions include exploring ways "to reduce the training times required during the transfer learning phase."
  - Why unresolved: While transfer learning is faster than full offline training, the authors concede that "real-time learning has not been achieved" and training times currently exceed limits for real-time applications.
  - What evidence would resolve it: Achieving training times that fall within the strict latency requirements of real-time systems (e.g., wind tunnel monitoring) without significant loss in prediction accuracy.

## Limitations

- Poor performance on noisy data domains where MAE increases by 1-2 orders of magnitude
- Limited effectiveness at domain boundaries where flow separation occurs
- Method depends heavily on source-target domain similarity for successful adaptation

## Confidence

- **High**: Domain adaptation effectiveness (10× MAE improvement), shallow network adaptation advantage (98.7% retrainable weights vs 70.4%), convolutional architecture superiority over fully connected networks for spatial pressure data
- **Medium**: Task adaptation performance (only works with 2-layer retraining), real-time learning claims (training time reduction but not true real-time adaptation)
- **Low**: Extrapolation capability (limited success at domain boundaries), noisy data handling (explicitly identified as failure mode)

## Next Checks

1. Test transfer learning performance when source and target domains differ by turbulence model and airfoil geometry to verify feature transferability beyond distribution shifts.
2. Evaluate adaptation with alternative normalization schemes (z-score vs min-max) to determine if input scaling affects transfer learning effectiveness.
3. Compare shallow vs. deep architecture performance on a domain adaptation task where target data is limited (128-256 samples) to confirm the weight percentage hypothesis.