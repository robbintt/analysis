---
ver: rpa2
title: Unpacking the Implicit Norm Dynamics of Sharpness-Aware Minimization in Tensorized
  Models
arxiv_id: '2508.10435'
source_url: https://arxiv.org/abs/2508.10435
tags:
- norm
- tensor
- learning
- dynamics
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the implicit regularization of Sharpness-Aware
  Minimization (SAM) in general multi-core tensorized models. It introduces Norm Deviation
  as a global measure of core norm imbalance and analyzes its dynamics under SAM and
  SGD using gradient flow analysis.
---

# Unpacking the Implicit Norm Dynamics of Sharpness-Aware Minimization in Tensorized Models

## Quick Facts
- arXiv ID: 2508.10435
- Source URL: https://arxiv.org/abs/2508.10435
- Reference count: 40
- This paper investigates the implicit regularization of Sharpness-Aware Minimization (SAM) in general multi-core tensorized models and proposes Deviation-Aware Scaling (DAS) to mimic SAM's norm dynamics without adversarial perturbations.

## Executive Summary
This paper analyzes the implicit regularization of Sharpness-Aware Minimization (SAM) in tensorized models by introducing Norm Deviation as a global measure of core norm imbalance. The authors prove that SAM's implicit control of Norm Deviation is governed by the covariance between core norms and their gradient magnitudes, with negative covariance driving cores toward equal norms. Based on this insight, they propose Deviation-Aware Scaling (DAS), which explicitly mimics SAM's norm dynamics through core scaling without computing adversarial perturbations. Experiments across tensor completion, noisy training, model compression, and parameter-efficient fine-tuning show that DAS achieves competitive or improved performance over SAM while offering reduced computational overhead, with runtime savings of over 90% in some cases.

## Method Summary
The paper analyzes Sharpness-Aware Minimization (SAM) in tensorized models by introducing Norm Deviation Q as a measure of core norm imbalance. SAM's implicit regularization is shown to depend on the covariance between core norms and gradient magnitudes under gradient flow analysis. The proposed Deviation-Aware Scaling (DAS) method mimics this effect through explicit per-core scaling λk = ηαu(t)/∥Gk∥²F · (∥gk∥²F − ḡ) without adversarial perturbations. DAS is tested across Tucker tensor completion, tensorized CNNs with label noise, model compression, and LoRA fine-tuning for transformers, comparing against SAM, Adam, and other baselines.

## Key Results
- DAS achieves competitive or improved performance over SAM across multiple tasks while reducing computational overhead by 90%+ in some cases
- SAM's implicit control of Norm Deviation is governed by covariance between core norms and gradient magnitudes, with negative covariance driving balancing
- DAS matches SAM's implicit regularization through explicit scaling without adversarial perturbations
- In tensor completion, DAS achieves R² = 0.9487 on COVID dataset, comparable to SAM
- On CIFAR-10 with 80% label noise, DAS achieves 60.74% accuracy, slightly below SAM's 67.13% but above Adam's 48.23%

## Why This Works (Mechanism)

### Mechanism 1: Norm Deviation Control via Covariance
- **Claim**: SAM implicitly regulates global norm imbalance (Norm Deviation Q) through the covariance between core norms and gradient magnitudes.
- **Mechanism**: Under SAM gradient flow, dQ/dt = 4ρu(t)K · Cov(∥Gk∥²F, ∥gk∥²F). When smaller cores have larger gradients (negative covariance), SAM accelerates their growth, pushing all cores toward equal norms. This balancing effect is amplified by data noise.
- **Core assumption**: Multilinear reconstruction Φ with scale-invariance (∏ck = 1), Lipschitz smoothness of f, infinitesimal stepsize η → 0.
- **Evidence anchors**:
  - [abstract]: "SAM's implicit control of Norm Deviation is governed by the covariance between core norms and their gradient magnitudes."
  - [Theorem 3]: Formal derivation of Q dynamics under SAM.
  - [corpus]: Related work (Li et al. 2024) establishes SAM promotes norm balancing in matrix factorization; this paper generalizes to multi-core tensors.
- **Break condition**: Positive covariance (large cores with large gradients) increases imbalance; O(ρ²L) term dominates for large ρ.

### Mechanism 2: Deviation-Aware Scaling Mimics SAM Without Perturbation
- **Claim**: DAS replicates SAM's norm regularization through explicit per-core scaling, eliminating the adversarial perturbation step.
- **Mechanism**: Scaling factor λk = ηαu(t)/∥Gk∥²F · (∥gk∥²F − ḡ) directly modifies core norms before the gradient step. This matches ∆Q ≈ ∆Q_SAM when α ≈ ρ, but α is tuned independently.
- **Core assumption**: SGD preserves Q (Corollary 1) so only scaling affects deviation; small λk approximation holds.
- **Evidence anchors**:
  - [Algorithm 2]: Complete DAS implementation with closed-form λk.
  - [Section 4]: Derivation matching ∆Q scaling to SAM's implicit change.
  - [corpus]: Weak—related SAM efficiency methods (Async-SAM, LightSAM) use different approximations; none use norm-based scaling.
- **Break condition**: Large λk invalidates linear approximation; base optimizer with momentum breaks Q conservation.

### Mechanism 3: Local Pairwise Shrinkage Accumulates to Global Balancing
- **Claim**: SAM shrinks pairwise norm differences when gaps are large, and this accumulates to global Q reduction.
- **Mechanism**: For cores i, j with ∥Gi∥²F > ∥Gj∥²F, the gap Bij shrinks when Bij > (ᾱ² − 1/ᾱ²)G² where ᾱ² = √(Ci/Cj). Local corrective dynamics aggregate via Q = (1/2K)∑(∥Gi∥²F − ∥Gj∥²F)².
- **Core assumption**: Positive gradient norms; reparameterization gradients scale as ∥gi∥²F ∝ 1/α².
- **Evidence anchors**:
  - [Proposition 1]: Formal shrinkage condition.
  - [Figure 1]: Empirical confirmation that Q decreases under SAM, faster with higher noise.
  - [corpus]: Related papers on implicit regularization in tensor factorization (Hariz et al. 2024) support norm balancing benefits.
- **Break condition**: Small ρ reduces shrinkage force; gradient ratios that don't favor balancing.

## Foundational Learning

- **Concept**: Tensor Decomposition (Tucker, CP, Tensor-Train, Tensor-Ring)
  - **Why needed here**: Models use multi-core tensor parameterization; understanding reconstruction Φ and scale-invariance is essential.
  - **Quick check question**: Given cores {G1, G2, G3} with norms [2, 5, 10], what is the Norm Deviation Q?

- **Concept**: Scale Invariance and Rescaling Ambiguity
  - **Why needed here**: The analysis relies on ∏ck = 1 preserving the reconstructed tensor; norm dynamics only make sense under this constraint.
  - **Quick check question**: If you double G1 and halve G2 in a two-core model, does the output change?

- **Concept**: Gradient Flow Analysis (η → 0)
  - **Why needed here**: All theorems use continuous-time dynamics; discrete step analysis would require additional O(ρ²) corrections.
  - **Quick check question**: Why does Corollary 1 (dQ/dt = 0 for SGD) fail when momentum is added?

## Architecture Onboarding

- **Component map**:
  Input: Core gradients {gk}, core norms {∥Gk∥²F}
  ↓
  Compute: ḡ = mean(∥gk∥²F), u(t) = 1/√(K·ḡ)
  ↓
  Per-core: λk = ηαu(t)/∥Gk∥²F · (∥gk∥²F − ḡ)
  ↓
  Update: Gk ← (1 + λk)Gk − ηgk

- **Critical path**: Computing ∥gk∥²F for all K cores each iteration. This requires K norm computations but avoids SAM's second forward-backward pass.

- **Design tradeoffs**:
  - **α vs ρ**: DAS uses α independently of SAM's ρ; empirical results suggest α ∈ [0.001, 0.1] depending on task.
  - **Efficiency vs robustness**: DAS achieves ~90% runtime savings (Table 6) but slightly weaker noise robustness than SAM (Table 2).
  - **Base optimizer**: SGD preserves Q better theoretically; Adam used in practice for stability.

- **Failure signatures**:
  - Exploding norms: λk too large (α too high or ∥Gk∥²F near zero).
  - No improvement over baseline: α too small or gradient variance too high.
  - Instability with momentum: Q conservation violated; consider reducing momentum.

- **First 3 experiments**:
  1. **Tucker completion sanity check**: Apply DAS to a small Tucker decomposition (COVID dataset setup). Verify Q decreases and R² improves over Adam baseline. Target: R² ≈ 0.948.
  2. **Ablation on α**: Sweep α ∈ {0.001, 0.01, 0.05, 0.1} on TR-ResNet-20/CIFAR-10. Plot accuracy vs α and Q dynamics. Expect optimal α ≈ 0.01–0.05.
  3. **Runtime benchmark**: Compare wall-clock time for SAM vs DAS on OPT-6.7B fine-tuning with LoRETTA. Target: DAS within 5–10% of Adam runtime, SAM ~2× Adam.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How do Norm Deviation dynamics evolve under SAM and DAS when momentum-based optimizers are used instead of vanilla gradient descent?
- **Basis in paper**: [explicit] The Limitations section states: "A rigorous analysis of the Norm Deviation dynamics under momentum would require modeling the system with a second-order ordinary differential equation... Such an investigation exceeds the scope of this paper but represents a crucial direction for future work."
- **Why unresolved**: Theorems 1-3 assume gradient flow (η → 0), but experiments use SGD with momentum. The authors acknowledge that Q is not strictly preserved empirically, creating a theory-practice gap.
- **What evidence would resolve it**: Formal derivation under a "heavy-ball" ODE model and empirical validation comparing predicted versus observed Q dynamics with momentum optimizers.

### Open Question 2
- **Question**: Can the conjecture that SAM promotes global norm balancing via accumulation of local pairwise shrinkage effects be formally proven?
- **Basis in paper**: [inferred] The paper states: "Empirical observations consistently show that Q decreases during training... supporting the conjecture that SAM promotes norm balancing through an accumulation of these local shrinkage effects." Proposition 1 only proves local shrinkage.
- **Why unresolved**: Local pairwise dynamics (Proposition 1) are established, but the link to global Q decrease remains empirical and conjectural without formal proof conditions.
- **What evidence would resolve it**: A theoretical proof that local shrinkage implies global Q decrease under specific assumptions, or counterexamples where local effects don't aggregate globally.

### Open Question 3
- **Question**: What SAM regularization effects beyond norm dynamics contribute to its superior label-noise robustness, and can they be incorporated into DAS?
- **Basis in paper**: [inferred] Table 2 shows DAS underperforms SAM under label noise (60.74% vs 67.13% at 80% noise). The text notes: "Prior work [5] attributes SAM's robustness to its influence mainly on network Jacobian effects that DAS does not replicate."
- **Why unresolved**: DAS captures norm dynamics but misses other SAM effects. The nature of these additional effects and their efficient integration into scaling-based methods is unknown.
- **What evidence would resolve it**: Ablation studies comparing Jacobian and loss landscape properties under SAM vs DAS; DAS extensions incorporating identified effects with improved noise robustness.

### Open Question 4
- **Question**: How robust are the theoretical predictions and DAS effectiveness to violations of scale-invariance and multilinearity in practical architectures?
- **Basis in paper**: [inferred] The framework assumes multilinear Φ and exact scale-invariance, but practical networks include batch normalization, residual connections, and activations that may violate these.
- **Why unresolved**: No analysis of sensitivity to assumption violations is provided, despite experiments using architectures with potentially non-scale-invariant components.
- **What evidence would resolve it**: Experiments with tensorized networks incorporating BN/residuals; perturbation analysis for approximately scale-invariant settings.

## Limitations
- Theoretical analysis relies on gradient flow (η → 0) and exact scale-invariance, which may not hold in practice
- DAS is competitive but not universally superior—especially in noise robustness where SAM still outperforms
- The choice of α is task-dependent and not directly derived from SAM's ρ, requiring per-task tuning
- The SAM-inspired mechanism is specific to the choice of multi-linear reconstruction Φ, limiting generalization

## Confidence
- **High**: Norm Deviation Q decreases under SAM and DAS (Theorem 3, empirical Q curves); DAS achieves significant runtime savings (Table 6)
- **Medium**: Covariance between core norms and gradients drives SAM's implicit balancing (Theorem 3 derivation); DAS matches SAM's implicit regularization via scaling (Section 4)
- **Low**: Generalization of SAM's implicit balancing to arbitrary tensor networks; precise relationship between α and ρ across tasks

## Next Checks
1. **Negative covariance case**: Construct a synthetic tensor task where smaller cores have smaller gradients (positive covariance). Verify that SAM increases Q (breaks balance) while DAS cannot correct this, confirming the mechanism's dependence on negative covariance.
2. **Momentum sensitivity**: Test DAS with SGD+momentum (ρ=0.9) on CIFAR-10. Measure Q dynamics and accuracy. Confirm theoretical prediction that Q is no longer conserved, leading to instability.
3. **Cross-architecture transfer**: Apply DAS to Tensor-Train Language Models (e.g., TT-LLM on WikiText-2). Track Q and perplexity. Determine if the norm-balancing mechanism extends beyond CNNs and transformer adapters.