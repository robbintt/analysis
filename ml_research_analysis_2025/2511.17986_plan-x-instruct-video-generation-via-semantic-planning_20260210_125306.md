---
ver: rpa2
title: 'Plan-X: Instruct Video Generation via Semantic Planning'
arxiv_id: '2511.17986'
source_url: https://arxiv.org/abs/2511.17986
tags:
- semantic
- video
- visual
- arxiv
- diffusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Plan-X introduces a semantic planner that generates structured,
  spatio-temporal visual tokens to guide video diffusion models, decoupling high-level
  semantic reasoning from low-level synthesis. This approach significantly reduces
  visual hallucinations and improves prompt alignment by enabling the language model
  to produce interpretable, text-grounded semantic "sketches" that direct the DiT's
  rendering process.
---

# Plan-X: Instruct Video Generation via Semantic Planning

## Quick Facts
- arXiv ID: 2511.17986
- Source URL: https://arxiv.org/abs/2511.17986
- Authors: Lun Huang, You Xie, Hongyi Xu, Tianpei Gu, Chenxu Zhang, Guoxian Song, Zenan Li, Xiaochen Zhao, Linjie Luo, Guillermo Sapiro
- Reference count: 40
- Primary result: Decouples semantic planning from visual synthesis to reduce hallucinations and improve instruction alignment in complex video generation

## Executive Summary
Plan-X introduces a semantic planner that generates structured, spatio-temporal visual tokens to guide video diffusion models, decoupling high-level semantic reasoning from low-level synthesis. This approach significantly reduces visual hallucinations and improves prompt alignment by enabling the language model to produce interpretable, text-grounded semantic "sketches" that direct the DiT's rendering process. Evaluated on complex video generation tasks involving human-object interactions and multi-stage actions, Plan-X achieves substantial gains in accuracy (0.7816), completeness (0.8263), fidelity (0.9500), and consistency (0.9816), outperforming state-of-the-art models and demonstrating strong human preference (0.262).

## Method Summary
Plan-X employs a two-stage pipeline: first, a Multimodal Large Language Model (MLLM) acts as a Semantic Planner, autoregressively generating text-grounded spatio-temporal semantic tokens (TA-Tok) from video context; second, these tokens guide a Diffusion Transformer (DiT) via a dedicated semantic branch with 3D RoPE positional embeddings. The semantic planner uses Qwen-2.5-Instruct with SigLIP2 encoding, trained on 4.5M video clips to predict 81-token keyframes sampled at 2 FPS. The DiT (Wan 2.2 or Seedance) is fine-tuned with staged text dropout and end-to-end joint training to learn semantic-visual alignment. This decoupling allows the DiT to focus on pixel-level synthesis while the planner handles high-level temporal logic and object permanence.

## Key Results
- Achieves 0.7816 Accuracy, 0.8263 Completeness, 0.9500 Fidelity, and 0.9816 Consistency on complex video tasks
- Outperforms state-of-the-art models in both MLLM-based and human preference evaluations (0.262 preference score)
- Ablation studies confirm 3D RoPE is critical (Accuracy drops from 0.7816 to 0.5889 without it) and staged training prevents semantic drift
- Successfully handles human-object interactions and multi-stage actions with minimal visual hallucinations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Decoupling semantic reasoning from visual synthesis reduces prompt misalignment and hallucination in complex, long-horizon video generation.
- **Mechanism:** A Multimodal Large Language Model (MLLM) acts as a "Semantic Planner," offloading the burden of temporal logic and object permanence from the Diffusion Transformer (DiT). The DiT consumes this pre-planned semantic trajectory rather than inferring it implicitly from text, allowing it to specialize in high-fidelity pixel synthesis.
- **Core assumption:** DiTs are fundamentally limited in high-level semantic abstraction and long-horizon reasoning due to the entanglement with pixel-level synthesis [Introduction].
- **Evidence anchors:**
  - [abstract]: "...decoupling high-level semantic reasoning from low-level synthesis."
  - [section 1]: "The key difficulty... lies in the entanglement between semantic reasoning and pixel-level synthesis."
  - [corpus]: Related work "The Best of Both Worlds" supports the integration of LLMs for reasoning and diffusion for generation, validating the decoupling paradigm.

### Mechanism 2
- **Claim:** Structured spatio-temporal tokens (TA-Tok) provide more precise conditioning than text captions alone, mitigating "semantic drift."
- **Mechanism:** The system translates text and visual context into "Text-Aligned Tokens" (TA-Tok), which are discrete visual semantic tokens derived from SigLIP2. These tokens form a "semantic sketch" that explicitly grounds abstract instructions (e.g., "pick up") into specific spatial and temporal coordinates (keyframes), bridging the modality gap between language and video.
- **Core assumption:** Text-alone conditioning lacks the spatial granularity required to enforce frame-level consistency without hallucination.
- **Evidence anchors:**
  - [abstract]: "...autoregressively generates a sequence of text-grounded spatio-temporal semantic tokens."
  - [section 3.1]: "Unlike abstract textual scripting... [tokens] serve as structured 'semantic sketches' describing what should happen and when."
  - [corpus]: Evidence is weak regarding specific TA-Tok mechanics in neighbors; mechanism relies primarily on internal paper description.

### Mechanism 3
- **Claim:** 3D RoPE (Rotary Position Embeddings) is required to align sparse semantic tokens with dense video latents.
- **Mechanism:** The semantic tokens are sparse (sampled at 2 FPS) compared to the full video. To guide the DiT effectively, 3D RoPE injects explicit positional correspondence (time, height, width) into the attention layers. This ensures the DiT attends to the correct semantic instruction for a specific spatial region at a specific time step.
- **Core assumption:** Without explicit positional alignment, the diffusion model cannot reliably map high-level semantic plans to the correct temporal segments of the video latent.
- **Evidence anchors:**
  - [section 3.2]: "...enrich S with 3D time-aligned spatio-temporal Rotary Position Embeddings (RoPE)... ensuring that each position in the semantic map maintains consistent relative alignment."
  - [section 4.2]: Ablation study "Plan-X-Wan w/o 3D RoPE" shows a drop in Accuracy (0.7816 -> 0.5889) and Consistency (0.9816 -> 0.7311).

## Foundational Learning

- **Concept: Diffusion Transformers (DiTs)**
  - **Why needed here:** The underlying generator is a DiT (specifically Wan 2.2 or Seedance). You must understand how flow-matching/denoising works on tokenized latent patches to integrate the new semantic guidance branch.
  - **Quick check question:** How does cross-attention differ from the concatenated attention used in MMDiT architectures like Seedance?

- **Concept: Discrete Visual Tokenization (VQ-VAE / TA-Tok)**
  - **Why needed here:** The Semantic Planner does not output pixels or continuous embeddings, but discrete tokens from a codebook. Understanding how SigLIP2 features are quantized into an LLM vocabulary is crucial for the "Semantic Planner" design.
  - **Quick check question:** How does the model map a continuous SigLIP2 embedding to a discrete token index in the LLM's vocabulary?

- **Concept: Autoregressive Generation**
  - **Why needed here:** The Semantic Planner is based on Qwen-2.5 and uses next-token prediction to generate the video plan. You need to grasp how to format training data (prompts + frame sequences) for causal attention.
  - **Quick check question:** How do you mask attention to ensure the model predicts the next frame's semantic tokens based only on past context?

## Architecture Onboarding

- **Component map:** Semantic Planner (Qwen2.5-Instruct 7B + SigLIP2 Encoder) -> TA-Tok Tokenizer (codebook) -> Guidance Injector (MLP + 3D RoPE) -> DiT (Wan 2.2-5B or Seedance)

- **Critical path:** The training pipeline is staged. First, train the *Semantic Planner* to predict valid TA-Tok sequences. Second, train the *DiT's guidance branch* using ground-truth tokens. Finally, run *end-to-end fine-tuning* using the Planner's predictions (not ground truth) to expose the DiT to planner noise.

- **Design tradeoffs:**
  - *Planner Scale vs. Speed:* The 1.5B planner is faster but struggles with complex logic compared to the 7B variant (Table 1).
  - *Token Sparsity:* Sampling at 2 FPS reduces computation but may miss rapid motion nuances. This is a deliberate tradeoff for long-horizon stability.
  - *Conditioning:* Plan-X adds a separate semantic branch rather than injecting noise directly (like ControlNet), preserving the pre-trained DiT's fidelity but requiring more architecture modification.

- **Failure signatures:**
  - *Visual Hallucination:* If the Semantic Planner is omitted, the DiT reverts to text-only conditioning and hallucinates object interactions (Figure 4).
  - *Physical Implausibility:* If the DiT backbone is weak or semantic guidance is dropped, objects may undergo "non-rigid morphing" (Supplementary Figure 7).
  - *Semantic Drift:* Without 3D RoPE, the model fails to align actions with the correct temporal segments.

- **First 3 experiments:**
  1. **Validate Semantic Injection:** Ablate the 3D RoPE vs. standard positional encoding to confirm alignment gains on a short "pick and place" clip.
  2. **Planner vs. Prompt Enhancer:** Compare Plan-X against a standard "Prompt Enhancement" baseline (using an LLM to rewrite prompts) to verify that visual tokens outperform detailed text descriptions.
  3. **Semantic Transfer:** Run the "Cross-Transfer" test shown in Figure 3—apply semantic tokens generated for Video A to the visual context of Video B—to test the decoupling of semantics from visual style.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does replacing the TA-Tok tokenizer with a tokenizer capable of handling abstract or symbolic concepts improve Plan-X's performance on complex reasoning tasks?
- **Basis in paper:** [explicit] The authors state that the current TA-Tok tokenizer has "limited expressiveness in representing complex concepts and abstract reasoning (such as mathematical or symbolic content)."
- **Why unresolved:** The current architecture relies on a quantizer initialized from LLM embeddings, but it is not validated on non-visual semantic logic.
- **What evidence would resolve it:** A comparative study evaluating Plan-X on prompts requiring abstract visual reasoning (e.g., math diagrams) using a specialized tokenizer versus the standard TA-Tok.

### Open Question 2
- **Question:** To what extent does the scale of the semantic planner's training data determine its capacity for general commonsense reasoning?
- **Basis in paper:** [explicit] The paper notes the semantic planner was trained on ~4.5M text-video pairs and consequently "lacks strong abstract reasoning and common-sense intelligence."
- **Why unresolved:** It is undetermined if the observed failures are due to the architectural design or simply the limited scale of the planner's pre-training data.
- **What evidence would resolve it:** Benchmarking the semantic planner's reasoning accuracy as a function of training dataset size (e.g., 4.5M vs. 100M pairs) while keeping the model architecture fixed.

### Open Question 3
- **Question:** Does the sparse sampling of semantic tokens at 2 FPS constrain the model's ability to handle high-dynamic or fast-paced actions?
- **Basis in paper:** [inferred] The methodology explicitly samples keyframes at 2 FPS to "avoid overburdening the language model," but the paper does not analyze failure rates on rapid motion tasks where intermediate frames carry critical semantic weight.
- **Why unresolved:** It is possible that the semantic planner "misses" fast-occurring events that fall between the sampled keyframes, leading to temporal aliasing in the final video.
- **What evidence would resolve it:** An ablation study varying the semantic token frame rate (e.g., 2 FPS vs. 8 FPS) specifically on a dataset of high-speed interactions (e.g., sports).

## Limitations

- The TA-Tok tokenizer architecture and codebook remain inaccessible despite citation [12], creating a critical dependency gap for faithful reproduction
- Internal datasets (Taste-Rob, HOIGen-1M) are not released, preventing direct validation of the reported evaluation benchmark results
- The semantic planner's performance heavily depends on SigLIP2-based tokenization quality, which is not independently verified in the paper

## Confidence

**High confidence (Strong supporting evidence):**
- The decoupling of semantic reasoning from visual synthesis demonstrably reduces hallucination (supported by Figure 4 and ablation studies)
- 3D RoPE provides measurable alignment improvements (confirmed by ablation: 0.7816→0.5889 Accuracy without it)
- The staged training pipeline produces stable, interpretable semantic plans (validated through qualitative examples)

**Medium confidence (Moderate evidence, some gaps):**
- TA-Tok provides superior conditioning compared to text alone (based on internal comparisons, but TA-Tok implementation unclear)
- The semantic planner generalizes across T2V, I2V, and video continuation tasks (supported by benchmark results, but datasets not public)
- The 7B planner outperforms the 1.5B variant for complex reasoning (Table 1 results, but planner architectures not fully specified)

**Low confidence (Limited verification possible):**
- The exact contribution of each component to the final performance (requires access to all ablations and datasets)
- The robustness of semantic transfer across significantly different visual styles (only demonstrated in controlled "Cross-Transfer" example)
- The long-term stability of generated videos beyond the tested 5-second clips (not evaluated)

## Next Checks

**Validation Check 1: Tokenizer Reproduction**
Recreate the TA-Tok tokenizer pipeline using publicly available SigLIP2 weights and test whether the quantized tokens maintain semantic correspondence to the original visual features. This validates whether the core semantic representation mechanism is reproducible without proprietary components.

**Validation Check 2: Independent Evaluation**
Run the Plan-X system on a held-out subset of publicly available video datasets (e.g., Kinetics, Something-Something) and evaluate using established video quality metrics (FVD, IS) plus human preference studies. This tests whether the performance gains generalize beyond the internal benchmark.

**Validation Check 3: Extreme Ablation Study**
Systematically remove each major component (semantic planner, 3D RoPE, text dropout schedule, E2E fine-tuning) in combination with varying token sampling rates (1-4 FPS) to quantify the marginal contribution of each design choice and identify potential overfitting to the training setup.