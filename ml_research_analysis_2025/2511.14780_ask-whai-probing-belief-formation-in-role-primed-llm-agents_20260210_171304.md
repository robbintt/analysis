---
ver: rpa2
title: Ask WhAI:Probing Belief Formation in Role-Primed LLM Agents
arxiv_id: '2511.14780'
source_url: https://arxiv.org/abs/2511.14780
tags:
- belief
- encounters
- encounter
- agent
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Ask WhAI is a framework for inspecting and perturbing belief states
  in multi-agent interactions. It enables out-of-band queries into agent beliefs,
  counterfactual evidence injection, and controlled replay of encounters.
---

# Ask WhAI:Probing Belief Formation in Role-Primed LLM Agents

## Quick Facts
- arXiv ID: 2511.14780
- Source URL: https://arxiv.org/abs/2511.14780
- Reference count: 28
- One-line primary result: Framework reveals how role-priming, encounter order, and reflection shape belief formation in multi-agent LLM reasoning

## Executive Summary
Ask WhAI is a framework for inspecting and perturbing belief states in multi-agent interactions. It enables out-of-band queries into agent beliefs, counterfactual evidence injection, and controlled replay of encounters. Applied to a medical simulation with LLM agents in distinct roles (e.g., neurologist, psychiatrist, rheumatologist, pediatrician), the system reveals how disciplinary priors, encounter order, and reflection shape diagnostic reasoning. Beliefs were probed before and after key encounters to distinguish entrenched priors from evidence-based updates. Results show that agent beliefs mirror real-world epistemic divides, with early notes anchoring thinking and reflection enabling belief change. The framework makes such dynamics visible and testable, offering a reproducible way to study belief formation and epistemic silos in multi-agent scientific reasoning.

## Method Summary
The framework uses role-primed LLM agents interacting via a shared Electronic Medical Record (EMR) to simulate multidisciplinary medical diagnosis. Agents are configured with personas and voices, then run through encounter sequences defined in YAML files. The Ask WhAI debugger intercepts the simulation to probe belief states at breakpoints and inject counterfactual evidence. Belief states are tracked on a 0-10 scale or categorical {skeptical, neutral, believes} before and after encounters. Experiments test how persona assignment, encounter order, and reflection prompts affect belief formation, with results analyzed via ANOVA to measure order effects and belief attribution.

## Key Results
- Agent beliefs mirror real-world disciplinary stances, with neurologists showing skepticism toward psychiatric explanations and psychiatrists anchoring on behavioral patterns
- Early EMR entries create strong anchoring effects, with later specialists forced to reconcile new evidence against established narratives rather than evaluating independently
- Reflection prompts enable belief change by forcing agents to resolve contradictions between priors and observed data, shifting belief states earlier than baseline queries

## Why This Works (Mechanism)

### Mechanism 1: Role-Priming as Prior Injection
- **Claim:** Assigning a persona loads domain-specific priors that bias how agents interpret identical evidence
- **Mechanism:** The system prompt activates latent associations in the LLM's training data, causing the agent to adopt the epistemic stance typical of that role
- **Core assumption:** The LLM's training data contains distinct, separable reasoning patterns for different expert personas
- **Evidence anchors:** Agents "primed with strong role-specific priors... mirror real-world disciplinary stances"; role prompts bring "a large set of priors... which can bias interpretation of new evidence"
- **Break condition:** If the model lacks sufficient domain-specific pre-training, the persona may fail to induce the desired rigid priors

### Mechanism 2: Sequential Anchoring via Shared Memory (EMR)
- **Claim:** Belief formation is path-dependent; early entries in the shared EMR create an "anchoring" effect that resists later contradictory evidence
- **Mechanism:** Agents read the shared EMR history before reasoning. Early notes establish a narrative frame, forcing subsequent agents to reconcile new data against this established baseline
- **Core assumption:** Agents prioritize coherence with existing context over re-evaluating the entire evidence base from scratch
- **Evidence anchors:** "Early notes anchoring thinking"; "Once a record is added to the EMR, it strongly influenced downstream reasoning... 'narrative lock-in'"
- **Break condition:** If the EMR context window is truncated or if a "reflection" prompt explicitly flags the bias, the anchoring effect diminishes

### Mechanism 3: Dissonance-Driven Revision (Reflection)
- **Claim:** Belief revision is unlikely unless agents are explicitly forced to resolve contradictions between their priors and the observed data
- **Mechanism:** Standard queries allow agents to defer to priors. "Entangled" prompts or moderator challenges create cognitive dissonance, triggering reasoning that updates the belief state
- **Core assumption:** LLMs optimize for internal coherence in their output, so forcing a specific output format drags the belief state along with it
- **Evidence anchors:** "Reflection enabling belief change"; in "Sherlock Mode," "belief becomes... a test of internal coherence," shifting stances earlier
- **Break condition:** If the prompt fails to enforce logical consistency, the agent may hallucinate a rationale that preserves their original prior

## Foundational Learning

- **Concept: Epistemic Priors vs. Evidence**
  - **Why needed here:** The core of the paper is distinguishing whether an agent is reasoning from *data* or from *training*. You must understand how "priors" (pre-existing beliefs) filter "evidence" (new facts)
  - **Quick check question:** If an agent rejects a lab result because it contradicts "standard guidelines," is this a prior or evidence-based decision? (Answer: Prior)

- **Concept: Shared State (Blackboard Systems)**
  - **Why needed here:** The EMR acts as a shared blackboard. You need to understand how writing to a shared memory differs from point-to-point messaging and how it creates "narrative lock-in"
  - **Quick check question:** In this architecture, does the Neurologist talk to the Psychiatrist directly, or do they communicate via the EMR? (Answer: Via the EMR/Moderator)

- **Concept: Counterfactual Reasoning**
  - **Why needed here:** The Ask WhAI debugger relies on "counterfactual evidence injection"â€”asking "What if X happened instead of Y?" to test causal links
  - **Quick check question:** If you want to test if a diagnosis relies on a specific lab result, what debugger operation do you perform? (Answer: Hide/Modify that lab result in a replay)

## Architecture Onboarding

- **Component map:** Simulator -> Agents (with Persona + Voice) -> EMR (shared memory) -> LabAgent (Oracle) -> Ask WhAI (Debugger)
- **Critical path:**
  1. Config: Load `config.yaml` (sets model, temperature, persona paths)
  2. Encounter Loop: For each encounter in `encounters.yaml`:
     - Agent reads EMR + Private Context
     - Agent converses with Moderator
     - Agent writes to EMR
  3. Breakpoint: Debugger intercepts flow; runs "Belief Probe" (sends out-of-band query to agent)
  4. Lab Check: LabAgent reviews encounter log; releases any ordered results to EMR
- **Design tradeoffs:**
  - Reproducibility vs. Emergence: The system caches responses to ensure reproducibility. Disabling the cache allows for stochastic exploration of belief dynamics
  - Persona Fidelity vs. Accuracy: Strict persona adherence can lower diagnostic accuracy but better models real-world epistemic silos
- **Failure signatures:**
  - Sherlock Mode Break: Agent abandons persona to solve the case logically (diagnosis shifts to "Believes" immediately). Fixed by strictly separating "List diagnoses" from "Belief stance" prompts
  - Inference Loop: Agent orders a test, but LabAgent doesn't recognize the order syntax (fuzzy matching fails). Result: EMR never updates with new data
  - Role Gap: Agent refuses to act (e.g., "Psychiatrists don't order strep tests") leading to diagnostic dead-ends
- **First 3 experiments:**
  1. Baseline Profiling: Run a standard encounter sequence and query the "Private Belief" after every interaction to graph the belief trajectory
  2. Order Perturbation: Swap the order of two key specialists and measure the delta in the final belief score to quantify anchoring bias
  3. Counterfactual Injection: Force the LabAgent to return a "Negative" result for a key test that was originally positive, and verify if the belief state updates or remains entrenched

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can Ask WhAI effectively analyze belief dynamics using real-world longitudinal Electronic Medical Records (EMRs) rather than synthetic scenarios?
- **Basis in paper:** The authors state in Section 7 that applying the debugger to real EMR cases "could reveal how lab orders, ICD codes, and notes evolve over time," noting that current scenarios are fully synthetic
- **Why unresolved:** The system has only been stress-tested on composite, literature-informed simulations rather than actual clinical data
- **What evidence would resolve it:** Successful application of the framework to retrospective, de-identified real-world patient cases to verify if observed anchoring effects persist

### Open Question 2
- **Question:** How can prompt design prevent "Sherlock mode" breaks where agents abandon their role-specific priors to act as general diagnostic synthesizers?
- **Basis in paper:** The paper notes in Section 7 and Appendix C that "occasional 'Sherlock mode' breaks occurred when models stepped out of persona," necessitating further work on prompt design to preserve in-character reasoning
- **Why unresolved:** While specific probing formats were tested, a robust solution to maintain persona fidelity during unstructured multi-turn dialogue remains undefined
- **What evidence would resolve it:** A set of prompt constraints or architectures that maintains persona fidelity across diverse probing styles

### Open Question 3
- **Question:** Why do role-primed medical agents impose stricter temporal constraints (requiring concurrent infection) than clinical evidence warrants?
- **Basis in paper:** In Appendix D, the authors note that agents assumed "associated with strep" meant "coincident with strep," requiring a current infection, contrary to analogs like Sydenham chorea or Long COVID. They state, "We are not sure what is causing the tighter requirement"
- **Why unresolved:** It is unclear if this behavior stems from training data biases, literal interpretation of terminology, or specific diagnostic guidelines in the pre-training corpus
- **What evidence would resolve it:** Causality tracing or attribution methods to identify the specific training text or reasoning heuristics enforcing the "coincident" constraint

## Limitations
- The framework's validity hinges on untested assumptions about the existence of sufficient domain-specific patterns in LLM pre-training data
- The extent to which shared EMR anchoring effects mirror real clinical decision-making versus LLM artifact remains unclear
- The belief probing methodology assumes agents maintain coherent belief states across turns, which may not hold under complex reasoning chains

## Confidence

**High Confidence:** The sequential anchoring effect via shared EMR is well-supported by experimental results showing order-dependent belief trajectories and narrative lock-in patterns. The core debugger functionality for belief state inspection and counterfactual injection is technically sound and reproducible.

**Medium Confidence:** The claim that role-priming induces realistic epistemic divides has experimental support but may be context-dependent. The reflection mechanism shows promise in specific scenarios but lacks systematic validation across diverse belief revision contexts.

**Low Confidence:** The generalizability of findings beyond the PANDAS medical case remains unproven. The framework's performance with other model families (beyond GPT-4o) and in non-medical domains has not been established.

## Next Checks

1. **Cross-Domain Transfer Test:** Apply the framework to a legal reasoning simulation where attorneys must evaluate evidence under different legal doctrines, comparing belief formation patterns to the medical case

2. **Model Family Generalization:** Run identical experiments using Claude, Gemini, and Llama models to determine if belief dynamics are architecture-dependent or reflect fundamental LLM behavior patterns

3. **Real-World Correlation Study:** Compare framework-predicted belief anchoring effects with actual clinical decision records from multidisciplinary case conferences, measuring the correlation between predicted and observed diagnostic divergence