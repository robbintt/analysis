---
ver: rpa2
title: Quantifying Uncertainty in Natural Language Explanations of Large Language
  Models for Question Answering
arxiv_id: '2509.15403'
source_url: https://arxiv.org/abs/2509.15403
tags:
- uncertainty
- language
- explanations
- natural
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces ULXQA and RULX, the first methods to provide\
  \ rigorous uncertainty quantification for natural language explanations generated\
  \ by large language models (LLMs) in medical question answering. The core approach\
  \ uses conformal prediction to construct uncertainty sets of explanation tokens,\
  \ ensuring that on average these sets contain at least a (1-\u03B1) fraction of\
  \ ground-truth explanation tokens."
---

# Quantifying Uncertainty in Natural Language Explanations of Large Language Models for Question Answering

## Quick Facts
- arXiv ID: 2509.15403
- Source URL: https://arxiv.org/abs/2509.15403
- Reference count: 24
- First methods providing rigorous uncertainty quantification for natural language explanations in LLM-based medical QA

## Executive Summary
This paper introduces ULXQA and RULX, the first methods to provide rigorous uncertainty quantification for natural language explanations generated by large language models (LLMs) in medical question answering. The core approach uses conformal prediction to construct uncertainty sets of explanation tokens, ensuring that on average these sets contain at least a (1-α) fraction of ground-truth explanation tokens. RULX extends this with robustness to token-level noise in questions. Experiments on MedMCQA and MedExpQA datasets show that ULXQA achieves empirical losses below desired risk levels and maintains efficient prediction set sizes, while RULX preserves validity under noise.

## Method Summary
The framework adapts conformal prediction to token-level natural language explanations by prompting LLMs to assign importance scores to question tokens, constructing candidate uncertainty sets for various thresholds, and calibrating the threshold via binary search to ensure expected loss ≤ α. RULX extends this by computing worst-case importance scores over synonym-based perturbations of tokens. The approach is post-hoc and model-agnostic, requiring only ground-truth explanation annotations for calibration. Prediction sets contain tokens whose importance scores exceed the calibrated threshold, with robust sets additionally considering synonym perturbations.

## Key Results
- ULXQA achieves empirical losses below desired risk levels (e.g., 0.44 at α=0.45 on MedExpQA with Gemini 2.0 Flash)
- Prediction set sizes remain efficient (average size ~3 tokens) while maintaining validity
- RULX preserves coverage guarantees under noise while keeping prediction set sizes comparable to non-robust method
- Framework works across different LLM models (GPT-4o, Gemini-2.0 Flash) and medical QA datasets

## Why This Works (Mechanism)

### Mechanism 1: Conformal Risk Control for Token-Level Explanations
- Claim: Provides provably valid coverage guarantees for natural language explanations in QA systems.
- Mechanism: ULXQA adapts conformal prediction to token-level explanations by: (1) prompting LLM to assign importance scores S(P, qi,j; M) ∈ [0,1] to each question token, (2) constructing candidate uncertainty sets Cλ(Qi; M) = {qi,j : S(P, qi,j; M) ≥ 1-λ} for various thresholds λ, (3) measuring empirical loss ℓ = 1 - |E* ∩ Cλ|/|E*| (fraction of ground-truth explanation tokens missed), and (4) calibrating λ̂ via binary search to ensure E[ℓ] ≤ α for desired risk level α. The guarantee emerges from exchangeability of calibration and test data.
- Core assumption: Calibration data and test data are exchangeable (weaker than i.i.d.).
- Evidence anchors: Abstract states "provides valid uncertainty guarantees in a post-hoc and model-agnostic manner"; Theorem 1 proves E[ℓ(Cλ̂(Qn+1; M), E*n+1, λ̂)] ≤ α with exchangeability assumption.
- Break condition: Exchangeability violated (distribution shift, adversarial inputs, or systematic noise patterns not captured by calibration data).

### Mechanism 2: Robust Uncertainty via Worst-Case Importance Scoring
- Claim: RULX maintains valid coverage guarantees under discrete token-level noise in questions.
- Mechanism: For each token, RULX defines a synonym set Bq (including the token itself). Given a noisy question Q' within edit distance d of clean Q*, RULX computes robust scores R(P, q̃; B, M) = sup_{Q̃ ∈ B_Q', q̃ ∈ Q̃} S(P, q̃; M)—taking the maximum importance score across all perturbations containing that token. The robust uncertainty set CRλ̂ includes tokens whose maximum scores exceed the threshold.
- Core assumption: Noise affects at most d tokens, each noisy token replaced by synonym; synonym sets are predefined and appropriate.
- Evidence anchors: Abstract states "novel robust uncertainty estimation method that maintains valid uncertainty guarantees even under noise"; Theorem 2 guarantees E[ℓ(CRλ̂(Qn+1; M), E*n+1, λ̂)] ≤ α for robust sets.
- Break condition: Noise model mis-specified (e.g., semantic noise not captured by synonyms, adversarial perturbations outside B sets).

### Mechanism 3: Coverage Guarantee Independence from Score Quality
- Claim: The coverage guarantee holds regardless of importance score quality; score quality affects only efficiency (set size).
- Mechanism: Conformal prediction's validity depends on exchangeability and the calibration procedure, not on whether S(P, q; M) faithfully reflects true importance. Poor scores yield larger sets (more tokens needed to achieve coverage), but E[ℓ] ≤ α remains valid because λ̂ adapts to whatever score distribution exists on calibration data.
- Core assumption: Exchangeability holds; calibration data is representative of test distribution.
- Evidence anchors: Section 2 states "our method preserves its coverage guarantee regardless of the quality of these scores"; Figure 2 shows set sizes vary (2-8 tokens) across models/datasets while validity maintained.
- Break condition: None for validity; efficiency degrades severely with poor scores (very large prediction sets become impractical).

## Foundational Learning

- **Concept: Conformal Prediction**
  - Why needed here: Core mathematical framework enabling provably valid coverage guarantees without distributional assumptions.
  - Quick check question: Given calibration losses [0.2, 0.3, 0.5, 0.6, 0.8] and n=5, what threshold λ̂ ensures E[ℓ] ≤ 0.5 using Eq. (3)?

- **Concept: Exchangeability vs. I.I.D.**
  - Why needed here: The theoretical guarantees rely on exchangeability (weaker than i.i.d.), which constrains what data splits and noise patterns are permissible.
  - Quick check question: If calibration data is from 2023 medical questions and test data is from 2025 with new medical terminology, is exchangeability violated?

- **Concept: Risk Control vs. Prediction Sets**
  - Why needed here: ULXQA controls the *expected* fraction of missed ground-truth tokens (risk), not a hard guarantee for every sample.
  - Quick check question: If α=0.2 and you observe ℓ=0.5 on a single test sample, does this violate the guarantee?

## Architecture Onboarding

- **Component map:**
  1. **Prompt Engineering Module**: Constructs confidence-aware prompts instructing LLM to output importance scores alongside answers.
  2. **Calibration Pipeline**: Takes calibration data Dcal = {(Pi, Qi, E*i, Ai)}, computes importance scores, constructs candidate sets Cλ for grid Λ, computes empirical losses R̂n(λ), finds λ̂ via binary search.
  3. **Inference Module**: For test question Qn+1, constructs uncertainty set Cλ̂(Qn+1; M) = {qn+1,j : S(Pn+1, qn+1,j; M) ≥ 1-λ̂}.
  4. **Robust Extension (RULX)**: Adds synonym set lookup, worst-case score computation R(·), and robust set construction CRλ̂.

- **Critical path:** Calibration → λ̂ selection → Inference set construction. RULX adds synonym expansion between score computation and set construction.

- **Design tradeoffs:**
  - λ grid granularity (Λ size) vs. calibration precision: finer grids yield tighter λ̂ but more computation.
  - Synonym set breadth vs. efficiency: larger B sets improve robustness but increase worst-case score computation (|B| queries per token).
  - Calibration set size vs. guarantee tightness: larger n reduces the (1-α)/n correction term but requires more annotated data.

- **Failure signatures:**
  - Empirical loss consistently exceeding α: exchangeability violated or implementation bug in loss calculation.
  - Prediction sets excessively large (>10 tokens for short questions): importance scores poorly calibrated; prompt may need refinement.
  - RULX sets still invalid under noise: synonym sets insufficient for noise type, or d parameter underestimated.

- **First 3 experiments:**
  1. **Validity sanity check**: Run ULXQA on MedMCQA with α ∈ {0.15, 0.3, 0.45, 0.8}; verify E[ℓ] ≤ α across 10 trials. Expected: Figure 1 reproduction.
  2. **Efficiency baseline**: Measure average prediction set sizes at each α; compare GPT-4o vs. Gemini-2.0. Expected: 2-4 tokens at α=0.45.
  3. **Robustness test**: Inject synthetic noise (typo substitution, synonym replacement) into MedMCQA test set; compare ULXQA vs. RULX validity. Expected: ULXQA exceeds α, RULX maintains ≤α (Figure 3 reproduction).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can ULXQA/RULX maintain valid uncertainty guarantees in multimodal settings (e.g., visual or audio question answering) where multiple modalities interact?
- Basis in paper: The limitations section states: "An important next step is to extend ULXQA/RULX to multimodal settings, such as visual or audio question answering, and maintain uncertainty guarantees when multiple modalities are involved."
- Why unresolved: Current experiments only evaluate on textual medical QA datasets; the interaction between modalities may introduce additional complexity in exchangeability assumptions and token-level noise modeling.
- What evidence would resolve it: Experiments applying ULXQA/RULX to multimodal QA benchmarks (e.g., VQA datasets) demonstrating empirical coverage validity comparable to single-modal results.

### Open Question 2
- Question: Do ULXQA/RULX generalize to non-medical QA domains such as legal or open-domain question answering?
- Basis in paper: The limitations section notes: "additional experiments on other types of QA (e.g., legal or open-domain) are needed to verify generality."
- Why unresolved: Medical QA has distinct characteristics (specialized vocabulary, high-stakes reasoning) that may not transfer; different domains may have different explanation styles and noise patterns.
- What evidence would resolve it: Empirical evaluation on legal QA (e.g., JECT) and open-domain QA (e.g., Natural Questions) showing valid coverage with reasonable prediction set sizes.

### Open Question 3
- Question: Does prompt-based self-assessment of token importance introduce systematic biases or self-reinforcing errors in uncertainty estimates?
- Basis in paper: The method relies on prompting the LLM to assign importance scores to its own tokens, which assumes the model can reliably self-assess without external grounding; this circularity is not critically examined.
- Why unresolved: Self-assessment may conflate model confidence with actual explanation relevance, particularly when models hallucinate or exhibit miscalibration.
- What evidence would resolve it: Comparative analysis between prompt-based importance scores and human-annotated or counterfactual-based importance measures across diverse examples.

### Open Question 4
- Question: Can the framework be extended to full conformal prediction settings while mitigating computational costs through machine unlearning techniques?
- Basis in paper: The methodology section mentions: "Our framework could be generalized to the full conformal prediction setting, where machine unlearning techniques could be explored to mitigate the associated high computational costs of retraining."
- Why unresolved: Full conformal prediction requires retraining for each test sample; the feasibility of combining this with unlearning for natural language explanations remains unexplored.
- What evidence would resolve it: A practical implementation of full conformal prediction for LLM explanations with unlearning-based approximation, demonstrating tractable runtime and maintained coverage guarantees.

## Limitations

- **Confidence: Low** - The validity guarantee depends critically on exchangeability between calibration and test data. While the theoretical framework is sound, real-world deployment faces threats from distribution shift, evolving medical terminology, and systematic noise patterns not captured in calibration data.
- **Confidence: Medium** - RULX's robustness relies on predefined synonym sets that may inadequately capture medical terminology's semantic complexity. The assumption that noise manifests as token-level synonym substitution is restrictive—medical questions could contain semantic paraphrases, abbreviation variations, or domain-specific jargon that synonym lists don't capture.
- **Confidence: Medium** - The framework assumes ground-truth explanations are available for calibration and evaluation, but these annotations are expensive and may be inconsistent across annotators. Medical explanations often involve subjective clinical judgment that may not have single "ground-truth" token sets.

## Confidence

- **High Confidence**: The theoretical validity guarantee (E[ℓ] ≤ α) holds under the stated exchangeability assumptions. This is a direct consequence of conformal prediction theory and the calibration procedure described in Algorithm 1.
- **Medium Confidence**: The robust variant RULX maintains validity under the specific noise model (token-level synonym substitution within edit distance d). The guarantee holds mathematically but depends on the adequacy of synonym sets for capturing realistic noise.
- **Low Confidence**: The practical efficiency (prediction set sizes of 2-4 tokens) will generalize across different medical QA datasets, prompt engineering approaches, and LLM models. The current results may be specific to the datasets and models tested.

## Next Checks

1. **Temporal Validation Test**: Train and calibrate ULXQA on medical questions from 2022-2023, then evaluate on 2024-2025 questions. Measure both validity (E[ℓ] ≤ α) and efficiency (prediction set size) to assess performance under distribution shift. Compare against domain adaptation baselines.

2. **Natural Noise Evaluation**: Collect a dataset of naturally occurring noisy medical questions (typos, abbreviations, paraphrasing) from real clinical documentation or user queries. Evaluate RULX against ULXQA on this dataset to test robustness to realistic noise patterns beyond synthetic synonym substitution.

3. **Annotation Quality Sensitivity**: Create multiple ground-truth explanation annotations for the same questions using different medical experts. Evaluate ULXQA's performance variability across these annotation sets to quantify sensitivity to ground-truth definition and assess practical reliability in clinical settings.