---
ver: rpa2
title: Can LLMs Reliably Simulate Real Students' Abilities in Mathematics and Reading
  Comprehension?
arxiv_id: '2507.08232'
source_url: https://arxiv.org/abs/2507.08232
tags:
- grade
- student
- reading
- prompt
- students
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates whether large language models (LLMs) can accurately
  simulate real students' abilities in mathematics and reading comprehension across
  grades 4, 8, and 12. Using data from the National Assessment of Educational Progress
  (NAEP), the authors apply Item Response Theory (IRT) to measure 11 diverse LLMs
  against authentic student performance patterns.
---

# Can LLMs Reliably Simulate Real Students' Abilities in Mathematics and Reading Comprehension?

## Quick Facts
- arXiv ID: 2507.08232
- Source URL: https://arxiv.org/abs/2507.08232
- Reference count: 23
- Major outcome: No evaluated model-prompt pair consistently matches average student performance across subjects and grades; strong general-purpose models consistently outperform average students.

## Executive Summary
This study investigates whether large language models (LLMs) can accurately simulate real students' abilities in mathematics and reading comprehension at grades 4, 8, and 12. Using NAEP data and Item Response Theory (IRT), the authors evaluate 11 diverse LLMs against authentic student performance patterns. Without guidance, strong general-purpose models consistently outperform average students at every grade, while weaker or domain-mismatched models may align incidentally. Grade-enforcement prompts change model performance, but alignment with grade-level students remains highly model- and prompt-specific. The study provides guidelines for selecting viable LLM proxies and highlights the need for dedicated model finetuning for faithful grade-level emulation.

## Method Summary
The authors use Item Response Theory (IRT) to measure the performance of 11 diverse LLMs against authentic student performance patterns from the National Assessment of Educational Progress (NAEP). They evaluate model responses in mathematics and reading comprehension across grades 4, 8, and 12, both with and without grade-enforcement prompts. IRT-based alignment metrics are used to compare model and student ability distributions, and the study systematically tests model-prompt combinations for consistency across subjects and grades.

## Key Results
- Without guidance, strong general-purpose models consistently outperform average students at every grade.
- Grade-enforcement prompts change model performance, but alignment with grade-level students remains highly model- and prompt-specific.
- No evaluated model-prompt pair consistently matches average student performance across subjects and grades.

## Why This Works (Mechanism)
The study leverages Item Response Theory (IRT) to model the relationship between student ability and item difficulty, enabling quantitative comparison between LLM performance and authentic student response patterns. Grade-enforcement prompts directly manipulate the expected response level, allowing systematic testing of whether LLMs can be guided to emulate specific grade-level abilities. However, the mechanism by which certain models incidentally align with student performance—especially weaker or domain-mismatched ones—remains unclear and may reflect superficial or unstable patterns rather than genuine ability emulation.

## Foundational Learning
- **Item Response Theory (IRT)**: Models the relationship between student ability and item difficulty; needed to compare model and student performance distributions. Quick check: Verify that IRT assumptions (e.g., unidimensionality, local independence) are met in the data.
- **NAEP dataset**: Standardized national assessment of student achievement; needed as a benchmark for authentic student performance. Quick check: Confirm dataset covers the intended grades and subjects.
- **Grade-enforcement prompts**: Explicit instructions to respond at a specific grade level; needed to test whether prompting can align model performance with student expectations. Quick check: Ensure prompts are standardized and applied consistently.

## Architecture Onboarding
- **Component map**: NAEP data -> IRT calibration -> LLM responses (with/without prompts) -> IRT alignment metrics -> model-prompt performance evaluation
- **Critical path**: Model evaluation relies on IRT-based comparison of model and student response patterns; prompt effects are tested by re-evaluating with grade-specific instructions.
- **Design tradeoffs**: Using a single dataset limits generalizability; prompt engineering is model-specific and may not transfer.
- **Failure signatures**: Domain-mismatched models may align with students "by accident," suggesting unstable or coincidental alignment.
- **First experiments**:
  1. Replicate IRT alignment using a different student assessment dataset.
  2. Test newer LLM versions to assess stability of findings over time.
  3. Evaluate impact of domain-specific finetuning on grade-level simulation fidelity.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions.

## Limitations
- Reliance on a single NAEP dataset may not represent diverse student populations or evolving assessment standards.
- IRT-based alignment metrics assume linearity and may not capture all dimensions of student ability or model behavior.
- Prompt engineering effects are highly model-specific, and the study does not explore the full space of prompt variations or adaptive prompting strategies.

## Confidence
- Confidence in the claim that no model-prompt pair consistently matches average student performance across subjects and grades: High
- Confidence in the observation that strong general-purpose models consistently outperform average students: Medium
- Confidence in the recommendation for dedicated model finetuning: Low

## Next Checks
1. Replicate the study using a different, contemporaneous student assessment dataset to test generalizability.
2. Evaluate newer LLM versions and additional prompt strategies to assess stability of findings over time.
3. Conduct a controlled study on the impact of domain-specific finetuning for grade-level simulation fidelity.