---
ver: rpa2
title: 'Know What You do Not Know: Verbalized Uncertainty Estimation Robustness on
  Corrupted Images in Vision-Language Models'
arxiv_id: '2504.03440'
source_url: https://arxiv.org/abs/2504.03440
tags:
- confidence
- severity
- frequency
- noise
- claude
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examined verbalized uncertainty estimation in vision-language
  models under image corruption. Three state-of-the-art VLMs (GPT-4V, Gemini Pro Vision,
  and Claude 3 Opus) were tested on corrupted images using Gaussian noise, defocus
  blur, and JPEG compression at five severity levels.
---

# Keep What You do Not Know: Verbalized Uncertainty Estimation Robustness on Corrupted Images in Vision-Language Models

## Quick Facts
- arXiv ID: 2504.03440
- Source URL: https://arxiv.org/abs/2504.03440
- Authors: Mirko Borszukovszki; Ivo Pascal de Jong; Matias Valdenegro-Toro
- Reference count: 40
- One-line primary result: Verbalized uncertainty in vision-language models exhibits systematic miscalibration under image corruption, with confidence remaining stable while accuracy degrades.

## Executive Summary
This study evaluates how well state-of-the-art vision-language models (VLMs) can estimate their uncertainty when answering visual questions about corrupted images. The researchers tested three VLMs (GPT-4V, Gemini Pro Vision, and Claude 3 Opus) on images with Gaussian noise, defocus blur, and JPEG compression at five severity levels. They prompted the models to express their confidence as a percentage alongside their answers. The results show that as corruption severity increased, model accuracy decreased but confidence scores remained relatively stable, indicating poor calibration. All models exhibited overconfidence, with confidence scores frequently clustering in the [80, 100] range regardless of accuracy.

## Method Summary
The researchers conducted API-based experiments using three vision-language models: GPT-4V, Gemini Pro Vision, and Claude 3 Opus. They evaluated these models on visual question-answering (VQA) tasks using corrupted images from the JUS dataset (29 hard VQA questions) and counting tasks (13 instances). Three corruption types were applied at five severity levels: Gaussian noise, defocus blur, and JPEG compression. The models were prompted to answer questions in the format "Answer (confidence%)" for VQA tasks and to provide 95% confidence intervals for counting tasks. Expected Calibration Error (ECE) was computed with 4 bins over the [0,100] confidence range, and accuracy, confidence scores, and refusal rates were tracked. No training was involved—the study relied entirely on API inference.

## Key Results
- Increased corruption severity led to decreased accuracy but not proportionally lower confidence, indicating miscalibration across all three models
- All models exhibited overconfidence, with confidence scores frequently clustering in the [80, 100] range regardless of accuracy
- GPT-4V generally outperformed the others in calibration, partially due to higher refusal rates on difficult questions
- Higher refusal rates improved calibration by preventing confident incorrect predictions, though this effect was not consistent across all corruption types

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Verbalized uncertainty estimation can be elicited through structured prompting, though it exhibits systematic miscalibration under input corruption.
- Mechanism: Models are prompted to express confidence as a percentage or confidence interval alongside their answer. The model generates a probability distribution over tokens and outputs verbalized confidence. This relies on the model's learned association between confidence language and epistemic states.
- Core assumption: That verbalized confidence reflects internal model uncertainty rather than linguistic patterns from training data.
- Evidence anchors:
  - [abstract] "We found that the severity of the corruption negatively impacted the models' ability to estimate their uncertainty and the models also showed overconfidence in most of the experiments."
  - [section 1] "To estimate the model's uncertainty in a given answer, we could ask the model in our prompt to quantify it. This is known as verbalized uncertainty."
  - [corpus] Related work (SIMBA UQ) notes uncertainty quantification is "crucial for trusted AI systems" but methods remain challenging for black-box models.

### Mechanism 2
- Claim: Corruption severity degrades calibration through a differential impact: accuracy declines while confidence remains relatively stable.
- Mechanism: Visual corruptions (Gaussian noise, defocus blur, JPEG compression) impair feature extraction at the vision encoder, reducing answer accuracy. However, the language model component does not proportionally reduce verbalized confidence, as the internal uncertainty signal is not properly propagated or calibrated.
- Core assumption: That the vision encoder degradation and confidence verbalization are not coupled in the model architecture.
- Evidence anchors:
  - [section 4] "As the severity of the corruption increased, the models' accuracy started to degrade slightly, but the confidence remained fairly stable."
  - [section 4.2, Table 2] R² values for ECE vs. severity in hard VQA range from 0.68-0.95, indicating severity explains significant variance in calibration error.
  - [corpus] "Do LVLMs Know What They Know?" finds LVLMs struggle to perceive knowledge boundaries, consistent with observed miscalibration.

### Mechanism 3
- Claim: Refusal to answer under high uncertainty improves aggregate calibration metrics by preventing confident incorrect predictions.
- Mechanism: Models with higher refusal rates (particularly GPT-4V on defocus blur) avoid generating answers when visual input is severely degraded. Since refusals are excluded from accuracy/ECE calculations, models that refuse more on difficult inputs show better calibration on answered queries.
- Core assumption: That refusal is triggered by detected inability rather than arbitrary safety filters, and that refusal is preferable to low-confidence guesses.
- Evidence anchors:
  - [section 4.1] "GPT-4V's refusal rates are much higher than the other two models' and it outperforms them both in ECE scores."
  - [section 6] "Higher refusal rates can improve calibration... when the model recognises that we are asking an impossible question and refuses to answer, it prevents itself from providing hallucinated answers."
  - [corpus] Weak corpus signal on refusal as calibration mechanism; this appears to be an underexplored area in VLM uncertainty research.

## Foundational Learning

- Concept: **Expected Calibration Error (ECE)**
  - Why needed here: Primary metric for quantifying miscalibration—measures the weighted gap between confidence and accuracy across binned predictions.
  - Quick check question: If a model has 80% average confidence but 60% accuracy, is ECE necessarily 20%? (Answer: No—ECE is computed across bins, not globally.)

- Concept: **Distribution Shift / Covariate Shift**
  - Why needed here: Image corruptions simulate real-world distribution shift; understanding how models respond to out-of-distribution inputs is central to the analysis.
  - Quick check question: Why might a model maintain high confidence on corrupted images it has never seen during training? (Answer: The model may lack an internal mechanism to detect out-of-distribution inputs.)

- Concept: **Verbalized vs. Token Probability Uncertainty**
  - Why needed here: For proprietary VLMs, internal token probabilities are inaccessible; verbalized uncertainty is the only feasible method for API users.
  - Quick check question: What is the key limitation of verbalized uncertainty compared to sampling-based methods? (Answer: It depends on the model's ability to accurately introspect and articulate its uncertainty, which may be unreliable.)

## Architecture Onboarding

- Component map:
  - Image → [Vision Encoder] → [Projection] → [LLM Backbone + Confidence Prompt] → Text Answer with Confidence

- Critical path: Image → [Vision Encoder] → [Projection] → [LLM Backbone + Confidence Prompt] → Text Answer with Confidence

- Design tradeoffs:
  - **Refusal threshold setting**: Higher thresholds reduce incorrect answers but may increase non-responses on answerable queries.
  - **Confidence granularity**: Continuous (0-100%) vs. interval-based ([lower, upper]) formats—intervals showed worse calibration in counting tasks.
  - **Corruption-aware training**: Including corrupted images in training may improve robustness but risks overfitting to specific corruption types.

- Failure signatures:
  - **Severe overconfidence**: Confidence clustering in [80, 100] range regardless of accuracy.
  - **Confidence-accuracy divergence**: Gap widening with corruption severity (ECE increasing with R² ≥ 0.7 in hard tasks).
  - **Interval miscalibration**: 95% confidence intervals achieving <25% coverage in counting tasks.
  - **Corruption misclassification**: Gaussian noise occasionally interpreted as artistic style (pointillism) rather than degradation.

- First 3 experiments:
  1. **Baseline calibration**: Test your VLM on clean images with verbalized confidence prompts; compute ECE and confidence distribution to establish overconfidence baseline.
  2. **Single corruption sweep**: Apply Gaussian noise at severity levels 0-5 to a fixed image set; plot accuracy, confidence, and ECE vs. severity to replicate the differential degradation pattern.
  3. **Refusal rate analysis**: Compare models or prompting strategies with different refusal behaviors; correlate refusal rate with ECE to validate whether refusal improves calibration.

## Open Questions the Paper Calls Out

- **Question:** How does verbalized uncertainty robustness vary across the 12 untested common corruptions (e.g., weather effects like snow/fog) defined by Michaelis et al. (2019)?
- **Basis in paper:** [explicit] Section 5 (Future Research) explicitly notes that the study was limited to three types and states, "Michaelis et al. (2019) defines 15 corruption types, but we only tested three. Studying the effect of the others could reveal more differences between the models."
- **Why unresolved:** The authors restricted the scope to Gaussian noise, defocus blur, and JPEG compression to manage the complexity of manual answer verification.
- **What evidence would resolve it:** Running the same verbalized confidence protocol on VLMs using the remaining corruption types from the CIFAR-10-C/ImageNet-C benchmarks.

- **Question:** Can advanced prompting strategies like Chain-of-Thought (CoT) reasoning or top-k sampling mitigate the overconfidence and miscalibration observed in corrupted images?
- **Basis in paper:** [explicit] Section 5 suggests that "Different prompting strategies, such as chain-of-thought reasoning or top-k explored by Xiong et al. (2024) could yield different results."
- **Why unresolved:** This study utilized a "vanilla" prompting strategy to establish a baseline, leaving the impact of reasoning-based prompting on visual corruption unexplored.
- **What evidence would resolve it:** A comparative analysis of calibration error (ECE) and confidence distributions when CoT prompting is applied to the same corrupted datasets.

- **Question:** Is temperature scaling an effective method for recalibrating RLHF-tuned VLMs that exhibit systematic overconfidence?
- **Basis in paper:** [explicit] Section 5 proposes, "it would be interesting to explore if this overconfidence in VLMs could be treated with temperature scaling in the same way as in Kadavath et al. (2022)."
- **Why unresolved:** The experiments relied on standard API settings, and the authors hypothesize that RLHF fine-tuning rewards confident language, causing high verbalized confidence scores regardless of accuracy.
- **What evidence would resolve it:** Experiments manipulating the temperature parameter via model APIs to observe if the gap between accuracy and confidence narrows.

## Limitations
- The evaluation uses a limited dataset (36 images) and three corruption types, which may not generalize to other visual domains or corruption patterns encountered in real-world applications.
- API-based experiments cannot control model hyperparameters or access internal representations, limiting the ability to diagnose miscalibration mechanisms or test architectural interventions.
- The calibration analysis relies on verbalized uncertainty via prompting rather than internal model probabilities, which may not reflect true epistemic uncertainty.

## Confidence
- High confidence: The observation that corruption severity degrades accuracy while confidence remains stable (supported by quantitative ECE vs. severity R² values 0.68-0.95).
- Medium confidence: The claim that refusal improves calibration (based on correlation between GPT-4V's higher refusal rates and better ECE, but limited to three models and specific corruption types).
- Medium confidence: The generalizability of overconfidence patterns across VLMs (observed consistently but in a small sample of three models with proprietary architectures).

## Next Checks
1. **Token-probability validation**: Replicate the calibration experiments using open-weight VLMs where internal token probabilities are accessible, comparing verbalized confidence calibration against probability-based calibration to assess prompt reliability.
2. **Cross-dataset robustness**: Test the corruption-induced miscalibration pattern on diverse visual datasets (e.g., COCO, ImageNet) to determine if the severity-accuracy-confidence divergence generalizes beyond the VQA-focused evaluation.
3. **Interval format stress test**: Systematically vary the confidence interval format (90%, 95%, 99%) in counting tasks to quantify the relationship between interval precision and calibration quality, testing whether models can reliably express epistemic uncertainty across granularities.