---
ver: rpa2
title: How Much 3D Do Video Foundation Models Encode?
arxiv_id: '2512.19949'
source_url: https://arxiv.org/abs/2512.19949
tags:
- video
- features
- probe
- arxiv
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper quantifies the 3D understanding of video foundation
  models (VidFMs) by probing their features with shallow readouts to estimate 3D properties
  like point maps, depth maps, and camera poses. The method extracts frozen spatial-temporal
  features from various VidFMs, then trains a lightweight transformer probe to predict
  these 3D attributes.
---

# How Much 3D Do Video Foundation Models Encode?

## Quick Facts
- arXiv ID: 2512.19949
- Source URL: https://arxiv.org/abs/2512.19949
- Reference count: 40
- Primary result: State-of-the-art video foundation models exhibit strong 3D awareness, often surpassing specialized 3D models on reconstruction tasks

## Executive Summary
This paper investigates the 3D understanding capabilities of video foundation models by probing their frozen features to predict 3D properties like depth maps, point clouds, and camera poses. The authors develop a methodology using lightweight transformer probes trained on spatial-temporal features extracted from various video foundation models. Through systematic experiments on synthetic datasets, they demonstrate that state-of-the-art video generation models like WAN2.1-14B and Open-Sora2.0 show remarkably strong 3D awareness, often outperforming specialized 3D models. The work reveals that temporal reasoning is critical for 3D understanding, while 3D fine-tuning improves in-domain performance but may hurt generalization.

## Method Summary
The authors extract frozen spatial-temporal features from various video foundation models and train lightweight transformer probes to predict 3D attributes including point maps, depth maps, and camera poses. The methodology involves selecting optimal feature layers and timesteps from the frozen models, then using these as input to the probes. Experiments are conducted on CO3Dv2 and DL3DV datasets, comparing performance across different video foundation models, specialized 3D models, and various probing configurations. The approach demonstrates that video foundation models encode substantial 3D information that can be extracted through simple probing mechanisms.

## Key Results
- State-of-the-art video generation models like WAN2.1-14B and Open-Sora2.0 exhibit strong 3D awareness, often surpassing specialized 3D models
- Temporal reasoning is critical for 3D understanding - models with temporal features significantly outperform spatial-only approaches
- Using frozen VidFM features in a VGGT model significantly outperforms the original DINO-based VGGT with limited 3D data

## Why This Works (Mechanism)
Video foundation models learn rich spatiotemporal representations through training on diverse video data, which inherently contains 3D geometric information. The frozen-feature probing approach works because these models develop implicit 3D understanding as a byproduct of learning to generate coherent video sequences. Temporal coherence requirements force the models to learn consistent spatial relationships across frames, effectively capturing 3D geometry without explicit supervision. The transformer probes can then extract this latent 3D information by learning simple mappings from the high-level video representations to specific 3D properties.

## Foundational Learning
- **Video foundation models**: Large-scale models trained on diverse video data that learn spatiotemporal representations
  - Why needed: Provide rich features encoding both spatial and temporal information
  - Quick check: Verify model was trained on sufficient video diversity

- **Feature probing**: Technique of using frozen model features with lightweight readouts to assess learned representations
  - Why needed: Allows assessment of internal representations without fine-tuning
  - Quick check: Ensure probe complexity is appropriate for task

- **3D reconstruction from video**: Process of recovering depth, point clouds, or camera poses from video sequences
  - Why needed: Provides ground truth for evaluating 3D understanding
  - Quick check: Validate reconstruction accuracy against known 3D structures

## Architecture Onboarding

**Component map:** Video foundation model -> Feature extractor -> Transformer probe -> 3D prediction

**Critical path:** Input video frames → Spatial-temporal feature extraction → Optimal layer/timestep selection → Transformer probe → 3D property output

**Design tradeoffs:** Frozen features provide stable assessment but prevent observation of 3D understanding emergence during adaptation; shallow probes minimize training complexity but may miss deeper geometric reasoning; synthetic datasets enable controlled experiments but limit real-world generalization

**Failure signatures:** Poor probe performance indicates insufficient 3D encoding; temporal-only improvements suggest reliance on motion cues rather than geometric reasoning; domain-specific gains with 3D fine-tuning indicate potential overfitting to training distributions

**3 first experiments:** 1) Probe baseline video foundation model on synthetic 3D dataset; 2) Compare temporal vs spatial-only feature performance; 3) Test probe performance across different model scales

## Open Questions the Paper Calls Out
None

## Limitations
- Probing methodology may conflate correlation with causation, as high probe performance doesn't necessarily indicate explicit 3D geometry representation
- Evaluation constrained to synthetic datasets limits conclusions about real-world generalization
- Comparison between VidFMs and specialized 3D models conflates architectural differences with training objectives and data distributions

## Confidence
- High confidence: VidFMs can be probed for 3D properties using shallow readouts; temporal features improve 3D reconstruction performance
- Medium confidence: State-of-the-art VidFMs exhibit stronger 3D awareness than specialized 3D models; 3D fine-tuning has trade-offs
- Low confidence: Temporal reasoning is fundamentally critical for 3D understanding; model scaling consistently improves 3D awareness

## Next Checks
1. Conduct ablation studies varying probe depth and complexity to establish whether shallow readouts are sufficient or optimal for 3D property extraction
2. Evaluate the same VidFM models on real-world 3D datasets with diverse scene types and occlusion conditions to test generalization beyond synthetic environments
3. Implement controlled experiments disentangling temporal reasoning from geometric reasoning by comparing VidFM performance against models with explicit 3D supervision on pure spatial tasks