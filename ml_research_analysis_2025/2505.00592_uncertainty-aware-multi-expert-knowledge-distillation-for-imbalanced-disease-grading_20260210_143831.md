---
ver: rpa2
title: Uncertainty-Aware Multi-Expert Knowledge Distillation for Imbalanced Disease
  Grading
arxiv_id: '2505.00592'
source_url: https://arxiv.org/abs/2505.00592
tags:
- grading
- knowledge
- distillation
- umkd
- expert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces UMKD, a knowledge distillation framework
  that addresses class imbalance in medical image grading. The method decouples task-agnostic
  and task-specific features through shallow feature alignment (SFA) and compact feature
  alignment (CFA), and dynamically adjusts knowledge transfer weights using an uncertainty-aware
  decoupled distillation (UDD) mechanism.
---

# Uncertainty-Aware Multi-Expert Knowledge Distillation for Imbalanced Disease Grading

## Quick Facts
- arXiv ID: 2505.00592
- Source URL: https://arxiv.org/abs/2505.00592
- Reference count: 23
- Primary result: State-of-the-art performance on prostate and fundus image grading with 91.02% overall accuracy and 90.23% mean accuracy on source-imbalanced prostate grading.

## Executive Summary
This paper introduces UMKD, a knowledge distillation framework that addresses class imbalance in medical image grading. The method decouples task-agnostic and task-specific features through shallow feature alignment (SFA) and compact feature alignment (CFA), and dynamically adjusts knowledge transfer weights using an uncertainty-aware decoupled distillation (UDD) mechanism. UMKD effectively mitigates domain shifts and bias propagation in both source-imbalanced and target-imbalanced scenarios, achieving state-of-the-art performance on SICAPv2 and APTOS datasets.

## Method Summary
UMKD is a multi-expert knowledge distillation framework designed for imbalanced medical image grading tasks. It uses two ResNet-50 experts (pre-trained on ImageNet) to distill knowledge into a ResNet-18 student. The framework comprises three key modules: SFA uses multi-scale low-pass filtering to align shallow features and preserve structural information, CFA projects penultimate-layer features to a compact spherical space and aligns them using MMD and reconstruction loss, and UDD dynamically weights knowledge transfer based on expert prediction uncertainty. The total loss combines classification loss with weighted SFA, CFA, and UDD losses.

## Key Results
- On SICAPv2 prostate grading, UMKD achieves 91.02% overall accuracy and 90.23% mean accuracy in source-imbalanced distillation.
- On APTOS fundus image grading, UMKD reaches 74.61% overall accuracy and 67.33% mean accuracy in source-imbalanced distillation.
- Ablation study confirms all three components (SFA, CFA, UDD) are essential for robust knowledge transfer.

## Why This Works (Mechanism)

### Mechanism 1: Shallow Feature Alignment (SFA) for Task-Agnostic Decoupling
SFA preserves structural information while removing noise, facilitating generalized feature alignment between heterogeneous expert and student models. It uses multi-scale low-pass filtering on expert features and a learnable low-pass filter on student features to align features in the frequency domain, decoupling structural from high-frequency components. The core assumption is that structural features are more generalizable across domains and architectures than high-frequency details.

### Mechanism 2: Compact Feature Alignment (CFA) for Task-Specific Knowledge Transfer
CFA enables the student to learn grading-related hierarchical knowledge from experts by aligning penultimate-layer features in a common spherical space. It projects expert/student encoder outputs to a compact high-dimensional spherical space and enforces alignment using Maximum Mean Discrepancy (MMD) and reconstruction loss (MSE). The core assumption is that task-specific semantic features for grading reside in a shared spherical space that can be meaningfully aligned across architectures.

### Mechanism 3: Uncertainty-Aware Decoupled Distillation (UDD) for Robust Logit Transfer
UDD dynamically reweights knowledge transfer from experts to student based on expert prediction uncertainty, reducing bias propagation from imbalanced training data. It partitions logits spatially at multiple scales, calculates an uncertainty coefficient from expert confidence, and uses this to weight target-class and non-target-class KD losses. The core assumption is that expert prediction uncertainty is a reliable proxy for bias/quality in their knowledge, particularly for imbalanced classes.

## Foundational Learning

- **Maximum Mean Discrepancy (MMD):**
  - Why needed here: Used as the core distance metric in both SFA and CFA to align feature distributions between expert and student models without requiring paired samples.
  - Quick check question: Can you explain how MMD measures the distance between two probability distributions using a kernel function, and why it might be preferred over simple Euclidean distance for feature alignment?

- **Knowledge Distillation (KD):**
  - Why needed here: The entire framework is built on transferring "dark knowledge" (logits and intermediate features) from large expert models to a smaller student model.
  - Quick check question: What is the fundamental idea behind KD, specifically why soft labels (logits) from a teacher model contain more information than hard one-hot labels?

- **Class Imbalanced Learning:**
  - Why needed here: The primary problem UMKD solves is performance degradation on minority classes caused by imbalanced training data in medical datasets.
  - Quick check question: How does training on an imbalanced dataset bias a classifier's decision boundary, and what are two common techniques to mitigate this (besides the method in this paper)?

## Architecture Onboarding

- **Component map:**
  - Images -> Expert Encoders (ResNet-50) -> Projection Layers (1x1 conv) -> CFA Module -> SFA Module -> Student Encoder (ResNet-18) -> UDD Module -> Classification Head -> Loss Aggregator

- **Critical path:**
  1. Forward pass through expert and student encoders.
  2. Apply SFA to shallow features and CFA to penultimate-layer features. Compute alignment losses.
  3. Generate logits from the final classification head.
  4. Compute UDD loss based on partitioned logits and expert uncertainty.
  5. Backpropagate using total loss: L_cls + α(L_SFA + L_CFA) + β L_UDD.

- **Design tradeoffs:**
  - SFA vs. CFA placement: SFA operates on shallow, general features; CFA on deep, semantic features. This division is critical.
  - Number of Experts: More experts increase computational cost and potential noise but may provide more robust knowledge.
  - Loss Weighting (α, β): The balance between classification, feature alignment, and logit distillation losses is sensitive and requires tuning.
  - Spherical Projection: CFA projects to a spherical space, which normalizes feature magnitude, focusing purely on angular similarity.

- **Failure signatures:**
  - Collapse to Majority Class: If UDD is ineffective, the student will show high Overall Accuracy (OA) but very low Mean Accuracy (mAcc).
  - Negative Transfer: If SFA/CFA force poor alignments, the student may perform worse than a baseline trained without distillation.
  - Instability: Training loss fails to converge if reconstruction loss in feature alignment dominates.

- **First 3 experiments:**
  1. Reproduce Baseline: Train ResNet-18 student on the target-imbalanced dataset without distillation to establish a performance baseline.
  2. Ablation on UDD: Run UMKD without the UDD component (only SFA+CFA) to isolate its contribution to handling class imbalance.
  3. Heterogeneity Test: Use two experts with different architectures (e.g., ResNet-50 and DenseNet-121) to test if SFA/CFA successfully bridge the architectural gap.

## Open Questions the Paper Calls Out

### Open Question 1
Can the UMKD framework be effectively generalized to other medical imaging tasks beyond grading, such as segmentation or detection?
Basis in paper: The conclusion explicitly states, "Future work will focus on extending UMKD to other medical imaging tasks."
Why unresolved: The current study validates the method only on ordinal regression/classification tasks (prostate grading and DR grading).
What evidence would resolve it: Successful application and maintenance of SOTA performance on tasks requiring pixel-level accuracy, such as tumor segmentation or lesion detection.

### Open Question 2
How can the interpretability of the UMKD framework be enhanced to facilitate reliable clinical adoption?
Basis in paper: The conclusion identifies "improving its interpretability for clinical adoption" as a specific direction for future work.
Why unresolved: While the method improves accuracy, the paper does not provide visualizations or qualitative analyses explaining why specific features were aligned or how uncertainty weights were assigned for individual patient cases.
What evidence would resolve it: Integration of explainable AI (XAI) techniques that map the decoupled features (SFA/CFA) back to pathological markers interpretable by clinicians.

### Open Question 3
Does UMKD maintain robust knowledge transfer when the heterogeneity gap between expert and student models is significantly increased?
Basis in paper: The paper claims to tackle "model architecture heterogeneity," but the experiments exclusively use ResNet variants.
Why unresolved: It is unclear if the Shallow Feature Alignment (SFA) and Compact Feature Alignment (CFA) are sufficient when the structural inductive biases differ drastically (e.g., distilling from a Transformer-based expert to a CNN-based student).
What evidence would resolve it: Experiments distilling knowledge across distinct architectural paradigms (e.g., Swin Transformer to ResNet or MobileNet).

## Limitations
- Performance critically depends on unspecified loss weights (α, β) and training schedules, making reproducibility uncertain.
- The method assumes structural features are domain-invariant and that expert uncertainty correlates with knowledge quality, which may not hold for all medical imaging modalities.
- The ablation study does not explore the effect of varying the number of experts or the impact of freezing expert weights during distillation.

## Confidence
- **High Confidence:** The core claim that decoupling task-agnostic and task-specific features via SFA and CFA improves generalization across architectures.
- **Medium Confidence:** The effectiveness of UDD in mitigating bias propagation from imbalanced data.
- **Low Confidence:** The claim of "state-of-the-art" performance without comparison to the most recent, specialized long-tailed classification methods.

## Next Checks
1. Perform hyperparameter sensitivity analysis by systematically varying α, β, and the number of spatial partitions (w_max) in UDD.
2. Compare distillation performance with frozen expert weights versus fine-tuned experts to assess resilience to domain shift.
3. Apply UMKD to a different medical imaging task (e.g., histopathology or X-ray) to test if the SFA/CFA decoupling principle is broadly applicable.