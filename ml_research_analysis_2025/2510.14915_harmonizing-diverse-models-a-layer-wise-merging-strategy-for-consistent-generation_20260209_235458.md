---
ver: rpa2
title: 'Harmonizing Diverse Models: A Layer-wise Merging Strategy for Consistent Generation'
arxiv_id: '2510.14915'
source_url: https://arxiv.org/abs/2510.14915
tags:
- consistency
- data
- variations
- query
- merging
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses inconsistency in Retrieval-Augmented Generation
  (RAG) systems where minor query variations cause large output differences. The authors
  propose a layer-wise model merging approach that combines knowledge from specialized
  models trained on diverse synthetic data, guided by consistency-aware weights derived
  from intermediate layer activations.
---

# Harmonizing Diverse Models: A Layer-wise Merging Strategy for Consistent Generation

## Quick Facts
- **arXiv ID:** 2510.14915
- **Source URL:** https://arxiv.org/abs/2510.14915
- **Reference count:** 6
- **Primary result:** A layer-wise model merging approach that significantly improves RAG output consistency, achieving a 47.5% improvement in response similarity (RS) over baseline.

## Executive Summary
This paper addresses a critical challenge in Retrieval-Augmented Generation (RAG) systems: inconsistent outputs when semantically equivalent queries are rephrased. The authors propose a layer-wise model merging strategy that combines specialized models trained on diverse synthetic data, guided by consistency-aware weights derived from intermediate layer activations. The approach demonstrates significant improvements in output consistency while maintaining strong accuracy metrics, making it particularly valuable for industrial RAG applications where reliability is paramount.

## Method Summary
The proposed method involves three key stages: (1) generating specialized synthetic datasets targeting specific query variation types (how-to/do, singular/plural/article, and semantic paraphrases), (2) fine-tuning separate models on each variation type using a combination of cross-entropy and triplet loss, and (3) merging these specialized models using a layer-wise approach weighted by consistency-aware metrics derived from intermediate layer activations. The consistency-aware weights are computed by comparing each model's layer activations to a semantic reference similarity matrix, with higher weights assigned to layers that better preserve semantic similarity.

## Key Results
- **47.5% improvement** in response similarity (RS) over baseline model
- **73.4% improvement** in exact match (EM) when using triplet loss during fine-tuning
- Merged model achieves **0.2521 EM** and **0.4129 RS** while maintaining competitive ROUGE and BLEU scores
- Consistent improvement across three variation types: how-to/do (37.1% RS), singular/plural/article (42.8% RS), and semantic variations (52.4% RS)

## Why This Works (Mechanism)

### Mechanism 1: Consistency-Aware Layer-wise Weighting
The method computes layer-specific weights by measuring how well each layer's activation similarity matrix aligns with a semantic reference. For each fine-tuned model and layer, activations are extracted, similarity matrices are computed, absolute deviation from reference is measured, then inverted normalization and sigmoid transformation produce layer-specific weights. This ensures layers that better preserve semantic consistency contribute more to the merged model.

### Mechanism 2: Triplet Loss for Semantic Embedding Alignment
Triplet loss is combined with cross-entropy during fine-tuning, using anchor-positive-negative samples constructed from query embeddings. The loss pulls similar queries closer in embedding space while pushing dissimilar ones apart, reducing internal representation variance for paraphrased inputs and improving consistency.

### Mechanism 3: Specialized Model Training on Targeted Variations
Training separate models on distinct query variation types before merging reduces internal parameter conflict compared to training a single model on all data. Each model specializes in one variation type (how-to/do, singular/plural/article, or semantic paraphrases), and layer-wise merging recovers combined capability without inference-time ensemble overhead.

## Foundational Learning

- **DARE-TIES Model Merging** - Why needed: The paper's merger builds directly on DARE-TIES (delta-based merging with sparsification, sign matching, and inverse scaling). Quick check: Can you explain why DARE-TIES merges delta-parameters rather than raw weights?
- **Metric Learning / Triplet Loss** - Why needed: The fine-tuning pipeline combines triplet loss with cross-entropy; knowing how anchors, positives, and negatives are constructed is essential. Quick check: Given an anchor query, how would you select a valid positive and negative sample from a retrieval corpus?
- **RAG Architecture (Retriever-Generator)** - Why needed: The paper explicitly assumes retriever stability and targets generator inconsistency. Quick check: If the retriever returns inconsistent contexts for paraphrased queries, would the proposed generator-side fix still be effective?

## Architecture Onboarding

- **Component map:** Synthetic Data Generator -> Specialized Model Trainer -> Activation Extractor -> Similarity Matrix Calculator -> Weight Derivation Module -> Layer-wise Merger
- **Critical path:** 1) Define variation types based on production query analysis, 2) Generate synthetic variations and retrieve corresponding contexts, 3) Train specialized models with triplet + cross-entropy loss, 4) Extract activations on dev set; compute similarity matrices, 5) Derive weights and perform layer-wise merging, 6) Evaluate on both accuracy and consistency test sets
- **Design tradeoffs:** Accuracy vs. Consistency (specialized models sometimes achieve higher accuracy), Number of Specialized Models (more types improve coverage but increase complexity), Sigmoid Scaling Parameters (control weight distribution), Retriever Stability Assumption (method doesn't address retrieval inconsistency)
- **Failure signatures:** Low consistency despite merging (mismatch between synthetic variations and actual production patterns), Accuracy drop after merging (weight miscalibration or insufficient positive samples), High variance across runs (triplet sampling noise or small dev sets)
- **First 3 experiments:** 1) Baseline consistency measurement on base LLM, 2) Ablation on triplet loss (with vs without), 3) Single-layer vs. full layer-wise merging (uniform vs consistency-aware weights)

## Open Questions the Paper Calls Out
1. Can the layer-wise merging strategy be effectively extended to mitigate inconsistencies originating from the retrieval component of RAG systems?
2. How does the model merging approach perform on diverse public benchmarks specifically designed to evaluate consistency?
3. Does the consistency improvement hold when query variations lead to significant changes in the retrieved context?

## Limitations
- The approach assumes retriever stability, which may not hold in production where context retrieval can vary across semantically equivalent queries
- Synthetic variation types may not fully represent real-world query distributions, potentially limiting generalization
- The consistency-aware weighting mechanism depends heavily on the sentence encoder's similarity judgments, which may not perfectly align with the task's notion of consistency

## Confidence
- **High Confidence:** The layer-wise merging framework (based on DARE-TIES) and overall experimental methodology are well-established and reproducible
- **Medium Confidence:** The consistency-aware weighting heuristic and triplet loss contribution are supported by results but rely on assumptions about semantic alignment
- **Low Confidence:** The generalizability of synthetic variation approach to unseen query patterns and robustness when retriever behavior is unstable

## Next Checks
1. Evaluate the merged model on a held-out production query set with variation types not present in synthetic training data to assess real-world robustness
2. Measure consistency when both retriever and generator are subject to input variations to quantify impact of assumed retriever stability
3. Perform ablation studies varying sigmoid scaling parameters (a, b) and development set size to determine stability of consistency-aware weighting scheme