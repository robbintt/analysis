---
ver: rpa2
title: 'Density-Ratio Weighted Behavioral Cloning: Learning Control Policies from
  Corrupted Datasets'
arxiv_id: '2510.01479'
source_url: https://arxiv.org/abs/2510.01479
tags:
- learning
- poisoning
- clean
- contamination
- weighted
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of learning robust control policies
  from offline datasets contaminated by adversarial poisoning, system errors, or low-quality
  samples. The proposed Density-Ratio Weighted Behavioral Cloning (Weighted BC) method
  uses a small, verified clean reference set to estimate trajectory-level density
  ratios via a binary discriminator, which are then clipped and used as weights in
  the BC objective to prioritize clean expert behavior while down-weighting or discarding
  corrupted data.
---

# Density-Ratio Weighted Behavioral Cloning: Learning Control Policies from Corrupted Datasets

## Quick Facts
- arXiv ID: 2510.01479
- Source URL: https://arxiv.org/abs/2510.01479
- Reference count: 39
- Key outcome: Achieves robust imitation learning on D4RL benchmarks with up to 100% data contamination, outperforming baselines by up to 200% in extreme cases

## Executive Summary
This paper introduces Density-Ratio Weighted Behavioral Cloning (Weighted BC), a method for learning control policies from offline datasets contaminated by adversarial poisoning, system errors, or low-quality samples. The key innovation is using a small, verified clean reference set to estimate trajectory-level density ratios via a binary discriminator, which are then clipped and used as weights in the BC objective to prioritize clean expert behavior. Theoretical analysis shows convergence to the clean expert policy with finite-sample bounds independent of contamination rate, while experiments demonstrate superior performance across various poisoning types and contamination levels on D4RL continuous control benchmarks.

## Method Summary
Weighted BC is a three-stage pipeline: (1) Train a binary discriminator to distinguish reference trajectories from main dataset trajectories, (2) Compute trajectory-level density ratios r(τ) = d_φ(τ)/(1 - d_φ(τ)) from the discriminator output, clip to [ε, C], and freeze these weights, (3) Train the policy using weighted behavioral cloning where the loss is scaled by these density ratios. The method requires a small clean reference set disjoint from the contaminated dataset and assumes bounded losses and sufficient discriminator accuracy. Weights are computed once and remain fixed during policy training, allowing the policy to focus on high-density-ratio (likely clean) trajectories while down-weighting or discarding corrupted samples.

## Key Results
- Maintains near-optimal performance with up to 100% data contamination across all poisoning types
- Outperforms traditional BC, BCQ, and BRAC by up to 200% in extreme contamination scenarios
- Retains over 80% performance up to 60% contamination across all poisoning types
- Achieves theoretical guarantees with convergence bounds independent of contamination rate

## Why This Works (Mechanism)

### Mechanism 1: Density-Ratio Estimation via Binary Discrimination
A binary discriminator trained to distinguish reference trajectories from main dataset trajectories can approximate the density ratio p_clean(τ)/p(τ) without knowledge of the contamination mechanism. Under balanced sampling between D_ref (label 1) and D (label 0), the optimal discriminator satisfies d*(τ) = p_clean(τ)/(p_clean(τ) + p(τ)), and the density ratio is recovered via r(τ) = d_φ(τ)/(1 - d_φ(τ)). This works because the reference set D_ref is genuinely clean and drawn from p_clean, allowing the discriminator to learn which trajectories are more likely to be clean.

### Mechanism 2: Importance Weighting Recovers Clean Distribution Objective
Weighting the behavioral cloning loss by density ratios approximates training on the clean distribution alone, with error bounded independently of contamination rate under proper clipping. The importance sampling identity E_{τ∼p_clean}[ℓ(τ)] = E_{τ∼p}[p_clean(τ)/p(τ) · ℓ(τ)] allows recovery of the clean objective from contaminated samples. With bounded loss 0 ≤ ℓ(τ;π) ≤ B and sufficient discriminator accuracy δ_d, this weighting scheme enables convergence to the clean expert policy.

### Mechanism 3: Clipping Bounds Error Amplification
Deterministic clipping of density ratios to [ε, C] ensures finite-sample bounds scale with O(C/√N) rather than unbounded weight variance. Clipping r(τ) via max(ε, min(r(τ), C)) trades off bias against variance. With C = 2.0 and ε = 10^-3, the method bounds worst-case weight influence while preserving relative ordering, ensuring stable policy learning even with potentially extreme density ratios.

## Foundational Learning

- **Concept: Importance Sampling / Density Ratio Estimation**
  - Why needed here: The core mechanism relies on reweighting samples to recover an expectation under a different distribution. Understanding how d_φ/(1-d_φ) approximates p_clean/p is essential for grasping the theoretical foundation.
  - Quick check question: Given a binary classifier trained to distinguish samples from distribution A vs. B under balanced sampling, how do you recover P(A)/P(B) from its output probability?

- **Concept: Behavioral Cloning as Supervised Learning**
  - Why needed here: The policy is trained via supervised loss -log π_θ(a_t|s_t) over trajectories. Understanding BC's limitations (distribution shift, no reward signal) clarifies why contamination is particularly damaging to standard imitation learning.
  - Quick check question: Why does standard BC fail when trained on a mixture of expert and random trajectories, even if the expert portion is substantial?

- **Concept: Offline RL Constraints and Distribution Shift**
  - Why needed here: The method operates without environment interaction, meaning corrupted data cannot be validated or corrected through exploration. This motivates the need for robust weighting strategies in offline settings.
  - Quick check question: In offline RL, why is it dangerous to query Q-values for out-of-distribution actions, and how does this relate to data corruption?

## Architecture Onboarding

- **Component map:** Binary Discriminator -> Density Ratio Computation -> Weighted BC Policy Training
- **Critical path:** Discriminator accuracy → weight quality → policy robustness. If the discriminator fails to distinguish clean from corrupted trajectories, downstream stages inherit the error, and the method degrades to standard BC.
- **Design tradeoffs:** Reference set size |D_ref| affects discriminator accuracy but requires more verified data (paper uses 20% of expert trajectories). Clipping threshold C balances variance reduction against information preservation. Trajectory-level weighting captures temporal consistency but may miss localized corruption.
- **Failure signatures:** Discriminator converging to ~0.5 confidence for all trajectories → weights near 1.0 → method degrades to standard BC. High variance in weights across seeds → discriminator overfitting to small reference set. Policy performing well on training trajectories but failing at deployment → contamination distribution matches expert in observable features but differs in dynamics.
- **First 3 experiments:**
  1. **Sanity check:** Train discriminator on D_ref vs. D with α=0 (no contamination). Verify weights concentrate near 1.0 and weighted BC matches standard BC performance.
  2. **Controlled corruption:** Apply single contamination type (e.g., action poisoning) at α=0.5. Plot weight distribution for known-corrupted vs. known-clean trajectories to verify discriminator separation.
  3. **Ablation on reference set size:** Vary |D_ref| from 5% to 50% of expert data at fixed contamination α=0.6. Measure performance retention and discriminator accuracy to identify minimum viable reference size.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but based on the content, several important unresolved issues emerge regarding reference set contamination, minimum reference size requirements, adversarial resistance, and adaptive clipping threshold selection.

## Limitations
- Requires access to a small, verified clean reference set, which may be impractical in many real-world scenarios
- Fixed clipping thresholds (ε=10^-3, C=2.0) may not generalize across domains with different reward scales or state-action distributions
- Theoretical guarantees assume the reference set is genuinely clean, but the paper doesn't address how to verify this in practice

## Confidence
- **High**: Convergence guarantees under stated assumptions, performance improvements on D4RL benchmarks with synthetic poisoning
- **Medium**: Applicability to real-world contamination patterns, effectiveness when reference set is not perfectly clean, scalability to high-dimensional state spaces
- **Low**: Performance with non-stationary contamination, ability to distinguish similar-but-not-identical behavior distributions (e.g., expert vs. near-expert)

## Next Checks
1. **Reference Set Contamination Robustness**: Systematically corrupt the reference set at varying rates (0-50%) and measure performance degradation to identify the maximum tolerable contamination level in the verification set.

2. **Adversarial Poisoning Resistance**: Implement structured adversarial poisoning where corrupted trajectories are designed to maximize discriminator confusion (e.g., expert-like early trajectories that degrade later) and measure whether Weighted BC maintains robustness.

3. **Cross-Domain Transferability**: Apply Weighted BC to non-D4RL control tasks (e.g., robotic manipulation from RoboSuite or real-world datasets) with naturally occurring data quality variations to assess performance outside the synthetic contamination framework.