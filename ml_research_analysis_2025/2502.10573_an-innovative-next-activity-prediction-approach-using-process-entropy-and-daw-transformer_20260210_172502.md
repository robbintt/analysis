---
ver: rpa2
title: An Innovative Next Activity Prediction Approach Using Process Entropy and DAW-Transformer
arxiv_id: '2502.10573'
source_url: https://arxiv.org/abs/2502.10573
tags:
- process
- entropy
- dataset
- prediction
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes an entropy-driven model selection approach
  and DAW-Transformer (Dynamic Attribute-Aware Transformer) to predict the next activity
  in business process management (BPM). DAW-Transformer integrates all event log attributes
  using a dynamic window mechanism and multi-head attention to capture long-range
  dependencies.
---

# An Innovative Next Activity Prediction Approach Using Process Entropy and DA
W-Transformer

## Quick Facts
- **arXiv ID**: 2502.10573
- **Source URL**: https://arxiv.org/abs/2502.10573
- **Reference count**: 9
- **Primary result**: DAW-Transformer achieves 70.14% accuracy on Sepsis dataset, outperforming CNN-BiLSTM by 9.51% and CNN-LSTM-SAtt by 3.07%

## Executive Summary
This paper introduces an entropy-driven model selection framework for next-activity prediction in business process management, arguing that dataset complexity should guide model architecture choice. The authors propose DAW-Transformer (Dynamic Attribute-Aware Transformer), which integrates all event log attributes through a dynamic window mechanism and multi-head attention to capture long-range dependencies. Experiments on six public datasets demonstrate that high-entropy datasets benefit from complex transformer models, while low-entropy datasets perform well with simpler, more interpretable models like Random Forest. The approach addresses a critical gap in BPM research by moving beyond one-size-fits-all modeling approaches.

## Method Summary
The research introduces a two-pronged approach: an entropy-driven model selection framework and the DAW-Transformer architecture. Entropy is used as a proxy for dataset complexity, with higher entropy indicating more unpredictable process behavior requiring sophisticated models. DAW-Transformer employs dynamic attribute aggregation through a window-based mechanism that captures temporal patterns, combined with multi-head self-attention to model complex dependencies between activities. The model processes event logs by integrating multiple attributes (activity, resource, timestamp) simultaneously rather than treating them separately. For model selection, the authors propose using entropy thresholds to determine whether to deploy complex transformer models or simpler alternatives like Random Forest, which offers better interpretability and handles imbalanced data more effectively.

## Key Results
- DAW-Transformer achieves 70.14% accuracy on Sepsis dataset, outperforming CNN-BiLSTM by 9.51% and CNN-LSTM-SAtt by 3.07%
- On low-entropy Road Traffic Fine dataset, Random Forest achieves 99.71% accuracy, demonstrating that simpler models suffice for predictable processes
- Entropy-driven model selection shows that complex transformer models provide diminishing returns on low-complexity datasets while offering significant advantages on high-entropy data

## Why This Works (Mechanism)
The paper's approach works by matching model complexity to data complexity through entropy measurement. High-entropy datasets contain more unpredictable process flows and complex dependencies, which require the sophisticated attention mechanisms of DAW-Transformer to capture long-range relationships between activities. The dynamic window mechanism allows the model to adapt to varying sequence lengths and temporal patterns, while multi-head attention enables parallel processing of different dependency types. For low-entropy datasets with more predictable patterns, simpler models like Random Forest can capture the underlying structure without the computational overhead and complexity of transformer architectures, while also providing better interpretability for business stakeholders.

## Foundational Learning

**Process Entropy**: Measures unpredictability in business process execution flows; needed to quantify dataset complexity and guide model selection; quick check: compute Shannon entropy on trace distributions

**Dynamic Window Mechanism**: Sliding window approach that adapts to varying sequence lengths; needed to capture temporal patterns without fixed-length constraints; quick check: visualize window sizes across different process traces

**Multi-head Self-Attention**: Parallel attention mechanisms that capture different dependency types; needed to model complex relationships between activities in high-entropy datasets; quick check: examine attention weight distributions across heads

**Event Log Attribute Integration**: Simultaneous processing of activity, resource, and timestamp attributes; needed to leverage all available information rather than treating attributes separately; quick check: compare performance with single-attribute models

**Model Interpretability**: Ability to explain model predictions to business stakeholders; needed for practical adoption in enterprise environments; quick check: analyze feature importance scores and decision paths

## Architecture Onboarding

**Component Map**: Event Log -> Dynamic Window -> Multi-head Attention -> Feed-forward Network -> Next Activity Prediction

**Critical Path**: Input sequence → Dynamic window segmentation → Multi-head attention computation → Contextual embedding aggregation → Classification layer → Predicted next activity

**Design Tradeoffs**: Transformer complexity vs. interpretability (favoring simpler models for low-entropy data), computational cost vs. accuracy gains (justifying complex models only for high-entropy cases), attribute integration completeness vs. model simplicity (requiring dynamic mechanisms)

**Failure Signatures**: Overfitting on low-entropy datasets (simple models outperform), poor performance on highly variable processes (indicating insufficient model complexity), inability to handle imbalanced classes (suggesting need for simpler, more robust models)

**First Experiments**:
1. Compute process entropy for all six datasets to validate the complexity-based model selection framework
2. Compare DAW-Transformer performance against CNN-BiLSTM and CNN-LSTM-SAtt on high-entropy datasets
3. Evaluate Random Forest performance on low-entropy datasets to demonstrate sufficiency of simpler models

## Open Questions the Paper Calls Out
None

## Limitations
- Experimental scope limited to only six public datasets, potentially limiting generalizability across diverse business process domains
- No clear thresholds or decision boundaries provided for when to switch between model types based on entropy values
- Comparison primarily focuses on CNN-based architectures while excluding other transformer variants or attention-based approaches from recent literature

## Confidence
- **High**: Entropy-guided model selection principle follows logically from observed performance differences
- **Medium**: Specific superiority of DAW-Transformer requires broader comparison with recent architectures
- **High**: Simpler models suffice for low-entropy datasets, supported by strong performance metrics

## Next Checks
1. Test the entropy-based model selection framework across a broader range of process domains beyond the current six datasets to validate general applicability
2. Conduct ablation studies to isolate the impact of individual DAW-Transformer components (dynamic window, multi-head attention) on performance
3. Evaluate the model's performance on real-time streaming data to assess practical utility in dynamic business environments where process entropy may fluctuate