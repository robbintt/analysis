---
ver: rpa2
title: 'Unlearning That Lasts: Utility-Preserving, Robust, and Almost Irreversible
  Forgetting in LLMs'
arxiv_id: '2509.02820'
source_url: https://arxiv.org/abs/2509.02820
tags:
- unlearning
- forget
- retain
- jensun
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of precise information removal
  from pre-trained large language models (LLMs), a crucial capability for ensuring
  safety by deleting private data or harmful knowledge acquired during pre-training.
  The authors introduce JensUn, an unlearning method based on Jensen-Shannon Divergence
  (JSD), which leverages the bounded and stable properties of JSD to create a well-behaved
  unlearning objective.
---

# Unlearning That Lasts: Utility-Preserving, Robust, and Almost Irreversible Forgetting in LLMs

## Quick Facts
- **arXiv ID**: 2509.02820
- **Source URL**: https://arxiv.org/abs/2509.02820
- **Reference count**: 40
- **Primary result**: Introduces JensUn, an unlearning method based on Jensen-Shannon Divergence that achieves better forget-utility trade-offs than competing methods, complete forgetting while preserving utility, and strong resilience to benign relearning.

## Executive Summary
This paper addresses the challenge of precisely removing specific information from pre-trained large language models (LLMs) while maintaining their general capabilities. The authors introduce JensUn, an unlearning method that uses Jensen-Shannon Divergence (JSD) as the training objective for both forgetting target information and preserving model utility. Through extensive experiments on two benchmarks including a newly introduced dataset of lesser-known facts, JensUn demonstrates superior performance compared to existing methods. The paper also critiques current evaluation protocols and proposes improved methods using an LLM as a semantic judge and worst-case analysis over paraphrased inputs, revealing that many existing methods are less effective than previously reported.

## Method Summary
JensUn is an unlearning method that leverages Jensen-Shannon Divergence (JSD) to create a well-behaved objective for both forgetting and retaining capabilities. The method minimizes JSD between the model's output distribution and a target refusal string (e.g., "No idea") for forget data, while simultaneously minimizing JSD between the unlearnt model and a frozen reference model on retain data. This dual use of JSD creates a self-regulating dynamic where the forget and retain losses automatically balance each other. The approach is trained using AdamW with learning rate 8e-6, batch size 4 (with gradient accumulation), and typically runs for 10 epochs with λ_F=0.5 and λ_R=0.5. The method is evaluated using worst-case accuracy over 15 paraphrases per forget question plus in-context retain samples, judged by an LLM judge rather than ROUGE scores.

## Key Results
- JensUn achieves complete forgetting (0% accuracy) on the LKF benchmark while preserving model utility (Win Rate 0.55 on AlpacaEval)
- Worst-case evaluation reveals existing methods are less effective than previously thought, with many showing >20% accuracy on paraphrased queries despite 0% on original queries
- JensUn demonstrates strong resilience to benign relearning, with forget accuracy recovering only to 20% after 2k fine-tuning steps on unrelated data
- The method maintains better Pareto-optimal trade-offs between forgetting quality and utility preservation compared to baseline approaches

## Why This Works (Mechanism)

### Mechanism 1: Bounded Divergence Stabilizes Unlearning Dynamics
The bounded nature of JSD (0 ≤ JSD ≤ log 2) prevents the loss from diverging to extreme values during optimization. Unlike unbounded losses like cross-entropy, JSD provides gradients that are strictly smaller than those of KL divergence, preventing catastrophic utility collapse that occurs when forget losses become extremely negative.

### Mechanism 2: Implicit Forget-Retain Balancing via JSD
At initialization (θ = θ_ref), the retain loss is zero because JSD between identical distributions is zero. As the model parameters diverge from the reference, the retain loss activates and penalizes excessive drift. This dynamic automatically adjusts the "forget pressure" against the "retain pressure" as training progresses.

### Mechanism 3: Robust Evaluation via Worst-Case Probing
By generating diverse paraphrases and using an LLM judge rather than ROUGE scores, the evaluation tests whether knowledge is truly inaccessible or merely hidden behind specific prompt formats. Reporting worst-case accuracy ensures knowledge is genuinely forgotten across semantically equivalent inputs.

## Foundational Learning

- **Concept: Machine Unlearning**
  - **Why needed here**: This is the core problem the paper addresses. Understanding the goal—to selectively remove information—is essential.
  - **Quick check question**: What is the primary tradeoff in machine unlearning? (Answer: Forgetting the target information vs. retaining general model utility)

- **Concept: Divergence Measures (KL vs. JSD)**
  - **Why needed here**: The proposed method replaces other losses with JSD. Grasping its mathematical properties (symmetric, bounded) is key.
  - **Quick check question**: Why is the boundedness of a loss function important for optimization? (Answer: It prevents loss values from diverging to extreme levels, which can destabilize training)

- **Concept: LLM Evaluation Metrics (ROUGE vs. Semantic Judges)**
  - **Why needed here**: A major contribution is critiquing evaluation. Understanding why ROUGE is flawed (sensitivity to wording) and why an LLM judge is better is crucial.
  - **Quick check question**: Why might a high ROUGE score not indicate successful unlearning? (Answer: The model could output a semantically incorrect answer with many word overlaps)

## Architecture Onboarding

- **Component map**: Pre-trained Model (θ) -> JSD Loss Module -> AdamW Optimizer -> Unlearnt Model (θ') -> Evaluation Module (LLM Judge + Paraphraser)
- **Critical path**: Prepare forget/retain datasets → Initialize model from reference → Optimize combined JensUn loss → Evaluate using worst-case paraphrases and LLM judge
- **Design tradeoffs**:
  - λ_R tuning: The main knob for trading off forget quality vs. retain quality
  - Target string choice: Explicit refusal ("No idea") vs. non-response (whitespace)
  - Unlearning duration: More steps improve robustness to relearning but require careful monitoring
- **Failure signatures**:
  - Utility collapse: Model generates repetitive text/gibberish (λ_R too low)
  - Superficial forgetting: Model fails worst-case evaluation despite passing standard evaluation
  - Relearning vulnerability: Model quickly re-learns forgotten facts after benign fine-tuning
- **First 3 experiments**:
  1. Reproduce JensUn on LKF with Llama-3.2-3B, sweep λ_R to plot tradeoff curve
  2. Re-evaluate an existing unlearning method using proposed worst-case protocol with paraphrases and LLM judge
  3. Unlearn with JensUn and baseline, then fine-tune both on disjoint dataset and measure forget-set accuracy recovery

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Does JensUn maintain its Pareto-optimal forget-utility trade-off when applied to significantly larger parameter scales (e.g., 70B+ models) where optimization dynamics may differ?
- **Basis in paper**: The authors state in the conclusion, "We leave scaling to even larger LLMs... for future work," noting current experiments are limited to models up to 3.8B parameters
- **Why unresolved**: It is unclear if the stability provided by JSD gradients scales linearly with model size or if compute constraints alter the trade-off
- **What evidence would resolve it**: Successful application and maintenance of low forget accuracy and high utility on models exceeding 7B or 70B parameters

### Open Question 2
- **Question**: Is there a theoretical guarantee linking the "depth" of unlearning (training duration) to the irreversibility of knowledge removal?
- **Basis in paper**: The authors state, "We hypothesize that stronger unlearning, i.e. moving further from the pre-trained state, makes benign relearning harder," based on empirical observations in Table 3
- **Why unresolved**: The relationship is presented as a hypothesis based on empirical trends rather than a proven theoretical property of the JSD loss landscape
- **What evidence would resolve it**: A theoretical analysis connecting the divergence from pre-trained weights to catastrophic forgetting during subsequent fine-tuning

### Open Question 3
- **Question**: Does the reliance on a specific LLM (e.g., Gemini-2.5-Flash) as a semantic judge introduce new evaluation biases that could be gamed by unlearning methods?
- **Basis in paper**: While the paper critiques ROUGE and proposes an LLM-judge, it relies on a single proprietary model for "truth"
- **Why unresolved**: Replacing one imperfect metric with a judge prone to different failure modes could create false security
- **What evidence would resolve it**: Evaluation results using an ensemble of diverse judges or testing judge's robustness against adversarial unlearning outputs

## Limitations
- Limited to models up to 3.8B parameters; scalability to larger models remains untested
- Relies on single LLM judge (Gemini-2.5-Flash) which may introduce evaluation biases
- Benign relearning tests use unspecified datasets, making direct comparison difficult

## Confidence
- **High confidence**: JSD stability claims (bounded loss prevents catastrophic forgetting), worst-case evaluation methodology (supported by prior work on evaluation limitations)
- **Medium confidence**: Forget-utility tradeoff improvements (dependent on dataset specifics), relearning robustness (limited comparative data in paper)
- **Low confidence**: Complete irreversibility claims (only tested with benign relearning, not adversarial attacks)

## Next Checks
1. **Reproduce LKF results with synthetic data**: Create controlled forget/retain pairs with known properties to isolate JSD effects from dataset artifacts
2. **Stress-test evaluation protocol**: Apply worst-case evaluation to multiple existing unlearning methods to confirm the reported degradation in performance is systematic
3. **Adversarial relearning benchmark**: Test JensUn against fine-tuning on semantically similar but distinct concepts to probe robustness beyond benign updates