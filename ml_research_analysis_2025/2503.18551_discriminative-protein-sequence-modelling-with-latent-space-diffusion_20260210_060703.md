---
ver: rpa2
title: Discriminative protein sequence modelling with Latent Space Diffusion
arxiv_id: '2503.18551'
source_url: https://arxiv.org/abs/2503.18551
tags:
- diffusion
- protein
- latent
- space
- sequence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes a framework for protein sequence representation
  learning that separates manifold learning (via autoencoders) from distributional
  modelling (via diffusion models) to overcome the limitations of discrete diffusion
  approaches. The authors introduce two autoencoder architectures: LSD-TN, which enforces
  identical distributions for embeddings of the same amino acid type, and LSD-NM,
  which applies noise-based masking for robust reconstruction.'
---

# Discriminative protein sequence modelling with Latent Space Diffusion

## Quick Facts
- arXiv ID: 2503.18551
- Source URL: https://arxiv.org/abs/2503.18551
- Reference count: 20
- Key outcome: Diffusion models trained on latent spaces from LSD-TN and LSD-NM autoencoders outperform diffusion on MLM embeddings, though MLM embeddings remain the strongest baseline

## Executive Summary
This paper proposes a framework for protein sequence representation learning that separates manifold learning (via autoencoders) from distributional modelling (via diffusion models) to overcome the limitations of discrete diffusion approaches. The authors introduce two autoencoder architectures: LSD-TN, which enforces identical distributions for embeddings of the same amino acid type, and LSD-NM, which applies noise-based masking for robust reconstruction. They train diffusion models on the learned latent spaces and evaluate discriminative performance across five protein property prediction tasks. Results show that diffusion models trained on both LSD variants outperform those trained on masked language model (MLM) baselines, though MLM embeddings themselves still achieve the highest performance. The LSD-NM model generally performs better than LSD-TN, with both providing complementary strengths to the diffusion model outputs. Visualizations reveal that LSD-NM captures biologically meaningful features in its latent space.

## Method Summary
The framework decomposes protein sequence representation learning into two stages: manifold learning via autoencoders and distributional modeling via diffusion. Two autoencoder architectures are proposed: LSD-TN applies per-amino-acid-type normalization to force identical latent distributions, while LSD-NM uses inhomogeneous noise masking during reconstruction. Both are trained on UniRef50 with reconstruction and normalization losses. Diffusion models are then trained on the frozen latent spaces using a v-target objective with cosine schedules. Representations can be extracted from either the encoder output or the denoised diffusion output. The approach is evaluated on five protein property prediction tasks comparing diffusion and encoder representations from LSD variants against MLM baselines.

## Key Results
- Diffusion models trained on LSD-NM and LSD-TN latent spaces outperform diffusion models trained on MLM embeddings across all five tasks
- LSD-NM outperforms LSD-TN on 4/5 tasks, with LSD-TN showing complementary strength on HumanPPI
- MLM embeddings themselves remain the strongest baseline, outperforming all diffusion representations
- LSD-NM representations show biologically meaningful clustering in latent space visualizations

## Why This Works (Mechanism)

### Mechanism 1: Manifold-Distribution Decomposition
Separating representation learning into manifold learning (autoencoder) and distribution modeling (diffusion) enables continuous-space operations on discrete protein sequences. The autoencoder learns a continuous latent embedding where amino acid sequences become differentiable vectors. The diffusion model then learns to denoise corrupted latents via the v-target objective, producing a score function s(z) = ∇_z log p(z) that captures distributional structure across the learned manifold.

### Mechanism 2: Token Normalization Bottleneck (LSD-TN)
Constraining same-type amino acids to share identical latent distributions forces the encoder to learn contextual/positional information rather than relying on amino acid identity. The normalization loss is applied separately to each of the 20 amino acid types within a batch, preventing the model from using latent space location to encode identity directly.

### Mechanism 3: Inhomogeneous Noise Masking (LSD-NM)
Training the autoencoder with position-varying noise levels aligns the learned latent space with diffusion model requirements. During training, embeddings are corrupted via z_a → cos(πt_a/2)z_a + sin(πt_a/2)ε where t_a varies per position. The reconstruction loss is weighted by sin²(πt_a/2), emphasizing heavily corrupted positions.

## Foundational Learning

- **Diffusion models and score matching**: The diffusion component learns s(z) = ∇_z log p(z), the score function, which the paper interprets as a "distributional force." Understanding v-target vs. ε-prediction is essential for implementing the loss.
  - Quick check question: Why does the paper use the v-target formulation rather than direct ε-prediction, and how does Eq. 8 convert the learned v_t into a score function?

- **KL divergence for latent regularization**: The normalization loss constrains latents to be approximately Gaussian. This is distinct from VAE approaches that learn mean/variance parameters.
  - Quick check question: Why might the univariate form of KL divergence (treating dimensions independently) outperform the multivariate form (which accounts for covariance)?

- **Masked Language Modeling (MLM) for proteins**: MLM is the baseline the paper critiques and compares against. Understanding why binary masking creates "abrupt" latent geometry helps motivate the continuous noise approach.
  - Quick check question: The paper argues MLM embeddings "retain much of the discreteness"—what evidence in Table 2 supports this claim about diffusion performance on MLM latents?

## Architecture Onboarding

- **Component map**: Tokenized sequences → Encoder (Transformer) → Latent embeddings z → Decoder (Transformer) or Diffusion model (Transformer) → Token logits or denoised representations

- **Critical path**:
  1. Implement autoencoder encoder/decoder with matching dimensions
  2. Select bottleneck: LSD-TN (per-amino-acid normalization) or LSD-NM (inhomogeneous noise during reconstruction)
  3. Train autoencoder on UniRef50 with L_recon weighted against L_norm
  4. Freeze encoder, train diffusion model on latents with cosine schedule and v-target loss
  5. Extract representations: encoder output z OR diffusion output v̄_t(z) + sin(πt/2)z (Eq. 8)

- **Design tradeoffs**:
  - LSD-TN vs LSD-NM: TN is architecturally simpler but NM outperforms on 4/5 tasks (Table 2)
  - Encoder vs diffusion representations: For LSD variants, diffusion reps match or exceed encoder reps; for MLM baseline, encoder reps strongly outperform diffusion
  - Importance sampling: Ablation shows it matters—disabling it for diffusion degrades performance

- **Failure signatures**:
  - Diffusion representations dramatically underperform encoder → latent space likely too discrete
  - Reconstruction loss fails to converge → normalization loss may be too aggressive
  - t-dependent representations don't vary smoothly → check Eq. 8 implementation includes +sin(πt/2)z correction

- **First 3 experiments**:
  1. Reproduce LSD-NM-S on a 10% subset of UniRef50; verify diffusion representations at t=0 outperform encoder representations on Thermostability
  2. Ablate normalization weight: Train with λ ∈ {0.1, 0.5, 1.0, 2.0} and plot reconstruction loss vs. latent distribution statistics
  3. Probe t-dependence: Extract representations at t ∈ {0, 0.15, 0.3, 0.5, 0.7} for LSD-NM-M and evaluate on all five tasks to replicate Fig. 2 curves

## Open Questions the Paper Calls Out

- **Open Question 1**: Can a latent space be constructed that simultaneously optimizes distributional modeling for diffusion and discriminative performance for representation learning? The authors note that while their proposed spaces aided diffusion, they fell short of MLM embedding performance.

- **Open Question 2**: Can the inherent discreteness of protein sequences be sufficiently smoothed in a latent space to justify continuous reconstructive learning? The authors write that it remains unclear whether discreteness can be sufficiently removed from the latent space.

- **Open Question 3**: Does the Latent Space Diffusion (LSD) framework possess generative capabilities competitive with discrete diffusion methods? The authors defer the more challenging generative aspect but conclude it would be of great interest to explore generative capabilities.

## Limitations

- The comparative advantage over standard MLM embeddings is modest—MLM embeddings remain the strongest baseline despite diffusion on LSD variants outperforming diffusion on MLM
- The normalization bottleneck in LSD-TN lacks direct corpus support for its effectiveness
- The specific noise schedule in LSD-NM shows no ablation study on its design
- The claim that "diffusion models trained on both LSD variants outperform those trained on MLM baselines" is well-supported, but the subsequent claim that "MLM embeddings still achieve the highest performance" creates an apparent contradiction

## Confidence

- **High**: Diffusion models can be trained on autoencoder-learned latent spaces; LSD-NM generally outperforms LSD-TN across tasks
- **Medium**: Diffusion representations from LSD variants match or exceed encoder representations (though this varies by task); LSD-NM captures meaningful biological features
- **Low**: The normalization bottleneck in LSD-TN provides consistent benefit; the specific noise schedule in LSD-NM is optimal; diffusion representations will consistently complement MLM embeddings

## Next Checks

1. Replicate the core Table 2 comparison on a held-out validation set to verify that diffusion on LSD-NM embeddings consistently outperforms diffusion on MLM embeddings across all five tasks

2. Perform an ablation study varying the normalization weight λ in LSD-TN to identify the optimal tradeoff between reconstruction fidelity and latent space regularization

3. Test whether the complementary strengths observed between LSD-NM and LSD-TN representations (particularly for HumanPPI) generalize to other interaction prediction tasks beyond the five evaluated