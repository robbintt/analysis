---
ver: rpa2
title: 'A3: Android Agent Arena for Mobile GUI Agents with Essential-State Procedural
  Evaluation'
arxiv_id: '2501.01149'
source_url: https://arxiv.org/abs/2501.01149
tags:
- evaluation
- agents
- agent
- tasks
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces A3, a novel benchmark and evaluation system
  for mobile GUI agents that addresses key limitations in existing static and offline
  evaluations. The authors curate 100 tasks from 20 popular online apps spanning 20
  categories, enabling assessment of agents in dynamic, real-world scenarios previously
  considered untestable.
---

# A3: Android Agent Arena for Mobile GUI Agents with Essential-State Procedural Evaluation

## Quick Facts
- arXiv ID: 2501.01149
- Source URL: https://arxiv.org/abs/2501.01149
- Reference count: 9
- Primary result: Introduces A3 benchmark with 100 tasks from 20 online apps using essential-state procedural evaluation; A3RM reward model achieves 95.7% precision vs 87.3% for commercial MLLMs

## Executive Summary
A3 introduces a novel benchmark and evaluation system for mobile GUI agents that addresses key limitations in existing static and offline evaluations. The authors curate 100 tasks from 20 popular online apps spanning 20 categories, enabling assessment of agents in dynamic, real-world scenarios previously considered untestable. They propose an "essential-state" based procedural evaluation method that uses MLLMs to progressively verify task completion through semantically meaningful milestones rather than binary success/failure judgments. Experiments with 10 diverse agents show the benchmark's difficulty: while the best commercial agent achieves 53% success rate, open-source agents struggle with 14-27% performance.

## Method Summary
A3 uses essential-state procedural evaluation where long-horizon tasks are decomposed into discrete, visually-verifiable milestones. Human annotators define essential states through a 3-stage protocol: visual verification, state definition, and negative mining. Agent trajectories are segmented using a sliding window mechanism (optimal: window size 3-4, interval 2) and evaluated by MLLMs or A3RM, a fine-tuned reward model from Qwen3-VL-8B trained on 8241 step-wise samples. The complete pipeline includes AITK for Android interaction, essential-state annotation, and aggregation metrics (ESAR and SR).

## Key Results
- A3RM achieves 95.7% precision at essential-state level, outperforming Gemini-2.5-pro's 87.3%
- Commercial agents achieve 53% success rate on benchmark; open-source agents range 14-27%
- Structured frameworks significantly enhance weak base models but backbone capability remains the performance ceiling
- Sliding window size 3-4 with interval 2 provides optimal balance between context and visual fidelity

## Why This Works (Mechanism)

### Mechanism 1
Decomposing long-horizon tasks into semantically-defined "essential states" enables more reliable MLLM-based evaluation than end-to-end judgment. Each essential state represents a discrete, visually-verifiable milestone, simplifying the reasoning required of the MLLM. Core assumption: essential states defined by high-level semantic outcomes remain invariant across UI variations. Evidence: "progressively verify task completion through semantically meaningful milestones" and "simplifies the reasoning required of the MLLM" [abstract, Page 5].

### Mechanism 2
A small, fine-tuned reward model (A3RM) can outperform commercial MLLMs on specialized GUI evaluation through hard negative mining and structured supervision. A3RM is fine-tuned from Qwen3-VL-8B using positive samples and negative samples from both expert trajectories and agent failures. Core assumption: domain-specific training yields higher precision than general-purpose reasoning. Evidence: A3RM achieves 95.7% precision vs Gemini's 87.3% [Page 7, Table 2].

### Mechanism 3
Structured agent frameworks can partially compensate for weak backbone models, but backbone capability remains the performance ceiling. Frameworks like T3A and Mobile-Use provide planning scaffolding and error recovery. Core assumption: frameworks reduce error propagation but cannot synthesize reasoning capabilities absent in the base model. Evidence: "structured frameworks significantly enhance weak base models... However, the backbone model remains the upper bound of performance" [Page 7-8].

## Foundational Learning

- **Static vs. Dynamic GUI Evaluation**: A3 enables dynamic evaluation of online apps where internal states cannot be instrumented. Why needed: clarifies why essential-state evaluation is necessary. Quick check: Can you explain why AndroidWorld's function-based evaluation fails for closed-source apps like Amazon or CNN?

- **MLLM-as-Judge Paradigm**: A3 uses MLLMs as evaluators; understanding their strengths (visual comprehension) and weaknesses (hallucination, cost) is essential for selecting between commercial APIs and A3RM. Quick check: What is the trade-off between using Gemini-2.5-pro vs. A3RM for evaluation in terms of precision, cost, and deployment flexibility?

- **Sliding Window for Sequential Verification**: The sliding window mechanism determines how trajectory segments are presented to evaluators. Why needed: window size affects context availability vs. visual fidelity loss. Quick check: Why does window size of 2 with non-overlapping stride degrade accuracy compared to overlapping windows?

## Architecture Onboarding

- **Component map**: AITK (Translator + Controller) -> Agent (Model-based or Framework-based) -> Evaluator (Essential-state verifier)
- **Critical path**: Task definition → Essential state annotation → Agent execution via AITK → Sliding window segmentation → MLLM/A3RM judgment → Aggregate achievements → Compute ESAR and SR
- **Design tradeoffs**: Commercial vs. A3RM evaluator (Gemini: convenience and high recall; A3RM: higher precision, lower cost, local deployment); Window size (3-4 optimal); Framework vs. base model investment (frameworks help weak models but strong base models yield higher ceiling)
- **Failure signatures**: Progress Unawareness (agents loop on completed essential states), Screen Misunderstanding (incorrect element identification), Dynamic Interference (stuck on pop-ups or sponsored content)
- **First 3 experiments**: 1) Baseline reproduction with Qwen3-VL and InfiGUI-R1 on 10 tasks; 2) Evaluator calibration comparing Gemini vs. A3RM judgments; 3) Framework ablation testing weak base model with and without T3A

## Open Questions the Paper Calls Out

### Open Question 1
How can mobile GUI agents be enhanced to resolve "progress unawareness," specifically the tendency to loop redundant actions or fail to terminate? The paper identifies this as a critical failure mode requiring improved self-awareness and goal recognition [Appendix A.5.1].

### Open Question 2
What architectural or training improvements are required to stabilize agent performance in "Hard" tasks (>11 steps)? Most models exhibit precipitous performance declines on hard tasks, lacking long-horizon robustness [Section 4.2].

### Open Question 3
How can agents be trained to effectively handle dynamic interference (e.g., pop-up ads, sponsored content) inherent to online apps? Agents often deadlock when encountering unpredictable elements like sudden whole-screen ads [Appendix A.5.2].

## Limitations
- Generalizability of A3RM to substantially different app categories or UI paradigms remains untested
- Scalability of annotation protocol resource-intensive with 3-stage human annotation process
- Android-specific constraints may limit cross-platform applicability to iOS or web-based GUI agents

## Confidence
- **High Confidence**: Essential-state procedural evaluation mechanism (systematic ablation studies and A3RM's precision advantage)
- **Medium Confidence**: Benchmark's representativeness of real-world complexity (covers 20 categories but may miss regional preferences)
- **Low Confidence**: Performance ceiling implications (demonstrates backbone dominance but doesn't test specialized fine-tuning)

## Next Checks
1. Test A3's evaluation pipeline on iOS apps or web-based interfaces to assess platform dependence
2. Evaluate agent performance on tasks requiring 10+ essential states to determine sliding window accuracy for complex workflows
3. Fine-tune A3RM on a disjoint set of 25 tasks from different app categories and measure performance degradation to quantify domain adaptation requirements