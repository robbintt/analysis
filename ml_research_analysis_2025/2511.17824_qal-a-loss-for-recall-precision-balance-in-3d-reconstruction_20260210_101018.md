---
ver: rpa2
title: 'QAL: A Loss for Recall Precision Balance in 3D Reconstruction'
arxiv_id: '2511.17824'
source_url: https://arxiv.org/abs/2511.17824
tags:
- coverage
- point
- loss
- infocd
- recall
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: QAL introduces a new loss function for 3D reconstruction that explicitly
  balances recall and precision by combining coverage-weighted matching with uncovered-ground-truth
  attraction. Unlike Chamfer Distance and Earth Mover's Distance, which fail to capture
  missing structures and spurious predictions, QAL decouples these components into
  tunable terms.
---

# QAL: A Loss for Recall Precision Balance in 3D Reconstruction

## Quick Facts
- arXiv ID: 2511.17824
- Source URL: https://arxiv.org/abs/2511.17824
- Reference count: 40
- Primary result: Improves coverage by +4.3 points over Chamfer Distance while maintaining comparable spurious point rates

## Executive Summary
QAL introduces a new loss function for 3D reconstruction that explicitly balances recall and precision by combining coverage-weighted matching with uncovered-ground-truth attraction. Unlike Chamfer Distance and Earth Mover's Distance, which fail to capture missing structures and spurious predictions, QAL decouples these components into tunable terms. Across point cloud completion, single-view reconstruction, and mesh generation, QAL improves coverage by an average of +4.3 points over Chamfer Distance and +2.8 points over alternative methods, while also enhancing downstream grasp utility. Extensive ablations confirm robust performance across hyperparameters and output resolutions. QAL serves as a practical, interpretable drop-in replacement that aligns training with thresholded evaluation metrics, making it particularly valuable for safety-critical applications where missing geometry is costly.

## Method Summary
QAL is a quality-aware loss function that addresses Chamfer Distance's inability to capture missing structures and spurious predictions. It combines a coverage-weighted distance term that down-weights already-accurate predictions with an uncovered-ground-truth attraction term that pulls predictions toward unmatched GT points. The loss is parameterized by ε (tolerance margin), ω (weight sharpness), and λ_attr (attraction weight), allowing explicit control over the recall-precision trade-off. QAL is implemented as a drop-in replacement for CD/EMD in standard 3D reconstruction pipelines, requiring only bidirectional nearest-neighbor distance computation and an uncovered mask to identify unmatched GT points.

## Key Results
- Coverage improvements of +4.3 points over Chamfer Distance and +2.8 points over alternative methods across multiple tasks
- Pareto-optimal trade-off between coverage and spurious points, with λ_attr ∈ [0.5, 1.0] capturing most gains while limiting spurious point erosion
- Downstream grasp utility improvements in GraspNet-1Billion simulation, validating coverage → grasp transfer
- Robust performance across output resolutions (2048, 4096, 8192 points) and multiple backbone architectures

## Why This Works (Mechanism)

### Mechanism 1: Coverage-Weighted Distance Rebalancing
- Claim: Down-weighting already-accurate predictions shifts optimization budget toward under-covered regions
- Mechanism: A soft-margin function w_cov(d) = 1 - (σ(ω(ϵ-d)) - 0.5) assigns lower weight to point pairs within tolerance ϵ and higher weight to residuals beyond it
- Core assumption: Ground-truth regions with large residuals indicate missing coverage rather than noise
- Evidence anchors: Section 3.2 shows alignment with thresholded evaluation metrics; Figure 3 ablation demonstrates ϵ=0.001 yields best trade-off

### Mechanism 2: Uncovered Ground-Truth Attraction
- Claim: Explicitly pulling predictions toward unmatched GT points closes holes that CD ignores
- Mechanism: A binary mask m_B(b) identifies GT points with no predicted neighbor, generating gradients that draw predictions toward these uncovered regions
- Core assumption: Missing correspondences indicate genuine geometric gaps rather than spurious GT noise
- Evidence anchors: Section 3.1 explains CD's inability to penalize uncovered regions; Figure 2a visualizes attraction links pulling toward uncovered GT

### Mechanism 3: Tunable Recall-Precision Trade-off via λ_attr
- Claim: Decoupling coverage and attraction terms allows explicit control over the recall-precision balance
- Mechanism: The final objective L_QAL = L_cov + λ_attr · L_attr lets practitioners tune λ_attr based on application tolerance
- Core assumption: Applications can tolerate some precision reduction in exchange for higher recall
- Evidence anchors: Section 4.1 shows increasing λ_attr raises coverage but reduces spurious points; Table 1 demonstrates backbone-dependent trade-offs

## Foundational Learning

- Concept: **Chamfer Distance (CD) and its blind spots**
  - Why needed here: QAL is motivated by CD's failure to penalize holes and spurious clustering
  - Quick check question: Given two point clouds where prediction A has a hole (no points near GT region B), does CD produce a gradient pointing toward that hole? (Answer: No—CD only measures distances to nearest neighbors, not absence of correspondences)

- Concept: **Recall vs. Precision in point clouds**
  - Why needed here: QAL explicitly decouples these via Coverage (recall proxy) and Spurious Points (precision proxy)
  - Quick check question: If you increase output point density uniformly across a reconstruction, what happens to Coverage vs. SP? (Answer: Coverage may stay flat if new points cluster near existing ones; SP increases if points spread to unmatched regions)

- Concept: **Thresholded evaluation metrics (Cov@τ, F1@τ)**
  - Why needed here: QAL aligns training with these metrics by using the same τ as its ϵ margin
  - Quick check question: Two models have identical CD but different Coverage. How is this possible? (Answer: CD averages distances; one model may have clustered predictions with small distances but leave large holes, while another distributes points evenly)

## Architecture Onboarding

- Component map: Input point sets A,B -> Bidirectional NN distances d_a,d_b -> Coverage weights w(d) -> Uncovered mask m_B -> L_cov + λ_attr·L_attr -> Scalar loss L_QAL

- Critical path:
  1. Compute all bidirectional nearest-neighbor distances (GPU cdist or kd-tree)
  2. Apply soft-margin weighting—requires ε and ω to be set before training
  3. Compute uncovered mask—requires tracking which GT points have at least one prediction within NN mapping
  4. Combine terms with λ_attr weighting

- Design tradeoffs:
  - **ε selection**: Must match evaluation tolerance (paper uses ε=0.001 for meter-scale objects, evaluates Cov@0.03)
  - **ω (sharpness)**: ω=10 balances smooth gradients vs. sharp margin
  - **λ_attr**: Higher values improve coverage but risk spurious points
  - **Output resolution**: Paper shows robustness across 2048, 4096, 8192 points

- Failure signatures:
  - Gradient instability during early training: Check if ε < 10^-3; increase to 10^-3 or reduce ω
  - Coverage plateaus despite high λ_attr: May indicate backbone capacity limit or severe occlusion
  - Excessive spurious points in flat regions: Reduce λ_attr or increase ε margin
  - No improvement over CD baseline: Verify NN distance computation is bidirectional

- First 3 experiments:
  1. Hyperparameter sanity check on single category: Train PCN/ECG on one MVP category with ε∈{0.001, 0.01}, ω∈{1, 10, 100}, λ_attr∈{0, 0.5, 1.0}
  2. Drop-in replacement validation: Take existing CD-trained checkpoint, freeze hyperparameters, replace loss with QAL using paper defaults, fine-tune for 20-50 epochs
  3. Downstream task probe: Use GraspNet-1Billion evaluation protocol—generate completions with QAL-trained vs. CD-trained models, compare grasp scores and number of feasible grasps

## Open Questions the Paper Calls Out

- Can the improved grasp scores observed in simulation translate to higher success rates on physical robotic hardware? (The authors note this relies on simulated scoring and remains future work)
- Does adaptive scheduling of the tolerance ε and attraction weight λ_attr improve reconstruction performance on highly occluded shapes? (Identified as a specific future direction to address severe incompleteness)
- Can the integration of stronger shape priors with QAL resolve failure cases involving severe incompleteness or ambiguous geometry? (Suggested as a way to handle thin or elongated parts under severe occlusion)

## Limitations

- Uncovered-GT attraction assumes missing correspondences indicate genuine geometric gaps rather than alignment errors or spurious GT noise
- The trade-off between coverage and spurious points is architecture-dependent, with some backbones showing slight quality degradation when tuned for coverage
- Performance improvements rely on careful hyperparameter selection (ε, ω, λ_attr) that may require re-tuning for different output resolutions or application domains

## Confidence

- High confidence: Coverage improvements (+4.3 points over CD baseline) and Pareto-optimal trade-off behavior across multiple backbones and datasets
- Medium confidence: Uncovered-GT attraction mechanism's effectiveness in safety-critical applications, as validation relies primarily on synthetic grasp score improvements
- Medium confidence: Cross-resolution robustness claims, as extensive ablation only covers 2048, 4096, and 8192 point outputs

## Next Checks

1. Test QAL on safety-critical 3D reconstruction tasks where missing geometry is costly (e.g., autonomous navigation) to validate uncovered-GT attraction's real-world effectiveness
2. Evaluate QAL with severely occluded inputs where GT contains ambiguously-positioned points to assess attraction term's behavior under uncertainty
3. Benchmark QAL against alternative robust CD formulations (e.g., APML) on thin-structure reconstruction to compare handling of sparse correspondences