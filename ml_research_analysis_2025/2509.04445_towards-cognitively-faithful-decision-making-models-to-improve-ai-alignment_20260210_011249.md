---
ver: rpa2
title: Towards Cognitively-Faithful Decision-Making Models to Improve AI Alignment
arxiv_id: '2509.04445'
source_url: https://arxiv.org/abs/2509.04445
tags:
- decision
- inner
- human
- feature
- such
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of building AI models that accurately
  simulate human decision-making processes, focusing on the need for "cognitively-faithful"
  models that capture heuristics and decision rules people use. The authors propose
  a two-stage hypothesis class for pairwise comparisons, where features are first
  processed/edited individually (or conditionally based on context), then aggregated
  to make a final choice.
---

# Towards Cognitively-Faithful Decision-Making Models to Improve AI Alignment

## Quick Facts
- arXiv ID: 2509.04445
- Source URL: https://arxiv.org/abs/2509.04445
- Reference count: 40
- Primary result: A two-stage model (feature editing → aggregation) for pairwise comparisons achieves accuracy comparable to or better than baselines while providing interpretable insights into individual decision rules.

## Executive Summary
This paper proposes a cognitively-faithful framework for modeling human decision-making from pairwise comparisons, motivated by axiomatic characterizations of decision processes. The key innovation is a two-stage hypothesis class where features are first individually edited/processed (potentially conditioned on context), then aggregated via a dominance testing rule. This structure emerges from basic decision-making axioms (complementarity, transitivity, feature independence) and allows recovery of interpretable heuristics like thresholds and diminishing returns. The method is validated on real kidney allocation data and synthetic datasets, demonstrating both predictive accuracy and interpretability of learned editing functions.

## Method Summary
The approach uses a two-stage hypothesis class for pairwise comparisons: first, individual features are edited by monotonic functions (possibly conditioned on a context variable ω), then these edited contributions are aggregated via a dominance testing rule (typically tallying). The model is trained via constrained optimization (SLSQP) with monotonicity constraints and regularization that penalizes divergence of editing functions across context values. Context selection is performed via cross-validation. The framework is evaluated on kidney allocation data from 15-40 participants and synthetic datasets simulating different heuristics.

## Key Results
- The proposed two-stage model achieves accuracy comparable to or better than logistic regression, decision trees, and MLP baselines on real kidney allocation data.
- Learned editing functions recover interpretable heuristics (e.g., threshold rules, diminishing returns) from both real and synthetic data.
- Context-conditional editing improves model expressiveness, with regularization stabilizing learning across context values.

## Why This Works (Mechanism)

### Mechanism 1: Axiomatic Factorization into Two-Stage Processing
Building on axiomatic characterizations, complementarity ensures predictions sum to 1 across swapped alternatives, implying an atomic rule difference structure. Weak transitivity plus continuity forces the outer aggregation to be a sigmoid (CDF). Noninteractive compositionality yields additive decomposition across features. This structural factorization emerges from axioms rather than arbitrary design.

### Mechanism 2: Learned Feature Editing Captures Heuristics
Feature-level transformations recover threshold rules, diminishing returns, and other heuristics that linear models miss. Each editing function maps raw feature values to scalar contribution scores, capturing step-function contributions (thresholding), log transforms (diminishing returns), and binary indicators (zeroing irrelevant values) through learned monotonic but nonlinear mappings.

### Mechanism 3: Constrained Optimization with Monotonicity and Regularization
Monotonicity constraints ensure contributions respect feature ordering, while cross-context regularization (λ · Σ ||hᵢ,ₐ − hᵢ,ᵦ||²) penalizes divergence across context values, stabilizing conditional editing. This constrained optimization improves generalization and interpretability by enforcing domain-appropriate structure.

## Foundational Learning

- **Generalized Linear Models and Link Functions**: The outer aggregation function σ(·) acts as a link function (logistic for Bradley-Terry, Gaussian CDF for probit). Understanding GLMs clarifies why transitivity axioms force sigmoid structure. *Quick check: Given a Bradley-Terry model P(A > B) = σ(w·(x_A − x_B)), what constraint does complementarity impose on σ?*

- **Preference Elicitation via Pairwise Comparisons**: The framework operates on pairwise choice data. Understanding how noisy preferences are aggregated (Bradley-Terry, Thurstone-Mosteller) grounds the outer aggregation stage. *Quick check: Why might pairwise comparisons be more reliable than direct rating scales for preference learning?*

- **Constrained Optimization (SLSQP)**: Training uses sequential least squares with monotonicity constraints. Understanding how to formulate and solve constrained problems is essential for implementation. *Quick check: How would you modify the constraint set if a feature should have non-monotonic contribution (e.g., "medium skill level preferred")?*

## Architecture Onboarding

- **Component map**: Input (x₁, x₂) ∈ ℝᵈ → Editing layer (d functions hᵢ,ωᵢ⁽inner⁾ : Xᵢ → ℝ) → Difference computation (Δᵢ = hᵢ(x₁⁽ⁱ⁾) − hᵢ(x₂⁽ⁱ⁾)) → Aggregation (h^outer(Σᵢ Δᵢ) where h^outer = σ) → Output P(x₁ preferred to x₂) ∈ [0,1]

- **Critical path**: Initialize editing functions → Forward pass (apply editing → compute differences → aggregate → sigmoid) → Loss (cross-entropy or hinge) → Backward (SLSQP with monotonicity constraints + regularization on conditional variants) → Validate context ω via held-out data

- **Design tradeoffs**: Larger context ω increases expressiveness but raises sample complexity and overfitting risk; cross-entropy yields probabilistic outputs while hinge loss provides cleaner decision boundaries; tabular editing is more interpretable while parametric generalizes better to unseen values

- **Failure signatures**: Accuracy plateaus below baselines (likely ω too large or regularization too strong); learned editing functions non-monotonic despite constraints (optimization tolerance too loose); conditional editing functions nearly identical (regularization too strong or context irrelevant); transitivity violations in predictions (check if aggregation rule matches domain)

- **First 3 experiments**: (1) Synthetic validation: Generate data from known heuristics, verify learned hᵢ⁽inner⁾ recover ground-truth transformations, compare to logistic regression baseline; (2) Ablation on context ω: Compare ω = ∅ vs. ω = {most important feature} vs. ω = all features on real kidney data, measure accuracy and inspect conditional editing divergence; (3) Interpretability check: For 3-5 real participants, visualize learned editing functions, have domain experts validate whether shapes match reported heuristics

## Open Questions the Paper Calls Out

### Open Question 1
Do end-users actually find the learned two-stage models more interpretable and trustworthy than standard interpretable models (e.g., logistic regression, decision trees) when evaluating AI decisions? While the paper provides interpretability case studies, it lacks user studies validating whether users genuinely find these models more interpretable than baselines.

### Open Question 2
How can the framework be extended to quantify and model systematic deviations from "ideal" decision-making axioms (transitivity, complementarity violations) in real human choices? The current framework assumes axiom compliance, but documented human violations mean learned models may not fully capture actual decision processes.

### Open Question 3
Do cognitively-faithful models learned from hypothetical moral judgments generalize to actual real-world moral decisions? The kidney allocation dataset captures stated preferences in hypothetical scenarios, which may differ from revealed preferences when real stakes are involved.

### Open Question 4
How does model performance and learning complexity scale with increasing context size (ω) for conditional feature interactions? The paper notes larger ω increases learning complexity but was artificially constrained without systematic analysis of this tradeoff.

## Limitations
- Small per-participant datasets (~300-400 samples) may not fully capture long-tail preference structures
- Context selection via cross-validation could overfit in high-dimensional settings
- Monotonic constraint assumption may not hold for all domains with non-monotonic relationships

## Confidence

- **Axiomatic foundation and two-stage structure**: High - formal proofs support the structural claims
- **Empirical heuristic recovery in real data**: Medium - supported by experimental results but limited ablation studies
- **Cross-context regularization effectiveness**: Medium - regularization helps but sensitivity analysis is limited

## Next Checks

1. **Sensitivity analysis**: Vary regularization λ across 10⁻⁵ to 10⁰ and document impact on accuracy, interpretability, and context selection stability

2. **Cross-domain transfer**: Apply the framework to a different pairwise comparison dataset (e.g., consumer choice or medical triage) with different feature types and evaluate heuristic recovery

3. **Intransitivity stress test**: Generate synthetic data with known cyclic preferences (e.g., rock-paper-scissors patterns) and measure model performance and structural violations