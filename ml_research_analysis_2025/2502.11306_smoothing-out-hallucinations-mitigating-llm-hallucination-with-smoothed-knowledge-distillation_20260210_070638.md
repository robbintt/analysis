---
ver: rpa2
title: 'Smoothing Out Hallucinations: Mitigating LLM Hallucination with Smoothed Knowledge
  Distillation'
arxiv_id: '2502.11306'
source_url: https://arxiv.org/abs/2502.11306
tags:
- hallucination
- labels
- language
- knowledge
- hard
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates knowledge distillation as a method to reduce
  hallucination in large language models (LLMs) by replacing hard-label training with
  smoothed soft labels from a teacher model. Experiments on summarization benchmarks
  show that models trained with knowledge distillation exhibit lower hallucination
  rates compared to standard supervised finetuning while maintaining strong performance
  on general NLP tasks.
---

# Smoothing Out Hallucinations: Mitigating LLM Hallucination with Smoothed Knowledge Distillation

## Quick Facts
- **arXiv ID:** 2502.11306
- **Source URL:** https://arxiv.org/abs/2502.11306
- **Reference count:** 32
- **Primary result:** Knowledge distillation with smoothed soft labels reduces hallucination rates in LLMs while maintaining general NLP task performance.

## Executive Summary
This paper addresses the critical problem of hallucination in large language models by replacing hard-label supervised finetuning with knowledge distillation from teacher models. The authors argue that hard labels force overconfidence and ignore the inherent uncertainty in natural language, leading to unfaithful outputs. Through experiments on summarization benchmarks across multiple model families (Llama-2, Llama-3.1, Qwen-2.5), they demonstrate that training with teacher-generated soft labels consistently reduces hallucination rates while preserving or improving general capabilities. The approach is particularly effective for faithfulness hallucination, where model outputs contradict the input context.

## Method Summary
The authors propose replacing standard supervised finetuning with knowledge distillation using smoothed soft labels from a teacher model. The training objective combines standard cross-entropy loss with KL divergence between student and teacher probability distributions: L = L_supervised + α × L_KD. Teacher models are first finetuned on the instruction dataset (Dolly) to ensure alignment, then used to generate probability distributions for training data. Students are trained on this combination, with the KD weight α tuned per model family. The approach is evaluated on CNN/DM and XSUM summarization benchmarks using ROUGE-L for n-gram overlap, Vectara factual consistency classifier, and Lookback-Lens attention-based factual rate metric.

## Key Results
- Knowledge distillation reduces faithfulness hallucination across Llama-2, Llama-3.1, and Qwen-2.5 model families
- KD consistently outperforms standard supervised finetuning on factual consistency metrics while maintaining or improving ROUGE-L scores
- Soft labels reduce overconfidence on incorrect predictions compared to hard labels, as shown by NLL distributions
- Optimal KD weight (α) varies significantly by model family: Llama-2 (0.1), Llama-3.1 (0.01), Qwen (1.0)

## Why This Works (Mechanism)

### Mechanism 1: Overconfidence Reduction via Soft Label Regularization
Hard labels force the model to assign probability 1.0 to a single token and 0.0 to all alternatives, leading to overconfident mispredictions on ambiguous inputs. Soft labels distribute probability mass across reasonable continuations, preventing brittle mappings that generalize poorly. This reduces faithfulness hallucination by avoiding overconfident predictions that contradict input context.

### Mechanism 2: Maximum Entropy Preservation
Hard labels impose zero entropy by selecting a single "correct" continuation even when context permits multiple valid options. Teacher-generated soft labels retain non-zero probabilities for contextually plausible alternatives, reducing spurious correlations and improving generalization. This aligns better with the principle of maximum entropy for natural language generation.

### Mechanism 3: Context-Dependent Probability Transfer
Teacher models encode contextual grounding into their probability distributions, which student models internalize via KL-divergence loss. The teacher computes softmax distributions conditioned on full input context, implicitly encoding which tokens are contextually grounded versus spurious. Minimizing divergence transfers this grounding signal without explicit annotation.

## Foundational Learning

- **Knowledge Distillation (KD)**: The core intervention replaces hard labels with teacher-generated soft labels. Understanding KD as transferring learned probability distributions (not just model compression) is essential. Quick check: Can you explain why minimizing KL-divergence between student and teacher logits transfers different information than cross-entropy against one-hot labels?

- **Model Calibration**: The hypothesis is that overconfidence (poor calibration) drives hallucination. You need to distinguish between accuracy (correctness) and calibration (confidence alignment). Quick check: If a model predicts the correct answer with 99% confidence vs. 55% confidence, which is better calibrated if the true correctness rate is 60%?

- **Faithfulness vs. Factuality Hallucination**: The paper evaluates faithfulness hallucination (output contradicts input context), not factuality hallucination (output contradicts world knowledge). Conflating these leads to misinterpretation. Quick check: If a summary correctly states what a document claims, but the document itself contains false information, is this a faithfulness error or a factuality error?

## Architecture Onboarding

- **Component map**: Teacher model (finetuned on Dolly) -> generates soft labels -> Student model (trained with combined loss L = L_supervised + α × L_KD)

- **Critical path**: 1) Finetune teacher model on instruction dataset using QLoRA for large teachers. 2) Run teacher inference on training data to generate soft labels. 3) Train student with combined loss, tuning α via validation performance. 4) Evaluate on held-out hallucination benchmarks without training on their splits.

- **Design tradeoffs**: 
  - Teacher size vs. transfer quality: Larger teachers may be more capable but add inference cost
  - α value: Low α (0.01-0.1) preserves more supervised signal; high α (1.0-10.0) increases soft label influence
  - Word-level vs. sequence-level KD: Llama-2 experiments use both; Llama-3.1 and Qwen use word-level only
  - Pretraining vs. finetuning KD: Paper applies KD only during SFT due to compute constraints

- **Failure signatures**:
  - Teacher hallucination inheritance: Monitor teacher outputs on validation data
  - Over-regularization at high α: Check general benchmarks for degradation
  - Metric disagreement: Use multiple metrics; prioritize faithfulness-specific measures
  - Domain mismatch: Models finetuned on Dolly may underperform on domain-specific benchmarks

- **First 3 experiments**:
  1. Baseline replication: Finetune a student model on Dolly with standard SFT. Evaluate on CNN/DM and XSUM using all three hallucination metrics.
  2. α sweep with fixed teacher: Run KD experiments with α ∈ {0.01, 0.1, 1.0} against a finetuned teacher. Plot hallucination metrics vs. α to identify optimal weighting.
  3. Ablation on teacher quality: Compare two teachers—one finetuned on Dolly, one off-the-shelf—to test whether teacher alignment affects student hallucination reduction.

## Open Questions the Paper Calls Out
None

## Limitations
- The paper does not report training epochs for either teacher finetuning or student KD training
- Results show metric inconsistency where ROUGE-L and factual rate sometimes move in opposite directions
- Only tested on summarization benchmarks; generalization to other generation tasks remains unknown
- Teacher model inference overhead during training adds computational cost not quantified in the analysis

## Confidence

**High confidence (3-4 sources per claim):**
- Knowledge distillation consistently reduces faithfulness hallucination across multiple model families
- Soft labels reduce overconfidence compared to hard labels, as evidenced by NLL distributions
- The proposed method maintains general NLP capability while improving factual consistency

**Medium confidence (2-3 sources per claim):**
- Overconfidence is a primary driver of hallucination, though the paper does not definitively prove this causal relationship
- The maximum entropy preservation mechanism provides theoretical justification, but empirical validation is limited to case studies
- Context-dependent probability transfer from teacher to student is plausible but not directly measured

**Low confidence (1-2 sources per claim):**
- KD during pretraining would yield stronger hallucination reduction (claimed but untested)
- The specific optimal α values per model family are definitive (based on single hyperparameter sweep)

## Next Checks

1. **Teacher quality ablation study**: Compare KD results using three different teachers for the same student—one finetuned on Dolly, one off-the-shelf, and one with known hallucination issues. This directly tests whether teacher grounding quality determines student hallucination reduction.

2. **Cross-task generalization experiment**: Apply the KD methodology to a non-summarization task like dialogue generation or question answering. Measure both task performance and hallucination using task-specific faithfulness metrics to validate whether soft labels generalize beyond summarization.

3. **Pretraining KD pilot**: Implement knowledge distillation during pretraining (1-2 epochs on a subset of data) for a small model (7B parameters) and compare hallucination rates against finetuning-only KD. This tests the authors' hypothesis that pretraining KD would yield stronger effects.