---
ver: rpa2
title: 'InstructMoLE: Instruction-Guided Mixture of Low-rank Experts for Multi-Conditional
  Image Generation'
arxiv_id: '2512.21788'
source_url: https://arxiv.org/abs/2512.21788
tags:
- routing
- image
- expert
- experts
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles catastrophic forgetting and task interference
  in parameter-efficient fine-tuning of diffusion transformers for multi-conditional
  image generation. It introduces InstructMoLE, which replaces token-level expert
  routing with instruction-guided global routing to maintain semantic consistency
  across tokens, and adds an output-space orthogonality loss to enforce expert diversity
  and prevent collapse.
---

# InstructMoLE: Instruction-Guided Mixture of Low-rank Experts for Multi-Conditional Image Generation

## Quick Facts
- **arXiv ID:** 2512.21788
- **Source URL:** https://arxiv.org/abs/2512.21788
- **Reference count:** 40
- **Primary result:** Achieves state-of-the-art performance on multi-conditional image generation benchmarks with improved compositional control and user intent fidelity

## Executive Summary
This paper tackles catastrophic forgetting and task interference in parameter-efficient fine-tuning of diffusion transformers for multi-conditional image generation. InstructMoLE replaces token-level expert routing with instruction-guided global routing to maintain semantic consistency across tokens, and adds an output-space orthogonality loss to enforce expert diversity and prevent collapse. Experiments show that InstructMoLE achieves state-of-the-art performance on multiple challenging benchmarks, including OmniContext (ID-Sim: 38.85 vs 36.29 for baseline), XVerseBench (71.07 average score vs 67.46), and GEdit-EN-full (6.17 average score), with robust generalization to unseen tasks and strong resilience in complex instruction scenarios.

## Method Summary
InstructMoLE fine-tunes Flux.1 Kontext using a Mixture of Low-rank Experts (MoLE) architecture with 8 experts, 4 active per layer, and rank 32. The key innovation is instruction-guided global routing (IGR), which uses a Perceiver attention network to distill instruction embeddings (from T5 and CLIP) into a global token that determines expert selection for all image tokens, preserving semantic consistency. An orthogonality loss on expert outputs prevents collapse by maximizing diversity. The model is trained on a mixed corpus of reference-based tasks, multi-subject composition, editing, spatial control, and inpainting for 100K steps on 64×H100 GPUs.

## Key Results
- Achieves ID-Sim of 38.85 on OmniContext benchmark vs 36.29 for baseline
- Scores 71.07 average on XVerseBench across DPG, ID-Sim, and IP-Sim metrics
- Reaches 6.17 average score on GEdit-EN-full benchmark
- Demonstrates robust generalization to unseen tasks and strong resilience in complex instruction scenarios

## Why This Works (Mechanism)
InstructMoLE addresses task interference in multi-conditional generation by replacing token-level expert routing with instruction-guided global routing. This ensures semantic consistency across all tokens while maintaining task specialization. The orthogonality loss on expert outputs prevents collapse by enforcing diversity, allowing the model to maintain distinct capabilities for different tasks without interference. The global routing policy based on instruction semantics enables better compositional control and fidelity to user intent.

## Foundational Learning
- **Diffusion Transformers:** Why needed: Core architecture for image generation; quick check: Verify model outputs follow diffusion sampling process
- **Mixture of Low-rank Experts:** Why needed: Parameter-efficient specialization for multiple tasks; quick check: Confirm k=4 experts selected per layer during inference
- **Orthogonality Regularization:** Why needed: Prevents expert collapse and maintains task diversity; quick check: Monitor pairwise cosine similarities of expert outputs
- **Instruction-guided Routing:** Why needed: Maintains semantic consistency across tokens; quick check: Verify logits shape is B×1×N (not B×L×N)
- **Load Balancing Loss:** Why needed: Ensures all experts are utilized; quick check: Track expert usage distribution during training
- **Perceiver Attention:** Why needed: Efficiently distills instruction features into global routing signal; quick check: Verify Zglobal dimensionality matches router input requirements

## Architecture Onboarding
**Component Map:** T5+CLIP features -> Perceiver Attention -> Zglobal -> Router Gl -> Expert Selection -> MoLE layers -> Image Generation
**Critical Path:** Instruction encoding (T5+CLIP) → Perceiver distillation → Global routing → Expert selection → MoLE application → Diffusion generation
**Design Tradeoffs:** Global routing preserves semantics but reduces per-token flexibility; orthogonality loss adds computational overhead but prevents collapse; instruction-guided approach requires additional embedding computation
**Failure Signatures:** Expert collapse (uniform outputs), routing inconsistency (uniform selection), poor instruction adherence (Zglobal lacks discriminability)
**First 3 Experiments:**
1. Verify expert output shapes match original layer dimensions after MoLE insertion
2. Test IGR module by checking logits shape is B×1×N and visualizing expert selection per task
3. Implement and monitor orthogonality loss by computing pairwise squared cosine similarities of expert outputs

## Open Questions the Paper Calls Out
None

## Limitations
- Critical hyperparameters (loss weights λ_aux, λ_ortho, router architecture details) are underspecified, requiring extensive tuning for reproduction
- Unclear which specific Flux.1 Kontext layers receive MoLE replacements, affecting parameter efficiency and performance
- Exact CLIP/T5 model variants and projection dimensions not stated, impacting instruction encoding quality

## Confidence
- **High Confidence:** Core innovation of instruction-guided global routing versus token-level routing is clearly described and logically sound
- **Medium Confidence:** General MoLE architecture and training setup are specified, but missing exact loss weights and router details
- **Low Confidence:** Performance claims difficult to verify independently due to missing critical hyperparameters and architectural specifics

## Next Checks
1. Conduct ablation studies to determine optimal λ_aux and λ_ortho values through sensitivity analysis on expert diversity metrics
2. Implement and test different Gl router architectures to identify configuration that best preserves instruction semantics in Zglobal
3. Experiment with applying MoLE to different subsets of Flux.1 Kontext layers to optimize multi-conditional generation performance