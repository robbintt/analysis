---
ver: rpa2
title: Red-Teaming Text-to-Image Systems by Rule-based Preference Modeling
arxiv_id: '2505.21074'
source_url: https://arxiv.org/abs/2505.21074
tags:
- rpg-rt
- images
- nsfw
- prompt
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces RPG-RT, a novel red-teaming framework for\
  \ text-to-image systems that iteratively modifies prompts using a large language\
  \ model and fine-tunes it based on system feedback. The approach uses rule-based\
  \ preference modeling to guide the LLM\u2019s exploration of unknown defense mechanisms\
  \ in commercial black-box systems."
---

# Red-Teaming Text-to-Image Systems by Rule-based Preference Modeling

## Quick Facts
- **arXiv ID**: 2505.21074
- **Source URL**: https://arxiv.org/abs/2505.21074
- **Reference count**: 40
- **Primary result**: RPG-RT achieves 80.98% ASR vs 34.56% baseline across 19 T2I systems

## Executive Summary
This paper introduces RPG-RT, a novel red-teaming framework for text-to-image systems that iteratively modifies prompts using a large language model and fine-tunes it based on system feedback. The approach uses rule-based preference modeling to guide the LLM's exploration of unknown defense mechanisms in commercial black-box systems. RPG-RT achieves significantly higher attack success rates (e.g., 80.98% ASR vs. 34.56% for the best baseline) across nineteen T2I systems with varied safety mechanisms, including three online commercial APIs, while maintaining competitive semantic similarity. It also generalizes effectively to unseen prompts and text-to-video models.

## Method Summary
RPG-RT employs a three-stage iterative framework: (1) an LLM generates N=30 prompt modifications per original prompt, (2) a rule-based preference model evaluates system feedback to construct preference pairs, and (3) the LLM is fine-tuned using Direct Preference Optimization (DPO) with LoRA. The method uses a custom scoring model that decouples CLIP embeddings into harmful-content and innocuous-semantic channels, trained with four losses (Lharm, Linno, Lsim, Lrec). The framework operates under a black-box threat model with only query-response access to target systems.

## Key Results
- RPG-RT achieves 80.98% ASR on average across 19 T2I systems, significantly outperforming baselines (best baseline: 34.56% ASR)
- Maintains strong transferability to unseen prompts (37.94% ASR on text-img, 96.77% on SD v3)
- Successfully attacks commercial APIs (ChatGPT, Gemini, Midjourney) with 66.67%, 71.77%, and 80.65% ASR respectively
- Balances attack success with semantic similarity, showing competitive FID scores compared to baselines

## Why This Works (Mechanism)

### Mechanism 1: Rule-based Preference Modeling
Constructing a binary partial order over prompt modifications enables fine-grained LLM guidance even with coarse black-box feedback. Three feedback types (TYPE-1 rejection, TYPE-2 SFW, TYPE-3 NSFW) are classified, with preference rules establishing TYPE-3 > TYPE-2 and TYPE-3 > TYPE-1. A scoring function combines harmfulness intensity and semantic similarity for finer ranking. If all queries return TYPE-1 or TYPE-2, manually generated SFW-NSFW pairs serve as fallback.

### Mechanism 2: Decoupled Scoring Model
Decomposing CLIP embeddings into separate harmful-content and innocuous-semantic channels improves preference ranking accuracy. A learnable transformation f = (fn, fs) splits CLIP embeddings, trained with four loss functions: Lharm (harmfulness ranking), Linno (benign semantic invariance), Lsim (semantic alignment), and Lrec (reconstruction fidelity). The method assumes harmful semantics can be linearly separated from innocuous semantics in CLIP space.

### Mechanism 3: Iterative LLM Adaptation via DPO
Fine-tuning an LLM with Direct Preference Optimization on system-feedback-derived preference pairs enables generalization to unseen prompts. Each iteration generates 30 modifications per prompt, queries the black-box, constructs preference pairs via rules, then fine-tunes Vicuna-7B with DPO+LoRA (rank=64). After ~10 rounds, the LLM can modify new prompts in a single forward pass. The method assumes target defense behavior remains stable during the attack window.

## Foundational Learning

- **Direct Preference Optimization (DPO)**: Core fine-tuning method that avoids explicit reward model training; preference pairs directly optimize the LLM policy. Why needed: Enables efficient adaptation without modeling the target system's reward function. Quick check: Can you explain why DPO uses a binary partial order rather than scalar rewards for this application?

- **CLIP Embedding Space**: The scoring model operates on CLIP image embeddings; understanding their semantic properties is essential for the decoupling transformation. Why needed: Provides the semantic representation space for the scoring model. Quick check: What happens if two images with different harmful content levels have high CLIP similarity—how does fn vs fs address this?

- **Black-Box Threat Model**: The entire framework assumes only query-response access (no gradients, no model internals). Design decisions stem from this constraint. Why needed: Defines the operational constraints and attack surface. Quick check: Why does the framework avoid gradient-based optimization despite potentially higher efficiency?

## Architecture Onboarding

- **Component map**: LLM Agent (Vicuna-7B) -> Target T2I System (black-box) -> Detector D (NSFW classifier) -> Scoring Model (CLIP + f=(fn,fs)) -> Preference Constructor (rules) -> DPO Trainer (LoRA) -> Reference Generator (SD v1.4)

- **Critical path**: 1. Initialize LLM with prompt template, 2. Query loop: Generate modifications → Query target → Detect NSFW → Score, 3. Build preference pairs from rules, 4. Train scoring model, 5. DPO fine-tune LLM, 6. Repeat for ~10 iterations

- **Design tradeoffs**: N=30 modifications/prompt balances coverage vs API costs; weight c=2.0 balances ASR vs semantic preservation (c=1.0 → 77.72% ASR but worse FID); unaligned Vicuna-7B required as safety-aligned LLMs may refuse NSFW modifications

- **Failure signatures**: All queries rejected (TYPE-1 only): Fallback to manual SFW-NSFW pairs; No TYPE-3 after many iterations: Defense may be too strong; High ASR but poor FID/CS: Reduce weight c or check Linno loss convergence; Scoring model F1 <0.85: Insufficient training pairs

- **First 3 experiments**: 1. Sanity check on text-img defense with 2-3 iterations and N=10, 2. Scoring model validation with F1>0.90 on held-out test set, 3. Ablation on weight c∈{1.0, 2.0, 3.0} to establish ASR-FID tradeoff curve

## Open Questions the Paper Calls Out

### Open Question 1
Can the computational efficiency of RPG-RT's training phase be significantly improved (e.g., via LLM distillation, few-shot DPO, or offline pre-training on proxy systems) without compromising its attack success rate or transferability? The method's training process is computationally intensive (Table 13: high GPU hours, long runtime), yet it is noted to be efficient at inference for new prompts. This suggests a trade-off between training cost and operational effectiveness that was not optimized.

### Open Question 2
How dependent is the scoring model's effectiveness on the specific NSFW category or target T2I system? Can a single, universally-trained scoring model perform well across all categories (nudity, violence, etc.) and defense types? The scoring model is trained on query data from the target system, and the paper uses different detectors for different NSFW categories. It is unclear if the learned `f_n` (harmful component) is genuinely category-agnostic or implicitly overfits to the semantics of the training data's dominant category.

### Open Question 3
What architectural or training-level defenses can be developed specifically to counter the adaptive, feedback-driven nature of attacks like RPG-RT? The authors state in the Impact Statement: "it is imperative to develop more robust and secure T2I models against our attack, which we leave to future work." The paper's scope is strictly offensive (red-teaming); it does not propose or evaluate any defensive countermeasures beyond showing the vulnerability of existing systems.

## Limitations

- **Temporal Stability**: Rule-based preference modeling assumes consistent defense mechanisms across query iterations, but this may break when target systems update their filters during the attack window
- **Structural Assumption**: The decoupled scoring model's effectiveness depends on CLIP embeddings having linearly separable harmful and innocuous components—this structural assumption lacks empirical validation
- **Computational Cost**: The training process is computationally intensive (9.9h training + 13.5h DPO), creating a temporal vulnerability window where defenses could change

## Confidence

- **High confidence**: ASR improvements (80.98% vs 34.56% baseline) and transfer results (37.94% on text-img, 96.77% on SD v3) are well-documented with multiple baselines and metrics
- **Medium confidence**: The rule-based preference framework's generalization to unseen prompts and text-to-video models, as transfer experiments show strong performance but limited prompt diversity (95 prompts total)
- **Medium confidence**: The decoupled scoring model's claimed superiority over single-vector approaches, as the paper provides ablation studies but doesn't compare against alternative multi-channel architectures

## Next Checks

1. **Temporal Stability Test**: Run RPG-RT against a target system with known periodic defense updates (e.g., hourly API changes) to measure degradation in ASR over time and validate the assumption of stable defense mechanisms

2. **CLIP Decoupling Validation**: Train an alternative scoring model using contrastive learning without explicit decoupling (fn, fs) and compare ASR/FID tradeoffs to test whether the structural assumption is necessary or if simpler architectures suffice

3. **Resource Efficiency Benchmark**: Measure RPG-RT's ASR vs. baseline methods while varying computational budgets (N modifications, training iterations) to establish the precise cost-accuracy tradeoff and identify diminishing returns points