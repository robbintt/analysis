---
ver: rpa2
title: 'Efficient Data Valuation Approximation in Federated Learning: A Sampling-based
  Approach'
arxiv_id: '2504.16668'
source_url: https://arxiv.org/abs/2504.16668
tags:
- data
- dataset
- combinations
- value
- clients
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of data valuation in federated
  learning (FL), where the goal is to fairly assess the contribution of each data
  provider to a collaboratively trained model. The authors focus on improving the
  efficiency and accuracy of Shapley value (SV)-based data valuation, which is computationally
  prohibitive due to the exponential number of dataset combinations that need to be
  evaluated.
---

# Efficient Data Valuation Approximation in Federated Learning: A Sampling-based Approach

## Quick Facts
- **arXiv ID**: 2504.16668
- **Source URL**: https://arxiv.org/abs/2504.16668
- **Reference count**: 40
- **Primary result**: IPSS algorithm achieves up to 99% computation time reduction with low approximation error for Shapley value data valuation in FL.

## Executive Summary
This paper addresses the computational bottleneck in Shapley value (SV)-based data valuation for federated learning by proposing an efficient sampling-based approach. The authors develop a unified stratified sampling framework supporting both marginal-contribution-based (MC-SV) and complementary-contribution-based (CC-SV) computation schemes. By identifying that only a limited number of dataset combinations ("key combinations") significantly impact data values, they design the IPSS algorithm which strategically selects high-impact combinations rather than evaluating all possible subsets. Extensive experiments demonstrate that IPSS outperforms existing methods in both efficiency and effectiveness, achieving substantial computation time savings while maintaining low approximation error.

## Method Summary
The paper proposes IPSS (Importance-Prioritized Stratified Sampling), an algorithm for approximating Shapley values in federated learning settings. The method first establishes a unified stratified sampling framework that partitions the exponential combination space into strata based on subset size. Under the MC-SV scheme with model-accuracy utilities, the authors identify a "key combinations" phenomenon where only small subsets have significant impact on data values. Building on this insight, IPSS computes a threshold k* based on the sampling budget γ, exhaustively evaluates all combinations with |S| ≤ k*, and samples remaining combinations of size k*+1 with equal client representation. The algorithm leverages FedAvg for FL training and demonstrates strong theoretical error bounds under linear regression assumptions.

## Key Results
- IPSS achieves up to 99% reduction in computation time compared to exact Shapley value computation
- The algorithm maintains low approximation error (less than 1% relative error for key combinations) while evaluating exponentially fewer dataset combinations
- Empirical results on FEMNIST and Adult datasets show IPSS outperforms representative baselines including Extended-TMC, CC-Shapley, and GTG-Shapley in both efficiency and effectiveness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Treating dataset combinations of the same cardinality as strata enables unbiased, lower-variance estimation of Shapley values.
- Mechanism: The stratified-sampling framework partitions the exponential combination space into n strata by subset size k. Within each stratum, Monte Carlo sampling estimates marginal contributions, and the final value averages across strata. This exploits the inherent hierarchical structure of MC-SV and CC-SV definitions.
- Core assumption: The utility function and data distribution permit variance reduction via stratification; unbiasedness holds for both schemes under random sampling within strata (Theorem 1).
- Evidence anchors:
  - [abstract] "We first propose a unified stratified-sampling framework for two widely-used schemes."
  - [section III-A] "By definition, both the MC-SV and CC-SV possess an inherent hierarchical structure based on the size of dataset combinations... it is natural to treat dataset combinations of the same size as strata."
  - [corpus] Corpus weak; no direct corroboration or refutation of this specific stratified design for FL valuation.

### Mechanism 2
- Claim: Under the MC-SV scheme with model-accuracy utilities, only a small fraction of combinations (small subsets) dominate the estimated data values.
- Mechanism: Two effects reduce the impact of large subsets: (i) diminishing marginal utility as S grows, and (ii) smaller combinatorial coefficients 1/(n-1 choose |S|) near |S|≈(n-1)/2. Empirically, subsets with ≤2 clients yield <1% relative error on FEMNIST (Fig. 4), motivating selective evaluation.
- Core assumption: The utility function is monotonic increasing with additional (non-adversarial) data; FL models are not data-saturated at small subset sizes.
- Evidence anchors:
  - [abstract] "We... identify a phenomenon termed key combinations, where only limited dataset combinations have a high-impact on final data value."
  - [section IV-A] "The empirical results show that for dataset combinations of size K ≤ 2, the relative error is less than 1%... dataset combinations with a larger number of clients have less impact."
  - [corpus] Corpus weak; neighboring papers (e.g., Owen Sampling, Faithful Group Shapley) address related valuation problems but do not confirm this specific phenomenon for MC-SV in FL.

### Mechanism 3
- Claim: Prioritizing full evaluation of small subsets and sampling a controlled set of larger subsets achieves bounded approximation error with O(τγ) time.
- Mechanism: IPSS computes k* = max{k | Σ_{j=0}^k C(n,j) ≤ γ}, exhaustively evaluates all subsets of size ≤ k*, and allocates remaining budget to size k*+1 with equal per-client sampling frequency. Theoretically, error is O((n−k*)/(k*nt)) under linear regression (Theorem 3).
- Core assumption: Linear regression analysis approximates general neural-network behavior; the utility is negative MSE and data are drawn i.i.d. within each client.
- Evidence anchors:
  - [abstract] "IPSS... strategically selects high-impact dataset combinations rather than evaluating all possible combinations, thus substantially reducing time cost with minor approximation error."
  - [section IV-C] "Theorem 3... the approximation error bound of Alg. 3 is O((n−k*)/(k*nt))."
  - [corpus] Corpus indicates interest in efficient SV approximation (e.g., Owen Sampling, eigenvalue-based methods) but no direct corroboration of IPSS specifics.

## Foundational Learning

- **Concept: Shapley Value (SV)**
  - Why needed here: SV is the fairness metric being approximated; understanding marginal vs. complementary formulations is essential to follow the scheme comparison.
  - Quick check question: Explain why SV requires evaluating O(2^n) subsets and how marginal contribution differs from complementary contribution in the definitions.

- **Concept: Federated Learning (FL) – FedAvg**
  - Why needed here: Data valuation is performed on FL-trained models; the utility function depends on the FL training dynamics and data partitioning.
  - Quick check question: Describe the FedAvg training loop and how utility U(M_S) is evaluated on a held-out test set T for a subset S of clients.

- **Concept: Monte Carlo and Stratified Sampling Variance**
  - Why needed here: The paper's efficiency claims rest on variance reduction via stratification and the key-combinations phenomenon.
  - Quick check question: Given two unbiased estimators with different variances, which requires fewer samples to achieve a fixed error tolerance, and why?

## Architecture Onboarding

- **Component map**: Preprocessing (partition clients, define test set) -> Stratum selector (compute k*, enumerate subsets) -> FL evaluator (train/evaluate M_S) -> Value aggregator (compute marginal contributions) -> Output (data values)
- **Critical path**: The FL evaluator dominates runtime (τ per model); minimizing the number of distinct S with |S| > k* is the primary lever for efficiency.
- **Design tradeoffs**: Lower γ → faster but higher approximation error; theoretical bound depends on (n−k*)/(k*nt). Using CC-SV instead of MC-SV within the same framework yields higher variance under linear regression (Theorem 2) with no obvious accuracy gain. Exhaustive evaluation of small subsets improves stability but scales combinatorially with k*; for large n, budget allocation to size k*+1 is essential.
- **Failure signatures**: Non-monotonic utilities causing large subsets to have high marginal impact, violating key-combinations assumption → higher error than expected. Severe non-IID data where small subsets underfit → utility differences may not reflect true contributions until larger coalitions are considered. Very small per-client datasets (t ≈ |x|) → linear-regression error bound becomes weak; empirical variance may increase. Gradient-based methods (OR, λ-MR, GTG-Shapley) show high error on non-linear models (e.g., CNN on FEMNIST, Table IV) → use sampling-based IPSS instead.
- **First 3 experiments**:
  1. Reproduce the key-combinations phenomenon on a small FL benchmark (e.g., FEMNIST with 10 clients) by running K-Greedy for K=1,2,3 and plotting relative error vs. K; validate that error drops sharply by K=2.
  2. Compare MC-SV vs. CC-SV variance under the stratified framework (Alg. 1) with fixed γ on a linear-regression FL task; confirm lower variance for MC-SV as predicted by Theorem 2.
  3. Run IPSS on FEMNIST and Adult with γ = n log n for n ∈ {10, 20, 50}; measure time cost and relative error; verify that time scales sub-exponentially and error remains low, consistent with Table IV–V trends.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Do the theoretical error bounds and variance reductions derived under the FL linear regression assumption generalize to non-linear models like deep neural networks?
- **Basis in paper**: [inferred] Theorems 2 and 3 explicitly rely on "FL linear regression" assumptions for their proofs, yet the empirical validation uses CNNs and XGBoost.
- **Why unresolved**: The mathematical proofs utilize properties of MSE in linear models which may not hold for the non-convex loss landscapes of deep learning.
- **What evidence would resolve it**: Formal proofs extending Theorem 3 to non-convex functions or empirical bounds showing the error rate holds across diverse non-linear architectures.

### Open Question 2
- **Question**: Does the "key combinations" phenomenon persist in scenarios with adversarial data or strong non-IID distributions where larger subsets might provide non-monotonic utility gains?
- **Basis in paper**: [inferred] The IPSS algorithm prunes larger combinations based on the observation that marginal utility typically decreases as more clients join.
- **Why unresolved**: If data is adversarial or highly complementary, larger subsets might unexpectedly trigger significant utility jumps (non-monotonicity), rendering the "pruning" of large combinations suboptimal.
- **What evidence would resolve it**: Evaluations on datasets specifically constructed to reward larger coalition sizes (super-linear utility) to test if IPSS misses high-impact large combinations.

### Open Question 3
- **Question**: Can the IPSS framework be effectively adapted for cross-device FL settings involving millions of clients?
- **Basis in paper**: [explicit] The authors state they "focus on the cross-silo FL setting" and limit experiments to a maximum of 100 clients.
- **Why unresolved**: The algorithm's efficiency depends on pruning combinations based on size ($k^*$), but the search space and communication rounds may become prohibitive as $n$ scales to millions of devices.
- **What evidence would resolve it**: A scalability analysis demonstrating the relationship between client count $n$ and the required sampling rounds $\gamma$ in a cross-device simulation.

## Limitations

- The theoretical analysis relies on FL linear regression assumptions that may not hold for non-linear models like deep neural networks.
- The "key combinations" phenomenon may not persist under severe non-IID data distributions or adversarial scenarios where larger subsets provide disproportionate utility gains.
- The algorithm focuses on cross-silo FL settings with up to 100 clients, and scalability to cross-device settings with millions of clients remains unexplored.

## Confidence

- **High confidence**: The stratified sampling framework provides unbiased SV estimates and the MC-SV scheme has lower variance than CC-SV under linear regression (Theorems 1-2).
- **Medium confidence**: The key-combinations phenomenon holds for the tested datasets and utility functions, but may not generalize to all FL scenarios.
- **Medium confidence**: IPSS achieves substantial efficiency gains (up to 99% time reduction) while maintaining low approximation error on tested datasets.

## Next Checks

1. Test IPSS under severe non-IID data partitioning (e.g., Dirichlet distribution with α→0) to assess robustness when small subsets cannot capture client contributions.
2. Implement and validate the linear regression error bound (Theorem 3) by comparing theoretical predictions with empirical approximation error across different per-client dataset sizes t.
3. Compare IPSS against gradient-based valuation methods (OR, λ-MR) on a non-linear FL task with known ground truth to quantify accuracy differences in realistic scenarios.