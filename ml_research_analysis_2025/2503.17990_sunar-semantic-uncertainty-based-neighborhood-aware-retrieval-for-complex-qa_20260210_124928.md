---
ver: rpa2
title: 'SUNAR: Semantic Uncertainty based Neighborhood Aware Retrieval for Complex
  QA'
arxiv_id: '2503.17990'
source_url: https://arxiv.org/abs/2503.17990
tags:
- answer
- sunar
- retrieval
- question
- documents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SUNAR addresses the recall problem in complex question answering
  by proposing a neighborhood aware retrieval method that uses semantic uncertainty
  estimates from LLM-generated answers to dynamically promote or penalize documents.
  The approach constructs a neighborhood graph based on document similarities and
  iteratively explores it, leveraging interim LLM feedback to rescore documents based
  on answer consistency.
---

# SUNAR: Semantic Uncertainty based Neighborhood Aware Retrieval for Complex QA

## Quick Facts
- **arXiv ID:** 2503.17990
- **Source URL:** https://arxiv.org/abs/2503.17990
- **Reference count:** 16
- **Primary result:** Achieves up to 31.84% improvement over state-of-the-art methods for complex question answering

## Executive Summary
SUNAR addresses the recall problem in complex question answering by proposing a neighborhood aware retrieval method that uses semantic uncertainty estimates from LLM-generated answers to dynamically promote or penalize documents. The approach constructs a neighborhood graph based on document similarities and iteratively explores it, leveraging interim LLM feedback to rescore documents based on answer consistency. Experiments on two complex QA datasets show SUNAR achieves significant improvements in both retrieval and downstream reasoning performance.

## Method Summary
SUNAR combines three core innovations: Neighborhood Aware Retrieval (NAR) that explores document similarity graphs beyond initial retrieval results, Answer Semantic Uncertainty (ASU) that uses bi-directional entailment clustering to detect inconsistent LLM responses and penalize uncertain documents, and Meta Evidence Reasoner (MER) that performs post-hoc reasoning over all sub-question evidence to correct cascading errors. The method requires first-stage retrieval (SPLADEv2), offline construction of a document neighborhood graph (ColBERTv2), iterative exploration with cross-encoder re-ranking, and LLM-based uncertainty estimation through multiple answer generations per document batch.

## Key Results
- Achieves up to 31.84% improvement over state-of-the-art methods on complex QA datasets
- SUNAR_R (NAR only) shows 17.27% improvement over existing NAR methods
- ASU and MER components further improve performance by 5.31% and 2.32% respectively
- Generalizes across different LLMs (gpt-3.5-turbo, gpt-4o-mini, Llama 3.1 8B, Mistral v0.2 7B)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Exploring document neighborhoods improves recall beyond first-stage retrieval
- Mechanism: NAR constructs an offline document graph using dense retrieval (ColBERTv2), then during inference alternates between initially retrieved documents and their neighbors. This surfaces documents that are semantically similar to relevant documents but may not directly match the query.
- Core assumption: The clustering hypothesis—documents near relevant documents tend to answer the same query.
- Evidence anchors:
  - [abstract] "iteratively explores a neighborhood graph of documents"
  - [Section 3.1.2] "NAR then explores a batch of documents from neighbor pool N instead of the next batch from R"
  - [corpus] Related work on adaptive retrieval (MacAvaney et al., 2022; Rathee et al., 2024) validates graph-based exploration but without LLM feedback
- Break condition: If documents form disconnected semantic clusters unrelated to query topics, neighbor exploration will surface irrelevant documents and increase noise.

### Mechanism 2
- Claim: Answer Semantic Uncertainty (ASU) identifies distractor documents
- Mechanism: For each document batch, generate m LLM answers, cluster them into semantic sets via bidirectional entailment, then penalize documents proportionally to the number of semantic sets (s). Fewer semantic clusters = more consistent answers = higher document relevance.
- Core assumption: Documents that produce inconsistent LLM answers across multiple generations contain insufficient or misleading evidence.
- Evidence anchors:
  - [abstract] "dynamically promoting or penalizing documents based on uncertainty estimates from interim LLM-generated answer candidates"
  - [Section 3.2] "more semantic sets indicate that the LLM is uncertain about the answer"
  - [corpus] Kuhn et al. (2023) introduced semantic uncertainty for hallucination detection; SUNAR applies this to retrieval scoring
- Break condition: If the LLM confidently produces consistently wrong answers (systematic hallucination), ASU will incorrectly promote harmful documents.

### Mechanism 3
- Claim: Meta Evidence Reasoner (MER) reduces cascading errors from sequential reasoning
- Mechanism: Instead of deriving final answers solely from the last sub-question's evidence, MER prompts the LLM with all reasoning paths and top-l evidence across all sub-questions to generate a corrected final answer.
- Core assumption: Errors in intermediate reasoning steps can be recovered by re-examining all evidence holistically.
- Evidence anchors:
  - [Section 3.2] "the final answer is derived solely based on answers to previous sub-questions and only the evidence for the last sub-question"
  - [Table 4] Ablation shows SUNAR(-MER) drops from 32.75 to 30.43 on MQA
  - [corpus] Limited direct corpus validation; this is a novel contribution of the paper
- Break condition: If distractors dominate the evidence pool across all sub-questions, MER cannot recover and may amplify errors.

## Foundational Learning

- Concept: **Clustering Hypothesis in IR**
  - Why needed here: NAR's entire design assumes nearby documents in embedding space share query relevance.
  - Quick check question: Can you explain why document-document similarity might capture relevance that query-document similarity misses?

- Concept: **Semantic Uncertainty via Entailment**
  - Why needed here: ASU uses bidirectional entailment (MNLI) to cluster answers; understanding this is essential for debugging the re-scoring pipeline.
  - Quick check question: Why is bidirectional entailment a stronger criterion for semantic equivalence than unidirectional?

- Concept: **Bounded-Recall Problem in Multi-hop QA**
  - Why needed here: The paper frames its contribution around this limitation; understanding it clarifies why standard RAG fails on complex QA.
  - Quick check question: In a multi-hop question, what happens when first-stage retrieval misses a document needed for an intermediate sub-question?

## Architecture Onboarding

- Component map: First-stage retriever (SPLADEv2) -> Neighborhood graph (ColBERTv2) -> Cross-encoder re-ranker (mMiniLMv2) -> ASU module -> NAR iteration -> Top-l selection -> MER -> Final answer

- Critical path: Query → Decomposition → Initial Retrieval → NAR iteration (retrieve batch → re-rank → ASU penalty → add neighbors) → Top-l selection → MER → Final answer

- Design tradeoffs:
  - ASU requires m LLM calls per batch; paper uses m=5–10 implicitly—increases latency but improves distractor detection
  - NAR budget c trades recall vs. computation; paper uses c=100 with 10 neighbors per document
  - Neighborhood graph built with different retriever than first-stage (dense vs. sparse) captures complementary signals

- Failure signatures:
  - High semantic set count (s) across all batches suggests query is ambiguous or corpus lacks relevant documents
  - MER produces different answer than sequential reasoning → check intermediate sub-questions for errors
  - NAR adds many neighbors but final recall doesn't improve → neighbor graph quality issue

- First 3 experiments:
  1. **Reproduce SUNAR_R (NAR only)** on a subset of MQA to validate neighborhood graph construction before adding ASU complexity
  2. **Ablate ASU penalty strength**: Test 1/s vs. 1/√s vs. fixed penalty to understand sensitivity to uncertainty scaling
  3. **Cross-retriever validation**: Build neighborhood graph with a different dense encoder (e.g., ANCE instead of ColBERTv2) to test robustness of the clustering hypothesis assumption

## Open Questions the Paper Calls Out

- **Question:** Can a principled early stopping criterion be developed for the Neighborhood Aware Retrieval (NAR) algorithm to improve efficiency?
  - **Basis in paper:** [explicit] The authors state in the Limitations section that a "more principled selection of candidates for ranking in line 18 of Algorithm 1 with early stopping criterion could make it more efficient."
  - **Why unresolved:** The current algorithm iterates until a fixed re-ranking budget is exhausted, potentially processing unnecessary candidates.
  - **What evidence would resolve it:** A dynamic stopping mechanism that terminates the search when retrieval gains diminish, demonstrated to lower latency without degrading QA performance.

- **Question:** Can conformal prediction approaches outperform semantic set clustering for calibrating LLM confidence in document re-scoring?
  - **Basis in paper:** [explicit] The paper notes that "More principled conformal prediction based approaches could be adopted to calibrate LLM confidence" as opposed to the current semantic set counting.
  - **Why unresolved:** The current uncertainty estimation relies on heuristic clustering of answer equivalence, which may lack statistical robustness.
  - **What evidence would resolve it:** Experiments replacing the Answer Semantic Uncertainty (ASU) module with conformal prediction metrics, showing tighter confidence intervals or higher precision in identifying hallucinated contexts.

- **Question:** How does the computational latency and token cost of SUNAR scale with the complexity of the question or corpus size?
  - **Basis in paper:** [inferred] The method requires constructing a neighborhood graph offline and, during inference, iteratively calling an LLM to generate multiple answers (m) per batch to calculate uncertainty.
  - **Why unresolved:** While the paper demonstrates accuracy gains, it does not provide a detailed analysis of the increased inference time or financial cost compared to single-pass retrieval baselines.
  - **What evidence would resolve it:** A comparative analysis of end-to-end latency and API token usage between SUNAR and standard retrieve-and-read pipelines on varying query complexities.

## Limitations
- Requires multiple LLM calls per batch (m=5-10), increasing latency and cost
- Performance depends on quality of document neighborhood graph construction
- May not recover when distractors dominate evidence across all sub-questions

## Confidence
- **High:** Core NAR algorithm design and implementation details
- **Medium:** ASU module effectiveness and specific hyperparameter values (m, c, b)
- **Low:** MER component's general applicability beyond the tested datasets

## Next Checks
1. Verify neighborhood graph construction by checking average neighbor relevance scores on a held-out validation set
2. Test ASU sensitivity by varying the number of answer generations (m=3,5,10) and measuring impact on final QA accuracy
3. Evaluate MER effectiveness by comparing final answers with and without post-hoc correction on a subset of questions where intermediate reasoning is known to fail