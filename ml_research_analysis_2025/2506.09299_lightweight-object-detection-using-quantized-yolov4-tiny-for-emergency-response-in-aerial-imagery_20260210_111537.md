---
ver: rpa2
title: Lightweight Object Detection Using Quantized YOLOv4-Tiny for Emergency Response
  in Aerial Imagery
arxiv_id: '2506.09299'
source_url: https://arxiv.org/abs/2506.09299
tags:
- detection
- yolov4-tiny
- emergency
- aerial
- object
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of real-time object detection
  for emergency response using aerial imagery captured by drones. The authors propose
  a lightweight solution based on YOLOv4-Tiny optimized through post-training quantization
  to INT8 precision.
---

# Lightweight Object Detection Using Quantized YOLOv4-Tiny for Emergency Response in Aerial Imagery

## Quick Facts
- **arXiv ID:** 2506.09299
- **Source URL:** https://arxiv.org/abs/2506.09299
- **Authors:** Sindhu Boddu; Arindam Mukherjee
- **Reference count:** 9
- **Primary Result:** YOLOv4-Tiny quantized to INT8 achieves 85.6% mAP, 71% smaller model, and 59% less power consumption for emergency aerial detection.

## Executive Summary
This paper presents a lightweight object detection solution for emergency response using aerial imagery captured by drones. The authors develop a custom dataset of 10,820 annotated aerial images across seven emergency-related classes and propose a YOLOv4-Tiny model optimized through post-training quantization to INT8 precision. The quantized model achieves comparable detection performance (85.6% mAP) while significantly reducing model size from 22.5 MB to 6.4 MB and improving inference speed by 44%. The solution demonstrates 71% model size reduction and 59% power consumption reduction, making it highly suitable for real-time emergency detection on low-power edge devices such as the Raspberry Pi 5.

## Method Summary
The study addresses the challenge of real-time object detection for emergency response in aerial imagery. A custom dataset of 10,820 annotated aerial images was curated covering seven emergency-related classes. YOLOv4-Tiny and YOLOv5-small models were trained from scratch on a GTX 1080 Ti with batch size 16, 100 epochs, learning rate 0.001 with cosine decay, and SGD with momentum. Post-training INT8 quantization was applied using ONNX Runtime with a 100-image calibration set. The models were evaluated on mAP@0.5, precision, recall, F1-score, inference time, FPS, model size, and power consumption metrics.

## Key Results
- YOLOv4-Tiny quantized to INT8 achieves 85.6% mAP@0.5 on emergency aerial imagery
- Model size reduced from 22.5 MB to 6.4 MB (71% reduction)
- Inference speed improved by 44% with 59% reduction in power consumption
- Demonstrated effective real-time performance on Raspberry Pi 5 for emergency response applications

## Why This Works (Mechanism)
The approach works by leveraging YOLOv4-Tiny's lightweight architecture combined with INT8 quantization to achieve significant computational efficiency without substantial accuracy loss. The custom dataset curation ensures the model is trained on relevant emergency scenarios, while quantization enables deployment on resource-constrained edge devices. The 416×416 resolution provides adequate detail for aerial object detection while maintaining computational feasibility.

## Foundational Learning
- **YOLOv4-Tiny architecture**: Why needed - Provides lightweight backbone for real-time inference; Quick check - Verify layer count and parameter reduction vs full YOLOv4
- **Post-training quantization**: Why needed - Enables deployment on edge devices with limited compute; Quick check - Measure accuracy drop after INT8 conversion
- **Aerial object detection challenges**: Why needed - Different perspective and scale requirements vs ground-level detection; Quick check - Compare AP for small vs large objects
- **Class imbalance handling**: Why needed - Emergency classes like "Car on Fire" have fewer instances; Quick check - Analyze per-class AP distribution
- **ONNX Runtime optimization**: Why needed - Provides efficient inference across hardware platforms; Quick check - Compare performance with different execution providers
- **Calibration set selection**: Why needed - Critical for accurate quantization without significant accuracy loss; Quick check - Test different calibration set sizes

## Architecture Onboarding

**Component Map:** Custom Dataset (10,820 images) -> YOLOv4-Tiny Training -> ONNX Export -> INT8 Quantization -> Raspberry Pi 5 Inference

**Critical Path:** Data preprocessing → Model training → Quantization → Edge deployment → Performance evaluation

**Design Tradeoffs:** The study prioritizes model size and power efficiency over marginal accuracy gains, accepting 85.6% mAP to achieve 71% model size reduction and 59% power savings suitable for emergency response scenarios.

**Failure Signatures:** 
- Class imbalance causes poor detection of rare emergency classes
- Quantization artifacts lead to false positives/negatives in critical detection
- Hardware-specific optimization may not generalize across edge devices

**First Experiments:**
1. Train baseline YOLOv4-Tiny on custom dataset and evaluate mAP@0.5
2. Apply INT8 quantization and measure accuracy degradation
3. Benchmark inference time and power consumption on Raspberry Pi 5

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent can re-weighted loss functions or advanced data augmentation strategies recover detection accuracy for underrepresented emergency classes (e.g., "Car on Fire")?
- Basis in paper: [explicit] The authors state in Section V.C that "Class imbalance in emergency datasets (e.g., fewer instances of 'Car on Fire') can be addressed using advanced data augmentation or re-weighted loss functions to improve minority class detection performance."
- Why unresolved: The current study acknowledges class imbalance in the custom dataset but relies on standard training configurations without implementing specific loss re-weighting or targeted augmentation for minority classes.
- What evidence would resolve it: A comparative ablation study showing per-class Average Precision (AP) improvements for minority classes when using Focal Loss or specialized augmentation techniques compared to the baseline model.

### Open Question 2
- Question: Can the trained emergency detection model generalize to non-emergency aerial domains (such as traffic monitoring or construction sites) through domain adaptation?
- Basis in paper: [explicit] Section V.C suggests "Leveraging the trained models on non-emergency aerial datasets—such as traffic monitoring, construction sites, or disaster zones—can demonstrate the generalizability of the selected model through domain adaptation."
- Why unresolved: The model was trained exclusively on a specific custom dataset of emergency imagery; its transferability to datasets with different object distributions or environmental contexts has not been evaluated.
- What evidence would resolve it: Performance metrics (mAP, inference time) of the fine-tuned model on standard external aerial datasets (e.g., VisDrone) compared to its performance on the custom emergency dataset.

### Open Question 3
- Question: How does the quantized model perform under challenging environmental conditions such as smoke, fog, or low-light scenarios common in disaster zones?
- Basis in paper: [explicit] The Conclusion notes that "improving detection robustness under challenging environmental conditions will be critical for advancing the reliability of aerial AI systems in emergency scenarios."
- Why unresolved: The paper evaluates the model on a test set derived from the curated dataset but does not explicitly report results on noisy or degraded imagery representing adverse weather or lighting.
- What evidence would resolve it: Benchmarking the quantized YOLOv4-Tiny model against a specifically curated "adverse conditions" test set containing smoke, fog, or night-time images to measure robustness degradation.

## Limitations
- Custom dataset (10,820 images) is not publicly available, preventing exact reproduction
- Missing implementation details including exact SGD momentum value and augmentation parameter ranges
- No evaluation of model robustness under adverse environmental conditions like smoke, fog, or low-light

## Confidence

**High Confidence Claims:**
- Post-training quantization methodology to INT8 using ONNX Runtime is standard and well-documented
- Architectural approach (YOLOv4-Tiny with quantization) is technically sound for lightweight object detection
- Reported improvements in model size (71% reduction) and inference speed (44% improvement) are consistent with typical quantization benefits

**Medium Confidence Claims:**
- mAP@0.5 score of 85.6% is plausible given the dataset curation and training approach, but cannot be verified without access to the custom dataset
- Power consumption reduction (59%) on Raspberry Pi 5 is reasonable for quantized models but depends on specific implementation details

**Low Confidence Claims:**
- Direct performance comparison between YOLOv4-Tiny and YOLOv5-small without access to the exact dataset and training configuration
- Absolute detection performance metrics across all classes, particularly for rare classes like "Car on Fire"

## Next Checks

1. **Dataset Reconstruction:** Curate a public aerial dataset (e.g., VisDrone filtered for emergency-relevant classes) with 10K+ images matching the described class distribution and annotation format to approximate the original experimental conditions.

2. **Hyperparameter Verification:** Implement the training pipeline with assumed momentum=0.9 and test multiple augmentation parameter ranges to identify the configuration that achieves optimal mAP@0.5 performance.

3. **Quantization Validation:** Perform systematic evaluation of INT8 quantization effects by testing different calibration set sizes (50, 100, 200 images) and quantization schemes (per-tensor vs per-channel) to understand the trade-off between model size reduction and accuracy retention.