---
ver: rpa2
title: Re-Visiting Explainable AI Evaluation Metrics to Identify The Most Informative
  Features
arxiv_id: '2502.00088'
source_url: https://arxiv.org/abs/2502.00088
tags:
- feature
- accuracy
- most
- features
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies critical limitations of ROAR and Permutation
  Importance (PI) as evaluation metrics for explainable AI (XAI) methods, particularly
  in cases of multicollinearity and when multiple significant features exist. The
  authors demonstrate that these metrics can produce misleading results, as removing
  the top feature does not always lead to a sharp performance decline, and in some
  cases, model accuracy may even improve.
---

# Re-Visiting Explainable AI Evaluation Metrics to Identify The Most Informative Features

## Quick Facts
- arXiv ID: 2502.00088
- Source URL: https://arxiv.org/abs/2502.00088
- Reference count: 14
- Key outcome: Proposes Expected Accuracy Interval (EAI) metric to address limitations of ROAR and Permutation Importance when evaluating XAI methods, particularly in cases of multicollinearity and multiple significant features.

## Executive Summary
This paper identifies critical limitations of ROAR and Permutation Importance metrics for evaluating explainable AI (XAI) methods. The authors demonstrate that these metrics can produce misleading results when multicollinearity exists among features or when multiple significant features are present in the model. To address these issues, the paper proposes the Expected Accuracy Interval (EAI), a metric that predicts upper and lower bounds of model accuracy after removing the most significant feature. EAI is validated using real datasets (CDC Diabetes Health Indicators, Wine Quality) and simulated data, showing improved reliability in feature importance evaluation.

## Method Summary
The paper evaluates ROAR and Permutation Importance metrics using SHAP to extract feature scores from trained models. EAI is calculated by first determining the Feature Contribution Percentage (FCP) as the ratio of the most significant feature's score to the sum of all feature scores. The expected accuracy change is computed by multiplying the initial model accuracy by FCP, then generating upper and lower bounds by adding and subtracting this expected change from the initial accuracy. The method is validated through iterative feature removal and retraining on classification and regression tasks using logistic regression and linear regression models.

## Key Results
- ROAR and Permutation Importance metrics can produce misleading evaluations when multicollinearity exists among features
- Removing the top feature does not always lead to a sharp performance decline; in some cases, model accuracy may even improve
- EAI provides more reliable predictions of accuracy bounds, particularly in scenarios with collinear features
- The metric is validated on both real (CDC Diabetes, Wine Quality) and simulated datasets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** ROAR and Permutation Importance metrics can produce misleading evaluations when multicollinearity exists among features, as correlated backup features can compensate for removed top features.
- **Mechanism:** When an XAI method identifies Feature A as most significant but Features B and C are highly correlated with A, removing A leaves B and C to provide similar predictive information. The model retrained without A may show minimal performance degradation—or even improvement—if the remaining correlated features reduce noise or collinearity artifacts.
- **Core assumption:** The XAI method provides valid importance rankings that reflect true contribution to model predictions, independent of feature correlations.
- **Evidence anchors:**
  - [abstract] "The efficiency of both metrics is significantly affected by multicollinearity, number of significant features in the model and the accuracy of the model."
  - [Section 6, Discussion] "If there are more than one significant feature in the model, then even removing some of them might not lead to decline in the model performance. In addition, the results show that in some cases the model performance might increase instead of decreasing because the left features might work better."
  - [corpus] Corpus evidence is weak for this specific mechanism—neighbor papers focus on saliency map evaluation and fraud detection, not multicollinearity effects on ROAR/PI.
- **Break condition:** If features are truly independent (no multicollinearity) and there is a single dominant predictive feature, ROAR/PI should produce expected sharp decline.

### Mechanism 2
- **Claim:** The Expected Accuracy Interval (EAI) provides bounded predictions of accuracy change by combining current model accuracy with feature contribution percentage derived from SHAP scores.
- **Mechanism:** EAI calculates: (1) Feature Contribution Percentage (FCP) = Score of Most Significant Feature / Sum of All Feature Scores; (2) Expected Change = Initial Accuracy × FCP; (3) Upper Interval = Initial Accuracy + Expected Change; (4) Lower Interval = Initial Accuracy − Expected Change. The resulting interval [LI, UI] represents uncertainty bounds for post-removal accuracy.
- **Core assumption:** SHAP scores accurately represent each feature's proportional contribution to model predictions, and accuracy changes linearly with feature removal proportional to these scores.
- **Evidence anchors:**
  - [Section 3, Proposed metric] Equations 1-5 define FCP, ExpectedΔ, UI, LI, and EAI explicitly.
  - [abstract] "we propose expected accuracy interval (EAI), a metric to predict the upper and lower bounds of the accuracy of the model when ROAR or IP is implemented."
  - [corpus] No direct corpus evidence for EAI specifically—this appears to be a novel contribution.
- **Break condition:** If model accuracy is already at ceiling (100%) due to overfitting or perfect prediction capability, the interval calculation becomes unreliable as stated in limitations.

### Mechanism 3
- **Claim:** Interval width provides diagnostic information about the relative impact of remaining features after top feature removal.
- **Mechanism:** A wider EAI indicates that the new most significant feature (after removal) has greater differential impact on predictions. A narrower interval suggests the replacement feature has similar contribution magnitude to the removed feature, implying redundancy or comparable importance.
- **Core assumption:** Interval width correlates meaningfully with feature importance differential rather than merely reflecting calculation artifacts.
- **Evidence anchors:**
  - [Section 6, Discussion] "A wider interval might be interpreted as after removing the most significant features from the model, the new significant one has greater impact on the predicted outcome. On the contrary, the narrower the interval indicates that the new significant feature has a similar impact to the one removed on the outcome."
  - [abstract] "The proposed metric found to be very useful especially with collinear features."
  - [corpus] No corpus validation—this interpretive framework is proposed but not externally validated.
- **Break condition:** Interpretation assumes valid SHAP rankings; if SHAP misranks features due to its own independence assumptions, interval interpretation may be unreliable.

## Foundational Learning

- **Concept: Multicollinearity in Feature Attribution**
  - Why needed here: The paper's core argument depends on understanding why correlated features undermine standard XAI evaluation metrics. Without this foundation, the motivation for EAI is unclear.
  - Quick check question: If features A and B have correlation 0.95, and a model uses both, what happens to predictions when only A is removed?

- **Concept: SHAP (SHapley Additive exPlanations) Values**
  - Why needed here: EAI explicitly depends on SHAP scores to calculate feature contribution percentage. Understanding that SHAP assigns contribution scores to each feature per prediction is essential.
  - Quick check question: If a model has 3 features with SHAP scores [0.5, 0.3, 0.2], what is the contribution percentage of the top feature?

- **Concept: Proxy-Based XAI Evaluation**
  - Why needed here: ROAR and PI are proxy metrics that evaluate XAI quality without human intervention. Understanding this evaluation paradigm clarifies what problem EAI addresses.
  - Quick check question: Why might a proxy metric be preferred over human evaluation for XAI quality assessment?

## Architecture Onboarding

- **Component map:**
  Input Data → Trained Model → SHAP Explainer → Feature Scores
                              ↓
                    FCP Calculation (Eq. 1)
                              ↓
  Initial Model Accuracy ────────→ ExpectedΔ (Eq. 2)
                              ↓
                      ┌───────────────┴───────────────┐
                      ↓                               ↓
                Upper Interval (Eq. 3)      Lower Interval (Eq. 4)
                      └───────────────┬───────────────┘
                              ↓
                    EAI = [LI, UI] (Eq. 5)

- **Critical path:**
  1. Train model and record initial accuracy (F1 for classification, R² for regression)
  2. Apply SHAP to extract feature importance scores
  3. Identify most significant feature by SHAP score magnitude
  4. Compute FCP = SMSF / SSOAF
  5. Calculate ExpectedΔ = initial_accuracy × FCP
  6. Generate EAI bounds [initial_acc − ExpectedΔ, initial_acc + ExpectedΔ]
  7. Remove top feature, retrain, and compare actual accuracy to predicted interval

- **Design tradeoffs:**
  - SHAP dependency: EAI requires XAI methods that provide numerical scores per feature. Other attribution methods without scalar outputs are incompatible.
  - Linear assumption: The mechanism assumes accuracy change scales linearly with FCP; non-linear feature interactions may violate this.
  - Model quality sensitivity: Tables 1-3 show that when initial model accuracy is low (<0.55 F1), interval predictions become less reliable and actual accuracies may fall outside predicted bounds (red-highlighted cells).

- **Failure signatures:**
  - Actual accuracy falling outside EAI bounds suggests either: (a) SHAP scores misrepresent true contributions, (b) non-linear feature interactions dominate, or (c) model is underfitting/overfitting.
  - Accuracy increasing after feature removal (e.g., iteration 16→17 in Table 1) signals multicollinearity harm—the removed feature was adding noise rather than signal.
  - Very wide intervals (>0.3 range) indicate high uncertainty, often corresponding to low base accuracy models.

- **First 3 experiments:**
  1. Replicate Table 1 results using CDC Diabetes Health Indicators dataset with logistic regression: train model, compute SHAP scores, calculate EAI for top 5 features, remove each iteratively, and verify whether actual retrained accuracies fall within predicted intervals.
  2. Stress-test EAI under controlled multicollinearity: use sklearn.datasets.make_classification with varying `n_informative` and `n_redundant` parameters to create datasets with known collinearity levels; compare EAI prediction accuracy against ROAR's ability to detect true important features.
  3. Boundary condition test: train a model to near-perfect accuracy (>0.95) using an easily separable dataset; compute EAI and assess whether interval predictions remain meaningful or break down as the paper's limitations section warns.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Expected Accuracy Interval (EAI) metric perform when applied to complex, non-linear models (e.g., Deep Neural Networks) or high-dimensional unstructured data like images?
- Basis in paper: [inferred] The paper validates EAI using only linear and logistic regression on tabular datasets, but the introduction and discussion mention applications involving "pixels in an image" and "deep neural networks."
- Why unresolved: The empirical validation is restricted to regression models; the behavior of EAI in highly non-linear decision boundaries or convolutional networks is not tested.
- What evidence would resolve it: Empirical results from applying EAI to image classification tasks or deep learning tabular benchmarks.

### Open Question 2
- Question: How can the EAI calculation be refined to prevent actual model accuracy from falling outside the predicted interval bounds?
- Basis in paper: [inferred] The results in Tables 1, 2, and 3 highlight specific instances (marked in red) where the actual accuracy falls outside the Expected Accuracy Interval, indicating the metric's bounds are not always reliable.
- Why unresolved: The paper proposes the metric but does not offer a mechanism to handle or correct the "misses" where the prediction interval fails to capture the actual post-removal accuracy.
- What evidence would resolve it: A modified formula or probabilistic calibration that reduces the frequency of out-of-bounds errors in validation datasets.

### Open Question 3
- Question: Does the reliance on SHAP scores limit EAI's applicability, and can the metric be adapted for XAI methods that handle feature dependence differently?
- Basis in paper: [explicit] The authors state that the metric is limited to XAI methods that provide a contribution score and note that SHAP "considers the features are independent," which affects the contribution percentage calculation.
- Why unresolved: The proposed EAI depends on the "Feature Contribution Percentage" derived specifically from SHAP scores, potentially inheriting SHAP's independence assumptions and limiting use with other explanation types.
- What evidence would resolve it: A generalized formulation of EAI that functions with other attribution methods (e.g., LIME) or dependency-aware extensions of SHAP.

## Limitations
- SHAP aggregation method for feature ranking is unspecified, affecting EAI calculation consistency
- Training/testing procedure ambiguity (whole data usage) creates potential reproducibility concerns
- The linear assumption underlying EAI (accuracy change proportional to FCP) may break with non-linear feature interactions
- Very low-performing models (<0.55 F1) show unreliable interval predictions with actual accuracies frequently falling outside predicted bounds

## Confidence
- Mechanism 1 (multicollinearity undermining ROAR/PI): Medium confidence - well-supported by theoretical reasoning but corpus evidence is weak
- Mechanism 2 (EAI calculation methodology): High confidence - clearly specified with explicit equations and defined procedures
- Mechanism 3 (interval width interpretation): Low confidence - proposed interpretive framework without external validation

## Next Checks
1. Verify EAI prediction accuracy on CDC Diabetes dataset: compute whether actual retrained accuracies fall within predicted intervals across top 5 feature removals
2. Test EAI under controlled multicollinearity conditions using make_classification with varying informative/redundant feature ratios
3. Assess boundary conditions by computing EAI on near-perfect accuracy models (>0.95) to evaluate prediction reliability at performance extremes