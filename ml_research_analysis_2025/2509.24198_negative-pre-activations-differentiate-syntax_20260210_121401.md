---
ver: rpa2
title: Negative Pre-activations Differentiate Syntax
arxiv_id: '2509.24198'
source_url: https://arxiv.org/abs/2509.24198
tags:
- neurons
- negative
- wasserstein
- arxiv
- pairs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work investigates a previously underexplored aspect of neural
  computation in large language models: the functional role of negative pre-activations
  in neurons immediately preceding smooth activation functions such as GELU and SiLU.
  It focuses on a recently discovered class of highly entangled neurons, termed Wasserstein
  neurons, which are disproportionately critical for model function despite comprising
  a small fraction of the network.'
---

# Negative Pre-activations Differentiate Syntax

## Quick Facts
- arXiv ID: 2509.24198
- Source URL: https://arxiv.org/abs/2509.24198
- Reference count: 7
- Primary result: Negative pre-activations in Wasserstein neurons are mechanistically functional for syntax in LLMs

## Executive Summary
This work investigates a previously underexplored aspect of neural computation in large language models: the functional role of negative pre-activations in neurons immediately preceding smooth activation functions such as GELU and SiLU. It focuses on a recently discovered class of highly entangled neurons, termed Wasserstein neurons, which are disproportionately critical for model function despite comprising a small fraction of the network. These neurons exhibit unique properties in the negative pre-activation space, where pairs of syntactically similar inputs are driven to highly distinct negative values, particularly in early layers. The authors demonstrate that this negative region is mechanistically functional rather than an optimization artifact by employing a minimal, sign-specific intervention that zeroes only the negative pre-activations of a small subset of Wasserstein neurons. This intervention significantly impairs overall model performance and disrupts grammatical behavior, as measured by increases in perplexity and drops in accuracy on syntactic benchmarks (BLiMP and TSE), while matching controls have minimal impact. Further analysis shows that this degradation localizes to syntactic scaffolding tokens and accumulates across layers, with the strongest effects in early layers. Across training checkpoints, the same ablation impairs grammatical behavior as Wasserstein neurons emerge and stabilize. The findings reveal that negative pre-activation differentiation in entangled neurons is a crucial mechanism for syntax, challenging assumptions about the inertness of the negative activation region and highlighting its importance in encoding linguistic structure.

## Method Summary
The study focuses on a class of highly entangled neurons termed Wasserstein neurons in large language models. The authors examine the pre-activation values (inputs to activation functions) of these neurons, particularly in the negative range. They employ a targeted intervention that zeroes out only the negative pre-activations of a small subset of Wasserstein neurons. This minimal intervention is designed to test whether the negative pre-activation region serves a functional purpose beyond optimization artifacts. The intervention's effects are measured through changes in model perplexity and performance on syntactic benchmarks (BLiMP and TSE), with careful controls using ablations on random neurons and positive pre-activation values. The study tracks how grammatical impairment accumulates across layers and examines the emergence of these effects during training.

## Key Results
- Targeted zeroing of negative pre-activations in Wasserstein neurons significantly impairs overall model performance and disrupts grammatical behavior, as measured by increased perplexity and decreased accuracy on syntactic benchmarks
- The degradation localizes to syntactic scaffolding tokens and accumulates across layers, with strongest effects in early layers
- Across training checkpoints, the same ablation impairs grammatical behavior as Wasserstein neurons emerge and stabilize

## Why This Works (Mechanism)
The negative pre-activation region in Wasserstein neurons serves as a computational mechanism for differentiating syntactically similar inputs. When pairs of inputs with similar surface structure but different syntax are processed, these neurons drive their negative pre-activation values to highly distinct ranges. This differentiation appears to be critical for the model's ability to maintain syntactic distinctions during processing. By zeroing out only the negative values, the intervention disrupts this differentiation mechanism while preserving the positive activation contributions, demonstrating that the negative region is not merely an optimization artifact but serves an essential computational function for syntax.

## Foundational Learning

**Wasserstein Neurons**: A class of highly entangled neurons that are disproportionately critical for model function despite being a small fraction of the network. They are named for their connection to Wasserstein distance in neuron analysis. Why needed: Understanding this specific neuron class is crucial as they are the focus of the study. Quick check: Are Wasserstein neurons identified through specific clustering or distance metrics?

**Pre-activation Values**: The values computed by neurons before applying activation functions like GELU or SiLU. Why needed: The study specifically examines the negative portion of this space. Quick check: What is the mathematical relationship between pre-activations and post-activation values?

**Activation Functions**: Smooth functions like GELU and SiLU that transform pre-activation values. Why needed: These functions determine how negative pre-activations are processed. Quick check: How do GELU and SiLU specifically handle negative inputs differently from ReLU?

**Entanglement in Neural Networks**: A state where neurons are highly interconnected and cannot be easily isolated without affecting overall function. Why needed: Wasserstein neurons are described as highly entangled, making targeted interventions more significant. Quick check: What metrics are used to measure neuronal entanglement?

## Architecture Onboarding

**Component Map**: Input -> Embedding Layer -> Multiple Transformer Layers (with attention and feed-forward networks) -> Output Layer. The critical components are the Wasserstein neurons in early layers.

**Critical Path**: Input tokens -> Token embeddings -> Early transformer layers -> Wasserstein neurons -> Feed-forward networks -> Output predictions. The intervention targets the negative pre-activations specifically in the Wasserstein neurons along this path.

**Design Tradeoffs**: The study employs a highly specific intervention (zeroing only negative pre-activations) versus broader ablations, trading off comprehensiveness for precision in isolating the functional role of the negative region.

**Failure Signatures**: The intervention produces increased perplexity and decreased accuracy on syntactic benchmarks, with degradation localizing to syntactic scaffolding tokens and accumulating across layers.

**3 First Experiments**:
1. Identify and visualize Wasserstein neurons in a pre-trained model
2. Measure pre-activation distributions for syntactically similar vs. dissimilar input pairs
3. Perform baseline ablations on random neurons to establish control conditions

## Open Questions the Paper Calls Out
None

## Limitations
- The claim that negative pre-activations are "mechanistically functional rather than an optimization artifact" is supported by targeted ablations showing performance degradation, yet the study does not rule out alternative explanations such as training dynamics or indirect effects through network connectivity
- The specificity of the intervention to negative values is novel, but the exact computational mechanism by which negative pre-activation separation enables syntax remains incompletely characterized
- The reliance on a single model family (GPT-style transformers) and specific datasets (BLiMP, TSE) limits generalizability to other architectures or linguistic phenomena

## Confidence

**High confidence**: The empirical observation that negative pre-activations in Wasserstein neurons differentiate syntactically similar inputs

**Medium confidence**: The functional importance of negative pre-activations for syntax, based on ablation results

**Medium confidence**: The accumulation of grammatical impairment across layers following targeted intervention

## Next Checks

1. Test whether similar negative pre-activation differentiation occurs in other neural architectures (e.g., RNNs, BERT variants) and whether ablations produce comparable grammatical deficits

2. Conduct control experiments ablating positive pre-activations or using different activation functions to isolate the specificity of negative pre-activation effects

3. Perform mechanistic analysis using activation patching or path attribution to trace how negative pre-activation values propagate through the network to affect downstream syntactic representations