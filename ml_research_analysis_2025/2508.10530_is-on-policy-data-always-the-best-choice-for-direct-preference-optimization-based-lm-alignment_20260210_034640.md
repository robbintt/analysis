---
ver: rpa2
title: Is On-Policy Data always the Best Choice for Direct Preference Optimization-based
  LM Alignment?
arxiv_id: '2508.10530'
source_url: https://arxiv.org/abs/2508.10530
tags:
- preference
- alignment
- stage
- candidates
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether on-policy data is always the best
  choice for direct preference optimization (DPO)-based language model (LM) alignment.
  The authors reveal that on-policy data is not always optimal, with systematic effectiveness
  differences emerging between static and on-policy preference candidates across different
  models.
---

# Is On-Policy Data always the Best Choice for Direct Preference Optimization-based LM Alignment?

## Quick Facts
- arXiv ID: 2508.10530
- Source URL: https://arxiv.org/abs/2508.10530
- Authors: Zetian Sun; Dongfang Li; Xuhui Chen; Baotian Hu; Min Zhang
- Reference count: 40
- Key outcome: On-policy data is not always optimal for DPO alignment; effectiveness depends on alignment stage, with static data sometimes being 2.5× more effective

## Executive Summary
This paper challenges the conventional wisdom that on-policy data is universally superior for Direct Preference Optimization (DPO)-based language model alignment. Through systematic experiments across five models (Llama-3, Zephyr, Phi-2, Qwen, Pythia) and two alignment methods (DPO, SLiC-HF), the authors demonstrate that the effectiveness of static versus on-policy preference data varies significantly depending on the model's alignment stage. They propose the alignment stage assumption, which divides the alignment process into preference injection (benefiting from diverse data) and preference fine-tuning (benefiting from high-quality data) stages, and introduce a boundary measurement algorithm to identify these stages.

## Method Summary
The authors propose a two-stage alignment framework where models transition from a preference injection stage (benefiting from diverse static data) to a preference fine-tuning stage (benefiting from high-quality on-policy data). The boundary measurement algorithm compares preference consistency between static (PC_off) and on-policy (PC_on) candidates using a preference model like PairRM. If the boundary score (V_off / (V_off + V_on)) is below 0.5, the model is in the injection stage and should use PC_off; otherwise, it's in the fine-tuning stage and should use PC_on. Experiments test all four combinations of data sources (PC_off→off, PC_off→on, PC_on→off, PC_on→on) across multiple models and alignment methods.

## Key Results
- On-policy data effectiveness varies dramatically: 3× better than static for Llama-3, but only 0.4× as effective for Zephyr
- Boundary measurement algorithm correctly predicts optimal data source with accuracy exceeding 80% across all tested models
- Model collapse occurs for Llama-3 when using SLiC-HF during stage transitions, a phenomenon not observed with DPO
- Phi-2 remains in the preference injection stage across all settings, never transitioning to benefit from on-policy refinement

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The alignment process transitions through two distinct stages with different optimal data characteristics.
- Mechanism: Early in alignment (preference injection stage), models lack knowledge of ground-truth preferences and benefit from diverse preference candidates that cover the reward landscape broadly. As models mature into the preference fine-tuning stage, they benefit more from high-quality candidates that refine the policy within high-reward regions.
- Core assumption: The Bradley-Terry preference model defines the ground-truth text distribution, and optimizing alignment requires estimating this distribution through sampled preference pairs.
- Evidence anchors:
  - [abstract]: "the alignment stage assumption, which divides the alignment process into two distinct stages: the preference injection stage, which benefits from diverse data, and the preference fine-tuning stage, which favors high-quality data"
  - [section 4.3]: "High diversity is more effective for models in the preference injection stage... High quality will be more effective for models in the preference fine-tuning stage"
  - [corpus]: Related work on exploration-exploitation trade-offs in RL provides theoretical grounding (e.g., Tajwar et al. 2024), though corpus lacks direct validation of the two-stage hypothesis.

### Mechanism 2
- Claim: Preference consistency between current policy and ground-truth distribution determines optimal data source selection.
- Mechanism: The boundary measurement algorithm compares preference consistency scores: if off-policy data has higher consistency with ground-truth preferences, the model is in the injection stage; if on-policy data scores higher, it's in the fine-tuning stage. This works because preference distribution consistency is a sufficient condition for text distribution identity under Bradley-Terry assumptions.
- Core assumption: Preference distributions derived from policies and text distributions form bijections (Theorem 5.4), allowing preference consistency to proxy for distribution distance.
- Evidence anchors:
  - [section 5.2]: "preference consistency is to determine if probabilities of identical samples exhibit identical rank orders for both text distributions"
  - [section 5.4]: Table 3 shows boundary scores (BS) correctly predict stage transitions across Llama-3, Zephyr, and Phi-2
  - [corpus]: "Coverage Improvement and Fast Convergence of On-policy Preference Learning" (arxiv 2601.08421) discusses on-policy coverage evolution but doesn't validate the boundary measurement approach directly.

### Mechanism 3
- Claim: Static (off-policy) data can outperform on-policy data when models are in the preference injection stage.
- Mechanism: Off-policy data from diverse sources provides broader coverage of the reward landscape during early alignment, enabling better initial preference injection. On-policy data from the current policy provides in-distribution but potentially narrow exploration. The 0.4× effectiveness ratio for Zephyr with on-policy data (vs. off-policy) demonstrates this reversal.
- Core assumption: Diversity of preference candidates directly impacts the accuracy of ground-truth preference distribution estimation during injection.
- Evidence anchors:
  - [abstract]: "on-policy data can result in a 3× effectiveness compared with static data for Llama-3, and a 0.4× effectiveness for Zephyr"
  - [section 4.2]: "PC off achieves a 12.7-point performance increase" for Zephyr initially, while "PC on achieves a 5.6-point increase"
  - [corpus]: "Optimizing LVLMs with On-Policy Data for Effective Hallucination Mitigation" (arxiv 2512.00706) confirms on-policy benefits for hallucination but doesn't test stage-dependent effects.

## Foundational Learning

- Concept: Bradley-Terry (BT) Preference Model
  - Why needed here: The entire DPO framework and the paper's theoretical analysis (Theorem 5.4, Definition 5.3) rely on BT assumptions linking preference probability to policy likelihood ratios.
  - Quick check question: Can you explain why the BT model implies a bijection between text distributions and preference distributions?

- Concept: On-policy vs. Off-policy Reinforcement Learning
  - Why needed here: The core research question investigates when on-policy sampling (generating candidates from current policy during training) outperforms off-policy static datasets.
  - Quick check question: What distribution shift problem does on-policy sampling aim to address in standard RL?

- Concept: KL Divergence in Policy Optimization
  - Why needed here: DPO optimizes a KL-regularized objective (Eq. 3), and the paper establishes theoretical equivalence between DPO and general alignment objectives through this regularization.
  - Quick check question: How does the KL penalty term in Eq. (3) prevent the policy from deviating too far from the reference model?

## Architecture Onboarding

- Component map:
  Boundary Measurement Algorithm -> Data Selection Pipeline -> Two-iteration Training Framework -> Preference Annotation Module

- Critical path:
  1. Initialize policy model (SFT checkpoint or partially aligned model)
  2. Run Algorithm 1 on subset of prompts (~2,000) to compute boundary score (V_off / (V_off + V_on))
  3. If BS < 0.5: select high-diversity off-policy data; if BS ≥ 0.5: select high-quality on-policy data
  4. Train for one epoch with selected preference data using DPO or SLiC-HF loss
  5. Repeat from step 2 for iterative alignment

- Design tradeoffs:
  - Computational overhead: On-policy sampling requires inference on full prompt set (~61K samples in UltraFeedback), but boundary measurement uses only ~2K samples (~3.2% overhead per epoch)
  - Preference model dependency: Stage boundary is relative to the chosen preference model; if PairRM is biased, alignment targets the wrong distribution
  - Binary stage decision: Current algorithm makes hard switch between stages; the paper notes future work could explore smoother adaptive blending

- Failure signatures:
  - Model collapse: Table 5 shows Llama-3 trained with PC_off→on under SLiC-HF exhibits severe performance degradation (-11.93 LC win rate)
  - Contradictory stage predictions: Zephyr with BS=0.58 still benefits more from PC_off (8.5-point gain) than PC_on (5.5-point gain) due to lower on-policy quality
  - Stuck in injection stage: Phi-2 remains in injection stage across all settings, never transitioning to benefit from on-policy refinement

- First 3 experiments:
  1. Baseline replication: Run two-iteration DPO on Zephyr-7B with all four data combinations (PC_off→off, PC_off→on, PC_on→off, PC_on→on) and verify LC win rates match Table 1 patterns
  2. Boundary algorithm validation: Compute boundary scores for three model checkpoints (SFT, post-PC_off, post-PC_on) and confirm BS transitions from <0.5 to >0.5 when expected
  3. Data characteristic isolation: Construct a controlled off-policy dataset (like PC_llama) with known diversity and quality metrics to decouple on-policy/off-policy effects from data characteristics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can adaptive data blending strategies be implemented to transition smoothly between the preference injection and fine-tuning stages?
- Basis: [explicit] The authors state in the conclusion that the current rigid switch inspires "exploration of smoother and more adaptive data blending strategies," which they leave for future research.
- Why unresolved: The paper currently relies on a binary decision boundary (Algorithm 1) to switch data sources; it does not define a mechanism for interpolating between static and on-policy data ratios dynamically.
- Evidence: Empirical results showing the performance of a weighted loss function where data weights are proportional to the boundary score rather than a discrete threshold.

### Open Question 2
- Question: To what extent does the reliability of the boundary measurement algorithm depend on the specific preference model used as the ground-truth annotator?
- Basis: [inferred] Appendix B.2 discusses the "Dependency of Preference Model," noting that if the model is weak or biased, the decision might be suboptimal, implying a sensitivity not fully tested across diverse annotators.
- Why unresolved: The experiments primarily rely on PairRM; the transferability of stage boundaries across different preference annotators (e.g., stronger models or human labels) remains unquantified.
- Evidence: Ablation studies measuring stage boundary consistency when using different reward models (e.g., GPT-4 vs. PairRM) to determine the alignment stage for the same policy.

### Open Question 3
- Question: What causes the observed model collapse in Llama-3 when using the SLiC-HF alignment method during stage transitions, which does not occur with DPO?
- Basis: [inferred] Section 6 notes a "model collapse phenomenon" for Llama-3 trained with SLiC-HF subsequent to preference injection, a severe degradation not observed with DPO.
- Why unresolved: The authors highlight the discrepancy but do not provide a theoretical or empirical analysis explaining the specific instability of SLiC-HF in this context compared to DPO.
- Evidence: A comparative analysis of the optimization landscape or gradient variance for SLiC-HF versus DPO during the boundary transition phase to identify the source of instability.

## Limitations

- The binary stage decision oversimplifies what may be a continuous transition, and the paper acknowledges that smoother adaptive blending could be more effective
- The boundary measurement algorithm's reliability depends heavily on the quality and bias of the preference model used for annotation
- Effectiveness ratios show substantial model dependence, suggesting the stage boundaries are not universal across architectures or training regimes

## Confidence

**High Confidence**: The empirical demonstration that on-policy data does not universally outperform static data (Table 1 and Table 5 results across 5 models and 2 methods). The systematic performance differences between PC_off→on and PC_on→on combinations are clearly reproducible.

**Medium Confidence**: The theoretical framework of alignment stages based on preference consistency. While the mathematical formulation (Theorem 5.4) is sound under Bradley-Terry assumptions, the practical application depends heavily on the quality and bias of the preference model used for boundary measurement.

**Low Confidence**: The universality of the 0.5 boundary score threshold across all model families and training stages. The observed variations (Zephyr with BS=0.58 still benefits more from PC_off) suggest this threshold may need model-specific calibration.

## Next Checks

1. **Cross-model boundary score validation**: Compute boundary scores for SFT checkpoints of Llama-3, Zephyr, and Phi-2, then verify that the stage predictions (injection vs. fine-tuning) correctly match observed performance patterns when using PC_off vs PC_on data.

2. **Preference model sensitivity analysis**: Repeat the boundary measurement and alignment experiments using different preference models (e.g., GPT-4, alternative PairRM variants) to test whether stage predictions and effectiveness ratios remain consistent.

3. **Continuous stage transition experiment**: Implement a weighted combination of PC_off and PC_on data based on continuous boundary scores rather than the current binary decision, and measure whether this provides smoother performance improvements across alignment stages.