---
ver: rpa2
title: Redefining Machine Translation on Social Network Services with Large Language
  Models
arxiv_id: '2504.07901'
source_url: https://arxiv.org/abs/2504.07901
tags:
- translation
- data
- language
- bleu
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of machine translation on social
  networking services (SNS), where existing models struggle with culturally nuanced
  content like memes, slang, and pop culture references. The authors propose RedTrans,
  a 72B parameter large language model tailored for SNS translation.
---

# Redefining Machine Translation on Social Network Services with Large Language Models

## Quick Facts
- arXiv ID: 2504.07901
- Source URL: https://arxiv.org/abs/2504.07901
- Reference count: 33
- RedTrans, a 72B parameter LLM, outperforms state-of-the-art models on SNS translation benchmarks including RedTrans-Bench and open MT benchmarks.

## Executive Summary
This paper addresses the challenge of machine translation on social networking services (SNS), where existing models struggle with culturally nuanced content like memes, slang, and pop culture references. The authors propose RedTrans, a 72B parameter large language model tailored for SNS translation. Key innovations include: (1) Supervised Fine-tuning with Dual-LLM Back-Translation Sampling, an unsupervised method using LLM-based back-translation to select diverse data for fine-tuning; (2) Rewritten Preference Optimization (RePO), an algorithm that identifies and corrects erroneous preference pairs through expert annotation to build reliable preference corpora; and (3) RedTrans-Bench, the first benchmark for SNS translation evaluating phenomena like humor localization, emoji semantics, and meme adaptation. Experiments show RedTrans outperforms state-of-the-art LLMs on multiple benchmarks including RedTrans-Bench and open MT-related benchmarks. The model has been deployed in a real-world production environment, demonstrating that domain-specific adaptation effectively bridges the gap between generic and culturally grounded translation systems.

## Method Summary
RedTrans employs a two-stage training pipeline: Supervised Fine-tuning (SFT) followed by preference optimization. For SFT, the authors use a Dual-LLM Back-Translation Sampling strategy where two different LLMs translate source text to target and back, and the absolute BLEU divergence between these back-translations is used as a proxy for sample informativeness. Samples with higher BLEU divergence are prioritized during stratified sampling for SFT training on a mix of general and SNS-specific corpora. For preference optimization, the authors propose Rewritten Preference Optimization (RePO), which improves upon standard DPO by having human experts rewrite suboptimal preference pairs that fall below a quality threshold, creating cleaner training signals. The model is based on Qwen-2.5-72B-Instruct and trained using DeepSpeed Zero-3 on 512 NVIDIA H800 GPUs.

## Key Results
- RedTrans-72B achieves state-of-the-art performance on RedTrans-Bench (0.5162 avg BLEU) significantly outperforming GPT-4o (0.4549 avg BLEU)
- Dual-LLM back-translation sampling with higher BLEU divergence ranges ([0.4,1]) yields better performance (0.6561 chrF++) than lower ranges ([0.0,0.1)) which introduce noise
- RePO consistently outperforms standard DPO (0.6562 vs 0.6521 chrF++ for ZH→EN) by identifying and correcting erroneous preference pairs through expert annotation

## Why This Works (Mechanism)

### Mechanism 1: Dual-LLM Back-Translation Sampling for Data Diversity
- Claim: Using BLEU divergence between two LLMs' back-translations enables stratified sampling of diverse, high-quality SFT data.
- Mechanism: Two different LLMs translate source text (A) to target (B) and back to source (C). The absolute difference in BLEU scores between the two back-translations ($\Delta BLEU = |BLEU(A, C_1) - BLEU(A, C_2)|$) serves as a proxy for translation difficulty or ambiguity. Samples with higher divergence are prioritized during sampling for SFT.
- Core assumption: Higher BLEU divergence between models correlates with more informative training examples for SNS translation, and BLEU is a sufficient proxy for identifying these cases.
- Evidence anchors:
  - [section 6.1] "employing stratified sampling based on BLEU divergence for weighted selection."
  - [Table 4] Shows that training with samples from higher BLEU difference ranges ([0.4,1]) yields better performance (0.6561 chrF++) than including lower-difference pairs ([0.0,0.1), 0.6563 chrF++), which introduces noise and slightly reduces performance on EN→ZH.
  - [corpus] Related work on SNS LLMs (e.g., RedOne) focuses on general post-training pipelines; this specific dual-LLM back-translation sampling strategy for diversity is not explicitly corroborated by the corpus, though back-translation itself is a known MT technique.
- Break condition: If BLEU divergence does not correlate with translation quality or diversity for a new language pair (especially low-resource), or if the two LLMs produce consistently similar (or both poor) outputs, the sampling signal degrades.

### Mechanism 2: Rewritten Preference Optimization (RePO) for Noisy Human Feedback
- Claim: RePO improves upon standard DPO by having experts rewrite suboptimal preference pairs, creating a cleaner signal with a "ground truth" reference for alignment.
- Mechanism: Standard DPO trains on preference pairs $(y_w, y_l)$. RePO adds a step where if both candidate responses are below a quality threshold (max score $< \tau$), a human expert rewrites a superior response $y_t$. New pairs $(y_t, y_1)$ and $(y_t, y_2)$ are constructed and added to the dataset. The loss function incorporates a KL divergence term to this "truth" distribution.
- Core assumption: Expert-rewritten responses provide a reliable "ground truth" distribution ($\pi_{truth}$) that is superior to the original noisy pairs, and the quality threshold $\tau$ can be effectively estimated.
- Evidence anchors:
  - [abstract] "Rewritten Preference Optimization (RePO), an algorithm that identifies and corrects erroneous preference pairs through expert annotation to build reliable preference corpora;"
  - [Table 3] RePO consistently outperforms DPO (0.6562 vs 0.6521 chrF++ for ZH→EN), showing the value of the rewriting step.
  - [corpus] A related paper, CRPO (Confidence-Reward Driven Preference Optimization), also aims to improve DPO for MT by addressing noisy rewards, suggesting a broader trend of modifying preference learning for this task.
- Break condition: If expert annotation is inconsistent, culturally biased, or unavailable at scale, or if the "ground truth" introduced is itself flawed, the model may overfit to the experts' specific stylistic preferences rather than general user preference.

### Mechanism 3: Domain-Specific Adaptation and Benchmarking
- Claim: Specialized SFT and preference data, combined with a targeted benchmark (RedTrans-Bench), are necessary to bridge the gap between generic and SNS translation performance.
- Mechanism: A general-purpose LLM is fine-tuned on a mix of general and SNS-specific corpora (3.8M samples), then further aligned using RePO. Evaluation on RedTrans-Bench, which includes culturally grounded posts, emoji semantics, and meme adaptation, specifically measures this domain adaptation.
- Core assumption: Performance on RedTrans-Bench is a strong proxy for real-world SNS translation quality, and the collected data (from a major social platform) is representative of general SNS challenges.
- Evidence anchors:
  - [abstract] "...model has been deployed in a real-world production environment, demonstrating that domain-specific adaptation effectively bridges the gap..."
  - [Table 2] Shows a performance gap between models on general benchmarks (WMT22-24) and RedTrans-Bench. RedTrans-72B achieves top scores on RedTrans-Bench (0.5162 avg BLEU), significantly outperforming general-purpose giants like GPT-4o (0.4549), validating the benefit of domain adaptation.
  - [corpus] The existence of related SNS-specific models and benchmarks (e.g., SNS-Bench-VL) in the corpus supports the premise that SNS is a distinct and challenging domain requiring specialized approaches.
- Break condition: If the SNS domain data used for training is not representative of future, evolving slang or cultural trends (data drift), the model's performance will degrade over time.

## Foundational Learning

- Concept: **Back-Translation for Data Augmentation**
  - Why needed here: A core mechanism of RedTrans involves using LLMs to perform back-translation to create synthetic training data. Understanding this classic MT technique is essential to grasp why they use it and how they modify it (Dual-LLM sampling).
  - Quick check question: Can you explain why using the output of a translation model as training data for that same model might lead to a feedback loop, and how the paper's approach of using *two different* LLMs might mitigate this?

- Concept: **Direct Preference Optimization (DPO)**
  - Why needed here: RedTrans builds upon DPO for its alignment stage. Understanding the standard DPO objective is a prerequisite to understanding the proposed RePO modification and its "truth alignment" term.
  - Quick check question: In standard DPO, what data is required, and how does the loss function implicitly define a reward?

- Concept: **Cultural Grounding in Translation**
  - Why needed here: The central problem defined by the paper is the failure of generic models on culturally nuanced SNS content (memes, slang, emoji semantics, pop culture references). Understanding that translation is not just literal word-mapping is critical.
  - Quick check question: Give an example from the paper where a literal translation is incorrect and how the culturally grounded translation differs. (e.g., "You are not my type" → "你不是我的菜" vs. "你不是我喜欢的类型").

## Architecture Onboarding

- Component map:
  1. Base Model: Qwen-2.5-72B-Instruct
  2. SFT Module: A dual-LLM data generation and sampling pipeline that feeds into a standard supervised fine-tuning process on a mix of general and SNS corpora
  3. RePO Alignment Module: A preference optimization layer that takes human-annotated pairs, identifies low-quality pairs, generates expert rewrites, and applies a modified DPO loss with a KL-divergence term to the expert-rewritten "truth"
  4. Evaluation Layer: RedTrans-Bench, a held-out test set for SNS-specific phenomena

- Critical path:
  1. Collect and preprocess general and SNS corpora
  2. Execute Dual-LLM Back-Translation to generate and select diverse SFT samples
  3. Perform Supervised Fine-Tuning on the base model
  4. Collect preference pairs and have human experts review and rewrite low-quality pairs
  5. Run RePO training to align the SFT model
  6. Evaluate final model on RedTrans-Bench

- Design tradeoffs:
  - **Synthetic Data Quality vs. Scale**: The Dual-LLM back-translation method generates data at scale without manual annotation, but quality depends on the strength of the two LLMs used. The paper ablates this, showing that poorly chosen samples (low BLEU diff) add noise
  - **Expert Annotation Cost vs. Signal Quality**: RePO requires costly expert intervention to rewrite pairs. This trades off against the noise and potential failure of standard RLHF/DPO on ambiguous SNS data. The paper claims this cost is justified by the performance gain
  - **Model Size vs. Deployment Cost**: The chosen architecture is a 72B parameter model. The paper notes this limits adoption in resource-constrained environments. This tradeoff is accepted for state-of-the-art performance on a complex task

- Failure signatures:
  - **Metric Collapse on SNS Tasks**: If performance on standard MT benchmarks (WMT) is high but low on RedTrans-Bench, it indicates a failure of domain adaptation
  - **Preference Overfitting**: If the model only adopts the specific phrasing of the expert annotators and fails to generalize to user preferences, the RePO mechanism may be too strong (high λ)
  - **Back-Translation Noise Amplification**: If performance degrades after the SFT stage with back-translated data, it indicates the filtering threshold was too permissive, introducing too much noise

- First 3 experiments:
  1. **Ablation on Back-Translation Sampling**: Replicate the experiment in Table 4 by training models on subsets of data with different BLEU difference ranges (e.g., [0.4,1] vs [0.0,0.1)). This validates the core claim of the sampling mechanism
  2. **RePO vs. DPO Comparison**: Implement the RePO loss function and train on a small, expert-rewritten dataset. Compare the results against a baseline model trained with standard DPO on the same data, replicating the findings in Table 3
  3. **Generalization Test**: Evaluate the fine-tuned model on both RedTrans-Bench and a general MT benchmark (e.g., WMT23). Check for the expected pattern: strong performance on both, with a larger relative gain on the SNS benchmark, as shown in Table 2

## Open Questions the Paper Calls Out
None

## Limitations
- **Synthetic Data Quality**: The Dual-LLM back-translation sampling method relies on BLEU divergence as a proxy for sample informativeness, but this correlation is not explicitly validated beyond controlled experiments for Chinese-English translation.
- **Expert Annotation Reliability**: RePO depends on expert annotations to identify and rewrite low-quality preference pairs, but the consistency and cultural representativeness of these expert annotations are not discussed.
- **Model Size and Deployment**: The 72B parameter model achieves state-of-the-art results but is resource-intensive, with no exploration of model compression or efficient deployment strategies.

## Confidence
- **High Confidence**: The overall claim that domain-specific adaptation (SFT + RePO) improves SNS translation performance over generic LLMs is well-supported by benchmark results (Table 2, Table 3). The performance gap between RedTrans and general-purpose models on RedTrans-Bench is substantial and consistent.
- **Medium Confidence**: The specific mechanisms (Dual-LLM back-translation sampling and RePO) are supported by ablation studies and comparative experiments, but their robustness across different language pairs, SNS domains, and evolving cultural trends is not tested. The paper's focus on Chinese-English translation limits generalizability.
- **Low Confidence**: The claim about real-world deployment demonstrating effectiveness is stated but not quantified. There is no data on user satisfaction, error reduction rates, or long-term performance in production.

## Next Checks
1. **Cross-Lingual Generalization**: Replicate the SFT and RePO pipeline for a different language pair (e.g., English-Spanish or English-Japanese). Evaluate on both a general MT benchmark and a culturally grounded test set for that language to assess whether the Dual-LLM sampling and RePO mechanisms are broadly applicable or specific to Chinese-English.

2. **Long-Term Domain Adaptation**: Deploy a model variant on a live SNS platform for a period (e.g., 3-6 months). Track performance drift by periodically evaluating on a held-out test set of recent posts. Measure whether specialized training on historical data maintains quality as new slang, memes, and cultural references emerge, or if continuous fine-tuning is required.

3. **Ablation on Expert Annotation Quality**: Conduct a controlled experiment where RePO is trained with different levels of expert annotation quality: (a) high-quality rewrites (as in the paper), (b) randomly selected rewrites, and (c) no rewrites (standard DPO). Compare final model performance to quantify the actual contribution of expert input versus the filtering of low-quality pairs alone.