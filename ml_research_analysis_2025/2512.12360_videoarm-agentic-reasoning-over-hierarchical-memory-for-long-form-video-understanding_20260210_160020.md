---
ver: rpa2
title: 'VideoARM: Agentic Reasoning over Hierarchical Memory for Long-Form Video Understanding'
arxiv_id: '2512.12360'
source_url: https://arxiv.org/abs/2512.12360
tags:
- video
- frame
- reasoning
- memory
- tool
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: VideoARM introduces an agentic reasoning-over-hierarchical-memory
  paradigm for long-form video understanding. Instead of exhaustive preprocessing,
  it performs adaptive, on-the-fly reasoning and memory construction using a controller
  that autonomously invokes tools for coarse-to-fine video interpretation, while a
  hierarchical multimodal memory continuously captures and updates multi-level clues.
---

# VideoARM: Agentic Reasoning over Hierarchical Memory for Long-Form Video Understanding

## Quick Facts
- arXiv ID: 2512.12360
- Source URL: https://arxiv.org/abs/2512.12360
- Authors: Yufei Yin; Qianke Meng; Minghao Chen; Jiajun Ding; Zhenwei Shao; Zhou Yu
- Reference count: 38
- Primary result: Achieves up to 82.8% accuracy on long-video tasks while using only 1/34 of the tokens compared to prior approaches

## Executive Summary
VideoARM introduces an agentic reasoning-over-hierarchical-memory paradigm for long-form video understanding that performs adaptive, on-the-fly reasoning rather than exhaustive preprocessing. The system uses a controller that autonomously invokes tools for coarse-to-fine video interpretation while maintaining a hierarchical multimodal memory that continuously captures and updates multi-level clues. Experiments on Video-MME, LongVideoBench, and EgoSchema show VideoARM outperforms state-of-the-art methods while dramatically reducing token consumption.

## Method Summary
VideoARM processes long-form videos through an agent loop (Observe-Think-Act-Memorize) using a controller (OpenAI o3) and a Hierarchical Multimodal Memory (HM3) with three tiers: Sensory (raw perceptual inputs), Result (tool outputs with temporal ordering), and Working (reasoning traces). The controller autonomously selects from three exposed tools (scene_snapper, audio_transcripter, clip_analyzer) to interpret video in a coarse-to-fine manner, updating memory pools after each tool call. The approach adapts frame sampling based on query relevance rather than pre-processing all clips at fixed rates.

## Key Results
- Achieves up to 82.8% accuracy on long-video tasks while using only 1/34 of tokens compared to prior approaches
- Consumes only 1/34 of tokens required by DVD when processing 10 videos (average 41.3 minutes) with 30 queries
- Outperforms state-of-the-art method DVD across Video-MME, LongVideoBench, and EgoSchema benchmarks

## Why This Works (Mechanism)

### Mechanism 1: Query-Guided Temporal Scoping Reduces Token Consumption
Adaptive coarse-to-fine focusing on query-relevant segments reduces token usage by ~97% versus exhaustive preprocessing. Temporal Scoping Tools constrain the working scope to query-aligned segments, sampling frames adaptively (N1=30-150) based on interval complexity rather than pre-processing all clips at fixed rates. This assumes query-relevant evidence clusters in identifiable temporal regions that early coarse sampling can localize without full video analysis.

### Mechanism 2: Hierarchical Memory Enables Progressive Evidence Aggregation
Three-tier memory transforms raw perceptual inputs into query-aware semantic evidence, supporting multi-step reasoning without context overflow. Sensory Memory holds current perceptual frames; Result Memory accumulates tool outputs with temporal ordering; Working Memory externalizes reasoning traces. After each tool call, short-term pools clear, preventing context bloat while preserving structured evidence.

### Mechanism 3: Controller Autonomy Avoids Hand-Crafted Pipeline Bottlenecks
Giving MLLM controllers tool-selection autonomy improves reasoning quality versus fixed pipelines, conditional on strong base reasoning capabilities. The controller follows observe–think–act–memorize loop with no pre-defined tool sequences, selecting tools based on HM3 state and generating reasoning traces. Step budget N=10 caps iterations.

## Foundational Learning

- **ReAct-style agent loops** (Reasoning + Acting interleaved): The controller must alternate between planning (thought), tool invocation (action), and evidence integration (observation) across multiple iterations. *Quick check*: Can you explain why a pure retrieval-augmented approach differs from ReAct-style iterative reasoning?

- **Hierarchical memory architectures** (sensory → working → long-term): HM3 organizes evidence from raw frames to reasoning traces; understanding this progression is essential for debugging memory state. *Quick check*: What would happen if you removed the short-term perception pool but kept the long-term pool?

- **Token budgeting in multimodal contexts**: VideoARM's core efficiency claim depends on understanding how frame sampling and context management affect token consumption. *Quick check*: How does tiling 30 frames into 3×2 grids affect token count versus passing 30 separate images?

## Architecture Onboarding

- **Component map**: Controller (o3) orchestrates observe–think–act–memorize loop → Temporal Scoping Tools (Interval Localizer, Clip Explorer) → Multimodal Understanding Tools (Scene Snapper, Audio Transcriber, Clip Analyzer) → HM3 Memory (Sensory → Result → Working)

- **Critical path**: Query + video metadata → Controller initializes HM3 → Interval Localizer samples frames → updates Pl → Controller selects understanding tool → processes Pl or Ps → Tool output → Result Memory; reasoning trace → Working Memory → Loop until ANSWER or step budget N=10

- **Design tradeoffs**: Higher N (10) improves long-video performance but adds latency; lower N (3) sufficient for short videos; adaptive N1 (avg 49.8) outperforms fixed N1=60 but requires controller to determine frame count; o3 controller outperforms gpt-4o/Qwen3-VL significantly but costs more per invocation

- **Failure signatures**: Early localization errors if Interval Localizer misses brief critical events; weak controller planning with GPT-4o/Qwen3-VL producing 40-55% accuracy versus 76-80% for o3; memory overflow without clearing Ps after each tool call

- **First 3 experiments**: 1) Reproduce Table 2 subset with o3 vs. GPT-4o controller on 50 Video-MME long questions; 2) Ablate single memory tier by removing Result Memory on same subset; 3) Profile token consumption by logging tokens per iteration on 10 long videos

## Open Questions the Paper Calls Out

### Open Question 1
Can VideoARM maintain state-of-the-art performance when fully migrated from proprietary models (o3, GPT-4o) to open-source backbones? The framework currently relies on superior planning and visual alignment of closed-source models, and it's unproven whether open-source alternatives can handle the dense grid interface effectively.

### Open Question 2
How can the initial temporal sampling strategy be improved to prevent the agent from missing brief, fine-grained events (e.g., one-frame changes)? The Perception Pool seeds the reasoning chain; if sparse initial sampling misses a critical moment, the controller has no mechanism to discover it later without inefficient exhaustive search.

### Open Question 3
Would integrating an explicit "self-correction" or "backtracking" mechanism improve robustness against early localization errors? The current loop relies on accumulating evidence forward; if the early "Think" stage commits to a wrong temporal interval, the memory structure propagates this error rather than questioning it.

## Limitations
- Tool-set inconsistency between main text descriptions and appendix implementations creates ambiguity about controller's exact role in temporal scoping
- Controller capability claims rely heavily on single ablation showing large accuracy gaps without intermediate model testing
- Token consumption verification lacks transparent comparison methodology with baseline approaches

## Confidence

**High Confidence (Level 4-5)**: Hierarchical memory architecture benefits are well-supported by ablation studies showing significant performance drops when removing memory tiers (9.5% drop without long-term perception, invalid cycles without Result Memory).

**Medium Confidence (Level 2-3)**: Adaptive coarse-to-fine temporal scoping mechanism's efficiency claims are plausible but implementation details in appendix don't fully align with main text descriptions.

**Low Confidence (Level 1)**: Controller autonomy versus hand-crafted pipeline superiority claim lacks comparative validation against DVD or other approaches on identical queries.

## Next Checks
1. Reproduce the controller strength ablation using exact prompts from Appendix E with o3, GPT-4o, and Qwen3-VL controllers on 50 Video-MME long questions to verify the ~35% accuracy gap claim.

2. Implement the full tool-set discrepancy resolution by either confirming the controller performs Interval Localizer/Clip Explorer functions through frame range generation, or implementing these as explicit tools and measuring impact on performance and token consumption.

3. Conduct a token accounting audit by instrumenting code to log tokens consumed at each iteration across 10 long videos, comparing VideoARM's consumption against DVD baseline's reported values to verify the 1/34 reduction claim.