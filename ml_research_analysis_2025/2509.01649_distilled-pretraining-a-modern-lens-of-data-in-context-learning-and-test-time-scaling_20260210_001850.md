---
ver: rpa2
title: 'Distilled Pretraining: A modern lens of Data, In-Context Learning and Test-Time
  Scaling'
arxiv_id: '2509.01649'
source_url: https://arxiv.org/abs/2509.01649
tags:
- distillation
- pretraining
- teacher
- data
- standard
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Distilled pretraining yields models with better test-time scaling\
  \ but worse in-context learning. While distillation improves pass@k scores and generation\
  \ diversity\u2014even matching models trained on twice the data\u2014it impairs\
  \ induction head learning and copying abilities critical for in-context tasks."
---

# Distilled Pretraining: A modern lens of Data, In-Context Learning and Test-Time Scaling

## Quick Facts
- arXiv ID: 2509.01649
- Source URL: https://arxiv.org/abs/2509.01649
- Authors: Sachin Goyal; David Lopez-Paz; Kartik Ahuja
- Reference count: 40
- Primary result: Distilled pretraining improves test-time scaling while degrading in-context learning

## Executive Summary
Distilled pretraining presents a modern approach to model development that yields improved test-time scaling capabilities but at the cost of impaired in-context learning abilities. The technique accelerates learning of high-entropy patterns while hindering the acquisition of low-entropy, induction-head-like behaviors critical for few-shot tasks. Through experiments across multiple datasets and model scales, the authors demonstrate that distillation can match the performance of models trained on twice the data in terms of pass@k scores and generation diversity, but with significant trade-offs for tasks requiring copying and induction capabilities.

## Method Summary
The study compares standard pretraining with distilled pretraining using decoder-only Transformers across multiple datasets including C4, OpenWebText2, and Python150M. Distillation is performed using a combination of token-level and sequence-level losses, with teacher models trained on the same data. The authors employ a bigram sandbox environment to analyze learning dynamics and develop token routing strategies to mitigate distillation's negative effects on in-context learning. Experiments are conducted across various model sizes (125M to 770M parameters) and training durations to understand the scaling behavior and trade-offs.

## Key Results
- Distilled pretraining improves pass@k scores and generation diversity, with models matching performance of those trained on twice the data
- Distillation impairs in-context learning abilities, particularly affecting induction head learning and copying tasks
- Token routing and teacher selection strategies can partially mitigate the degradation of in-context learning capabilities
- The trade-off is explained by distillation accelerating learning of high-entropy rows while hindering low-entropy, induction-head-like rows in the bigram sandbox

## Why This Works (Mechanism)
Distilled pretraining works by transferring knowledge from a larger teacher model to a smaller student model through a combination of token-level and sequence-level losses. This process accelerates the learning of high-entropy patterns in the data by providing more informative gradients that capture complex relationships. However, this same mechanism impairs the development of low-entropy, induction-head-like behaviors that are crucial for in-context learning. The teacher model's smoother probability distributions and more confident predictions create a learning environment that prioritizes memorization and pattern completion over the development of flexible, copy-based reasoning mechanisms.

## Foundational Learning
- **Test-time scaling**: The ability of models to improve performance with increased inference-time computation. Critical for understanding how models can leverage additional resources during deployment. Quick check: measure performance improvements as inference budget increases.
- **In-context learning**: The capability to perform tasks based on examples provided in the prompt without parameter updates. Essential for few-shot generalization. Quick check: evaluate performance on copying and induction tasks with varying numbers of examples.
- **Knowledge distillation**: The process of transferring knowledge from a larger teacher model to a smaller student model. Key for understanding the trade-offs between model efficiency and capability. Quick check: compare student performance with and without distillation across different tasks.
- **Induction heads**: Specific attention patterns that enable models to identify and apply patterns from examples. Important for understanding how models generalize from in-context examples. Quick check: analyze attention patterns during copying tasks to identify induction head behavior.
- **Entropy-based learning dynamics**: The relationship between token entropy and learning speed during training. Crucial for understanding why certain capabilities are gained or lost during distillation. Quick check: measure learning rates for high vs low entropy tokens during training.

## Architecture Onboarding

**Component Map**: Data -> Pretraining (Standard/Distilled) -> Evaluation (Test-time scaling/In-context learning) -> Analysis (Bigram sandbox/Token routing)

**Critical Path**: Data preparation → Pretraining with distillation → Evaluation on standard benchmarks → Bigram sandbox analysis → Token routing implementation → Teacher selection optimization

**Design Tradeoffs**: The primary tradeoff involves balancing improved test-time scaling (higher pass@k, better generation diversity) against degraded in-context learning capabilities (impaired copying, induction head learning). This represents a fundamental choice between efficiency at deployment versus flexibility in few-shot scenarios.

**Failure Signatures**: Models exhibit strong performance on standard pretraining benchmarks and test-time scaling tasks but show significant degradation on copying tasks, few-shot learning evaluations, and tasks requiring induction capabilities. Generation quality metrics may improve while the ability to follow complex in-context instructions deteriorates.

**3 First Experiments**:
1. Compare standard vs distilled pretraining on C4 dataset with 125M parameter models, measuring both pass@k scores and copying task performance
2. Implement token routing strategy on OpenWebText2 and evaluate its impact on preserving in-context learning while maintaining test-time scaling benefits
3. Analyze learning dynamics in the bigram sandbox by measuring learning rates for high vs low entropy rows under standard and distilled pretraining

## Open Questions the Paper Calls Out
None

## Limitations
- Experimental scope limited to decoder-only Transformers, uncertain generalizability to other architectures
- Bigram sandbox analysis may not fully capture complexity of real-world pretraining data distributions
- Proposed mitigation strategies (token routing, teacher selection) require broader empirical validation across diverse tasks and model scales

## Confidence

**High Confidence**: The observation that distilled pretraining improves test-time scaling metrics while simultaneously impairing in-context learning abilities is well-supported by controlled experiments across multiple datasets and model sizes.

**Medium Confidence**: The mechanistic explanation linking distillation effects to high-entropy vs low-entropy token learning patterns in the bigram sandbox is plausible but requires validation in more complex settings.

**Low Confidence**: The claim that distilled models can match performance of models trained on twice the data is based on specific benchmarks and may not hold across all task domains or evaluation metrics.

## Next Checks

1. Evaluate distilled pretraining trade-offs on encoder-decoder architectures and assess whether observed patterns in test-time scaling versus in-context learning persist across different model families.

2. Test token routing and teacher selection strategies on real-world pretraining corpora (beyond bigram sandbox) to verify their effectiveness in mitigating in-context learning degradation while preserving test-time scaling benefits.

3. Conduct ablation studies varying distillation ratios and teacher-student size differences to map out the full trade-off space and identify optimal configurations for specific downstream tasks.