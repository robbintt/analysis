---
ver: rpa2
title: Deep Learning--Accelerated Multi-Start Large Neighborhood Search for Real-time
  Freight Bundling
arxiv_id: '2512.11187'
source_url: https://arxiv.org/abs/2512.11187
tags:
- pomo
- search
- problem
- pickup
- solution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work tackles real-time freight bundling on online freight
  exchange systems by modeling it as a multi-commodity one-to-one pickup-and-delivery
  selective traveling salesperson problem (m1-PDSTSP), where the challenge is to couple
  combinatorial bundle selection with pickup-and-delivery routing under sub-second
  latency constraints. To address this, the authors propose a learning-accelerated
  hybrid search pipeline that combines a Transformer-based policy-gradient constructor
  with a Multi-Start Large Neighborhood Search (MSLNS) metaheuristic within a rolling-horizon
  framework.
---

# Deep Learning--Accelerated Multi-Start Large Neighborhood Search for Real-time Freight Bundling

## Quick Facts
- arXiv ID: 2512.11187
- Source URL: https://arxiv.org/abs/2512.11187
- Authors: Haohui Zhang; Wouter van Heeswijk; Xinyu Hu; Neil Yorke-Smith; Martijn Mes
- Reference count: 40
- One-line primary result: Hybrid POMO+MSLNS achieves <2% optimality gap vs exact baselines on freight bundling with <0.5 s latency.

## Executive Summary
This work tackles real-time freight bundling on online freight exchange systems by modeling it as a multi-commodity one-to-one pickup-and-delivery selective traveling salesperson problem (m1-PDSTSP). The key challenge is coupling combinatorial bundle selection with pickup-and-delivery routing under sub-second latency constraints. To address this, the authors propose a learning-accelerated hybrid search pipeline that combines a Transformer-based policy-gradient constructor with a Multi-Start Large Neighborhood Search (MSLNS) metaheuristic within a rolling-horizon framework. The neural constructor generates high-quality initial solutions that position the search close to promising attraction basins, while MSLNS refines them using softmax-biased removal and adaptive destroy sizes for effective diversification and intensification. Across benchmarks, the method achieves less than 2% optimality gap in total revenue relative to the best exact baseline and outperforms state-of-the-art neural and metaheuristic baselines in both solution quality and runtime. This is the first demonstration that a deep neural network constructor can reliably provide high-quality seeds for multi-start improvement heuristics, offering practical potential for real-time routing.

## Method Summary
The approach uses a Transformer-based policy-gradient constructor (POMO) to generate initial solutions, followed by a Multi-Start Large Neighborhood Search (MSLNS) for refinement. The constructor is trained using REINFORCE with shared baseline across multiple start nodes, generating diverse, high-quality initial solutions. MSLNS then iteratively improves these solutions using a softmax-biased destroy operator that prioritizes requests appearing frequently in the learned trajectories, combined with an increasing destroy size schedule for balancing intensification and diversification. The method is evaluated on synthetic PDSTSP instances with up to 40 nodes, demonstrating superior performance to both pure learning and pure metaheuristic baselines under strict latency constraints.

## Key Results
- Achieves <2% optimality gap vs exact Gurobi baseline on tested instances
- Outperforms state-of-the-art neural (AM) and metaheuristic (SA) baselines in both solution quality and runtime
- Maintains sub-second latency (0.5-10s scaling with instance size) suitable for real-time freight bundling
- First demonstration that learned constructor can reliably seed multi-start LNS for combinatorial optimization

## Why This Works (Mechanism)

### Mechanism 1: Learned Initialization Positions Search in High-Quality Attraction Basins
- Claim: A Transformer-based policy-gradient constructor generates initial solutions that are closer to high-quality local optima than random or greedy starts, reducing the search effort required by the improvement heuristic.
- Mechanism: The DNN is trained via REINFORCE (POMO) to maximize collected revenue. The policy learns to prioritize requests that contribute to high-revenue, feasible routes. The resulting solutions, while not necessarily local optima themselves, statistically fall within the attraction basins of superior local optima (i.e., within a small k-neighborhood of a high-quality solution). This allows the downstream Large Neighborhood Search (LNS) to reach a better solution with a smaller and cheaper destroy size (k).
- Core assumption: The structure of the attraction basins is such that high-quality solutions cluster together, and a learned policy can capture this structure better than a myopic greedy heuristic.
- Evidence anchors:
  - [abstract] "...neural constructor generates high-quality initial solutions that position the search close to promising attraction basins..."
  - [section 4.1] "...neural constructor is empirically shown to rapidly produce high-quality seeds that position the search close to promising basins, enabling efficient improvement."
  - [section 5.2, Fig. 4] "Hybrid methods exhibit significantly stronger basin alignment than classical metaheuristics."
  - [corpus] Related work (Learning with Local Search MCMC Layers) discusses integrating combinatorial optimization layers, providing context for learned components in search pipelines. Reinforcement Learning Methods for Neighborhood Selection in Local Search supports the use of RL to improve search.
- Break condition: If the learned policy overfits to the training distribution, it may generate solutions in poor basins for out-of-distribution instances, causing the hybrid method to perform worse than a robust, general-purpose heuristic like Simulated Annealing initialized greedily.

### Mechanism 2: Multi-Start Strategy Explores Diverse High-Quality Basins
- Claim: Generating multiple diverse initial solutions from the learned policy and refining them in parallel increases the probability of finding the global optimum or a near-optimal solution compared to a single-start approach.
- Mechanism: The POMO training framework naturally produces a policy that can generate multiple, diverse trajectories from different start nodes. These diverse initial solutions are then refined by a Multi-Start LNS (MSLNS). This parallel exploration allows the search to probe multiple high-quality attraction basins simultaneously, mitigating the risk of getting trapped in a single, potentially suboptimal, basin.
- Core assumption: The problem's search space has multiple, scattered high-quality local optima, and no single starting point reliably leads to the best one.
- Evidence anchors:
  - [abstract] "...Multi-Start Large Neighborhood Search (MSLNS) metaheuristic..."
  - [section 4.4] "...multi-start strategies accelerate convergence to superior local optima... Starting from multiple diverse and high-quality initial solutions can mitigate this issue by probing multiple high-quality basins in parallel."
  - [section 5.3] "Multi-start also helps: Single-start and 2-Opt+LNS drops notably (mean<4.700)."
  - [corpus] ParBalans discusses parallelization for accelerating LNS, which aligns with the multi-start concept.
- Break condition: If the diverse starting points generated by the policy are not of high quality (i.e., they lie in low-quality basins), the multi-start approach will waste computational resources refining poor solutions.

### Mechanism 3: Softmax-Biased Removal and Adaptive Destroy Size Balance Diversification and Intensification
- Claim: Using information from the learned trajectories (request frequency) to bias the destroy operator, combined with an increasing destroy size, allows the search to more effectively navigate the solution space.
- Mechanism: MSLNS uses a softmax function over the frequency of requests across all neural trajectories to select which requests to remove from the current solution. Requests that appear more often are considered "backbone" and are prioritized for removal. This perturbs the most stable (and potentially critical) parts of the current solution, fostering diversification. An increasing destroy size schedule further enables transitions between different attraction basins. The process is intensified by a greedy repair operator that finds the best re-insertion.
- Core assumption: The frequency of a request in the neural trajectories is a proxy for its structural importance or substitutability in high-quality solutions.
- Evidence anchors:
  - [abstract] "...MSLNS refines them using softmax-biased removal and adaptive destroy sizes for effective diversification and intensification."
  - [section 4.4] "...we design MSLNS with the following components: (i) destroy operator with softmax-biased removals... (ii) an increasing destroy-size schedule..."
  - [section 5.3] "Fixing k or replacing softmax with uniform removal degrades quality..."
  - [corpus] SPL-LNS and Solving the Pod Repositioning Problem with Deep Reinforced Adaptive Large Neighborhood Search explore related ideas of using neural networks to guide or enhance LNS destroy/repair decisions.
- Break condition: If the request frequency does not correlate with structural importance, or if the initial solutions are of poor quality, the biased removal may be counterproductive, leading to worse performance than random removal.

## Foundational Learning

- Concept: **Attraction Basins**
  - Why needed here: This is the core theoretical lens through which the authors explain why high-quality initialization works. Understanding that an improvement heuristic converges to the attractor within whose basin it starts is essential.
  - Quick check question: If an improvement heuristic can only perform k=1 moves (removing and re-inserting one request), can it escape from the attraction basin of a local optimum? (Answer: No, it can only converge to a 1-attractor).

- Concept: **Large Neighborhood Search (LNS)**
  - Why needed here: The improvement phase of the hybrid pipeline. It works by iteratively destroying part of a solution and repairing it.
  - Quick check question: What is the trade-off between the destroy size (k) and the quality of the solution found? (Answer: Larger k allows for bigger jumps in the search space, potentially escaping a local optimum's basin, but is computationally more expensive per iteration).

- Concept: **Policy Gradient / REINFORCE**
  - Why needed here: The training method for the Transformer-based constructor. It optimizes the policy by encouraging actions that lead to higher cumulative rewards (revenue).
  - Quick check question: In the POMO framework, how is the baseline for variance reduction typically computed? (Answer: As a shared baseline, often the mean reward over multiple trajectories from the same instance, as seen in the paper's Eq. 10).

## Architecture Onboarding

- Component map: Transformer Encoder -> Transformer Decoder (Constructor) -> POMO Trainer -> MSLNS Improver -> CountUnique -> Destroy -> Repair -> 2-Opt

- Critical path:
  1. Instance Generation: Create random OFEX instances (nodes, demands, revenues, capacity, route limit).
  2. Training: Train the Transformer model using POMO on these instances.
  3. Inference - Construction: For a new instance, run the trained policy with multiple start nodes to get M initial solutions.
  4. Inference - Improvement: Run MSLNS on the M solutions, using the softmax-biased removal, increasing k, and greedy repair until a time budget is met.

- Design tradeoffs:
  - Constructor vs. Improver Compute: More powerful, slower improver vs. faster, simpler improver. The paper finds a lightweight LNS on top of a strong DNN constructor is effective.
  - Masking Accuracy vs. Training Speed: Exact feasibility masking during training is NP-hard. The authors use an approximate lower bound surrogate (Eq. 6) and a penalty in the reward function (Eq. 7) to trade off accuracy for tractability.
  - Destroy Size (k): Small k is for intensification; large k is for diversification. The adaptive schedule attempts to get the best of both.

- Failure signatures:
  - Infeasible solutions during training: This indicates the approximate masking is too loose. Increase the penalty coefficient ρ or tighten the lower bound in the mask.
  - MSLNS does not improve upon the constructor: This suggests the constructor is already finding a strong local optimum that the improver cannot escape. Increase k_max or check if the removal bias is working correctly.
  - Poor generalization to larger instances: This is a common failure mode for NCO. The paper notes performance drops when evaluating on larger instances than trained on. Solution: Train on a distribution that includes larger sizes or use curriculum learning.

- First 3 experiments:
  1. Validate Constructor: Train the POMO model and evaluate its performance on a held-out test set against a greedy baseline. Confirm it produces higher-revenue solutions.
  2. Ablate MSLNS Components: Run MSLNS with softmax removal, uniform removal, fixed k, and adaptive k to quantify the contribution of each proposed component (replicate Fig. 5).
  3. End-to-End Comparison: Compare the full POMO+MSLNS pipeline against the best metaheuristic (SA) and neural (AM) baselines across different time budgets to establish the new Pareto frontier (replicate Fig. 6).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the proposed hybrid pipeline maintain sub-second latency and high solution quality when applied to real-world Online Freight Exchange (OFEX) instances that include time windows?
- Basis in paper: [explicit] The authors state in the Conclusion: "We also plan to validate the algorithm on real-world OFEX instances with additional operational constraints (e.g., time windows), quantifying latency, stability, and improvements in utilization and revenue."
- Why unresolved: The current work relies on synthetic benchmarks generated via uniform distributions and explicitly excludes time windows from the m1-PDSTSP formulation, focusing instead on capacity and route-length constraints.
- What evidence would resolve it: Performance metrics (optimality gap, winning rate, and wall-clock time) obtained from testing the POMO+MSLNS pipeline on industrial datasets containing hard time windows.

### Open Question 2
- Question: Can hierarchical region decomposition and distributed Large Neighborhood Search (LNS) effectively preserve sub-second response times when scaling to very large snapshots?
- Basis in paper: [explicit] The Conclusion identifies a key limitation: "under bursty arrivals or very large instances, latency escalates and the method's advantage over simpler heuristics diminishes." It proposes future work to investigate "scaling our hybrid pipeline via hierarchical (e.g., region) decomposition and distributed LNS."
- Why unresolved: The current centralized MSLNS architecture faces latency bottlenecks on large instances, and it is untested whether the "learning-seeded" approach can be effectively parallelized or decomposed without losing the global optimality benefits of the Transformer constructor.
- What evidence would resolve it: A modified architecture running on distributed hardware that achieves sub-second latency on instances significantly larger than PDSTSP122 while maintaining a low optimality gap.

### Open Question 3
- Question: To what extent does the reliance on fixed-budget rolling-horizon snapshots degrade performance under highly dynamic market conditions, such as bursty arrival patterns?
- Basis in paper: [explicit] The Conclusion highlights this as a limitation: "A key limitation is our reliance on rolling-horizon snapshots with fixed per-snapshot budgets: under bursty arrivals... latency escalates and the method's advantage over simpler heuristics diminishes, thereby limiting deployability at scale."
- Why unresolved: The paper evaluates static snapshots and does not simulate the "bursty" or streaming nature of real-time markets where the "freeze-and-solve" paradigm might fail to react quickly enough or might waste computation on obsolete data.
- What evidence would resolve it: A simulation study of a dynamic OFEX environment with bursty request arrivals, comparing the rolling-horizon approach against continuous or reactive optimization methods in terms of final accepted revenue and constraint violations.

## Limitations

- Scalability: Performance degrades when evaluated on larger instances than those used in training (n=40 vs n≤20), a common challenge for neural combinatorial optimization approaches.
- Hyperparameter Sensitivity: Critical parameters like the penalty coefficient ρ for constraint violations and the softmax temperature α are not fully explored across problem scales, suggesting potential sensitivity to problem size.
- Generalization: The method is tailored specifically to the freight bundling context with its particular constraint structure, limiting direct applicability to other combinatorial optimization problems without significant adaptation.

## Confidence

**High Confidence Claims:**
- The hybrid POMO+MSLNS pipeline achieves superior performance compared to both pure metaheuristics and pure learning approaches on the tested PDSTSP instances
- The softmax-biased removal strategy with adaptive destroy sizes provides measurable quality improvements over uniform or fixed-size alternatives

**Medium Confidence Claims:**
- The learned constructor consistently positions search in promising attraction basins (supported by empirical evidence but limited theoretical analysis of basin structure)
- The multi-start strategy provides meaningful diversification benefits (demonstrated but could benefit from more rigorous analysis of solution diversity metrics)

**Low Confidence Claims:**
- The specific hyperparameter values (ρ, α) are optimal across different instance scales
- The approach will maintain the same performance advantages when scaled to significantly larger problem instances

## Next Checks

1. **Robustness Testing**: Evaluate the approach on a wider range of instance sizes (e.g., n=50, 80, 100) to quantify performance degradation and identify the practical scalability limits.

2. **Hyperparameter Sensitivity Analysis**: Systematically vary the penalty coefficient ρ and softmax temperature α across different instance scales to establish their impact on solution quality and runtime.

3. **Cross-Problem Transferability**: Adapt the methodology to a related combinatorial optimization problem (e.g., vehicle routing with time windows) to assess the generality of the learned initialization and destruction strategies.