---
ver: rpa2
title: 'CDT: A Comprehensive Capability Framework for Large Language Models Across
  Cognition, Domain, and Task'
arxiv_id: '2509.24422'
source_url: https://arxiv.org/abs/2509.24422
tags:
- data
- capability
- capabilities
- framework
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces CDT, a comprehensive framework that evaluates
  large language models (LLMs) across three orthogonal dimensions: Cognition, Domain,
  and Task. The Cognition dimension incorporates the Cattell-Horn-Carroll theory,
  identifying 18 core cognitive abilities; the Domain dimension organizes 33 subdomains;
  and the Task dimension categorizes 16 task types.'
---

# CDT: A Comprehensive Capability Framework for Large Language Models Across Cognition, Domain, and Task

## Quick Facts
- arXiv ID: 2509.24422
- Source URL: https://arxiv.org/abs/2509.24422
- Reference count: 40
- Primary result: Framework evaluating LLMs across Cognition, Domain, and Task dimensions improves data selection and model performance

## Executive Summary
CDT introduces a comprehensive framework for evaluating large language models across three orthogonal dimensions: Cognition, Domain, and Task. The framework incorporates 18 cognitive abilities based on Cattell-Horn-Carroll theory, 33 domain subdomains, and 16 task types. By developing specialized tag models for each dimension to annotate instruction datasets, CDT enables both dataset evaluation and data selection applications. Experimental results demonstrate that CDT metrics correlate well with downstream performance and guide effective dataset construction, achieving improved model performance on both general and specific benchmarks.

## Method Summary
The CDT framework systematically evaluates LLMs through three orthogonal dimensions. For Cognition, it maps 18 core cognitive abilities derived from the Cattell-Horn-Carroll theory. The Domain dimension organizes 33 subdomains spanning various knowledge areas. The Task dimension categorizes 16 different task types that LLMs commonly encounter. The authors develop specialized tag models for each dimension to annotate instruction datasets, enabling quantitative assessment of model capabilities across these dimensions. These annotated datasets are then used for both evaluating existing datasets and selecting optimal data for model training, with the goal of improving downstream performance through targeted data curation.

## Key Results
- CDT-guided data selection improved model performance on general benchmarks to 44.3 (1.6 points above baseline)
- Specific benchmark performance reached 45.4 (2.2 points above baseline)
- Tag models achieved high accuracy rates (98.3% for Cognition, 94.6% for Domain) on validation sets

## Why This Works (Mechanism)
CDT works by providing a structured, multidimensional approach to understanding and evaluating LLM capabilities. By decomposing model evaluation into Cognition, Domain, and Task dimensions, the framework captures nuanced aspects of model performance that single-dimensional approaches miss. The specialized tag models enable systematic annotation of instruction datasets, creating a rich semantic understanding of what capabilities each data point targets. This granular understanding allows for more effective data selection and dataset construction, as models can be trained on data that specifically targets their weaknesses across different cognitive abilities, domains, and task types.

## Foundational Learning

Cattell-Horn-Carroll Theory
- Why needed: Provides scientific basis for decomposing cognitive abilities into measurable components
- Quick check: Verify 18 cognitive abilities map to established CHC theory constructs

Instruction Dataset Annotation
- Why needed: Enables systematic evaluation of data quality and relevance across multiple dimensions
- Quick check: Ensure tag models maintain high accuracy across diverse dataset types

Multidimensional Evaluation Framework
- Why needed: Captures complex interactions between cognitive abilities, knowledge domains, and task types
- Quick check: Validate orthogonality of the three dimensions through correlation analysis

## Architecture Onboarding

Component Map: Tag Models (Cognition, Domain, Task) -> Dataset Annotation -> Evaluation Metrics -> Data Selection Pipeline -> Model Training

Critical Path: The core workflow involves tag models annotating instruction datasets, generating CDT metrics, using these metrics for dataset evaluation or data selection, and ultimately improving model performance through targeted training data curation.

Design Tradeoffs: The framework prioritizes comprehensive coverage across three dimensions over simplicity, accepting increased complexity in annotation and evaluation for more nuanced capability assessment. This trade-off enables more targeted improvements but requires more sophisticated infrastructure.

Failure Signatures: Poor tag model performance on specific subdomains or cognitive abilities would manifest as inaccurate CDT metrics, leading to suboptimal data selection. Overfitting of tag models to validation sets could result in poor generalization to real-world datasets.

First 3 Experiments:
1. Evaluate tag model accuracy on diverse, multilingual instruction datasets to assess generalization
2. Conduct ablation studies removing each CDT dimension to isolate their individual contributions
3. Benchmark CDT against HELM and BIG-bench to establish relative performance in real-world scenarios

## Open Questions the Paper Calls Out
None

## Limitations

The semantic accuracy of tag models remains uncertain, as reported accuracy figures are based on limited validation sets. The framework's performance on multilingual and culturally diverse datasets has not been validated, restricting its global applicability. Experimental comparisons lack transparency regarding exact baseline methods and implementation details, making it difficult to assess the true magnitude of improvements.

## Confidence

High confidence in the conceptual framework and theoretical grounding of CDT across the three dimensions (Cognition, Domain, Task).
Medium confidence in the reported experimental results, given the limited detail on baseline methods and validation procedures.
Low confidence in the generalizability of the tag models and the framework's performance on non-English or highly specialized datasets.

## Next Checks

1. Conduct a large-scale, cross-lingual evaluation of CDT tag models to assess their robustness and accuracy across diverse languages and cultural contexts.
2. Perform ablation studies to isolate the contribution of each CDT dimension (Cognition, Domain, Task) to downstream performance, clarifying which aspects drive improvements.
3. Benchmark CDT against a broader set of established LLM evaluation frameworks (e.g., HELM, BIG-bench) to establish its relative strengths and weaknesses in real-world scenarios.