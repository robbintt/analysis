---
ver: rpa2
title: 'One Instruction Does Not Fit All: How Well Do Embeddings Align Personas and
  Instructions in Low-Resource Indian Languages?'
arxiv_id: '2601.10205'
source_url: https://arxiv.org/abs/2601.10205
tags:
- retrieval
- languages
- cross-lingual
- across
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces the first benchmark for evaluating persona-instruction
  alignment across 12 Indian languages in both monolingual and cross-lingual settings.
  The benchmark includes four tasks: monolingual retrieval, cross-lingual retrieval,
  reverse retrieval, and binary compatibility classification.'
---

# One Instruction Does Not Fit All: How Well Do Embeddings Align Personas and Instructions in Low-Resource Indian Languages?

## Quick Facts
- arXiv ID: 2601.10205
- Source URL: https://arxiv.org/abs/2601.10205
- Reference count: 18
- Primary result: Introduces first benchmark for persona-instruction alignment across 12 Indian languages; E5-Large-Instruct achieves 27.4% Recall@1 monolingual and 20.7% cross-lingual.

## Executive Summary
This paper introduces the first benchmark for evaluating persona-instruction alignment across 12 Indian languages in both monolingual and cross-lingual settings. The benchmark includes four tasks: monolingual retrieval, cross-lingual retrieval, reverse retrieval, and binary compatibility classification. Eight multilingual embedding models were evaluated in a frozen-encoder setting. Results show that no single model dominates all tasks, with cross-lingual performance dropping approximately 25% relative to monolingual retrieval. Script-family alignment yields 6.7 percentage points higher transfer than cross-script pairs.

## Method Summary
The benchmark consists of 50K persona-instruction pairs synthesized via GPT-4o-mini and translated to 12 Indian languages using NLLB-200 (3.3B), creating 600K total records. Human validation on 150 pairs per language confirms compatibility preservation. Four evaluation tasks are defined: monolingual retrieval (persona→instruction), cross-lingual retrieval (different languages/scripts), reverse retrieval (instruction→persona), and binary compatibility classification. Eight multilingual embedding models are evaluated in frozen-encoder mode with logistic regression head for classification. Hardware: RTX A6000 (48GB). Software: Python 3.10, PyTorch 2.0, Transformers 4.35, Sentence-Transformers 2.2. Random seed: 42.

## Key Results
- E5-Large-Instruct achieves highest Recall@1 of 27.4% on monolingual retrieval and 20.7% on cross-lingual transfer
- BGE-M3 leads reverse retrieval at 32.1% Recall@1
- LaBSE attains 75.3% AUROC for classification with strong calibration
- Cross-lingual performance drops approximately 25% relative to monolingual retrieval
- Script-family alignment yields 6.7 percentage points higher transfer than cross-script pairs

## Why This Works (Mechanism)

### Mechanism 1: Instruction-Tuning Enhances Compatibility Encoding
Models trained with instruction-aware objectives produce embeddings that better capture functional compatibility between personas and instructions. Instruction-tuning aligns embedding space geometry with task-relevant relationships through contrastive learning on paired instruction-response data. Core assumption: persona-instruction compatibility is a functional relationship distinct from general semantic similarity. Evidence: E5-Large-Instruct outperforms non-instruction-tuned models in retrieval tasks. Break condition: If compatibility is purely semantic, instruction-tuning provides no advantage.

### Mechanism 2: Script-Family Alignment Reduces Cross-Lingual Transfer Gap
Cross-lingual retrieval performance degrades less when source and target languages share a script or belong to the same script family. Models with shared subword vocabularies across scripts have overlapping token representations, reducing distance between embeddings. Core assumption: Subword tokenization creates script-specific embedding neighborhoods. Evidence: 6.7 percentage point advantage for script-family pairs. Break condition: If models use character-level or language-agnostic tokenization, script-family effects disappear.

### Mechanism 3: Retrieval Direction Asymmetry Reflects Embedding Space Structure
Model performance differs between forward and reverse retrieval due to asymmetric embedding space topology. Some models create spaces where instructions cluster around persona centroids, optimizing reverse retrieval. Core assumption: Embedding space geometry is not direction-agnostic. Evidence: BGE-M3 outperforms E5 in reverse retrieval but not forward retrieval. Break condition: If retrieval is inherently symmetric, model rankings would be consistent across directions.

## Foundational Learning

- **Multilingual Sentence Embeddings**: Understanding how multilingual encoders create shared embedding spaces is essential to interpret cross-lingual transfer results. Quick check: Can you explain why two semantically similar sentences in different languages might have different cosine similarities in a multilingual embedding model?

- **Cross-Lingual Transfer and Script Diversity**: Grasping how script differences interact with tokenization and pretraining data is essential to understand the 6.7 percentage point gap. Quick check: Why might a multilingual model trained on Hindi and Marathi transfer better between them than between Hindi and Tamil?

- **Retrieval Evaluation Metrics**: Fluency in Recall@k, MRR, AUROC, and ECE is required to assess model tradeoffs. Quick check: If a model achieves 75% AUROC but 0.5 ECE, what does that imply about its ranking quality versus probability calibration?

## Architecture Onboarding

- **Component map**: Dataset Pipeline (GPT-4o-mini synthesis → NLLB-200 translation → Human validation) → Evaluation Framework (4 tasks with frozen encoders + logistic head) → Model Zoo (8 multilingual embedding models)

- **Critical path**: Data quality depends on synthesis and translation fidelity; human validation confirms compatibility. Frozen-encoder protocol isolates representation quality but provides lower-bound performance. Task-specific model selection: E5 for cross-lingual retrieval, BGE-M3 for reverse retrieval, LaBSE for calibrated classification.

- **Design tradeoffs**: Synthetic data enables scale but may not capture real-world diversity; human validation mitigates but doesn't eliminate this risk. Frozen encoders ensure reproducibility but likely underperform fine-tuned models. Language coverage varies across models (e.g., E5 excludes Assamese).

- **Failure signatures**: Cross-script transfer degradation (25% drop), overconfidence in classification (high ECE), directional asymmetry mismatch between deployment direction and model choice.

- **First 3 experiments**: 1) Baseline replication with E5-Large-Instruct and LaBSE on 10K subset to verify Recall@1 and AUROC. 2) Script-family probe: isolate Hindi-Marathi vs. Hindi-Tamil pairs in cross-lingual retrieval. 3) Calibration ablation: apply histogram binning to T4 outputs for all models.

## Open Questions the Paper Calls Out

- To what extent does supervised fine-tuning on Indic persona-instruction pairs improve benchmark performance over frozen-encoder baselines? The study intentionally restricted methodology to frozen encoders to isolate existing representation quality, leaving adaptation potential unexplored.

- Do observed model rankings and script-transfer patterns persist when evaluated on organic, human-authored persona-instruction pairs rather than synthetic data? The entire benchmark relies on GPT-4o-mini synthesis, making it unclear if learned "compatibility" reflects genuine user intent or synthetic artifacts.

- Can calibration techniques or model ensembling resolve the trade-off where high-retrieval models exhibit poor calibration while well-calibrated models show lower discriminative performance? The paper identifies this inverse relationship but doesn't test methods to unify these strengths.

## Limitations

- Benchmark relies on synthetic data that may not capture full diversity of real user behaviors
- Unaccounted factors in cross-lingual transfer (pretraining data coverage, transliteration frequency)
- Frozen-encoder setting provides lower-bound performance estimates that don't reflect fine-tuned capabilities

## Confidence

- **High confidence** in model rankings and task-specific performance patterns (Recall@k, MRR, AUROC, ECE provide robust comparative evidence)
- **Medium confidence** in mechanism explanations (supported by patterns and related work but limited direct corpus evidence)
- **Low confidence** in absolute performance numbers as real-world utility indicators (frozen-encoder setting, synthetic data, lack of fine-tuning)

## Next Checks

1. **Real-world data validation**: Collect and evaluate the benchmark on naturally occurring persona-instruction pairs from actual conversational AI systems. Compare synthetic vs. real data performance degradation.

2. **Pretraining data ablation**: Analyze pretraining corpus statistics for each language across models. Correlate pretraining token counts with zero-shot performance, particularly for low-resource languages.

3. **Fine-tuning impact study**: Fine-tune E5-Large-Instruct on 1K-10K persona-instruction pairs for one low-resource language pair. Measure performance gains relative to frozen-encoder baseline.