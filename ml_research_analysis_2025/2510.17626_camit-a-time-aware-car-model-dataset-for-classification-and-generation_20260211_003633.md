---
ver: rpa2
title: 'CaMiT: A Time-Aware Car Model Dataset for Classification and Generation'
arxiv_id: '2510.17626'
source_url: https://arxiv.org/abs/2510.17626
tags:
- temporal
- data
- images
- learning
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CaMiT, a time-aware dataset for fine-grained
  car model classification and generation. CaMiT includes 787K labeled and 5.1M unlabeled
  images spanning 2005-2023, capturing temporal shifts in car model appearances.
---

# CaMiT: A Time-Aware Car Model Dataset for Classification and Generation

## Quick Facts
- **arXiv ID**: 2510.17626
- **Source URL**: https://arxiv.org/abs/2510.17626
- **Reference count**: 40
- **Primary result**: Introduces CaMiT, a time-aware car model dataset demonstrating that static in-domain pretraining and time-incremental classifier learning achieve strong performance under temporal distribution shifts.

## Executive Summary
This paper introduces CaMiT, a time-aware dataset for fine-grained car model classification and generation. CaMiT includes 787K labeled and 5.1M unlabeled images spanning 2005-2023, capturing temporal shifts in car model appearances. The authors demonstrate that static pretraining on domain-specific data achieves competitive performance compared to large-scale generic models, yet accuracy degrades when tested across different years. To address this, they evaluate time-incremental classification strategies, showing that updating only the classifier (TICL) provides the best temporal robustness. Additionally, they propose time-aware image generation, which improves output realism by incorporating temporal metadata. CaMiT offers a rich benchmark for studying temporal adaptation in fine-grained visual recognition and generation tasks.

## Method Summary
The CaMiT dataset was curated through a semi-automatic pipeline: images were scraped from Flickr, filtered using YOLOv11x for car detection and CLIP embeddings for deduplication, and labeled using vision-language models and classifiers. For classification, models were pretrained on a 2005-2007 subset and evaluated under time-incremental settings, updating either the entire backbone (TIP) or only the classifier (TICL). For generation, Stable Diffusion 1.5 was fine-tuned with temporal captions to produce year-specific car images. Evaluation metrics included temporal accuracy (forward, backward, cross-year) and generation quality via KID and classifier accuracy on generated images.

## Key Results
- Static pretraining on in-domain data achieves competitive performance with large-scale generalist models while being more resource-efficient.
- Time-incremental classifier learning (TICL) provides the best temporal robustness, with improvements over TIP reaching 8.1 and 11.3 points.
- Time-aware image generation improves output realism by incorporating temporal metadata, yielding lower KID and higher accuracy on generated images.

## Why This Works (Mechanism)

### Mechanism 1: In-Domain Pretraining Efficiency
Specialized pretraining on domain-specific data (cars) may achieve competitive performance with large-scale generalist models (CLIP, DINOv2) while being more resource-efficient. Pretraining on a curated, narrow domain allows the model to learn fine-grained features that are diluted or absent in massive, generic datasets.

### Mechanism 2: Time-Incremental Classifier Learning (TICL) Stability
Freezing the backbone and updating only the classification layer likely provides better temporal robustness and backward transfer than updating the entire backbone. By freezing the feature extractor, the model preserves the structural knowledge of previous car designs while allowing decision boundaries to adjust to new data distributions.

### Mechanism 3: Temporal Conditioning in Generation
Incorporating explicit temporal metadata into image generation captions likely improves output realism and alignment with real-world distributions. Conditioning on the "Year" forces the model to disentangle the visual appearance of the object from the temporal context, reducing the "averaging" of designs across time.

## Foundational Learning

- **Concept**: **Catastrophic Forgetting (in Continual Learning)**
  - **Why needed here**: The paper's core problem is that models trained on new car years lose the ability to recognize older models.
  - **Quick check question**: If I fine-tune a model on 2023 cars only, what happens to its accuracy on 2010 cars?

- **Concept**: **Fine-Grained Visual Categorization (FGVC)**
  - **Why needed here**: Standard classification datasets distinguish "car" vs. "truck," but CaMiT requires distinguishing "Car Model A 2015" vs. "Car Model A 2016."
  - **Quick check question**: Does the model need to see the whole car to identify it, or are specific parts (lights, badges) sufficient?

- **Concept**: **Kernel Inception Distance (KID) / FrÃ©chet Inception Distance (FID)**
  - **Why needed here**: To evaluate the quality and realism of generated images relative to real distributions.
  - **Quick check question**: Does a lower KID score mean the image is sharper, or just more statistically similar to the training set?

## Architecture Onboarding

- **Component map**: Flickr API -> Filters (YOLO, SAM2) -> Labelers (VLMs, DeiT) -> Labeled/Unlabeled Splits -> Model Core (ViT-S/B backbone) -> Adapters (LoRA) -> Head (Classifier)

- **Critical path**:
  1. **Dataset Curation**: Apply the semi-automatic pipeline to ensure temporal labels match visual content.
  2. **Baseline Setup**: Establish a Static Pretraining baseline to quantify the "temporal drop."
  3. **Mitigation**: Implement Time-Incremental Classifier Learning using a frozen backbone and a non-parametric classifier.

- **Design tradeoffs**:
  - **TIP vs. TICL**: TIP is computationally heavy and risks destabilizing past knowledge; TICL is cheap and stable but cannot learn new low-level features.
  - **Generalist vs. Specialist**: Generalist models require massive data; Specialist models are efficient but may lack robustness to out-of-domain noise.

- **Failure signatures**:
  - **Temporal Obsolescence**: High accuracy on $T_{train}$, rapid decay on $T_{test} > T_{train}$.
  - **Backward Collapse**: Accuracy drops to near-zero on $T_{test} < T_{train}$ (Catastrophic Forgetting).
  - **Anachronistic Generation**: The generator produces cars with mixed design cues from different years.

- **First 3 experiments**:
  1. **Baseline Temporal Decay**: Train a standard classifier on 2007-2012 data and evaluate on 2013-2023 to measure the "natural" degradation rate.
  2. **Classifier Update (TICL)**: Freeze the backbone and incrementally update a RanPAC classifier with new yearly data. Check if backward accuracy is maintained.
  3. **Generation Fidelity**: Fine-tune Stable Diffusion with temporal captions and generate samples for a fixed year. Classify them with a standard model to see if they are recognized as the correct year.

## Open Questions the Paper Calls Out

### Open Question 1
Can temporal models be trained to disentangle visual shifts caused by new model variants (design evolution) from the physical aging of older vehicles? The authors note that current temporal annotations conflate genuine design evolution with physical aging, and suggest future work could disentangle these factors.

### Open Question 2
Does the temporal visual drift observed in CaMiT generalize to other data sources or platforms beyond Flickr? The authors raise a "generalization question" regarding the use of Flickr as the sole data source, noting potential selection and geographic biases.

### Open Question 3
Can more sophisticated conditioning mechanisms further improve the temporal fidelity of generated images compared to simple text-based prompting? The paper only demonstrates time-aware generation by appending the year to the caption, leaving complex architectural conditioning unexplored.

## Limitations

- The dataset focuses on a single object category (cars) with clear visual progression, which may limit the generalizability of the approaches to domains with less predictable temporal shifts.
- The semi-automatic labeling pipeline introduces potential noise, and no human validation statistics are provided for the final labeled set.
- Absolute compute comparisons are absent, so the resource efficiency claims rest on relative dataset size rather than actual training cost metrics.

## Confidence

- **High Confidence**: The core findings on temporal degradation in static classifiers and the efficacy of TICL are well-supported by quantitative results.
- **Medium Confidence**: The claim that in-domain pretraining matches generalist model performance is plausible but depends on specific conditions.
- **Low Confidence**: The long-term stability of TICL under continuous incremental updates and the generation model's ability to generalize to unseen models or years are not tested.

## Next Checks

1. **Label Quality Audit**: Manually validate a random 1% sample of the labeled dataset to estimate annotation accuracy and identify systematic errors.
2. **Cross-Domain Temporal Transfer**: Apply the TICL approach to a non-automotive FGVC dataset with temporal splits to test domain generalization.
3. **Compute Efficiency Benchmarking**: Measure and compare the total GPU hours and energy consumption for training MoCo v3 on CaMiT versus CLIP on LAION-400M, controlling for backbone size and epochs.