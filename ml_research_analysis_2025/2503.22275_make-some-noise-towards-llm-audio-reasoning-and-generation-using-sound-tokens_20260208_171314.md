---
ver: rpa2
title: 'Make Some Noise: Towards LLM audio reasoning and generation using sound tokens'
arxiv_id: '2503.22275'
source_url: https://arxiv.org/abs/2503.22275
tags:
- audio
- arxiv
- generation
- language
- speech
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of integrating audio comprehension
  and generation into large language models (LLMs) by proposing a novel approach that
  combines Variational Quantization with Conditional Flow Matching to convert audio
  into ultra-low bitrate discrete tokens (0.23kbps). The authors fine-tuned a pretrained
  text-based LLM using Low-Rank Adaptation (LoRA) to assess its effectiveness in achieving
  true multimodal capabilities.
---

# Make Some Noise: Towards LLM audio reasoning and generation using sound tokens

## Quick Facts
- arXiv ID: 2503.22275
- Source URL: https://arxiv.org/abs/2503.22275
- Authors: Shivam Mehta; Nebojsa Jojic; Hannes Gamper
- Reference count: 40
- Primary result: Novel audio tokenizer converts audio to ultra-low bitrate discrete tokens (0.23kbps) enabling unified comprehension and generation in LLMs

## Executive Summary
This paper addresses the challenge of integrating audio comprehension and generation into large language models (LLMs) by proposing a novel approach that combines Variational Quantization with Conditional Flow Matching to convert audio into ultra-low bitrate discrete tokens (0.23kbps). The authors fine-tuned a pretrained text-based LLM using Low-Rank Adaptation (LoRA) to assess its effectiveness in achieving true multimodal capabilities. Their audio tokenizer outperforms traditional VQ-VAE across various datasets with diverse acoustic events. Despite substantial loss of fine-grained details through audio tokenization, the multimodal LLM trained with discrete tokens achieves competitive results in audio comprehension with state-of-the-art methods, though audio generation remains poor. The work highlights the need for larger, more diverse datasets and improved evaluation metrics to advance multimodal LLM performance.

## Method Summary
The approach converts audio into discrete tokens using a variational quantization framework with Conditional Flow Matching (CFM), enabling seamless integration with text tokens in LLMs. The tokenizer employs a frozen VAE encoder (from Stable Audio Open) to map 10-second stereo audio at 44.1kHz to latent representations, which are then encoded by a causal transformer and quantized using an 8196-entry codebook. A Diffusion Transformer (DiT) decoder with CFM reconstructs audio from tokens at 0.23 kbps. For multimodal integration, the authors fine-tune a Vicuna 1.5 7b LLM with LoRA adapters (rank=64, alpha=128) on all layers, resizing the embedding layer for audio tokens and applying 10x loss weighting on audio tokens with z-loss regularization. The system is pretrained on a caption subset plus WavCaps and FMA datasets, then fine-tuned on the full OpenAQA dataset.

## Key Results
- The CFM-based tokenizer achieves lower reconstruction error (0.6359 avg) compared to VQ-MSE (0.7004) across ESC50, VCTK, and FMA datasets
- LM-MSN achieves competitive captioning performance with 0.357 SPICE and 0.414 FENSE on Clotho, outperforming LTU baseline (0.338/0.381)
- Music genre classification accuracy reaches 61.2% on GTZAN, demonstrating effective audio-text reasoning capabilities
- Audio generation remains poor, highlighting current limitations in cross-modal generative modeling

## Why This Works (Mechanism)

### Mechanism 1
Discrete audio tokens enable unified comprehension and generation by aligning audio with the LLM's native next-token prediction paradigm. Continuous audio representations occupy smooth, high-dimensional latent spaces that conflict with LLM training on discrete token prediction. By quantizing audio into discrete tokens at ultra-low bitrate (0.23 kbps), audio and text token counts become aligned, reducing vocabulary size and temporal resolution to make transformer training computationally feasible. The information loss from ultra-low bitrate quantization retains sufficient semantic content for comprehension tasks while enabling generation capabilities.

### Mechanism 2
Conditional Flow Matching (CFM) with Diffusion Transformers outperforms MSE-based reconstruction for audio tokenization by avoiding over-averaging in complex audio domains. Traditional VQ-VAE minimizes MSE loss, which causes "over-averaging" when reconstructing from VAE latents—particularly for music where the frozen VAE was trained. CFM learns a vector field that maps probability paths between noise and data distributions using ODE-based flow, producing straighter trajectories that are easier to learn and require fewer sampling steps. This probabilistic reconstruction objective better captures multi-modal distributions in VAE latent space than deterministic MSE.

### Mechanism 3
LoRA fine-tuning with 10x loss weighting on audio tokens enables multimodal adaptation while preserving text capabilities. LoRA adds trainable low-rank matrices (rank=64, alpha=128) to all LLM layers while freezing original weights, training only ~2-3% of parameters. Audio tokens receive 10x loss weighting to accelerate learning of new vocabulary. Embedding layer is resized for audio tokens, with z-loss regularization applied. LoRA's parameter-efficient adaptation is sufficient for learning cross-modal reasoning without catastrophic forgetting.

## Foundational Learning

- **Vector Quantization (VQ-VAE)**
  - Why needed here: The paper builds on VQ-VAE by replacing its MSE decoder with CFM. Understanding the baseline quantization mechanism is essential for interpreting the claimed improvements.
  - Quick check question: Can you explain how VQ maps continuous latent vectors to discrete codebook entries and why this creates reconstruction bottlenecks?

- **Conditional Flow Matching**
  - Why needed here: The tokenizer's core innovation applies CFM to reconstruction. CFM learns vector fields that transport noise to data via ODEs.
  - Quick check question: How does CFM differ from diffusion models in terms of sampling trajectory and training objective?

- **LoRA (Low-Rank Adaptation)**
  - Why needed here: The multimodal adaptation strategy relies entirely on LoRA. Understanding rank, alpha, and their relationship to learning capacity is critical for reproducing results.
  - Quick check question: If LoRA rank is increased from 64 to 128, what tradeoffs should you expect in terms of training cost and potential overfitting?

## Architecture Onboarding

- **Component map:** Raw audio → Frozen VAE encoder → Causal encoder → VQ → Discrete tokens → Concat with text tokens → LLM with LoRA → Cross-entropy loss (10x on audio) + z-loss → Text output (comprehension) OR audio tokens → DiT decoder → VAE decoder → Reconstructed audio (generation)

- **Critical path:**
  Training: Raw audio → Frozen VAE encoder → Causal encoder → VQ → Discrete tokens → Concat with text tokens → LLM with LoRA → Cross-entropy loss (10x on audio) + z-loss
  Inference: Audio tokens + text prompt → LLM → Text output (comprehension) OR audio tokens → DiT decoder → VAE decoder → Reconstructed audio (generation)

- **Design tradeoffs:**
  1. **Bitrate vs fidelity:** 0.23 kbps enables LLM integration but sacrifices fine-grained detail, hurting generation more than comprehension.
  2. **LoRA vs full fine-tuning:** LoRA reduces compute but may limit cross-modal reasoning capacity; paper notes generation remains poor.
  3. **Causal vs non-causal decoder:** Causal encoder introduces left-to-right bias for LLM alignment; non-causal DiT decoder enables bidirectional reconstruction.

- **Failure signatures:**
  1. **Speech degradation:** VAE was trained primarily on music; speech MOS suffers (Section III, VCTK results).
  2. **Genre confusion:** Blues, country, disco, reggae misclassified as "folk" or "singer-songwriter" (Table III)—indicates dataset bias in VAE representations.
  3. **Poor generation quality:** Paper does not formally evaluate generation; samples uploaded to webpage. Likely causes: small dataset, VAE bias, LoRA capacity.

- **First 3 experiments:**
  1. **Tokenizer ablation:** Train identical architectures with VQ+FM vs VQ+MSE on AudioSet, evaluate reconstruction error on ESC50, VCTK, FMA (replicating Table I and Figure 3).
  2. **Captioning baseline:** Pretrain LM-MSN on audio captioning subset, evaluate FENSE and SPICE on Clotho and AudioCaps (replicating Table II) against LTU baseline.
  3. **Generation probe:** Generate audio from text prompts and evaluate with FAD scores against ground truth; identify failure modes by domain (speech, music, environmental sounds).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do neural scaling laws specifically apply to Low-Rank Adaptation (LoRA) when fine-tuning large language models (LLMs) for unified audio comprehension and generation?
- Basis in paper: [explicit] The authors explicitly state in Section V that "further research is needed to better understand neural scaling laws with LoRA and fine-tuning" in the context of their proposed pipeline.
- Why unresolved: The paper utilizes LoRA for efficiency but does not conduct a comparative scaling analysis against full fine-tuning or varying model sizes, leaving the parameter-efficiency trade-offs for audio modalities undefined.
- What evidence would resolve it: A series of experiments varying LLM parameter counts and LoRA ranks trained on increasing dataset sizes, comparing performance convergence rates against fully fine-tuned baselines.

### Open Question 2
- Question: Is the poor audio generation performance observed in LM-MSN primarily a result of limited dataset scale, or is it caused by inherent biases in the pretrained VAE representations?
- Basis in paper: [explicit] The authors note in Section V that "audio generation is poor" and suggest "This limitation may stem from the small dataset and bias in VAE representations," but they do not isolate the root cause.
- Why unresolved: The current experimental design conflates potential tokenizer information loss (due to the frozen VAE) with the model's lack of exposure to sufficient generative training data.
- What evidence would resolve it: An ablation study training the model on a significantly larger generative dataset while substituting the underlying VAE to measure the independent impact of data scale versus representation bias on generation fidelity.

### Open Question 3
- Question: Can improved evaluation metrics be developed to accurately assess audio captioning performance without penalizing models for vocabulary size or divergent linguistic styles?
- Basis in paper: [explicit] The authors argue in Section V that standard metrics like SPICE and FENSE "may penalize LTU's larger vocabulary," leading to a call for "improved evaluation metrics."
- Why unresolved: Current metrics may not fairly compare models with different tokenization strategies or linguistic complexities, potentially masking the actual semantic understanding capabilities of the model.
- What evidence would resolve it: The proposal and validation of a new evaluation framework that correlates strongly with human judgment while remaining invariant to vocabulary size and syntactic variation.

## Limitations

- Audio generation quality remains poor due to ultra-low bitrate quantization (0.23 kbps) sacrificing fine-grained details and potential LoRA capacity constraints
- The VAE autoencoder was trained primarily on music, creating systematic biases that degrade performance on speech and environmental sounds
- Limited dataset scale and representation bias in VAE may be conflated causes of generation failures, requiring further investigation

## Confidence

**High Confidence:**
- The discrete tokenization approach successfully enables unified audio comprehension within LLMs, as demonstrated by competitive performance on captioning and classification tasks
- The CFM-based tokenizer outperforms traditional VQ-VAE in reconstruction error across multiple datasets, with the effect being particularly pronounced on music data
- LoRA fine-tuning with 10x audio token loss weighting is effective for multimodal adaptation while preserving text capabilities, as evidenced by maintained performance on text tasks

**Medium Confidence:**
- The ultra-low bitrate (0.23 kbps) represents an optimal tradeoff between computational efficiency and semantic preservation for comprehension tasks
- The causal encoder design effectively aligns audio tokens with the LLM's next-token prediction paradigm
- The z-loss regularization during LoRA fine-tuning meaningfully contributes to cross-modal learning stability

**Low Confidence:**
- The current architecture and training approach will scale effectively to larger datasets and more complex audio generation tasks
- The observed generation limitations are primarily due to dataset constraints rather than fundamental architectural issues
- The CFM sampling efficiency and stability are sufficient for production deployment

## Next Checks

**Validation Check 1: Generation Quality Benchmarking**
Implement systematic evaluation of audio generation using FAD and human perceptual studies. Generate audio from diverse text prompts across multiple domains (speech, music, environmental sounds) and compare against ground truth using both objective metrics and MOS scoring. This will quantify the current generation gap and identify specific failure modes.

**Validation Check 2: VAE Domain Transfer Analysis**
Train the tokenizer using different VAE autoencoders (speech-focused vs music-focused) and evaluate performance across all domains. This will isolate whether the observed domain biases stem from the VAE representations or the downstream LLM training, and inform whether specialized tokenizers per domain are necessary.

**Validation Check 3: LoRA Capacity Scaling Study**
Systematically vary LoRA rank (e.g., 32, 64, 128, 256) and alpha parameters while monitoring generation quality, comprehension performance, and training stability. This will determine whether the poor generation results stem from insufficient LoRA capacity or other factors, and establish scaling relationships for future multimodal LLM designs.