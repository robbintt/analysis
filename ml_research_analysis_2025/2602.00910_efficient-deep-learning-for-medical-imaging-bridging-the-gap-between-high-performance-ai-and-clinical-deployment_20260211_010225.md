---
ver: rpa2
title: 'Efficient Deep Learning for Medical Imaging: Bridging the Gap Between High-Performance
  AI and Clinical Deployment'
arxiv_id: '2602.00910'
source_url: https://arxiv.org/abs/2602.00910
tags:
- medical
- deployment
- latency
- efficiency
- architectures
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This review synthesizes the evolution of efficient deep learning
  architectures for medical imaging, focusing on bridging the deployment gap between
  high-performance AI and clinical environments. The study categorizes modern efficient
  models into three streams: CNNs, Lightweight Transformers, and Linear Complexity
  Models (Mamba/SSMs).'
---

# Efficient Deep Learning for Medical Imaging: Bridging the Gap Between High-Performance AI and Clinical Deployment

## Quick Facts
- arXiv ID: 2602.00910
- Source URL: https://arxiv.org/abs/2602.00910
- Reference count: 40
- Primary result: CNNs achieve sub-millisecond latency (0.79 ms) on mobile NPUs; hybrid architectures approach this (0.9 ms) when tensor dimensions remain consistent.

## Executive Summary
This review synthesizes the evolution of efficient deep learning architectures for medical imaging, focusing on bridging the deployment gap between high-performance AI and clinical environments. The study categorizes modern efficient models into three streams: CNNs, Lightweight Transformers, and Linear Complexity Models (Mamba/SSMs). It examines model compression strategies including pruning, quantization, knowledge distillation, and low-rank factorization. The key finding is that theoretical FLOPs-based efficiency metrics often fail to predict real-world latency on edge devices due to memory access patterns and compiler support. Among architectures with verified mobile NPU latency, pure CNNs achieve sub-millisecond inference (0.79 ms), while carefully designed hybrids approach this efficiency (0.9 ms). The review establishes that deployment-ready medical AI requires system-level optimization beyond architectural design, emphasizing hardware-aware co-design and standardized evaluation protocols for reproducible clinical deployment.

## Method Summary
The review synthesizes existing literature on efficient deep learning architectures for medical imaging, categorizing approaches into three streams: convolutional neural networks (CNNs), lightweight transformers, and linear complexity models (Mamba/SSMs). It examines model compression techniques including pruning, quantization, knowledge distillation, and low-rank factorization. The methodology includes comprehensive benchmarking of latency and energy consumption on mobile and embedded devices, with specific focus on memory access patterns and compiler support. The study advocates for hardware-aware co-design and standardized evaluation protocols to enable reproducible clinical deployment.

## Key Results
- CNNs achieve sub-millisecond latency (0.79 ms) on mobile NPUs, while carefully designed hybrids approach this efficiency (0.9 ms).
- Theoretical FLOPs-based efficiency metrics often fail to predict real-world latency on edge devices due to memory access patterns and compiler support.
- State Space Models (SSMs) offer O(N) complexity for global context but face deployment challenges due to limited inference engine support.
- Quantization-aware training (QAT) is more reliable than post-training quantization (PTQ) for preserving rare-class sensitivity in medical imaging.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hardware-aware co-design, not FLOPs minimization, predicts real-world edge deployment efficiency.
- Mechanism: Memory access patterns and compiler operator fusion dominate latency on edge NPUs. Architectures with contiguous memory layouts and dimension-consistent operations minimize MAC overhead and maximize compiler-supported kernel fusion, translating low theoretical cost into actual speed.
- Core assumption: Edge devices are memory-bandwidth bound rather than compute-bound; inference engines can fuse supported operators.
- Evidence anchors: Theoretical FLOPs-based efficiency metrics often fail to predict real-world latency on edge devices due to memory access patterns and compiler support; MobileOne enables sub-millisecond latency on mobile devices by minimizing MAC and maximizing operator fusion.

### Mechanism 2
- Claim: Linear-complexity global context models (SSMs) can capture long-range dependencies at O(N) cost, enabling high-resolution medical imaging under strict memory budgets.
- Mechanism: State Space Models like Mamba replace O(N²) pairwise attention with O(N) selective state updates that propagate global context without explicit token-token interactions. Cross-scan and wavefront scanning preserve 2D/3D spatial coherence while maintaining linear scaling, making them viable for volumetric CT/MRI and gigapixel WSI.
- Core assumption: Medical imaging tasks benefit sufficiently from global context; scanning strategy aligns with anatomical structure.
- Evidence anchors: Mamba offers a compelling proposition for medical imaging: global information propagation with strictly linear computational complexity (O(N)); Cross-Scan Module (CSM) allows the SSM to aggregate contextual information across spatial axes without explicit attention.

### Mechanism 3
- Claim: Hybrid architectures achieve near-CNN latency only when tensor dimension consistency is preserved across the computational graph.
- Mechanism: Dimension-consistent hybrids (e.g., EfficientFormerV2) retain 4D convolutional structure throughout most layers and confine attention to late stages, minimizing reshape operations that fragment compiler graphs. This allows mobile backends to optimize the full pipeline, closing the latency gap to pure CNNs.
- Core assumption: Target inference engine supports fused 4D operators; attention blocks are small enough not to dominate runtime.
- Evidence anchors: Pure CNNs achieve sub-millisecond inference (0.79 ms), while carefully designed hybrids approach this efficiency (0.9 ms); EfficientFormerV2 prioritizes dimension-consistent design by retaining a 4D convolutional structure, minimizing reshaping overhead.

## Foundational Learning

- Concept: Memory Access Cost (MAC) vs FLOPs
  - Why needed here: The paper repeatedly shows FLOPs are poor predictors of edge latency; MAC explains why architectures like MobileViT underperform despite low FLOPs.
  - Quick check question: If model A has 50% fewer FLOPs than model B but requires 3× more memory transfers per inference, which is faster on a bandwidth-limited NPU?

- Concept: State Space Models (SSMs) and Selective Scan
  - Why needed here: Mamba-based architectures are central to the "linear revolution" for medical imaging; understanding how selective state updates replace attention is essential for evaluating SSM tradeoffs.
  - Quick check question: How does an SSM's O(N) scaling differ fundamentally from an attention mechanism's O(N²), and what does this imply for 3D volumetric segmentation?

- Concept: Post-Training Quantization (PTQ) vs Quantization-Aware Training (QAT)
  - Why needed here: The paper identifies quantization as critical for edge deployment but flags calibration risks; QAT is more reliable for medical safety-critical tasks.
  - Quick check question: Why might a model retain high AUC after INT8 PTQ yet become dangerously overconfident on rare pathology classes?

## Architecture Onboarding

- Component map: Encoder backbone -> Compression layer -> Deployment runtime -> Safety instrumentation
- Critical path: (1) Choose backbone paradigm based on task structure (local vs global context needs); (2) Profile MAC and latency on target hardware using actual inference engine; (3) Apply QAT (not just PTQ) with calibration on representative medical data; (4) Validate safety metrics (ECE, rare-class sensitivity) before deployment.
- Design tradeoffs:
  - CNN vs Hybrid vs SSM: CNNs offer lowest latency and mature toolchains; Hybrids add global context at moderate overhead; SSMs enable high-resolution scalability but have weaker compiler support.
  - Compression depth: aggressive pruning/quantization gains speed but risks rare-class sensitivity loss; always evaluate beyond aggregate metrics.
- Failure signatures:
  - FLOPs/latency mismatch: low reported FLOPs but slow inference → check MAC, operator fusion, compiler support.
  - Calibration collapse: high AUC but low rare-class recall post-quantization → inspect class-wise sensitivity and ECE.
  - SSM scan pathology: boundary artifacts or anatomical discontinuity in segmentation → adjust scan strategy (cross-scan vs wavefront).
- First 3 experiments:
  1. Benchmark MobileOne-S0 vs EfficientFormerV2-S0 on your target edge device (iPhone/Android/Jetson) with your inference engine; measure median/p90 latency and energy per inference using the paper's deployment-first checklist (Table 4).
  2. Apply INT8 PTQ and QAT to your chosen model; compare AUC, per-class sensitivity, and ECE—confirm QAT preserves rare-class performance.
  3. For segmentation tasks, compare a CNN baseline (U-Lite/MK-UNet) against an SSM variant (LightM-UNet/Ultralight VM-UNet) on volumetric data; document memory footprint, scan strategy, and any compiler/kernel availability gaps.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the "compiler gap" for State Space Models (SSMs) be effectively bridged to translate their theoretical linear complexity into verified latency gains on edge hardware?
- Basis in paper: The paper notes that while SSMs are mathematically efficient, they "rely on custom CUDA kernels that lack native support in widely used inference engines," making CNNs currently more reliable for deployment.
- Why unresolved: Existing mobile inference frameworks lack optimized operators for selective scan kernels, causing architectures like Mamba to underperform on non-NVIDIA edge platforms compared to mature CNN toolchains.
- What evidence would resolve it: Integration of standardized SSM operators into major inference engines (e.g., TensorRT, ONNX) and demonstrated latency parity or superiority of SSMs over CNNs on mobile NPUs.

### Open Question 2
- Question: What specific community-wide benchmarking standards are required to enable reproducible comparison of efficiency claims across heterogeneous clinical environments?
- Basis in paper: The authors state that "the absence of standardized evaluation practices" and "heterogeneous and poorly specified conditions" prevent reliable cross-study comparisons, advocating for a "deployment-first reporting standard."
- Why unresolved: Current literature relies on inconsistent metrics (FLOPs vs. latency), hardware setups, and precision levels, making it impossible to distinguish architectural superiority from evaluation artifacts.
- What evidence would resolve it: Widespread adoption of the proposed reporting checklist (Table 4) in major venues, resulting in a unified corpus of models with comparable median/tail latency and energy measurements.

### Open Question 3
- Question: How can model compression techniques be constrained to preserve uncertainty calibration and sensitivity to rare pathologies while maximizing efficiency?
- Basis in paper: The review highlights that "aggressive sparsification may silently erode sensitivity to rare but clinically significant findings" and that optimization must be reframed as a "safety-critical design decision."
- Why unresolved: Standard compression protocols optimize for aggregate metrics (Dice/AUC) which can mask the degradation of safety-oriented indicators like Expected Calibration Error (ECE) or "pruning-identified exemplars" (PIEs).
- What evidence would resolve it: Development of compression algorithms that explicitly penalize increases in ECE or drops in minority-class sensitivity, validated on long-tailed medical datasets.

## Limitations
- The review's core efficiency claims rely heavily on benchmarking infrastructure that is not fully specified (e.g., exact compiler flags, operator fusion details).
- The evidence for SSM linear-complexity benefits in medical imaging is primarily theoretical, with limited empirical validation across diverse clinical tasks.
- Safety-instrumentation recommendations (ECE, rare-class sensitivity) are sound but not systematically validated across the reviewed architectures.

## Confidence
- **High:** The identification of MAC as the primary bottleneck on edge NPUs is strongly supported by the quantitative benchmarks showing FLOPs-latency mismatches.
- **Medium:** The dimension-consistency mechanism for hybrid architectures is logically sound but lacks comprehensive empirical validation across different inference engines.
- **Low:** The theoretical advantages of SSMs (O(N) scaling, cross-scan modules) are convincingly presented, but the practical performance on diverse medical imaging tasks remains under-validated in the corpus.

## Next Checks
1. Benchmark the exact MobileOne-S0 vs EfficientFormerV2-S0 latency gap on your target edge device with your inference engine, measuring not just median latency but p90 latency and energy consumption using the deployment-first checklist.
2. Implement both INT8 PTQ and QAT on your model; compare not just aggregate AUC but per-class sensitivity and ECE, specifically testing rare pathology detection where medical safety is most critical.
3. For volumetric segmentation tasks, implement both a CNN baseline (U-Lite/MK-UNet) and an SSM variant (LightM-UNet/Ultralight VM-UNet) on 3D medical data; document the memory footprint differences and any compiler/kernel support gaps that emerge.