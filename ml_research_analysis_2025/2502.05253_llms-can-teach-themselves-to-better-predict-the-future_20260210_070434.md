---
ver: rpa2
title: LLMs Can Teach Themselves to Better Predict the Future
arxiv_id: '2502.05253'
source_url: https://arxiv.org/abs/2502.05253
tags:
- arxiv
- forecasting
- base
- phi-4
- deepseek-r1
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a self-play based fine-tuning method that
  improves LLM forecasting accuracy by 7-10% without human-curated data. The approach
  generates reasoning traces via model self-play, ranks them by proximity to actual
  outcomes, and applies Direct Preference Optimization (DPO) to refine forecasting
  capabilities.
---

# LLMs Can Teach Themselves to Better Predict the Future

## Quick Facts
- arXiv ID: 2502.05253
- Source URL: https://arxiv.org/abs/2502.05253
- Reference count: 27
- Self-play fine-tuning improves LLM forecasting accuracy by 7-10% without human-curated data

## Executive Summary
This paper introduces a novel self-play based fine-tuning method that significantly improves large language model (LLM) forecasting accuracy by 7-10% without requiring human-curated training data. The approach generates reasoning traces through model self-play, ranks them based on proximity to actual outcomes, and applies Direct Preference Optimization (DPO) to refine forecasting capabilities. Tested on Phi-4 14B and DeepSeek-R1 14B models, the method achieved mean Brier scores of 0.200 and 0.197 respectively, matching the performance of much larger GPT-4o models.

The key innovation lies in using outcome-based preference learning rather than traditional supervised fine-tuning. By generating multiple reasoning traces for each forecasting question and ranking them according to their alignment with actual outcomes, the method creates a self-supervised learning signal that improves model calibration and accuracy. The improvement was statistically significant compared to both base models and control models with randomized labels, demonstrating that learning from outcome-based rankings rather than just additional information drives the performance gains.

## Method Summary
The methodology generates reasoning traces via model self-play, ranks them by proximity to actual outcomes, and applies Direct Preference Optimization (DPO) to refine forecasting capabilities. For each forecasting question, the model generates multiple reasoning traces with different random seeds, then ranks these traces based on their proximity to the actual outcome. The top-ranked trace serves as the positive example in preference pairs, while lower-ranked traces serve as negative examples. DPO is then applied to fine-tune the model using these preference pairs, creating a self-supervised learning signal from the model's own reasoning traces.

## Key Results
- Self-play fine-tuning improved forecasting accuracy by 7-10% on tested models
- Phi-4 14B achieved mean Brier score of 0.200; DeepSeek-R1 14B achieved 0.197
- Performance matched GPT-4o despite being 14B parameter models versus GPT-4o's much larger size
- Improvements were statistically significant compared to both base models and control models with randomized labels

## Why This Works (Mechanism)
The method works by creating a self-supervised learning loop where models generate their own training data through self-play. By generating multiple reasoning traces for each question and ranking them based on actual outcomes, the system identifies which reasoning patterns lead to more accurate predictions. DPO then reinforces these successful patterns while de-emphasizing less effective ones. This approach effectively teaches models to recognize and reproduce the reasoning strategies that historically led to better forecasting performance, without requiring human experts to manually curate training examples.

## Foundational Learning
- **Brier Score**: A proper scoring rule for probabilistic forecasts that measures the mean squared difference between predicted probabilities and actual outcomes. Why needed: Provides the primary metric for evaluating forecasting accuracy and comparing model performance across different approaches.
- **Direct Preference Optimization (DPO)**: A reinforcement learning method that optimizes models based on pairwise preference comparisons rather than direct labels. Why needed: Enables the model to learn from the relative quality of different reasoning traces without requiring explicit correctness labels for each trace.
- **Self-Play in Language Models**: A technique where models generate multiple responses to the same prompt and use these responses to create training data. Why needed: Provides a mechanism for generating diverse reasoning approaches without human input, creating the raw material for preference-based learning.
- **Reasoning Trace Generation**: The process of having models generate step-by-step reasoning for forecasting questions. Why needed: Creates interpretable intermediate outputs that can be evaluated and ranked based on their proximity to actual outcomes.
- **Outcome-Based Ranking**: The process of evaluating reasoning traces based on their alignment with actual outcomes rather than intermediate correctness. Why needed: Provides a practical signal for preference learning when ground truth intermediate steps are unavailable or ambiguous.
- **Statistical Significance Testing**: Methods for determining whether observed performance differences are likely due to chance or represent genuine improvements. Why needed: Validates that the observed improvements are not random fluctuations but represent meaningful advances in forecasting capability.

## Architecture Onboarding

**Component Map**: Question Input -> Self-Play Generation -> Multiple Reasoning Traces -> Outcome-Based Ranking -> Top Trace Selection -> Preference Pair Creation -> DPO Fine-tuning -> Improved Forecasting Model

**Critical Path**: The critical path is the sequence from self-play generation through preference pair creation to DPO fine-tuning. This path determines the quality of the final model because each step builds on the previous one - poor reasoning trace generation leads to weak ranking signals, which creates poor preference pairs, which results in ineffective fine-tuning.

**Design Tradeoffs**: The primary tradeoff is between diversity of generated reasoning traces and computational cost. Generating more traces per question increases the likelihood of finding high-quality reasoning patterns but requires more computation. The choice of ranking criteria (e.g., proximity to outcome vs. intermediate step correctness) also involves tradeoffs between interpretability and practical effectiveness.

**Failure Signatures**: 
- If reasoning traces show low diversity, the model may overfit to specific reasoning patterns and miss alternative valid approaches
- Poor outcome-based ranking may create misleading preference pairs, teaching the model incorrect associations between reasoning patterns and accuracy
- Insufficient trace generation may result in suboptimal top traces, limiting the quality of the positive examples used for fine-tuning

**First Experiments**:
1. Generate 5-10 reasoning traces per question with varying random seeds to establish baseline diversity and identify optimal trace count
2. Compare different ranking criteria (outcome proximity vs. intermediate step correctness) to determine which provides better preference signals
3. Test DPO fine-tuning with different learning rates and batch sizes to optimize the preference learning process

## Open Questions the Paper Calls Out
None

## Limitations
- Limited evaluation scope with only 10 general-domain and 12 domain-specific questions, restricting generalizability
- Reliance on single top-ranked trace may introduce bias toward specific reasoning patterns and miss alternative valid approaches
- No comparison to conventional fine-tuning on human-curated forecasting data to establish relative advantages
- Absence of testing with different base model sizes to determine minimum effective model sizes and scaling properties

## Confidence

**High confidence**: Core finding that self-play with outcome-based preference ranking improves forecasting accuracy compared to baseline models

**Medium confidence**: Relative advantage over control models with randomized labels, as this demonstrates importance of outcome-based learning but doesn't fully establish superiority over alternative approaches

**Medium confidence**: Claim that method achieves performance matching GPT-4o, given limited question set and potential domain-specific factors

## Next Checks

1. **Domain Expansion Validation**: Test methodology across 50-100 forecasting questions spanning multiple domains (economics, geopolitics, technology, health) to assess generalizability and identify domain-specific limitations.

2. **Alternative Self-Improvement Comparison**: Implement and compare against alternative self-improvement methods including iterative prompting, chain-of-thought refinement, and temperature-based diversity sampling to isolate specific contribution of self-play + DPO approach.

3. **Transfer Learning Assessment**: Evaluate whether models trained on general forecasting questions show improved performance on specialized domains, and test approach with different base model sizes to establish scaling properties and identify minimum effective model sizes.