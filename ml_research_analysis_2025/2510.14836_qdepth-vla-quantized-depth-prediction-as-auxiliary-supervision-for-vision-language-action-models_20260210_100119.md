---
ver: rpa2
title: 'QDepth-VLA: Quantized Depth Prediction as Auxiliary Supervision for Vision-Language-Action
  Models'
arxiv_id: '2510.14836'
source_url: https://arxiv.org/abs/2510.14836
tags:
- depth
- qdepth-vla
- tasks
- spatial
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: QDepth-VLA addresses the challenge of spatial perception in Vision-Language-Action
  (VLA) models for fine-grained manipulation tasks by introducing quantized depth
  prediction as auxiliary supervision. The method uses a dedicated depth expert to
  predict quantized latent tokens of depth maps obtained from a VQ-VAE encoder, enabling
  the model to learn depth-aware representations that capture critical geometric cues.
---

# QDepth-VLA: Quantized Depth Prediction as Auxiliary Supervision for Vision-Language-Action Models

## Quick Facts
- arXiv ID: 2510.14836
- Source URL: https://arxiv.org/abs/2510.14836
- Reference count: 40
- Success rate on LIBERO Goal tasks: 94.0% (outperforming open π0 by 6.1%)

## Executive Summary
QDepth-VLA addresses spatial perception limitations in Vision-Language-Action (VLA) models for fine-grained manipulation by introducing quantized depth prediction as auxiliary supervision. The method uses a dedicated Depth Expert to predict discrete depth tokens from a VQ-VAE encoder, enabling the model to learn depth-aware representations that capture critical geometric cues. This approach avoids disrupting the pretrained VLM's semantic alignment while enhancing geometric understanding. Experimental results show significant performance improvements on both simulation benchmarks (LIBERO and Simpler) and real-world robotic manipulation tasks.

## Method Summary
QDepth-VLA extends VLA models by adding a Depth Expert that predicts quantized latent tokens of depth maps obtained from a VQ-VAE encoder. The model uses PaliGemma-3B as the VLM backbone, with separate 18-layer transformers for Action and Depth Experts. The Depth Expert processes visual tokens before language fusion using a hybrid attention mask to prevent semantic interference. Training combines the primary Conditional Flow Matching loss for actions with an auxiliary cross-entropy depth loss. The approach is trained on datasets like LIBERO and Simpler with depth annotations generated using Video-Depth-Anything (ViDA), achieving improved spatial reasoning for manipulation tasks.

## Key Results
- Achieves 94.0% success rate on LIBERO Goal tasks, outperforming open π0 by 6.1%
- Demonstrates 72.6% success rate on LIBERO Long tasks, a 7.7% improvement over open π0
- Shows 68.5% average success rate on Simpler benchmarks, outperforming open π0 by 7.7%
- Real-world robotic manipulation tasks demonstrate a 10.0% improvement over baseline
- Ablation studies confirm depth supervision and hybrid attention mask contribute to performance gains

## Why This Works (Mechanism)

### Mechanism 1
Quantizing depth maps into discrete latent tokens via VQ-VAE provides a more compact and noise-resistant supervision signal than pixel-wise regression for learning geometric cues. A VQ-VAE encoder compresses depth maps into a discrete codebook (K=256), and the model predicts these code indices rather than regressing continuous depth values. This forces the model to focus on salient structural features while ignoring low-level pixel redundancy and sensor noise. The discrete codebook captures sufficient geometric information for manipulation policies, and the VQ-VAE reconstruction loss aligns with the structural features needed for action generation.

### Mechanism 2
Isolating depth processing in a dedicated "Depth Expert" with a specific attention mask preserves the pre-trained VLM's semantic alignment while injecting geometric reasoning. The Depth Expert processes visual tokens before language fusion to avoid semantic interference. The Hybrid Attention Mask allows depth tokens to attend to visual/text tokens but prevents depth noise from flowing back into the core VLM or action tokens until necessary. This isolation ensures the pre-trained VLM's semantic reasoning capabilities remain intact while still benefiting from geometric understanding.

### Mechanism 3
Treating depth prediction as an auxiliary co-training objective encourages the visual encoder to learn geometry-aware embeddings that implicitly improve action generation. The total loss combines the primary action objective (Conditional Flow Matching) with the auxiliary depth token loss. The gradients from the depth loss shape the visual encoder (SigLIP) to structure its latent space around 3D geometry, which the Action Expert then utilizes. The features required to predict depth tokens overlap with the features required to estimate spatial relationships for manipulation.

## Foundational Learning

- **Concept: Vector Quantization (VQ-VAE)**
  - Why needed here: The core innovation relies on compressing continuous depth maps into discrete tokens. You must understand how VQ-VAE uses a codebook to discretize continuous latent space and how the commitment loss works.
  - Quick check question: Can you explain why predicting a discrete class index (codebook entry) is different from regressing a continuous value, and how this affects noise sensitivity?

- **Concept: Auxiliary Supervision / Multi-Task Learning**
  - Why needed here: The model learns depth not as an output for inference, but as a helper task for training. Understanding how gradient signals from an auxiliary head regularize a shared backbone is critical.
  - Quick check question: How does the gradient from the depth loss flow back to the SigLIP encoder, and why would we decay this loss weight (λ_t) over time?

- **Concept: Conditional Flow Matching (CFM)**
  - Why needed here: The Action Expert uses CFM (inherited from π0) to generate actions. This differs from standard MSE or diffusion policies.
  - Quick check question: How does the CFM objective transport noisy action samples to ground-truth actions, and what role does the observation conditioning play?

## Architecture Onboarding

- **Component map:** RGB Image -> SigLIP Encoder -> (A) VLM/Gemma for semantic fusion, (B) Depth Expert for geometric prediction -> Action Expert -> Actions via CFM

- **Critical path:**
  1. Input: RGB Image -> SigLIP Encoder
  2. Split: Visual tokens go to (A) VLM/Gemma for semantic fusion and (B) Depth Expert for geometric prediction
  3. Depth Path: Depth Expert predicts code indices -> Cross-Entropy Loss against VQ-VAE codes
  4. Action Path: VLM embeddings + Proprioception -> Action Expert -> Flow Matching Loss
  5. Attention: Hybrid Mask ensures Depth tokens attend to Image/Text, but Text/Image do not attend to Depth tokens (preventing noise injection)

- **Design tradeoffs:**
  - Latent Resolution: Paper uses 16x16 for depth tokens (vs. 32x32) for efficiency. Higher res might capture finer detail but increases sequence length/compute
  - Pixel vs. Quantized: Ablation shows pixel regression is inferior (-3.9%) due to redundancy
  - Depth Expert Input: Using visual tokens before language fusion avoids semantic interference but might lose task-relevant context that language provides

- **Failure signatures:**
  - Semantic Drift: If Depth Expert gradients are too strong, the VLM may lose its ability to understand instructions
  - Noisy Reconstruction: If the VQ-VAE is under-trained, the depth codes are meaningless, resulting in a noisy auxiliary signal that degrades policy learning
  - Attention Bleed: If the hybrid attention mask is implemented incorrectly, performance drops by ~5.5%

- **First 3 experiments:**
  1. VQ-VAE Reconstruction Check: Train the VQ-VAE on the target dataset (e.g., LIBERO depth maps) and visualize reconstructions. Ensure object boundaries and gripper geometry are preserved before training the main model
  2. Depth Loss Ablation: Train the VLA with L_depth = 0 vs. L_depth > 0 on a specific spatial task (e.g., Stack Block). Verify the ~3-8% improvement reported in the paper
  3. Attention Mask Validation: Run a forward pass with a debug hook to visualize the attention map. Confirm that Text-to-Depth attention weights are zero (masked) while Depth-to-Text is active

## Open Questions the Paper Calls Out

- **Open Question 1:** Can extending the current framework to predict future depth tokens (rather than current-timestep tokens) significantly improve temporal reasoning and long-horizon planning? The conclusion states this presents a promising direction for future research, as the current implementation only predicts depth tokens corresponding to the current observation.

- **Open Question 2:** To what extent does the noise inherent in the Video-Depth-Anything (ViDA) estimator limit the geometric grounding of the VLA model compared to ground-truth depth supervision? Section 3.1 notes that ground-truth depth is unavailable for datasets like OXE, necessitating the use of the viDA monocular estimator for annotations.

- **Open Question 3:** Are there more efficient or continuous VAE-based representations that could outperform the discrete VQ-VAE approach in balancing reconstruction fidelity with optimization stability? The conclusion identifies exploring more efficient VAE-based depth representations as a future direction.

## Limitations

- The quantized depth supervision assumes that the VQ-VAE codebook captures the geometric features relevant for manipulation tasks, but this assumption is not independently verified
- The real-world improvement of 10.0% over the baseline is based on only 10 trials, which is statistically weak
- The hybrid attention mask prevents semantic interference but may also block beneficial cross-modal interactions, leaving the optimal mask design unresolved

## Confidence

- **High confidence:** The experimental results showing QDepth-VLA outperforms baselines on LIBERO and Simpler benchmarks (94.0% vs 87.9% on Goal tasks, 72.6% vs 64.9% on Long tasks)
- **Medium confidence:** The claim that quantized depth prediction is more effective than pixel-level regression relies on a single ablation result
- **Low confidence:** The real-world improvement of 10.0% over the baseline is based on only 10 trials

## Next Checks

1. **VQ-VAE reconstruction validation:** Train the VQ-VAE on the target dataset (LIBERO depth maps) and visualize reconstructions. Measure reconstruction error and ensure object boundaries and gripper geometry are preserved before training the main model.

2. **Cross-dataset depth generalization:** Test the depth expert's performance when trained on LIBERO depth data but evaluated on Simpler or real-world data with different depth distributions. Measure depth prediction accuracy and action success rates to assess whether the VQ-VAE representation generalizes across domains.

3. **Attention mask sensitivity analysis:** Systematically vary the hybrid attention mask parameters (e.g., allow limited text-to-depth attention, adjust depth-to-text weights) and measure performance impact. This would reveal whether the current mask design is optimal or if there's a sweet spot that balances noise prevention with information flow.