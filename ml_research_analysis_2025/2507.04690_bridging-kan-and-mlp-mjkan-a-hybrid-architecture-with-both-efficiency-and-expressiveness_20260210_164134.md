---
ver: rpa2
title: 'Bridging KAN and MLP: MJKAN, a Hybrid Architecture with Both Efficiency and
  Expressiveness'
arxiv_id: '2507.04690'
source_url: https://arxiv.org/abs/2507.04690
tags:
- mjkan
- basis
- functions
- function
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "MJKAN addresses the computational inefficiency and poor generalization\
  \ of Kolmogorov-Arnold Networks (KANs) by introducing a hybrid architecture that\
  \ combines radial basis function (RBF) activations with a FiLM (Feature-wise Linear\
  \ Modulation) mechanism. This approach enables learnable univariate functions with\
  \ efficient feature modulation, preserving KAN\u2019s theoretical expressiveness\
  \ while improving practicality."
---

# Bridging KAN and MLP: MJKAN, a Hybrid Architecture with Both Efficiency and Expressiveness

## Quick Facts
- **arXiv ID:** 2507.04690
- **Source URL:** https://arxiv.org/abs/2507.04690
- **Reference count:** 3
- **Primary result:** MJKAN outperforms MLPs in function regression with large basis functions but achieves competitive classification accuracy only with small basis sizes

## Executive Summary
MJKAN introduces a hybrid neural network architecture that combines radial basis function (RBF) activations with a Feature-wise Linear Modulation (FiLM) mechanism to address the computational inefficiency and poor generalization of Kolmogorov-Arnold Networks (KANs). The method achieves efficient learnable univariate function approximation through RBF expansions modulated by learned scaling and bias parameters. Experiments demonstrate that MJKAN excels at function regression tasks when using large numbers of basis functions, while requiring smaller basis sizes for competitive classification performance, highlighting a fundamental trade-off between capacity and generalization. The architecture also offers symbolic interpretability, allowing extraction of each feature's exact functional contribution.

## Method Summary
MJKAN is a hybrid architecture that integrates RBF activations with FiLM-like modulation to create efficient learnable univariate functions. Each input feature is expanded through K Gaussian RBF kernels, then modulated via learned scaling (γ) and bias (β) parameters. The FiLM output is computed as FiLM_i(x_i) = γ_i · x_i + β_i, where γ_i and β_i are weighted sums of RBF activations. This creates per-feature learnable nonlinear transformations without the computational overhead of B-splines. The method preserves KAN's theoretical expressiveness while improving practicality for real-world tasks.

## Key Results
- Function regression: MJKAN significantly outperforms MLPs in function regression as basis function count increases (RMSE improves from 0.0014 to 0.0002 on Local Bumps function)
- Image classification: Competitive accuracy with MLPs only when using small basis sizes (CIFAR-10: 50.2% at basis=5 vs MLP at 55.6%)
- Classification trade-off: Performance degrades sharply with larger basis sizes (CIFAR-100 drops from 19.2% to 2.4% as basis increases from 5 to 50)
- Interpretability: Each feature's contribution can be extracted symbolically from the learned RBF-based functions

## Why This Works (Mechanism)

### Mechanism 1
RBF activations with FiLM modulation provide efficient learnable univariate function approximation. Each input feature x_i is expanded through K Gaussian RBF kernels, then modulated via learned scaling (γ) and bias (β) parameters. This creates per-feature learnable nonlinear transformations without B-spline computational overhead.

### Mechanism 2
Smaller basis sizes improve generalization on classification tasks by constraining decision boundary complexity. The number of basis functions K directly controls functional degrees of freedom, with larger K enabling higher-frequency components and more convoluted decision boundaries that lead to overfitting.

### Mechanism 3
Symbolic interpretability emerges from additive per-feature RBF expansions. Each logit is a sum of per-feature functions where each Ψ_{i,j} is an explicit RBF-based polynomial, allowing extraction of each feature's contribution in closed form.

## Foundational Learning

- **Kolmogorov-Arnold Representation Theorem**: Why needed: MJKAN's architecture is motivated by this theorem, which states multivariate continuous functions can be composed from univariate functions. Quick check: Can you explain why KANs place learnable functions on edges rather than fixed activations at nodes?
- **Feature-wise Linear Modulation (FiLM)**: Why needed: MJKAN's core innovation is applying FiLM (γ scaling, β bias) to RBF activations. Quick check: How does FiLM differ from a standard learned affine transformation, and why is it applied per-feature here?
- **Bias-Variance Tradeoff in Basis Function Count**: Why needed: The paper's key finding is that basis size controls the capacity-generalization tradeoff differently for regression vs. classification. Quick check: On CIFAR-100, why does increasing basis functions from 5 to 50 cause accuracy to collapse from 19.2% to 2.4%?

## Architecture Onboarding

- **Component map:** Input (d_in) → [Per-feature branch] → RBF Expansion (K basis functions per feature) → FiLM Modulation (γ, β from RBF weights) → Aggregation (sum across features) → Optional Base Projection + Output (d_out)

- **Critical path:** 1) RBF center initialization and width selection 2) FiLM parameter initialization 3) Forward pass: RBF expansion → FiLM modulation → sum aggregation 4) Backprop through RBF parameters and FiLM weights jointly

- **Design tradeoffs:** Basis size K: Low K (5-10) favors generalization in classification; high K (25-50) favors accuracy in function regression. Start with K=5 for classification, K=25+ for regression.

- **Failure signatures:** Classification accuracy degrades sharply as K increases (overfitting signal: CIFAR-100 drops to 2.4% at K=50). Training instability if RBF widths σ are too narrow or too wide.

- **First 3 experiments:**
  1. Replicate Table 2 basis sweep: Train MJKAN on CIFAR-10 with K ∈ {5, 10, 25, 50} for 10 epochs. Verify accuracy drops as K increases.
  2. Function regression sanity check: Test on one function from Figure 1 (e.g., Local Bumps) with K ∈ {5, 25, 50}. Confirm RMSE improves with K.
  3. NLP embedding classification: Using pre-trained SimCSE embeddings on AG News, compare MJKAN (K=5) vs. MLP baseline. Verify MJKAN at 90.78% vs. MLP at 91.86%.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can specific regularization strategies decouple the number of basis functions from overfitting in complex classification tasks?
- **Open Question 2:** Does the symbolic extraction capability of MJKAN provide interpretable formulas that are more accurate or actionable than those from standard KANs?
- **Open Question 3:** How does MJKAN performance scale when integrated into deep architectures compared to the shallow 2-layer networks tested?

## Limitations
- Hyperparameters (learning rates, batch sizes, RBF initialization) are unspecified, requiring assumptions for reproduction
- Symbolic interpretability claims rely on post-hoc symbolification not fully detailed in the paper
- CIFAR-100 accuracy drop to 2.4% at K=50 suggests potential implementation sensitivity or dataset-specific issues

## Confidence
- **High confidence**: Function regression performance improvements with larger basis functions
- **Medium confidence**: Classification generalization claims at small basis sizes
- **Low confidence**: Symbolic interpretability claims

## Next Checks
1. Replicate basis size sweep on CIFAR-10: Train MJKAN with K ∈ {5, 10, 25, 50} for 10 epochs and verify accuracy degrades monotonically as K increases.
2. Test RBF parameter sensitivity: Systematically vary RBF width σ and center initialization strategies to determine impact on both regression accuracy and classification generalization.
3. Implement symbol extraction pipeline: Develop post-hoc symbolic regression for learned RBF functions and test on simple regression tasks to validate interpretability claims.