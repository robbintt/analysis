---
ver: rpa2
title: Towards Reliable Multi-Agent Systems for Marketing Applications via Reflection,
  Memory, and Planning
arxiv_id: '2508.11120'
source_url: https://arxiv.org/abs/2508.11120
tags:
- memory
- user
- which
- audience
- queries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RAMP, a multi-agent framework for the audience
  curation task in marketing applications. The framework addresses the challenge of
  selecting relevant customer subsets from large datasets based on natural language
  queries.
---

# Towards Reliable Multi-Agent Systems for Marketing Applications via Reflection, Memory, and Planning

## Quick Facts
- arXiv ID: 2508.11120
- Source URL: https://arxiv.org/abs/2508.11120
- Reference count: 12
- Primary result: Base model accuracy 58.3%, increasing to 87.0% with planning and semantic memory

## Executive Summary
This paper introduces RAMP, a multi-agent framework for the audience curation task in marketing applications. The framework addresses the challenge of selecting relevant customer subsets from large datasets based on natural language queries. RAMP employs a modular approach with specialized sub-agents for planning, execution, verification, and reflection. Key components include a planner that generates detailed filtering steps, an actor that executes these steps through code generation, a verifier that checks query satisfaction via automated tests, and a reflector that proposes improvements based on feedback. The framework incorporates both semantic memory (client-specific facts and metadata) and episodic memory (past query experiences) to enhance performance. Experimental results demonstrate significant improvements: the base model achieves 58.3% accuracy, which increases to 87.0% with the addition of planning and semantic memory. The verification and reflection components show particular value on more ambiguous queries, with recall improvements of approximately 20 percentage points. User studies indicate higher satisfaction with the system's transparency and accuracy compared to baseline approaches.

## Method Summary
The RAMP framework uses a modular approach to audience curation, where specialized agents handle distinct aspects of the task. The planner generates detailed filter specifications using table metadata and semantic memory. The actor executes these plans by generating Python code for data filtering. The verifier checks query satisfaction by generating and running Python unit tests on the results. The reflector proposes plan improvements based on feedback and retrieved episodic memories. The system employs both semantic memory (client-specific facts and metadata) and episodic memory (past query experiences) retrieved via BM25. The framework supports iterative verification and reflection loops for complex queries, with the option for model-generated self-learning insights when episodic memory is scarce.

## Key Results
- Base model accuracy of 58.3% increases to 87.0% with planning and semantic memory
- Verification and reflection components show 20 percentage point recall improvements on ambiguous queries
- User studies indicate higher satisfaction with system transparency and accuracy compared to baseline approaches
- Precision peaks at ~6 semantic memories before declining due to retrieval noise

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Separating planning from execution with domain-specific semantic memory substantially improves accuracy on structured filtering tasks.
- Mechanism: The planner generates explicit, stepwise filter specifications using table metadata and retrieved facts. This intermediate representation reduces hallucination by constraining the actor's code generation to well-grounded operations. Semantic memory supplies client-specific knowledge (e.g., column semantics, common pitfalls) that raw metadata cannot convey.
- Core assumption: The task can be decomposed into deterministic, filter-based operations amenable to symbolic verification.
- Evidence anchors:
  - [abstract] "LLM planning and memory, which increases accuracy by 28 percentage points on a set of 88 evaluation queries"
  - [Table 1] Actor + Planner + Semantic Memory achieves 87.0% accuracy vs 58.3% baseline; ablation shows both planner and memory contribute.
  - [Figure 3/Results] Without semantic memory, models "tend to hallucinate... add unnecessary filters" leading to incorrect audience sizes.
  - [corpus] Related work on neuro-symbolic verification (Yang et al. 2025b, Bougzime et al. 2025) supports hybrid architectures for reliability, but corpus evidence on marketing-specific applications is limited.
- Break condition: If queries require subjective judgment or goal-based criteria (e.g., "users likely to buy"), the filter-based decomposition fails. If semantic memory is noisy or poorly retrieved, distraction increases and precision drops (Figure 4 shows precision peaks at ~6 memories then declines).

### Mechanism 2
- Claim: Iterative verify/reflect loops with sufficient episodic memory improve recall on ambiguous or constraint-violating queries, but not on straightforward filter-based queries.
- Mechanism: The verifier generates Python unit tests from query criteria; failed tests trigger the reflector, which retrieves episodic memories of similar failures and proposed fixes. The plan is modified and re-executed. This closed-loop correction addresses cases where initial execution violates hidden constraints (e.g., insufficient users meeting strict thresholds).
- Core assumption: Episodic memory contains relevant past failure-solution pairs, and retrieval surfaces them appropriately.
- Evidence anchors:
  - [abstract] "impact of iterative verification and reflection on more ambiguous queries, showing progressively better recall (roughly +20 percentage points) with more verify/reflect iterations"
  - [Figure 5] On 10 challenge queries, recall increases with iterations only when episodic memory ≥ 6 items; with 0-2 memories, recall stays flat.
  - [Table 2] On 88 filter-based queries, verify/reflect adds no significant gain; "additional looping then introduces other conditions or filters, that potentially degrade performance."
  - [corpus] Reflexion (Shinn et al. 2023) and related reflection frameworks show gains in code/math domains; corpus does not provide direct evidence on marketing query ambiguity.
- Break condition: If episodic memory is absent or irrelevant, reflection has no useful priors and may propose harmful modifications. On simple queries, iteration is unnecessary cost without benefit.

### Mechanism 3
- Claim: When episodic memory is scarce, model-generated self-learning insights can partially compensate and improve recall.
- Mechanism: The reflector summarizes lessons from the current interaction (e.g., which filters failed, what adjustments worked) and stores them as synthetic episodic memories for future use within the session. This provides a bootstrap when human-authored memory is thin.
- Core assumption: The model can reliably identify and generalize transferable insights from single-interaction feedback.
- Evidence anchors:
  - [Figure 7] With only 2 episodic memories, adding self-learning improves recall and precision on challenge queries.
  - [Figure 13] With 0 or excessive memories, self-learning harms precision/recall—"can negatively impact precision and recall... should be applied with caution."
  - [corpus] No direct corpus evidence on self-learning for memory-scarce regimes; related work on memory management (Tan et al. 2025) discusses adaptation but not self-synthesis.
- Break condition: If the model distills incorrect or overfit insights, performance degrades. Self-learning is beneficial only in a narrow regime of memory scarcity.

## Foundational Learning

- Concept: Neuro-symbolic verification (NL2Code for test generation)
  - Why needed here: The verifier translates natural language criteria into executable Python tests, enabling objective pass/fail checks on LLM outputs.
  - Quick check question: Can you write a Python function that, given a dataframe, returns True iff the "age" column values are all < 30?

- Concept: Semantic vs episodic memory in LLM agents
  - Why needed here: Semantic memory stores general facts (column meanings, client conventions); episodic memory stores past interaction traces. Both are retrieved via BM25 and fed to different modules.
  - Quick check question: Which memory type would contain "the 'loyalty_tier' column uses values 'bronze', 'silver', 'gold'" vs "last week, lowering the propensity threshold from 75 to 60 solved the audience size issue"?

- Concept: Multi-agent decomposition (planner, actor, verifier, reflector)
  - Why needed here: RAMP separates concerns across specialized agents rather than monolithic prompting. Each has distinct inputs, outputs, and failure modes.
  - Quick check question: If the verifier's tests all pass but the user is unhappy with the audience, which agent(s) might be at fault?

## Architecture Onboarding

- Component map:
  - Planner: Input = user query + table metadata + semantic memory; Output = stepwise filter plan.
  - Actor: Input = plan; Output = filtered dataframe via generated Python functions.
  - Verifier: Input = user query + filtered dataframe; Output = list of passed/failed test rules.
  - Reflector: Input = failed rules + episodic memory; Output = plan modifications + distilled insights.
  - Memory store: BM25-indexed semantic facts and episodic traces; shared across planner/reflector.

- Critical path:
  1. Implement planner with metadata + semantic memory retrieval (start with 2-6 memories; tune retrieval).
  2. Implement actor with sandboxed Python execution for filter functions.
  3. Add verifier with rule extraction → test code generation → execution.
  4. Add reflector with episodic memory retrieval; enable 1-3 iteration loops.
  5. Only then add self-learning; monitor for degradation.

- Design tradeoffs:
  - More semantic memory improves recall but can hurt precision if retrieval is noisy (Figure 4).
  - Verify/reflect helps ambiguous queries but adds latency and may degrade simple queries (Table 2).
  - Self-learning helps in memory-scarce regimes but risks negative transfer (Figure 13).
  - Temperature=0 used for reproducibility; may sacrifice diversity in ambiguous cases.

- Failure signatures:
  - Hallucinated filters (e.g., adding "web_destinations contains NY" when only "state=NY" is needed) → check semantic memory coverage and retrieval relevance.
  - Empty or too-small audiences on challenge queries → verify episodic memory exists for threshold relaxation patterns.
  - Iteration loops that keep failing same tests → episodic memory may lack relevant solutions; consider human-in-the-loop override.
  - Precision collapse with self-learning → disable or gate self-learning behind validation checks.

- First 3 experiments:
  1. Baseline vs Planner+SemanticMemory on 20 representative queries (aim for ~85% accuracy before proceeding).
  2. Ablate semantic memory size (0, 2, 6, 10 memories) on 10 challenge queries; plot precision/recall to find operating point.
  3. Add verify/reflect with 6 episodic memories on challenge set; measure recall gain per iteration (target ~20pp improvement over 2-3 iterations).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the RAMP framework generalize to alternative LLM architectures and non-marketing domains?
- Basis in paper: [explicit] The authors explicitly list as a limitation that they "only test OpenAI GPT 4.1 on a narrow domain," suggesting future work explore applicability with other models.
- Why unresolved: The experimental results are tied to GPT-4.1's specific reasoning capabilities and the structured nature of audience curation; it is unclear if smaller or open-source models can support the complex planning/reflection modules.
- What evidence would resolve it: Benchmarking RAMP on open-source models (e.g., Llama 3) or applying the architecture to tasks like database querying or code generation.

### Open Question 2
- Question: How can the verify/reflect loop be optimized to reduce user friction and latency?
- Basis in paper: [explicit] The authors note that "users mention how the reflect/verify paradigm leads to long and tedious back-and-forth loops," identifying system efficiency as a target for future work.
- Why unresolved: While iterative reflection improves recall on ambiguous queries, the current implementation trades off user experience for accuracy.
- What evidence would resolve it: A study evaluating adaptive stopping criteria for the reflection loop that maintains accuracy while minimizing the number of turns.

### Open Question 3
- Question: What specific conditions allow LLM self-learning (generating own insights) to improve performance rather than degrade it?
- Basis in paper: [inferred] The paper observes that self-learning improves recall when episodic memory is scarce (n=2) but negatively impacts precision/recall when memory is absent or abundant.
- Why unresolved: The mechanism determining when a model's synthesized "insights" are beneficial versus distracting (hallucinated) is not fully characterized.
- What evidence would resolve it: An ablation study identifying the "confidence" or "novelty" thresholds at which self-generated memories should be accepted or discarded.

## Limitations
- The study is limited to OpenAI GPT 4.1 and does not test generalizability to other models or domains.
- The specific 15K-row dataset and memory corpora are not released, preventing direct replication.
- The framework focuses on structured, filter-based queries and may not generalize to subjective or goal-based marketing criteria.
- Iterative verify/reflect adds latency and user friction, with limited benefit on simple queries.

## Confidence

- **High confidence**: Mechanism showing planner + semantic memory improves accuracy on structured filter tasks (supported by ablation Table 1).
- **Medium confidence**: Verify/reflect mechanism's value on ambiguous queries (supported by Figure 5 but limited to 10 challenge queries).
- **Low confidence**: Self-learning's reliability as a general solution (evidence from Figure 13 shows potential for harm when memory is scarce or excessive).

## Next Checks

1. Implement ablation testing on semantic memory retrieval count (k=2, 6, 10) using a synthetic dataset to verify the precision/recall tradeoff curve shown in Figure 4.
2. Test the verify/reflect loop on a mixed set of simple and complex queries to confirm the claim that iteration helps only on ambiguous cases (replicating Table 2 findings).
3. Evaluate self-learning in memory-scarce regimes (2 memories) versus no self-learning to validate the directional improvement shown in Figure 7, while monitoring for precision degradation.