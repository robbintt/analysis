---
ver: rpa2
title: 'Soup-of-Experts: Pretraining Specialist Models via Parameters Averaging'
arxiv_id: '2502.01804'
source_url: https://arxiv.org/abs/2502.01804
tags:
- domain
- experts
- weights
- soup-of-experts
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of efficiently creating specialist
  models from a large pre-training set containing multiple domains, when the specific
  specialization data is revealed later. The key challenge is to avoid the cost of
  retraining a full model for each specialization task.
---

# Soup-of-Experts: Pretraining Specialist Models via Parameters Averaging

## Quick Facts
- **arXiv ID**: 2502.01804
- **Source URL**: https://arxiv.org/abs/2502.01804
- **Reference count**: 40
- **Primary result**: SoE achieves best performance on specialized domains compared to baselines while maintaining generic performance

## Executive Summary
This paper addresses the challenge of creating specialist models from multi-domain pre-training data when specialization targets are unknown during pre-training. The proposed Soup-of-Experts (SoE) architecture consists of shared parameters plus a bank of expert parameters that can be linearly combined using learned coefficients to instantiate a model. During pretraining, random domain weights are sampled, the corresponding model is instantiated, and parameters are updated via backpropagation. Experiments demonstrate that SoE outperforms baselines on specialized domains while maintaining comparable generic performance, requiring only a few samples to estimate domain weights for specialization.

## Method Summary
The method trains a bank of expert parameters that can be linearly combined with shared parameters to instantiate models. During pretraining, random domain weight samples are drawn from a meta-distribution, an MLP computes combination coefficients, and the resulting model is trained on data from the corresponding domain mixture. At specialization time, domain weights are estimated from the target dataset and passed through the MLP to instantiate the specialist model. The approach uses GPT-2 architecture with 110M parameters, 128 experts, and trains on RedPajama v2 clustered into 4096 domains.

## Key Results
- SoE achieves the best performance on specialized domains compared to generic pretraining, domain experts, and CRISP
- Maintains comparable performance to generic models on generic data
- Requires only 100-1000 samples to estimate domain weights for effective specialization
- Performance advantages are consistent across different model scales

## Why This Works (Mechanism)

### Mechanism 1: Linear Combinability of Pretrained Parameters
Parameters from different training distributions can be linearly combined to produce models that perform well on target distributions. The SoE trains expert parameter sets plus shared parameters such that any linear combination yields a coherent model. The MLP learns to map domain weights to coefficients that select the right expert mixture. Core assumption: parameter space contains linear paths where interpolated models maintain functional coherence.

### Mechanism 2: Meta-Distribution Training Amortizes Specialization
Training over random domain weight samples from a meta-distribution enables zero-shot specialization to unseen domain mixtures. During pretraining, sampling domain weights, computing coefficients, instantiating models, and backpropagating forces the system to learn a general mapping from domain compositions to expert combinations. Core assumption: the meta-distribution covers the space of domain mixtures likely encountered at test time.

### Mechanism 3: Shared-Expert Decomposition Separates Generic from Specific Knowledge
Decomposing parameters into shared plus expert-specific components allows efficient knowledge reuse. Shared parameters capture cross-domain patterns while experts capture domain-specific deviations. At specialization, the MLP routes to relevant experts while shared parameters remain constant. Core assumption: domain-specific knowledge can be expressed as additive deviations from a shared baseline.

## Foundational Learning

- **Model Merging / Weight Averaging**: Why needed? SoE extends fine-tuning-time model merging to pretraining-time expert design. Understanding why averaging fine-tuned models works is prerequisite. Quick check: Can you explain why averaging two fine-tuned models often outperforms either individually?

- **Domain Weight Estimation (Importance Sampling)**: Why needed? At specialization time, SoE requires estimating domain weights from a small specialist dataset. Quick check: Given a target dataset, how would you compute which pretraining domains it resembles?

- **Meta-Learning / Amortized Optimization**: Why needed? SoE amortizes the cost of training specialist models by learning a mapping from domain weights to model parameters. Quick check: How does training over a distribution of tasks differ from training on a single fixed distribution?

## Architecture Onboarding

- **Component map**: Domain clustering → k-means on sentence-BERT embeddings → 4096 centroids → Meta-distribution sampling → Domain weights h → MLP ϕ → Coefficients α → Model instantiation Θ = S + ΣαⱼEⱼ → Training/data sampling → Backpropagation

- **Critical path**: 
  1. Pretraining: Sample h ~ π → α = MLP(h) → Θ = S + ΣαⱼEⱼ → sample data from mix(h) → backprop
  2. Specialization: Estimate hₛₚₑ via nearest-neighbor on centroids → α = MLP(hₛₚₑ) → instantiate Θ

- **Design tradeoffs**: More experts (n↑) enables better specialization granularity but increases memory (total params = (n+1)×base_size). Sparse meta-distribution (s↓) better for narrow specialists, worse for generic. Dense experts outperform low-rank despite parameter count.

- **Failure signatures**: Training throughput drops (expected 77% of generic pretraining speed). Specialized loss plateaus above generic (check meta-distribution support size). Instability with many experts (reduce learning rate from 3e-4 to 1e-4).

- **First 3 experiments**:
  1. Train 2 separate domain experts, manually average parameters, verify interpolated performance on mixed-domain data.
  2. Train SoE with support sizes s∈{1,2,4,8} and evaluate on both sparse specialist and generic benchmarks.
  3. Given 10-100 samples from a held-out domain, estimate domain weights and instantiate SoE; compare to generic model with same fine-tuning budget.

## Open Questions the Paper Calls Out

### Open Question 1
Can SoE be effectively combined with Mixture-of-Experts architectures? While MoEs improve inference efficiency via conditional computation, they typically require keeping all parameters in memory. A hybrid could theoretically maximize both latency and memory efficiency, but the interaction between MoE routing and SoE coefficient learning is unexplored. Experiments training an SoE where experts are MoE layers would resolve this.

### Open Question 2
Can optimization techniques be improved to make low-rank expert representations parameter-efficient? The paper found low-rank experts underperform dense experts, positing this is an optimization issue. Theoretically, low-rank experts should allow many more experts at fixed parameter budget, but the current training pipeline fails to capitalize on this capacity. Modified training algorithm or regularization could resolve this.

### Open Question 3
How robust is specialist performance when downstream target domain distribution diverges from pre-training meta-distribution π? The model is trained on random mixtures of s domains. If a downstream task requires a specialization outside the support of sampled training mixtures, the MLP may fail to generalize to necessary coefficient interpolation. Evaluation on excluded or rarely sampled domain mixtures would resolve this.

## Limitations
- Strong assumption that domain-specific knowledge can be expressed as additive parameter deviations from shared baseline
- Critical dependence on quality of domain clustering - poor clustering yields mixed-domain centroids that don't correspond to coherent specialization targets
- Computational overhead (77% of generic pretraining speed) may be prohibitive for very large-scale applications
- Requires access to large pre-training corpus with meaningful domain structure

## Confidence

- **High Confidence**: The core claim that Soup-of-Experts can instantiate specialist models without retraining is well-supported by experimental results.
- **Medium Confidence**: The claim that SoE outperforms domain experts and CRISP on specialized domains is supported but could benefit from more rigorous ablation studies.
- **Medium Confidence**: The assertion that shared-expert decomposition efficiently separates generic from specific knowledge is plausible but lacks deep analysis of what shared vs expert parameters actually learn.

## Next Checks

1. **Linear Combination Validity**: Train two separate domain experts on disjoint data, manually average their parameters, and evaluate performance on mixed-domain data to validate the core assumption of linear combability.

2. **Meta-Distribution Sensitivity**: Systematically vary support size s in meta-distribution (s∈{1,2,4,8}) and evaluate both on sparse specialist benchmarks and generic benchmarks to reveal whether s=4 is truly optimal.

3. **Clustering Quality Impact**: Intentionally degrade domain clustering quality (e.g., by using fewer clusters or noisy embeddings) and measure impact on SoE specialization performance to test sensitivity to the fundamental clustering assumption.