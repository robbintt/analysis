---
ver: rpa2
title: 'DiagramIR: An Automatic Pipeline for Educational Math Diagram Evaluation'
arxiv_id: '2511.08283'
source_url: https://arxiv.org/abs/2511.08283
tags:
- diagram
- code
- elements
- evaluation
- diagrams
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We introduce DiagramIR, an automatic pipeline for evaluating geometric
  diagrams generated from LaTeX TikZ code. Our approach uses back-translation to convert
  TikZ into an intermediate representation, then applies rule-based checks for mathematical
  and spatial correctness.
---

# DiagramIR: An Automatic Pipeline for Educational Math Diagram Evaluation

## Quick Facts
- arXiv ID: 2511.08283
- Source URL: https://arxiv.org/abs/2511.08283
- Reference count: 40
- Primary result: Automatic pipeline achieves higher agreement with human raters (κ = 0.48–0.56) than LLM-as-a-Judge baselines (κ = 0.39–0.47) while enabling smaller models to match larger model performance at 10× lower cost

## Executive Summary
This paper introduces DiagramIR, an automatic pipeline for evaluating geometric diagrams generated from LaTeX TikZ code. The approach uses back-translation to convert TikZ into an intermediate representation, then applies rule-based checks for mathematical and spatial correctness. The method achieves higher agreement with human raters compared to LLM-as-a-Judge baselines while enabling smaller models to match the performance of larger models at significantly lower cost, making scalable, accessible diagram evaluation feasible for educational AI tools.

## Method Summary
DiagramIR employs a two-stage pipeline: first, an LLM back-translates TikZ code into a structured intermediate representation (IR) capturing geometric entities and relationships; second, rule-based algorithms validate the diagram against six rubric criteria including mathematical correctness (lengths, angles), spatial correctness (canvas bounds, readability, label association, overlap), and completeness. The IR decouples parsing from verification, allowing smaller models to perform as well as larger ones when paired with deterministic checks. The pipeline was evaluated on 386 diagrams from the Coteach dataset using Cohen's kappa to measure agreement with human raters.

## Key Results
- DiagramIR achieves κ = 0.48–0.56 agreement with human raters versus κ = 0.39–0.47 for LLM-as-a-Judge baselines
- GPT-4.1-Mini with back-translation matches GPT-5 performance at 10.3× lower cost
- Rule-based checks outperform MLMs on spatial criteria like canvas bounds (κ = 0.573–0.604 vs. 0.184–0.398) and overlap detection (κ = 0.489–0.608 vs. 0.000–0.315)
- MLMs remain superior for angle label positioning (κ = 0.829 vs. 0.652 for rule-based)

## Why This Works (Mechanism)

### Mechanism 1
Converting TikZ code into a structured intermediate representation (IR) improves evaluation reliability by reducing format variability. Back-translation maps high-entropy TikZ (varied formatting, macros, drawing patterns) into a low-entropy, schema-constrained "pivot language" where coordinates, shapes, and relationships are explicitly represented. This allows deterministic rule-based checks to operate on normalized data. The core assumption is that the LLM can reliably extract geometric entities from TikZ code into the IR schema without introducing errors that propagate to downstream checks.

### Mechanism 2
Decoupling perception (parsing) from verification (checking) enables smaller models to match larger model performance at lower cost. The pipeline separates concerns—an LLM handles semantic analysis (TikZ→IR), while rule-based algorithms perform evaluation (checking bounds, proportions, overlap). This reduces the reasoning burden on any single component and allows verification logic to be deterministic and auditable. The core assumption is that rule-based checks can adequately capture the rubric criteria without probabilistic reasoning.

### Mechanism 3
Programmatic checks on structured IR outperform multimodal LLM-as-a-Judge on spatial correctness criteria. Rule-based checks directly compute geometric properties (bounding boxes, distances, overlaps) from coordinate data in the IR, whereas MLMs must infer spatial relationships from rendered pixels without guaranteed precision. The core assumption is that the IR accurately captures all geometric entities relevant to the checks; no critical information is lost during TikZ→IR translation.

## Foundational Learning

- **Concept: Intermediate Representations in Compilers**
  - Why needed here: The IR design borrows directly from compiler theory—source code (TikZ) is translated into a machine-interpretable form before analysis. Understanding this analogy clarifies why IR enables deterministic verification.
  - Quick check question: Can you explain why a compiler uses an IR instead of analyzing source code directly?

- **Concept: Cohen's Kappa (κ) for Inter-Rater Agreement**
  - Why needed here: The paper uses κ to measure how well automatic evaluation agrees with human raters. Values of 0.48–0.56 indicate "moderate" agreement; understanding this metric is essential for interpreting results.
  - Quick check question: What does κ=0.40 mean vs. κ=0.80 in terms of agreement quality?

- **Concept: TikZ/LaTeX Graphics Primitives**
  - Why needed here: The IR schema (nodes, shapes, line segments, arcs, transforms) maps directly to TikZ drawing commands. Understanding TikZ primitives is necessary to debug IR extraction failures.
  - Quick check question: What TikZ command creates a closed polygon vs. an open path?

## Architecture Onboarding

- **Component map:** TikZ code → LLM back-translation → IR JSON → Pydantic schema validation → Rule-based checks → Aggregated results

- **Critical path:** TikZ → LLM → IR (JSON) → Schema validation → Rule-based checks → Aggregated results. The LLM translation step is the highest-latency and highest-cost component.

- **Design tradeoffs:**
  - IR expressiveness vs. check simplicity: More complex IR (e.g., 3D coordinate systems) requires more complex checks but enables broader diagram coverage.
  - LLM model size vs. cost: Larger models (GPT-4.1, GPT-5) achieve higher κ but at 10× cost; the paper shows GPT-4.1-Mini is sufficient when paired with rule-based checks.
  - Rule strictness vs. false positive rate: Tighter tolerances (e.g., overlap thresholds) catch more errors but may flag acceptable diagrams.

- **Failure signatures:**
  - Empty or incomplete IR: TikZ uses unsupported macros or coordinate syntax.
  - High false positive rate on "labels associated": Proximity-based association fails in dense diagrams.
  - Low κ on angle labels: Rule-based checks struggle with angle label positioning; MLMs outperform here (κ=0.829 vs. 0.652).

- **First 3 experiments:**
  1. IR extraction validation: Run back-translation on 50 diverse TikZ samples; manually verify IR completeness and accuracy against original code.
  2. Check threshold calibration: Vary tolerance parameters (e.g., overlap area threshold, label distance threshold) and measure impact on κ agreement with human ratings.
  3. Model size ablation: Compare GPT-4.1-Mini vs. GPT-4.1 vs. GPT-5 on IR extraction quality and downstream check accuracy to validate cost-performance tradeoff claims.

## Open Questions the Paper Calls Out

- **Question:** Can the evaluation rubric be effectively extended to assess pedagogical usefulness, given the current focus on mathematical and spatial correctness?
  - Basis in paper: The authors state in the Future Work section that they should explore "extending the rubric toward pedagogical dimensions," acknowledging that the current rubric leaves out this "critical but more subjective dimension."
  - Why unresolved: The current study intentionally restricted the rubric to unambiguous, observable criteria compatible with automated checks, excluding the subjective nature of pedagogical value.
  - What evidence would resolve it: A modified rubric that includes pedagogical criteria (e.g., clarity for students) and demonstrates significant agreement with human educational experts.

- **Question:** Does the DiagramIR pipeline generalize to other visual domains such as physics diagrams or freehand sketches?
  - Basis in paper: The Limitations section notes that the dataset is grounded in one curriculum and "additional validation on other domains (e.g., physics diagrams, freehand sketches) is needed to establish broader generalizability."
  - Why unresolved: The current dataset and Intermediate Representation (IR) schema were designed specifically for geometric constructions (2D shapes, 3D prisms) derived from a specific math curriculum.
  - What evidence would resolve it: Performance metrics (Cohen's κ) from testing the pipeline on datasets of physics diagrams or sketches showing comparable agreement with human raters.

- **Question:** Can the Intermediate Representation (IR) schema be expanded to support complex diagram constructs like coordinate plots and multi-step constructions without losing verification accuracy?
  - Basis in paper: The authors note in the Limitations and Future Work that the "IR captures a restricted set of geometric primitives" and requires "expanding the IR to cover a broader set of diagram constructs."
  - Why unresolved: While the current IR handles basic shapes well, more complex figures may require significant schema iteration to ensure deterministic rule-based checks remain valid.
  - What evidence would resolve it: Successful extraction and verification of multi-step constructions or coordinate plots using an expanded IR schema with low error rates.

## Limitations
- Results are tied to the specific 398-diagram dataset from Coteach; generalization to other educational domains or diagram types remains untested.
- The IR schema may not handle all TikZ constructs, particularly custom macros or advanced coordinate systems, potentially limiting applicability.
- Spatial correctness criteria rely on heuristic thresholds that may not capture all edge cases or pedagogical contexts.

## Confidence
- **High confidence**: The core mechanism of using structured IR for deterministic checks is well-supported by the κ improvements over LLM-as-a-Judge baselines.
- **Medium confidence**: The cost-performance tradeoff is demonstrated but depends on the specific API pricing and model configurations used.
- **Medium confidence**: The superiority of rule-based spatial checks is supported by Table 3 results but may not generalize to more complex diagram layouts.

## Next Checks
1. Cross-domain validation: Test the pipeline on diagrams from different educational contexts (e.g., physics, chemistry) to assess generalizability beyond the Coteach dataset.
2. IR schema stress test: Systematically generate TikZ code with increasing complexity (nested macros, 3D transformations, custom coordinate systems) to identify IR extraction failure modes.
3. Human-in-the-loop threshold tuning: Have human raters evaluate a subset of false positives/negatives to refine the rule-based thresholds and improve agreement metrics.