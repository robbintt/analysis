---
ver: rpa2
title: 'EmoHarbor: Evaluating Personalized Emotional Support by Simulating the User''s
  Internal World'
arxiv_id: '2601.01530'
source_url: https://arxiv.org/abs/2601.01530
tags:
- user
- evaluation
- emotional
- support
- dialogue
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces EmoHarbor, a novel evaluation framework for
  emotional support conversations that simulates users' internal psychological states
  to assess personalization quality. The framework employs a Chain-of-Agent architecture
  with three specialized roles (Thinker, Talker, Evaluator) to model user cognition,
  generate realistic dialogue, and produce subjective evaluations.
---

# EmoHarbor: Evaluating Personalized Emotional Support by Simulating the User's Internal World

## Quick Facts
- arXiv ID: 2601.01530
- Source URL: https://arxiv.org/abs/2601.01530
- Reference count: 40
- Primary result: Chain-of-Agent framework achieves Pearson correlations of 0.4-0.5 with human judgments for evaluating personalized emotional support

## Executive Summary
This paper introduces EmoHarbor, a novel evaluation framework that simulates users' internal psychological states to assess personalization quality in emotional support conversations. The framework employs a Chain-of-Agent architecture with three specialized roles (Thinker, Talker, Evaluator) to model user cognition, generate realistic dialogue, and produce subjective evaluations. Tested on 20 LLMs using 100 real-world user profiles across 16 scenarios, EmoHarbor demonstrates strong alignment with human judgments while revealing that while models excel at generating empathetic responses, they consistently fail to tailor support to individual user contexts.

## Method Summary
EmoHarbor uses a Chain-of-Agent architecture where a User Thinker agent maintains and updates an internal state (cognitive appraisals, emotions, goals) at each turn, a User Talker agent generates personality-consistent dialogue based on this state, and a User Evaluator agent scores responses from the perspective of the specific user persona. The framework uses 100 curated user profiles with demographic, preference-related, counseling-related, and scenario-specific attributes. Each conversation is simulated over up to 15 turns, with the User Thinker tracking complete memory including internal states while the Supporter tracks only observable dialogue. The system evaluates 10 dimensions of support quality and produces metrics including Pearson correlation with human judgments, Model Separation Ratio (0.745), and Model Agreement Coefficient (0.427).

## Key Results
- Achieved Pearson correlations of 0.4-0.5 with human judgments for evaluating personalized emotional support
- Model Separation Ratio of 0.745 indicates strong discrimination between different LLM models
- Chain-of-Agent architecture outperforms single-agent baselines with 64% win rate in human evaluations
- One-way ANOVA shows significant differences between models (F=112, p<0.001) with pairwise discriminability of 0.87

## Why This Works (Mechanism)

### Mechanism 1
Simulating a user's latent internal state allows evaluation to capture subjective personalization gaps that generic "empathy" metrics miss. The User Thinker agent generates a hidden internal state (IS_t) comprising cognitive appraisals, emotions, and goals, updated iteratively based on the supporter's response. The User Evaluator then assesses the dialogue based on the trajectory of these states rather than just surface text, penalizing responses that are empathetic but misaligned with the specific user's constraints.

### Mechanism 2
Decomposing the simulation into specialized agents (Thinker, Talker, Evaluator) increases fidelity by isolating the generation of "internal reactions" from "external dialogue." This prevents the "talker" from simply generating optimal, cooperative dialogue, allowing the "thinker" to model hesitation, skepticism, or rejection which the Evaluator subsequently scores.

### Mechanism 3
Using a "User-as-a-Judge" paradigm (subjective assessment) yields higher alignment with human preferences for personalization than "LLM-as-a-Judge" (objective assessment). Standard LLM judges act as external experts evaluating "empathy" objectively, while EmoHarbor forces the judge to adopt the specific user's persona and internal state history, changing the evaluation criterion from "Is this a good response?" to "Is this response good for me (this specific persona)?"

## Foundational Learning

- **Persona-Driven Role-Playing (Persona-Act)**: Why needed: EmoHarbor relies on the ability of an LLM to robustly inhabit a specific character (MBTI, history, preferences) and maintain that constraint over 15 turns. Quick check: Can you explain why a "generic empathetic" response might receive a low score from a simulated user with a highly pragmatic personality type?

- **Latent State Tracking**: Why needed: The core innovation is maintaining a "hidden" variable (the user's internal state) alongside the "visible" dialogue history. Understanding this separation is key to debugging the framework. Quick check: What is the difference between the "Supporter Memory" (H_s) and the "User Memory" (H_u) in the architecture?

- **Evaluation Metrics for Open-Ended Dialogue**: Why needed: Traditional metrics (BLEU, ROUGE) fail here. You need to understand why "Model Separation Ratio" and "Pearson Correlation" with human judgment are better signals for this task. Quick check: Why might a high "Empathy" score fail to predict a high "Problem Resolution" score in this framework?

## Architecture Onboarding

- **Component map**: User Profile + Supporter System -> User Thinker (updates IS_t) -> User Talker (generates U_t) -> Supporter (generates R_t) -> User Evaluator (scores 10 dimensions)
- **Critical path**: The User Thinker is the bottleneck. If the internal monologue generation is low-quality or hallucinates reactions not grounded in the profile, the Talker produces inconsistent dialogue, and the Evaluator provides noisy feedback.
- **Design tradeoffs**: Cost vs. Depth (running 3 specialized agents per turn is expensive at ~188.8s per dialogue); Stability vs. Diversity (temperature settings of 0.1 for Thinker vs 0.7 for Talker require careful tuning).
- **Failure signatures**: Homogenization (simulated users converge on identical dialogue behaviors); Premature Termination (conversations end early due to brittle simulation logic); Over-Criticality (Evaluator consistently rates all models poorly).
- **First 3 experiments**: 1) Sanity Check: Run pairwise comparison between Single-Agent and Chain-of-Agent architectures; 2) Correlation Calibration: Evaluate a known model with and without "Internal State" visibility; 3) Sensitivity Analysis: Modify single attributes in one profile to test score sensitivity.

## Open Questions the Paper Calls Out
None

## Limitations
- The framework's effectiveness critically depends on the quality of simulated internal states, which may not reliably capture diverse psychological responses
- Reliance on a specific set of 100 user profiles limits generalizability to different demographic or psychological characteristics
- Computational cost of ~188.8s per dialogue makes large-scale evaluation prohibitive without optimization

## Confidence
- **High Confidence**: Framework architecture design and evaluation metrics are well-specified and technically sound
- **Medium Confidence**: Chain-of-Agent architecture outperforming single-agent baselines is supported but could benefit from more extensive ablation studies
- **Low Confidence**: Long-term stability of simulation across extended conversations and robustness to diverse user profile types remain uncertain

## Next Checks
1. **Profile Generalization Test**: Create 10-15 additional user profiles with characteristics not represented in the original 100 and measure whether EmoHarbor maintains evaluation accuracy
2. **Thinker State Quality Audit**: Conduct a blind study where human raters evaluate the psychological coherence of internal states generated by the User Thinker
3. **Cost-Performance Tradeoff Analysis**: Implement and test a simplified single-agent variant to quantify degradation in evaluation quality versus computational costs