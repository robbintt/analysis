---
ver: rpa2
title: A Representation Level Analysis of NMT Model Robustness to Grammatical Errors
arxiv_id: '2505.21224'
source_url: https://arxiv.org/abs/2505.21224
tags:
- robustness
- errors
- clean
- attention
- heads
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a representation-level analysis of NMT model
  robustness to grammatical errors. The authors introduce synthetic grammatical errors
  (article, preposition, and noun number) into clean sentences and analyze how models
  detect and correct these errors through internal representations.
---

# A Representation Level Analysis of NMT Model Robustness to Grammatical Errors

## Quick Facts
- arXiv ID: 2505.21224
- Source URL: https://arxiv.org/abs/2505.21224
- Reference count: 40
- Primary result: NMT encoders implement an implicit GEC pipeline, detecting errors in early layers and correcting them in deeper layers via Robustness Heads.

## Executive Summary
This paper presents a representation-level analysis of NMT model robustness to grammatical errors, focusing on how encoder layers detect and correct synthetic article, preposition, and noun number errors. Using GED probing and representational similarity analysis, the authors demonstrate that encoders implement an implicit grammatical error correction pipeline: detection occurs in early layers, followed by correction as representations converge toward grammatical forms. The study identifies "Robustness Heads" - specific attention heads that contribute to this correction process by attending to linguistically informative tokens. Fine-tuning on noisy data improves robustness by increasing reliance on these heads, particularly in deeper layers. The analysis reveals consistent patterns across different architectures while highlighting language-specific variations in error handling.

## Method Summary
The study introduces synthetic grammatical errors into clean sentences from the Europarl-ST dataset across five language pairs (En-Es, En-De, En-It, En-Nl, Fr-Es). Models analyzed include OPUS-MT, M2M100, MBART, and NLLB. Three variants are fine-tuned per model: Base (no changes), Clean-Finetuned (clean split), and Noise-Finetuned (noisy split). GED probes (single linear layers) are trained per encoder layer to detect grammatical errors. CKA distance measures representation similarity between ungrammatical and grammatical forms. Robustness Heads are identified by masking individual attention heads and measuring their influence on representation distance. The analysis examines attention patterns to POS tags for error handling.

## Key Results
- NMT encoders implement an implicit GEC pipeline, detecting errors in early layers and correcting them in deeper layers
- Robustness Heads are identified that move ungrammatical word representations toward grammatical forms
- Fine-tuning on noisy data improves robustness by increasing model reliance on Robustness Heads, especially in deeper layers
- Models across different architectures respond similarly to grammatical errors, with notable differences across languages attributable to linguistic features

## Why This Works (Mechanism)

### Mechanism 1: Detection-Correction Pipeline in Encoder Layers
- Claim: NMT encoders implement an implicit Grammatical Error Correction (GEC) pipeline, detecting errors in early layers and correcting them in deeper layers.
- Mechanism: GED probing accuracy increases through roughly the first half of encoder layers, then plateaus or decreases—suggesting error information is being "corrected out" of representations. Simultaneously, the CKA distance between ungrammatical and grammatical word representations decreases across layers.
- Core assumption: Decreasing probing accuracy in deeper layers indicates correction rather than forgetting; representation convergence toward correct forms implies functional error repair.
- Evidence anchors:
  - [abstract] "Our findings indicate that the encoder first detects the grammatical error, then corrects it by moving its representation toward the correct form."
  - [section 5.2] "GED probing performance improves during roughly the first half layers of the model, then generally plateaus in the second half for Base and Clean-Finetuned but decreases for Noise-Finetuned models."
  - [corpus] Limited direct corpus support; related work on LLM grammaticality judgments (e.g., "What Can String Probability Tell Us About Grammaticality?") does not address representation-level correction dynamics.
- Break condition: If probing accuracy decreases due to catastrophic forgetting rather than correction, or if representation convergence reflects context integration unrelated to error repair.

### Mechanism 2: Robustness Heads as Correction Mechanism
- Claim: Specific attention heads ("Robustness Heads") contribute to moving ungrammatical word representations toward grammatical forms by attending to linguistically informative tokens.
- Mechanism: Robustness Heads are identified by computing 1 - CKA(gwhi, w)—the distance from the noisy word representation (with head hi masked) to the clean word representation. These heads preferentially attend to POS tags relevant to error correction (e.g., determiners/adjectives for noun number errors in French).
- Core assumption: Attention patterns to specific POS tags reflect functional error-handling behavior; masking methodology isolates causal head contributions.
- Evidence anchors:
  - [abstract] "We find that Robustness Heads attend to interpretable linguistic units when responding to grammatical errors."
  - [section 3.3.4] "We define Robustness Heads as heads that influence the ungrammatical word's representation toward its grammatical form."
  - [corpus] No direct corpus validation; related work on attention interpretability (e.g., "What does BERT look at?") supports attention-to-POS analysis but not robustness-specific claims.
- Break condition: If attended POS tags are incidental to error type, or if head masking effects are confounded by representation distribution shifts.

### Mechanism 3: Fine-tuning Reallocates Reliance to Robustness Heads
- Claim: Fine-tuning on noisy data improves robustness by increasing model reliance on Robustness Heads, particularly in deeper layers.
- Mechanism: Noise-Finetuned models show higher overlap between Influential Heads and Robustness Heads in deep layers, meaning the heads that most affect word representations are also those that move them toward correct forms. CKA distance between noisy and clean representations drops to near zero.
- Core assumption: Increased Influential/Robustness head overlap is causal to robustness gains; fine-tuning reweights existing structure rather than creating new capabilities.
- Evidence anchors:
  - [section 5.4.2] "The accuracy between Influential Heads and Robustness Heads is higher in Noise-Finetuned models especially in deeper layers, which means models after fine-tuning on noise tend to employ more Robustness Heads."
  - [section 5.3] "Noise-Finetuned models exhibit similar behavior to their Base model but they learn to drive the representation to be closer (almost 0 CKA distance in most cases)."
  - [corpus] No corpus evidence on fine-tuning-induced attention head specialization for robustness.
- Break condition: If robustness gains stem from other factors (e.g., decoder changes, data augmentation effects) rather than head reallocation.

## Foundational Learning

- Concept: **Attention Head Masking and Influence**
  - Why needed here: Robustness Head identification relies on masking individual heads and measuring representation shifts.
  - Quick check question: Can you explain why masking a head and computing representation distance isolates its contribution?

- Concept: **Probing Classifiers for Linguistic Information**
  - Why needed here: GED probing determines where error detection peaks and declines across encoder layers.
  - Quick check question: What does a decrease in probing accuracy in deeper layers typically indicate about encoded information?

- Concept: **Centered Kernel Alignment (CKA) for Representation Similarity**
  - Why needed here: CKA quantifies how closely ungrammatical representations converge to grammatical forms across layers.
  - Quick check question: Why might CKA be preferred over cosine similarity for comparing representation spaces?

## Architecture Onboarding

- Component map:
  Encoder layers -> GED probes per layer -> CKA computation -> Robustness Head identification -> POS attention analysis

- Critical path:
  1. Introduce synthetic grammatical errors (article, preposition, noun number) into clean sentences.
  2. Extract word representations from each encoder layer for both noisy and clean versions.
  3. Train GED probes per layer; plot accuracy trajectory.
  4. Compute CKA distance between noisy and clean representations per layer.
  5. Identify Robustness Heads via masking + CKA-to-clean; analyze attention to POS tags.

- Design tradeoffs:
  - Synthetic errors enable controlled analysis but may not generalize to natural noise.
  - Masking one head at a time ignores head interactions; joint masking could reveal combinatorial effects.
  - Probing probes linear separability only; non-linear probing might detect different patterns.

- Failure signatures:
  - GED probing accuracy flat or decreasing from layer 1: encoder may not detect error type.
  - CKA distance increasing in deeper layers: correction mechanism failing; representations diverging.
  - Robustness Head attention distributed uniformly across POS tags: no interpretable error-handling pattern.

- First 3 experiments:
  1. Replicate GED probing for a new error type (e.g., subject-verb agreement) to validate detection-phase generality.
  2. Mask Robustness Heads at inference time and measure translation quality degradation on noisy inputs.
  3. Fine-tune with frozen Robustness Heads (identified from base model) to test whether preventing reallocation blocks robustness gains.

## Open Questions the Paper Calls Out
None

## Limitations
- Analysis is limited to synthetic errors and may not generalize to naturally occurring noise
- GED probing framework may not capture non-linear error correction dynamics
- Single-head masking may miss distributed or combinatorial error-handling mechanisms
- Representation convergence may conflate context integration with grammatical correction

## Confidence
- Detection-Correction Pipeline (High): GED probing trajectories and CKA convergence patterns are directly observed and consistently replicated across models and languages.
- Robustness Heads as Correction Mechanism (Medium): Head masking and POS attention analyses are methodologically sound, but functional validation (e.g., translation degradation when masking heads) is not provided.
- Fine-tuning Reallocates to Robustness Heads (Medium): Head overlap metrics are robust, but causal links to robustness gains are inferred rather than experimentally tested.

## Next Checks
1. Introduce naturally occurring grammatical errors from learner corpora or language model samples and repeat the GED probing and Robustness Head identification pipeline.
2. Mask combinations of Robustness Heads (e.g., top-k by influence) to assess whether error correction is distributed and to what extent heads act independently.
3. At inference, mask identified Robustness Heads and measure translation quality degradation on noisy inputs to establish causal contribution to robustness.