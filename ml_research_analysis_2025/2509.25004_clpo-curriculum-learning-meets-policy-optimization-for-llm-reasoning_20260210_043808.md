---
ver: rpa2
title: 'CLPO: Curriculum Learning meets Policy Optimization for LLM Reasoning'
arxiv_id: '2509.25004'
source_url: https://arxiv.org/abs/2509.25004
tags:
- learning
- clpo
- problems
- training
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'CLPO introduces a dynamic curriculum learning framework for LLM
  reasoning that addresses the inefficiency of uniform sampling in RLVR. The method
  constructs an online curriculum by assessing problem difficulty through the model''s
  own rollout performance, then guides adaptive problem restructuring: diversifying
  medium-difficulty problems to promote generalization and simplifying hard problems
  to make them learnable.'
---

# CLPO: Curriculum Learning meets Policy Optimization for LLM Reasoning

## Quick Facts
- **arXiv ID:** 2509.25004
- **Source URL:** https://arxiv.org/abs/2509.25004
- **Reference count:** 24
- **One-line primary result:** Qwen3-8B achieves state-of-the-art pass@1 accuracy on 8 mathematical and general reasoning benchmarks with an average improvement of 6.96% over baselines.

## Executive Summary
CLPO introduces a dynamic curriculum learning framework that addresses the inefficiency of uniform sampling in RLVR for LLM reasoning. The method constructs an online curriculum by assessing problem difficulty through the model's own rollout performance, then guides adaptive problem restructuring: diversifying medium-difficulty problems to promote generalization and simplifying hard problems to make them learnable. This creates a pedagogical feedback loop where the model acts as its own teacher. The framework integrates this curriculum signal into policy optimization via dynamic KL regularization to balance exploration and exploitation.

## Method Summary
CLPO builds on GRPO by introducing three core innovations: (1) Online curriculum assessment through on-policy rollouts that classify problems as hard (Acc ≤ 0.3) or medium (0.3 < Acc ≤ 0.7), (2) Adaptive problem restructuring where hard problems are simplified and medium problems are diversified using the model as its own teacher, and (3) Dynamic KL regularization that weakens the trust region constraint on hard problems (λ=0.3) to encourage exploration while maintaining stability on easier problems (λ=1.0). The final training batch consists of problems that survive value filtering (0 < Acc < 1) after restructuring.

## Key Results
- Achieves state-of-the-art pass@1 accuracy across eight benchmarks: MATH-500, Minerva-Math, Olympiad, AMC23, AIME24, TheoremQA, GPQA Diamond, and MMLU Pro
- Average improvement of 6.96% over baselines with 3.3% improvement on the challenging AIME2024 benchmark
- Ablation studies confirm each component's contribution: combining both restructuring strategies outperforms either alone
- Dynamic KL regularization with α=0.3 outperforms both weaker (0.1) and stronger (0.5, 0.8, 1.0) constraints

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Dynamic, performance-based curriculum improves training efficiency over uniform sampling.
- **Mechanism:** CLPO evaluates each problem's difficulty using on-policy rollouts, categorizing problems based on accuracy thresholds to create a real-time feedback loop.
- **Core assumption:** On-policy accuracy is a reliable proxy for a problem's true difficulty relative to the current model state.
- **Evidence anchors:** Abstract states CLPO uses "model's own rollout performance to conduct real-time difficulty assessment," Section 3.3 defines accuracy calculations and curriculum classification.
- **Break condition:** Mechanism fails if on-policy accuracy poorly proxies difficulty or if rollout computation costs outweigh efficiency gains.

### Mechanism 2
- **Claim:** Adaptive problem restructuring transforms static data into a tailored, learnable set.
- **Mechanism:** Hard problems are simplified and medium problems are diversified using the model as its own teacher, followed by value filtering to remove trivial cases.
- **Core assumption:** The model can reliably generate high-quality restructured problems that preserve the original answer while altering difficulty/surface form.
- **Evidence anchors:** Abstract states problems are "simplified hard ones and diversified medium ones," Section 3.4 defines restructuring function with prompts in Appendix D.
- **Break condition:** Mechanism fails if restructuring generates degenerate, incorrect, or overly similar problems.

### Mechanism 3
- **Claim:** Difficulty-aware KL regularization balances exploration and stability during policy optimization.
- **Mechanism:** Dynamic KL scaling applies lower constraints (λ=0.3) to hard problems for exploration and higher constraints (λ=1.0) to easier problems for stability.
- **Core assumption:** A single static KL penalty is a global compromise, and sample-level dynamic constraints lead to better optimization.
- **Evidence anchors:** Abstract mentions "difficulty-aware KL regularization," Section 3.5 and Figure 4 show ablation results with optimal α=0.3.
- **Break condition:** Benefit breaks if λ values are poorly tuned, causing excessive deviation or insufficient exploration.

## Foundational Learning

**Concept: Reinforcement Learning with Verifiable Rewards (RLVR)**
- Why needed here: Core training paradigm CLPO builds upon using reward signals based on final answer correctness
- Quick check question: How is the reward signal generated in RLVR, and what is its main limitation that CLPO tries to address?

**Concept: Group Relative Policy Optimization (GRPO)**
- Why needed here: CLPO's objective directly modifies GRPO objective (critic-free, group-relative advantages)
- Quick check question: In GRPO, how is the advantage Â_i,t for a response computed, and what is the role of the KL penalty?

**Concept: Curriculum Learning**
- Why needed here: High-level learning strategy CLPO implements - presenting training samples in meaningful order
- Quick check question: In traditional machine learning, what is the primary goal of curriculum learning, and how does CLPO make it dynamic?

## Architecture Onboarding

**Component map:**
Rollout & Evaluation Module -> Online Curriculum Classifier -> Adaptive Restructuring Module (Self-Teacher) -> Value-Driven Filter -> Policy Optimizer

**Critical path:** The most critical sequence is: Rollout/Eval -> Curriculum Classify -> Restructure -> Filter -> Optimize. Any failure in early steps propagates directly to final training batch and policy update.

**Design tradeoffs:**
1. Evaluation Cost: Curriculum and filtering steps require multiple rollouts, adding significant compute per training step
2. Restructuring Quality: Relying on model as its own teacher is cost-effective but may introduce noise or errors
3. Hyperparameter Sensitivity: Performance sensitive to thresholds (τ_hard, τ_med) and KL scalers (λ_hard, λ_non-hard)

**Failure signatures:**
1. Accuracy Collapse: If Acc(q) becomes 0 or 1 for most problems, filter discards nearly all data, halting learning
2. Curriculum Stagnation: Poor threshold settings may cause over-simplifying easy problems or failing to make hard problems learnable
3. Exploration Collapse: If λ_hard is too high, policy may not explore enough on hard problems

**First 3 experiments:**
1. Baseline Reproduction: Reproduce GRPO baseline on DAPO-Math-17k dataset with Qwen3-8B
2. Ablation on Core Mechanisms: Test "hard-only," "medium-only," and "both" restructuring strategies plus dynamic KL with different α values
3. Curriculum Dynamics Visualization: Track proportion of hard/medium/easy problems in B_mix over time to verify dynamic adaptation

## Open Questions the Paper Calls Out

**Open Question 1**
- Question: Can CLPO effectively generalize to reasoning domains beyond mathematics, such as code generation or scientific QA?
- Basis in paper: Conclusion states "extending this dynamic learning paradigm to other complex reasoning domains, such as code generation and scientific question-answering, presents a promising direction"
- Why unresolved: Current validation relies on datasets with verifiable final answers, whereas code or science may require different verification and restructuring strategies
- What evidence would resolve it: Application to code benchmarks (e.g., HumanEval) showing consistent pass@1 improvements over GRPO baselines

**Open Question 2**
- Question: Does replacing outcome-based accuracy with fine-grained process signals improve Online Curriculum Learning difficulty assessment precision?
- Basis in paper: Conclusion notes "our current approach to assessing problem difficulty relies primarily on the final answer's correctness; future work could explore incorporating more fine-grained process signals"
- Why unresolved: Binary accuracy metrics may misclassify problems where model guesses correctly with flawed reasoning
- What evidence would resolve it: Ablation studies comparing curriculum construction using binary rewards versus Process Reward Models (PRMs)

**Open Question 3**
- Question: Do relative performance gains of CLPO persist when scaling base model size significantly beyond 8B parameters?
- Basis in paper: Authors write "we plan to further validate its scalability on larger-scale models and datasets"
- Why unresolved: Curriculum learning effects often diminish as model capacity increases and "hard" problems become intrinsically easier
- What evidence would resolve it: Training results on 70B+ parameter models demonstrating adaptive curriculum maintains performance gap over uniform sampling

## Limitations

- **Curriculum Quality Dependency**: Method's performance hinges on model's ability to generate meaningful restructured problems, with no guarantee core reasoning challenges are preserved
- **Verifier Implementation Gap**: Paper specifies accuracy thresholds but doesn't detail Verifier operation, creating critical reproducibility bottleneck
- **Computational Overhead**: Multiple rollouts per problem introduce substantial compute overhead, though paper claims efficiency gains through better sample utilization

## Confidence

- **High Confidence**: Core algorithmic framework (online curriculum + adaptive restructuring + dynamic KL) is clearly specified and ablation studies provide strong evidence for each component's contribution
- **Medium Confidence**: State-of-the-art results depend on undisclosed Verifier implementation and specific restructuring prompts
- **Low Confidence**: Long-term generalization behavior and potential for reward hacking through problem restructuring remain unexplored

## Next Checks

1. **Verifier Implementation Audit**: Implement and test multiple Verifier variants (exact match, normalized string, symbolic solver) to quantify their impact on curriculum classification and final performance
2. **Curriculum Dynamics Analysis**: Log and visualize the evolution of hard/medium/easy problem ratios over training time to verify curriculum is genuinely adapting to model capabilities
3. **Restructuring Quality Control**: Implement automated checks to verify simplified problems maintain semantic equivalence to originals and diversified problems generate genuinely novel surface forms