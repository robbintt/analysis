---
ver: rpa2
title: Reducing Hallucinations of Medical Multimodal Large Language Models with Visual
  Retrieval-Augmented Generation
arxiv_id: '2502.15040'
source_url: https://arxiv.org/abs/2502.15040
tags:
- v-rag
- image
- images
- entity
- probing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Visual RAG (V-RAG) to mitigate hallucinations
  in Medical Multimodal Large Language Models (Med-MLLMs). The core idea is to incorporate
  both text and visual data from retrieved images during inference, allowing the model
  to make more informed decisions.
---

# Reducing Hallucinations of Medical Multimodal Large Language Models with Visual Retrieval-Augmented Generation

## Quick Facts
- **arXiv ID:** 2502.15040
- **Source URL:** https://arxiv.org/abs/2502.15040
- **Reference count:** 3
- **Primary result:** Visual RAG (V-RAG) improves entity grounding accuracy in medical images and reduces hallucinations in chest X-ray report generation

## Executive Summary
This paper addresses hallucinations in Medical Multimodal Large Language Models (Med-MLLMs) by introducing Visual Retrieval-Augmented Generation (V-RAG). The method incorporates text and visual data from retrieved images during inference, enabling the model to make more informed decisions about entity presence in medical images. Entity probing is used to evaluate accuracy, with improvements shown for both frequent and rare medical entities. The approach is enhanced through fine-tuning tasks that improve image-text association abilities, and downstream application to chest X-ray report revision demonstrates higher clinical accuracy.

## Method Summary
The paper proposes V-RAG, which extends retrieval-augmented generation to multimodal medical contexts. The method uses BiomedCLIP embeddings to retrieve relevant images and reports from a medical database. A fine-tuning pipeline prepares models for multi-image reasoning through three specific tasks: identifying which image matches given text, generating reports focused on specific images, and simulating V-RAG retrieval. During inference, top-k retrieved context is used for entity probing, and results are fed to a large language model for final report revision. The approach is evaluated on MIMIC-CXR and MultiCaRe datasets using entity probing F1 scores and RadGraph-F1 metrics.

## Key Results
- V-RAG improves entity probing F1 scores for both frequent and rare medical entities compared to baseline Med-MLLMs
- The fine-tuning tasks enable effective multi-image reasoning in single-image native models like LLaVA
- Downstream application to chest X-ray report revision shows higher RadGraph-F1 scores, indicating improved clinical accuracy
- V-RAG demonstrates effectiveness in reducing hallucinations while maintaining or improving report quality

## Why This Works (Mechanism)
The core mechanism leverages the principle that grounding generation in retrieved evidence reduces hallucinations. By providing the model with relevant visual and textual context from similar cases, V-RAG enables more informed decision-making about entity presence. The fine-tuning tasks specifically train the model to understand relationships between multiple images and text, addressing the multi-image reasoning capability gap in many Med-MLLMs. The entity probing approach provides a direct evaluation of whether the model can accurately ground entities in visual evidence, rather than relying solely on generation quality metrics.

## Foundational Learning
- **Concept: Retrieval-Augmented Generation (RAG)**
  - Why needed here: V-RAG is fundamentally a visual extension of RAG. Understanding how retrieved evidence grounds generation is essential.
  - Quick check question: Can you explain how providing retrieved documents to an LLM at inference differs from knowledge stored in its weights?

- **Concept: Multimodal Embedding Alignment**
  - Why needed here: BiomedCLIP's shared embedding space enables image-text retrieval. Misalignment causes irrelevant retrievals.
  - Quick check question: What properties must a joint image-text embedding space have to be useful for retrieval?

- **Concept: Entity Probing vs. NLG Metrics**
  - Why needed here: Entity probing provides direct evaluation of entity grounding accuracy, which traditional metrics like ROUGE may miss.
  - Quick check question: Why might a metric like ROUGE fail to detect a clinically significant hallucination in a medical report?

## Architecture Onboarding

**Component Map:** MIMIC-CXR/MultiCaRe Data -> Stanza NER -> Llama-2-7B -> VQA Pairs -> BiomedCLIP Embeddings -> FAISS Index -> Retrieval -> V-RAG Model -> Entity Probing -> Llama 3.1 70B -> Revised Reports

**Critical Path:** Query -> BiomedCLIP Retrieval (k=3-5) -> V-RAG Fine-tuned Model -> Entity Probing -> Llama 3.1 70B Revision -> RadGraph-F1 Evaluation

**Design Tradeoffs:** 
- Single vs. multi-image model architecture (RadFM native vs. LLaVA adapted)
- Retrieval count k (k=5 for RadFM, k=3 for LLaVA due to context limits)
- Model scale (LoRA fine-tuning vs. full fine-tuning)

**Failure Signatures:**
- Context overflow with large k values in LLaVA
- Entity probing noise with imbalanced rare entity examples
- Retrieval irrelevance due to BiomedCLIP embedding misalignment

**First Experiments to Run:**
1. Test BiomedCLIP retrieval quality on MIMIC-CXR using k=1,3,5 retrievals
2. Evaluate entity probing F1 on a small validation set before full fine-tuning
3. Run report revision on 10 examples to check for catastrophic forgetting

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but based on the methodology and results, several important questions arise:

### Open Question 1
- Question: What is the optimal number of retrieved images (k) for V-RAG across different medical imaging tasks and model architectures?
- Basis in paper: The paper arbitrarily sets k=5 for RAG-based methods and k=3 for LLaVA experiments without systematic ablation.
- Why unresolved: The optimal k likely depends on context length constraints, retrieval quality, and task complexity.
- What evidence would resolve it: Ablation experiments varying k (1, 3, 5, 7, 10) across both datasets measuring entity probing F1 scores, latency, and report quality.

### Open Question 2
- Question: How robust is V-RAG to noisy or irrelevant retrievals in real-world clinical settings?
- Basis in paper: The paper uses BiomedCLIP embeddings but does not evaluate performance degradation with dissimilar or conflicting retrievals.
- Why unresolved: Clinical databases may contain mislabeled images or reports, and retrieval systems are imperfect.
- What evidence would resolve it: Experiments injecting varying proportions of irrelevant retrievals and measuring performance degradation.

### Open Question 3
- Question: Does V-RAG generalize effectively to other medical imaging modalities beyond chest X-rays?
- Basis in paper: The paper primarily focuses on chest X-rays despite testing on MultiCaRe (various medical images).
- Why unresolved: Different imaging modalities have distinct visual characteristics and report structures.
- What evidence would resolve it: Evaluation on diverse medical imaging datasets (CT, MRI, pathology) with modality-specific metrics.

### Open Question 4
- Question: What is the computational and latency overhead of V-RAG during inference compared to baseline Med-MLLMs?
- Basis in paper: The paper introduces retrieval and multi-image encoding but does not report inference time or memory usage.
- Why unresolved: Clinical deployment requires real-time responses; overhead may limit practical applicability.
- What evidence would resolve it: Benchmarks of inference latency, memory consumption, and throughput for V-RAG versus baselines.

## Limitations
- Entity probing methodology relies on automatically generated VQA pairs using Stanza NER and Llama-2-7B, potentially introducing systematic biases
- Fine-tuning procedure modifies base models for multi-image inputs, but exact prompt templates are not provided, making exact replication difficult
- Effectiveness is primarily demonstrated on chest X-rays from MIMIC-CXR with limited validation on other medical imaging modalities
- Significant computational resources required (Llama 3.1 70B for final report revision) may limit practical deployment

## Confidence

**High confidence:** The core V-RAG retrieval methodology and its application to entity probing (Entity Probing F1 improvements are clearly demonstrated)

**Medium confidence:** The effectiveness of fine-tuning tasks for multi-image reasoning (task descriptions are clear but implementation details are sparse)

**Medium confidence:** Downstream report revision results (RadGraph-F1 improvements shown, but evaluation is limited to one dataset)

## Next Checks

1. Replicate the entity probing results on an independently generated test set using different NER tools and LLM models to verify robustness

2. Implement the fine-tuning tasks using only the paper descriptions and test on a held-out subset to verify the multi-image reasoning capabilities

3. Apply V-RAG to a different medical imaging domain (e.g., pathology slides or CT scans) to assess generalization beyond chest X-rays