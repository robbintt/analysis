---
ver: rpa2
title: 'Reviving Your MNEME: Predicting The Side Effects of LLM Unlearning and Fine-Tuning
  via Sparse Model Diffing'
arxiv_id: '2507.21084'
source_url: https://arxiv.org/abs/2507.21084
tags:
- fine-tuning
- mneme
- arxiv
- unlearning
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MNEME addresses the challenge of detecting unintended side effects
  from LLM fine-tuning or unlearning without requiring access to the fine-tuning data.
  It uses sparse model diffing, comparing base and fine-tuned model activations on
  task-agnostic data to identify behavioral shifts.
---

# Reviving Your MNEME: Predicting The Side Effects of LLM Unlearning and Fine-Tuning via Sparse Model Diffing

## Quick Facts
- arXiv ID: 2507.21084
- Source URL: https://arxiv.org/abs/2507.21084
- Reference count: 11
- Primary result: MNEME achieves up to 95% accuracy in predicting LLM fine-tuning side effects without access to fine-tuning data

## Executive Summary
MNEME addresses the challenge of detecting unintended side effects from LLM fine-tuning or unlearning without requiring access to the fine-tuning data. It uses sparse model diffing, comparing base and fine-tuned model activations on task-agnostic data to identify behavioral shifts. By learning sparse latent directions and quantifying feature changes via latent scaling, MNEME can attribute these shifts to specific semantic categories. Across three scenarios—WMDP knowledge unlearning, emergent misalignment, and benign fine-tuning—MNEME achieves up to 95% accuracy in predicting side effects, often approaching oracle-level performance.

## Method Summary
MNEME trains a shared sparse encoder with model-specific decoders (Cross-Coder) on layer-14 activations from base and fine-tuned models using task-agnostic data. The Cross-Coder uses BatchTopK for sparsification and learns to reconstruct each model's activations separately. Latent scaling coefficients quantify feature changes between models, while auto-interpretation using a large LLM maps activated features to semantic categories. The method predicts which knowledge areas will be affected without access to the fine-tuning data itself.

## Key Results
- Achieves 95% accuracy in predicting knowledge categories affected by WMDP unlearning
- Detects emergent misalignment side effects with 68.2% accuracy
- Approaches oracle performance in benign fine-tuning scenarios (82-94% accuracy)

## Why This Works (Mechanism)

### Mechanism 1: Sparse Feature Decomposition via Cross-Coders
Joint sparse cross-coders isolate semantically meaningful differences between base and fine-tuned models by learning a shared latent space with model-specific decoders. A shared encoder maps concatenated base and fine-tuned activations to sparse codes via BatchTopK; dual decoders reconstruct each model's activations separately. Latents that contribute asymmetrically to reconstruction capture model-specific behavioral shifts. Core assumption: Polysemantic neurons can be disentangled into approximately monosemantic features through sparse overcomplete representations (expansion factor 32), and changes in decoder norms reflect behavioral importance changes.

### Mechanism 2: Latent Scaling for Attribution
Regression-based scaling coefficients quantify whether each latent feature is amplified, suppressed, or unchanged after fine-tuning/unlearning. For each latent j, remove it from the decoder and fit scaling coefficients β(b)_j and β(f)_j that minimize reconstruction error for base and fine-tuned models respectively. Magnitude changes indicate amplification (increase) or minimization (decrease). Core assumption: Decoder weight norm changes correlate with behavioral importance of features; loss-based attribution provides meaningful signal for side-effect detection.

### Mechanism 3: Task-Agnostic Corpus Coverage
Diverse corpora (The Pile, LMSYS-Chat-1M) contain sufficient semantic coverage to activate affected features without requiring access to fine-tuning data. Same task-agnostic inputs pass through both models; corpus diversity ensures broad concept coverage including domain-specific and harmful knowledge, serving as a proxy for detecting behavioral shifts. Core assumption: Task-agnostic data activates latent features corresponding to affected semantic categories, even when fine-tuning is narrow.

## Foundational Learning

- **Concept: Sparse Autoencoders and Cross-Coders**
  - Why needed here: MNEME extends SAEs to cross-model comparison via Cross-Coders. Understanding sparsity, reconstruction loss, dead latents, and expansion factors is essential for diagnosing feature quality.
  - Quick check question: Why does an overcomplete dictionary (expansion factor >1) help disentangle polysemantic neurons into monosemantic features?

- **Concept: Polysemanticity and Superposition**
  - Why needed here: Side effects arise because multiple features share representations. Understanding why narrow unlearning causes broad degradation (e.g., biology → chemistry) requires grasping superposition.
  - Quick check question: According to superposition theory, why might removing biology knowledge inadvertently impair chemistry performance?

- **Concept: Model Diffing**
  - Why needed here: The core operation compares base vs. fine-tuned representations. Distinguishing what diffing reveals versus single-model probing clarifies why this approach detects side effects without fine-tuning data.
  - Quick check question: What information does cross-model diffing provide that single-model sparse autoencoder analysis cannot?

## Architecture Onboarding

- **Component map:**
  Base activations (layer 14) ──┐
                             ├──> Shared Encoder Ψϕ ──> BatchTopK(k=100) ──> z_i
  Fine-tuned activations ──────┘                                               │
                                                                               ├──> Decoder D^(b) ──> â^(b)
                                                                               └──> Decoder D^(f) ──> â^(f)

  z_i ──> Latent Scaling (β coefficients) ──> Attribution (amplified/suppressed/shared)

  Top-k activating contexts ──> LLaMA-3.1-70B-Instruct ──> Feature descriptions ──> Semantic category mapping

- **Critical path:**
  1. Extract layer-14 activations from both models on ~200M tokens of task-agnostic corpus
  2. Train Cross-Coder with BatchTopK sparsification (expansion factor 32, k=100)
  3. Compute latent scaling coefficients to attribute features
  4. Run auto-interpretation on top-activating contexts for each relevant latent
  5. Map descriptions to semantic categories for downstream evaluation

- **Design tradeoffs:**
  - Layer selection: Layer 14 balances semantic and syntactic signals; earlier layers are too syntactic, later too abstract (per Minder et al. 2025)
  - Expansion factor: 32 yields fine-grained monosemantic features; lower factors (6, 12) produce broader, less interpretable features
  - Corpus choice: The Pile for base/pretrained models; LMSYS-Chat-1M for instruction-tuned models (paper reports minimal performance difference)
  - k value: 100 balances sparsity with reconstruction quality

- **Failure signatures:**
  - Dead latent rate >15% or explained variance <95% → Cross-Coder quality insufficient
  - Low semantic overlap (<40%) between activated latents and auto-interpretation → feature quality issues
  - Performance degrades on larger models → representational entanglement obscuring attribution
  - Reconstruction loss insensitive to latent removal → salience scores near zero

- **First 3 experiments:**
  1. Validation run: Train Cross-Coder on 10M token subset; verify dead latent rate <15% and explained variance >95% before committing to 200M token training
  2. Hyperparameter sweep: Compare k ∈ {50, 100, 200} on held-out evaluation to confirm optimal sparsity for your model size
  3. Corpus sensitivity test: Run MNEME on same model pair using The Pile vs. LMSYS-Chat-1M; verify detected features remain consistent (paper reports minimal difference for instruction-tuned settings)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can MNEME's correlational insights be extended to provide causal guarantees regarding the relationship between specific latent shifts and observed side effects?
- Basis in paper: [explicit] The authors explicitly state in the Limitations section that "MNEME provides correlational insights rather than causal guarantees, and cannot definitively attribute observed side effects to specific interventions without controlled experiments."
- Why unresolved: The current framework detects associations between feature amplification and behavioral changes but lacks a mechanism to verify if modifying the identified latent features directly causes the outcome.
- What evidence would resolve it: Experiments involving causal interventions, such as manually suppressing or amplifying the specific latents identified by MNEME to observe if the predicted side effect is induced or reversed.

### Open Question 2
- Question: How can Cross-Coder architectures be optimized to better detect side effects in scenarios involving narrow or highly specialized fine-tuning?
- Basis in paper: [explicit] The Analysis section notes that "narrow fine-tuning remains a challenge, as sparse autoencoders are not designed for diffing; however, using dual decoders partially addresses this, and architectural improvements could further improve performance."
- Why unresolved: Standard sparse autoencoders struggle with the distribution shifts in narrow fine-tuning, and the current dual-decoder adaptation is a partial solution rather than a fully optimized architecture for diffing.
- What evidence would resolve it: The development of modified Cross-Coder architectures specifically tailored for diffing, showing improved performance on benchmarks with limited fine-tuning data compared to the current BatchTopK method.

### Open Question 3
- Question: To what extent does the reliance on general-purpose, task-agnostic corpora limit MNEME's ability to detect behavioral shifts in specialized or out-of-distribution domains?
- Basis in paper: [explicit] The Limitations section states that MNEME "relies on task-agnostic corpora... which may not always reflect the specific distribution of fine-tuning tasks—particularly in narrow or specialized domains—limiting its ability to capture certain shifts."
- Why unresolved: It is unclear if the semantic coverage of general datasets like The Pile is sufficient to surface features relevant to all specialized fine-tuning tasks (e.g., obscure coding languages or niche medical fields).
- What evidence would resolve it: An evaluation of MNEME on models fine-tuned on highly specialized datasets not well-represented in The Pile, measuring the false negative rate for side effects.

## Limitations
- Performance relies on task-agnostic corpora containing sufficient semantic coverage for narrow fine-tuning scenarios
- Detection accuracy degrades on larger models due to representational entanglement
- Cannot definitively distinguish intended fine-tuning effects from unintended side effects without human judgment

## Confidence
- **High Confidence:** The core mechanism of sparse model diffing via Cross-Coders has strong theoretical grounding in sparse autoencoders and geometric disentanglement literature. Reported performance metrics (up to 95% accuracy) are well-supported by ablation studies and error analysis.
- **Medium Confidence:** The task-agnostic corpus assumption is reasonable but untested across diverse fine-tuning scenarios. The scaling-based attribution method works well empirically but lacks theoretical guarantees about its relationship to behavioral importance.
- **Medium Confidence:** The semantic mapping approach using LLaMA-3.1-70B-Instruct for auto-interpretation is effective but introduces potential LLM-based bias and requires careful prompt engineering that is not fully specified in the paper.

## Next Checks
1. **Corpus Coverage Validation:** Test MNEME on a narrow fine-tuning scenario (e.g., domain-specific terminology) where the fine-tuning data is known but withheld from MNEME. Measure how performance degrades as fine-tuning becomes increasingly specialized relative to the task-agnostic corpus.

2. **Scaling Attribution Ablation:** Systematically disable the scaling coefficient computation and instead use alternative attribution methods (e.g., SHAP values, integrated gradients) on the same Cross-Coder latents. Compare whether detection accuracy remains comparable or if the scaling method provides unique signal.

3. **Cross-Layer Consistency Test:** Apply MNEME at multiple layers (layers 8, 12, 16 in addition to layer 14) for the same model pairs. Quantify whether side effects detected at different layers correlate with different semantic granularities or whether layer 14 consistently provides optimal signal across scenarios.