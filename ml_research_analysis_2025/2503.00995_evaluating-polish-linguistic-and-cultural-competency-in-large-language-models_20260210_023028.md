---
ver: rpa2
title: Evaluating Polish linguistic and cultural competency in large language models
arxiv_id: '2503.00995'
source_url: https://arxiv.org/abs/2503.00995
tags:
- polish
- language
- questions
- uni00000013
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Polish linguistic and cultural competency
  benchmark, a novel evaluation dataset consisting of 600 manually crafted questions
  across six categories (history, geography, culture & tradition, art & entertainment,
  grammar, and vocabulary) designed to assess large language models' knowledge of
  Polish cultural context and language nuances. The evaluation employs a deterministic
  rule-based system with binary scoring for each question.
---

# Evaluating Polish linguistic and cultural competency in large language models

## Quick Facts
- arXiv ID: 2503.00995
- Source URL: https://arxiv.org/abs/2503.00995
- Reference count: 40
- Large performance gap exists between commercial (83%) and open (62%) models on Polish cultural understanding

## Executive Summary
This paper introduces the Polish Linguistic and Cultural Competency (PLCC) benchmark, a novel evaluation dataset of 600 manually crafted questions across six categories designed to assess large language models' knowledge of Polish cultural context and language nuances. The benchmark employs a deterministic rule-based system with binary scoring to evaluate both linguistic understanding and cultural knowledge. Experiments with over 30 commercial and open-weight models reveal a substantial performance gap, with commercial models significantly outperforming open models, and demonstrate that specialized Polish models can match much larger multilingual models despite fewer parameters.

## Method Summary
The PLCC benchmark consists of 600 questions across six categories (history, geography, culture & tradition, art & entertainment, grammar, and vocabulary) with deterministic verification rules. Questions are manually crafted to elicit specific entities and facts, and responses are normalized through lowercasing and lemmatization before evaluation. The rule-based system uses four condition types: Include (keyword presence with CNF logic), Exclude (forbidden words), Order (sequence requirements), and Regex (pattern matching). Evaluation uses temperature=0, no system prompt, and single user messages with model-specific chat templates. Each question receives binary scoring (0 or 1), with all conditions needing to pass for credit.

## Key Results
- Commercial models significantly outperform open models (Gemini-Exp-1206 at 83% vs. best open at 62%)
- Specialized Polish model Bielik-2.3 (11B parameters) matches much larger multilingual models despite fewer parameters
- Performance gap persists across all categories, with largest gaps in history and geography
- Some newer model versions show regression on cultural competency benchmarks

## Why This Works (Mechanism)

### Mechanism 1: Deterministic Rule-Based Verification
- Claim: Cultural and linguistic competency can be evaluated deterministically without relying on LLM-as-a-judge by using strict, binary rule-sets applied to normalized model outputs.
- Mechanism: The authors replace subjective scoring with a programmatic pipeline. Questions are manually crafted to elicit specific entities (names, dates, phrases). The evaluation system normalizes the response (lowercasing, lemmatization) and applies logical conditions (Include, Exclude, Order, Regex). If the normalized output satisfies all conjunctive conditions, the score is 1; otherwise, it is 0.
- Core assumption: Cultural knowledge can be effectively reduced to atomic facts (keywords, word order) that survive lemmatization, and the model can be instructed to output these facts in a strict format.
- Evidence anchors:
  - [abstract] Mentions a "deterministic rule-based system with binary scoring."
  - [section 2.1] Details the grading process: "normalization involves... lemmatizing the text... check each of the conditions defined in the question."
  - [corpus] *HKCanto-Eval* utilizes a similar structured approach for Cantonese, suggesting generalizability of deterministic evaluation for cultural benchmarks, though specific implementation details vary.
- Break condition: This mechanism fails for questions requiring complex reasoning, long-form explanation, or nuance that cannot be captured by keyword presence or regex patterns (e.g., "Explain the sentiment of this poem").

### Mechanism 2: Language-Specific Knowledge Density
- Claim: Continuous pre-training on a high-quality, language-specific corpus allows smaller models (7B-11B parameters) to match or exceed the cultural competency of much larger multilingual models.
- Mechanism: General multilingual models suffer from the "curse of multilinguality" or simple underrepresentation, diluting cultural specificities. By upscaling a base model (Mistral-7b to 11B) and continuously training on Polish data (Bielik), the model increases the density of relevant cultural weights, compensating for the lack of general reasoning capacity found in larger models.
- Core assumption: The bottleneck for cultural competency is primarily the availability and density of specific training data rather than the model's general reasoning capabilities (parameter count).
- Evidence anchors:
  - [section 3] The Bielik-2.3 model (11B) achieved 62.17%, beating Qwen-2.5-72b (39.17%) and approaching Llama-3.3-70b (48.83%).
  - [table 1] Shows Bielik improving by approx. 40 points over the base Mistral-7b-v0.3.
  - [corpus] *Bielik 11B v2 Technical Report* provides further evidence that depth up-scaling and specific optimization for Polish text yield state-of-the-art results in this domain.
- Break condition: The performance advantage may degrade on tasks requiring heavy general-world reasoning or logic that benefits from extreme scale (e.g., obscure riddles requiring vast knowledge fusion), where larger models still dominate.

### Mechanism 3: Closed-Weight Performance Asymmetry
- Claim: There is a persistent performance gap between commercial and open-weight models in nuanced cultural understanding, contrasting with the trends seen in general English-centric benchmarks.
- Mechanism: Commercial providers likely utilize proprietary, high-quality data pipelines that include curated cultural sources (literature, textbooks) and advanced alignment techniques (RLHF) specifically tuned for nuance. Open models, often fine-tuned on synthetic or generic web data, lack this "deep" cultural alignment, resulting in hallucinations or cultural misinterpretations (e.g., Qwen's morphological errors in Figure 1).
- Core assumption: The gap is driven by data quality and alignment methodology rather than just model architecture or raw size.
- Evidence anchors:
  - [abstract] "Commercial models... significantly outperform open models (best at 62%)."
  - [section 3] "For multilingual abilities... open models are still significantly behind."
  - [corpus] *The Shrinking Landscape of Linguistic Diversity* suggests that without specific intervention, LLMs default to dominant cultural narratives, supporting the need for the proprietary alignment likely present in commercial leaders.
- Break condition: If a new open-weight model is released with a specifically curated, massive-scale Polish cultural instruction set (applying Mechanism 2 at scale), this gap would likely narrow significantly.

## Foundational Learning

- Concept: **Lemmatization vs. Stemming in Inflected Languages**
  - Why needed here: Polish is a highly inflected language. The evaluation pipeline (Section 2.1) relies on lemmatization—reducing words to their base dictionary form—to match model outputs against ground truth keywords. Without this, a correct answer using a different grammatical case would be marked incorrect.
  - Quick check question: If the target keyword is "woda" (water) and the model outputs "wody" (genitive case), would a simple string match succeed?

- Concept: **Conjunctive Normal Form (CNF) in Evaluation Logic**
  - Why needed here: The "Include" condition in Section 2.1 verifies logical formulas in CNF. This means you can require (A AND B) OR (C AND D). Understanding this logic is crucial for reading the benchmark's scoring rules.
  - Quick check question: Does the condition `(A OR B) AND (C OR D)` represent the CNF structure described for the "Include" check?

- Concept: **Depth Up-Scaling**
  - Why needed here: The successful Bielik model (Corpus & Section 3) was created by taking Mistral-7b and "upscaling" it to 11B parameters before training. This is a distinct architectural choice compared to training a native 11B model or using a Mixture of Experts (MoE).
  - Quick check question: Why might adding depth (layers) to a smaller pre-trained model be more parameter-efficient for language adaptation than widening the layers?

## Architecture Onboarding

- Component map:
  - Input Processor -> Inference Engine -> Normalizer -> Rule Engine -> Scorer
  - Raw question -> Generate response -> Normalize text (lemmatize) -> Apply conditions -> Binary score

- Critical path: The **Normalizer** and **Rule Engine** are the most fragile components. The lemmatizer must perfectly handle Polish morphology, or valid answers will fail the "Include" checks. Similarly, the Regex engine must handle the "unnormalized" version of the response correctly for specific spelling checks.

- Design tradeoffs:
  - **Binary vs. Partial Scoring**: The authors chose strict binary scoring (0 or 1). This simplifies analysis but penalizes models that get 90% of a multi-part answer correct. It prioritizes precision over partial credit.
  - **Deterministic vs. LLM-Judge**: Using rules removes judge bias and cost but limits the benchmark to questions with structured answers (closed-ended or short factual), ignoring open-ended cultural fluency.

- Failure signatures:
  - **False Negatives**: A model provides a correct answer using a synonym or a valid grammatical form not covered by the lemmatizer or the specific "Include" list.
  - **Format mismatch**: The model provides the correct fact but wraps it in conversational filler ("The answer is X"), and the regex/rule fails to extract the target entity (though the paper claims to instruct models to be short/precise).

- First 3 experiments:
  1. **Lemmatization Ablation**: Run a subset of the benchmark (e.g., Grammar category) using only stemming instead of lemmatization to quantify the error rate introduced by morphological complexity.
  2. **Error Taxonomy Analysis**: Take the failing responses from the best open model (DeepSeek-V3) and categorize them: are they hallucinations, format failures, or cultural misunderstandings?
  3. **Cross-lingual Transfer Check**: Evaluate a strong English-centric model (e.g., Llama-3-70b) using the benchmark questions translated to English vs. the native Polish questions to isolate the "cultural" gap from the "language processing" gap.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific factors drive the ~12 percentage point gap between the best commercial models (83%) and best open-weight models (71%) on Polish cultural and linguistic competency?
- Basis in paper: [explicit] Authors state: "In contrast, our evaluation shows that for multilingual abilities, particularly understanding cultural and linguistic context, open models are still significantly behind those offered as a service."
- Why unresolved: The paper identifies the gap but does not isolate whether it stems from training data composition, model scale, proprietary training techniques, or post-training alignment procedures.
- What evidence would resolve it: Ablation studies comparing open and commercial models with controlled training data and techniques, or analysis of training corpus composition for Polish content across model families.

### Open Question 2
- Question: What mechanisms enable small language-specific models like Bielik-2.3 (11B parameters) to match or exceed much larger multilingual models on cultural competency tasks?
- Basis in paper: [explicit] Authors note: "Bielik-2.3... despite its small size of just 11B parameters... reached second place among open models... If we compare both models, we can see that Bielik improved by more than 40 points over Mistral."
- Why unresolved: The paper demonstrates the phenomenon but does not explain whether gains come from continuous pre-training on Polish corpora, instruction fine-tuning specifics, or architectural modifications.
- What evidence would resolve it: Systematic comparison of Bielik training stages with controlled baselines, or replication of the language-specific training approach on other base models.

### Open Question 3
- Question: Why do some newer model versions show regression on cultural competency benchmarks (e.g., GPT-4o, Mistral-Large, Bielik in Figure 3)?
- Basis in paper: [inferred] Figure 3 shows performance comparisons across model versions released in 2024, with authors noting: "In most cases, we did not see an improvement. For the Mistral-Large, Bielik, and GPT-4o models, the latest model even achieves a slightly worse score than earlier versions."
- Why unresolved: The paper does not investigate whether regression reflects changes in training objectives, alignment procedures affecting multilingual performance, or benchmark-specific artifacts.
- What evidence would resolve it: Longitudinal evaluation across multiple Polish benchmarks, or controlled analysis of training objective changes between model versions.

### Open Question 4
- Question: To what extent does rule-based binary scoring with lemmatization capture genuine cultural understanding versus surface-level pattern matching?
- Basis in paper: [inferred] The methodology relies on deterministic conditions checking for word presence, order, or regex patterns, with authors acknowledging: "assessing the correctness of a textual response is more challenging than evaluating a structured response."
- Why unresolved: The binary scoring system cannot award partial credit for partially correct cultural knowledge, and normalization through lemmatization may miss morphological nuances important for grammatical competency.
- What evidence would resolve it: Comparison with human evaluation or LLM-as-judge approaches on a subset of questions, or analysis of failure cases where models produce culturally appropriate but rule-failing responses.

## Limitations
- Binary scoring system may not fully capture nuanced cultural understanding
- Benchmark restricted to factual knowledge that can be reduced to discrete entities
- Performance gap may reflect data availability differences rather than fundamental architectural advantages
- Evaluation methodology may systematically undervalue models with different linguistic expressions

## Confidence

**High Confidence**: The deterministic rule-based evaluation mechanism is technically sound and well-documented. The binary scoring approach is clearly defined, and the performance gap between commercial and open models is consistently observed across multiple model sizes and categories. The correlation between language-specific training data and cultural competency performance is strongly supported by the Bielik-2.3 results.

**Medium Confidence**: The claim that commercial models significantly outperform open models specifically due to cultural alignment methodology, rather than just scale or data volume, is plausible but not definitively proven. The mechanism explaining how depth up-scaling compensates for parameter count in cultural tasks is theoretically reasonable but would benefit from more ablation studies comparing different architectural choices.

**Low Confidence**: The assertion that the binary scoring system adequately measures cultural competency in its entirety is questionable. The benchmark may be systematically undervaluing models that demonstrate cultural understanding through different linguistic expressions or reasoning patterns not captured by the current rule set.

## Next Checks

1. **Cross-linguistic Transfer Validation**: Evaluate the same set of questions (translated to English) using English-centric models to quantify how much of the performance gap is due to language processing versus cultural knowledge gaps.

2. **Error Pattern Analysis**: Conduct a detailed failure mode analysis of the top-performing open model (DeepSeek-V3) to categorize errors into hallucinations, format violations, or genuine cultural misunderstandings, and compare these patterns against commercial model errors.

3. **Partial Credit Experiment**: Implement a partial scoring variant of the benchmark that awards fractional points for partially correct answers, then re-run evaluations to assess whether the commercial-open gap persists under less stringent evaluation criteria.