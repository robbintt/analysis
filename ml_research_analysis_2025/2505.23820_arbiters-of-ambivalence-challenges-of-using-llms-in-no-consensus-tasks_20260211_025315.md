---
ver: rpa2
title: 'Arbiters of Ambivalence: Challenges of Using LLMs in No-Consensus Tasks'
arxiv_id: '2505.23820'
source_url: https://arxiv.org/abs/2505.23820
tags:
- arxiv
- answer
- than
- more
- stance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines the reliability of large language models (LLMs)
  in roles where human annotators typically disagree, such as judges or debaters.
  The authors develop a "no-consensus" benchmark consisting of 10 datasets covering
  moral dilemmas, causal ambiguity, linguistic inference, controversial questions,
  cultural norms, subjective topics, and known unknowns.
---

# Arbiters of Ambivalence: Challenges of Using LLMs in No-Consensus Tasks

## Quick Facts
- **arXiv ID**: 2505.23820
- **Source URL**: https://arxiv.org/abs/2505.23820
- **Reference count**: 40
- **Primary result**: LLMs maintain neutrality as answer generators but consistently adopt stances when serving as judges or debaters on no-consensus topics

## Executive Summary
This paper examines whether large language models can appropriately handle tasks where human annotators inherently disagree. The authors create a "no-consensus" benchmark spanning 10 datasets covering moral dilemmas, causal ambiguity, linguistic inference, controversial questions, cultural norms, subjective topics, and known unknowns. They evaluate five state-of-the-art models across three operational modes: answer generation, judging (pairwise/pointwise), and debating. Results show that while models maintain high neutrality as answer generators, they consistently adopt stances when serving as judges or debaters, with pairwise judging showing the most dramatic neutrality drop. This suggests current alignment methods may create an illusion of neutrality that only appears when LLMs function as answer generators rather than evaluators.

## Method Summary
The study constructs a No-Consensus Benchmark by filtering 10 datasets for high entropy (questions where no single answer exceeds 70% frequency). For each question, opposing stances are generated by prompting the same model to argue each position. Five models (Claude 3.5 Haiku, GPT-4o, Llama 3.1 8B/70B, Mistral 7B) are evaluated across four modes: constrained answer generation with explicit neutral option, pointwise judge (0-10 scoring), pairwise judge (A/B/Tie), and debater (3-round transcripts judged by another model). Neutrality is measured as the percentage of neutral/tie responses across three inference runs, with position counterbalancing to control for presentation bias.

## Key Results
- LLMs show high neutrality (>70%) as answer generators but drop significantly when acting as judges or debaters
- Pairwise judging produces the largest neutrality degradation, with models exhibiting strong preferences regardless of answer quality
- Judge models maintain consistent stances across different debater combinations, suggesting stance preferences are inherent rather than contextually constructed
- The neutrality drop persists across diverse task categories and model types

## Why This Works (Mechanism)

### Mechanism 1: Mode-Dependent Neutrality Degradation
LLM neutrality varies dramatically across operational modes, with answer generators remaining neutral but judges/debaters consistently taking stances on no-consensus topics. The prompt structure and task framing force models into comparative evaluation modes that activate implicit preferences. Answer generation allows nuanced hedging; pairwise comparison requires selection, triggering stance adoption even when both options are equally valid.

### Mechanism 2: Comparative Evaluation Bias in Pairwise Settings
Pairwise judging produces the largest neutrality drop because forced comparison activates implicit ranking preferences regardless of answer quality. When asked "Which is better?", models retrieve learned associations between response features (length, citations, authoritative tone) and quality judgments. These surface features become proxies for correctness even when content is equivalent.

### Mechanism 3: Judge Stance Persistence Across Debater Quality
When models serve as debate judges, their verdict remains stable regardless of which models produced the debate transcripts or argument quality. Judge models bring pre-existing stance preferences into evaluation. Debate transcripts provide post-hoc rationalization rather than genuine persuasion, suggesting stance preferences are baked into model weights rather than contextually constructed.

## Foundational Learning

- **High-entropy label distributions**: Why needed - The paper defines "no-consensus" questions using entropy thresholds (t ≤ 0.7) to identify where human annotators genuinely disagree. Quick check - Given a binary question with 60/40 human split, would it pass the no-consensus threshold?

- **Position bias in LLM evaluation**: Why needed - The paper explicitly counterbalances response order (swapping Yes/No positions) because models prefer first-presented options. Quick check - If you run pairwise comparison without position swapping, what artifact would appear in results?

- **Pointwise vs. pairwise evaluation paradigms**: Why needed - The paper shows these paradigms produce different neutrality outcomes. Pointwise allows independent scoring; pairwise forces relative ranking that triggers stance-taking. Quick check - In which paradigm would you expect higher neutrality for ambivalent questions?

## Architecture Onboarding

- **Component map**: Benchmark construction → stance generation → evaluation modes → neutrality calculation
- **Critical path**: Curate high-entropy questions → generate opposing stances → run evaluation in target mode → extract verdicts → compute neutrality percentages → compare across modes
- **Design tradeoffs**: Constrained vs. open-ended generation (constrained maximizes neutrality potential), expanded stances vs. one-word responses (one-word responses produce artificial neutrality), multiple inference rounds (increases cost but captures variance)
- **Failure signatures**: Neutrality near 0% in pairwise judge mode indicates working correctly, neutrality near 100% in pointwise mode for one-word responses indicates input inadequacy, large variance across inference rounds suggests unstable stance preferences
- **First 3 experiments**: 1) Replicate pairwise vs. pointwise comparison on Scruples to confirm mode-dependent neutrality drop, 2) Test whether explicit "both answers are equally valid" preambles reduce stance-taking, 3) Ablate response length by testing neutrality at 50-word, 100-word, and 200-word limits

## Open Questions the Paper Calls Out

### Open Question 1
Can LLMs be trained to explicitly model the distribution of human disagreement rather than outputting a single stance? The abstract and conclusion state that "LLMs cannot fully capture human disagreement" and highlight the need for methods that allow models to express ambiguity. Experiments showing models outputting probability distributions or confidence scores that closely match the statistical spread of human annotator labels on ambivalent topics would resolve this.

### Open Question 2
How does LLM neutrality performance change when evaluating no-consensus tasks with multiple valid open-ended answers rather than binary stances? Section 3.1 notes the benchmark focuses on binary stances to simplify analysis, explicitly acknowledging that other datasets elicit "multiple correct open-ended answers" which are difficult to assess. An extension of the No-Consensus Benchmark to include multi-class or free-form answers, comparing neutrality scores against the binary results, would resolve this.

### Open Question 3
How does the granularity of defining "no-consensus" (task-level vs. example-level) impact the validity of neutrality benchmarks? Section 8 states that "Task-based designations of 'no agreement' may be too coarse-grained" and acknowledges potential "meta-disagreement" about specific examples. An ablation study comparing model neutrality on datasets where "no-consensus" is defined at the task level versus those annotated at the individual instance level would resolve this.

## Limitations
- The no-consensus benchmark may not fully represent all domains where human disagreement occurs due to entropy filtering
- Results depend heavily on specific prompt structures that may artificially constrain or enable stance-taking behavior
- Only five state-of-the-art models are evaluated, limiting generalizability to other architectures
- Comprehensive human comparison across all tasks is absent

## Confidence
- **High Confidence**: Pairwise judging produces significantly lower neutrality than other modes across all models and datasets
- **Medium Confidence**: Stance preferences are "baked into" model weights rather than contextually constructed
- **Medium Confidence**: Current alignment methods create an "illusion of neutrality" that only appears in answer generation mode

## Next Checks
1. Test the same methodology with models from different architectural families (e.g., Gemini, Claude, open-weight alternatives) to determine whether the neutrality degradation pattern is universal
2. Systematically vary the explicit neutral option in answer generation prompts and judge prompt framing to quantify how much measured neutrality depends on prompt engineering
3. Conduct controlled experiments where both humans and LLMs perform the same judge/debater tasks on identical no-consensus questions, measuring not just neutrality but also agreement with expert consensus where it exists