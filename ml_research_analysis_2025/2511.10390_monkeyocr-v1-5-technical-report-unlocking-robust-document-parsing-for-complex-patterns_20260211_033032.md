---
ver: rpa2
title: 'MonkeyOCR v1.5 Technical Report: Unlocking Robust Document Parsing for Complex
  Patterns'
arxiv_id: '2511.10390'
source_url: https://arxiv.org/abs/2511.10390
tags:
- table
- document
- recognition
- monkeyocr
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'MonkeyOCR v1.5 is a unified vision-language framework for robust
  document parsing, addressing challenges in multi-level tables, embedded images,
  formulas, and cross-page layouts. It uses a two-stage pipeline: a multimodal model
  jointly predicts layout and reading order for global visual consistency, followed
  by localized recognition of text, formulas, and tables.'
---

# MonkeyOCR v1.5 Technical Report: Unlocking Robust Document Parsing for Complex Patterns

## Quick Facts
- **arXiv ID**: 2511.10390
- **Source URL**: https://arxiv.org/abs/2511.10390
- **Reference count**: 39
- **Primary result**: Outperforms PPOCR-VL and MinerU 2.5 on OmniDocBench v1.5 for complex document parsing, with 8.2% improvement on OCRFlux-complex for table recognition.

## Executive Summary
MonkeyOCR v1.5 introduces a unified vision-language framework for robust document parsing that tackles complex patterns including multi-level tables, embedded images, formulas, and cross-page layouts. The system employs a two-stage pipeline where a multimodal model first jointly predicts layout and reading order for global visual consistency, followed by localized recognition of text, formulas, and tables. A novel visual consistency-based reinforcement learning scheme improves table recognition without manual annotations by evaluating render-and-compare alignment. Specialized modules handle embedded images and reconstruct cross-page/column tables, achieving state-of-the-art performance on OmniDocBench v1.5.

## Method Summary
MonkeyOCR v1.5 uses a two-stage pipeline: Stage I employs a large multimodal model to jointly predict layout (bounding boxes, labels, rotations) and reading order from the full-page image. Stage II performs localized recognition by cropping and rotating detected regions, then processing each independently for content recognition using specialized branches for text, formulas, and tables. The system introduces a visual consistency-based reinforcement learning scheme for table recognition that evaluates quality via render-and-compare alignment without manual annotations. Additional specialized modules—Image-Decoupled Table Parsing and Type-Guided Table Merging—handle embedded images and reconstruct tables crossing pages or columns. The framework achieves strong robustness in complex scenarios while maintaining high visual fidelity.

## Key Results
- Achieves state-of-the-art performance on OmniDocBench v1.5, outperforming PPOCR-VL and MinerU 2.5
- Outperforms PPOCR-VL by 8.2% on OCRFlux-complex for table recognition
- Demonstrates strong robustness in complex scenarios including multi-level tables, embedded images, formulas, and cross-page layouts
- Shows improved structural accuracy without requiring additional manual annotations through visual consistency-based reinforcement learning

## Why This Works (Mechanism)

### Mechanism 1: Two-Stage Pipeline with Decoupled Global-Local Processing
The system first uses a large vision-language model to jointly predict layout and reading order from the full-page image, generating structured bounding boxes with indices and categories. Each detected region is then cropped, rotated, and processed independently for content recognition, allowing parallel processing and high visual fidelity within local regions. This design reduces error propagation while maintaining visual-structural consistency.

### Mechanism 2: Visual Consistency-Based Reinforcement Learning for Table Recognition
A reinforcement learning scheme using a render-and-compare reward signal improves table structural accuracy without requiring dense manual annotations. A reward model trained on positive-negative pairs evaluates the visual consistency between the original table image and a re-rendered version of the predicted HTML structure. The GRPO algorithm uses this reward signal to optimize the policy model, guiding it toward outputs that reconstruct the original visual layout more faithfully.

### Mechanism 3: Specialized Heuristic-Neural Modules for Complex Table Patterns
Targeted modules reliably handle edge cases like embedded images and cross-page/cross-column tables. The Image-Decoupled Table Parsing module uses YOLOv10 to detect images, masks them with placeholders, and re-inserts them post-recognition. The Type-Guided Table Merging module identifies three patterns of split tables via rule-based header matching and a BERT-based semantic classifier to merge fragments correctly.

## Foundational Learning

**Vision-Language Models (VLMs) for Document Understanding**: MonkeyOCR's core is a VLM used for both global layout analysis and local content recognition. Understanding how these models process image patches and generate text is fundamental. *Quick check*: Can you explain how a VLM converts a cropped image of a table into an HTML sequence, and what role the visual encoder plays versus the language decoder?

**Reinforcement Learning from Verifiable Rewards**: The paper uses GRPO with a learned reward model based on visual consistency. Understanding RL fundamentals (policy, reward, optimization) is crucial. *Quick check*: In the render-and-compare scheme, what is the 'policy,' what is the 'reward,' and how does maximizing this reward lead to better table structure prediction?

**Layout Analysis and Reading Order Detection**: The first stage jointly predicts layout (what is on the page) and reading order (how to sequence it). *Quick check*: How does predicting the reading order index for each bounding box differ from simply outputting a list of elements, and why is this important for reconstructing multi-column or complex documents?

## Architecture Onboarding

**Component map**: Document Image -> VLM (Layout/Order) -> JSON layout (bboxes, labels, indices) -> Parallel processing of cropped regions -> VLM (specialized recognition for text/formula/table) -> Raw text/HTML/LaTeX -> Post-processing (IDTP re-inserts images, TGTM merges tables). The Visual Consistency RL module operates as a training wrapper around the table recognition VLM component.

**Critical path**: Document Image -> VLM (Layout/Order) -> [Table Region Crop] -> VLM (Table Recognition, policy model) -> HTML -> Render -> Compare (Reward Model) -> Gradient update (during training only). The inference critical path is Layout -> Recognition -> Merge.

**Design tradeoffs**: The two-stage design trades potential error propagation for computational efficiency. The IDTP/TGTM modules trade architectural simplicity for robustness on specific, hard cases. Visual RL trades training complexity for reduced annotation dependency.

**Failure signatures**: 1. Incorrect reading order points to Stage I VLM failure. 2. Hallucinated or missing table cells points to Stage II table VLM or IDTP failure. 3. Cross-page tables not merged correctly points to TGTM classifier/rule failure.

**First 3 experiments**:
1. **Reproduce layout analysis**: Feed sample multi-column PDF pages and visually inspect predicted bounding boxes and reading order indices. Compare against ground truth to establish Stage I baseline error rate.
2. **Isolate table recognition**: Use pre-cropped, ground-truth table images from PubTabNet. Run Stage II table VLM and evaluate TEDS scores. Compare this "oracle layout" performance to end-to-end performance to quantify error propagation from Stage I.
3. **Ablate the RL component**: Train two table recognition models—one with standard SFT and another with visual consistency RL on top of SFT. Compare their performance on OCRFlux-complex to validate the claimed improvement from the RL scheme.

## Open Questions the Paper Calls Out

### Open Question 1
**Question**: Can the visual consistency-based reinforcement learning scheme be effectively extended to improve formula recognition, or is it inherently limited to structures with HTML rendering capabilities like tables?
**Basis in paper**: Section 2.2 describes the visual consistency RL loop specifically for table structures using a "render-and-compare" mechanism via a VLM reward model. The paper does not discuss applying this RL strategy to formula recognition.
**Why unresolved**: The efficacy of render-and-compare relies on converting structured text back to an image. It's unclear if this feedback loop functions effectively for LaTeX or Markdown representations of complex formulas where rendering fidelity might vary.
**Evidence to resolve**: Ablation studies applying the same visual consistency RL reward to the formula recognition module and reporting changes in FormulaCDM scores on OmniDocBench.

### Open Question 2
**Question**: How robust is the Type-Guided Table Merging module when the upstream text recognition stage introduces errors into the table headers used for continuity detection?
**Basis in paper**: Section 2.4 details that merging logic for "Pattern 1" relies on "rule-based header matching" to identify cross-page tables.
**Why unresolved**: The method depends on the text in the first row of the second page being "identical" to the first page's header to trigger a merge. Minor OCR errors in the repeated header would break this rule-based dependency, causing the table to remain fragmented.
**Evidence to resolve**: Stress tests measuring the table merging success rate on cross-page datasets where synthetic OCR noise is systematically injected into the header rows.

### Open Question 3
**Question**: Does the reward model fail to penalize "adversarial" table structures that are semantically incorrect but visually resemble the ground truth when rendered?
**Basis in paper**: Section 2.2 states the reward model is trained on "positive-negative sample pairs" generated by modifying ground truth and sampling incorrect results, potentially biasing it towards specific error patterns.
**Why unresolved**: A reward model trained on "typical error patterns" might assign high rewards to novel hallucinations (e.g., swapped rows with similar visual density) that look structurally correct upon rendering but contain wrong data.
**Evidence to resolve**: Evaluation of the reward model's correlation with human evaluation scores on adversarially crafted table recognitions that are visually consistent but semantically false.

## Limitations

- The paper lacks specification of the specific vision-language model architecture and size used as the foundation
- Training hyperparameters remain unspecified, including learning rates, batch sizes, and RL-specific settings
- The reward model training procedure lacks detail on how positive-negative table recognition pairs are constructed
- While claiming visual consistency RL improves table accuracy, the evidence shows only an 8.2% improvement on OCRFlux-complex

## Confidence

**High Confidence**: The two-stage pipeline architecture and specialized modules (IDTP, TGTM) are well-specified and align with established patterns in document parsing literature.

**Medium Confidence**: The visual consistency-based RL mechanism is theoretically sound but lacks critical implementation details for reward model training and GRPO optimization.

**Low Confidence**: The exact VLM architecture, training hyperparameters, reward model construction procedure, and constrained decoding implementation are completely unspecified.

## Next Checks

1. **Layout Analysis Baseline Validation**: Run Stage I on a set of 10+ multi-column PDF pages with known ground truth layouts. Measure the accuracy of predicted bounding boxes and reading order indices against human annotations to establish the error rate of the global layout model.

2. **Table Recognition Isolation Test**: Using ground-truth table crops from PubTabNet, run only the Stage II table recognition VLM and measure TEDS scores. Compare this "oracle layout" performance against end-to-end results to quantify error propagation from incorrect layout predictions.

3. **RL Component Ablation Study**: Train two identical table recognition models—one with only supervised fine-tuning (SFT) and one with the visual consistency RL (GRPO) applied on top of SFT. Evaluate both on OCRFlux-complex to measure the specific contribution of the RL mechanism to table structural accuracy.