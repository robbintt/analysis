---
ver: rpa2
title: Emergent Bayesian Behaviour and Optimal Cue Combination in LLMs
arxiv_id: '2512.02719'
source_url: https://arxiv.org/abs/2512.02719
tags:
- bayesian
- take
- units
- llms
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BayesBench, a psychophysics-inspired framework
  to evaluate whether large language models (LLMs) exhibit Bayesian-like behaviour
  in magnitude estimation tasks. The authors develop four synthetic estimation tasks
  (length, location, distance, duration) and evaluate nine diverse LLMs alongside
  human baselines.
---

# Emergent Bayesian Behaviour and Optimal Cue Combination in LLMs

## Quick Facts
- arXiv ID: 2512.02719
- Source URL: https://arxiv.org/abs/2512.02719
- Reference count: 38
- Primary result: Introduces BayesBench framework showing that highly capable LLMs like GPT-5 Mini and Claude 3.7 Sonnet exhibit stronger Bayesian tendencies in magnitude estimation tasks, though accuracy alone doesn't guarantee optimal cue integration

## Executive Summary
This paper introduces BayesBench, a psychophysics-inspired framework to evaluate whether large language models exhibit Bayesian-like behavior in magnitude estimation tasks. The authors develop four synthetic estimation tasks (length, location, distance, duration) and evaluate nine diverse LLMs alongside human baselines. Through controlled ablations (noise, context, steering), they measure task accuracy, cue-combination efficiency, and introduce a Bayesian Consistency Score (BCS) that tracks Bayes-consistent behavioral shifts even when accuracy saturates. Results show that highly accurate models like GPT-5 Mini and Claude 3.7 Sonnet often exhibit stronger Bayesian tendencies, though accuracy alone doesn't guarantee robust cue integration—GPT-5 Mini, despite near-perfect text accuracy, fails to optimally downweight noisy visual cues. Llama-4 Maverick outperforms Bayesian linear fusion, suggesting nonlinear integration. The BCS reveals nuanced behavior changes across tasks, highlighting that principled uncertainty handling emerges in capable models but remains brittle in others. The work bridges human psychophysics and AI, providing tools to probe implicit computational strategies in multimodal LLMs.

## Method Summary
The authors develop BayesBench, a psychophysics-inspired framework featuring four synthetic magnitude estimation tasks: length, location, distance, and duration. They evaluate nine diverse LLMs including GPT-5 Mini, Claude 3.7 Sonnet, Llama-4 Maverick, and others. The framework employs controlled ablations varying noise levels, context conditions, and steering prompts. Performance is measured through task accuracy, cue-combination efficiency (how well models integrate information from multiple sources), and a novel Bayesian Consistency Score (BCS) that quantifies adherence to Bayesian inference principles. Human baselines are included for comparison, with tasks designed to mimic classic psychophysics experiments. The study systematically varies input reliability and modality combinations to probe how models handle uncertainty and combine evidence.

## Key Results
- Highly accurate models like GPT-5 Mini and Claude 3.7 Sonnet exhibit stronger Bayesian tendencies, though accuracy alone doesn't guarantee optimal cue integration
- GPT-5 Mini, despite near-perfect text accuracy, fails to optimally downweight noisy visual cues
- Llama-4 Maverick outperforms Bayesian linear fusion, suggesting nonlinear integration strategies
- Bayesian Consistency Score (BCS) reveals nuanced behavioral shifts across tasks, tracking Bayes-consistent behavior even when accuracy saturates
- Cue integration remains brittle in less capable models, with principled uncertainty handling emerging primarily in high-performing systems

## Why This Works (Mechanism)
The framework works by creating controlled experimental conditions that isolate specific computational behaviors related to Bayesian inference. By varying noise levels, context, and steering, the authors can measure how models adjust their responses based on reliability of different information sources. The BCS metric captures subtle shifts in behavior that correlate with Bayesian reasoning principles, even when raw accuracy metrics plateau. The synthetic nature of the tasks allows precise control over variables that would be difficult to manipulate in naturalistic settings, enabling rigorous testing of whether models truly implement Bayesian updating or simply memorize task-specific patterns.

## Foundational Learning
- **Bayesian inference principles**: Understanding how prior knowledge combines with evidence under uncertainty is essential for interpreting BCS results and distinguishing genuine Bayesian behavior from heuristic approximations
- **Psychophysics methodology**: The experimental design draws from human sensory integration research, providing established frameworks for measuring cue combination and uncertainty handling
- **Multimodal integration**: Models must process and combine information across text and visual modalities, requiring understanding of cross-modal representation alignment
- **Uncertainty quantification**: Evaluating how models handle and propagate uncertainty through their decision-making process is central to assessing Bayesian consistency
- **Cue reliability weighting**: The ability to appropriately weight different information sources based on their reliability is a key indicator of Bayesian-like behavior
- **Quick check**: Test whether models can correctly identify which cue (text vs. visual) is more reliable in simple binary comparison tasks before evaluating full cue combination

## Architecture Onboarding
**Component Map**: Input Processing -> Cue Integration -> Uncertainty Estimation -> Output Generation
**Critical Path**: Multimodal inputs → Reliability assessment → Bayesian updating (or approximation) → Final estimate
**Design Tradeoffs**: The framework prioritizes controlled experimental conditions over ecological validity, sacrificing real-world complexity for precise measurement of specific computational behaviors
**Failure Signatures**: Models may show perfect accuracy on individual cues but fail at integration, or exhibit inconsistent BCS across similar task variations, indicating brittle rather than principled uncertainty handling
**First Experiments**:
1. Test individual cue performance (text-only, visual-only) to establish baseline capabilities
2. Evaluate simple reliability discrimination tasks to assess whether models can identify more reliable information sources
3. Compare performance across noise levels to measure adaptive weighting of unreliable cues

## Open Questions the Paper Calls Out
The paper acknowledges that the four synthetic tasks (length, location, distance, duration) may not fully capture the diversity of real-world reasoning scenarios where LLMs operate. Additionally, while the BCS metric is well-motivated, its sensitivity to task-specific factors versus genuine Bayesian inference remains unclear. The role of model steering and context in influencing Bayesian behavior needs more systematic exploration.

## Limitations
- The four synthetic tasks may not generalize to broader reasoning scenarios beyond magnitude estimation
- BCS metric sensitivity to task-specific factors versus genuine Bayesian inference remains unclear
- Limited sample of only nine models constrains broader claims about LLM population
- Comparison to human baselines is limited by differences in task presentation and artificial nature of synthetic tasks

## Confidence
- Major finding (Bayesian-like behavior in capable models): **Medium**
- BCS metric validity: **Medium**
- Generalizability to real-world scenarios: **Low**
- Comparison to human baselines: **Medium**

## Next Checks
1. Test BCS robustness across a broader range of task types including non-magnitude estimation scenarios to assess generalizability of Bayesian-like behavior
2. Conduct ablation studies on model architecture (e.g., comparing frozen vs. fine-tuned models) to isolate whether Bayesian tendencies emerge from pretraining or can be induced through training
3. Implement cross-modal transfer experiments where models trained on one type of cue combination task are evaluated on novel modality combinations to test for genuine Bayesian generalization versus task-specific optimization