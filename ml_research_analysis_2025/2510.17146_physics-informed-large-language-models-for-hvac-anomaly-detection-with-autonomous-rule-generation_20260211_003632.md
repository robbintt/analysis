---
ver: rpa2
title: Physics-Informed Large Language Models for HVAC Anomaly Detection with Autonomous
  Rule Generation
arxiv_id: '2510.17146'
source_url: https://arxiv.org/abs/2510.17146
tags:
- detection
- anomaly
- rules
- physical
- building
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PILLM addresses the challenge of anomaly detection in HVAC systems
  by integrating Large Language Models (LLMs) with physics-informed evolutionary search
  to automatically generate diagnostic rules. The method embeds thermodynamic and
  control-theoretic constraints into LLM-driven reflection and crossover operators,
  ensuring that generated rules remain physically plausible while evolving toward
  higher detection accuracy.
---

# Physics-Informed Large Language Models for HVAC Anomaly Detection with Autonomous Rule Generation

## Quick Facts
- arXiv ID: 2510.17146
- Source URL: https://arxiv.org/abs/2510.17146
- Reference count: 15
- Primary result: PILLM achieves 0.968 precision, 0.859 recall, and 0.926 F1 on LBNL HVAC anomaly detection dataset

## Executive Summary
PILLM introduces a novel framework that combines Large Language Models with physics-informed evolutionary search to automatically generate diagnostic rules for HVAC anomaly detection. The method embeds thermodynamic and control-theoretic constraints into LLM-driven reflection and crossover operators, ensuring that generated rules remain physically plausible while evolving toward higher detection accuracy. Evaluated on the public LBNL Automated Fault Detection dataset, PILLM outperforms classical deep learning and LLM baselines, demonstrating the value of integrating domain physics into automated rule generation.

## Method Summary
PILLM implements an evolutionary loop where LLM-generated rules evolve through physics-informed operators. The framework initializes a population of candidate rules, then iteratively applies physics-informed reflection (PIR) using feature metadata, physics-informed crossover (PIC) combining parent rules with physical rationale, and elitist mutation. Rules are evaluated on labeled data using Event-F1 PA as the fitness metric. The evolutionary process continues until convergence, producing executable Python rules that provide both anomaly detection and human-readable explanations grounded in building physics.

## Key Results
- Achieves state-of-the-art performance with precision 0.968, recall 0.859, and F1 score 0.926
- Outperforms classical deep learning baselines and LLM-only approaches
- Ablation studies confirm critical role of physics-informed components (F1 drops ~0.058 when removing PIR or PIC)

## Why This Works (Mechanism)

### Mechanism 1: Physics-Informed Reflection as Semantic Constraint Injection
Providing physical context during reflection guides the LLM to evaluate rules against domain semantics rather than pure statistical performance. The Reflection LLM receives feature metadata alongside rule performance comparisons, producing structured hints about which physical dynamics are captured or neglected, constraining subsequent rule generation toward thermodynamically coherent hypotheses.

### Mechanism 2: Physics-Anchored Crossover Preventing Semantic Drift
Crossbreeding rules based on their physical interpretations (not just code) produces offspring with coherent diagnostic coverage. The Crossover LLM synthesizes new rules that integrate causal relationships from both parents—for example, combining temperature-gradient logic with airflow-pressure dynamics—rather than arbitrary syntactic hybrids.

### Mechanism 3: Evolutionary Selection with Executable Fitness Feedback
An evolutionary loop with concrete fitness scores provides gradient-like signal for LLM-based rule improvement without differentiable parameters. Rules are evaluated on held-out data, ranked by F1 score, with high-performing rules becoming parents for crossover and elite rules undergoing targeted mutation, closing the feedback loop for iterative refinement.

## Foundational Learning

- **Concept: Genetic operators (initialization, crossover, mutation, selection)**
  - Why needed here: PILLM maps LLM reasoning onto evolutionary search; understanding how populations evolve through these operators is prerequisite to debugging convergence.
  - Quick check question: Can you explain why elite-preserving mutation differs from random mutation in maintaining population quality?

- **Concept: HVAC thermodynamic basics (heat transfer, airflow, control loops)**
  - Why needed here: Physics-informed reflection requires interpreting feature metadata meaningfully; without domain grounding, you cannot validate whether generated rules are physically plausible.
  - Quick check question: What does a stuck damper imply for the relationship between outdoor air fraction and zone temperature?

- **Concept: Prompt engineering for code generation with constraints**
  - Why needed here: The framework relies on carefully structured prompts to produce executable, physically-grounded Python code.
  - Quick check question: How would you modify the initialization prompt to bias rules toward energy-efficiency diagnostics rather than pure fault detection?

## Architecture Onboarding

- **Component map:**
  Initialization LLM → Initial Rule Population → Fitness Evaluation → Reflection LLM (+ Physical Context) → Crossover LLM (+ Parent Contexts) → Mutation LLM (on Elite Rules) → [Loop back to Fitness Evaluation]

- **Critical path:**
  1. Define input features with physical metadata
  2. Specify fitness function (Event-F1 PA)
  3. Configure population size and iteration count
  4. Validate generated rules on held-out data before deployment

- **Design tradeoffs:**
  - Precision vs. Recall: PILLM optimizes F1 (0.926); if false negatives are costlier, consider weighting recall higher in selection.
  - Interpretability vs. Complexity: Rules remain human-readable by design, but highly mutated elites may grow complex—consider code-length penalties.
  - Physics grounding vs. Flexibility: Strong physics constraints reduce false positives but may limit detection of novel fault patterns.

- **Failure signatures:**
  - Stagnant fitness: Population converges to similar rules; mutation too conservative or diversity too low.
  - Physically implausible rules: Reflection prompts missing critical feature context or LLM hallucinating relationships.
  - Poor generalization: High training fitness but low test performance; dataset not representative or overfitting to specific fault types.

- **First 3 experiments:**
  1. **Baseline comparison:** Run PILLM vs. ARGOS vs. LSTMAD on the same LBNL split to reproduce reported metrics and identify sensitivity to random seeds.
  2. **Ablation validation:** Disable PIR and PIC separately to confirm ablation magnitudes and inspect generated rules for physical plausibility loss.
  3. **Feature-metadata sensitivity:** Remove physical context from reflection prompts for half the features; measure impact on rule coherence and detection performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can PILLM maintain low-latency performance when applied to real-time anomaly detection in large-scale smart infrastructure?
- Basis in paper: The conclusion explicitly identifies "investigating its scalability to real-time anomaly detection in large-scale smart infrastructure" as a direction for future work.
- Why unresolved: The current evaluation relies on an evolutionary loop involving multiple LLM calls, which is computationally intensive compared to single-pass inference methods.
- What evidence would resolve it: Latency and throughput metrics from a live deployment in a large building, demonstrating that rule generation and evaluation occur faster than the building's sampling rate.

### Open Question 2
- Question: Does PILLM effectively characterize novel or compound faults not explicitly present in the training dataset?
- Basis in paper: The appendix suggests the framework "may allow it to characterize novel or compound faults," but the reported experiments only evaluate performance on the labeled LBNL dataset which contains predefined fault types.
- Why unresolved: While the method aims for zero-shot capability through physical reasoning, the paper does not provide empirical results on faults outside the training distribution.
- What evidence would resolve it: An evaluation of PILLM on a test set containing hybrid or unseen fault scenarios, measuring the diagnostic accuracy of the generated rules.

### Open Question 3
- Question: Can the framework improve recall rates to match or exceed baselines like ARGOS without sacrificing the high precision achieved through physics-informed constraints?
- Basis in paper: Table 1 shows PILLM achieves a precision of 0.968 but a recall of 0.859, which is lower than the ARGOS baseline (0.885), suggesting a potential trade-off between physical plausibility and detection coverage.
- Why unresolved: It is unclear if the physics-informed operators overly constrain the search space, causing the model to miss anomalies that valid but physically complex or noisy.
- What evidence would resolve it: A modified loss function or search mechanism that balances physical constraints with coverage, resulting in recall >0.885 while maintaining F1 >0.926.

## Limitations

- Several key implementation details remain unspecified, including exact hyperparameters and dataset preprocessing procedures
- The framework's reliance on LLM API calls introduces cost and reproducibility concerns
- Physical metadata quality directly constrains reflection quality, creating a dependency on domain expertise
- Results may not generalize beyond the HVAC domain due to the physics-informed approach

## Confidence

- **High confidence**: Evolutionary framework effectiveness (demonstrated by ARGOS baseline), general approach validity (outperforms classical deep learning baselines)
- **Medium confidence**: Physics-informed components' specific contributions (ablations show impact but exact mechanisms unclear), LLM-based rule generation quality (depends on prompt engineering and API stability)
- **Low confidence**: Generalization beyond HVAC domain (physics metadata approach may not transfer), exact reproducibility without hyperparameter specification

## Next Checks

1. **Hyperparameter sensitivity analysis**: Systematically vary population size (N=10, 20, 30), generation count (K=10, 20, 30), and mutation rates to identify stable configurations and assess robustness to parameter choices.

2. **Cross-domain transferability test**: Apply PILLM to a non-HVAC time-series anomaly detection dataset (e.g., network traffic or industrial sensor data) with appropriate physical metadata to evaluate whether the physics-informed approach generalizes beyond building systems.

3. **LLM API dependency assessment**: Run identical experiments using different LLM providers/models (e.g., Claude vs. Gemini) and varying temperature settings to quantify sensitivity to LLM choice and establish reproducibility boundaries.