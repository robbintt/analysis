---
ver: rpa2
title: Improving Counterfactual Truthfulness for Molecular Property Prediction through
  Uncertainty Quantification
arxiv_id: '2504.02606'
source_url: https://arxiv.org/abs/2504.02606
tags:
- uncertainty
- counterfactual
- error
- truthfulness
- reduction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of generating truthful counterfactual
  explanations for molecular property prediction models, where truthfulness means
  the explanations not only reflect the model's behavior but also the underlying data
  distribution. The core method involves integrating uncertainty quantification techniques
  to filter counterfactual candidates with high predicted uncertainty.
---

# Improving Counterfactual Truthfulness for Molecular Property Prediction through Uncertainty Quantification

## Quick Facts
- arXiv ID: 2504.02606
- Source URL: https://arxiv.org/abs/2504.02606
- Reference count: 40
- Primary result: Uncertainty-based filtering substantially improves counterfactual truthfulness for molecular property prediction, especially for out-of-distribution scenarios

## Executive Summary
This paper addresses the challenge of generating truthful counterfactual explanations for molecular property prediction models. The authors demonstrate that integrating uncertainty quantification techniques—specifically deep ensembles, mean-variance estimation, and their combination—can significantly improve counterfactual truthfulness by filtering out high-uncertainty candidates that are likely to be incorrect. The method is particularly effective for out-of-distribution test scenarios, where traditional uncertainty methods often fail.

## Method Summary
The authors generate counterfactual samples by enumerating minimal perturbations to molecular graphs (1-edit neighborhood) and filter them based on predicted uncertainty estimates from various methods including deep ensembles, mean-variance estimation, and their combination. They train Graph Neural Networks (GNNs) with MVE heads, create ensembles of 3 models with bootstrap sampling, calibrate uncertainties via isotonic regression, and apply threshold-based filtering (ξ20) to select high-confidence counterfactuals. The approach is evaluated on multiple molecular property prediction datasets including ClogP, with metrics focusing on uncertainty-error correlation and counterfactual truthfulness.

## Key Results
- Traditional uncertainty estimation methods substantially reduce average prediction error and increase counterfactual truthfulness
- Deep ensembles + MVE combination yields greatest improvement in both relative model error reduction and counterfactual truthfulness
- Achieves UER-AUCmean values of 0.27-0.28 and improves counterfactual truthfulness by 4-9% through uncertainty-based filtering
- Effectiveness is particularly notable for out-of-distribution test scenarios, where mean and max error reduction potential cross the UER-AUC≥ 0.5 threshold

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Filtering counterfactual candidates by predicted uncertainty reduces cumulative prediction error and improves the fraction of truthful explanations.
- Mechanism: Uncertainty quantification methods produce a scalar uncertainty estimate per sample. When uncertainty correlates with prediction error, thresholding removes a disproportionate share of high-error samples from the candidate set. This increases the probability that retained counterfactuals reflect both the model and the true underlying property distribution.
- Core assumption: Predicted uncertainty is positively correlated with absolute prediction error on counterfactual candidates; the correlation holds sufficiently well out-of-distribution.
- Evidence anchors:
  - [abstract] "we demonstrate that traditional uncertainty estimation methods, such as ensembles and mean-variance estimation, can already substantially reduce the average prediction error and increase counterfactual truthfulness, especially for out-of-distribution settings."
  - [Section 3.4, Eq. 10–12] Formal definition of cumulative error reduction and UER-AUC as a threshold-agnostic metric.
  - [corpus] Neighbor papers on UQ for scientific discovery (Constraint-Aware Neurosymbolic UQ) mention similar error-uncertainty links but do not validate this exact filtering mechanism.
- Break condition: If uncertainty–error correlation is near-zero or negative (e.g., poorly calibrated models, severe distribution shift beyond training), filtering will not reduce error and may discard valid explanations.

### Mechanism 2
- Claim: Complete enumeration of chemically valid 1-edit graph perturbations is tractable for small molecules and yields high-quality counterfactual candidates.
- Mechanism: For molecular graphs, the space of single-atom/bond additions, deletions, and substitutions respecting valence constraints is small enough to exhaustively generate. Each candidate is scored by prediction divergence from the original, and top-k are selected.
- Core assumption: Chemically valid 1-edit perturbations cover the relevant local decision boundary; k=1 is sufficient to capture meaningful structure–property relationships.
- Evidence anchors:
  - [Section 3.2] "we find it computationally feasible to generate all possible modifications to a given input molecule x. As possible modifications, we consider the addition, deletion, and substitution of individual atoms and bonds that satisfy the constraints of atomic valence."
  - [corpus] "Enhancing Chemical Explainability Through Counterfactual Masking" also uses perturbation-based counterfactuals but via masking strategies, not full enumeration.
- Break condition: For large molecules or higher k, enumeration becomes intractable; approximation or learned generation is required.

### Mechanism 3
- Claim: Combining deep ensembles (DE) with mean-variance estimation (MVE) yields higher error reduction potential and better uncertainty calibration than either method alone.
- Mechanism: DE captures between-model variance; MVE captures within-model predicted variance. Averaging both uncertainty sources aggregates epistemic and aleatoric signals, improving the correlation with true error.
- Core assumption: The two uncertainty estimates provide complementary information and are not perfectly redundant.
- Evidence anchors:
  - [Section 4.1] "We combine deep ensembles and mean-variance estimation by constructing an ensemble of 3 independent MVE models... The total uncertainty is calculated as the average of the ensemble uncertainty and the mean MVE uncertainty."
  - [Table 1–3] DE+MVE generally achieves the highest or near-highest UER-AUC and RLL across datasets.
  - [corpus] No direct validation of DE+MVE combination in neighbor papers; most use single UQ methods.
- Break condition: If ensemble members are highly correlated or MVE variance collapses during training, the combined estimate offers no gain.

## Foundational Learning
- Concept: Counterfactual explanations
  - Why needed here: Core object of study; requires understanding that counterfactuals are minimal perturbations causing large prediction changes.
  - Quick check question: For a given molecule, can you identify a single bond change that would flip the predicted property value?
- Concept: Uncertainty quantification (epistemic vs. aleatoric)
  - Why needed here: Enables the filtering mechanism; must distinguish between model uncertainty and data noise.
  - Quick check question: Would adding more training data reduce epistemic uncertainty, aleatoric uncertainty, or both?
- Concept: Graph Neural Networks (GNNs) for molecules
  - Why needed here: The predictive model; must understand node/edge representations and message passing.
  - Quick check question: How does a GNN propagate information across a molecular graph during forward pass?

## Architecture Onboarding
- Component map:
  GNN regressor (GCN/GATv2/GIN) -> Uncertainty head -> Counterfactual generator -> Filtering module -> Evaluation
- Critical path:
  1. Train base GNN with MVE loss (if using MVE)
  2. Train ensemble of 3 models with bootstrap sampling
  3. Calibrate uncertainties on held-out set via isotonic regression
  4. For each test molecule, enumerate 1-edit candidates
  5. Score candidates by prediction divergence
  6. Filter by uncertainty threshold (e.g., ξ20)
  7. Evaluate truthfulness (requires ground truth oracle)
- Design tradeoffs:
  - DE vs. MVE vs. DE+MVE: trade compute for calibration
  - Threshold selection: stricter thresholds increase truthfulness but reduce candidate count
  - k-edit depth: higher k explores more of decision boundary at computational cost
- Failure signatures:
  - Low UER-AUC near 0: uncertainty uncorrelated with error
  - Negative RLL: poorly calibrated uncertainty estimates
  - Many filtered candidates with low actual error: over-conservative threshold
  - Retained candidates with high error: missed OOD samples
- First 3 experiments:
  1. Replicate Table 1: Compare UQ methods on ClogP with a GATv2 model, reporting UER-AUC and RLL.
  2. Replicate OOD-Value scenario: Train on ClogP, test on extreme target values, measure if UER-AUC ≥ 0.5.
  3. Qualitative inspection: For 10 test molecules, visualize retained vs. filtered counterfactuals and manually check if filtered samples are chemically implausible or high-error.

## Open Questions the Paper Calls Out
- Can uncertainty estimation be employed beneficially to different explanation modalities, such as local attributional or global concept-based explanations?
- Does uncertainty-based filtering significantly improve truthfulness in complex property prediction tasks with inherently low predictive performance?
- Can a suitable distance metric be defined for graph-structured data to make Trust Scores effective for uncertainty-based counterfactual filtering?

## Limitations
- The paper does not specify key hyperparameters for the GNN models (layer depth, attention heads, learning rate), which could affect reproducibility and generalizability
- While the combination of deep ensembles and MVE shows promise, the mechanism for why this combination outperforms individual methods is not rigorously explained
- The study focuses on small molecules with 1-edit perturbations; scalability to larger molecules and higher-order edits remains untested
- The counterfactual truthfulness metric requires ground truth oracles, which may not be available in many real-world applications

## Confidence
- **High confidence**: The core finding that uncertainty quantification improves counterfactual truthfulness, particularly for out-of-distribution settings
- **Medium confidence**: The specific advantage of combining deep ensembles with MVE over individual methods, pending hyperparameter sensitivity analysis
- **Low confidence**: The claim that complete enumeration is computationally feasible for all molecular sizes, given no explicit timing or complexity analysis

## Next Checks
1. **Hyperparameter sensitivity**: Systematically vary GNN depth, learning rate, and ensemble size to determine if DE+MVE's advantage persists across different configurations
2. **Scaling experiment**: Test the method on molecules with 50+ atoms and k=2 edits to verify enumeration remains tractable and filtering remains effective
3. **Real-world applicability**: Apply the framework to a property prediction task where ground truth counterfactuals cannot be obtained, evaluating performance using proxy metrics like chemical plausibility scores