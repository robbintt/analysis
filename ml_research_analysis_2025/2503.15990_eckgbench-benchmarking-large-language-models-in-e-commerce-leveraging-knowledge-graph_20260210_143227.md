---
ver: rpa2
title: 'ECKGBench: Benchmarking Large Language Models in E-commerce Leveraging Knowledge
  Graph'
arxiv_id: '2503.15990'
source_url: https://arxiv.org/abs/2503.15990
tags:
- knowledge
- llms
- e-commerce
- arxiv
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes ECKGBench, a benchmark dataset for evaluating
  large language models (LLMs) in e-commerce using a knowledge graph. The authors
  address the factuality problem in LLMs (hallucinations) by creating high-quality
  multiple-choice questions automatically from a large-scale e-commerce knowledge
  graph.
---

# ECKGBench: Benchmarking Large Language Models in E-commerce Leveraging Knowledge Graph

## Quick Facts
- **arXiv ID**: 2503.15990
- **Source URL**: https://arxiv.org/abs/2503.15990
- **Reference count**: 40
- **Primary result**: LLM performance in e-commerce remains unsatisfactory (accuracy below 60% in most cases)

## Executive Summary
This paper introduces ECKGBench, a novel benchmark dataset designed to evaluate large language models in e-commerce contexts using a knowledge graph approach. The authors address the prevalent issue of LLM hallucinations by creating high-quality multiple-choice questions automatically extracted from a large-scale e-commerce knowledge graph. Through comprehensive experiments on 12 advanced LLMs, the benchmark demonstrates that while these models show potential, their performance in e-commerce knowledge remains notably limited, with most models achieving accuracy below 60%. The work also explores knowledge boundaries and the impact of post-training methods on model performance.

## Method Summary
The ECKGBench framework employs a three-stage negative sampling workflow to automatically generate high-quality multiple-choice questions from a large-scale e-commerce knowledge graph. The system uses a simple question-answering paradigm for efficient evaluation across two dimensions: common and abstract e-commerce knowledge. The negative sampling process ensures question quality by carefully selecting distractors that are plausible yet incorrect. The benchmark evaluates models based on their ability to retrieve factual knowledge from the structured e-commerce domain, providing insights into both their capabilities and limitations in this specialized context.

## Key Results
- LLM performance in e-commerce remains unsatisfactory, with accuracy below 60% in most cases
- The benchmark effectively evaluates LLMs across two dimensions: common and abstract e-commerce knowledge
- Post-training methods can enhance knowledge outside the essential boundary, though base model limitations persist

## Why This Works (Mechanism)
The effectiveness of ECKGBench stems from its structured approach to evaluating LLMs in a domain-specific context. By leveraging a knowledge graph as the foundation, the benchmark ensures that questions are grounded in factual e-commerce knowledge rather than general web data. The three-stage negative sampling workflow creates challenging multiple-choice questions that test not just retrieval ability but also discrimination between closely related concepts. This methodology helps expose the hallucination tendencies of LLMs while providing a standardized evaluation framework for the e-commerce domain.

## Foundational Learning
- **Knowledge Graph Construction**: Creating structured representations of e-commerce entities and relationships is essential for grounding questions in factual domain knowledge. Quick check: Verify graph completeness and accuracy through sampling and cross-referencing with source data.
- **Negative Sampling Strategies**: Selecting appropriate distractors is crucial for creating challenging yet fair multiple-choice questions. Quick check: Evaluate distractor plausibility through human validation of a sample of questions.
- **Benchmark Evaluation Metrics**: Standardizing evaluation across different models requires consistent scoring methodologies and clear performance thresholds. Quick check: Ensure inter-rater reliability and establish baseline human performance for comparison.

## Architecture Onboarding
**Component Map**: E-commerce Knowledge Graph -> Question Generation Pipeline -> Multiple-choice Question Set -> LLM Evaluation Framework
**Critical Path**: Knowledge Graph Construction → Negative Sampling → Question Generation → Model Evaluation
**Design Tradeoffs**: Multiple-choice format provides efficiency and standardization but may not fully capture open-ended reasoning capabilities; automated question generation ensures scalability but requires careful quality control
**Failure Signatures**: Poor performance may indicate gaps in the knowledge graph, inadequate negative sampling, or fundamental limitations in model understanding of e-commerce concepts
**First Experiments**: 1) Validate question quality through human evaluation of randomly sampled questions, 2) Compare performance across different LLM families to identify architecture-specific strengths/weaknesses, 3) Test model performance on progressively more abstract e-commerce concepts to map knowledge boundaries

## Open Questions the Paper Calls Out
The paper does not explicitly identify open questions for future research.

## Limitations
- Multiple-choice format may artificially inflate performance compared to open-ended responses where hallucination tendencies become more apparent
- The knowledge graph construction process, while automated, may introduce biases or gaps that affect question quality and coverage
- Evaluation focuses on two dimensions of e-commerce knowledge without exploring other potentially important subdomains like customer service interactions or fraud detection

## Confidence
- **High confidence**: The assertion that LLM performance in e-commerce remains unsatisfactory (below 60% accuracy) is well-supported by the comprehensive evaluation across 12 advanced models.
- **Medium confidence**: Claims about the effectiveness of the three-stage negative sampling workflow for ensuring question quality are plausible but could benefit from human validation of the generated questions.
- **Medium confidence**: The observation that post-training methods can enhance knowledge outside the essential boundary requires further exploration to determine if these improvements generalize beyond the specific benchmark.

## Next Checks
1. Conduct human evaluation of a sample of generated questions to verify the quality and difficulty calibration of the multiple-choice format.
2. Test model performance on an open-ended version of the same questions to assess the impact of format constraints on reported accuracy.
3. Expand evaluation to include domain-specific e-commerce tasks (e.g., customer query resolution, product recommendation reasoning) to validate the benchmark's coverage of practical e-commerce scenarios.