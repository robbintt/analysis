---
ver: rpa2
title: Post-Training Quantization of OpenPangu Models for Efficient Deployment on
  Atlas A2
arxiv_id: '2512.23367'
source_url: https://arxiv.org/abs/2512.23367
tags:
- quantization
- think
- reasoning
- int8
- fp16
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of deploying Huawei's openPangu-Embedded
  models on Ascend NPUs, which have memory and latency constraints when running the
  original FP16 versions. To tackle this, the authors implement a unified low-bit
  inference framework supporting INT8 (W8A8) and W4A8 quantization, optimized for
  the Atlas A2 NPU.
---

# Post-Training Quantization of OpenPangu Models for Efficient Deployment on Atlas A2

## Quick Facts
- **arXiv ID:** 2512.23367
- **Source URL:** https://arxiv.org/abs/2512.23367
- **Reference count:** 6
- **Primary result:** INT8 quantization preserves >90% FP16 accuracy on code generation benchmarks while achieving 1.5x prefill speedup and 40% memory reduction on Atlas A2 NPU.

## Executive Summary
This paper addresses the challenge of deploying Huawei's openPangu-Embedded models on Ascend NPUs, which have memory and latency constraints when running the original FP16 versions. To tackle this, the authors implement a unified low-bit inference framework supporting INT8 (W8A8) and W4A8 quantization, optimized for the Atlas A2 NPU. They evaluate these quantization methods on code generation benchmarks (HumanEval and MBPP) across three CoT reasoning modes. Results show that INT8 quantization preserves over 90% of FP16 accuracy, achieves up to 1.5x prefill speedup, and reduces memory consumption by up to 40%. W4A8 offers further compression with moderate accuracy trade-offs, improved by techniques like SmoothQuant and Hadamard rotation. Notably, quantization minimally impacts CoT reasoning behavior, indicating that structured multi-step inference is retained. These findings demonstrate that low-bit quantization is effective for efficient openPangu-Embedded deployment on Ascend NPUs.

## Method Summary
The paper implements a unified low-bit inference framework for Huawei's openPangu-Embedded models (1B and 7B) on Atlas A2 Ascend NPUs. The approach uses symmetric post-training quantization (PTQ) with two modes: INT8 (W8A8) and W4A8. For W4A8, they apply SmoothQuant or Hadamard rotation to handle activation outliers. The quantization scales are derived from calibration data collected from downstream tasks. The framework leverages CATLASS, Huawei's operator template library, to implement quantized GEMM kernels optimized for the Atlas A2 hardware. The method is evaluated across three CoT reasoning modes (slow_think, auto_think, no_think) on code generation benchmarks.

## Key Results
- INT8 quantization preserves >90% FP16 accuracy on HumanEval and MBPP benchmarks
- Achieves 1.5x prefill speedup and 40% memory reduction compared to FP16
- W4A8 quantization with SmoothQuant/Hadamard rotation improves accuracy over baseline W4A8
- Quantization minimally impacts CoT reasoning structure and reduces repetitive generation failures

## Why This Works (Mechanism)

### Mechanism 1: Hardware-Mapped Integer Arithmetic
Converting FP16 tensors to INT8 reduces memory bandwidth pressure and enables utilization of optimized integer compute primitives on Ascend NPUs. The framework maps floating-point operations to integer matrix multiplication using CATLASS operator templates. By halving the bit-width, the system moves less data per clock cycle and leverages the NPU's native integer execution units, which are more efficient than floating-point units for these quantized operations.

### Mechanism 2: Outlier Smoothing for Low-Bit Fidelity
W4A8 quantization causes accuracy degradation due to activation outliers, but this is reversible via mathematical transformations. SmoothQuant migrates quantization difficulty from activations to weights using a diagonal scaling matrix, while Hadamard rotation uses orthogonal transforms to distribute outlier energy more uniformly across channels. This prevents high-magnitude outliers from saturating the low-bit integer range.

### Mechanism 3: Reasoning Structure Retention
INT8 quantization preserves the functional logic and structural integrity of Chain-of-Thought reasoning. The precision reduction appears insufficient to disrupt the high-level semantic planning required for CoT. The model retains the ability to generate necessary intermediate steps and logical sequencing, merely shifting phrasing or explanation depth without functional loss.

## Foundational Learning

- **Concept: Symmetric vs. Asymmetric Quantization**
  - Why needed: The paper explicitly states "All quantized models employ symmetric quantization" (Section 4.1). You must understand why zero-point alignment is simplified here (scaling factor $s$ only) versus asymmetric quantization.
  - Quick check: If a tensor has values ranging from -5.0 to 100.0, why might symmetric quantization waste dynamic range compared to asymmetric?

- **Concept: Outlier Channels in Transformers**
  - Why needed: The entire motivation for SmoothQuant and Hadamard rotation is the existence of "heavy-tailed distributions" and "large outliers."
  - Quick check: In a transformer, do outlier activations typically appear randomly across tokens, or are they concentrated in specific feature channels (huge activation values for specific dimensions)?

- **Concept: Prefill vs. Decode Latency**
  - Why needed: The efficiency results specifically report "prefill latency" speedup, which is the prompt processing phase.
  - Quick check: Why does quantization provide the most significant speedup during the prefill phase compared to the decode phase? (Hint: Memory bandwidth usage patterns).

## Architecture Onboarding

- **Component map:** openPangu-Embedded (1B/7B) FP16 -> Calibration Dataset -> Quantizer (Symmetric PTQ with SmoothQuant/Hadamard) -> CATLASS GEMM templates -> Atlas A2 NPU

- **Critical path:**
  1. Select CoT mode to determine calibration data distribution
  2. Apply SmoothQuant scaling or Hadamard rotation to weights/activations
  3. Configure CATLASS GEMM templates for INT8 or W4A8 layouts
  4. Deploy to Atlas A2 and verify memory usage matches predictions

- **Design tradeoffs:**
  - INT8 (W8A8): High fidelity (>90% accuracy), 1.5x speed, safe default
  - W4A8 (Baseline): High memory savings, but high accuracy loss; risky for complex reasoning
  - W4A8 (Smooth/Hadamard): Best compression/accuracy balance for memory-constrained edge cases

- **Failure signatures:**
  - Repetitive Generation: Model outputs terminal segments with identical phrases repeated until stop
  - CoT Collapse: "slow_think" mode regresses to "no_think" style outputs unexpectedly

- **First 3 experiments:**
  1. Run FP16 vs. INT8 on HumanEval at Batch Size 32 on Atlas A2 to confirm ~1.5x prefill speedup and ~40% memory drop
  2. Run 1B model on MBPP in FP16 vs. INT8 to check for repetitive generation metric improvement
  3. Run W4A8 baseline vs. W4A8+SmoothQuant on 7B slow_think mode to verify accuracy recovery on HumanEval

## Open Questions the Paper Calls Out

### Open Question 1
Can advanced calibration or mixed-precision strategies further close the accuracy gap between W4A8 quantization and the FP16 baseline? The authors state that W4A8 results in "moderate trade-off in accuracy" despite improvements from SmoothQuant and Hadamard rotation. While preprocessing techniques helped, W4A8 still underperforms compared to INT8 and FP16, particularly on MBPP. Evaluation of mixed-precision quantization on the Atlas A2 would determine if the accuracy drop is fundamental to 4-bit weights or a calibration issue.

### Open Question 2
How does low-bit quantization impact general natural language understanding and reasoning tasks outside of code generation? The evaluation is strictly limited to code generation benchmarks, yet the openPangu models are general-purpose LLMs. Quantization effects on code syntax may not correlate with semantic reasoning or text comprehension required for other tasks. Benchmarking the INT8 and W4A8 models on standard NLU datasets would verify if the >90% fidelity holds across domains.

### Open Question 3
What mechanistic interaction between INT8 quantization and the attention mechanism reduces repetitive generation in the 1B model? The paper explicitly notes a counter-intuitive finding where INT8 quantization reduced repetitive outputs in the 1B model from 34.15% to 21.95%, but offers no causal explanation. It is unclear if this improvement is a result of noise regularization or a hardware-specific artifact. A perturbation analysis comparing FP16 and INT8 activation patterns in the decoding phase would identify where the repetition loop is broken.

## Limitations
- Calibration data size and selection strategy are not specified, creating uncertainty in replication fidelity
- CATLASS configuration details for achieving reported efficiency gains are not fully detailed
- Results are limited to code generation benchmarks, with unclear generalization to broader reasoning tasks

## Confidence
- **High Confidence**: INT8 quantization achieves 1.5x prefill speedup and 40% memory reduction on Atlas A2 (direct measurements in Table 3)
- **Medium Confidence**: INT8 preserves >90% FP16 accuracy and CoT reasoning structure (supported by Table 2 and Figure 5, but benchmark-specific)
- **Medium Confidence**: W4A8 quantization with SmoothQuant/Hadamard rotation recovers accuracy (Figure 1 and Table 2 show improvements, dependent on outlier distribution)

## Next Checks
1. **Verify Calibration Stability**: Run the same quantization pipeline with varying calibration dataset sizes (100, 1000, 10000 samples) and measure accuracy variance on HumanEval to quantify sensitivity to calibration data volume.
2. **Test Broader Reasoning Tasks**: Apply the quantized models to non-coding reasoning benchmarks (GSM8K or BBH) and compare CoT output quality and accuracy retention against reported coding benchmarks.
3. **Benchmark Decode Latency**: Measure token generation (decode) latency and memory usage for INT8 vs. FP16, as the paper only reports prefill phase improvements, to validate if decode efficiency gains match prefill results.