---
ver: rpa2
title: '"I Wrote, I Paused, I Rewrote" Teaching LLMs to Read Between the Lines of
  Student Writing'
arxiv_id: '2506.08221'
source_url: https://arxiv.org/abs/2506.08221
tags:
- feedback
- writing
- students
- they
- more
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study explored whether incorporating keystroke logging and\
  \ snapshot data into LLM feedback could better reflect students\u2019 writing processes.\
  \ Twenty students used a custom interface to write essays while their typing and\
  \ revision behaviors were captured."
---

# "I Wrote, I Paused, I Rewrote" Teaching LLMs to Read Between the Lines of Student Writing

## Quick Facts
- arXiv ID: 2506.08221
- Source URL: https://arxiv.org/abs/2506.08221
- Reference count: 6
- Twenty students wrote essays while keystroke logging captured typing and revision behaviors

## Executive Summary
This study investigated whether incorporating keystroke logging and snapshot data into LLM feedback could better reflect students' writing processes. Students used a custom interface to write essays while their typing and revision behaviors were captured. Gemini-generated feedback was evaluated alongside student survey responses to assess the value of process-aware feedback.

The research found that process-aware feedback was perceived as more accurate and aligned with student thinking, particularly in detecting revision struggles and essay structure issues. However, some students felt the system overly highlighted past mistakes or missed nuanced rhetorical choices. Overall, integrating process data improved feedback relevance and personalization, suggesting potential for more context-aware, pedagogically aligned AI writing support.

## Method Summary
Twenty students participated in a single-session study using a custom interface to write essays while their typing and revision behaviors were captured through keystroke logging. The system collected snapshot data of the writing process, including pauses, deletions, and structural changes. Gemini was used to generate feedback for these essays, with some conditions incorporating process data and others not. Student perceptions were collected through surveys to evaluate the effectiveness and accuracy of the feedback. The study design compared process-aware feedback against traditional feedback approaches, though it did not include a direct baseline comparison with standard LLM feedback.

## Key Results
- Process-aware feedback was perceived as more accurate and aligned with student thinking, especially for detecting revision struggles and essay structure issues
- Some students felt the system overly highlighted past mistakes or missed nuanced rhetorical choices
- Integrating process data improved feedback relevance and personalization

## Why This Works (Mechanism)
The mechanism works by providing the LLM with temporal and behavioral context beyond the final text. Keystroke logging captures hesitation patterns, revision cycles, and structural changes that indicate where students struggled or changed their thinking. This process data allows the LLM to infer intention, difficulty points, and developmental stages that aren't visible in the final draft alone. By understanding the "how" of writing, not just the "what," the feedback can be more targeted to actual student needs and cognitive processes during composition.

## Foundational Learning
- **Keystroke logging**: Captures typing patterns, pauses, and deletions to reveal writing behavior - needed to understand writing process beyond final product - quick check: verify capture of timing and deletion patterns
- **Process-aware feedback**: Incorporates behavioral data into feedback generation - needed to provide context-sensitive guidance - quick check: ensure behavioral features are integrated into prompt
- **LLM contextual prompting**: Designing prompts that include process metadata - needed to guide model attention to behavioral signals - quick check: verify prompt structure includes process context
- **Student self-reporting bias**: Understanding limitations of survey-based evaluation - needed for interpreting subjective feedback data - quick check: assess survey question clarity and response consistency
- **Small-sample generalization**: Recognizing limitations of n=20 studies - needed for interpreting statistical significance - quick check: verify sample diversity and session duration
- **Pedagogical alignment**: Ensuring feedback matches educational objectives - needed for practical classroom adoption - quick check: compare feedback to rubric criteria

## Architecture Onboarding
**Component Map**: Custom interface -> Keystroke logger -> Snapshot collector -> Gemini API -> Feedback generator -> Student survey
**Critical Path**: Writing session (data capture) -> Process data aggregation -> LLM prompt construction -> Feedback generation -> Student evaluation
**Design Tradeoffs**: Process data provides rich context but increases complexity and privacy concerns; simpler feedback is easier to implement but less personalized
**Failure Signatures**: Overemphasis on past mistakes, missing rhetorical nuances, privacy concerns from data collection, misalignment with pedagogical goals
**First Experiments**:
1. Implement keystroke logging with basic timing and deletion capture
2. Test Gemini feedback generation with and without process data inclusion
3. Conduct small-scale pilot with 5 students to validate data collection pipeline

## Open Questions the Paper Calls Out
None

## Limitations
- Small sample size (n=20) and single-session design restrict generalizability
- No direct comparison against baseline LLM feedback without process data
- Evaluation relied entirely on student perceptions rather than objective writing quality improvements

## Confidence
- Perception of better capture of essay structure and revision struggles: **Medium confidence** (relies on student survey responses with potential self-reporting bias)
- Students feeling system overemphasized past mistakes or missed rhetorical nuances: **High confidence** (consistent across survey data)
- Overall improvement in feedback relevance and personalization: **Medium confidence** (based on subjective student feedback)

## Next Checks
1. Conduct a controlled experiment comparing process-aware feedback against standard LLM feedback with larger, diverse student samples across multiple writing assignments
2. Implement blinded third-party assessment of writing quality improvements between process-aware and non-process feedback conditions
3. Test the system's ability to accurately detect and appropriately respond to nuanced rhetorical choices through expert composition instructor review of feedback quality