---
ver: rpa2
title: Exploring EEG and Eye Movement Fusion for Multi-Class Target RSVP-BCI
arxiv_id: '2501.03596'
source_url: https://arxiv.org/abs/2501.03596
tags:
- target
- uni00000013
- rsvp
- uni00000003
- uni00000011
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of multi-class target detection
  in RSVP-BCI systems, which requires distinguishing between multiple target categories.
  The authors introduce eye movement (EM) signals into the decoding process and propose
  a multi-modal fusion network (MTREE-Net) that enhances classification by leveraging
  both EEG and EM signals.
---

# Exploring EEG and Eye Movement Fusion for Multi-Class Target RSVP-BCI

## Quick Facts
- arXiv ID: 2501.03596
- Source URL: https://arxiv.org/abs/2501.03596
- Reference count: 40
- Key outcome: Multi-modal fusion network (MTREE-Net) achieves up to 74.42% balanced accuracy on multi-class RSVP-BCI tasks by integrating EEG and eye movement signals

## Executive Summary
This paper addresses the challenge of multi-class target detection in RSVP-BCI systems, which requires distinguishing between multiple target categories. The authors introduce eye movement (EM) signals into the decoding process and propose a multi-modal fusion network (MTREE-Net) that enhances classification by leveraging both EEG and EM signals. The core method includes a dual-complementary module to improve feature discrimination, a contribution-guided reweighting module to optimize modality fusion, and a hierarchical self-distillation module to reduce misclassification of non-target samples. Experiments on a newly collected dataset with 43 subjects demonstrate that MTREE-Net significantly outperforms existing methods.

## Method Summary
The authors propose MTREE-Net, a multi-modal fusion network that combines EEG and eye movement signals for multi-class target detection in RSVP-BCI systems. The architecture includes a dual-complementary module using cross-attention to enhance feature discrimination between modalities, a contribution-guided reweighting module that optimizes fusion weights based on modality contributions, and a hierarchical self-distillation module that improves non-target classification. The model was evaluated on a new dataset with 43 subjects using 96 static images across three classes, achieving significant improvements over existing methods.

## Key Results
- MTREE-Net achieves up to 74.42% balanced accuracy on multi-class RSVP tasks
- The model significantly outperforms existing methods on a newly collected dataset with 43 subjects
- Confusion matrices show MTREE-Net significantly reduces misclassification between Target-1 and Target-2 classes
- EM integration reduces non-target misclassification rates by 15-20% compared to EEG-only baselines

## Why This Works (Mechanism)

### Mechanism 1: Cross-Modal Complementary Enhancement
- **Claim:** If EEG and eye movement (EM) signals are fused, the resulting model can better distinguish between visually similar target categories (e.g., "harbor" vs. "parking lot") that evoke highly similar ERPs in EEG alone.
- **Mechanism:** The **Dual-Complementary Module (DCM)** uses cross-attention to transfer feature representations between the EEG (strong modality) and EM (weak modality) streams. This bidirectional enhancement prevents the strong modality from dominating gradients, ensuring the EM stream learns discriminative features (like pupil area changes) that aid classification.
- **Core assumption:** Subjects generate consistent, involuntary ocular responses (pupil dilation, fixation shifts) that differ statistically between target categories, providing a secondary signal source.
- **Evidence anchors:**
  - [Section 6.3] Confusion matrices show MTREE-Net significantly lowers the misclassification rate between Target-1 and Target-2 compared to the EEG baseline.
  - [Section 6.7.2] Statistical analysis confirms significant differences in pupil area and horizontal position between target classes, validating the discriminative potential of EM.
  - [Corpus: Consumer-grade EEG-based Eye Tracking] Highlights the physiological link between EEG artifacts and eye movements, supporting the feasibility of fusion.
- **Break condition:** If the visual stimulus presentation rate (10 Hz) is increased significantly, ocular responses may lag or blur, reducing EM discriminability and breaking the fusion benefit.

### Mechanism 2: Contribution-Guided Reweighting
- **Claim:** Explicitly supervising the fusion weights with a theoretically derived "contribution ratio" yields better multi-modal fusion than standard concatenation or attention mechanisms optimized solely by final classification loss.
- **Mechanism:** The **Contribution-Guided Reweighting Module (CG-RM)** calculates the contribution of EEG and EM features by decomposing the final logits. It forces a lightweight network ($\phi$) to predict weights that match these contributions ($L_{cg}$), ensuring the model dynamically prioritizes the modality that is currently more confident for a specific sample.
- **Core assumption:** The unimodal logits ($f(x_{eeg})$ and $f(x_{em})$) provide a reliable proxy for the "ground truth" contribution of each modality to the final decision.
- **Evidence anchors:**
  - [Section 4.3.1] Provides the mathematical derivation of decomposing logits $f(x)$ into modality-specific contributions.
  - [Section 5.4] Ablation study shows removing $L_{cg}$ causes a drop in Balanced Accuracy, specifically impairing Recall compared to the full model.
- **Break condition:** If one modality is consistently noisy (e.g., poor EEG contact), its logits may be random, making the "contribution ratio" a misleading supervision signal for the reweighting network.

### Mechanism 3: Hierarchical Self-Distillation
- **Claim:** Using a robust binary classifier (Target vs. Non-target) to supervise a more complex triplet classifier (Target-1 vs. Target-2 vs. Non-target) reduces the false positive rate for non-target samples.
- **Mechanism:** The **Hierarchical Self-Distillation Module (HSM)** trains a binary classifier in parallel with the main triplet classifier. It transfers knowledge via Kullback-Leibler divergence ($L_{sd}$), forcing the triplet classifier to inherit the binary model's confidence in rejecting non-targets.
- **Core assumption:** The binary discrimination task (Target/Non-target) is significantly easier and more robust than the multi-class discrimination task.
- **Evidence anchors:**
  - [Section 6.6] t-SNE visualizations demonstrate that features from the model with HSM have significantly reduced overlap between target and non-target clusters compared to the ablated model.
  - [Section 6.3] Shows a significant reduction in the misclassification of non-targets as targets when EM is integrated (facilitated by this hierarchical structure).
- **Break condition:** If the dataset has extreme class imbalance where the binary task is unstable, distilling from the binary classifier could propagate noise rather than signal.

## Foundational Learning

- **Concept:** **Rapid Serial Visual Presentation (RSVP)**
  - **Why needed here:** This is the core experimental paradigm. You must understand that images flash at 10 Hz, evoking time-locked ERPs (like P300) that are the primary decoding targets.
  - **Quick check question:** Why is single-class RSVP (Target/Non-target) easier than Multi-class RSVP (Target-1/Target-2/Non-target)?
- **Concept:** **Cross-Attention Mechanisms**
  - **Why needed here:** The Dual-Complementary Module relies on cross-attention to allow the EEG stream to "query" the EM stream and vice versa.
  - **Quick check question:** In the context of this paper, does the DCM use self-attention or cross-attention to solve the "modality imbalance" problem?
- **Concept:** **Modality Competition/Imbalance**
  - **Why needed here:** A central problem identified is that the strong EEG modality can prevent the weak EM modality from learning during gradient descent.
  - **Quick check question:** How does the DCM specifically prevent the EM stream from being ignored during training?

## Architecture Onboarding

- **Component map:** Multi-scale CNN (EEG) -> Dual-Complementary Module (DCM) -> Contribution-Guided Reweighting (CG-RM) -> Hierarchical Self-Distillation (HSM) -> Classifier

- **Critical path:** The implementation of the **CG-RM loss ($L_{cg}$)** is the most complex logic. You must correctly implement Eq. (5-9), where you calculate uni-modal logits on the fly, derive the contribution ratio, and use L1-norm to force the reweighting network to match it.

- **Design tradeoffs:**
  - **Complexity vs. Accuracy:** The architecture adds three distinct loss terms ($L_{cls}, L_{cg}, L_{sd}$). This improves BA by ~5-9% but complicates hyperparameter tuning (e.g., balancing $\lambda$ in $L_{cls}$ vs $L_{sd}$).
  - **Hardware:** Requires synchronized EEG and Eye-tracking hardware (EyeLink 1000 plus), increasing setup complexity compared to EEG-only systems.

- **Failure signatures:**
  - **EM stream collapse:** If the BA of the EM stream (checked via uni-modal features in DCM) stays near random chance (33%), the DCM is failing to balance optimization.
  - **Non-target flooding:** If the model predicts "Target" for almost everything, the Self-Distillation ($L_{sd}$) weight may be too low or the binary classifier is failing to converge.

- **First 3 experiments:**
  1. **Baseline Verification:** Run the EEG-only baseline vs. MTREE-Net on a single subject to confirm the performance gap exists and the fusion is working.
  2. **Ablate DCM:** Remove the Dual-Complementary Module to verify if the EM features degrade (as suggested in Table 4), confirming the optimization imbalance hypothesis.
  3. **Saliency Check:** Generate saliency maps (Section 6.8) for the EM modality to ensure the model is actually attending to the pupil area and horizontal position during the 400-800ms window, rather than overfitting to noise.

## Open Questions the Paper Calls Out
- How do varying RSVP presentation rates impact the characteristics of induced EEG and eye movement signals and the subsequent decoding performance of the model?
- Can the fusion network maintain robust decoding performance when faced with incomplete multi-modal samples or partially missing modalities?
- How does the model's classification performance scale when the number of target categories increases beyond two?

## Limitations
- The dataset uses 96 static images across 3 classes with 48 subjects for training and 43 for testing, which may not generalize to more diverse real-world applications
- The EM signal quality depends on precise calibration and subject fixation maintenance with the EyeLink 1000 Plus system
- The model extracts EM features from 400-800ms post-stimulus onset using a fixed window that may not adapt well to individual differences

## Confidence
- **High Confidence:** Core mechanism of using cross-attention in DCM to prevent modality imbalance is well-supported by ablation studies
- **Medium Confidence:** Claims about broad applicability to other multi-class RSVP scenarios are limited by the constrained experimental setup
- **Medium Confidence:** Contribution-Guided Reweighting assumption that unimodal logits reliably represent "ground truth" modality contributions may not hold in noisier real-world conditions

## Next Checks
1. Test MTREE-Net on a dataset with significantly more subjects (n>100) and diverse image categories to assess generalizability
2. Systematically introduce controlled calibration drift or eye-tracking noise to evaluate how sensitive the EM-fusion benefits are to signal quality degradation
3. Evaluate computational latency and memory requirements of the full MTREE-Net architecture to determine feasibility for real-time BCI deployment