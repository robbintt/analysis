---
ver: rpa2
title: Leveraging LLMs for Predicting Unknown Diagnoses from Clinical Notes
arxiv_id: '2503.22092'
source_url: https://arxiv.org/abs/2503.22092
tags:
- configurations
- accuracy
- clinical
- voting
- majority
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explores using large language models (LLMs) to predict
  unknown diagnoses from clinical notes and link them to medications. A new dataset
  of 240 expert-annotated medication-diagnosis pairs from 20 MIMIC-IV clinical notes
  was created.
---

# Leveraging LLMs for Predicting Unknown Diagnoses from Clinical Notes

## Quick Facts
- **arXiv ID**: 2503.22092
- **Source URL**: https://arxiv.org/abs/2503.22092
- **Reference count**: 0
- **Primary result**: Majority voting across diverse LLM configurations achieved 75% accuracy in predicting unknown diagnoses from clinical notes, outperforming the best single configuration at 66%.

## Executive Summary
This study explores using large language models (LLMs) to predict unknown diagnoses from clinical notes and link them to medications. A new dataset of 240 expert-annotated medication-diagnosis pairs from 20 MIMIC-IV clinical notes was created. GPT-3.5 Turbo was tested across 18 configurations with varying temperature, top-p, and summary lengths. Majority voting across diverse LLM configurations achieved 75% accuracy, outperforming the best single configuration at 66%. Shorter summaries generally improved accuracy, while longer summaries were effective only with deterministic settings. Combining deterministic, balanced, and exploratory strategies yielded the best results.

## Method Summary
The study created a dataset of 240 expert-annotated medication-diagnosis pairs from 20 MIMIC-IV clinical notes. GPT-3.5 Turbo was configured with 18 parameter combinations (3 temperature × 3 top-p × 2 summary lengths) using zero-shot prompting. Each configuration generated diagnosis predictions for medication-note pairs. Outputs were standardized and compared to ground truth using fuzzy matching at 60% similarity. Five-configuration subsets were evaluated using majority voting to identify the optimal ensemble approach.

## Key Results
- Majority voting across diverse LLM configurations achieved 75% accuracy, outperforming the best single configuration at 66%
- Shorter summaries (2000 chars) generally improved accuracy compared to longer summaries (4000 chars)
- Combining deterministic, balanced, and exploratory strategies yielded better results than using any single approach

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Majority voting across diverse LLM configurations reduces individual configuration biases and improves robustness.
- Mechanism: Different temperature and top-p settings cause the model to sample from different regions of the probability distribution. Deterministic settings (low temp/top-p) provide stable, high-confidence predictions; balanced settings offer moderate exploration; exploratory settings surface less obvious candidates. Aggregating these diverse predictions filters out configuration-specific errors while preserving correct predictions that multiple configurations converge on.
- Core assumption: Different configurations make independent errors that cancel out, while correct predictions tend to converge.
- Evidence anchors:
  - [abstract] "Majority voting across diverse LLM configurations achieved 75% accuracy, outperforming the best single configuration at 66%"
  - [section] "no single hyperparameter setting dominated; instead, combining diverse configurations aligned with deterministic, balanced, and exploratory strategies yielded better performance"
  - [corpus] Related ensemble work (LLM-Synergy, LLM-TOPLA) supports diversity-based voting in medical QA, though not specifically for medication-diagnosis linkage
- Break condition: If configurations make systematically correlated errors (same wrong diagnosis across settings), majority voting will amplify rather than correct the error.

### Mechanism 2
- Claim: Shorter summaries improve accuracy by constraining context, reducing the error surface for generation.
- Mechanism: Longer contexts provide more information but introduce more tokens the model can attend to incorrectly. When combined with high temperature/top-p, extended context amplifies generation variance. Deterministic settings (low temp/top-p) counteract this by restricting token selection to high-probability choices, making longer summaries viable only under those conditions.
- Core assumption: Diagnostic signals are concentrated in key sentences rather than distributed across long notes.
- Evidence anchors:
  - [abstract] "Shorter summaries generally improved accuracy, while longer summaries were effective only with deterministic settings"
  - [section] "excessive variability in generation settings may negatively impact majority voting outcomes when longer summaries are used"
  - [corpus] No direct corpus evidence for this context-length–variance interaction; mechanism is paper-specific
- Break condition: If a diagnosis requires synthesizing scattered clues across a long note, truncation will lose critical signal.

### Mechanism 3
- Claim: Zero-shot prompting leverages implicit medical knowledge from pretraining without task-specific fine-tuning.
- Mechanism: GPT-3.5 Turbo's pretraining on diverse medical text encodes statistical associations between medications, symptoms, and diagnoses. Given a clinical note and medication, the model retrieves relevant patterns from parametric knowledge to predict the most likely diagnosis.
- Core assumption: Pretraining data contained sufficient medication-diagnosis co-occurrences and clinical reasoning patterns for the target domain.
- Evidence anchors:
  - [abstract] "LLMs can predict implicitly mentioned diagnoses from clinical notes and link them to corresponding medications"
  - [section] "The zero-shot approach allows for flexible and scalable diagnostic prediction without requiring extensive task-specific fine-tuning"
  - [corpus] Related work (Gu et al., Wang et al.) shows LLMs extracting medical knowledge, supporting pretraining-based reasoning
- Break condition: If medication-diagnosis relationships are novel, rare, or institution-specific (not in pretraining data), predictions will fail or hallucinate.

## Foundational Learning

- **Concept**: Temperature and Top-p Sampling
  - Why needed here: These hyperparameters control output diversity. Temperature (0.1–0.95) governs randomness; top-p (0.1–0.9) restricts sampling to a probability mass threshold. The paper's ensemble strategy depends on systematically varying both.
  - Quick check question: Given temperature=0.1 and top-p=0.1 versus temperature=0.95 and top-p=0.9, which pair produces more deterministic outputs?

- **Concept**: Majority Voting for Ensembles
  - Why needed here: The core accuracy-boosting technique. Aggregating predictions from multiple configurations selects the most frequent output, filtering noise.
  - Quick check question: If 5 configurations output [hypertension, hypertension, GERD, hypertension, diabetes], what diagnosis does majority voting return?

- **Concept**: Zero-Shot Prompting
  - Why needed here: The paper uses no task-specific training. Understanding prompt design is critical—poorly framed prompts yield unusable outputs.
  - Quick check question: What key elements should a zero-shot prompt include to predict a diagnosis from a clinical note and medication?

## Architecture Onboarding

- **Component map**: Clinical notes → Summarization (2000/4000 chars) → Medication extraction → GPT-3.5 Turbo (18 configs) → Raw predictions → Text normalization → Fuzzy matching (60% threshold) → Majority voting (5-config subsets) → Final diagnosis
- **Critical path**: Note selection → Summarization → Prompt construction → Multi-configuration inference → Output standardization → Majority voting → Accuracy evaluation
- **Design tradeoffs**:
  - Summary length: 2000 chars reduces noise but may miss context; 4000 chars preserves detail but increases error surface unless paired with deterministic settings
  - Ensemble composition: Paper found mixing deterministic/balanced/exploratory optimal; pure deterministic lacks diversity, pure exploratory adds noise
  - Subset size: 5 configurations chosen from 18; larger subsets increase cost without proportional accuracy gains
  - Matching threshold: 60% fuzzy threshold balances strictness with flexibility for terminology variations
- **Failure signatures**:
  - Low inter-configuration agreement (all different predictions): Note may lack sufficient signal; medication ambiguous
  - Majority voting underperforms single best: Configurations making correlated errors
  - Systematic errors on specific medication classes: Gap in pretraining knowledge
  - Long summary + high temperature yields poor results: Violates the paper's interaction constraint
- **First 3 experiments**:
  1. Replicate the 18-configuration grid on a held-out sample; compare single-best vs. majority voting accuracy to validate the 66% → 75% improvement.
  2. Ablation study: Remove one configuration at a time from the optimal subset (turns 2, 7, 10, 13, 14) to quantify each configuration's contribution.
  3. Boundary test: Vary summary length continuously (1500, 2000, 2500, 3000, 3500, 4000 chars) under deterministic settings to locate the accuracy cliff.

## Open Questions the Paper Calls Out

- **Open Question 1**: Does the majority voting ensemble strategy maintain its effectiveness when applied to domain-specific LLMs (e.g., Med-PaLM 2) or newer general architectures (e.g., GPT-4)?
  - Basis in paper: The authors explicitly state that future research should compare different LLM architectures, including "domain-specific models like Med-PaLM 2," to offer insights into model robustness and performance variations.
  - Why unresolved: The study restricted its evaluation to a single model version (GPT-3.5 Turbo), leaving the transferability of the hyperparameter diversity strategy untested on specialized medical models.
  - What evidence would resolve it: A comparative study applying the specific temperature/top-p voting strategy to Med-PaLM 2 or GPT-4 using the same 240 annotated pairs.

- **Open Question 2**: How does the diagnostic prediction accuracy of majority voting scale when applied to larger, more diverse clinical datasets?
  - Basis in paper: The conclusion notes that the dataset was relatively small (20 notes) and suggests that "future work should explore scalability with larger datasets... to validate the generalizability of the findings."
  - Why unresolved: The limited sample size (240 pairs from a single institution, MIMIC-IV) may restrict the statistical confidence and generalizability of the 75% accuracy rate.
  - What evidence would resolve it: Replicating the experiment on a multi-institutional dataset with thousands of clinical notes to verify if the accuracy improvements hold.

- **Open Question 3**: Can alternative ensemble techniques, such as weighted voting or stacking, outperform the simple majority voting method used in this study?
  - Basis in paper: The discussion acknowledges that the findings are based on majority voting and that "alternative ensemble methods such as weighted voting or stacking were not explored."
  - Why unresolved: It is unclear if assigning weights to specific configurations (e.g., favoring deterministic settings) could yield higher than 75% accuracy.
  - What evidence would resolve it: Benchmarking the performance of weighted voting schemes against the unweighted majority voting baseline established in the paper.

- **Open Question 4**: To what extent do LLM hallucinations impact the safety and reliability of the predicted medication-diagnosis links in this ensemble approach?
  - Basis in paper: The authors list "investigating the impact of LLM hallucinations and developing mitigation strategies" as a necessary area for future research to improve reliability.
  - Why unresolved: While the study measured accuracy, it did not qualitatively analyze the nature of the incorrect predictions (the 25% error rate) to determine if they contained plausible but hallucinated diagnoses.
  - What evidence would resolve it: An error analysis of the incorrect predictions to categorize the frequency and severity of hallucinated diagnoses versus simple extraction failures.

## Limitations

- Results are based on only 20 MIMIC-IV notes with 240 annotated medication-diagnosis pairs, limiting generalizability to broader clinical contexts.
- The paper does not detail the expert annotation protocol or inter-rater agreement, leaving ambiguity about ground truth quality and consistency.
- The optimal 5-configuration subset was chosen from 8,732 possible combinations, with unknown sensitivity to different subset selections.

## Confidence

- **High Confidence**: The core finding that majority voting across diverse LLM configurations improves diagnostic prediction accuracy (75% vs 66%) is well-supported by the experimental results and aligns with ensemble learning principles.
- **Medium Confidence**: The claim that shorter summaries generally improve accuracy while longer summaries require deterministic settings is supported by results but lacks extensive ablation or boundary testing to fully validate the interaction.
- **Medium Confidence**: The mechanism that zero-shot prompting leverages pretraining knowledge for medical reasoning is plausible given related work, but the paper does not directly test whether pretraining data contained the specific medication-diagnosis associations used.

## Next Checks

1. **Generalization Test**: Apply the method to a new set of clinical notes from MIMIC-IV or another EHR system to assess performance stability and identify potential domain-specific limitations.
2. **Annotation Reliability Check**: Re-annotate a subset of the dataset with multiple experts to measure inter-rater agreement and evaluate how annotation subjectivity affects model performance.
3. **Configuration Robustness Test**: Systematically vary the 5-configuration subsets (e.g., test all combinations of deterministic, balanced, and exploratory settings) to quantify sensitivity and ensure the reported improvement is not due to a particular lucky selection.