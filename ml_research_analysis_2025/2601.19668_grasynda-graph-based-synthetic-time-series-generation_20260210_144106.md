---
ver: rpa2
title: 'Grasynda: Graph-based Synthetic Time Series Generation'
arxiv_id: '2601.19668'
source_url: https://arxiv.org/abs/2601.19668
tags:
- series
- time
- data
- forecasting
- synthetic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of data augmentation for time series
  forecasting, where deep learning models require large training sets but real-world
  data is often limited. The core method, Grasynda, converts univariate time series
  into graph representations where each state is a node and transitions between states
  are directed edges.
---

# Grasynda: Graph-based Synthetic Time Series Generation

## Quick Facts
- **arXiv ID:** 2601.19668
- **Source URL:** https://arxiv.org/abs/2601.19668
- **Reference count:** 25
- **Primary result:** Graph-based augmentation method consistently improves time series forecasting accuracy across multiple datasets and neural architectures

## Executive Summary
Grasynda addresses data augmentation for time series forecasting by converting univariate series into graph representations where states are nodes and transitions are directed edges. The method builds a transition probability matrix from these transitions to generate synthetic series that preserve temporal dynamics. Experiments on six benchmark datasets and three neural network architectures show Grasynda outperforms other augmentation methods, achieving the lowest average MASE in two out of three models and ranking second in the remaining one. It demonstrated better results than baseline in 72% of cases with 7 statistically significant improvements.

## Method Summary
Grasynda converts univariate time series into graph representations by discretizing values into states (nodes) and mapping sequential observations to directed edges. It builds a transition probability matrix from these transitions, which is then used to generate synthetic time series. The method handles non-stationarity through STL decomposition, applying the graph generation only to the stationary remainder component while preserving original trend and seasonal structures. Synthetic values are sampled from uniform distributions within discrete state bounds, maintaining learned transition rules while injecting realistic variability.

## Key Results
- Grasynda achieved the lowest average MASE in two out of three neural network models (NHITS, MLP, KAN)
- Method outperformed TSMixup, which is used to train the state-of-the-art Chronos foundation model
- Showed better results than baseline in 72% of cases with 7 statistically significant improvements

## Why This Works (Mechanism)

### Mechanism 1
Converting time series into directed graphs preserves local temporal dynamics better than magnitude-based transformations. The method discretizes continuous values into k states and maps sequential observations to directed edges, building a transition probability matrix where rows sum to 1. This encodes the "rules" of series movement rather than just statistical distribution, assuming first-order Markov processes capture dependencies.

### Mechanism 2
Applying graph generation only to the stationary remainder component preserves global non-stationary structures. STL decomposition separates series into Trend, Seasonal, and Remainder components, with Grasynda applied exclusively to the Remainder. This augments local variability while keeping trend/seasonality rigid structures, assuming the Remainder contains meaningful dynamics worth augmenting.

### Mechanism 3
Sampling synthetic values from uniform distributions within discrete state bounds injects realistic variability while adhering to learned transition rules. After generating discrete state sequences, the method converts back to continuous values by sampling from actual observed values that fell into each bin, preserving local statistical properties through empirical sampling.

## Foundational Learning

- **Concept: Markov Chains and Transition Matrices**
  - Why needed here: Grasynda assumes next step depends only on current state. Understanding how to build and sample from a Transition Matrix is required to comprehend the generation engine.
  - Quick check question: If the probability of moving from State A to State B is 0.8, and we are currently in State A, how do we determine the next state in a simulation?

- **Concept: Time Series Decomposition (STL)**
  - Why needed here: The paper handles non-stationarity by stripping trend and seasonality before graph conversion. Without understanding STL, one cannot understand why synthetic data preserves original series shape.
  - Quick check question: Why would applying a transition-matrix approach directly to a series with a strong upward trend likely result in a "drifting" synthetic series that loses the trend structure?

- **Concept: Discretization / Quantization**
  - Why needed here: Grasynda converts continuous values into discrete "states" to form graph nodes. The choice of discretization determines the resolution of generated dynamics.
  - Quick check question: What happens to the transition matrix if you choose too few discrete states for a highly volatile time series?

## Architecture Onboarding

- **Component map:** Preprocessing (Raw Series → Trend, Seasonal, Remainder) → Discretizer (Remainder → Discrete State Sequence) → Graph Engine (State Sequence → Transition Matrix P) → Sampler (Matrix P → Synthetic State Sequence) → Projector (Synthetic States → Continuous Values) → Postprocessing (Synthetic Remainder + Original Trend/Seasonal → Final Synthetic Series)

- **Critical path:** The Graph Engine (Transition Matrix) and the Sampler. If matrix construction doesn't correctly normalize counts to probabilities, or if sampler doesn't respect directed edges, temporal dynamics will be lost.

- **Design tradeoffs:**
  - Number of Quantiles (k): Low k (e.g., 10) gives coarse dynamics with robust matrix estimation but loss of nuance. High k (e.g., 100) provides high fidelity but risks sparse matrices with "dead ends" or repetitive loops.
  - Decomposition strategy: STL assumes additive seasonality. Multiplicative seasonality requires Log transform first (not explicitly detailed in defaults).

- **Failure signatures:**
  - "Loopy" output: Synthetic series gets stuck repeating S1 → S2 → S1. Cause: Sparsity in transition matrix creating trap state.
  - Loss of Trend: Synthetic series looks like noise. Cause: Failure to strip trend before generation or failure to re-add after.
  - Over-smoothing: Synthetic series lacks original variance. Cause: Discretization bins too wide, or sampling method incorrect.

- **First 3 experiments:**
  1. Ablation on Quantile Count: Run with k={10, 25, 50, 100} to find stability threshold.
  2. Visual Inspection of State Sequences: Compare frequency of n-grams against original data.
  3. STL vs. Raw Generation: Compare synthetic outputs generated directly on raw data vs. STL-decomposed pipeline.

## Open Questions the Paper Calls Out

- **Question:** How can the Grasynda framework be extended to explicitly address multivariate time series or long-range dependencies?
  - Basis: Current version "is specifically tailored to univariate time series and does not explicitly address multivariate data or long-range dependencies."
  - Why unresolved: Current graph construction maps single-value states to nodes, lacking mechanism to encode cross-series correlations or memory beyond immediate transitions.
  - Evidence: Modified graph construction method successfully generates synthetic data for multivariate benchmarks while preserving cross-series correlations.

- **Question:** Can non-stationarity sources like trend and seasonality be modeled directly within the graph structure?
  - Basis: Method "does not directly model non-stationarity" and integrating these aspects is identified as "promising direction."
  - Why unresolved: Current methodology relies on external STL decomposition preprocessing rather than capturing non-stationary components natively.
  - Evidence: Generative model that replicates trend and seasonality patterns without preprocessing, validated by statistical tests for non-stationarity preservation.

- **Question:** Do alternative discretization strategies, such as visibility graphs, offer improved flexibility over quantile-based approach?
  - Basis: Discussion suggests "more sophisticated discretization strategies (e.g., visibility graphs) may further enhance its flexibility."
  - Why unresolved: Paper utilizes straightforward quantile-based discretization, leaving potential benefits of other structural mapping techniques unexplored.
  - Evidence: Comparative experiments showing superior forecasting accuracy or domain generalization when using visibility graphs versus quantiles.

## Limitations

- Method's reliance on first-order Markov assumptions may not capture long-range dependencies in time series with complex temporal structures
- Performance on multivariate time series or series with complex, non-linear patterns remains untested
- Computational complexity and scalability to larger datasets is not discussed

## Confidence

- **High Confidence:** Core graph construction methodology and STL decomposition approach
- **Medium Confidence:** Empirical performance improvements across multiple datasets and models
- **Low Confidence:** Claim that Grasynda is "more effective" than TSMixup lacks direct statistical comparison

## Next Checks

1. **Hyperparameter Sensitivity Analysis:** Systematically vary k (number of quantiles) from 10 to 100 and measure both synthetic data quality metrics and downstream forecasting performance to identify optimal ranges and stability thresholds.

2. **Long-range Dependency Test:** Generate synthetic series and compute autocorrelation functions at multiple lags to verify that first-order Markov assumption does not eliminate important long-range temporal dependencies present in original data.

3. **Cross-domain Generalization Test:** Apply Grasynda to time series from domains not represented in benchmark datasets (e.g., physiological signals, financial high-frequency data) to assess whether performance improvements generalize beyond tested domains.