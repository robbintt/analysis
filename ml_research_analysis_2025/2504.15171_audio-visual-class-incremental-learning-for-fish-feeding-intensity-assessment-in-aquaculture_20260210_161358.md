---
ver: rpa2
title: Audio-Visual Class-Incremental Learning for Fish Feeding intensity Assessment
  in Aquaculture
arxiv_id: '2504.15171'
source_url: https://arxiv.org/abs/2504.15171
tags:
- learning
- fish
- species
- feeding
- intensity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces HAIL-FFIA, a hierarchical audio-visual class-incremental
  learning framework specifically designed for fish feeding intensity assessment in
  aquaculture. The method addresses catastrophic forgetting when adapting to new fish
  species while maintaining performance on previously learned ones.
---

# Audio-Visual Class-Incremental Learning for Fish Feeding intensity Assessment in Aquaculture

## Quick Facts
- arXiv ID: 2504.15171
- Source URL: https://arxiv.org/abs/2504.15171
- Reference count: 40
- Key outcome: HAIL-FFIA achieves 75.92% average accuracy with 9.36% forgetting on the new AV-CIL-FFIA dataset for fish feeding intensity assessment.

## Executive Summary
This paper introduces HAIL-FFIA, a hierarchical audio-visual class-incremental learning framework specifically designed for fish feeding intensity assessment in aquaculture. The method addresses catastrophic forgetting when adapting to new fish species while maintaining performance on previously learned ones. HAIL-FFIA employs a dual-path architecture that separates general intensity knowledge from species-specific characteristics, using prototype-based knowledge preservation without storing raw data. A dynamic modality balancing system adaptively adjusts the importance of audio versus visual information based on feeding behavior stages. The authors introduce AV-CIL-FFIA, the first dataset for audio-visual class-incremental learning in FFIA, comprising 81,932 labeled audio-visual clips across six fish species. Experimental results demonstrate that HAIL-FFIA achieves 75.92% average accuracy with only 9.36% forgetting on this dataset, significantly outperforming state-of-the-art methods while requiring just 0.1% of the storage needed for raw data.

## Method Summary
HAIL-FFIA addresses catastrophic forgetting in fish feeding intensity assessment through a hierarchical dual-path architecture. The framework uses pretrained S3D (visual) and PANNs MobileNetV2 (audio) encoders with bidirectional cross-modal attention for feature fusion. A general intensity layer captures shared feeding patterns across species using closed-form ridge regression, while species-specific layers independently learn unique audio-visual signatures per fish. Prototype-based knowledge preservation maintains compact class representations via k-means clustering (5 prototypes per intensity level per species), enabling exemplar-free incremental learning. Dynamic modality balancing with learnable attention weights adaptively weights audio and visual contributions based on feeding stages and environmental conditions. The model is trained initially on Red Tilapia with full fine-tuning, then incrementally adds new species while freezing encoders and updating weights analytically.

## Key Results
- HAIL-FFIA achieves 75.92% average accuracy and 9.36% forgetting on the AV-CIL-FFIA dataset
- The method requires only 0.1% of storage compared to raw data (prototype-based approach)
- Dynamic modality balancing improves robustness by adaptively weighting audio and visual information
- Prototype count of m=5 per intensity level provides optimal balance between accuracy and storage efficiency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hierarchical representation learning with a dual-path architecture enables cross-species knowledge transfer while preserving species-specific discriminative features.
- Mechanism: A general intensity layer captures shared feeding patterns (None, Weak, Medium, Strong) across all fish species using closed-form ridge regression: W^av = ((F^av)^T F^av + ηI)^{-1}(F^av)^T Y. Species-specific layers independently learn unique audio-visual signatures per fish, allowing new species to leverage common intensity knowledge without overwriting prior species' characteristics.
- Core assumption: Feeding intensity patterns share common features across species while maintaining species-distinctive behavioral signatures that require separate representation.
- Evidence anchors:
  - [abstract] "employs a dual-path architecture that separates general intensity knowledge from fish-specific characteristics"
  - [Section III-B-2a] "The general intensity layer is essential for cross-species knowledge transfer... Meanwhile, the species-specific layers account for the unique behavioural manifestations"
  - [corpus] Related work on multimodal fish feeding intensity confirms species-specific behavioral variations impact feeding patterns.

### Mechanism 2
- Claim: Prototype-based knowledge preservation mitigates catastrophic forgetting with minimal storage by maintaining compact class representations instead of raw exemplars.
- Mechanism: k-means clustering generates m=5 prototypes per intensity level per species via p_{i,j} = (1/|S_{i,j}|) Σ F^av ∈ S_{i,j} F^av. During incremental learning, prototypes augment the feature matrix: F̃^av_k = [F^av_k; λ_p · P^av], enabling knowledge distillation through direct feature-space integration. Prototypes update via EMA: p^{new} = α·p^{old} + (1-α)·p^{new_cluster} with α=0.7.
- Core assumption: Cluster centroids in learned feature space sufficiently capture class-conditional distributions for discrimination.
- Evidence anchors:
  - [abstract] "prototype-based approach that achieves exemplar-free efficiency while preserving essential knowledge through compact feature representations"
  - [Section VI-C-2] "5 prototypes per intensity level effectively capture the essential characteristics... while maintaining minimal storage requirements (approximately 0.10% of the size of the original dataset)"
  - [corpus] FeTrIL and PASS demonstrate prototype-based CIL viability, though corpus lacks aquaculture-specific validation.

### Mechanism 3
- Claim: Dynamic modality balancing adaptively compensates for environmental degradation by weighting audio and visual contributions based on current conditions.
- Mechanism: Learnable attention weights β^a_{k,i} = σ(w^T_{k,i}[F^a_k, F^v_k]) adjust modality importance per species k and intensity i. The general-vs-specific balance decays as γ_k = γ_{max} - (γ_{max} - γ_{min})·k/K, shifting reliance toward species-specific knowledge as more species are learned.
- Core assumption: Feeding stages and environmental conditions differentially affect modality reliability in predictable, learnable ways.
- Evidence anchors:
  - [abstract] "dynamic modality balancing system that adaptively adjusts the importance of audio versus visual information based on feeding behaviour stages"
  - [Section III-B-1] "visual performance deteriorates in turbid water and insufficient light, while acoustic detection suffers in noisy environments"
  - [corpus] Corpus evidence on multimodal FFIA confirms cross-modal compensation improves robustness under environmental variation.

## Foundational Learning

- Concept: **Catastrophic Forgetting in Class-Incremental Learning**
  - Why needed here: The core problem this paper addresses—neural networks lose previously learned knowledge when fine-tuned on new species.
  - Quick check question: Can you explain why storing exemplars versus exemplar-free methods involve different tradeoffs for resource-constrained aquaculture deployments?

- Concept: **Prototype-based Classification**
  - Why needed here: Understanding how cluster centroids can represent class distributions enables comprehending the storage-accuracy tradeoff in this approach.
  - Quick check question: Why might k-means centroids be insufficient for highly multi-modal class distributions?

- Concept: **Bidirectional Cross-Modal Attention**
  - Why needed here: The fusion mechanism relies on mutual guidance between modalities rather than unidirectional attention.
  - Quick check question: How does spatial-temporal attention differ from simple feature concatenation for video-audio fusion?

## Architecture Onboarding

- Component map:
  - **Dual-Encoder Feature Extraction**: S3D (visual, pretrained on Kinetics-400) + PANNs MobileNetV2 (audio, pretrained on AudioSet)
  - **Cross-Modal Fusion Module**: Bidirectional attention with spatial-temporal pooling (Equations 1-6)
  - **Feature Expansion Layer**: Random fixed projection W^{up} for increased representational capacity
  - **Hierarchical Classifier**: General intensity layer (shared) + species-specific layers (per-fish)
  - **Prototype Memory Bank**: ~0.1% storage, 5 prototypes per intensity level per species
  - **Dynamic Balancing Module**: Learnable β weights + decaying γ parameter

- Critical path:
  1. **Initial Training** (Task 1): Full fine-tuning on first species → extract features → train hierarchical layers via closed-form solution → store prototypes
  2. **Incremental Training** (Tasks 2-K): Freeze encoders → extract features from new species → augment with stored prototypes → update general weights analytically → compute new species-specific weights → update prototype bank via EMA
  3. **Inference**: Extract features → compute both general and species-specific logits → dynamically weight via learned β and γ → softmax output

- Design tradeoffs:
  - **Prototype count (m)**: Paper finds m=5 optimal; m<3 underfits, m>7 yields diminishing returns (Figure 5)
  - **Stability coefficient (α=0.7)**: Controls prototype update speed; lower values adapt faster but risk forgetting
  - **Feature expansion dimension**: Must balance representational capacity against computational cost
  - **Encoders frozen after Task 1**: Reduces adaptation capacity but ensures feature-space consistency for prototype reuse

- Failure signatures:
  - Accuracy drops sharply after new species → check prototype integration weight λ_p
  - Audio-visual fusion underperforms single-modality → inspect attention weight distributions for collapse
  - Late-stage species show high forgetting → verify γ decay schedule allows sufficient species-specific learning
  - Sunfish-level degradation on all methods → dataset quality issue, not architecture failure (noted in paper)

- First 3 experiments:
  1. **Baseline replication**: Train on Red Tilapia (Task 1), incrementally add Tilapia → verify 75.92% avg accuracy and 9.36% forgetting metrics match Table I
  2. **Ablation: prototype count sweep**: Test m ∈ {1, 2, 3, 5, 7, 10} → confirm 5-7 is optimal region per Figure 5
  3. **Modality ablation**: Run audio-only, visual-only, and audio-visual → verify multi-modal advantage holds (should show ~5% improvement over best single modality)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the HAIL-FFIA framework be effectively generalized to other aquaculture monitoring tasks beyond feeding intensity assessment?
- Basis in paper: [explicit] The conclusion states, "Future work will explore extending our approach to other aquaculture tasks."
- Why unresolved: The current architecture, dataset, and experiments are specifically tailored to the four-class classification of feeding intensity, utilizing unique audio-visual signatures for that behavior.
- What evidence would resolve it: Successful application of the hierarchical dual-path architecture to distinct tasks like fish disease detection or stress assessment, demonstrating maintained performance on new behavioral targets.

### Open Question 2
- Question: Do more sophisticated prototype selection strategies yield significant improvements over the current K-means clustering approach?
- Basis in paper: [explicit] The conclusion lists "...investigating more sophisticated prototype selection strategies..." as a specific avenue for future research.
- Why unresolved: The current method relies on a fixed number (5) of K-means centroids per class, which may not be the optimal method for capturing complex, subtle feature variations across diverse species.
- What evidence would resolve it: Comparative benchmarks showing improved accuracy or reduced forgetting when using advanced prototype generation techniques (e.g., learnable prototypes) versus the standard K-means method.

### Open Question 3
- Question: How can the framework be optimized for real-time inference on resource-constrained hardware in commercial aquaculture settings?
- Basis in paper: [explicit] The authors mention "...developing deployment-optimized versions for commercial settings" as a necessary future step.
- Why unresolved: While the paper demonstrates storage efficiency (0.1% of raw data), it does not analyze the computational latency or resource requirements of the dual-encoder system for live, edge-based processing.
- What evidence would resolve it: A deployment study measuring inference latency and energy consumption on embedded devices (e.g., NVIDIA Jetson) while maintaining the reported 75.92% accuracy.

### Open Question 4
- Question: How does the model robustness fare against severe biological anomalies or environmental domain shifts not represented in the training data?
- Basis in paper: [inferred] The results section notes a performance drop for Sunfish due to "health problems," suggesting the model may confuse species-specific features with anomaly features.
- Why unresolved: It is unclear if the "species-specific" layer learns to recognize the fish or the fish's current state (health), and how the model handles domain shifts from tanks to open-water environments.
- What evidence would resolve it: Testing the model on out-of-distribution data containing sick fish or open-water noise to see if the dynamic modality balancing mechanism compensates for the shift.

## Limitations

- The hierarchical dual-path architecture's effectiveness depends on meaningful cross-species intensity feature sharing, which may not generalize beyond the 6 species tested
- The closed-form weight updates assume sufficient regularization (η=1.0) but sensitivity to this hyperparameter remains unclear
- The dynamic modality balancing relies on learnable attention weights that could collapse toward single-modality solutions under extreme environmental degradation

## Confidence

- **High confidence**: Prototype-based knowledge preservation mechanics and storage efficiency claims (directly verifiable from equations and ablation)
- **Medium confidence**: Cross-species transfer benefits (requires broader species validation beyond 6 fish types)
- **Medium confidence**: Dynamic modality balancing effectiveness (depends on environmental variation not fully characterized in current dataset)

## Next Checks

1. **Cross-species generalization test**: Evaluate HAIL-FFIA on fish species not present in AV-CIL-FFIA to verify whether the general intensity layer provides meaningful transfer beyond the training distribution
2. **Environmental robustness validation**: Systematically degrade audio and visual quality independently and jointly to quantify when adaptive modality balancing fails versus when it provides protection
3. **Prototype sensitivity analysis**: Sweep the number of prototypes per class (m=1,3,5,7,10) across all incremental stages to verify the claimed optimal range holds under different learning trajectories