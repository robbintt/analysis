---
ver: rpa2
title: Toward Using Machine Learning as a Shape Quality Metric for Liver Point Cloud
  Generation
arxiv_id: '2508.02482'
source_url: https://arxiv.org/abs/2508.02482
tags:
- liver
- shape
- expert
- good
- pointnet
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of evaluating quality of synthetic
  3D liver shapes when ground truth is unavailable. Instead of relying solely on expert
  review, the authors propose using classical machine learning and PointNet classifiers
  trained on real liver point clouds labeled as "good" or "bad" to assess generated
  liver shapes.
---

# Toward Using Machine Learning as a Shape Quality Metric for Liver Point Cloud Generation

## Quick Facts
- arXiv ID: 2508.02482
- Source URL: https://arxiv.org/abs/2508.02482
- Reference count: 36
- Authors: Khoa Tuan Nguyen, Gaeun Oh, Ho-min Park, Francesca Tozzi, Wouter Willaert, Joris Vankerschaver, Niki Rashidian, Wesley De Neve
- Primary result: ML classifiers can serve as interpretable, task-relevant quality metrics for medical shape generation, achieving >90% accuracy in classifying liver shapes and strong agreement with expert evaluation

## Executive Summary
This paper addresses the challenge of evaluating quality of synthetic 3D liver shapes when ground truth is unavailable. Instead of relying solely on expert review, the authors propose using classical machine learning and PointNet classifiers trained on real liver point clouds labeled as "good" or "bad" to assess generated liver shapes. Geometric features were extracted from 20,000-point liver samples to train models including Random Forest, Extra Trees, SVM, and deep learning-based PointNet variants. Results showed Random Forest and Extra Trees achieved over 90% accuracy in classifying real liver shapes, while PointNet++ demonstrated perfect agreement (κ=1.00) with expert evaluation on generated livers despite lower test set accuracy. SHAP analysis revealed that the min_z feature was most influential, reflecting challenges in accurately segmenting the liver's inferior boundary. The study demonstrates that ML classifiers can serve as interpretable, task-relevant quality metrics for medical shape generation, offering a scalable alternative to labor-intensive expert review.

## Method Summary
The authors sampled 20,000 points uniformly from liver surfaces and extracted 14 geometric features (min/max/mean/std for x,y,z axes plus radius statistics) to train classical ML classifiers. They also trained PointNet and PointNet++ on raw point clouds. Models were trained on 939 real livers (452 "Good", 487 "Bad") with 80/5/15 train/val/test split. Binary labels were derived from 5-category expert annotations. Classifier performance was evaluated on held-out test sets and agreement with expert labels was measured on 63 generated livers using Cohen's κ. SHAP analysis was used to interpret feature importance in the Random Forest model.

## Key Results
- Random Forest and Extra Trees achieved over 90% accuracy in classifying real liver shapes
- PointNet++ demonstrated perfect agreement (κ=1.00) with expert evaluation on generated livers despite lower test set accuracy (86.7%)
- SHAP analysis revealed that min_z feature was most influential, reflecting challenges in accurately segmenting the liver's inferior boundary
- Geometric features can discriminate "good" from "bad" liver point clouds with >90% accuracy, capturing gross shape abnormalities

## Why This Works (Mechanism)

### Mechanism 1
Simple 14-dimensional geometric features can discriminate "good" from "bad" liver point clouds with >90% accuracy. Statistical aggregates (min/max/mean/std along each axis, plus radius statistics) capture gross shape abnormalities—extreme size variations, unusual positioning, and severely incomplete objects create detectable signatures in bounding box dimensions and centroid location that deviate from typical liver geometry. Core assumption: "Bad" liver shapes exhibit systematic geometric deviations from "good" livers that are expressible in low-dimensional statistical summaries.

### Mechanism 2
PointNet++ achieves stronger alignment with expert clinical judgment than its test-set accuracy would suggest, through hierarchical learning of clinically-relevant shape features. Unlike classical ML operating on pre-computed statistics, PointNet++ processes raw point clouds through hierarchical feature aggregation layers that learn to weight spatial regions by their relevance to overall shape quality—emphasizing "clinical utility" patterns over isolated boundary irregularities. Core assumption: Expert evaluation prioritizes holistic anatomical plausibility over local geometric perfection, and PointNet++'s learned representations capture similar prioritization.

### Mechanism 3
The min_z feature's prominence reflects systematic segmentation challenges at the liver's inferior boundary where adjacent organs create ambiguous borders. In CT coordinates, z-axis represents cranio-caudal direction; higher min_z values indicate truncated inferior boundaries or superior displacement—both signatures of failed segmentation at the liver-gallbladder-duodenum-kidney interface. Core assumption: Segmentation failures are not randomly distributed but concentrate at anatomically challenging boundaries, creating learnable patterns.

## Foundational Learning

### Concept: Cohen's Kappa (κ) for Agreement Measurement
Why needed here: The paper relies on κ to claim PointNet++ "perfectly agrees" with experts (κ=1.00) while classical methods show only "moderate agreement" (κ=0.49), despite higher test accuracy. Understanding that κ measures agreement beyond chance—not raw accuracy—is essential to interpret why lower-accuracy PointNet++ may be clinically preferable.
Quick check question: If a classifier predicts "Good" for 98% of samples in a dataset where 98% are truly "Good," would raw accuracy or Cohen's κ better reveal whether the classifier actually learned meaningful patterns?

### Concept: SHAP Values for Feature Attribution
Why needed here: The paper's interpretability claims rest entirely on SHAP analysis revealing min_z importance. SHAP quantifies each feature's contribution to individual predictions using Shapley values from cooperative game theory—understanding this helps distinguish true causal importance from spurious correlation.
Quick check question: If min_z has high SHAP importance but max_z doesn't, what does this suggest about whether the model learned about absolute liver size vs. inferior boundary integrity?

### Concept: Point Cloud Sampling and Density
Why needed here: The method uniformly samples 20,000 points from liver surfaces. This sampling density determines whether fine-grained features (small artifacts, boundary irregularities) are representable or lost to quantization—directly affecting what "quality" signals classifiers can access.
Quick check question: Would sampling 2,000 points vs. 20,000 points be more likely to miss a small artifact that makes a liver "bad"?

## Architecture Onboarding

### Component map:
Raw Liver Mesh → Point Sampling (20K pts) → [BRANCH A: Feature Extraction → 14-D Vector] → Classical ML Classifiers
                              [BRANCH B: Raw 20K pts] → PointNet/PointNet++ → Classification Head
Both branches → Good/Bad prediction → SHAP analysis (Branch A only for interpretability)

### Critical path:
1. Data preparation: Binary labeling from 5-category expert annotations (mapping "Usable"→Good, others→Bad)
2. Point cloud sampling at 20K points (sufficient density for boundary features)
3. Feature engineering (14 geometric stats) for classical ML OR raw point processing for PointNet variants
4. Model training on real livers, inference on generated livers
5. Agreement evaluation via Cohen's κ against expert labels on generated set

### Design tradeoffs:
- Interpretability vs. Performance: Random Forest (90.9% test accuracy, κ=0.38, SHAP-interpretable) vs. PointNet++ (86.7% test accuracy, κ=1.00, black-box). Paper suggests hybrid: PointNet++ for screening, Random Forest+SHAP for explaining failures.
- Feature Complexity: 14 handcrafted features (fast, interpretable, may miss subtle issues) vs. learned features (computationally expensive, opaque, captures nuanced patterns).
- Dataset Split: 80/5/15 train/val/test—limited generated set (63 samples) constrains statistical power for κ estimation.

### Failure signatures:
- High test accuracy, low κ: Model learned spurious features from training distribution that don't generalize to generated shapes (e.g., KNN at 89.5% accuracy but κ=-0.02)
- Conservative bias: Model predicts nearly all "Good" with low variance (e.g., Logistic Regression: 98.41% predicted "Good")—fails to flag problematic cases
- Feature-specific overfitting: Random Forest flags acceptable shapes as "Bad" due to minor boundary irregularities (Table 3c)

### First 3 experiments:
1. **Baseline replication**: Train Random Forest and PointNet++ on the 939 labeled livers with 80/5/15 split; verify test accuracies (~90% RF, ~87% PointNet++) and measure κ on held-out generated set
2. **Feature ablation**: Remove min_z from the 14-feature vector and retrain; quantify accuracy drop and SHAP importance shift to validate the claimed mechanism
3. **Agreement analysis**: On the 63 generated livers, identify cases where RF/Extra Trees disagree with PointNet++; manually inspect with clinical annotator to characterize what each method prioritizes (boundary precision vs. overall plausibility)

## Open Questions the Paper Calls Out

### Open Question 1
Would multi-expert consensus labeling significantly change classifier performance compared to the single-expert ground truth used in this study? The authors state that relying on a "single expert" introduces subjectivity and list "multi-expert evaluation and inter-rater reliability analysis" as a requirement for future research. This remains unresolved because the current dataset relies entirely on annotations from one expert, making it impossible to distinguish between model errors and expert subjectivity. Evidence to resolve this would be a comparison of model accuracy and Cohen's Kappa when trained on labels derived from a consensus panel of three or more experts versus the current single-expert labels.

### Open Question 2
Can incorporating advanced morphological features (e.g., surface curvature, liver lobe segmentation) improve the detection of complex anatomical errors beyond the 14 basic geometric features used? The paper notes the current 14 geometric features "may miss complex anatomical details" and suggests "advanced (engineered) features" as a future direction. This remains unresolved because the current feature set is limited to basic statistical measures which may fail to capture subtle but clinically significant shape anomalies. Evidence to resolve this would be an ablation study showing increased F1 scores or detection of "false good" cases when topology-aware or curvature-based features are added to the Random Forest classifier.

### Open Question 3
Does a multi-class classification schema provide better clinical utility than the binary "Good/Bad" system without sacrificing model reliability? The authors acknowledge that the binary classification "oversimplifies the nuanced grading required in clinical practice" and omits intermediate categories like "Requires editing." This remains unresolved because the study collapsed five original clinical categories into two to simplify the task, potentially discarding actionable information for surgical planning. Evidence to resolve this would be training models on the original 5-class schema and evaluating if they maintain sufficient per-class accuracy to be clinically useful for filtering specific error types.

## Limitations
- Single-expert labeling introduces subjectivity that may affect model training and evaluation
- Binary classification oversimplifies clinical grading requirements and omits intermediate quality categories
- Limited generated set (63 samples) constrains statistical power for κ estimation and agreement analysis

## Confidence

**High confidence**: Random Forest and Extra Trees achieving >90% accuracy on real liver classification using 14 geometric features; the mechanical interpretation of min_z as reflecting inferior boundary segmentation challenges.

**Medium confidence**: PointNet++'s perfect κ agreement with experts indicating superior clinical relevance despite lower test accuracy; the generalizability of geometric features to other organ shapes or generation methods.

**Low confidence**: SHAP analysis definitively proving causal importance of min_z (correlation vs. causation uncertainty); the 63-sample generated set being sufficient for robust κ estimation.

## Next Checks

1. **Generated set expansion**: Apply trained classifiers to a larger set of generated livers (n>200) to verify κ stability and identify whether current results reflect sampling variance or true model properties.

2. **Cross-organ generalization**: Train identical models on another organ (e.g., kidneys) to test whether 14 geometric features and PointNet++ architecture generalize beyond liver-specific boundary challenges.

3. **Feature importance ablation**: Systematically remove top SHAP features (min_z, max_z, std_z) and retrain to quantify their contribution to accuracy vs. agreement metrics, distinguishing essential from incidental predictors.