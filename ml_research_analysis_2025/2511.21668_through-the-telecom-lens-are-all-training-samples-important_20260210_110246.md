---
ver: rpa2
title: 'Through the telecom lens: Are all training samples important?'
arxiv_id: '2511.21668'
source_url: https://arxiv.org/abs/2511.21668
tags:
- training
- data
- samples
- telecom
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a gradient-based sample importance framework
  for efficient telecom ML training, showing that selective training on high-impact
  samples reduces data needs by up to 37% without compromising accuracy. By analyzing
  per-sample gradient norms across epochs, the method dynamically prioritizes influential
  data, leading to faster convergence, lower computational overhead, and ~31% reduction
  in carbon emissions.
---

# Through the telecom lens: Are all training samples important?

## Quick Facts
- **arXiv ID:** 2511.21668
- **Source URL:** https://arxiv.org/abs/2511.21668
- **Reference count:** 21
- **Primary result:** Gradient-based sample importance reduces telecom ML data needs by up to 37% without accuracy loss, with 31% carbon emission reduction

## Executive Summary
This paper introduces a gradient-based framework for identifying and prioritizing high-impact training samples in telecom machine learning. The method computes per-sample gradient norms across training epochs to determine importance scores, then selectively trains only on the most influential data points. Applied to three real-world telecom datasets, the approach reduces data requirements by up to 37% while maintaining or improving predictive performance. The framework demonstrates substantial benefits in computational efficiency and environmental impact, with 31% reduction in carbon emissions during training.

## Method Summary
The proposed framework computes gradient norms for each training sample across all epochs during initial full training. For each sample, the importance score is calculated as the mean of its gradient norm magnitudes over all epochs. Training proceeds in two phases: first, a full model is trained while logging per-sample gradient statistics; second, a new model is initialized and trained only on the top p% most important samples. This selective training approach is validated on three datasets: Telecom Italia Internet activity forecasting, proprietary vendor network KPI data, and 5G beam selection tasks. Performance is measured using MAE for forecasting, RMSE for beam angles, training time, and carbon emissions via CodeCarbon.

## Key Results
- Up to 37% reduction in training data while maintaining or improving model accuracy
- 31% reduction in carbon emissions during training
- Sustained or improved predictive performance with selective training on high-impact samples
- Optimal data fraction varies by dataset (65-90%) but consistently improves efficiency

## Why This Works (Mechanism)
The framework leverages the insight that training samples with larger gradient magnitudes have greater influence on parameter updates and model learning. By computing and averaging gradient norms across all epochs, the method identifies samples that consistently contribute meaningful information to the model. These high-impact samples drive learning more effectively than low-gradient samples, which contribute minimal information. Selective training on these influential samples accelerates convergence and reduces computational overhead while preserving model quality.

## Foundational Learning
- **Per-sample gradient computation**: Needed to assess individual sample influence; Quick check: Verify gradients are computed per-sample, not accumulated per-batch
- **Gradient norm magnitude**: Indicates sample importance; Quick check: Ensure gradient norms are properly normalized and not dominated by loss scale
- **Temporal data splitting**: Critical for avoiding data leakage in time-series telecom data; Quick check: Confirm chronological split without shuffling
- **Model reinitialization**: Required for fair comparison between full and subset training; Quick check: Verify model weights are reset before subset training
- **Statistical validation**: Ensures results are robust across runs; Quick check: Confirm NRUNS=5 with bootstrap confidence intervals

## Architecture Onboarding

### Component Map
Input Data -> Gradient Computation Module -> Importance Scoring -> Sample Selection -> Model Training -> Performance Evaluation

### Critical Path
The most critical sequence is: Gradient Computation during full training -> Importance Score Calculation -> Model Reinitialization -> Selective Training on top samples -> Performance Validation

### Design Tradeoffs
The method trades initial computational overhead (full training for gradient logging) against substantial savings in subsequent training runs. This makes it most suitable for scenarios with repeated training cycles or where upfront cost is justified by long-term efficiency gains.

### Failure Signatures
- Gradient norms near zero or unstable: Indicates normalization issues or vanishing gradients
- Subset model underperformance: Suggests incorrect gradient computation or insufficient model reinitialization
- High variance across runs: Points to random seed issues or data leakage in temporal splits

### First Experiments
1. Train LSTM on Telecom Italia dataset while logging per-sample gradient norms, verify gradient statistics are sensible
2. Implement importance score computation and select top 50% samples, retrain and compare MAE to full model
3. Measure training time and carbon emissions for full vs. subset training, validate ~31% emission reduction claim

## Open Questions the Paper Calls Out

### Open Question 1
Can gradient-norm statistics be formally linked to generalization behavior, enabling theoretical guarantees on when sample selection preserves model performance?
The paper provides empirical validation but no theoretical framework explaining why gradient-based importance correlates with generalization. Formal analysis establishing conditions under which gradient norm correlates with out-of-sample performance, validated across diverse datasets, would resolve this.

### Open Question 2
How should optimal data fraction selection be operationalized under specific latency, cost, or energy constraints in production telecom systems?
The paper shows optimal fractions vary by dataset (65-90%) but provides no principled method for determining this a priori. Decision frameworks or algorithms that map constraint specifications to recommended sample fractions with quantified performance bounds would resolve this.

### Open Question 3
Does the gradient-based importance framework generalize across diverse model architectures (transformers, GNNs) and telecom tasks beyond time-series forecasting?
Experiments use only LSTM for two datasets and a specific architecture for beam selection. Systematic evaluation across transformer-based models, graph neural networks, and tasks such as anomaly detection, handover optimization, and resource allocation would help assess generality.

### Open Question 4
Can sample importance be estimated online during a single training run, avoiding the overhead of computing full gradient histories?
The current method requires storing gradient norms across all epochs before selection, implying retrospective analysis. Online variants achieving comparable sample selection quality with reduced memory and computation, validated on streaming telecom data, would resolve this.

## Limitations
- Proprietary vendor dataset limits reproducibility and broader validation
- Gradient-based importance requires initial full training pass, creating upfront computational overhead
- Method validated primarily on time-series forecasting and beam selection, limiting generalizability claims

## Confidence
High: Reproducibility is well-specified with clear metrics and validation procedures
Medium: Proprietary data limits complete verification of all claims
Low: Generalization to other architectures and tasks remains unproven

## Next Checks
1. Verify gradient computation is truly per-sample by checking batch vs. sample gradient calculations
2. Confirm model reinitialization between full and subset training runs to ensure fair comparison
3. Validate CodeCarbon integration and measurement methodology for carbon emission claims