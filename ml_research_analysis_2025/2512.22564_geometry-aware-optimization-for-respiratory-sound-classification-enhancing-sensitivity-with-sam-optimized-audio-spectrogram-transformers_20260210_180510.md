---
ver: rpa2
title: 'Geometry-Aware Optimization for Respiratory Sound Classification: Enhancing
  Sensitivity with SAM-Optimized Audio Spectrogram Transformers'
arxiv_id: '2512.22564'
source_url: https://arxiv.org/abs/2512.22564
tags:
- respiratory
- sound
- classification
- sensitivity
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study tackles the challenge of classifying respiratory sounds
  in noisy, imbalanced datasets, specifically the ICBHI 2017 dataset. The core method
  involves enhancing the Audio Spectrogram Transformer (AST) with Sharpness-Aware
  Minimization (SAM), an optimization technique that guides the model toward flatter
  minima in the loss landscape for improved generalization.
---

# Geometry-Aware Optimization for Respiratory Sound Classification: Enhancing Sensitivity with SAM-Optimized Audio Spectrogram Transformers

## Quick Facts
- arXiv ID: 2512.22564
- Source URL: https://arxiv.org/abs/2512.22564
- Reference count: 23
- Primary result: State-of-the-art Score of 68.10% with Sensitivity of 68.31% on ICBHI 2017 dataset

## Executive Summary
This study addresses the challenge of classifying respiratory sounds in imbalanced, noisy datasets by enhancing Audio Spectrogram Transformers (AST) with Sharpness-Aware Minimization (SAM) optimization. The approach combines cyclic padding for signal preservation, weighted sampling to handle class imbalance, and SAM to guide the model toward flatter minima in the loss landscape. The method achieves a state-of-the-art Score of 68.10% with particularly high Sensitivity (68.31%), significantly reducing false negatives compared to previous CNN and hybrid baselines. This makes the approach highly suitable for clinical screening applications where minimizing missed diagnoses is critical.

## Method Summary
The method involves training an Audio Spectrogram Transformer (AST) with AudioSet pre-trained weights on the ICBHI 2017 dataset. Audio is resampled to 16 kHz, cyclic padding is applied to ensure all clips are 8 seconds long, and log-Mel spectrograms are generated. The AST model uses patch splitting (16×16) and a classification head for binary output (Normal/Abnormal). Training employs Sharpness-Aware Minimization (SAM) with ρ=0.05, AdamW optimizer (lr=1×10⁻⁵, weight_decay=1×10⁻⁴), and Weighted Random Sampling to address class imbalance. The model is trained for 20 epochs with batch size 8 on an NVIDIA Tesla L4 GPU using PyTorch.

## Key Results
- Achieved state-of-the-art Score of 68.10% on ICBHI 2017 dataset
- Significantly improved Sensitivity to 68.31% (outperforming previous baselines of 63-65%)
- Maintained Specificity around 68% while dramatically improving Sensitivity
- Demonstrated superior performance over CNN and hybrid baselines in clinical screening context

## Why This Works (Mechanism)

### Mechanism 1: SAM for Flatter Minima
Sharpness-Aware Minimization guides the model toward flatter minima in the loss landscape by reformulating optimization as a min-max problem. During each training step, SAM computes a perturbation vector in the direction of steepest loss ascent, then updates weights using the gradient at this perturbed position. This explicitly penalizes sharp loss basins where small input perturbations cause large loss spikes, leading to better generalization on noisy medical datasets. The core assumption is that flat minima generalize better than sharp minima—solutions robust to weight perturbations will also be robust to input variations like different stethoscope pressures or background noise.

### Mechanism 2: Cyclic Padding for Signal Preservation
Cyclic padding preserves pathological signal content better than zero-padding for variable-length respiratory recordings. Instead of padding short recordings with silence (which dilutes spectral features), the signal is repeated cyclically until reaching the target duration of 8 seconds. This ensures the input is always signal-dense, maximizing the extraction of pathological patterns like crackles and wheezes without being misled by silence artifacts. The approach assumes that repeating actual respiratory content preserves diagnostic cues better than introducing artificial silence.

### Mechanism 3: Weighted Random Sampling for Imbalance Handling
Weighted Random Sampling combined with SAM enables high sensitivity without sacrificing overall score on severely imbalanced data. WRS over-samples minority classes (Crackle, Wheeze, Both) during training so each batch has more balanced class representation. SAM then stabilizes the resulting higher-variance gradients from the imbalanced sampling. The core assumption is that the performance bottleneck is insufficient exposure to minority class examples during training, not the inherent difficulty of those classes.

## Foundational Learning

### Concept: Loss Landscape Geometry (Sharp vs. Flat Minima)
Why needed: The central hypothesis depends on understanding why flat minima generalize better. Without this, SAM appears to be just another optimizer. Quick check: If you perturb model weights slightly at a sharp minimum, would loss change dramatically or minimally? (Answer: dramatically—that's why sharp minima generalize poorly.)

### Concept: Self-Attention in Vision Transformers
Why needed: AST treats spectrograms as image patches and applies self-attention globally. Understanding how attention weights capture long-range dependencies is essential for interpreting attention maps. Quick check: How does AST differ from a CNN in terms of receptive field? (Answer: AST has global receptive field from layer 1; CNNs build it progressively.)

### Concept: Transfer Learning from Large-Scale Audio Pretraining
Why needed: The model initializes from AudioSet weights (2M+ clips). Understanding what acoustic primitives transfer (pitch, rhythm, spectral textures) explains why fine-tuning works on 920 ICBHI samples. Quick check: Why might AudioSet pretraining help detect crackles if no crackles exist in AudioSet? (Answer: Low-level spectral features and temporal patterns transfer even when specific sound classes don't match.)

## Architecture Onboarding

### Component map:
Raw Audio (16kHz) → Cyclic Padding (8s) → Log-Mel Spectrogram → Patch Splitting (16×16) → AST Encoder (ViT-style, AudioSet pretrained) → Classification Head → Binary Output (Normal/Abnormal)

### Critical path:
1. SAM hyperparameter ρ (perturbation radius) is sensitive—paper uses ρ=0.05 empirically. Wrong value can under-regularize or over-smooth.
2. Weighted sampler must align with class distribution; incorrect weights can destabilize SAM's gradient estimates.
3. Transfer learning checkpoint must match AST architecture exactly (AudioSet pretrained weights from Gong et al.).

### Design tradeoffs:
- **High Sensitivity / Moderate Specificity**: Model is tuned for clinical screening (minimize false negatives). Accepts ~32% false positive rate as acceptable trade-off.
- **Signal-Preserving vs. Denoising**: Paper deliberately avoids aggressive preprocessing, arguing that SAM robustness handles noise better than hand-crafted filtering.
- **Binary vs. 4-Class Output**: ICBHI protocol evaluates binary (Normal/Abnormal); the 4-class confusion is shown but not optimized for.

### Failure signatures:
- Model achieves high Specificity but poor Sensitivity (~40%) → WRS not working or SAM ρ too small
- Training loss oscillates wildly → SAM step size or learning rate mismatch; try reducing lr or increasing ρ gradually
- Attention maps focus on silence regions → Cyclic padding may be repeating artifacts; inspect padded samples
- Good training accuracy, poor validation → Sharp minima; increase SAM ρ or add explicit regularization

### First 3 experiments:
1. **Reproduce baseline**: Train AST with AdamW only (no SAM, no WRS). Confirm high Specificity (~80%) and low Sensitivity (~40%) as paper reports.
2. **Ablation check**: Add WRS alone, then SAM alone. Verify that WRS boosts Sensitivity but increases variance, and that SAM stabilizes training.
3. **SAM sensitivity sweep**: Test ρ ∈ {0.01, 0.03, 0.05, 0.1} and measure Score/Sensitivity trade-off. Identify the point where Sensitivity plateaus or Specificity degrades sharply.

## Open Questions the Paper Calls Out
None

## Limitations
- Results demonstrated only on ICBHI 2017 dataset with specific characteristics (short clips, variable quality, patient overlap)
- SAM hyperparameter sensitivity not systematically analyzed; ρ=0.05 used without exploring robustness window
- Contribution of cyclic padding not separately quantified; benefit over standard zero-padding remains unproven in isolation

## Confidence
- **High Confidence**: SAM improves Sensitivity when added to AST + WRS (supported by ablation: 63% → 68.31%)
- **Medium Confidence**: The mechanism that flat minima generalize better is plausible but not proven for respiratory audio specifically
- **Low Confidence**: Claims about cyclic padding's superiority over zero-padding lack corpus validation

## Next Checks
1. **Hyperparameter robustness sweep**: Systematically vary SAM's ρ ∈ {0.01, 0.03, 0.05, 0.1} and report Sensitivity/Score trade-offs
2. **Padding strategy ablation**: Replace cyclic padding with zero-padding and standard short-padding to isolate cyclic padding's contribution
3. **Dataset generalization test**: Apply the exact AST+SAM+WRS pipeline to a different respiratory sound dataset (e.g., ICBHI 2022 or RSDD) to verify improvements transfer beyond ICBHI 2017