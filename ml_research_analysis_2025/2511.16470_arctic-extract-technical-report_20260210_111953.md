---
ver: rpa2
title: Arctic-Extract Technical Report
arxiv_id: '2511.16470'
source_url: https://arxiv.org/abs/2511.16470
tags:
- table
- dataset
- document
- arctic-extract
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Arctic-Extract is a state-of-the-art, resource-efficient model
  for extracting structured data from business documents, including question answering,
  entity extraction, and table parsing. It leverages the Qwen2.5-VL architecture with
  token compression, enabling up to 125 A4 pages to be processed on a single 24GB
  GPU.
---

# Arctic-Extract Technical Report

## Quick Facts
- arXiv ID: 2511.16470
- Source URL: https://arxiv.org/abs/2511.16470
- Reference count: 40
- Achieves 78.94 ANLS* on SQuAD2.0, outperforming or matching much larger models while processing up to 125 A4 pages on a single 24GB GPU

## Executive Summary
Arctic-Extract is a state-of-the-art, resource-efficient model for extracting structured data from business documents, including question answering, entity extraction, and table parsing. It leverages the Qwen2.5-VL architecture with token compression, enabling up to 125 A4 pages to be processed on a single 24GB GPU. The model was fine-tuned using LoRA on 372K data points across 35 datasets, including proprietary and public sources, and quantized to 4-bit precision for deployment efficiency. Arctic-Extract achieves strong performance, scoring 64.2 on internal vision tasks, 70.7 on multilingual benchmarks, and 78.94 ANLS* on SQuAD2.0, outperforming or matching much larger models. It supports 29 languages and processes both scanned and digital-born documents with high accuracy, making it suitable for enterprise-grade document understanding at low cost and resource usage.

## Method Summary
Arctic-Extract fine-tunes the Qwen2.5-VL model using LoRA (rank=32) on 372,544 datapoints spanning 35 datasets including QA, table extraction, and multilingual tasks. The model employs 4-to-1 vision token merging for long-document processing, reducing a standard A4 page to approximately 1000 tokens. After LoRA merge, weights are quantized to 4-bit AWQ precision (group size 128) to fit within 6.6 GiB, enabling deployment on a single 24GB GPU. The training used 8x NVIDIA H200s with bf16 precision, ModelScope Swift Trainer, and a cosine learning rate scheduler. The model supports 29 languages and processes up to 125 pages in a single context window.

## Key Results
- Achieves 78.94 ANLS* on SQuAD2.0, outperforming or matching much larger models
- Scores 64.2 on internal visual tasks and 70.7 on multilingual benchmarks
- Processes up to 125 A4 pages on a single 24GB GPU with 6.6 GiB memory footprint

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Token compression via 4-to-1 vision token merging enables long-document processing under memory constraints.
- Mechanism: Qwen2.5-VL architecture merges every 4 visual tokens into a single vision token. This compresses a standard A4 page to approximately 1000 tokens, allowing a 128,000-token context window to hold up to 125 pages.
- Core assumption: Token merging preserves sufficient spatial and semantic fidelity for extraction tasks.
- Evidence anchors: [Section 3]: "unique property that is the merging of the 4 tokens into one vision token. This allows standard A4 page to be compressed to around 1000 tokens" and "context window of the model is 128,000 tokens, and is able to process 125 pages at once"
- Break condition: High-density tables or fine-grained spatial tasks (e.g., small checkboxes in dense forms) may degrade if critical visual cues fall below compression granularity.

### Mechanism 2
- Claim: LoRA fine-tuning (rank=32) matched or exceeded full-parameter fine-tuning for document extraction while reducing training complexity on long contexts.
- Mechanism: Low-rank adaptation was applied to all linear layers matching a specific regex (excluding embeddings, output heads, and classifier layers). The 372K datapoint training corpus covered QA (112,901), table extraction (33,916), and multilingual tasks (225,735).
- Core assumption: The proprietary/internal datasets are representative of real enterprise document distributions.
- Evidence anchors: [Section 3.2]: "LoRA demonstrated superior performance compared to full-parameter fine-tuning in our use case" and [Section 3.3]: "372,544 datapoints... spanning across 35 datasets"
- Break condition: Tasks requiring fundamentally new visual-language grounding not present in the fine-tuning mix (e.g., new scripts, domain-specific symbols) may not transfer well.

### Mechanism 3
- Claim: 4-bit AWQ quantization reduces deployment memory to 6.6 GiB without significant performance degradation.
- Mechanism: After LoRA merge, weights are quantized using Activation-Aware Weight Quantization with group size 128 and GEMM kernel. The quantized model loads via vLLM on a 24GB GPU, leaving headroom for KV cache across 125 pages.
- Core assumption: Activation statistics used for quantization calibration are representative of inference-time document distributions.
- Evidence anchors: [Section 3.2]: "utilizing a group size of 128 and the GEMM quantization type. The final model... occupies only 6.6 GiB" and "process up to 125 A4 pages... on a single A10 GPU with 24 GB"
- Break condition: Edge cases with out-of-distribution visual patterns may suffer more under quantization.

## Foundational Learning

- **Vision Tokenization and Compression**
  - Why needed here: Arctic-Extract's core efficiency claim rests on understanding how vision tokens are produced, merged, and mapped to spatial regions.
  - Quick check question: Given a 224×224 image patch, how many vision tokens does ViT produce, and what is the effective compression ratio after 4-to-1 merging?

- **LoRA Mechanics**
  - Why needed here: The model is trained via LoRA on linear layers; understanding rank, alpha scaling, and merge behavior is essential for debugging or extending training.
  - Quick check question: If LoRA rank is 32 and alpha is 32, what is the effective scaling applied to LoRA outputs at inference after merge?

- **KV Cache Memory for Long Context**
  - Why needed here: The 6.6 GiB weight budget is only part of the story; 125 pages of KV cache must fit in the remaining GPU memory.
  - Quick check question: For 128,000 tokens at bf16, estimate the KV cache memory required for a model with 32 layers and 4096 hidden dimension (2 key/value heads per layer).

## Architecture Onboarding

- **Component map:**
  - Vision Encoder -> Naive ViT-style encoder with 4-to-1 token merging
  - Vision-Language Adapter -> Projects compressed vision tokens to LLM embedding space
  - LLM Backbone -> Qwen2.5-VL decoder (128K context)
  - LoRA Layers -> Rank-32 adapters on all linear layers (merged post-training)
  - Quantization Wrapper -> AWQ 4-bit with group size 128 (GEMM)

- **Critical path:**
  1. Document image → resize and patch embedding
  2. Vision token merging (4→1)
  3. Vision token projection to LLM space
  4. Concatenate with text tokens (question/prompt)
  5. Autoregressive decoding → extraction output

- **Design tradeoffs:**
  - Compression vs. fine-grained accuracy: Token merging enables length but risks small-detail loss (acknowledged in Appendix D for checkboxes, merged cells).
  - LoRA vs. full fine-tuning: Faster, lower-memory training but potentially less adaptation on unseen document layouts.
  - 4-bit vs. higher precision: Fits 24GB GPU but may increase hallucination on edge-case documents (no explicit ablation provided).

- **Failure signatures:**
  - High error rates on tables with hierarchical headers or transposed layouts (Appendix D): model must flatten or transpose correctly; failures manifest as missing columns or misaligned rows.
  - Long-document processing failures for cloud APIs (Table 12): 100% error rate on SEC-based dataset for GPT5/Claude—Arctic-Extract avoids this by design but may still fail if page count exceeds 125 or resolution is extremely high.
  - Multilingual degradation: Table 13 shows lower scores on Arabic/Hindi/Vietnamese; failure signature is partial or mistranslated entity extraction.

- **First 3 experiments:**
  1. **Token compression ablation:** Re-run inference on the internal visual datasets (Table 8) with raw (unmerged) vision tokens on a high-memory GPU. Compare ANLS* to merged-token baseline to quantify compression loss.
  2. **LoRA rank sweep:** Train with rank ∈ {8, 16, 32, 64} on a held-out 50K subset. Evaluate on table extraction (Business Tables) and QA (DocVQA) to verify the rank=32 claim.
  3. **Quantization sensitivity:** Run 4-bit, 8-bit, and bf16 inference on the Hard Business Cases dataset (which contains poor-quality scans). Measure ANLS* delta and per-example failure modes.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the removal of the 20-50% of "unprocessable" data points (due to competitor model size limits) skew the relative performance assessment between Arctic-Extract and large proprietary models like GPT-5 or Claude?
- Basis in paper: [inferred] The paper excludes approximately 20% of visual data points and 50% of table extraction points to ensure "fair comparison," while noting that failures in cloud models were often due to the Cortex AI platform limitations rather than inherent model capability.
- Why unresolved: The paper compares Arctic-Extract against baselines only on the subset of data the baselines could handle, potentially inflating Arctic-Extract's relative standing by removing "hard" cases that only Arctic-Extract could process.
- What evidence would resolve it: A comparative evaluation on the full dataset where competitor timeouts/errors are scored as "partial" or analyzed qualitatively, or re-running baselines on infrastructure without the file size constraints of the Cortex AI platform.

### Open Question 2
- Question: To what extent does the 4-bit AWQ quantization degrade the model's fine-grained visual recognition capabilities compared to the full-precision (bf16) merged model?
- Basis in paper: [inferred] The paper mentions the application of 4-bit quantization to meet memory constraints but does not provide an ablation study comparing the quantized model's accuracy against the unquantized version on the reported benchmarks.
- Why unresolved: While the quantized model achieves high scores, the specific loss in precision for tasks requiring fine detail (e.g., checkbox detection, signature verification) caused by the reduction from 16-bit to 4-bit precision is not quantified.
- What evidence would resolve it: Benchmark results (ANLS* and Exact Match) for the merged LoRA model (bf16) vs. the final AWQ 4-bit model on the internal visual datasets.

### Open Question 3
- Question: Can the specific "Table Extraction" methodology (transforming unstructured data to structured formats) generalize to standard public benchmarks for table structure recognition?
- Basis in paper: [inferred] The authors define a specific "Table Extraction" problem distinct from standard Table Recognition and note that "no publicly available datasets currently exist" for this specific task, relying entirely on proprietary data for training and evaluation.
- Why unresolved: The model's performance is validated on an internal, purpose-built dataset (Business Tables). It is unclear if the model has overfit to the specific schema or layout conventions of this proprietary data or if the capability transfers to public datasets like PubTabNet or FinTabNet.
- What evidence would resolve it: Evaluation of Arctic-Extract on established public table recognition benchmarks using standard metrics (e.g., TEDS).

## Limitations

- Dataset opacity: The bulk of the 372K training samples are proprietary and undisclosed, preventing independent verification of model generalization and potential bias.
- Compression fidelity: No ablation study quantifies the information loss for high-density tables or fine-grained spatial tasks, potentially degrading performance on layouts with small checkboxes or merged cells.
- Quantization sensitivity: The 4-bit AWQ deployment claim lacks edge-case testing and per-task quantization error analysis, leaving open the possibility of increased hallucination on out-of-distribution visual patterns.

## Confidence

- **High confidence**: Core technical claims about LoRA fine-tuning (rank=32), token compression (4-to-1 merging), and 4-bit AWQ quantization are internally consistent and grounded in established techniques. The 6.6 GiB model size and 125-page context window are verifiable through the stated architecture and quantization scheme.
- **Medium confidence**: Performance metrics (ANLS* scores, multilingual benchmarks) are reported but rely on proprietary datasets and internal benchmarks. The "outperforms GPT-4o, Claude-3.5-Sonnet" claim is based on internal comparisons, not independent evaluation.
- **Low confidence**: Generalization claims for enterprise-grade document understanding are not independently validated. The 29-language support is plausible but lacks detailed per-language performance breakdowns.

## Next Checks

1. **Token compression ablation**: Re-run inference on the internal visual datasets (Table 8) with raw (unmerged) vision tokens on a high-memory GPU. Compare ANLS* to merged-token baseline to quantify compression loss.
2. **LoRA rank sweep**: Train with rank ∈ {8, 16, 32, 64} on a held-out 50K subset. Evaluate on table extraction (Business Tables) and QA (DocVQA) to verify the rank=32 claim.
3. **Quantization sensitivity**: Run 4-bit, 8-bit, and bf16 inference on the Hard Business Cases dataset (which contains poor-quality scans). Measure ANLS* delta and per-example failure modes.