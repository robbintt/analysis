---
ver: rpa2
title: 'PSRB: A Comprehensive Benchmark for Evaluating Persian ASR Systems'
arxiv_id: '2505.21230'
source_url: https://arxiv.org/abs/2505.21230
tags:
- speech
- persian
- sw-wer
- performance
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PSRB, a comprehensive benchmark for evaluating
  Persian ASR systems across diverse linguistic and acoustic conditions. The benchmark
  includes ten ASR models, including state-of-the-art commercial and open-source models,
  to examine performance variations and inherent biases.
---

# PSRB: A Comprehensive Benchmark for Evaluating Persian ASR Systems

## Quick Facts
- arXiv ID: 2505.21230
- Source URL: https://arxiv.org/abs/2505.21230
- Reference count: 37
- Primary result: Introduced PSRB benchmark with 10 ASR models showing performance gaps across linguistic/acoustic conditions

## Executive Summary
This paper introduces PSRB, a comprehensive benchmark for evaluating Persian ASR systems across diverse linguistic and acoustic conditions. The benchmark includes ten ASR models, including state-of-the-art commercial and open-source models, to examine performance variations and inherent biases. A novel metric called Substitution Weighted WER (SW-WER) is proposed to enhance evaluation robustness by reducing the impact of minor and partial errors. The benchmark results show that while ASR models generally perform well on standard Persian, they struggle with regional accents, children's speech, and specific linguistic challenges.

## Method Summary
The PSRB benchmark evaluates 10 pre-trained Persian ASR models (including commercial and open-source) using a dataset of 3,372 utterances (10.4h, 16kHz audio) with 8-category metadata annotations. Models are evaluated using CER, WER, and a novel SW-WER metric that weights substitution errors by character-level similarity. The benchmark computes metrics per utterance and aggregates by metadata categories to identify performance degradation sources. A public subset is available on HuggingFace for reproduction.

## Key Results
- Avanegar achieved best performance: CER 8.75%, WER 19.3%
- Open-source Faster-Whisper: CER 13.72%, WER 33.93%
- Children's speech shows highest error rates; adult speech lowest
- Mashhadi/Yazdi accents show 2-3× higher SW-WER than Standard Persian

## Why This Works (Mechanism)

### Mechanism 1: Substitution Weighted WER (SW-WER) for Partial Error Credit
SW-WER computes weighted substitution penalties based on character-level similarity between reference and hypothesis segments, normalizing by total words. This provides more discriminative evaluation than binary WER penalties by giving partial credit for minor typos.

### Mechanism 2: Stratified Benchmark Design for Bias Attribution
PSRB enforces minimum 50 utterances per category across 8 dimensions (age, accent, formality, noise, spontaneity). Disaggregated SW-WER computation per stratum reveals whether errors concentrate in specific subpopulations, attributing performance gaps to data imbalance rather than architectural limitations.

### Mechanism 3: CTC Decoder Selection Reduces Hallucination Risk
CTC decoders with frame-level independence constrain outputs closer to acoustic evidence, while transducer decoders with predictor networks risk generating text detached from audio. FC-Fa CTC achieved 19.42% CER vs Transducer's 40.73% CER.

## Foundational Learning

- **Levenshtein Distance & Edit Operations (S/I/D)**: Understanding base alignment is prerequisite for SW-WER which extends standard alignment by reweighting substitutions. Quick check: Given "hello world" vs "helo words", what are S, I, D counts?

- **Self-Supervised Speech Representations (wav2vec 2.0, Whisper)**: Explains why multilingual models underperform on Persian despite strong English results—limited fine-tuning data. Quick check: Why doesn't self-supervised pre-training guarantee equal performance across all languages?

- **CTC vs Transducer Decoding**: Decoder choice directly impacts hallucination propensity; understanding frame-level vs sequence-level prediction clarifies tradeoffs. Quick check: Which decoder would you expect to be more robust to silence/nonspeech segments, and why?

## Architecture Onboarding

- **Component map**: Audio clips (1-100s, 16kHz) → ASR model → transcript → alignment with reference → metrics (CER, WER, SW-WER) → disaggregated performance tables

- **Critical path**: 1) Verify audio format/sample rate consistency, 2) Run inference on 10 models, 3) Compute SW-WER using equations 1-5, 4) Slice results by metadata categories to identify bias patterns

- **Design tradeoffs**: CER ignores word structure; WER over-penalizes minor typos; SW-WER adds computation but better reflects partial correctness. Benchmark duration (10.4h) vs annotation cost. Open-source vs commercial models tradeoff.

- **Failure signatures**: Large WER-SW-WER gap → many partial-character substitutions (common in Faster-Whisper). High multi-speaker/single-speaker error ratio → model not adapted for overlapping speech. Transducer >> CTC error rate → likely hallucination issue.

- **First 3 experiments**: 1) Reproduce Avanegar vs Faster-Whisper SW-WER gap on 100-utterance subset, 2) Ablate formality vs noise to test error correlation, 3) Run FC-Fa with both decoders on 50 samples to confirm hallucination differential

## Open Questions the Paper Calls Out

- How do fine-tuning and combination of diverse training datasets impact Persian ASR robustness against domain mismatch? The paper evaluates existing models but doesn't verify if semantic diversity improves generalization.

- To what extent can speaker normalization or targeted data augmentation mitigate performance disparity for children's speech? Current models are trained predominantly on adult speech with insufficient balanced training data for children.

- Which architectural modifications or post-processing techniques most effectively reduce hallucination errors in transducer-based Persian ASR decoders? The paper identifies semantic modeling in transducers as hallucination cause but doesn't implement suggested solutions.

## Limitations
- Benchmark scope limited to 10.4 hours across 8 categories, potentially limiting statistical power for detecting subtle performance differences
- Metric generalizability concerns as SW-WER assumes character-level similarity correlates with semantic preservation
- Commercial model opacity limits understanding of whether performance advantages stem from architecture or data quality

## Confidence

**High Confidence**:
- Persian ASR models perform significantly worse on children's speech vs adult speech
- Regional accent variation substantially impacts ASR accuracy
- CTC decoders show greater robustness than transducer decoders

**Medium Confidence**:
- SW-WER provides more nuanced evaluation than standard WER
- Benchmark design effectively isolates performance degradation sources
- Open-source models lag commercial offerings by 2-3× in accuracy

**Low Confidence**:
- Generalization of decoder hallucination findings beyond FC-Fa model
- Long-term stability of model rankings as ASR technology evolves
- Direct correlation between SW-WER improvements and downstream task performance

## Next Checks

1. **Cross-linguistic SW-WER validation**: Apply SW-WER metric to English ASR evaluation on dataset with known partial error patterns; compare whether SW-WER better predicts human acceptability ratings than standard WER.

2. **Decade-spanning benchmark comparison**: Replicate PSRB methodology on Persian ASR models from 2018-2024 to quantify improvement trajectories and identify persistent failure modes.

3. **Data augmentation impact study**: Using PSRB accent categories, systematically evaluate whether accent-specific data augmentation reduces the observed 2-3× performance gap between Standard Persian and regional accents across multiple model architectures.