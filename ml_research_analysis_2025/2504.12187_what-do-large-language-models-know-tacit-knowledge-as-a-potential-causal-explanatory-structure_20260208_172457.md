---
ver: rpa2
title: What Do Large Language Models Know? Tacit Knowledge as a Potential Causal-Explanatory
  Structure
arxiv_id: '2504.12187'
source_url: https://arxiv.org/abs/2504.12187
tags:
- knowledge
- causal
- tacit
- llms
- network
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes tacit knowledge as a conceptual framework for
  understanding semantic knowledge in large language models (LLMs). While LLMs are
  trained on next-word prediction, they exhibit impressive performance in generating
  coherent texts, leading to questions about what they actually know.
---

# What Do Large Language Models Know? Tacit Knowledge as a Potential Causal-Explanatory Structure

## Quick Facts
- arXiv ID: 2504.12187
- Source URL: https://arxiv.org/abs/2504.12187
- Reference count: 0
- One-line primary result: Transformer-based LLMs may possess tacit knowledge through causal common factors in MLP layers, satisfying a weakened version of Davies' syntactic structure criterion.

## Executive Summary
This paper proposes tacit knowledge as a conceptual framework for understanding semantic knowledge in large language models (LLMs). While LLMs are trained on next-word prediction, they exhibit impressive performance in generating coherent texts, leading to questions about what they actually know. The paper argues that transformer-based LLMs meet Martin Davies' criteria for tacit knowledge, particularly when a weaker version of the syntactic structure constraint is applied. Unlike earlier connectionist networks, LLMs use embedding layers that map semantically similar inputs to proximate regions in vector space, enabling them to recognize and process semantically similar inputs via shared causal mechanisms. Recent work by Meng et al. (2022) provides preliminary evidence that LLMs acquire representations of factual associations that function as causal common factors, supporting the claim that LLMs have tacit knowledge.

## Method Summary
The paper proposes a conceptual framework for attributing tacit knowledge to LLMs based on Davies' (1990) criteria. The approach involves: (1) causal tracing to identify MLP layers storing factual associations, (2) rank-one model editing (ROME) to update key-value pairs representing facts, and (3) intervention-based verification measuring generalization (edit propagates to paraphrases) and specificity (edit doesn't affect unrelated facts). The framework distinguishes between behavioral conformity and causal systematicity, arguing that LLMs possess the latter through their embedding and MLP layer architectures.

## Key Results
- Transformer-based LLMs satisfy a "weaker" constraint of syntactic structure by mapping semantically similar inputs to proximate regions in vector space via embedding layers
- Factual associations in LLMs are stored in MLP modules, which function as "causal common factors" mediating transitions for specific semantic types
- Interventionist approaches (e.g., ROME) can verify and modify tacit knowledge, with successful edits showing high generalization and specificity

## Why This Works (Mechanism)

### Mechanism 1: Semantic Proximity via Embeddings
- **Claim:** Transformer-based LLMs satisfy a "weaker" constraint of syntactic structure by mapping semantically similar inputs to proximate regions in vector space, rather than requiring identical symbolic tokens.
- **Mechanism:** The embedding layer functions as a semantic categorizer. It converts input tokens into high-dimensional vectors where distance correlates with semantic similarity (distributional hypothesis). This clustering allows the model to engage shared downstream processing pathways for distinct but related inputs (e.g., "The capital of France is ___" and "Paris is located in ___").
- **Core assumption:** Proximity in embedding space reliably triggers similar causal processing pathways in subsequent transformer blocks.
- **Evidence anchors:**
  - [Section 4] argues that while LLMs lack identical syntactic properties, embeddings ensure semantically similar inputs are processed via shared mechanisms.
  - [Abstract] highlights that embedding layers map semantically similar inputs to proximate regions, enabling recognition via shared causal mechanisms.
  - [Corpus] Corpus evidence is indirect; related work discusses parametric knowledge boundaries but does not explicitly validate the philosophical "syntactic structure" constraint.
- **Break condition:** If inputs are semantically related but occupy distant vector regions (e.g., antonyms or domain-specific jargon sharing context with common terms), the shared causal mechanism may fail to engage.

### Mechanism 2: MLP Layers as Causal Common Factors
- **Claim:** Factual associations in LLMs are stored in MLP (Multi-Layer Perceptron) modules, which function as "causal common factors" mediating transitions for specific semantic types.
- **Mechanism:** MLP layers act as key-value memories. When processing a subject (e.g., "The Eiffel Tower"), the MLP recalls properties (values) associated with that subject (key). This internal representation acts as a single causal node that influences the output for all semantically related prompts.
- **Core assumption:** Facts are localized sufficiently enough in specific MLP layers to function as discrete causal variables rather than being fully holographically distributed.
- **Evidence anchors:**
  - [Section 5.1] cites Meng et al. (2022), suggesting MLP modules function as key-value pairs storing factual associations.
  - [Section 3.3.2] defines causal systematicity as the presence of a component mechanism serving as a "causal common factor."
  - [Corpus] Corpus papers support the existence of parametric knowledge but do not specifically confirm the MLP-centric "causal common factor" architecture proposed here.
- **Break condition:** If the model relies on distributed superposition (polysemanticity) where single neurons encode multiple unrelated concepts, intervening on a "factor" may cause unintended side effects (crosstalk).

### Mechanism 3: Interventionist Generalization (ROME)
- **Claim:** Tacit knowledge can be verified and modified through interventions (e.g., Rank-One Model Editing) that target specific factual associations, resulting in generalization across paraphrases without affecting unrelated facts.
- **Mechanism:** By calculating a rank-one update to the weights of a specific MLP module, researchers can overwrite a factual association (e.g., changing "Paris" to "Berlin" for the Eiffel Tower). If the model possesses tacit knowledge (vs. rote memorization), this edit should systematically alter outputs for all inputs referring to that fact (generalization) while leaving other subjects untouched (specificity).
- **Core assumption:** The model's internal causal structure mirrors the semantic structure of the data sufficiently that localized weight changes propagate logically through related contexts.
- **Evidence anchors:**
  - [Section 5.2] notes that successful edits show high generalization (affecting paraphrases) and specificity (sparing unrelated facts), behaving like a causally systematic system.
  - [Abstract] states that preliminary evidence supports the claim that LLMs acquire representations functioning as causal common factors.
  - [Corpus] Corpus papers discuss knowledge boundaries and hallucinations but do not provide empirical validation of the ROME intervention mechanism.
- **Break condition:** If the model has merely memorized surface statistics (overfitting), interventions will fail to generalize to paraphrased inputs.

## Foundational Learning

- **Concept: Distributed Representations & Embeddings**
  - **Why needed here:** The paper's defense of LLMs having "tacit knowledge" hinges on rejecting the requirement for identical symbolic rules (classical AI) and replacing it with vector space proximity. You must understand how words map to vectors to grasp the "weaker syntactic structure" argument.
  - **Quick check question:** Can you explain why two different sentences with identical meanings might result in different vector representations, yet still be processed similarly by an LLM?

- **Concept: Causal Systematicity vs. Behavioral Conformity**
  - **Why needed here:** The paper distinguishes between a system merely *acting* correctly (conformity) and *using an internal rule* to do so (systematicity). Understanding this distinction is critical for evaluating the "black box" problem.
  - **Quick check question:** If a model correctly answers "Paris" for "Capital of France" but fails when the question is rephrased, does it have "tacit knowledge" according to Davies' criteria?

- **Concept: Interventionism (Causal Tracing)**
  - **Why needed here:** The paper relies on intervention studies (Meng et al.) to prove the existence of internal causal structures. You need to understand that we prove causality by *changing* internal variables and observing systematic effects in the output.
  - **Quick check question:** Why is "restoring" a corrupted activation in a hidden layer a stronger proof of knowledge location than simply observing high activations?

## Architecture Onboarding

- **Component map:**
  Input/Embedding Layer -> Transformer Blocks (Attention + MLP) -> Output Layer

- **Critical path:**
  1.  **Input:** Text enters the system.
  2.  **Semantic Clustering:** Embedding layer ensures semantically related inputs are proximate (Enabling Causal Systematicity).
  3.  **Fact Retrieval:** MLP layers in middle blocks retrieve factual associations (Causal Common Factors).
  4.  **Prediction:** Output layer predicts the token based on the retrieved fact.

- **Design tradeoffs:**
  - **Distributed vs. Localized:** The paper argues for localization in MLPs to support "tacit knowledge," but acknowledges that neural networks are distributed. **Tradeoff:** Highly distributed representations (superposition) make interventions difficult (low specificity), while localized representations are easier to edit but may lose robustness.

- **Failure signatures:**
  - **"Clever Hans" Solutions:** The model relies on spurious correlations (e.g., watermarks in images) rather than semantic rules; interventions will fail to generalize.
  - **Lack of Generalization:** Editing "Paris" to "Berlin" works for the exact training prompt but fails for paraphrases (indicating rote memorization, not tacit knowledge).
  - **Catastrophic Forgetting:** Interventions to update one fact unintentionally degrade the model's performance on unrelated facts due to polysemantic interference.

- **First 3 experiments:**
  1.  **Causal Tracing (Noise Corruption):** Corrupt specific activations in the MLP layers while processing a factual query (e.g., "The Eiffel Tower is in ___") to identify which layers are critical for the correct prediction ("Paris").
  2.  **Rank-One Model Editing (ROME):** Apply a calculated weight update to the identified MLP layer to change the prediction (e.g., force the model to predict "Rome" instead of "Paris").
  3.  **Generalization & Specificity Testing:** Verify the edit by prompting the model with paraphrases (Generalization: "The Iron Lady is in...") and unrelated subjects (Specificity: "The Brandenburg Gate is in...") to confirm the mechanism is causally systematic.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do more recent large language models beyond GPT-J acquire tacit knowledge, and how does model scale affect the acquisition and representation of tacit knowledge?
- Basis in paper: [explicit] "Since Meng and colleagues only analyze an early type of LLM, GPT-J, the question remains whether more recent LLMs also acquire tacit knowledge."
- Why unresolved: The empirical evidence presented relies on a single study of one relatively early model.
- What evidence would resolve it: Replication of causal tracing and intervention studies across multiple LLM architectures and scales, demonstrating consistent patterns of generalization and specificity.

### Open Question 2
- Question: Are causal common factors in LLMs localized to single layers or distributed across multiple layers?
- Basis in paper: [explicit] "Causal common factors, and thus tacit knowledge, might be represented over multiple layers... causal tracing might only partially characterize the relevant causal common factors."
- Why unresolved: Current causal tracing methods assume single-layer localization, but neural networks exhibit redundancy where multiple layers perform similar tasks.
- What evidence would resolve it: Multi-layer intervention studies and analysis of representational redundancy across network depth.

### Open Question 3
- Question: Can causal common factors in LLMs be given semantic descriptions that are interpretable to human observers?
- Basis in paper: [explicit] "Determining if potential causal common factors can be given a semantic description seems like a particularly promising direction of future research."
- Why unresolved: Davies' account does not require semantic descriptions of internal structures, and the black-box nature of LLMs makes interpretability challenging.
- What evidence would resolve it: Systematic analysis using mechanistic interpretability methods to map internal representations to human-interpretable semantic categories.

### Open Question 4
- Question: To what extent are interventions in LLMs causally interpretable, and can interventionist frameworks validate current causal tracing methods?
- Basis in paper: [explicit] "It is not clear, however, to what extent such a causal interpretation is warranted for interventions as currently applied in machine learning."
- Why unresolved: Hase et al. (2023) found that interventions at different locations had similar efficacy, suggesting causal tracing may not reliably localize relevant causal common factors.
- What evidence would resolve it: Application of formal interventionist criteria (e.g., Woodward 2003) to evaluate and validate causal claims from LLM intervention studies.

## Limitations
- The empirical validation of Davies criteria for specific LLMs is largely theoretical, relying on a single study of GPT-J
- The "weaker syntactic structure" constraint via embeddings, while plausible, lacks systematic testing against philosophical requirements
- The claim that MLP layers function as discrete causal common factors may oversimplify the distributed, polysemantic nature of transformer representations

## Confidence
- **Medium Confidence**: The framework's applicability to LLMs and the claim that embedding layers satisfy a "weaker syntactic structure" constraint
- **Low Confidence**: The assertion that MLP layers function as discrete "causal common factors" storing factual associations
- **Medium Confidence**: The intervention-based verification approach (ROME-style edits) as a valid test for tacit knowledge

## Next Checks
1. **Direct Testing of Davies Criteria**: Systematically test whether specific LLMs satisfy all three criteria—semantic description, causal systematicity, and syntactic structure—using a diverse set of factual prompts and intervention studies.

2. **Polysemanticity Assessment**: Probe edited MLP neurons across diverse prompts to measure crosstalk and determine whether factual associations are truly localized or distributed in superposition, potentially invalidating the causal common factor claim.

3. **Paraphrase Generalization Testing**: Conduct controlled experiments comparing ROME-style edits on models trained with different data distributions to determine whether generalization across paraphrases is consistent or an artifact of specific training regimes.