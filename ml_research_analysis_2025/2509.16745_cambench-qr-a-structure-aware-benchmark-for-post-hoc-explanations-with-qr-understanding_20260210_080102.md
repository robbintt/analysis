---
ver: rpa2
title: 'CAMBench-QR : A Structure-Aware Benchmark for Post-Hoc Explanations with QR
  Understanding'
arxiv_id: '2509.16745'
source_url: https://arxiv.org/abs/2509.16745
tags:
- eigengrad-cam
- layercam
- mass
- explanations
- leakage
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CAMBENCH-QR introduces a structure-aware benchmark for evaluating
  class activation maps (CAMs) on QR codes, leveraging their canonical geometry to
  assess whether saliency methods concentrate on essential substructures (finder patterns,
  timing lines) while avoiding background. The benchmark includes synthetic QR/non-QR
  datasets with exact part masks and controlled distortions, alongside metrics like
  finder/timing mass ratios, background leakage, coverage AUCs, and distance-to-structure.
---

# CAMBench-QR : A Structure-Aware Benchmark for Post-Hoc Explanations with QR Understanding

## Quick Facts
- arXiv ID: 2509.16745
- Source URL: https://arxiv.org/abs/2509.16745
- Reference count: 27
- Primary result: Structure-aware evaluation of CAMs on QR codes, showing EigenGrad-CAM achieves best structural fidelity with minimal background leakage

## Executive Summary
CAMBENCH-QR introduces a benchmark for evaluating class activation maps (CAMs) on QR codes, leveraging their canonical geometry to assess whether saliency methods concentrate on essential substructures (finder patterns, timing lines) while avoiding background. The benchmark includes synthetic QR/non-QR datasets with exact part masks and controlled distortions, alongside metrics like finder/timing mass ratios, background leakage, coverage AUCs, and distance-to-structure. Experiments on ResNet-50 and ConvNeXt-B show that EigenGrad-CAM achieves the best structural fidelity with minimal background leakage and low distance-to-structure, especially under fine-tuning with a leakage-minimizing penalty.

## Method Summary
The authors created CAMBench-QR to evaluate post-hoc explanation methods using QR codes as a test domain. They generated synthetic QR and non-QR datasets with precise ground truth masks for finder patterns, timing lines, and background regions. The benchmark evaluates CAM methods across three training regimes: zero-shot (frozen backbone), fine-tuning only the last convolutional block, and fine-tuning with a leakage-minimizing penalty. Metrics include finder/timing mass ratios, background leakage, distance-to-structure, and coverage AUCs. The study tests ResNet-50 and ConvNeXt-B backbones with three CAM variants (EigenGrad-CAM, LayerCAM, XGrad-CAM).

## Key Results
- EigenGrad-CAM consistently achieves lowest background leakage (0.002-0.004) and distance-to-structure (0.105-0.170) across all regimes and backbones
- Last-block fine-tuning (L4) captures most structural gains with limited side effects, outperforming head-only and deeper fine-tuning approaches
- FT-LeakMin penalty effectively reduces background leakage and distance-to-structure without collapsing in-QR coverage when λ≈0.25 and α∈[0,0.25]

## Why This Works (Mechanism)

### Mechanism 1
QR codes provide objective ground truth for evaluating whether saliency methods attend to requisite substructures. QR codes have canonical, non-negotiable components—three finder patterns, timing lines, and module grid—whose locations are precisely known and parameterizable. This allows direct measurement of whether attribution mass falls on semantically essential regions versus correlational shortcuts (backgrounds, borders, textures). The core assumption is that visual explanations should be structurally faithful, not merely visually plausible; models that leverage shortcut features are problematic even if accurate.

### Mechanism 2
EigenGrad-CAM's subspace projection reduces background leakage by denoising gradient signals. EigenGrad-CAM projects gradients onto dominant eigenvectors of the activation covariance, suppressing noisy derivative directions that would otherwise activate on spurious textures. This yields consistently low background leakage and distance-to-structure across regimes. The core assumption is that noisy gradients are a primary source of off-structure saliency; principal components capture class-relevant signal.

### Mechanism 3
Fine-tuning only the last convolutional block reorients high-level features toward canonical parts with minimal leakage increase. The last block carries class-level semantics; adapting it while freezing earlier texture-sensitive filters allows the feature basis to shift toward QR motifs without relearning mid-level shortcuts. Going deeper (L3-L4 or full backbone) yields marginal FMR gains but substantially higher BL. The core assumption is that representational hierarchy places task semantics in late layers; early layers encode domain-agnostic texture/edge detectors that should not be perturbed.

## Foundational Learning

- **Class Activation Mapping (CAM)**: Understanding how gradients, activations, and pooling combine to produce saliency maps is prerequisite. Quick check: Given a convolutional feature map and class logits, can you sketch how Grad-CAM weights channels and produces a spatial heatmap?

- **Gradient-Weighted Attribution**: EigenGrad-CAM, XGrad-CAM, and the leakage penalty all rely on backpropagated gradients. Quick check: Why does a gradient-weighted map highlight regions that, if changed, would most affect the class score?

- **Distance Transform and Spatial Metrics**: The Distance-to-Structure (DtS) metric uses a Euclidean distance transform to penalize off-structure mass. Quick check: Given a binary mask of "structure" pixels, how would you compute the per-pixel distance to the nearest structure?

## Architecture Onboarding

- **Component map**: Data synthesis pipeline (QR/non-QR generation with masks) -> Training regimes (ZS/FT-Struct/FT-LeakMin) -> Explainer methods (LayerCAM/EigenGrad-CAM/XGrad-CAM) -> Metrics computation (FMR/TMR/BL/DtS/AUCs)

- **Critical path**: 1) Generate synthetic QR dataset with precise masks 2) Train classifier under chosen regime 3) Compute CAM for test images using selected explainer 4) Normalize saliency, compute structural metrics against masks 5) Aggregate across distortions; report BL, FMR/TMR, DtS, AUCs

- **Design tradeoffs**: EigenGrad-CAM: Best structure fidelity, lowest leakage, modest latency cost (4.5-5.6 ms). LayerCAM: Strong fine-detail preservation, near-XGrad speed (4.1-5.0 ms), slightly higher BL. XGrad-CAM: Fastest (3.8-4.7 ms), highest FMR but also highest BL. FT-LeakMin: Reduces BL and DtS across all methods; small λ (0.25) with α∈[0,0.25] is sufficient.

- **Failure signatures**: High BL with high FMR: Model is using finder-like textures in background (shortcut learning). Low FMR/TMR with low BL: Saliency concentrates inside QR but misses canonical parts; features not aligned. DtS increasing under distortion: Saliency drifts off-structure when images are perturbed; robustness issue. Deeper fine-tuning (L3-L4 or full) showing higher BL without FMR gain: Overfitting to texture shortcuts.

- **First 3 experiments**: 1) Baseline structure check: Run EigenGrad-CAM and XGrad-CAM on zero-shot ResNet-50; report BL, FMR, TMR, DtS. Expect EigenGrad lower BL/XGrad higher FMR. 2) Last-block fine-tuning ablation: Compare head-only vs. L4 vs. L3-L4 vs. full backbone on ResNet-50 with EigenGrad-CAM. Expect L4 as sweet spot. 3) Leakage penalty sweep: On ResNet-50, vary (λ, α) in {0, 0.25, 0.5} and measure BL/FMR/TMR/accuracy. Expect λ≈0.25, α∈[0,0.25] to minimize BL without collapsing in-QR mass.

## Open Questions the Paper Calls Out
1. Does structural faithfulness achieved via leakage-minimizing penalties generalize to domains with non-rigid or ambiguous canonical structures, such as medical imagery or documents?
2. Does the observed Pareto ordering (EigenGrad-CAM ≳ LayerCAM ≫ XGrad-CAM) persist when evaluated on Vision Transformers (ViTs) or video models?
3. How does the FT-LeakMin penalty perform in multi-object scenes where the background of one class may constitute the structure of another?

## Limitations
- The benchmark's reliance on synthetic QR data with exact masks limits ecological validity; real-world QR detection may involve occlusion, low contrast, or printing artifacts not fully captured by controlled distortions
- The study compares only three CAM variants and two backbone architectures, leaving open whether findings generalize to other explainer methods or vision transformers
- The leakage penalty's optimal (λ, α) settings are determined empirically for QR codes and may not transfer to domains lacking canonical geometry

## Confidence
- **High confidence**: EigenGrad-CAM consistently achieves lowest background leakage and distance-to-structure across regimes and backbones
- **Medium confidence**: Last-block fine-tuning optimally reorients features toward QR substructures with minimal side effects
- **Medium confidence**: The leakage-minimizing penalty effectively suppresses off-structure mass without collapsing in-QR coverage

## Next Checks
1. **Ecological realism test**: Evaluate CAM methods on a real-world QR dataset (e.g., open images with QR codes) to assess whether synthetic benchmark findings hold under uncontrolled conditions
2. **Method diversity check**: Benchmark additional CAM variants (e.g., Score-CAM, FullGrad) and vision transformer backbones to test generalizability of structural fidelity rankings
3. **Downstream impact check**: Measure whether improved structural fidelity (lower DtS, BL) correlates with better out-of-distribution robustness or user trust metrics in a human-subject study