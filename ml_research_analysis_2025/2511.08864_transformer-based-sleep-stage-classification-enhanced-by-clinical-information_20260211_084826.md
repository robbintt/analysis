---
ver: rpa2
title: Transformer-Based Sleep Stage Classification Enhanced by Clinical Information
arxiv_id: '2511.08864'
source_url: https://arxiv.org/abs/2511.08864
tags:
- sleep
- staging
- clinical
- stage
- event
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of automating sleep stage classification
  from polysomnography (PSG) while reducing inter-scorer variability. The authors
  propose a two-stage transformer-based architecture that incorporates expert event
  annotations (apneas, desaturations, arousals) and clinical metadata (age, sex, BMI)
  alongside raw PSG signals.
---

# Transformer-Based Sleep Stage Classification Enhanced by Clinical Information

## Quick Facts
- **arXiv ID:** 2511.08864
- **Source URL:** https://arxiv.org/abs/2511.08864
- **Reference count:** 40
- **Primary result:** Achieved macro-F1 0.8031 and micro-F1 0.9051 on SHHS dataset using transformer-based architecture with clinical information fusion

## Executive Summary
This study addresses the challenge of automating sleep stage classification from polysomnography (PSG) while reducing inter-scorer variability. The authors propose a two-stage transformer-based architecture that incorporates expert event annotations (apneas, desaturations, arousals) and clinical metadata (age, sex, BMI) alongside raw PSG signals. Using the SHHS dataset (n=8,357), the method integrates these contextual cues through feature fusion rather than multi-task learning. The final model achieves macro-F1 0.8031 and micro-F1 0.9051, substantially outperforming the PSG-only baseline (macro-F1 0.7745, micro-F1 0.8774).

## Method Summary
The proposed method uses a two-stage training approach. Stage 1 employs a 6-layer transformer (128 dim, 8 heads) to learn robust epoch-level representations from 30-second PSG epochs, pre-trained on all epochs (7.3M). Stage 2 freezes this transformer and uses a 12-layer 1D CNN (512 dim) to aggregate these embeddings across full-night sequences (padded to 1,500 epochs). Clinical context is fused by concatenating subject-level metadata (age, sex, BMI) and per-epoch expert event annotations (apneas, desaturations, arousals, periodic breathing) with the transformer outputs before the CNN. The model is trained with Adam (lr=1e-4), batch size 16, 100 epochs with learning rate decay of 0.90 per epoch, using Xavier initialization and selecting the best checkpoint based on validation loss.

## Key Results
- Achieved macro-F1 0.8031 and micro-F1 0.9051 on the SHHS test set
- Outperformed PSG-only baseline (macro-F1 0.7745, micro-F1 0.8774)
- Per-epoch event annotations contributed the largest performance gains
- Feature fusion approach outperformed multi-task learning alternatives
- Clinical metadata (age, sex, BMI) also improved accuracy over baseline

## Why This Works (Mechanism)

### Mechanism 1
Feature fusion of explicit event labels (apneas, arousals) likely outperforms multi-task learning (MTL) because it provides direct conditional signals rather than implicit optimization pressure. The model bypasses the difficult step of learning to detect events from raw waveforms and map them to stages simultaneously. By concatenating pre-computed event vectors directly to the Transformer embeddings, the classifier receives high-fidelity "hints" that disambiguate complex transition epochs (e.g., arousal-related stage shifts).

### Mechanism 2
Subject-level clinical metadata (Age, BMI, Sex) improves accuracy by allowing the model to learn population-level physiological priors. These static features are concatenated to every epoch feature vector, allowing the 1D CNN aggregator to bias its predictions based on demographics—for example, adjusting the prior probability of N3 (deep sleep) based on the subject's age, or fragmentation patterns based on BMI.

### Mechanism 3
Decoupling intra-epoch feature extraction (Transformer) from inter-epoch context aggregation (1D CNN) stabilizes training for long sequences. By pre-training the Transformer on 7M+ individual epochs (Stage 1) and freezing it, the system ensures robust waveform representation learning before attempting to model complex temporal dependencies across whole nights (Stage 2).

## Foundational Learning

**Feature Fusion (Late Fusion)**
- Why needed: Understanding that raw PSG signals are augmented by concatenating external vectors (Events, Metadata) at the embedding level, rather than at the input waveform level
- Quick check: Where in the pipeline are the clinical vectors injected—before the Transformer encoder or after it? (Answer: After, before the CNN aggregator)

**Transformer Positional Encoding vs. 1D CNN Aggregation**
- Why needed: The model uses Transformers for spatial/feature extraction within an epoch but switches to a 1D CNN for temporal aggregation across epochs
- Quick check: Why might a 1D CNN be preferred over a Transformer for the "Stage 2" aggregation of 1,500-epoch sequences? (Hint: Inductive bias for local temporal continuity)

**Macro-F1 vs. Micro-F1**
- Why needed: Sleep data is imbalanced (lots of N2, little N1). Macro-F1 treats all classes equally, making it the stricter metric for clinical utility
- Quick check: If a model improves Micro-F1 but Macro-F1 drops, what is likely happening to the minority classes (N1, N3)?

## Architecture Onboarding

**Component map:**
Input PSG signals + Per-epoch Event Annotations + Subject-level Clinical Metadata -> 6-layer Transformer (frozen) -> Feature Fusion Node -> 12-layer 1D CNN -> Linear layer -> 5-class Softmax

**Critical path:**
The Fusion Node. If the event vectors are missing or NaN during inference, the model will fail. The alignment of the "Per-epoch" event vector with the PSG epoch index is the most fragile part of the data pipeline.

**Design tradeoffs:**
- Frozen vs. Fine-tuned Encoder: Freezing reduces overfitting risk on small cohorts but may limit adaptability to new PSG montages
- Explicit Events vs. Auto-detection: The paper relies on expert event annotations. In deployment, you must first run a separate event detector (e.g., apnea detector) to generate these inputs, adding pipeline complexity

**Failure signatures:**
- Over-smoothing: The 1D CNN might smooth over short micro-arousals if the kernel size is too large or receptive field is misconfigured
- Context Leakage: If Subject-level metadata is normalized improperly, the model might learn to rely solely on BMI/Age priors and ignore the PSG signal

**First 3 experiments:**
1. Baseline Restoration: Implement Stage 1 (Transformer) + MLP head only to verify per-epoch accuracy matches expectations before adding complexity
2. Ablation (Vector Injection): Run Stage 2 with only random noise in place of Event vectors to confirm the performance gain isn't just from adding extra parameters to the CNN
3. Sequence Length Test: Test the Stage 2 CNN on padded sequences vs. truncated sequences to ensure the zero-masking strategy correctly prevents the model from learning padding artifacts

## Open Questions the Paper Calls Out

**Open Question 1**
Can the performance gains from expert event annotations be preserved when substituting them with automatically detected event labels in a real-time pipeline? The authors identify the "availability of event inputs at inference" as a major limitation and propose evaluating a "two-stage system (event detector -> context-aware stager)" in future work. The reported results rely on gold-standard human annotations, which are unavailable during fully automated inference.

**Open Question 2**
Does the feature-fusion approach generalize to external datasets with differing demographics, acquisition protocols, or evolving AASM scoring definitions? The authors state that "External validation... remains untested" and explicitly call for reproducing findings on independent cohorts and addressing "definition drift" in future work. The study is restricted to the SHHS cohort, which involves specific demographic characteristics and historical scoring rules.

**Open Question 3**
Can the explicit fusion of clinical context improve the performance of full-night foundation transformer models that already encode global temporal dependencies? The authors ask to explore "Integration with full-night encoders" to determine if context fusion can mitigate boundary misclassifications (e.g., N1 ↔ N2) in architectures that process the entire recording at once.

## Limitations

- The method requires expert event annotations (apneas, arousals, etc.) that are not always available in clinical settings
- Generalizability to other PSG montages or clinical populations is unclear, as the model was trained on SHHS with a specific channel configuration
- The paper does not report per-class performance metrics, making it difficult to assess potential biases for rare stages like N1

## Confidence

**High Confidence:** The overall architecture and training procedure are well-specified, and the reported macro-F1 and micro-F1 scores are consistent with the described methodology.

**Medium Confidence:** The claim that feature fusion of event annotations outperforms multi-task learning is supported by the ablation results, but the specific mechanism is not fully explored.

**Low Confidence:** The claim that clinical metadata improves accuracy is based on the reported performance gain, but the magnitude of the effect and its clinical significance are not discussed.

## Next Checks

1. Evaluate the model's performance using automatically-detected event annotations instead of expert labels to assess real-world applicability
2. Test the model on a different PSG dataset with a different montage or population to evaluate its robustness and potential for bias
3. Report and analyze the model's performance on each individual sleep stage (Wake, N1, N2, N3, REM) to identify potential failure modes or biases, especially for underrepresented classes