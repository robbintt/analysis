---
ver: rpa2
title: Knowledge Graph Fusion with Large Language Models for Accurate, Explainable
  Manufacturing Process Planning
arxiv_id: '2506.13026'
source_url: https://arxiv.org/abs/2506.13026
tags:
- knowledge
- graph
- drill
- size
- machining
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents ARKNESS, a hybrid framework that fuses automatically
  constructed knowledge graphs with large language models to deliver precise, traceable
  answers for CNC process planning. By distilling heterogeneous machining documents
  into semantically rich triples and retrieving context-aware subgraphs, ARKNESS addresses
  hallucinations and missing provenance in LLMs.
---

# Knowledge Graph Fusion with Large Language Models for Accurate, Explainable Manufacturing Process Planning

## Quick Facts
- **arXiv ID:** 2506.13026
- **Source URL:** https://arxiv.org/abs/2506.13026
- **Reference count:** 40
- **Primary result:** KG-augmented 3B-parameter Llama-3 matches GPT-4o accuracy on CNC process planning, reducing hallucinations by 22 pp.

## Executive Summary
This paper introduces ARKNESS, a hybrid framework that fuses automatically constructed knowledge graphs with large language models to deliver precise, traceable answers for CNC process planning. By distilling heterogeneous machining documents into semantically rich augmented triples and retrieving context-aware subgraphs, ARKNESS addresses hallucinations and missing provenance in LLMs. Experiments on 155 industry-curated questions show that a lightweight 3B-parameter Llama-3 augmented with ARKNESS matches GPT-4o accuracy, achieving up to +25 percentage points in multiple-choice accuracy, +22.4 pp in F1-score, and 8.1× ROUGE-L in open-ended responses. The framework enables smaller models to run fully on-premise, reducing hallucinations by 22 pp and supporting privacy-preserving, real-time shop-floor inference.

## Method Summary
ARKNESS processes 9 heterogeneous CNC machining documents into a knowledge graph with 4,329 triples using Docling for extraction and GPT-4o for triple generation. Queries are embedded and retrieved via cosine similarity (Top-K=10) with optional beam search expansion. Retrieved context and the query are formatted into a prompt for an LLM (tested: Llama 3.2 3B/3.1 8B, Qwen 2.5 7B, GPT-4o/mini, Gemini 2.0 Flash/Flash-Lite). The LLM generates answers conditioned on the explicit evidence, constraining outputs to values present in the KG. The framework is evaluated on 155 industry-curated questions, measuring multiple-choice accuracy, F1-score, ROUGE metrics, and hallucination reduction.

## Key Results
- KG-augmented 3B-parameter Llama-3 matches GPT-4o accuracy on CNC process planning queries.
- Up to +25 percentage points in multiple-choice accuracy, +22.4 pp in F1-score, and 8.1× ROUGE-L in open-ended responses.
- Reduces hallucinations by 22 percentage points compared to baseline LLMs.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Augmented triples with relationship descriptions improve retrieval precision over conventional subject-predicate-object triples.
- Mechanism: Each triple stores `(ENTITY_1, RELATIONSHIP_TYPE, ENTITY_2, "RELATIONSHIP_DESCRIPTION")` where the description preserves verbatim or paraphrased source text. This retained context disambiguates relations during similarity matching and provides provenance for downstream verification.
- Core assumption: The marginal context window cost of storing descriptions is outweighed by improved retrieval relevance and reduced ambiguity.
- Evidence anchors:
  - [section 3.2] "The purpose of including the 'Relationship Description' in the triplet information is to retain the relevant contextual information by which the triplet was created. This removes ambiguity and often missing information found in traditional knowledge graphs."
  - [abstract] "automatically distills heterogeneous machining documents... into augmented triple, multi-relational graphs without manual labeling"
  - [corpus] Weak direct evidence; related work "Graph RAG-Tool Fusion" (arXiv:2502.07223) similarly augments retrieval with structured dependencies but does not validate description-augmented triples specifically.
- Break condition: If descriptions introduce noise that degrades embedding quality (e.g., verbose or contradictory text), retrieval precision may decrease rather than improve.

### Mechanism 2
- Claim: Beam search graph traversal captures relevant triples missed by top-K cosine similarity alone, particularly for quantitative machining queries.
- Mechanism: Initial top-K retrieval identifies seed triples. Beam search then expands to adjacent triples (sharing nodes) up to depth `d_max` with beam width `b`. This multi-hop exploration surfaces connected procedural knowledge (e.g., tool→material→speed chains) that single-hop retrieval cannot reach.
- Core assumption: Relevant triples for a query are often structurally proximate to semantically similar triples, even if their direct embedding similarity is lower.
- Evidence anchors:
  - [section 3.3] Equations 5-7 define recursive beam expansion; "higher depths allows one to explore deeper through the graph allowing more information to be retrieved."
  - [section 5, Figure 4b] "machining specific questions... benefit from a deeper graph traversal to capture the necessary relevant information" with stabilization around depth 2.
  - [corpus] "Memory-augmented Query Reconstruction for LLM-based Knowledge Graph Reasoning" (arXiv:2503.05193) shows improved KGQA via multi-step reasoning but does not isolate beam search as the causal factor.
- Break condition: If the graph topology is sparse or poorly connected, beam expansion yields diminishing returns and increases latency without relevance gains.

### Mechanism 3
- Claim: Injecting retrieved subgraphs into LLM prompts grounds responses in domain-validated information, reducing hallucinations for numerical parameters.
- Mechanism: Retrieved triples and their context are concatenated with system instructions and the user query to form prompt `P(q, C)`. The LLM generates answers conditioned on this explicit evidence, constraining outputs to values present in the KG rather than internal model weights.
- Core assumption: The LLM can reliably prioritize injected context over its parametric knowledge when instructions direct it to do so.
- Evidence anchors:
  - [section 3.4] Equation 10 defines prompt construction; system instruction explicitly directs the LLM to "use retrieved knowledge."
  - [section 5, Tables 3-6] Baseline LLMs produce incorrect drill sizes and broad parameter ranges; KG-augmented outputs converge to correct grounded values (e.g., 0.0125 inches for Drill Size 82).
  - [corpus] "Large Language Model Powered Decision Support for a Metal Additive Manufacturing Knowledge Graph" (arXiv:2505.20308) demonstrates similar grounding benefits for AM process queries but lacks controlled ablation.
- Break condition: If retrieved context is irrelevant, contradictory, or exceeds context window limits, the LLM may ignore it or produce incoherent outputs.

## Foundational Learning

- Concept: **Knowledge Graphs as multi-relational tuple sets**
  - Why needed here: ARKNESS constructs and queries graphs defined as `G = {(v, r, w)}` triples. Understanding subject-predicate-object structure is prerequisite to following the storage schema (Section 3.1) and beam search logic.
  - Quick check question: Given triples `(Milling, requiresTool, EndMill)` and `(EndMill, hasDiameter, 0.5in)`, what adjacent triples would beam search explore starting from the first?

- Concept: **Retrieval-Augmented Generation (RAG)**
  - Why needed here: The framework is fundamentally a RAG pipeline where the retriever is a KG rather than a document store. Distinguishing retrieval from generation is essential to understand how grounding occurs.
  - Quick check question: How does KG-based retrieval differ from dense passage retrieval in what it returns to the generator?

- Concept: **Beam Search in Graph Traversal**
  - Why needed here: Section 3.3 uses beam search with depth and beam width parameters. Understanding this algorithm is necessary to interpret Figure 4's depth/Top-K experiments.
  - Quick check question: If beam width `b=5` and depth `d_max=2`, what is the maximum number of candidate triples that could be added (assuming no overlap)?

## Architecture Onboarding

- Component map:
  Document Preprocessor (Docling) -> Triple Extractor (GPT-4o) -> Graph Database (PostgreSQL) -> Embedding Encoder -> Retrieval Engine (cosine similarity + beam search) -> Prompt Compiler -> LLM Generator

- Critical path:
  1. Document ingestion -> triple extraction (one-time or incremental)
  2. Query embedding -> top-K retrieval -> beam expansion -> context assembly
  3. Prompt construction -> LLM inference -> response

- Design tradeoffs:
  - Top-K vs. retrieval quality: Higher K increases context but risks noise and context-window pressure (Figure 4 shows diminishing returns).
  - Beam depth vs. latency: Depth 2 helps machining-specific queries but adds computation; depth 0 suffices for content questions.
  - Model size vs. deployment constraints: 3B models with KG match GPT-4o accuracy, enabling on-premise deployment at cost of smaller model's general reasoning capacity.
  - Description inclusion vs. storage/embedding cost: Augmented triples improve precision but increase DB size and embedding computation.

- Failure signatures:
  - Retrieval returns irrelevant triples: Check embedding model domain alignment; consider fine-tuning embeddings on machining corpus.
  - LLM ignores retrieved context: Verify system prompt formatting; test with stronger instruction-following models.
  - Beam search timeout on large graphs: Reduce beam width or depth; pre-filter candidate pool by entity type.
  - Hallucinations persist: Audit KG for missing or incorrect triples; ensure source documents cover query domain.

- First 3 experiments:
  1. Baseline vs. KG-augmented accuracy on 20 held-out machining questions using a single model (e.g., Llama 3.2 3B). Measure multiple-choice accuracy and open-ended ROUGE-L to replicate paper's claims in a minimal setup.
  2. Ablation on beam depth (0, 1, 2) for machining-specific vs. content-specific queries. Log retrieval latency and accuracy to quantify the depth-accuracy tradeoff.
  3. Description-stripped triple comparison: Remove relationship descriptions from the KG and re-run retrieval. Compare accuracy delta to isolate the contribution of augmented context.

## Open Questions the Paper Calls Out

- **Extending to multimodal knowledge graphs:** How can ARKNESS be extended to heterogeneous multimodal knowledge graphs? The conclusion states future work focuses on "extending ARKNESS to a heterogeneous multimodal knowledge graphs." The current implementation processes only text (manuals, G-code) and lacks integration of visual data (CAD models) or sensor telemetry. A framework that ingests image and sensor data into the graph, validated by performance on multimodal manufacturing queries, would resolve this.

- **Enabling autonomous closed-loop decision support:** Can the framework enable autonomous closed-loop decision support? The conclusion lists enabling "closed-loop decision support in advanced manufacturing environments" as a future goal. The current system is a static planning assistant; it does not demonstrate real-time feedback ingestion or autonomous adjustment of process parameters. A system demonstration where live machine telemetry updates the knowledge graph and triggers automatic parameter re-optimization would resolve this.

- **Scaling retrieval latency to industrial-sized graphs:** How does retrieval latency scale to industrial-sized knowledge graphs? Section 3.3 notes graphs can reach "millions of connections," but experiments are limited to a graph of 4,329 triples. The recursive beam search expansion over millions of triples may introduce latency unacceptable for the claimed "real-time shop-floor" deployment. Latency benchmarks on graphs exceeding 100,000 triples to validate real-time feasibility would resolve this.

## Limitations

- The framework's performance relies on proprietary document quality and a carefully constructed knowledge graph; the exact embedding model, beam search hyperparameters, and benchmark dataset are not disclosed.
- The 3B-parameter model matching GPT-4o accuracy is based on a single comparison and may not hold across different question distributions or document corpora.
- The paper's claim that description-augmented triples are the key differentiator for retrieval precision lacks direct comparative ablation; only a related work citation is provided.

## Confidence

- **High confidence**: The core claim that KG-augmented LLMs reduce hallucinations and improve numerical precision in machining queries is well-supported by the ablation results and error analysis in Tables 3-6.
- **Medium confidence**: The assertion that a 3B-parameter model with ARKNESS matches GPT-4o accuracy is based on a single experimental comparison and may not hold across different question distributions or document corpora.
- **Low confidence**: The paper's claim that description-augmented triples are the key differentiator for retrieval precision lacks direct comparative ablation; only a related work citation is provided.

## Next Checks

1. Replicate the Top-K=10, depth=0 retrieval baseline vs. augmented performance on 20 held-out machining questions using a single model (e.g., Llama 3.2 3B).
2. Conduct a controlled ablation removing relationship descriptions from the KG and measure the accuracy delta to isolate their contribution.
3. Test the framework on a different set of manufacturing documents (e.g., additive manufacturing) to assess domain transfer and document processing robustness.