---
ver: rpa2
title: Application and Evaluation of Large Language Models for Forecasting the Impact
  of Traffic Incidents
arxiv_id: '2507.04803'
source_url: https://arxiv.org/abs/2507.04803
tags:
- incident
- traffic
- prediction
- impact
- incidents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explores the use of large language models (LLMs) for
  forecasting traffic incident impact, addressing the limitations of existing machine
  learning approaches that require large labeled datasets and cannot utilize free-text
  incident logs. A fully LLM-based solution is proposed that combines traffic features
  with incident features extracted from unstructured text logs using a small-scale
  LLM.
---

# Application and Evaluation of Large Language Models for Forecasting the Impact of Traffic Incidents

## Quick Facts
- **arXiv ID**: 2507.04803
- **Source URL**: https://arxiv.org/abs/2507.04803
- **Reference count**: 20
- **Primary result**: Best-performing LLM matches accuracy of top ML model for traffic incident impact forecasting without task-specific training

## Executive Summary
This study explores using large language models (LLMs) for traffic incident impact prediction, addressing limitations of traditional machine learning approaches that require large labeled datasets and cannot utilize unstructured incident logs. The authors propose a fully LLM-based solution that extracts structured incident features from free-text logs using a small-scale LLM and combines them with traffic features. A key innovation is an effective method for selecting examples for in-context learning that improves LLM performance by focusing on near-boundary incidents. Experiments with Claude 3.7 Sonnet, Gemini 2.0 Flash, and GPT 4.1 on real traffic data show the best LLM achieves accuracy comparable to Random Forest and XGBoost, despite not being trained on the prediction task.

## Method Summary
The solution extracts incident features from CHP free-text logs using GPT-4o mini, then combines them with traffic features (pre-incident relative speed and speed decrease ratio) to form a five-feature input set. LLMs (Claude 3.7 Sonnet, Gemini 2.0 Flash, GPT 4.1) perform classification via in-context learning using prompts with 24 examples selected through a near-boundary sampling method. The method identifies non-outlier incidents closest to neighboring class centroids in normalized feature space, then samples from these near-boundary cases. Performance is evaluated against Random Forest and XGBoost baselines trained with combined resampling and 5-fold grid-search cross-validation, using macro-average F1 as the primary metric due to class imbalance.

## Key Results
- Best-performing LLM (Claude 3.7 Sonnet) matches accuracy of top ML model despite no task-specific training
- Proposed near-boundary example selection method significantly outperforms random sampling
- Top 5 features yield marginally better results than all features in most cases
- LLMs achieve macro-F1 scores competitive with established ML baselines

## Why This Works (Mechanism)

### Mechanism 1: Near-Boundary Example Selection for In-Context Learning
- Selecting ICL examples from near class decision boundaries improves classification accuracy compared to random sampling
- Focuses LLM's attention on edge cases where classification is hardest, improving its ability to distinguish between similar instances
- Core assumption: LLMs learn task decision boundaries more effectively when shown ambiguous or borderline examples rather than prototypical ones
- Evidence: Substantial and consistent improvement over random sampling in experiments
- Break condition: Poorly defined class boundaries or mislabeled near-boundary samples could reinforce errors

### Mechanism 2: LLM-Based Feature Extraction from Unstructured Logs
- Small-scale LLM extracts structured incident features from free-text logs with human-level correctness
- Enables use of previously inaccessible information from CHP incident logs
- Core assumption: Extracted features accurately represent original text and preprocessing preserves semantic meaning
- Evidence: Examination of sampled incident logs shows LLM extracts features with human-level correctness
- Break condition: Novel abbreviations, domain-specific jargon, or ambiguous references not in glossary degrade extraction quality

### Mechanism 3: Traffic Feature Dominance with Incident Feature Augmentation
- Initial speed decrease ratio and pre-incident relative speed are strongest predictors
- Incident features provide marginal but useful augmentation to traffic-derived features
- Core assumption: 5-minute time step granularity and 2-mile upstream measurement adequately capture incident impact dynamics
- Evidence: Random Forest feature importance analysis shows traffic features and time account for most predictive power
- Break condition: Sparse sensor coverage upstream or unusual incident locations degrade traffic feature quality

## Foundational Learning

- **In-Context Learning (ICL)**
  - Why needed: Entire solution relies on LLMs performing classification without weight updates using only examples in prompts
  - Quick check: Given 3-class imbalanced problem, would you include equal examples per class or proportional to class frequency?

- **Macro-average vs Weighted-average F1 Score**
  - Why needed: Dataset is imbalanced (89.6% mild, 8.3% moderate, 2.1% severe); macro-average reveals minority-class performance
  - Quick check: If model achieves 90% accuracy but misclassifies all "severe" incidents, which metric would reveal this failure?

- **Feature Importance Analysis with Random Forest**
  - Why needed: Feature selection based on RF importance scores; understanding helps evaluate whether same features optimal for LLMs
  - Quick check: Could feature with low importance in RF still be valuable to LLM? Why or why not?

## Architecture Onboarding

- **Component map**: Log preprocessing -> Small-scale LLM extractor -> Traffic feature calculator -> Example selector -> Prompt constructor -> LLM predictor
- **Critical path**: Log preprocessing → Feature extraction → Example selection → Prompt construction → LLM prediction. Example selection is key differentiator.
- **Design tradeoffs**: More examples show diminishing returns with increased token costs; top 5 features vs all features simplifies prompts with marginally better accuracy; temperature=0 ensures deterministic output
- **Failure signatures**: High mild→moderate misclassification rate; near-zero severe recall; extraction errors from unknown abbreviations
- **First 3 experiments**:
  1. Baseline replication: Run random example selection (24 samples) with GPT-4.1, confirm macro-F1 gap vs proposed method
  2. Feature ablation: Test traffic-only vs traffic+incident features to quantify incident feature contribution
  3. Boundary sensitivity: Vary near-boundary threshold (30%, 50%, 70%) to find optimal sampling depth

## Open Questions the Paper Calls Out

- **Open Question 1**: Can additional contextual features improve LLM performance beyond current five features? Basis: Authors state "judicious addition of more features, including contextual ones, could possibly improve performance further." Unresolved because current study restricted to top five features identified by RF importance.

- **Open Question 2**: Do sophisticated prompt optimization strategies outperform near-boundary sampling method? Basis: Conclusion suggests "more sophisticated prompt optimization strategies could potentially yield better results." Unresolved because study implemented specific heuristic without comparing against algorithmic prompt tuning.

- **Open Question 3**: Does providing domain-specific expert knowledge to LLM improve classification accuracy? Basis: Authors explicitly note "usefulness of providing LLM with domain-specific expert knowledge on traffic incident scenarios could be investigated." Unresolved because system relies on generic "common knowledge" rather than explicit expert rules.

- **Open Question 4**: How well does proposed solution generalize to geographic regions with different traffic dynamics? Basis: Evaluation restricted to PEMS-BAY dataset (San Francisco Bay Area) and CHP logs. Unresolved because uncertain if in-context learning examples and feature extraction logic transfer effectively to regions with distinct infrastructure.

## Limitations
- Absence of published prompt templates and CHP glossary critical for exact replication
- Claim that best LLM matches top ML model depends on specific dataset and traffic conditions
- Near-boundary example selection method may not generalize to other imbalanced classification problems with poorly defined class boundaries
- Paper does not explore ensemble approaches or temperature tuning beyond 0

## Confidence

- **High confidence**: Experimental methodology for evaluating LLM performance against established ML baselines, use of macro-average F1 for imbalanced data, reported accuracy gap between random and near-boundary example selection
- **Medium confidence**: Claim that LLMs are viable option for traffic incident impact prediction (depends on specific dataset and task configuration)
- **Medium confidence**: Feature importance analysis showing traffic features dominate incident features (may vary with different traffic patterns)

## Next Checks

1. Replicate the macro-F1 gap: Run proposed method and random example selection (24 samples) with GPT-4.1 on same dataset, verifying ~0.10-0.15 improvement reported in Figure 5
2. Test feature ablation sensitivity: Conduct experiments with traffic-only features versus traffic+incident features to quantify marginal contribution of incident features across different traffic conditions
3. Validate boundary selection robustness: Vary near-boundary threshold (30%, 50%, 70% of closest-to-neighbor incidents) to determine optimal sampling depth and test generalizability to other imbalanced classification tasks