---
ver: rpa2
title: 'EmoMeta: A Multimodal Dataset for Fine-grained Emotion Classification in Chinese
  Metaphors'
arxiv_id: '2505.13483'
source_url: https://arxiv.org/abs/2505.13483
tags:
- emotion
- domain
- emotions
- dataset
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces EmoMeta, a novel multimodal dataset for
  fine-grained emotion classification in Chinese metaphors, addressing the scarcity
  of research in this area. The dataset comprises 5,000 text-image pairs from metaphorical
  advertisements, annotated for metaphor occurrence, domain relations, and 10 emotion
  categories: joy, love, trust, fear, sadness, disgust, anger, surprise, anticipation,
  and neutral.'
---

# EmoMeta: A Multimodal Dataset for Fine-grained Emotion Classification in Chinese Metaphors

## Quick Facts
- **arXiv ID:** 2505.13483
- **Source URL:** https://arxiv.org/abs/2505.13483
- **Reference count:** 27
- **Primary result:** Novel multimodal dataset of 5,000 Chinese metaphorical advertisements annotated for metaphor occurrence, domain relations, and 10 fine-grained emotions

## Executive Summary
This paper introduces EmoMeta, a novel multimodal dataset for fine-grained emotion classification in Chinese metaphors. The dataset addresses the scarcity of research in this area by providing 5,000 text-image pairs from metaphorical advertisements, annotated for metaphor occurrence, domain relations, and 10 emotion categories. Native Chinese-speaking annotators with metaphor research backgrounds were employed, and their annotations were cross-checked for quality control. The dataset's emotional distribution shows balance between public service and commercial advertisements, with fear and anticipation being the most common emotions. The authors anticipate that EmoMeta will contribute valuable data for advancing research on metaphors and emotions.

## Method Summary
The EmoMeta dataset was constructed by collecting metaphorical advertisements from multiple sources, including existing datasets and search engines. Images were deduplicated using MD5 encoding, and text was extracted using PaddleOCR with manual correction. Three groups of native Chinese-speaking annotators with metaphor research backgrounds labeled each text-image pair for metaphor occurrence, source/target domain relations, and emotion category. The annotation process used a conflict resolution rule where the more intense emotion from either text or image was selected when modalities disagreed. Quality control was implemented through inter-annotator agreement scoring, with Kappa scores ranging from 0.58-0.68 across different annotation types.

## Key Results
- EmoMeta dataset contains 5,000 text-image pairs of Chinese metaphorical advertisements
- Annotations include metaphor occurrence, source/target domain relations, and 10 emotion categories (joy, love, trust, fear, sadness, disgust, anger, surprise, anticipation, neutral)
- Dataset shows 87% public service and 13% commercial advertisements with fear and anticipation being most common emotions
- Inter-annotator agreement measured by Kappa scores: α = 0.68 (metaphor), α = 0.61 (source domain), α = 0.63 (target domain), α = 0.58 (emotion)

## Why This Works (Mechanism)

### Mechanism 1: Cross-Modal Semantic Incongruity for Metaphor Detection
Metaphors are identified by mapping a concrete source domain to an abstract target domain, often creating semantic incongruity between text and image that signals non-literal meaning. The dataset annotation model requires identifying an "irreversible 'A is B' identity relationship" where the target domain (abstract concept) is understood through the source domain (concrete concept). This cross-domain mapping is the core signal for metaphorical content, rooted in Conceptual Metaphor Theory (CMT).

### Mechanism 2: Dominant Modality Arbitration for Emotion Labeling
Fine-grained emotion labels for multimodal metaphors are determined by identifying the modality (text or image) that conveys the most intense emotional signal, rather than simple fusion. When text and image convey different emotions, the annotation process uses a specific conflict resolution rule where the more intense emotion from either source is used as the final annotation.

### Mechanism 3: Contextual Pragmatics for Emotion Elicitation
The specific emotion evoked by a metaphor is inferred by relating the mapped domains to the communicative purpose (pragmatics) of the content, such as public service ads using fear as a warning. Emotion is not just a lexical property but is derived from the metaphor's function, linking emotion to the intent behind the metaphor.

## Foundational Learning

- **Concept: Conceptual Metaphor Theory (CMT)**
  - Why needed here: The entire dataset is structured around CMT, where metaphors are a cognitive mechanism mapping a concrete source domain to an abstract target domain. You cannot interpret the annotations without understanding this mapping.
  - Quick check question: In the metaphorical expression "time is money," which concept is the source domain and which is the target domain?

- **Concept: Fine-Grained Emotion Taxonomy**
  - Why needed here: The dataset uses a specific 10-category scheme (Ekman's 6 + Plutchik's 2 + neutral), moving beyond simple sentiment. You must be able to distinguish between subtle states like "trust" and "anticipation" to use this data effectively.
  - Quick check question: According to the paper's definitions, what is the primary distinction between the emotion "joy" and the emotion "anticipation"?

- **Concept: Multimodal Conflict Arbitration**
  - Why needed here: The annotation protocol explicitly defines how to handle cases where text and image disagree. A system using this data must model this non-linear fusion rule, not simply average the signals.
  - Quick check question: If an image depicts a dangerous situation ("fear") but the text provides a hopeful solution ("anticipation"), which single emotion should be annotated according to the paper's rule?

## Architecture Onboarding

- **Component map:** Data Collection -> OCR & Cleaning -> Metaphor Identification -> Domain Mapping -> Emotion Annotation
- **Critical path:**
  1. Data Collection: Acquire image-text pairs from multiple sources (existing datasets, search engines)
  2. OCR & Cleaning: Extract text using PaddleOCR, manually correct errors, filter non-Chinese text
  3. Metaphor Identification: Annotators determine if a metaphor exists based on an "A is B" identity relation
  4. Domain Mapping: Identify source (concrete) and target (abstract) domains
  5. Emotion Annotation: Assign one of 10 labels, using the intensity rule to resolve cross-modal conflicts

- **Design tradeoffs:**
  - Single-label vs. Multi-label: Uses single-label classification, which simplifies modeling but may not capture complex, mixed emotional states
  - Intensity-Based Arbitration: Selecting the "more intense" emotion is subjective, introducing a specific type of label noise that models should be robust to
  - Ad-Centric Domain: Focusing on advertisements ensures vivid metaphors but may limit generalizability to more subtle, non-persuasive contexts

- **Failure signatures:**
  - Low Agreement on Emotion: The Kappa score for emotion categories is α = 0.58, indicating only moderate agreement. Models should expect significant label noise
  - OCR Artifacts: The pipeline relies on PaddleOCR, which can introduce typos and word order issues. Systems must be robust to noisy text input
  - Cultural Specificity: Metaphors are culturally dependent (e.g., "dinosaur" in Chinese). Models may fail on examples lacking specific cultural priors

- **First 3 experiments:**
  1. Baseline Metaphor Detection: Train a classifier to identify metaphor occurrence (binary) and compare performance against the dataset's metaphor labels
  2. Cross-Modal Emotion Classification: Develop a model that fuses image and text features to predict the fine-grained emotion label, explicitly evaluating if the model learns an "intensity arbitration" rule
  3. Domain Mapping Extraction: Create a system to automatically extract the source and target domains from the text-image pairs, validating against the annotated domain labels

## Open Questions the Paper Calls Out

### Open Question 1
How do state-of-the-art multimodal models perform on fine-grained emotion classification when applied to the EmoMeta dataset? The paper introduces a novel dataset to address the scarcity of resources but does not report benchmark results using current Large Multimodal Models (LMMs) on this specific 10-category task.

### Open Question 2
Can models trained on EmoMeta effectively generalize to metaphorical emotion classification in other languages or cultural contexts? While the dataset fills a gap for Chinese, the "potential variations in emotional nuances across languages" mentioned in the abstract require cross-linguistic validation.

### Open Question 3
What computational methods are most effective for resolving conflicting emotional signals between text and image modalities in metaphors? The annotation protocol dictates that when text and image emotions conflict, annotators choose the "more intense emotion," but no automated mechanism is proposed for this specific subjective resolution.

## Limitations
- Narrow domain focus on Chinese metaphorical advertisements may limit generalizability to other contexts
- Moderate inter-annotator agreement (Kappa scores ranging from 0.58-0.68) indicates substantial label noise, particularly for emotion classification
- Single-label emotion annotation approach may oversimplify complex emotional states where text and image convey different but equally valid emotions
- Dataset relies on PaddleOCR for text extraction, which introduces potential OCR artifacts that models must be robust to

## Confidence
- **High Confidence:** The dataset construction methodology, annotation schema, and domain mapping framework (Conceptual Metaphor Theory) are clearly specified and theoretically grounded
- **Medium Confidence:** The intensity-based arbitration rule for resolving text-image emotion conflicts is explicitly stated, though its subjective nature may introduce label noise
- **Low Confidence:** Generalization of findings beyond Chinese metaphorical advertisements remains uncertain due to cultural specificity of metaphors and the ad-centric domain

## Next Checks
1. **Label Noise Analysis:** Conduct a small-scale annotation study with independent annotators to measure inter-rater reliability on a subset of the dataset, particularly focusing on emotion classification where agreement is lowest
2. **OCR Artifact Impact:** Systematically evaluate model performance when trained on raw OCR output versus corrected text to quantify the impact of OCR errors on downstream tasks
3. **Cross-Cultural Transferability:** Test a subset of the dataset with non-Chinese speakers to assess whether the metaphors and their associated emotions are interpretable without cultural context