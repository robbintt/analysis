---
ver: rpa2
title: Do LLMs Memorize Recommendation Datasets? A Preliminary Study on MovieLens-1M
arxiv_id: '2505.10212'
source_url: https://arxiv.org/abs/2505.10212
tags:
- memorization
- recommendation
- llms
- dataset
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether Large Language Models (LLMs) have
  memorized the MovieLens-1M dataset during training, a critical issue for the reliability
  of recommender systems research. The authors define dataset memorization as the
  ability to retrieve item attributes, user profiles, and user-item interactions via
  prompting.
---

# Do LLMs Memorize Recommendation Datasets? A Preliminary Study on MovieLens-1M

## Quick Facts
- arXiv ID: 2505.10212
- Source URL: https://arxiv.org/abs/2505.10212
- Authors: Dario Di Palma; Felice Antonio Merra; Maurizio Sfilio; Vito Walter Anelli; Fedelucio Narducci; Tommaso Di Noia
- Reference count: 40
- Key outcome: All tested LLMs show varying degrees of MovieLens-1M memorization, with GPT-4o achieving 80.76% item coverage and 9.37% interaction coverage. Recommendation performance correlates with memorization extent, raising concerns about overoptimistic evaluations in recommender systems research.

## Executive Summary
This study investigates whether Large Language Models have memorized the MovieLens-1M dataset during training, a critical issue for the reliability of recommender systems research. Using coverage-based metrics, the authors evaluate multiple GPT and Llama models on three aspects of the dataset: items, users, and interactions. Results show that larger models memorize more data and exhibit clear popularity bias, with popular items being more easily retrieved. The findings suggest that current LLM-based recommender systems may be leveraging memorized data rather than generalizing, highlighting the need for new evaluation protocols and mitigation strategies.

## Method Summary
The authors define dataset memorization as the ability to retrieve item attributes, user profiles, and user-item interactions via prompting. They use few-shot prompting to extract MovieLens-1M content from GPT and Llama models, measuring coverage as the proportion of dataset entries successfully retrieved. The study evaluates memorization across three data types (items, users, interactions) and correlates memorization extent with recommendation performance on an 80/20 leave-n-out split. Recommendation performance is measured using HR@k and nDCG@k metrics and compared against traditional baselines like UserKNN, ItemKNN, BPRMF, and others.

## Key Results
- GPT-4o retrieves 80.76% of MovieLens-1M items, while Llama-3.2 1B retrieves only 1.93%
- Memorization extent correlates with recommendation performance, with higher coverage models achieving better HR/nDCG scores
- Larger models memorize disproportionately more content, with Llama-3.1 405B achieving 12.9% mean memorization vs. 5.82% for Llama-3.1 8B
- Clear popularity bias exists, with GPT-4o retrieving 89.06% of highly popular items versus 63.97% of rarely interacted items

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can retrieve memorized dataset content through few-shot prompting when dataset appears in training corpus.
- Mechanism: The paper operationalizes memorization as successful retrieval via structured prompts. When given a MovieID prefix, models with training exposure can autocomplete the exact title. Few-shot prompting establishes the output format, and the model completes from parametric knowledge rather than reasoning.
- Core assumption: The MovieLens-1M dataset files were included in pre-training corpora for both GPT and Llama model families.
- Evidence anchors:
  - [abstract] "define dataset memorization as the extent to which item attributes, user profiles, and user-item interactions can be retrieved by prompting the LLMs"
  - [section 2.2] "We identified few-shot prompting as the optimal approach for extracting the MovieLens-1M dataset from LLMs"
  - [corpus] Related work (arXiv:2601.02002) extends memorization detection approaches for recommender data, suggesting this is an active research direction with converging methodology
- Break condition: If datasets are excluded from pre-training, prompting-based retrieval should drop to near-zero coverage.

### Mechanism 2
- Claim: Memorization extent correlates with recommendation performance, potentially inflating evaluation metrics.
- Mechanism: Models that retrieve more interactions from training data achieve higher HR/nDCG scores. GPT-4o (80.76% item coverage, 9.37% interaction coverage) achieves HR@1=0.2796, while Llama-3.2 1B (1.93% item coverage) achieves HR@1=0.0222. The model may be recalling user-item associations rather than learning collaborative patterns.
- Core assumption: Performance gains come from memorized test interactions rather than generalization to unseen patterns.
- Evidence anchors:
  - [abstract] "recommendation performance is related to the extent of memorization"
  - [section 3.2] "Within each group, the model with higher memorization also demonstrates superior performance in the recommendation task"
  - [corpus] Weak direct corpus support; related work on hallucination vs. memorization trade-offs (arXiv:2511.08877) discusses similar phenomena but not specifically for recommendation
- Break condition: If you evaluate on temporally held-out interactions or synthetic users, performance gains should diminish if driven by memorization.

### Mechanism 3
- Claim: Larger models memorize disproportionately more content, including popularity bias from the training distribution.
- Mechanism: Model capacity increases with parameter count, enabling more training data to be stored verbatim. Llama-3.1 405B achieves 12.9% mean memorization vs. 5.82% for Llama-3.1 8B (54.88% reduction). Popular items are retrieved at higher rates because they appear more frequently in training.
- Core assumption: Popularity bias in retrieval reflects frequency-weighted exposure in pre-training data.
- Evidence anchors:
  - [abstract] "Larger models tend to memorize more data, and there is a clear popularity bias"
  - [section 3.4] "GPT-4o retrieves 89.06% of highly popular items, 86.68% of moderately popular items, and 63.97% of rarely interacted items"
  - [corpus] Limited corpus support; related memorization work in medicine (arXiv:2509.08604) and copyright extraction (arXiv:2505.12546) shows domain-specific memorization patterns but does not address popularity effects
- Break condition: If popularity bias is purely from learning collaborative patterns, it should persist even on completely synthetic datasets with controlled popularity distributions unknown during pre-training.

## Foundational Learning

- Concept: **Training Data Extraction Attacks**
  - Why needed here: The paper's methodology builds on adversarial extraction literature that shows LLMs can regurgitate training data verbatim. Understanding this background explains why simple prompting works.
  - Quick check question: Can you explain why temperature=0 and top_p=1 are used in the extraction experiments?

- Concept: **Contamination/Benchmark Leakage**
  - Why needed here: The core concern is that benchmark datasets appearing in training invalidate evaluation. This extends beyond recommendation to all LLM benchmarking.
  - Quick check question: Why does the paper argue that memorization leads to "non-generalizable test results"?

- Concept: **Popularity Bias in Recommendation**
  - Why needed here: The paper shows memorization amplifies existing biases. You need to understand what popularity bias is to interpret Figure 4 correctly.
  - Quick check question: If an LLM recommends popular items at higher rates, how can you distinguish whether this comes from memorization vs. learned preference for popularity?

## Architecture Onboarding

- Component map:
  - Few-shot extraction prompts (MovieID→Title, UserID→Attributes, UserID+history→next interaction) -> API calls (temperature=0, seed=42) -> Coverage calculation -> Correlation analysis with recommendation performance

- Critical path: Dataset → Few-shot prompt construction → API calls (temperature=0, seed=42) → Coverage calculation → Correlation analysis with recommendation performance

- Design tradeoffs:
  - Prompting technique: Authors chose few-shot over zero-shot/CoT after pilot testing; automatic prompt engineering could improve extraction but was deferred
  - Model selection: Only GPT and Llama families tested; other architectures (Claude, Mistral, etc.) behavior unknown
  - Coverage vs. exact match: Metrics require exact string matches; partial retrieval is not credited

- Failure signatures:
  - Low coverage could mean either (a) dataset not in training or (b) extraction prompt is suboptimal—distinguish by trying multiple prompt variants
  - High performance on memorized test sets that crashes on synthetic/controlled datasets indicates overfitting to training data
  - Inconsistent retrieval across runs (with temperature=0) suggests stochastic extraction issues

- First 3 experiments:
  1. Replicate item coverage extraction on 100 random MovieIDs using the Figure 1 prompt with temperature=0; compare your retrieval rate to Table 1
  2. Test whether prompt engineering improves extraction: try zero-shot vs. 2-shot vs. 5-shot on a held-out subset of 50 items
  3. Evaluate recommendation performance on a synthetic dataset with controlled popularity distribution (not in MovieLens) to isolate memorization from generalization; compare HR@10 to Table 2 baselines

## Open Questions the Paper Calls Out

- What mitigation approaches can effectively reduce memorization of recommendation datasets in LLMs while preserving their recommendation capabilities? (Authors state future work will develop ML-optimized memorization extraction techniques and mitigation approaches to reduce memorization and enhance reliability of LLM evaluation and usage in recommendation tasks.)

- Do LLMs memorize other widely-used recommendation datasets beyond MovieLens-1M, and does memorization extent vary across dataset domains? (The study examines only MovieLens-1M, acknowledging it as "one of the most widely used dataset" but leaving other datasets unexplored.)

- To what extent can automatic prompt engineering improve memorization extraction compared to the hand-engineered few-shot prompting used in this study? (Authors note that finding the best prompt through techniques such as automatic prompt engineering could improve performance, but leave this exploration to future work.)

## Limitations
- Memorization detection relies exclusively on exact string matching, potentially underestimating partial or paraphrased retrieval
- Only GPT and Llama model families were evaluated, limiting generalizability to other LLM architectures
- The paper assumes MovieLens-1M appears in pre-training data but cannot definitively prove this contamination
- Correlation between memorization and recommendation performance does not establish causation

## Confidence
- **High confidence**: The memorization coverage metrics and their comparative analysis across models (80.76% vs 1.93% item coverage) are methodologically sound and reproducible
- **Medium confidence**: The correlation between memorization extent and recommendation performance is plausible but requires further validation to rule out confounding factors
- **Medium confidence**: The popularity bias findings align with known frequency effects in language modeling but need testing on controlled synthetic datasets

## Next Checks
1. Create a controlled MovieLens-like dataset with known popularity distributions not present in training, then test whether LLMs still exhibit popularity bias when making recommendations

2. Verify whether memorization patterns differ between interactions that would have been available before vs. after the model's training cutoff date

3. Systematically vary few-shot examples, temperature settings, and prompt formats to determine whether reported coverage rates are stable or highly sensitive to extraction methodology