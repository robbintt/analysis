---
ver: rpa2
title: 'NEAT: Concept driven Neuron Attribution in LLMs'
arxiv_id: '2508.15875'
source_url: https://arxiv.org/abs/2508.15875
tags:
- neurons
- concept
- neuron
- arxiv
- bias
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces NEAT, a concept-driven neuron attribution
  method for large language models that reduces computational complexity from O(nm)
  to O(n) by using concept vectors to identify neurons associated with specific human-interpretable
  concepts. The approach involves creating a concept vector from representative examples,
  using it to probe all neurons in a single forward pass, and identifying the most
  influential neurons based on their effect on concept-related vocabulary.
---

# NEAT: Concept driven Neuron Attribution in LLMs

## Quick Facts
- **arXiv ID**: 2508.15875
- **Source URL**: https://arxiv.org/abs/2508.15875
- **Reference count**: 36
- **Primary result**: Introduces NEAT method that reduces neuron attribution computational complexity from O(n*m) to O(n) by using concept vectors to identify neurons associated with human-interpretable concepts.

## Executive Summary
This paper introduces NEAT, a concept-driven neuron attribution method for large language models that dramatically reduces computational complexity from O(n*m) to O(n) by using concept vectors. The approach involves creating a single concept vector from representative examples, using it to probe all neurons in a single forward pass, and identifying the most influential neurons based on their effect on concept-related vocabulary. The method was evaluated on hate speech detection and gender bias tasks, demonstrating superior performance compared to baselines by identifying neurons that generalize across examples rather than overfitting to individual instances.

## Method Summary
NEAT operates by first creating a concept vector through averaging last-layer token representations across all N examples of a concept. This single vector is then fed to the model as an input embedding, and each neuron's effect is computed by measuring the absolute change in log-probability of target tokens when that neuron is deactivated. Neurons are ranked by their effect scores and the top M are selected as concept neurons. The method was tested on GPT-2 Large using hate speech detection (Wiki Hate dataset) and gender bias analysis (Bias in Bios dataset), with evaluation metrics including MRR for hate speech and resolution accuracy/gender gap for bias detection.

## Key Results
- NEAT reduced MRR to 6.50e-05 for hate speech detection compared to 0.102 for the best baseline
- Deactivating top-10 neurons identified by NEAT significantly degraded model performance on hate-related tasks
- In gender bias experiments, targeted neuron deactivation systematically altered model predictions, with male neuron removal dropping male accuracy from 98.37% to 5.00% while increasing female accuracy
- NEAT successfully identified concept-relevant neurons that generalize across examples rather than overfitting to individual instances

## Why This Works (Mechanism)
NEAT works by creating a compact representation of a concept through vector averaging, then using causal intervention via neuron ablation to identify which neurons causally influence the concept's expression. The method measures neuron importance through the change in log-probability of target tokens, providing a stable metric that captures the neuron's causal role in concept generation. By reducing the search space from individual examples to a single concept vector, NEAT achieves massive computational efficiency while maintaining interpretability.

## Foundational Learning
- **Concept: Vector Space Averaging**
  - Why needed here: To understand how multiple example representations can be collapsed into a single "concept vector" that serves as a proxy for a human-defined idea
  - Quick check question: How does averaging embeddings reduce noise and potentially overfitting to individual examples?

- **Concept: Causal Intervention via Ablation**
  - Why needed here: The paper's core mechanism relies on the principle that removing a component (a neuron) and observing the change in output reveals that component's causal role
  - Quick check question: What is the difference between correlational attribution (e.g., attention weights) and causal attribution via ablation?

- **Concept: Log-Probability as a Faithfulness Metric**
  - Why needed here: The method quantifies a neuron's effect by the change in the *log-probability* of target tokens
  - Quick check question: Why is the absolute change in log-probability a more stable and informative metric than the raw probability change for this task?

## Architecture Onboarding
- **Component map**: Example Encoder -> Neuron Scorer -> Concept Neuron Selector -> Evaluator
- **Critical path**: The computation of the `concept_vector` (Section 4.1) is the most critical step. If this single vector fails to capture the concept, the entire downstream process is invalid
- **Design tradeoffs**:
  - **Efficiency vs. Granularity**: The method gains massive efficiency (O(n) vs O(n*m)) but only identifies neurons relevant to a *single* conceptual direction defined by the vector
  - **Token Set Selection**: The choice of the target token set $T$ is manual and can bias the neuron selection
- **Failure signatures**:
  - **Poor Generalization**: Identified neurons cause large effects on specific examples but fail on other instances of the concept
  - **Collateral Damage**: Deactivating top neurons degrades model performance on all language tasks, not just the target concept
- **First 3 experiments**:
  1. **Concept Vector Stability**: Generate concept vectors from different subsets of example data and measure correlation between identified neuron sets
  2. **Baseline Comparison on Known Concepts**: Apply NEAT to simple concepts (e.g., "French language") and compare against ground truth methods
  3. **Cross-Model Transfer**: Train concept vectors on one LLM and test if identified neurons transfer to architecturally similar models

## Open Questions the Paper Calls Out
- **Multi-concept analysis**: How does NEAT perform when multiple concepts overlap or interact within the same model region?
- **Unsupervised vocabulary**: Can the concept neuron identification process be adapted to remove dependency on manually defined target word lists?
- **Modern architecture generalization**: Does the method generalize to state-of-the-art LLM architectures beyond GPT-2?
- **Abstract vs syntactic encoding**: To what extent do identified "concept neurons" encode the abstract concept versus low-level syntactic correlates?

## Limitations
- **Concept vector construction**: The averaging step could obscure important variations in how concepts manifest across different contexts
- **Single-forward-pass limitation**: The O(n) complexity assumes the concept vector captures all relevant information, which may fail for complex, multi-faceted concepts
- **Target token set dependency**: Method's effectiveness heavily depends on manually curated target token sets that aren't systematically constructed

## Confidence
- **High confidence**: Computational complexity reduction from O(n*m) to O(n) is mathematically sound and well-demonstrated
- **Medium confidence**: Effectiveness of NEAT in identifying concept-relevant neurons is supported by experimental results
- **Low confidence**: Claim that identified neurons "generalize across examples rather than overfitting" needs more rigorous validation

## Next Checks
- **Cross-concept transferability**: Apply NEAT to identify neurons for one concept and test whether deactivating these same neurons affects semantically related but distinct concepts
- **Concept vector sensitivity analysis**: Systematically vary the number and diversity of examples used to construct the concept vector and measure stability of identified neuron sets
- **Temporal concept drift evaluation**: Test whether neurons identified for concepts in older training data remain effective for detecting those concepts in newer language patterns