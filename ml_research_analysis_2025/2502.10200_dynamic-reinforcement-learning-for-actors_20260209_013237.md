---
ver: rpa2
title: Dynamic Reinforcement Learning for Actors
arxiv_id: '2502.10200'
source_url: https://arxiv.org/abs/2502.10200
tags:
- learning
- dynamic
- agent
- exploration
- state
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces Dynamic Reinforcement Learning (Dynamic\
  \ RL), a novel RL framework that directly controls system dynamics instead of just\
  \ actor outputs, enabling flexible and deterministic exploration without external\
  \ noise. The key innovation is using \"sensitivity\"\u2014a local measure of how\
  \ input neighborhoods contract or expand through neuron processing\u2014to adjust\
  \ global system dynamics."
---

# Dynamic Reinforcement Learning for Actors

## Quick Facts
- arXiv ID: 2502.10200
- Source URL: https://arxiv.org/abs/2502.10200
- Authors: Katsunari Shibata
- Reference count: 15
- Primary result: Dynamic RL controls system dynamics via per-neuron sensitivity, achieving comparable/better performance than conventional RL with BPTT while avoiding backward computation through time.

## Executive Summary
This paper introduces Dynamic Reinforcement Learning (Dynamic RL), a novel RL framework that directly controls system dynamics instead of just actor outputs, enabling flexible and deterministic exploration without external noise. The key innovation is using "sensitivity"—a local measure of how input neighborhoods contract or expand through neuron processing—to adjust global system dynamics. Sensitivity Adjustment Learning (SAL) maintains chaotic dynamics to prevent excessive convergence, while Sensitivity-controlled Reinforcement Learning (SRL) modulates dynamics based on TD error: converging for better reproducibility with positive TD error and diverging for enhanced exploration with negative TD error. Tested on two dynamic tasks (sequential navigation and slider-crank control), Dynamic RL achieved comparable or better success rates than conventional RL using BPTT while requiring no backward computation through time. It demonstrated faster adaptation to environmental changes and excellent exploration-exploitation balance.

## Method Summary
Dynamic RL modifies only the actor within an actor-critic setup, leaving the critic trained via conventional BPTT. The actor uses a Multiple Timescale Recurrent Neural Network (MTRNN) with dynamic neurons. Per-neuron sensitivity s = f'(U)||w|| measures local input-output neighborhood contraction/expansion. SAL activates when moving-average sensitivity falls below threshold sth, increasing sensitivity to maintain chaos. SRL activates otherwise, adjusting sensitivity proportionally to TD error sign—positive TD error reduces sensitivity for convergence, negative increases it for exploration. The upper hidden layer uses self-feedback with spectral radius 3.0 to seed chaotic dynamics. Regularization prevents output saturation, and critic raising adjusts critic bias when values drop.

## Key Results
- Dynamic RL achieved comparable or better success rates than conventional RL on sequential navigation and slider-crank control tasks
- Demonstrated faster adaptation to environmental changes (output swapping) compared to conventional RL
- Eliminated need for backward computation through time for actor learning while maintaining performance

## Why This Works (Mechanism)

### Mechanism 1: Local Sensitivity Controls Global Dynamics
- Claim: Adjusting per-neuron sensitivity propagates to control the global system's exploration behavior (measured via exploration exponent/Lyapunov-like metric)
- Mechanism: Sensitivity s(U;w) = f'(U)||w|| measures local input-output neighborhood contraction/expansion per neuron. When weights/biases are updated to modify sensitivity, this affects eigenvalues of the system Jacobian at each state, which aggregates into measurable changes in the Lyapunov exponent of the full agent-environment system
- Core assumption: The RNN-environment loop is sufficiently connected that local sensitivity changes compound into meaningful global chaoticity shifts
- Evidence anchors: [abstract], [page 7, Eq. 4-5], [corpus]
- Break condition: If network connectivity is poor or sth is misconfigured, local adjustments fail to propagate into useful global dynamics changes

### Mechanism 2: TD Error-Gated Convergence/Divergence
- Claim: Positive TD error triggers dynamics convergence (better reproducibility), negative TD error triggers divergence (more exploration), yielding automatic exploration-exploitation balancing
- Mechanism: SRL uses TD error sign to modulate sensitivity—positive TD error reduces sensitivity (Eq. 14-17), creating attractor-like behavior around good transitions; negative TD error increases sensitivity, restoring chaotic search. SAL enforces a sensitivity floor to prevent collapse
- Core assumption: TD error reliably signals "goodness" of transitions in a way that maps to desired local dynamics
- Evidence anchors: [abstract], [page 8, Eq. 11-17], [corpus]
- Break condition: If reward shaping produces misleading TD errors, or critic learning is unstable, the convergence/divergence signal corrupts

### Mechanism 3: Deterministic Exploration via Chaotic Dynamics
- Claim: Chaotic system dynamics enable deterministic, flexible exploration without stochastic noise injection
- Mechanism: Large spectral radius (3.0) in self-feedback weights seeds chaotic dynamics; similar initial states diverge exponentially (positive Lyapunov exponent), generating diverse behaviors deterministically. SAL maintains baseline chaos; SRL modulates it
- Core assumption: Initial chaotic dynamics cover enough state-action space to discover rewarding trajectories
- Evidence anchors: [abstract], [page 12, Table 2], [corpus]
- Break condition: If initial weights fail to induce chaos or SAL/SRL over-compress dynamics, exploration collapses

## Foundational Learning

Concept: **Lyapunov Exponents and Chaotic Dynamics**
- Why needed here: Sensitivity relates directly to local dynamics; understanding how local contraction/expansion aggregates into global chaoticity is essential
- Quick check question: Can you explain why a positive Lyapunov exponent indicates chaotic behavior?

Concept: **Actor-Critic Reinforcement Learning**
- Why needed here: Dynamic RL modifies only the actor within an actor-critic setup; the critic still uses conventional TD learning
- Quick check question: What role does the critic play in computing the TD error?

Concept: **Backpropagation Through Time (BPTT)**
- Why needed here: Dynamic RL explicitly avoids BPTT for actor learning; understanding what is avoided clarifies the computational gain
- Quick check question: Why does BPTT require storing past states, and what cost does this impose?

## Architecture Onboarding

- Component map: Sensor inputs → Actor (MTRNN with 200 lower + 100 upper hidden neurons) → Actions; Critic (Elman RNN) → TD error; SAL/SRL modules → Actor weight updates

- Critical path:
  1. Initialize actor self-feedback with spectral radius 3.0 to ensure chaotic dynamics
  2. Compute per-neuron sensitivity s = f'(U)||w|| each step
  3. Per neuron: if s̄ < sth, apply SAL (Eq. 9-10); else apply SRL (Eq. 16-17) using TD error
  4. Critic computes TD error via BPTT (Eq. 13, 20)
  5. Broadcast TD error to all actor neurons for SRL; no backward-through-time for actor

- Design tradeoffs:
  - Higher sth → more maintained exploration, but potentially less stability
  - Larger spectral radius → stronger chaos, but higher instability risk
  - Larger τ in upper hidden → slower dynamics and temporal abstraction, but possibly slower adaptation
  - Stronger output regularization → less saturation, but reduced expressiveness

- Failure signatures:
  - Actor outputs saturating near ±1 (e.g., diagonal movement bias in navigation)
  - Exploration exponent dropping below 0 → dynamics over-converged; learning stalls
  - Sudden spikes in episode steps during learning (instability events)
  - Post-environment-change trapping in corners (insufficient exploration recovery)

- First 3 experiments:
  1. Replicate sequential navigation with spectral radius 3.0 vs 1.5 to verify chaos requirement for exploration
  2. Ablate SAL (condition A-2) to confirm it prevents premature convergence
  3. Track exploration exponent over episodes to observe the hypothesized "exploration → thinking" trajectory (decreasing irregularity with maintained rationality)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can Dynamic RL be extended to train the critic network without relying on Backpropagation Through Time (BPTT)?
- Basis in paper: [explicit] The abstract and methodology section note that "applying it [Dynamic RL] to the critic remains a challenge," and the paper currently relies on conventional BPTT for the critic
- Why unresolved: The mechanism for controlling dynamics via sensitivity (SRL) was designed for the actor's exploration-exploitation balance, and it is unclear how "convergence" or "divergence" applies to value function approximation in the critic
- What evidence would resolve it: A modified learning rule that allows the critic to approximate value functions using local sensitivity adjustments and TD error, successfully eliminating backward computation through time for both actor and critic

### Open Question 2
- Question: How can the saturation of actor outputs be mitigated to enable finer motor control?
- Basis in paper: [explicit] The results section states, "the actor outputs were easily saturated and the agent was apt to move diagonally... This problem needs to be solved"
- Why unresolved: The paper's regularization technique (Eq. 21) was insufficient to prevent outputs from sticking to the bounds of the hyperbolic tangent activation function, leading to less smooth behaviors than conventional RL
- What evidence would resolve it: A regularization method or network architecture that maintains the chaotic dynamics required for exploration while keeping actor outputs within the linear (non-saturated) range for precise control

### Open Question 3
- Question: Does the "exploration grows into thinking" hypothesis hold true for abstract or lateral thinking tasks?
- Basis in paper: [explicit] The author hypothesizes that "exploration grows into thinking through learning" and suggests this method could overcome the "lateral thinking" limitations of current LLMs
- Why unresolved: The experiments were limited to navigation and motor control; the paper provides no evidence that the learned autonomous state transitions evolve into abstract reasoning or novel inspiration in semantic spaces
- What evidence would resolve it: Successful application of Dynamic RL to a symbolic or logic-based environment where the agent demonstrates the ability to autonomously derive novel, non-interpolated solutions ("lateral thinking")

## Limitations
- Requires careful tuning of sensitivity threshold sth to balance exploration and exploitation
- Actor output saturation near ±1 limits fine motor control capabilities
- Relies on conventional BPTT for critic training, not fully eliminating backward computation
- Chaotic dynamics may introduce instability requiring careful spectral radius tuning

## Confidence

- Mechanism 1 (Local Sensitivity Controls Global Dynamics): Medium - theoretical basis exists but local-to-global propagation is not empirically validated
- Mechanism 2 (TD Error-Gated Convergence/Divergence): Medium - intuitive but depends on critic stability
- Mechanism 3 (Deterministic Exploration via Chaotic Dynamics): High - well-established in prior work, but parameter sensitivity is a concern

## Next Checks

1. Test sensitivity control by artificially scaling local sensitivity changes and measuring global Lyapunov exponent changes
2. Ablate SAL and SRL separately to confirm their roles in maintaining exploration and balancing exploitation
3. Evaluate performance under reward-shaping variations to stress-test TD error reliability