---
ver: rpa2
title: 'When the Gold Standard isn''t Necessarily Standard: Challenges of Evaluating
  the Translation of User-Generated Content'
arxiv_id: '2512.17738'
source_url: https://arxiv.org/abs/2512.17738
tags:
- translation
- guidelines
- rocs-mt
- language
- pfsmb
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of evaluating translations of
  user-generated content (UGC), which often contains non-standard language like slang,
  typos, and emojis. The authors analyze four UGC translation datasets to derive a
  taxonomy of twelve non-standard phenomena and five translation actions (NORMALISE,
  COPY, TRANSFER, OMIT, CENSOR).
---

# When the Gold Standard isn't Necessarily Standard: Challenges of Evaluating the Translation of User-Generated Content

## Quick Facts
- **arXiv ID**: 2512.17738
- **Source URL**: https://arxiv.org/abs/2512.17738
- **Reference count**: 40
- **One-line primary result**: Translation scores for user-generated content are highly sensitive to explicit guidelines in prompts, with scores improving when these align with dataset-specific translation standards.

## Executive Summary
This paper addresses the challenge of evaluating translations of user-generated content (UGC), which often contains non-standard language like slang, typos, and emojis. The authors analyze four UGC translation datasets to derive a taxonomy of twelve non-standard phenomena and five translation actions (NORMALISE, COPY, TRANSFER, OMIT, CENSOR). They find significant variation in how UGC is handled across datasets, creating a spectrum of standardness in reference translations. Through a case study on large language models (LLMs), they demonstrate that translation scores are highly sensitive to prompts with explicit translation instructions for UGC, and that scores improve when prompts align with dataset guidelines. The authors argue that fair evaluation of UGC translation requires both models and metrics to be aware of translation guidelines, calling for clearer guidelines during dataset creation and the development of controllable, guideline-aware evaluation frameworks.

## Method Summary
The study evaluates LLM translation of UGC across four datasets (RoCS-MT, FooTweets, MMTC, PFSMB) using three conditions: zero-shot without guidelines, matching corpus-specific guidelines, and mismatching guidelines from other corpora. Four models are tested (NLLB-3B, LLaMA-3.1-8B-Instruct, Gemma-2-9B-it, Tower-7B-v0.2) via vLLM with greedy sampling, BF16 precision, and context length limits. Translation quality is assessed using reference-based metrics (COMET-22, BLEU) and reference-less COMET-Kiwi, comparing performance across guideline configurations.

## Key Results
- LLM translation scores are highly sensitive to explicit guidelines in prompts for UGC
- Scores improve when prompts align with dataset-specific translation guidelines
- Reference-based metrics (COMET) show greater sensitivity to guideline alignment than reference-less metrics (COMET-Kiwi)
- Different datasets exhibit varying levels of "standardness" in their reference translations, from highly normalized to style-preserving

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prompting LLMs with explicit translation guidelines that align with dataset-specific translation standards improves reference-based evaluation scores for user-generated content (UGC).
- Mechanism: Explicit guidelines in the prompt reduce the ambiguity in translating non-standard phenomena (like slang, typos, emojis) by steering the model's output style to match the "standardness" level of the human reference translations. When the model's output distribution aligns with the specific expectations (e.g., NORMALISE vs. TRANSFER) encoded in the reference, semantic similarity metrics like COMET assign higher scores.
- Core assumption: The reference-based metric (e.g., COMET) implicitly rewards translations that match the style and annotation philosophy of the human reference, and the LLM is capable of following the provided instructions to a sufficient degree.
- Evidence anchors:
  - [abstract] "...translation scores are highly sensitive to prompts with explicit translation instructions for UGC, and that scores improve when these align with the dataset's guidelines."
  - [section] (4.1.3) "Using matching guidelines yields the best COMET results for Gemma-2-9B... the best score on MMTC is observed when applying MMTC guidelines, and likewise the best score for RoCS-MT."
  - [corpus] Related work supports LLMs' ability to adapt style via prompting, but corpus evidence for this specific mechanism on UGC translation is weak and limited.
- Break condition: The LLM has poor instruction-following capabilities or its safety filters prevent it from generating content that adheres to the guidelines (e.g., refusing to translate profanity).

### Mechanism 2
- Claim: A taxonomy of non-standard phenomena and a defined set of translation actions can systematically capture the variation in UGC translation "gold standards" across different datasets.
- Mechanism: By categorizing non-standard elements (e.g., spelling, emojis, profanity) and defining discrete actions for them (NORMALISE, COPY, TRANSFER, OMIT, CENSOR), the framework makes implicit, subjective translation decisions explicit and comparable. This allows for the analysis of datasets along a continuum of "standardness."
- Core assumption: Translation guidelines from dataset creation are a reliable proxy for the stylistic preferences embedded in the human reference translations.
- Evidence anchors:
  - [abstract] "To explore this, we examine the human translation guidelines of four UGC datasets, and derive a taxonomy of twelve non-standard phenomena and five translation actions..."
  - [section] (Table 1) Summarizes the guidelines for four datasets, showing systematic differences in how phenomena like word elongation and informal acronyms are treated.
  - [corpus] No direct corpus support for this specific taxonomy was found in related papers.
- Break condition: Translation guidelines are vague, contradictory, or inconsistently applied during dataset creation, making the taxonomy difficult to apply reliably.

### Mechanism 3
- Claim: Reference-less evaluation metrics (e.g., COMET-Kiwi) are more robust to variations in translation style than reference-based metrics, but still struggle with highly non-standard content.
- Mechanism: Reference-less metrics assess translation quality based on the source and target alone, without comparing to a single, style-specific reference. This makes them less sensitive to the "standardness" mismatch. However, their training data may still introduce a bias toward standard language.
- Core assumption: The quality of a translation can be judged independently of a specific reference style.
- Evidence anchors:
  - [section] (4.2) "...score variations between the different guideline configurations are negligible, suggesting that COMET-Kiwi is somewhat robust to UGC... This is consistent with Aepli et al. (2023)'s conclusion that COMET-Kiwi is not robust to non-standard orthographic variation in dialects and is biased towards standardised outputs."
  - [corpus] Corpus signals mention related frameworks for evaluating LLMs against English varieties, but do not directly support this mechanism.
- Break condition: The metric's training data has a strong bias toward standard language, causing it to penalize valid but highly non-standard translations.

## Foundational Learning

- Concept: **Reference-Based vs. Reference-Less Automatic Evaluation**
  - Why needed here: The core argument of the paper depends on understanding that these two families of metrics behave differently. Reference-based metrics are sensitive to style, while reference-less metrics are potentially more style-agnostic.
  - Quick check question: If you translate a sentence with slang into its fully formal, standard equivalent, would a reference-based metric prefer it over a translation that preserves the slang, assuming the human reference is also informal?

- Concept: **Prompting for Style Control in LLMs**
  - Why needed here: The paper's intervention relies on using natural language instructions to control the output style of LLMs. This requires understanding that LLMs can be guided by context.
  - Quick check question: How would you modify a translation prompt to ask the model to correct grammatical errors in the source text but preserve informal acronyms?

- Concept: **The "Standardness" Spectrum in Translation**
  - Why needed here: The paper challenges the idea of a single correct translation. It argues that the appropriate translation depends on the desired level of "standardness"â€”from a fully corrected, formal output to one that preserves the informal, noisy style of the original UGC.
  - Quick check question: For a customer support chatbot, should the translation of a user's typo-filled query be normalized or preserved?

## Architecture Onboarding

- **Component map**:
  UGC Dataset -> Taxonomy & Guidelines Engine -> LLM Translator -> Evaluator

- **Critical path**:
  1. **Guideline Extraction/Formulation**: Convert human-readable guidelines into a structured prompt. This is the most crucial step. Use the paper's taxonomy to ensure all 12 phenomena are addressed.
  2. **Prompt Construction**: Inject the formulated guidelines into the LLM's system or user prompt template (see Appendix B).
  3. **Translation & Evaluation**: Generate translations and score them. Critically, *the choice of evaluation metric must be aligned with the goal*. For fairness in style-sensitive evaluation, use reference-based metrics. For style-agnostic quality assessment, use reference-less metrics.

- **Design tradeoffs**:
  - **Granularity vs. Complexity**: A more detailed, example-based guideline prompt improves control but increases prompt length and the risk of conflicting instructions.
  - **Control vs. LLM Capability**: Not all LLMs are equally good at following complex, compositional instructions. A highly capable but unsteerable model may be worse than a slightly less capable but more instructable one.
  - **Fairness vs. Objectivity**: The paper argues that "fair" evaluation is guideline-aware. This makes evaluation more subjective to the chosen guidelines, as opposed to a single, objective standard.

- **Failure signatures**:
  - **Safety Filter Refusal**: The model refuses to translate due to profanity or explicit content in the source. Signature: A high percentage of empty or refusal outputs.
  - **Instruction Ignoring**: The model produces outputs with low lexical variation across different guideline prompts. Signature: High BLEU scores when comparing outputs from different prompt configurations.
  - **Contradictory Guidelines**: The model fails to apply guidelines consistently when they conflict (e.g., "normalise spelling" vs. "transfer character repetitions"). Signature: Inconsistent application of specific rules.

- **First 3 experiments**:
  1. **Baseline Evaluation**: Translate a UGC test set with a baseline model (e.g., NLLB-3B) and a default-prompted LLM. Evaluate with COMET and COMET-Kiwi to establish a performance baseline and identify the default style of the models.
  2. **Guideline Alignment Test**: Take a dataset with known guidelines. Prompt the chosen LLM with matching, mismatching, and no guidelines. Measure the delta in COMET scores. A positive delta with matching guidelines confirms the core mechanism.
  3. **Instruction Following Analysis**: Manually analyze a sample of outputs from the best-performing prompt configuration. Check for consistent application of the 12 guideline categories. This identifies failure modes like instruction ignoring or contradiction.

## Open Questions the Paper Calls Out
1. **Few-shot prompting**: The authors suggest that few-shot or chain-of-thought prompting strategies might improve LLMs' adherence to UGC translation guidelines compared to the zero-shot approach tested.
2. **LLM-as-a-judge**: The paper recommends exploring LLM-as-a-judge evaluation frameworks configured with style-specific guidelines as a potential improvement over current neural metrics.
3. **Guideline compositionality**: The authors identify contradictions between conflicting guidelines (e.g., normalizing spelling while transferring character repetitions) and call for prompting strategies that are compositional and interpretable.

## Limitations
- Dataset guideline reliability: Translation guidelines extracted from papers may not capture full complexity or may be inconsistently applied
- LLM instruction-following capability: Effectiveness depends heavily on model's ability to follow complex instructions, which varies significantly across model families
- Evaluation metric biases: Reference-based metrics' sensitivity to style may reflect biases in training data rather than true translation quality differences

## Confidence
- **High Confidence**: LLM translation scores are highly sensitive to prompts with explicit translation instructions for UGC, and scores improve when these align with dataset guidelines.
- **Medium Confidence**: The proposed taxonomy of twelve non-standard phenomena and five translation actions effectively captures variation in UGC translation "gold standards."
- **Low Confidence**: Reference-less evaluation metrics (e.g., COMET-Kiwi) are more robust to variations in translation style.

## Next Checks
1. **Guideline Consistency Analysis**: Manually examine reference translations from each dataset to verify extracted guidelines accurately reflect actual translation decisions.
2. **Cross-Dataset Translation Transfer**: Translate a single UGC test set using guidelines from different datasets and evaluate with both metric types to quantify style-dependent score variations.
3. **Instruction-Following Capability Benchmark**: Test diverse LLMs with progressively complex guideline prompts to establish relationship between model capability and ability to follow compositional instructions.