---
ver: rpa2
title: 'From Generative Modeling to Clinical Classification: A GPT-Based Architecture
  for EHR Notes'
arxiv_id: '2601.21955'
source_url: https://arxiv.org/abs/2601.21955
tags:
- training
- fine-tuning
- clinical
- classification
- transformer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of adapting large pretrained
  language models to clinical text classification, where limited labeled data and
  high computational costs are major constraints. A selective fine-tuning strategy
  is proposed that freezes the majority of the GPT-2 model and updates only the final
  Transformer block, the final layer normalization, and a lightweight classification
  head.
---

# From Generative Modeling to Clinical Classification: A GPT-Based Architecture for EHR Notes

## Quick Facts
- arXiv ID: 2601.21955
- Source URL: https://arxiv.org/abs/2601.21955
- Authors: Fariba Afrin Irany
- Reference count: 12
- Achieves up to 96.23% accuracy on aggregate disease outcome prediction in radiology reports

## Executive Summary
This study presents a selective fine-tuning approach for adapting large pretrained language models to clinical text classification tasks. The method addresses the challenges of limited labeled data and high computational costs by freezing most of the GPT-2 model and updating only the final Transformer block, final layer normalization, and a lightweight classification head. This reduces trainable parameters by over 94% compared to full fine-tuning while maintaining strong classification performance on MIMIC-IV radiology reports.

## Method Summary
The proposed approach selectively fine-tunes a pretrained GPT-2 model by freezing all layers except the final Transformer block, the final layer normalization, and a lightweight classification head. This selective approach reduces the number of trainable parameters by more than 94% compared to full fine-tuning. The model is evaluated on MIMIC-IV radiology reports using uncertainty-aware CheXpert-style labels, with performance measured through accuracy and ROC analysis. The computational efficiency gains are validated alongside classification performance metrics.

## Key Results
- Achieves up to 96.23% accuracy on aggregate disease outcome prediction
- Reduces trainable parameters by over 94% compared to full fine-tuning
- Demonstrates effective learning without overfitting through training dynamics and ROC analyses

## Why This Works (Mechanism)
The approach works by leveraging the rich semantic representations learned during GPT-2's pretraining while minimizing parameter updates to reduce computational cost. By freezing most of the model and updating only the final layers and classification head, the method preserves the general language understanding capabilities while adapting to the specific classification task. This selective fine-tuning strategy balances the need for task-specific adaptation with computational efficiency.

## Foundational Learning
- **Transformer Architecture**: The backbone of GPT-2; needed to understand how attention mechanisms work in the model
- **Fine-tuning vs. Full Training**: Understanding when and how to adapt pretrained models; needed to grasp the computational efficiency benefits
- **CheXpert Uncertainty Labels**: Medical imaging labeling scheme with uncertainty handling; needed to understand the evaluation framework
- **Layer Freezing Strategy**: Technique for reducing trainable parameters; needed to comprehend the efficiency gains
- **Classification Head Design**: How classification is implemented on top of language models; needed to understand the output mechanism

## Architecture Onboarding
- **Component Map**: Input Text -> GPT-2 Base (mostly frozen) -> Final Transformer Block -> Layer Normalization -> Classification Head -> Output
- **Critical Path**: The path from input through the final Transformer block to the classification head is the most important for task performance
- **Design Tradeoffs**: Balance between freezing layers for efficiency vs. allowing adaptation for task-specific performance
- **Failure Signatures**: Overfreezing may lead to poor task adaptation; underfreezing negates computational benefits
- **First Experiments**:
  1. Compare full fine-tuning vs. selective fine-tuning on a small validation set
  2. Measure parameter reduction achieved by the selective approach
  3. Evaluate classification performance across different disease categories

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to radiology reports with CheXpert-style uncertainty labels
- May not generalize to other clinical note types or disease categories
- Performance metrics focus on accuracy without extensive precision-recall trade-offs

## Confidence
- **High confidence**: Computational efficiency gains and parameter reduction claims
- **Medium confidence**: Classification performance metrics on MIMIC-IV radiology reports
- **Low confidence**: Generalizability to other clinical domains, languages, or institutions

## Next Checks
1. Evaluate the selective fine-tuning approach on non-radiology clinical notes from MIMIC-IV and external EHR datasets
2. Conduct ablation studies comparing freeze-based selective fine-tuning against full fine-tuning and adapter-based methods
3. Test model robustness to distribution shifts by evaluating performance on radiology reports from different institutions or time periods