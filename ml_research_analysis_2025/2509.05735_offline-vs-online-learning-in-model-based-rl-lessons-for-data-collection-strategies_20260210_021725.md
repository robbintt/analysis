---
ver: rpa2
title: 'Offline vs. Online Learning in Model-based RL: Lessons for Data Collection
  Strategies'
arxiv_id: '2509.05735'
source_url: https://arxiv.org/abs/2509.05735
tags:
- environment
- agent
- data
- world
- steps
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the performance gap between online and
  offline model-based reinforcement learning (MBRL) agents. While MBRL is often assumed
  to be robust to offline training due to its task-agnostic dynamics learning, we
  find that offline agents still suffer from significant performance degradation compared
  to online agents, primarily due to encountering out-of-distribution (OOD) states
  at test time.
---

# Offline vs. Online Learning in Model-based RL: Lessons for Data Collection Strategies

## Quick Facts
- **arXiv ID:** 2509.05735
- **Source URL:** https://arxiv.org/abs/2509.05735
- **Reference count:** 40
- **Primary result:** Offline MBRL agents suffer significant performance degradation compared to online agents due to encountering out-of-distribution states, but this can be mitigated through exploration data or adaptive interaction.

## Executive Summary
This study investigates the performance gap between online and offline model-based reinforcement learning (MBRL) agents, specifically focusing on DreamerV3. While MBRL is often assumed to be robust to offline training due to its task-agnostic dynamics learning, the authors find that offline agents still suffer from significant performance degradation compared to online agents. This occurs because offline agents lack the self-correction mechanism of online agents, leading to a mismatch between imagined and real rollouts when encountering out-of-distribution (OOD) states. The study demonstrates that incorporating exploration data—either through mixed task-exploration rewards or minimal additional online interactions—can significantly improve offline agent performance.

## Method Summary
The study compares Active (online) agents with Passive (offline) agents using the same DreamerV3 architecture but different data collection strategies. Passive agents train on static replay buffers from Active agents, while Tandem agents replay the exact batch sequence used during Active training. To address offline degradation, the authors implement two remedies: (1) incorporating exploration data collected via Plan2Explore's ensemble disagreement rewards, and (2) an adaptive interaction strategy that triggers environment interaction when world model loss exceeds a threshold. The key metric for the adaptive agent is the OOD ratio, calculated as world model loss on evaluation trajectories divided by loss on training data. Performance is evaluated across 31 environments from DMC, Metaworld, and MinAtar suites.

## Key Results
- Offline agents experience significant performance degradation compared to online agents due to OOD state encounters
- Mixed task-exploration datasets outperform pure task-oriented or pure exploration datasets in mitigating OOD issues
- An adaptive interaction strategy achieves similar performance to online agents with only 6% additional interactions
- Training solely on expert demonstrations exacerbates OOD problems, highlighting the need for broader state-space coverage

## Why This Works (Mechanism)

### Mechanism 1: The "Catastrophic Cycle" of Imagination-Reality Mismatch
Offline model-based agents fail because they lack a self-correction loop, causing policies to exploit model errors and drift into out-of-distribution (OOD) states during deployment. In DreamerV3, the policy is trained inside the world model's "imagination." If the offline dataset has limited state coverage, the world model hallucinates in unvisited regions. Without an active feedback loop (self-correction), the policy learns to exploit these hallucinations. During evaluation, this compromised policy steers the agent into novel states where the model is inaccurate, leading to compounding errors and failure.

### Mechanism 2: Remediation via State-Space Coverage (Exploration Data)
Performance degradation is mitigated by training on datasets that maximize state-space coverage, specifically through mixed task-exploration rewards. Pure task-oriented data covers only a narrow "ribbon" of the state space (successful trajectories). If the agent deviates slightly, it enters an OOD region. Adding exploration data (via ensemble disagreement rewards) expands the world model's "support," ensuring that even if the policy deviates, the model remains accurate enough to recover or guide the agent correctly.

### Mechanism 3: Targeted Recovery via Adaptive Interaction
An offline agent can recover online-level performance with minimal interaction (~6%) by adaptively querying the environment only when its internal world model loss indicates an OOD state. The agent monitors an "OOD ratio"—the world model loss on current evaluation trajectories versus the loss on training data. If this ratio exceeds a threshold (e.g., 1.35), the agent briefly interacts with the environment (2k steps) to update its model. This acts as a targeted "self-correction" mechanism, patching holes in the world model only where necessary.

## Foundational Learning

- **Concept: Recurrent State-Space Models (RSSM)**
  - **Why needed here:** You must understand how DreamerV3 constructs its "imagination" (latent dynamics) to grasp why a mismatch between imagined and real rollouts is fatal to the policy.
  - **Quick check question:** Can you explain how the world model predicts the next latent state ($z_t, h_t$) and how the policy uses this for planning without real environment steps?

- **Concept: Distributional Shift in Offline RL**
  - **Why needed here:** The core failure mode is the policy visiting states outside the training distribution. Understanding distributional shift is required to diagnose why "Passive" agents fail even when trained on "Active" data.
  - **Quick check question:** Why does a policy trained on a static dataset of expert trajectories potentially visit states *not* contained in that dataset?

- **Concept: Actor-Critic in Model-Based Settings**
  - **Why needed here:** The paper notes that the critic (value function) overestimates values due to model errors. Understanding the actor-critic loop explains the "blind updates" that degrade the policy.
  - **Quick check question:** In DreamerV3, does the critic learn from real rewards or imagined rewards? (Answer: Imagined, hence the vulnerability to model error).

## Architecture Onboarding

- **Component map:**
  - World Model (RSSM) -> Encodes observations $x_t$ into latent state $s_t$ and predicts dynamics
  - Actor/Critic -> Trained *inside* the latent dynamics of the World Model
  - Replay Buffer -> Stores experience
  - OOD Monitor -> (For Adaptive Agent) Calculates the ratio of eval-loss vs. buffer-loss

- **Critical path:**
  1. Data Collection: Active agent interacts; Passive agent consumes static buffer
  2. Model Training: RSSM learns $p(s_{t+1}|s_t, a_t)$
  3. Imagination: Actor generates rollouts *using* the RSSM
  4. Policy Update: Critic evaluates imagined returns; Actor updates to maximize them
  5. Failure Trigger (Passive): Policy exploits model inaccuracies -> Steers to OOD state -> Real rollout fails

- **Design tradeoffs:**
  - Pure Offline vs. Adaptive: Pure offline is safe/cheap but fails on complex tasks. Adaptive requires ~6% interaction budget but restores performance
  - Task vs. Exploration Data: Pure task data is efficient for learning the goal but brittle (low coverage). Mixed data (task + exploration bonus) is robust but introduces a hyperparameter ($w_{expl}$) and potentially noisier data

- **Failure signatures:**
  - High World Model Loss on Eval: A clear indicator the agent is visiting states the model cannot predict
  - Value Overestimation: The Critic predicts high reward while the actual return is low (Imagination-Reality mismatch)
  - Drift: The agent appears to act randomly or repeats sub-optimal actions, unable to return to the "familiar" state distribution

- **First 3 experiments:**
  1. Reproduce the "Tandem" Baseline: Train two DreamerV3 agents on the exact same data stream (one Active, one Tandem) to confirm that performance degrades solely due to the lack of the feedback loop (initialization variance)
  2. Ablate Data Composition: Train Passive agents on three dataset types: Pure Task, Pure Exploration, and Mixed ($w_{task}=0.5, w_{expl}=0.5$). Measure state-space coverage vs. final return
  3. Implement Adaptive Interaction: Build the "OOD Ratio" monitor. Run a Passive agent that triggers a 2k-step interaction burst only when the eval-loss/buffer-loss ratio > 1.35. Compare total interaction steps vs. performance recovery

## Open Questions the Paper Calls Out

- **Question:** Can a purely offline measure effectively detect OOD states to trigger adaptive data collection without requiring online evaluation rollouts?
  - **Basis in paper:** The authors note regarding their adaptive interaction strategy: "However, our method still requires evaluation rollouts. An offline measure would be desirable and is left for future research."
  - **Why unresolved:** The current adaptive strategy relies on computing an OOD ratio using world model loss from on-policy evaluation episodes, which necessitates environment interaction.
  - **What evidence would resolve it:** A metric derived strictly from the static dataset and model weights that correlates strongly with the OOD ratio threshold used for triggering self-generated data.

- **Question:** Do the degradation dynamics and remedies identified in DreamerV3 transfer to other model-based or model-free RL architectures?
  - **Basis in paper:** The conclusion states: "We plan to extend our experiments to other RL methods... to identify optimal data collection strategies."
  - **Why unresolved:** The study is conducted exclusively using DreamerV3; it is unverified whether the coupling between world models and policies behaves similarly in other architectures like TD-MPC2 or model-free agents.
  - **What evidence would resolve it:** Replication of the Active vs. Passive performance gap and the success of exploration data remedies across a diverse set of non-DreamerV3 baselines.

- **Question:** Can the exploration bonus weight ($w_{expl}$) be dynamically adjusted during training to remove the need for task-specific hyperparameter tuning?
  - **Basis in paper:** Section 4.1 states: "A downside of this approach is the introduction of the hyperparameter $w_{expl}$, the optimal value of which can depend on the specific task."
  - **Why unresolved:** The paper demonstrates that different tasks require different $w_{expl}$ values for optimal performance but does not propose a method to automate this selection.
  - **What evidence would resolve it:** An adaptive algorithm that sets $w_{expl}$ based on runtime statistics (e.g., model uncertainty or visitation entropy) and consistently achieves performance comparable to the optimal fixed setting across all tasks.

## Limitations
- The adaptive interaction strategy's 6% interaction claim relies on specific threshold tuning (1.35 ratio) that may not generalize across different environments or task complexities
- The study assumes DreamerV3 as the representative MBRL architecture, which may not generalize to other world model approaches
- The ensemble disagreement reward mechanism for exploration data lacks specification of ensemble size and architecture details

## Confidence
- **High Confidence:** The core mechanism explaining offline MBRL degradation (imagination-reality mismatch leading to OOD state exploitation) is well-supported by empirical evidence across multiple environments
- **Medium Confidence:** The effectiveness of exploration data in mitigating OOD issues is demonstrated but relies on Plan2Explore implementation details that aren't fully specified
- **Medium Confidence:** The adaptive interaction strategy's 6% interaction claim is based on specific threshold tuning (1.35 ratio) that may not generalize across different environments or task complexities

## Next Checks
1. **Threshold Sensitivity Analysis:** Systematically vary the OOD ratio threshold (1.35) across a range of values to determine the robustness of the adaptive interaction strategy and identify optimal thresholds for different environment types
2. **Ensemble Architecture Specification:** Replicate the Plan2Explore exploration data generation with varying ensemble sizes (2, 4, 8 heads) to quantify the impact of ensemble complexity on state-space coverage and final performance
3. **Cross-Architecture Generalization:** Test the offline degradation mechanism and mitigation strategies (exploration data, adaptive interaction) on alternative MBRL architectures like MuZero or PlaNet to assess whether the findings extend beyond DreamerV3