---
ver: rpa2
title: 'Towards Fundamental Language Models: Does Linguistic Competence Scale with
  Model Size?'
arxiv_id: '2509.02225'
source_url: https://arxiv.org/abs/2509.02225
tags:
- factual
- knowledge
- linguistic
- competence
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates whether linguistic competence in language
  models scales with model size, proposing a new paradigm called Fundamental Language
  Models (FLMs) that separate linguistic ability from factual knowledge. The authors
  evaluate models ranging from 135M to 32B parameters across three dimensions: linguistic
  competence (lexical, grammatical, semantic), external factual knowledge, and internal
  factual knowledge.'
---

# Towards Fundamental Language Models: Does Linguistic Competence Scale with Model Size?

## Quick Facts
- **arXiv ID:** 2509.02225
- **Source URL:** https://arxiv.org/abs/2509.02225
- **Authors:** Jaime Collado-Montañez; L. Alfonso Ureña-López; Arturo Montejo-Ráez
- **Reference count:** 19
- **Primary result:** Linguistic competence stabilizes at moderate model sizes (5-7B parameters) while internal factual knowledge scales more efficiently with size.

## Executive Summary
This paper investigates whether linguistic competence in language models scales with model size, proposing a new paradigm called Fundamental Language Models (FLMs) that separate linguistic ability from factual knowledge. The authors evaluate models ranging from 135M to 32B parameters across three dimensions: linguistic competence (lexical, grammatical, semantic), external factual knowledge, and internal factual knowledge. Results show that while both linguistic competence and factual knowledge improve with scale, internal factual knowledge grows significantly faster, indicating that model size is more closely tied to memorization than core language ability. Linguistic competence stabilizes at moderate model sizes (5-7B parameters) with diminishing returns beyond this point. These findings support FLMs as a viable approach, suggesting that smaller, linguistically proficient models can serve as foundations for tool-augmented systems, potentially leading to more efficient, interpretable, and sustainable NLP solutions.

## Method Summary
The study evaluates pre-trained models ranging from 135M to 32B parameters using LM Evaluation Harness in zero-shot mode across 12 benchmarks: linguistic tasks (WiC, BLiMP, RTE, MNLI, QQP), external knowledge tasks (LAMBADA, BoolQ, COPA, MultiRC, ReCoRD), and internal knowledge tasks (TriviaQA, TruthfulQA). Models are aggregated into Small, Medium, and Large groups based on parameter quartiles. Performance is measured by averaging scores within each competence category, with statistical analysis using regression slopes and Mann-Whitney U tests to compare scaling behaviors.

## Key Results
- Linguistic competence stabilizes at moderate model sizes (5-7B parameters) with diminishing returns beyond this point
- Internal factual knowledge scales significantly faster than linguistic competence (slope ratio of 2:1)
- Models show distinct scaling patterns for different types of knowledge, supporting the FLM paradigm

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Internal factual memorization scales more efficiently with parameter count than linguistic competence.
- **Mechanism:** As model capacity increases, parameters disproportionately act as storage buffers for specific data points (internal knowledge) rather than improving the processing of structural rules (linguistic competence). The paper finds the regression slope for internal factual knowledge (0.059) is twice as steep as for linguistic competence (0.029).
- **Core assumption:** Benchmarks like TriviaQA validly isolate "memorization," while BLiMP and WiC validly isolate "linguistic structure" independent of world knowledge.
- **Evidence anchors:** Section 4.1.1 shows $R^2 = 0.81$ for internal factual knowledge vs. size, compared to $R^2 \approx 0.50$ for linguistic competence.

### Mechanism 2
- **Claim:** Linguistic competence saturates at a moderate parameter threshold (approx. 5B–7B).
- **Mechanism:** Learning syntax and grammar follows a bounded complexity curve. Once a model has sufficient capacity to represent the grammatical rules of a language, additional parameters yield diminishing returns for pure language processing tasks.
- **Core assumption:** The selected benchmarks (BLiMP, RTE) cover the sufficient scope of "linguistic competence" defined by CEFR.
- **Evidence anchors:** Section 4.1.2 shows Mann-Whitney U tests indicate no statistically significant gain in linguistic competence between Medium and Large models (p=0.062).

### Mechanism 3
- **Claim:** Decoupling language processing from knowledge retrieval enables efficient, modular architectures (FLMs).
- **Mechanism:** By arresting model growth at the linguistic saturation point and offloading facts to external tools, the system avoids the "parameter tax" of memorization. The model acts as a semantic processor/router rather than an encyclopedia.
- **Core assumption:** External retrieval mechanisms (RAG) can match or exceed the speed/accuracy of internal parametric recall for the targeted facts.
- **Evidence anchors:** The paper's categorization of tasks into Linguistic, External Factual, and Internal Factual to test this decoupling.

## Foundational Learning

- **Concept: CEFR-Aligned Competencies**
  - **Why needed here:** The paper grounds its definition of "language skill" not in AI benchmarks, but in the human-centric Common European Framework of Reference (CEFR).
  - **Quick check question:** Can you distinguish between *lexical* competence (vocabulary usage) and *semantic* competence (meaning/inference) as defined in Section 1?

- **Concept: Parametric vs. Non-Parametric Knowledge**
  - **Why needed here:** The core argument relies on the difference between knowledge stored in weights (parametric/internal) and knowledge retrieved from context (non-parametric/external).
  - **Quick check question:** Does a model answering a question using a provided prompt demonstrate "Internal Factual Knowledge" or "External Factual Knowledge"?

- **Concept: Scaling Laws & Diminishing Returns**
  - **Why needed here:** Interpreting the results requires understanding that performance does not scale linearly with size; different capabilities (facts vs. grammar) have different scaling exponents.
  - **Quick check question:** If doubling model size improves fact recall by 20% but grammar accuracy by only 2%, which capability is driving the scaling?

## Architecture Onboarding

- **Component map:** FLM Core (3B–7B params) -> Tool Interface -> External Knowledge Store
- **Critical path:** Identifying the "Efficiency Frontier." You must benchmark models in the 1B–10B range to find the specific point where *grammatical* performance plateaus but before *factual* storage begins to dominate the parameter count.
- **Design tradeoffs:**
  - **Size vs. Factual Recall:** A 32B model hallucinates less on trivia than a 7B model, but the 7B model is sufficient for sentence parsing. The tradeoff is between "standalone capability" and "deployment cost/interpretability."
  - **Assumption:** The paper suggests Qwen2.5 and Llama-3.1 have different "efficiency frontiers" based on training data quality, meaning architecture is not the only variable.
- **Failure signatures:**
  - **The "Knowledge Gap":** An FLM deployed without tools will sound fluent but will confidently lie about facts (higher hallucination rate than a monolithic LLM of equivalent size).
  - **Context Blindness:** If the FLM has poor "External Factual Knowledge" (reasoning over context), it cannot use the tools effectively even if it retrieves the right documents.
- **First 3 experiments:**
  1. **Saturation Point Identification:** Run the BLiMP (grammar) benchmark across 4 model sizes (e.g., 1B, 3B, 7B, 13B) to verify the plateau effect in your specific architecture.
  2. **Competency Decoupling Test:** Fine-tune a 7B model on a dataset stripped of factual entities (only structure/syntax) and compare its linguistic score against a baseline to see if "forgetting" facts hurts grammar.
  3. **Hybrid Integration:** Connect a 3B model (FLM candidate) to a standard RAG pipeline and compare factual accuracy against a 13B standalone model to validate if the FLM+Tools paradigm closes the performance gap.

## Open Questions the Paper Calls Out

- **Open Question 1:** Does the stabilization of linguistic competence at moderate sizes hold for morphologically rich or non-English languages?
  - **Basis in paper:** The authors explicitly note in the Limitations and Conclusion that the study focuses primarily on English and that the size-competence relationship could vary significantly for languages with different syntactic structures.
  - **Why unresolved:** The empirical evidence for the 5–7B parameter "sweet spot" is currently restricted to English benchmarks, leaving the cross-linguistic generalizability of the FLM paradigm unproven.
  - **What evidence would resolve it:** Replicating the evaluation suite (BLiMP, WiC, etc.) on diverse, morphologically rich languages to determine if the scaling curves shift.

- **Open Question 2:** To what extent does the entanglement of world knowledge and language understanding limit FLM performance on tasks involving metaphors or cultural references?
  - **Basis in paper:** The Limitations section highlights that understanding metaphors, cultural references, and domain-specific terminology often inherently requires world knowledge that cannot be cleanly separated from linguistic competence.
  - **Why unresolved:** The paper assumes a clean separation is possible for core tasks, but does not quantify the performance degradation when linguistic nuance relies on internalized facts.
  - **What evidence would resolve it:** Targeted evaluation of FLMs on datasets specifically designed to test metaphor comprehension and cultural nuance, comparing performance against models with high internal factual knowledge.

- **Open Question 3:** Does the computational overhead of external tool retrieval negate the efficiency gains achieved by reducing the model size in the FLM paradigm?
  - **Basis in paper:** While the paper argues for FLMs as a path toward "efficient" and "sustainable" NLP, it evaluates competencies using static benchmarks rather than measuring the latency or resource costs of the necessary external retrieval mechanisms.
  - **Why unresolved:** The trade-off between a smaller model's faster inference and the potential latency introduced by querying external knowledge bases in a live system remains unmeasured.
  - **What evidence would resolve it:** System-level benchmarks comparing end-to-end latency and energy consumption of a tool-augmented 7B FLM against a monolithic 70B model performing the same tasks.

## Limitations
- The separation between linguistic competence and factual knowledge may not be as clean as the benchmarks suggest, with linguistic tasks potentially requiring world knowledge and factual tasks requiring linguistic processing
- Results are based on English language models and may not generalize to morphologically rich or low-resource languages
- The study relies on zero-shot evaluation and does not explore how few-shot prompting might shift the efficiency frontier

## Confidence
- **High Confidence:** The empirical finding that linguistic competence shows diminishing returns beyond 5-7B parameters is well-supported by statistical analysis (Mann-Whitney U tests, R² values) and is consistent across multiple linguistic benchmarks.
- **Medium Confidence:** The claim that internal factual knowledge scales more efficiently than linguistic competence rests on the assumption that the selected benchmarks validly separate these constructs. While the statistical evidence is strong (slope ratio of 2:1), the conceptual separation is more debatable.
- **Low Confidence:** The broader architectural claims about FLMs as a sustainable paradigm require real-world deployment validation. The paper provides theoretical justification and benchmark evidence, but the practical tradeoffs of tool-augmented systems versus monolithic models remain largely untested.

## Next Checks
1. **Construct Validity Assessment:** Conduct a factor analysis on the benchmark results to empirically verify that linguistic and factual tasks form distinct clusters, strengthening the claim that we're measuring fundamentally different capabilities.
2. **Cross-Linguistic Scaling Study:** Evaluate the same scaling patterns on multilingual models across 3-5 diverse languages (e.g., English, Chinese, Arabic, Finnish, Swahili) to test whether the 5-7B saturation point for linguistic competence is language-specific or universal.
3. **Tool Integration Performance Test:** Implement a prototype FLM system (3-7B parameters) with RAG-based factual retrieval and compare it against monolithic models of equivalent size on a mixed linguistic-factual benchmark, measuring not just accuracy but latency and computational efficiency.