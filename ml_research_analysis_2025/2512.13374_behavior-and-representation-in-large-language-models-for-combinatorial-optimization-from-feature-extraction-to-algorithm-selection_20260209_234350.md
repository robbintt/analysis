---
ver: rpa2
title: 'Behavior and Representation in Large Language Models for Combinatorial Optimization:
  From Feature Extraction to Algorithm Selection'
arxiv_id: '2512.13374'
source_url: https://arxiv.org/abs/2512.13374
tags:
- representation
- mean
- instance
- features
- problem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how Large Language Models (LLMs) represent
  combinatorial optimization problems and whether their internal representations can
  support algorithm selection. Using direct querying and probing analyses across four
  benchmark problems, we assess whether LLMs can extract instance features and whether
  their hidden-layer activations encode meaningful structural information.
---

# Behavior and Representation in Large Language Models for Combinatorial Optimization: From Feature Extraction to Algorithm Selection

## Quick Facts
- arXiv ID: 2512.13374
- Source URL: https://arxiv.org/abs/2512.13374
- Reference count: 40
- Primary result: LLM-derived representations achieve comparable predictive power to traditional ISA features for per-instance algorithm selection

## Executive Summary
This study investigates how Large Language Models (LLMs) represent combinatorial optimization problems and whether their internal representations can support algorithm selection. Using direct querying and probing analyses across four benchmark problems, we assess whether LLMs can extract instance features and whether their hidden-layer activations encode meaningful structural information. Results show that LLMs can reliably infer simple features but struggle with complex computations. Probing reveals that complex features are more accurately decoded from internal representations than through direct querying. Notably, LLM-derived representations achieve comparable predictive power to traditional ISA-based features for per-instance algorithm selection, suggesting that LLMs capture relevant structural information despite limited explicit reasoning ability.

## Method Summary
The study evaluates LLMs' ability to extract combinatorial optimization instance features via direct querying and encode structural information in hidden layers via probing, extending to per-instance algorithm selection. Experiments use four COPs (Bin Packing, Graph Coloring, Job Shop Scheduling, Knapsack) with instances from MATILDA/ISA studies, represented in three formats: standard DIMACS-like, MiniZinc code-like, and natural language. The Llama-3.2-3B-Instruct model is used with RTF prompts and JSON-structured decoding to extract features. Activations from the last layer are pooled using mean/max/last methods, then regression probes (LinearRegression, MLP, LightGBM) are trained to predict ground-truth features. Algorithm selection performance is evaluated using set-aware accuracy with 70/30 train-validation splits and stratified 5-fold CV.

## Key Results
- LLMs reliably infer simple features through direct querying but struggle with complex computations
- Probing internal representations achieves better accuracy for complex features than direct querying
- LLM-derived representations match the predictive power of traditional ISA features for algorithm selection

## Why This Works (Mechanism)
The study leverages LLMs' ability to process multiple input representations of combinatorial optimization problems and extract latent structural information. Direct querying uses structured decoding to elicit feature values, while probing analyzes the hidden-layer activations to uncover encoded problem characteristics. The combination of representation diversity (DIMACS, MiniZinc, natural language) and probing methodology allows researchers to determine whether LLMs capture problem structure at a semantic level rather than just surface patterns.

## Foundational Learning
- **Combinatorial Optimization Problems (COPs)**: Classification of optimization problems where the solution space is discrete and finite, requiring systematic approaches to find optimal solutions
  - Why needed: Forms the domain context for feature extraction and algorithm selection
  - Quick check: Verify understanding of BPP, GCP, JSP, KP problem formulations

- **Algorithm Portfolios**: Sets of algorithms where the goal is to select the best-performing algorithm per instance rather than finding a single universal solver
  - Why needed: Provides the evaluation framework for measuring representation effectiveness
  - Quick check: Confirm understanding of per-instance vs. per-portfolio algorithm selection

- **Feature Extraction**: Process of deriving quantitative characteristics from problem instances that correlate with algorithmic performance
  - Why needed: Core task being evaluated for both direct querying and probing approaches
  - Quick check: Review the 10+ handcrafted features listed in Table 2

- **Model Probing**: Technique of training auxiliary models to decode information from neural network activations to understand what representations encode
  - Why needed: Alternative to direct querying for assessing latent representation quality
  - Quick check: Verify understanding of pooling methods and probe model architectures

## Architecture Onboarding

**Component Map**: DIMACS/MiniZinc/NL representations -> Llama-3.2-3B-Instruct -> Last-layer activations -> Pooling -> Probe models -> Feature prediction

**Critical Path**: Instance representation → LLM processing → Activation extraction → Pooling → Probe training → Feature prediction accuracy

**Design Tradeoffs**: Single model family (reproducibility vs. generalizability), last-layer only probing (simplicity vs. potential information loss), structured decoding (precision vs. complexity)

**Failure Signatures**: Context length exceeded (token overflow errors), structured decoding invalid JSON (schema mismatch), pooling shape mismatch (activation extraction errors)

**Three First Experiments**:
1. Direct querying of simple features (size, density) using RTF prompts with JSON schema-constrained decoding
2. Activation extraction and pooling for a small instance subset, verifying 3072-dimensional output shape
3. Train LinearRegression probe to predict a single ground-truth feature, comparing MAE against direct querying results

## Open Questions the Paper Calls Out
- Do other LLM architectures (beyond Llama-3.2-3B-Instruct) encode combinatorial optimization problem structures differently, and would larger models improve feature extraction or algorithm selection performance?
- Can structured reasoning or tool-augmented generation improve LLMs' direct querying performance on complex feature extraction tasks?
- What is the optimal balance between interpretability, computational cost, and predictive power when using LLM-derived representations versus handcrafted features in optimization pipelines?

## Limitations
- Study limited to single LLM family (Llama-3.2-3B-Instruct) and four benchmark COPs
- Only last-layer activations analyzed, potentially missing information in earlier layers
- Higher computational cost and lower interpretability compared to handcrafted features

## Confidence
- High: Experimental methodology clearly specified with appropriate metrics and robust evaluation (70/30 split, 5-fold CV)
- Medium: Comparable predictive power claim supported but limited to tested model and problems; natural language results need validation
- Low: Interpretation that LLMs capture relevant structural information extrapolates beyond measured metrics without establishing causality

## Next Checks
1. Cross-validate handcrafted features across all three representation formats on held-out subset to ensure preprocessing consistency
2. Extend probing experiments to intermediate layers to determine whether earlier layers encode different information types
3. Replicate experimental pipeline using both smaller (1B) and larger (8B+) LLMs to establish model-size dependency of representation capabilities