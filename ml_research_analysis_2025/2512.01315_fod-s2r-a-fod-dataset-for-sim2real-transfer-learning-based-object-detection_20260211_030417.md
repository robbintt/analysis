---
ver: rpa2
title: 'FOD-S2R: A FOD Dataset for Sim2Real Transfer Learning based Object Detection'
arxiv_id: '2512.01315'
source_url: https://arxiv.org/abs/2512.01315
tags:
- synthetic
- object
- dataset
- real
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FOD-S2R is a novel dataset for Foreign Object Debris (FOD) detection
  in aircraft fuel tanks, addressing the lack of annotated data for confined, enclosed
  environments. It combines 3,114 real images from a simulated fuel tank and 3,137
  synthetic images generated using Unreal Engine, covering 14 object classes with
  diverse lighting, occlusions, and object scales.
---

# FOD-S2R: A FOD Dataset for Sim2Real Transfer Learning based Object Detection

## Quick Facts
- **arXiv ID**: 2512.01315
- **Source URL**: https://arxiv.org/abs/2512.01315
- **Reference count**: 40
- **Primary result**: FOD-S2R dataset enables Sim2Real transfer learning for foreign object debris detection, achieving mAP50:95 = 0.740 on real fuel tank images through synthetic pretraining and real-world fine-tuning.

## Executive Summary
FOD-S2R addresses the critical shortage of annotated data for foreign object debris detection in confined aircraft fuel tank environments. The dataset combines 3,114 real images from a simulated fuel tank and 3,137 synthetic images generated in Unreal Engine, covering 14 object classes with diverse lighting, occlusions, and scales. By enabling Sim2Real transfer learning, the dataset allows object detection models to train on synthetic data and adapt to real-world conditions, significantly reducing the need for extensive manual annotations. Experiments demonstrate that models like RF-DETR achieve substantial performance improvements through synthetic pretraining followed by real-world fine-tuning.

## Method Summary
The FOD-S2R dataset was constructed using a two-pronged approach: real images captured from a physical fuel tank replica using a Canon DSLR, and synthetic images generated in Unreal Engine 5 from a Blender CAD model. Annotations were automated through a hybrid pipeline - initial manual annotations trained a YOLOv11 model to generate synthetic annotations, while Unreal Engine's blueprint scripting system automatically created bounding boxes for synthetic data. The dataset provides 70-10-20 train/val/test splits, with images tiled to 640×640 pixels for consistent input dimensions. Sim2Real transfer learning experiments involved pretraining on synthetic data followed by fine-tuning on real images using Detectron2 framework with ResNet-50 backbone.

## Key Results
- RF-DETR achieved mAP50:95 = 0.740 after Sim2Real adaptation, outperforming real-only training approaches.
- Sim2Real transfer learning improved detection performance by approximately 15% compared to training on real data alone.
- Small object detection (mAP_S) improved significantly from 0.383 to 0.676 when using synthetic pretraining followed by real fine-tuning.
- The dataset covers 14 object classes with 3,114 real images and 3,137 synthetic images, providing sufficient diversity for robust training.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Synthetic pretraining followed by real-world fine-tuning improves detection accuracy over real-only training in confined environments.
- Mechanism: Synthetic data provides wider distribution coverage across lighting, occlusion, and object configurations; fine-tuning on real samples then converges on domain-specific features without overfitting to limited real annotations.
- Core assumption: The synthetic environment captures sufficient geometric and texture fidelity to transfer useful spatial priors, even if surface irregularities differ from real fuel tanks.
- Evidence anchors:
  - [abstract] "Experiments on anchor-based and anchor-free object detection models... show improved detection accuracy when using synthetic pretraining followed by real-world fine-tuning. For instance, RF-DETR achieved mAP50:95 = 0.740 after Sim2Real adaptation, outperforming real-only training."
  - [section VI] "Sim2Real transfer learning experiments demonstrate that integrating synthetic data improves detection performance by 15%"
  - [corpus] Weak direct corpus support for this specific FOD domain; neighbor papers (e.g., "Bridging the Sim2Real Gap: Vision Encoder Pre-Training") address Sim2Real in robotics but not fuel-tank inspection.
- Break condition: If synthetic textures lack high-frequency surface irregularities (grease stains, scratches) present in real fuel tanks, pretraining may fail to provide useful priors, reducing or negating performance gains.

### Mechanism 2
- Claim: Training order matters—synthetic-first → real fine-tuning outperforms real-first → synthetic fine-tuning.
- Mechanism: Early exposure to diverse synthetic variations builds scale-invariant and occlusion-robust features; subsequent real fine-tuning refines these on domain-specific textures. Reversing the order causes the model to overfit to limited real features, then lose them when exposed to synthetic data lacking those textures.
- Core assumption: The model retains synthetic-learned representations through fine-tuning without catastrophic forgetting.
- Evidence anchors:
  - [section V-B] "When pretrained on synthetic samples and fine-tuned on real images, the model achieved its highest performance... mAP50:95 = 0.740... small-object performance also improved significantly from mAP S = 0.383 to mAP S = 0.676."
  - [section V-B] "Reversing the adaptation sequence (real-first, synthetic-second) resulted in weaker performance, with mAP50:95 dropping to 0.712"
  - [corpus] No direct corpus evidence on training order effects for this task; this appears underexplored in neighbors.
- Break condition: If real data volume increases substantially, the relative benefit of synthetic pretraining may diminish; optimal order could shift.

### Mechanism 3
- Claim: Automated annotation via initial model training reduces manual labeling effort while maintaining annotation quality.
- Mechanism: A small set of manual annotations trains an initial detector (YOLOv11), which generates bounding boxes for remaining images; human review corrects errors iteratively, improving both dataset quality and subsequent model rounds.
- Core assumption: The initial manually-annotated subset is sufficiently representative to bootstrap a reasonable detector.
- Evidence anchors:
  - [section III-A] "initial manual annotations were used to train a YOLOv11 object detection model, which subsequently generated annotations for the remainder of the dataset. Each model-generated annotation then underwent a rigorous iterative review process."
  - [section III-B] "To eliminate the need for manual annotation, the Unreal Engine blueprint scripting system was used to automatically generate bounding-box annotations."
  - [corpus] No corpus evidence on annotation quality validation for this specific pipeline.
- Break condition: If initial annotations contain systematic errors (e.g., consistent class confusion), the model will propagate and amplify these across the dataset.

## Foundational Learning

- Concept: **Anchor-based vs. anchor-free object detection**
  - Why needed here: The paper benchmarks both paradigms; understanding the distinction helps interpret results (e.g., why anchor-free YOLOv11 and transformer-based RF-DETR perform differently on small objects).
  - Quick check question: Can you explain why anchor-free methods might struggle differently with small vs. large objects compared to anchor-based methods?

- Concept: **Domain shift / Sim2Real gap**
  - Why needed here: The core problem is bridging the distribution difference between Unreal-rendered images and real fuel-tank photos; without this concept, the motivation for transfer learning is unclear.
  - Quick check question: What visual differences between synthetic and real fuel-tank images would most likely cause a detector trained only on synthetic data to fail on real images?

- Concept: **Transfer learning and fine-tuning strategies**
  - Why needed here: The paper's main intervention is pretraining on synthetic data then fine-tuning on real data; understanding learning rate schedules, layer freezing, and catastrophic forgetting is essential.
  - Quick check question: If you fine-tune all layers vs. only the final detection head, what tradeoffs do you expect in terms of overfitting vs. feature adaptation?

## Architecture Onboarding

- Component map:
  Synthetic data pipeline: Unreal Engine 5 -> Blender CAD model -> Blueprint scripting -> automatic bounding-box annotations -> 3,137 images
  Real data pipeline: Physical fuel-tank replica -> Canon DSLR capture -> manual + model-assisted annotation -> 3,114 images
  Training pipeline: 70/10/20 split -> 640×640 tiling -> ResNet-50 backbone (ImageNet pretrained) -> detector head (YOLOv11, RF-DETR, etc.) -> 100 epochs, lr=0.001
  Evaluation: COCO metrics (mAP50, mAP75, mAP50:95, scale-aware mAP S/M/L)

- Critical path:
  1. Generate synthetic data with sufficient variation (lighting, occlusion, object scale)
  2. Pretrain detector on synthetic data for full epochs
  3. Fine-tune on real data (not the reverse)
  4. Evaluate on held-out real test set using scale-aware metrics

- Design tradeoffs:
  - Synthetic resolution (12MP) vs. real (25MP): lower synthetic resolution may miss fine textures but speeds generation
  - Controlled synthetic variation vs. real-world unpredictability: synthetic covers more scenarios but lacks high-frequency surface irregularities
  - Full-model fine-tuning vs. head-only: full fine-tuning adapts better but risks overfitting to limited real data

- Failure signatures:
  - Large drop in mAP50:95 vs. mAP50 indicates poor localization precision (likely due to domain gap in texture/edge fidelity)
  - Low mAP S with high mAP L indicates small-object detection failure (common in confined spaces with occlusion)
  - Performance degradation when reversing training order (real→synthetic) suggests synthetic data lacks critical real-world features

- First 3 experiments:
  1. Baseline real-only training: Train RF-DETR or YOLOv11 on real data only; record mAP50, mAP50:95, mAP S to establish baseline
  2. Synthetic-only evaluation: Train on synthetic, test on real; quantify domain gap (expect 10–20% mAP drop per Table I)
  3. Sim2Real adaptation: Pretrain on synthetic, fine-tune on real; compare mAP S improvement (target: 0.383 → 0.676 as in paper) to validate transfer benefit

## Open Questions the Paper Calls Out

- Question: How does the detection performance on the controlled fuel tank replica translate to actual operational aircraft fuel tanks which may contain hazardous residues or different surface degradation patterns?
- Question: Can the models trained on this specific fuel tank geometry generalize to diverse internal aircraft structures (e.g., different wing sections or fuselage tanks)?
- Question: Can advanced domain adaptation techniques (beyond simple fine-tuning) further close the performance gap for small-scale debris (mAP_S) between synthetic and real domains?

## Limitations
- The dataset was constructed using a simulated fuel tank replica rather than actual operational aircraft fuel tanks, limiting real-world validation
- The model has not been tested on certified aircraft fuel tanks where fuel residue, grease, and structural differences may differ significantly from the replica
- The dataset does not contain multiple fuel tank designs, potentially causing overfitting to specific structural features of the studied tank

## Confidence
- **High**: Dataset construction pipeline (synthetic/real image generation, annotation automation) and general Sim2Real workflow (pretraining on synthetic, fine-tuning on real)
- **Medium**: Claim that training order (synthetic→real vs. real→synthetic) critically affects performance, supported by single experiment without extensive hyperparameter ablation
- **Low**: Claims about annotation quality and dataset generalization to other confined inspection domains, due to lack of cross-dataset validation

## Next Checks
1. **Domain Gap Quantification**: Train on synthetic only, evaluate on real; compare mAP50:95 drop to paper's Table I (10-20% gap) to verify reported domain shift
2. **Annotation Quality Audit**: Sample synthetic annotations and compare to human-reviewed versions; calculate precision/recall to estimate bootstrapping error rate
3. **Alternative Domain Adaptation**: Replace Sim2Real pretraining with domain adversarial training or style transfer; compare mAP50:95 to isolate contribution of pretraining vs. fine-tuning strategy