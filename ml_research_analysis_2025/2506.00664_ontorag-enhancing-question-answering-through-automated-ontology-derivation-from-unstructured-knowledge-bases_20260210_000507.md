---
ver: rpa2
title: 'OntoRAG: Enhancing Question-Answering through Automated Ontology Derivation
  from Unstructured Knowledge Bases'
arxiv_id: '2506.00664'
source_url: https://arxiv.org/abs/2506.00664
tags:
- ontology
- ontorag
- knowledge
- graph
- relationships
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: OntoRAG is an automated pipeline that transforms unstructured electrical
  relay documents into a queryable ontology to enhance question-answering. It integrates
  web scraping, PDF parsing, hybrid chunking, information extraction, knowledge graph
  construction, and ontology creation using LLMs and graph-based methods.
---

# OntoRAG: Enhancing Question-Answering through Automated Ontology Derivation from Unstructured Knowledge Bases

## Quick Facts
- arXiv ID: 2506.00664
- Source URL: https://arxiv.org/abs/2506.00664
- Reference count: 33
- OntoRAG achieves 85% comprehensiveness win rate against vector RAG and 75% against GraphRAG's best configuration

## Executive Summary
OntoRAG is an automated pipeline that transforms unstructured electrical relay documents into a queryable ontology to enhance question-answering. It integrates web scraping, PDF parsing, hybrid chunking, information extraction, knowledge graph construction, and ontology creation using LLMs and graph-based methods. The pipeline derives ontology classes and relationships from the knowledge graph using community detection and LLM-based property synthesis, preserving structural integrity and enabling multi-hop reasoning. Experimental results demonstrate OntoRAG's superiority over traditional vector RAG and GraphRAG, achieving a comprehensiveness win rate of 85% against vector RAG and 75% against GraphRAG's best configuration, while maintaining high diversity and comparable empowerment.

## Method Summary
OntoRAG processes electrical relay PDFs through a six-stage pipeline: web scraping via BeautifulSoup/Scrapy, PDF parsing with Unstructured and PyMuPDF (coordinate transformation with padding), hybrid/semantic chunking at Title elements with cosine similarity merging, information extraction via Gemini 2.5 Flash (NER, atomic facts, chunk graph JSON), knowledge graph construction via K-means-inspired clustering on name/definition embeddings, and ontology creation via Leiden community detection plus LLM property synthesis. Retrieval occurs across ontology levels (O0-O3) using similarity search with configurable context expansion. The system uses proprietary corpus of ~1M tokens (1600 chunks) and 125 generated sensemaking questions, evaluated via LLM-as-judge (Gemini 2.5 Flash) on comprehensiveness, diversity, empowerment, and directness metrics.

## Key Results
- Comprehensiveness win rate: 85% vs vector RAG, 75% vs GraphRAG
- Best-performing ontology level: O2 (balanced precision and recall)
- Runtime: 300 minutes for 1-million-token corpus on modest VM
- Directness tradeoff: SS achieves 92% directness; OntoRAG (O2) achieves 58%

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical community detection preserves ontological integrity
The Leiden algorithm partitions the knowledge graph into nested communities (O0→O3), then sub-community detection creates hierarchical IS-A/HAS-A relationships. This maintains semantic dependencies that GraphRAG's flat clustering fragments. Break condition: If documents lack inherent hierarchical structure, community detection may impose artificial hierarchies that reduce retrieval precision.

### Mechanism 2: Hybrid chunking with semantic boundary detection improves entity-relationship extraction
Title-based chunk boundaries combined with cosine distance thresholds between sentence embeddings create contextually coherent "First Retrieval Granular Units." This prevents mid-entity splits that would fragment atomic facts. Break condition: If embedding model fails to capture domain-specific semantics, semantic chunking may merge unrelated content or split related content.

### Mechanism 3: LLM-based property synthesis from aggregated intra-community properties produces generalizable class definitions
Properties from all entities within a community are aggregated, then Gemini 2.5 Flash synthesizes generalized properties, handling synonyms/variations. This replaces manual ontology curation. Break condition: If source documents contain contradictions or errors, LLM synthesis may produce inconsistent or hallucinated class properties.

## Foundational Learning

- **Knowledge Graph Construction (Entity-Relationship Extraction)**
  - Why needed: The entire pipeline depends on extracting accurate (subject, predicate, object) triples from chunks. Understanding NER, disambiguation, and entity resolution is prerequisite.
  - Quick check: Can you explain why entity resolution (mapping "relay" and "electrical relay" to the same canonical entity) matters for graph quality?

- **Community Detection Algorithms (Leiden/Louvain)**
  - Why needed: The ontology hierarchy emerges from Leiden partitioning. Understanding modularity optimization helps diagnose why certain clusters form.
  - Quick check: What happens to community quality if the graph has many weak edges (low-confidence relationships)?

- **RAG Retrieval Scoring and Metrics**
  - Why needed: Evaluating OntoRAG requires understanding comprehensiveness, diversity, empowerment, and directness metrics used in GraphRAG comparisons.
  - Quick check: Why might a system score high on comprehensiveness but low on directness?

## Architecture Onboarding

- Component map: Web Scraping → PDF Parsing (Unstructured + PyMuPDF) → Chunking (Hybrid + Semantic) → Information Extraction (Gemini 2.5 Flash: NER, atomic facts, chunk graph JSON) → Knowledge Graph Construction (entity clustering via embedding similarity) → Ontology Creation (Leiden community detection → property synthesis → hierarchical sub-communities) → Retrieval (query key element extraction → ontology class similarity search → chunk retrieval)

- Critical path: **Entity clustering → Community detection** — Errors here propagate to ontology structure. If clustering merges unrelated entities or splits related ones, all downstream retrieval suffers.

- Design tradeoffs:
  - Granularity vs abstraction: O0 (root) offers breadth; O3 offers specificity. O2 performed best in experiments.
  - Computational cost vs scalability: 300 minutes for 1M tokens on modest VM; impractical for real-time without optimization.
  - Directness vs comprehensiveness: SS wins directness (92%); OntoRAG wins comprehensiveness (88%). Hybrid retrieval may balance both.

- Failure signatures:
  - Empty communities: Leiden produces clusters with no meaningful properties → check entity extraction quality upstream.
  - Redundant classes: Multiple communities with similar names → embedding similarity thresholds may be too permissive.
  - Missing relationships: Query retrieval finds no matching ontology classes → check query key element extraction or ontology level configuration.

- First 3 experiments:
  1. Chunking sensitivity: Run pipeline with different semantic distance thresholds (0.3, 0.5, 0.7) and measure impact on atomic fact extraction quality.
  2. Community level sweep: Query test set at all four levels (O0-O3), plot comprehensiveness vs directness to find domain-optimal level.
  3. Entity clustering threshold ablation: Vary θname and θdef (Eq. 5) to observe effects on ontology class count and redundancy.

## Open Questions the Paper Calls Out

- Can OntoRAG maintain its performance advantages when scaling to corpora significantly larger than 1 million tokens?
  - Basis: Authors state ontology creation is computationally intensive (300 minutes for 1M tokens) and impractical for larger corpora or real-time applications.
  - Resolution: Evaluation on 10M+ token corpora with runtime and performance metrics compared to baselines.

- How effectively does OntoRAG generalize to domains beyond electrical relay documentation without extensive prompt engineering?
  - Basis: Authors acknowledge OntoRAG's reliance on domain-specific prompts reduces generalizability across diverse domains.
  - Resolution: Cross-domain evaluation on at least 3 distinct domains (medical, legal, financial) with minimal prompt adaptation.

- Can hybrid retrieval strategies combining OntoRAG's global sensemaking with local retrieval improve directness scores?
  - Basis: Results show SS achieved 92% directness while OntoRAG achieved only 58%; authors suggest OntoRAG may benefit from local retrieval strategies for specific queries.
  - Resolution: Hybrid system evaluation showing improved directness scores while maintaining comprehensiveness and diversity.

## Limitations
- Evaluation methodology relies entirely on LLM-as-a-judge without human annotation validation
- Proprietary corpus prevents independent verification and limits generalizability
- Lacks detailed prompt specifications for critical LLM interactions, making exact reproduction challenging
- Computational cost (300 minutes for 1M tokens) makes it impractical for larger corpora or real-time applications

## Confidence

- **High Confidence**: Technical pipeline architecture and implementation details (hybrid chunking, Leiden community detection, knowledge graph construction)
- **Medium Confidence**: Superiority claims given LLM-judge methodology and proprietary data constraints
- **Low Confidence**: Generalizability to non-technical domains or different document structures, as hierarchical community detection assumes inherent document hierarchy

## Next Checks

1. **Human validation study**: Have domain experts manually evaluate a subset of OntoRAG vs GraphRAG answers to verify LLM-judge comprehensiveness scores align with human judgment.

2. **Cross-domain reproducibility**: Apply OntoRAG to a non-technical corpus (e.g., news articles, legal documents) and measure whether hierarchical community detection still improves retrieval over flat clustering.

3. **Prompt transparency test**: Re-implement OntoRAG with open-source LLMs (e.g., LLaMA, Mistral) using publicly documented prompts to assess whether performance depends critically on proprietary Gemini 2.5 Flash capabilities.