---
ver: rpa2
title: Simulation-Driven Reinforcement Learning in Queuing Network Routing Optimization
arxiv_id: '2507.18795'
source_url: https://arxiv.org/abs/2507.18795
tags:
- agent
- network
- reward
- learning
- queueing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors developed a simulation-driven reinforcement learning
  framework to optimize routing in complex queueing networks, particularly for manufacturing
  and communication systems. The framework uses Deep Deterministic Policy Gradient
  (DDPG) combined with Dyna-style planning (Dyna-DDPG), employing separate predictive
  models for next-state transitions and rewards to improve stability and sample efficiency.
---

# Simulation-Driven Reinforcement Learning in Queuing Network Routing Optimization

## Quick Facts
- arXiv ID: 2507.18795
- Source URL: https://arxiv.org/abs/2507.18795
- Reference count: 4
- Developed simulation-driven reinforcement learning framework using DDPG with Dyna-style planning for queueing network routing optimization

## Executive Summary
This paper presents a simulation-driven reinforcement learning framework for optimizing routing in complex queueing networks, specifically targeting manufacturing and communication systems. The framework combines Deep Deterministic Policy Gradient (DDPG) with Dyna-style planning (Dyna-DDPG), using separate predictive models for next-state transitions and rewards to improve stability and sample efficiency. A flexible simulation environment models diverse queueing scenarios, disruptions, and unpredictable conditions, enabling the agent to learn continuous routing policies that adapt to network changes including server breakdowns.

## Method Summary
The framework employs DDPG with Dyna-style planning to learn optimal routing policies in queueing networks. It uses separate predictive models for state transitions and rewards, trained on simulated experiences to improve sample efficiency. The simulation environment allows modeling of various queueing scenarios with different network topologies, arrival rates, and service distributions. The agent learns to route jobs through the network by selecting the best available server at each node, adapting to changing conditions and disruptions.

## Key Results
- Effective learning of robust routing policies that scale to networks up to 100 nodes
- Successful adaptation to network disruptions including server breakdowns
- Standard deviation in agent decisions serves as a novel performance metric
- Demonstrated sample efficiency improvements through Dyna-style planning

## Why This Works (Mechanism)
The framework's effectiveness stems from combining model-free reinforcement learning with model-based planning. By maintaining separate predictive models for transitions and rewards, the agent can simulate experiences and learn from them without requiring additional real interactions with the environment. This Dyna-style approach enhances sample efficiency while maintaining the flexibility of deep reinforcement learning to handle continuous action spaces and complex state representations typical in queueing networks.

## Foundational Learning
- Queueing theory fundamentals: Understanding arrival processes, service distributions, and network topology is essential for modeling realistic scenarios
- Why needed: Provides the theoretical foundation for designing appropriate simulation environments and reward structures
- Quick check: Verify that basic queueing metrics (throughput, wait times) match analytical solutions in simple cases

- Reinforcement learning basics: DDPG algorithm mechanics including actor-critic architecture, experience replay, and target networks
- Why needed: Core algorithm driving policy learning and optimization
- Quick check: Confirm stable learning curves and policy improvement over training episodes

- Dyna-style planning: Integration of model-based and model-free learning approaches
- Why needed: Enables more efficient learning by leveraging simulated experiences
- Quick check: Compare sample efficiency with and without Dyna planning component

## Architecture Onboarding

Component map:
Environment simulator -> State/reward predictors -> DDPG agent -> Policy execution -> Environment simulator

Critical path:
Simulation environment generates states → Predictors model transitions and rewards → DDPG agent learns policy → Agent selects actions → Environment updates state

Design tradeoffs:
- Separate vs. joint prediction models: Separate models provide better stability but require more parameters
- Continuous vs. discrete actions: Continuous actions offer finer control but increase exploration complexity
- Centralized vs. distributed architecture: Single agent simplifies coordination but may limit scalability

Failure signatures:
- Predictor model collapse: Manifests as unstable learning or divergence
- Experience replay imbalance: Shows as biased policy updates or poor generalization
- Hyperparameter sensitivity: Visible through inconsistent performance across runs

First experiments:
1. Test basic DDPG learning on simple single-server queue
2. Validate predictor models on known transition dynamics
3. Evaluate sample efficiency gains from Dyna-style planning

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can the agent learn robust policies when multiple servers experience simultaneous breakdowns?
- Basis in paper: The authors state in Section 0.9 that current blockage exploration "only expose[s] the agent to scenarios where only one server is being blocked" and that "future studies are needed" for multiple server scenarios.
- Why unresolved: The current training methodology is restricted to single-node failures, leaving the agent's resilience to complex, multi-node disruption patterns unknown.
- What evidence would resolve it: Experimental results showing policy convergence and throughput maintenance in simulations featuring concurrent failures at multiple servers.

### Open Question 2
- Question: How does the framework perform under non-Markovian interarrival and service time distributions?
- Basis in paper: The conclusion identifies adapting the framework to "models of greater complexity which feature non-Markovian distributions" as a crucial scope for additional development.
- Why unresolved: The current environment relies on Markovian assumptions (e.g., exponential service times), which may not hold in all real-world manufacturing or communication systems.
- What evidence would resolve it: Benchmarking the Dyna-DDPG agent's stability and convergence speed using datasets with general distributions (e.g., deterministic or heavy-tailed distributions).

### Open Question 3
- Question: Would employing multi-agent reinforcement learning (MARL) improve optimization for complex network configurations?
- Basis in paper: The conclusion suggests extending the package to support multi-agent reinforcement learning to "facilitate the exploration of more complex configurations."
- Why unresolved: The current system utilizes a single centralized agent, which may face scalability limitations or training instability as the state-action space grows exponentially in larger networks.
- What evidence would resolve it: A comparative analysis of convergence times and routing efficiency between the current single-agent architecture and a MARL architecture on large-scale networks.

## Limitations
- The simulation environment may not fully capture the complexity and variability of real-world queueing networks
- The agent's ability to generalize to completely novel network topologies beyond tested configurations remains uncertain
- Standard deviation in agent decisions as a performance metric requires further validation against traditional metrics like throughput and average wait times

## Confidence
- Framework effectiveness and sample efficiency: Medium (Strong experimental evidence in simulation, but real-world validation needed)
- Robustness to disruptions: Medium (Demonstrated in simulation, but complexity of real-world disruptions may differ)
- Scalability: Medium (Proven up to 100 nodes, but computational requirements for larger networks unclear)

## Next Checks
1. Test the framework on a real-world manufacturing or communication network to validate simulation results
2. Conduct ablation studies to quantify the contribution of the separate predictive models for next-state transitions and rewards
3. Evaluate the framework's performance under different job arrival distributions and routing constraints not covered in the original experiments