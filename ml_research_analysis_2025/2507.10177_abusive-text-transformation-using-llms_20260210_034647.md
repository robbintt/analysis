---
ver: rpa2
title: Abusive text transformation using LLMs
arxiv_id: '2507.10177'
source_url: https://arxiv.org/abs/2507.10177
tags:
- text
- abusive
- groq
- gemini
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluates Large Language Models (LLMs) for transforming\
  \ abusive text into non-abusive versions while preserving meaning and sentiment.\
  \ Using datasets from Twitter and Reddit, we compare four LLMs\u2014GPT-4o, DeepSeek,\
  \ Gemini, and Groq\u2014on their ability to detect and transform abusive content."
---

# Abusive text transformation using LLMs

## Quick Facts
- **arXiv ID**: 2507.10177
- **Source URL**: https://arxiv.org/abs/2507.10177
- **Reference count**: 40
- **Key outcome**: GPT-4o and DeepSeek achieve 99% and 98.5% transformation success rates while maintaining semantic similarity to original text

## Executive Summary
This study evaluates four Large Language Models (GPT-4o, DeepSeek, Gemini, and Groq) for transforming abusive text from Twitter and Reddit datasets into non-abusive versions while preserving meaning and sentiment. The evaluation reveals significant performance differences among models, with GPT-4o and DeepSeek demonstrating superior transformation capabilities and semantic preservation. Sentiment analysis shows all models effectively reduce negative sentiment post-transformation, while semantic analysis indicates model-specific approaches to text detoxification, ranging from minor edits to extensive rephrasing.

## Method Summary
The study employs a comparative evaluation framework using datasets from Twitter and Reddit containing abusive content. Four LLMs are tested for their ability to detect and transform abusive text while maintaining semantic similarity and reducing negative sentiment. The evaluation measures transformation success rates, sentiment scores before and after transformation, and pairwise semantic similarity between original and transformed text. The analysis focuses on English-language content and examines model-specific approaches to text detoxification.

## Key Results
- GPT-4o and DeepSeek achieve the highest transformation success rates at 99% and 98.5% respectively
- All models effectively reduce negative sentiment post-transformation, with Groq producing the most optimistic outputs
- Semantic similarity analysis shows GPT-4o and DeepSeek maintain 80% pairwise similarity with original text

## Why This Works (Mechanism)
The study demonstrates that LLMs can effectively identify and transform abusive content through pattern recognition and contextual understanding. Models leverage their training on diverse text corpora to recognize abusive language patterns and generate appropriate non-abusive alternatives while maintaining semantic coherence. The transformation process involves both content modification and sentiment adjustment, with models employing different strategies ranging from minor edits to extensive rephrasing depending on their architectural characteristics and training approaches.

## Foundational Learning
- **Semantic similarity metrics**: Essential for measuring meaning preservation during text transformation; quick check: compare cosine similarity scores across different transformation approaches
- **Sentiment analysis techniques**: Critical for evaluating emotional tone changes; quick check: verify sentiment score distributions before and after transformation
- **LLM-based text generation**: Core mechanism for producing non-abusive alternatives; quick check: analyze generation patterns across different model architectures
- **Abusive content detection**: Foundational for identifying target text for transformation; quick check: validate detection accuracy on diverse abusive language samples
- **Cross-model comparison methodology**: Necessary for evaluating relative performance; quick check: ensure consistent evaluation criteria across all tested models

## Architecture Onboarding
**Component map**: Input text -> Abusive content detection -> Text transformation -> Sentiment analysis -> Semantic similarity evaluation -> Output assessment

**Critical path**: The transformation pipeline requires accurate abusive content detection as the prerequisite for effective transformation, followed by semantic preservation verification and sentiment analysis to ensure comprehensive detoxification.

**Design tradeoffs**: Models must balance between aggressive content removal (risking meaning loss) and conservative edits (risking incomplete detoxification). The study shows different models prioritize these tradeoffs differently, with GPT-4o and DeepSeek favoring meaning preservation while Groq opts for more extensive rephrasing.

**Failure signatures**: Performance degradation occurs when models encounter complex contextual abuse, ambiguous language, or cultural references. The study notes that models struggle with sarcasm and context-dependent abusive content, potentially leading to false positives or negatives.

**First experiments**: 1) Test model performance on benchmark abusive text datasets, 2) Compare transformation outputs with human-annotated non-abusive alternatives, 3) Evaluate cross-cultural and multilingual performance on abusive content

## Open Questions the Paper Calls Out
None

## Limitations
- The evaluation relies on synthetic datasets without clear documentation of abusive text generation process
- Semantic similarity metric of 80% suggests significant text alterations that may affect meaning preservation claims
- The study does not address potential bias amplification or over-sanitization concerns
- Comparison focuses on English-language models without examining cross-linguistic performance

## Confidence
- **High confidence**: Transformation success rates (99%, 98.5%) for GPT-4o and DeepSeek
- **Medium confidence**: Sentiment analysis showing reduced negativity across all models
- **Medium confidence**: Semantic similarity measurements indicating meaning preservation
- **Low confidence**: Claims about optimal model selection for specific use cases

## Next Checks
1. Test model performance on real-world abusive text samples from multiple platforms and languages to verify generalizability
2. Conduct human evaluation studies comparing original, transformed, and control texts for meaning preservation and contextual appropriateness
3. Implement adversarial testing with deliberately crafted abusive content designed to bypass model detection mechanisms