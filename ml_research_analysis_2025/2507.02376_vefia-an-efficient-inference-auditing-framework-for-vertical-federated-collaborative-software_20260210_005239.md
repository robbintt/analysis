---
ver: rpa2
title: 'VeFIA: An Efficient Inference Auditing Framework for Vertical Federated Collaborative
  Software'
arxiv_id: '2507.02376'
source_url: https://arxiv.org/abs/2507.02376
tags:
- inference
- vefia
- data
- software
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'VeFIA is the first framework to audit inference correctness in
  Vertical Federated Learning. It uses a dual execution path: untrusted inference
  by the data party and trusted inference via a TEE-coordinator pipeline, enabling
  statistical validation without adding online latency.'
---

# VeFIA: An Efficient Inference Auditing Framework for Vertical Federated Collaborative Software

## Quick Facts
- arXiv ID: 2507.02376
- Source URL: https://arxiv.org/abs/2507.02376
- Reference count: 40
- Primary result: First framework to audit inference correctness in Vertical Federated Learning with 99.99% detection probability when anomalies exceed 5.4%, 100% PPV/TPR/NPV, 72.9% privacy leakage reduction, and 0.56% latency overhead

## Executive Summary
VeFIA introduces the first framework for auditing inference correctness in Vertical Federated Learning (VFL) without adding online latency. The framework establishes a dual execution path: an untrusted path where the data party performs inference and a trusted path using TEE-coordinator pipelines for validation. By partitioning models into shallow (TEE) and deep (coordinator) components and employing confidential random sampling, VeFIA detects execution anomalies with 99.99% probability when anomaly rates exceed 5.4%. The approach also reduces privacy leakage by 72.9% through mutual information minimization during training.

## Method Summary
VeFIA creates a parallel trusted execution path using TEE-COO (Trusted Execution Environment - Coordinator) pipeline to audit inference correctness in VFL. The data party's model is partitioned into a shallow, privacy-sensitive part running in the TEE and a deep, computation-intensive part running on the coordinator. During inference, the task party compares results from the untrusted inference path with results from the trusted path. To maintain efficiency, VeFIA uses statistically grounded confidential sampling to validate a subset of inferences rather than checking every single one. The framework also incorporates privacy-aware training with mutual information minimization to limit information leakage from intermediate representations shared with the coordinator.

## Key Results
- Achieves 100% precision, recall, and negative predictive value in sampling validation
- Detects execution anomalies with 99.99% probability when anomaly rate exceeds 5.4%
- Reduces privacy leakage by 72.9% through information-theoretic bounds
- Adds only 0.56% latency overhead on average through optimal sampling ratio calculation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: VeFIA achieves efficient and privacy-preserving auditing of inference correctness in Vertical Federated Learning (VFL) by establishing a dual execution path (untrusted and trusted) and statistically comparing their results.
- Mechanism: The framework creates a parallel trusted execution path using a TEE-COO (Trusted Execution Environment - Coordinator) pipeline. The data party's model is partitioned into a shallow, privacy-sensitive part running in the TEE, and a deep, computation-intensive part running on the coordinator. During inference, the task party compares the results from the standard untrusted inference path with results from the trusted path. To maintain efficiency, the task party uses a statistically grounded confidential sampling strategy to validate a subset of inferences rather than checking every single one.
- Core assumption: The TEE provides genuine hardware-isolated execution that the data party cannot tamper with. The coordinator is semi-honest (follows protocol but may try to infer data). Anomalies are systemic and affect a significant portion (e.g., >5.4%) of inferences, making them detectable via sampling.
- Evidence anchors:
  - [abstract] "The core of VeFIA is that the task party can use the inference results from a framework with Trusted Execution Environments (TEE) and the coordinator to validate the correctness of the data party's computation results."
  - [abstract] "VeFIA guarantees that, as long as the abnormal inference exceeds 5.4%, the task party can detect execution anomalies in the inference software with a probability of 99.99%, without incurring any additional online inference latency."
  - [section 4.5] "VeFIA employs a confidential random sampling mechanism. P_t secretly selects a small, randomized subset of inference requests for validation via the trusted TEE-COO path."
  - [corpus] Related papers like "PRIVEE" and "PBM-VFL" acknowledge that VFL is susceptible to feature inference attacks during prediction, validating the problem VeFIA addresses, though they propose different defense mechanisms.
- Break condition: The mechanism fails if the TEE's integrity is compromised (e.g., by side-channel attacks), if the coordinator is fully malicious and colludes with the data party, or if the data party can predict which samples will be audited.

### Mechanism 2
- Claim: VeFIA maintains data privacy during outsourced computation on the coordinator by using privacy-aware training to limit information leakage from intermediate representations.
- Mechanism: During the VFL training phase, a perturbation is learned and added to the output of the shallow model (running in the TEE) before it is sent to the coordinator. This perturbation is optimized to minimize the mutual information between the original private input data and the perturbed intermediate representation shared with the coordinator. This creates an information-theoretic privacy bound, making it harder for the coordinator to reconstruct the original data via model inversion attacks.
- Core assumption: The mutual information neural estimation accurately captures and minimizes the statistical dependency. The learned perturbation provides robust protection against a range of inversion attacks without destroying the representation's utility for the main task.
- Evidence anchors:
  - [abstract] "...reduces privacy leakage by 72.9%..."
  - [section 4.3.1] "VeFIA introduces the concept of an information-theoretic privacy budget. We leverage mutual information to precisely measure the statistical dependency between the private input data x_d and the intermediate representation z_d."
  - [section 5.5] "VeFIA reduces the similarity between original inputs and reconstructed data by 72.9%, demonstrating its efficacy in mitigating information exposure risks."
  - [corpus] Multiple related papers (e.g., PRIVEE, PBM-VFL) frame feature inference attacks as a key threat in VFL, providing context for why this specific privacy mechanism is necessary.
- Break condition: The mechanism fails if the mutual information minimization is insufficient to stop a more advanced inversion attack, or if the noise added to protect privacy degrades model accuracy below an acceptable level.

### Mechanism 3
- Claim: VeFIA ensures the authenticity of the software artifacts (data and models) loaded into the trusted execution path at runtime using a TEE-based integrity validation protocol.
- Mechanism: Before the VFL process begins, cryptographic hashes of the approved dataset and trained model weights are computed and securely stored inside the TEE. During inference, whenever artifacts are loaded for the trusted audit path, their hashes are recomputed and compared against these stored "golden" values. A mismatch indicates that the data party is attempting to load incorrect or tampered data/models into the trusted path, triggering an alert and preventing a false audit.
- Core assumption: The initial "golden" hash represents the correct and trustworthy state of the data and model. The cryptographic hash function is collision-resistant.
- Evidence anchors:
  - [section 4.4] "...VeFIA implements a hash consistency validation protocol that functions as a runtime assertion, validating the integrity of data and model artifacts before their use in the trusted inference path."
  - [section 4.4.1] "Before the VFL process begins... the TEE on P_d's machine computes a cryptographic hash signature of the entire dataset... securely managed by the TEE, making it inaccessible to P_d."
  - [section 4.4.2] "Upon successful validation, the TEE computes a cryptographic hash of the model weights... For each subsequent inference query that is part of the audit, VeFIA loads the model weights into the TEE and re-computes their hash."
  - [corpus] This is a standard TEE technique. While no corpus paper describes this exact runtime artifact validation for VFL auditing, it is a foundational concept in TEE-based systems literature.
- Break condition: The mechanism fails if the data party can tamper with the data or model *before* the initial hash is computed and stored, or if a hash collision can be forged.

## Foundational Learning

### Concept: Trusted Execution Environments (TEEs, e.g., Intel SGX)
- Why needed here: TEEs are the core hardware primitive that provides the isolated, secure environment required to execute the trusted part of the model and the validation logic. Without understanding their basic guarantees (confidentiality, integrity, attestation) and limitations (side-channel risks, limited EPC memory, performance overhead), one cannot reason about VeFIA's security assumptions and performance.
- Quick check question: What prevents the data party (who controls the machine with the TEE) from simply reading the memory of the secure enclave while it's processing?

### Concept: Vertical Federated Learning (VFL) Architecture & Threat Model
- Why needed here: It's essential to grasp the roles of the Task Party (P_t), Data Party (P_d), and Coordinator (C), and how features and models are split across them. The problem VeFIA solves is specific to this architecture where an untrusted participant performs essential computation. One must understand the "semi-honest" assumption for the coordinator.
- Quick check question: In the VFL setting described, who holds the labels and who holds the complementary features? What does "semi-honest" mean for the coordinator?

### Concept: Information-Theoretic Privacy (Mutual Information)
- Why needed here: The paper's defense against the coordinator reconstructing data is rooted in minimizing mutual information. Understanding that this is a statistical bound on how much information one variable reveals about another is key to evaluating the strength of the privacy protection.
- Quick check question: Does minimizing the mutual information between the original data and the shared intermediate representation guarantee perfect secrecy? If not, what tradeoff does it represent?

## Architecture Onboarding

### Component map
Task Party (P_t) -> TEE on Data Party (P_d) -> Coordinator (C) -> Task Party (P_t)
Task Party (P_t) -> Data Party (P_d) -> Coordinator (C) -> Task Party (P_t)

### Critical path
1. **Training (Stage 2):** Standard VFL training + VeFIA's *privacy-aware training* to learn perturbation (σ).
2. **Deployment:** Model split. Shallow part + σ -> P_d's TEE. Deep part -> C. Hashes of data/model stored in TEE.
3. **Inference (Stage 3):**
   a. **Untrusted Path:** P_d processes full batch, sends result to C -> P_t.
   b. **Trusted Path:** P_t sends a confidentially sampled subset of IDs to P_d's TEE.
   c. **TEE Validation:** TEE validates hashes; if valid, runs inference (shallow model + σ). Output sent to C -> P_t.
   d. **Audit:** P_t compares results from the two paths.

### Design tradeoffs
- **Privacy vs. Model Utility:** Minimizing mutual information via perturbation can reduce model accuracy. The hyperparameter λ balances this.
- **Detection Rate vs. Latency:** Sampling ratio (W) determines audit coverage. VeFIA optimizes W* to match untrusted path latency, masking overhead.
- **Security vs. Cost/Complexity:** Requires specific TEE hardware and a semi-honest coordinator.

### Failure signatures
- **High False Positive Rate:** TEE path results consistently differ from untrusted path on normal data. Could indicate numerical precision mismatches or a bug in hash validation.
- **Privacy Leak:** Successful model inversion by the coordinator, indicated by high SSIM/HR scores on reconstructed data.
- **No Anomalies Detected Despite Suspicion:** Sampling ratio (W) is too low to catch a low-level anomaly rate (K), or the data party has found a way to predict which samples will be audited.

### First 3 experiments
1. **Baseline Latency Profiling:** Measure T_un (untrusted inference time) and T_tr (trusted TEE-COO inference time) on target hardware. Use these values to calculate the theoretical optimal sampling ratio W* (Eq. 4).
2. **Hash Validation Test:** Attempt to load a subtly different model checkpoint or a modified dataset sample. Verify the system correctly flags this as an integrity violation.
3. **End-to-End Anomaly Detection:** Inject a known percentage of anomalies (e.g., K=5%, 10%) into the data party's untrusted inference. Run VeFIA with the calculated W* and verify the Detection Success Rate (DSR) matches theoretical prediction (Eq. 3).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can VeFIA be extended to audit the integrity of the training phase (Stage 2) and preprocessing phase (Stage 1) in VFL?
- Basis in paper: [explicit] The authors state, "Future work should aim to extend our paradigm across the entire VFL lifecycle," specifically including mechanisms to audit Stage 1 and Stage 2.
- Why unresolved: The current design and evaluation focus exclusively on the inference phase (Stage 3), leaving the integrity of data alignment and model training unverified.
- Evidence to resolve: A protocol extension that validates data provenance during preprocessing and gradient/parameter integrity during distributed backpropagation.

### Open Question 2
- Question: What automated response or self-healing mechanisms can be integrated into VeFIA upon detecting an anomaly?
- Basis in paper: [explicit] The authors propose future work on "automated anomaly response and system resilience capabilities" to move from simple detection to "self-healing" systems.
- Why unresolved: Currently, VeFIA flags inconsistencies but does not define or implement protocols to isolate non-compliant parties, roll back states, or trigger diagnostics automatically.
- Evidence to resolve: A system design that includes automated rollback protocols or isolation logic triggered immediately upon TEE-COO validation failures.

### Open Question 3
- Question: Can VeFIA operate effectively if the Coordinator (C) is malicious or colludes with a data party, rather than being semi-honest?
- Basis in paper: [inferred] The Threat Model assumes C is "semi-honest" and "will not collude with P_t and P_d." However, the Future Work section explicitly mentions exploring domains where C is "less trusted."
- Why unresolved: The privacy-aware training relies on C acting honestly; a malicious C could potentially bypass the mutual information bounds or manipulate the deep model computation to corrupt the audit trail.
- Evidence to resolve: An evaluation of VeFIA's detection and privacy guarantees under a malicious Coordinator threat model, or a modified protocol that removes the requirement for a semi-honest C.

## Limitations

- VeFIA relies on TEE hardware availability and is vulnerable to potential side-channel attacks against SGX implementations
- The framework requires careful hyperparameter tuning (λ for privacy-utility tradeoff, W for sampling ratio) for optimal performance
- Detection guarantees depend on anomalies being systemic rather than isolated incidents affecting <5.4% of inferences

## Confidence

- **High Confidence:** The framework's core architecture and dual-execution-path design are well-specified and technically sound. The claim of 100% PPV, TPR, and NPV for sampling validation is directly supported by experimental results.
- **Medium Confidence:** Privacy leakage reduction (72.9%) is supported by SSIM/HR metrics, but the effectiveness against sophisticated inversion attacks beyond the tested scenarios remains uncertain. The theoretical optimal sampling ratio calculation is sound but may require adjustment in real-world conditions.
- **Medium Confidence:** The pipeline acceleration mechanism's claimed 4.38× speedup is supported by benchmarks, but the extent of benefit may vary with different hardware configurations and model architectures.

## Next Checks

1. **Side-Channel Vulnerability Assessment:** Conduct comprehensive testing of the TEE implementation against known SGX side-channel attacks (cache timing, page faults, etc.) to verify that the secure execution path cannot be compromised through timing or memory access patterns.

2. **Dynamic Threat Modeling:** Implement and test scenarios where the coordinator becomes malicious and attempts to collude with the data party, evaluating whether VeFIA's detection mechanisms remain effective when multiple parties are compromised simultaneously.

3. **Real-World Performance Benchmarking:** Deploy VeFIA on production-scale VFL workloads with heterogeneous hardware configurations to validate that the theoretical latency optimizations (W* calculation, pipeline acceleration) translate to consistent performance gains in practical settings with variable network conditions and computational loads.