---
ver: rpa2
title: Event Classification of Accelerometer Data for Industrial Package Monitoring
  with Embedded Deep Learning
arxiv_id: '2506.05435'
source_url: https://arxiv.org/abs/2506.05435
tags:
- data
- class
- dataset
- precision
- package
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces an embedded deep learning pipeline for classifying
  accelerometer data to monitor industrial packages in real time. The method addresses
  imbalanced multi-class time series classification using a one-dimensional convolutional
  neural network (CNN) on IoT devices.
---

# Event Classification of Accelerometer Data for Industrial Package Monitoring with Embedded Deep Learning

## Quick Facts
- arXiv ID: 2506.05435
- Source URL: https://arxiv.org/abs/2506.05435
- Reference count: 23
- Method achieves 94.54% precision on "Forklift" events and 95.83% precision on "Truck" events while reducing model size by 4× to 14 kB

## Executive Summary
This work introduces an embedded deep learning pipeline for classifying accelerometer data to monitor industrial packages in real time. The method addresses imbalanced multi-class time series classification using a one-dimensional convolutional neural network (CNN) on IoT devices. Data augmentation with ADASYN improves classification performance by oversampling challenging minority samples. Adaptive thresholding enhances prediction reliability, while compression techniques (pruning, quantization, Huffman coding) reduce the model size by a factor of four to 14 kB without significant accuracy loss. The optimized CNN achieves 94.54% precision on "Forklift" events and 95.83% precision on "Truck" events, with low power consumption of 316 mW during inference, enabling multi-year device operation on limited battery resources.

## Method Summary
The method employs a 1D CNN architecture to classify tri-axial accelerometer data for industrial package monitoring. Data augmentation with ADASYN addresses class imbalance by oversampling minority samples near decision boundaries. Adaptive thresholding routes low-confidence predictions to a "Dummy" class to reduce false positives and energy consumption. The model undergoes deep compression including 50% L1-norm pruning with weight rewinding, INT8 quantization using a representative dataset, and Huffman coding. The compressed model is deployed on an ESP32 microcontroller using ESP-IDF, achieving 94.54% precision on "Forklift" events and 95.83% precision on "Truck" events while maintaining 316 mW power consumption during inference.

## Key Results
- Achieves 94.54% precision on "Forklift" events and 95.83% precision on "Truck" events
- Reduces model size by 4× from 51 kB to 14 kB using pruning, quantization, and Huffman coding
- Maintains 316 mW power consumption during ESP32 inference with 27 ms inference duration
- Improves minority class precision through ADASYN data augmentation compared to SMOTE baseline

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ADASYN-based data augmentation improves classification of minority classes in imbalanced time series by focusing synthetic sample generation near decision boundaries.
- Mechanism: Unlike uniform oversampling (SMOTE), ADASYN adaptively generates more synthetic samples for minority instances that are difficult to classify—specifically those with many majority-class neighbors. This concentrates learning capacity on ambiguous regions where class boundaries overlap, improving the model's discriminative ability where it matters most.
- Core assumption: Assumption: Minority samples near majority samples represent the most challenging decision boundary cases, and strengthening these regions improves generalization.
- Evidence anchors:
  - [section II-E2] "It focuses on minority data that are difficult to learn, by increasing minority data samples that are close to majority ones."
  - [section III-C, Table I] ADASYN achieved 98.05% precision (Forklift) vs SMOTE's 97.95%; Truck improved from 94.86% to 95.75% precision.
  - [corpus] Limited direct corpus support for ADASYN on time-series; neighbor papers focus on embedded inference efficiency rather than augmentation strategies.
- Break condition: If minority samples near majority boundaries are noise rather than meaningful edge cases, ADASYN will amplify noise and degrade performance.

### Mechanism 2
- Claim: Adaptive thresholding on softmax outputs reduces energy consumption by routing uncertain predictions to a "Dummy" class, avoiding costly retry operations.
- Mechanism: After softmax, if the maximum class probability falls below a tuned threshold, the sample is assigned to "Dummy" (representing misclassified or irrelevant events). This allows the system to prioritize high-confidence predictions, reducing false positives that would trigger energy-intensive verification procedures.
- Core assumption: Assumption: The energy cost of additional verification operations for false positives exceeds the energy cost of occasionally misclassifying low-confidence samples as "Dummy."
- Evidence anchors:
  - [abstract] "Adaptive thresholding enhances prediction reliability."
  - [section II-D] "In case of a bad prediction, some algorithms can be implemented to perform corrections. However, this produces a relatively important number of operations, hence high energy consumption."
  - [corpus] MOTION paper (neighbor) supports on-device motion recognition with latency/energy tradeoffs, but does not explicitly address thresholding strategies.
- Break condition: If thresholds are set too conservatively, too many valid predictions route to "Dummy," increasing overall system latency from repeated sampling.

### Mechanism 3
- Claim: Deep compression (pruning + INT8 quantization + Huffman coding) reduces model footprint by 4× without significant accuracy loss, enabling deployment on SRAM-constrained microcontrollers.
- Mechanism: L1-norm unstructured pruning removes 50% of low-magnitude weights; weight rewinding retrains remaining weights to recover accuracy. INT8 quantization converts 32-bit floats to 8-bit integers using a representative calibration dataset. Huffman coding exploits sparsity from pruning, encoding frequent zero-weight values with fewer bits.
- Core assumption: Assumption: Pruned low-magnitude weights contribute minimally to network output; quantization error from INT8 approximation remains within tolerance for this classification task.
- Evidence anchors:
  - [section III-E1, Table II] Model compressed from 51 kB to 14 kB; precision dropped only from 98.05% to 94.54% (Forklift) and 95.75% to 95.83% (Truck).
  - [section III-E3] "Power consumption of 316 mW during inference for the pruned and quantized model, with an inference duration of 27 ms."
  - [corpus] "Send Less, Save More" (neighbor) benchmarks embedded CNN inference vs. data transmission energy, supporting compression as an energy-saving strategy.
- Break condition: If quantization introduces systematic bias in softmax outputs, threshold-based routing may misroute predictions unexpectedly.

## Foundational Learning

- **Class imbalance in time-series classification**
  - Why needed here: The dataset has extreme imbalance (9,912 Dummy vs. 2,520,000 Truck samples). Standard loss functions and accuracy metrics become misleading, and models may default to predicting the majority class.
  - Quick check question: Given a dataset with 100× more Truck samples than Forklift samples, would accuracy alone reliably indicate model performance? Why or why not?

- **1D CNN architectures for sensor time series**
  - Why needed here: The input is tri-axial accelerometer data (Vl × 3 axes). 1D convolutions capture temporal patterns (vibration signatures) while preserving the relationship across time steps—unlike 2D spatial convolutions intended for images.
  - Quick check question: Why would a 1D kernel sliding over time be more appropriate for accelerometer signals than a 2D kernel treating time and axis as spatial dimensions?

- **Quantization-aware deployment tradeoffs**
  - Why needed here: The paper quantizes to INT8 using a representative dataset. Engineers must understand that quantization can shift decision boundaries, especially for confidence-based thresholding systems.
  - Quick check question: If your model's softmax outputs shift by ±0.05 after INT8 quantization, how would you adjust your classification threshold to maintain the same precision-recall tradeoff?

## Architecture Onboarding

- **Component map:**
  - Input layer: Tri-axial accelerometer data at 20 Hz, shaped (Vl, 3), where Vl is configurable time window length
  - 1D Conv layers + BatchNorm + ReLU: Feature extraction from temporal vibration patterns
  - Fully connected + Softmax: Maps features to class probabilities (Forklift, Truck, Dummy)
  - Threshold gate: Routes low-confidence predictions (below class-specific thresholds) to Dummy
  - Compression pipeline: PyTorch → ONNX → TFLite (pruned + quantized) → C++ source (ESP-IDF) → ESP32 SRAM

- **Critical path:**
  1. Data collection → ADASYN augmentation → Class-balanced training set
  2. Training with weighted CrossEntropy + label smoothing (0.1) → SGD optimizer (lr=0.5, momentum=0.9)
  3. Pruning (50%) with weight rewinding → Fine-tune remaining weights
  4. Quantization (INT8) using 100-sample representative dataset
  5. Threshold tuning via precision-recall curves → Deploy to ESP32

- **Design tradeoffs:**
  - **Precision vs. Recall:** Higher thresholds increase precision but lower recall, potentially requiring more re-sampling events.
  - **Model size vs. Accuracy:** Aggressive pruning/quantization reduces footprint but may shift decision boundaries; paper shows ~3% precision drop acceptable.
  - **Wake time vs. Battery life:** Inference draws 316 mW for 27 ms; baseline is 300 mW. Minimizing wake events is critical for multi-year operation.

- **Failure signatures:**
  - **Precision collapse on minority class:** Likely indicates ADASYN failed to generate meaningful boundary samples, or threshold is too low.
  - **Model size not shrinking after pruning + Huffman:** Check that pruning is unstructured (not channel-wise), and Huffman dictionary is applied to sparse weight representation.
  - **Inference latency spike after quantization:** May indicate model not fully loaded into SRAM; verify ESP32 memory allocation.

- **First 3 experiments:**
  1. **Baseline sanity check:** Train the 1D CNN on the original imbalanced dataset without augmentation. Measure per-class precision/recall. Expect: high Truck accuracy, poor Forklift/Dummy performance.
  2. **Threshold sweep:** After training with ADASYN, vary the confidence threshold from 0.5 to 0.95 in 0.05 increments. Plot precision-recall curves for each class. Identify the threshold where precision ≥ 94% while recall remains ≥ 88%.
  3. **Compression impact validation:** Deploy three model variants (original, pruned-only, pruned+quantized) to ESP32. Measure: (a) model size in flash/SRAM, (b) inference latency, (c) per-class precision. Confirm compression factor ≥ 4× with <5% precision degradation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would multimodal sensor fusion (e.g., combining accelerometer with gyroscope or magnetometer data) affect classification performance and energy consumption compared to the accelerometer-only approach?
- Basis in paper: [explicit] "Future work includes exploring multimodal sensor fusion to enhance model performance. Including new sensors will diversify input types and might give better performance results."
- Why unresolved: The current study uses only accelerometer data; no other sensor modalities were tested.
- What evidence would resolve it: Comparative experiments with multi-sensor models, measuring classification metrics and power consumption with additional sensors active.

### Open Question 2
- Question: Can semi-supervised or self-supervised learning approaches effectively leverage unlabeled accelerometer data to improve classification while reducing labeling costs?
- Basis in paper: [explicit] "We plan to train the model with new unsupervised data, as for logistic reasons these data are easier to collect than supervised ones."
- Why unresolved: All experiments relied on supervised learning; no unsupervised or semi-supervised techniques were evaluated.
- What evidence would resolve it: Experiments showing comparable performance when using semi-supervised methods on mixed labeled/unlabeled datasets from real logistics operations.

### Open Question 3
- Question: How well does the model generalize across different industrial facilities, package types, and equipment not represented in the single training facility environment?
- Basis in paper: [inferred] Data were collected at a single "training facility for logistics technicians" with no cross-facility validation.
- Why unresolved: All training and validation data originated from one controlled environment.
- What evidence would resolve it: Cross-facility validation experiments demonstrating consistent metrics across diverse deployment contexts.

### Open Question 4
- Question: How does model performance scale when extending from two event classes to finer-grained event taxonomies (e.g., loading, unloading, different handling methods)?
- Basis in paper: [inferred] The methodology was validated only on Forklift, Truck, and a catch-all Dummy class.
- Why unresolved: No experiments evaluated the approach with additional meaningful event categories.
- What evidence would resolve it: Experiments with expanded class taxonomies showing how precision, recall, and model size requirements scale with classification granularity.

## Limitations
- Lack of full architectural transparency with CNN layer specifications not provided
- Limited validation of ADASYN performance on time-series data compared to image data
- Single facility data collection limits generalizability to diverse industrial environments
- INT8 quantization assumes representative calibration data adequately covers operational input ranges

## Confidence
- **High Confidence**: 1D CNN architecture suitability for time-series classification; effectiveness of deep compression (pruning + quantization) in reducing model size by 4× with <5% precision loss; power consumption measurements (316 mW) for pruned model during ESP32 inference
- **Medium Confidence**: ADASYN-based augmentation improving minority class precision; adaptive thresholding reducing energy consumption via low-confidence routing; label smoothing (0.1) improving generalization
- **Low Confidence**: Long-term stability of quantized model thresholds under real-world noise; scalability of the approach to more complex event classes; robustness of ADASYN-generated samples for sensor time-series with inherent temporal dependencies

## Next Checks
1. **Precision-recall threshold analysis**: Sweep softmax confidence thresholds from 0.5 to 0.95 in 0.05 increments for each class. Plot precision-recall curves to identify optimal thresholds balancing precision ≥94% with recall ≥88%.
2. **Compression impact verification**: Deploy three model variants (original, pruned-only, pruned+quantized) to ESP32. Measure model size, inference latency, and per-class precision to confirm compression factor ≥4× with <5% precision degradation.
3. **ADASYN temporal coherence check**: Visually inspect synthetic samples generated by ADASYN for discontinuities or unrealistic patterns. Verify that interpolation preserves temporal smoothness across tri-axial accelerometer channels.