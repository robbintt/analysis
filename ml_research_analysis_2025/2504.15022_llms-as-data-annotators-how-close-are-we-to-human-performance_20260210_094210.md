---
ver: rpa2
title: 'LLMs as Data Annotators: How Close Are We to Human Performance'
arxiv_id: '2504.15022'
source_url: https://arxiv.org/abs/2504.15022
tags:
- label
- entity
- examples
- llms
- context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates large language models for data annotation
  tasks, particularly Named Entity Recognition, by comparing retrieval-augmented generation
  (RAG) approaches with in-context learning and zero-shot baselines. The proposed
  RAG method automatically retrieves relevant context examples using embedding models,
  improving annotation quality over manually selected examples.
---

# LLMs as Data Annotators: How Close Are We to Human Performance

## Quick Facts
- **arXiv ID:** 2504.15022
- **Source URL:** https://arxiv.org/abs/2504.15022
- **Reference count:** 40
- **Primary result:** RAG-based retrieval improves LLM annotation F1 by 9% over ICL on CoNLL-2003, with gpt-4o-mini reaching 89.72 F1 vs human 92.12

## Executive Summary
This study evaluates large language models for Named Entity Recognition annotation tasks, comparing retrieval-augmented generation (RAG) with in-context learning and zero-shot baselines. The proposed RAG method automatically retrieves relevant context examples using embedding models, significantly improving annotation quality over manually selected examples. Experiments on four datasets show that RAG consistently outperforms other methods, with the gpt-4o-mini model achieving performance within 3% of human-level annotation on structured datasets. However, performance significantly declines on more complex datasets like SKILLSPAN, where the best F1 score reaches only 34.06%. The study demonstrates that dataset complexity and embedding choice critically impact LLM performance, while larger models do not always yield statistically significant improvements over smaller models when paired with effective retrieval strategies.

## Method Summary
The study evaluates LLM annotation quality by splitting training data into a sample space X (context examples) and target set T (sentences to annotate). For each target sentence, the RAG approach retrieves top-k similar examples from X using embedding models, constructs a prompt with task description and context, and generates structured annotations. The annotated data is then used to fine-tune RoBERTa, which is evaluated on held-out test sets. The approach is compared against zero-shot and in-context learning baselines across four NER datasets (CoNLL-2003, WNUT-17, GUM, SKILLSPAN) using multiple LLM models (gpt-4o-mini, Qwen2.5, Llama3.5) and embedding models (OpenAI, sentence-transformers).

## Key Results
- RAG-based retrieval improves F1 scores by 9% over ICL on CoNLL-2003 (89.72 vs 80.89)
- gpt-4o-mini achieves 89.72 F1 on CoNLL-2003 vs human 92.12 F1
- SKILLSPAN dataset shows significant performance gap: best F1 34.06 vs human 54.79
- Embedding quality creates performance bottlenecks—OpenAI embeddings outperform ST embeddings by 1.66 F1 points on CoNLL-2003
- Larger models don't guarantee better performance—7B models with good embeddings can match 70B models statistically

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** RAG-based automatic retrieval of context examples improves annotation quality over randomly sampled ICL examples.
- **Mechanism:** Embedding-based similarity matching retrieves semantically relevant examples from sample space X, providing the LLM with context that better aligns with the target sentence's structure and entity patterns. This reduces irrelevant context that can cause hallucinations or misannotation.
- **Core assumption:** Semantic similarity between input sentences and context examples correlates with annotation quality—similar examples provide better task guidance.
- **Evidence anchors:** [abstract]: "a method that addresses the limitations of ICL by automatically retrieving contextual examples, thereby enhancing performance"; [section 5.1]: "RAG-based approaches improve annotation quality" with gpt-4o-mini achieving 89.72 F1 on CoNLL-2003 vs 80.89 for ICL; [corpus]: Related work (TDR: arXiv 2507.18340) confirms retrieval quality heavily impacts ICL effectiveness, though evidence for NER-specific annotation is limited in corpus
- **Break condition:** When sample space X is too small (<20% of training data), RAG converges to ICL performance due to insufficient retrieval candidates (Figure 3).

### Mechanism 2
- **Claim:** Embedding model quality creates a retrieval bottleneck—better embeddings yield more relevant context and higher annotation F1.
- **Mechanism:** Higher-capacity embedding models (text-embedding-3-large vs all-MiniLM-L6-v2) capture finer semantic distinctions, enabling retrieval of examples that share not just surface similarity but entity structure and labeling patterns.
- **Core assumption:** The embedding space preserves task-relevant similarity structure for NER contexts.
- **Evidence anchors:** [section 5.2]: "OpenAI embeddings lead to better F1 scores compared to smaller-scale ST embeddings especially for gpt-4o-mini model"; [table 2]: gpt-4o-mini with OpenAI embeddings achieves 89.72 F1 vs 88.06 with ST embeddings on CoNLL-2003; [corpus]: Weak direct evidence; corpus papers focus on ICL example selection but not specifically on embedding model comparison for annotation
- **Break condition:** On highly ambiguous datasets (SKILLSPAN), embedding choice matters less because the task requires domain knowledge beyond semantic similarity.

### Mechanism 3
- **Claim:** Dataset complexity—not model size—is the primary performance limiter for LLM annotation.
- **Mechanism:** Structured entities (PER, ORG, LOC) have consistent patterns well-represented in pretraining data. Abstract entities (soft skills) lack syntactic/semantic regularity, requiring deeper reasoning that retrieval alone cannot supply.
- **Core assumption:** LLMs cannot acquire new entity recognition capabilities solely through in-context examples for conceptually complex categories.
- **Evidence anchors:** [section 5.1]: SKILLSPAN best F1 = 34.06% vs human 54.79%, while CoNLL-2003 reaches 89.72% vs human 92.12%; [table 1]: SKILLSPAN average entity length = 4.72 tokens vs CoNLL-2003 at 1.60 tokens; [corpus]: No corpus papers directly address complexity thresholds for annotation tasks
- **Break condition:** When entity definitions require domain expertise not present in context examples (e.g., distinguishing "communication skills" from generic text), performance degrades regardless of retrieval quality.

## Foundational Learning

- **Concept:** Named Entity Recognition (NER) as token classification
  - **Why needed here:** The entire annotation pipeline assumes understanding that NER assigns labels to individual tokens, requiring token-label alignment in structured outputs.
  - **Quick check question:** Can you explain why decoder-only LLMs struggle with token-label alignment compared to sequence-to-sequence models?

- **Concept:** In-Context Learning (ICL) vs. Zero-Shot prompting
  - **Why needed here:** The baseline comparison depends on distinguishing what the model knows inherently (zero-shot) vs. what it learns from examples (ICL/RAG).
  - **Quick check question:** Why might random ICL examples perform worse than zero-shot on some tasks?

- **Concept:** Vector embeddings and similarity search
  - **Why needed here:** RAG-based retrieval relies on embedding the sample space and querying with input sentences—without this, you cannot implement the core contribution.
  - **Quick check question:** What distance metric would you use to retrieve examples for NER, and why might cosine similarity fail for certain entity types?

## Architecture Onboarding

- **Component map:**
  Sample space X -> Embedding model -> Vector store -> Retriever -> LLM annotator -> Structured output handler -> RoBERTa fine-tuner -> Evaluator

- **Critical path:**
  1. Split D_train into X (sample space) and T (annotation targets)
  2. Embed all examples in X and store in vector database
  3. For each T_i in T: embed T_i, retrieve top-M similar examples from X
  4. Construct prompt with task description + retrieved examples + T_i
  5. LLM generates structured annotations
  6. Fine-tune RoBERTa on LLM-annotated T, evaluate on D_test

- **Design tradeoffs:**
  - Context size (M): 25→50→75 examples. Larger helps but with diminishing returns; some models saturate at 50
  - Sample space size: 30% of training data used; smaller (10-20%) increases variance and reduces RAG advantage over ICL
  - Embedding choice: OpenAI embeddings better for gpt-4o-mini; ST embeddings viable for open-source models with resource constraints
  - Model size: 7B models with good embeddings can match 70B models statistically (see Appendix E), but with lower inference cost

- **Failure signatures:**
  - Token-label misalignment: LLM outputs entities not matching input tokens (mitigated by structured output features)
  - Low recall on sparse entities: WNUT-17 shows ICL recall drops when entities are rare in context examples
  - Hallucination on complex datasets: SKILLSPAN shows LLMs annotating non-existent skills (Table 6, examples 1 and 4)
  - Context saturation: Adding examples beyond 50-75 may not improve or can degrade performance

- **First 3 experiments:**
  1. **Baseline validation:** Run zero-shot annotation on CoNLL-2003 with gpt-4o-mini to confirm baseline F1 (~71.66) matches paper before implementing RAG
  2. **Embedding ablation:** On a single dataset (WNUT-17), compare OpenAI vs. ST embeddings with fixed M=50 to quantify embedding impact before scaling
  3. **Sample space sensitivity:** Test RAG vs. ICL at 10%, 20%, 30% sample sizes on SKILLSPAN to determine minimum viable sample space for your domain

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the performance benefits of RAG-based annotation observed in Named Entity Recognition (NER) be effectively generalized to other structured prediction or text classification tasks?
- **Basis in paper:** [explicit] The authors state in the "Limitations" section that experiments focused solely on NER tasks and that extending the analysis to tasks like text classification or question answering would offer a more comprehensive understanding.
- **Why unresolved:** The current study only validates the proposed RAG approach on token-label alignment tasks, leaving its efficacy on tasks requiring different output structures (e.g., multi-class classification) unknown.
- **What evidence would resolve it:** Experimental results demonstrating that RAG-augmented LLMs maintain performance parity with human annotators on non-NER benchmarks (e.g., sentiment analysis, QA datasets).

### Open Question 2
- **Question:** Do sophisticated retrieval strategies, such as re-ranking mechanisms or hybrid sparse-dense retrieval, yield significantly higher annotation quality than the naïve RAG approach used in this study?
- **Basis in paper:** [explicit] The "Limitations" section notes the use of a "naïve RAG approach" and suggests that future work should explore adaptive retrieval strategies or hybrid approaches to optimize performance.
- **Why unresolved:** The paper establishes a baseline for RAG-annotation but does not determine if the retrieval of context examples is optimal or if more complex retrieval logic could close the performance gap on difficult datasets like SKILLSPAN.
- **What evidence would resolve it:** Comparative experiments on complex datasets (like SKILLSPAN) showing that advanced retrieval methods reduce the F1 score gap between LLMs and human annotators more effectively than the current similarity-based search.

### Open Question 3
- **Question:** What specific annotation biases do LLMs introduce in specialized domains, and how do they differ between proprietary and open-source models?
- **Basis in paper:** [explicit] The "Limitations" section explicitly states that the study "does not explicitly examine the biases introduced by LLMs in the data annotation process," particularly regarding diverse and underrepresented datasets.
- **Why unresolved:** While the paper measures performance drops (F1 scores) on complex datasets, it does not conduct a qualitative analysis of *how* the models fail (e.g., systematic exclusion of specific entity types, cultural biases, or hallucination patterns).
- **What evidence would resolve it:** A qualitative error analysis comparing the distribution of error types (false positives/negatives) between models like GPT-4o-mini and Llama-3.1 on domain-specific subsets of the data.

## Limitations

- Dataset complexity and generalization boundaries are not well-established; the study doesn't identify clear thresholds for when LLM annotation becomes ineffective
- Embedding quality is treated as a black box factor without examining what semantic features they capture for NER tasks
- Statistical significance findings don't address practical significance or cost-benefit tradeoffs of model scaling

## Confidence

**High Confidence Claims:**
- RAG-based retrieval consistently improves annotation quality over random ICL examples across all tested datasets and models
- Dataset complexity (structured vs. abstract entities) is the primary performance differentiator, not model size
- Retrieval quality depends critically on embedding model choice, with higher-capacity embeddings yielding better results

**Medium Confidence Claims:**
- gpt-4o-mini achieves within 3% of human performance on structured datasets (statistically significant but narrow margin)
- Larger models do not provide statistically significant improvements when paired with effective retrieval strategies
- 30% sample space size provides optimal balance between RAG effectiveness and computational efficiency

**Low Confidence Claims:**
- LLM annotation can fully replace human annotation for structured NER tasks (extrapolated from single dataset performance)
- Embedding improvements will generalize across all NER domains (based on limited dataset testing)
- The 3% performance gap to humans is acceptable for all practical applications (value judgment without domain-specific analysis)

## Next Checks

1. **Cross-Domain Embedding Analysis**: Test the same RAG approach with OpenAI vs. ST embeddings on three additional NER datasets from different domains (biomedical, legal, social media) to determine if embedding advantages generalize beyond the original four datasets. Measure not just F1 scores but also retrieval quality metrics (average similarity scores of top-k examples).

2. **Complexity Threshold Determination**: Systematically vary dataset characteristics (entity length, ambiguity, domain specificity) on a synthetic NER benchmark to identify the complexity threshold where RAG performance drops below 80% of human level. This would establish clear boundaries for when LLM annotation becomes unreliable.

3. **Cost-Benefit Analysis of Model Scaling**: Implement the same RAG pipeline across gpt-4o-mini, Qwen2.5-7B, and Llama3.1-8B models, measuring both F1 scores and inference costs (tokens processed, wall-clock time). Determine the marginal cost per F1 point improvement to establish practical guidelines for model selection in annotation workflows.