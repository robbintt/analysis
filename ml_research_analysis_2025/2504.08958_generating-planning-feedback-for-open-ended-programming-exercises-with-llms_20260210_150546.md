---
ver: rpa2
title: Generating Planning Feedback for Open-Ended Programming Exercises with LLMs
arxiv_id: '2504.08958'
source_url: https://arxiv.org/abs/2504.08958
tags:
- programming
- feedback
- plans
- code
- submissions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of providing feedback on students'
  programming plans in open-ended coding exercises, which are typically only graded
  on final test-case correctness without considering the planning process. The authors
  propose using large language models (LLMs) to detect high-level programming plans
  in student code submissions, even when the code contains syntax errors.
---

# Generating Planning Feedback for Open-Ended Programming Exercises with LLMs

## Quick Facts
- arXiv ID: 2504.08958
- Source URL: https://arxiv.org/abs/2504.08958
- Authors: Mehmet Arif DemirtaÅŸ; Claire Zheng; Max Fowler; Kathryn Cunningham
- Reference count: 40
- Key result: Fine-tuned GPT-4o-mini achieves 0.782 micro-F1 on detecting student programming plans

## Executive Summary
This paper addresses the challenge of providing feedback on students' programming plans in open-ended coding exercises, which are typically only graded on final test-case correctness without considering the planning process. The authors propose using large language models (LLMs) to detect high-level programming plans in student code submissions, even when the code contains syntax errors. They formulate this as a multi-label classification task and evaluate their approach on 1,616 student submissions from a CS1 course, using both GPT-4o and the smaller, more cost-effective GPT-4o-mini model. The results show that LLM approaches significantly outperform traditional code analysis baselines, achieving micro-F1 scores of 0.782 with fine-tuned GPT-4o-mini, compared to 0.589 for the best baseline.

## Method Summary
The authors collected 1,616 student submissions from a CS1 course spanning 116 Python problems, each with one instructor solution annotated with programming plans. They defined 9 specific programming plans (plus UNKNOWN category) and implemented two baseline approaches: AST-Rules using hand-crafted syntax patterns and CodeBERT-kNN with k=3 nearest neighbors. For LLM approaches, they used GPT-4o and GPT-4o-mini with either few-shot prompting (3 examples per plan from instructor solutions) or fine-tuning (3 epochs, batch size 2, on 116 instructor solutions). The models classify code submissions into the 9 predefined programming plans, with evaluation using exact match ratio, micro-F1 score, and weighted-F1 score as primary metrics.

## Key Results
- LLM approaches significantly outperform traditional baselines (0.782 micro-F1 with fine-tuned GPT-4o-mini vs 0.589 for best baseline)
- Fine-tuned GPT-4o-mini achieves performance parity with GPT-4o (0.782 vs 0.778 micro-F1) at 1/16th the cost
- LLMs provide more robust feedback for code with errors, maintaining ~0.69-0.73 F1 for submissions with syntax/semantic errors
- Variable name ablation shows no significant performance drop for GPT models (p > .05), while CodeBERT drops ~20% (p < .001)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LLMs rely on structural semantics (control flow, logic patterns) rather than surface-level features like variable names
- **Mechanism:** Ablating variable names and function signatures showed no statistically significant performance drop for GPT models (p > .05), whereas CodeBERT baseline dropped by ~20% (p < .001), suggesting the LLM builds an abstract representation of the code's goal
- **Core assumption:** The model's pre-training on vast code corpora allows it to map obfuscated syntax to known algorithmic patterns (plans) independent of identifier semantics
- **Evidence anchors:** Abstract states LLMs detect overall code structure even for submissions with syntax errors; ablation study shows no significant differences for GPT models
- **Break condition:** Performance would degrade significantly if the code structure itself is obfuscated (e.g., control flow flattening) or if the logic is novel and not represented in the pre-training distribution

### Mechanism 2
- **Claim:** LLMs infer intended plans from broken code by leveraging a capacity for "overcorrection," effectively hallucinating the correct structure underlying the syntax errors
- **Mechanism:** The paper frames the LLM's tendency to generate correct code from incorrect artifacts as a feature, identifying the intended structure despite implementation errors (Table 4 shows ~0.69-0.73 F1 for submissions with syntax/semantic errors)
- **Core assumption:** The errors in novice code are local or superficial enough that the global semantic intent remains recoverable by the model
- **Evidence anchors:** Section 4.2 states LLM approaches are especially valuable for providing feedback for code with errors while baseline models are almost unusable
- **Break condition:** If the student's code is too sparse or directionally wrong (a "planning" error rather than an implementation error), the model may misclassify the intent or hallucinate a plan that doesn't exist

### Mechanism 3
- **Claim:** Fine-tuning smaller models (GPT-4o-mini) on a narrow domain (instructor solutions) acts as a distillation mechanism, closing the performance gap with larger models
- **Mechanism:** A small, cost-effective model was fine-tuned on only 116 instructor solutions, achieving a Micro-F1 of 0.782, surpassing the few-shot prompted GPT-4o (0.778)
- **Core assumption:** The instructor solutions are sufficiently representative of the possible student variations to teach the smaller model the necessary decision boundaries
- **Evidence anchors:** Abstract states fine-tuning the smaller GPT-4o-mini model achieved performance on par with GPT-4o; section 4.2 notes GPT-4o-mini is less computationally expensive and can be deployed in real-time at 1/16th the cost
- **Break condition:** If the test distribution drifts significantly from the instructor solutions (e.g., a new programming language or radically different problem type), the fine-tuned small model would likely underperform the generalist large model

## Foundational Learning

- **Concept: Programming Plans (Schemata)**
  - **Why needed here:** This is the target variable. You must understand that the system is not grading correctness, but identifying "chunks" of logic (e.g., `filterACollection`, `evennessCheck`) defined in Table 1
  - **Quick check question:** If a student writes a loop to count items but forgets to initialize the counter variable, which "plan" did they attempt, and did they succeed in implementation?

- **Concept: Abstract Syntax Trees (AST) vs. Semantic Analysis**
  - **Why needed here:** The baseline mechanism (AST-Rules) fails because it relies on strict syntax trees. Understanding this contrast explains why LLMs are necessary for error-tolerant feedback
  - **Quick check question:** Why would a rule-based AST classifier fail on code with a missing indentation or a syntax error, while an LLM might still succeed?

- **Concept: Multi-Label Classification Evaluation**
  - **Why needed here:** A single submission can contain multiple plans (e.g., `processAllItems` AND `filterACollection`). Evaluation requires Micro-F1 (treating the problem as a collection of independent binary classifications) rather than simple accuracy
  - **Quick check question:** Why is "Exact Match Ratio" a stricter metric than "Micro-F1" in this context?

## Architecture Onboarding

- **Component map:** Student Code + Problem Statement -> Feature Extractor (CodeBERT/AST Parser or GPT-4o/Fine-tuned GPT-4o-mini) -> Context (Plan Definitions + 3-shot examples) -> Set of detected plans

- **Critical path:**
  1. Define the plan taxonomy (9 specific plans)
  2. Gather instructor solutions (Gold Standard)
  3. Fine-tune GPT-4o-mini on these solutions (if optimizing for cost/speed) OR construct few-shot prompt for GPT-4o
  4. Inference: Pass student code + Plan definitions to model
  5. Post-process: Map model output to plan labels

- **Design tradeoffs:**
  - AST vs. LLM: AST is free and fast but fails on syntax errors (0.17 F1). LLM is slow/costly but robust to errors (0.72 F1)
  - Few-shot vs. Fine-tuning: Few-shot requires no training data management but costs more at inference (16x). Fine-tuning has upfront effort/cost but lowers latency and inference cost

- **Failure signatures:**
  - "UNKNOWN" Bias: Models struggle to predict "No plan found" (UNKNOWN), often forcing a plan label where none exists
  - False Positives: The system may confirm a student's plan is correct even if the implementation is totally broken, potentially confusing the student if not phrased carefully
  - Hallucination: The LLM might invent a plan not in the taxonomy if the output constraints are not strictly enforced

- **First 3 experiments:**
  1. **Sanity Check (AST vs. LLM):** Run the 30 student submissions for a single problem through both the AST-Rules classifier and a GPT-4o prompt. Compare failure rates on syntactically incorrect submissions
  2. **Robustness Test (Ablation):** Take 50 correct submissions, obfuscate the variable names using a Python script, and verify that the LLM classification remains stable (checking for p > .05 significance)
  3. **Fine-tuning Loop:** Fine-tune GPT-4o-mini on the 116 instructor solutions. Plot the Micro-F1 curve against the un-tuned GPT-4o to verify if the smaller model successfully converges to the larger model's performance

## Open Questions the Paper Calls Out

- **Question:** Does providing LLM-generated planning feedback improve student learning outcomes and problem-solving performance?
- **Basis:** Explicit - The paper states in limitations: "evaluating this detection technique in a real-time environment with students can yield a greater understanding of how getting feedback on the problem-solving process can shape the student experience"
- **Why unresolved:** The study validates detection accuracy but does not measure whether the feedback actually helps students learn or select better plans
- **What evidence would resolve it:** A classroom study comparing students receiving plan-based feedback versus standard test-case feedback, measuring improvements in plan selection and code correctness

## Limitations
- Evaluation relies on a single course's student submissions (1,616 from one CS1 course) and instructor solutions, raising generalizability concerns
- Annotation process lacks detailed inter-annotator agreement metrics, making it difficult to assess reliability of gold standard labels
- Study focuses exclusively on Python, limiting applicability to other languages
- AST-Rules baseline may not fairly represent modern static analysis approaches, as rules appear hand-crafted rather than optimized

## Confidence
- **High Confidence:** LLM approaches significantly outperforming traditional baselines (0.782 vs 0.589 micro-F1) is well-supported by experimental results and ablation studies
- **Medium Confidence:** Fine-tuned GPT-4o-mini achieving performance parity with GPT-4o (0.782 vs 0.778 micro-F1) is supported but margin is small and could vary with different datasets
- **Medium Confidence:** LLMs providing "more robust feedback for code with errors" is supported by error-category analysis but practical pedagogical value is not directly measured

## Next Checks
1. **Cross-Course Validation:** Test the fine-tuned GPT-4o-mini model on student submissions from a different CS1 course or programming language to assess generalizability beyond the original dataset
2. **Annotation Reliability Assessment:** Calculate and report inter-annotator agreement scores for the plan labeling process, and conduct a blind re-annotation of a subset of submissions to verify label stability
3. **Novel Plan Detection:** Design an experiment where student submissions contain plans outside the predefined taxonomy, then evaluate whether the system correctly identifies these as UNKNOWN or inappropriately forces them into existing categories