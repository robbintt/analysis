---
ver: rpa2
title: In-Run Data Shapley for Adam Optimizer
arxiv_id: '2602.00329'
source_url: https://arxiv.org/abs/2602.00329
tags:
- data
- shapley
- training
- adam
- in-run
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the optimizer dependence of data attribution\
  \ in machine learning. Standard In-Run Data Shapley methods, designed for SGD, fail\
  \ to capture the complex dynamics of adaptive optimizers like Adam, producing attribution\
  \ scores that poorly correlate with true marginal contributions (Pearson R \u2248\
  \ 0.11)."
---

# In-Run Data Shapley for Adam Optimizer
## Quick Facts
- arXiv ID: 2602.00329
- Source URL: https://arxiv.org/abs/2602.00329
- Reference count: 27
- Standard In-Run Data Shapley methods fail on Adam, producing near-zero attribution fidelity (Pearson R ≈ 0.11)

## Executive Summary
This paper addresses a critical gap in data attribution methods: standard In-Run Data Shapley approaches, designed for SGD, fail catastrophically when applied to adaptive optimizers like Adam. The authors demonstrate that Adam's stateful and non-linear update rules fundamentally break the assumptions underlying traditional In-Run Shapley estimation, resulting in attribution scores that poorly reflect true marginal contributions to model utility. To solve this, they propose Adam-Aware In-Run Data Shapley, introducing a Linearized Ghost Approximation that linearizes Adam's variance-dependent scaling term, enabling efficient computation of pairwise gradient dot-products without materializing per-sample gradients.

## Method Summary
The authors develop Adam-Aware In-Run Data Shapley by deriving a closed-form estimator that accounts for Adam's exponential moving averages (m_t, v_t) and non-linear update rules. The key innovation is the Linearized Ghost Approximation, which approximates the variance-dependent scaling term through linearization, enabling efficient computation of the utility change ΔU(a,b) via pairwise gradient dot-products. This approximation maintains the adaptive optimizer's behavior while allowing tractable In-Run Shapley computation. The method introduces ghost vectors that capture the influence of each sample on the optimizer's internal state, enabling accurate attribution without full retraining or costly Monte Carlo sampling.

## Key Results
- Standard In-Run Shapley methods achieve near-zero fidelity with Adam (Pearson R ≈ 0.11)
- Adam-Aware method achieves near-perfect correlation with ground-truth utility changes (R > 0.99)
- Maintains ~95% of training throughput compared to standard In-Run Shapley implementations
- Significantly outperforms SGD-based baselines in data pruning and semantic source identification tasks

## Why This Works (Mechanism)
Adam-Aware In-Run Data Shapley works by recognizing that Adam's stateful nature (exponential moving averages of gradients and squared gradients) creates non-linear, history-dependent updates that standard In-Run Shapley methods cannot capture. The Linearized Ghost Approximation addresses this by linearizing the variance-dependent scaling term β_t√v_t/(1-β_t) while maintaining the essential adaptive behavior. This linearization enables efficient computation of pairwise gradient dot-products through the ghost vectors, which track each sample's influence on the optimizer's internal state. The closed-form estimator then uses these dot-products to compute Shapley values that accurately reflect each sample's marginal contribution to model utility.

## Foundational Learning
- **Adam Optimizer Dynamics**: Understanding how Adam's m_t and v_t updates create stateful, non-linear behavior is crucial because this is precisely what breaks standard In-Run Shapley methods.
  - *Why needed*: The paper's entire contribution stems from addressing Adam's unique update mechanism
  - *Quick check*: Can you explain why Adam's variance normalization creates non-linearities that SGD doesn't have?

- **Shapley Value Theory**: The theoretical foundation of fair attribution in cooperative games provides the framework for data attribution.
  - *Why needed*: The method builds on Shapley's axioms of efficiency, symmetry, and additivity
  - *Quick check*: What are the three axioms that define a valid Shapley value?

- **Linearization of Non-linear Functions**: The ability to approximate complex functions with linear terms while preserving essential behavior is the core technical innovation.
  - *Why needed*: Direct computation of Adam's utility changes is intractable, requiring approximation
  - *Quick check*: What conditions must hold for a linearization to maintain acceptable fidelity?

## Architecture Onboarding
**Component Map**: Data samples → Ghost vectors → Pairwise dot-products → Utility change estimator → Shapley values → Attribution scores

**Critical Path**: The computation flows from maintaining ghost vectors that track each sample's influence on Adam's internal state, through efficient pairwise dot-product calculations using the Linearized Ghost Approximation, to the final Shapley value computation that produces attribution scores.

**Design Tradeoffs**: The linearization approximation sacrifices some theoretical exactness for computational tractability, trading potential fidelity loss in extreme regimes for ~95% throughput retention. The method prioritizes efficiency over absolute precision, accepting that higher-order terms may be non-negligible in certain training regimes.

**Failure Signatures**: Poor attribution fidelity (low Pearson correlation with ground truth), excessive computational overhead beyond the claimed 95% throughput retention, or attribution scores that fail to identify semantically important samples would indicate method failure.

**First Experiments**:
1. Replicate the Pearson correlation comparison between standard In-Run Shapley and Adam-Aware method on a small Transformer model
2. Verify the 95% throughput claim by benchmarking training time with and without attribution computation
3. Test attribution accuracy on a controlled data pruning task where ground truth importance is known

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can the Linearized Ghost Approximation framework be generalized to other complex, adaptive optimizers (e.g., AdaFactor, LAMB) with different state variable structures?
- Basis in paper: [explicit] The authors state that standard methods "limit applicability to stateful and non-linear optimizers such as Adam," and their derivation is specific to Adam's exponential moving averages.
- Why unresolved: The closed-form estimator relies on linearizing Adam's specific $m_t$ and $v_t$ updates; optimizers with different variance or momentum mechanisms may require fundamentally different linearization strategies.
- What evidence would resolve it: Derivation of closed-form In-Run Shapley estimators for non-Adam adaptive optimizers that maintain similar fidelity levels.

### Open Question 2
- Question: Does the proposed method maintain high fidelity and throughput when scaling to Large Language Models (LLMs) with billions of parameters?
- Basis in paper: [inferred] Experiments are restricted to DistilBERT and GPT-2 Small (124M parameters).
- Why unresolved: The computational overhead of calculating pairwise dot-products and maintaining ghost vectors may scale differently with model depth and width in larger architectures.
- What evidence would resolve it: Benchmarking the Adam-aware In-Run method on models exceeding 7B parameters, measuring throughput degradation and memory overhead.

### Open Question 3
- Question: How does the approximation fidelity degrade in training regimes with extremely high learning rates where higher-order Taylor terms are non-negligible?
- Basis in paper: [explicit] Appendix D notes the method operates in a "small-perturbation regime" and validates it mostly where perturbation magnitude $u$ is small ($10^{-2}$).
- Why unresolved: The linearization truncates higher-order terms. In chaotic early training or aggressive curricula, these terms might dominate, causing the "near-perfect fidelity" to drop significantly.
- What evidence would resolve it: Fidelity analysis (Pearson R) specifically targeting high learning rate or high gradient variance regimes to test the boundary of the linearization assumption.

## Limitations
- The linearization approximation may introduce fidelity loss in highly non-linear regimes, particularly during late-stage training when gradient variance patterns shift dramatically
- The method's computational efficiency claims are benchmarked only against standard In-Run Shapley implementations, not alternative efficient attribution methods
- The theoretical analysis focuses on gradient covariance estimation accuracy without fully characterizing approximation error propagation through the entire Shapley computation

## Confidence
- Adam-specific attribution superiority: **High** (supported by extensive empirical validation)
- Linearized Ghost Approximation fidelity: **Medium** (strong theoretical basis but limited theoretical error bounds)
- 95% throughput retention: **Medium** (benchmark context limited)

## Next Checks
1. Conduct ablation studies varying the linearization threshold to quantify approximation-induced attribution error
2. Test method stability across diverse architectures (Transformers, ResNets) and learning rate schedules
3. Compare against alternative efficient attribution methods like influence functions or gradient-based saliency metrics