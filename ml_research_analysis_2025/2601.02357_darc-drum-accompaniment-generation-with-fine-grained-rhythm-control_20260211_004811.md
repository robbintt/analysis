---
ver: rpa2
title: 'DARC: Drum accompaniment generation with fine-grained rhythm control'
arxiv_id: '2601.02357'
source_url: https://arxiv.org/abs/2601.02357
tags:
- rhythm
- drum
- timbre
- musical
- audio
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DARC addresses the challenge of generating drum accompaniments
  that both maintain musical coherence with a given mix and precisely follow a user-provided
  rhythm prompt. It fine-tunes STAGE, a state-of-the-art drum stem generator, using
  parameter-efficient techniques to condition on both musical context and detailed
  rhythm features.
---

# DARC: Drum accompaniment generation with fine-grained rhythm control

## Quick Facts
- arXiv ID: 2601.02357
- Source URL: https://arxiv.org/abs/2601.02357
- Reference count: 0
- Generates drum accompaniments that follow user-provided rhythm prompts while maintaining musical coherence with input mixes

## Executive Summary
DARC addresses the challenge of generating drum accompaniments that both maintain musical coherence with a given mix and precisely follow a user-provided rhythm prompt. It fine-tunes STAGE, a state-of-the-art drum stem generator, using parameter-efficient techniques to condition on both musical context and detailed rhythm features. Rhythm features are extracted via nonnegative matrix factorization, encoding onset timings and timbre classes without timbre leakage. The model uses jump fine-tuning and adaptive in-attention to efficiently incorporate rhythm conditioning while preserving the base model's capabilities. Experiments on the A VP Beatbox and MUSDB18 datasets show that DARC can follow rhythm prompts, but audio quality issues from stem separation degrade performance on quantitative metrics.

## Method Summary
DARC fine-tunes STAGE, a MusicGen-Small based model, to generate drum stems conditioned on both a drumless mix and a rhythm prompt. The drumless mix is tokenized and prepended to the input sequence, framing generation as a continuation task. Rhythm features are extracted from beatboxing/tapping prompts using NMF, which decomposes the spectrogram into timbre basis and activation matrices, retaining only the timing information. The model uses jump fine-tuning to freeze ~80% of parameters and adaptive in-attention to reintroduce rhythm conditioning at specific decoder layers. Training uses data augmentation and 7 epochs on FMA dataset with Demucs-separated stems.

## Key Results
- DARC can follow rhythm prompts from beatboxing/tapping inputs
- Audio fidelity issues from source separation degrade quantitative metrics
- Musical coherence (COCOLA) is lower than base model, likely due to fidelity problems
- Quantitative rhythm adherence scores are low despite qualitative adherence

## Why This Works (Mechanism)

### Mechanism 1: NMF-Based Rhythm Feature Extraction
Non-negative matrix factorization isolates onset timing and timbre class information from rhythm prompts while eliminating timbre leakage from the input audio. NMF decomposes the magnitude spectrogram S into W (timbre basis) and H (activation/timing). By discarding W and retaining only H, the representation becomes MIDI-likeâ€”encoding (onset time, timbre class index) pairs without carrying the source timbre into the generated output. Core assumption: Timbre classes can be meaningfully ordered by total component energy, roughly mapping to kick/snare/hi-hat for the first three classes. Break condition: If the rhythm prompt contains overlapping timbres with similar energy profiles, NMF may misclassify onsets or fail to separate classes cleanly.

### Mechanism 2: Parameter-Efficient Fine-Tuning via Jump Fine-Tuning and Adaptive In-Attention
Freezing ~80% of STAGE's parameters and selectively fine-tuning only the first self-attention layer per decoder block (jump fine-tuning) with reintroduced conditioning (adaptive in-attention) allows incorporation of rhythm control without catastrophic forgetting. Jump fine-tuning reduces trainable parameters by updating only layers at indices 0, 4, 8, 12, ..., 44. Adaptive in-attention re-injects the rhythm embedding at layers 4, 8, ..., 32 (first 75% of blocks), providing structured conditioning pathways. Core assumption: The rhythm conditioning signal is sufficiently expressive in the NMF-derived H matrix to guide generation without requiring full model retraining. Break condition: If rhythm conditioning requires integration with deeper layers or cross-attention mechanisms, the selective fine-tuning approach may underfit the conditioning signal.

### Mechanism 3: Prefix-Based Musical Context Conditioning
Prepending tokenized drumless mix audio to the input sequence frames drum generation as a continuation task, enabling the model to infer stylistically appropriate timbres from musical context rather than requiring explicit timbre prompts. The drumless mix is encoded as EnCodec tokens and prepended with a delimiter token. The model generates drum tokens autoregressively as a continuation, learning to match timbre and style from the prefix context. Core assumption: The base STAGE model's prefix-based conditioning, which was shown superior to cross-attention for stem generation, transfers effectively when augmented with additional rhythm conditioning. Break condition: If the rhythm conditioning conflicts with the stylistic cues from the musical context prefix, the model may generate incoherent outputs.

## Foundational Learning

- **Concept: Non-negative Matrix Factorization (NMF)**
  - Why needed here: NMF is the core rhythm feature extractor. Without understanding how S = WH decomposes spectrograms into basis and activation matrices, you cannot debug rhythm classification errors or tune the number of timbre classes.
  - Quick check question: Given a spectrogram S decomposed as S = WH, which matrix would you discard to prevent timbre leakage, and what information would remain?

- **Concept: Parameter-Efficient Fine-Tuning (PEFT) / Attention Freezing**
  - Why needed here: DARC's training strategy relies on freezing most layers and selectively fine-tuning. Understanding why this prevents catastrophic forgetting is essential for modifying the architecture or extending to other stem types.
  - Quick check question: If you fine-tuned all 48 self-attention layers instead of using jump fine-tuning, what failure mode would you expect relative to the base STAGE model?

- **Concept: Autoregressive Audio Token Generation (EnCodec + Transformer)**
  - Why needed here: DARC generates drums by predicting EnCodec tokens autoregressively. Understanding token prediction, delimiter tokens, and continuation framing is required to modify the conditioning pipeline or debug token-level artifacts.
  - Quick check question: Why does prepending the drumless mix as tokens frame the task as "continuation" rather than "generation from scratch"?

## Architecture Onboarding

- **Component map:**
  Drumless mix audio -> EnCodec encoder -> Tokenized prefix sequence
  Rhythm prompt audio -> NMF decomposition -> H matrix (onset times + timbre class indices) -> Rhythm embedding
  STAGE model with frozen text encoder, frozen audio embeddings, selectively fine-tuned self-attention layers
  Rhythm embedding injected via adaptive in-attention at layers 4, 8, 12, ..., 32
  EnCodec audio tokens -> EnCodec decoder -> Drum stem waveform

- **Critical path:**
  1. Extract drum stems from FMA dataset using Demucs (source separation)
  2. Compute NMF features on rhythm prompts; tokenize drumless mixes
  3. Fine-tune with jump fine-tuning + adaptive in-attention for 7 epochs
  4. At inference: concatenate drumless mix tokens + rhythm embedding -> autoregressive generation -> decode to waveform

- **Design tradeoffs:**
  - Convenience vs. control: DARC infers timbre from context, avoiding explicit timbre prompts (faster iteration) but sacrificing precise timbre control compared to timbre-transfer methods like TRIA
  - Parameter efficiency vs. expressiveness: Freezing 80% of parameters preserves base capabilities but may limit how tightly the model can adhere to complex rhythm prompts
  - Dataset scale vs. quality: Using separated stems (Demucs) enables larger datasets but introduces artifacts; ground-truth stem datasets (MoisesDB) are smaller but cleaner

- **Failure signatures:**
  - Audio artifacts/non-drum sounds: Likely from source separation errors in training data (bleed, artifacts from Demucs)
  - Low Onset/Kick/Snare F1 despite qualitative adherence: Evaluation models (Beat-This, FrameRNN) are not robust to audio fidelity degradation; post-processing may be required
  - COCOLA score lower than base model: Fidelity issues degrade coherence metrics; also, COCOLA may reward excessive embellishment

- **First 3 experiments:**
  1. Ablate the NMF rhythm features: Replace NMF-derived (onset, timbre class) pairs with binary onset-only features to quantify the contribution of timbre class encoding
  2. Substitute Demucs with a higher-quality separator (e.g., Band-Split RoPE) or use MoisesDB ground-truth stems: Isolate whether audio fidelity issues stem from source separation artifacts
  3. Human listening study for rhythm adherence and coherence: Compare human ratings against Beat-This/FrameRNN F1 and COCOLA scores to validate whether quantitative metrics align with perceptual quality

## Open Questions the Paper Calls Out

### Open Question 1
Would training on datasets with ground-truth stems (e.g., MoisesDB) or using alternative source separation models significantly improve DARC's audio fidelity and downstream quantitative metrics?
Basis: "we hypothesize that...altering our dataset, either by using a different source separator model or a dataset such as MoisesDB that contains ground-truth drum stems, could be effective methods."
Unresolved because authors suspect stem separation artifacts from Demucs caused audio quality issues, but cannot confirm causality without controlled experiments.
Evidence: Train DARC on MoisesDB (with ground-truth stems) and compare Onset F1, Kick/Snare F1, and COCOLA scores against the current Demucs-extracted dataset.

### Open Question 2
Do human listening studies align with COCOLA's musical coherence assessments, particularly regarding STAGE's "embellished" outputs?
Basis: "This provides motivation for future work to conduct human listening studies to evaluate musical coherence, as well as design musical coherence metrics that exhibit greater robustness to audio fidelity and alignment with human preferences."
Unresolved because STAGE unexpectedly outperformed ground truth on COCOLA; authors suspect this metric rewards note quantity over perceptual coherence.
Evidence: Conduct A/B tests where human listeners rate coherence of STAGE outputs vs. ground truth; correlate ratings with COCOLA scores.

### Open Question 3
Can evaluation metrics for rhythm adherence be made robust to audio artifacts and varying fidelity levels?
Basis: "we encourage future work to explore robust rhythm adherence and musical coherence evaluation metrics that can handle various levels of audio fidelity."
Unresolved because Beat-This and FrameRNN performed poorly on DARC's outputs due to artifacts, requiring post-processing that was "far from a perfect solution."
Evidence: Develop and validate drum transcription/onset detection models that maintain accuracy across controlled degradation levels (noise, compression artifacts, bleed).

### Open Question 4
Can DARC's rhythm conditioning approach be adapted for real-time drum accompaniment or drum fill generation?
Basis: "We leave the adaptation of our methods for real-time or fill generation as future work."
Unresolved because DARC operates offline with 10-30 second chunks; latency and causal inference requirements for real-time use remain unexplored.
Evidence: Implement streaming inference with causal attention; measure latency and compare rhythm adherence against offline DARC.

## Limitations

- Audio fidelity issues from source separation artifacts propagate through to generated outputs and degrade quantitative metrics
- Evaluation metrics (Beat-This F1, FrameRNN) are not robust to audio quality degradation, creating disconnect between quantitative scores and qualitative performance
- NMF-based rhythm feature extraction lacks corpus validation and may not generalize to diverse rhythm sources beyond beatboxing/tapping

## Confidence

- **High Confidence:** NMF mechanism for extracting onset-timing without timbre leakage is technically sound and directly supported by the decomposition equation S = WH
- **Medium Confidence:** Architectural design choices (prefix-based context conditioning, rhythm embedding integration) are reasonable given STAGE's design but effectiveness hasn't been extensively validated
- **Low Confidence:** Claims about musical coherence degradation relative to base model are confounded by fidelity issues in training data

## Next Checks

1. Ablate NMF rhythm features: Replace the full (onset, timbre class) encoding with simple binary onset-only features to quantify how much the timbre classification contributes to generation quality versus just the timing information
2. Substitute source separation quality: Train on a smaller dataset of ground-truth stems (MoisesDB) versus Demucs-separated stems to isolate whether the quantitative performance issues stem from audio fidelity problems or the model architecture itself
3. Human evaluation study: Conduct controlled listening tests comparing DARC outputs against baseline models using both rhythm adherence and musical coherence criteria, validating whether automated metrics (Beat-This F1, COCOLA) align with human perception