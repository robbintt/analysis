---
ver: rpa2
title: Developing multilingual speech synthesis system for Ojibwe, Mi'kmaq, and Maliseet
arxiv_id: '2502.02703'
source_url: https://arxiv.org/abs/2502.02703
tags:
- multilingual
- uni00000013
- speech
- mamba2
- uni00000011
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper develops multilingual text-to-speech (TTS) systems for
  Ojibwe, Mi'kmaq, and Maliseet using lightweight flow matching models. Training a
  multilingual TTS model on three typologically similar Indigenous languages improved
  performance over monolingual models, especially in low-resource settings.
---

# Developing multilingual speech synthesis system for Ojibwe, Mi'kmaq, and Maliseet

## Quick Facts
- arXiv ID: 2502.02703
- Source URL: https://arxiv.org/abs/2502.02703
- Reference count: 13
- Multilingual TTS training on typologically similar languages improves performance over monolingual models in low-resource settings

## Executive Summary
This paper develops multilingual text-to-speech (TTS) systems for three Indigenous languages (Ojibwe, Mi'kmaq, and Maliseet) using lightweight flow matching models. The study demonstrates that training a multilingual TTS model on these genetically related Algonquian languages improves performance over monolingual models, particularly in low-resource scenarios. Attention-free architectures like Mamba2 and Hydra achieve comparable synthesis quality to self-attention while offering substantially higher memory efficiency, making them suitable for deployment in resource-constrained environments.

## Method Summary
The system uses MatchaTTS with conditional flow matching for mel-spectrogram generation. Text is encoded with speaker and language embeddings (256/192 dim) to disentangle identity from linguistic content. Duration prediction and upsampling connect the encoder to a flow-matching decoder with 10 inference steps. Four attention-free architectures (Mamba2, Hydra, FNet) replace self-attention for efficiency. A Vocos vocoder converts mel-spectrograms to waveforms. Models are trained for 200 epochs on 4 speakers across 3 languages (2.4-11.9h each) with oversampling to balance multilingual training.

## Key Results
- Multilingual models generally outperform monolingual models in all languages
- Attention-free architectures achieve comparable quality to self-attention with 2-4× memory savings at batch 400
- Mamba2 and Hydra closely match self-attention performance while FNet lags behind
- Flow matching with 10 inference steps generates mel-spectrograms faster than diffusion alternatives

## Why This Works (Mechanism)

### Mechanism 1
Training multilingual TTS on typologically similar languages improves performance over monolingual models in low-resource settings. The model learns shared phonetic and prosodic patterns across Algonquian family languages, transferring knowledge from higher-resource pairs (e.g., Ojibwe with 11h) to lower-resource ones (e.g., Mi'kmaq with 2.4h). Speaker and language embeddings disentangle identity from linguistic content, enabling cross-language generalization.

### Mechanism 2
Attention-free architectures (Mamba2, Hydra) achieve comparable synthesis quality to self-attention with substantially lower memory footprint. Mamba2 uses selective state-space models with input-dependent weights, enabling content-aware filtering across timesteps. Hydra extends this bidirectionally via quasiseparable matrix decomposition, processing sequences forward and backward without quadratic attention complexity.

### Mechanism 3
Conditional flow matching generates mel-spectrograms faster than diffusion-based alternatives with competitive quality. Flow matching learns continuous-time transport maps between noise and data distributions, enabling fewer denoising steps (10 by default) than diffusion. The decoder iteratively refines spectrograms conditioned on upsampled text encoder outputs.

## Foundational Learning

- **State-Space Models (SSMs) for sequence modeling**: Mamba2 and Hydra replace attention with SSMs; understanding how ht = At·ht-1 + Bt·xt enables content-dependent sequence mixing is essential for debugging and tuning.
- **Flow matching vs. diffusion**: MatchaTTS uses flow matching rather than diffusion; understanding the inference speed tradeoff helps explain architecture choice.
- **Vocoder role in TTS pipeline**: Vocos converts mel-spectrograms to waveforms; vocoder quality can mask or amplify TTS model artifacts.

## Architecture Onboarding

- **Component map**: Text → [Character Tokenizer] → [Text Encoder + Speaker/Lang Embeddings] → [Duration Predictor] → [Upsampling] → [Flow Matching Decoder] → [Mel-Spectrogram] → [Vocos Vocoder] → Audio
- **Critical path**: Text encoder quality → duration alignment → decoder conditioning. Multilingual models add speaker/language embedding concatenation as critical disentanglement point.
- **Design tradeoffs**: Self-attention offers best quality but 2× memory at batch 400; Mamba2 provides best throughput in float16; Hydra matches attention quality with lowest throughput; FNet is most memory-efficient but lowest quality.
- **Failure signatures**: Robotic/monotone speech indicates duration predictor not learning variation; language confusion suggests embeddings too small or not properly disentangled; memory OOM at batch > 100 requires switching to Mamba2/Hydra/FNet.
- **First 3 experiments**: 1) Monolingual vs. multilingual baseline comparison using MCD and MOS; 2) Architecture sweep measuring quality metrics and memory/throughput at different batch sizes; 3) Ablate oversampling strategy comparing equal-duration vs. proportional sampling.

## Open Questions the Paper Calls Out

- **How can subjective evaluation protocols be redesigned to accommodate Indigenous cultural norms where providing negative feedback is discouraged?** Standard MOS tests failed because one rater assigned perfect scores to all samples, likely due to reluctance to comment negatively on the voice.
- **What data governance frameworks can balance the need for open scientific replication with community concerns regarding malicious use of voice data?** The authors could not make data publicly available due to consent issues and fears of impersonation and deception.
- **To what extent does the deployment of attention-free, multilingual TTS systems improve learning outcomes in Indigenous language education programs?** While the technology is efficient and intelligible, the study currently measures technical success rather than educational efficacy.

## Limitations

- Limited to three specific Indigenous languages with restricted data availability (2.4-11.9h per speaker), constraining generalizability
- Unable to conduct standard human evaluation due to cultural norms around providing negative feedback
- Data governance concerns prevent public release of training data, limiting independent validation

## Confidence

- **High Confidence**: Memory efficiency benefits of attention-free architectures over self-attention are well-established through direct measurements
- **Medium Confidence**: Multilingual training benefits rely on assumption of shared Algonquian phonetic structure, though direct corpus evidence is weak
- **Low Confidence**: Claims about attention-free architectures "closely matching" self-attention quality require careful interpretation due to varying performance across metrics and languages

## Next Checks

1. **Transfer Learning Scope Validation**: Train multilingual models on language pairs from different families to test whether transfer benefits are specific to genetically related languages or represent a more general phenomenon.

2. **Community-Centered Evaluation Protocol**: Develop and implement a culturally appropriate human evaluation framework with Indigenous language communities, focusing on criteria beyond standard MOS.

3. **Architecture Ablation with Matched Parameters**: Retrain Mamba2, Hydra, and FNet with identical hidden dimensions to self-attention to isolate architectural effects from capacity differences.