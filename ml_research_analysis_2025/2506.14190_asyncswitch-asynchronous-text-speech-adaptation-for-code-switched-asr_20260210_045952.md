---
ver: rpa2
title: 'AsyncSwitch: Asynchronous Text-Speech Adaptation for Code-Switched ASR'
arxiv_id: '2506.14190'
source_url: https://arxiv.org/abs/2506.14190
tags:
- text
- speech
- data
- language
- malay
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "AsyncSwitch introduces a three-stage asynchronous adaptation framework\
  \ that leverages large-scale unpaired text and limited speech-text data to improve\
  \ ASR performance in low-resource, code-switched settings. The method first adapts\
  \ the decoder\u2019s internal language model on code-switched text, then aligns\
  \ the encoder and decoder via cross-attention using paired data, and finally fine-tunes\
  \ the full model."
---

# AsyncSwitch: Asynchronous Text-Speech Adaptation for Code-Switched ASR

## Quick Facts
- arXiv ID: 2506.14190
- Source URL: https://arxiv.org/abs/2506.14190
- Authors: Tuan Nguyen; Huy-Dat Tran
- Reference count: 37
- Key outcome: Achieves 9.02% relative WER reduction vs Whisper-Large-v3 on Malay-English code-switching; outperforms I2R A*STAR and Azure by 7.7% and 11.9%.

## Executive Summary
AsyncSwitch is a three-stage asynchronous adaptation framework that leverages abundant unpaired code-switched text and limited paired speech-text data to improve automatic speech recognition (ASR) in low-resource code-switching scenarios. By first adapting the decoder’s internal language model on monolingual and code-switched text, then aligning encoder-decoder via cross-attention using paired data, and finally fine-tuning the full model, AsyncSwitch addresses data scarcity and improves both code-switched and monolingual ASR performance. Evaluated on Malay-English code-switching, it reduces WER by 9.02% relative to Whisper-Large-v3 and outperforms two commercial systems, while also generalizing to the OpenASR Leaderboard without catastrophic forgetting.

## Method Summary
AsyncSwitch addresses the challenge of low-resource code-switched ASR by utilizing large-scale unpaired text data alongside limited paired speech-text data. The method unfolds in three asynchronous stages: (1) decoder internal language model adaptation on monolingual and code-switched text, (2) encoder-decoder cross-attention alignment using paired data, and (3) full model fine-tuning. This staged approach leverages abundant textual resources to improve both code-switched and monolingual ASR performance, with ablation studies confirming the importance of each stage. The framework is specifically designed to work within the constraints of limited paired speech-text data, using text data to bolster the decoder’s language modeling capabilities before integrating speech.

## Key Results
- 9.02% relative WER reduction compared to Whisper-Large-v3 on Malay-English code-switching.
- Outperforms commercial systems (I2R A*STAR and Azure) by 7.7% and 11.9% respectively.
- Improves monolingual performance on Singlish and Malay, not just code-switched utterances.
- Demonstrates strong generalization on the OpenASR Leaderboard without catastrophic forgetting.

## Why This Works (Mechanism)
AsyncSwitch works by strategically leveraging the abundance of unpaired text data to compensate for the scarcity of paired speech-text data in low-resource code-switching scenarios. By first adapting the decoder’s internal language model on monolingual and code-switched text, the framework strengthens the model’s ability to predict likely word sequences in mixed-language contexts. The subsequent alignment of encoder and decoder via cross-attention using limited paired data ensures that the model can effectively map acoustic features to these improved language predictions. Finally, fine-tuning the full model consolidates these gains, enabling robust performance across both code-switched and monolingual utterances. This staged, asynchronous approach allows the model to benefit from large textual corpora without overfitting to limited paired data.

## Foundational Learning
- **Code-switching**: The alternating use of two or more languages within a single utterance or conversation. *Why needed*: Central to the problem domain; AsyncSwitch is specifically designed to handle such mixed-language scenarios where standard monolingual ASR fails. *Quick check*: Does the target application involve bilingual/multilingual user communities or content?
- **Whisper architecture**: A large-scale speech recognition model with an encoder for audio features and a decoder for language modeling. *Why needed*: AsyncSwitch builds upon and adapts the Whisper framework; understanding its structure is key to grasping the adaptation stages. *Quick check*: Is the underlying ASR system based on an encoder-decoder transformer like Whisper?
- **Cross-attention alignment**: Mechanism by which the decoder attends to encoder outputs, crucial for mapping speech to text. *Why needed*: Stage 2 of AsyncSwitch explicitly adapts this alignment for code-switched contexts. *Quick check*: Does the adaptation involve fine-tuning cross-attention weights between encoder and decoder?
- **Language model adaptation**: Updating a model’s language modeling component using text-only data. *Why needed*: Stage 1 leverages this to improve decoder predictions for code-switched text. *Quick check*: Is the approach using unpaired text to improve language modeling before integrating speech data?
- **Catastrophic forgetting**: Loss of previously learned knowledge when adapting to new data or tasks. *Why needed*: AsyncSwitch’s design aims to improve code-switching without degrading monolingual performance. *Quick check*: Does the method show stable performance on both new and original (monolingual) test sets after adaptation?
- **Code-Mixing Index (CMI)**: A metric quantifying the degree and type of code-mixing in text. *Why needed*: Used to characterize and select appropriate text corpora for adaptation. *Quick check*: Are text corpora selected or filtered based on CMI scores to match target code-switching patterns?

## Architecture Onboarding
- **Component map**: Whisper encoder (audio features) -> Whisper decoder (language model) -> Cross-attention alignment (Stage 2) -> Full model fine-tuning (Stage 3)
- **Critical path**: Text-only LM adaptation (Stage 1) -> Cross-attention alignment (Stage 2) -> Full model fine-tuning (Stage 3)
- **Design tradeoffs**: Leverages abundant unpaired text to offset limited paired data, but requires careful stage sequencing and hyperparameter tuning (e.g., merging ratio). May not generalize immediately to distant language pairs or very different code-switching patterns.
- **Failure signatures**: Degraded monolingual performance (catastrophic forgetting), poor adaptation if text corpus CMI mismatches target domain, overfitting if paired data is too scarce.
- **3 first experiments**: (1) Stage 1 adaptation with monolingual + code-switched text only; (2) Stage 2 alignment using paired speech-text data; (3) Full Stage 1+2+3 pipeline evaluation on code-switching benchmark.

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Does AsyncSwitch generalize to code-switching language pairs beyond Malay–English, particularly those with different linguistic typologies or less available code-switched text?
- Basis in paper: [explicit] The authors state in Section VI that the study was limited to Malaysian/Singaporean contexts and that "future applications of AsyncSwitch must carefully evaluate their target text characteristics to ensure domain compatibility."
- Why unresolved: The framework was evaluated only on Malay–English code-switching, leaving its applicability to typologically different pairs (e.g., Mandarin–English, Arabic–French) or low-resource pairs with scarce text unknown.
- What evidence would resolve it: Apply AsyncSwitch to additional code-switched language pairs (e.g., SEAME for Mandarin–English, or other benchmarks) and report WER changes under comparable text and speech data conditions.

### Open Question 2
- Question: What is the optimal scale and composition of the text corpus for Stage 1 adaptation, and is there diminishing returns beyond 38M utterances?
- Basis in paper: [inferred] The ablation (Table III) compares 1.7M vs 38M text utterances, showing trade-offs (1.7M better on Malay/CS but worse on Singlish). The authors note "significantly larger and more diverse 38M corpus better handles real-world code-switching scenarios" but do not test intermediate or larger scales.
- Why unresolved: The relationship between text scale, corpus diversity (CMI), and downstream ASR performance remains unquantified; it is unclear whether further scaling yields continued gains or plateaus.
- What evidence would resolve it: Systematic experiments varying text corpus size (e.g., 5M, 10M, 20M, 50M, 100M utterances) and measuring WER across code-switched, monolingual, and English-only test sets to identify scaling behavior.

### Open Question 3
- Question: Can any stage be safely omitted or simplified without significant performance degradation, enabling a more efficient adaptation pipeline?
- Basis in paper: [inferred] Table II shows incremental improvements from Stage 1 to Stage 3, with Stage 1–2 alone underperforming on Malay and code-switching. However, the necessity of each individual component (e.g., freezing cross-attention in Stage 1, separate alignment stage) is not fully isolated.
- Why unresolved: While the three-stage design is motivated, ablations do not test whether combining stages (e.g., joint text+speech training) or skipping Stage 2 yields comparable results with lower complexity.
- What evidence would resolve it: Additional ablations testing (a) Stage 1 + Stage 3 without Stage 2, (b) joint training of text and speech from initialization, and (c) varying which decoder components are frozen, to measure impact on WER and training efficiency.

### Open Question 4
- Question: How does the choice of merging ratio interact with target domain characteristics, and can it be set automatically?
- Basis in paper: [inferred] Table V shows merging ratio 0.4 optimizes code-switching (17.04 WER) while 0.8 optimizes average (16.77 WER). The authors manually select 0.4 but note the trade-off across scenarios.
- Why unresolved: The optimal ratio appears dataset-dependent; a principled or automated method for selecting this hyperparameter—potentially based on text statistics or validation performance—is not explored.
- What evidence would resolve it: Develop and validate a heuristic or validation-based procedure for setting the merging ratio (e.g., based on CMI of target domain, or Bayesian optimization on a held-out validation set) and report whether it consistently matches or exceeds manually tuned performance.

## Limitations
- Performance gains are demonstrated only for Malay-English code-switching; generalization to other language pairs is untested.
- The optimal size and composition of the text corpus for Stage 1 adaptation is not fully characterized, with no evidence of diminishing returns beyond 38M utterances.
- Direct comparison to commercial systems may not be entirely fair due to lack of transparency into their model configurations and adaptation strategies.

## Confidence
- **High**: Core framework and main performance improvements, as evidenced by systematic three-stage approach and quantitative results.
- **Medium**: Robustness and generalizability to diverse code-switching contexts, due to limited cross-domain and cross-language evaluation.
- **Medium**: Superiority over commercial baselines, since direct model comparability is not established.

## Next Checks
1. Evaluate AsyncSwitch on code-switching datasets involving language pairs with greater typological distance from Malay-English to test generalization.
2. Conduct detailed ablation studies isolating the impact of each adaptation stage, particularly stage 2, and test robustness to hyperparameter variations.
3. Perform long-term incremental learning tests to verify sustained performance and absence of catastrophic forgetting on both seen and unseen code-switching patterns.