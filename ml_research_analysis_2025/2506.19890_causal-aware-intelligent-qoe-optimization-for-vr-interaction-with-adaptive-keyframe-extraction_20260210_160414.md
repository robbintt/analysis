---
ver: rpa2
title: Causal-Aware Intelligent QoE Optimization for VR Interaction with Adaptive
  Keyframe Extraction
arxiv_id: '2506.19890'
source_url: https://arxiv.org/abs/2506.19890
tags:
- keyframe
- user
- state
- motion
- causal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses QoE optimization in multi-user VR interactions
  by balancing ultra-low latency, high-fidelity motion synchronization, and fair resource
  allocation. It proposes a causal-aware RL framework combining adaptive keyframe
  extraction with causal inference to guide efficient exploration.
---

# Causal-Aware Intelligent QoE Optimization for VR Interaction with Adaptive Keyframe Extraction

## Quick Facts
- **arXiv ID:** 2506.19890
- **Source URL:** https://arxiv.org/abs/2506.19890
- **Reference count:** 40
- **Primary result:** VR QoE framework using causal-aware RL achieves >99% delivery ratio and >30% reduction in training iterations

## Executive Summary
This paper addresses quality of experience (QoE) optimization for multi-user VR interactions by balancing ultra-low latency, high-fidelity motion synchronization, and fair resource allocation. The authors propose a causal-aware reinforcement learning framework that combines adaptive keyframe extraction with causal inference to guide efficient exploration. The system jointly optimizes keyframe ratios, bandwidth, and CPU frequency through a mixed-integer programming formulation, using a novel QoE metric based on the Weber-Fechner Law that incorporates perceptual sensitivity and attention-driven priorities. Experiments with the CMU Motion Capture Database demonstrate significant improvements in latency, QoE, and fairness compared to baseline methods.

## Method Summary
The approach formulates VR QoE optimization as a Markov Decision Process where an agent selects actions comprising keyframe ratios, bandwidth allocation, and CPU frequency for each user. The system uses a Partial State Causal DDPG (PS-CDDPG) algorithm that integrates causal influence detection with noise-based active exploration. A key innovation is the causal inference model that predicts next-state distributions and computes causal action influence (CAI) scores to guide exploration. The framework divides states into action-relevant and action-irrelevant components, using only relevant states for CAI calculation to improve training efficiency. The optimization balances perceptual QoE based on Weber-Fechner Law with fairness across users, using a Horizon-fairness metric.

## Key Results
- Achieves successful delivery ratio above 99% while maintaining QoE above 0.75
- Reduces training iterations by over 30% compared to baseline methods
- Demonstrates superior performance in latency reduction and fairness metrics compared to traditional RL approaches

## Why This Works (Mechanism)
The framework's effectiveness stems from its causal-aware exploration strategy that focuses learning on action-relevant state variables. By computing CAI scores through an inference model that predicts next-state distributions, the agent can prioritize exploration in directions most likely to improve QoE. The partial state approach excludes action-irrelevant variables (like user attention levels) from CAI calculations, preventing noise from slowing convergence. The Weber-Fechner Law-based QoE metric accurately captures human perceptual sensitivity to motion changes, while the attention-driven prioritization ensures resources are allocated where users are most likely to notice improvements. The mixed-integer optimization formulation provides a tractable representation of the complex resource allocation problem.

## Foundational Learning
**Weber-Fechner Law for QoE:** Models human perception as logarithmic response to stimulus intensity - needed because VR users perceive motion quality non-linearly; quick check: verify logarithmic scaling in QoE calculations produces diminishing returns for high-quality improvements.
**Causal Action Influence (CAI):** Measures expected change in next state given action - needed to guide exploration toward high-impact decisions; quick check: confirm CAI scores correlate with actual QoE improvements in validation.
**Partial State Separation:** Divides state space into action-relevant and action-irrelevant components - needed to prevent irrelevant variables from degrading causal inference; quick check: verify convergence speed improves when excluding attention variables from CAI computation.

## Architecture Onboarding

**Component Map:** BVH Motion Data → Attention Level Mapping → State Vector → PS-CDDPG Agent → Resource Allocation Actions → Network Transmission → QoE Feedback → Agent Update

**Critical Path:** Motion capture data → attention level discretization → state construction → causal inference model → CAI-weighted exploration → DDPG action selection → resource allocation → QoE calculation → reward signal

**Design Tradeoffs:** The framework trades computational complexity in the causal inference model for faster RL convergence and better exploration efficiency. Using partial state separation reduces inference model complexity but requires careful identification of action-relevant variables.

**Failure Signatures:** Non-convergence occurs when action-irrelevant states are included in CAI calculations; unstable exploration results from unweighted random noise selection rather than CAI-based ranking.

**First Experiments:**
1. Verify BVH preprocessing correctly maps joint coordinates to 4 discrete attention levels (0-3)
2. Test CAI computation with and without action-irrelevant states to confirm convergence differences
3. Validate exploration selection mechanism by comparing CAI-weighted vs. random noise selection on training stability

## Open Questions the Paper Calls Out
**Open Question 1:** Can advanced generative models (e.g., diffusion models, transformers, or normalizing flows) replace DNNs as the inference model for causal influence detection to improve accuracy in approximating next-state distributions?

**Open Question 2:** How does the PS-CDDPG framework scale to larger multi-user VR environments with tens or hundreds of simultaneous users?

**Open Question 3:** Can the framework be extended to integrate real-time user attention prediction rather than assuming attention