---
ver: rpa2
title: 'TMCIR: Token Merge Benefits Composed Image Retrieval'
arxiv_id: '2504.10995'
source_url: https://arxiv.org/abs/2504.10995
tags:
- image
- token
- retrieval
- visual
- fusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces TMCIR, a composed image retrieval (CIR)
  framework designed to address the limitations of existing feature fusion methods
  that fail to accurately capture user intent. TMCIR proposes two key innovations:
  Intent-Aware Cross-Modal Alignment (IACMA) that uses pseudo-target images generated
  by a diffusion model to fine-tune CLIP encoders for better text intent capture,
  and Adaptive Token Fusion (ATF) that dynamically balances visual and textual representations
  through weighted token integration with positional encoding.'
---

# TMCIR: Token Merge Benefits Composed Image Retrieval

## Quick Facts
- **arXiv ID:** 2504.10995
- **Source URL:** https://arxiv.org/abs/2504.10995
- **Reference count:** 40
- **Primary result:** Achieves state-of-the-art performance on Fashion-IQ and CIRR datasets with Recall@10 of 56.57 on Fashion-IQ and Recall@1 of 54.12 on CIRR

## Executive Summary
This paper introduces TMCIR, a composed image retrieval (CIR) framework that addresses the limitations of existing feature fusion methods in accurately capturing user intent. The framework proposes two key innovations: Intent-Aware Cross-Modal Alignment (IACMA) that uses diffusion-generated pseudo-target images to fine-tune CLIP encoders for better text intent capture, and Adaptive Token Fusion (ATF) that dynamically balances visual and textual representations through weighted token integration with positional encoding. TMCIR significantly outperforms state-of-the-art methods on Fashion-IQ and CIRR datasets, demonstrating the effectiveness of its dual approach to improving CIR performance.

## Method Summary
TMCIR consists of a two-stage training process. First, it generates pseudo-target images using a diffusion model (FLUX.1-dev-edit-v0) conditioned on reference images and relative descriptions. These pseudo-targets are then used to fine-tune CLIP encoders contrastively, improving their ability to capture nuanced textual intent. In the second stage, the framework employs Adaptive Token Fusion to dynamically merge visual and textual tokens based on cosine similarity matching. The fusion process preserves fine-grained visual details while encoding textual modifications, with the resulting joint embeddings used for retrieval via cosine similarity to target image embeddings.

## Key Results
- Achieves Recall@10 of 56.57 on Fashion-IQ, outperforming previous best SPRC (54.92)
- Achieves Recall@1 of 54.12 on CIRR, surpassing SPRC (51.96)
- Ablation study shows token merging is critical, with performance dropping from 56.57 to 29.68 on Fashion-IQ without it
- Fine-tuning encoders on pseudo-targets improves performance compared to using pre-trained encoders or real target images

## Why This Works (Mechanism)

### Mechanism 1: Pseudo-Target Generation for Cleaner Supervision
Diffusion-generated pseudo-target images provide less noisy supervisory signals than real target images for encoder alignment. The diffusion model conditions on reference images and relative descriptions to synthesize pseudo-targets that encode only the intended modification, excluding irrelevant background or stylistic noise. This provides purer and more precise supervisory signals for contrastive fine-tuning.

### Mechanism 2: Cross-Modal Token Alignment via Contrastive Fine-Tuning
Task-specific contrastive fine-tuning of CLIP encoders on pseudo-target/description pairs harmonizes token distributions between visual and textual modalities. Using InfoNCE loss, the fine-tuning process maximizes similarity between pseudo-target image embeddings and their corresponding relative descriptions, aligning token representations in a shared embedding space before fusion.

### Mechanism 3: Similarity-Weighted Token Merging with Positional Encoding
Adaptive token fusion via cosine similarity matching preserves fine-grained visual details while encoding textual modifications. For each visual token and text token pair exceeding threshold τ, weighted averaging with positional encoding creates fused tokens. Unmatched tokens are retained with positional residuals, resulting in a balanced joint embedding that outperforms fixed fusion strategies.

## Foundational Learning

- **Concept: Contrastive Learning (InfoNCE Loss)**
  - Why needed here: TMCIR uses contrastive loss in both alignment fine-tuning and final retrieval training to shape embedding spaces.
  - Quick check question: Given a batch of query-target pairs, can you explain why increasing the temperature parameter τ makes the loss more sensitive to hard negatives?

- **Concept: CLIP Architecture (Dual Encoder with Shared Embedding Space)**
  - Why needed here: TMCIR builds on CLIP's pre-trained visual and text encoders, fine-tuning them to align CIR-specific distributions before fusion.
  - Quick check question: What does it mean for visual and text tokens to have "consistent token distributions" in a shared embedding space, and why might pre-trained CLIP fail this on niche datasets?

- **Concept: Vision Transformer Token Representations**
  - Why needed here: The token merging mechanism operates on ViT token sequences, requiring understanding of positional encoding and token-wise operations.
  - Quick check question: In a ViT with 196 spatial tokens (14×14 patches), how would you compute a similarity matrix between visual tokens and M text tokens, and what would its dimensions be?

## Architecture Onboarding

- **Component map:** Pseudo-Target Generation Module -> Encoder Fine-Tuning Module -> Adaptive Token Fusion Module -> Retrieval Head
- **Critical path:** Generate pseudo-targets → Fine-tune CLIP encoders on (pseudo-target, description) pairs → Extract token sequences → Compute similarity matrix → Fuse matching pairs at τ=0.7 → Pool fused tokens → Project to joint embedding → Retrieve by cosine similarity
- **Design tradeoffs:** Pseudo-target vs. real target supervision (cleaner but diffusion-dependent vs. grounded but noisy); similarity threshold τ (lower admits noise, higher loses information); fine-tuning vs. frozen encoders (improves alignment but risks overfitting); token merging vs. simple pooling (superior performance but adds complexity)
- **Failure signatures:** Diffusion hallucination creating spurious correlations; threshold mismatch causing retrieval failures; encoder overfitting to diffusion artifacts; token misalignment producing generic embeddings
- **First 3 experiments:** Validate pseudo-target quality through human evaluation; conduct threshold sensitivity sweep across τ values; ablate encoder fine-tuning strategies comparing frozen, real-target fine-tuned, and pseudo-target fine-tuned conditions

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several implicit questions arise from the methodology and results presented.

## Limitations
- The quality of pseudo-targets depends entirely on the diffusion model's ability to faithfully represent textual modifications, which may fail for complex or abstract attributes
- The adaptive token fusion mechanism introduces hyperparameter sensitivity, particularly the cosine similarity threshold τ=0.7, which may not generalize across different visual domains
- The framework's performance on datasets outside Fashion-IQ and CIRR is unknown, raising questions about generalizability to different CIR variants and visual characteristics

## Confidence

**High Confidence (Mechanistic Understanding):** The token merging framework itself is well-specified and reproducible, with strong empirical support from ablation showing catastrophic failure without token merging (R@10 drops from 56.57 to 29.68).

**Medium Confidence (Empirical Claims):** Superiority over SPRC and other baselines is demonstrated across two datasets with multiple metrics, but evaluation protocol details are limited and statistical significance tests are absent.

**Low Confidence (Generalizability):** The framework's performance on datasets outside Fashion-IQ and CIRR is unknown, and it's unclear whether the same hyperparameters work across diverse visual domains.

## Next Checks

1. **Pseudo-Target Fidelity Analysis:** Generate pseudo-targets for a held-out validation subset and conduct human evaluation to measure how often generated images correctly reflect the modification description, then correlate quality scores with retrieval performance.

2. **Cross-Dataset Hyperparameter Transfer:** Train TMCIR on Fashion-IQ with optimal hyperparameters, then evaluate on CIRR without any hyperparameter tuning to assess whether gains are due to dataset-specific optimization or generalizable improvements.

3. **Ablation of Encoder Fine-Tuning Strategy:** Compare frozen CLIP encoders, fine-tuned on real target images only, and fine-tuned on pseudo-targets only, measuring both retrieval metrics and token similarity distributions to determine whether fine-tuning actually improves alignment or adapts to dataset-specific patterns.