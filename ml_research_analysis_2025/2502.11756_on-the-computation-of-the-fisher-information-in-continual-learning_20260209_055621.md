---
ver: rpa2
title: On the Computation of the Fisher Information in Continual Learning
arxiv_id: '2502.11756'
source_url: https://arxiv.org/abs/2502.11756
tags:
- fisher
- information
- learning
- continual
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates how different methods of computing the Fisher
  Information matrix affect the performance of Elastic Weight Consolidation (EWC)
  in continual learning. The Fisher Information is crucial for EWC's effectiveness
  in preventing catastrophic forgetting when training neural networks on sequential
  tasks.
---

# On the Computation of the Fisher Information in Continual Learning

## Quick Facts
- arXiv ID: 2502.11756
- Source URL: https://arxiv.org/abs/2502.11756
- Reference count: 23
- The study shows that exact computation of Fisher Information diagonal elements yields superior EWC performance compared to approximate methods like empirical or batched Fisher.

## Executive Summary
This paper investigates how different methods of computing the Fisher Information matrix affect Elastic Weight Consolidation (EWC) performance in continual learning. The Fisher Information is crucial for EWC's ability to prevent catastrophic forgetting when training neural networks on sequential tasks. Through experiments on Split MNIST and Split CIFAR-10 benchmarks, the study reveals that computation method significantly impacts EWC's effectiveness, with exact computation yielding the best results even when applied to a subset of training data.

## Method Summary
The paper compares five approaches to computing Fisher Information for EWC: exact computation over all data, sampling data points, sampling labels, empirical Fisher, and a batched approximation of the empirical Fisher. These methods vary in computational cost and accuracy of the Fisher Information approximation. Experiments use Split MNIST with MLP (2×400 ReLU) and Split CIFAR-10 with reduced ResNet-18, training with Adam (lr=0.001) for 2000 iterations per task. The EWC loss combines standard cross-entropy with a quadratic penalty weighted by λ. Code is available at https://github.com/GMvandeVen/continual-learning.

## Key Results
- EXACT Fisher computation achieves highest accuracy (84.91% on CIFAR-10) compared to approximations
- SAMPLE method shows only marginal 0.4-0.5% improvement over EMPIRICAL
- BATCHED approximation requires λ values orders of magnitude larger than EXACT for comparable performance
- EXACT(n=500) on subset of data outperforms full-data approximations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Exact computation of Fisher Information diagonal elements provides a more accurate measure of parameter importance for preventing catastrophic forgetting than approximate methods.
- **Mechanism:** The Fisher diagonal element $F_{i,i}^{old}$ measures sensitivity of network outputs to parameter changes through $\mathbb{E}[(\delta \log p_\theta(y|x)/\delta\theta_i)^2]$. EXACT computation performs full expectation over both data distribution and model's conditional distribution $p_{\hat{\theta}^{old}}(y|x)$, capturing true curvature of the loss landscape. Approximations introduce bias in this curvature estimate.
- **Core assumption:** The diagonal Fisher accurately captures parameter importance; off-diagonal terms (parameter correlations) are negligible.
- **Evidence anchors:**
  - [abstract]: "highlights that many currently reported results for EWC could likely be improved by changing the way the Fisher Information is computed"
  - [Page 5-6, Results]: On Split CIFAR-10, EXACT achieves 84.91% vs EMPIRICAL 83.28%, with EXACT(n=500) at 84.57%—showing that exact computation on subset outperforms full-data approximations
  - [corpus]: Related work on Fisher-Orthogonal Projected Natural Gradient Descent (arXiv:2601.12816) similarly relies on Fisher Information quality for continual learning effectiveness
- **Break condition:** When off-diagonal Fisher elements contain significant information about parameter correlations; when task boundaries are poorly defined.

### Mechanism 2
- **Claim:** The batched approximation of empirical Fisher fundamentally changes the mathematical quantity being computed, requiring substantially different hyperparameter tuning.
- **Mechanism:** BATCHED computes $(\sum_i g_i)^2$ rather than $\sum_i g_i^2$, where $g_i$ are individual gradients. By Jensen's inequality, these differ systematically—batched gradients can be larger or smaller depending on gradient alignment across samples. This changes the scale of Fisher estimates by orders of magnitude.
- **Core assumption:** PyTorch batched gradient computation efficiency justifies the approximation error introduced.
- **Evidence anchors:**
  - [Page 4, Section 5.5]: Defines BATCHED as squaring "aggregated gradients of mini-batches" rather than aggregating squared gradients
  - [Page 5, Figure 1]: BATCHED requires λ values "orders of magnitude larger" than EXACT for comparable performance
  - [corpus]: Limited direct corpus validation; this is primarily an implementation artifact specific to deep learning frameworks
- **Break condition:** When batch gradients have high variance or negative correlation across samples; when batch size varies significantly.

### Mechanism 3
- **Claim:** Sample-based unbiased estimation (SAMPLE) provides marginal improvement over empirical Fisher because model predictions at convergence still diverge from ground truth.
- **Mechanism:** SAMPLE uses Monte Carlo sampling from $p_{\hat{\theta}^{old}}(y|x)$ to estimate inner expectation unbiasedly. EMPIRICAL uses ground-truth labels. The difference depends on how well $p_{\hat{\theta}^{old}}(y|x) \approx p_{true}(y|x)$—at imperfect optima, this approximation degrades.
- **Core assumption:** The model has reached a reasonably good optimum where predicted and true distributions approximately align.
- **Evidence anchors:**
  - [Page 3, Section 5.4]: Chaudhry et al. argued empirical Fisher should behave similarly to true Fisher "because at a good optimum the model distribution approaches the ground-truth output distribution"
  - [Page 5-6, Tables 1-2]: SAMPLE shows only 0.4-0.5% improvement over EMPIRICAL on Split CIFAR-10; Kunstner et al. (cited) cautioned against empirical Fisher in optimization literature
  - [corpus]: EWC-Guided Diffusion Replay paper (arXiv:2509.23906) pairs EWC with generative replay, suggesting Fisher-based methods benefit from complementary approaches
- **Break condition:** On poorly-trained models with large train-test gap; on tasks with high label noise or ambiguity.

## Foundational Learning

- **Concept: Fisher Information Matrix**
  - Why needed here: Central to EWC's regularization; measures how informative data is about parameters; diagonal used as importance weights
  - Quick check question: Can you explain why diagonal Fisher elements measure parameter sensitivity to output changes?

- **Concept: Laplace Approximation for Bayesian Inference**
  - Why needed here: Bayesian motivation for EWC approximates posterior as Gaussian with Fisher as precision matrix
  - Quick check question: How does approximating $p(\theta|D_{old})$ as Gaussian with Fisher precision connect to the quadratic penalty in EWC?

- **Concept: Catastrophic Forgetting in Sequential Learning**
  - Why needed here: The core problem EWC addresses; understanding why standard SGD fails on sequential tasks motivates the regularization approach
  - Quick check question: Why does optimizing $\ell_{new}(\theta)$ alone cause performance degradation on old tasks?

## Architecture Onboarding

- **Component map:**
  [Task 1 Training] → [Compute Fisher F¹ on D₁] → [Store θ¹, F¹]
                          ↓
  [Task 2 Training] → [Loss = ℓ₂(θ) + λ/2 Σ F¹ᵢᵢ(θᵢ - θ¹ᵢ)²]
                          ↓
  [Task 2 Complete] → [Compute Fisher F² on D₂] → [Update stored params]

- **Critical path:**
  1. After task training converges, compute Fisher on task data
  2. Store diagonal Fisher + optimal parameters for that task
  3. During subsequent training, add weighted quadratic penalty
  4. Choice of Fisher computation method (EXACT vs EMPIRICAL vs BATCHED) determines both accuracy and computational cost

- **Design tradeoffs:**
  | Method | Accuracy | Compute Cost | λ Scale Needed |
  |--------|----------|--------------|----------------|
  | EXACT | Highest | O(|D| × classes) | Baseline |
  | EXACT(n) | High | O(n × classes) | Baseline |
  | SAMPLE | Medium-High | O(|D|) | ~Baseline |
  | EMPIRICAL | Medium | O(|D|) | ~Baseline |
  | BATCHED | Medium | O(|D|/b) | 10-1000× higher |

- **Failure signatures:**
  - λ tuning appears impossible → likely using BATCHED with EXACT-scale hyperparameters
  - Performance degrades on harder tasks → using approximation when EXACT feasible
  - Different reproduction results with "same EWC" → hidden Fisher computation differences
  - Training instability → Fisher values too large/small for chosen λ

- **First 3 experiments:**
  1. **Baseline establishment:** Implement EXACT Fisher computation; validate on Split MNIST with λ sweep [0.1, 1, 10, 100, 1000]; confirm ~99% accuracy
  2. **Ablation by method:** Compare EXACT, EMPIRICAL, BATCHED on Split CIFAR-10 with matched λ ranges; quantify performance gap and optimal λ shift
  3. **Sample efficiency test:** Using EXACT(n=500), measure performance vs n∈{100, 500, 1000, full}; identify minimum samples for <1% accuracy drop

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the superiority of exact Fisher computation over empirical or batched approximations persist in more complex class-incremental learning scenarios?
- Basis: [inferred] The experiments were limited to the task-incremental learning scenario with separate output heads, which the author noted might be "not difficult enough to elicit significant differences" in some comparisons (e.g., on Split MNIST).
- Why unresolved: The paper did not test scenarios where the network must distinguish between all classes seen so far without task-specific heads, a setting where the precision of the Fisher Information might be more critical.
- What evidence would resolve it: Replicating the comparison of EXACT, EMPIRICAL, and BATCHED methods on class-incremental benchmarks (e.g., CIFAR-100) using a single output head.

### Open Question 2
- Question: Is there a theoretical scaling relationship between the batch size used in the batched empirical Fisher and the required regularization hyperparameter $\lambda$?
- Basis: [explicit] The paper notes that when using the BATCHED method, EWC requires a hyperparameter "orders of magnitude larger" than the best hyperparameter for the EXACT option.
- Why unresolved: The paper empirically demonstrates the shift in optimal hyperparameters but does not derive a mathematical explanation for why aggregating gradients before squaring necessitates such a large increase in $\lambda$.
- What evidence would resolve it: A theoretical analysis or empirical study mapping the scaling factor of $\lambda$ relative to batch size $b$ to normalize the penalty term across different implementations.

### Open Question 3
- Question: Do the findings regarding Fisher computation accuracy generalize to other continual learning or optimization methods that utilize the Fisher Information?
- Basis: [inferred] The author focuses solely on EWC but notes that the Fisher Information is "frequently used in the optimization literature" and that previous recommendations against the empirical Fisher have not reached the continual learning community.
- Why unresolved: It remains unclear if the robustness of the empirical Fisher observed in EWC transfers to methods like Online EWC, Synaptic Intelligence, or natural gradient descent.
- What evidence would resolve it: Evaluating the performance of other Fisher-based algorithms using the EXACT versus BATCHED computation methods on the same benchmarks.

## Limitations

- Limited to classification tasks with diagonal Fisher approximations
- Does not explore full Fisher matrices or tasks with complex output distributions
- Recommendations about sample reduction efficiency not tested across diverse architectures

## Confidence

- **High:** EXACT Fisher computation yields superior performance to approximations; the mathematical distinction between BATCHED and EMPIRICAL methods is correctly identified.
- **Medium:** SAMPLE Fisher provides meaningful but marginal improvement over EMPIRICAL; the computational efficiency claims are plausible but not benchmarked.
- **Low:** The recommendation to reduce training samples rather than compromise on Fisher accuracy has not been tested across diverse architectures or dataset complexities.

## Next Checks

1. **Hyperparameter Transferability:** Systematically test whether optimal λ values transfer between Fisher computation methods when scaled by the theoretical factor suggested in the paper.
2. **Sample Size Optimization:** For EXACT(n), measure accuracy degradation as n varies from 100 to 10,000 samples to identify the minimum viable sample size for near-optimal performance.
3. **Method Robustness:** Evaluate Fisher computation methods on non-image tasks (e.g., language modeling) and with full Fisher matrices to assess generalizability beyond diagonal approximations.