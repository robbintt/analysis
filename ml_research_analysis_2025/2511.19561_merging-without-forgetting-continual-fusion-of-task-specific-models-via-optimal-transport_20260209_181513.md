---
ver: rpa2
title: 'Merging without Forgetting: Continual Fusion of Task-Specific Models via Optimal
  Transport'
arxiv_id: '2511.19561'
source_url: https://arxiv.org/abs/2511.19561
tags:
- merging
- task
- continual
- fusion
- otmf
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of merging models fine-tuned for
  different tasks into a single unified model while preserving task-specific knowledge
  and reducing distribution shift. The authors propose OTMF (Optimal Transport-based
  Masked Fusion), a novel framework that uses optimal transport theory to align the
  semantic geometry of task-specific models by discovering common masks applied to
  task vectors.
---

# Merging without Forgetting: Continual Fusion of Task-Specific Models via Optimal Transport

## Quick Facts
- arXiv ID: 2511.19561
- Source URL: https://arxiv.org/abs/2511.19561
- Reference count: 40
- Key outcome: OTMF achieves state-of-the-art performance in continual model merging with 79.7% average accuracy on ViT-B/32 benchmarks while using less CUDA memory than Task-Wise AdaMerging

## Executive Summary
This paper addresses the challenge of merging models fine-tuned for different tasks into a single unified model while preserving task-specific knowledge and reducing distribution shift. The authors propose OTMF (Optimal Transport-based Masked Fusion), a novel framework that uses optimal transport theory to align the semantic geometry of task-specific models by discovering common masks applied to task vectors. These masks selectively extract transferable and task-agnostic components while preserving the unique structural identities of each task. The method also supports a continual fusion paradigm that incrementally integrates each new task vector without revisiting previous ones, maintaining a bounded memory footprint.

## Method Summary
OTMF works by extracting task vectors (Δθ = θ_task - θ_pretrained) from fine-tuned models and fusing them through learnable element-wise masks optimized via optimal transport (Sinkhorn distance). The framework alternates mask updates between pre- and post-task distributions over 100 epochs, using α=0.8 to favor knowledge retention. For vision models, optional classification head fine-tuning (25% data, 100 epochs) addresses residual distribution shift. The continual paradigm recursively uses the merged model as the "pre" model for subsequent tasks, enabling bounded memory scaling.

## Key Results
- Achieves 79.7% average accuracy on ViT-B/32 across 8-20 vision benchmarks
- Outperforms sequential methods like MINGLE and Task-Wise AdaMerging
- Uses less CUDA memory than Task-Wise AdaMerging while maintaining superior accuracy
- Demonstrates effective continual fusion without revisiting previous tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Optimal transport-based mask learning reduces distribution shift by aligning merged model features with task-specific feature distributions.
- Mechanism: The framework computes Sinkhorn distances between the merged model's latent distribution (μ_merged) and both pre/post task distributions (μ_pre, μ_post). Learnable masks are updated via gradients of this OT loss to minimize transport cost, selectively preserving parameters that contribute to distributional alignment.
- Core assumption: Feature-space distribution alignment is a sufficient proxy for preserving task knowledge; semantic geometry is more informative than parameter-space proximity.
- Evidence anchors:
  - [abstract] "OTMF aligns the semantic geometry of task-specific models by discovering common masks applied to task vectors through optimal transport plans."
  - [section 3.2] "Minimizing ∆λ_total ensures that the merged representation remains aligned with both task-specific distributions."
  - [corpus] RECALL paper similarly leverages representation alignment for catastrophic forgetting alleviation, supporting the representation-focused approach.
- Break condition: If task distributions are highly heterogeneous with minimal shared structure, the OT plan may converge to suboptimal mappings (explicitly noted in limitations).

### Mechanism 2
- Claim: Learnable element-wise masks enable selective extraction of transferable (task-agnostic) components while preserving unique task-specific structure.
- Mechanism: Two masks (M_pre, M_post) modulate task vectors via element-wise multiplication before convex combination: Δθ_m = α·(M_pre ⊙ Δθ_pre) + (1-α)·(M_post ⊙ Δθ_post). Only masks are optimized; task vectors remain frozen. The α≈0.8 bias toward pre-model favors knowledge retention.
- Core assumption: Task-agnostic and task-specific components are separable via element-wise masking; gradient flow through OT loss reveals this separation.
- Evidence anchors:
  - [abstract] "These masks selectively extract transferable and task-agnostic components while preserving the unique structural identities of each task."
  - [section 4.2] "These masks, which are initialized uniformly, are optimized to balance the contributions of each task vector during merging."
  - [corpus] LoRI addresses similar cross-task interference through low-rank adaptation, suggesting interference reduction is a shared mechanism across approaches.
- Break condition: If task vectors have complex, non-axis-aligned interference patterns, element-wise masks may be insufficient.

### Mechanism 3
- Claim: Continual fusion with recursive supervision enables bounded memory scaling while maintaining knowledge accumulation.
- Mechanism: At step t, the previously merged model becomes the "pre" model for step t+1. The fusion operator F produces Δθ_m^(t) = F(Δθ_m^(t-1), Δθ^(t)). Classification head fine-tuning (100 epochs, 25% data) adapts to residual distribution shift.
- Core assumption: Recursive supervision through merged-model-as-pre provides sufficient signal for knowledge retention without explicit replay.
- Evidence anchors:
  - [abstract] "OTMF further supports a continual fusion paradigm that incrementally integrates each new task vector without revisiting previous ones, maintaining a bounded memory footprint."
  - [section 4.1] "This recursive supervision mechanism allows the model to retain knowledge from earlier tasks and incrementally adapt to new ones."
  - [corpus] Weak direct corpus validation for this specific recursive mechanism; MINGLE and Merge-before-Forget address continual merging but with different approaches (null-space gating, LoRA merging).
- Break condition: If the merged model degrades significantly after many tasks, recursive errors may compound.

## Foundational Learning

- Concept: **Optimal Transport / Wasserstein Distance**
  - Why needed here: Core mathematical framework for measuring distributional alignment cost; Sinkhorn regularization makes it tractable for high-dimensional features.
  - Quick check question: Can you explain why Wasserstein distance captures geometric structure better than KL divergence for this application?

- Concept: **Task Vectors (Model Arithmetic)**
  - Why needed here: The paper represents fine-tuned knowledge as Δθ = θ_finetuned - θ_pretrained; all fusion operates on this differential representation.
  - Quick check question: What assumption does task-vector-based merging make about the pretrained model's role?

- Concept: **Catastrophic Forgetting in Continual Learning**
  - Why needed here: The problem formulation directly addresses forgetting through distribution shift minimization; BWT (Backward Transfer) is a key metric.
  - Quick check question: Why is distribution shift in feature space more problematic than parameter shift for task performance?

## Architecture Onboarding

- Component map:
  Frozen PTM backbone (θ^(0)) -> Task vectors (Δθ^(t)) -> Learnable masks (M_pre, M_post) -> OT loss module -> Classification heads -> Fusion controller

- Critical path:
  1. Load frozen PTM + current merged model + incoming task model
  2. Extract task vectors (differences from PTM)
  3. Initialize/reset masks to ones
  4. Forward pass: Apply masks → fuse → reconstruct merged model
  5. Compute OT loss between merged and pre/post feature distributions
  6. Backprop through masks only (100 epochs, lr=0.01, Adam)
  7. Optional: Fine-tune pre-task classification head (100 epochs, 25% data)
  8. Replace merged model; proceed to next task

- Design tradeoffs:
  - α=0.8 favors retention over adaptation; ablation shows this is optimal but may underfit new tasks in fast-changing domains
  - Head fine-tuning improves ViT (+7.5 avg acc) but adds overhead; not needed for Flan-T5
  - Sinkhorn vs. exact OT: Regularization trades precision for O(n² log(1/ε)) vs. O(n³) complexity
  - Element-wise masking: Simple and scalable, but may miss structured interference patterns

- Failure signatures:
  - Accuracy drops sharply on early tasks → α too low or mask learning diverged
  - New task underperforms → α too high (over-retention)
  - CUDA OOM during OT computation → reduce batch size for feature extraction
  - Negative BWT despite training → distribution shift not being minimized; check OT loss convergence

- First 3 experiments:
  1. **Sanity check**: Run 2-task merging on SUN397 + Cars with α=0.8; verify merged model achieves >65% average (Table 2 shows 68.8 + 66.5 = 67.7% for OTMF vs. baseline degradation)
  2. **Ablation on α**: Sweep α ∈ {0.5, 0.6, 0.7, 0.8, 0.9} on 8-task ViT-B/32; confirm peak at 0.8 per Table 5
  3. **Component isolation**: Run without OT masks (direct task arithmetic), without head fine-tuning, and full OTMF; quantify each contribution (ablation shows masks: 79.7→64.0; heads: 79.7→72.2)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the OTMF framework be adapted to prevent suboptimal mappings when merging models from highly heterogeneous domains or dissimilar fine-tuning objectives?
- Basis in paper: [explicit] The authors state in the conclusion that the method "assumes that the incoming task vectors are reasonably aligned in distribution space," and warns that for "drastically dissimilar model fine-tuning objectives... the learned optimal transport plan may converge to suboptimal mappings."
- Why unresolved: The current theoretical formulation relies on the assumption of reasonable distribution alignment between tasks, which breaks down in scenarios with high domain heterogeneity, limiting the ability to extract transferable components.
- What evidence would resolve it: A modification to the OT cost function or masking strategy that maintains high accuracy when merging models trained on fundamentally different data distributions (e.g., merging a text-to-image model with a sentiment analysis model).

### Open Question 2
- Question: Can the memory overhead introduced by storing learnable mask parameters be minimized without compromising the fine-grained alignment performance?
- Basis in paper: [explicit] The paper notes that while effective, "learnable masks... necessitates extra training and introduces additional memory overhead from storing mask parameters" as a distinct limitation.
- Why unresolved: While OTMF reduces memory usage compared to joint merging methods, the requirement to store and update masks for every parameter conflicts with extreme resource-constrained environments.
- What evidence would resolve it: The development of a sparse or structured mask variant within the OTMF framework that reduces the parameter count for masks (e.g., via low-rank approximation) while retaining the 79.7% average accuracy on ViT-B/32 benchmarks.

### Open Question 3
- Question: Is it possible to mitigate the residual distribution shift in ViT models without requiring the auxiliary classification head fine-tuning step?
- Basis in paper: [explicit] The authors identify that "classification head fine-tuning introduces a small amount of additional training overhead," implying a dependency on this step to resolve residual shifts.
- Why unresolved: Relying on head fine-tuning requires labeled data (25% of samples) and extra epochs, partially violating the goal of a data-free or strictly "no-replay" fusion paradigm.
- What evidence would resolve it: An ablation study or methodological tweak where the OT plan is extended to explicitly align the output space/logits, achieving comparable performance to the fine-tuned baseline (Table 1) with the classification head frozen.

## Limitations

- Suboptimal mappings for highly heterogeneous task distributions where OT plans may fail to find meaningful alignments
- Additional memory overhead from storing learnable mask parameters for every model parameter
- Residual distribution shift in ViT models requiring auxiliary classification head fine-tuning

## Confidence

- **High** for the core OT-based mask learning mechanism, which is mathematically rigorous and experimentally validated across vision and language tasks
- **Medium** for the continual fusion paradigm, as recursive supervision lacks extensive ablation and may compound errors over many tasks
- **Low** for cross-task applicability, given limited exploration of highly heterogeneous task distributions where OT plans may fail to find meaningful alignments

## Next Checks

1. **Stress Test Heterogeneity**: Run OTMF on deliberately dissimilar task pairs (e.g., medical imaging + natural language) to quantify performance degradation when OT alignment fails.
2. **Recursive Error Analysis**: Track per-task BWT over 20+ sequential merges to measure compounding error rates and identify thresholds where recursive supervision breaks down.
3. **Memory-OT Scaling**: Implement approximate OT (e.g., sliced Wasserstein or mini-batch) and compare against exact Sinkhorn on CLIP-ViT-L/14 to establish practical scalability limits.