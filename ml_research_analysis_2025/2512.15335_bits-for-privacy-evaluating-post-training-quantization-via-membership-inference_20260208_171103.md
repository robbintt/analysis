---
ver: rpa2
title: 'Bits for Privacy: Evaluating Post-Training Quantization via Membership Inference'
arxiv_id: '2512.15335'
source_url: https://arxiv.org/abs/2512.15335
tags:
- quantization
- privacy
- accuracy
- precision
- brecq
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically investigates how post-training quantization
  (PTQ) affects privacy against membership inference attacks (MIAs) for the first
  time. The authors evaluate three PTQ methods (AdaRound, BRECQ, and OBC) across three
  quantization levels (4-bit, 2-bit, and 1.58-bit) on CIFAR-10, CIFAR-100, and TinyImageNet
  using state-of-the-art LiRA attacks.
---

# Bits for Privacy: Evaluating Post-Training Quantization via Membership Inference

## Quick Facts
- arXiv ID: 2512.15335
- Source URL: https://arxiv.org/abs/2512.15335
- Reference count: 37
- Primary result: Post-training quantization at extreme low-bit (1.58-bit) reduces membership inference vulnerability by up to an order of magnitude compared to full-precision models, at the cost of utility.

## Executive Summary
This study systematically investigates how post-training quantization (PTQ) affects privacy against membership inference attacks (MIAs) for the first time. The authors evaluate three PTQ methods (AdaRound, BRECQ, and OBC) across three quantization levels (4-bit, 2-bit, and 1.58-bit) on CIFAR-10, CIFAR-100, and TinyImageNet using state-of-the-art LiRA attacks. Results consistently show that lower-precision PTQs reduce privacy leakage - with 1.58-bit quantization achieving up to an order of magnitude reduction in TPR@0.1%FPR compared to full-precision models. However, this privacy benefit comes at the cost of decreased utility, particularly for the more complex TinyImageNet dataset. An ablation study demonstrates that selectively quantizing only the last layer to higher precision (e.g., 8-bit) can effectively balance the privacy-utility trade-off while maintaining strong privacy protection. The findings provide actionable guidance for practitioners to optimize efficiency, utility, and privacy in real-world deployments of quantized models.

## Method Summary
The paper evaluates post-training quantization methods (AdaRound, BRECQ, OBC) on ResNet18, ResNet50, and DenseNet121 architectures across CIFAR-10, CIFAR-100, and TinyImageNet datasets. Models are trained on 50% of training data for 100 epochs using SGD with cosine annealing. PTQ is applied using 1024 calibration samples from the training set at target bit-widths (4-bit, 2-bit, 1.58-bit). Membership inference vulnerability is measured using LiRA attacks (online and offline modes) with 64 shadow models per architecture, where each training sample appears in exactly half of shadow datasets. Privacy is quantified via TPR@0.1%FPR and log-AUROC metrics.

## Key Results
- Lower-precision PTQs consistently reduce membership inference vulnerability, with 1.58-bit quantization achieving up to 99.2% reduction in TPR@0.1%FPR
- Privacy gains come at significant utility cost, especially for TinyImageNet where 1.58-bit quantization causes 26% accuracy drop
- Decoupled quantization (8-bit last layer) effectively recovers utility while maintaining strong privacy protection
- Different PTQ methods show varying privacy-utility profiles, with BRECQ generally achieving better accuracy at equivalent bit-widths

## Why This Works (Mechanism)

### Mechanism 1
Lower-precision quantization reduces membership inference vulnerability by limiting the information capacity of model parameters. Quantization maps continuous 32-bit floating-point weights to discrete low-bit values (e.g., {0,1,2} for 1.58-bit), reducing the model's ability to encode fine-grained training signal patterns that MIAs exploit. The discrete parameter space acts as implicit regularization, smoothing loss distributions between members and non-members.

### Mechanism 2
Decoupled quantization preserves utility while maintaining privacy benefits by selectively maintaining precision in task-critical layers. The final classification layer directly determines output predictions. Keeping it at 8-bit while quantizing earlier layers to 1.58-bit preserves discriminative power for the primary task, while earlier layers (feature extractors) remain quantized and privacy-protective.

### Mechanism 3
Different PTQ optimization objectives produce quantized models with varying privacy-utility profiles due to their reconstruction error minimization strategies. BRECQ's block-level reconstruction preserves cross-layer dependencies better than layer-wise AdaRound, achieving higher accuracy at equivalent bit-width. OBC's greedy weight selection based on Hessian importance creates different parameter perturbation patterns that affect both utility and privacy non-uniformly.

## Foundational Learning

- Concept: **Post-Training Quantization (PTQ)**
  - Why needed here: PTQ is the intervention being studied; understanding its calibration data requirements and weight-only vs. activation quantization is essential for reproducing results.
  - Quick check question: Can you explain why PTQ uses calibration data distinct from training data, and what risks arise if calibration data overlaps with evaluation data?

- Concept: **Membership Inference Attacks (MIAs) and LiRA specifically**
  - Why needed here: LiRA is the evaluation framework; comprehending shadow model training, likelihood ratio testing, and TPR@low-FPR metrics is necessary to interpret privacy claims.
  - Quick check question: Why does TPR@0.1%FPR provide a more meaningful privacy threat measure than AUROC or overall accuracy?

- Concept: **Privacy-Utility Trade-offs**
  - Why needed here: The paper's core contribution is characterizing this trade-off; understanding why privacy improvements typically cost utility helps contextualize the decoupled quantization contribution.
  - Quick check question: What are two reasons why reducing model capacity (via quantization) might simultaneously harm utility and improve privacy?

## Architecture Onboarding

- Component map: Train full-precision model → Apply PTQ with specific method + bit-width → Train shadow model ensemble → Run LiRA (online/offline) → Compute TPR@0.1%FPR

- Critical path: 1. Train full-precision target model → 2. Apply PTQ with specific method + bit-width → 3. Train shadow model ensemble → 4. Run LiRA (online/offline) → 5. Compute TPR@0.1%FPR

- Design tradeoffs:
  - 4-bit vs. 2-bit vs. 1.58-bit: 4-bit preserves utility (~full-precision accuracy) with minimal privacy gain; 2-bit offers moderate privacy-utility trade-off; 1.58-bit maximizes privacy but causes significant accuracy drops on complex datasets
  - LiRA online vs. offline: Online is stronger attack (higher TPR) but computationally expensive; offline is efficient but may underestimate vulnerability
  - Uniform vs. decoupled quantization: Uniform is simpler; decoupled (8-bit last layer) recovers utility at cost of slightly higher privacy leakage

- Failure signatures:
  - Calibration data leakage: Using held-out test data for quantization calibration inflates accuracy and may distort privacy estimates
  - Insufficient shadow models: Fewer than 64 shadow models increases variance in TPR estimates, particularly for BRECQ at 1.58-bit
  - Ignoring fixed variance mode: When shadow model count is low, not using fixed variance degrades LiRA performance

- First 3 experiments:
  1. Baseline establishment: Train ResNet18 on 50% of CIFAR-100, apply BRECQ at 4-bit/2-bit/1.58-bit, run LiRA offline with 16 shadow models to verify TPR@0.1%FPR reduction trend
  2. Decoupled quantization validation: Repeat with 1.58-bit body + 8-bit last layer on TinyImageNet, measure accuracy recovery and TPR increase
  3. Architecture generalization test: Apply OBC quantization to ResNet50 and DenseNet121 on CIFAR-100, verify privacy-utility trade-off pattern holds across architectures

## Open Questions the Paper Calls Out

### Open Question 1
How do PTQ methods affect privacy leakage in non-vision domains such as natural language processing or graph neural networks? All experiments are limited to vision benchmarks (CIFAR-10, CIFAR-100, TinyImageNet).

### Open Question 2
What is the combined effect of activation quantization and weight quantization on membership inference vulnerability? Experiments quantize weights only; activations remain full-precision throughout.

### Open Question 3
Can PTQ be combined with other privacy defenses (e.g., differential privacy) to achieve superior privacy-utility trade-offs? No experiments investigate stacked defenses combining PTQ with techniques like DP-SGD.

### Open Question 4
How does model scale affect the privacy-utility trade-off in quantized versus full-precision models? Experiments focus on ResNet18-scale models; larger models require extensive computation not performed here.

## Limitations

- Experiments limited to vision datasets (CIFAR-10, CIFAR-100, TinyImageNet) without validation in other domains like NLP or graphs
- Only weight quantization studied, with activations remaining full-precision throughout
- Theoretical justification for why different PTQ methods produce systematically different privacy profiles is lacking
- High variance in privacy measurements, particularly for BRECQ at extreme low-bit precision

## Confidence

- Privacy-utility trade-off characterization: High (consistent TPR@0.1%FPR reductions across all PTQ methods at extreme low-bit)
- Decoupled quantization contribution: Medium (results show utility recovery but privacy cost quantification is limited to one attack mode)
- Method-specific privacy differences: Low (high variance and lack of theoretical grounding for why BRECQ produces different privacy profiles than AdaRound)

## Next Checks

1. Reproduce the 1.58-bit TPR@0.1%FPR reduction on CIFAR-10 with all three PTQ methods using the exact calibration data protocol to verify the claimed 99.2% reduction is reproducible.

2. Evaluate the decoupled quantization approach under LiRA offline mode on TinyImageNet to determine if the 8-bit last layer increases privacy leakage beyond the reported 2× TPR increase.

3. Compare privacy-utility trade-offs for 1.58-bit PTQ against other capacity-reduction techniques (pruning, knowledge distillation) on CIFAR-100 to isolate whether quantization's privacy benefit is unique or generic to model compression.