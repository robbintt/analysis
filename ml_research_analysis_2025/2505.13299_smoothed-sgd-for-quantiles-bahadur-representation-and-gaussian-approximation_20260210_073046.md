---
ver: rpa2
title: 'Smoothed SGD for quantiles: Bahadur representation and Gaussian approximation'
arxiv_id: '2505.13299'
source_url: https://arxiv.org/abs/2505.13299
tags:
- quantile
- where
- algorithm
- theorem
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses memory and monotonicity issues in quantile
  estimation for streaming data by proposing a smoothed version of the stochastic
  gradient descent (SGD) algorithm. The key innovation is replacing the indicator
  function in the standard SGD quantile estimator with a smooth, piece-wise linear
  approximation, ensuring monotonicity in the quantile level and eliminating quantile
  crossing.
---

# Smoothed SGD for quantiles: Bahadur representation and Gaussian approximation

## Quick Facts
- **arXiv ID:** 2505.13299
- **Source URL:** https://arxiv.org/abs/2505.13299
- **Reference count:** 40
- **Primary result:** Proposes smoothed SGD quantile estimator with monotonicity guarantees, non-asymptotic tail bounds, and uniform Bahadur representation enabling Gaussian approximation for simultaneous inference over quantile levels and dimensions.

## Executive Summary
This paper addresses the memory and monotonicity issues in quantile estimation for streaming data by proposing a smoothed version of the stochastic gradient descent (SGD) algorithm. The key innovation is replacing the indicator function in the standard SGD quantile estimator with a smooth, piece-wise linear approximation, ensuring monotonicity in the quantile level and eliminating quantile crossing. Theoretical contributions include non-asymptotic tail probability bounds for both smoothed SGD with and without Polyak-Ruppert averaging, and a uniform Bahadur representation for the averaged case, enabling Gaussian approximation. The Gaussian approximation result supports uniform confidence bands over both quantile levels and data dimensions, with dimension p allowed to grow exponentially in sample size n. Simulations confirm good finite-sample performance, with empirical coverage close to nominal levels, especially for large n and appropriate learning rate β. The method is shown to work well for both unconditional and conditional quantile estimation, including in high-dimensional settings.

## Method Summary
The paper proposes a smoothed SGD algorithm for online quantile estimation in streaming data. The method recursively updates quantile estimates using a piece-wise linear approximation of the indicator function in the quantile loss, ensuring monotonicity across quantile levels. Polyak-Ruppert averaging is applied to achieve √n-rate convergence. The algorithm works for both unconditional and conditional quantile estimation in high dimensions. For inference, a uniform Bahadur representation is established for the averaged algorithm, enabling Gaussian approximation using a Brownian bridge process. The approach allows simultaneous inference over both quantile levels and dimensions, with the dimension p permitted to grow exponentially with sample size n.

## Key Results
- Smoothed SGD with piece-wise linear approximation ensures monotonicity in estimated quantile curves, preventing quantile crossing
- Non-asymptotic tail probability bounds established for both smoothed SGD with and without Polyak-Ruppert averaging
- Uniform Bahadur representation for averaged case enables Gaussian approximation with convergence rate log^(7/6)(np)n^(-1/6) + log²(np)n^(-r)
- Gaussian approximation supports simultaneous inference over quantile levels and dimensions, with p allowed to grow exponentially in n
- Empirical simulations show good finite-sample performance with coverage close to nominal levels for large n and appropriate learning rate β

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Smoothing the quantile score function via piecewise linear approximation preserves monotonicity in estimated quantile curves, preventing quantile crossing.
- Mechanism: The standard SGD quantile algorithm uses a discontinuous indicator function 1{Y_{i,k}(τ) - X_{i,k+1} ≥ 0}, which causes non-monotonic estimates across τ. Replacing this with g_{aγ_k}(·), a Lipschitz-continuous piecewise linear function with constant (2aγ_k)^(-1), ensures that Y_{i,k+1}(τ) - Y_{i,k+1}(τ') ≥ γ_k(τ - τ') ≥ 0 when τ ≥ τ', by induction.
- Core assumption: The smoothing bandwidth decreases adaptively as γ_k = c_γ k^(-β), so the kernel-width shrinks as iterations progress, allowing convergence while maintaining monotonicity.
- Evidence anchors:
  - [abstract]: "By smoothing the score function in the conventional SGD quantile algorithm, we achieve monotonicity in the quantile level in that the estimated quantile curves do not cross."
  - [section]: Lemma 2.1 provides the formal monotonicity proof via induction on the Lipschitz property of g_{aγ_k}.
  - [corpus]: No direct corpus evidence on piecewise linear smoothing; related work (Fernandes et al. 2021, He et al. 2023) uses convolution-type smoothing in batch quantile regression, not adaptive online smoothing.
- Break condition: If a ≤ 1/2 or if γ_k does not decrease, the Lipschitz bound (2aγ_k)^(-1)γ_k may not guarantee monotonicity preservation.

### Mechanism 2
- Claim: Polyak-Ruppert averaging provides √n-rate convergence and enables the uniform Bahadur representation.
- Mechanism: Averaging Ȳ_{i,n}(τ) = n^(-1)∑_{k=1}^n Y_{i,k}(τ) reduces variance from martingale noise ξ_{i,k+1}(τ). The averaging cancels higher-order noise while preserving the signal, yielding a linear term ξ̄_{i,n}(τ)/f_i(Q_i(τ)) plus a smaller remainder. This linearity is what permits the Bahadur representation.
- Core assumption: Learning rate satisfies 1/2 < β < (1+√5)/4 ≈ 0.809 (Assumption 2); this balances initial noise suppression against final-stage variance reduction.
- Evidence anchors:
  - [abstract]: "For the latter [averaged version], we also provide a uniform Bahadur representation and a resulting Gaussian approximation result."
  - [section]: Theorem 4.1 establishes the uniform Bahadur representation with remainder bounds in probability.
  - [corpus]: "Online Inference for Quantiles by Constant Learning-Rate Stochastic Gradient Descent" addresses SGD quantile inference with constant learning rates, not decreasing-rate averaging—highlighting that this mechanism relies specifically on decaying γ_k.
- Break condition: If β ≤ 1/2, the SGD iterates may not converge; if β is too close to 1, the effective sample size for averaging is too small.

### Mechanism 3
- Claim: The Gaussian approximation holds simultaneously over quantile levels τ and dimensions p, with p allowed to grow exponentially in n.
- Mechanism: The uniform Bahadur representation linearizes the SGD estimate to a martingale average. The approximating Gaussian process B_i(τ) is a Brownian bridge with covariance Cov(B_i(t), B_j(s)) = P(X_{i,k+1} ≤ Q_i(t), X_{j,k+1} ≤ Q_j(s)) - ts. Anti-concentration inequalities for maxima of Gaussian vectors extend this to uniform suprema over τ.
- Core assumption: Assumption 1 requires f_i(Q_i(τ)) ≥ c_L > 0 uniformly in τ ∈ [τ_0, τ_1], ensuring the linearization denominator is bounded away from zero.
- Evidence anchors:
  - [abstract]: "A uniform Bahadur representation is established for the averaged algorithm, and a Gaussian approximation result is provided, allowing for simultaneous inference over both quantile levels and dimensions."
  - [section]: Theorem 4.2 gives the explicit approximation rate log^(7/6)(np)n^(-1/6) + log²(np)n^(-r) where r = min{β/4, β(1-β)}.
  - [corpus]: "Gaussian Approximation and Multiplier Bootstrap for Stochastic Gradient Descent" provides bootstrap-based inference for SGD, complementing this analytical Gaussian approximation approach.
- Break condition: If log(p) ≍ n^v with v ≥ r/2, the approximation error bound diverges.

## Foundational Learning

- **Concept:** Martingale concentration inequalities (Freedman's inequality)
  - Why needed here: The proofs decompose SGD error into martingale differences ξ_{i,k+1}(τ); bounding their sum requires martingale-specific tail bounds.
  - Quick check question: Given bounded martingale differences with conditional variance σ², what is the probability that their sum exceeds x?

- **Concept:** Bahadur representation
  - Why needed here: This is the core theoretical device showing the averaged SGD estimate is asymptotically linear, enabling inference.
  - Quick check question: For sample quantiles, what is the linear term in the Bahadur representation, and what is the order of the remainder?

- **Concept:** Brownian bridge
  - Why needed here: The limiting Gaussian process in Theorem 4.2 is a Brownian bridge; understanding its covariance structure is essential for computing critical values.
  - Quick check question: What is the covariance function of a standard Brownian bridge on [0,1], and how does it differ from Brownian motion?

## Architecture Onboarding

- **Component map:**
  - Data stream: i.i.d. vectors W_t = (X_{1,t}, ..., X_{p,t})^T
  - SGD core: Recursive update Y_{i,k+1}(τ) = Y_{i,k}(τ) + γ_k[τ - g_{aγ_k}(Y_{i,k}(τ) - X_{i,k+1})]
  - Smoothing kernel: g(x) piecewise linear, scaled by aγ_k
  - Averaging module: Ȳ_{i,n}(τ) = (k/(k+1))Ȳ_{i,k}(τ) + (1/(k+1))Y_{i,k+1}(τ)
  - Density estimator: Required for inference; kernel density estimation of f_i(Q_i(τ))

- **Critical path:**
  1. Initialize Y_{i,1}(τ) = y_i (bounded initial values)
  2. For each new observation X_{i,k+1}, update all p dimensions and all τ grid points
  3. Maintain running averages Ȳ_{i,k}(τ)
  4. For inference: estimate f_i(Q_i(τ)) via kernel density or recursive estimation

- **Design tradeoffs:**
  - Larger a (smoothing constant): more bias in early iterations, but stronger monotonicity guarantee
  - β near 0.7: optimal coverage in simulations; smaller β under-covers, larger over-covers
  - Grid granularity for τ: finer grids improve uniformity but increase computation; δ-net argument suggests O(x^(-1)) points suffice for tolerance x

- **Failure signatures:**
  - Quantile crossing observed: check that g_{aγ_k} is applied correctly (not the original indicator)
  - Coverage too low: β may be too small (try 0.7) or sample size insufficient
  - Coverage too high: β may be too large (try 0.7) or density estimation poor

- **First 3 experiments:**
  1. Replicate Table 1: Generate n ∈ {100, 250, 500, 1000, 2000, 4000} samples from N(0,1), estimate quantiles at τ ∈ {0.1, ..., 0.9} with β ∈ {0.6, 0.7, 0.8}, compare empirical coverage to nominal levels (15%, 10%, 5%, 1%) using Brownian bridge critical values.
  2. Test monotonicity: Run both standard SGD and smoothed SGD on the same data stream; plot quantile curves for τ ∈ {0.4, 0.5, 0.6} and verify smoothed curves never cross while standard may cross (as in Figure 2).
  3. Density estimation sensitivity: Repeat experiment 1 with estimated f(Q(τ)) via kernel density (bandwidth selection by rule-of-thumb) vs. known f; quantify coverage degradation from estimation error.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal data-driven rule for selecting the learning rate parameter β, given that coverage depends sensitively on this choice?
- Basis in paper: [explicit] The simulation study (Tables 1 and 3) shows that β = 0.7 achieves nominal coverage for large n, but β = 0.6 undershoots and β = 0.8 overshoots the nominal level. The authors state: "the empirical size crucially depends on the learning rate parameter β."
- Why unresolved: The theory only assumes 1/2 < β < (1+√5)/4 ≈ 0.809 without providing guidance on optimal selection.
- What evidence would resolve it: A theoretical characterization of the optimal β as a function of sample size n, dimension p, and data distribution, validated through simulations across a range of scenarios.

### Open Question 2
- Question: How do the theoretical guarantees and Bahadur representation extend to non-i.i.d. streaming data with temporal dependence?
- Basis in paper: [inferred] The paper explicitly assumes i.i.d. random vectors throughout (Section 2), but streaming data in practical applications often exhibit serial correlation or non-stationarity.
- Why unresolved: The martingale decomposition and concentration inequalities rely critically on the independence assumption.
- What evidence would resolve it: Extension of Theorems 3.3 and 4.2 to mixing or martingale-difference sequences, with modified tail bounds that account for dependence structure.

### Open Question 3
- Question: How does estimation of the sparsity function fi(Qi(τ)) affect the finite-sample coverage of uniform confidence bands in high dimensions?
- Basis in paper: [explicit] The simulations distinguish between known and estimated sparsity (Tables 1-2), with the authors noting: "As a robustness check, we consider the case in which this quantity is estimated via kernel density estimation." The theoretical results assume the sparsity is either known or estimated consistently.
- Why unresolved: The Gaussian approximation in Theorem 4.2 does not account for estimation uncertainty in the sparsity function itself, which may dominate in high dimensions.
- What evidence would resolve it: A uniform Bahadur representation that incorporates sparsity estimation error, plus simulation evidence showing coverage degradation as p grows relative to n when sparsity is estimated.

## Limitations

- The method requires careful tuning of learning rate decay parameter β and smoothing constant a, with optimal ranges that depend on sample size and dimension
- Theoretical guarantees depend on density f_i(Q_i(τ)) being bounded away from zero, which may not hold uniformly for heavy-tailed distributions or extreme quantiles
- Inference relies on kernel density estimation of the sparsity function, introducing additional uncertainty not fully characterized in the theory

## Confidence

- **High confidence:** The monotonicity property (Mechanism 1) is formally proven via induction and supported by the core mathematical structure
- **Medium confidence:** The Bahadur representation and Gaussian approximation results (Mechanisms 2-3) hold under stated assumptions, but the finite-sample performance depends on proper tuning of β and a, which requires empirical calibration
- **Medium confidence:** The uniform inference results over dimensions and quantile levels are theoretically sound, but practical performance may degrade when the dimension p grows close to the exponential bound or when density estimation is imprecise

## Next Checks

1. Conduct a systematic sensitivity analysis of empirical coverage to the learning rate parameter β across the full theoretical range, verifying the optimal window (0.5, 0.809)
2. Test the method on distributions with near-zero density regions (e.g., bimodal distributions) to assess robustness to the density boundedness assumption
3. Compare the coverage performance when using different bandwidth selection methods for the kernel density estimation of f_i(Q_i(τ)), quantifying the impact of density estimation error on inference quality