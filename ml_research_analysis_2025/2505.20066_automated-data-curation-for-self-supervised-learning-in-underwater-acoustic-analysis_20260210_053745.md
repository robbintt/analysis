---
ver: rpa2
title: Automated data curation for self-supervised learning in underwater acoustic
  analysis
arxiv_id: '2505.20066'
source_url: https://arxiv.org/abs/2505.20066
tags:
- data
- dataset
- curation
- learning
- underwater
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first automated pipeline for curating large-scale
  underwater acoustic recordings for self-supervised learning. The method combines
  Automatic Identification System (AIS) data with raw Passive Acoustic Monitoring
  (PAM) recordings from U.S.
---

# Automated data curation for self-supervised learning in underwater acoustic analysis

## Quick Facts
- arXiv ID: 2505.20066
- Source URL: https://arxiv.org/abs/2505.20066
- Reference count: 0
- Primary result: First automated pipeline for curating underwater acoustic recordings for SSL, achieving 56.72% accuracy on Deepship and 53.11% on ShipsEar for ship type classification

## Executive Summary
This paper presents the first automated pipeline for curating large-scale underwater acoustic recordings for self-supervised learning. The method combines Automatic Identification System (AIS) data with raw Passive Acoustic Monitoring (PAM) recordings from U.S. waters, using hierarchical k-means clustering and ship-based sampling to create balanced datasets. The curated dataset was used to train a Data2Vec SSL model, which achieved 56.72% accuracy on Deepship and 53.11% on ShipsEar for ship type classification, outperforming models trained on randomly curated data by 7.56% and 1.13% respectively. The results demonstrate that automated data curation significantly improves SSL performance for underwater acoustic analysis, addressing the challenge of analyzing large volumes of unlabeled PAM data for ocean ecosystem monitoring.

## Method Summary
The method combines AIS data with PAM recordings through a three-stage curation pipeline: (1) AIS-based sampling balances ship distributions by inversely proportional sampling of ship occurrences, with ships exceeding threshold t=250 sampled with probability inversely proportional to their frequency; (2) Audio segments are embedded using a pre-trained underwater acoustic model, then hierarchically clustered with levels [6000, 400, 40, 10] using streaming K-means, with samples drawn from clusters based on distance to centroids; (3) The curated dataset trains a Data2Vec SSL model with 15% masking and EMA teacher updates, followed by linear probing with logistic regression for ship type classification. The final curated dataset contains 323,532 PAM samples and 25,021 AIS-aligned samples totaling approximately 970 hours of 10-second windows at 16kHz.

## Key Results
- Curated model achieved 56.72% accuracy on Deepship benchmark versus 49.16% for random baseline (7.56% improvement)
- Curated model achieved 53.11% accuracy on ShipsEar benchmark versus 51.98% for random baseline (1.13% improvement)
- The smaller gain on ShipsEar was attributed to environmental differences between training data (U.S. waters) and test domain

## Why This Works (Mechanism)

### Mechanism 1: AIS-based Distribution Balancing
- Claim: Inversely proportional sampling of ship occurrences reduces long-tailed distribution bias in PAM data.
- Mechanism: AIS pulses are aligned with PAM recordings within a 4km×4km hydrophone range. Ships exceeding occurrence threshold t are sampled with probability inversely proportional to their frequency, rare ships are kept fully. This transforms a skewed distribution into a more balanced one.
- Core assumption: Ship acoustic signatures correlate with ship identity/type, and repeated recordings of the same ship provide diminishing returns for representation learning.
- Evidence anchors:
  - [abstract] "integrating Automatic Identification System (AIS) data with passive acoustic monitoring (PAM) recordings, using AIS-based sampling to balance ship distributions"
  - [Section 3.3] "This probability is inversely proportional to the occurrence of the ship in Ds, the higher the occurrence, the lower the sampling probability"
  - [corpus] Limited direct corpus validation; neighboring papers focus on embeddings rather than curation strategies.
- Break condition: If ship acoustic signatures vary more by environmental factors (depth, temperature, propagation) than by ship identity, balancing by ship occurrence may not improve generalization.

### Mechanism 2: Hierarchical K-means for Acoustic Diversity Sampling
- Claim: Multi-scale clustering approximates uniform sampling over the natural acoustic distribution without requiring labeled metadata.
- Mechanism: A 4-level hierarchical KMeans [6000, 400, 40, 10] clusters embeddings from a pre-trained underwater acoustic model. Upper levels capture global features; lower levels capture fine-grained details. Samples are drawn from each level, preferentially retaining points closer to cluster centers.
- Core assumption: The pre-trained embedding model (from prior work [13]) meaningfully separates acoustic features relevant to downstream tasks.
- Evidence anchors:
  - [abstract] "hierarchical k-means clustering to sample diverse audio segments"
  - [Section 3.5] "the lower cluster levels focus on detailed information, while the upper levels capture global features... once the target number of N samples is exceeded, the samples with the greatest distance to the cluster center are replaced"
  - [corpus] The embedding model reference [13] appears in corpus neighbors with FMR=0.55, suggesting related validation but not direct replication of hierarchical sampling.
- Break condition: If the pre-trained embeddings encode irrelevant features (e.g., hydrophone-specific noise), clustering will not yield semantically diverse samples.

### Mechanism 3: Linear Probing of SSL Embeddings for Transfer Assessment
- Claim: Curation quality can be measured by training a simple linear classifier on frozen SSL embeddings.
- Mechanism: Data2Vec is fine-tuned on curated PAM data with masked prediction (15% masking). Learned representations are extracted and fed to logistic regression for ship-type classification on Deepship and ShipsEar benchmarks.
- Core assumption: Better linear probe performance indicates more transferable representations; SSL benefits from diverse/balanced pretraining data.
- Evidence anchors:
  - [Section 3.6] "A simple logistic regression is optimized using the learned embeddings for the classification of the type of ships"
  - [Section 4.2] Curated model outperforms random baseline: Deepship 56.72% vs 49.16%; ShipsEar 53.11% vs 51.98%
  - [corpus] Neighbor paper "Decodable but not structured" validates linear probing for underwater embeddings, supporting this evaluation paradigm.
- Break condition: If downstream tasks require non-linear feature interactions, linear probing may underestimate curation benefits or mask SSL failure modes.

## Foundational Learning

- **Self-supervised learning with teacher-student architectures (e.g., Data2Vec, BYOL)**
  - Why needed here: The core training paradigm uses EMA teacher updates and masked prediction; understanding this is essential to diagnose training dynamics.
  - Quick check question: Can you explain why the teacher weights are an exponential moving average of student weights rather than trained directly?

- **Hierarchical clustering and streaming K-means**
  - Why needed here: The curation pipeline depends on scalable clustering of millions of audio embeddings; standard batch K-means would be infeasible.
  - Quick check question: How does streaming K-means approximate the batch solution, and what tradeoffs does it introduce?

- **Embedding space evaluation via probing classifiers**
  - Why needed here: Model quality is assessed via linear probing; interpreting these results requires understanding what probing does and does not measure.
  - Quick check question: Why might a linear probe underperform on a task even if the embedding contains relevant information?

## Architecture Onboarding

- **Component map:**
  - Raw PAM audio (NOAA hydrophones) → Resample to 16kHz, 10s windows → Pre-trained embedding model → Hierarchical K-means (streaming) → Curated PAM samples (Da)
  - AIS data → Align with PAM (4km range) → Occurrence-based sampling (threshold t=250) → Curated AIS samples (D*s)
  - Combined dataset D* = Da + D*s → Data2Vec SSL pretraining → Frozen embeddings → Logistic regression probe → Ship-type classification

- **Critical path:**
  1. AIS-PAM alignment correctness (geospatial join within 4km squares)
  2. Embedding quality from pre-trained model [13]
  3. Threshold selection (t=250 from distribution knee)
  4. Hierarchical K-means convergence in streaming mode

- **Design tradeoffs:**
  - Smaller threshold t → more samples per rare ship, but fewer total ship samples; risk of overfitting to few ships
  - More cluster levels → finer-grained diversity, but higher computational cost and potential over-fragmentation
  - Dataset size (970 hours) → balanced against compute budget; larger may not help if curation quality is poor

- **Failure signatures:**
  - Random baseline outperforms curated → embedding model or clustering is discarding task-relevant information
  - Large Deepship gain but minimal ShipsEar gain → environmental mismatch between pretraining and test domains
  - Training loss decreases but probing accuracy flat → SSL objective misaligned with downstream task structure

- **First 3 experiments:**
  1. **Ablate AIS curation only**: Train on D*s (AIS-curated) without Da (PAM-curated) to isolate ship-balancing contribution.
  2. **Ablate hierarchical levels**: Compare full 4-level clustering vs. single-level K-means to validate multi-scale benefit.
  3. **Domain shift probe**: Evaluate on a benchmark with similar environmental conditions to PAM data (e.g., same geographic region) to test whether ShipsEar's smaller gain is due to domain mismatch rather than curation failure.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would a contrastive learning approach outperform the masked modeling approach (Data2Vec) for underwater acoustic SSL, given the stationarity characteristics of PAM data?
- Basis in paper: [explicit] The authors state in the conclusion: "Due to the stationarity of the data, the masking strategy may be suboptimal, and the results may benefit from a more contrastive approach."
- Why unresolved: This study only evaluated Data2Vec with masked modeling; no contrastive methods were benchmarked against it.
- What evidence would resolve it: A comparative study training contrastive SSL methods (e.g., SimCLR, CLAR) on the same curated dataset and evaluating on identical downstream tasks.

### Open Question 2
- Question: Does expanding geographic diversity of hydrophone sources improve cross-region generalization on downstream tasks?
- Basis in paper: [explicit] The authors note the smaller performance gain on ShipsEar (1.13%) versus Deepship (7%+) "likely due to environmental factors" since ShipsEar's ocean environment differs from the U.S. waters in the training data. They explicitly propose: "Expanding the diversity by incorporating more hydrophones from various regions would make the representations more robust."
- Why unresolved: The curated dataset only includes hydrophones from U.S. waters, limiting environmental diversity.
- What evidence would resolve it: Retraining the model with hydrophone data from additional geographic regions and measuring performance changes on geographically diverse benchmarks.

### Open Question 3
- Question: How sensitive is the AIS curation threshold parameter (t) to dataset characteristics, and can it be automatically determined?
- Basis in paper: [inferred] The threshold t = 250 was selected heuristically as "the knee of the skewed distribution," and Figure 3 shows different threshold options (250, 500, 1000) without systematic comparison of their downstream effects.
- Why unresolved: The paper does not ablate or analyze the impact of different threshold values on SSL performance.
- What evidence would resolve it: An ablation study training SSL models with varying threshold values and comparing downstream classification accuracy.

### Open Question 4
- Question: How does the arbitrary 4 km × 4 km spatial range for AIS-PAM alignment affect the accuracy and noise level of ship-labeled samples?
- Basis in paper: [inferred] The paper defines a fixed 4 km × 4 km square to align AIS pulses with hydrophone recordings without justification or sensitivity analysis for this distance parameter.
- Why unresolved: Acoustic propagation depends on environmental conditions; a fixed range may include distant ships that are inaudible or exclude nearby ships in some conditions.
- What evidence would resolve it: Varying the spatial alignment range and measuring the resulting signal-to-noise ratio or downstream task performance.

## Limitations
- Unknown pre-trained embedding model from [13] is not publicly available and lacks implementation details
- Limited ablation studies to isolate contributions of AIS curation versus hierarchical clustering
- Environmental domain shift between U.S. waters training data and evaluation benchmarks

## Confidence
- **High confidence**: The core methodology of AIS-based sampling with occurrence threshold t=250 is well-specified and theoretically sound for balancing ship distributions
- **Medium confidence**: The hierarchical K-means clustering approach for acoustic diversity sampling is reasonable, but the streaming implementation details and cluster level sampling strategy are not fully specified
- **Low confidence**: The final SSL training procedure (Data2Vec fine-tuning) lacks details on optimizer settings, training duration, and data augmentation strategies, making it difficult to assess the completeness of the reported results

## Next Checks
1. **Ablate AIS curation only**: Train on D*s (AIS-curated) without Da (PAM-curated) to isolate the contribution of ship-balancing curation
2. **Ablate hierarchical levels**: Compare full 4-level clustering versus single-level K-means to validate the multi-scale benefit of hierarchical sampling
3. **Domain shift probe**: Evaluate on a benchmark with similar environmental conditions to the PAM data (e.g., same geographic region) to test whether ShipsEar's smaller gain is due to domain mismatch rather than curation failure