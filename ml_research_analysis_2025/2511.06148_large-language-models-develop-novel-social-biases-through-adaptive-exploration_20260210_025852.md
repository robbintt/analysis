---
ver: rpa2
title: Large Language Models Develop Novel Social Biases Through Adaptive Exploration
arxiv_id: '2511.06148'
source_url: https://arxiv.org/abs/2511.06148
tags:
- demo
- quadrant
- aquadrant
- bquadrant
- cquadrant
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that large language models (LLMs) can develop
  novel social biases through adaptive exploration, even when no inherent differences
  exist between demographic groups. Using a sequential hiring game paradigm from social
  science, the authors show that LLMs assign jobs to artificial demographic groups
  more unequally than human participants, with newer and larger models exhibiting
  stronger stratification effects.
---

# Large Language Models Develop Novel Social Biases Through Adaptive Exploration

## Quick Facts
- arXiv ID: 2511.06148
- Source URL: https://arxiv.org/abs/2511.06148
- Authors: Addison J. Wu; Ryan Liu; Xuechunzi Bai; Thomas L. Griffiths
- Reference count: 40
- Key outcome: LLMs develop novel social biases through adaptive exploration, with newer and larger models exhibiting stronger stratification effects

## Executive Summary
This paper demonstrates that large language models (LLMs) can develop novel social biases through adaptive exploration, even when no inherent differences exist between demographic groups. Using a sequential hiring game paradigm from social science, the authors show that LLMs assign jobs to artificial demographic groups more unequally than human participants, with newer and larger models exhibiting stronger stratification effects. The study identifies this as an exploration-exploitation trade-off problem where models over-rely on early feedback. Among various interventions tested—including adjusting success probabilities, adding contextual features, and prompt steering—explicitly incentivizing diversity in the objective function proved most effective at reducing bias. The findings reveal that LLMs are not merely passive mirrors of human biases but can actively create new ones, highlighting the urgent need for carefully designed objectives that incorporate societal values to ensure equitable outcomes.

## Method Summary
The paper employs a sequential hiring game paradigm where LLMs act as hiring consultants allocating jobs across four artificial demographic groups (Tufa, Aima, Reku, Weki) over 40 rounds. Each job has an associated success/failure feedback signal (p=0.9 probability of success for all candidate-job pairs). The study tests 18 different models across three scales (small, medium, large) using both direct prompting and chain-of-thought approaches. Metrics include Stratification Index (SI), Between-Group Divergence (BGD), and Group Assignment Stochasticity Index (GASI). The experiment compares LLM behavior against human participant baselines and random assignment baselines across 30 runs per model/condition. Various interventions are tested, including adjusting success probabilities, adding contextual features, and prompt steering with explicit diversity incentives.

## Key Results
- LLMs exhibit significantly higher stratification than both human participants (SI ≈ 0.84) and random baselines (SI ≈ 0.25), with newer and larger models showing the strongest effects
- Stratification emerges from exploration-exploitation trade-offs where models over-rely on early feedback rather than continuing to explore alternatives
- Explicitly incentivizing diversity in the objective function proved most effective at reducing bias, while generic fairness prompts had limited impact
- Models misidentify best-performing groups when true differences exist (only 26% accuracy), indicating biases are emergent rather than inherited

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Insufficient exploration leads to premature stereotype formation from noisy early feedback.
- **Mechanism:** LLMs act as in-context learners that generalize from early observations. When initial assignments succeed, models exploit this pattern rather than continuing to explore alternatives, creating self-reinforcing feedback loops.
- **Core assumption:** In-context learning mechanisms that enable few-shot generalization also drive premature pattern-matching from limited samples.
- **Evidence anchors:** Abstract states "These biases result from exploration-exploitation trade-offs, where the decision-maker explores too little, allowing early observations to strongly influence impressions about entire demographic groups." Section 2.2 documents LLMs' "notable shortcomings" in multi-armed bandit tasks where they "fixate on the same option that first results in a successful reward."

### Mechanism 2
- **Claim:** Stronger reasoning capabilities paradoxically increase stratification rather than reduce it.
- **Mechanism:** More capable models draw more precise inferences from past outcomes. This reasoning strength becomes maladaptive when it leads to overfitting on early feedback signals, reducing exploration breadth.
- **Core assumption:** Reasoning capability and exploration breadth are inversely related in current LLM architectures.
- **Evidence anchors:** Abstract notes "newer and larger models exhibit stronger stratification effects." Section 4.2 states "One simple reason for this trend is that better models draw more precise inferences about past outcomes...However, this reasoning-based tendency can be maladaptive." Figure 4 shows negative correlation between BBQ benchmark scores and stratification.

### Mechanism 3
- **Claim:** Explicit objective function modification is necessary because implicit value alignment is insufficient.
- **Mechanism:** LLMs optimize toward specified objectives. Generic fairness prompts ("be fair") or value statements have limited effect because they lack concrete, measurable incentives. Only when diversity is explicitly incorporated into the reward structure do models reliably change behavior.
- **Core assumption:** LLMs are instruction-following optimizers that respond to measurable incentives more than abstract values.
- **Evidence anchors:** Section 5.3 states "explicitly asking the model to optimize for diversity was most robust and effective." Other steering approaches (system prompts, fairness appeals, community values) "were sometimes successful but did not reduce stratification nearly as much."

## Foundational Learning

- **Concept: Exploration-Exploitation Trade-off (RL)**
  - **Why needed here:** The paper's central thesis frames bias emergence as an RL problem. Understanding why agents must balance trying new options (exploration) versus repeating successful ones (exploitation) is essential.
  - **Quick check question:** Why would an optimal exploration strategy reduce stereotype formation in sequential hiring?

- **Concept: In-Context Learning**
  - **Why needed here:** Mechanism through which LLMs adapt beliefs across rounds. Models generalize from observed feedback without weight updates—but this capability also enables premature pattern-matching.
  - **Quick check question:** How does in-context learning differ from gradient-based learning, and why might it be more vulnerable to early feedback?

- **Concept: Allocative vs. Representational Bias**
  - **Why needed here:** Paper distinguishes bias in outcomes (allocative) from bias in representations. Most benchmarks test representational bias; this paper demonstrates allocative bias from sequential decisions.
  - **Quick check question:** Why might a model that passes BBQ (representational bias benchmark) still produce stratified allocations?

## Architecture Onboarding

- **Component map:** Hiring game environment -> Multi-turn LLM interaction loop -> Success/failure feedback -> Assignment tracking -> Metric computation (SI/BGD/GASI) -> Comparison to baselines
- **Critical path:** Run baseline experiments (40 rounds × 30 runs) to establish SI/BGD/GASI -> Compare against human baseline (SI ≈ 0.84) and random baseline (SI ≈ 0.25) -> Test interventions systematically; prioritize objective steering (diversity bonus) if stratification exceeds human baseline
- **Design tradeoffs:** Higher success probabilities (p=0.9) increase stratification; lower probabilities (p=0.1) reduce it but create unrealistic scenarios; adding contextual features (age, education) reduces stratification but depends on feature salience; diversity incentives reduce stratification but may decrease success rates when genuine demographic-job mappings exist
- **Failure signatures:** SI > 1.5 with direct prompting on frontier models (Claude 4 Sonnet, Gemini 2.5, GPT-4o); CoT prompts producing no statistically significant reduction vs. direct prompts; models misidentifying best-performing groups when true differences exist (only 26% accuracy); GASI values similar to human levels (~0.5) indicating emergent rather than inherited biases
- **First 3 experiments:** 1) Replicate core finding: Run 30 trials of 40-round hiring game on GPT-4o with default parameters; expect SI > 1.3, confirming frontier model stratification exceeds human baseline. 2) Test diversity incentive: Add explicit diversity bonus to objective function; expect SI reduction to near-random baseline (~0.25) for most models. 3) Validate emergent nature: Run single-round experiments (400 total assignments) to verify low initial SI (< 0.25), confirming stratification emerges from sequential feedback rather than pre-existing associations.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can harmful generalization and premature stratification be limited in sequential decision-making without suppressing the constructive reasoning capabilities that make LLMs powerful?
- **Basis in paper:** The authors explicitly ask: "How do we limit generalization in sensitive cases without suppressing reasoning as a whole?" noting that the capacity to extrapolate patterns is the same capacity that drives stratification.
- **Why unresolved:** The paper highlights a "central tension in alignment" where interventions that improve reasoning might inadvertently exacerbate emergent biases, yet suppressing pattern-matching reduces overall model utility.
- **What evidence would resolve it:** The development of a "circuit-breaker" mechanism or specific fine-tuning curriculum that distinguishes between statistical noise and valid patterns in few-shot sequential tasks.

### Open Question 2
- **Question:** What mechanistic factors cause newer and larger models to exhibit stronger emergent stratification, despite performing better on single-turn static bias benchmarks like BBQ?
- **Basis in paper:** The paper notes a "dangerous trend" and "concerning divergence" where frontier models show increased stratification in the hiring game (SI increases) while showing improved BBQ scores, suggesting current benchmarks are "too isolated to capture the downstream societal outcomes."
- **Why unresolved:** The paper speculates that better models draw "more precise inferences about past outcomes" which leads to faster lock-in, but this contradicts the expectation that higher reasoning capabilities should lead to better handling of trade-offs.
- **What evidence would resolve it:** A causal analysis of in-context learning dynamics comparing how different model scales weigh early vs. late feedback in a sequential setting versus a single-turn setting.

### Open Question 3
- **Question:** Can diversity-steering interventions be applied robustly in real-world scenarios where ground-truth performance differences between demographic groups exist?
- **Basis in paper:** The authors state: "Another assumption we make is that groups have equal success rates across all jobs. However, if unequal success rates exist... enforcing diversity can reduce overall success."
- **Why unresolved:** The effective intervention identified (explicit diversity incentive) relies on the assumption of equal potential, which may not hold in all complex, real-world contexts, potentially penalizing optimal allocations if applied blindly.
- **What evidence would resolve it:** A follow-up study applying the diversity-incentive prompt to scenarios with known, structured performance differences (covariates) to see if the model can distinguish between spurious noise and genuine qualification signals.

## Limitations

- The study assumes equal success rates across all demographic groups and job types, which may not reflect real-world scenarios where genuine performance differences exist
- The intervention effectiveness (diversity incentives) may not generalize to complex real-world objectives where merit-based differences between groups are present
- The corpus provides limited direct support for the in-context learning mechanism driving premature pattern-matching, relying primarily on the authors' own experimental evidence

## Confidence

- **High confidence:** Core finding that LLMs develop novel stratification beyond inherited biases, supported by clear statistical separation from random and human baselines
- **Medium confidence:** Exploration-exploitation mechanism, though corpus provides limited direct support for in-context learning driving premature pattern-matching
- **Medium confidence:** Reasoning capability paradox—negative correlation between fairness benchmarks and emergent bias is suggestive but not definitively causal
- **Low confidence:** Generalizability of intervention effectiveness to real-world scenarios with genuine performance differences

## Next Checks

1. **Cross-task generalization:** Test whether diversity incentives reduce emergent bias in non-hiring sequential decision tasks (e.g., resource allocation, loan approval) to verify the intervention's broader applicability.

2. **Long-term feedback dynamics:** Extend experiments beyond 40 rounds to examine whether stratification plateaus, reverses, or intensifies with more data, clarifying the temporal stability of emergent biases.

3. **Realistic merit variation:** Introduce genuine demographic-job success differences (rather than uniform p=0.9) to test whether diversity incentives remain effective when real performance gaps exist, addressing the practical limitation noted in the discussion.