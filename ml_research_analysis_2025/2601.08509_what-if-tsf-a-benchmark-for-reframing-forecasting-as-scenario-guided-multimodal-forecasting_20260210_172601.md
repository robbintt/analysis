---
ver: rpa2
title: 'What If TSF: A Benchmark for Reframing Forecasting as Scenario-Guided Multimodal
  Forecasting'
arxiv_id: '2601.08509'
source_url: https://arxiv.org/abs/2601.08509
tags:
- forecasting
- data
- time
- series
- domain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the What If TSF (WIT) benchmark, which addresses
  the challenge of evaluating multimodal forecasting by focusing on scenario-guided
  prediction rather than historical pattern extrapolation. Unlike existing benchmarks
  that rely on retrospective or misaligned context, WIT provides expert-crafted plausible
  and counterfactual future scenarios to test whether models can condition forecasts
  on forward-looking information.
---

# What If TSF: A Benchmark for Reframing Forecasting as Scenario-Guided Multimodal Forecasting

## Quick Facts
- arXiv ID: 2601.08509
- Source URL: https://arxiv.org/abs/2601.08509
- Reference count: 40
- Introduces a benchmark focusing on scenario-guided multimodal forecasting rather than historical pattern extrapolation

## Executive Summary
This paper introduces the What If TSF (WIT) benchmark, which addresses the challenge of evaluating multimodal forecasting by focusing on scenario-guided prediction rather than historical pattern extrapolation. Unlike existing benchmarks that rely on retrospective or misaligned context, WIT provides expert-crafted plausible and counterfactual future scenarios to test whether models can condition forecasts on forward-looking information. The benchmark covers four domains—Politics, Society, Energy, and Economy—with over 5,300 samples and includes tasks for short-term, long-term, and counterfactual forecasting.

Experiments show that large language models leveraging scenario guidance significantly outperform unimodal methods in directional accuracy, highlighting the importance of contextual future scenarios. Further analysis reveals that historical context offers limited predictive value compared to future scenario guidance. WIT enables a more realistic and rigorous evaluation of multimodal forecasting approaches, encouraging models to move beyond pattern matching toward conditional reasoning under external information.

## Method Summary
The WIT benchmark reframes forecasting as a scenario-guided multimodal task by providing expert-crafted plausible and counterfactual future scenarios across four domains. The benchmark contains over 5,300 samples with tasks spanning short-term, long-term, and counterfactual forecasting. Models are evaluated on their ability to condition predictions on these future scenarios rather than relying solely on historical patterns. The evaluation framework emphasizes directional accuracy and tests whether models can reason under different contextual conditions.

## Key Results
- Large language models with scenario guidance significantly outperform unimodal methods in directional accuracy
- Historical context provides limited predictive value compared to future scenario guidance
- The benchmark enables more realistic evaluation of multimodal forecasting approaches across four domains

## Why This Works (Mechanism)
The benchmark works by shifting the forecasting paradigm from pattern extrapolation to conditional reasoning. By providing expert-crafted future scenarios as context, models must learn to reason about how external information influences outcomes rather than simply extending historical trends. This approach better reflects real-world forecasting where decisions are made under uncertainty and conditional on various possible futures.

## Foundational Learning
- Scenario-guided forecasting: Models must condition predictions on provided future scenarios rather than historical patterns
  - Why needed: Real-world forecasting often involves conditional reasoning under uncertainty
  - Quick check: Can the model adjust predictions based on different scenario contexts?

- Multimodal forecasting: Integrating information across different data types and sources
  - Why needed: Real forecasting problems involve diverse information sources
  - Quick check: Does the model effectively use both scenario text and other input modalities?

- Directional accuracy metrics: Evaluating whether predictions correctly identify the direction of change
  - Why needed: Often more important than precise numerical forecasts in many domains
  - Quick check: Does the model correctly predict up/down trends rather than exact values?

## Architecture Onboarding

**Component Map:**
WIT Task Handler -> Scenario Processor -> Forecast Generator -> Accuracy Evaluator

**Critical Path:**
Task Handler receives domain/scenario pair → Scenario Processor extracts relevant information → Forecast Generator produces prediction → Accuracy Evaluator computes directional accuracy

**Design Tradeoffs:**
- Expert-crafted scenarios vs. automated generation (quality vs. scalability)
- Directional accuracy vs. numerical precision (practical relevance vs. theoretical completeness)
- Domain specificity vs. generalization (focused evaluation vs. broad applicability)

**Failure Signatures:**
- Over-reliance on historical patterns when scenarios suggest different outcomes
- Inability to distinguish between plausible and counterfactual scenarios
- Poor handling of domain-specific terminology and context

**3 First Experiments:**
1. Run baseline model with historical context only vs. scenario-guided context
2. Test model performance across all four domains with identical architectures
3. Evaluate counterfactual forecasting task separately from plausible scenario tasks

## Open Questions the Paper Calls Out
None provided in the source material.

## Limitations
- Expert-crafted scenario generation lacks quantitative validation of inter-annotator agreement
- Sample size imbalances across domains may affect generalization
- Performance claims rely on authors' own model variants, potentially introducing selection bias

## Confidence
- High confidence in benchmark design and task definitions
- Medium confidence in relative performance claims between historical and scenario-guided approaches
- Medium confidence in superiority of multimodal over unimodal methods
- Low confidence in cross-domain generalization without further validation

## Next Checks
1. Conduct inter-annotator reliability analysis on scenario quality and consistency across all domains
2. Test the benchmark with external, independently developed forecasting models to validate relative performance claims
3. Perform domain-specific analysis to understand whether observed patterns hold equally across Politics, Society, Energy, and Economy domains, particularly given sample size imbalances