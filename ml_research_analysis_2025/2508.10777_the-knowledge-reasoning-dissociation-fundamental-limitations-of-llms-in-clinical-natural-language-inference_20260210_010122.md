---
ver: rpa2
title: 'The Knowledge-Reasoning Dissociation: Fundamental Limitations of LLMs in Clinical
  Natural Language Inference'
arxiv_id: '2508.10777'
source_url: https://arxiv.org/abs/2508.10777
tags:
- reasoning
- clinical
- causal
- risk
- premise
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates whether scaling data and parameters in large
  language models (LLMs) leads to structured, generalizable internal representations
  for clinical reasoning. It introduces a Clinical Trial Natural Language Inference
  benchmark with four reasoning families (Causal Attribution, Compositional Grounding,
  Epistemic Verification, and Risk State Abstraction), each paired with Ground Knowledge
  and Meta-Level Reasoning Verification (GKMRV) probes to separate factual knowledge
  access from inferential reasoning.
---

# The Knowledge-Reasoning Dissociation: Fundamental Limitations of LLMs in Clinical Natural Language Inference

## Quick Facts
- **arXiv ID:** 2508.10777
- **Source URL:** https://arxiv.org/abs/2508.10777
- **Reference count:** 12
- **Primary result:** Models show high knowledge access (GKMRV accuracy 0.918) but fail on reasoning tasks (mean accuracy 0.25), revealing fundamental knowledge-reasoning dissociation.

## Executive Summary
This paper investigates whether scaling data and parameters in large language models (LLMs) leads to structured, generalizable internal representations for clinical reasoning. It introduces a Clinical Trial Natural Language Inference benchmark with four reasoning families (Causal Attribution, Compositional Grounding, Epistemic Verification, and Risk State Abstraction), each paired with Ground Knowledge and Meta-Level Reasoning Verification (GKMRV) probes to separate factual knowledge access from inferential reasoning. Six LLMs were evaluated under direct and chain-of-thought prompting. Models achieved near-ceiling GKMRV accuracy (mean 0.918) but performed poorly on the main reasoning tasks (mean 0.25), with some tasks near total failure (e.g., Compositional Grounding at 0.04). Despite low accuracy, outputs were highly consistent across samples (mean 0.87), indicating systematic heuristic application. The results reveal that current LLMs often possess relevant clinical knowledge but lack structured, composable internal representations to deploy it reliably, demonstrating fundamental limitations in reasoning.

## Method Summary
The study introduces a Clinical Trial Natural Language Inference (CTNLI) benchmark with four reasoning task families, each paired with Ground Knowledge and Meta-Level Reasoning Verification (GKMRV) probes. Parametric templates generate controlled clinical scenarios requiring specific reasoning types. Six LLMs (OpenAI o3, GPT-4o, GPT-4o-mini, Gemini 2.5 Pro, DeepSeek R1, LLaMA 3.2 3B) are evaluated using both direct and chain-of-thought prompts, with 10 completions per example. The key diagnostic is the dissociation between GKMRV accuracy (measuring knowledge access) and main task accuracy (measuring reasoning deployment), along with consistency metrics showing systematic heuristic use despite low accuracy.

## Key Results
- Models achieved near-ceiling GKMRV accuracy (mean 0.918) but performed poorly on main reasoning tasks (mean 0.25)
- Some tasks showed near-total failure (Compositional Grounding at 0.04 accuracy)
- Despite low accuracy, model outputs were highly consistent across samples (mean 0.87), indicating systematic heuristic application
- Chain-of-thought prompting yielded only marginal improvements (+0.03), suggesting surface-level reasoning traces rather than genuine structural inference

## Why This Works (Mechanism)
The study demonstrates a fundamental dissociation between knowledge access and reasoning deployment in LLMs. Models can correctly state clinical rules in GKMRV probes but fail to apply those rules when evaluating complex clinical cases. High consistency across low-accuracy outputs indicates systematic heuristic application rather than principled inference. The architecture lacks structured, composable internal representations needed to combine multiple pieces of information into verifiable concepts, leading to failures in constraint-checking tasks like compositional grounding.

## Foundational Learning

**Concept:** Knowledge-Reasoning Dissociation
- **Why needed here:** The paper's central claim is that having knowledge (proven by high GKMRV scores) is fundamentally different from being able to use it in a structured way for inference (shown by low task accuracy). Understanding this gap is crucial for interpreting the benchmark results.
- **Quick check question:** If a model correctly states a clinical rule in a GKMRV probe (e.g., "Drug X is contraindicated in condition Y"), but fails to apply that rule when evaluating a complex clinical case, which side of the dissociation does this exemplify?

**Concept:** Heuristic vs. Principled Inference
- **Why needed here:** The paper argues that model consistency is evidence of systematic shortcut use (heuristics), not genuine, principled reasoning. Distinguishing between these is necessary to understand why models can be fluently wrong.
- **Quick check question:** A model concludes "treatment is effective" because the input mentions "80% responded." Is this an example of principled causal reasoning or a heuristic? Why?

**Concept:** Compositional Grounding
- **Why needed here:** Several tasks, especially Compositional Grounding, rely on the ability to combine multiple pieces of information into a single, verifiable concept (e.g., a drug-dose-diagnosis tuple). This is a core area of failure highlighted by the study.
- **Quick check question:** Why would a model that knows "Metformin treats diabetes" and "1000mg is a dose of Metformin" still fail to correctly judge a case where "Metformin 1000mg is given to a patient with severe renal impairment"?

## Architecture Onboarding

**Component map:** Clinical Trial NLI Benchmark -> Parametric Templates -> GKMRV Probes -> Evaluation Harness -> LLM Models

**Critical path:**
1. Task Instantiation: Use parametric templates to generate main task instances and paired GKMRV probes
2. Model Querying: For each instance, prompt the target LLM with both direct and CoT prompts, collecting multiple completions
3. Dissociation Analysis: Calculate and compare mean accuracy on GKMRV probes vs. main reasoning tasks
4. Consistency Analysis: Calculate the ratio of majority label count to total responses per instance

**Design tradeoffs:**
- Control vs. Realism: Formal templates allow precise control but may not capture real-world complexity
- Label Granularity: Three-way NLI schema is strict; clinical reasoning is often probabilistic
- Benchmark Size: 10 items per task family designed for diagnostic precision over broad statistical power

**Failure signatures:**
- Causal Heuristic Drift: Correctly identifying control group need but stating "40% healed â†’ treatment effective"
- Constraint Decomposition: Failing by judging drug-dose-diagnosis tuple based on pairwise associations
- Authority Deference: Deferring to physician's diagnosis even when contradicting objective evidence
- Risk Miscalculation: Equating highest frequency adverse event with highest patient risk

**First 3 experiments:**
1. Baseline Dissociation Check: Run full benchmark on new model. High GKMRV (>0.85) and low main task accuracy (<0.35) confirms dissociation
2. Prompting Ablation: Compare direct vs. CoT performance. Small gain (+0.03) indicates more "thought" tokens don't fix representational deficit
3. Heuristic Probe: Vary surface cues in Causal Attribution tasks. Predictable output changes tracking surface cues vs. causal structure confirms shallow heuristics

## Open Questions the Paper Calls Out

**Open Question 1:** Can neuro-symbolic architectures successfully bridge the knowledge-reasoning dissociation by explicitly integrating symbolic reasoning frameworks with neural models?
- **Basis in paper:** [explicit] The authors explicitly propose "Neuro-symbolic Integration" as a primary future direction
- **Why unresolved:** Unknown if embedding symbolic layers is sufficient to force counterfactual simulation and causal inference criteria
- **What evidence would resolve it:** Empirical results showing neuro-symbolic models achieve high accuracy on main reasoning tasks while maintaining high GKMRV scores

**Open Question 2:** To what extent can representation learning methods (e.g., disentanglement) induce the structured, composable latent representations required for clinical reasoning?
- **Basis in paper:** [explicit] The paper calls for "Representation Disentanglement" to encourage unique latent dimensions for manipulable clinical concepts
- **Why unresolved:** Unknown if current disentanglement techniques can create coherent internal models from text data alone
- **What evidence would resolve it:** Ablation studies showing disentanglement objectives improve constraint-checking task performance

**Open Question 3:** Why does Chain-of-Thought (CoT) prompting fail to mitigate heuristic drift in these tasks, and what prompting mechanisms could enforce valid constraint checking?
- **Basis in paper:** [explicit] Appendix notes CoT yielded negligible gains (+0.03) and often rationalized pre-selected answers
- **Why unresolved:** Unknown if this is due to prompts used, model's inability to self-correct, or deeply embedded heuristics
- **What evidence would resolve it:** Identification of specific prompting strategies that successfully force execution of risk state abstraction pipeline

**Open Question 4:** Does the knowledge-reasoning dissociation persist in non-transformer architectures or models explicitly trained on procedural/logical data?
- **Basis in paper:** [inferred] Study evaluates only transformer-based LLMs; limitation may be specific to autoregressive nature
- **Why unresolved:** Paper demonstrates failure in current standard architectures but doesn't verify if limitation applies universally
- **What evidence would resolve it:** Comparative evaluation of non-transformer or logic-specialized architectures on CTNLI benchmark

## Limitations
- The benchmark uses synthetic clinical scenarios that may not capture full complexity and ambiguity of real-world clinical documentation
- The 10-item per task sample size limits statistical power and may not represent full variability of clinical reasoning challenges
- The three-way NLI schema imposes discrete categorization that may not reflect the probabilistic nature of clinical inference

## Confidence
- **High Confidence:** The GKMRV vs. main task accuracy gap (0.918 vs. 0.25) reliably demonstrates knowledge access without systematic reasoning deployment
- **Medium Confidence:** Characterization of specific failure modes is well-supported but requires validation on more diverse, naturalistic cases
- **Low Confidence:** The claim that this represents a "fundamental limitation" of current LLM architectures is plausible but not definitively proven

## Next Checks
1. **Naturalistic Case Replication:** Apply GKMRV methodology to real-world clinical trial reports and patient cases to test dissociation persistence in ambiguous contexts
2. **Reasoning Representation Probe:** Design experiment requiring models to explicitly construct and manipulate structured representations (e.g., knowledge graphs) during inference
3. **Fine-tuning Intervention:** Fine-tune base LLM on CTNLI benchmark with explicit reasoning supervision and re-evaluate to determine if dissociation can be partially bridged through targeted training