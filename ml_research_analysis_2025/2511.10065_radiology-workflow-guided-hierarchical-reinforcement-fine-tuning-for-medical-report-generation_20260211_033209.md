---
ver: rpa2
title: Radiology Workflow-Guided Hierarchical Reinforcement Fine-Tuning for Medical
  Report Generation
arxiv_id: '2511.10065'
source_url: https://arxiv.org/abs/2511.10065
tags:
- report
- clinical
- diagnostic
- hierarchical
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'RadFlow addresses the challenge of generating clinically coherent
  medical reports by treating them as hierarchical structures rather than flat sequences.
  The method introduces a hierarchical reinforcement fine-tuning framework that mirrors
  radiologists'' workflow through three components: a global reward for cross-sectional
  consistency and overall report quality, a local reward focused on impression accuracy,
  and a critical-aware policy optimization that adapts learning for high-risk cases.'
---

# Radiology Workflow-Guided Hierarchical Reinforcement Fine-Tuning for Medical Report Generation

## Quick Facts
- arXiv ID: 2511.10065
- Source URL: https://arxiv.org/abs/2511.10065
- Authors: Bodong Du; Honglong Yang; Xiaomeng Li
- Reference count: 40
- Primary result: Hierarchical RL fine-tuning framework improves clinical coherence and diagnostic accuracy in medical report generation

## Executive Summary
RadFlow introduces a hierarchical reinforcement fine-tuning framework for medical report generation that mirrors radiologists' clinical workflow. The method decomposes the learning process into global and local reward components, with critical-aware policy optimization that adapts learning rates for high-risk cases. By treating medical reports as hierarchical structures and selectively targeting diagnostically challenging examples, RadFlow achieves substantial improvements in diagnostic coherence and clinical alignment compared to state-of-the-art baselines across both chest X-ray and carotid ultrasound datasets.

## Method Summary
RadFlow operates in three stages: (1) Supervised Fine-Tuning (SFT) on a pretrained VLLM using next-token prediction, (2) Target Exploration that ranks training samples by composite scores (BLEU-2, BERTScore, RadGraph) and selects the bottom-10% as hard examples, and (3) Reinforcement Fine-Tuning (RFT) using hierarchical rewards with Critical-Aware Policy Optimization (CAPO). The hierarchical reward combines global components (syntax, domain correctness, cross-sectional consistency via GPT-4o) with a local Impression reward (F1 score from expert model like CheXpert). CAPO modifies GRPO by using tighter clipping bounds (ε_normal/4) for critical cases pre-annotated by GPT-4o.

## Key Results
- RadFlow achieves state-of-the-art performance on MIMIC-CXR with 1/RadCliQ-v1 score of 0.939
- Significant improvements in cross-sectional consistency between Findings and Impression sections
- 1.8% absolute gain in RadGraph F1 score compared to baseline approaches
- Effective generalization demonstrated on both chest X-ray and carotid ultrasound modalities

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Alignment Reward
The decomposition into global and local rewards prevents reward hacking by ensuring models optimize both surface fluency and diagnostic fidelity. The global reward captures linguistic quality, medical correctness, and cross-sectional consistency between Findings and Impression, while the local reward emphasizes Impression accuracy through section-specific evaluation. This structure mirrors the logical progression radiologists follow when writing reports.

### Mechanism 2: Critical-Aware Policy Optimization (CAPO)
CAPO introduces adaptive clipping bounds that make policy updates more conservative for high-risk cases identified by GPT-4o. By using ε_critical = ε_normal/4 for critical reports, the method reduces hallucination risk in high-stakes scenarios while allowing flexible learning on routine cases. This selective regularization balances safety with learning efficiency.

### Mechanism 3: Target Exploration via Multi-Metric Ranking
The selective application of reinforcement fine-tuning to diagnostically challenging samples improves computational efficiency without degrading performance. By ranking training samples using a composite of BLEU-2, BERTScore, and RadGraph, and selecting the bottom-10%, RadFlow concentrates learning signal on cases where SFT produces inconsistent or incorrect outputs.

## Foundational Learning

- **Group Relative Policy Optimization (GRPO)**: The base RL algorithm that normalizes rewards within groups of candidate responses, eliminating the need for a learned value function baseline. Why needed: Provides stable policy updates without value function estimation. Quick check: Can you explain why GRPO uses group-wise reward normalization instead of a value function baseline?

- **Vision Large Language Models (VLLMs)**: The architecture combining vision encoder, connector, and LLM that RadFlow fine-tunes. Why needed: Understanding this architecture is essential for implementing SFT and RFT stages. Quick check: What are the three components of a VLLM, and which component is typically frozen during LoRA-based fine-tuning?

- **Medical Report Structure (Findings vs. Impression)**: The hierarchical organization of radiology reports that enables section-specific reward computation. Why needed: The hierarchical reward explicitly depends on parsing these sections. Quick check: If a report lacks explicit section headers, how should the reward computation handle it according to RadFlow's design?

## Architecture Onboarding

- **Component map**: SFT Stage (VLLM + LoRA → π_SFT) → Target Exploration (rank by composite score → select bottom-10% D̃) → Hierarchical Reward (R_global + γ·R_imp) → CAPO (adaptive clipping) → RFT Stage (fine-tune on D̃)

- **Critical path**: SFT must produce fluent outputs; section parser must correctly identify Findings/Impression; GPT-4o annotations must be pre-computed; expert model must be available for Impression labels

- **Design tradeoffs**: LLM-as-judge dependency adds latency/cost; multi-metric threshold selection trades specificity for adaptability; criticality ratio ε_critical = ε_normal/4 is heuristic

- **Failure signatures**: Consistency reward always -1 (GPT-4o prompt misconfigured); R_imp always skipped (section parser failing); no improvement over SFT (target exploration selecting easy samples); training instability on critical cases (ε_critical too large)

- **First 3 experiments**: 1) Ablate hierarchical reward components to verify individual contributions, 2) Vary criticality clipping ratio (ε_normal/2, /4, /8) to measure hallucination vs. learning speed, 3) Stress test section parser with synthesized reports having missing/malformed headers

## Open Questions the Paper Calls Out

- **Open Question 1**: Can adaptive or learned reward models effectively replace the current handcrafted metrics to better capture deeper clinical reasoning? The paper identifies this as a direction for future work, noting that handcrafted metrics may not fully capture nuanced clinical logic.

- **Open Question 2**: How robust is RadFlow when applied to medical domains lacking high-quality pre-trained expert labelers for the Local Impression Reward? The paper had to train tailored metric labelers for ultrasound data, suggesting transferability constraints.

- **Open Question 3**: How does potential misclassification of "critical" cases by GPT-4o triage affect CAPO's safety and convergence? The paper assumes GPT-4o accuracy without analyzing error rates or their impact on theoretical safety bounds.

## Limitations

- Heavy reliance on GPT-4o for consistency checking, criticality annotation, and potentially section parsing creates cost, latency, and reproducibility barriers
- Performance claims primarily validated on MIMIC-CXR and one private ultrasound dataset, limiting generalizability evidence
- Key hyperparameters (γ, β, ε_normal) remain unspecified, affecting reproducibility and potential for optimization

## Confidence

**High Confidence**: Hierarchical reward decomposition improves cross-sectional consistency; Target exploration identifies beneficial samples for RFT; CAPO reduces hallucination risk in high-risk cases

**Medium Confidence**: State-of-the-art performance on MIMIC-CXR and CarotidUS-MRG; approach generalizes across modalities; computational overhead is manageable

**Low Confidence**: Long-term clinical utility beyond automatic metrics; performance on rare pathologies or edge cases; scalability to larger models or different VLLM architectures

## Next Checks

1. **Ablation of Reward Components**: Train variants with global reward only, local reward only, and hierarchical reward on MIMIC-CXR to measure individual contributions to cross-sectional consistency, Impression accuracy, and hallucination rate

2. **Criticality Classification Validation**: Manually annotate 100 random reports as critical/non-critical and compare against GPT-4o classifications to measure precision, recall, and false positive rate for CAPO

3. **Section Parsing Robustness**: Synthesize 50 reports with missing, malformed, or unconventional section headers to verify R_imp is correctly skipped while R_global remains computable, measuring impact on overall reward computation