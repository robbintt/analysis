---
ver: rpa2
title: 'Advancing Conversational AI with Shona Slang: A Dataset and Hybrid Model for
  Digital Inclusion'
arxiv_id: '2509.14249'
source_url: https://arxiv.org/abs/2509.14249
tags:
- intent
- dataset
- shona
- language
- african
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study addresses the underrepresentation of African languages\
  \ in NLP by creating a novel Shona\u2013English slang dataset annotated for intent,\
  \ sentiment, dialogue acts, code-mixing, and tone. A fine-tuned multilingual DistilBERT\
  \ classifier achieves 96.4% accuracy and 96.3% F1-score on intent recognition, integrated\
  \ into a hybrid chatbot combining rule-based responses with retrieval-augmented\
  \ generation (RAG)."
---

# Advancing Conversational AI with Shona Slang: A Dataset and Hybrid Model for Digital Inclusion

## Quick Facts
- **arXiv ID**: 2509.14249
- **Source URL**: https://arxiv.org/abs/2509.14249
- **Reference count**: 4
- **Primary result**: Fine-tuned multilingual DistilBERT achieves 96.4% accuracy on Shona-English slang intent classification

## Executive Summary
This study addresses the critical underrepresentation of African languages in NLP by creating the first annotated dataset of Shona-English conversational slang, containing 34,000 utterances labeled for intent, sentiment, dialogue acts, code-mixing, and tone. The research develops a hybrid chatbot system that combines a fine-tuned multilingual DistilBERT intent classifier with rule-based cultural responses and retrieval-augmented generation (RAG), achieving superior cultural relevance compared to pure RAG baselines. The system demonstrates practical application in assisting prospective students with graduate program information at Pace University, advancing digital inclusion for low-resource African languages.

## Method Summary
The methodology involves collecting 34,000 Shona-English conversational utterances from social media, manually annotating them across five dimensions, and fine-tuning a multilingual DistilBERT model for intent classification. The hybrid chatbot architecture routes high-confidence cultural intents (greetings, farewells) to rule-based Shona responses while directing domain-specific queries to a RAG pipeline using ChromaDB retrieval and flan-t5-small generation. The system includes fallback handling for low-confidence or uncovered intents, with an interactive workflow for multi-turn slot collection in the graduate program use case.

## Key Results
- Fine-tuned multilingual DistilBERT classifier achieves 96.4% accuracy and 96.3% F1-score on intent recognition
- Hybrid system demonstrates superior cultural relevance compared to RAG-only baselines, avoiding generic or off-topic responses
- The dataset, model, and methodology are publicly available, advancing NLP resources for low-resource African languages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning a multilingual transformer on slang-annotated data transfers formal language representations to informal code-mixed inputs.
- Mechanism: DistilBERT's pre-trained multilingual embeddings capture cross-lingual structure; fine-tuning on 34,000 Shona-English slang utterances with intent labels adapts the classifier to informal patterns, achieving 96.4% accuracy. The model learns to map code-mixed tokens (e.g., "Hie swit mom") to intent categories despite limited formal Shona training data.
- Core assumption: Pre-trained multilingual representations transfer sufficiently to Bantu language slang with code-mixing; the slang dataset captures representative linguistic variation.
- Evidence anchors:
  - [abstract]: "fine-tuned a multilingual DistilBERT classifier for intent recognition, achieving 96.4% accuracy"
  - [section 3.2]: "fine-tuned a multilingual DistilBERT model... for intent classification... evaluation loss: 0.387"
  - [corpus]: Limited direct evidence—neighbor papers focus on formal African NLP (AfriBERTa, spaCy morphological analyzers); this work appears novel for slang/code-mixed Shona conversational AI.
- Break condition: If slang vocabulary shifts rapidly beyond training distribution, or if code-mixing patterns diverge significantly from social media sources, intent classification accuracy degrades.

### Mechanism 2
- Claim: Hybrid routing (rule-based + RAG) outperforms pure RAG for cultural relevance in low-resource language chatbots.
- Mechanism: Intent classification routes predictable cultural intents (greetings, farewells) to handcrafted Shona responses with cultural nuances (e.g., "Hesi shamwari! Uri sei hako?"), while domain-specific queries (education, finance) trigger RAG retrieval from ChromaDB. This prevents the generator from producing culturally generic or off-target responses for non-informational exchanges.
- Core assumption: Intent classification is accurate enough to route correctly; rule-based templates cover sufficient cultural variations; RAG knowledge base is complete for domain queries.
- Evidence anchors:
  - [section 4.3]: "hybrid system was compared to a RAG-only baseline... hybrid model provided culturally relevant responses... baseline generated generic or off-topic replies"
  - [section 3.3]: "Predefined Shona responses for intents like greetings or farewells, incorporating cultural nuances"
  - [corpus]: Weak—neighbor papers don't evaluate hybrid architectures for African languages; most focus on monolingual retrieval or classification pipelines.
- Break condition: If intent classifier confidence is miscalibrated, rule-based responses may trigger incorrectly; if knowledge base lacks coverage (noted for finance intent), RAG fails without graceful fallback.

### Mechanism 3
- Claim: Multi-dimensional annotation (intent, sentiment, dialogue acts, code-mixing, tone) enables richer dialogue management than intent-only classification.
- Mechanism: The dataset includes 5 annotation dimensions, though the current system primarily leverages intent. The additional labels provide infrastructure for future components (e.g., sentiment-aware responses, tone-appropriate generation). Hybrid oversampling/downsampling addresses class imbalance across intent categories.
- Core assumption: Annotations are reliable and consistent; downstream components will eventually consume non-intent dimensions.
- Evidence anchors:
  - [section 3.1]: "manually annotated for: Intent, Sentiment, Dialogue Acts, Code-Mixing Features, Tone"
  - [section 4.1]: "high alignment of metrics indicates robust generalization across imbalanced intent classes"
  - [corpus]: No neighbor papers explicitly compare single vs. multi-task annotation for African language dialogue; evidence is extrapolative.
- Break condition: If annotations are noisy (inter-annotator agreement unreported), or if only intent is utilized, the additional annotation cost yields no system improvement.

## Foundational Learning

- Concept: **Code-mixing in NLP**
  - Why needed here: The dataset contains Shona-English switches at word and phrase levels (e.g., "pace inoita mari?"); standard tokenizers and monolingual models fail on such inputs.
  - Quick check question: Can you explain why a monolingual English tokenizer would missegment "wadii" (Shona slang for "what's up")?

- Concept: **Transfer learning for low-resource languages**
  - Why needed here: DistilBERT was not trained on Shona, yet fine-tuning enables intent classification; understanding what transfers (subword sharing, multilingual attention) is critical.
  - Quick check question: What properties of multilingual BERT enable zero-shot or few-shot transfer to unseen languages?

- Concept: **Retrieval-Augmented Generation (RAG)**
  - Why needed here: The hybrid chatbot uses all-MiniLM-L6-v2 embeddings, ChromaDB retrieval, and flan-t5-small generation; understanding the retrieval-generation pipeline is essential for debugging.
  - Quick check question: If the retriever returns irrelevant documents, how will the generator respond, and where should you intervene?

## Architecture Onboarding

- Component map:
  Input → DistilBERT Intent Classifier → Intent Router → Rule-Based Templates (cultural intents) OR RAG Pipeline (domain intents) → Fallback Handler (low confidence)

- Critical path: Intent classification accuracy → correct routing → appropriate response type (rule vs. RAG). A single misclassification sends a greeting to RAG (yielding off-topic output) or a domain query to templates (yielding no information).

- Design tradeoffs:
  - DistilBERT (faster, smaller) vs. larger multilingual models (XLM-R, AfriBERTa)—compute constrained on Colab.
  - Rule-based cultural responses vs. generated responses—preserves authenticity but lacks scalability.
  - Top-5 retrieval vs. fewer—more context but higher latency and potential noise.

- Failure signatures:
  - "Fallback reply due to limited training data" (Section 4.2, finance query)—indicates intent coverage gap.
  - Generic/off-topic responses for non-educational queries in RAG-only mode—indicates missing cultural grounding.
  - Low confidence scores on slang variants—indicates vocabulary out-of-distribution.

- First 3 experiments:
  1. Intent confusion analysis: Run classifier on validation set; identify which intent pairs are confused; prioritize data augmentation for those classes (e.g., finance vs. education).
  2. Routing threshold tuning: Vary confidence threshold for rule-based vs. RAG routing; measure precision/recall for each response type on held-out dialogues.
  3. Retrieval ablation: Test top-1, top-3, top-5 retrieval; measure generation quality (human or LLM judge) to find optimal retrieval depth for this domain.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the integration of multimodal audio inputs affect the intent classification accuracy and latency of the hybrid chatbot compared to the text-only DistilBERT pipeline?
- Basis in paper: [explicit] The authors explicitly list "Integrating audio inputs for multimodal dialogue" as a primary direction for future work.
- Why unresolved: The current study is restricted to text-based inputs curated from social media, lacking an architecture to handle spoken slang or prosodic features.
- What evidence would resolve it: A comparative benchmark evaluating Word Error Rate (WER) and intent classification F1-scores on a spoken Shona slang test set.

### Open Question 2
- Question: To what extent does human-in-the-loop (HITL) evaluation align with the automated metrics regarding the system's cultural relevance and user engagement?
- Basis in paper: [explicit] The conclusion identifies "Conducting human-in-the-loop evaluations" as necessary future work to validate the qualitative findings.
- Why unresolved: The current evaluation relies heavily on standard NLP metrics (accuracy, F1) and basic qualitative observations, which may not fully capture nuanced cultural appropriateness.
- What evidence would resolve it: Results from a user study involving native Shona speakers rating response quality and cultural resonance on standardized scales (e.g., Likert scores).

### Open Question 3
- Question: Can targeted data augmentation or cross-lingual transfer techniques effectively resolve the performance degradation observed in under-represented intent categories like finance?
- Basis in paper: [inferred] The limitations section notes that the dataset's size limits coverage for specific intents, resulting in fallback replies for finance-related queries.
- Why unresolved: The current model struggles with class imbalance and data scarcity in specific domains, leading to functional gaps in the chatbot's capabilities.
- What evidence would resolve it: Ablation studies showing improved F1-scores for the "finance" intent class after applying synthetic oversampling or transfer learning from high-resource languages.

## Limitations
- Intent classification performance on out-of-domain slang remains uncertain due to potential vocabulary evolution and social media dataset limitations
- Hybrid routing reliability depends on untested confidence calibration thresholds for routing decisions
- Knowledge base completeness for domain queries is explicitly acknowledged as insufficient, particularly for finance-related intents

## Confidence
- **High confidence**: Dataset creation methodology is sound with clear annotation guidelines and reasonable sample size
- **Medium confidence**: 96.4% accuracy appears robust but real-world generalization remains untested; cultural relevance improvements demonstrated but not fully quantified
- **Low confidence**: Multi-dimensional annotation benefits are weakly supported as current system primarily uses only intent labels

## Next Checks
1. Real-world deployment study: Deploy the chatbot with actual prospective students over a 3-month period, tracking user satisfaction, task completion rates, and identifying failure patterns in intent classification and routing.

2. Cross-cultural generalization test: Evaluate the system with users from different cultural backgrounds (e.g., South African, Nigerian) to assess whether the rule-based cultural responses remain relevant or require adaptation.

3. Knowledge base expansion and fallback evaluation: Systematically test the finance and other low-coverage intents with synthetic queries, measuring retrieval quality and implementing graceful degradation mechanisms when knowledge base coverage is insufficient.