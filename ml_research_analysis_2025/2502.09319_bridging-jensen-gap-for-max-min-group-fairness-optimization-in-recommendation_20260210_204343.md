---
ver: rpa2
title: Bridging Jensen Gap for Max-Min Group Fairness Optimization in Recommendation
arxiv_id: '2502.09319'
source_url: https://arxiv.org/abs/2502.09319
tags:
- group
- fairness
- conference
- jensen
- fairdual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses a critical gap in fairness-aware recommendation
  systems by showing that integrating max-min group fairness (MMF) constraints disrupts
  the sample independence assumption, causing non-linear loss functions and introducing
  a Jensen gap between convergence and optimal points during mini-batch optimization.
  The authors theoretically prove that MMF optimization can be reformulated as a group-weighted
  accuracy objective, then propose FairDual, an efficient dual-optimization algorithm
  using mirror gradient descent to minimize the Jensen gap.
---

# Bridging Jensen Gap for Max-Min Group Fairness Optimization in Recommendation

## Quick Facts
- arXiv ID: 2502.09319
- Source URL: https://arxiv.org/abs/2502.09319
- Reference count: 40
- Primary result: FairDual algorithm achieves 30% faster convergence while improving both accuracy (NDCG, MRR) and fairness (MMF) in recommendation systems

## Executive Summary
This paper addresses a fundamental optimization challenge in fairness-aware recommendation systems: integrating max-min group fairness (MMF) constraints disrupts the sample independence assumption during mini-batch training, creating a Jensen gap between convergence and optimal points. The authors theoretically prove that MMF optimization can be reformulated as a group-weighted accuracy objective and propose FairDual, a dual-optimization algorithm using mirror gradient descent to minimize this gap. FairDual achieves sub-linear convergence rates and effectively bounds the Jensen gap even with small mini-batches and large group sizes, demonstrating consistent improvements across six large-scale recommender models and three public datasets.

## Method Summary
FairDual reformulates the MMF-constrained problem as a dual optimization task where group fairness constraints become sample weights. The algorithm alternates between updating model parameters using weighted loss and updating dual variables via mirror descent with momentum. A frozen copy of item embeddings (updated every β batches) and sampling Q items approximate ranking scores for efficient gradient estimation. The method provably bounds the Jensen gap with sub-linear convergence, addressing the non-linear additivity issue that arises when applying fairness constraints to mini-batch optimization.

## Key Results
- 30% improvement in convergence speed compared to baseline methods
- Statistically significant gains across all accuracy metrics (NDCG, MRR) and fairness metrics (MMF)
- Jensen gap bounded by O(1/√B) with appropriate hyperparameter settings
- Consistent performance improvements across six recommender models and three public datasets

## Why This Works (Mechanism)

### Mechanism 1: Jensen Gap Formation from Non-Linear Additivity
Integrating MMF constraints violates sample independence during optimization, causing loss functions to deviate from linear additivity. The fairness constraint aggregates utility across all users (Equation 1: group utility requires summing over entire user set U), then applies a max operation. When mini-batch sampling partitions users, the per-batch max-min operation differs from the full-dataset max-min due to convexity properties. Theorem 2 formalizes this: batch-level optimization LB ≠ full objective L, with gap widening as batch size B decreases or group size |G| increases.

### Mechanism 2: Dual Reformulation as Group-Weighted Optimization
The MMF-constrained problem can be exactly reformulated as a re-weighted accuracy objective. Theorem 3 shows the primal problem (Equation 1) has a dual form L' = min Σ_g s_g · (group accuracy loss) where s_g = 1 - μ_g. The dual variable μ (shadow price) represents constraint tightness: high μ_g means group g's fairness constraint dominates. This transforms a constrained max-min problem into unconstrained weighted optimization, where weights adapt to identify worst-off groups.

### Mechanism 3: Mirror Gradient Descent with Momentum for Dual Optimization
FairDual alternates between updating model parameters on weighted loss and updating dual variables μ via mirror descent (Equation 8). Momentum gradient g_j = α·eg_j + (1-α)·g_{j-1} smooths noisy batch estimates. Sampling Q items approximates full ranking scores. Frozen item embeddings (updated every β batches) stabilize ew estimates. Theorem 4 proves Jensen gap bound: J(B) ≤ H/η + O(|G|²/√B).

## Foundational Learning

- **Concept: Jensen Gap / Jensen's Inequality**
  - **Why needed here:** Core theoretical contribution showing why standard mini-batch SGD fails for MMF objectives.
  - **Quick check question:** Given f(x) = x^{1+t} with t > 0, why does E[f(X)] ≠ f(E[X]) cause optimization divergence when X represents batch-level group utilities?

- **Concept: Lagrangian Duality and Shadow Prices**
  - **Why needed here:** Enables reformulation of constrained MMF into tractable weighted optimization; shadow prices interpret group weights.
  - **Quick check question:** In the dual form L' = min Σ_g s_g · L_g, what does a high shadow price μ_g indicate about group g's fairness constraint?

- **Concept: Mirror Gradient Descent**
  - **Why needed here:** Efficiently solves the dual problem with convergence guarantees in high-dimensional group-weight space.
  - **Quick check question:** How does mirror descent differ from standard gradient descent when optimizing over the constrained dual variable space M?

## Architecture Onboarding

- **Component map:** Loss Constructor -> Item Embedding Store -> Ranking Estimator -> Dual Optimizer -> Weight Computer

- **Critical path:**
  1. Every β batches: Copy mi(·) → mi_freeze(·), compute all item embeddings E
  2. Per batch: Compute loss l_j, estimate ranking scores ew via sampling, update momentum gradient g_j
  3. Solve μ_j = argmin_{μ∈M} [g_j^T μ + η||μ - μ_{j-1}||²] (convex projection)
  4. Backprop on weighted loss s_j^T l_j

- **Design tradeoffs:**
  - **Batch size B:** Smaller B → larger Jensen gap risk but faster iterations; Theorem 4 shows J(B) ∝ 1/√B with FairDual mitigation
  - **Sample size Q:** Larger Q → better ew estimation but +1.5s per item; Table 5 shows diminishing returns after Q=200
  - **Freeze interval β:** Too frequent → instability; too rare → stale embeddings; Figure 4 shows peak at β∈[640, 1280]
  - **Dual learning rate η:** Controls weight adaptation speed; Figure 5 shows η=5e-5 optimal for MIND

- **Failure signatures:**
  - **Jensen gap expansion:** MMF improves but accuracy collapses → batch size too small or group count too high
  - **Weight oscillation:** Group weights s_g fluctuate wildly → dual learning rate η too high or β too small
  - **Stale embeddings:** Frozen embedding distribution diverges from active model → β too large
  - **Projection failure:** Dual solver fails on M → constraint violation, check λ setting

- **First 3 experiments:**
  1. **Jensen gap validation:** Replicate Figure 1 simulation with MF backbone, varying B∈{2,8,32} and |G|∈{3,7,15}, measuring distance to known optimal point. Confirms Theorem 2 prediction.
  2. **Ablation on dual components:** Compare (a) FairDual, (b) FairDual without momentum (α=0), (c) FairDual with full ranking (Q=|I|) vs sampling. Isolate contribution of momentum and sampling approximation.
  3. **Hyperparameter sweep on target dataset:** Grid search η∈{1e-5, 5e-5, 1e-4}, β∈{256, 640, 1280}, λ∈{1, 2, 5} on validation set. Establishes operating region before production deployment.

## Open Questions the Paper Calls Out

- **Question:** Can FairDual be effectively combined with existing debiasing methods (e.g., Inverse Propensity Score) without breaking convergence guarantees when the fairness trade-off coefficient λ is large?
  - **Basis in paper:** [explicit] Appendix N states "when λ is large, adding the IPS will not perform very well. This is because IPS will break the convergence condition of FairDual."
  - **Why unresolved:** The paper identifies the incompatibility but does not propose a unified framework that preserves both debiasing benefits and FairDual's convergence properties.
  - **What evidence would resolve it:** A modified algorithm or theoretical analysis showing that FairDual+IPS maintains sub-linear convergence across all λ values, with empirical validation on datasets with known popularity bias.

- **Question:** How does FairDual perform under non-uniform mini-batch sampling strategies (e.g., stratified sampling, importance sampling) compared to the random shuffle strategy analyzed in Theorem 4?
  - **Basis in paper:** [inferred] The theoretical convergence bound (Theorem 4) specifically assumes "a mini-batch sampling strategy with random shuffle," leaving other sampling regimes unexplored.
  - **Why unresolved:** The Jensen gap bounds may differ under biased sampling, potentially improving or degrading performance depending on group distribution within batches.
  - **What evidence would resolve it:** Theoretical bounds on Jensen gap under alternative sampling strategies, coupled with empirical comparison across different batch composition methods.

- **Question:** Can the dual optimization framework in FairDual be extended to handle dynamic group memberships or hierarchical group structures where items belong to multiple overlapping groups?
  - **Basis in paper:** [inferred] Section 3 defines groups as fixed categories, and while ni (number of groups an item belongs to) is mentioned, the algorithm treats group assignment as static adjacency matrix A without addressing temporal or hierarchical relationships.
  - **Why unresolved:** Real-world recommendation scenarios often involve evolving categories (e.g., trending topics) or hierarchical taxonomies (e.g., product categories with subcategories).
  - **What evidence would resolve it:** Modified algorithm handling time-varying adjacency matrices or hierarchical dual variables, with experiments on datasets with dynamic or multi-level group structures.

## Limitations
- The Jensen gap analysis assumes convex loss functions, but neural network non-linearities may invalidate this assumption in practice
- The frozen embedding approach introduces staleness that could accumulate over long training
- The constraint set M requires solving over all subsets of groups, which may not scale beyond ~20 groups despite cvxpy optimizations

## Confidence
- **Dual reformulation (Theorem 3):** High confidence - complete mathematical derivation provided
- **Jensen gap bounds (Theorem 4):** Medium confidence - depends on empirical hyperparameter tuning
- **Frozen embedding approach:** Medium confidence - introduces potential staleness but shown effective empirically

## Next Checks
1. Test FairDual on non-convex loss functions (e.g., transformer-based recommenders) to verify Jensen gap bounds hold beyond MF backbones
2. Measure staleness accumulation by tracking ranking score distribution divergence between frozen and active embeddings over training epochs
3. Profile memory and runtime overhead when scaling to 50+ groups to identify practical scalability limits