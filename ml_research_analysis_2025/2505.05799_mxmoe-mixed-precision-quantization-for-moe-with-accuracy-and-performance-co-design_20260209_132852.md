---
ver: rpa2
title: 'MxMoE: Mixed-precision Quantization for MoE with Accuracy and Performance
  Co-Design'
arxiv_id: '2505.05799'
source_url: https://arxiv.org/abs/2505.05799
tags:
- quantization
- mxmoe
- arxiv
- mixed-precision
- expert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MxMoE introduces a mixed-precision quantization framework for Mixture-of-Experts
  (MoE) models that jointly optimizes model accuracy and hardware performance. The
  method identifies heterogeneous quantization sensitivity across linear blocks within
  MoE experts and exploits varying activation frequencies to assign different precision
  levels.
---

# MxMoE: Mixed-precision Quantization for MoE with Accuracy and Performance Co-Design

## Quick Facts
- arXiv ID: 2505.05799
- Source URL: https://arxiv.org/abs/2505.05799
- Reference count: 40
- Primary result: Achieves up to 3.4x speedup over full precision and 29.4% over uniform quantization at equivalent accuracy

## Executive Summary
MxMoE introduces a mixed-precision quantization framework for Mixture-of-Experts models that jointly optimizes model accuracy and hardware performance. The method identifies heterogeneous quantization sensitivity across linear blocks within MoE experts and exploits varying activation frequencies to assign different precision levels. By navigating the design space defined by parameter sensitivity, expert activation patterns, and hardware resources, MxMoE automatically generates optimized mixed-precision GroupGEMM kernels enabling parallel execution of operations with different precisions.

## Method Summary
MxMoE employs a four-stage pipeline: (1) profiling quantization sensitivity and expert activation frequencies on calibration data, (2) solving an ILP problem to allocate bit-widths to linear blocks under memory constraints, (3) applying randomized Hadamard transformation followed by GPTQ-based quantization, and (4) generating mixed-precision GroupGEMM kernels with specialized micro-kernels and a greedy tile scheduler. The framework balances accuracy and efficiency through a composite objective function, minimizing quantization error while maximizing computational throughput.

## Key Results
- Achieves 2.4x lower Wikitext-2 perplexity than GPTQ at 2.25-bit
- Delivers up to 3.4x speedup over full precision
- Provides up to 29.4% speedup over uniform quantization at equivalent accuracy with 5-bit weight-activation quantization

## Why This Works (Mechanism)

### Mechanism 1: Linear-block Precision Allocation
Quantization sensitivity is significantly heterogeneous at the linear-block level within MoE experts. MxMoE profiles quantization loss for each linear block (gate, up, down) and uses ILP to assign precision schemes that minimize a composite objective balancing quantization loss against execution time.

### Mechanism 2: Frequency-Aware Scheme Selection
Divergent expert activation frequencies create heterogeneous computational characteristics. MxMoE uses roofline analysis to classify operations as memory or compute-bound based on activation patterns and batch size, then assigns weight-only schemes to memory-bound experts and weight-activation schemes to compute-bound experts.

### Mechanism 3: Fused Mixed-Precision Kernels
Automatically generated mixed-precision GroupGEMM kernels enable efficient parallel execution with heterogeneous precision. MxMoE generates fused kernels from specialized micro-kernels with uniform warp counts, enabling "horizontal fusion" into a single kernel with minimal overhead.

## Foundational Learning

- **Concept: Quantization Sensitivity & Perplexity**
  - **Why needed:** To understand the core accuracy trade-off; MxMoE aims to minimize perplexity increase by identifying sensitive blocks.
  - **Quick check:** In MxMoE, what is the unit of analysis for quantization sensitivity and how is the loss metric (Δ) formally defined?

- **Concept: Arithmetic Intensity & Roofline Model**
  - **Why needed:** To understand hardware-aware optimization; MxMoE uses this to classify computations as memory or compute-bound.
  - **Quick check:** How does arithmetic intensity change with expert activation frequency, and how does this influence choice between weight-only and weight-activation quantization?

- **Concept: GroupGEMM & Horizontal Fusion**
  - **Why needed:** To understand system implementation; custom kernel solves parallel execution with different shapes and precisions.
  - **Quick check:** What critical constraint does MxMoE place on tile configurations to enable fusion into a single kernel?

## Architecture Onboarding

- **Component map:** Offline Profiler -> ILP Allocator -> Quantizer -> Kernel Generator -> Runtime Engine
- **Critical path:** The Allocator's ILP formulation is the decision-making core; the Kernel Generator's tile scheduler is performance-critical.
- **Design tradeoffs:** Accuracy vs. Speed (r parameter), granularity of allocation (linear-block vs. expert-level), kernel universality vs. specialization.
- **Failure signatures:** Accuracy collapse from aggressive allocation, performance regression from suboptimal scheduling, kernel launch failure from incompatible resource requirements.
- **First 3 experiments:**
  1. Baseline Profiling: Visualize per-block sensitivity and activation frequencies to validate heterogeneity assumptions.
  2. Allocator Validation (r=1): Measure perplexity against uniform GPTQ baseline to validate sensitivity mechanism.
  3. End-to-End Performance Run: Generate kernel for W5A5 config and measure throughput/latency against FP16 and uniform baselines.

## Open Questions the Paper Calls Out

### Open Question 1: Cross-layer Sensitivity Metrics
Can cross-layer sensitivity metrics improve bitwidth allocation accuracy compared to current layer-wise loss formulation, particularly for models like Qwen2-MoE where inter-layer dependencies may amplify sensitivity estimation errors? The authors note this could mitigate current accuracy issues but leave it to future work.

### Open Question 2: Design Space for Sparse Experts
Can the mixed-precision design space be effectively expanded for models with fewer experts (e.g., Mixtral-8×7B) to achieve more substantial performance gains over uniform quantization? The limited number of experts constrains combinatorial flexibility for heterogeneous precision assignment.

### Open Question 3: Automatic r Selection
How can the hyperparameter r (balancing accuracy vs. efficiency) be automatically selected for a given deployment scenario without requiring manual tuning? The paper introduces r as a manual hyperparameter but provides no principled method for automatic selection based on workload or hardware characteristics.

## Limitations

- Design space complexity with large ILP search space (64 experts × 3 blocks × multiple schemes) may require specific solver configurations
- Method's effectiveness appears sensitive to quality and diversity of calibration dataset
- Performance gains may vary across GPU architectures due to hardware model assumptions

## Confidence

**High Confidence Claims:**
- Mixed-precision GroupGEMM kernel implementation is technically sound and outperforms sequential execution
- Linear-block allocation approach provides accuracy benefits over expert-level allocation
- Method achieves stated perplexity and throughput improvements on tested models and hardware

**Medium Confidence Claims:**
- Roofline-based scheme selection effectively exploits computational heterogeneity
- Mixed-precision approach consistently outperforms uniform quantization across different models and bit-widths

## Next Checks

1. **Calibration Sensitivity Test:** Run allocator with progressively smaller and less diverse calibration datasets to quantify sensitivity to calibration data quality.

2. **Architecture Portability Benchmark:** Implement and benchmark MxMoE on different GPU architecture (e.g., AMD Instinct or AWS Trainium) to validate hardware model assumptions.

3. **Inter-layer Dependency Analysis:** For models sensitive to calibration, conduct controlled ablation study with and without considering inter-layer dependencies to isolate impact on accuracy degradation.