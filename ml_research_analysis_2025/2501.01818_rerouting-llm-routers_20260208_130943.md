---
ver: rpa2
title: Rerouting LLM Routers
arxiv_id: '2501.01818'
source_url: https://arxiv.org/abs/2501.01818
tags:
- queries
- attack
- query
- strong
- perplexity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces LLM control plane integrity as a novel AI
  safety problem, defining it as the robustness of LLM orchestration systems to adversarial
  inputs. The authors demonstrate that attackers can generate query-independent "confounder
  gadgets" - short token sequences that, when added to any query, reliably cause LLM
  routers to route queries to stronger, more expensive models.
---

# Rerouting LLM Routers

## Quick Facts
- arXiv ID: 2501.01818
- Source URL: https://arxiv.org/abs/2501.01818
- Reference count: 40
- Key outcome: Attackers can generate query-independent "confounder gadgets" that reliably cause LLM routers to route queries to stronger, more expensive models with upgrade rates often exceeding 90%.

## Executive Summary
This paper introduces LLM control plane integrity as a novel AI safety problem, defining it as the robustness of LLM orchestration systems to adversarial inputs. The authors demonstrate that attackers can generate query-independent "confounder gadgets" - short token sequences that, when added to any query, reliably cause LLM routers to route queries to stronger, more expensive models. Their evaluation shows this attack is highly effective in both white-box and black-box settings against multiple open-source and commercial routers, with upgrade rates often exceeding 90%. Notably, the attack does not degrade response quality. The authors also show that perplexity-based filtering defenses can be evaded by incorporating low-perplexity objectives into gadget generation, and discuss alternative defense strategies including user workload monitoring and active paraphrasing.

## Method Summary
The attack uses hill-climbing optimization to generate query-independent gadgets that maximize a router's complexity score. Starting with an initial gadget of n=10 tokens (all '!'), the algorithm iteratively replaces tokens to find high-scoring variants. For T=100 iterations, it randomly selects a position j, generates B=32 candidate replacements from the vocabulary, and selects the candidate maximizing the router's scoring function S_θ(c). The attack is evaluated on three benchmark datasets (MT-Bench, MMLU, GSM8K) using four router types (similarity-weighted, matrix factorization, BERT classifier, LLM-based) and measures upgrade rates as the percentage of weak-routed queries successfully rerouted to strong models.

## Key Results
- White-box attacks achieve upgrade rates exceeding 90% across all tested router architectures
- Black-box attacks transfer successfully between different open-source routers with upgrade rates often above 60%
- Perplexity-based filtering defenses can be evaded by incorporating low-perplexity objectives into gadget generation
- The attack does not degrade response quality, with refusal rates below 1% in most cases
- Commercial routers are vulnerable, though exact upgrade rates vary based on internal configurations

## Why This Works (Mechanism)

### Mechanism 1: Gradient-Free Score Maximization
The attack exploits routers' sensitivity to specific token patterns by using hill-climbing optimization to iteratively replace tokens in a gadget until the router's scoring function is maximized. The router's classifier responds to these token patterns regardless of semantic meaning, allowing attackers to generate effective gadgets without access to model gradients.

### Mechanism 2: Cross-Model Transferability
Routers trained on similar data or with similar architectures learn analogous decision boundaries, enabling adversarial patterns that confuse one classifier to confuse another. This transferability allows black-box attacks where attackers generate gadgets on a surrogate router that transfer to unknown target routers.

### Mechanism 3: Low-Perplexity Objective Alignment
Perplexity-based defenses can be evaded by modifying the optimization objective to simultaneously maximize the routing score while keeping perplexity close to natural query values. This demonstrates that the feature spaces of "complexity" for routing and "naturalness" for perplexity are not mutually exclusive.

## Foundational Learning

- **LLM Control Plane**: Understanding that the attack targets the orchestration logic (router), not the underlying LLM. Why needed: To recognize that the router is a distinct component with its own integrity constraints. Quick check: Does the attack attempt to change the output of the Strong LLM, or the decision to use the Strong LLM?

- **Input Adaptation Attacks**: This defines the threat model where the adversary must modify a specific query to flip the routing decision while preserving the query's utility. Why needed: To understand the constraints on the attack - it must be effective without degrading response quality. Quick check: If an adversary adds a gadget that routes to the strong model but causes the LLM to output gibberish, has the "arbitrage" attack succeeded?

- **Router Calibration (τ)**: The attack efficacy is measured relative to the router's threshold τ. Understanding calibration is necessary to interpret why a specific score increase triggers a route change. Quick check: If a router is recalibrated to route 90% of queries to the strong model by default, does the "upgrade" attack become more or less relevant?

## Architecture Onboarding

- Component map: Attacker -> Surrogate Router (black-box) -> Target Router -> Control Plane -> Underlying LLMs (Strong M_s, Weak M_w)
- Critical path: 1) User query x 2) Prepend gadget c → ĉ = c \| x 3) Router computes S_θ(ĉ) 4) If S_θ(ĉ) ≥ τ, route to M_s 5) Response from M_s
- Design tradeoffs: Gadget length (shorter = stealthier but less reliable vs longer = more reliable but more obvious), attack precision (query-specific = highly effective but expensive vs query-independent = "fire-and-forget" but slightly less effective)
- Failure signatures: Downgrades (rare, gadget causes complex query to route to weak model), refusals (<1%, LLM refuses due to "unnatural" gadget), detection (perplexity spikes if attacker doesn't optimize for low perplexity)
- First 3 experiments: 1) White-box replication against RouteLLM to verify 90%+ upgrade rate on GSM8K, 2) Transfer testing using Matrix Factorization router gadgets against BERT-based router to quantify transfer degradation, 3) Defense evasion implementing perplexity filter then activating α-weighted loss to verify ROC AUC drops below 0.7

## Open Questions the Paper Calls Out

### Open Question 1
Can control-plane integrity attacks be extended to internal Mixture-of-Experts (MoE) architectures? The authors note that vulnerabilities exist in MoE routing and explicitly state "future work could explore this connection further." This remains unresolved because the current study focused on external control planes orchestrating distinct models rather than internal expert selection mechanisms. Evidence that would resolve it includes demonstrating that "confounder gadgets" can force internal MoE layers to overload specific experts or bypass safety-trained experts.

### Open Question 2
Can subverting router integrity facilitate evasion attacks against safety guardrails? Section 3 states "We leave evaluation of how control-plane integrity attacks can enable evasion to future work." This remains unresolved because the paper focused on cost inflation and quality arbitrage, not on whether forcing a specific route allows adversarial prompts to bypass content filters. Evidence that would resolve it includes experiments showing routing queries to weaker or specific models via gadgets increases jailbreaking success rates.

### Open Question 3
What low-cost detection methods can robustly identify confounder gadgets without neutralizing economic benefits of routing? The paper demonstrates perplexity-based filtering fails while paraphrasing and LLM-based filtering are dismissed as too expensive. This remains unresolved because there is currently no proposed defense that effectively detects adversarial inputs while remaining computationally cheaper than the cost savings provided by the router itself. Evidence that would resolve it includes a lightweight defense mechanism that maintains high detection rates against low-perplexity gadgets without adding significant latency or inference cost.

## Limitations
- The attack's effectiveness against commercial routers is inferred from performance against open-source variants rather than direct measurement
- The paper focuses on query-independent gadgets, which may not be necessary in practice when query-specific gadgets could achieve higher success rates
- The claim that attackers can achieve their goals "without degrading response quality" relies on <1% refusal rates but doesn't account for potential quality degradation in non-refusal responses

## Confidence

- **High Confidence**: White-box attack mechanics and gadget generation algorithm are clearly specified and reproducible. Effectiveness of perplexity-based defenses and their evasion through low-perplexity optimization is demonstrated with quantitative evidence.
- **Medium Confidence**: Black-box transferability results depend on architectural similarities that are not fully characterized. Attack effectiveness against commercial routers is inferred rather than directly measured.
- **Low Confidence**: Claim that attackers achieve goals "without degrading response quality" relies on <1% refusal rates but doesn't measure semantic coherence or task-specific performance degradation.

## Next Checks

1. **Transferability Validation**: Generate gadgets using a surrogate router with fundamentally different architecture (rules-based vs learned classifier) and measure transfer success rates to a BERT-based target router to establish minimum architectural similarity required.

2. **Commercial Router Testing**: Implement the attack against publicly documented commercial router APIs (OpenAI's function calling, Anthropic's tool use) to measure actual upgrade rates and compare with open-source router performance under black-box conditions.

3. **Quality Degradation Analysis**: Evaluate task-specific performance metrics (exact match scores on GSM8K, accuracy on MMLU) for confounded queries routed to strong models versus non-confounded queries to detect subtle quality degradation beyond simple refusal patterns.