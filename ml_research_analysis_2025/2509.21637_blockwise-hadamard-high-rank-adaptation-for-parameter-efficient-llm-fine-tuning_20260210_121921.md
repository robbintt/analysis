---
ver: rpa2
title: Blockwise Hadamard high-Rank Adaptation for Parameter-Efficient LLM Fine-Tuning
arxiv_id: '2509.21637'
source_url: https://arxiv.org/abs/2509.21637
tags:
- rank
- bhra
- hira
- lora
- hadamard
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the limitation of classical low-rank adaptation
  (LoRA) and Hadamard-style adapters in parameter-efficient fine-tuning (PEFT) for
  large language models, where global modulation couples updates to the frozen weight
  matrix's energy pattern, restricting effective rank. To overcome this, the authors
  propose Block Hadamard High-Rank Adaptation (BHRA), which partitions each weight
  matrix into a grid and applies independent HiRA-style multiplicative modulation
  within each block, preserving the PEFT parameter footprint while enabling localized
  rank amplification.
---

# Blockwise Hadamard high-Rank Adaptation for Parameter-Efficient LLM Fine-Tuning

## Quick Facts
- arXiv ID: 2509.21637
- Source URL: https://arxiv.org/abs/2509.21637
- Reference count: 16
- Key outcome: BHRA achieves 0.69%–4.70% accuracy improvements over LoRA/HiRA baselines across 10 commonsense reasoning and arithmetic tasks while maintaining matched parameter budgets.

## Executive Summary
This paper addresses the rank limitation in parameter-efficient fine-tuning (PEFT) methods like LoRA and Hadamard-style adapters, where global modulation restricts effective rank updates. The authors propose Block Hadamard High-Rank Adaptation (BHRA), which partitions weight matrices into grids and applies independent HiRA-style modulation within each block. This preserves the PEFT parameter footprint while enabling localized rank amplification. Experimental results demonstrate consistent improvements across eight commonsense reasoning tasks and two arithmetic benchmarks using Llama-3.2 1B/3B, Mistral-7B, and Gemma-2 9B models.

## Method Summary
BHRA implements blockwise Hadamard modulation by partitioning each frozen weight matrix into a b×b grid and applying independent low-rank factorization within each block. Each block (i,j) receives update ΔWᵢⱼ = W₀,ᵢⱼ ⊙ (BᵢⱼAᵢⱼ), where Bᵢⱼ and Aᵢⱼ are block-specific low-rank factors. The total rank budget r is distributed as r_b×b, with r_b being the per-block rank. This design preserves pretrained inductive bias while localizing gradient signals to prevent global suppression. The method maintains PEFT parameter efficiency through sparse updates and enables precompute+fold inference matching LoRA/HiRA computational costs.

## Key Results
- BHRA achieves consistent accuracy improvements of 0.69%–4.70% across 10 benchmarks compared to LoRA and HiRA baselines
- Spectral analysis confirms BHRA maintains richer singular-value structures and higher effective ranks, particularly in deeper layers
- Block heterogeneity (measured by block Gini) shows strong correlation with task performance, validating the localized modulation design
- Optimal configuration found at r_b=4, b=8 for total rank budget r=32

## Why This Works (Mechanism)

### Mechanism 1: Blockwise Spatial Decoupling of Global Modulation
HiRA couples every element of ΔW to the global energy pattern of W₀, limiting rank amplification. BHRA partitions W₀ into blocks, with each block update ΔWᵢⱼ = W₀,ᵢⱼ ⊙ (BᵢⱼAᵢⱼ). By the Hadamard rank inequality, rank(ΔWᵢⱼ) ≤ rank(W₀,ᵢⱼ) × rank(BᵢⱼAᵢⱼ). Summing across blocks yields rank(ΔW) ≤ b · r₀ · r. This enables b-fold rank amplification while preserving the parameter budget. Break condition: If blocks become too large (b → 1), collapses to HiRA with global coupling.

### Mechanism 2: Gradient Localization via Blockwise Pretrained Slicing
Blockwise modulation localizes gradient signals to corresponding slices of W₀. For block (i,j), gradients factor through Gᵢⱼ = gᵢxⱼᵀ, with ∇Aᵢⱼ L = Bᵢⱼᵀ(W₀,ᵢⱼ ⊙ Gᵢⱼ) and ∇Bᵢⱼ L = (W₀,ᵢⱼ ⊙ Gᵢⱼ)Aᵢⱼᵀ. Each block is guided only by its slice W₀,ᵢⱼ, unlike HiRA where the full W₀ modulates all gradients. This prevents globally small entries from suppressing updates elsewhere. Break condition: If W₀,ᵢⱼ has near-zero entries in critical regions, local gradient suppression may still occur.

### Mechanism 3: Spectral Preservation Across Layers via Block Heterogeneity
BHRA maintains richer singular-value structures and higher effective ranks compared to LoRA and HiRA, particularly in deeper layers. Spectral analysis shows BHRA closely tracks full fine-tuning in singular value counts and energy, whereas LoRA remains flat and HiRA improves only at large r. Block heterogeneity (measured by block Gini) correlates with accuracy, suggesting diverse intra-layer variation drives performance. Break condition: If tasks require highly specialized low-rank updates rather than broad spectral coverage, block heterogeneity may introduce noise without performance gain.

## Foundational Learning

- Concept: Low-Rank Adaptation (LoRA)
  - Why needed here: Baseline PEFT method; understanding its rank bound r and additive update ΔW = BA is essential to contrast with multiplicative and blockwise extensions
  - Quick check question: Given W₀ ∈ ℝ^{m×n} and rank budget r, what is the parameter count and rank upper bound of LoRA?

- Concept: Hadamard Product and Rank Inequality
  - Why needed here: Core mathematical tool enabling high-rank updates via element-wise multiplication; Lemma 3.1 (rank(O₁ ⊙ O₂) ≤ rank(O₁) × rank(O₂)) underpins BHRA's expressivity
  - Quick check question: If rank(W₀) = 64 and rank(BA) = 8, what is the upper bound on rank(W₀ ⊙ BA)?

- Concept: Stable Rank and Effective Rank
  - Why needed here: Paper uses stable rank ∥ΔW∥_F² / ∥ΔW∥₂² as a surrogate for effective rank; essential for interpreting spectral analyses
  - Quick check question: If stable rank ≈ 1, what does that imply about the concentration of adaptation energy?

## Architecture Onboarding

- Component map:
  1. Block Partition: Divides W₀ into b×b blocks (e.g., 8×8 for rb=4, b=8)
  2. Per-Block Low-Rank Factors: Each block has Bij ∈ ℝ^{(m/b)×(r/b)}, Aij ∈ ℝ^{(r/b)×(n/b)}
  3. Capacity Tensor C_BHRA: Block matrix of Cij = BijAij
  4. Block Hadamard Update: ΔW_BHRA = C_BHRA □ W₀ (element-wise product within each block)

- Critical path:
  1. Implement block partitioning and per-block factor initialization
  2. Compute Cij = BijAij per block during forward pass
  3. Apply block Hadamard product with frozen W₀,ᵢⱼ slices
  4. Backpropagate gradients using localized gradient formulas (∇Aij, ∇Bij)
  5. Precompute masks and fold into W₀ before inference

- Design tradeoffs:
  - Block granularity (b) vs. per-block rank (r_b): Fixed budget r_b × b = r. Small r_b with large b → many tiny blocks (underfitting); large r_b with small b → loss of diversity (approaches HiRA)
  - Training overhead: Additional mn/b² activation cache for block masks; mitigated by gradient checkpointing
  - Inference overhead: Zero after mask folding; matches LoRA/HiRA FLOPs ≈ 2r(m+n)T

- Failure signatures:
  - Accuracy plateaus or degrades at extreme b values (rb=1 or rb≥16 per ablation)
  - Singular value counts remain low in deeper layers (insufficient spectral preservation)
  - Block Gini remains low despite blockwise design (suggests initialization or learning rate issues)

- First 3 experiments:
  1. Rank sweep: Fix total budget r=32, vary (r_b, b) pairs {(1,32), (2,16), (4,8), (8,4), (16,2), (32,1)} on commonsense reasoning; expect peak at r_b=4, b=8
  2. Spectral analysis: Compare singular value counts and stable rank across layers for LoRA, HiRA (r=32,128,512), BHRA (r_b=4, b=8), and FFT; verify BHRA tracks FFT
  3. Gradient localization test: Monitor gradient norms per block on a small downstream task; confirm ∇Aij, ∇Bij are modulated by local W₀,ᵢⱼ, not global W₀

## Open Questions the Paper Calls Out
- Can data-driven partitioning strategies outperform the fixed uniform grid (b×b) used in BHRA?
- Does the blockwise Hadamard modulation strategy generalize effectively to multi-modal architectures, such as Vision-Language Models?
- How does BHRA interact with catastrophic forgetting and plasticity in continual learning scenarios?

## Limitations
- Optimal block size (b=8) and per-block rank (r_b=4) may not generalize across model scales or task types
- Causal relationship between spectral richness and downstream performance is not definitively established
- Effectiveness on very large models (>9B parameters) and diverse task families beyond commonsense reasoning requires further validation

## Confidence
- Claim that BHRA outperforms LoRA/HiRA under matched parameter budgets: High confidence
- Claim that BHRA preserves richer spectral structures than baselines: Medium confidence
- Claim that block heterogeneity directly causes performance improvements: Medium confidence

## Next Checks
1. Cross-domain generalization test: Apply BHRA to non-reasoning tasks (e.g., summarization, code generation, or multilingual translation) with Llama-3.2 3B/8B and evaluate whether the b=8, r_b=4 configuration remains optimal
2. Ablation on gradient dynamics: Compare gradient norm distributions and update patterns across LoRA, HiRA, and BHRA during training on a controlled subset (e.g., 10K examples from Commonsense170K)
3. Scaling law analysis: Systematically vary model size (1B→3B→7B→13B) and task complexity to identify whether BHRA's benefits scale linearly, sublinearly, or superlinearly with model capacity