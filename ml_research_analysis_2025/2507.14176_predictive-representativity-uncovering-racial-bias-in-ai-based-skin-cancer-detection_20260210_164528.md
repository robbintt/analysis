---
ver: rpa2
title: 'Predictive Representativity: Uncovering Racial Bias in AI-based Skin Cancer
  Detection'
arxiv_id: '2507.14176'
source_url: https://arxiv.org/abs/2507.14176
tags:
- skin
- representativity
- fairness
- data
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses racial bias in AI-based skin cancer detection
  by introducing Predictive Representativity (PR), a framework that evaluates fairness
  through outcome-level alignment rather than input dataset composition. Using HAM10000
  and an independent Colombian dataset (BOSQUE Test set), the authors trained five
  CNN models and found substantial disparities in precision, recall, and F1-score
  for darker skin tones, despite proportional sampling.
---

# Predictive Representativity: Uncovering Racial Bias in AI-based Skin Cancer Detection

## Quick Facts
- arXiv ID: 2507.14176
- Source URL: https://arxiv.org/abs/2507.14176
- Reference count: 8
- One-line primary result: AI models show significant racial bias in skin cancer detection despite proportional training data, with PR metric revealing performance gaps for darker skin tones.

## Executive Summary
This study addresses racial bias in AI-based skin cancer detection by introducing Predictive Representativity (PR), a framework that evaluates fairness through outcome-level alignment rather than input dataset composition. Using HAM10000 and an independent Colombian dataset (BOSQUE Test set), the authors trained five CNN models and found substantial disparities in precision, recall, and F1-score for darker skin tones, despite proportional sampling. PR values revealed statistically significant performance gaps, demonstrating that standard metrics mask subgroup inequities. The work highlights the need for post-hoc fairness auditing, transparent dataset documentation, and context-sensitive validation, contributing a scalable tool for diagnosing structural biases in AI systems and advancing discussions on equity in healthcare AI.

## Method Summary
The authors trained five CNN architectures (ResNet-50, DenseNet-121, MobileNet-V2, EfficientNet-V2-B0, VGG-16) on HAM10000 (10,015 images) with binary classification (benign vs. malignant). They evaluated models on BOSQUE Test set (n=165 dermoscopic images from Colombia) with Fitzpatrick phototype labels. The Predictive Representativity metric measures fairness by calculating the difference in divergence between predicted and true distributions for subgroups versus the whole population. They used standard metrics (precision, F1-score) as proxy metrics for PR calculation, and applied oversampling and class weighting to handle HAM10000's class imbalance.

## Key Results
- CNN models consistently underperformed for individuals with darker skin despite proportional sampling in training data
- PR values revealed statistically significant performance gaps masked by aggregate metrics
- External Transportability Criterion ($|PR(P', S', A)| \le \epsilon$) identified fairness drift when models transferred from HAM10000 to BOSQUE
- Light skin subgroup (n=107) achieved higher precision while Dark skin subgroup (n=58) showed marked performance degradation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Proportional sampling of demographic subgroups in a training dataset (Input Representativity) does not guarantee equitable model performance (Predictive Representativity) for those subgroups.
- **Mechanism:** Machine learning models rely on conditional predictions $P(Y|X)$. If the absolute sample size of a subgroup (e.g., darker skin tones) is low, even if proportionally accurate to the global population, the model fails to converge on robust decision boundaries for that specific covariate space, resulting in high variance or bias for the underrepresented subgroup masked by high aggregate accuracy.
- **Core assumption:** Performance drop is driven by distributional properties of data and model capacity rather than purely algorithmic architecture flaws.
- **Evidence anchors:** Classifiers consistently underperforming for individuals with darker skin —despite proportional sampling in the source data; representativity based solely on input population proportions is inadequate for contexts involving conditional predictions.
- **Break condition:** If training dataset includes sufficient absolute samples of minority subgroup to effectively learn conditional distribution $P(Y|X)$ for that group.

### Mechanism 2
- **Claim:** The Predictive Representativity (PR) metric functions as a diagnostic divergence measure that reveals hidden performance gaps that aggregate metrics conceal.
- **Mechanism:** PR operationalizes fairness by calculating difference in divergence between true distribution and predicted distribution for specific subgroup versus whole population. A positive value indicates model is "underperforming" specifically in that subgroup relative to average.
- **Core assumption:** Divergence metric $D$ (e.g., Jensen-Shannon or Total Variation) accurately captures "distance" between prediction and truth in way that aligns with clinical or fairness relevance.
- **Evidence anchors:** PR values revealed statistically significant performance gaps, demonstrating that standard metrics mask subgroup inequities; PR uses statistical divergence to operationalize fairness.
- **Break condition:** If global model performance is exceptionally poor (random guessing), divergence term for whole population becomes large, potentially masking relative gap with subgroup.

### Mechanism 3
- **Claim:** Fairness generalization (External Transportability) requires independent threshold criterion, as models trained on one population may violate fairness constraints when applied to different population.
- **Mechanism:** Authors propose "External Transportability Criterion" ($|PR(P', S', A)| \le \epsilon$). This formalizes requirement that fairness gap must remain within tolerance $\epsilon$ when model crosses domains (e.g., from Austria to Colombia). Detects "Fairness Drift" caused by covariate shifts in target population.
- **Core assumption:** Target population has sufficiently different characteristics to stress-test model, and ground truth labels are available in target to compute metric.
- **Evidence anchors:** Evaluates fairness through outcome-level alignment using HAM10000 and independent Colombian dataset; introduces "External Transportability Criterion" and illustrates transfer from Source $P$ to Target $P'$.
- **Break condition:** If Source and Target populations are effectively i.i.d., mechanism of transportability failure will not trigger.

## Foundational Learning

- **Concept: Statistical Divergence (KL/JS Divergence)**
  - **Why needed here:** Core PR metric relies on measuring "distance" between predicted probability distribution and true label distribution. Cannot understand PR formula without understanding $D(\cdot || \cdot)$.
  - **Quick check question:** If model predicts [0.9, 0.1] and truth is [1.0, 0.0], does divergence measure capture this error differently than simple accuracy?

- **Concept: Hidden Stratification / Subgroup Shift**
  - **Why needed here:** Paper demonstrates high global accuracy masks failure in minority subgroups. Understanding that datasets contain latent subgroups which may shift between train and test is motivation for this work.
  - **Quick check question:** Can model have 90% accuracy on dataset while having 0% accuracy on specific subgroup within it?

- **Concept: Covariate Shift vs. Concept Shift**
  - **Why needed here:** Paper discusses "transportability." Must distinguish between input features changing (Covariate Shift, e.g., more dark skin in test set) and relationship between features and labels changing (Concept Shift).
  - **Quick check question:** In BOSQUE test set, is distribution of skin tones different from HAM10000, and does definition of "malignant" remain constant?

## Architecture Onboarding

- **Component map:** HAM10000 (Source, metadata-scarce) -> Model -> BOSQUE (Target, metadata-rich) -> PR Auditor -> Threshold Gate
- **Critical path:**
  1. Train: Fit CNN on HAM10000 (Source)
  2. Inference: Run inference on BOSQUE (Target). Ensure subgroup metadata (Fitzpatrick type) is preserved
  3. Compute PR: Calculate divergence for global population vs. specific subgroups (Light vs. Dark)
  4. Audit: Check if $|PR_{Dark}| > \epsilon$ to flag transportability failure

- **Design tradeoffs:**
  - PR as Objective vs. Diagnostic: Authors warn against using PR as training loss function (optimizing for it directly) as this may lead to overfitting on idiosyncrasies of subgroup. Designed as diagnostic tool.
  - Proxy Metrics: While PR is theoretically robust, paper operationalizes it using standard metrics (Precision, F1) for interpretability. Trade theoretical purity for ease of debugging.

- **Failure signatures:**
  - "Fairness Illusion": High global Precision (e.g., 0.8) but negative PR for "Dark Skin" subgroup (indicating precision $\ll$ 0.8)
  - High Variance in PR: If subgroup sample size ($n=58$ in BOSQUE) is too small, PR estimate may be noisy. Use bootstrap confidence intervals to verify stability.

- **First 3 experiments:**
  1. Baseline Transportability: Train ResNet50 on HAM10000. Evaluate on BOSQUE. Report Global Accuracy vs. PR for Light/Dark subgroups. Confirm "precision drop" finding.
  2. Metric Sensitivity: Calculate PR using different divergence metrics (KL vs. Jensen-Shannon) or proxy metrics (F1-score). Check if identified bias is consistent across metric definitions.
  3. Threshold Tuning: Define clinical risk tolerance $\epsilon$. Determine how many models in architecture zoo violate External Transportability Criterion for dark skin at that $\epsilon$.

## Open Questions the Paper Calls Out

- **Open Question 1:** What constitutes acceptable stakeholder tolerance ($\epsilon$) for External Transportability Criterion, and how can this threshold be standardized without compromising clinical safety?
  - **Basis in paper:** Authors ask "What degree of performance trade-off is acceptable to ensure that no group is left behind?" and note tolerance $\epsilon$ must be defined by stakeholders, acknowledging achieving low disparity often sacrifices overall accuracy.
  - **Why unresolved:** Paper defines mathematical criterion $|PR(P', S', A)| \leq \epsilon$ but does not establish specific values for $\epsilon$, leaving it as normative or regulatory decision rather than technical one.
  - **What evidence would resolve it:** Multi-stakeholder consensus study defining clinically safe error margins for specific demographic subgroups in dermatology AI.

- **Open Question 2:** How can causal inference frameworks be integrated into Predictive Representativity to predict fairness transportability based on structural data-generating processes rather than just statistical divergence?
  - **Basis in paper:** Authors state "future research should draw more explicitly on causal inference frameworks... to enhance transportability assessment" and bridge gap between empirical performance and real-world reliability.
  - **Why unresolved:** Current PR framework relies on statistical divergence and distributional alignment but does not model underlying causal mechanisms that might ensure fairness persists under covariate and concept shifts.
  - **What evidence would resolve it:** Theoretical extension of PR formula incorporating causal diagrams (DAGs) to estimate transportability in domains with unobserved confounders.

- **Open Question 3:** Can Predictive Representativity be operationalized as direct optimization objective (loss function) during model training without inducing overfitting to specific subgroups?
  - **Basis in paper:** Authors explicitly warn that "optimizing model solely to minimize PR... could induce overfitting" and recommend it primarily as post-hoc auditing tool, leaving potential for in-processing fairness methods unexplored.
  - **Why unresolved:** Paper demonstrates PR's utility as diagnostic metric but does not test whether models can be trained to minimize PR directly while maintaining robustness and calibration.
  - **What evidence would resolve it:** Empirical study comparing models trained with PR-regularized loss function against standard baselines to measure generalization gaps on unseen external datasets.

## Limitations
- Study relies on relatively small independent test set (BOSQUE, n=165) for External Transportability analysis, which may limit statistical power for detecting bias in Dark skin subgroup (n=58)
- HAM10000's demographic metadata is sparse, forcing use of external labeling for phototype assessment, which introduces potential labeling inconsistencies
- PR metric uses proxy metrics (Precision, F1) for practical implementation, potentially diluting its sensitivity to true distributional divergence

## Confidence
- **High Confidence:** Core finding that proportional sampling ≠ equitable performance is well-supported by experimental results across multiple architectures
- **Medium Confidence:** PR metric's mathematical formulation is sound, but practical sensitivity to bias depends on choice of divergence measure and subgroup sample size
- **Medium Confidence:** External Transportability Criterion is conceptually valuable but requires validation across more diverse datasets and skin tone distributions

## Next Checks
1. **Bootstrap Stability:** Calculate confidence intervals for PR values using bootstrap resampling to assess statistical stability of bias detection in small subgroups
2. **Metric Sensitivity Analysis:** Recompute PR using alternative divergence measures (e.g., KL vs. Jensen-Shannon) and compare results to ensure identified bias is robust to metric choice
3. **Cross-Dataset Generalizability:** Apply PR framework to different skin lesion dataset (e.g., ISIC 2019) with varying skin tone distributions to validate transportability criterion's broader applicability