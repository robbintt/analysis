---
ver: rpa2
title: Source Attribution in Retrieval-Augmented Generation
arxiv_id: '2507.04480'
source_url: https://arxiv.org/abs/2507.04480
tags:
- attribution
- scores
- shapley
- document
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the challenge of attributing a large language\
  \ model\u2019s output in retrieval-augmented generation (RAG) systems back to specific\
  \ retrieved documents. The core method idea involves applying Shapley value-based\
  \ attribution to measure the influence of each document by evaluating the log-likelihood\
  \ of the target response when conditioned on subsets of documents."
---

# Source Attribution in Retrieval-Augmented Generation

## Quick Facts
- arXiv ID: 2507.04480
- Source URL: https://arxiv.org/abs/2507.04480
- Reference count: 36
- This paper introduces Shapley value-based attribution methods to identify which retrieved documents influence an LLM's response in RAG systems, demonstrating that Kernel SHAP and ContextCite achieve high correlation to exact Shapley values while effectively identifying top-k impactful documents.

## Executive Summary
This paper addresses the challenge of attributing a large language model's output in retrieval-augmented generation (RAG) systems back to specific retrieved documents. The core method applies Shapley value-based attribution to measure document influence by evaluating the log-likelihood of the target response when conditioned on subsets of documents. Experiments on BioASQ and Natural Questions datasets demonstrate that Kernel SHAP and ContextCite achieve the highest correlation to exact Shapley values (Pearson > 0.95, Kendall's Tau > 0.7) with as few as 100 samples, significantly outperforming alternatives. These methods also effectively identify the top-k most impactful documents with precision up to 0.82. However, complex inter-document relationships such as redundancy and synergy pose challenges, with all methods underestimating the contribution of documents needed for answer synthesis.

## Method Summary
The authors apply Shapley value-based attribution to measure document influence in RAG systems using log-likelihood utility. For each document subset S, they compute v(S) as the sum of log-probabilities of target response tokens via teacher-forcing. They compare exact Shapley values (requiring 2^n evaluations) with approximations like Kernel SHAP and ContextCite that use 32-100 sampled coalitions. Experiments use LLaMA-3.2-8B-Instruct, Mistral-7B-Instruct, and Qwen-3B-Instruct models on BioASQ and Natural Questions datasets with 100 queries each, 10 documents per query. The methods are evaluated based on correlation to exact Shapley values and precision@k for identifying top-k impactful documents.

## Key Results
- Kernel SHAP and ContextCite achieve Pearson > 0.95 and Kendall's Tau > 0.7 correlation to exact Shapley values with 100 samples
- These methods identify top-k most impactful documents with precision up to 0.82
- All methods systematically underestimate contributions in synergy scenarios where multiple documents are needed for answer synthesis
- Position bias affects attribution, with earlier duplicate documents receiving higher scores regardless of content

## Why This Works (Mechanism)

### Mechanism 1: Log-Likelihood Utility as Attribution Signal
- Conditional log-likelihood of the target response provides a tractable utility function for measuring document influence
- For each document subset S, compute v(S) = Σ log P(token_t | token_{<t}, Q, S) using teacher-forcing
- Core assumption: The target response is a stable reference; utility differences reflect document contributions rather than generation variability
- Break condition: If the LLM changes its response pattern significantly across contexts, log-likelihood becomes an unreliable proxy for contribution

### Mechanism 2: Surrogate Linear Models Approximate Shapley Values Efficiently
- Kernel SHAP and ContextCite approximate exact Shapley values with high fidelity using ~100 utility evaluations, reducing exponential cost
- Sample document coalitions, evaluate utilities, fit weighted linear regression (Kernel SHAP) or simpler linear model (ContextCite)
- Core assumption: The true utility function can be locally approximated by a linear model over coalition indicators
- Break condition: When inter-document interactions are strongly non-linear (e.g., synergy requiring multi-hop reasoning), linear surrogate models systematically underestimate contributions

### Mechanism 3: Attribution Precision Identifies High-Impact Documents
- Utility-based methods can identify the top-k most impactful documents with precision up to 0.82
- Rank documents by attribution scores; compare top-k against exhaustive search baseline (documents whose removal maximizes utility drop)
- Core assumption: High utility drop upon removal implies genuine causal influence on the response
- Break condition: In synergy scenarios, documents essential for synthesis receive disproportionately low scores, breaking the precision claim

## Foundational Learning

- Concept: **Shapley Values (Cooperative Game Theory)**
  - Why needed here: Core attribution method; requires understanding marginal contribution averaging over all coalitions
  - Quick check question: Given players {A, B} with utilities v({})=0, v({A})=1, v({B})=2, v({A,B})=4, compute Shapley values for A and B

- Concept: **Teacher-Forcing in LLMs**
  - Why needed here: Utility computation conditions on ground-truth previous tokens, not model predictions; critical for stable log-likelihood estimation
  - Quick check question: Why does teacher-forcing produce different log-likelihoods than autoregressive sampling with the same prefix?

- Concept: **Inter-Document Relationships in RAG (Redundancy, Complementarity, Synergy)**
  - Why needed here: These relationships violate Shapley axioms (especially additivity) and cause systematic attribution errors
  - Quick check question: In a synergy scenario where Document A identifies an entity and Document B provides its attribute, why does LOO underestimate A's contribution?

## Architecture Onboarding

- Component map:
  - Input Layer: Query Q, retrieved document set D (typically 10 docs), pre-generated target response R_target
  - Utility Evaluator: LLM forward pass with teacher-forcing; returns log-likelihood v(S) for any subset S ⊆ D
  - Attribution Engine: Implements Shapley, Kernel SHAP, ContextCite, TMC-Shapley, Beta-Shapley, LOO; samples coalitions and computes scores
  - Output: Attribution vector Φ = {φ_1, ..., φ_n} for documents in D

- Critical path:
  1. Generate R_target = LLM(Q, D_full) once
  2. Sample coalitions (32–100 subsets depending on method)
  3. For each coalition S, compute v(S) via teacher-forced log-likelihood
  4. Fit surrogate model (Kernel SHAP, ContextCite) or aggregate marginal contributions (Shapley variants)
  5. Return ranked documents by attribution scores

- Design tradeoffs:
  - Exact Shapley vs. Approximations: Exact requires 2^n evaluations (infeasible for n>10); approximations need 32–100 samples but introduce estimation error
  - Kernel SHAP vs. ContextCite: Kernel SHAP uses SHAP kernel weighting; ContextCite uses simpler linear model. Both achieve similar performance; Kernel SHAP slightly higher correlation
  - LOO: Cheapest (n evaluations) but fails to capture interactions; systematically underperforms in synergy scenarios

- Failure signatures:
  - Redundancy: Position bias—earlier duplicate receives higher attribution regardless of content equivalence
  - Synergy: "Inference document" (enabling multi-hop reasoning) receives near-zero attribution while "answer document" gets all credit
  - Complementarity: Methods identify both documents but cannot distinguish complementary from redundant pairs using scores alone

- First 3 experiments:
  1. Baseline Correlation Check: Run Kernel SHAP and ContextCite on 10 queries with n=10 docs each; measure Pearson/Kendall correlation to exact Shapley (2^10=1024 evaluations feasible). Target: ρ>0.90 with 64–100 samples
  2. Top-k Precision Validation: For k∈{2,3,4,5}, exhaustively identify S*_k (max utility drop removal); compare against method-predicted top-k. Target: Precision@k ≥0.70
  3. Synergy Scenario Stress Test: Construct synthetic multi-hop queries (as in paper's Figure 4); verify whether "inference document" attribution <20% of "answer document" attribution. If so, document this limitation explicitly

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can attribution methods be developed to accurately account for complex inter-document relationships, such as synergy and redundancy, rather than treating documents independently?
- Basis in paper: The conclusion states that "advanced techniques that account for deeper inter-document relationships are needed to improve attribution quality."
- Why unresolved: The experiments reveal that current methods significantly underestimate the contribution of documents required for answer synthesis (synergy) and exhibit position bias with duplicate documents (redundancy)
- What evidence would resolve it: New attribution algorithms that explicitly model document interactions or interaction terms, demonstrating higher precision in multi-hop reasoning scenarios without position bias

### Open Question 2
- Question: Is it possible to distinguish between redundant and complementary documents relying solely on utility-based attribution scores?
- Basis in paper: Section 4.3 notes that "when relying solely on the utility values of the documents, it becomes impossible to distinguish between scenarios involving complementary documents and those involving duplicate documents."
- Why unresolved: In both scenarios, the utility scores appear similar (one high, one marginally lower), making the qualitative nature of the relationship opaque to the current scoring mechanism
- What evidence would resolve it: A method that successfully assigns distinct interaction profiles or labels to document pairs, validated against a dataset with known complementary and redundant structures

### Open Question 3
- Question: Can Shapley-based attribution provide meaningful insights in RAG systems despite the inherent violation of theoretical axioms like additivity?
- Basis in paper: The introduction asks to "evaluate whether Shapley-based attribution can still provide meaningful insights in the RAG scenario" given that the log-likelihood utility is non-additive
- Why unresolved: While the paper shows high correlation with exact Shapley values, the consistent failure to identify necessary synthesis documents suggests the violation of axioms limits practical explainability
- What evidence would resolve it: Theoretical bounds on the error introduced by axiom violations in RAG, or empirical results showing correct identification of "necessary" documents in synthetic synergy tasks

## Limitations

- Inter-document relationship modeling remains a fundamental challenge, with current methods systematically underestimating synergy document contributions
- Attribution score variance across sampling runs is not characterized, making it unclear whether performance differences are statistically significant
- The synthetic dataset construction methodology for testing redundancy/complementarity/synergy is not detailed, limiting ecological validity assessment

## Confidence

- **High Confidence:** The core claim that Kernel SHAP and ContextCite achieve Pearson > 0.95 correlation to exact Shapley values with 100 samples
- **Medium Confidence:** The precision@k results (up to 0.82) for identifying top-k impactful documents
- **Low Confidence:** The systematic underestimation of synergy document contributions due to lack of quantitative error bounds

## Next Checks

1. **Statistical Significance Analysis:** Replicate correlation experiments with 10-fold cross-validation, reporting mean and standard deviation for Pearson/Kendall metrics across runs. Verify that Kernel SHAP's slight performance edge over ContextCite is statistically significant (p < 0.05)

2. **Synergy Attribution Error Quantification:** Construct 5-10 diverse multi-hop reasoning queries where document sets require specific combinations (A+B needed for answer, A alone insufficient). Measure average attribution score gap between "answer document" and "inference document" across all methods. Report mean absolute error versus ground-truth equal attribution

3. **Position Bias Elimination Test:** For 20 queries, randomly permute document order within each set and recompute attribution scores. Calculate rank correlation between original and permuted orderings. If significant correlation exists (ρ > 0.3), document position bias as a systematic error requiring correction