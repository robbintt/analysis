---
ver: rpa2
title: 'PEARL: Towards Permutation-Resilient LLMs'
arxiv_id: '2502.14628'
source_url: https://arxiv.org/abs/2502.14628
tags:
- performance
- pearl
- learning
- permutations
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the vulnerability of large language models (LLMs)
  to in-context learning (ICL) performance drops when the order of provided demonstrations
  is permuted. It demonstrates that this issue can be exploited to design nearly undetectable
  attacks, reducing LLM performance by up to 80%.
---

# PEARL: Towards Permutation-Resilient LLMs

## Quick Facts
- arXiv ID: 2502.14628
- Source URL: https://arxiv.org/abs/2502.14628
- Reference count: 40
- One-line primary result: PEARL improves worst-case ICL performance by up to 40% and average performance by up to 9.8% through adversarial training against permutation attacks.

## Executive Summary
Large language models are vulnerable to in-context learning (ICL) performance drops when the order of provided demonstrations is permuted. This paper demonstrates that this issue can be exploited to design nearly undetectable attacks, reducing LLM performance by up to 80%. To address this, the authors propose PEARL (Permutation-resilient learning), a novel framework based on distributionally robust optimization (DRO). PEARL employs a permutation-proposal network (P-Net) that identifies challenging permutations using optimal transport and an entropy-constrained Sinkhorn algorithm. Through adversarial training, the P-Net and LLM iteratively improve, enhancing the model's robustness. Experiments on synthetic tasks and real-world instruction tuning show PEARL effectively mitigates permutation attacks and boosts average performance by up to 9.8% while improving worst-case performance by up to 40%, even when scaling to many-shot and long-context scenarios.

## Method Summary
PEARL is a framework that makes LLMs robust to permutation attacks on ICL demonstrations via distributionally robust optimization (DRO). The core approach is adversarial training between a target LLM and a permutation-proposal network (P-Net). The P-Net, implemented as a FLAN-large encoder, generates adversarial permutations by treating permutation generation as an optimal transport problem solved with an entropy-constrained Sinkhorn algorithm and Gumbel sampling. The LLM is fine-tuned using LoRA (rank=8, alpha=32) to minimize loss under these adversarially selected permutations. Training alternates between updating the P-Net to increase loss and the LLM to decrease it, with entropy regularization (β=1.0) encouraging diverse permutations. The framework is evaluated on Super-Natural Instructions tasks with 2-6 shots, reporting average and worst-case ROUGE-L across all possible permutations.

## Key Results
- PEARL reduces attack success rate (ASR) to 0% on 2-shot tasks and ≤20% on 6-shot tasks, compared to 35-80% for baselines
- Worst-case performance improves by up to 40% while average performance improves by up to 9.8% on held-out instruction tuning tasks
- PEARL achieves significant gains in many-shot (5-6 shots) and long-context scenarios where baselines fail
- The method outperforms standard ERM, ERM+DS (differentiable sorting), and ERM+IM (importance mixing) baselines

## Why This Works (Mechanism)

### Mechanism 1: Distributionally Robust Optimization (DRO) for Permutation Ambiguity
- Claim: Optimizing for the worst-case permutation during training enhances the model's inherent robustness to any permutation encountered during inference.
- Mechanism: PEARL treats all possible permutations as an "ambiguity set" and uses DRO to minimize loss under the most challenging permutation, rather than standard ERM which optimizes for average performance.
- Core assumption: ERM-trained models overfit to limited permutations seen during training and fail to generalize to unseen valid permutations.
- Evidence anchors: Abstract states PEARL "optimizes model performance against the worst-case input permutation" using DRO.

### Mechanism 2: Adversarial Training with a Permutation-Proposal Network (P-Net)
- Claim: An adversarial minimax game between a target LLM and a permutation-generating network effectively identifies and trains against the most harmful permutations.
- Mechanism: P-Net learns to generate permutations that maximize the LLM's loss, while the LLM trains to minimize loss on these adversarially selected examples.
- Core assumption: A neural network can efficiently approximate finding worst-case permutations from the exponentially large search space.
- Evidence anchors: Abstract states P-Net "generates the most challenging permutations" through minimax optimization.

### Mechanism 3: Optimal Transport for Permutation Generation
- Claim: Modeling permutation generation as an optimal transport problem with Sinkhorn algorithm allows efficient sampling of challenging permutations.
- Mechanism: P-Net uses feature extractor to model demonstration relationships, then applies entropy-constrained Sinkhorn to transform this into a doubly stochastic matrix representing permutation probabilities.
- Core assumption: Optimal transport framework is appropriate for mapping demonstration relationships into meaningful adversarial permutations.
- Evidence anchors: Abstract states P-Net "treats it as an optimal transport problem, which is solved using an entropy-constrained Sinkhorn algorithm."

## Foundational Learning

### Concept: Distributionally Robust Optimization (DRO)
- Why needed here: To understand the core training paradigm shift from optimizing for average performance (ERM) to optimizing for worst-case performance across permutations.
- Quick check question: How does the objective function in PEARL fundamentally differ from a standard supervised fine-tuning loss?

### Concept: Optimal Transport and the Sinkhorn Algorithm
- Why needed here: To grasp how P-Net generates permutations through iterative matrix normalization to create valid probability distributions over permutations.
- Quick check question: What property must the output matrix S(R) from the Sinkhorn operator have, and why is that property essential for representing a distribution over permutations?

### Concept: Minimax Games and Adversarial Training
- Why needed here: To understand the dynamic competitive game where one network's gain is the other's loss, driving iterative improvement.
- Quick check question: In PEARL, what is the goal of the P-Net and what is the goal of the LLM within each iteration of the adversarial optimization loop?

## Architecture Onboarding

### Component map:
Target LLM (θ) <-minimizes loss-> Permutation-Proposal Network (P-Net, ϕ) <-generates permutations->
- P-Net Feature Extractor: Encodes demonstrations into representations
- Cross-Demonstration Layer: Models pairwise relationships to create relationship matrix R
- Sinkhorn Operator: Transforms R into doubly stochastic matrix representing permutation distribution
- Gumbel Sampler: Samples concrete permutation matrix Π from distribution

### Critical path:
1. Input batch of samples (prompt p, input x, output y) to P-Net
2. P-Net's feature extractor and cross-demonstration layer compute relationship matrix R
3. Sinkhorn operator transforms R into doubly stochastic matrix
4. Gumbel sampler draws permutation matrix Π
5. Apply permutation Π to demonstrations in prompt p, creating permuted prompt Π·p
6. Feed (Π·p, x) to target LLM and compute loss
7. Update both networks: increase P-Net parameters (to increase loss) and decrease LLM parameters (to decrease loss)

### Design tradeoffs:
- LLM vs. P-Net Size: P-Net is ~1/20th the size of LLM; larger P-Net could find more devastating permutations but slows training
- Entropy Constraint (β): Critical hyperparameter; too low produces trivial permutations, too high over-constrains finding difficult ones
- Sinkhorn Iterations/Temperature: Impact convergence speed; method is surprisingly robust to these hyperparameters

### Failure signatures:
- Training Collapse: Loss becomes unstable or diverges; check if learning rates are balanced
- Trivial P-Net: P-Net might learn fixed simple permutation; monitor entropy H(Π); increase β or reduce P-Net LR if needed
- No Robustness Gain: If P-Net fails to find genuinely difficult permutations; check P-Net's gradient norm should be active

### First 3 experiments:
1. Reproduce Permutation Attack Vulnerability: Verify core problem by evaluating pre-trained LLM on random permutations, original order, and worst-case permutation found via exhaustive/greedy search on small validation set.
2. Baseline PEARL Training Loop: Implement full adversarial training loop with small LLM and P-Net on synthetic task (linear function learning), monitoring losses for both networks.
3. Ablation on Entropy Constraint: Train with different β values (0.3, 1.0, 3.0) and evaluate average and worst-case performance on held-out set.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can PEARL be effectively extended to handle set-structured inputs in multimodal domains, such as images or video sequences?
- Basis in paper: The conclusion states PEARL theoretically provides a general framework for handling set-structured inputs like "multiple documents, images, or videos."
- Why unresolved: Current work restricts validation to synthetic function learning and text-based instruction tuning tasks.
- What evidence would resolve it: Experiments applying PEARL to multimodal models or video understanding tasks demonstrating robustness against frame reordering.

### Open Question 2
- Question: What is the computational overhead of the minimax optimization process compared to standard empirical risk minimization (ERM)?
- Basis in paper: Framework necessitates alternating optimization between LLM and P-Net using adversarial algorithm, inherently more complex than standard training.
- Why unresolved: Paper focuses on model performance but doesn't report wall-clock training times or FLOPs comparisons.
- What evidence would resolve it: Detailed efficiency analysis comparing training duration and resource consumption against ERM baselines.

### Open Question 3
- Question: Does PEARL maintain robustness against hybrid attacks that combine demonstration permutation with adversarial content perturbations?
- Basis in paper: Defined threat model is restricted to "simply permuting the demonstrations" without altering semantic content.
- Why unresolved: Real-world adversaries might combine ordering attacks with token-level perturbations, not covered by current "natural attack" definition.
- What evidence would resolve it: Evaluation of PEARL-trained models against white-box attacks optimizing both input tokens and their ordering simultaneously.

## Limitations

- Worst-Case Permutation Feasibility: P-Net's ability to discover truly worst-case permutations from exponential search space remains heuristic; optimal transport formulation may not guarantee finding global worst-case.
- Generalization to Real-World Tasks: Experiments focus on synthetic tasks and curated Super-Natural Instructions subset; framework's performance on more diverse, complex real-world tasks with longer contexts is not fully characterized.
- Computational Overhead: Adversarial training with P-Net doubles computational cost; paper doesn't provide detailed cost-benefit analysis for large-scale deployment.

## Confidence

- **High Confidence**: Successfully demonstrates ICL sensitivity to demonstration order and that PEARL improves average and worst-case performance on tested tasks with technically sound DRO mechanism.
- **Medium Confidence**: P-Net's consistent generation of meaningful adversarial permutations and framework's robustness to hyperparameters are demonstrated but could be more extensively validated across wider settings.
- **Low Confidence**: Claims about PEARL's effectiveness in "many-shot and long-context scenarios" are not fully supported by experimental evidence in the paper.

## Next Checks

1. **Robustness to P-Net Architecture**: Validate PEARL's performance using different P-Net architectures (smaller encoders, non-encoder-only models) to ensure gains aren't specific to FLAN-large encoder used.
2. **Generalization to Unseen Tasks**: Test PEARL on broader set of real-world tasks beyond Super-Natural Instructions benchmark, particularly those with longer contexts and more complex demonstration structures.
3. **Analysis of P-Net's Discovered Permutations**: Conduct qualitative analysis of permutations generated by P-Net during training to verify they are semantically meaningful and not trivial or colluding with LLM, ensuring adversarial training genuinely improves robustness.