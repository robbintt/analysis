---
ver: rpa2
title: Toward Scalable and Valid Conditional Independence Testing with Spectral Representations
arxiv_id: '2512.19510'
source_url: https://arxiv.org/abs/2512.19510
tags:
- conditional
- learning
- independence
- testing
- spectral
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Toward Scalable and Valid Conditional Independence Testing with Spectral Representations

## Quick Facts
- arXiv ID: 2512.19510
- Source URL: https://arxiv.org/abs/2512.19510
- Reference count: 40
- Authors: Alek Frohlich, Vladimir Kostic, Karim Lounici, Daniel Perazzo, Massimiliano Pontil

## Executive Summary
This paper proposes a neural conditional independence test (SpecCIT) that learns spectral representations of the partial covariance operator. The method addresses the scalability and validity challenges of existing CI tests by using a bi-level contrastive loss to approximate the leading singular functions. Under certain representation quality conditions, the test statistic converges to a chi-squared distribution, enabling threshold-based decisions without permutation.

## Method Summary
SpecCIT learns d-dimensional spectral features uθ(X), vθ(Y,Z), and wθ(Z) via bi-level optimization. The outer loss trains uθ and vθ to capture the singular structure of ΣX¨Y·Z, while the inner loss trains wθ to handle the conditioning variable Z. After training, features are whitened and the test statistic bTn = n‖ĈbUθbVθ − ĈbUθĉWθĈĉWθbVθ‖²F is computed. Under H₀ and when representation error Em → 0, bTn converges to χ²(d²), providing asymptotic validity without permutation.

## Key Results
- Test statistic converges to χ²(d²) under H₀ when representation error vanishes
- Power bounds show separation threshold depends on signal strength minus representation error
- Type I error controlled empirically on post-nonlinear synthetic data across varying conditioning dimensions
- Empirical power exceeds KCIT, RCIT, and GCIT on synthetic benchmarks

## Why This Works (Mechanism)

### Mechanism 1
Learning spectral features of the partial covariance operator enables scalable CI testing with asymptotic validity. Neural networks uθ, vθ, wθ approximate the leading d singular functions of ΣX¨Y·Z via a bi-level contrastive loss. The outer loss learns uθ, vθ for X and (Y,Z); the inner loss learns wθ for Z to handle the operator composition ΣXZΣZ¨Y. This factorization avoids direct estimation of the partial covariance, which lacks an unbiased empirical estimator. Core assumption: ΣX¨Y·Z is compact (Assumption 3.1), and learned features satisfy sub-Gaussianity (Assumption 4.1) with orthonormality error Em → 0 as m → ∞. Break condition: If Em does not converge (e.g., insufficient training samples, inadequate network capacity), the test may not control Type I error.

### Mechanism 2
The test statistic bTn converges to χ²(d²) under H₀, enabling threshold-based decisions without permutation or bootstrap. After whitening, the statistic bTn = n‖ĈbUθbVθ − ĈbUθĉWθĈĉWθbVθ‖²F captures the empirical partial covariance in learned feature space. Under H₀ and conditional independence, the diagonal terms dominate with covariance structure Id₂, yielding a sum of d² independent χ²(1) variables via the Lindeberg-Feller CLT. Core assumption: d = o(√n) and √d·Em → 0 as m,n → ∞ (Lemmas B.3–B.5). Sub-Gaussian features ensure Lindeberg condition holds. Break condition: If n is small relative to d², or if whitening fails (covariance matrices near singular), the χ² approximation degrades.

### Mechanism 3
Power against local alternatives scales with signal strength ϵ²n = ‖ΣX¨Y·Z‖²HS minus representation and estimation errors. Under H₁, the test statistic has non-centrality from the truncated SVD signal Σdⱼ₌₁ σ²ⱼ. Power ≥ 1−δ requires ϵ²n ≳ dE²m + (d² + log δ⁻¹)/n (Theorem 4.2). Larger d captures more signal but increases estimation variance. Core assumption: Spectral decay of ΣX¨Y·Z is sufficiently fast that truncated rank d captures ≥ 50% of HS norm. Singular value gap σd > σd+1 ensures identifiable leading directions. Break condition: If the partial covariance operator has slow spectral decay (requires large d) or if Em is large (poor representation learning), power may not exceed the Type I error rate.

## Foundational Learning

- **Covariance and partial covariance operators**
  - Why needed here: The entire method recasts CI testing as ‖ΣX¨Y·Z‖HS = 0 vs. > 0. Understanding how ΣAB·C = ΣAB − ΣACΣCB generalizes partial covariance to Hilbert spaces is essential.
  - Quick check question: Given ΣXY, ΣXZ, ΣZY, write the partial covariance operator. What does ‖ΣXY·Z‖HS = 0 imply?

- **Singular value decomposition of compact operators**
  - Why needed here: The algorithm learns the truncated SVD [ΣX¨Y·Z]d = Σdᵢ₌₁ σᵢuᵢ⊗vᵢ. The Eckart-Young theorem justifies this as the optimal rank-d approximation.
  - Quick check question: How does the Eckart-Young theorem extend to Hilbert-Schmidt operators? What is the relationship between singular values and Hilbert-Schmidt norm?

- **Contrastive learning and spectral methods**
  - Why needed here: The bi-level loss (Eq. 14, Algorithm 1) is a contrastive objective where positive pairs are (Xi, ¨Yi) and negatives are (Xi, ¨Yj) for i≠j. This recovers spectral structure without explicit kernel matrix computation.
  - Quick check question: Explain how the contrastive loss Lout encourages uθ(Xi) and vθ(¨Yi) to align while decorrelating across samples. What role does the orthonormality regularizer play?

- **Asymptotic hypothesis testing and contiguity**
  - Why needed here: The validity proof relies on CLT convergence and contiguity arguments for local alternatives. Understanding why no uniformly valid test exists (Shah & Peters, 2020) clarifies why this method conditions on Em.
  - Quick check question: State the Lindeberg condition for multivariate CLT. Why does sub-Gaussianity ensure it holds here?

## Architecture Onboarding

- **Component map**:
  - uθ: X → ℝᵈ (left singular functions)
  - vθ: Y × Z → ℝᵈ (right singular functions)
  - wθ: Z → ℝ²ᵈ (conditioning features)
  - Mθ ∈ ℝᵈˣᵈ (outer contrastive layer)
  - Nθ ∈ ℝ²ᵈˣ²ᵈ (inner contrastive layer, symmetric)

- **Critical path**:
  1. Initialize uθ, vθ, wθ, Mθ, Nθ with small random weights
  2. Alternate inner/outer optimization: update wθ, Nθ with ∇θ[Lin + γΩin]; then uθ, vθ, Mθ with ∇θ[Lout + γΩout]
  3. After n_steps, apply whitening: compute empirical covariances on full training set; transform features to ensure ĈUθUθ ≈ Id, etc.
  4. On test set: compute bTn via Eq. (10); reject H₀ if bTn > cα where cα = q₁₋α(χ²(d²))

- **Design tradeoffs**:
  - d vs. power: Larger d captures more signal but increases estimation variance (∝ d²/n) and requires more training samples for Em → 0
  - m vs. n split: More training samples improve representation quality; more test samples improve power
  - γ (regularization): Higher γ enforces orthonormality more strongly but may compete with contrastive objective. Post-hoc whitening reduces sensitivity to γ.
  - Network depth/width: Paper does not specify architectures; preliminary experiments suggest moderate MLPs suffice for synthetic benchmarks.

- **Failure signatures**:
  - Type I error inflation: If Em is large (poor training convergence, insufficient m, or network underfitting), the χ² approximation fails. Monitor ‖ĈUθUθ − Id‖F during whitening.
  - Power collapse in high dZ: As conditioning dimension increases, wθ must capture more complex structure. If inner loss stalls, the correction term becomes unreliable.
  - Numerical instability: Whitening requires invertible covariance matrices. If d is too large relative to m, empirical covariances may be singular.

- **First 3 experiments**:
  1. Post-nonlinear null validation (H₀): Replicate Fig. 2. Generate data with X⊥⊥Y|Z under the post-nonlinear model. Vary dZ from 100–500. Verify Type I error ≤ α + small margin (e.g., ≤ 0.07 for α = 0.05).
  2. Post-nonlinear power curve (H₁): Replicate Fig. 3. Under H₁, vary dZ and n. Compare SpecCIT against KCIT, RCIT, GCIT. Confirm power increases with n and decreases with dZ.
  3. Ablation on representation quality: Corrupt representation learning by (a) reducing m, (b) removing inner optimization (set wθ = 0), (c) removing whitening. Measure Type I error and power degradation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the truncation dimension d be selected in a data-dependent manner to satisfy the theoretical power guarantees?
- Basis in paper: Theorem 4.2 explicitly requires choosing d "large enough such that Σdⱼ₌₁ σ²ⱼ ≥ ‖ΣX¨Y·Z‖²HS / 2" to ensure power, but the true singular values σⱼ are unknown in practice.
- Why unresolved: The paper provides no heuristic or algorithm for selecting d beyond the theoretical requirement, leaving a gap between the theoretical condition and practical implementation.
- What evidence would resolve it: A sensitivity analysis showing how the choice of d impacts Type II error, or a proposed heuristic based on eigenvalue decay of the empirical covariance that adapts to the data.

### Open Question 2
- Question: How robust is the test's Type I error control when the sub-Gaussian assumption (Assumption 4.1) is violated?
- Basis in paper: Theorem 4.1 (Validity) relies on Assumption 4.1, which states that learned features must be K-sub-Gaussian. The experiments are limited to synthetic data with Gaussian/Laplace noise, which generally satisfies these tail bounds.
- Why unresolved: Real-world data often exhibits heavy tails or outliers. It is unknown if the asymptotic normality required for the chi-squared approximation holds under weaker moment assumptions.
- What evidence would resolve it: Empirical experiments on synthetic data with heavy-tailed noise (e.g., Cauchy or log-normal distributions) to observe if the Type I error inflates beyond the significance level α.

### Open Question 3
- Question: Does the method retain its validity and power advantages on real-world, high-dimensional datasets compared to existing kernel and neural tests?
- Basis in paper: The introduction cites "cancer genetics" as a motivation involving high-dimensional variables, but Section 6 (Experiments) restricts evaluation to synthetic "post-nonlinear model" benchmarks.
- Why unresolved: While the method is theoretically grounded and scalable, the "preliminary" nature of the experiments means it is unclear if the representation learning step generalizes to complex, unstructured real-world distributions without overfitting.
- What evidence would resolve it: Benchmarking against RCIT, GCIT, and CCIT on standard real-world CI testing datasets (e.g., flow-based or protein expression data) to validate practical performance.

## Limitations
- Representation quality critical: Validity and power explicitly depend on representation error Em → 0, which requires sufficient training samples and network capacity
- Theoretical guarantees asymptotic: Finite-sample validity and power bounds are not established; χ² approximation may degrade in small-sample regimes
- Synthetic evaluation only: Experiments limited to post-nonlinear synthetic data; real-world performance and robustness to model misspecification remain unclear

## Confidence
- Mechanism claims: High - Theoretical framework is rigorous with explicit assumptions and proofs
- Empirical claims: Medium - Limited to synthetic benchmarks with unspecified hyperparameters
- Practical implementation: Low - Key details like network architectures, hyperparameters, and train/test splits are unspecified

## Next Checks
1. Verify Type I error control on purely independent Gaussian data before testing post-nonlinear benchmarks
2. Measure representation error Em during training to confirm it converges toward zero
3. Test sensitivity to d dimension by running with d ∈ {5, 10, 20, 50} and measuring power degradation