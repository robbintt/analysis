---
ver: rpa2
title: 'DiscoX: Benchmarking Discourse-Level Translation task in Expert Domains'
arxiv_id: '2511.10984'
source_url: https://arxiv.org/abs/2511.10984
tags:
- translation
- evaluation
- metric-s
- accuracy
- judge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DiscoX introduces a new benchmark for evaluating discourse-level
  and expert-level Chinese-English translation, addressing the gap in current segment-focused
  translation benchmarks. The benchmark includes 200 professionally-curated texts
  across 7 domains, averaging over 1700 tokens, reflecting real-world professional
  translation demands.
---

# DiscoX: Benchmarking Discourse-Level Translation task in Expert Domains

## Quick Facts
- arXiv ID: 2511.10984
- Source URL: https://arxiv.org/abs/2511.10984
- Reference count: 40
- Introduces a 200-text discourse-level Chinese-English translation benchmark across 7 expert domains

## Executive Summary
DiscoX introduces a new benchmark for evaluating discourse-level and expert-level Chinese-English translation, addressing the gap in current segment-focused translation benchmarks. The benchmark includes 200 professionally-curated texts across 7 domains, averaging over 1700 tokens, reflecting real-world professional translation demands. To assess performance, the authors developed Metric-S, a reference-free evaluation system using a multi-LLM judge workflow that evaluates accuracy, fluency, and appropriateness, with error deduplication to ensure fair scoring. Metric-S achieves 70.3% consistency with human judgments, outperforming existing metrics like XCOMET-QE (34.7%). Experiments with 20 models show that even the best-performing models lag behind human experts, particularly on discourse-heavy or domain-intensive texts, confirming DiscoX's difficulty and the challenges in achieving professional-grade machine translation. The benchmark and Metric-S provide a robust framework for future advancements in LLM-based translation.

## Method Summary
DiscoX is a benchmark for discourse-level translation that uses professional texts from 7 expert domains, each averaging 1712 tokens. It employs a reference-free evaluation system called Metric-S, which uses a multi-agent LLM judging workflow to evaluate accuracy, fluency, and appropriateness. The system includes error deduplication to prevent inflated penalty counts from root-cause propagation. Metric-S is built to dynamically filter tasks based on SOTA model failures, ensuring the benchmark remains challenging. The evaluation uses specialized LLM agents for different aspects of translation quality, achieving 70.3% consistency with human judgments.

## Key Results
- DiscoX benchmark includes 200 texts across 7 expert domains with average length of 1712 tokens
- Metric-S evaluation achieves 70.3% consistency with human judgments, outperforming XCOMET-QE (34.7%)
- Best-performing models (GPT-5-high at 76.66) still trail human experts (80.16) on discourse-heavy texts

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Multi-agent LLM judging with structured workflow substantially improves evaluation consistency over single-LLM or neural metrics for discourse-level translation.
- **Mechanism:** Metric-S decomposes evaluation into specialized agents (instruction-following check, accuracy judge, fluency judge, appropriateness judge, de-duplication judge) rather than relying on a single generalist LLM. Each agent focuses on a narrow scope, reducing cognitive load and error propagation.
- **Core assumption:** Error detection accuracy improves when evaluation dimensions are isolated and assigned to specialized evaluators rather than combined.
- **Evidence anchors:**
  - [abstract]: "Metric-S achieves 70.3% consistency with human judgments, outperforming existing metrics like XCOMET-QE (34.7%)"
  - [Section A.2.1]: Ablation shows single LLM judge with simple prompt achieves only 24.9% consistency vs. 70.3% for full Metric-S
  - [corpus]: GRAFT paper (arXiv:2507.03311) confirms document-level MT struggles with discourse phenomena capture—weak direct evidence for multi-agent evaluation specifically

### Mechanism 2
- **Claim:** Hierarchical error de-duplication prevents inflated penalty counts from root-cause propagation.
- **Mechanism:** When a single error (e.g., mistranslation) causes downstream symptoms (disfluent phrasing, style mismatch), the system attributes all related issues to the root cause dimension and penalizes once. Priority ordering: Extremely Critical accuracy errors > rubric violations > causally-derived symptoms.
- **Core assumption:** Human evaluators naturally attribute compound errors to their primary cause; metrics should replicate this cognitive normalization.
- **Evidence anchors:**
  - [Section 3.3]: "errors marked as 'Extremely Critical' in Accuracy take overriding priority, rubric-defined violations are systematically attributed to Accuracy"
  - [Section A.2.1]: Removing de-duplication reduces system-level consistency from 90% to 80%
  - [corpus]: No direct corpus evidence for de-duplication mechanism specifically

### Mechanism 3
- **Claim:** Difficulty filtering via dual-SOTA failure threshold ensures benchmark tasks remain challenging as models improve.
- **Mechanism:** Tasks pass to final selection only if both tested SOTA models fail on ≥8 predefined rubrics. This establishes a dynamic difficulty floor tied to current model capabilities rather than fixed heuristics.
- **Core assumption:** Rubric coverage adequately samples the error space; SOTA models' failure patterns indicate genuine difficulty rather than prompt mismatch.
- **Evidence anchors:**
  - [Section 2.1]: "A task advances to the next stage only if both models fail on a minimum of eight predefined rubrics"
  - [Table 3]: Even GPT-5-high scores 76.66 vs. human expert 80.16, confirming retained difficulty
  - [corpus]: Doc-Guided Sent2Sent++ (arXiv:2501.08523) notes long-context dependency challenges persist—consistent with difficulty filtering rationale

## Foundational Learning

- **Concept: Discourse-level coherence in translation**
  - **Why needed here:** DiscoX explicitly targets texts averaging 1712 tokens where segment-level accuracy metrics fail. Understanding that coherence spans cross-sentence reference resolution, terminology consistency, and logical flow is prerequisite to interpreting Metric-S outputs.
  - **Quick check question:** Given a document where "it" in sentence 5 refers to a concept introduced in sentence 2, would a segment-level metric catch a mistranslation of "it"? Why or why not?

- **Concept: Reference-free evaluation**
  - **Why needed here:** Metric-S operates without reference translations, unlike ChrF or standard COMET. This is necessary because discourse-level translations admit multiple valid outputs that no single reference can capture.
  - **Quick check question:** What are the tradeoffs of reference-free vs. reference-based evaluation when assessing a creative literary translation?

- **Concept: LLM-as-a-judge biases**
  - **Why needed here:** The paper notes single-LLM judges suffer hallucinations and biases (Section 6.2). Understanding self-preference bias (o3 ranking itself first vs. human ranking it third, Section A.3) is critical for judge model selection.
  - **Quick check question:** If evaluating Model A's outputs using Model A as the judge, what systematic distortion would you expect?

## Architecture Onboarding

- **Component map:**
  Input Translation -> Instruction-Following Check (filter invalid outputs)
                   -> Quality Estimation (parallel: Accuracy + Fluency + Appropriateness judges)
                   -> De-duplication Judge (hierarchical root-cause attribution)
                   -> Score Calculation (severity-weighted sum)

- **Critical path:** Accuracy Judge -> De-duplication. Accuracy errors propagate most heavily (60-point max weight, up to 50-point deductions for extremely critical errors). Fluency/Appropriateness are subsidiary (20 points each).

- **Design tradeoffs:**
  - Multi-agent vs. single-LLM: +45% consistency gain (70.3% vs. 24.9%) but ~5x inference cost
  - Reference-free vs. reference-based: Enables evaluation without gold standards but requires stronger judge model; Metric-S with Gemini-2.5-Pro (70.3%) still trails reference-based XCOMET on WMT tasks (68.8% vs. 72.3% in Table 10)
  - Severity granularity: 4 levels (minor/major/critical/extremely critical) adds expressiveness but requires reliable severity classification from judge

- **Failure signatures:**
  - Zero-score outputs: Instruction-following check triggered (model summarized, continued, or code-switched instead of translating)
  - Compressed score distribution: De-duplication over-aggressive; check if distinct errors are being collapsed
  - Judge-model dependency: Switching from Gemini-2.5-Pro to o3-high drops consistency from 70.3% to 58.2% (Table 8)—indicates judge selection is load-bearing

- **First 3 experiments:**
  1. Validate judge consistency on held-out subset: Sample 20 DiscoX tasks with human annotations; run Metric-S with your intended judge model; compute pairwise consistency. Target: ≥65% before proceeding.
  2. Ablate de-duplication: Run evaluation pipeline with de-duplication disabled; compare score distributions and identify tasks with largest score inflation. If max inflation >15 points, de-duplication is essential.
  3. Cross-domain stress test: Evaluate models on both Academic and Non-Academic task splits; verify that domain-specific terminology rubrics are being triggered (check Accuracy Judge outputs for rubric citations). If rubric mention rate <30%, prompt engineering needed.

## Open Questions the Paper Calls Out
None

## Limitations

- Domain Representativeness: The 200-text corpus spans 7 expert domains but may overrepresent certain discourse structures while underrepresenting others
- Judge Model Ceiling: Metric-S achieves 70.3% consistency with human judgments, but this still leaves 29.7% disagreement
- Reference-Free Tradeoffs: While enabling flexible scoring, reference-free evaluation introduces judge bias as the primary evaluation mechanism

## Confidence

- **High Confidence:** The benchmark successfully identifies performance gaps between current models and human experts, with even top models (GPT-5-high at 76.66) trailing human experts (80.16). The multi-agent architecture demonstrably outperforms single-LLM evaluation (70.3% vs. 24.9% consistency).
- **Medium Confidence:** The difficulty filtering mechanism effectively maintains benchmark challenge as models improve, though this relies on rubric adequacy and SOTA model failure patterns. De-duplication meaningfully reduces score inflation (90% to 80% consistency when removed).
- **Low Confidence:** The generalizability of Metric-S beyond Chinese-English discourse translation, particularly to languages with different discourse structures or lower-resource settings where specialized judge models may be unavailable.

## Next Checks

1. **Judge Model Cross-Validation:** Run Metric-S with three different judge models (e.g., Gemini-2.5-Pro, o3-high, GPT-4o) on the same 50-task subset. Compute inter-judge agreement scores and identify tasks with highest variance. If agreement <60% on any task, the evaluation pipeline lacks robustness.

2. **Domain Coverage Audit:** For each of the 7 domains in DiscoX, sample 10 random sentences and manually annotate discourse phenomena (coreference chains, terminological consistency, logical flow markers). Compare against rubric coverage—if any phenomenon appears in >30% of sampled sentences but triggers rubrics <10% of the time, update rubric inventory.

3. **Translation Diversity Impact Test:** Select 20 DiscoX tasks and generate 3-5 alternative valid translations per task (maintaining meaning but varying style/terminology). Evaluate all variants with Metric-S. If score variance >15 points across valid translations, the metric's reference-free approach may be overly sensitive to stylistic choices rather than substantive quality differences.