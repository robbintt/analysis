---
ver: rpa2
title: 'Working My Way Back to You: Resource-Centric Next-Activity Prediction'
arxiv_id: '2508.19016'
source_url: https://arxiv.org/abs/2508.19016
tags:
- prediction
- resource
- encoding
- activity
- process
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces resource-centric next-activity prediction\
  \ in predictive process monitoring, shifting focus from case-level to individual\
  \ resource behavior. The authors evaluate four encoding strategies\u2014SeqOnly\
  \ (baseline), SCap (capability), S2g (2-gram transitions), and S2gR (2-gram + repetition)\u2014\
  across four real-life datasets and four models: Random Forest, LightGBM, LSTM, and\
  \ Transformer."
---

# Working My Way Back to You: Resource-Centric Next-Activity Prediction

## Quick Facts
- arXiv ID: 2508.19016
- Source URL: https://arxiv.org/abs/2508.19016
- Reference count: 17
- Best model combination: Random Forest with S2gR encoding

## Executive Summary
This paper introduces resource-centric next-activity prediction in predictive process monitoring, shifting focus from case-level to individual resource behavior. The authors evaluate four encoding strategies—SeqOnly (baseline), SCap (capability), S2g (2-gram transitions), and S2gR (2-gram + repetition)—across four real-life datasets and four models: Random Forest, LightGBM, LSTM, and Transformer. The best-performing combination was Random Forest with S2gR, achieving the highest average accuracy. Encodings incorporating activity transition patterns and repetition features significantly outperformed the baseline, with improvements up to +0.25 accuracy in some cases. The results highlight the potential of resource-centric prediction for improving workload balancing, workforce planning, and personalized employee support, while also demonstrating the unique challenges posed by high behavioral variability and low example leakage in resource-centric settings.

## Method Summary
The authors develop a resource-centric approach to next-activity prediction by encoding individual resource behavior patterns. Four encoding strategies are proposed: SeqOnly uses only resource capabilities, SCap adds capability features, S2g captures 2-gram transition patterns, and S2gR combines 2-gram transitions with repetition features. These encodings are evaluated using four machine learning models—Random Forest, LightGBM, LSTM, and Transformer—across four BPIC datasets. The approach shifts from traditional case-centric predictions to modeling individual resource workflows, capturing sequential patterns and behavioral tendencies. Feature selection uses mutual information for S2g and S2gR encodings, while SCap uses domain knowledge. The study compares accuracy across encoding-model combinations to identify optimal approaches for resource-centric prediction.

## Key Results
- Random Forest with S2gR encoding achieved the highest average accuracy across all datasets
- S2g and S2gR encodings significantly outperformed the baseline SeqOnly approach, with improvements up to +0.25 accuracy
- Encoding strategies incorporating activity transition patterns and repetition features showed superior performance
- BPIC2019 dataset (highest activity specialization) achieved 0.85-0.95 accuracy, while lower specialization datasets showed more variability

## Why This Works (Mechanism)
The resource-centric approach works by shifting the prediction focus from case-level to individual resource behavior patterns. By encoding sequential activity transitions (2-grams) and repetition features, the model captures how individual resources tend to work and repeat activities. This behavioral modeling is particularly effective when resources have high activity specialization, as seen in BPIC2019. The S2gR encoding's combination of transition patterns and repetition features provides richer behavioral context than simpler encodings, enabling better predictions of which activities a resource will perform next.

## Foundational Learning
- **Resource-centric vs case-centric prediction**: Why needed - Traditional process mining focuses on cases; quick check - Can you explain the difference between predicting next case activity vs next resource activity?
- **Sequential encoding (2-gram transitions)**: Why needed - Captures behavioral patterns in resource workflows; quick check - Can you identify 2-gram patterns in a sample resource activity sequence?
- **Mutual information feature selection**: Why needed - Reduces dimensionality while preserving predictive features; quick check - Can you explain why MI is used instead of correlation for categorical features?
- **Behavioral variability modeling**: Why needed - Resources show different work patterns requiring adaptive modeling; quick check - Can you identify which BPIC dataset would have highest behavioral variability?

## Architecture Onboarding

**Component Map**: Datasets (BPIC) -> Preprocessing -> Encoding (SeqOnly, SCap, S2g, S2gR) -> Feature Selection -> Models (RF, LGBM, LSTM, Transformer) -> Evaluation

**Critical Path**: Encoding creation → Feature selection → Model training → Accuracy evaluation

**Design Tradeoffs**: Simplicity (SeqOnly) vs performance (S2gR); computational cost of complex encodings vs accuracy gains; model interpretability (tree-based) vs sequence modeling capability (deep learning)

**Failure Signatures**: Low accuracy indicates insufficient behavioral patterns in data; poor performance of deep learning models suggests fixed hyperparameters; high variability across datasets indicates sensitivity to activity specialization

**First Experiments**: 1) Compare baseline SeqOnly accuracy across all four datasets; 2) Evaluate S2gR encoding with Random Forest on BPIC2019; 3) Test whether adding repetition features to S2g improves accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would systematic hyperparameter tuning enable LSTM and Transformer models to match or exceed Random Forest's performance in resource-centric next-activity prediction?
- Basis in paper: [explicit] "We acknowledge that not using hyperparameter tuning for the LSTM and Transformer models is a limitation of our current work. In future work, we aim to explore tuning strategies to potentially improve their performance."
- Why unresolved: LSTM and Transformer used fixed configurations while tree-based models received GridSearchCV tuning, creating an uneven comparison.
- What evidence would resolve it: Controlled ablation study with systematic hyperparameter search for deep learning models across all four BPIC datasets.

### Open Question 2
- Question: Does incorporating higher-order n-gram transitions (3-gram, 4-gram) improve accuracy beyond the 2-gram transitions in S2g and S2gR?
- Basis in paper: [explicit] Conclusion lists "investigating the effect of more granular transition encoding" as a promising future direction.
- Why unresolved: S2g encoding captures only 2-gram transitions with top 20 features selected via mutual information; longer sequential dependencies remain unexplored.
- What evidence would resolve it: Comparative evaluation of n-gram encodings (n=2 to n=5) measuring accuracy gains against computational cost.

### Open Question 3
- Question: Can individual resource-centric activity predictions be reliably aggregated to forecast role-level demand over specific time horizons?
- Basis in paper: [explicit] Section VI-C identifies "Forecasting Role Demand" as an opportunity, noting predictions "can be aggregated over time to forecast demand for specific roles."
- Why unresolved: Paper establishes prediction capability at individual level but does not validate aggregated forecasting accuracy.
- What evidence would resolve it: Time-series evaluation comparing predicted vs. actual role-level workload distribution.

### Open Question 4
- Question: Do the best-performing encoding-model combinations generalize to domains with different activity specialization profiles than BPIC datasets?
- Basis in paper: [inferred] Results show accuracy strongly correlates with activity specialization (0.78 in BPIC2019 yielded 0.85–0.95 accuracy; lower specialization yielded higher variability), suggesting context-dependency.
- Why unresolved: Only BPIC datasets were evaluated; optimal combinations may differ in healthcare, manufacturing, or service domains.
- What evidence would resolve it: Cross-domain benchmarking analyzing whether activity specialization predicts encoding effectiveness.

## Limitations
- Limited generalizability across different process domains and organizational contexts
- Absence of ablation study on encoding components makes it difficult to isolate specific contributions
- No analysis of prediction accuracy variation across different resource experience levels or activity types

## Confidence

**High confidence**: The core experimental findings showing S2gR encoding outperforming baseline methods across multiple datasets and models are well-supported by the presented results. The comparative analysis between different encoding strategies is methodologically sound.

**Medium confidence**: The generalizability of findings to other process mining domains and the practical impact on resource allocation decisions are less certain, given the limited scope of evaluation datasets and absence of real-world deployment testing.

**Low confidence**: The theoretical claims about the broader implications for workforce planning and personalized support lack empirical validation beyond the presented accuracy metrics.

## Next Checks
1. Conduct cross-domain validation using datasets from different industries and process types to assess generalizability of the resource-centric approach.

2. Perform ablation studies to quantify the individual contributions of activity transition patterns and repetition features to overall prediction performance.

3. Implement a pilot study in a real organizational setting to measure the practical impact of predictions on resource allocation efficiency and workforce satisfaction.