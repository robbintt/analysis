---
ver: rpa2
title: A Survey of Multimodal Retrieval-Augmented Generation
arxiv_id: '2504.08748'
source_url: https://arxiv.org/abs/2504.08748
tags:
- arxiv
- multimodal
- retrieval
- text
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This survey comprehensively reviews Multimodal Retrieval-Augmented
  Generation (MRAG), an emerging paradigm that enhances large language models by integrating
  multimodal data (text, images, videos) into retrieval and generation processes.
  The paper categorizes MRAG development into three stages: MRAG1.0 (pseudo-MRAG)
  extending traditional RAG with multimodal captions, MRAG2.0 introducing true multimodal
  retrieval and generation using MLLMs, and MRAG3.0 advancing to end-to-end multimodality
  with enhanced document parsing and output generation.'
---

# A Survey of Multimodal Retrieval-Augmented Generation

## Quick Facts
- **arXiv ID:** 2504.08748
- **Source URL:** https://arxiv.org/abs/2504.08748
- **Reference count:** 40
- **Primary result:** Comprehensive survey of MRAG development across three stages (MRAG 1.0, 2.0, 3.0) with focus on challenges in document parsing, search planning, retrieval efficiency, and generation coherence.

## Executive Summary
This survey provides a systematic review of Multimodal Retrieval-Augmented Generation (MRAG), an emerging paradigm that enhances large language models by integrating multimodal data into retrieval and generation processes. The paper traces MRAG development through three evolutionary stages: MRAG 1.0 extending traditional RAG with multimodal captions, MRAG 2.0 introducing true multimodal retrieval and generation using MLLMs, and MRAG 3.0 advancing to end-to-end multimodality with enhanced document parsing and output generation. The survey identifies key challenges across MRAG components including document parsing complexity, search planning adaptability, retrieval efficiency for cross-modal data, and generation coherence across modalities.

## Method Summary
The survey analyzes MRAG systems through a structured pipeline: document parsing (OCR/text extraction vs. screenshot capture), indexing (text vector DB vs. multimodal vector DB), search planning (query classification and reformulation), retrieval (dense ANN search with reranking and refiner modules), and generation (MLLM synthesis). The methodology involves categorizing existing approaches by their architectural choices and identifying open challenges. The paper provides implementation guidance through a minimum viable reproduction plan focusing on multimodal indexing with MLLM-based parsers, dual-stream retrieval using models like CLIP, and multimodal generation with MLLMs like InternLM-XComposer.

## Key Results
- MRAG systems reduce hallucinations by grounding generation in retrieved multimodal evidence rather than internal parametric memory
- Three-stage evolution identified: MRAG 1.0 (text + captions), MRAG 2.0 (multimodal retrieval/generation), MRAG 3.0 (end-to-end multimodality with search planning)
- Key challenges identified: document parsing complexity, adaptive search planning, unified cross-modal representation learning, and multimodal output coherence

## Why This Works (Mechanism)

### Mechanism 1: Grounding via External Multimodal Knowledge
MRAG systems reduce hallucinations by forcing the generation model to synthesize answers based on retrieved evidence rather than internal parametric memory alone. The system retrieves relevant text and images from a knowledge base and concatenates them with the user query as context. If the retrieval is accurate, the model's generation is "grounded" in the retrieved facts, mitigating fabrication. Break condition: If the retrieval step fetches irrelevant or "poisoned" documents, the grounding mechanism propagates noise or adversarial content into the final answer.

### Mechanism 2: Representation-Based Document Parsing (Screenshots)
Indexing document screenshots directly (using Vision Language Models) preserves structural and visual information (charts, layouts) that text-extraction (OCR) misses. Instead of parsing text into chunks, the system takes a screenshot of the document page and embeds it into a vector space using a VLM. This allows retrieval of information based on "visual semantics" (e.g., a specific chart trend) rather than just keywords. Break condition: High computational cost during indexing; failure to capture fine-grained text if the VLM resolution is insufficient.

### Mechanism 3: Adaptive Retrieval Planning (Search Planning)
Adaptive planning (deciding whether to search and what modality to search) improves efficiency and accuracy over fixed "always-search" pipelines. A "Search Planner" module analyzes the user query, classifying if external knowledge is needed and reformulating the query if necessary. This prevents unnecessary retrieval of irrelevant images for simple text queries. Break condition: The planner incorrectly skips retrieval for a knowledge-intensive query (false negative) or triggers complex multimodal search for a simple greeting (false positive), wasting resources.

## Foundational Learning

- **Vector Embeddings & Cross-Modal Retrieval**
  - Why needed here: MRAG relies on "Unified Multimodal Retrieval" where text and images must be mapped to the same vector space to be compared
  - Quick check question: How does a vector database calculate "relevance" between a text query and an image file? (Expected: Cosine similarity of their respective embeddings)

- **Context Window vs. Knowledge Retention**
  - Why needed here: The "Refiner" component exists because MLLMs have limited context windows
  - Quick check question: What is the "Catastrophic Forgetting" issue mentioned regarding long contexts?

- **MLLM Architecture (Visual Encoders + LLM)**
  - Why needed here: MRAG 2.0 and 3.0 utilize MLLMs for generation
  - Quick check question: In MRAG 2.0, how is the "image" from the knowledge base handled by the generator? (Expected: It is processed by a visual encoder and projected into the LLM's embedding space)

## Architecture Onboarding

- **Component map:** User Query -> Search Planner (Router) -> Retriever (Vector Search) -> Reranker (Re-scoring) -> Refiner (Token reduction) -> Generator (MLLM) -> Answer
- **Critical path:** The system routes queries through a planner, retrieves relevant multimodal context, refines the results to fit context windows, and generates coherent answers using MLLMs
- **Design tradeoffs:** MRAG 1.0 (Text+Captions) offers speed and low cost; MRAG 3.0 (Screenshots) provides high accuracy on visual documents but incurs high compute penalties
- **Failure signatures:** The "Compulsive Retrieval" Loop (retrieving irrelevant images), Modality Disconnection (retrieved image inconsistent with generated text), and Privacy Leakage (exposing sensitive training data)
- **First 3 experiments:**
  1. Baseline vs. Multimodal: Run queries on text-only RAG vs. MRAG system (using captions for images) to quantify hallucination reduction
  2. Parsing Accuracy Test: Compare OCR-based text extraction vs. Screenshot-based retrieval on complex PDFs to measure information loss
  3. Adaptive Planning A/B Test: Implement "Always Retrieve" vs. "Classifier-Based" search planner to measure latency savings and answer quality degradation

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can multi-agent collaborative systems be effectively designed to enable specialized agents to coordinate parallel reasoning paths and complementary retrieval strategies for complex MRAG queries?
- **Basis in paper:** Section 7.2 (Multimodal Search Planning) explicitly suggests future research on "Multi-Agent Collaborative Systems" where "specialized agents can simultaneously explore multiple reasoning trajectories" and employ "complementary retrieval strategies"
- **Why unresolved:** Current systems rely on fixed planning pipelines that cannot adapt to query diversity or computational constraints
- **What evidence would resolve it:** A functional MRAG framework demonstrating that multi-agent coordination improves accuracy and efficiency in multi-hop or creative queries compared to single-agent baselines

### Open Question 2
- **Question:** How can unified cross-modal representation frameworks effectively align heterogeneous data types (text, images, audio, video) into a shared semantic space without losing modality-specific richness?
- **Basis in paper:** Section 7.3 (Retrieval) identifies "Unified Cross-Modal Representation Learning" as a primary objective, noting the difficulty that "current approaches often fail to create a common embedding space that adequately captures the semantic richness"
- **Why unresolved:** The inherent heterogeneity of data complicates feature alignment, often requiring specialized encoders that struggle to align in a single shared space
- **What evidence would resolve it:** A single unified model achieving state-of-the-art retrieval performance across text, image, audio, and video modalities simultaneously

### Open Question 3
- **Question:** How can models intelligently identify optimal insertion points for non-textual elements within a narrative flow to ensure coherence in multimodal outputs?
- **Basis in paper:** Section 6.4 (Generation) explicitly lists "Positioning and Integration of Multimodal Elements" as a challenge, requiring identification of optimal insertion points
- **Why unresolved:** This requires deep understanding of narrative structure and context which current models struggle to maintain while generating across modalities
- **What evidence would resolve it:** A model capable of generating interleaved text-image content where automated metrics confirm that image placement enhances rather than disrupts text readability and logic

## Limitations
- The survey lacks implementation details for critical components like search planning module architectures and fusion strategies for combining text/image retrieval scores
- Evaluation metrics referenced (Answer Precision, Retrieved Context Recall, Hallucination Rate) are not standardized across the field
- Specific implementation details for adaptive search planning, cross-modal fusion logic, and screenshot-based parsing effectiveness remain underspecified

## Confidence
- **High Confidence:** The three-stage evolution framework and general pipeline components are well-established and consistently described
- **Medium Confidence:** Mechanisms for hallucination reduction and computational trade-offs are logically sound but need empirical validation
- **Low Confidence:** Specific implementation details for adaptive planning, fusion logic, and parsing method effectiveness remain underspecified

## Next Checks
1. **Implement and compare OCR-based parsing (MRAG 1.0) versus screenshot-based parsing (MRAG 3.0) on a dataset of visually complex documents to quantify information retention differences.**

2. **Build a simple classifier-based search planner and conduct A/B testing against a fixed "always-retrieve" baseline to measure latency savings and answer quality trade-offs.**

3. **Test the grounding hypothesis by running queries on a poisoned knowledge base (following the Poisoned-MRAG attack model) to observe how retrieval-based hallucination manifests when external knowledge is compromised.**