---
ver: rpa2
title: 'When Safety Blocks Sense: Measuring Semantic Confusion in LLM Refusals'
arxiv_id: '2512.01037'
source_url: https://arxiv.org/abs/2512.01037
tags:
- confusion
- prompts
- semantic
- arxiv
- token-level
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces "semantic confusion" as a measurable failure
  mode in safety-aligned language models, where models accept one phrasing of a benign
  intent but reject semantically equivalent paraphrases. To quantify this, the authors
  build ParaGuard, a 10k-prompt dataset of controlled paraphrase clusters, and propose
  three token-level metrics: Confusion Index (CI), Confusion Rate (CR), and Confusion
  Depth (CD).'
---

# When Safety Blocks Sense: Measuring Semantic Confusion in LLM Refusals

## Quick Facts
- arXiv ID: 2512.01037
- Source URL: https://arxiv.org/abs/2512.01037
- Reference count: 35
- One-line primary result: Global false rejection rates mask local semantic inconsistency; token-level metrics reveal boundary instability and enable targeted safety tuning.

## Executive Summary
The paper introduces "semantic confusion" as a measurable failure mode in safety-aligned language models, where models accept one phrasing of a benign intent but reject semantically equivalent paraphrases. To quantify this, the authors build ParaGuard, a 10k-prompt dataset of controlled paraphrase clusters, and propose three token-level metrics: Confusion Index (CI), Confusion Rate (CR), and Confusion Depth (CD). Experiments across diverse models and guards show that global false rejection rates mask critical local inconsistencies. CI/CR/CD reveal globally unstable boundaries in some settings, localized pockets of inconsistency in others, and cases where stricter refusal does not increase inconsistency, enabling more targeted safety tuning.

## Method Summary
ParaGuard is constructed from 2,000 seeds sampled from OR-Bench, USEBench, and PHTest, each with 5 generated paraphrases passing quality gates (cosine similarity ≥0.60, Jaccard overlap ≤0.90, risk score [0.30, 0.70]). Target models are queried at temperature 0 with a safety instruction wrapper, and responses classified as ACCEPT/REJECT via refusal cue matching. For each rejection, top-5 nearest accepted neighbors are retrieved using Sentence Transformer + FAISS, and three token-level signals (Drift, ProbShift, ΔPPL) are combined into a confusion score. Metrics are aggregated to dataset-level CI, CR@τ=0.75, and CD.

## Key Results
- Global FRR masks local inconsistency: Qwen 3 has higher FRR than Qwen 2.5 but dramatically lower confusion, showing stricter refusal need not produce unstable decisions.
- Guard-specific patterns: SafeUnlearn has FRR=97.2% and CR@0.60=1.00 (globally erratic), while Hate Speech has FRR=2.4% and CR@0.60=1.00 (few rejections but arbitrary).
- Risk-cohort analysis: Safe paraphrases show ~60% confusion; rejections near truly risky wording tend to be locally coherent.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Token-level signals detect local decision boundary inconsistencies that global refusal rates obscure.
- Mechanism: For each rejected prompt, retrieve k=5 nearest accepted neighbors via sentence-embedding cosine similarity, then compute three divergence signals—token drift (embedding distance between aligned tokens), probability shift (L1 difference in next-token predictions), and perplexity contrast (uncertainty delta). These capture whether semantically near-identical prompts trigger divergent model behavior.
- Core assumption: Token embeddings, next-token probabilities, and perplexity together form a sufficient proxy for the model's internal decision state around semantic boundaries.
- Evidence anchors: [abstract]: "token embeddings, next-token probabilities, and perplexity signals...reveal globally unstable boundaries in some settings, localized pockets of inconsistency in others"; [Section 3.2]: "Drift captures shifts in meaning...ProbShift captures how changes in wording affect the model's confidence...PPL captures the level of uncertainty"; [corpus]: Weak direct support. Neighbor papers address confusion in related contexts (emoticon semantic confusion, factual inconsistency scaling), but do not validate this specific token-level combination for safety boundaries.
- Break condition: If token-level signals correlate strongly (r > 0.8) with sentence-level similarity alone, the added complexity is unwarranted. Figure 3 shows r = -0.03, confirming orthogonality.

### Mechanism 2
- Claim: Conditioning rejection analysis on semantic neighborhoods exposes whether refusals are locally coherent or arbitrary.
- Mechanism: Global FRR treats each prompt independently. By structuring ParaGuard into paraphrase clusters (1 seed + 5 variants) with controlled gates (similarity ≥ 0.60, overlap ≤ 0.90, risk [0.30–0.70]), each rejection is evaluated relative to accepted paraphrases of identical intent. CI(r) averages confusion scores over Nₖ(r), making inconsistency measurable.
- Core assumption: Paraphrases passing the quality gates are semantically equivalent enough that differing decisions indicate boundary brittleness rather than legitimate nuance.
- Evidence anchors: [abstract]: "local inconsistency, where a model accepts one phrasing of an intent but rejects a close paraphrase...missed by global rates"; [Section 5.5, Table 2]: "safe paraphrase cohorts show substantially higher confusion (around 60%)...rejections near truly risky wording tend to be locally coherent"; [corpus]: No direct validation. Related work on semantic similarity through classification confusion (arXiv:2502.05704) supports confusion as a similarity signal, but not the neighborhood-conditioning approach specifically.
- Break condition: If confusion scores correlate tightly with risk scores (high-risk = high-confusion), neighborhood structure adds no information. Table 2 shows the opposite: high-risk cohort has lowest confusion rate (27.8%).

### Mechanism 3
- Claim: CI/CR/CD separate "how often" models refuse from "how sensibly" they refuse, enabling targeted tuning.
- Mechanism: CI measures average local contradiction per rejection. CR@τ quantifies severe confusion prevalence. CD captures whether confusion is concentrated or dispersed. Models with similar FRR can have vastly different CI/CR/CD profiles (e.g., Mistral 7B vs. Qwen 3 8B), revealing whether stricter refusal policies inherently increase inconsistency.
- Core assumption: Lower confusion metrics indicate better-calibrated boundaries, and reducing confusion won't increase false acceptances of harmful content.
- Evidence anchors: [Section 5.2]: "Qwen 3 has a higher FRR than Qwen 2.5, yet dramatically lower confusion, showing that stricter refusal policies need not produce unstable decisions"; [Section 5.6, Table 3]: "low FRR does not guarantee sensible boundaries...Hate Speech guard rejects rarely (≈2.4%), yet its few rejections cluster in high-confusion pockets"; [corpus]: No direct evidence. Factual inconsistency scaling paper (arXiv:2502.12372) suggests inconsistency grows with model size, but doesn't address the refusal-sensibility tradeoff.
- Break condition: If confusion reduction correlates with increased attack success rate on genuinely harmful prompts, the utility for safety tuning is limited. Paper does not test this.

## Foundational Learning

- Concept: **Token embeddings and semantic similarity**
  - Why needed here: Confusion Index relies on comparing token-level drift via cosine similarity in embedding space. Understanding that embeddings encode semantic proximity is prerequisite for interpreting why small paraphrases can have divergent drift scores.
  - Quick check question: If two tokens have cosine similarity 0.95, what does that imply about their semantic relationship? (Answer: They occupy nearly the same position in the learned semantic space, likely interchangeable in context.)

- Concept: **Perplexity as uncertainty measure**
  - Why needed here: Perplexity contrast is one of three confusion signals. Understanding that perplexity = exp(cross-entropy loss) and that higher perplexity indicates greater model uncertainty is essential for interpreting ΔPPL(a,r).
  - Quick check question: A model assigns perplexity 15 to prompt A and 45 to prompt B. Which prompt does the model find more surprising/unexpected? (Answer: Prompt B—higher perplexity = higher uncertainty.)

- Concept: **Decision boundaries in classification**
  - Why needed here: The paper frames safety refusal as a binary decision boundary. CI/CR/CD diagnose whether this boundary is smooth (consistent under paraphrase) or irregular (local pockets of confusion). Visualizing boundary geometry in high-dimensional space helps interpret "globally unstable" vs. "localized pockets."
  - Quick check question: If a decision boundary has high local variance, what happens when you perturb an input slightly? (Answer: Classification can flip unpredictably—small changes cross the irregular boundary.)

## Architecture Onboarding

- Component map:
  - ParaGuard corpus (10k prompts in ~2k clusters) -> Quality gate filtering (similarity ≥0.60, overlap ≤0.90, risk [0.30, 0.70]) -> Model querying (temperature 0, safety wrapper) -> Labeling (ACCEPT/REJECT via refusal cues) -> FAISS retrieval (top-5 accepted neighbors) -> Token-level scoring (Drift, ProbShift, ΔPPL) -> Metric aggregation (CI, CR, CD)

- Critical path: Seed selection → Paraphrase generation → Quality gate filtering → Model querying → Neighbor retrieval → Token-level scoring → Metric aggregation. The retrieval step is rate-limiting: FAISS indexing requires all ACCEPT labels first.

- Design tradeoffs:
  - Weight configuration (0.4, 0.1, 0.5) emphasizes perplexity over probability shift. Paper states this was grid-searched but doesn't report sensitivity.
  - k=5 neighbors balances computational cost with neighborhood stability. Smaller k increases noise; larger k dilutes local signal.
  - τ=0.75 threshold for CR is arbitrary but provides stable operating point per Section 4.2.

- Failure signatures:
  - **SafeUnlearn guard** (Table 3): FRR=97.2%, CR@0.60=1.00. Almost all rejections are locally inconsistent—boundary is globally erratic, not just conservative.
  - **Hate Speech guard**: FRR=2.4%, CR@0.60=1.00. Low FRR masks that the few rejections are arbitrary relative to accepted paraphrases.
  - **Mistral models**: Higher CD (0.038–0.040) with lower CR indicates localized confusion pockets rather than global instability.

- First 3 experiments:
  1. **Baseline sanity check**: Run ParaGuard on your model. If CI correlates strongly (|r| > 0.7) with sentence-level similarity alone, the token-level signals are redundant—investigate weighting or feature selection.
  2. **Cohort stratification**: Split your model's rejections into risk × similarity bins (following Figure 5). If high-similarity/low-risk cohort shows CR > 50%, your boundary is brittle on safe paraphrases—target these clusters for fine-tuning.
  3. **Guard comparison**: If using a deployment guard, compute CI/CR on rejections before and after enabling it. If CR increases substantially while FRR stays similar, the guard is destabilizing local coherence without improving coverage.

## Open Questions the Paper Calls Out

- **Open Question 1**: Does a CI-based veto that overturns high-confusion refusals improve usability without compromising safety?
  - Basis in paper: [explicit] The authors propose "a practical fix is to keep the guard's threshold but add a CI-based veto that overturns only obviously inconsistent rejections, improving usability without broadly weakening safety" but do not evaluate this intervention.
  - Why unresolved: The paper measures confusion but stops short of implementing or testing whether acting on CI signals (e.g., overriding high-CI refusals) actually reduces over-refusal while preserving true attack rejection.
  - What evidence would resolve it: A controlled experiment comparing guard behavior with and without CI-based overrides, measuring both benign-task success rate and attack success rate.

- **Open Question 2**: Can semantic confusion metrics be computed reliably from black-box APIs that do not expose token-level signals?
  - Basis in paper: [inferred] The metrics require token embeddings, next-token probabilities, and perplexity. Many production LLMs (e.g., GPT-4 via API) do not expose these internals, limiting real-world applicability.
  - Why unresolved: The paper claims model-agnostic applicability but all experiments use models where internal states are accessible. No proxy or approximation method is tested for black-box settings.
  - What evidence would resolve it: Development and validation of approximation techniques (e.g., using only output text features or sampling-based proxies) that correlate strongly with full-access CI/CR/CD scores.

- **Open Question 3**: Do jailbreak defenses that appear effective globally inadvertently increase semantic confusion on benign inputs?
  - Basis in paper: [explicit] The authors state that "CI/CR/CD reveal whether a defense that appears strong globally inadvertently increases semantic confusion" but do not present experimental results on jailbreak defenses.
  - Why unresolved: The paper frames this as a key application of their metrics but focuses evaluation on base models and deployment guards, not on adversarial defense mechanisms.
  - What evidence would resolve it: Systematic evaluation of common jailbreak defenses (e.g., SmoothLLM, self-reminders) showing whether defense deployment correlates with increased CI/CR/CD on benign paraphrase clusters.

- **Open Question 4**: How robust are the confusion metrics to paraphrase quality, and do false semantic-equivalence assumptions inflate confusion estimates?
  - Basis in paper: [inferred] The quality gates use heuristic thresholds (cosine similarity ≥0.60, Jaccard ≤0.90, risk score 0.30–0.70). Some generated paraphrases may not be truly intent-preserving, potentially misclassifying legitimate semantic differences as confusion.
  - Why unresolved: No human verification of paraphrase equivalence is reported, and the paper acknowledges that filtering must ensure harmlessness "without collapsing variants into trivial duplicates."
  - What evidence would resolve it: Human annotation of a subset of high-CI rejected prompts to verify whether accepted neighbors genuinely share intent, with analysis of how paraphrase quality affects metric validity.

## Limitations
- The paper assumes paraphrase clusters are semantically equivalent based on heuristic quality gates, but does not validate this equivalence with human annotation, potentially conflating genuine semantic differences with confusion.
- The safety tradeoff is unproven: reducing confusion may improve consistency but could increase false acceptance of genuinely harmful content, which the paper does not test.
- The interpretation that high CI/CR indicates poor safety alignment is plausible but not conclusively validated; the paper shows correlation but not causation or safety impact.

## Confidence
- **High confidence**: The corpus construction (ParaGuard) is well-specified, and the three confusion metrics (CI, CR, CD) are clearly defined and reproducible. The experimental observation that global FRR masks local inconsistency is empirically supported across multiple models and guards.
- **Medium confidence**: The interpretation that high CI/CR indicates poor safety alignment is plausible but not conclusively proven. The paper shows correlation between confusion and inconsistency but does not establish causation or safety impact.
- **Low confidence**: The claim that targeted confusion reduction will improve safety without increasing harm is speculative. The paper does not test whether confusion reduction leads to higher attack success rates on genuinely risky prompts.

## Next Checks
1. **Semantic fidelity validation**: Manually annotate a random sample of ParaGuard paraphrase clusters to verify that high-similarity pairs are truly semantically equivalent. If ≥10% contain meaningful intent shifts, the confusion metric conflates boundary instability with genuine semantic nuance.

2. **Safety tradeoff experiment**: For a model with high confusion, apply targeted fine-tuning to reduce CI/CR. Then evaluate both (a) whether refusal consistency improves and (b) whether attack success rate on high-risk prompts increases. If confusion reduction correlates with higher attack success, the safety utility is limited.

3. **Baseline comparison**: Apply CI/CR/CD to a non-safety classification task (e.g., sentiment analysis) with known decision boundaries. If confusion metrics correlate with label noise or class overlap in the baseline, their diagnostic value for safety boundaries is undermined.