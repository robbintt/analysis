---
ver: rpa2
title: 'CleanMAP: Distilling Multimodal LLMs for Confidence-Driven Crowdsourced HD
  Map Updates'
arxiv_id: '2504.10738'
source_url: https://arxiv.org/abs/2504.10738
tags:
- lane
- data
- confidence
- score
- visibility
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "CleanMAP introduces an MLLM-driven framework for crowdsourced\
  \ HD map updates by filtering and refining data through lane visibility scoring\
  \ and confidence-driven local map fusion. The system evaluates key visibility factors\u2014\
  blur, illumination, weather, and degradation\u2014assigning quantitative scores\
  \ (0-10) that align with human assessments."
---

# CleanMAP: Distilling Multimodal LLMs for Confidence-Driven Crowdsourced HD Map Updates

## Quick Facts
- arXiv ID: 2504.10738
- Source URL: https://arxiv.org/abs/2504.10738
- Reference count: 40
- One-line primary result: MLLM-driven framework filters crowdsourced HD map updates via lane visibility scoring and confidence-driven fusion, achieving 0.28m error vs 0.37m baseline

## Executive Summary
CleanMAP introduces an MLLM-driven framework for crowdsourced HD map updates by filtering and refining data through lane visibility scoring and confidence-driven local map fusion. The system evaluates key visibility factors—blur, illumination, weather, and degradation—assigning quantitative scores (0-10) that align with human assessments. A dynamic piecewise confidence-scoring function ensures robust filtering, while an optimal fusion strategy selects the top-k local maps within a 10% confidence range. Evaluations show that fusing the top three local maps achieves the lowest mean error of 0.28m, outperforming the baseline (0.37m) and meeting accuracy thresholds (≤0.32m). Real-world validation confirms 84.88% alignment with human evaluators, demonstrating CleanMAP’s effectiveness for scalable and reliable autonomous navigation.

## Method Summary
CleanMAP uses a pretrained MLLM (EVA + LLaMA 2-7B with LoRA) fine-tuned for lane detection to score crowdsourced images on 10 visibility factors, producing discrete scores (0-10) per factor. A dynamic piecewise confidence-scoring function adapts confidence calculation based on lane visibility score, prioritizing lane-specific features. The system then fuses the top-k local maps within a 10% confidence range using ICP alignment and DBSCAN clustering. Training used 10,000 images (crowdsourced + synthetic) and real-world validation on 18,937 images from a Xiaopeng G3 vehicle in Beijing, achieving 84.88% human alignment and 0.28m mean error.

## Key Results
- Fusing top three local maps achieves lowest mean map update error of 0.28m
- Outperforms baseline (0.37m) and meets accuracy threshold (≤0.32m)
- Real-world validation shows 84.88% alignment with human evaluators

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MLLM-driven lane visibility scoring provides task-specific quality assessment superior to general image quality metrics.
- Mechanism: A lane detection-pretrained MLLM (EVA + LLaMA 2-7B with LoRA) evaluates images on 10 visibility factors and outputs discrete scores (0-10) per factor via structured prompts. A Visibility-Aware Confidence Weighting (WL = 1 - CL) prioritizes lane-specific features.
- Core assumption: MLLMs fine-tuned on lane detection tasks can transfer visual reasoning to quality scoring without requiring explicit quality labels.
- Evidence anchors: [abstract] "MLLM-driven lane visibility scoring model that systematically quantifies key visual parameters, assigning confidence scores (0–10)"; [section 3.1.2-3.1.7] Logit computation per score factor and softmax probability distribution; [corpus] Limited direct corpus evidence.

### Mechanism 2
- Claim: Dynamic piecewise confidence scoring aligns better with human judgment than fixed-threshold methods.
- Mechanism: A piecewise function adapts confidence calculation based on lane visibility score (SL): For SL<5, confidence is reduced by weighted degradation factors; for 5≤SL≤7, confidence equals lane visibility alone; for SL>7, slight adjustment subtracts degradation impact; SL=0 yields zero confidence.
- Core assumption: Human evaluators implicitly apply different reasoning strategies depending on baseline visibility conditions.
- Evidence anchors: [abstract] "A novel dynamic piecewise confidence-scoring function adapts scores based on lane visibility, ensuring strong alignment with human evaluations"; [section 3.2] Explicit piecewise formula and comparison showing DPCS achieves 84.88% alignment vs 66.28% for General Confidence Scoring.

### Mechanism 3
- Claim: Fusing top-3 local maps within 10% confidence range optimally balances quality and quantity.
- Mechanism: Local maps are ranked by average confidence score; fusion selects maps within [C_best, C_best - 0.1×C_best]. Top-3 fusion (Seq3) achieved 0.28m error vs 0.30m (Seq1) and 0.34m (Seq5).
- Core assumption: Confidence scores monotonically correlate with map update accuracy, and optimal k is invariant across road complexity levels.
- Evidence anchors: [abstract] "fusing the top three local maps achieves the lowest mean map update error of 0.28m, outperforming the baseline (0.37m)"; [section 3.3 & Table 7] Optimal range definition; comparative error rates across Seq1/3/5 and baselines.

## Foundational Learning

- Concept: Vision-Language Model Fine-Tuning with LoRA
  - Why needed here: The system adapts a pretrained MLLM for lane-specific scoring without full retraining; LoRA enables efficient parameter updates.
  - Quick check question: Given a base VLM, how would you add task-specific scoring capability while preserving general vision-language alignment?

- Concept: Confidence Calibration in Multimodal Outputs
  - Why needed here: Raw MLLM logits are converted to calibrated confidence scores via sigmoid and softmax; miscalibration propagates to map fusion errors.
  - Quick check question: If an MLLM outputs logits of [2.1, 1.4, 0.8] for scores 7, 8, 9, what is the softmax probability for score 8?

- Concept: Iterative Closest Point (ICP) and DBSCAN for Map Fusion
  - Why needed here: Local maps are aligned via ICP before fusion; DBSCAN clusters valid lane points and filters outliers.
  - Quick check question: How does DBSCAN handle points that don't belong to any dense cluster, and why is this useful for noise filtering in map data?

## Architecture Onboarding

- Component map: Crowdsourced images → EVA encoder → LLaMA 2-7B (with prompts) → Factor logits → DPCS confidence → Map ranking → ICP alignment → DBSCAN clustering
- Critical path: Image → EVA → LLaMA 2-7B (with prompts) → Factor logits → DPCS confidence → Map ranking → Fusion. Any failure in MLLM scoring propagates to all downstream map updates.
- Design tradeoffs:
  - LoRA rank (r=64): Higher rank improves adaptation but increases memory; paper notes batch size of 1 due to GPU constraints.
  - Top-k selection (k=3): Lower k improves quality but risks coverage gaps; higher k increases coverage but introduces noise.
  - Weight assignment (W_L=1.0, others=0.2): Lane visibility dominates; may underweight rare but severe factors like sandstorms (W_SS=0.1).
- Failure signatures:
  - Hallucinated scores: MLLM assigns non-zero scores to irrelevant factors (e.g., fog score in clear conditions) → implement context-aware parameter filtering.
  - Confidence miscalibration: Systematic over/under-scoring → validate on held-out human-labeled data; recalibrate sigmoid threshold.
  - Fusion divergence: ICP alignment fails when local maps lack sufficient overlap → require minimum overlap threshold before fusion.
- First 3 experiments:
  1. Scoring validation: Run MLLM scoring on 100 held-out images with human labels; compute per-factor correlation and overall confidence score alignment.
  2. Ablation on DPCS: Compare piecewise vs. fixed-threshold scoring on same dataset; measure alignment percentage and error rate impact.
  3. Fusion sweep: Test k=1,2,3,4,5 on held-out link areas with ground truth; plot error vs. k to validate optimal k=3 claim and identify boundary conditions where k differs.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can dynamic confidence thresholds be refined to mitigate conservative estimations under extreme lighting conditions (e.g., glare, low illumination) and partial occlusions?
- Basis in paper: [explicit] The authors state that despite strong performance, the model exhibits limitations under extreme lighting and partial occlusions, leading to conservative confidence estimations, and suggest future research should focus on "improved adaptability through refined dynamic confidence thresholds."
- Why unresolved: The current Dynamic Piecewise Confidence Scoring (DPCS) function uses fixed weights and interval logic that may not adequately adjust for the complex interplay of severe environmental factors.
- What evidence would resolve it: A comparative evaluation showing improved alignment with human evaluators and reduced mapping error (AME) on a dataset specifically curated for extreme weather and lighting edge cases.

### Open Question 2
- Question: To what extent does the integration of advanced real-time map fusion techniques improve CleanMAP’s scalability compared to the current confidence-driven local map selection?
- Basis in paper: [explicit] The conclusion lists the "integration of advanced real-time map fusion techniques" as a direction for future research to enhance scalability and performance.
- Why unresolved: The current framework processes confidence scores to select top-k local maps, but the paper does not evaluate the system's latency or throughput in a continuous, real-time streaming deployment scenario.
- What evidence would resolve it: Latency benchmarks and error rates demonstrating that the system maintains sub-0.32m accuracy while processing crowdsourced data streams in real-time onboard an ICV.

### Open Question 3
- Question: How do alternative vision encoders and LLM architectures compare to the EVA and LLaMA 2-7B baseline regarding lane visibility scoring accuracy?
- Basis in paper: [explicit] The authors explicitly identify the "exploration of alternative encoders and LLM architectures" as a future step to enhance reliability and performance.
- Why unresolved: The current implementation relies on a specific combination (EVA + LLaMA 2-7B), leaving the impact of different model capacities or architectural inductive biases on lane-specific visibility assessment unexplored.
- What evidence would resolve it: An ablation study reporting alignment accuracy and Mean Absolute Error (MAE) against human scores across various encoder-LLM pairs.

## Limitations
- The system's performance depends on the quality and representativeness of the pretrained MLLM for lane detection tasks, which may not generalize across diverse road conditions.
- The dynamic piecewise confidence-scoring function assumes human evaluators apply different reasoning strategies based on visibility conditions, which may not hold true across different cultural or regional contexts.
- The optimal fusion strategy (top-3 within 10% confidence range) is empirically derived and may not generalize to environments with significantly different road complexities or traffic patterns.

## Confidence
- High Confidence: The alignment with human evaluators (84.88%) and the achieved mean error (0.28m) are well-supported by the presented experiments and comparisons with baselines.
- Medium Confidence: The effectiveness of the MLLM-driven lane visibility scoring and the dynamic piecewise confidence-scoring function is supported by the experiments, but the lack of direct comparisons with alternative scoring methods introduces some uncertainty.
- Low Confidence: The generalizability of the optimal fusion strategy (top-3 within 10% confidence range) to diverse real-world scenarios and the potential for systematic miscalibration of confidence scores in different environmental conditions.

## Next Checks
1. **Cross-Validation on Diverse Datasets**: Validate the system's performance on crowdsourced HD map data from multiple cities, countries, and road types to assess generalizability and identify potential biases in the MLLM-based scoring.
2. **Comparison with Alternative Scoring Methods**: Conduct a thorough comparison of the MLLM-driven lane visibility scoring and dynamic piecewise confidence-scoring function against other state-of-the-art quality assessment methods for crowdsourced mapping data.
3. **Robustness Testing Under Extreme Conditions**: Evaluate the system's performance under extreme weather conditions (e.g., heavy rain, snow, fog) and lighting scenarios (e.g., nighttime, sunrise/sunset) to identify potential failure modes and calibrate confidence scores accordingly.