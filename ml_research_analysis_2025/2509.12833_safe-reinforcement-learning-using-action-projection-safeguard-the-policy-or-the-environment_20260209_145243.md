---
ver: rpa2
title: 'Safe Reinforcement Learning using Action Projection: Safeguard the Policy
  or the Environment?'
arxiv_id: '2509.12833'
source_url: https://arxiv.org/abs/2509.12833
tags:
- policy
- sp-rl
- action
- se-rl
- safe
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper provides a formal comparison of two safe RL integration
  strategies: safe environment RL (SE-RL), where the safeguard is treated as part
  of the environment, and safe policy RL (SP-RL), where it is embedded within the
  policy through differentiable optimization layers. The authors identify action aliasing
  as a key distinction between the two approaches, where multiple unsafe actions map
  to the same safe action, causing information loss in policy gradients.'
---

# Safe Reinforcement Learning using Action Projection: Safeguard the Policy or the Environment?

## Quick Facts
- arXiv ID: 2509.12833
- Source URL: https://arxiv.org/abs/2509.12833
- Authors: Hannah Markgraf; Shamburaj Sawant; Hanna Krasowski; Lukas Schäfer; Sebastien Gros; Matthias Althoff
- Reference count: 22
- One-line primary result: Safe policy RL (SP-RL) with differentiable projection layers can match or outperform safe environment RL (SE-RL) when equipped with appropriate improvements, despite higher computational overhead.

## Executive Summary
This paper provides a formal comparison of two safe RL integration strategies: safe environment RL (SE-RL), where the safeguard is treated as part of the environment, and safe policy RL (SP-RL), where it is embedded within the policy through differentiable optimization layers. The authors identify action aliasing as a key distinction between the two approaches, where multiple unsafe actions map to the same safe action, causing information loss in policy gradients. In SE-RL, this manifests as a flat-lining critic problem where the critic cannot distinguish between unsafe actions that map to the same safe action, while in SP-RL it appears as rank-deficient Jacobians during backpropagation through the safeguard. The authors prove that optimal value functions are equivalent in both frameworks but demonstrate practical differences in learning dynamics.

## Method Summary
The paper compares SE-RL and SP-RL frameworks for safe RL using action projection as the safeguard mechanism. SE-RL treats the projection as a black-box component of the environment, while SP-RL embeds the projection layer within the policy network using differentiable optimization (cvxpylayers). The authors implement both approaches using TD3 (deterministic) and A2C (stochastic) actor-critic algorithms on three environments: Pendulum stabilization, Quadrotor stabilization (6D state), and Seeker navigation (2D obstacle avoidance). They propose improvement strategies including proportional penalty rewards for SE-RL and per-sample loss or penalty critic for SP-RL. The penalty critic estimates discounted cumulative penalties proportional to the distance between safe and unsafe actions, aligning SP-RL more closely with penalty-based SE-RL approaches.

## Key Results
- Action aliasing causes flat-lining critics in SE-RL and rank-deficient Jacobians in SP-RL, leading to distinct learning failures
- Theoretical equivalence exists between SE-RL and SP-RL when using stochastic policies with state-value critics (GAE), but not for deterministic policies
- Improved SP-RL variants with penalty critics match or outperform improved SE-RL across all tested environments
- The computational overhead of SP-RL is 2-10× higher due to sensitivity computations, though future advances may mitigate this

## Why This Works (Mechanism)

### Mechanism 1: Action Aliasing as a Shared Information Bottleneck
- Claim: Action aliasing, where multiple unsafe actions project to the same safe action, causes distinct but related learning failures in both frameworks.
- Mechanism: In SE-RL, aliasing creates a flat-lining critic that cannot distinguish between aliased actions (Lemma 2), eliminating gradients in the normal direction to the safe set boundary. In SP-RL, aliasing manifests as rank-deficient Jacobians during backpropagation through the projection layer, directly corrupting policy gradient estimates.
- Core assumption: The safe action set is convex and the projection is deterministic, ensuring a well-defined mapping from unsafe to safe actions.
- Evidence anchors:
  - [abstract]: "action aliasing...causing information loss in the policy gradients"
  - [section 6.3]: "any components in the normal direction to the boundary vanish...leading to a rank-deficient Jacobian"
  - [corpus]: Related work on "Leveraging Constraint Violation Signals" explicitly mentions zero-gradient problems in projection-based ACRL methods.

### Mechanism 2: Theoretical Equivalence Under Specific Conditions
- Claim: For stochastic policies using Generalized Advantage Estimation (GAE) with state-value critics, SE-RL and SP-RL produce identical parameter updates and converge to the same solution.
- Mechanism: When learning a state value function v(s) rather than an action-value function q(s,a), the critic does not depend on the action, neutralizing the structural differences between SE-RL and SP-RL. The policy gradient reduces to the same expectation in both cases (Lemma 1).
- Core assumption: Both frameworks start with identical initial parameters θ₀ and φ₀ and use the same hyperparameters.
- Evidence anchors:
  - [abstract]: "optimal value functions are equivalent in both frameworks"
  - [section 6.2]: "for any initial parameters...the parameter updates...are identical in both the SE-RL and SP-RL framework"
  - [corpus]: Weak direct evidence; corpus focuses on different safety mechanisms (shields, graph-based policies) not this equivalence.

### Mechanism 3: Penalty Critic for SP-RL Aligns Learning Dynamics
- Claim: A dedicated penalty critic trained on unsafe actions provides principled long-term gradient signals, enabling SP-RL to match or outperform improved SE-RL.
- Mechanism: The penalty critic estimates the discounted cumulative penalty (Eq. 29), providing a gradient term that pulls the policy away from actions that will trigger the safeguard. This aligns SP-RL's objective with the modified objective of penalty-based SE-RL (Eq. 25), re-establishing their theoretical connection.
- Core assumption: The penalty function h (e.g., distance between safe and unsafe action) is differentiable and correlates with the true safety violation cost.
- Evidence anchors:
  - [section 7.2]: "we suggest an alternative that consists of training an additional critic conditioned on the unsafe policy"
  - [section 8.2.2]: "statistical analysis...confirmed that compared to TD3-SE-RL with penalties, one of the improved SP-RL versions always performs on par or better"
  - [corpus]: "Leveraging Constraint Violation Signals" uses a similar philosophy of extracting signal from constraint violations but applies it differently.

## Foundational Learning

**Concept**: Policy Gradient Theorem and Actor-Critic Algorithms
- Why needed here: The entire analysis rests on how gradients flow differently in SE-RL vs. SP-RL through the policy gradient estimator.
- Quick check question: Can you derive the deterministic policy gradient ∇θJ(π) using the chain rule through a projection layer?

**Concept**: Differentiable Optimization Layers (e.g., cvxpylayers)
- Why needed here: SP-RL requires backpropagating through a constrained optimization problem (the projection), which necessitates implicit differentiation via the KKT conditions.
- Quick check question: Explain why the Jacobian ∇uΦ becomes rank-deficient at the boundary of the safe set.

**Concept**: Generalized Advantage Estimation (GAE)
- Why needed here: This is the specific algorithmic choice that creates practical equivalence between SE-RL and SP-RL for stochastic policies by using a state-value critic.
- Quick check question: Why does using v(s) instead of q(s,a) make the critic immune to the action-aliasing problem?

## Architecture Onboarding

**Component map**:
SE-RL: Policy Network → (Unsafe Action) → Environment + Projection (black-box) → Reward & State → Critic (learns q(s,a) or v(s)) → Policy Update (via sampled actions).
SP-RL: Policy Network → (Unsafe Action) → Differentiable Projection Layer → (Safe Action) → Environment → Reward & State → Critic (learns q(s,a_safe) or v(s)) → Policy Update (via analytic gradient through projection).

**Critical path**: The key difference is whether the projection operation is in the backward pass. In SP-RL, the projection's sensitivity (∇uΦ) is explicitly computed; in SE-RL, it's implicitly learned by the critic through environmental interaction.

**Design tradeoffs**:
- SE-RL: Pros—Simple, no modification to RL algorithm, retains original theoretical guarantees. Cons—Suffers from flat-lining critic; learning depends on critic's ability to approximate projection effects.
- SP-RL: Pros—Direct gradient information through projection sensitivities. Cons—2-10× computational overhead; susceptible to zero-gradient problem; more complex implementation.

**Failure signatures**:
- SE-RL (deterministic): Policy converges to an unsafe action that lies in the normal cone to the safe set boundary (Figure 2b).
- SP-RL (deterministic): Policy gets stuck on a vertex of the safe action set due to rank-deficient Jacobians (Figure 6c); high variance or non-convergence during training.
- Remedied SP-RL: If the penalty critic is used but the optimal safe action is on the boundary (not interior), it may be outperformed by the simpler per-sample loss (e.g., in navigation tasks).

**First 3 experiments**:
1. **Baseline Equivalence Check**: Run A2C (stochastic policy with GAE) on Pendulum with both SE-RL and SP-RL to verify they produce identical learning curves and final returns, confirming Lemma 1.
2. **Determine Policy Type Sensitivity**: Run TD3 (deterministic policy) on Quadrotor to observe how vanilla SE-RL outperforms vanilla SP-RL, highlighting the zero-gradient problem's severity.
3. **Mitigation Strategy Comparison**: On the Seeker navigation task, compare SP-RL with per-sample loss vs. SP-RL with penalty critic to understand when each improvement strategy is preferable (task performance vs. safety alignment).

## Open Questions the Paper Calls Out

**Open Question 1**
- Question: How do SE-RL and SP-RL compare when extended to actor-critic algorithms with additional modifications such as PPO's clipping or SAC's entropy regularization?
- Basis in paper: [explicit] The authors state "we limit our analysis to RL algorithms derived directly from the policy gradient theorem, as including methods with additional modifications (e.g., PPO, Soft-Actor Critic (SAC)) would require extensive theoretical derivations beyond the scope of this paper."
- Why unresolved: The theoretical equivalence established for stochastic policies relies on specific gradient estimators; clipping or entropy terms may interact differently with the flat-lining critic versus rank-deficient Jacobian problems.
- What evidence would resolve it: A comparative study extending the formal analysis to PPO and SAC, examining whether the equivalence in Lemma 1 holds and empirically comparing performance on the same benchmark environments.

**Open Question 2**
- Question: Can the penalty scaling factor w be automatically adapted during training rather than treated as a fixed hyperparameter?
- Basis in paper: [explicit] "The weighting factor w is an important hyperparameter. Therefore, we do not tune it, but instead test for different choices" and results show performance varies significantly across w values with no single optimal choice across environments.
- Why unresolved: Different environments and task-safety alignments require different penalty strengths, but manual tuning is impractical for real-world deployment.
- What evidence would resolve it: Development and evaluation of an adaptive w scheme (e.g., based on safeguard intervention frequency or constraint violation severity) demonstrating consistent performance across diverse tasks without manual tuning.

**Open Question 3**
- Question: How do emerging efficient sensitivity computation methods affect the computational overhead trade-off between SE-RL and SP-RL?
- Basis in paper: [explicit] "improved SP-RL variants typically match or exceed SE-RL performance for deterministic policies, but incur 2−10 times computational overhead due to sensitivity computations via cvxpylayers. While our evaluation excludes detailed wall-clock analysis due to software-specific variations, future advances in sensitivity computation (Frey et al., 2025; Nguyen & Donti, 2025) may mitigate this overhead."
- Why unresolved: The practical viability of SP-RL depends on whether computational advances can reduce the sensitivity computation bottleneck to make overhead negligible.
- What evidence would resolve it: Integration of FSNet or differentiable NMPC approaches into the SP-RL framework with wall-clock timing comparisons showing overhead reduction to within 50% of SE-RL.

## Limitations
- Theoretical equivalence claims rely on specific algorithmic choices (GAE with state-value critics) that may not generalize to other policy gradient methods
- Computational overhead of SP-RL (2-10×) is substantial but exact scaling with problem complexity is not characterized
- Empirical evaluation uses only three environments, limiting generalizability to other safety-critical domains

## Confidence

**Major Uncertainties and Limitations:**
The paper's theoretical claims rely heavily on specific algorithmic choices (GAE with state-value critics) that may not generalize to other policy gradient methods or value-function types. The computational overhead of SP-RL (2-10×) is substantial but the exact scaling relationship with problem complexity is not characterized. The proposed penalty critic for SP-RL requires additional hyperparameter tuning (penalty weight w) that may be problem-dependent. The empirical evaluation uses only three environments, limiting generalizability to other safety-critical domains.

**Confidence Labels:**
- **High**: Theoretical equivalence results (Lemma 1, 2) and their conditions; the action aliasing problem description and its manifestations in both frameworks.
- **Medium**: Empirical superiority claims of improved SP-RL over improved SE-RL; practical significance of the flat-lining critic vs. rank-deficient Jacobian distinction.
- **Low**: Generalization of results to other safe RL settings (different constraint types, continuous vs discrete action spaces, non-convex safe sets).

## Next Checks

1. **Cross-Algorithm Validation**: Test the theoretical equivalence (Lemma 1) with PPO (using GAE) and DDPG (using action-value critics) to determine how robust the equivalence is to different actor-critic architectures.

2. **Constraint Structure Sensitivity**: Evaluate both frameworks on problems with non-convex safe sets or state-dependent constraints that cannot be expressed as zonotopes to test the robustness of the proposed improvements.

3. **Computational Overhead Analysis**: Characterize the exact computational scaling of SP-RL with respect to state and action dimensionality by testing on higher-dimensional variants of the Quadrotor task.