---
ver: rpa2
title: Relational Knowledge Distillation Using Fine-tuned Function Vectors
arxiv_id: '2601.08169'
source_url: https://arxiv.org/abs/2601.08169
tags:
- function
- relations
- relation
- vector
- word
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method to distill and refine relational
  knowledge from large language models using function vectors, which are compact representations
  of task-specific information. By fine-tuning these vectors with small amounts of
  data, the authors improve performance on relation-based word completion tasks and
  better align the model's relational representations with human judgments.
---

# Relational Knowledge Distillation Using Fine-tuned Function Vectors

## Quick Facts
- arXiv ID: 2601.08169
- Source URL: https://arxiv.org/abs/2601.08169
- Reference count: 31
- Key outcome: Improves relational reasoning in LLMs via fine-tuned function vectors for analogies and word completion.

## Executive Summary
This paper introduces a method to distill and refine relational knowledge from large language models using function vectors, which are compact representations of task-specific information. By fine-tuning these vectors with small amounts of data, the authors improve performance on relation-based word completion tasks and better align the model's relational representations with human judgments. Additionally, the study proposes composite function vectors, which are weighted combinations of fine-tuned vectors, to solve challenging analogy problems involving novel relations. The approach demonstrates significant gains in accuracy, especially for cross-domain analogies, across both small and large language models, highlighting the potential of activation patching for interpretable relational reasoning in LLMs.

## Method Summary
The authors propose using function vectors—compact, task-specific representations—to distill relational knowledge from large language models (LLMs). These function vectors are fine-tuned with small datasets to enhance performance on relation-based tasks like word completion. To tackle analogies with novel relations, composite function vectors (CFVs) are created by combining fine-tuned vectors with learned weights. The method leverages activation patching to inject refined relational knowledge into LLMs, improving their ability to reason about complex relational structures. Evaluations show significant accuracy gains, particularly for cross-domain analogies, while also enabling interpretable relational reasoning through vector arithmetic.

## Key Results
- Composite function vectors improve cross-domain analogy performance across both small and large language models.
- Fine-tuning function vectors with minimal data enhances alignment with human relational judgments.
- The method enables interpretable relational reasoning through activation patching, demonstrating gains in word completion tasks.

## Why This Works (Mechanism)
The method works by distilling relational knowledge from LLMs into compact function vectors, which are then fine-tuned to better capture task-specific relationships. Composite function vectors combine these fine-tuned representations to solve analogies involving novel relations. Activation patching injects the refined knowledge back into the model, enabling improved relational reasoning. The approach leverages the linear properties of vector spaces to compose and manipulate relational representations, aligning model behavior with human judgments.

## Foundational Learning
- **Function Vectors**: Compact, task-specific representations derived from LLM activations. *Why needed*: Enable efficient distillation and manipulation of relational knowledge. *Quick check*: Verify that function vectors capture meaningful relational information through vector arithmetic.
- **Activation Patching**: Technique to inject fine-tuned function vectors into LLMs. *Why needed*: Allows targeted refinement of relational reasoning without retraining the full model. *Quick check*: Confirm that patched models outperform baseline on relational tasks.
- **Composite Function Vectors**: Weighted combinations of fine-tuned vectors for novel relations. *Why needed*: Extends the method to analogies involving unseen relational structures. *Quick check*: Test CFVs on cross-domain analogy benchmarks.
- **Vector Arithmetic**: Manipulating relational representations via linear combinations. *Why needed*: Enables composition and refinement of relational knowledge. *Quick check*: Validate that vector operations preserve relational consistency.
- **Relational Knowledge Distillation**: Extracting and refining relational knowledge from LLMs. *Why needed*: Improves model alignment with human judgments. *Quick check*: Compare model performance before and after distillation.

## Architecture Onboarding

### Component Map
LLM -> Function Vector Extraction -> Fine-tuning Module -> Composite Function Vector Generation -> Activation Patching -> Refined Model Output

### Critical Path
The critical path involves extracting function vectors from LLM activations, fine-tuning them with small datasets, generating composite function vectors for novel relations, and injecting them back into the model via activation patching to produce improved relational reasoning outputs.

### Design Tradeoffs
- **Small vs. Large Models**: Fine-tuning works across model sizes but may yield different gains.
- **Data Efficiency**: Minimal fine-tuning data reduces computational cost but risks overfitting.
- **Interpretability vs. Performance**: Activation patching enables interpretability but may not always maximize accuracy.

### Failure Signatures
- Overfitting during fine-tuning with small datasets.
- Poor generalization of composite function vectors to unseen relational structures.
- Suboptimal activation patching leading to degraded performance on near-analogies.

### First Experiments to Run
1. Test composite function vectors on multi-hop reasoning tasks to evaluate broader applicability.
2. Analyze overfitting risk by varying fine-tuning dataset sizes and measuring generalization gaps.
3. Conduct ablation studies comparing activation patching interpretability to other explainable AI methods.

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can the composite function vector framework be effectively extended to support analogical reasoning between narratives and stories containing complex relational structures?
- Basis in paper: [explicit] The Conclusion section states that the current study is restricted to four-term problems and suggests that "Future work should expand the framework to analogies between narratives and stories."
- Why unresolved: The current method relies on single-word tokens and pairwise relations; it is unclear if sentence-level context can be incorporated into function vectors without losing the linear properties that make them composable.
- What evidence would resolve it: Successful application of composite function vectors to story-level analogy benchmarks, demonstrating that sentence embeddings can be manipulated to preserve relational consistency across narratives.

### Open Question 2
- Question: Do large language models naturally acquire hierarchical structures of relational representations (e.g., subtypes of whole-part relations) within their vector space?
- Basis in paper: [explicit] The Conclusion notes a limitation regarding the "hierarchical nature of human relational representations" and asks if "LLMs naturally acquire these hierarchical structures... or whether additional alignment is required."
- Why unresolved: While the paper shows FFVs separate semantic from syntactic relations (Appendix A.6), it does not investigate if fine-grained hierarchies (e.g., distinguishing object-component from collection-member) emerge naturally in the vector geometry.
- What evidence would resolve it: Probing tasks demonstrating that vector arithmetic can capture taxonomic relationships between relations (e.g., $v_{whole-part} \approx v_{object-component} + \Delta$), or showing that current vectors fail to distinguish subtypes without specific fine-tuning.

### Open Question 3
- Question: Why does the injection of composite function vectors fail to improve—or occasionally hinder—performance on semantically near analogies?
- Basis in paper: [inferred] Section 4.3 observes that composite function vectors (CFVs) did not substantially affect performance on near-analogy problems, hypothesizing that "strategies based on simple semantic associations may be deemphasized."
- Why unresolved: The paper identifies the performance plateau on near-analogies but does not isolate whether this is due to interference between the injected vector and the model's pre-existing semantic heuristics or a saturation of the model's capacity for the task.
- What evidence would resolve it: Ablation studies measuring the activation norms of semantic association circuits during CFV injection, or experiments that modulate the CFV weight to find an optimal balance between relational steering and semantic association.

## Limitations
- The method relies on pre-trained relational knowledge from LLMs, constraining performance by the quality of that knowledge.
- Fine-tuning function vectors with small data is efficient but risks overfitting, especially for rare relations.
- Composite function vectors improve cross-domain analogies but may not generalize to all relational structures.
- Most evaluations focus on word completion and analogy tasks, leaving unclear whether gains transfer to other domains like QA or code.
- The interpretability benefits of activation patching are promising but not quantitatively validated.

## Confidence
- **Method Effectiveness**: Medium
- **Generalization Beyond Tested Tasks**: Low
- **Interpretability Claims**: Low

## Next Checks
1. Test composite function vectors on broader relational tasks such as multi-hop reasoning and graph link prediction.
2. Analyze overfitting risk by varying fine-tuning dataset sizes and measuring generalization gaps.
3. Conduct ablation studies comparing activation patching interpretability to other explainable AI methods.