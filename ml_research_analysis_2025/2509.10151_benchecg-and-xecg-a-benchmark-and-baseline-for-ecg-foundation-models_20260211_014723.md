---
ver: rpa2
title: 'BenchECG and xECG: a benchmark and baseline for ECG foundation models'
arxiv_id: '2509.10151'
source_url: https://arxiv.org/abs/2509.10151
tags:
- xecg
- tasks
- across
- learning
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BenchECG, the first comprehensive benchmark
  for evaluating ECG foundation models across diverse datasets, signals, and tasks.
  The authors also propose xECG, a novel foundation model based on xLSTM architecture
  combined with SimDINOv2 self-supervised learning.
---

# BenchECG and xECG: a benchmark and baseline for ECG foundation models

## Quick Facts
- arXiv ID: 2509.10151
- Source URL: https://arxiv.org/abs/2509.10151
- Authors: Riccardo Lunelli; Angus Nicolson; Samuel Martin Pröll; Sebastian Johannes Reinstadler; Axel Bauer; Clemens Dlaska
- Reference count: 40
- This paper introduces BenchECG, the first comprehensive benchmark for evaluating ECG foundation models across diverse datasets, signals, and tasks. The authors also propose xECG, a novel foundation model based on xLSTM architecture combined with SimDINOv2 self-supervised learning. xECG outperforms existing publicly available state-of-the-art models across the benchmark, achieving the best overall BenchECG score (0.868±0.003) and demonstrating superior performance particularly on long-context tasks.

## Executive Summary
This paper introduces BenchECG, the first comprehensive benchmark for evaluating ECG foundation models across diverse datasets, signals, and tasks. The authors also propose xECG, a novel foundation model based on xLSTM architecture combined with SimDINOv2 self-supervised learning. xECG outperforms existing publicly available state-of-the-art models across the benchmark, achieving the best overall BenchECG score (0.868±0.003) and demonstrating superior performance particularly on long-context tasks. The model shows strong generalisation across different populations and tasks while being more computationally efficient than transformer-based alternatives, establishing a new baseline for future ECG foundation model development.

## Method Summary
The method introduces BenchECG, a benchmark of 10 tasks (classification, segmentation, detection, regression, survival analysis) across 8 datasets, and xECG, an xLSTM-based foundation model. xECG uses a 9-layer bidirectional architecture with alternating sLSTM/mLSTM blocks, pretrained via SimDINOv2 self-supervised learning on ~8M ECGs from CODE, INCART, Chapman, and Ningbo datasets. The model processes 100Hz resampled signals in 250ms non-overlapping patches. Evaluation uses linear probing and finetuning across the benchmark tasks, with performance measured by AUROC, F1, SMAPE, and concordance index depending on task type.

## Key Results
- xECG achieves the best overall BenchECG score (0.868±0.003) compared to publicly available state-of-the-art models
- Superior performance on long-context tasks: 0.932±0.014 AUROC on sleep apnea vs. 0.702±0.020 for ST-MEM (p=0.0000001)
- Strong cross-population generalization: PPG atrial fibrillation AUROC of 0.751±0.013 (linear probe) without PPG pretraining

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: xLSTM architectures provide computational and representational advantages for long-context ECG tasks compared to transformer-based alternatives.
- **Mechanism**: xLSTM combines scalar memory (sLSTM) and matrix memory (mLSTM) blocks with exponential gating, enabling linear scaling with sequence length while maintaining parallelizable training. Bidirectional processing captures forward and backward temporal dependencies without quadratic attention costs.
- **Core assumption**: Long-range temporal dependencies in ECG signals (minutes to hours) are better captured by recurrent structures with extended memory capacity than by fixed-context transformers.
- **Evidence anchors**:
  - [abstract] "xECG...demonstrating superior performance particularly on long-context tasks"
  - [section: Results] xECG achieved 0.932±0.014 AUROC on sleep apnea vs. 0.702±0.020 for ST-MEM (p=0.0000001)
  - [corpus] Related work (TiRex, xLSTM-ECG) shows xLSTM success in forecasting and ECG classification, but corpus lacks direct comparisons on long-context medical signals
- **Break condition**: If transformer variants with efficient attention (e.g., linear attention, state-space models) match xLSTM on long-context benchmarks while offering easier deployment, the advantage diminishes.

### Mechanism 2
- **Claim**: SimDINOv2 self-supervised pretraining yields transferable ECG representations across diverse tasks and modalities.
- **Mechanism**: A teacher-student distillation framework where: (1) student matches teacher representations across multi-view augmentations (global/local temporal crops), (2) patch-level reconstruction loss forces local feature learning, (3) coding rate regularizer prevents feature collapse. EMA-updated teacher provides stable targets without requiring negative samples.
- **Core assumption**: Multi-view consistency learning captures physiologically meaningful features that generalize across recording conditions, populations, and related biosignals (e.g., PPG).
- **Evidence anchors**:
  - [abstract] "xECG achieves the best BenchECG score compared to publicly available state-of-the-art models"
  - [section: Results] xECG linear probing rank: 1.2 across all tasks; PPG AF AUROC 0.751±0.013 (linear probe) without PPG pretraining
  - [corpus] CLEF and ECG-Soup papers use contrastive/JEPA approaches for ECG SSL, but no direct SimDINOv2 comparisons exist in corpus
- **Break condition**: If task-specific supervised pretraining outperforms SSL on target tasks, or if representations fail to transfer to unseen modalities, the generalization claim weakens.

### Mechanism 3
- **Claim**: Bidirectional alternating sLSTM/mLSTM architecture improves ECG feature quality over homogeneous designs.
- **Mechanism**: Alternating block types (s, s, m, m, s, s, m, m, s) process sequences bidirectionally—each layer has one forward and one backward block. sLSTM provides scalar memory with flexible gating; mLSTM offers matrix memory for richer storage. This mix balances capacity and efficiency.
- **Core assumption**: ECG signals benefit from both fine-grained local memory (beat morphology) and broader context (rhythm patterns), which different memory mechanisms capture.
- **Evidence anchors**:
  - [section: xECG Architecture] "recent evidence suggests a mixed block architecture improves ECG modelling [39]"
  - [section: Methods] Nine alternating blocks with bidirectional processing per layer
  - [corpus] Corpus lacks direct ablations comparing sLSTM-only vs. mLSTM-only vs. mixed architectures for ECG
- **Break condition**: If ablation studies show minimal difference between mixed and homogeneous designs, the architectural complexity may not be justified.

## Foundational Learning

- **Concept: xLSTM (Extended Long Short-Term Memory)**
  - **Why needed here**: Core backbone of xECG; differs from standard LSTM via matrix memory, exponential gating, and parallelizable training.
  - **Quick check question**: Can you explain how mLSTM's matrix memory differs from standard LSTM's scalar cell state, and why this matters for long sequences?

- **Concept: Self-Distillation with No Labels (DINO)**
  - **Why needed here**: SimDINOv2 builds on DINO; understanding EMA teacher-student dynamics and collapse avoidance is essential.
  - **Quick check question**: How does the coding rate regularizer prevent representation collapse without contrastive negatives?

- **Concept: Foundation Model Evaluation Paradigms**
  - **Why needed here**: BenchECG introduces standardized comparison; linear probing vs. finetuning reveal different aspects of representation quality.
  - **Quick check question**: Why might a model excel at linear probing but show smaller gains from finetuning, and what does this indicate about pretrained features?

## Architecture Onboarding

- **Component map**:
  Raw ECG -> resample to 100Hz -> temporal patches (250ms each) -> linear projection -> 9 alternating sLSTM/mLSTM blocks (bidirectional) -> task-specific heads

- **Critical path**:
  1. Patch embedding dimension (E) and patch size (Ps=25 samples at 100Hz) determine token count
  2. Block configuration (s/m alternation pattern) affects memory and capacity
  3. View generation strategy during pretraining shapes feature generality

- **Design tradeoffs**:
  - Longer patches → fewer tokens, faster training, but coarser temporal resolution (problematic for detection tasks)
  - More mLSTM blocks → higher capacity, but more parameters and memory
  - Aggressive augmentation → better robustness, but risks removing diagnostic signal

- **Failure signatures**:
  - Poor performance on detection tasks (R-peak, segmentation) → patch size too large or temporal aggregation too aggressive
  - Representation collapse (uniform outputs) → coding rate regularizer too weak or EMA momentum too low
  - Weak long-context performance → model may not be processing full sequences; check windowing

- **First 3 experiments**:
  1. **Ablate block composition**: Compare sLSTM-only vs. mLSTM-only vs. alternating on sleep apnea task to validate mixed design.
  2. **Probe patch size sensitivity**: Test 125ms, 250ms (default), 500ms patches on R-peak detection (20ms tolerance) to find resolution threshold.
  3. **Cross-population stress test**: Finetune on CODE-15% (Brazil), evaluate on PTB-XL (Germany) and CPSC2018 (China) to quantify generalization gap vs. baselines.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the SimDINOv2 pretraining methodology outperform alternatives like Masked Autoencoders (MAE) when pretraining is performed on identical datasets?
- Basis in paper: [explicit] The authors state that substantial differences in pretraining datasets among models "limits claims on which pretraining methodology is most suitable for ECG feature learning" and explicitly call for "direct comparisons of pretraining methods using the same pretraining data."
- Why unresolved: The paper compares models trained on different datasets (HEEDB, CODE, CODE-15%), making it impossible to disentangle the effects of data volume vs. pretraining objective.
- What evidence would resolve it: Ablation studies where xECG, ST-MEM, and ECG-JEPA are pretrained on the same corpus (e.g., the CODE dataset) and evaluated on BenchECG.

### Open Question 2
- Question: Do high BenchECG scores on technical metrics, such as mortality risk prediction, correlate with clinical utility or improved patient safety in prospective settings?
- Basis in paper: [explicit] The Discussion notes the "evaluation focuses on technical metrics and does not directly assess clinical utility or safety," which would require prospective validation.
- Why unresolved: A model might score high on AUROC but fail in deployment due to data leakage, population shifts, or unmeasured clinical factors not captured in retrospective benchmarks.
- What evidence would resolve it: Prospective clinical trials assessing whether xECG risk predictions improve clinical decision-making compared to standard care.

### Open Question 3
- Question: Is the superior performance of xLSTMs on long-context tasks inherent to the recurrent architecture, or does it stem from the inability of the evaluated transformers to process inputs longer than 10 seconds?
- Basis in paper: [inferred] The paper highlights that xLSTMs scale linearly with length while transformers scale quadratically; however, it notes that the transformer baselines were "limited to fixed 10s inputs" whereas xECG processed full signals.
- Why unresolved: The comparison confounds architectural efficiency with input duration; transformers might perform better on long tasks if efficient attention mechanisms were utilized to process longer windows.
- What evidence would resolve it: BenchECG evaluation of transformer models adapted with efficient attention (e.g., linear attention or sparse transformers) to ingest the same long-duration signals as xECG.

## Limitations
- Dependency on access-controlled datasets (CODE and MIMIC-IV-ECG) restricts independent validation
- Architectural advantage of xLSTM over transformers on long-context tasks remains inferential rather than empirically established
- Generalization benefits across populations are supported but not exhaustively validated through systematic out-of-distribution testing

## Confidence
- **High Confidence**: xLSTM architecture implementation details, SimDINOv2 pretraining pipeline specifications, and BenchECG benchmark construction methodology
- **Medium Confidence**: Relative performance gains of xECG over existing baselines and generalization benefits across populations
- **Low Confidence**: Superiority of xLSTM specifically for long-context medical signals compared to alternative efficient architectures

## Next Checks
1. Implement and evaluate transformer variants with efficient attention (e.g., linear attention, state-space models) on the sleep apnea task to quantify whether xLSTM's linear scaling advantage is unique or shared by other architectures.

2. Conduct systematic ablation studies varying the pretraining data composition (CODE only, Chapman only, mixed) to determine whether the reported generalization benefits stem from dataset diversity or pretraining methodology.

3. Extend cross-population evaluation to explicitly test demographic generalization by training on CODE and evaluating on datasets stratified by age, sex, and ethnicity to identify potential performance disparities.