---
ver: rpa2
title: Equivariant Neural Networks for General Linear Symmetries on Lie Algebras
arxiv_id: '2510.22984'
source_url: https://arxiv.org/abs/2510.22984
tags:
- equivariant
- reln
- learning
- form
- matrix
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Reductive Lie Neurons (ReLNs), a neural architecture
  that achieves exact equivariance to the adjoint action of the general linear group
  GL(n) on its Lie algebra gl(n). The key innovation is a non-degenerate Ad-invariant
  bilinear form that resolves degeneracy issues of the Killing form on reductive algebras,
  enabling stable invariant gating and expressive nonlinearities in a single framework.
---

# Equivariant Neural Networks for General Linear Symmetries on Lie Algebras

## Quick Facts
- arXiv ID: 2510.22984
- Source URL: https://arxiv.org/abs/2510.22984
- Reference count: 40
- One-line result: Introduces Reductive Lie Neurons (ReLNs) achieving exact equivariance to GL(n) adjoint action on gl(n) Lie algebras using a non-degenerate Ad-invariant bilinear form.

## Executive Summary
This paper introduces Reductive Lie Neurons (ReLNs), a neural architecture that achieves exact equivariance to the adjoint action of the general linear group GL(n) on its Lie algebra gl(n). The key innovation is a non-degenerate Ad-invariant bilinear form that resolves degeneracy issues of the Killing form on reductive algebras, enabling stable invariant gating and expressive nonlinearities in a single framework. ReLNs unify processing of heterogeneous geometric inputs (vectors, matrices, covariances) by embedding them into a shared Lie-algebraic space, avoiding the need for separate specialized architectures. Across multiple benchmarks—including algebraic tasks on sl(3) and sp(4), uncertainty-aware drone state estimation, 3D Gaussian splat learning, and the EMLP double-pendulum dynamics benchmark—ReLNs match or outperform strong equivariant and self-supervised baselines while using substantially fewer parameters and compute, improving the accuracy-efficiency trade-off and providing a practical, reusable backbone for learning with broad linear symmetries.

## Method Summary
ReLNs achieve equivariance by operating directly on the Lie algebra gl(n) using a non-degenerate Ad-invariant bilinear form eB that overcomes the degeneracy of the Killing form on reductive algebras. The architecture consists of ReLN-Linear (channel mixing), ReLN-ReLU (invariant-gated activation), and ReLN-Bracket (commutator) layers. Geometric inputs like vectors and covariances are unified by embedding them into gl(n) via the hat map and matrix logarithm, respectively. The framework enables learning equivariant functions on gl(n) without decomposing into irreps, providing a more efficient alternative to EMLP while supporting heterogeneous geometric features.

## Key Results
- Matches or outperforms EMLP and self-supervised baselines on double-pendulum dynamics while using 11× fewer FLOPs
- Improves 3D Gaussian splatting accuracy by 1.5× over state-of-the-art methods
- Achieves superior uncertainty-aware drone state estimation with 15% reduction in ATE vs. velocity-only models
- Maintains exact equivariance under rotated camera views for Platonic solid classification

## Why This Works (Mechanism)

### Mechanism 1: Non-Degenerate Ad-Invariant Bilinear Form
ReLNs construct a modified bilinear form eB that remains non-degenerate where the Killing form fails on reductive algebras. The Killing form B(X,Y) = tr(ad_X ∘ ad_Y) degenerates because ad_Z = 0 for all Z in the center z(g). ReLNs define eB(Z₁+X₁, Z₂+X₂) := ⟨Z₁,Z₂⟩_z + B(X₁,X₂), preserving non-degeneracy. For gl(n), this yields eB(X,Y) = 2n·tr(XY) - tr(X)tr(Y).

### Mechanism 2: Unified Lifting of Heterogeneous Geometric Inputs
Vectors v ∈ R³ lift to so(3) ⊂ gl(3) via the hat map (v → v̂ where v̂w = v × w). Covariances C ∈ SPD(3) map to Sym(3) ⊂ gl(3) via matrix logarithm. Both transform under Ad_R(X) = RXR^⊤ under rotations R ∈ SO(3), which matches congruence for symmetric matrices. The network operates entirely in this unified gl(3) space.

### Mechanism 3: Equivariant Gating via Invariant Scalar Contraction
ReLNs construct equivariant nonlinearities by gating feature updates with Ad-invariant scalars derived from eB. For input features x ∈ R^(K×C), compute reference direction d = xW. The gate condition eB(x̂_c, d̂_c) ≤ 0 is invariant because eB is Ad-invariant. Features update as x'_c = x_c + σ(eB(x̂_c, d̂_c))·d_c when the gate activates, where σ is ReLU.

## Foundational Learning

- **Reductive vs. Semisimple Lie Algebras**: Why needed: The paper extends equivariant architectures from semisimple (where Killing form works) to reductive algebras (where it doesn't). Quick check: On gl(3), is the Killing form non-degenerate? (Answer: No—it vanishes on the center RI.)

- **Adjoint Action and Equivariance**: Why needed: ReLNs enforce equivariance to Ad_g(X) = gXg^(-1). Quick check: Does a channel-wise linear map xW commute with Ad_g? (Answer: Yes—group acts on geometric dimension, weights on channel dimension.)

- **SPD Manifold and Matrix Logarithm**: Why needed: Covariance inputs live on SPD(3), a curved manifold. The log map C → log C embeds into flat Sym(3) ⊂ gl(3) while preserving SO(3)-equivariance. Quick check: Why does log(RCR^⊤) = R(log C)R^⊤ hold? (Answer: Spectral decomposition—rotation conjugates eigenvectors but not eigenvalues.)

## Architecture Onboarding

- **Component map**: Input → Lift to gl(n) → Stack (ReLN-Linear → ReLN-ReLU → ReLN-Bracket)×L → Project to output space
- **Critical path**: Geometric lifting → ReLN layers → Output projection. For velocity extraction, project output matrix to skew-symmetric component: ṽ = (½(A - A^⊤))^∨
- **Design tradeoffs**: ReLNs avoid irrep decomposition but require n²-dimensional features per channel. For n=3, K=9, which is larger than vector-only methods but enables matrix-valued input. ReLNs trade flexibility (only reductive algebras) for efficiency (11× fewer FLOPs in HNN benchmark).
- **Failure signatures**: Degeneracy in eB if center inner product is poorly conditioned; non-orthogonal transforms may violate physical congruence rule; high n, low C leads to underfitting with small effective width
- **First 3 experiments**: 1) Platonic solid classification (sl(3)): Verify adjoint equivariance on semisimple subgroup. Input: 3×3 homography matrix. Expected: >99% accuracy under rotated camera views. 2) Velocity–covariance fusion (SO(3)): Lift v to so(3), log(C) to Sym(3). Compare (v, C) vs. (v, log C) vs. velocity-only. Expected: log C improves ATE by ~15% over velocity-only; equivariance holds under test-time SO(3) rotations. 3) Double pendulum HNN (O(2)/SO(2)/D₆): Replace EMLP layers with ReLN in Hamiltonian network. Expected: Match EMLP accuracy with ~10× fewer FLOPs; same architecture across symmetry groups.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can ReLNs be extended to handle groups with translations, such as SE(3) or affine symmetries?
- Basis: Section 6 states a further direction is to extend the framework to groups with translations by leveraging their semidirect-product structure and induced actions.
- Why unresolved: The current architecture is restricted to the linear GL(n) adjoint action and does not natively support translation equivariance without structural modification.
- What evidence would resolve it: A modified ReLN architecture that strictly preserves equivariance for semidirect products like SE(3) on the Lie algebra level.

### Open Question 2
- Question: Can the framework be adapted to reconcile covariance congruence transformations with the adjoint action for general GL(n) coordinate changes?
- Basis: Section 6 notes that while adjoint action matches congruence for rigid rotations, extending measurement–uncertainty coupling to general GL(n) changes requires "alternative liftings or invariants."
- Why unresolved: The unified lifting currently relies on the isomorphism between congruence and adjoint action, which breaks down for non-orthogonal general linear transformations.
- What evidence would resolve it: The derivation of a specific lifting map or non-degenerate bilinear form that remains equivariant under congruence Σ → AΣA^⊤ for arbitrary invertible A.

### Open Question 3
- Question: Under what specific conditions does the learned velocity–covariance fusion in ReLNs approximate classical uncertainty-adaptive integration?
- Basis: Section 5.2.1 identifies as a "natural direction" the characterization of when the learned fusion approximates uncertainty-adaptive integration.
- Why unresolved: While empirical results show effective uncertainty-aware estimation, the theoretical link between the network's learned fusion mechanism and classical filtering theory (e.g., Kalman gains) remains undefined.
- What evidence would resolve it: A theoretical analysis or empirical demonstration showing a correspondence between ReLN gating weights and the uncertainty weights found in optimal estimators.

## Limitations
- Restricted to reductive Lie algebras due to eB construction, excluding non-reductive cases like affine(n)
- Unified lifting assumes adjoint action matches congruence, which fails for general GL(n) coordinate changes
- Numerical conditioning of the center component in eB can lead to instability in the invariant gating mechanism

## Confidence
- High confidence in the eB construction and equivariance proofs for gl(n)
- Medium confidence in practical stability across diverse datasets
- Low confidence in performance guarantees for non-reductive algebras or non-orthogonal GL(n) subgroups

## Next Checks
1. Test ReLNs on a non-reductive Lie algebra (e.g., affine(n)) to characterize failure modes and potential workarounds
2. Implement coordinate transformation robustness tests where input features undergo general GL(n) transforms to verify preservation of physical constraints
3. Benchmark computational scaling with increasing n to quantify the efficiency trade-off for high-dimensional Lie algebras