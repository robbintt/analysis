---
ver: rpa2
title: Joint Continual Learning of Local Language Models and Cloud Offloading Decisions
  with Budget Constraints
arxiv_id: '2602.00166'
source_url: https://arxiv.org/abs/2602.00166
tags:
- collaboration
- task
- cloud
- training
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of continual learning for local
  small language models (SLMs) under strict memory and computation constraints, where
  naive reward-based reinforcement learning often yields unstable offloading behavior
  and exacerbates catastrophic forgetting. The authors propose DA-GRPO, a dual-advantage
  extension of Group Relative Policy Optimization that incorporates cloud-usage constraints
  directly into advantage computation, avoiding fixed reward shaping and external
  routing models.
---

# Joint Continual Learning of Local Language Models and Cloud Offloading Decisions with Budget Constraints

## Quick Facts
- arXiv ID: 2602.00166
- Source URL: https://arxiv.org/abs/2602.00166
- Reference count: 40
- Key outcome: DA-GRPO improves post-switch accuracy and reduces catastrophic forgetting in local SLMs while maintaining stable cloud usage under budget constraints

## Executive Summary
This paper addresses the challenge of continual learning for local small language models (SLMs) under strict memory and computation constraints, where naive reward-based reinforcement learning often yields unstable offloading behavior and exacerbates catastrophic forgetting. The authors propose DA-GRPO, a dual-advantage extension of Group Relative Policy Optimization that incorporates cloud-usage constraints directly into advantage computation, avoiding fixed reward shaping and external routing models. This design enables the local model to jointly learn task competence and collaboration behavior, allowing cloud requests to emerge naturally during post-training while respecting a prescribed assistance budget.

## Method Summary
The paper introduces DA-GRPO, a dual-advantage extension of Group Relative Policy Optimization (GRPO) that integrates cloud-usage constraints directly into the advantage computation. Unlike traditional approaches that use fixed reward shaping or external routing models, DA-GRPO enables the local SLM to jointly learn task competence and collaboration behavior. The method avoids the instability of naive reinforcement learning approaches by incorporating budget constraints into the policy optimization process, allowing cloud requests to emerge naturally during post-training while respecting a prescribed assistance budget.

## Key Results
- DA-GRPO improves post-switch accuracy on mathematical reasoning and code generation benchmarks
- The method substantially reduces catastrophic forgetting compared to baseline approaches
- Maintains stable cloud usage patterns while respecting budget constraints

## Why This Works (Mechanism)
The dual-advantage mechanism works by incorporating cloud-usage constraints directly into the advantage computation during policy optimization. This integration allows the model to learn both task competence and collaboration behavior simultaneously, rather than treating them as separate problems. By avoiding fixed reward shaping, the method prevents the instability that typically arises in naive reinforcement learning approaches. The cloud requests emerge naturally from the optimization process rather than being predetermined by external routing models, enabling more adaptive and stable behavior.

## Foundational Learning
- **Group Relative Policy Optimization (GRPO)**: A policy gradient method that normalizes advantages within groups; needed for stable policy updates in constrained environments
- **Catastrophic forgetting**: The tendency of neural networks to rapidly forget previously learned tasks when trained on new ones; critical metric for continual learning
- **Reward shaping**: Techniques for modifying reward signals to guide learning; why needed to understand what DA-GRPO avoids
- **Budget constraints**: Hard limits on resource usage that must be respected during learning; essential for real-world deployment
- **Offloading decisions**: When to use cloud resources versus local computation; fundamental to collaborative learning systems
- **Advantage computation**: The difference between expected return and baseline value; core to policy gradient methods

## Architecture Onboarding
- **Component map**: Local SLM -> DA-GRPO optimizer -> Cloud resource manager -> Budget constraint module -> Performance evaluator
- **Critical path**: Local SLM inference → Advantage computation with budget constraints → Policy update → Cloud resource request decision
- **Design tradeoffs**: Joint learning vs. separate training of task competence and offloading decisions; direct constraint incorporation vs. reward shaping
- **Failure signatures**: Unstable cloud usage patterns, rapid performance degradation on previous tasks, budget violations, oscillating policy updates
- **3 first experiments**:
  1. Single task training with varying budget constraints to verify basic functionality
  2. Two-task sequential training to measure forgetting and cloud usage stability
  3. Comparison against fixed reward shaping baseline on simple mathematical reasoning tasks

## Open Questions the Paper Calls Out
None

## Limitations
- Limited empirical validation of catastrophic forgetting across multiple task sequences
- Performance gains evaluated only on mathematical reasoning and code generation benchmarks
- Cloud usage budget constraint implementation lacks specification for dynamic or task-dependent budgets
- "Stable" cloud usage claim not quantified with statistical measures or variability analysis
- No ablation studies to isolate dual-advantage mechanism impact

## Confidence
High confidence: The core problem formulation of joint continual learning for local SLMs with cloud offloading is well-defined and the technical contribution of DA-GRPO's dual-advantage approach is clearly articulated.

Medium confidence: The experimental results showing improved post-switch accuracy and reduced forgetting are plausible but require verification across more diverse task sequences and longer training horizons.

Low confidence: Claims about the method's superiority in maintaining stable cloud usage and its generalization to other domains beyond mathematical reasoning and code generation are not sufficiently supported by the presented evidence.

## Next Checks
1. Conduct longitudinal experiments across at least 5 diverse task sequences to measure catastrophic forgetting using standard metrics like forgetting rate and backward transfer, not just post-switch accuracy.
2. Implement and compare against multiple external routing model architectures (including learned and heuristic approaches) to validate the claim that DA-GRPO outperforms routing-based solutions.
3. Perform extensive cloud usage analysis including statistical measures of variance, correlation with task difficulty, and budget adherence across multiple budget scenarios to verify the "stable" cloud usage claim.