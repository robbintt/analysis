---
ver: rpa2
title: How does the optimizer implicitly bias the model merging loss landscape?
arxiv_id: '2510.04686'
source_url: https://arxiv.org/abs/2510.04686
tags:
- learning
- accuracy
- rate
- merging
- gain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies how optimizer choices shape the loss landscape
  for model merging. The authors introduce the effective noise scale as a unifying
  factor that captures the joint influence of learning rate, batch size, weight decay,
  and data augmentation.
---

# How does the optimizer implicitly bias the model merging loss landscape?

## Quick Facts
- arXiv ID: 2510.04686
- Source URL: https://arxiv.org/abs/2510.04686
- Authors: Chenxiang Zhang; Alexander Theus; Damien Teney; Antonio Orvieto; Jun Pang; Sjouke Mauw
- Reference count: 40
- Primary result: Effective noise scale unifies how optimizer choices shape model merging compatibility, exhibiting a non-monotonic optimum.

## Executive Summary
This paper investigates how optimizer choices (learning rate, batch size, weight decay, data augmentation) shape the loss landscape for model merging. The authors introduce the effective noise scale as a unifying factor that captures the joint influence of these hyperparameters. Across architectures (MLP, ResNet, DenseNet, Transformer, GPT) and datasets (SVHN, CIFAR, TinyImageNet, WILDS, TinyStories), merging effectiveness follows a non-monotonic curve with respect to effective noise, exhibiting a distinct optimum. The results extend prior work linking noise to individual model generalization, showing it also governs the compatibility of independently trained solutions.

## Method Summary
The study uses a Warmup-Stable-Decay (WSD) scheduler to train models with constant learning rate for T_stable epochs (e.g., 2000 for CIFAR), saving checkpoints every 20 epochs. For each checkpoint, two endpoint models are trained for T_decay epochs (e.g., 30) with decayed learning rate. Models are merged via linear interpolation (α=0.5) or task arithmetic. The effective noise scale is computed as Š = η/(B·(1-μ)²), where η is learning rate, B is batch size, and μ is momentum. Experiments sweep across architectures, datasets, and optimizer hyperparameters to measure merging performance gain.

## Key Results
- Merging effectiveness follows a non-monotonic function of effective noise, with a distinct optimum ("sweet spot")
- Weight decay improves merging for scale-invariant networks by maintaining effective learning rate
- Data augmentation and smaller batch sizes increase effective noise in ways equivalent to larger learning rates
- The optimal noise region generalizes across architectures and datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The effective noise scale unifies how optimizer hyperparameters jointly control model merging compatibility.
- Mechanism: Stochastic gradient noise magnitude shapes which basin of attraction solutions converge to. Moderate noise levels guide optimizers toward regions where independently trained solutions lie in mergeable (near-convex) portions of the loss landscape; insufficient noise leads to isolated sharp minima; excessive noise destabilizes training entirely.
- Core assumption: The gradient noise covariance structure remains approximately consistent across hyperparameter configurations, allowing the normalized proxy η/(B(1-μ)²) to capture the relevant dynamics.
- Evidence anchors:
  - [abstract] "We show that a single quantity – the effective noise scale – unifies the impact of optimizer and data choices on model merging."
  - [Section 3.1, Figure 1] Curves for different learning rate/batch size combinations align when reparameterized by effective noise, revealing a non-monotonic optimum.
  - [corpus] Related work "Entropic Confinement and Mode Connectivity" discusses how optimization dynamics confine solutions to connected basins, though direct evidence linking noise scale to merging is limited in corpus.
- Break condition: If gradient noise becomes highly non-Gaussian (heavy-tailed) or if architecture exhibits strong permutation symmetries requiring re-basin, the simple noise scale proxy may lose predictive power.

### Mechanism 2
- Claim: Merging effectiveness follows a non-monotonic function of effective noise, with a distinct optimum.
- Mechanism: Low noise → solutions converge to isolated sharp minima with high loss barriers between them. Optimal noise → solutions explore broadly enough to find flat, connected regions. High noise → training destabilizes, solutions fail to converge or occupy unstable regions.
- Core assumption: The loss landscape has connected low-loss regions that moderate noise can access but low noise cannot.
- Evidence anchors:
  - [abstract] "Across architectures and datasets, the effectiveness of merging success is a non-monotonic function of effective noise, with a distinct optimum."
  - [Section 3.1] Figure 1c shows accuracy gain peaks at intermediate effective noise values (~10⁰–10¹) and degrades at both extremes.
  - [corpus] "Transient learning dynamics drive escape from sharp valleys" supports the mechanism that noise-driven dynamics escape sharp minima, though corpus lacks direct merging experiments.
- Break condition: If models share identical initialization and training trajectory (same seed), merging succeeds regardless of noise; break condition also includes adversarial loss landscapes where no connected low-loss path exists.

### Mechanism 3
- Claim: Weight decay improves merging for scale-invariant networks by maintaining effective learning rate throughout training.
- Mechanism: In networks with normalization layers, weight scale does not affect output (f(x, αθ) = f(x, θ)). Without regularization, weights grow unbounded, causing effective LR to decay toward zero. Weight decay counteracts this growth, sustaining noise levels that promote mergeable solutions.
- Core assumption: Network has normalization layers making it scale-invariant; weight decay is applied via decoupled (AdamW-style) regularization.
- Evidence anchors:
  - [Section 3.3] "Van Laarhoven (2017); Hoffer et al. (2018) answer this question by demonstrating that weight decay controls the effective learning rate."
  - [Section 3.3, Figure 3] MLP (non-scale-invariant) shows no weight decay effect; ResNet/DenseNet (scale-invariant) show clear merging improvement with larger weight decay.
  - [corpus] No direct corpus evidence for weight decay's role in merging; corpus focuses on single-model generalization.
- Break condition: For architectures without normalization (e.g., MLPs, some CNNs without batch norm), weight decay acts as traditional L2 regularization and does not modulate effective noise.

## Foundational Learning

- **Concept: Linear Mode Connectivity**
  - Why needed here: Model merging fundamentally relies on the property that independently trained solutions can be connected by low-loss linear paths. Without understanding this geometric constraint, the noise-mechanism claims are unmotivated.
  - Quick check question: Given two trained models θ_A and θ_B with 90% accuracy each, if their midpoint θ_0.5 = 0.5θ_A + 0.5θ_B achieves 85% accuracy, do they exhibit linear mode connectivity?

- **Concept: Stochastic Gradient Noise Structure**
  - Why needed here: The effective noise scale derivation assumes minibatch gradient noise has covariance Σ/B. Understanding this helps diagnose when the simple noise scale proxy fails.
  - Quick check question: If batch size doubles from 32 to 64 while learning rate doubles from 0.1 to 0.2, what happens to the effective noise scale?

- **Concept: Task Arithmetic vs. Linear Interpolation**
  - Why needed here: The paper shows different merging methods have different loss landscape geometries. Initialization dependence matters for task arithmetic but not simple interpolation.
  - Quick check question: For task arithmetic merging, if θ_base is a pretrained model and τ_t = θ_t - θ_base, what subspace does θ_base + α·τ_t traverse as α varies?

## Architecture Onboarding

- **Component map:**
  ```
  Training Pipeline:
    Optimizer (SGD/AdamW) → injects noise via η, B, μ
    Weight Decay λ → modulates effective LR (scale-invariant only)
    Data Augmentation → adds noise via Σ_A
  
  Merging Pipeline:
    Checkpoint Selection → T_stable epochs with constant LR
    Endpoint Generation → T_decay epochs with LR decay
    Linear Interpolation → θ_merge = (1-α)θ_A + αθ_B
    Task Arithmetic → θ_merge = θ_base + Σα_i τ_i
  
  Evaluation:
    Performance Gain = f(θ_merge) - f(θ_single)
    Normalized Accuracy = (1/T) Σ [acc(θ_merge)/acc(θ_i)]
  ```

- **Critical path:**
  1. Identify target task(s) and architecture (verify scale-invariance for weight decay effects)
  2. Sweep effective noise scale via joint (η, B) grid; normalize using η/B(1-μ)²
  3. For each checkpoint, generate endpoint models via decayed training
  4. Merge and compute performance gain; identify optimum noise region
  5. Validate: check training loss convergence, verify non-monotonic trend

- **Design tradeoffs:**
  - Large LR → better merging but risk of instability (Figure 15 shows lr=0.5 fails)
  - Small batch → better merging but slower training (more gradient updates)
  - Strong weight decay → better merging for scale-invariant nets but may hurt single-model accuracy
  - Task arithmetic: large LR with pretrained init → flat landscape; without pretrain → sharp landscape

- **Failure signatures:**
  - Accuracy gain negative or zero despite converged training → effective noise too low
  - Training loss fails to converge → effective noise too high (reduce LR or increase batch)
  - Task arithmetic unstable with large LR without pretraining → initialization mismatch
  - MLP shows no weight decay benefit → architecture not scale-invariant (expected)

- **First 3 experiments:**
  1. **Noise scale validation:** Train ResNet18 on CIFAR100 with WSD scheduler. T_stable=2000 epochs constant LR, save checkpoint every 20 epochs. Sweep LRs: 1e-3, 5e-3, 1e-2, 5e-2, 1e-1, 2e-1. Batch size=128, weight decay=5e-4, random flip+crop augmentation. For each checkpoint, create two endpoint models by training T_decay=30 epochs with LR decay (Assumption: square root decay schedule details from Hägele et al. 2024; use cosine as fallback).
  2. Perform linear interpolation merging with α=0.5 between endpoint pairs. Compute accuracy gain = merged accuracy - single model accuracy.
  3. Compute normalized effective noise proxy Š = η/(B·(1-μ)²). Plot accuracy gain vs Š to verify non-monotonic "sweet spot" relationship.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical mechanism that causes the relationship between effective noise scale and merging effectiveness to be non-monotonic?
- Basis in paper: [explicit] The abstract and conclusion characterize the relationship as a "non-monotonic function" with a "distinct optimum" (sweet spot), but the paper provides empirical trends rather than a theoretical proof explaining why the curve peaks before degrading at high noise levels.
- Why unresolved: The paper demonstrates the existence of the "sweet spot" empirically across datasets but notes in the limitations that "no new theoretical guarantees are developed."
- What evidence would resolve it: A formal analysis (e.g., using stochastic differential equations) deriving the optimal noise scale $\tilde{S}$ relative to the loss landscape curvature, or empirical verification that the degradation at high noise correlates specifically with training instability rather than landscape geometry.

### Open Question 2
- Question: Why does the presence of a pretrained initialization invert the robustness of high learning rate solutions in task arithmetic merging?
- Basis in paper: [inferred] Section 4.1 and Figure 7 show a "clear dichotomy": large learning rates yield flatter, more robust landscapes with pretraining but sharper, less robust landscapes when training from scratch. The paper presents this observation but does not explain the underlying cause.
- Why unresolved: The text describes the dichotomy and advises using a "suitable initialization," but leaves the interaction between the optimizer's implicit bias and the pretraining weights as an open empirical phenomenon.
- What evidence would resolve it: Ablation studies analyzing the directional alignment of task vectors with the pretraining weights under different learning rates, or Hessian trace analysis comparing the connectivity barriers in pretrained vs. scratch settings.

### Open Question 3
- Question: Does the effective noise scale maintain its predictive power for merging compatibility in more advanced, non-linear merging algorithms (e.g., Fisher merging, RegMean)?
- Basis in paper: [explicit] The "Limitations" section states that the study focused on "standard merging methods" (linear interpolation, task arithmetic) and suggests that future work should verify if these findings generalize to "state-of-the-art approaches."
- Why unresolved: The unified factor of effective noise was only validated on simple linear combinations of weights; its relevance to methods that weight parameters based on curvature or importance metrics is unknown.
- What evidence would resolve it: Experiments applying the effective noise scale sweep (varying LR, batch size, weight decay) to models merged via Fisher-weighted averaging or TIES-Merging to see if the "sweet spot" persists.

## Limitations
- The study focuses on standard merging methods (linear interpolation, task arithmetic) and does not explore advanced non-linear merging algorithms
- No new theoretical guarantees are developed; findings are primarily empirical
- The noise scale proxy assumes approximately Gaussian minibatch gradients, which may break down in heavy-tailed regimes

## Confidence
- Noise scale unification across hyperparameters: **High** (robust empirical pattern across 5 architectures)
- Weight decay effect for scale-invariant nets: **High** (clear architecture dependence)
- Data augmentation equivalence to noise: **Medium** (theoretical link established but fewer ablation studies)

## Next Checks
1. Test effective noise proxy in a heavy-tailed gradient regime (e.g., ViT on JFT) to probe Gaussianity assumption.
2. Isolate weight decay's effect on effective LR by training scale-invariant nets with decoupled AdamW vs. SGD with L2.
3. Validate merging landscape connectivity directly via loss barrier measurements between endpoint models across noise levels.