---
ver: rpa2
title: 'StressTransfer: Stress-Aware Speech-to-Speech Translation with Emphasis Preservation'
arxiv_id: '2510.13194'
source_url: https://arxiv.org/abs/2510.13194
tags:
- speech
- translation
- emphasis
- arxiv
- stress
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of preserving emphasis and stress
  in speech-to-speech translation (S2ST), which is critical for conveying speaker
  intent and expressiveness. The proposed method, StressTransfer, leverages large
  language models (LLMs) to automatically generate cross-lingual emphasis-aligned
  training data (EmphST-Instruct) and introduces an LLM-as-Judge framework for evaluation
  (EmphST-Bench).
---

# StressTransfer: Stress-Aware Speech-to-Speech Translation with Emphasis Preservation

## Quick Facts
- arXiv ID: 2510.13194
- Source URL: https://arxiv.org/abs/2510.13194
- Authors: Xi Chen; Yuchen Song; Satoshi Nakamura
- Reference count: 0
- Primary result: StressTransfer achieves 78.0% SSR vs 72.9% best baseline on EmphST-Bench while maintaining BLEU 46.9 and COMET 0.8347

## Executive Summary
This paper addresses the problem of preserving emphasis and stress in speech-to-speech translation (S2ST), which is critical for conveying speaker intent and expressiveness. The proposed method, StressTransfer, leverages large language models (LLMs) to automatically generate cross-lingual emphasis-aligned training data (EmphST-Instruct) and introduces an LLM-as-Judge framework for evaluation (EmphST-Bench). The core approach involves translating source-language stress into target-language emphasis tags that guide a controllable TTS model to preserve prosody in the translated speech.

The primary results demonstrate that StressTransfer substantially outperforms baseline methods in preserving emphasis while maintaining competitive translation quality. On the EmphST-Bench benchmark, StressTransfer achieves a Sentence Stress Reasoning Accuracy (SSR) of 78.0%, compared to 72.9% for the best baseline. The system also achieves competitive BLEU (46.9) and COMET (0.8347) scores on the CoVOST2 test set, matching recent advanced speech translation models. Subjective human evaluations on synthesized speech confirm superior emphasis preservation, with StressTransfer achieving an SSR of 46.0% compared to 11.0% for SeamlessExpressive and 41.3% for GPT-4o-audio.

## Method Summary
StressTransfer is a cascaded speech-to-speech translation system that preserves emphasis by converting source-language stress into target-language emphasis tags. The method uses a Whisper-large-v3 encoder with a 3-layer MLP adaptor and downsampling to project speech embeddings into LLM token space. A Qwen-2.5-3B LLM (fine-tuned with LoRA) generates target text with interleaved Markdown emphasis markers derived from source speech prosody. These tags guide a CosyVoice2 TTS model to render the translated speech with preserved emphasis. To overcome data scarcity, the authors developed a multi-LLM pipeline that generates cross-lingual emphasis-aligned training data (EmphST-Instruct) from monolingual English stress corpora, using translation experts to generate candidates and a selection expert to choose optimal outputs.

## Key Results
- StressTransfer achieves 78.0% SSR on EmphST-Bench vs 72.9% for best baseline (SeamlessExpressive)
- Maintains competitive BLEU (46.9) and COMET (0.8347) scores on CoVOST2 test set
- Outperforms even proprietary models in subjective human evaluation: 46.0% SSR vs 11.0% for SeamlessExpressive and 41.3% for GPT-4o-audio
- LoRA fine-tuning enables efficient adaptation of both speech encoder and LLM components

## Why This Works (Mechanism)

### Mechanism 1
LLMs can generate cross-lingual emphasis-aligned training data that enables stress-aware translation. Multi-LLM data pipeline uses "translation experts" to generate diverse candidate translations with stress annotations, then a "selection expert" identifies optimal outputs. This ensemble approach mitigates individual model biases and creates EmphST-Instruct from monolingual English stress corpora (Stress17k, TinyStress). Core assumption: LLMs possess sufficient cross-lingual semantic understanding to correctly project emphasis markers across languages despite stress realization lacking linguistic universals. Break condition: If source stress patterns have no semantic equivalent in target language (e.g., contrastive focus that doesn't translate), LLM may generate spurious emphasis markers.

### Mechanism 2
Explicit emphasis tags in target text enable more reliable prosody transfer than implicit acoustic feature propagation. The S2TT model generates target text with interleaved Markdown emphasis markers (`**word**`) derived from source speech prosody, rather than directly transferring acoustic features. These explicit tags provide unambiguous guidance to downstream TTS. Core assumption: Emphasis markers predicted from acoustic input accurately reflect speaker intent, and the mapping from stress detection to emphasis tag placement is learnable. Break condition: If stress detection from source speech is noisy or ambiguous, emphasis tags will be misplaced, propagating errors to TTS.

### Mechanism 3
Cascaded S2TT + controllable TTS outperforms end-to-end models on emphasis preservation when equipped with explicit emphasis representations. Separating translation (S2TT with emphasis tags) from synthesis (TTS with prosody control) allows each component to specialize. The TTS receives structured emphasis instructions rather than implicit acoustic targets. Core assumption: CosyVoice2's text-based prosody control interface can accurately render emphasis tags as perceptible prosodic prominence. Break condition: Latency-sensitive applications may find cascaded pipeline too slow; TTS emphasis rendering may sound unnatural if emphasis density is high.

## Foundational Learning

- **Prosody and Emphasis in Speech**
  - Why needed here: The entire system is built on detecting stress in source speech and rendering emphasis in target speech. Understanding that stress conveys focus, contrast, and intent—not just volume—is essential.
  - Quick check question: In the sentence "I didn't say HE stole the money," what semantic information does stressing "HE" convey that would be lost in flat prosody?

- **LoRA Fine-tuning for Speech-LLM Integration**
  - Why needed here: StressTransfer uses LoRA to efficiently adapt both Whisper encoder and Qwen LLM. Understanding parameter-efficient fine-tuning explains how this works with a 3B LLM.
  - Quick check question: What are the trade-offs between full fine-tuning and LoRA when adapting a pre-trained LLM for speech-text tasks?

- **LLM-as-Judge Evaluation Paradigm**
  - Why needed here: The EmphST-Bench evaluation relies on LLM-as-Judge (GPT-4.1) to score emphasis prediction accuracy. Understanding this paradigm is critical for interpreting results.
  - Quick check question: What validation step did the authors perform to ensure LLM-as-Judge correlates with human judgments?

## Architecture Onboarding

- **Component map:** [Source Speech] → [Whisper-large-v3 Encoder] → [3-layer MLP Adaptor + Downsampling] → [Qwen-2.5-3B LLM w/ LoRA] → [Target Text with **emphasis** tags] → [CosyVoice2 TTS] → [Translated Speech]

- **Critical path:** The Adaptor is the only fully fine-tuned component; speech encoder and LLM use LoRA. Training data quality (EmphST-Instruct) directly determines emphasis prediction capability.

- **Design tradeoffs:** 3B LLM chosen over larger models for efficiency—limits reasoning capacity but enables deployment. Cascaded S2TT+TTS vs. end-to-end—trades latency for interpretable intermediate representation (emphasis tags). Synthetic training data (EmphST-Instruct) vs. manual annotation—scales easily but may contain LLM hallucinations.

- **Failure signatures:** Low SSR with high BLEU: Model learned translation but not emphasis (check EmphST-Instruct mixing ratio). Emphasis tags in wrong positions: Adaptor not aligning speech prosody to text tokens correctly. TTS ignoring emphasis: CosyVoice2 prompt formatting issue (verify `<strong>` conversion).

- **First 3 experiments:** 1) Ablate EmphST-Instruct: Train on CoVoST2 only, verify SSR drops (Table 5 shows ~5% BLEU maintained but no emphasis capability without EmphST-Instruct). 2) LLM-as-Judge validation: Sample 50 predictions, have humans annotate emphasis, compare to GPT-4.1 verdicts (Table 6 shows F1=0.9271). 3) Stress detection robustness: Test on noisy/reverberant audio, measure SSR degradation to identify encoder limitations.

## Open Questions the Paper Calls Out

### Open Question 1
Can StressTransfer generalize to language pairs with syntactically divergent stress rules or non-English sources, given the current study is restricted to English-to-Chinese translation? Basis: [explicit] Page 2 notes the translation target is "Chinese in this work" and explicitly states that "stress realization lacks linguistic universals across languages." Why unresolved: The evaluation and model fine-tuning are exclusively performed on English-Chinese pairs, leaving cross-lingual robustness unproven. What evidence would resolve it: Evaluation on diverse language pairs (e.g., English-to-German or Japanese-to-English) to test the pipeline's adaptability to different prosodic structures.

### Open Question 2
Does the reliance on synthetic, LLM-generated training data (EmphST-Instruct) limit the model's ability to handle natural, spontaneous emphasis compared to human-annotated corpora? Basis: [inferred] The method relies on the EmphST-Instruct pipeline (Page 2) derived from synthetic sources to overcome data scarcity, with the authors noting the source is "synthetic data" while the eval set is "recording." Why unresolved: Synthetic data may lack the acoustic variability and nuance of natural speech, potentially creating a domain gap. What evidence would resolve it: Ablation studies using human-verified emphasis annotations or testing performance on datasets of fully spontaneous, conversational speech.

### Open Question 3
Does the explicit text-tagging approach in the cascaded architecture lose prosodic nuance compared to a direct end-to-end model? Basis: [inferred] The paper adopts a cascaded pipeline (Page 3) that converts stress to text tags, despite acknowledging the existence of direct E2E models (Page 1) that avoid intermediate representations. Why unresolved: Quantizing prosody into markdown tags (`**word**`) simplifies the signal; it is unclear if this acts as a bottleneck for subtle intonation cues. What evidence would resolve it: Comparative experiments against a direct speech-to-speech model trained on the same emphasis preservation objectives.

## Limitations

- Reliance on LLM-generated training data (EmphST-Instruct) introduces potential hallucination and quality variability that could propagate to the final model
- Narrow language pair (English to Chinese) and focus on word-level stress may not generalize to other language pairs or different types of emphasis
- Evaluation framework using LLM-as-Judge may be vulnerable to systematic biases in the judging model, and human evaluation sample size (50) is relatively small

## Confidence

- **High Confidence**: The cascaded architecture (S2TT + TTS) achieving competitive BLEU/COMET scores (46.9/0.8347) while adding emphasis preservation capability is well-supported by direct comparisons in Tables 4-5. The mechanism of explicit emphasis tags guiding TTS is clearly demonstrated and technically sound.
- **Medium Confidence**: The SSR metric and its superiority (78.0% vs 72.9% best baseline) are convincing given the validation with human judges (F1=0.9271), but the reliance on LLM-as-Judge for the primary evaluation metric introduces some uncertainty about systematic biases.
- **Medium Confidence**: The claim that LLMs can reliably generate cross-lingual emphasis-aligned data is supported by the results but depends heavily on the quality of the multi-LLM pipeline, which is not fully detailed in the paper.

## Next Checks

1. **Cross-lingual emphasis alignment robustness test**: Apply StressTransfer to a different language pair (e.g., English to Spanish or German) and measure SSR degradation. This would validate whether the LLM-generated emphasis alignment generalizes beyond Chinese or is specific to this language pair's stress patterns.

2. **Synthetic data quality audit**: Manually annotate a random sample of 100 EmphST-Instruct examples to measure: (a) emphasis placement accuracy compared to human judgment, (b) hallucination rate in translations, and (c) correlation between data quality and model SSR performance. This would quantify the reliability of the data generation pipeline.

3. **End-to-end latency and quality tradeoff analysis**: Implement a lightweight version of StressTransfer (e.g., smaller encoder, quantized LLM) and measure the tradeoff curve between inference speed and SSR/BLEU scores. This would determine if the emphasis preservation capability can be maintained in production scenarios with strict latency constraints.