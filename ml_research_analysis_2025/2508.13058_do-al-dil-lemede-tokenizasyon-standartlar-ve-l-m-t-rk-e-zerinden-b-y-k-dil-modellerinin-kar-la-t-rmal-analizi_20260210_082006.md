---
ver: rpa2
title: "Do\u011Fal Dil \u0130\u015Flemede Tokenizasyon Standartlar\u0131 ve \xD6l\xE7\
  \xFCm\xFC: T\xFCrk\xE7e \xDCzerinden B\xFCy\xFCk Dil Modellerinin Kar\u015F\u0131\
  la\u015Ft\u0131rmal\u0131 Analizi"
arxiv_id: '2508.13058'
source_url: https://arxiv.org/abs/2508.13058
tags:
- tokenizasyon
- token
- kelime
- daha
- mmlu
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study proposes a novel evaluation framework for tokenization\
  \ in morphologically-rich and low-resource languages, using Turkish as a case study.\
  \ It introduces two new metrics\u2014language-specific token percentage (TR%) and\
  \ token purity (Pure%)\u2014to assess how well tokenizers preserve linguistic structures."
---

# Doğal Dil İşlemede Tokenizasyon Standartları ve Ölçümü: Türkçe Üzerinden Büyük Dil Modellerinin Karşılaştırmalı Analizi

## Quick Facts
- arXiv ID: 2508.13058
- Source URL: https://arxiv.org/abs/2508.13058
- Reference count: 0
- Primary result: Novel framework shows language-specific token percentage (TR%) correlates more strongly with downstream task performance than token purity for morphologically-rich languages

## Executive Summary
This study introduces a novel evaluation framework for tokenization quality in morphologically-rich and low-resource languages, using Turkish as a case study. The framework proposes two new metrics—language-specific token percentage (TR%) and token purity (Pure%)—to assess how well tokenizers preserve linguistic structures. Tested on the TR-MMLU dataset of 6,200 multiple-choice questions, the evaluation compares four tokenizers and demonstrates that TR% shows stronger correlation with downstream performance (e.g., MMLU scores) than Pure%, highlighting the importance of tailored tokenization methods for agglutinative languages like Turkish.

## Method Summary
The framework evaluates tokenization quality using the TR-MMLU dataset (6,200 Turkish multiple-choice questions) and four tokenizers (gemma-2, llama-3.1, Qwen2.5, aya-expanse). Novel metrics include language-specific token percentage (TR%) and token purity (Pure%), calculated using ITU Turkish NLP Web Service and Kalbur library for morphological validation. The study correlates these metrics with MMLU benchmark scores and examines relationships with vocabulary size and processing time. The methodology is implemented in a publicly available benchmark repository.

## Key Results
- TR% shows strong positive correlation with MMLU scores (r = 0.90), stronger than Pure% correlation
- Larger vocabulary sizes positively correlate with both TR% (r = 0.77) and Pure% (r = 0.82)
- Gemma-2 (27.2B parameters) achieves higher MMLU score (72.10%) than llama-3.1 (70.6B parameters, 70.42%), demonstrating that parameter count alone doesn't guarantee better linguistic performance
- Tokenization processing times range from 2.77s to 3.31s across evaluated models

## Why This Works (Mechanism)

### Mechanism 1
Language-specific token percentage (TR%) correlates more strongly with downstream task performance than token purity (Pure%). When tokenizers' vocabularies contain valid words from the target language, models receive semantically coherent units rather than fragmented subword sequences, reducing computational burden of reconstructing meaning. This facilitates more efficient learning and inference in transformer architectures.

### Mechanism 2
Larger vocabulary sizes support better morphological coverage in agglutinative languages. Turkish forms words by concatenating morphemes, and larger vocabularies can store more complete morpheme combinations, reducing over-fragmentation into meaningless character sequences. The positive correlation (r = 0.77) between vocabulary size and TR% supports this mechanism.

### Mechanism 3
Increased model parameter count alone does not compensate for poor tokenization quality. Even large models must allocate capacity to learn token boundary reconstruction when tokenizers fragment morphologically complex words. Models with better tokenization achieve higher MMLU scores because more capacity is available for learning semantic relationships rather than token reconstruction.

## Foundational Learning

- **Concept: Agglutinative morphology**
  - Why needed here: Turkish words combine root + multiple suffixes, creating thousands of surface forms from single roots. Standard BPE trained on English lacks morphological awareness.
  - Quick check question: Given "uygarlaşmıyorlardı" (they were not becoming civilized), would you expect English-trained BPE to segment this into ~6-8 tokens or ~12-15 tokens?

- **Concept: Subword tokenization algorithms (BPE vs. SentencePiece)**
  - Why needed here: These algorithms build vocabularies differently. BPE merges frequent character pairs iteratively; SentencePiece treats input as raw character streams. Their behavior differs on morphologically rich text.
  - Quick check question: If a tokenizer produces "e", "vl", "er" for "evler" (houses), what does this suggest about its training data composition?

- **Concept: Tokenization fertility (tokens per word ratio)**
  - Why needed here: Higher fertility indicates over-fragmentation. The paper reports token counts ranging from 434,526 to 561,866 for identical input, directly affecting sequence length and computational cost.
  - Quick check question: If Model A produces 1.8 tokens/word and Model B produces 2.6 tokens/word for Turkish text, which will have longer inference latency for the same content?

## Architecture Onboarding

- **Component map:**
```
TR-MMLU Dataset (6,200 questions)
    ↓
Tokenizer (gemma-2 / llama-3.1 / Qwen2.5 / aya-expanse)
    ↓
Metrics Layer:
├── Basic: vocab_size, token_count, processing_time
├── Novel: TR% (via ITU Turkish NLP Service + Kalbur library)
│          Pure% (morphological validity check)
    ↓
Correlation Analysis → MMLU benchmark scores
```

- **Critical path:** Vocabulary construction → Token segmentation quality (TR%, Pure%) → Downstream task performance (MMLU). The correlation matrix shows TR% → MMLU has the strongest relationship (r=0.90).

- **Design tradeoffs:**
  - Larger vocabulary (128K-256K tokens) improves TR%/Pure% but increases embedding memory and softmax computation
  - Higher TR% requires more language-specific pre-training data for vocabulary construction
  - Processing time (2.77-3.31s range) favors fewer tokens but may sacrifice linguistic integrity

- **Failure signatures:**
  - Pure% < 35% with TR% > 45%: Valid Turkish words but morphologically fragmented (e.g., aya-expanse pattern)
  - Large parameter count + low MMLU + low TR%: Model capacity wasted on token reconstruction (e.g., Qwen2.5 pattern)
  - Excessive unique tokens relative to vocabulary: Indicates over-fragmentation of morphologically complex words

- **First 3 experiments:**
  1. **Baseline replication:** Run the paper's evaluation code on TR-MMLU with all 4 tokenizers to verify TR% and Pure% calculations using ITU Turkish NLP Service
  2. **Vocabulary ablation:** Train BPE tokenizers with vocabulary sizes [32K, 64K, 128K, 256K] on Turkish-only corpus and measure TR%/Pure% trajectory to validate the r=0.77 correlation claim
  3. **Cross-linguistic transfer test:** Apply the TR% metric methodology to Finnish or Hungarian using translated MMLU subsets to test framework generalizability

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Do the proposed metrics (TR%, Pure%) retain their predictive validity for downstream performance when applied to generative tasks such as machine translation or summarization?
- **Basis in paper:** The study validates the framework solely on TR-MMLU (multiple-choice questions), yet claims applications in translation and sentiment analysis.
- **Why unresolved:** The correlation between TR% and MMLU score is established for discriminative tasks, but tokenization requirements and the impact of token "purity" may differ significantly for open-ended sequence generation.
- **What evidence would resolve it:** Benchmarking the evaluated tokenizers on Turkish generative tasks (e.g., WMT translation) and correlating scores (BLEU/ROUGE) with TR% and Pure%.

### Open Question 2
- **Question:** Can tokenization methods be dynamically optimized for specific domains (e.g., law, medicine) to outperform general-purpose tokenizers in specialized NLP tasks?
- **Basis in paper:** The conclusion states that future work should focus on "tokenization methods that can be dynamically optimized for specific tasks and domains."
- **Why unresolved:** Current models use static, general-purpose vocabularies. It is unknown if adapting the tokenizer to domain-specific morphology improves performance without causing catastrophic forgetting or requiring full model retraining.
- **What evidence would resolve it:** Experiments involving vocabulary augmentation or adapter-based tokenizer adjustments applied to domain-specific subsets of TR-MMLU or legal/medical corpora.

### Open Question 3
- **Question:** Do the strong correlations observed between language-specific token percentages and performance generalize to other morphologically rich, low-resource languages like Finnish or Hungarian?
- **Basis in paper:** The authors state that "comparing tokenization performance in different languages... will help us understand universal and language-specific tokenization principles."
- **Why unresolved:** The study is restricted to Turkish. It is unclear if the strong correlation (r=0.90) between TR% and MMLU is a universal feature of morphologically rich languages or an artifact of Turkish's specific structure.
- **What evidence would resolve it:** Applying the proposed evaluation framework to Finnish and Hungarian datasets to verify if %Lang correlates with downstream accuracy similarly.

## Limitations

- **Metric Definition Ambiguity:** %Pure metric's exact validation rules through Kalbur library and ITU Turkish NLP Service are not fully specified, making precise reproduction challenging
- **Generalizability Constraint:** Results based on exactly four models with specific parameter counts and vocabulary sizes may not represent broader model population
- **Benchmark Scope:** Framework effectiveness for other morphologically rich languages remains theoretical rather than empirically validated

## Confidence

**High Confidence** (empirical evidence directly supports):
- %TR metric calculation and its correlation with MMLU scores (r=0.90)
- Vocabulary size correlation with both %TR (r=0.77) and %Pure (r=0.82)
- Parameter count vs. performance relationship (gemma-2 outperforms llama-3.1 despite smaller size)

**Medium Confidence** (supported but with methodological gaps):
- %Pure metric's relationship to downstream performance (weaker correlation explained but validation rules unclear)
- Framework generalizability to other morphologically rich languages (conceptual argument strong but untested)

**Low Confidence** (largely theoretical or insufficiently validated):
- Exact %Pure calculation methodology (algorithm details missing)
- Processing time measurements' impact on real-world deployment (only 4 models tested)

## Next Checks

1. **Metric Algorithm Validation:** Replicate the exact %Pure calculation by instrumenting the Kalbur library and ITU Turkish NLP Service calls with test cases. Create a test suite with known Turkish words to verify the purity classification boundaries match the paper's claims.

2. **Cross-Linguistic Framework Testing:** Apply the TR% metric methodology to Finnish using translated MMLU subsets or Finnish-specific benchmarks. Compare whether the strong correlation (r=0.90) between TR% and downstream performance holds for another agglutinative language.

3. **Vocabulary Size Sensitivity Analysis:** Systematically train BPE tokenizers with Turkish-only corpora at 32K, 64K, 128K, and 256K vocabulary sizes. Measure TR% and Pure% at each step to empirically validate the claimed r=0.77 correlation and identify the point of diminishing returns for vocabulary expansion.