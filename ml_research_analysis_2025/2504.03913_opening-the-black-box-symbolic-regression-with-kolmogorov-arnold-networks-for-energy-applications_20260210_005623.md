---
ver: rpa2
title: 'Opening the Black-Box: Symbolic Regression with Kolmogorov-Arnold Networks
  for Energy Applications'
arxiv_id: '2504.03913'
source_url: https://arxiv.org/abs/2504.03913
tags:
- dataset
- nuclear
- symbolic
- work
- power
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluates Kolmogorov-Arnold Networks (KANs) against\
  \ traditional feedforward neural networks (FNNs) across eight diverse nuclear engineering\
  \ datasets. KANs, which transform into interpretable symbolic equations after training,\
  \ demonstrate comparable or superior accuracy to FNNs, with R\xB2 scores typically\
  \ ranging from 0.976 to 0.999."
---

# Opening the Black-Box: Symbolic Regression with Kolmogorov-Arnold Networks for Energy Applications

## Quick Facts
- arXiv ID: 2504.03913
- Source URL: https://arxiv.org/abs/2504.03913
- Reference count: 40
- Primary result: KANs demonstrate comparable or superior accuracy to FNNs (R² 0.976-0.999) while producing interpretable symbolic equations

## Executive Summary
This study evaluates Kolmogorov-Arnold Networks (KANs) against traditional feedforward neural networks (FNNs) across eight diverse nuclear engineering datasets. KANs, which transform into interpretable symbolic equations after training, demonstrate comparable or superior accuracy to FNNs, with R² scores typically ranging from 0.976 to 0.999. Unlike FNNs, KANs produce interpretable symbolic equations, enhancing transparency. SHAP analysis reveals KANs better capture known physical relationships, while FNNs offer only statistical accuracy. KANs also exhibit feature elimination capabilities, simplifying problem spaces. The study highlights KANs as a promising alternative for sensitive industries requiring both accuracy and interpretability, though some limitations exist for high-dimensional output problems.

## Method Summary
The study compared KANs and FNNs on eight nuclear engineering regression datasets using pykan-0.2.8 with L-BFGS optimizer and hyperopt (TPE, 200 max evals) for hyperparameter tuning. Data underwent min-max scaling and 70/30 train-test splitting. KANs were trained, pruned, and refit, then converted to symbolic equations using symbolic regression tools. Kernel SHAP with K-means background sampling provided explainability analysis. The tuning objective weighted 80% toward symbolic R² and 20% toward spline R². Both models were evaluated on R², MAE, MAPE, MSE, RMSE, and RMSPE metrics.

## Key Results
- KANs achieved R² scores of 0.976-0.999 across datasets, comparable to or better than FNNs
- KANs produced interpretable symbolic equations while FNNs remained black-box models
- KANs eliminated irrelevant features (e.g., 3 of 7 features in HEAT dataset) without sacrificing accuracy
- SHAP analysis showed KANs better captured known physical relationships versus FNNs' statistical correlations
- KANs struggled with high-dimensional output problems (PC dataset with 22 outputs achieved only R²=0.735)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** KANs achieve interpretability by converting learned spline-based activation functions into symbolic mathematical equations, enabling human-reproducible predictions while maintaining accuracy.
- **Mechanism:** KANs place learnable B-spline activation functions on network edges rather than fixed activations on nodes. After training via L-BFGS optimization, these splines are fitted to a symbolic function library (sin, cos, tanh, etc.). The resulting equations allow exact computation of derivatives and direct feature contribution analysis without post-hoc methods.
- **Core assumption:** The underlying function being learned is smooth and can be decomposed into finite compositions of continuous univariate functions (per the Kolmogorov-Arnold Representation Theorem).
- **Evidence anchors:**
  - [abstract] "KANs, which transform into symbolic equations after training, yield perfectly interpretable models while FNNs remain black-boxes."
  - [Section 2.1] "Unlike a multi-layer perceptron (MLP) network, which places activation functions on the nodes of its network, a KAN's activation functions are on its edges."
  - [Section 4.3] "To enhance interpretability, these splines can be transformed into symbolic equations using available symbolic regression tools."
  - [corpus] KAN-SR paper confirms symbolic regression capability via divide-and-conquer approach; corpus average FMR=0.383 with multiple papers validating KAN interpretability claims.
- **Break condition:** Non-smooth underlying functions or excessively deep architectures producing equations too complex for practical human interpretation.

### Mechanism 2
- **Claim:** KANs perform automatic feature elimination through regularization-induced sparsity, reducing problem dimensionality without sacrificing accuracy.
- **Mechanism:** L1 regularization metrics (edge_forward_sum, edge_forward_spline_n, etc.) penalize redundant activation functions during training. Pruning removes low-importance edges post-training. This produces symbolic equations that explicitly exclude irrelevant features (zero SHAP values), rather than assigning small but non-zero weights as FNNs do.
- **Core assumption:** The true underlying physics depends on fewer features than provided; irrelevant features can be safely zeroed out.
- **Evidence anchors:**
  - [Section 5.3] "the KAN actually eliminated three of the seven variables used to generate the dataset, and then predicted the values with an R2 score of 0.996"
  - [Section 5.4] "KANs are more likely to 'eliminate' a variable (and thereby reduce the complexity of their problem space) than a KAN [sic - should be FNN]"
  - [Figure B.10] Nuclear cross-section dataset: "the KAN predicts with this accuracy using three less input features than the FNN" while maintaining R² > 0.999.
  - [corpus] Limited direct corpus validation of feature elimination mechanism specifically.
- **Break condition:** Problems where all features contribute meaningfully (KAN may underfit by oversparsifying); highly correlated features where elimination choice becomes arbitrary.

### Mechanism 3
- **Claim:** KANs capture physically meaningful relationships rather than purely statistical correlations, as evidenced by SHAP feature importance alignment with known physics.
- **Mechanism:** By learning univariate continuous functions on edges that compose into the final prediction, KANs structurally enforce a functional decomposition that can match physical law structure. When splines convert to symbolic form, the resulting equations preserve these learned relationships, which SHAP analysis then reveals as matching known thermodynamic/engineering dependencies.
- **Core assumption:** Physical relationships in the data can be expressed as compositions of continuous univariate functions; the training data sufficiently samples the relevant physics.
- **Evidence anchors:**
  - [abstract] "KANs learn real, physical relations from experimental data, while FNNs simply produce statistically accurate results."
  - [Section 6.3] "the FNN merely yields statistical accuracy... the KAN results shown in Figure 4 tell a different story, demonstrating heightened importance for five of the six input features, even when they are correlated."
  - [Section 5.3] For CHF dataset: "KAN gives them both importance [pressure and outlet quality], while the FNN uses one over the other... both models yielding reliable prediction results."
  - [corpus] Shift-Invariant Attribute Scoring paper (h-index 56 author) validates KAN feature-outcome relationship learning; Softly Symbolifying KANs paper notes trained activations may lack symbolic fidelity without careful practice.
- **Break condition:** Insufficient training data to distinguish physical relationships from noise; extrapolation beyond training distribution (authors explicitly note this limitation applies to all ML models).

## Foundational Learning

- **Concept: Kolmogorov-Arnold Representation Theorem**
  - **Why needed here:** KANs are theoretically grounded in KART rather than the Universal Approximation Theorem. Understanding that any multivariate continuous function can be decomposed into sums of continuous univariate functions explains why KANs can achieve both accuracy and interpretability.
  - **Quick check question:** Can you explain why placing learnable univariate functions on edges is fundamentally different from fixed activations on nodes?

- **Concept: B-splines and symbolic regression**
  - **Why needed here:** KANs use B-splines as initial activation functions, which are later converted to symbolic expressions. Understanding spline flexibility and symbolic fitting tradeoffs is essential for tuning the conversion process.
  - **Quick check question:** What happens to accuracy when you increase the "simplicity weight" during symbolic conversion?

- **Concept: SHAP values for explainability**
  - **Why needed here:** The paper uses Kernel SHAP for post-hoc explainability comparison between KANs and FNNs. Interpreting SHAP feature importance plots is necessary to evaluate whether KANs capture physical relationships.
  - **Quick check question:** Why does a mean absolute SHAP value of 0 indicate feature elimination, and how does this differ from a small non-zero value?

## Architecture Onboarding

- **Component map:** Input layer → Edge activation functions (B-splines) → Hidden layer(s) → Output layer
- **Critical path:**
  1. Preprocess data (min-max scale inputs and outputs, 70/30 train-test split)
  2. Hyperparameter tune depth, grid, k, λ, steps, learning rates (use Tree of Parzen Estimators, weight objective 80% toward symbolic R²)
  3. Train with L-BFGS (LR1), prune, refit (LR2)
  4. Convert splines to symbolic equations (simplicity_weight=0 for accuracy priority)
  5. Run Kernel SHAP on symbolic equation for explainability

- **Design tradeoffs:**
  - **Grid size vs. overfitting**: Larger grid (8-10) captures more complexity but risks overfitting; paper found optimal around 7-9
  - **Simplicity weight vs. accuracy**: Weight=0 prioritizes accuracy (complex equations); weight=1 favors simpler functions (may reduce R²)
  - **Depth vs. interpretability**: Depth 1 produces shorter equations; Depth 2+ increases accuracy for complex problems but equation length grows substantially
  - **Output dimensionality**: High-output problems (>10 outputs) showed degraded performance; consider splitting by physical regions

- **Failure signatures:**
  - **R² drops significantly after symbolic conversion**: Spline-to-symbolic fit failed; increase function library or reduce simplicity weight
  - **SHAP shows all features equally important (flat distribution)**: Regularization too weak; increase λ or try different reg_metric
  - **Training diverges (NaN/inf)**: Learning rate too high; reduce LR1 below 1.0
  - **Symbolic equation extremely long (>50 terms)**: Network overparameterized; reduce depth or increase pruning threshold
  - **High-output problem shows R² < 0.8**: KAN struggles with many outputs; split problem spatially/functionally

- **First 3 experiments:**
  1. **Replicate single-output HEAT dataset**: Start with 7 inputs, 1 output, depth=1, grid=7, k=3. Verify you achieve R² > 0.99 and that the symbolic equation excludes ≥2 features. This builds confidence in the feature elimination mechanism.
  2. **Test simplicity weight sweep**: On CHF dataset, train three KANs with simplicity_weight = [0, 0.5, 1.0]. Compare symbolic equation length vs. R² tradeoff. Establish your own heuristic for accuracy-interpretability balance.
  3. **Compare SHAP rankings against domain knowledge**: On the nuclear cross-section dataset, verify that KAN eliminates scatter11, scatter21, scatter22 (upscattering terms with low physical importance). If FNN assigns non-zero importance to these, document the discrepancy as evidence of KAN's physical relationship capture.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can KANs be modified to maintain high accuracy in problems with high-dimensional output spaces, or is their performance fundamentally limited compared to FNNs in such scenarios?
- Basis in paper: [explicit] The authors observe that the full Power Control dataset (22 outputs) significantly underperformed (R² = 0.73) and state, "Future work should seek to clarify the cause and specific nature of this shortcoming and whether there are any rules of thumb that KAN follows for input and output dimensionality."
- Why unresolved: The study identified a performance gap in high-output problems but only offered spatial subdivision as a workaround rather than an architectural solution.
- What evidence would resolve it: A systematic ablation study varying output dimensionality on KANs versus FNNs to identify the breaking point and testing deeper KAN architectures to see if they recover the lost accuracy.

### Open Question 2
- Question: How can the symbolic equations generated by KANs be simplified or structured to ensure they remain "interpretable" for human operators in sensitive industries?
- Basis in paper: [explicit] The authors question the practical interpretability of their results, asking, "is a long equation still an interpretable equation?" and suggest, "Future work should inquire how to make equations like Eq.(17) more interpretable for specific audiences."
- Why unresolved: While KANs produce mathematically transparent equations, the resulting formulas (e.g., Eq. 18) were often highly complex and lengthy, potentially hindering the "comprehensibility" required for industries like medicine or nuclear power.
- What evidence would resolve it: Research into post-pruning techniques or symbolic simplification methods that reduce equation complexity without significantly degrading the R² score.

### Open Question 3
- Question: Do Physics-Informed Neural Networks (PINNs) outperform KANs in capturing physical relationships and ensuring safety in nuclear engineering applications?
- Basis in paper: [explicit] The conclusion explicitly proposes that "Additional models should include physics-informed neural networks, which may better handle physical relationships than a simple FNN."
- Why unresolved: This study only benchmarked KANs against standard FNNs; it remains unknown if the physics-constrained loss functions of PINNs would yield better accuracy or physical consistency than the data-driven symbolic regression of KANs.
- What evidence would resolve it: A direct comparison of KANs and PINNs on the same nuclear datasets, evaluating both predictive error and adherence to known physical laws (e.g., conservation laws).

## Limitations
- Dataset availability constraints - nuclear engineering datasets not publicly accessible at time of writing
- Symbolic conversion performance gap - symbolic R² scores consistently lower than spline-based predictions (typical drops of 0.02-0.05)
- High-dimensional output problems (>10 outputs) show degraded performance, suggesting scalability limitations
- All ML models, including KANs, struggle with extrapolation beyond training distributions

## Confidence

*High Confidence:* The core claim that KANs produce interpretable symbolic equations while maintaining accuracy (R² typically 0.976-0.999) is well-supported by systematic comparison across eight diverse datasets. The feature elimination mechanism through regularization is demonstrated convincingly with concrete examples (HEAT dataset eliminating 3/7 features).

*Medium Confidence:* The claim that KANs capture "real physical relationships" rather than statistical correlations relies heavily on SHAP analysis interpretation. While SHAP differences between KANs and FNNs are documented, the physical interpretability claim would benefit from domain expert validation.

*Low Confidence:* The scalability of KANs to high-output problems (PC dataset with 22 outputs achieving only R²=0.735) suggests fundamental limitations not fully explored. The paper proposes spatial subdivision as a workaround but doesn't validate this comprehensively.

## Next Checks

1. **Reproduce the HEAT dataset results independently** - Verify that the KAN eliminates 3 of 7 input features while maintaining R² > 0.99, and confirm the symbolic equation excludes the eliminated features with zero SHAP values.

2. **Test symbolic conversion sensitivity** - Systematically vary the simplicity_weight parameter (0, 0.5, 1.0) on the CHF dataset and document the accuracy-interpretability tradeoff curve, particularly noting equation length and R² degradation.

3. **Validate physical relationship claims** - On the nuclear cross-section dataset, confirm that KAN eliminates scatter11, scatter21, scatter22 (upscattering terms) while FNN assigns non-zero importance, then consult nuclear engineering domain experts to assess whether this elimination aligns with known physics.