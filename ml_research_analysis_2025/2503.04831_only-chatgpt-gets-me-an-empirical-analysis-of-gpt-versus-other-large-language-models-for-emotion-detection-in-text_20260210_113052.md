---
ver: rpa2
title: '"Only ChatGPT gets me": An Empirical Analysis of GPT versus other Large Language
  Models for Emotion Detection in Text'
arxiv_id: '2503.04831'
source_url: https://arxiv.org/abs/2503.04831
tags:
- emotion
- emotions
- chatgpt
- language
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# "Only ChatGPT gets me": An Empirical Analysis of GPT versus other Large Language Models for Emotion Detection in Text

## Quick Facts
- arXiv ID: 2503.04831
- Source URL: https://arxiv.org/abs/2503.04831
- Reference count: 40
- Primary result: GPT-4o achieves highest Macro F1 (30.95%) among LLMs but still lags behind fine-tuned BERT SOTA (49-52%)

## Executive Summary
This paper compares various Large Language Models (GPT, Llama, Mistral, Phi) against a fine-tuned BERT model for detecting 28 fine-grained emotions in text using the GoEmotions dataset. The study evaluates zero-shot and few-shot prompting strategies, finding that while GPT-4o performs best among LLMs, it remains significantly behind specialized BERT models. The research highlights that explicit prompt constraints improve model adherence to the emotion taxonomy, but dictionary-based mapping of invalid labels generally reduces precision.

## Method Summary
The study evaluates multiple LLMs (GPT-3.5-Turbo, GPT-4o, Llama-3-70b, etc.) on the GoEmotions test set using "Variant 3" prompts that specify exact emotion counts and formatting. Responses are mapped to 'neutral' if they don't match the 28-label taxonomy. Performance is measured against a fine-tuned BERT baseline (monologg/bert-base-cased-goemotions-original) using Macro F1 score. The methodology focuses on prompt engineering rather than model fine-tuning to test inherent capabilities.

## Key Results
- GPT-4o achieves the highest Macro F1 (30.95%) among LLMs but remains ~20 percentage points below the fine-tuned BERT SOTA
- Prompt Variant 3 (specifying exact emotion count and formatting) improves GPT-3.5-Turbo's Macro F1 from ~22.8% to ~29.0%
- Dictionary mapping of invalid labels to valid emotions generally reduces precision and is deemed "not optimal" by the authors

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Constraining the output space via specific prompt engineering appears to improve LLM adherence to a predefined emotional taxonomy.
- **Mechanism:** By providing explicit instructions on the exact number of emotions and formatting (Variant 3), the model is forced to map its internal semantic representations to the 28 discrete GoEmotions categories rather than generating free-form affective text. This reduces the search space for the autoregressive model.
- **Core assumption:** The model's pre-training has sufficiently encoded the semantic meaning of the specific labels (e.g., "admiration" vs. "approval") such that eliciting them is primarily a constraint/formatting problem rather than a knowledge gap.
- **Evidence anchors:** Section 5.5 shows that "Variant 3" (specifying exact number and examples) improved GPT-3.5-Turbo's Macro F1 score from a baseline of ~22.8% to ~29.0%.
- **Break condition:** This mechanism fails if the text contains ambiguous emotional states that do not map cleanly to single labels, causing the model to hallucinate emotions to satisfy the "exact number" constraint.

### Mechanism 2
- **Claim:** Performance in fine-grained emotion detection correlates with model parameter scale and architecture type (Generative vs. Encoder).
- **Mechanism:** Larger generative models (e.g., GPT-4o, Llama-3-70b) generally capture more nuanced semantic relationships than smaller models (e.g., Gemma-2-9b, Phi-3). However, encoder-only models (BERT) optimized for discrimination currently outperform generative decoders on classification because their architecture and training objectives (masked language modeling) are specifically tuned for deep bidirectional context understanding.
- **Core assumption:** The "SOTA" BERT model has been sufficiently fine-tuned on GoEmotions, establishing a high benchmark that zero-shot or few-shot LLMs cannot match without similar fine-tuning.
- **Evidence anchors:** Table 5 demonstrates that GPT-4o (30.95%) outperforms smaller models like Llama-2-7b (20.24%), but Section 5.3 confirms the fine-tuned SOTA BERT model significantly outperforms all generative LLMs (~52% vs ~29%).
- **Break condition:** This scale-advantage breaks down when smaller models are heavily fine-tuned while larger models are run zero-shot, or when the larger model's reasoning capabilities lead to over-analyzing simple emotional cues.

### Mechanism 3
- **Claim:** Mapping invalid LLM outputs to valid emotion tags via semantic similarity dictionaries generally degrades precision in this specific multi-class context.
- **Mechanism:** When an LLM outputs a label not in the taxonomy (e.g., "happiness" instead of "joy"), using a dictionary (spaCy) to force-map it to the closest valid tag introduces noise. It converts a "hard error" (wrong label) into a "soft error" (semantically close but potentially incorrect label), which penalizes the Macro F1 score by reducing precision across classes.
- **Core assumption:** The penalty for an "out-of-vocabulary" response is less severe than the penalty for a "semantically similar but incorrect" classification, or that the embedding space used for similarity aligns perfectly with the psychological emotion model.
- **Evidence anchors:** Section 5.7 states that integrating dictionaries "generally reduces macro F1 score and precision but improves recall," leading to the conclusion that this approach is "not optimal."
- **Break condition:** This mechanism implies that for strict taxonomy adherence, it is better to discard or penalize invalid responses rather than attempting to "save" them via semantic similarity.

## Foundational Learning

### Concept: Macro F1 Score
- **Why needed here:** Emotion datasets like GoEmotions are imbalanced (e.g., "Neutral" is common, "Grief" is rare). Accuracy would be misleading if the model just predicts the majority class. Macro F1 averages the score for each class equally, forcing the model to perform well on rare emotions.
- **Quick check question:** If a model achieves 90% accuracy by predicting "Neutral" for every sentence in a skewed dataset, would it have a high or low Macro F1 score?

### Concept: GoEmotions Taxonomy
- **Why needed here:** Unlike basic sentiment (Positive/Negative) or Ekman's 6 basic emotions, GoEmotions uses 28 categories (e.g., "Curiosity," "Disappointment"). Understanding this granularity is essential to interpreting why models struggle (confusing "Annoyance" with "Anger").
- **Quick check question:** Why is detecting "Optimism" considered harder than detecting "Sadness" in this dataset context?

### Concept: Zero-shot Classification
- **Why needed here:** The paper evaluates LLMs primarily via prompting (zero/few-shot) rather than weight updates (fine-tuning). This tests the model's inherent ability to follow instructions and apply pre-trained knowledge without specific dataset training.
- **Quick check question:** Does the "SOTA" BERT model used in the paper operate in a zero-shot manner, or has it seen the test data distribution during training?

## Architecture Onboarding

### Component map:
GoEmotions Test Set (Text strings) -> Prompting Layer (Python script constructing "Variant 3" prompts) -> Inference Engines (OpenAI API, HuggingChat API, Local Ollama) -> Validation Layer (spaCy dictionary mapping + Scikit-learn F1 Macro calculation) -> Baseline (Monologg/bert-base-cased-goemotions-original)

### Critical path:
The prompt construction is the most brittle component. If the prompt does not explicitly constrain the output format (e.g., "Return exactly {n} items"), the LLM generates conversational text, breaking the evaluation parser.

### Design tradeoffs:
- *Generality vs. Accuracy:* Using GPT-4o offers the best generative performance but is costly and still ~20 percentage points behind the specialized BERT model.
- *Dictionary Mapping:* Attempting to autocorrect invalid labels (e.g., "excited" -> "excitement") reduces precision. The tradeoff is strict parsing (higher precision, lower recall) vs. fuzzy matching (lower precision, higher recall). The paper recommends strict parsing.

### Failure signatures:
- **Format Drift:** The model explains its reasoning ("I think this is joy because...") instead of outputting the label "Joy".
- **Neutral-blindness:** LLMs tend to force an emotion classification on objective text where the SOTA model would predict "Neutral".
- **Hallucinated Labels:** Models output emotions present in general English but absent from the 28-label GoEmotions taxonomy (e.g., "Happiness").

### First 3 experiments:
1. **Baseline Verification:** Run the provided "Variant 3" prompt against the GoEmotions test set using GPT-3.5-Turbo to verify if the Macro F1 approximates the reported ~29%.
2. **Ablation on Constraints:** Remove the "Return exactly N emotions" constraint from the prompt and measure the drop in F1 score to quantify the value of the prompt engineering mechanism.
3. **Binary vs. Granular:** Test the best-performing LLM on a binary task (Positive vs. Negative) vs. the 28-class task to determine if the failure mode is the *granularity* of the taxonomy or the *detection* of affect in general.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a semantic proximity-based evaluation metric better capture LLM performance nuances than macro F1?
- Basis in paper: Section 7 states future efforts include "introducing a new evaluation metric that accounts for semantic proximity."
- Why unresolved: Section 5.6 notes that current metrics treat all errors equally, failing to capture semantic closeness (e.g., predicting "annoyance" instead of "anger" is better than "joy").
- What evidence would resolve it: Development of a metric that weights errors based on semantic distance in emotion models (like Plutchik's wheel), validated against human judgment.

### Open Question 2
- Question: How robust are these findings across datasets with diverse structures and cultural contexts?
- Basis in paper: Section 6 identifies the "primary reliance on the GoEmotions dataset" as a limitation regarding generalizability.
- Why unresolved: The study restricted testing to a single dataset of English Reddit comments.
- What evidence would resolve it: Testing the same LLMs on datasets with different annotation schemes (e.g., ISEAR, CARER) and non-English texts to confirm if GPT models maintain their dominance.

### Open Question 3
- Question: Are the observed performance differences between models statistically significant?
- Basis in paper: Section 7 states the intent to "incorporate rigorous statistical validation" in future work.
- Why unresolved: The current study reports macro F1 scores but does not calculate p-values or confidence intervals to prove differences are not due to chance.
- What evidence would resolve it: Statistical tests (e.g., bootstrap sampling or significance tests) applied to the model outputs to formally verify performance gaps.

## Limitations

- The zero-shot/few-shot prompting approach cannot match the performance of fine-tuned BERT models, creating a ~20 percentage point performance gap
- The study relies on a single dataset (GoEmotions) of English Reddit comments, limiting generalizability to other domains and languages
- The paper does not specify generation parameters (temperature, top_p) or the exact prompt wording, making precise reproduction difficult

## Confidence

- **High Confidence:** Encoder-based BERT models outperform generative LLMs on this fine-grained emotion classification task when BERT is fine-tuned on the dataset. Dictionary mapping degrades precision.
- **Medium Confidence:** Specific performance rankings of different LLM variants are sensitive to unknown generation parameters and exact prompt wording.
- **Low Confidence:** Generalizability of findings to other emotion taxonomies or different text domains beyond Reddit comments.

## Next Checks

1. **Prompt Engineering Ablation:** Systematically remove or modify components of the "Variant 3" prompt (e.g., remove the "exactly N emotions" constraint, remove the example formatting) and measure the impact on Macro F1 score.

2. **Generation Parameter Sweep:** Evaluate the same LLM models across a range of temperature and top_p values to determine if reported performance differences are robust to these hyperparameters.

3. **Cross-Domain Evaluation:** Test the best-performing LLM (GPT-4o) on a different emotion dataset (e.g., ISEAR or EmoBank) to assess whether the performance gap with the BERT baseline persists outside the GoEmotions domain.