---
ver: rpa2
title: 'Poodle: Seamlessly Scaling Down Large Language Models with Just-in-Time Model
  Replacement'
arxiv_id: '2512.05525'
source_url: https://arxiv.org/abs/2512.05525
tags:
- search
- task
- cost
- surrogate
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Poodle enables automatic replacement of large language models (LLMs)
  with smaller specialized models for recurring tasks, reducing cost and latency while
  maintaining accuracy. It monitors LLM usage, identifies recurring tasks using wrapper
  prompts, and collects labeled training data.
---

# Poodle: Seamlessly Scaling Down Large Language Models with Just-in-Time Model Replacement

## Quick Facts
- arXiv ID: 2512.05525
- Source URL: https://arxiv.org/abs/2512.05525
- Reference count: 34
- One-line result: Achieves up to 82× cost reduction and 10× faster inference compared to LLMs for recurring tasks

## Executive Summary
Poodle automatically replaces large language models (LLMs) with smaller specialized models for recurring tasks, significantly reducing cost and latency while maintaining accuracy. The system monitors LLM usage, identifies recurring tasks through wrapper prompts, and collects labeled training data. Model search selects appropriate base models from Hugging Face, which are then fine-tuned using knowledge distillation from the LLM. Experiments on sentiment classification demonstrate up to 82× cost reduction and 10× faster inference, with model search plus fine-tuning being 4-19× faster than naive alternatives.

## Method Summary
Poodle implements just-in-time model replacement by intercepting user requests and applying wrapper prompts to extract task metadata and collect labeled data from LLM responses. It then searches a model store for suitable base models using transfer learning predictions, fine-tunes the selected model via knowledge distillation, and deploys it when usage crosses a break-even threshold. The approach amortizes the initial development overhead across recurring requests, with experiments showing significant savings at scale particularly for self-hosted models.

## Key Results
- Up to 82× cost reduction and 10× faster inference compared to LLMs
- Model search followed by fine-tuning is 4-19× faster than naive search-and-train approaches
- Break-even point of 10,000 requests for large models like GPT-4.1 or Llama 405 Turbo
- Accuracy competitive with original LLM (≥0.89 on IMDB sentiment classification)

## Why This Works (Mechanism)

### Mechanism 1: Self-Supervised Task Extraction via Wrapper Prompts
Poodle automates labeled dataset creation by injecting wrapper prompts around user requests, forcing LLMs to output structured metadata (task type, input modality) in JSON format. This mechanism assumes LLMs can accurately infer user intent from single requests and that their outputs serve as high-quality pseudo-labels. The approach fails when prompts are ambiguous or complex, leading to misclassification and noisy datasets.

### Mechanism 2: Search-Guided Knowledge Distillation
The system uses model search engines to rank existing pre-trained models based on predicted transfer performance, then distills LLM knowledge into the selected smaller model. This assumes suitable base models exist in the model store for identified tasks. The mechanism fails when the store lacks relevant models, forcing fallback to generic models that may not converge effectively.

### Mechanism 3: Cost Amortization over Recurring Inference
Development overhead (wrapper tokens + GPU hours) is economically justified when task recurrence crosses a break-even point, after which lower inference costs pay back initial investment. This assumes predictable, stable task volumes. The approach fails when usage patterns change shortly after deployment, preventing cost recovery.

## Foundational Learning

- **Knowledge Distillation**: Transfer capabilities from teacher LLM to student surrogate model. Quick check: Can you explain why a student model trained on LLM labels might outperform the LLM on a specific narrow task?

- **Transfer Learning & Model Zoos**: Poodle searches for base models rather than training from scratch. Quick check: Why is fine-tuning a pre-trained sentiment analysis model faster than training a classifier on raw text?

- **Prompt Engineering for Structured Output**: Wrapper prompts force LLMs to output valid JSON. Quick check: How would you modify a prompt to guarantee the output is always a valid JSON object?

## Architecture Onboarding

- **Component map**: Data Collector -> Model Generator -> Model Store -> Model Monitor
- **Critical path**: Request Intercept -> Wrapper Injection -> Data Logging -> Threshold Trigger -> Model Search -> Distillation -> Deployment
- **Design tradeoffs**: 
  - Wrapper verbosity vs. Cost: Complex wrappers extract better metadata but increase token costs
  - Search depth vs. Time: Exhaustive search finds better fits but delays deployment
  - Accuracy threshold vs. Savings: High accuracy requirements reduce risk but disqualify cheaper models
- **Failure signatures**:
  - Endless loop: Model search fails to find candidates meeting accuracy thresholds
  - Label drift: LLM output style changes invalidate previously collected datasets
  - Latency spikes: Wrapper prompts double input token count, causing noticeable user latency
- **First 3 experiments**:
  1. Unit Test the Wrapper: Run 50 diverse prompts through wrapper logic to verify JSON field extraction
  2. Reproduce Break-even Analysis: Simulate cost calculation using local token counts to verify 10,000 request claim
  3. Mock Model Search: Run search against 10 models to ensure ranking logic prioritizes domain-specific models

## Open Questions the Paper Calls Out

- **Balancing metadata quality against detection overhead**: How to optimize the trade-off between high-quality metadata extraction and wrapper prompt costs. Requires comparative analysis across different metadata extraction methods.

- **Effective metadata types for model search scaling**: Which specific metadata types best prune model search space when scaling to millions of models. Needs benchmarks evaluating search latency and accuracy using different metadata indexing strategies.

- **Searching over approximated models**: Whether compressed model searches yield comparable results to full-precision searches. Requires correlation analysis between rankings from approximated vs. full models.

## Limitations
- Heavy dependence on LLM accuracy for task extraction, which can fail on ambiguous or complex prompts
- Lacks detailed hyperparameter specifications for fine-tuning, making exact reproduction challenging
- Model search effectiveness depends on quality and coverage of underlying model store
- Break-even analysis assumes stable task volumes without addressing usage pattern changes

## Confidence
- Cost reduction claims (82×): Medium confidence - well-supported but usage-dependent
- Model search + fine-tuning speed (4-19×): Medium confidence - results specific to sentiment classification
- Accuracy maintenance (≥0.89): Medium confidence - validated on IMDB but limited task diversity
- Break-even analysis (10,000 requests): Low confidence - based on specific cost assumptions

## Next Checks
1. **Wrapper prompt robustness test**: Run wrapper system on 100 diverse, real-world prompts across multiple domains to measure task classification accuracy and identify failure modes
2. **Model store coverage audit**: Analyze distribution of models in search space to determine what fraction of common NLP tasks have suitable base models available
3. **Dynamic usage simulation**: Create simulation modeling varying request patterns (bursty vs steady, task drift over time) to stress-test amortization model and identify breakdown conditions