---
ver: rpa2
title: Analyzing the Effects of Supervised Fine-Tuning on Model Knowledge from Token
  and Parameter Levels
arxiv_id: '2509.16596'
source_url: https://arxiv.org/abs/2509.16596
tags:
- subject
- uni00000013
- data
- performance
- uni00000018
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how supervised fine-tuning (SFT) affects
  the factual knowledge of large language models (LLMs) on the closed-book question
  answering (CBQA) task. By systematically varying the category and scale of fine-tuning
  data, we observe that models fine-tuned on 1,920 samples can perform up to 14% worse
  than those fine-tuned on only 240 samples.
---

# Analyzing the Effects of Supervised Fine-Tuning on Model Knowledge from Token and Parameter Levels

## Quick Facts
- **arXiv ID:** 2509.16596
- **Source URL:** https://arxiv.org/abs/2509.16596
- **Reference count:** 32
- **Primary result:** Models fine-tuned on 1,920 samples can perform up to 14% worse than those fine-tuned on only 240 samples due to knowledge erosion and redundant parameter updates.

## Executive Summary
This study investigates how supervised fine-tuning (SFT) affects the factual knowledge of large language models (LLMs) on closed-book question answering tasks. By systematically varying the category and scale of fine-tuning data, we observe that models fine-tuned on 1,920 samples can perform up to 14% worse than those fine-tuned on only 240 samples. Further analysis at both the token and parameter levels reveals that up to 90% of parameter updates during SFT do not contribute to knowledge enhancement. Restoring these unnecessary updates significantly improves model performance, with gains exceeding 10% in some cases. These findings highlight the potential for optimizing fine-tuning strategies to better preserve and strengthen model knowledge.

## Method Summary
The study uses LLaMA-2 and LLaMA-3 models, fine-tuning them on ENTITYQUESTIONS data categorized by mastery level (0-100% completion accuracy). The experiments compare different data scales (60, 240, 1,920 samples) and mastery levels. SFT is performed using AdamW optimizer with cosine learning rate schedule, batch size 8, and learning rate 1e-5 for one epoch. Performance is measured using accuracy on in-domain and out-of-domain test sets. The analysis includes KL divergence between fine-tuned and pre-trained logits, and parameter-level analysis using relative change calculations to identify and restore unnecessary updates.

## Key Results
- Performance degrades as fine-tuning data scale increases from 240 to 1,920 samples, with up to 14% accuracy loss
- Up to 90% of parameter updates during SFT are unnecessary for knowledge enhancement
- Restoring unnecessary parameter updates improves performance by over 10% in some cases
- Low-mastery data causes steeper performance drops than mid-mastery data
- Excessive KL divergence between fine-tuned and pre-trained logits correlates with performance decline

## Why This Works (Mechanism)

### Mechanism 1: Distributional Drift and Knowledge Erosion
Performance degradation in fine-tuning appears correlated with excessive deviation of the output distribution from the pre-trained baseline. As fine-tuning data scales up, the KL divergence initially decreases but then rises sharply, particularly with "low-mastery" data (facts the model initially got wrong). This excessive shift in the logit distribution correlates with a drop in Closed-Book Question Answering (CBQA) accuracy, suggesting the model is "forgetting" or distorting its pre-trained knowledge base.

### Mechanism 2: Redundant Parameter Updates
A significant majority (up to 90%) of parameter updates during SFT are unnecessary for knowledge enhancement and may actively harm performance. When these unnecessary parameters are restored to their pre-trained values, performance improves by over 10%. This implies standard SFT introduces "noise" or detrimental updates alongside useful signal.

### Mechanism 3: Data Mastery-Dependent Interference
Fine-tuning on data the model has not mastered ("low-mastery" data) causes significantly more damage than fine-tuning on data it already knows. Training on unfamiliar facts forces the model to alter its internal representations more aggressively, leading to higher KL divergence and "catastrophic forgetting" of other domains.

## Foundational Learning

- **Concept: KL Divergence (Kullback-Leibler Divergence)**
  - **Why needed here:** Used as the primary metric to detect "catastrophic forgetting" by measuring how far the fine-tuned model's output probabilities have drifted from the pre-trained baseline
  - **Quick check question:** If KL divergence is 0, what does that imply about the relationship between the two probability distributions? (Answer: They are identical)

- **Concept: Relative Parameter Change**
  - **Why needed here:** The paper ranks parameters not by absolute weight, but by how much they changed relative to their original magnitude
  - **Quick check question:** Why use relative change rather than absolute difference when analyzing weight updates across different layers? (Answer: To account for layers with naturally smaller magnitude weights)

- **Concept: Closed-Book Question Answering (CBQA)**
  - **Why needed here:** The benchmark used to measure "knowledge" rather than "reasoning" or "tool use"
  - **Quick check question:** Why is CBQA a better test for "knowledge preservation" than a standard text generation benchmark? (Answer: It strictly isolates internal factual recall without external context)

## Architecture Onboarding

- **Component map:** Pre-trained Backbone -> Mastery Evaluator -> SFT Loop -> Restoration Filter
- **Critical path:** 1. Data Selection: Categorize training samples by the base model's initial success rate (Mastery Level). 2. Fine-Tuning: Run standard SFT. 3. Analysis: Calculate KL divergence on a held-out test set to check for distribution collapse. 4. Intervention: Calculate relative parameter change. Restore top 10-20% of parameters (keep changes) and revert the rest (undo changes).
- **Design tradeoffs:**
  - Data Scale: 240 samples often outperforms 1,920 samples. "More data" â‰  "Better performance" for knowledge retention
  - Mastery Level: Low-mastery data teaches new facts but kills old ones. High-mastery data preserves old facts but teaches less. Mid-level (D^M_train-2) is the sweet spot
- **Failure signatures:**
  - Logit Collapse: Sudden spikes in KL divergence during training
  - Inverse Scaling: Accuracy drops as dataset size increases from 240 to 1920
  - Training Set Overfitting: High training accuracy but degraded test accuracy
- **First 3 experiments:**
  1. The "Less is More" Test: Fine-tune the base model on 240 samples vs. 1920 samples of the same data category. Verify if the smaller dataset yields higher CBQA accuracy
  2. The Mastery Interference Test: Train three models using low-mastery (D^M_train-0), mid-mastery (D^M_train-2), and high-mastery (D^M_train-4) data. Compare their performance drops on out-of-domain test sets
  3. Parameter Restoration Sweep: Take a degraded model (e.g., trained on 1920 samples). Iteratively restore 1%, 5%, 10%, 20%, 40% of parameters to their original values and plot the accuracy curve to find the optimal retention ratio

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can adaptive fine-tuning strategies be designed to actively minimize unnecessary parameter updates during the training process rather than restoring them post-hoc?
- **Basis in paper:** The "Limitations" section states that the study focuses on phenomenological analysis and that "Future work should focus on designing adaptive fine-tuning strategies that minimize unnecessary updates while maximizing performance gains"
- **Why unresolved:** The paper demonstrates that restoration improves performance but does not propose a method to prevent the accumulation of redundant updates during the initial fine-tuning run

### Open Question 2
- **Question:** Is the concentration of redundant parameter updates in specific layers and modules (e.g., initial layers, FFNs) mechanistically linked to the Lottery Ticket Hypothesis?
- **Basis in paper:** Appendix E.1 notes the distribution of redundant updates and states, "We also acknowledge that the emergence of redundant parameters may be linked to the lottery ticket hypothesis," but the paper does not verify this connection
- **Why unresolved:** While the paper identifies where redundant updates occur, it does not determine if these updates overwrite specific "winning ticket" subnetworks essential for knowledge retention

### Open Question 3
- **Question:** Do the negative effects of large-scale fine-tuning and the benefits of parameter restoration generalize to non-decoder-only architectures?
- **Basis in paper:** Section 3.2 states the analysis "focuses exclusively on models of this type [decoder-only]" and the Limitations section notes the "analysis is limited to the LLaMA-2 and LLaMA-3 model series"
- **Why unresolved:** The architectural specificities of decoder-only models might influence how knowledge is stored and overwritten; it is unclear if encoder-decoder or encoder-only models exhibit the same 90% redundancy

## Limitations
- The analysis relies on a specific mastery categorization scheme that is not fully reproducible without access to the appendix or code
- The paper's findings on parameter restoration assume that 90% of unchanged parameters are "harmless" when restored, conflating absence of positive impact with presence of negative impact
- The KL divergence analysis is correlational rather than proven causal

## Confidence
- **High Confidence:** The core empirical finding that 240 samples often outperforms 1,920 samples on the same data category
- **Medium Confidence:** The mechanism that excessive KL divergence correlates with performance decline
- **Medium Confidence:** The parameter restoration approach improving performance by >10%

## Next Checks
1. **Mastery Binning Sensitivity Test:** Re-run the entire experimental pipeline using alternative mastery binning strategies to determine if the performance degradation pattern holds across different categorization schemes
2. **KL Divergence Intervention Experiment:** During SFT training, implement a KL penalty term that constrains the fine-tuned logits to stay within a threshold of the pre-trained distribution
3. **Parameter Attribution Ablation:** After SFT, systematically restore parameters in different groups (by layer, by magnitude change, by random selection) rather than the top-k% by relative change