---
ver: rpa2
title: 'Deciding When Not to Decide: Indeterminacy-Aware Intrusion Detection with
  NeutroSENSE'
arxiv_id: '2507.00003'
source_url: https://arxiv.org/abs/2507.00003
tags:
- neutrosophic
- indeterminacy
- uncertainty
- ensemble
- abstention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces NeutroSENSE, a neutrosophic-enhanced ensemble
  framework for interpretable intrusion detection in IoT environments. The system
  integrates Random Forest, XGBoost, and Logistic Regression with neutrosophic logic
  to decompose prediction confidence into truth (T), falsity (F), and indeterminacy
  (I) components, enabling uncertainty quantification and abstention.
---

# Deciding When Not to Decide: Indeterminacy-Aware Intrusion Detection with NeutroSENSE

## Quick Facts
- arXiv ID: 2507.00003
- Source URL: https://arxiv.org/abs/2507.00003
- Reference count: 24
- Primary result: NeutroSENSE achieved 97% accuracy on IoT-CAD dataset, with misclassified samples showing significantly higher indeterminacy (I = 0.62) than correct ones (I = 0.24)

## Executive Summary
This paper introduces NeutroSENSE, a neutrosophic-enhanced ensemble framework for interpretable intrusion detection in IoT environments. The system integrates Random Forest, XGBoost, and Logistic Regression with neutrosophic logic to decompose prediction confidence into truth (T), falsity (F), and indeterminacy (I) components, enabling uncertainty quantification and abstention. High-indeterminacy predictions are flagged for review using global and class-specific thresholds. Evaluated on the IoT-CAD dataset, NeutroSENSE achieved 97% accuracy. Misclassified samples exhibited significantly higher indeterminacy (I = 0.62) than correct ones (I = 0.24), validating indeterminacy as a proxy for uncertainty. This approach enhances interpretability, supports trust-aware AI decisions, and provides a practical foundation for edge-based cybersecurity systems.

## Method Summary
NeutroSENSE employs an ensemble of Random Forest, XGBoost, and Logistic Regression classifiers trained on the IoT-CAD dataset with 140,000+ samples across 8 classes. The system computes soft voting probabilities, then applies neutrosophic logic to decompose confidence into T, I, and F components. Indeterminacy (I) is calculated as normalized entropy across class probabilities. Predictions exceeding class-specific 80th percentile I-score thresholds are routed to a human review queue. The framework uses SMOTE for class balancing, StandardScaler for normalization, and evaluates performance using accuracy, coverage, and Youden Index metrics.

## Key Results
- Achieved 97% classification accuracy on IoT-CAD dataset
- Misclassified samples showed significantly higher indeterminacy (I = 0.62) versus correct predictions (I = 0.24)
- Demonstrated clear separation between correct and incorrect predictions based on indeterminacy scores
- Implemented adaptive class-specific thresholds achieving balanced abstention across different attack types

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Neutrosophic logic decomposes ensemble uncertainty into distinct Truth (T), Falsity (F), and Indeterminacy (I) components, providing a richer signal than simple probability scores.
- **Mechanism:** The system aggregates predictions from Random Forest, XGBoost, and Logistic Regression. Instead of using the max probability alone, it calculates an **Indeterminacy (I)** score based on normalized entropy across the class probabilities. High entropy indicates the ensemble is conflicted or the sample lies in an ambiguous region.
- **Core assumption:** The paper assumes that entropy across ensemble probabilities is a reliable proxy for epistemic uncertainty and that high entropy correlates with ambiguous or overlapping traffic patterns.
- **Evidence anchors:**
  - [abstract]: "...system decomposes prediction confidence into truth (T), falsity (F), and indeterminacy (I) components, enabling uncertainty quantification..."
  - [section]: Page 4, Section C: "The indeterminacy score I is normalized entropy, scaled between 0 and 1. Values near 0 reflect confident predictions, while values near 1 indicate uncertainty..."
  - [corpus]: Corpus signals (e.g., "UNCA") support the general use of Neutrosophic logic for handling indeterminacy, but do not directly validate this specific ensemble-entropy method.
- **Break condition:** If base learners are highly correlated (lack diversity), they may exhibit low entropy (low I) while still being confidently wrong, causing the uncertainty proxy to fail.

### Mechanism 2
- **Claim:** The Indeterminacy score (I) functions as a statistically valid proxy for error likelihood, allowing the system to predict its own failure modes.
- **Mechanism:** By analyzing the distribution of I-scores, the paper establishes that misclassified samples exhibit significantly higher Indeterminacy (I = 0.62) compared to correct ones (I = 0.24). This separation allows operators to use I-score thresholds to filter out likely errors.
- **Core assumption:** The relationship between high indeterminacy and misclassification is stable across different attack types and is not merely an artifact of the IoT-CAD dataset's specific noise profile.
- **Evidence anchors:**
  - [abstract]: "...demonstrating that misclassified samples exhibit significantly higher indeterminacy (I = 0.62) than correct ones (I = 0.24)."
  - [section]: Page 6, Table VI and Figure 7 validate the clear separation in I-scores between correct and incorrect predictions.
  - [corpus]: General IDS literature accepts uncertainty quantification as valuable, but specific validation of I=0.62 vs I=0.24 is isolated to this paper.
- **Break condition:** If an attacker crafts adversarial examples specifically designed to lower entropy (confident wrong answers), the I-score will no longer flag these critical errors.

### Mechanism 3
- **Claim:** Adaptive, class-specific thresholds on Indeterminacy scores enable a practical "abstention" mechanism, improving system safety by deferring ambiguous cases to humans.
- **Mechanism:** Rather than a global threshold, the system applies class-conditional thresholds (e.g., the 80th percentile of I-scores per class). This accounts for inherent differences in class overlap (e.g., "Normal" traffic is naturally more ambiguous than "Malware"). Predictions exceeding this threshold are routed to a Review Queue rather than being automated.
- **Core assumption:** The cost of manual review (delay/resource usage) is lower than the cost of automated false positives/negatives in an edge environment.
- **Evidence anchors:**
  - [abstract]: "Predictions with high indeterminacy are flagged for review using both global and adaptive, class-specific thresholds."
  - [section]: Page 6, Section E: "...computed the 80th percentile indeterminacy score I within each class... flagging approximately 20% of each class's samples..."
  - [corpus]: Corpus evidence on "Adaptive Intrusion Detection" supports adaptive mechanisms, but does not specifically validate the 80th percentile abstention strategy.
- **Break condition:** If the review queue volume exceeds human capacity (alert fatigue), the system creates a denial-of-service for the human analysts, breaking the operational "human-in-the-loop" model.

## Foundational Learning

- **Concept: Neutrosophic Logic (T, I, F)**
  - **Why needed here:** Standard probability gives a single confidence number (e.g., 80%). Neutrosophic logic splits this into Truth (support for class), Falsity (support against class), and Indeterminacy (uncertainty/ambiguity). You need this to understand how the system measures "unknowns."
  - **Quick check question:** If a model predicts "Attack" with 51% probability, how does Neutrosophic logic interpret this compared to a standard classifier?

- **Concept: Ensemble Diversity & Entropy**
  - **Why needed here:** The mechanism relies on Random Forest, XGBoost, and Logistic Regression disagreeing to calculate Indeterminacy. If these models always agree, entropy is zero and the mechanism fails.
  - **Quick check question:** Why would averaging the predictions of three identical decision trees result in zero Indeterminacy (I-score) in this framework?

- **Concept: Selective Prediction (Abstention)**
  - **Why needed here:** The core value proposition is "Deciding When Not to Decide." You must understand the trade-off between Coverage (how much you automate) and Risk (how often you are wrong).
  - **Quick check question:** In the context of IoT security, what is the operational risk of setting the abstention threshold too low (abstaining too often)?

## Architecture Onboarding

- **Component map:** Edge Data -> Preprocessing (One-hot/Normalize) -> Ensemble (RF, XGBoost, LR) -> Neutrosophic Reasoning (T/I/F) -> Decision Logic (I-score threshold) -> Outflow (Action or Review Queue)

- **Critical path:** The **Neutrosophic Reasoning Module**. The conversion of raw probabilities into the Indeterminacy (I) score is the step where interpretability is generated. If this math is flawed, the downstream abstention is meaningless.

- **Design tradeoffs:**
  - **Accuracy vs. Coverage:** You can achieve ~99.8% accuracy by abstaining on all high-I cases, but you lose automation coverage (dropping to ~60-80% automation).
  - **Global vs. Adaptive Thresholds:** Global thresholds are simpler to implement but flag too many "Normal" traffic cases (which naturally have higher ambiguity). Adaptive thresholds require more tuning data but balance the load better.

- **Failure signatures:**
  - **High "Normal" Indeterminacy:** Figure 4 shows "Normal" traffic often has high I-scores. Watch out for the system flagging legitimate but complex user behavior as suspicious.
  - **Queue Overflow:** If the Review Queue grows faster than the review rate, the threshold logic needs adjustment.

- **First 3 experiments:**
  1. **Reproduce I-Score Distribution:** Run the ensemble on a holdout set and plot the histogram of Indeterminacy scores. Verify the bimodal or skewed distribution shown in Figure 3.
  2. **Threshold Sensitivity Analysis:** Sweep thresholds (0.1 to 0.9) and plot Accuracy vs. Coverage to find the "knee" of the curve (Youden Index peak) for your specific data.
  3. **Calibration Check:** Select 50 samples with I > 0.6 and 50 with I < 0.2. Manually verify if high-I samples are indeed ambiguous or mislabeled.

## Open Questions the Paper Calls Out

- **Question:** How does NeutroSENSE performance compare to standard uncertainty quantification methods like prediction entropy or margin sampling?
  - **Basis in paper:** [inferred] The Discussion section explicitly notes, "Although we did not benchmark against other uncertainty methods (e.g., entropy, margin), our findings show that neutrosophic reasoning enables behaviors beyond conventional ensembles."
  - **Why unresolved:** It remains unclear if the computational overhead of neutrosophic decomposition provides a significant advantage in error detection accuracy over simpler, standard ensemble confidence metrics.
  - **What evidence would resolve it:** A comparative ablation study measuring the correlation between misclassification rates and uncertainty scores derived from neutrosophic logic versus Shannon entropy.

- **Question:** Can the adaptive thresholding mechanism be automated for real-time deployment without relying on historical percentiles?
  - **Basis in paper:** [explicit] The Conclusion states that future work "will focus on real-time deployment and adaptive threshold tuning in edge environments."
  - **Why unresolved:** The current methodology relies on post-hoc analysis using the 80th percentile of indeterminacy scores on the test set; a dynamic mechanism for live traffic is yet to be defined.
  - **What evidence would resolve it:** An algorithm capable of adjusting indeterminacy thresholds in real-time streaming data while maintaining a stable Youden Index (balancing coverage and accuracy).

- **Question:** Does the high indeterminacy signal effectively identify novel zero-day attacks rather than just modeling class overlap?
  - **Basis in paper:** [inferred] The Introduction highlights the inadequacy of traditional IDS against "zero-day vulnerabilities," and the system flags ambiguous samples, but evaluation is conducted on a labeled dataset with known attack types.
  - **Why unresolved:** It is undetermined if high indeterminacy scores are merely proxies for "hard" known samples or if they successfully generalize to completely unseen attack categories.
  - **What evidence would resolve it:** Testing the framework on a dataset containing zero-day attacks to verify if the I-score acts as a reliable Out-of-Distribution (OOD) detector.

## Limitations
- Performance depends on base learner diversity; highly correlated models may fail to capture true uncertainty
- Manual review queue volume could become unmanageable in high-traffic edge environments
- Framework's generalization to zero-day attacks and datasets beyond IoT-CAD requires validation

## Confidence
- **High Confidence:** The general concept of using indeterminacy scores to flag ambiguous predictions is well-supported by the data separation (I=0.62 vs I=0.24 for misclassified vs correct samples). The neutrosophic decomposition mechanism is clearly specified.
- **Medium Confidence:** The abstention strategy's practical effectiveness depends on the cost-benefit analysis of manual review vs automated errors, which isn't fully quantified. The framework's robustness to adversarial manipulation remains unproven.
- **Low Confidence:** Generalization to datasets beyond IoT-CAD and real-world deployment scenarios with dynamic attack patterns requires validation.

## Next Checks
1. **Adversarial Robustness Test:** Generate adversarial examples designed to minimize entropy (appear confident) and verify if the I-score still correctly identifies them as high-risk predictions.
2. **Dataset Transferability:** Evaluate the framework on a different IoT intrusion detection dataset (e.g., TON_IoT) to assess generalization beyond IoT-CAD.
3. **Human-in-the-Loop Simulation:** Implement a simulated review queue system with varying human analyst capacities to determine the maximum sustainable traffic volume before alert fatigue degrades performance.