---
ver: rpa2
title: 'Structure and Destructure: Dual Forces in the Making of Knowledge Engines'
arxiv_id: '2509.00949'
source_url: https://arxiv.org/abs/2509.00949
tags:
- knowledge
- language
- forgetting
- page
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This thesis explores the connections between structured and unstructured
  paradigms for building knowledge engines, revealing that both rely on structure
  formation through language modeling objectives. In structured learning, language
  modeling helps complete knowledge graphs by predicting missing relations, while
  in unstructured learning, it uncovers latent n-gram structures in large language
  models that enhance interpretability.
---

# Structure and Destructure: Dual Forces in the Making of Knowledge Engines

## Quick Facts
- **arXiv ID:** 2509.00949
- **Source URL:** https://arxiv.org/abs/2509.00949
- **Reference count:** 0
- **Primary result:** Active forgetting improves cross-lingual transfer in low-resource settings by periodically resetting token embeddings during pretraining.

## Executive Summary
This thesis bridges structured and unstructured knowledge representation paradigms by revealing that language modeling objectives serve dual roles in both knowledge graph completion and large language model interpretability. The work introduces active forgetting as a destructuring technique that improves model plasticity by periodically resetting token embeddings, enabling better generalization to unseen entities and languages. Through systematic analysis of structure formation in factorization models and transformers, the research demonstrates that balancing structuring and destructuring forces is essential for creating adaptive, general-purpose knowledge engines.

## Method Summary
The thesis develops two complementary approaches: structure formation through language modeling objectives and destructuring via active forgetting. For structured learning, it adds relation prediction as an auxiliary objective for knowledge base completion and extracts n-gram structures from transformers by decomposing them into factorization-like paths. For destructuring, it introduces periodic embedding resets during pretraining (every K=1000 steps) that reset token embeddings and optimizer states while preserving the transformer body. This approach is evaluated through zero-shot cross-lingual transfer on XNLI, MLQA, and XQuAD datasets, comparing standard pretraining against forgetting pretraining in low-resource language adaptation scenarios.

## Key Results
- Active forgetting improves zero-shot cross-lingual transfer accuracy by up to 15% in low-resource settings (5M tokens per language)
- Transformer architectures contain latent n-gram structures that can be systematically extracted through residual expansion and factorization analysis
- Periodic embedding resets enable models to recover from catastrophic forgetting and maintain plasticity for novel languages and entities

## Why This Works (Mechanism)
The thesis reveals that language modeling objectives inherently drive structure formation in both structured and unstructured paradigms. In knowledge graphs, predicting missing relations completes latent structures, while in transformers, self-attention layers implicitly capture n-gram factorizations. Active forgetting works by preventing the embedding space from becoming overly specialized to training data, maintaining representational flexibility through periodic resets that force the model to relearn from scratch while preserving learned semantic relationships in the transformer body.

## Foundational Learning
- **Factorization Models (FMs):** Decompose probability distributions into product of factors; needed for understanding how structured knowledge can be represented as joint probability distributions over entities and relations. Quick check: Can represent knowledge graphs as bipartite factorization problems.
- **Message-Passing GNNs:** Aggregate neighbor information through iterative message passing; needed for understanding how graph neural networks learn node representations through local structure propagation. Quick check: Can implement GNNs as message passing with node/edge feature updates.
- **Transformer Self-Attention:** Computes weighted sum of value vectors based on query-key similarity; needed for understanding how transformers capture long-range dependencies and can be decomposed into factorization-like operations. Quick check: Can visualize attention patterns as soft selection matrices.
- **Active Forgetting Mechanism:** Periodic parameter reset with continued training; needed for understanding how to maintain model plasticity and prevent overfitting to training distribution. Quick check: Can implement through conditional parameter reset during training loop.

## Architecture Onboarding
- **Component Map:** Pretraining Corpus -> Active Forgetting Loop -> Token Embeddings Reset -> Transformer Body Training -> Cross-lingual Evaluation
- **Critical Path:** The forgetting frequency K determines the balance between structure formation and destructuring, directly impacting cross-lingual adaptation performance
- **Design Tradeoffs:** Higher K values (5000) reduce plasticity benefits but stabilize training; lower K values (100) increase plasticity but risk training collapse
- **Failure Signatures:** Loss plateaus at high values (>11.0) indicate forgetting frequency too high; training divergence indicates frequency too low
- **First Experiments:** 1) Implement active forgetting with K=1000 on standard RoBERTa pretraining; 2) Evaluate cross-lingual transfer on XNLI with 5M adaptation tokens; 3) Compare attention patterns between standard and forgetting pretraining runs

## Open Questions the Paper Calls Out
- **Open Question 1:** What is the precise systematic relationship between the order of n-gram structures (n) and the number of self-attention layers in a Transformer? The author notes that higher-order n-grams are computationally constrained and speculates an exponential relationship that requires formal analysis.
- **Open Question 2:** How does periodic embedding resetting influence internal model dynamics, specifically attention patterns and representational drift? The thesis focuses on functional outcomes rather than mechanistic changes in transformer body during forgetting-relearning cycles.
- **Open Question 3:** Can a unified theoretical framework be established to define expressiveness for Factorization Models, GNNs, and ReFactor GNNs? Current expressiveness definitions are mathematically disjoint and bridging them requires substantial theoretical work.

## Limitations
- Computational intensity of n-gram structure extraction (1TB RAM, 128 CPUs) limits reproducibility and scalability
- Active forgetting effectiveness depends heavily on hyperparameter K, which may not generalize across different model sizes and datasets
- Cross-lingual evaluation in low-resource regime depends on quality and representativeness of 5M token samples per language

## Confidence
- **Active forgetting mechanism:** Medium confidence - shows promise but results depend on specific hyperparameters and require broader validation
- **Structure extraction methodology:** Medium confidence - innovative approach but computationally intensive and needs validation across multiple architectures
- **Theoretical framework bridging paradigms:** High confidence - conceptual bridge is compelling and well-supported by empirical evidence

## Next Checks
1. Systematically vary forgetting frequency K (100, 1000, 5000) during pretraining to identify optimal intervals for different model sizes and tasks
2. Validate n-gram structure extraction methodology on multiple transformer architectures (T5, BERT, GPT-style models) to confirm generalizability
3. Test active forgetting mechanism in high-resource adaptation scenarios (>100M tokens) to determine if plasticity benefits persist with ample adaptation data