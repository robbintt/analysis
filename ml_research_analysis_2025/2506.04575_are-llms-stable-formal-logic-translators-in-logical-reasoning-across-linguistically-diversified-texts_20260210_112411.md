---
ver: rpa2
title: Are LLMs Stable Formal Logic Translators in Logical Reasoning Across Linguistically
  Diversified Texts?
arxiv_id: '2506.04575'
source_url: https://arxiv.org/abs/2506.04575
tags:
- logical
- reasoning
- symbol
- mental
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Current benchmarks fail to capture the challenge of maintaining\
  \ consistent logical symbols under linguistic variation. We introduce SoLT, a benchmark\
  \ that systematically rewrites reasoning tasks into diverse yet logically equivalent\
  \ forms, and MenTaL, a mental representation table\u2013guided framework that enforces\
  \ consistent symbol mapping during translation."
---

# Are LLMs Stable Formal Logic Translators in Logical Reasoning Across Linguistically Diversified Texts?

## Quick Facts
- arXiv ID: 2506.04575
- Source URL: https://arxiv.org/abs/2506.04575
- Reference count: 40
- Current benchmarks fail to capture the challenge of maintaining consistent logical symbols under linguistic variation

## Executive Summary
This paper investigates whether LLMs can reliably translate linguistically diverse texts into consistent formal logical representations for symbolic reasoning. The authors find that when semantically equivalent concepts appear in different linguistic forms, LLMs often assign them different logical symbols, breaking the reasoning chain. To address this, they introduce SoLT, a benchmark that systematically diversifies reasoning tasks while preserving logical equivalence, and MenTaL, a mental representation table-guided framework that enforces consistent symbol mapping during translation. Experiments show that MenTaL significantly reduces symbol drift and improves accuracy by up to 40.74% across tasks and models.

## Method Summary
The paper introduces SoLT (Symbol Consistency under Linguistic Diversification), a benchmark that generates linguistically varied but logically equivalent problem variants through three steps: identifying repeated concepts, applying diversification strategies (WordNet/PPDB for word/phrase, LLM for sentence), and filtering candidates via embedding similarity and logical invariance criteria. MenTaL (Mental Representation Table-Guided Translation) is a framework that maintains a concept-to-symbol table during translation, using three operations: Extending (new concept → new symbol), Reusing (known equivalent → existing symbol), and Checking & Refining (partial overlap → compositional symbol). The framework is evaluated using Accuracy (solver correctness) and Symbol Dispersion Score (SDS = avg distinct symbols per concept minus 1) across multiple datasets and LLM models.

## Key Results
- LLM-based translators suffer accuracy drops up to 0.33 and high symbol dispersion scores under SoLT diversification
- MenTaL reduces symbol drift and improves accuracy by up to 40.74% across tasks and models
- Human performance on original vs. SoLT versions is nearly identical (196/200 and 188/200 correct), confirming preserved interpretability

## Why This Works (Mechanism)

### Mechanism 1: Linguistic Variation Induces Symbol Drift
When semantically equivalent expressions appear in different surface forms, LLMs assign them different logical symbols, breaking the reasoning chain. LLM translators process each expression independently without maintaining a global concept registry. Synonyms, paraphrases, and third-person references trigger fresh symbol assignments because the model lacks explicit cross-reference during translation. If the LLM maintains a persistent symbol registry during translation (as MenTaL enforces), drift is suppressed; if not, drift scales with diversification intensity.

### Mechanism 2: Mental Representation Table Enforces Symbol Consistency
MenTaL introduces a Mental Representation Table (MRT) that maps semantically equivalent expressions to shared symbols via three operations: Extending (new concept → new symbol), Reusing (known equivalent → existing symbol), and Checking & Refining (partial overlap → compositional symbol). The MRT is updated incrementally and earlier translations are retroactively revised. By linking equivalent expressions to shared symbols, MenTaL maintains consistency and mitigates symbol drift.

### Mechanism 3: Semantic Filtering Preserves Logic Under Diversification
SoLT replaces repeated concepts with word/phrase/sentence-level variants, then filters candidates via sentence embedding similarity (threshold θ=0.90) and logical invariance scoring (semantic, reasoning, answer invariance). This yields surface-diverse but logically equivalent problems. High embedding similarity correlates with preserved logical structure; human/LLM annotators can reliably verify invariance.

## Foundational Learning

- **Concept: Neuro-symbolic reasoning pipeline**
  - Why needed here: The paper assumes LLMs act as translators feeding symbolic solvers; understanding this decomposition clarifies why symbol consistency matters for downstream deduction.
  - Quick check question: Can you explain why a symbolic solver cannot infer that `Cold(x)` and `Chilly(x)` refer to the same predicate without explicit unification?

- **Concept: First-order logic (FOL) predicates and variables**
  - Why needed here: MenTaL operates at the level of predicate symbols and their arguments; recognizing how concepts map to predicates is essential for implementing the MRT.
  - Quick check question: Given "Bob feels cold" and "The man is chilly," what predicate-argument structures would you extract, and how should they be unified?

- **Concept: Semantic equivalence vs. surface variation**
  - Why needed here: SoLT's logic-invariant diversification relies on distinguishing synonymy (preserves meaning) from relatedness (may alter logical role).
  - Quick check question: Are "cold" and "frosty" always interchangeable in logical contexts? Why or why not?

## Architecture Onboarding

- **Component map:** SoLT Generator -> MenTaL Translator -> Evaluation Suite (Accuracy + SDS) -> Symbolic Solvers
- **Critical path:** 1) Run baseline LLM translator on original datasets to establish accuracy and SDS≈0. 2) Apply SoLT diversification to generate test variants; verify semantic preservation via embedding similarity and human review. 3) Measure accuracy drop and SDS increase on SoLT to quantify drift. 4) Integrate MenTaL (in-context or SFT) and measure recovery in accuracy and SDS reduction.
- **Design tradeoffs:** Diversification intensity vs. semantic preservation risk; MenTaL complexity vs. inference cost (adds ~1.3× input tokens and ~2.1× output tokens); In-context learning vs. SFT (in-context works for capable models; SFT needed for smaller models).
- **Failure signatures:** Parsing errors remain low, logic errors spike (confirms symbol drift, not syntax issues); Prompt-tuning fails to lower SDS (indicates instructing consistency without explicit tracking is insufficient); Small model accuracy drops with MenTaL in-context (suggests capacity limits in following MRT demonstrations).
- **First 3 experiments:** 1) Replicate baseline accuracy vs. SoLT accuracy gap on ProofWriter with GPT-4 to confirm symbol drift. 2) Implement MenTaL via in-context learning with MRT demonstrations; measure SDS reduction and accuracy gain on SoLT. 3) Fine-tune LLaMA-3-8B-Instruct on MenTaL-guided translations; compare SDS and accuracy against non-MenTaL SFT baseline on SoLT.

## Open Questions the Paper Calls Out

### Open Question 1
Can smaller, open-source LLMs be adapted to utilize the MenTaL framework via in-context learning without relying on supervised fine-tuning? The authors explicitly state that smaller models like LLaMA-3-8B failed to follow in-context demonstrations and that this limitation "warrants further investigation."

### Open Question 2
Can the MenTaL framework be optimized to reduce the significant token overhead (2.1× output tokens) observed during logical translation? Appendix D reports the detailed token consumption, showing MenTaL significantly increases output token usage compared to direct translation.

### Open Question 3
What specific linguistic or structural factors contribute to the "other reasons" for error in logical relation translation that MenTaL fails to address? The error attribution analysis shows that even with MenTaL, approximately 30-40% of errors persist due to factors "unrelated to symbol mapping."

## Limitations
- MenTaL's effectiveness depends on LLM's ability to accurately judge semantic equivalence and conflict
- Computational overhead increases input and output tokens by approximately 30%
- Generalizability across domains beyond tested reasoning datasets remains uncertain

## Confidence
- Mechanism 1 (Symbol Drift): Medium-High confidence (empirical evidence compelling but not fully exploring beneficial drift scenarios)
- Mechanism 2 (MenTaL Effectiveness): Medium-High confidence (substantial improvements documented but dependent on LLM judgment reliability)
- Mechanism 3 (Semantic Filtering Quality): Medium confidence (embedding similarity adequate but potential false positives acknowledged)

## Next Checks
1. Implement controlled experiment where MenTaL is tested with synthetic equivalence judgments (both correct and incorrect) to quantify impact of LLM judgment errors on symbol consistency and downstream accuracy.
2. Apply MenTaL to a different reasoning domain (e.g., mathematical proof generation) and compare performance degradation under linguistic variation with and without MenTaL.
3. Systematically vary the size and complexity of the MRT to identify the point where accuracy gains no longer justify computational overhead.