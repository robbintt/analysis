---
ver: rpa2
title: Reinforcement Learning for Safe Autonomous Two Device Navigation of Cerebral
  Vessels in Mechanical Thrombectomy
arxiv_id: '2503.24140'
source_url: https://arxiv.org/abs/2503.24140
tags:
- navigation
- reward
- autonomous
- training
- cerebral
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a reinforcement learning (RL) approach for
  autonomous navigation of micro-guidewires and micro-catheters in cerebral vessels
  during mechanical thrombectomy (MT). It uses a modified Soft Actor-Critic RL algorithm
  trained with a combined reward model incorporating inverse reinforcement learning
  and force feedback to improve safety.
---

# Reinforcement Learning for Safe Autonomous Two Device Navigation of Cerebral Vessels in Mechanical Thrombectomy

## Quick Facts
- arXiv ID: 2503.24140
- Source URL: https://arxiv.org/abs/2503.24140
- Reference count: 36
- Primary result: 96% success rate on unseen patient-specific vasculatures using combined IRL + force feedback reward model

## Executive Summary
This paper presents the first demonstration of autonomous navigation of both a micro-guidewire and micro-catheter to cerebral vessels using reinforcement learning during mechanical thrombectomy. The approach employs a modified Soft Actor-Critic algorithm with an LSTM-based policy to handle trajectory-dependent decisions. A combined reward model incorporating dense distance metrics, inverse reinforcement learning from expert demonstrations, and force feedback enables safe navigation while maintaining high success rates. The system achieves a 96% success rate, 7.0s procedure time, and mean tip force of 0.24N on unseen patient anatomies, all while keeping forces well below the 1.5N vessel rupture threshold.

## Method Summary
The method trains a modified Soft Actor-Critic (SAC) algorithm with LSTM policy to navigate two devices (Echelon 10 micro-catheter and Synchro 14 micro-guidewire) through patient-specific cerebral vasculatures from internal carotid artery to middle cerebral artery. The system uses 2D Cartesian coordinates of three points per device plus target location as observations. Six reward functions are tested, with the combined reward (R6) incorporating dense distance metrics, IRL-learned expert demonstrations, and force penalties performing best. Training uses 10 vasculatures with random scaling augmentation (0.7-1.3) over 10^7 exploration steps. Evaluation is performed on 2 held-out vasculatures with a 200-step timeout.

## Key Results
- 96% success rate (26/27 trials) on unseen patient-specific vasculatures
- Procedure time: 7.0 seconds average
- Mean tip force: 0.24N (well below 1.5N rupture threshold)
- Force feedback reduced procedure time by ~5.3s compared to dense reward only
- Combined reward model (R6) outperformed single-objective rewards on unseen anatomies

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Combined reward model (R6) integrating dense metrics, IRL, and force penalties provides superior generalization to unseen patient anatomies compared to single-objective rewards.
- **Mechanism:** Dense reward provides local gradient information, IRL encodes expert navigation strategies, and force penalty prevents wall collisions. Combined, these balance goal-directed efficiency with robustness for unseen geometries.
- **Core assumption:** Linear combination effectively arbitrates between conflicting objectives of speed, expert mimicry, and safety.
- **Evidence anchors:** Table 2 shows R6 achieving 96% success vs 93% for R1 and 78% for R4; abstract confirms combined reward approach.
- **Break condition:** Mis-calibrated scaling factors or force thresholds could cause timeout (over-cautious) or collision (under-cautious).

### Mechanism 2
- **Claim:** Tip force feedback reduces mean contact forces and procedure times by discouraging high-friction wall collisions.
- **Mechanism:** Penalizing forces >0.85N teaches agent to seek vessel centerline, reducing friction that could cause snagging and slow navigation.
- **Core assumption:** Reduced friction from wall-centering increases velocity, assuming rigid vessel walls and translation of simulation friction to physical behavior.
- **Evidence anchors:** Section 4.2 states force feedback significantly reduced procedure time; Figure 4 shows reduced high-force regions for R6 vs R1.
- **Break condition:** In highly tortuous anatomy where wall contact is unavoidable, agent may exhibit hesitant behavior or fail to progress.

### Mechanism 3
- **Claim:** LSTM-based policy enables navigation decisions based on trajectory history, resolving branch ambiguities where instantaneous coordinates are insufficient.
- **Mechanism:** Distinguishing correct branches (e.g., MCA vs ACA) requires temporal context of approach, not just current geometry. LSTM retains this history.
- **Core assumption:** Markov property doesn't hold sufficiently; history is required to infer correct path when branches appear similar.
- **Evidence anchors:** Section 2.4 states LSTM allows probing correct vessel when target branch isn't unambiguously located.
- **Break condition:** If episode length exceeds LSTM's memory capacity or critical branching decision happens too early, temporal context may be lost.

## Foundational Learning

- **Concept:** **Soft Actor-Critic (SAC)**
  - **Why needed here:** SAC is an off-policy actor-critic algorithm optimized for continuous control tasks (rotation/translation) and robust to reward noise.
  - **Quick check question:** Can you explain why an off-policy algorithm like SAC is preferred over an on-policy algorithm like PPO for sample efficiency in high-fidelity simulation?

- **Concept:** **Inverse Reinforcement Learning (IRL)**
  - **Why needed here:** Designing hand-crafted reward functions for complex endovascular maneuvers is difficult; IRL infers reward structures from expert demonstrations.
  - **Quick check question:** How does Maximum Entropy IRL handle multiple "expert" trajectories achieving the same goal differently?

- **Concept:** **Simulation-to-Real (Sim2Real) Transfer**
  - **Why needed here:** System trained entirely in silico with domain randomization to bridge gap to physical patient anatomies.
  - **Quick check question:** What specific augmentation strategies (e.g., random scaling) are used here to mitigate the "reality gap"?

## Architecture Onboarding

- **Component map:** CTA scans -> 3D meshes -> SOFA simulation -> Observer (extracts coordinates) -> Modified SAC Controller (LSTM + Feedforward) -> Reward Engine -> Force collision monitor
- **Critical path:** 1) Data Prep: CTA scans to 3D meshes to SOFA; 2) IRL Training: 1M iterations on keyboard demonstrator data; 3) RL Training: 10^7 steps using R6 reward on 10 training vasculatures; 4) Evaluation: deterministic policy tested on 2 unseen vasculatures
- **Design tradeoffs:** R2 (pure IRL) failed at 44% success, requiring combination with dense rewards (R3/R6) for convergence. Force threshold set at 0.85N (vs 1.5N rupture threshold) creates safety buffer but risks over-cautious navigation.
- **Failure signatures:** Timeout (200 steps) indicates limit cycle or physical stuck; Wrong Branch indicates LSTM fails to disambiguate; High Force Snagging occurs in dense-reward-only models where speed is prioritized over centering.
- **First 3 experiments:** 1) Sanity Check: Train/test R1 vs R2 on single vasculature to verify R3 replicates prior success; 2) Generalization Test: Train R6 on 10 vasculatures, test on 2 unseen, measure success rate drop and force profile changes; 3) Ablation Study: Run R1, R3, R4, R6 side-by-side on unseen set to isolate contributions of force feedback and combined reward.

## Open Questions the Paper Calls Out

- **Question:** Can the proposed in silico trained policy successfully translate to in vitro environments while maintaining safety metrics without requiring force feedback sensors during deployment?
- **Basis in paper:** Explicit statement that future work should focus on translating to in vitro experiments and extending validation to in vitro models.
- **Why unresolved:** Current study is entirely simulation-based (TRL 3); physical factors like friction and device mechanics may differ from SOFA simulation.
- **What evidence would resolve it:** Successful navigation results and force profiles from physical phantoms or cadaver models using trained RL agent without online force feedback.

- **Question:** Does the model maintain high success rates and generalization capabilities when scaled to a dataset including full diversity of Circle of Willis configurations?
- **Basis in paper:** Explicit statement that small dataset cannot replicate different Circle of Willis configurations and future work should extend dataset to integrate more diverse vasculatures.
- **Why unresolved:** Training set of only 12 cases may not capture wide anatomical variance found in general population.
- **What evidence would resolve it:** Study evaluating agent on significantly larger dataset of unseen vascular geometries with varied anatomical complexities.

- **Question:** Do visual-based reinforcement learning algorithms provide superior generalization compared to tracking-based coordinate inputs used in this study?
- **Basis in paper:** Explicit statement that future work may compare tracking- and visual-based RL algorithms to investigate whether same benefits can be gained from providing entire image frame.
- **Why unresolved:** Current method relies on abstract 2D coordinates rather than raw visual data; unknown if processing full fluoroscopy-like images would improve context interpretation or hinder training efficiency.
- **What evidence would resolve it:** Comparative analysis of agent performance when trained on image frames versus coordinate tracking in similar navigation tasks.

## Limitations
- Small dataset (12 patient cases) unable to replicate full diversity of Circle of Willis configurations
- Entirely simulation-based (TRL 3) without in vitro or in vivo validation
- Does not specify key architectural hyperparameters (LSTM size, layer depth, SAC learning rates)

## Confidence
- **High:** Success rate (96%) and force safety (<0.85N) on unseen vasculatures directly supported by Table 2 and Figure 4
- **Medium:** Mechanism linking force feedback to reduced procedure time is inferred from data and plausible but not directly proven
- **Low:** Claim that LSTM resolves branch ambiguities based on single sentence with no empirical evidence provided

## Next Checks
1. **Architecture Validation:** Train reduced version (without LSTM or smaller network) on single vasculature to verify necessity of claimed architectural components
2. **Force Feedback Ablation:** Conduct controlled experiment comparing R1 (dense reward only) and R4 (force feedback only) on same unseen vasculature to isolate contribution to success rate and mean force reduction
3. **IRL Contribution Test:** Train model with R3 (dense + IRL) and compare performance on unseen data to R6 (R3 + force feedback) to quantify specific benefit of combining IRL with other reward components