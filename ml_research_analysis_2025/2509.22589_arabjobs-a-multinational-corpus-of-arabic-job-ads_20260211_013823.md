---
ver: rpa2
title: 'ArabJobs: A Multinational Corpus of Arabic Job Ads'
arxiv_id: '2509.22589'
source_url: https://arxiv.org/abs/2509.22589
tags:
- gender
- arabic
- salary
- male
- corpus
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ArabJobs introduces the first large-scale corpus of Arabic job
  advertisements from four countries, addressing the lack of Arabic labor market datasets.
  The corpus captures regional dialectal variation, gender representation, and occupational
  diversity across over 8,500 postings.
---

# ArabJobs: A Multinational Corpus of Arabic Job Ads

## Quick Facts
- **arXiv ID**: 2509.22589
- **Source URL**: https://arxiv.org/abs/2509.22589
- **Reference count**: 7
- **Primary result**: First large-scale corpus of Arabic job ads from four countries, enabling analysis of labor market bias, dialectal variation, and salary patterns.

## Executive Summary
ArabJobs introduces the first large-scale corpus of Arabic job advertisements from four countries (Egypt, Jordan, Saudi Arabia, UAE), addressing the lack of Arabic labor market datasets. The corpus captures regional dialectal variation, gender representation, and occupational diversity across over 8,500 postings. Structured fields and LLM-generated metadata enable salary estimation and job classification. Gender analysis reveals pronounced occupational segregation and salary gaps, with male-targeted roles generally higher-paid. Linguistic bias is evident through appearance-based and demographic criteria, especially in female-targeted ads. Unsupervised clustering confirms dialectal distinctions between regions, and code-switching analysis highlights English usage in non-UAE postings. The corpus supports fairness-aware NLP and socio-linguistic research, with applications in bias detection, salary prediction, and job categorization. Ethical scraping and data anonymization were followed throughout.

## Method Summary
The corpus was constructed by scraping job advertisements from seven platforms across four Arab countries, resulting in 8,546 structured postings. GPT-4 was employed via few-shot prompting to estimate missing salary data and unify job categories, achieving low error rates validated against human annotators. Dialectal variation was analyzed through unsupervised clustering using TF-IDF vectorization and Truncated SVD on job descriptions. Gender bias was detected using CAMeL Tools for Arabic tokenization and concordance analysis. The dataset includes both structured fields (title, location, salary, gender, description) and LLM-generated metadata (profession, salary_local, salary_usd, job_category, sub_category).

## Key Results
- GPT-4 salary estimation achieved MAE of 11.83 and Pearson correlation of 0.997 on held-out test set
- Gender analysis revealed occupational segregation with male-targeted roles generally higher-paid
- Unsupervised clustering successfully separated regional dialects, with Gulf dialects clustering together
- Linguistic bias detected through appearance-based criteria, especially in female-targeted advertisements

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A corpus of job advertisements can capture structured socio-economic and linguistic signals that reflect real-world labor market biases.
- **Mechanism:** Job ads encode employer demands, expectations, and language practices. By collecting and structuring these ads across regions (Egypt, Jordan, Saudi Arabia, UAE), the corpus aggregates textual evidence of occupational segregation, salary gaps, and linguistic variation. The structured nature of the ads (title, salary, gender preference) allows for direct quantitative analysis, while the free-text descriptions capture qualitative societal norms.
- **Core assumption:** The language used in job ads is a reliable proxy for actual labor market conditions and societal biases.
- **Evidence anchors:**
  - [abstract] The dataset captures "linguistic, regional, and socio-economic variation in the Arab labour market."
  - [section 3] The authors state each entry includes structured fields (gender, salary) and free-text descriptions, enabling "analyses of how job markets communicate expectations."
  - [corpus] Evidence is internal. The link to *real-world* conditions is inferred by the authors, but supported by their analysis revealing disparities that mirror known societal issues.
- **Break condition:** This mechanism fails if the scraped platforms are systematically unrepresentative of the broader labor market (e.g., only listing low-quality or informal jobs), which would distort the inferred socio-economic picture.

### Mechanism 2
- **Claim:** Large Language Models (LLMs) like GPT-4 can infer missing structured data (salary) from unstructured job ad text with high reliability.
- **Mechanism:** The model is prompted with a few-shot learning approach, where it is given examples of job ads with known salaries and then asked to predict the salary for a new ad. The model leverages learned associations between job titles, descriptions, locations, and compensation levels to fill in the missing data fields.
- **Core assumption:** The LLM has been trained on sufficient data to understand the relationship between job attributes and salary in the specific cultural and linguistic context of the Arab world, and that these relationships are stable and consistent.
- **Evidence anchors:**
  - [section 5.1] The paper states, "we used GPT-4... to estimate salaries... The model achieved a low mean absolute error (MAE) of 11.83" and a high Pearson correlation of 0.997 on a held-out test set.
  - [section 5.1, Table 3] Shows high agreement between GPT-4 and human annotators (e.g., 85% agreement within ±20%), suggesting the model's inferences are comparable to human judgment.
  - [corpus] Evidence is strong and direct. The paper validates the LLM's performance against a held-out set of 3,265 ads with known salaries.
- **Break condition:** This mechanism fails if the LLM hallucinates values for novel job titles or if market conditions change drastically from its training data cutoff, making its internal salary benchmarks obsolete.

### Mechanism 3
- **Claim:** Unsupervised clustering of job description text can reveal regional dialectal distinctions even within a formal domain.
- **Mechanism:** By converting job descriptions into numerical vectors (TF-IDF features) and reducing their dimensionality, the method groups texts with similar vocabulary together. Since dialects use different words for the same concept (e.g., different terms for "hairdresser"), ads from the same region will cluster together based on these lexical choices.
- **Core assumption:** The lexical choices (specific words used) are a primary differentiator of regional dialects and these differences are strong enough to be detected despite the common formal register of job ads (Modern Standard Arabic).
- **Evidence anchors:**
  - [section 4] The text states, "Dimensionality reduction via Truncated Singular Value Decomposition (SVD) revealed clear regional clusters."
  - [section 4] Provides qualitative evidence: "Saudi and Emirati ads (Gulf dialects) clustered closely, while Egyptian and Jordanian postings formed separate regions."
  - [corpus] Evidence is based on the paper's internal experiment and qualitative interpretation. There is no external linguistic corpus used for validation.
- **Break condition:** This mechanism breaks if the job ads are predominantly written in a standardized, non-regional form of Arabic that suppresses dialectal markers, which would cause the regional clusters to merge.

## Foundational Learning

- **Concept: TF-IDF (Term Frequency-Inverse Document Frequency)**
  - **Why needed here:** This is the core technique used to represent job descriptions as numbers for the dialect analysis. It down-weights common words and highlights words unique to a document (and thus a region), making clustering possible.
  - **Quick check question:** Given two job ads, one that uses the word "Sales" five times and another that uses it once, how would TF-IDF help a clustering algorithm distinguish them from other ads that don't use the word at all?

- **Concept: Few-Shot Prompting for LLMs**
  - **Why needed here:** This is the specific method used to make GPT-4 predict salaries. Understanding that providing examples ("shots") in the prompt conditions the model is crucial for reproducing the results or applying the technique to new tasks.
  - **Quick check question:** Why would providing three examples of job ads with their salaries in the prompt be more effective for this task than just instructing the model to "predict the salary"?

- **Concept: Inter-Annotator Agreement**
  - **Why needed here:** The paper uses human annotators to validate the LLM's salary predictions. Understanding this metric (e.g., agreement within a ±20% margin) is key to judging the reliability of the GPT-4 generated metadata.
  - **Quick check question:** If GPT-4 and a human annotator agree on a salary prediction within ±10%, what does that suggest about the model's "understanding" of the job market compared to a scenario where they disagree by 200%?

## Architecture Onboarding

- **Component map:** Data ingestion pipeline (web scrapers for 7 platforms) -> Storage layer (corpus with structured and unstructured fields) -> Processing module (TF-IDF vectorizer, SVD for clustering) -> LLM inference engine (GPT-4 for salary/category prediction)
- **Critical path:** The most critical sequence is the **LLM-based metadata enrichment**. Without the inferred salary and unified job categories, many of the higher-level analyses (pay gap, occupational segregation) would be impossible or severely limited due to sparse and inconsistent raw data.
- **Design tradeoffs:**
  1. **Scraping vs. API Access:** The authors chose web scraping, which is brittle and requires maintenance, but provides access to data not available via official APIs. This introduces an ethical and stability tradeoff.
  2. **Automation vs. Human Verification:** The pipeline is highly automated (scraping, LLM inference), but the authors acknowledge a need for manual verification. This trades speed and scale for the risk of propagating LLM errors.
  3. **Broad vs. Deep Collection:** The corpus covers four countries. The tradeoff is a potentially shallower dive into any single country's market compared to a more focused dataset, potentially missing local nuances.
- **Failure signatures:**
  1. **Salary Hallucination:** If GPT-4 predicts salaries that are wildly off-market for a specific niche role, it could distort the pay gap analysis. The paper mitigates this with human spot-checks.
  2. **Category Fragmentation:** If the LLM fails to unify job categories semantically, the "job_category" field becomes too granular, rendering the occupational segregation analysis ineffective.
  3. **Dialect Dilution:** If the job ads become more standardized over time, the unsupervised dialect clustering will fail to separate regions, limiting sociolinguistic utility.
- **First 3 experiments:**
  1. **Re-run Salary Inference:** Take a random sample of ads with known salaries, remove the salary, and attempt to predict it using the described few-shot prompt with a different LLM (e.g., a locally hosted open-source model) to test the method's portability.
  2. **Category Drift Analysis:** Scrape a new batch of ads six months later and run the category unification prompt. Compare the distribution of new vs. old categories to see if the labor market is shifting.
  3. **Dialect vs. Industry:** Instead of clustering by region, cluster ads by job category and see if the dialectal markers persist. This tests if certain industries are more resistant to using formal Modern Standard Arabic.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can supervised dialect identification models outperform the paper's unsupervised clustering approach in classifying regional varieties within the corpus?
- **Basis in paper:** [explicit] The authors state they "do not explicitly annotate dialects" and rely on unsupervised SVD clustering, while suggesting the corpus offers opportunities for future sociolinguistic research.
- **Why unresolved:** The paper demonstrates that dialectal signals exist (clustering works) but does not train or benchmark specific dialect classifiers on the data.
- **What evidence would resolve it:** Benchmarking accuracy and F1-scores of supervised models (e.g., MARBERT, AraBERT) against the unsupervised clusters using country of origin as a weak proxy label.

### Open Question 2
- **Question:** To what extent do job advertisements labeled as "neutral" contain the implicit gendered language (e.g., appearance requirements) found in explicitly targeted ads?
- **Basis in paper:** [inferred] The paper analyzes linguistic bias in male/female-targeted subsets, but the dataset contains a large volume of "neutral" ads (e.g., 1,200 in Egypt). The linguistic content of these neutral ads regarding implicit bias is not quantified.
- **Why unresolved:** It is unclear if "neutral" ads successfully avoid the gendered coding found in targeted ads or if they perpetuate similar biases implicitly.
- **What evidence would resolve it:** A comparative lexical analysis measuring the frequency of biased concordances (e.g., appearance/soft skills) in the "neutral" category versus the gendered categories.

### Open Question 3
- **Question:** Do fine-tuned, smaller encoder models provide more accurate salary estimates than the few-shot GPT-4 approach used for data enrichment?
- **Basis in paper:** [inferred] The paper utilizes GPT-4 to fill missing salary data, validating it against human annotators. However, it does not compare this against fine-tuned regression baselines, leaving the optimal methodological approach for this specific task undefined.
- **Why unresolved:** While GPT-4 performed well, it is computationally expensive; specialized models might capture domain-specific nuances more efficiently.
- **What evidence would resolve it:** A comparative study of Mean Absolute Error (MAE) between GPT-4 predictions and a fine-tuned BERT-based regression model on the held-out test set.

## Limitations

- **Platform Representativeness:** The dataset's representativeness of the broader labor market depends on the sampled platforms, which may skew toward certain sectors or job qualities.
- **LLM Generalization:** Salary estimates rely on GPT-4's training distribution and may not generalize to rapidly changing or novel job markets.
- **Dialect Validation:** Dialect clustering is qualitative and lacks external linguistic validation against established dialect corpora.

## Confidence

- **High Confidence:** The corpus construction methodology (scraping, structuring) is reproducible and well-documented. The gender bias analysis based on explicit field values is directly supported by the data.
- **Medium Confidence:** The salary estimation results (MAE, RMSE, agreement scores) are internally validated but depend on the LLM's training distribution and may not generalize. Dialect clustering patterns are interpretable but lack external validation.
- **Low Confidence:** Claims about "real-world" socio-economic conditions are inferred from the corpus but not independently verified against official labor statistics or alternative data sources.

## Next Checks

1. **External Validation of Salary Estimates:** Compare a sample of GPT-4 predicted salaries against official salary surveys or reports for the same roles and regions to assess accuracy.
2. **Platform Representativeness Audit:** Analyze the proportion of jobs by sector and quality level on the scraped platforms versus known labor market composition in each country.
3. **Dialectal Marker Verification:** Cross-reference the most discriminative words identified in the dialect clusters against a recognized Arabic dialect dictionary or linguistic study to confirm their regional specificity.