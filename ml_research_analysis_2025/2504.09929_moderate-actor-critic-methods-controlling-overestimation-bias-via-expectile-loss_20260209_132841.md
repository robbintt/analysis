---
ver: rpa2
title: 'Moderate Actor-Critic Methods: Controlling Overestimation Bias via Expectile
  Loss'
arxiv_id: '2504.09929'
source_url: https://arxiv.org/abs/2504.09929
tags:
- target
- moderate
- algorithms
- learning
- overestimation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a novel method called Moderate Actor-Critic
  to address overestimation bias in model-free reinforcement learning. The core idea
  is to formulate a moderate target as a convex combination of an overestimated Q-function
  and its lower bound, where the lower bound is efficiently estimated using the lower
  expectile of the Q-value distribution.
---

# Moderate Actor-Critic Methods: Controlling Overestimation Bias via Expectile Loss

## Quick Facts
- **arXiv ID:** 2504.09929
- **Source URL:** https://arxiv.org/abs/2504.09929
- **Reference count:** 40
- **Primary result:** Introduces Moderate Actor-Critic methods (MPG, MAC, MQC) that consistently outperform baseline RL algorithms on MuJoCo tasks by reducing overestimation bias through convex combination of Q-function and its lower expectile.

## Executive Summary
This paper addresses the overestimation bias problem in model-free reinforcement learning by introducing a novel "moderate target" that combines an overestimated Q-function with its lower bound. The key innovation is using expectile regression to efficiently estimate the lower bound of the Q-value distribution, which is then integrated into state-of-the-art algorithms like DDPG, SAC, and TQC. Experimental results on continuous control tasks show significant improvements in both performance and stability while maintaining computational efficiency comparable to baseline methods.

## Method Summary
The paper proposes Moderate Actor-Critic methods that formulate a new TD target as a convex combination of the standard Q-function estimate and a lower-bound estimate. The lower bound is computed using expectile regression with Ï„=0.01, which trains a "protester" network to predict the lower tail of the Q-distribution. This approach is seamlessly integrated into DDPG (creating MPG), SAC (creating MAC), and TQC (creating MQC) by replacing the standard target computation with the moderate target formula. The method requires only one additional network compared to two-critic approaches, maintaining computational efficiency while providing improved bias correction.

## Key Results
- MPG, MAC, and MQC consistently outperform their baseline counterparts on MuJoCo continuous control tasks
- Average rewards reach up to 6460.96 while maintaining improved stability
- Computational complexity remains comparable to or better than baseline algorithms
- The moderate target effectively reduces overestimation bias without introducing significant underestimation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Constructing the TD target as a convex combination of a standard Q-estimate and a lower-bound estimate reduces overestimation bias.
- **Mechanism**: Standard Q-learning targets (`max Q`) are biased high due to Jensen's inequality acting on noisy function approximations. The authors introduce a "moderate target" ($y_{mt}$) calculated as $(1-\omega)Q + \omega V_{\tau}$. By weighting a lower-bound value function $V_{\tau}$, the target is pulled downward, counteracting the positive bias inherent in the maximization operator.
- **Core assumption**: There exists a cautious weight $\omega$ such that the convex combination approximates the true Q-value more closely than the standard max operator; this relies on the lower bound being sufficiently distinct from the overestimate.
- **Evidence anchors**:
  - [abstract] "formulated as a convex optimization of an overestimated Q-function and its lower bound."
  - [page 5, Eq. 26] Mathematical formulation showing the true Q-value exists between the max estimate and the lower expectile.
  - [corpus] Related work like "ADDQ" and "Stochastic Actor-Critic" confirms that adaptive bias reduction is a critical active research area for stabilizing RL.
- **Break condition**: If the overestimation is minimal (e.g., in deterministic environments) or the lower bound is estimated too low (excessive penalty), this mechanism may cause underestimation bias.

### Mechanism 2
- **Claim**: The lower bound of the Q-value distribution can be efficiently estimated using expectile regression.
- **Mechanism**: The paper utilizes an asymmetrically weighted squared error loss (expectile loss) to train a "protester" network $V_\psi$. By setting the expectile level $\tau < 0.5$ (e.g., $\tau=0.01$), the loss penalizes over-prediction more than under-prediction, forcing the network to predict the lower tail of the Q-distribution rather than the mean.
- **Core assumption**: The distribution of Q-values conditioned on a state is sufficiently well-behaved that the lower expectile serves as a valid proxy for the lower bound of the true value.
- **Evidence anchors**:
  - [page 5, Eq. 21-23] Definition of expectile loss $\ell_\tau$ and the generalized state-value function $V^\pi_\tau$.
  - [page 7, Section 4.5] "Our moderate target employs the lower expectile... thus integrating the associated risk information."
  - [corpus] Corpus signals are weak for specific "expectile" usage in actor-critic bias control (most neighbors use Double-Q or ensembles), suggesting this is a novel application of the statistic.
- **Break condition**: If the Q-distribution is multi-modal or extremely sparse, a single expectile value may not capture the risk profile effectively, leading to a noisy lower bound.

### Mechanism 3
- **Claim**: Replacing a second critic network with the protester network maintains computational complexity while improving stability.
- **Mechanism**: Algorithms like TD3 or SAC typically use two critics (min-Q) to lower bias. MPG/MAC replace the second critic with the protester ($V_\psi$). The protester provides a distinct signal (distributional lower bound) compared to the standard critic, reducing variance and bias without increasing the number of network forward/backward passes significantly.
- **Core assumption**: The gradient from the expectile loss provides a stronger or more useful learning signal for bias correction than simply taking the minimum of two standard Q-networks.
- **Evidence anchors**:
  - [page 7, Section 4.5] "MPG-SD is one of the simplest actor-critic algorithms... while yielding attractive performance."
  - [page 8, Table 2] Complexity comparison showing MPG/MAC training times and GPU memory are comparable to or better than baselines.
  - [corpus] "Dual Ensembled Multiagent Q-Learning" and "Mitigating Estimation Bias..." corroborate the general efficacy of modified critic architectures.
- **Break condition**: If the expectile level $\tau$ is set too close to 0.5, the protester simply mimics the main critic, acting as a redundant regularizer rather than a bias correction tool.

## Foundational Learning

- **Concept: Overestimation Bias (Jensen's Inequality)**
  - **Why needed here**: This is the fundamental problem the paper solves. You must understand why the `max` operator in TD learning inevitably leads to optimistic value estimates when noise is present.
  - **Quick check question**: If you take the maximum of two noisy estimates of a scalar, is the expected result higher, lower, or equal to the true scalar?

- **Concept: Expectiles vs. Quantiles**
  - **Why needed here**: The paper relies on expectile loss (asymmetric MSE), not quantile loss (asymmetric absolute error). Understanding this distinction is crucial for implementing the loss function correctly.
  - **Quick check question**: Does expectile regression rely on the ordering of samples (like quantiles) or the squared distance from the mean (like MSE)?

- **Concept: Actor-Critic Architecture (DDPG/SAC)**
  - **Why needed here**: The paper modifies standard baselines (DDPG, SAC, TQC). Knowing where the "Target Q" is calculated in these architectures is necessary to know where to inject the "Moderate Target."
  - **Quick check question**: In SAC, how does the entropy term modify the standard Bellman backup, and does the Moderate Target replace the Q-term or the entropy term?

## Architecture Onboarding

- **Component map**: Critic ($Q_\theta$) -> Protester ($V_\psi$) -> Moderate Target Computation -> Target Q-value
- **Critical path**:
  1. **Batch Sampling**: Draw $(s, a, r, s')$.
  2. **Protester Update**: Train $V_\psi(s)$ to minimize expectile loss against current $Q_\theta(s,a)$ (Eq. 27).
  3. **Target Computation**: Compute $y_{mt} = r + \gamma [(1-\omega)Q_{\bar{\theta}}(s', a') + \omega V_\psi(s')]$ (Eq. 28).
  4. **Critic Update**: Train $Q_\theta$ towards $y_{mt}$.
  5. **Actor Update**: Standard gradient ascent on $Q_\theta$.

- **Design tradeoffs**:
  - **Cautious Weight ($\omega$)**: Higher $\omega$ reduces overestimation but risks underestimation. The paper finds optimal $\omega$ varies by algorithm (e.g., higher for DDPG, lower for TQC).
  - **Expectile Level ($\tau$)**: Lower $\tau$ (e.g., 0.01) focuses strictly on the lower tail, providing a more conservative bound but potentially ignoring valid high-probability Q-values.
  - **Network Count**: MAC/MPG use 1 Critic + 1 Protester vs SAC/TD3's 2 Critics. This trades distinct bias correction (Protester) for variance reduction via ensembling (Twin Critics).

- **Failure signatures**:
  - **Collapse to Zero**: If $\omega$ is too high, the target Q-values collapse, causing the agent to learn a policy that avoids all risk (stagnation).
  - **Unstable Protester**: If $\tau$ is very small, the protester gradient becomes sparse or volatile, failing to provide a stable lower bound.
  - **High Variance**: If the critic overestimates significantly, the protester might lag behind, causing a gap between the target and the estimated Q that destabilizes training.

- **First 3 experiments**:
  1. **Ablation on $\omega$**: Run MPG on `HalfCheetah-v4` with $\omega \in [0.0, 0.1, 0.2, 0.5]$ to observe the shift from overestimation to underestimation and identify the sweet spot (paper suggests ~0.2).
  2. **Protester Visualization**: Log the values of $Q(s,a)$ and $V_\psi(s)$ during training on `Ant-v4`. Verify that $V_\psi$ tracks the lower tail of the Q-distribution and doesn't diverge.
  3. **Baseline Efficiency Comparison**: Compare MAC vs. SAC(min-Q) on `Humanoid-v4` for 2M steps. Measure max reward and standard deviation (stability) to verify if the "Moderate" target provides the claimed stability improvements (Table 1).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the moderate target be effectively integrated into discrete control algorithms (e.g., DQN) or on-policy methods (e.g., PPO)?
- Basis: [explicit] The authors state in Section 4.5 that the moderate target "can be readily incorporated into other RL algorithms," specifically naming Q-learning and PPO.
- Why unresolved: Experiments were restricted to off-policy continuous control tasks (MuJoCo) using DDPG, SAC, and TQC variants.
- What evidence would resolve it: Benchmark results on Atari games for discrete control and comparison plots against PPO for on-policy tasks.

### Open Question 2
- Question: Can the cautious weight $\omega$ be adapted dynamically during training rather than set as a fixed hyperparameter?
- Basis: [inferred] Section 5.2 shows that optimal $\omega$ values vary significantly (0.01 vs. 0.2) depending on the base algorithm's bias, implying a need for manual tuning.
- Why unresolved: The paper provides "guidelines" for selection but offers no mechanism for the agent to adjust this weight as the estimation bias changes over time.
- What evidence would resolve it: An ablation study showing performance stability with an adaptive $\omega$ schedule or a theoretical bound for automatic tuning.

### Open Question 3
- Question: Does the convergence guarantee hold when using the practical expectile value function instead of the strict minimum action-value assumption?
- Basis: [inferred] The convergence proof in Appendix A relies on Assumption 1 ($V(s) = \min_a Q(s,a)$), whereas the practical implementation uses the expectile $V^\pi_\tau$ (Eq 23), which is an approximation.
- Why unresolved: There is a theoretical gap between the condition required for the contraction mapping proof and the actual function approximator used in the algorithms.
- What evidence would resolve it: A convergence proof that accounts for the error bounds of the expectile loss relative to the true minimum.

## Limitations

- **Evaluation Scope**: Experiments are limited to MuJoCo continuous control tasks. The efficacy of Moderate Actor-Critic methods on other domains (e.g., image-based environments, Atari, or real-world robotics) remains untested.
- **Hyperparameter Sensitivity**: The choice of cautious weight $\omega$ is stated as "optimized for each environment," but the tuning process is not detailed. It's unclear if these values are robust to other task configurations.
- **Distributional Assumptions**: The method assumes the Q-value distribution is well-behaved enough that the lower expectile is a meaningful lower bound. For tasks with multi-modal or highly skewed Q-distributions, the protester network may not accurately estimate the true lower bound.

## Confidence

- **High Confidence**: The theoretical framework for combining an overestimated Q-function with its lower bound is sound and addresses a well-known problem in RL. The mathematical formulation and the integration of the moderate target into standard algorithms (DDPG, SAC, TQC) are clearly specified.
- **Medium Confidence**: The experimental results demonstrate consistent improvements in both performance and stability across MuJoCo tasks. However, the lack of ablation studies on the key hyperparameters ($\omega$, $\tau$) and the limited scope of the experiments reduce confidence in the method's generalizability.
- **Low Confidence**: The claim that Moderate Actor-Critic methods "maintain similar computational complexity" is based on a single comparison table. Without detailed profiling or analysis of training stability during optimization, it's difficult to verify this claim or understand the trade-offs in terms of convergence speed and memory usage.

## Next Checks

1. **Ablation on Cautious Weight**: Run MPG on HalfCheetah-v4 with $\omega \in [0.0, 0.1, 0.2, 0.5]$. Plot the average return vs. $\omega$ to identify the optimal value and confirm that too high a weight causes underestimation and performance collapse.

2. **Protester Network Analysis**: During training on Ant-v4, log the values of $Q(s,a)$ and $V_\psi(s)$ at regular intervals. Plot the distributions to verify that $V_\psi$ consistently tracks the lower tail of the Q-distribution and doesn't diverge or collapse to a constant value.

3. **Generalization Test**: Apply MAC to a non-MuJoCo environment, such as a continuous control task from PyBullet or a custom sparse-reward environment. Compare its performance and stability against the baseline SAC to assess the method's robustness beyond the standard benchmark suite.