---
ver: rpa2
title: 'CoCoA: Confidence and Context-Aware Adaptive Decoding for Resolving Knowledge
  Conflicts in Large Language Models'
arxiv_id: '2508.17670'
source_url: https://arxiv.org/abs/2508.17670
tags:
- cocoa
- adacad
- context
- decoding
- conflict
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of knowledge conflicts between
  a language model's parametric memory and external context, which can lead to unfaithful
  generation. Existing contrastive decoding methods like CAD and AdaCAD often lack
  adaptability and degrade in low-conflict settings.
---

# CoCoA: Confidence and Context-Aware Adaptive Decoding for Resolving Knowledge Conflicts in Large Language Models

## Quick Facts
- arXiv ID: 2508.17670
- Source URL: https://arxiv.org/abs/2508.17670
- Reference count: 37
- Primary result: CoCoA achieves up to 9.2 points higher QA accuracy and up to 2.5 points better factuality than AdaCAD by resolving knowledge conflicts between model memory and external context.

## Executive Summary
CoCoA addresses the challenge of knowledge conflicts between a language model's parametric memory and external context, which can lead to unfaithful generation. The method introduces a token-level adaptive decoding algorithm that resolves conflicts using confidence-aware measures—entropy gap and contextual peakedness—combined with Rényi divergence between prior and contextual distributions. Crucially, CoCoA maintains strong performance even in low-conflict settings where existing methods degrade. Experiments across diverse QA, summarization, and LFQA benchmarks show CoCoA outperforms strong baselines like AdaCAD, achieving significant gains in accuracy and factuality while demonstrating superior sensitivity to conflict variations.

## Method Summary
CoCoA resolves knowledge conflicts through a token-level adaptive decoding mechanism that blends the model's prior distribution with context-conditioned distributions. The method computes Rényi divergence (α < 1) between prior and context distributions to detect conflicts, particularly in low-confidence regimes. It combines this with entropy gap (ΔH = H(prior) - H(context)) to measure uncertainty reduction and contextual peakedness (margin between top-2 tokens) to assess context confidence. A dynamic gating mechanism blends the distributions based on conflict score and peakedness, favoring context in high-conflict/confident situations and prior otherwise. The approach requires two forward passes per token and adds ~32% latency compared to CAD.

## Key Results
- CoCoA achieves up to 9.2 points higher QA accuracy than AdaCAD on conflict-heavy NQ-SWAP dataset
- Maintains NQ accuracy within 2.6 points of greedy decoding in low-conflict settings
- Improves factuality by up to 2.5 points on summarization and LFQA tasks
- Shows superior sensitivity to conflict variations with |Δρ| = 0.21 correlation gap vs AdaCAD's 0.08

## Why This Works (Mechanism)

### Mechanism 1: Tail-Sensitive Conflict Detection via Rényi Divergence
Replacing Jensen-Shannon Divergence (JSD) with Rényi divergence (order α < 1) improves detection of subtle conflicts between prior and context distributions, particularly in low-confidence regimes. JSD saturates on peaked or heavy-tailed distributions typical in autoregressive decoding, missing subtle distributional shifts. Rényi divergence with α < 1 amplifies sensitivity to low-probability events in the distribution tail, surfacing sharp contextual shifts even when overall token distributions appear similar.

### Mechanism 2: Contextual Confidence Estimation via Entropy Gap and Peakedness
Combining entropy gap (ΔH = H(prior) - H(context)) with contextual peakedness (margin between top-2 tokens) provides reliable signal for distinguishing confident context from noise. The entropy gap captures uncertainty reduction when context concentrates probability mass. Contextual peakedness measures confidence in context's preferred token. Together, these prevent over-reliance on noisy or ambiguous context while amplifying confident contextual signals.

### Mechanism 3: Adaptive Gating with Divergence-Stabilized Entropy Ratios
A dynamic gating mechanism blends prior and context distributions based on conflict score and contextual peakedness, maintaining performance across both high and low conflict settings. The conflict score measures severity, while blending weight integrates peakedness with conflict: high conflict AND confident context pushes toward context; low conflict OR weak context pushes toward prior. This achieves context-aware interpolation without heuristic warm-up.

## Foundational Learning

- **Concept: Contrastive Decoding**
  - Why needed here: CoCoA builds on CAD/AdaCAD's core idea: contrast context-conditioned distribution against unconditional prior to amplify context-grounded tokens. Understanding this baseline clarifies what CoCoA improves.
  - Quick check question: Given pctx("Paris") = 0.8 and p("Paris") = 0.3, what does contrastive decoding with α = 1.0 produce for the adjusted probability ratio?

- **Concept: Divergence Measures (KL, JSD, Rényi)**
  - Why needed here: The key architectural choice replacing JSD with Rényi divergence requires understanding how different divergence measures weight distribution tails vs. modes.
  - Quick check question: For two distributions P and Q where Q has higher probability on a rare token, which divergence (KL, JSD, or Rényi with α < 1) would most strongly penalize this difference?

- **Concept: Entropy as Uncertainty Quantification**
  - Why needed here: The entropy gap signal assumes you understand how H(P) measures distribution concentration and how comparing entropies reveals confidence changes.
  - Quick check question: If H(prior) = 2.5 and H(context) = 0.8, what does ΔH = 1.7 indicate about the context's effect on model certainty?

## Architecture Onboarding

- **Component map:** Input: query x, context c → [Forward Pass 1] → pθ(yt | x, y<t) [prior distribution] → [Forward Pass 2] → pctx(yt | c, x, y<t) [context distribution] → [Rényi Divergence] → Dα(pθ || pctx) [conflict signal 1] → [Entropy Gap] → ΔH = H(pθ) - H(pctx) [conflict signal 2] → [Contextual Peakedness] → mt = pctx(y¹) - pctx(y²) [confidence signal] → [Conflict Score] → st = σ(Dα + γΔH + δ) → [Blending Weight] → λt = σ(z·log(mt) + log((1-st)/st)) → [Blended Distribution] → q(yt) ∝ pctx^λt · p^(1-λt) → [Sample/Argmax] → output token

- **Critical path:** The two forward passes (prior + context) are the latency bottleneck. Parallelize across 2 GPUs as authors do. The divergence/confidence computation is O(vocab_size) but negligible vs. forward passes.

- **Design tradeoffs:**
  - Latency vs. accuracy: CoCoA adds ~0.4s per token vs. CAD (Table 17: 1.63s vs 1.23s). Acceptable for batched/offline, may need caching for real-time.
  - Rényi order α: Authors fix α = 0.5 (Table 14 ablation). Lower α increases tail sensitivity but may over-amplify noise.
  - Peakedness weight z: Authors fix z = 5.0. Higher z makes gating more sensitive to context confidence.

- **Failure signatures:**
  - Over-correction in low-conflict: If λt stays high when context agrees with prior, you'll see degraded performance on standard QA (check NQ accuracy vs. greedy baseline).
  - Under-correction in high-conflict: If λt stays low on NQ-SWAP, the model ignores context substitutions.
  - Latency spikes: If forward passes aren't parallelized, expect 2x slowdown.

- **First 3 experiments:**
  1. Reproduce Table 1 subset: Run CoCoA vs. AdaCAD vs. greedy on NQ and NQ-SWAP with Llama3-8B. Verify ~3-12 point gains on NQ-SWAP and ~5 point gains on NQ.
  2. Ablate Rényi → JSD: Replace Dα with JSD in conflict score. Expect ~3-10 point drop on NQ-SWAP (per Table 4: 79.15 → 76.69).
  3. Visualize λt over generation: Plot λt values for high-conflict vs. low-conflict examples. Should see λt ≈ 0.8-1.0 for conflicting tokens and λt ≈ 0.1-0.3 for agreed tokens (per Figure 2 pattern).

## Open Questions the Paper Calls Out

- **Can CoCoA's conflict detection and adaptive blending mechanisms be effectively approximated for fully black-box LLM APIs that do not expose underlying logits or softmax scores?**
  - Basis: The paper states CoCoA relies on fine-grained access to token-level probability distributions, posing a "barrier when working with fully black-box APIs (e.g., GPT-4)."
  - Unresolved because: The methodology mathematically depends on calculating Rényi divergence, entropy gaps, and peakedness directly from log-likelihoods, which proprietary APIs typically withhold.
  - Evidence needed: A study demonstrating a proxy method (e.g., using sampling-based entropy estimation) that achieves comparable faithfulness scores on black-box models without direct logit access.

- **How does CoCoA's performance and calibration hold up in specialized domains (e.g., legal, medical, code) and non-English languages with different linguistic structures?**
  - Basis: The authors note the study is "confined to English-language benchmarks and a handful of widely-used open-source models" and suggest validating mechanisms under different linguistic characteristics and domain-specific knowledge structures.
  - Unresolved because: The current experiments focus on general-purpose English QA and summarization; it is unknown if the entropy-gap and Rényi divergence signals function similarly in agglutinative languages or high-stakes domains where "conflict" may be more nuanced.
  - Evidence needed: Empirical results from evaluating CoCoA on multilingual QA datasets (e.g., TyDi QA) and specialized domain benchmarks (e.g., legal or biomedical NLP) showing consistent improvements over baselines.

- **Does CoCoA's stronger reliance on external context inadvertently amplify the generation of harmful or misleading content when the retrieved context is adversarial or biased?**
  - Basis: The Limitations section explicitly calls for future work to "ensure that CoCoA's stronger reliance on external knowledge does not inadvertently amplify misleading or harmful content."
  - Unresolved because: While the paper demonstrates CoCoA resolves conflicts effectively, it assumes the provided context is the "ground truth" to be faithful to. It does not evaluate scenarios where the context itself is low-quality or toxic.
  - Evidence needed: Experiments using adversarial retrieval datasets where the context contains false or toxic information, measuring whether CoCoA increases the rate of hallucinations or toxicity compared to standard decoding.

## Limitations

- Dataset and evaluation dependencies on gold contexts curated by AdaCAD authors without public release details, making full reproduction contingent on external data access
- Architectural scaling assumptions unproven for smaller models (<7B parameters) where distributional behavior may differ significantly
- Generalizability of conflict patterns limited as NQ-SWAP synthetic conflicts may not fully represent naturally occurring knowledge conflicts

## Confidence

- **High Confidence (80-95%)**: CoCoA outperforms AdaCAD and CAD on NQ-SWAP (high-conflict) by 9.2-12.8 EM points; CoCoA maintains NQ performance within 2.6 points of greedy decoding; Rényi divergence with α < 1 provides better tail-sensitive conflict detection than JSD
- **Medium Confidence (60-79%)**: Contextual peakedness reliably distinguishes trustworthy from untrustworthy context; the entropy gap consistently indicates confidence changes across diverse tasks; generalization to naturally occurring conflicts matches synthetic performance
- **Low Confidence (Below 60%)**: The specific hyperparameter values (α=0.5, z=5.0) are optimal across all model scales; MiniCheck FaithScore provides reliable factuality measurement without calibration issues; real-time deployment feasibility without approximation strategies

## Next Checks

1. **Zero-shot conflict detection evaluation**: Run CoCoA on NQ-SWAP without any gold contexts, using only the model's own context retrieval. Measure whether the entropy gap and Rényi divergence signals still correctly identify conflicting tokens when context quality varies naturally.

2. **Failure mode analysis on confident wrong context**: Construct adversarial examples where context is confidently incorrect (e.g., "The capital of France is London" with high token probability). Verify whether peakedness-based gating amplifies these errors and measure the magnitude of degradation.

3. **Cross-model generalization test**: Evaluate CoCoA on a 3B parameter model (e.g., Mistral-3B) using identical hyperparameters. Document whether the tail-sensitive Rényi advantage persists or if distributional differences require parameter recalibration.