---
ver: rpa2
title: Online Learning of Optimal Sequential Testing Policies
arxiv_id: '2509.03707'
source_url: https://arxiv.org/abs/2509.03707
tags:
- testing
- test
- online
- learning
- regret
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies the Online Testing Problem (OTP), where an\
  \ agent sequentially selects tests to probe optimal decisions for a sequence of\
  \ subjects. The key challenge is that rewards depend on missing data\u2014when tests\
  \ are skipped, the true reward cannot be observed, making it fundamentally harder\
  \ than standard episodic MDPs."
---

# Online Learning of Optimal Sequential Testing Policies

## Quick Facts
- **arXiv ID:** 2509.03707
- **Source URL:** https://arxiv.org/abs/2509.03707
- **Reference count:** 7
- **Primary result:** Proves OTP has Ω(T^2/3) minimax regret vs Θ(√T) for episodic MDPs; achieves Õ(d|P|^1/3 T^2/3) via Explore-Then-Commit algorithm

## Executive Summary
This paper studies the Online Testing Problem (OTP), where an agent sequentially selects tests to make optimal decisions for a sequence of subjects. The key challenge is that rewards depend on missing data—when tests are skipped, the true reward cannot be observed, making it fundamentally harder than standard episodic MDPs. The authors prove that the minimax regret for OTP scales as Ω(T^2/3), compared to Θ(√T) for episodic MDPs. This elevated lower bound arises from the inherent conflict between exploration and exploitation due to missing rewards. To address this, they propose an Explore-Then-Commit (ETC) algorithm that achieves Õ(d|P|^1/3 T^2/3) regret for discrete distributions and Õ(dσ^2 T^2/3) for Gaussian distributions, matching the lower bound up to logarithmic factors. The authors also introduce the Online Cost-sensitive Maximum Entropy Sampling Problem (OCMESP), a variant where rewards depend only on observed tests, and show it achieves faster Õ(d^3σ√T) regret.

## Method Summary
The paper addresses OTP through an Explore-Then-Commit (ETC) approach. The algorithm first runs N = ⌊|P|^(1/3) T^(2/3)⌋ episodes where it performs all possible tests on each subject, collecting complete observations to build an unbiased empirical estimate P̂ of the unknown distribution. Using this estimate, it then computes an optimal testing policy via dynamic programming (Algorithm 1). For the remaining T-N episodes, it commits to this policy without further exploration. The paper also develops an iterative-elimination algorithm for the OCMESP variant where rewards depend only on observed tests, achieving Õ(d^3σ√T) regret through repeated testing and elimination of suboptimal policies.

## Key Results
- OTP has Ω(T^2/3) minimax regret vs Θ(√T) for episodic MDPs due to missing reward observations
- ETC algorithm achieves Õ(d|P|^1/3 T^2/3) regret for discrete distributions and Õ(dσ^2 T^2/3) for Gaussian distributions
- OCMESP variant (rewards depend only on observed tests) achieves faster Õ(d^3σ√T) regret
- The hardness gap between OTP and OCMESP demonstrates that missing rewards are the fundamental source of difficulty

## Why This Works (Mechanism)

### Mechanism 1: The Missing Reward Hardness
In standard MDPs, you observe the reward for the action taken. In OTP, if you skip a test to save cost, the latent outcome remains unobserved. Because the final reward depends on the full vector (including skipped tests), the true reward for a decision made with partial information is structurally unobservable. This breaks standard exploration strategies that rely on observing the value of the current policy.

### Mechanism 2: Explore-Then-Commit (ETC) Unbiasing
To bypass the "missing reward" trap, the agent initially ignores costs and runs all tests on N subjects. This brute-force exploration guarantees an unbiased empirical estimate of the joint distribution because there are no missing entries in the training set. The agent then "commits" to a policy derived from this estimate using dynamic programming.

### Mechanism 3: Structure-Based Rate Recovery
If the reward function depends only on observed tests, the Ω(T^2/3) barrier breaks and Õ(√T) regret is achievable. In this case, the agent can use iterative elimination: test candidates, estimate their value based only on what was seen, and eliminate the weak ones. The missing data no longer hides the reward signal.

## Foundational Learning

- **Markov Decision Processes (MDPs) & Value Iteration**: Needed to understand how testing policies are framed as MDPs and how optimal policies are computed via dynamic programming. Quick check: Why does the paper suppress the "horizon" in the MDP formulation?
- **Missing Data Mechanisms (MAR vs MCAR)**: The paper's central theoretical contribution hinges on why "Missing At Random" (MAR) data is problematic for online learning. In OTP, stopping a test depends on observed values (MAR), biasing the dataset. Quick check: Why does the "Iterative Elimination" algorithm for OCMESP effectively treat data as Missing-Completely-At-Random (MCAR)?
- **Minimax Regret**: The paper is a theoretical characterization of difficulty. Understanding the difference between O(√T) and O(T^2/3) regret rates quantifies the "price" of having missing rewards. Quick check: Does a higher exponent in the regret bound (e.g., 2/3 vs 1/2) imply faster or slower convergence to the optimal policy?

## Architecture Onboarding

- **Component map:** Data Generator -> State Builder -> Policy Engine (Exploration/Commit modes) -> Distribution Estimator
- **Critical path:**
  1. Input: Horizon T
  2. Explore: For t=1 to N, run every test regardless of cost. Accumulate full vectors
  3. Estimate: Compute P̂ from the N full vectors
  4. Solve: Run Dynamic Programming on P̂ to generate π*
  5. Commit: For t=N+1 to T, execute π*

- **Design tradeoffs:**
  - Cost vs. Convergence: The algorithm deliberately "wastes" budget on unnecessary tests during exploration to guarantee an unbiased P̂
  - Parametric vs. Non-parametric: Discrete regret scales with support size |P|; Gaussian regret scales with condition number σ

- **Failure signatures:**
  - Stagnation: If exploration budget N is too low, P̂ will be inaccurate and the committed policy remains suboptimal indefinitely
  - Distribution Shift: If P changes after exploration phase, ETC has no mechanism to detect the shift

- **First 3 experiments:**
  1. Implement the single-test example from Theorem 1 to empirically confirm T^2/3 scaling
  2. Compare regret of Algorithm 2 (OTP, missing rewards) vs Algorithm 4 (OCMESP, observed rewards) on same Gaussian distribution
  3. Vary exploration multiplier N around optimal |P|^(1/3)T^(2/3) threshold to verify U-shape of ETC regret

## Open Questions the Paper Calls Out

### Open Question 1
Can the Ω(T^2/3) lower bound for OTP be achieved with algorithms that do not require knowledge of the time horizon T, other than via the doubling trick which introduces additional overhead? The authors note that ETC takes T as input and suggest doubling trick as a remedy, but do not analyze whether alternative any-time algorithms could achieve tighter bounds.

### Open Question 2
Can the regret bounds for OTP be extended beyond Gaussian distributions to other parametric or nonparametric continuous distributions? The authors state that Theorem 3 does not automatically generalize to continuous distributions since the support becomes uncountable and L1 deviations do not converge without further assumptions.

### Open Question 3
What are the regret bounds for OTP when test outcomes are noisy and require repeated measurements? The paper explicitly assumes noiseless observations but does not analyze the setting where this assumption is violated.

### Open Question 4
Can the |P|^(1/3) dependence in the discrete OTP regret bound be improved when the distribution has additional structure (e.g., sparsity, graphical model structure, or low-rank correlations)? The current analysis treats the distribution as a black box; exploiting structure could potentially reduce the effective support size.

## Limitations

- **Distribution Assumptions**: ETC's regret guarantees depend critically on distributions being either discrete (bounded support) or Gaussian; performance under heavy-tailed or non-parametric settings is not addressed
- **Stationarity Requirement**: Analysis assumes stationary environment where distribution P remains constant, limiting applicability to evolving real-world scenarios
- **Computational Complexity**: While proving optimal regret rates, the paper does not thoroughly analyze computational complexity of the dynamic programming step for high-dimensional problems

## Confidence

- **High Confidence**: The theoretical lower bound proof (Ω(T^2/3)) is rigorous and well-supported by reduction to a bandit problem
- **Medium Confidence**: ETC algorithm's regret bound matching the lower bound up to logarithmic factors is convincing but practical performance depends on correctly setting exploration budget N
- **Medium Confidence**: OCMESP variant's faster √T regret rate is well-proven but assumes reward function decomposes cleanly to depend only on observed tests

## Next Checks

1. **Robustness to Distribution Misspecification**: Implement ETC on synthetic data where true distribution is neither discrete nor Gaussian (e.g., Laplace or t-distribution) to test whether algorithm maintains sublinear regret
2. **Non-Stationary Environment Test**: Create variant where distribution P shifts after exploration phase and measure how quickly ETC's committed policy becomes suboptimal compared to adaptive algorithm
3. **Computational Scaling Experiment**: Systematically vary number of tests d and measure both computational time for dynamic programming step and resulting regret to identify practical limits