---
ver: rpa2
title: 'Moonbeam: A MIDI Foundation Model Using Both Absolute and Relative Music Attributes'
arxiv_id: '2505.15559'
source_url: https://arxiv.org/abs/2505.15559
tags:
- music
- moonbeam
- data
- relative
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Moonbeam is a transformer-based foundation model for symbolic music,
  pretrained on 81.6K hours of diverse MIDI data (18 billion tokens). It introduces
  a novel tokenizer supporting multi-instrument and expressive MIDI data, and a Multidimensional
  Relative Attention (MRA) mechanism that encodes both absolute and relative musical
  attributes across onset, duration, octave, pitch, and velocity.
---

# Moonbeam: A MIDI Foundation Model Using Both Absolute and Relative Music Attributes

## Quick Facts
- arXiv ID: 2505.15559
- Source URL: https://arxiv.org/abs/2505.15559
- Reference count: 40
- Key outcome: Moonbeam outperforms other large-scale pretrained models on player, emotion, and composer classification tasks across four datasets in terms of accuracy and F1 score.

## Executive Summary
Moonbeam is a transformer-based foundation model for symbolic music, pretrained on 81.6K hours of diverse MIDI data (18 billion tokens). It introduces a novel tokenizer supporting multi-instrument and expressive MIDI data, and a Multidimensional Relative Attention (MRA) mechanism that encodes both absolute and relative musical attributes across onset, duration, octave, pitch, and velocity. Moonbeam outperforms other large-scale pretrained models on player, emotion, and composer classification tasks across four datasets in terms of accuracy and F1 score. For conditional generation, it also outperforms a strong transformer baseline on controllability and musicality, as validated by objective metrics and expert listening tests.

## Method Summary
Moonbeam uses a transformer backbone with LLaMA-style architecture, replacing Grouped Query Attention with Multidimensional Relative Attention (MRA). The model employs a novel tokenizer that converts MIDI events into compound tokens with six attributes (onset, duration, octave, pitch, instrument, velocity). Absolute onset times are embedded using Fundamental Music Embedding (FME), while other attributes use standard embeddings. A GRU-based decoder generates the next event sequentially. The model is pretrained on 18 billion tokens from diverse MIDI sources and evaluated on classification and conditional generation tasks.

## Key Results
- Moonbeam achieves 2.423 test perplexity vs 2.512 for standard attention
- Outperforms other large-scale pretrained models on player, emotion, and composer classification tasks
- GRU decoder improves perplexity from 3.245 (MLP) to 2.423

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Encoding relative positional information across multiple musical dimensions (onset, pitch, velocity, etc.) reduces test perplexity compared to standard 1-D attention.
- **Mechanism:** MRA extends Rotary Position Embeddings (RoPE) by rotating query/key vectors based on specific attribute values rather than just token index. Attention heads are partitioned into groups, each rotating by different musical attributes.
- **Core assumption:** Musical semantics rely more heavily on relative relationships between attributes than on absolute sequence indices.
- **Evidence anchors:** Abstract mentions MRA captures relative music information; Section 3.2.1 describes the 5-D space and G=6 head configuration; Table 1 shows 2.423 PPL vs 2.512 PPL.

### Mechanism 2
- **Claim:** Using absolute onset times embedded via FME mitigates arithmetic accumulation errors typical of delta-time tokenization.
- **Mechanism:** FME projects absolute onset times into continuous sinusoidal embeddings, allowing the model to read time directly without summing previous deltas.
- **Core assumption:** Standard transformers struggle with the arithmetic required to sum previous time-shifts to determine current onset.
- **Evidence anchors:** Section 3.1 states transformers struggle with arithmetic and that absolute onsets mitigate this; FME enables conversion without increasing dictionary size.

### Mechanism 3
- **Claim:** Sequential decoding of compound tokens via GRU improves modeling of inter-attribute conditional probabilities.
- **Mechanism:** Transformer output passes to GRU which generates 6 attributes sequentially, allowing conditioning (e.g., instrument prediction conditioned on pitch).
- **Core assumption:** Attributes within compound tokens are highly correlated and autoregressive dependencies exist.
- **Evidence anchors:** Section 3.2.2 mentions modeling conditional distribution of subsequent attribute tokens; Table 1 shows "Moonbeam w/o GRU" rising to 3.245 PPL.

## Foundational Learning

- **Concept: Rotary Position Embeddings (RoPE)**
  - Why needed: Moonbeam's MRA is an extension of RoPE; understanding complex rotation matrices is essential for debugging attention mechanism.
  - Quick check: If you rotate vector q by angle θm and vector k by angle θn, how does their dot product depend on relative distance m-n?

- **Concept: Inductive Biases in Symbolic Music**
  - Why needed: The paper optimizes for "translation equivariance"; understanding why pitch transposition shouldn't change musical pattern is necessary to evaluate MRA.
  - Quick check: Why would standard embedding tables struggle to generalize if you shifted a melody up by one octave compared to relative encoding?

- **Concept: Sinusoidal/Continuous Embeddings (FME)**
  - Why needed: FME handles continuous values and extrapolation without infinite vocabulary.
  - Quick check: How does sinusoidal function allow model to represent values it never saw exactly in training set?

## Architecture Onboarding

- **Component map:** Raw MIDI -> Tokenizer (compound tokens) -> Embedder (FME + standard embeddings) -> Transformer backbone (MRA) -> GRU decoder (6 attributes)

- **Critical path:** MRA implementation - ensuring partitioning of attention heads matches orthogonal decomposition of 5-D musical space.

- **Design tradeoffs:**
  - Absolute vs. Relative Time: Moonbeam uses Absolute Onsets (via FME) to solve arithmetic issues and enable infilling vs. delta-time's scale-invariance but error-proneness
  - GRU vs. Parallel Head: GRU introduces sequential latency but yields 2.423 vs 3.245 PPL; parallel is faster but less accurate

- **Failure signatures:**
  - "Drifting" Velocity/Duration: FME parameters not tuned causes continuous attributes to smear
  - Wrong Instrument Assignment: GRU removed/undertrained leads to bass notes assigned to soprano instruments
  - Loss of Temporal Coherence: MRA head groups reduced to 1 reverts to standard behavior, increasing perplexity

- **First 3 experiments:**
  1. Ablate GRU: Replace with standard linear projection (MLP) and verify perplexity rises from ~2.4 to ~3.2
  2. MRA Dimension Stress Test: Retrain with G=1 vs G=6 and compare validation loss on high rhythmic complexity dataset
  3. Extrapolation Test: Feed onset times/durations larger than max_duration and verify FME handles gracefully

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent does predominance of Western music in pretraining data bias Moonbeam's ability to model non-Western musical structures effectively?
- Basis: Appendix B acknowledges training data mainly consists of Western music likely to introduce biases
- Why unresolved: Evaluation focuses exclusively on Western genres (classical, pop, jazz) via datasets like GPM30 and Emopia
- Evidence needed: Benchmarking Moonbeam on non-Western music datasets compared to Western performance

### Open Question 2
- Question: How does exclusion of MIDI controller data (CC values) and metadata limit model's capability to generate highly expressive performances?
- Basis: Appendix B notes model limited to note-level MIDI information, excluding controller data for modulation, volume, sustain
- Why unresolved: Paper doesn't analyze quality loss from discarding continuous control information
- Evidence needed: Comparative study with CC value tokenization variant followed by expressiveness evaluation

### Open Question 3
- Question: Can MRA mechanism effectively replace standard positional encodings in non-musical domains with multi-dimensional relative dependencies?
- Basis: Conclusion states architecture has potential applications beyond music in domains involving multi-dimensional data
- Why unresolved: MRA validated only on symbolic music tasks; applicability in other domains remains proposed future direction
- Evidence needed: Implementing MRA in transformer models for non-musical tasks and comparing against standard RoPE

## Limitations

- Dataset composition lacks detailed breakdown of genre, instrument, or cultural representation, making generalization assessment difficult
- Evaluation scope focuses on narrow classification tasks rather than open-ended composition or cross-instrument style transfer
- Human evaluation lacks details on evaluator expertise, sample size, or blinding procedures

## Confidence

**High Confidence:**
- MRA reduces test perplexity compared to standard attention
- GRU decoder improves perplexity over parallel MLP head
- FME mitigates arithmetic accumulation in onset modeling

**Medium Confidence:**
- Moonbeam outperforms other large-scale pretrained models on classification tasks
- Moonbeam outperforms transformer baseline on controllability and musicality

**Low Confidence:**
- Generalizability of performance gains across diverse musical styles
- Long-term coherence in extended conditional generation

## Next Checks

1. **Dataset Bias Audit:** Analyze training corpus for genre, instrument, and cultural representation; retrain on balanced subset and re-evaluate classification performance

2. **Extended Generation Coherence:** Use Moonbeam for long-form conditional generation (2+ minutes) and evaluate temporal coherence, harmonic progression, and stylistic consistency using automated metrics and expanded expert panels

3. **Ablation of MRA Dimensions:** Systematically vary number of MRA head groups (G) and measure impact on perplexity and downstream task performance