---
ver: rpa2
title: 'RayRoPE: Projective Ray Positional Encoding for Multi-view Attention'
arxiv_id: '2601.15275'
source_url: https://arxiv.org/abs/2601.15275
tags:
- rayrope
- depth
- encoding
- attention
- rope
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of designing positional encodings
  for multi-view transformers that process tokens from multiple posed input images.
  Prior approaches either lack SE(3) invariance, uniqueness, or geometry adaptiveness.
---

# RayRoPE: Projective Ray Positional Encoding for Multi-view Attention

## Quick Facts
- arXiv ID: 2601.15275
- Source URL: https://arxiv.org/abs/2601.15275
- Authors: Yu Wu, Minsik Jeon, Jen-Hao Rick Chang, Oncel Tuzel, Shubham Tulsiani
- Reference count: 40
- Key outcome: RayRoPE consistently outperforms prior methods, with 15% relative improvement on LPIPS in CO3D and up to 29% improvement when known depths are available.

## Executive Summary
RayRoPE introduces a novel ray-based positional encoding for multi-view transformers that addresses the challenge of representing patch positions across multiple posed input images. The method achieves SE(3) invariance by projecting rays to the query camera frame, incorporates geometry awareness through predicted depth along rays, and models uncertainty via expected RoPE computations. Experimental results demonstrate significant improvements over prior approaches on novel view synthesis tasks, particularly when incorporating known depth information.

## Method Summary
RayRoPE represents patch positions using ray segments parameterized by camera center and predicted point along the ray. It achieves SE(3) invariance by projecting rays to the query camera frame and applying multi-frequency rotary position embeddings. The method also incorporates uncertainty modeling through expected RoPE computations and seamlessly integrates known depth information. The encoding is computed by projecting global rays into the local coordinate system of the query token's camera, predicting depth and uncertainty per token, and applying an analytically computed expected value of the RoPE over the depth distribution.

## Key Results
- Achieves 15% relative improvement on LPIPS metric in CO3D dataset
- Shows up to 29% improvement when known depths are available
- Outperforms all prior multi-view attention methods across experimental settings
- Demonstrates geometry awareness through depth prediction without additional supervision

## Why This Works (Mechanism)

### Mechanism 1: Query-Frame Projective Invariance
RayRoPE achieves SE(3) invariance by transforming global ray coordinates into the local coordinate system of the query token's camera before applying positional encoding. Instead of encoding positions in a global world frame, which varies with arbitrary coordinate choices, RayRoPE projects all rays onto the query camera's image plane and local depth axis. Standard Rotary Position Embeddings (RoPE) are then applied to these relative coordinates.

### Mechanism 2: Geometry-Adaptive Depth Prediction
Representing patch positions as ray segments with a predicted depth d (rather than rays at infinity) allows the encoding to adapt to scene geometry, increasing similarity for tokens viewing the same 3D point. The model predicts a depth d per token using a linear layer without direct supervision. By parameterizing the position as (c, p_d) instead of (c, p_∞), the positional encoding becomes a function of the estimated distance to the surface.

### Mechanism 3: Analytical Expected RoPE (Uncertainty Modeling)
Analytically computing the expected value of the RoPE over a depth distribution smooths high-frequency noise when predicted depths are uncertain. Along with depth d, the model predicts an uncertainty σ. The RoPE operation (a complex exponential rotation) is integrated over the interval [d-σ, d+σ]. This results in a dampened rotation magnitude for high frequencies when uncertainty is high, preventing jittery attention maps.

## Foundational Learning

- **Concept: Rotary Position Embeddings (RoPE)**
  - Why needed here: RayRoPE is essentially the RoPE mechanism applied to 6D projective coordinates. You must understand how RoPE uses complex exponentials to rotate feature vectors to encode relative distance.
  - Quick check question: How does RoPE ensure that the dot product between two vectors depends only on their relative position (translation invariance)?

- **Concept: Projective Geometry & Camera Models**
  - Why needed here: The method relies on projecting 3D rays into the query camera's coordinate frame using P = K[R|t]. Understanding how a 3D point maps to pixel coordinates and disparity is required to implement the projection operator π.
  - Quick check question: Given a 3D point in world coordinates and a camera matrix P, how do you compute its 2D pixel location and depth in the camera's local frame?

- **Concept: SE(3) Invariance**
  - Why needed here: A core motivation is removing dependence on the arbitrary "global coordinate system." Understanding SE(3) (3D rotation and translation) helps clarify why prior methods using global coordinates failed under pose variations.
  - Quick check question: Why does a positional encoding dependent on global coordinates fail if we rotate the entire scene (and all cameras) by 90 degrees, while an SE(3)-invariant method succeeds?

## Architecture Onboarding

- **Component map:** Backbone/Tokenizer -> Ray Constructor -> Depth/Uncertainty Head -> Projector -> Expected RoPE Layer
- **Critical path:** The "depth prediction" → "projection" → "expected rotation" loop is the critical novelty. Specifically, the projection step (Sec 4.2) must be implemented efficiently as it changes for every query camera pair in the batch.
- **Design tradeoffs:**
  - Depth Prediction vs. Ray Direction: Using predicted depth (p_d) adds geometry awareness but introduces noise. Using ray direction (p_∞) is stable but ignores scene geometry.
  - Computational Cost: RayRoPE requires computing encodings per query camera group, which may increase memory traffic compared to simple global encodings, though the paper notes it is comparable to PRoPE.
- **Failure signatures:**
  - High-frequency artifacts: If uncertainty σ is under-predicted or the expected RoPE is omitted, high-frequency bands in the RoPE become noisy.
  - Loss of Uniqueness: If standard 2D RoPE on patch indices (u,v) is added naively, the system loses the "uniqueness" property where the same patch seen in different crops should have the same encoding.
- **First 3 experiments:**
  1. Invariance Test: Rotate the global coordinate system of a scene arbitrarily and verify that attention maps and output metrics (PSNR/LPIPS) remain identical.
  2. Depth Emergence Visualization: Visualize the predicted depth maps from intermediate layers to confirm the model is learning geometry, not just outputting constant depth.
  3. Uncertainty Ablation: Compare LPIPS on high-frequency textures between the "deterministic RoPE" variant and the "expected RoPE" variant to quantify the value of uncertainty modeling.

## Open Questions the Paper Calls Out
- How can RayRoPE be extended to model uncertainty in camera matrices (poses and intrinsics) in addition to the currently modeled depth uncertainty?
- Can RayRoPE be adapted for multi-view transformers that process unposed or mixed image sets where camera parameters are unknown?
- How does the assumption of a uniform distribution for depth uncertainty in the Expected RoPE calculation impact performance compared to other probabilistic models?

## Limitations
- The method's robustness to extreme baselines, wide FOV distortions, or scenes with many views remains unclear
- Computational overhead for larger batch sizes or longer sequences is not quantified
- The uniform distribution assumption for depth uncertainty may not capture all real-world ambiguities

## Confidence
- **High:** SE(3) invariance through query-frame projection, depth prediction improving geometry awareness, and the overall superiority of RayRoPE over baselines (CO3D results)
- **Medium:** The effectiveness of expected RoPE for uncertainty smoothing and the method's performance with known depth
- **Low:** The method's scalability to large-scale multi-view datasets (e.g., 100+ cameras) and its behavior under extreme geometric configurations

## Next Checks
1. Scale Test: Evaluate RayRoPE on a dataset with many more cameras (e.g., DTU or BlendedMVS) to assess scalability and attention sparsity.
2. Extreme Pose Test: Create synthetic scenes with extreme baselines or rotations and measure the degradation in PSNR/LPIPS compared to moderate baselines.
3. Uncertainty Ablation with Ground Truth: Use a dataset with ground truth depth uncertainty (e.g., CO3D with depth estimates) to compare the effectiveness of uniform vs. learned uncertainty distributions in the expected RoPE computation.