---
ver: rpa2
title: Critical Appraisal of Fairness Metrics in Clinical Predictive AI
arxiv_id: '2506.17035'
source_url: https://arxiv.org/abs/2506.17035
tags:
- fairness
- metrics
- clinical
- metric
- parity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper conducts a scoping review to identify and critically
  appraise fairness metrics used in clinical predictive AI. It defines a fairness
  metric as a measure quantifying whether a model discriminates against individuals
  or groups based on sensitive attributes.
---

# Critical Appraisal of Fairness Metrics in Clinical Predictive AI

## Quick Facts
- **arXiv ID:** 2506.17035
- **Source URL:** https://arxiv.org/abs/2506.17035
- **Reference count:** 3
- **Primary result:** 62 distinct fairness metrics extracted from 820 screened records; classified by performance-dependency, model output level, and base performance metric; reveals fragmented landscape with limited clinical validation and overreliance on threshold-dependent measures.

## Executive Summary
This scoping review identifies and critically appraises fairness metrics used in clinical predictive AI, defining a fairness metric as a measure quantifying whether a model discriminates against individuals or groups based on sensitive attributes. From 820 screened records, 62 distinct metrics were extracted and classified across three dimensions. The review reveals a fragmented landscape dominated by threshold-dependent measures with minimal clinical validation. Only one clinical utility metric was identified, highlighting a critical gap between statistical fairness and patient outcomes. The authors recommend prioritizing clinically meaningful metrics aligned with decision-making and patient outcomes.

## Method Summary
The review followed PRISMA-ScR guidelines, searching five databases (PubMed, ACM Digital Library, IEEE Xplore, arXiv, medRxiv) from January 2014 to October 2024. A single reviewer (JM) screened 820 records and extracted data using structured forms. Metrics were classified by performance-dependency (independent vs. dependent), model output level (probability vs. threshold-based), and base performance metric type. Critical appraisal guidance provided three-level recommendations: Recommended, Use with caution, or Inadvisable. Supplementary materials include detailed extraction forms and search queries.

## Key Results
- 62 distinct fairness metrics extracted and classified across three taxonomic dimensions
- Overreliance on threshold-dependent measures that fluctuate with arbitrary decision cutoffs
- Only one clinical utility metric identified (subgroup net benefit)
- Critical gaps identified in clinical validation, uncertainty quantification, and intersectionality analysis

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Selecting a fairness metric requires determining whether outcome labels represent valid "status quo" or encode historical biases
- **Mechanism:** The taxonomy distinguishes between performance-dependent metrics (assume labels are valid) and performance-independent metrics (ignore labels to check for output equity)
- **Core assumption:** Historical outcome data accurately reflects biological reality rather than systemic access disparities
- **Evidence anchors:** Abstract states "classifying by performance-dependency... revealing a fragmented landscape"; Section 6 notes performance-dependent metrics are justified if labels are fair, otherwise performance-independent metrics are more appropriate
- **Break condition:** If labels are proxies rather than direct clinical outcomes, performance-dependent metrics will perpetuate bias

### Mechanism 2
- **Claim:** Prioritizing probability-based metrics (e.g., Calibration, AUROC) over threshold-dependent metrics improves robustness
- **Mechanism:** Threshold-dependent metrics fluctuate based on chosen probability cutoff; probability-based metrics assess the entire risk spectrum
- **Core assumption:** Clinical decision threshold is not fixed by external guidelines or is subject to change
- **Evidence anchors:** Abstract notes "overreliance on threshold-dependent measures"; Section 10 states these metrics are "inherently limited by the fact that they rely on a decision threshold that may be chosen with no clinical rationale"
- **Break condition:** If model is strictly used for binary classification with fixed threshold, threshold-dependent metrics become necessary

### Mechanism 3
- **Claim:** Aligning fairness metrics with Clinical Utility (Net Benefit) rather than statistical parity prevents "leveling down"
- **Mechanism:** Statistical parity demands equal performance; clinical utility demands model-guided decisions result in better outcomes than standard care
- **Core assumption:** Cost/utility weight for false positives vs. false negatives can be defined specific to clinical context
- **Evidence anchors:** Abstract states "only one clinical utility metric identified"; Section 11 notes existing metrics focus on parity in statistical attributes but not downstream impact on patient outcomes
- **Break condition:** If intervention associated with prediction has negligible cost or risk, strict statistical parity might be sufficient

## Foundational Learning

**Concept: Calibration vs. Discrimination**
- **Why needed here:** The paper critiques relying solely on discrimination (AUROC); a model can perfectly rank patients yet assign systematically wrong risk scores to minority groups
- **Quick check question:** Can a model have an AUROC of 0.99 for both Group A and Group B, yet predict 90% risk for Group A patients who actually have only 10% risk?

**Concept: Impossibility Theorem (Fairness Trade-offs)**
- **Why needed here:** The review notes metrics often conflict; when prevalence differs between groups, you generally cannot satisfy Calibration, Equalized Odds, and Predictive Parity simultaneously
- **Quick check question:** If Disease prevalence is 10% in Group A and 40% in Group B, can a model have equal False Positive Rates and equal Positive Predictive Values for both groups?

**Concept: Intersectionality**
- **Why needed here:** The paper identifies this as a major gap; evaluating "Race" and "Sex" independently misses unique risks at the intersection
- **Quick check question:** Why might a model appear fair when audited by Race alone and by Sex alone, but fail specific subgroups at the intersection of these attributes?

## Architecture Onboarding

**Component map:**
- Input: Predictions ($\hat{p}$), Outcomes ($y$), Sensitive Attributes ($A$)
- Taxonomy Router: Logic to select metric class (Performance-dependent vs Independent)
- Probability Layer: Calculates AUROC Parity, Calibration Parity (Invariant to threshold)
- Threshold Layer: Calculates Sensitivity/Specificity Parity (Dependent on $\tau$)
- Utility Layer: Calculates Subgroup Net Benefit (Requires cost weights)

**Critical path:**
1. Validate Labels: Determine if ground truth is biased (selects Bias-Preserving vs Bias-Transforming approach)
2. Probability Audit: Compute Calibration plots and AUROC per subgroup (Primary check)
3. Utility Audit: If thresholds exist, compute Net Benefit per subgroup

**Design tradeoffs:**
- Granularity vs. Variance: Analyzing many small intersectional subgroups increases fairness granularity but reduces statistical power
- Parity vs. Minimum Floor: Aiming for strict parity vs. ensuring a minimum performance floor

**Failure signatures:**
- "Fairness Wash": Reporting only "Accuracy Parity" in a highly imbalanced dataset where accuracy is misleading
- Hidden Stratification: High global performance masking 0% performance on a minority intersectional subgroup

**First 3 experiments:**
1. Calibration Stratification: Plot calibration curves for each sensitive attribute group; differing slopes/intercepts indicate unfair risk assignment
2. AUROC Disparity Analysis: Calculate AUROC for all groups; gaps exceeding pre-defined $\delta$ (e.g., 0.05) flag for bias mitigation
3. Net Benefit Simulation: At clinically relevant threshold (e.g., 10% risk), calculate "Net Benefit" for each subgroup to see which derives most clinical value

## Open Questions the Paper Calls Out

**Open Question 1**
- **Question:** How can fairness assessments be systematically integrated with clinical utility metrics, such as net benefit, to align with patient outcomes?
- **Basis:** The authors state there is "little work integrating fairness assessments with clinical utility-related metrics," identifying this as a priority for future research
- **Why unresolved:** Only one clinical utility metric identified; most existing metrics focus on statistical parity rather than downstream impact on clinical decision-making
- **What evidence would resolve it:** Development and validation of new fairness metrics derived from decision curve analysis or net benefit frameworks that correlate with improved patient outcomes

**Open Question 2**
- **Question:** What are the required sample sizes and uncertainty quantification methods for robust fairness evaluation?
- **Basis:** The paper explicitly lists "what are the sample size requirements to evaluate fairness" and lack of confidence intervals as critical gaps
- **Why unresolved:** Subgroups defined by sensitive attributes often have limited sample sizes, making point estimates unreliable without uncertainty measures
- **What evidence would resolve it:** Simulation studies establishing minimum sample sizes for specific metrics or derivation of statistical uncertainty bounds for fairness estimates

**Open Question 3**
- **Question:** How can fairness metrics account for intersectionality effectively when intersectional subgroup sizes are too small for reliable analysis?
- **Basis:** The authors note that considering intersections is a gap, but operationalizing this is "difficult due to small intersectional subgroup sizes even in large datasets"
- **Why unresolved:** Standard metrics often treat sensitive attributes independently or fail when data is too sparse to support stable calculations for overlapping groups
- **What evidence would resolve it:** New methodological approaches or modified metrics that can reliably detect bias in sparse, multi-dimensional subgroups without resulting in overfitting or unstable estimates

## Limitations

- Single-reviewer extraction process introduces potential classification errors
- Clinical validation status of individual metrics remains largely unknown
- Taxonomy may not capture all emerging fairness concepts

## Confidence

- **High Confidence:** The taxonomy classification system (performance-dependent vs. performance-independent, probability vs. threshold-based) is well-supported by the literature and represents a sound conceptual framework
- **Medium Confidence:** The identified gaps in clinical utility metrics and intersectionality analysis are likely accurate but may underestimate the scope of work addressing these issues
- **Low Confidence:** The critical appraisal of individual metrics' clinical validation status may contain errors due to the single-reviewer approach and limited documentation of validation in source papers

## Next Checks

1. **Clinical Utility Validation:** Replicate the Net Benefit calculations for at least two published clinical predictive models to verify the claim that only one clinical utility metric exists in the literature
2. **Threshold Sensitivity Analysis:** Select three threshold-dependent metrics and demonstrate how their values change across a clinically relevant range of decision thresholds
3. **Intersectionality Gap Analysis:** Review 10 recent clinical AI fairness papers to quantify the actual prevalence of intersectional analysis versus single-attribute analysis