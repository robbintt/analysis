---
ver: rpa2
title: 'Zero Data Retention in LLM-based Enterprise AI Assistants: A Comparative Study
  of Market Leading Agentic AI Products'
arxiv_id: '2510.11558'
source_url: https://arxiv.org/abs/2510.11558
tags:
- data
- retention
- salesforce
- microsoft
- zero
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares zero data retention implementations in two
  leading enterprise AI assistants, Salesforce AgentForce and Microsoft Copilot. The
  study defines zero data retention as processing data only in memory without persistent
  storage, and analyzes architectural designs, policy commitments, security mechanisms,
  and usability trade-offs of both systems.
---

# Zero Data Retention in LLM-based Enterprise AI Assistants: A Comparative Study of Market Leading Agentic AI Products

## Quick Facts
- arXiv ID: 2510.11558
- Source URL: https://arxiv.org/abs/2510.11558
- Reference count: 11
- Primary result: Comparative analysis of zero data retention implementations in Salesforce AgentForce and Microsoft Copilot, highlighting architectural designs, policy commitments, and usability tradeoffs

## Executive Summary
This paper compares zero data retention implementations in two leading enterprise AI assistants, Salesforce AgentForce and Microsoft Copilot. The study defines zero data retention as processing data only in memory without persistent storage, and analyzes architectural designs, policy commitments, security mechanisms, and usability trade-offs of both systems. Salesforce employs an Einstein Trust Layer for preprocessing and grounding CRM data with strict masking, while Microsoft integrates OpenAI models via Azure with content filtering and tenant isolation. Both achieve near-zero retention but require client-side context management for multi-turn conversations, introducing potential latency. The analysis also evaluates other providers (Anthropic, Google, DeepSeek), noting varying compliance and privacy controls. The paper concludes that while both systems balance functionality and privacy, careful configuration and periodic audits are essential to maintain zero-retention compliance in regulated industries.

## Method Summary
The study employs literature review and architectural analysis of Salesforce and Microsoft documentation, compliance frameworks, and privacy policies. The authors explicitly state that empirical testing was not conducted, noting that future work could complement this design-oriented approach with experimental validation. The analysis focuses on publicly available documentation from Salesforce, Microsoft, Anthropic, Google, and DeepSeek, examining architectural patterns, data flow designs, and stated retention policies.

## Key Results
- Both Salesforce AgentForce and Microsoft Copilot achieve near-zero data retention through stateless inference processing and preprocessing trust layers
- Salesforce's Einstein Trust Layer performs data masking and prompt defense before external LLM processing, while Microsoft uses Azure OpenAI Service with content filtering and tenant isolation
- Multi-turn conversation support requires client-side context management in both systems, introducing latency trade-offs of 200-500ms (AgentForce) and 100-300ms (Copilot) overhead

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Stateless inference processing reduces data persistence risk by processing requests independently without storing conversation state.
- Mechanism: Each request is processed in memory and flushed after response generation. No conversation history persists on the model side; context for multi-turn interactions must be managed client-side and resent with each request.
- Core assumption: In-memory processing without disk persistence is sufficient to prevent unauthorized data recovery; transient memory cannot be externally accessed during processing.
- Evidence anchors:
  - [abstract] "processing data only in memory without persistent storage"
  - [section 3] "ideal (R(S) = 0) is achieved through stateless inference, where each request is processed independently"
  - [corpus] Weak direct evidence; neighbor papers focus on business process automation rather than ZDR architectural patterns.
- Break condition: If any intermediate logging, caching, or monitoring systems write to persistent storage, R(S) > 0 and ZDR is compromised.

### Mechanism 2
- Claim: Pre-processing trust layers with data masking prevent sensitive information from reaching external LLM providers.
- Mechanism: Before prompts reach the LLM, a middleware layer (e.g., Einstein Trust Layer) detects PII using regex patterns and metadata-driven classification, replaces sensitive values with tokens, maintains a temporary mapping for demasking, and only sends masked prompts externally.
- Core assumption: Pattern-based and metadata-based detection reliably identifies sensitive data; masking algorithms are not reversible by external parties.
- Evidence anchors:
  - [section 5.1.1] "Prior to external LLM processing, the Trust Layer performs data masking, substituting sensitive items (e.g., Social Security numbers) with tokens via regex and metadata-driven detection"
  - [section 5.1.1] "Einstein Trust Layer temporarily saves the mapping between the original entities and their corresponding placeholders"
  - [corpus] No directly comparable masking architectures in neighbor papers.
- Break condition: If masking fails to detect novel PII formats, or if mapping tables are logged/cached persistently, sensitive data may leak.

### Mechanism 3
- Claim: Tenant isolation within provider infrastructure prevents cross-customer data exposure.
- Mechanism: Enterprise deployments use logical and physical boundaries (private endpoints, regional deployment) to ensure customer data never co-mingles. Azure OpenAI models run on Microsoft infrastructure, not OpenAI's public API, with customer-managed keys providing additional isolation.
- Core assumption: Tenant boundary enforcement is technically robust; no side-channel attacks or infrastructure-level leaks exist.
- Evidence anchors:
  - [section 5.2.1] "All request and response interactions are only done in isolation within tenant boundaries, and as a design, completions or responses are neither stored nor used for training"
  - [section 5.2.1] "Private endpoints limit operations to regions specified by the customer"
  - [corpus] Limited; neighbor papers on ERP security touch on isolation but not LLM-specific tenant boundaries.
- Break condition: If multi-tenant infrastructure has configuration errors or shared resources leak data across boundaries, isolation fails.

## Foundational Learning

- Concept: **Zero Data Retention (ZDR)**
  - Why needed here: The paper's core subject; understanding what "zero" means (ephemeral in-memory processing, no training use, no persistent logs) is prerequisite to evaluating any implementation.
  - Quick check question: If a provider retains prompts for 30 days for abuse monitoring but does not use them for training, does this qualify as ZDR under the paper's definition?

- Concept: **Stateless vs. Stateful Processing**
  - Why needed here: Both architectures achieve ZDR through stateless inference, which forces client-side context management for multi-turn conversations—a key usability tradeoff.
  - Quick check question: In a stateless architecture, where must conversation history be stored to enable multi-turn dialogue?

- Concept: **Trust Layer Middleware Pattern**
  - Why needed here: Salesforce's Einstein Trust Layer is the canonical example of a preprocessing middleware that enforces policies (masking, toxicity detection, auditing) before and after LLM calls.
  - Quick check question: What three functions does the Einstein Trust Layer perform before a prompt reaches the external LLM?

## Architecture Onboarding

- Component map:
  - **Salesforce AgentForce**: Prompt → Einstein Trust Layer (secure data retrieval, grounding, masking, prompt defense) → LLM Gateway (TLS encrypted) → External LLM (OpenAI, etc.) → Response Journey (toxicity detection, demasking, citations) → CRM
  - **Microsoft Copilot**: User request (Entra ID auth) → Microsoft Graph (data retrieval) → Azure OpenAI Service (inference layer with content filtering, private endpoints) → Response with citations → Microsoft 365 apps
  - **Shared pattern**: Client-side/pre-processing → Secure gateway → Stateless LLM inference → Post-processing → Client

- Critical path:
  1. User authentication (Entra ID / Salesforce permissions)
  2. Data retrieval and grounding (CRM or Graph APIs, respecting RBAC)
  3. PII detection and masking (trust layer only)
  4. Stateless inference at LLM (no retention by contract/Architecture)
  5. Response filtering and demasking
  6. Optional audit logging (customer-controlled, not LLM-side)

- Design tradeoffs:
  - Latency vs. privacy: Trust layer preprocessing adds 200-500ms (Salesforce) or 100-300ms (Microsoft grounding lag)
  - Ecosystem lock-in: AgentForce requires CRM dependency; Copilot requires Azure/Microsoft 365
  - Auditability vs. ephemerality: Both allow customer-side audit trails but these must be explicitly configured and retained by the enterprise, not the LLM provider
  - Multi-turn UX: Stateless design requires client to resend full context each turn, increasing payload size and latency

- Failure signatures:
  - Masking gaps: Novel PII formats not caught by regex/metadata detection
  - Configuration drift: Default settings (e.g., 30-day abuse monitoring logs in Azure OpenAI) not overridden for ZDR
  - Cross-region processing: "Global" deployment types may process data outside compliance regions
  - Feature-level exceptions: Google's "Grounding with Google Search" retains data for 30 days with no opt-out, breaking ZDR

- First 3 experiments:
  1. **Verify masking coverage**: Send test prompts with known PII patterns (SSNs, emails, custom sensitive fields) through the trust layer and inspect what reaches the LLM endpoint (via audit logs or gateway telemetry). Confirm tokenization.
  2. **Validate stateless behavior**: Initiate a multi-turn conversation, then start a new session and verify no prior context is retained by the model without explicit client resend. Measure latency overhead.
  3. **Audit retention configuration**: Review provider configuration dashboards (Azure OpenAI, Salesforce Data Cloud) to confirm: (a) abuse monitoring is in real-time mode without persistent logs, (b) no fine-tuning on customer data is enabled, (c) audit data retention periods match enterprise policy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do Salesforce AgentForce and Microsoft Copilot perform under empirical testing regarding actual data artifacts and latency compared to their architectural specifications?
- Basis in paper: [explicit] "Empirical testing in the future could complement this design-oriented approach."
- Why unresolved: The current study relies on literature review and architectural analysis rather than live data or controlled experiments.
- What evidence would resolve it: Experimental results measuring actual log files, cache persistence, and response times in real-time, multi-turn enterprise scenarios.

### Open Question 2
- Question: What specific client-side context management strategies effectively mitigate the latency trade-offs (e.g., 200–500ms overhead) inherent in stateless zero data retention architectures?
- Basis in paper: [inferred] The paper notes usability trade-offs where "client-side context management" is needed but may introduce latency and complexity in multi-turn dialogues.
- Why unresolved: The analysis identifies the latency issue as a necessary cost of ZDR but does not explore technical solutions to minimize it.
- What evidence would resolve it: Benchmarks of multi-turn conversation performance using various client-side caching or grounding protocols compared to standard stateless processing.

### Open Question 3
- Question: How can enterprises technically verify "absolute zero data retention" when relying on third-party LLM providers (like OpenAI in AgentForce) where enforcement is primarily contractual?
- Basis in paper: [inferred] The conclusion states that "absolute zero data retention... remains an ongoing goal" and notes that Salesforce relies on agreements with external partners where dependency risks exist.
- Why unresolved: Current architectures often depend on policy compliance and contracts with external model providers rather than provable, technical isolation for all components.
- What evidence would resolve it: Development of independent auditing frameworks or trusted execution environments that validate data ephemerality across third-party model boundaries.

## Limitations
- The study relies entirely on literature review without empirical testing, making all claims dependent on vendor documentation accuracy
- No independent verification of ZDR compliance claims or actual retention risk quantification was performed
- The analysis cannot confirm whether stated architectural patterns are actually implemented as described in production environments

## Confidence
- **High confidence**: Architectural patterns (stateless processing, trust layer middleware, tenant isolation) are well-documented and technically sound based on vendor specifications
- **Medium confidence**: ZDR definitions and compliance requirements are accurately represented from public sources, though practical implementation may vary
- **Low confidence**: Retention risk quantification and specific latency figures lack methodological transparency and empirical validation

## Next Checks
1. **Verify Trust Layer Effectiveness**: Conduct controlled testing by sending prompts containing various PII patterns (including novel formats not covered by standard regex) through Salesforce's Einstein Trust Layer. Capture and analyze the actual content that reaches the LLM endpoint to confirm complete masking coverage.
2. **Validate Stateless Architecture**: Test multi-turn conversation capabilities by initiating dialogue sequences, then starting new sessions to confirm no prior context persists on the model side. Measure the actual latency overhead of client-side context resending versus expected 200-500ms figures.
3. **Audit Configuration Compliance**: Review and document the actual configuration settings in production environments for both Salesforce AgentForce and Microsoft Copilot, specifically checking: (a) Azure OpenAI abuse monitoring retention mode, (b) Salesforce Data Cloud audit log retention periods, (c) grounding data retention settings for any search-integrated features.