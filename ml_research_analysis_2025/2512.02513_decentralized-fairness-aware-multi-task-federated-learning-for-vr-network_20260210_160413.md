---
ver: rpa2
title: Decentralized Fairness Aware Multi Task Federated Learning for VR Network
arxiv_id: '2512.02513'
source_url: https://arxiv.org/abs/2512.02513
tags:
- caching
- algorithm
- learning
- each
- proposed
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a decentralized multi-task fair federated
  learning (DMTFL) approach for caching VR content in wireless networks. The method
  personalizes caching strategies at each base station by learning individual models
  based on local data distributions, while ensuring fairness across all users and
  base stations.
---

# Decentralized Fairness Aware Multi Task Federated Learning for VR Network

## Quick Facts
- arXiv ID: 2512.02513
- Source URL: https://arxiv.org/abs/2512.02513
- Reference count: 15
- Decentralized multi-task FL for VR caching achieves 55%+ higher average cache hit rates with improved fairness

## Executive Summary
This paper introduces a decentralized multi-task fair federated learning (DMTFL) approach for caching VR content in wireless networks. The method personalizes caching strategies at each base station by learning individual models based on local data distributions, while ensuring fairness across all users and base stations. By leveraging Rademacher complexity and probably approximately correct (PAC) bounds, the approach provides theoretical guarantees on performance. Simulation results using a realistic VR head-tracking dataset demonstrate that the proposed DMTFL algorithm significantly outperforms baseline methods in terms of average cache hit rate (at least 55% higher) and minimum per-base-station cache hit rate, indicating both improved efficiency and fairness in VR content delivery.

## Method Summary
The DMTFL approach addresses non-IID data distributions across base stations by learning personalized caching models while maintaining fairness through worst-case distribution optimization. Each base station maintains its own caching strategy vector optimized via local empirical loss while incorporating weighted information from neighboring BSs. The algorithm uses discrepancy metrics to measure distributional differences between BSs and assigns weights accordingly. Theoretical guarantees are provided through Rademacher complexity bounds, ensuring generalization performance. The decentralized architecture eliminates the need for a central server, making it suitable for wireless network environments.

## Key Results
- DMTFL achieves at least 55% higher average cache hit rate compared to baseline methods
- The algorithm significantly improves minimum per-BS cache hit rate, demonstrating enhanced fairness
- Theoretical PAC bounds provide provable guarantees on performance generalization
- Personalization effectively captures statistical heterogeneity across BSs and users

## Why This Works (Mechanism)

### Mechanism 1: Personalized Multi-Task Model Divergence
- Claim: Learning separate caching models per base station improves hit rates under non-IID user request patterns compared to single global model aggregation.
- Mechanism: Each BS maintains its own caching strategy vector ϕ_b optimized via local empirical loss while incorporating weighted information from neighboring BSs. The weight assignment α_b,i depends on estimated distributional discrepancy between BS pairs, giving higher weight to BSs with similar data distributions.
- Core assumption: User request distributions differ significantly across BSs (non-IID), and these differences are measurable via loss discrepancy.
- Evidence anchors:
  - [abstract] "single global model fails to capture the statistical heterogeneity across users and BSs"
  - [Section II] "In its standard form such as FedAvg... aggregating these into a single global model can lead to suboptimal performance"
  - [corpus] Related work "Personalized Federated Learning for Cellular VR" addresses similar heterogeneity; corpus evidence is moderate (avg FMR=0.38) with limited citation data.
- Break condition: If user request distributions are approximately IID across BSs, personalization overhead provides no benefit over global averaging.

### Mechanism 2: Agnostic Fairness via Worst-Case Distribution Optimization
- Claim: Optimizing for the worst-case mixture of BS distributions ensures equitable cache hit rates across all base stations, preventing bias toward data-rich locations.
- Mechanism: The optimization uses sup_{w∈Λ} over mixture weights rather than fixing a target distribution. The PAC bound (Theorem 1) explicitly penalizes high discrepancy between BS pairs through the term Σ_w_b·α_b,i·v_b,i, forcing the algorithm to reduce performance gaps.
- Core assumption: The target distribution at inference time is unknown but lies within the convex hull of observed BS distributions.
- Evidence anchors:
  - [Section II] "we consider an agnostic FL setting, where the caching strategy is optimized for any possible target distribution formed by a mixture of the BS distributions"
  - [Figure 4 description] "minimum per-BS cache hit rate, which serves as a fairness indicator... proposed DMTFL algorithm achieves a significantly higher minimum per-BS hit rate"
  - [corpus] "CoRe-F