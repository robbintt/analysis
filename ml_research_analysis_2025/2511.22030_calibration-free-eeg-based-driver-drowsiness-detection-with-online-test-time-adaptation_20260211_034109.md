---
ver: rpa2
title: Calibration-Free EEG-based Driver Drowsiness Detection with Online Test-Time
  Adaptation
arxiv_id: '2511.22030'
source_url: https://arxiv.org/abs/2511.22030
tags:
- drowsiness
- target
- statistics
- driver
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses driver drowsiness detection using EEG signals,
  which are highly variable across subjects and sessions. To eliminate the need for
  calibration, the authors propose an online test-time adaptation (TTA) framework
  that dynamically adapts to the target subject's EEG distribution during inference.
---

# Calibration-Free EEG-based Driver Drowsiness Detection with Online Test-Time Adaptation

## Quick Facts
- arXiv ID: 2511.22030
- Source URL: https://arxiv.org/abs/2511.22030
- Reference count: 39
- F1-score: 81.73% average across subjects

## Executive Summary
This paper addresses driver drowsiness detection using EEG signals, which are highly variable across subjects and sessions. To eliminate the need for calibration, the authors propose an online test-time adaptation (TTA) framework that dynamically adapts to the target subject's EEG distribution during inference. The core method updates only the learnable parameters in batch normalization layers while preserving pretrained normalization statistics, and incorporates a memory bank to manage streaming EEG segments based on reliability and persistence. Prototype learning is also used to ensure robust predictions against distribution shifts. Evaluated on a sustained-attention driving dataset, the proposed method achieved an average F1-score of 81.73%, outperforming all baseline methods by up to 11.73%.

## Method Summary
The method uses a pretrained EEGNet8,2 backbone with fixed batch normalization statistics and trainable affine parameters. During inference, streaming EEG segments are processed through the network, with samples stored in a memory bank that maintains reliability through energy-score and persistence-based filtering. The model adapts by updating only the batch normalization affine parameters using a loss combining entropy minimization and energy-bounded regularization. Prototype learning with exponential moving average updates provides stable predictions against distribution shifts. The system operates without target subject calibration data, making it suitable for real-world deployment.

## Key Results
- Achieved 81.73% average F1-score across subjects in leave-one-subject-out validation
- Outperformed baseline methods by up to 11.73% F1-score
- Demonstrated effectiveness of calibration-free TTA with fixed BN statistics
- Showed memory bank and prototype learning contributions through ablation studies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fixing pretrained batch normalization statistics while adapting only affine parameters stabilizes online EEG adaptation under non-i.i.d. streaming conditions.
- Mechanism: The BN layer computes: `BN(z; μ_S, σ²_S) = γ·(z - μ_S)/√(σ²_S + ε) + β`. By preserving source statistics (μ_S, σ²_S) trained on multiple subjects, the model avoids collapse caused by estimating statistics from single streaming samples or temporally correlated batches. Only γ and β are updated via gradient descent, allowing domain-specific affine transformations without corrupting normalization.
- Core assumption: Source statistics capture a sufficiently general EEG distribution that, once affine-adjusted, can accommodate target subject variability without recomputing normalization moments.
- Evidence anchors: [abstract] "Our proposed method updates the learnable parameters in batch normalization (BN) layers, while preserving pretrained normalization statistics"; [section III-B] "To address this issue, we update only the learnable parameters within the BN layers using input test samples, while preserving the pretrained statistics from the multiple source subjects"; [Table III] The configuration with fixed statistics + updated parameters achieved F1=81.73%, while tracking statistics alone achieved only 40.72%

### Mechanism 2
- Claim: Energy-score-weighted memory bank filtering maintains reliable pseudo-labels for adaptation by penalizing both low-confidence and stale samples.
- Mechanism: Removal score `S(x) = -E(x)/A` combines negative energy score (confidence proxy) and persistence time A (staleness penalty). Lower energy indicates in-distribution samples; higher persistence discounts outdated entries. When the bank is full, the highest S(x) sample is discarded, preserving samples that are both confident and recent.
- Core assumption: Energy scores correlate with label reliability, and temporal drift in EEG distributions makes older samples progressively less representative of current states.
- Evidence anchors: [abstract] "selecting samples based on their reliability determined by negative energy scores and persistence time"; [section III-D] "Our proposed removal criterion selectively removes overconfident and outdated samples, allowing the memory bank to align with the current distribution of the streaming data"; [corpus] Limited direct corpus evidence on energy-based memory filtering for EEG-TTA; mechanism is primarily validated within this work

### Mechanism 3
- Claim: Prototype learning with exponential moving average (EMA) updates provides stable predictions under distribution shift by anchoring classification to evolving class centroids.
- Mechanism: Pseudo-prototypes are computed from high-confidence memory samples via `P̂_k = mean(z_i)` for class k. Final prototypes use EMA: `P_k = α·P_k + (1-α)·P̂_k`. Predictions use dot-product similarity between input features and prototypes rather than the original classifier head, decoupling predictions from potentially misaligned linear weights.
- Core assumption: Feature representations after BN adaptation are sufficiently discriminative that averaging within pseudo-class clusters yields meaningful prototypes.
- Evidence anchors: [abstract] "prototype learning to ensure robust predictions against distribution shifts over time"; [section III-E] "We combine the PL with a memory bank that captures the current target distributions"; [Table II] Removing PL dropped F1 from 81.73% to 75.84%, confirming its contribution

## Foundational Learning

- Concept: Test-Time Adaptation (TTA)
  - Why needed here: The core framework; differs from domain adaptation (requires target data during training) and domain generalization (no adaptation at all). TTA enables calibration-free deployment by adapting during inference.
  - Quick check question: Given unlabeled streaming EEG from a new driver, how would you distinguish between DA, DG, and TTA approaches?

- Concept: Batch Normalization Decomposition (statistics vs. affine parameters)
  - Why needed here: Critical design choice. Statistics (μ, σ²) reflect data distribution; affine parameters (γ, β) apply scale/shift. The paper's key insight is separating these two for stable EEG adaptation.
  - Quick check question: If you update BN statistics using only batch size of 1, what failure mode occurs and why?

- Concept: Energy-Based OOD Detection
  - Why needed here: Energy scores `E(x) = -log(Σ exp(f_k(x)/τ))` provide confidence estimates without requiring softmax calibration. Used for memory bank filtering.
  - Quick check question: How does the energy score differ from entropy as a confidence measure, and when would you prefer one over the other?

## Architecture Onboarding

- Component map: EEG segment -> EEGNet8,2 backbone -> Fixed BN statistics + trainable affine parameters -> Memory bank (capacity=16) -> Energy/persistence filtering -> Loss computation (entropy + energy) -> BN affine parameter updates -> Prototype updates (EMA) -> Prediction via prototype similarity

- Critical path: 1. Receive streaming EEG segment x_t; 2. Forward pass through backbone with fixed BN statistics; 3. Compute energy score; if memory full, evict sample with highest S(x) = -E(x)/persistence; 4. Insert x_t into memory bank; 5. Backward pass: update only γ, β via L_total on memory samples; 6. Update prototypes using pseudo-labels from adapted model; 7. Predict via prototype similarity

- Design tradeoffs: Memory capacity (16): Larger banks hold more distribution information but increase latency and may retain stale samples; EMA α (0.9): Higher values slow prototype updates (stable but less responsive); lower values adapt faster but risk instability; Energy margins (m_in=-15, m_out=-7): Control stringency of in/out-distribution classification for prototype inclusion

- Failure signatures: F1 → 0% or near-random: BN statistics being estimated from batch size 1 (model collapse); Performance degrades over time: Memory bank accumulating corrupted samples; check removal criterion; High variance across subjects: Source pretraining insufficient; consider multi-source augmentation

- First 3 experiments: 1. BN Configuration Ablation: Compare three settings on a single held-out subject: (a) update statistics only, (b) track statistics + update affine, (c) fixed statistics + update affine. Replicate Table III to confirm the paper's finding; 2. Memory Bank Capacity Sweep: Test capacities {8, 16, 32, 64} with and without persistence penalty to isolate the contribution of staleness filtering; 3. Prototype vs. Classifier Head: Replace prototype-based prediction with direct classifier output using the same adapted features. Measure the gap to quantify prototype learning's contribution

## Open Questions the Paper Calls Out

- How does the proposed TTA framework perform in real-world driving environments with vehicle vibrations, changing weather conditions, and external noise sources? Basis: "this study was conducted in a controlled simulator, which lacks real-world factors such as vehicle vibrations, changing weather, and external noise. Future work should validate the framework in real-world driving environments"

- Would regression-based or ordinal classification approaches using continuous drowsiness labels (e.g., Karolinska Sleepiness Scale) improve detection of transitional drowsiness phases compared to the current binary classification? Basis: "drowsiness is inherently a continuous physiological phenomenon, this simplification may not fully capture transitional phases. Future studies could utilize fine-grained labels"

- What is the optimal EEG segment window size that balances detection latency against information content for the proposed TTA framework? Basis: "we employed a fixed 3-second time window for EEG segments. Future investigations could analyze the impact of varying window sizes"

## Limitations

- Energy score calibration: The paper assumes energy scores reliably indicate sample reliability for memory filtering, but this relationship is not empirically validated across the full subject population. Cross-subject EEG variability may cause systematic energy score biases.

- Generalization beyond sustained-attention driving: The method is validated only on one driving dataset with specific event-triggered labeling. Performance may degrade in naturalistic driving scenarios with continuous drowsiness assessment rather than lane-departure events.

- Memory bank capacity trade-offs: The fixed capacity of 16 samples is justified empirically but not theoretically. Optimal capacity likely varies with subject adaptation rate and drowsiness progression speed.

## Confidence

- High confidence: The calibration-free design principle (eliminating subject-specific calibration) and the core BN decomposition strategy (freezing statistics while adapting affine parameters) are well-supported by ablation studies showing catastrophic failure when statistics are tracked.

- Medium confidence: Prototype learning and memory bank contributions are validated through ablation, but the underlying assumptions about feature discriminability and energy score reliability warrant further investigation.

- Low confidence: The generalizability claim to other EEG-based BCI tasks beyond drowsiness detection is largely extrapolated from the single-task results.

## Next Checks

1. Energy score reliability validation: Compute energy scores on held-out calibration data from each subject and compare their distributions across alert/drowsy states to verify the assumption that lower energy indicates higher reliability.

2. Memory bank capacity sensitivity: Systematically vary memory capacity from 4 to 128 samples while measuring F1-score and adaptation latency to identify optimal capacity as a function of subject adaptation speed.

3. Cross-task generalization test: Apply the trained drowsiness detection model (without retraining) to a different EEG BCI task (e.g., motor imagery) and measure performance degradation to assess the method's broader applicability.