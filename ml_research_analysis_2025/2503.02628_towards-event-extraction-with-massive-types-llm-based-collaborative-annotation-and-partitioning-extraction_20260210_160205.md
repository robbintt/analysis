---
ver: rpa2
title: 'Towards Event Extraction with Massive Types: LLM-based Collaborative Annotation
  and Partitioning Extraction'
arxiv_id: '2503.02628'
source_url: https://arxiv.org/abs/2503.02628
tags:
- event
- annotation
- types
- type
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an LLM-based framework for event extraction
  (EE) with massive types, addressing annotation inefficiency and context-length limitations.
  The proposed collaborative annotation method leverages multiple LLMs to refine distant
  supervision annotations through filtering, type refinement, and argument annotation,
  producing the EEMT dataset (200k samples, 3,465 event types, 6,297 roles).
---

# Towards Event Extraction with Massive Types: LLM-based Collaborative Annotation and Partitioning Extraction

## Quick Facts
- **arXiv ID**: 2503.02628
- **Source URL**: https://arxiv.org/abs/2503.02628
- **Reference count**: 40
- **Primary result**: LLM-PEE framework achieves 5.4% improvement in event detection and 6.1% in argument extraction in supervised settings, and up to 12.9% improvement over mainstream LLMs in zero-shot settings

## Executive Summary
This paper introduces a comprehensive framework for event extraction with massive event type schemas, addressing annotation inefficiency and context-length limitations. The authors develop LLM-based collaborative annotation to create the EEMT dataset (200k samples, 3,465 event types, 6,297 roles) through multi-LLM voting and arbitration. To handle the massive type space, they propose LLM-PEE with similarity-based type recall and partitioning prompting strategies. The framework demonstrates state-of-the-art performance on the EEMT dataset and shows strong zero-shot generalization capabilities.

## Method Summary
The method consists of two main components: (1) LLM-based collaborative annotation pipeline that uses three LLMs (DeepSeek-V3, Qwen-Plus, GPT-4o-mini) with voting and arbitration to refine distant supervision annotations from GLEN, producing the EEMT dataset; and (2) LLM-PEE extraction framework that employs similarity-based type recall using ColBERT to reduce candidate event types, then partitions them using the "Level" strategy for efficient LLM processing. The extraction model is built on LLaMA2-7B with LoRA fine-tuning, using a pipelined approach for Event Detection and Argument Extraction.

## Key Results
- LLM-PEE achieves 5.4% improvement in Trigger Classification F1 and 6.1% improvement in Argument Extraction F1 over state-of-the-art methods in supervised settings
- Zero-shot generalization shows up to 12.9% improvement over mainstream LLMs when evaluated on ACE 2005
- Collaborative annotation produces high-quality data with F1 scores above 85% for all steps, reaching 96.2% for event type refinement
- Similarity-based type recall is critical, with ablation showing 18.3% drop in Trigger Classification F1 without it

## Why This Works (Mechanism)

### Mechanism 1
Multi-LLM collaborative annotation with voting improves annotation quality over single-LLM annotation by reducing individual model biases and improving span consistency. Three LLMs independently annotate triggers, types, and arguments, then a voting phase consolidates results with majority agreement validating annotations. GPT-4o arbitrates disagreements, and offset alignment rules standardize span boundaries before voting.

### Mechanism 2
Similarity-based event type recall reduces the candidate event type space from thousands to a manageable subset (k=15), enabling efficient LLM extraction without exceeding context limits. ColBERT encodes sentences and event type names separately, ranking event types by similarity scores to retrieve top-k candidates before prompting LLMs.

### Mechanism 3
Partitioning recalled event types into smaller groups with the "Level" strategy improves LLM extraction by reducing prompt complexity and grouping hard negative types together. After recalling k event types, they are divided into N partitions with the Level strategy sorting types by confidence score before partitioning, putting similar-confidence types together.

## Foundational Learning

- **Event Extraction (EE) subtasks—Event Detection (ED) and Event Argument Extraction (EAE)**: The framework separates these subtasks with ED identifying triggers and types, and EAE identifying arguments and roles. Understanding this decomposition is prerequisite to following the two-stage extraction pipeline.

- **Distant supervision for EE**: The annotation method starts from distant supervision (Propbank + Wikidata mappings) rather than from scratch. Understanding its limitations (unreasonable triggers, coarse types, missing arguments) explains why LLM refinement is needed.

- **Multi-agent/voting ensemble methods**: The collaborative annotation relies on voting across multiple LLMs. Understanding majority voting, tie-breaking, and arbitration (via GPT-4o) clarifies how disagreement is resolved.

## Architecture Onboarding

- **Component map**: Distant supervision → Event Trigger Filtering (LLMs + voting) → Event Type Refinement (LLMs + voting) → Event Argument Annotation (LLMs + offset alignment + voting + GPT-4o arbitration) → EEMT dataset (200k samples, 3,465 event types, 6,297 roles) → LLM-PEE extraction pipeline (Sentence → Similarity-based type recall (ColBERT, k=15) → Type partitioning (Level strategy, N partitions) → LLM-based ED → LLM-based EAE)

- **Critical path**: High-quality collaborative annotation depends critically on voting threshold and offset alignment rules—errors here propagate to all downstream training. The similarity-based recall (k=15) and partitioning strategy (Level, N=2) determine whether the correct event type is even presented to the LLM.

- **Design tradeoffs**: k=15 balances recall vs. partitioning burden; N=2 partitions reduce prompt length while maintaining context; majority voting (≥2 of 3 LLMs) balances robustness vs. throughput; LLM selection balances cost/accuracy.

- **Failure signatures**: Trigger Classification F1 drops sharply (18.3%) without type recall; over-generation by mainstream LLMs without fine-tuning; argument span inconsistency across LLMs.

- **First 3 experiments**: 1) Reproduce annotation quality check on held-out GLEN subset to measure F1 for trigger filtering, type refinement, and argument annotation; 2) Ablate partition strategy comparing Random, Average, and Level partitioning on ED Trigger Classification; 3) Zero-shot generalization test running LLM-PEE on ACE 2005 without fine-tuning.

## Open Questions the Paper Calls Out

### Open Question 1
How can event definitions or sampling strategies be optimized to improve the model's understanding of the hierarchical granularity of massive event types? The authors identify the lack of sufficient distinction in event hierarchy as a limitation and propose exploring better definitions or positive/negative sample strategies.

### Open Question 2
Can the LLM-PEE framework be adapted to perform efficient end-to-end event extraction without separating Event Detection (ED) and Argument Extraction (EAE)? The limitations section states the method currently divides EE into two sub-tasks and expresses intent to pursue direct end-to-end extraction.

### Open Question 3
How does the LLM-PEE method perform when applied to document-level event extraction, given its current reliance on sentence-level annotations? The authors note that the EEMT dataset is sentence-level only and express desire to annotate document-level data.

## Limitations
- Dataset dependency: EEMT quality hinges on collaborative annotation performance with F1 scores above 85% for all steps
- Generalizability concerns: Partitioning strategy's effectiveness assumes confidence scores correlate with semantic difficulty
- Limited zero-shot generalization: Performance gains on ACE 2005 (2.6-3.9%) suggest limited transfer when schema differences exist

## Confidence
- **High confidence**: Dataset construction methodology and core experimental results (supervised settings show consistent 5.4-6.1% improvements)
- **Medium confidence**: Zero-shot generalization claims (based on single dataset ACE 2005, limited to 5 event types)
- **Medium confidence**: Partitioning strategy superiority (no ablation against alternative sorting methods)

## Next Checks
1. **Replicate annotation pipeline degradation**: Run collaborative annotation on 1,000 GLEN samples and measure step-by-step F1 degradation to identify whether argument annotation (85.3% F1) is the bottleneck.

2. **Test partitioning sensitivity**: Compare Level strategy against random partitioning with confidence-aware shuffling on ED Trigger Classification to verify that confidence sorting provides measurable benefit.

3. **Cross-domain generalization**: Evaluate LLM-PEE on MAVEN or RAMS datasets without fine-tuning to measure schema alignment costs and performance drop relative to supervised settings.