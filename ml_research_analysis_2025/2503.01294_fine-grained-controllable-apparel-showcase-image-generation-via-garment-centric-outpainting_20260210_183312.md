---
ver: rpa2
title: Fine-Grained Controllable Apparel Showcase Image Generation via Garment-Centric
  Outpainting
arxiv_id: '2503.01294'
source_url: https://arxiv.org/abs/2503.01294
tags:
- image
- garment
- showcase
- images
- pose
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of fine-grained controllable
  apparel showcase image generation. The key limitation of existing methods is their
  inability to faithfully preserve garment details and lack of precise control over
  the synthesized model's appearance, such as hairstyle and skin color.
---

# Fine-Grained Controllable Apparel Showcase Image Generation via Garment-Centric Outpainting

## Quick Facts
- **arXiv ID:** 2503.01294
- **Source URL:** https://arxiv.org/abs/2503.01294
- **Reference count:** 40
- **Primary result:** Fine-grained controllable apparel showcase image generation with garment-centric outpainting, outperforming SOTA methods in realism and controllability

## Executive Summary
This paper introduces Garment-Centric Outpainting (GCO), a two-stage framework for fine-grained controllable apparel showcase image generation. The key innovation is treating the task as outpainting rather than virtual try-on, preserving garment details by eliminating cloth deformation learning. The framework takes a segmented garment image from a dressed mannequin or person and generates showcase images with precise control over model appearance including hairstyle, skin color, and facial features. Extensive experiments on VITON-HD demonstrate superior performance across multiple metrics including Clo-SSIM (0.97), FID (11.36), and DreamSim (0.09).

## Method Summary
GCO operates in two stages: (1) a garment-adaptive pose predictor generates diverse poses fitting the given garment, and (2) a garment-centric outpainting model generates showcase images conditioned on the garment, predicted poses, text prompts, and facial images. The method employs a multi-scale appearance customization module (MS-ACM) with dual BLIP encoders for global and fine-grained facial attribute control. A lightweight feature fusion operation concatenates conditions efficiently without introducing additional encoders or modules. The framework modifies the input channels and attention mechanisms of a standard Stable Diffusion UNet to accommodate multiple conditioning inputs.

## Key Results
- Outperforms state-of-the-art methods in realism and controllability on VITON-HD dataset
- Achieves Clo-SSIM of 0.97, FID of 11.36, and DreamSim of 0.09
- User studies show highest preference for face preservation, garment matching, and overall image quality
- Ablation studies confirm effectiveness of pose predictor and lightweight feature fusion over IP-Adapter alternatives

## Why This Works (Mechanism)

### Mechanism 1: Outpainting vs. Wrapping
- **Claim:** Eliminating explicit garment deformation preserves cloth details better than traditional try-on methods.
- **Mechanism:** Treats task as "outpainting" rather than "wrapping" by using garment image already segmented from dressed person/mannequin, avoiding distortion-prone warping steps.
- **Core assumption:** Input garment image is already in "worn" shape; flat, laid-out shirts may fail to infer correct body proportions.
- **Evidence:** Abstract states framework "eliminates the need for learning cloth deformation and ensuring faithful preservation of garment details."

### Mechanism 2: Lightweight Feature Fusion
- **Claim:** Spatial concatenation of conditions enables efficient feature fusion without heavy adapters.
- **Mechanism:** Fuses inputs by concatenating them directly with noisy latent - spatially aligned inputs (garment, pose, mask) concatenated channel-wise; face image concatenated spatially.
- **Core assumption:** Pre-trained UNet's self-attention mechanism is powerful enough to correlate concatenated face pixels with generated face region.
- **Evidence:** Ablation shows replacing with IP-Adapter increases FID (24.34 vs 11.36), suggesting lightweight fusion is unexpectedly more effective.

### Mechanism 3: Multi-Scale Text Control
- **Claim:** Decoupling text prompts into "Showcase" and "Face" descriptions enables fine-grained attribute control.
- **Mechanism:** MS-ACM uses two text encoders - "Showcase BLIP" for global context and "Face BLIP" (fine-tuned on CelebA) for specific facial attributes.
- **Core assumption:** Standard text encoders fail to disentangle specific facial features from global sentence embedding.
- **Evidence:** Visual evidence shows baseline methods ignoring attributes like "red lips" while GCO adheres to them.

## Foundational Learning

- **Latent Diffusion Models (LDMs) & UNet:**
  - **Why needed here:** Entire framework modifies input channels and attention mechanisms of standard Stable Diffusion UNet
  - **Quick check:** Do you understand how to modify input convolution layer of UNet to accept 4+ channels while retaining pre-trained weights?

- **DensePose / Pose Maps:**
  - **Why needed here:** Stage 1 generates pose maps as intermediate representation to guide outpainting process
  - **Quick check:** Can you explain why pose map is necessary for outpainting model to infer scale and position of human body relative to fixed garment?

- **BLIP (Bootstrapping Language-Image Pre-training):**
  - **Why needed here:** MS-ACM relies on BLIP variants to caption images and extract text features
  - **Quick check:** How does fine-tuning BLIP model on face dataset (CelebA) change its output compared to generic BLIP model?

## Architecture Onboarding

- **Component map:** Extract/Segment Garment -> Pose Predictor -> Sample Pose -> (Garment + Pose + Mask) -> Channel Concat -> (Target Image + Face Reference) -> Spatial Concat -> UNet Denoising -> Showcase Image

- **Critical path:** 1) Extract/Segment Garment 2) Pose Predictor generates pose map 3) (Garment + Pose + Mask) channel concatenated 4) (Target Image + Face Reference) spatially concatenated 5) Combined inputs processed by UNet denoising guided by MS-ACM text embeddings

- **Design tradeoffs:**
  - Efficiency vs. Robustness: Lightweight fusion avoids ControlNet/IP-Adapter overhead but relies heavily on base model's self-attention
  - Data Bias: Model limited by VITON-HD dataset (mostly young, thin females)

- **Failure signatures:**
  - Pose-Body Mismatch: Anatomically impossible pose generates distorted limbs
  - Identity Bleed: Incorrect face masking generates floating head artifacts
  - Text Ignoring: Rare or OOV facial attributes fail to render

- **First 3 experiments:**
  1. Sanity Check: Train on single (Garment, Showcase) pair to verify modified input conv layers propagate gradients correctly
  2. Ablation: Compare "Spatial Concatenation" vs. "Channel Concatenation" for face image
  3. Ablation: Run inference with only Showcase BLIP vs. only Face BLIP to visualize control gap

## Open Questions the Paper Calls Out
- Can the framework effectively generalize to flat, unposed garment images rather than relying on inputs from dressed mannequins?
- How robust is the generation pipeline when the garment-adaptive pose predictor produces physically infeasible poses?
- To what extent does training on broader datasets improve generation of diverse ages, body types, and genders?

## Limitations
- Requires "worn" garment silhouette as input, failing on flat catalog-style images
- Dataset bias explicitly acknowledged (mostly young, thin females in VITON-HD)
- Missing training hyperparameters (learning rates, batch sizes, epochs) for faithful reproduction

## Confidence
- **High Confidence:** Outpainting mechanism avoiding garment deformation is well-supported by experimental results
- **Medium Confidence:** Effectiveness of lightweight feature fusion over heavier adapters demonstrated but reasons for superior performance need investigation
- **Medium Confidence:** Fine-grained attribute control via MS-ACM shows strong qualitative results but robustness to conflicting prompts needs validation

## Next Checks
1. **Input Dependency Validation:** Test model with flat, rectangular garment images versus already-worn silhouettes to quantify failure mode threshold
2. **Prompt Conflict Resolution:** Systematically test conflicting text prompts to measure artifact generation and signal suppression
3. **Pose-Anatomy Consistency:** Evaluate Pose Predictor's ability to generate anatomically possible poses for various garment types, measuring pose map quality and subsequent outpainting fidelity