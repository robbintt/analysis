---
ver: rpa2
title: 'Exploring Performance Variations in Finetuned Translators of Ultra-Low Resource
  Languages: Do Linguistic Differences Matter?'
arxiv_id: '2511.22482'
source_url: https://arxiv.org/abs/2511.22482
tags:
- languages
- guarani
- mbya
- nheengatu
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates performance differences in fine-tuned translators
  for two Brazilian Indigenous languages, Guarani Mbya and Nheengatu, despite their
  linguistic similarity. Using the NLLB-200 pre-trained model, translators were fine-tuned
  with comparable datasets (3K vs 7K pairs) in both translation directions.
---

# Exploring Performance Variations in Finetuned Translators of Ultra-Low Resource Languages: Do Linguistic Differences Matter?

## Quick Facts
- arXiv ID: 2511.22482
- Source URL: https://arxiv.org/abs/2511.22482
- Reference count: 24
- Key outcome: Fine-tuned translators for linguistically similar Brazilian Indigenous languages (Guarani Mbya vs Nheengatu) show dramatic performance differences (40.2 vs 66.9 chrF) despite comparable training data sizes

## Executive Summary
This study investigates why fine-tuned machine translation models for two related Brazilian Indigenous languages perform dramatically differently despite similar training data and pre-trained model conditions. Using NLLB-200 as the base model, translators were developed for Guarani Mbya and Nheengatu in both translation directions. Despite linguistic similarity, Nheengatu achieved significantly higher translation quality scores (66.9 chrF) compared to Guarani Mbya (40.2 chrF). The research systematically ruled out model size, pre-trained model limitations, and training data quantity as primary causes, finding that only extreme data undersampling (to 500 pairs) could close the performance gap. The results suggest that structural linguistic differences, particularly in morphology and word order, significantly impact the efficiency of fine-tuning for ultra-low-resource languages.

## Method Summary
The study fine-tuned NLLB-200 (600M and 3.3B parameter versions) on bilingual corpora for Guarani Mbya-Portuguese and Nheengatu-Portuguese translation. Datasets were cleaned, deduplicated, and normalized, with test sets curated to ensure all source materials were represented in training. Training used 57K steps with batch size 16, max sequence length 128, and linear warmup to 1e-4 learning rate. Zero-shot baselines were established before finetuning. The Paraguayan Guarani tokenizer was repurposed for both languages. Experiments included model size comparison (600M vs 3.3B), data downsampling from 6,699 to 500 pairs for Nheengatu, and polynomial regression to estimate scaling relationships between data size and performance.

## Key Results
- Nheengatu achieved 66.9 chrF score vs Guarani Mbya's 40.2 chrF despite similar training data sizes
- Zero-shot performance was nearly identical for both languages (~15-17 chrF), ruling out pre-trained model limitations
- Model size scaling (600M to 3.3B parameters) had minimal impact (<2 chrF difference)
- Extreme downsampling to 500 pairs was required to reduce Nheengatu performance to match Guarani Mbya
- Polynomial regression estimated Guarani Mbya would need ~60,000 pairs to match Nheengatu's performance with 3,319 pairs

## Why This Works (Mechanism)

### Mechanism 1: Cross-Linguistic Transfer via Related Pre-Training Languages
Finetuning effectiveness for ultra-low resource languages appears mediated by structural proximity to languages present in the pre-training corpus. NLLB-200 includes Paraguayan Guarani, a related Tupi-Guarani language, enabling the model to leverage subword representations and transfer patterns from a higher-resource relative. This shared tokenizer and linguistic ancestry provide usable inductive biases for downstream finetuning.

### Mechanism 2: Morphological and Syntactic Complexity Constrains Finetuning Efficiency
Languages with more complex morphology and variable word order may require more data or alternative methods to achieve comparable translation quality under standard finetuning. Guarani Mbya's agglutinative morphology with argument cross-referencing and mixed SVO/SOV word order contrasts with Nheengatu's simplified grammar and predominantly SVO order. Standard transformer finetuning may align more readily with structurally simpler patterns.

### Mechanism 3: Training Data Scaling Yields Diminishing Returns Differentially by Language
Performance gains from additional training data are not uniform across languages. Nheengatu showed steep gains per sample, while Guarani Mbya plateaued earlier. Downsampling experiments showed Nheengatu retained high performance until extreme undersampling, while polynomial regression estimated a 20x efficiency differential in data requirements between languages.

## Foundational Learning

- **Finetuning vs. Zero-Shot Transfer**: Why needed here - The paper contrasts zero-shot NLLB performance (~15-17 chrF, nearly identical for both languages) with post-finetuning gaps (40 vs. 67 chrF), demonstrating that finetuning reveals underlying learnability differences masked at zero-shot. Quick check question: Given a pre-trained multilingual model, when would you expect zero-shot translation to perform comparably across related languages, but finetuning to diverge?

- **Agglutinative Morphology**: Why needed here - Guarani Mbya's complex verb morphology (argument cross-referencing via affixes) increases token-level sparsity and may reduce subword sharing, challenging standard byte-pair encoding tokenizers trained on less morphologically complex languages. Quick check question: How might a tokenizer trained primarily on analytic languages (e.g., English, Chinese) differ in coverage for agglutinative languages compared to fusional languages?

- **Word Order Variation (SVO vs. SOV)**: Why needed here - Guarani Mbya's mixed word order introduces positional uncertainty for attention mechanisms, potentially requiring more examples to learn reordering patterns compared to consistent SVO languages like Nheengatu. Quick check question: In a transformer decoder, how might consistent vs. variable source-target word order affect learned positional embeddings and cross-attention patterns?

## Architecture Onboarding

- **Component map**: Pre-trained backbone (NLLB-200 encoder-decoder) -> Tokenizer (Paraguayan Guarani SentencePiece/BPE) -> Finetuning head (task-specific output layer) -> Training configuration (57K steps, batch 16, lr=1e-4)

- **Critical path**: Data cleaning → Zero-shot baseline evaluation → Full finetuning with standardized hyperparameters → Ablation experiments (model size, data downsampling)

- **Design tradeoffs**:
  - Tokenizer choice: Shared Paraguayan Guarani tokenizer vs. language-specific tokenization (tradeoff: cross-lingual transfer vs. morphological coverage)
  - Model scale: 600M vs. 3.3B parameters (result: no significant difference, suggesting data/structure bottleneck)
  - Data allocation: Source-aware splitting vs. random split (tradeoff: domain shift reduction vs. rare construction representation)

- **Failure signatures**: Zero-shot plateau (both languages ~15-17 chrF), persistent finetuning gap (40 vs. 67 chrF despite matched data), scale insensitivity (<2 chrF change from 600M to 3.3B)

- **First 3 experiments**:
  1. Zero-shot baseline evaluation on both languages and translation directions
  2. Full-data finetuning comparison with identical hyperparameters
  3. Progressive data downsampling for Nheengatu to determine data-quantity effects

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Dataset sizes (3K-7K pairs) remain extremely small for neural MT, potentially exaggerating structural effects
- Reliance on a single pre-trained model (NLLB-200) with repurposed tokenizer limits generalizability
- Qualitative rather than quantitative analysis of morphological complexity
- Polynomial regression extrapolation beyond observed data range introduces significant uncertainty

## Confidence

**High Confidence:**
- The observed performance gap between Nheengatu and Guarani Mbya after finetuning is real and substantial
- Zero-shot performance similarity rules out pre-trained model limitations
- Model size scaling has minimal impact on results

**Medium Confidence:**
- Structural linguistic differences are the primary drivers of finetuning efficiency differences
- Downsampling experiments convincingly show data quantity alone cannot explain the gap
- NLLB-200's inclusion of Paraguayan Guarani provides meaningful transfer benefits

**Low Confidence:**
- Extrapolated scaling relationship (60K pairs needed) is highly speculative
- Specific morphological features causing difficulties are based on linguistic description rather than quantitative analysis
- Assumption of perfect control over all other factors between languages

## Next Checks
1. **Language-Specific Tokenization Experiment**: Train and evaluate Guarani Mbya and Nheengatu using language-specific tokenizers rather than the repurposed Paraguayan Guarani tokenizer to determine whether tokenizer sharing artificially benefits one language.

2. **Controlled Complexity Ablation**: Create synthetic datasets that systematically vary word order consistency and morphological complexity while holding vocabulary and domain constant, then fine-tune models to quantify independent effects on sample efficiency.

3. **Cross-Linguistic Scaling Study**: Conduct a broader study across 5-10 ultra-low-resource language pairs varying in structural similarity to pre-trained languages and morphological complexity, using consistent dataset sizes to establish generalizability and better estimate scaling curves.