---
ver: rpa2
title: 'RECAP: REwriting Conversations for Intent Understanding in Agentic Planning'
arxiv_id: '2509.04472'
source_url: https://arxiv.org/abs/2509.04472
tags:
- intent
- user
- plan
- plans
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "RECAP addresses the challenge of intent understanding in multi-agent\
  \ planning systems, where real-world dialogues are often ambiguous, underspecified,\
  \ or dynamic. The paper introduces RECAP, a benchmark designed to evaluate intent\
  \ rewriting\u2014reframing user-agent dialogues into concise, clarified representations\
  \ of user goals."
---

# RECAP: REwriting Conversations for Intent Understanding in Agentic Planning

## Quick Facts
- arXiv ID: 2509.04472
- Source URL: https://arxiv.org/abs/2509.04472
- Reference count: 23
- One-line primary result: RECAP is a benchmark for intent rewriting in multi-agent planning, showing DPO-fine-tuned rewriters improve plan quality.

## Executive Summary
RECAP addresses the challenge of intent understanding in multi-agent planning systems, where real-world dialogues are often ambiguous, underspecified, or dynamic. The paper introduces RECAP, a benchmark designed to evaluate intent rewriting—reframing user-agent dialogues into concise, clarified representations of user goals. The dataset captures diverse challenges such as ambiguity, intent drift, vagueness, and mixed-goal conversations. The authors develop prompt-based and DPO-trained rewriters that significantly improve downstream plan quality compared to baselines. Human and LLM-based evaluations show that plans from advanced rewrites are consistently preferred, especially in complex scenarios. DPO-trained models further enhance performance by aligning rewrites with human preferences. The results demonstrate that intent rewriting is a critical, tractable component for improving agentic planning in open-domain dialogue systems.

## Method Summary
RECAP introduces a benchmark for intent rewriting in agentic planning, where raw user-agent dialogues are transformed into concise, clarified intent representations to improve downstream planning. The method involves a rewriter module (prompt-based or DPO-trained) that processes dialogue, identifies the latest user intent(s), and generates a task-aware summary. This rewritten intent is then fed to a planner (e.g., GPT-4o-mini) to generate a plan as a Directed Acyclic Graph (DAG). Plan quality is evaluated by human annotators or an LLM-as-Judge, comparing pairs of plans based on rubric criteria like latest intent, fabrication, and completeness. DPO fine-tuning uses plan preference labels to align rewriter outputs with downstream utility.

## Key Results
- Advanced prompt-based rewriter outperforms baselines, especially on complex intent challenges like ambiguity and drift.
- DPO-fine-tuned rewriters (DPO:human) show additional gains, particularly for shifted and multi-intent scenarios.
- LLM-as-Judge evaluator, when fine-tuned, achieves high accuracy in plan preference prediction, enabling scalable evaluation.

## Why This Works (Mechanism)

### Mechanism 1: Explicit Intent Modeling Reduces Planning Ambiguity
- **Claim:** Rewriting multi-turn dialogue into a concise, clarified intent representation improves downstream planner performance by reducing noise and focusing on the latest goal.
- **Mechanism:** A rewriter processes USER-AGENT dialogue, identifies the user’s most recent intent(s) (handling shifts and multi-intent), filters irrelevant/outdated turns, and produces a task-aware summary. This explicit intent provides a cleaner target for the planner’s task decomposition, mitigating issues from raw dialogue such as outdated intents or conversational noise.
- **Core assumption:** The planner is sensitive to input formulation; clearer summaries reduce cognitive load and task-decomposition errors.
- **Evidence anchors:**
  - [abstract] "RECAP captures diverse challenges such as ambiguity, intent drift... a prompt-based rewriting approach that outperforms baselines, in terms of plan preference."
  - [Section 2, Figure 2] Raw dialogue confuses planners (e.g., mistaking agent suggestions for user requests); explicit intent ("search for a Mexican restaurant") leads to correct plans.
  - [Section 5.2] Sensitivity analysis shows planners generate structurally/semantically different plans given different intent formulations, with greater divergence in longer conversations.
- **Break condition:** If planners are robust to noise/ambiguity or rewriting introduces errors (e.g., misclassifying refinements as intent shifts), benefits may diminish or reverse.

### Mechanism 2: Preference-Aligned Rewriting via DPO Enhances Utility
- **Claim:** Fine-tuning a rewriter using Direct Preference Optimization (DPO) with plan preference labels aligns outputs with downstream planning utility.
- **Mechanism:** Pairs of plans from different rewrites are compared; the rewrite leading to the preferred plan (human or LLM-judged) is positive, the other negative. DPO adjusts parameters to favor outputs more likely to yield preferred plans, without gold-standard rewrites.
- **Core assumption:** Plan preference reliably proxies rewriting quality and downstream task success; sufficient preference data exists for optimization.
- **Evidence anchors:**
  - [abstract] "We further demonstrate that fine-tuning two DPO-based rewriters yields additional utility gains."
  - [Section 4.2] Describes DPO:human (human labels) and DPO:LLM (LLM-as-judge labels).
  - [Section 5.5, Table 5] DPO:human shows higher win rates vs. Advanced, especially on Shifted/Multi-/Underspecified Intent.
  - [corpus] Related work supports conversational strategy tuning (e.g., *From Simulation to Strategy*) and prompt rewriting (*Conversational User-AI Intervention*), but does not directly validate this specific DPO-rewriting mechanism; evidence is limited to the paper.
- **Break condition:** If preference labels are noisy or uncorrelated with actual task success, DPO may optimize spurious signals.

### Mechanism 3: LLM-as-Judge Evaluator Enables Scalable Plan Assessment
- **Claim:** A fine-tuned LLM can reliably evaluate plan preference, replacing expensive human annotation for large-scale experimentation.
- **Mechanism:** An LLM is prompted/fine-tuned with the same rubric as humans (latest intent, fabrication, granularity, completeness, logical order) to compare plan pairs. Fine-tuning on human labels improves alignment.
- **Core assumption:** LLM preferences correlate sufficiently with human judgments and the rubric captures key plan-quality aspects.
- **Evidence anchors:**
  - [Section 3.3] Introduces LLM-as-Judge evaluator.
  - [Section 5.4, Table 4] Fine-tuned models (e.g., gpt-4.1) achieve significantly higher accuracy/F1 vs. zero-shot baselines on plan preference prediction.
  - [corpus] Corpus discusses robust evaluation (e.g., *MTMCS-Bench* for multimodal safety) but does not directly validate this specific LLM-as-judge for planning preference; evidence is primarily from the paper.
- **Break condition:** If the LLM evaluator exhibits bias, fails to generalize, or the rubric misses critical failure modes, assessments may misguide development.

## Foundational Learning

- **Concept: Intent Rewriting vs. Classification**
  - Why needed: Understand why explicit rewriting (open-ended, flexible) is proposed over fixed-schema intent classification for agentic planning in open-domain dialogue.
  - Quick check: Can you explain one key limitation of intent classification that rewriting addresses?

- **Concept: Planner Sensitivity to Input Formulation**
  - Why needed: Grasp the core empirical finding that planners are sensitive to how intent is presented, motivating the rewriting module.
  - Quick check: What metric would you use to quantify if a planner’s outputs change significantly given different rewrites of the same dialogue?

- **Concept: Direct Preference Optimization (DPO)**
  - Why needed: Understand how preference labels are used to fine-tune the rewriter to optimize downstream utility without gold rewrites.
  - Quick check: In DPO training, what constitutes a positive and a negative sample for the rewriter?

## Architecture Onboarding

- **Component map:**
  - USER-AGENT dialogue -> Rewriter -> Rewritten Intent -> Planner -> Plan DAG -> Evaluator -> Preference Label -> (Optional) DPO Trainer -> Updated Rewriter

- **Critical path:**
  1. Dialogue input -> Rewriter -> Rewritten Intent
  2. Rewritten Intent -> Planner -> Plan DAG
  3. Plan DAGs -> Evaluator -> Preference Label
  4. (Optional) Preference Labels -> DPO Trainer -> Updated Rewriter

- **Design tradeoffs:**
  - **Rewriter complexity:** Simple summarization vs. task-aware prompts vs. DPO-fine-tuned models (cost vs. performance).
  - **Evaluation source:** Human labels (high quality, expensive) vs. LLM-as-Judge (scalable, potential bias).
  - **Training data:** Using human preferences (DPO:human) vs. LLM-judged preferences (DPO:LLM) for rewriter fine-tuning (quality vs. scalability).

- **Failure signatures:**
  - Rewriter over-optimizes for detecting intent shifts, misclassifying intent refinements as new goals (Section B.1, Figure 5 "Fake Intent Shifts").
  - Planner ignores rewritten intent and hallucinates steps or relies on outdated dialogue context.
  - LLM evaluator shows systematic bias (e.g., preferring longer plans) or low agreement with human judgments.

- **First 3 experiments:**
  1. **Baseline sensitivity check:** Run Dummy (raw dialogue) vs. Advanced rewriter on a held-out set of 20 conversations. Measure structural (GED) and semantic (BERTScore) differences in generated plans to confirm planner sensitivity in your setup.
  2. **Rewriter ablation:** Compare three rewriter variants (Basic, Advanced, DPO:human) using human evaluation on a small set (e.g., 20-30 conversations). Track win/tie/loss rates and inter-annotator agreement to validate paper trends.
  3. **Evaluator alignment:** Compare a zero-shot LLM (e.g., GPT-4o-mini) vs. a fine-tuned evaluator against human labels on a validation set. Measure accuracy/F1 to justify using the LLM evaluator for larger-scale experiments.

## Open Questions the Paper Calls Out
None

## Limitations
- **Planner Robustness:** The claimed sensitivity of planners to intent formulation is based on a single planner type (GPT-4o-mini). It remains unclear whether other planners or smaller models exhibit similar sensitivity.
- **DPO Generalization:** The effectiveness of the DPO-trained rewriters is demonstrated primarily on human preference labels (DPO:human). The results for DPO:LLM are less conclusive, and the method's performance when scaling to noisy LLM judgments is uncertain.
- **Evaluation Stability:** While the LLM-as-judge shows improved performance after fine-tuning, the robustness of this evaluation method across different model versions or rubric interpretations is not established.

## Confidence
- **High Confidence:** The core finding that planners are sensitive to input formulation is well-supported by controlled ablation studies and diverse intent challenges.
- **Medium Confidence:** The preference alignment via DPO is promising but requires more validation on noisy labels and different planner types.
- **Medium Confidence:** The LLM-as-judge is effective in this context but may not generalize to all planning domains or evaluator models.

## Next Checks
1. **Planner Generalization:** Test the rewriting approach with a different planner (e.g., Claude-3.5-Sonnet or a smaller open model) to verify that the sensitivity to input formulation is not specific to GPT-4o-mini.
2. **DPO with Noisy Labels:** Conduct a controlled experiment where DPO is trained using LLM-judged preferences with varying levels of label noise. Compare the resulting rewriter's performance against the human-preference baseline.
3. **Evaluator Bias Analysis:** Perform an error analysis on the LLM-as-judge by identifying systematic preferences (e.g., for longer plans) and validating these against human judgments on a diverse set of plan pairs.