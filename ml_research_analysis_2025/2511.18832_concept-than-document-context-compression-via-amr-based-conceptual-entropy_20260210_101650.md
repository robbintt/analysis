---
ver: rpa2
title: 'Concept than Document: Context Compression via AMR-based Conceptual Entropy'
arxiv_id: '2511.18832'
source_url: https://arxiv.org/abs/2511.18832
tags:
- context
- semantic
- compression
- language
- concepts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of information overload in long-context
  processing for large language models, particularly in retrieval-augmented generation
  tasks where extensive supporting documents introduce redundant content that degrades
  reasoning accuracy and increases computational overhead. The proposed method introduces
  an unsupervised context compression framework that leverages Abstract Meaning Representation
  (AMR) graphs and information-theoretic principles to identify and preserve semantically
  essential information while filtering out irrelevant content.
---

# Concept than Document: Context Compression via AMR-based Conceptual Entropy

## Quick Facts
- **arXiv ID:** 2511.18832
- **Source URL:** https://arxiv.org/abs/2511.18832
- **Reference count:** 32
- **Primary result:** ~50% context reduction while maintaining or improving accuracy on RAG tasks

## Executive Summary
This paper introduces an unsupervised context compression framework for long-context large language models, particularly addressing information overload in retrieval-augmented generation (RAG) systems. The method leverages Abstract Meaning Representation (AMR) graphs and information-theoretic principles to identify semantically essential information while filtering irrelevant content. By computing node-level conceptual entropy and applying statistical significance testing, the framework preserves core semantic information while reducing context length by approximately 50%. Experiments on PopQA and EntityQuestions datasets demonstrate substantial performance improvements over vanilla RAG and existing compression baselines across different model sizes.

## Method Summary
The method constructs AMR graphs from raw contexts and computes node-level conceptual entropy to estimate concept importance. Statistical significance testing identifies high-information nodes that form the backbone for reconstructing a condensed context preserving core semantic information. The pipeline involves parsing documents to sentence-level AMR graphs using an mBART-based parser, extracting concept nodes with token-level entropy from the parser output, filtering concepts via one-sample t-test with α=0.3, and applying post-processing (temporal reconstruction, redundancy removal, surface realization). The approach achieves ~50% compression ratio while maintaining or improving accuracy on RAG tasks.

## Key Results
- Achieves ~50% context length reduction while maintaining accuracy on PopQA and EntityQuestions datasets
- Shows consistent performance gains across different backbone LLMs (0.87-2.97% AUC improvement on PopQA)
- Particularly effective in long-context scenarios (K=6-10), with 1.96-3.45% AUC improvement on EntityQuestions
- Lower variance across backbone LLMs (0.02-0.08 standard deviation) compared to baselines

## Why This Works (Mechanism)

### Mechanism 1: Semantic Structure Extraction via AMR
- **Claim:** AMR graphs capture core semantic content while abstracting away from syntactic variations, enabling more precise identification of informative content than surface-level text analysis.
- **Mechanism:** AMR parsing converts raw text into directed acyclic graphs where nodes represent semantic concepts and edges represent semantic relationships. This structural representation exposes the semantic roles concepts play across contexts.
- **Core assumption:** Concepts that assume diverse semantic roles across contexts carry more informative value for inference, which can be quantified through their relational patterns in the graph structure.
- **Evidence anchors:** [abstract] "By quantifying node-level entropy within AMR graphs, our method estimates the conceptual importance of each node"; [Page 2] "AMR graphs provide a structured representation that abstracts away from surface syntactic variations while retaining core semantic content"
- **Break condition:** AMR parser errors or incomplete graphs degrade concept extraction.

### Mechanism 2: Information-Theoretic Concept Scoring
- **Claim:** Token-level perplexity measurements aggregated to concept-level entropy scores can discriminate between generic and content-specific semantic units.
- **Mechanism:** For each concept node, the method computes entropy using the AMR generation model's uncertainty when predicting the concept token sequence. Subword tokens are aggregated using the formula H(v) = (1/m) × Σ E(sⱼ), where E(sⱼ) = exp(-log P(sⱼ|s<ⱼ, Gᵢ).
- **Core assumption:** The AMR parsing model's predictive uncertainty correlates with informational importance for downstream reasoning tasks.
- **Evidence anchors:** [Page 4] "High-entropy nodes often represent content-specific, less redundant meanings, thus providing more discriminative signals for downstream reasoning"
- **Break condition:** If the parsing model has systematic biases or vocabulary gaps, entropy scores may not reflect true informational importance.

### Mechanism 3: Statistical Significance Filtering
- **Claim:** One-sample t-testing identifies concepts with statistically significant entropy deviations from the population mean, filtering generic concepts while preserving a relative conceptual basis.
- **Mechanism:** For each document, compute t-statistic t(vⱼ) = (H(vⱼ) - H̄) / (s/√n) and corresponding p-values. Retain concepts where p(vⱼ) < α (α = 0.3).
- **Core assumption:** Concepts with entropy significantly above the document's mean represent query-relevant information rather than document-specific noise.
- **Evidence anchors:** [Page 5] "We then screen out concepts whose p-values satisfy p(vⱼ) < α as statistically significant high-information concepts"
- **Break condition:** In documents with uniform entropy distributions or very few concepts, the t-test may fail to identify meaningful outliers.

## Foundational Learning

- **Abstract Meaning Representation (AMR):**
  - **Why needed here:** Core representation format. Without understanding that AMR is a graph-based semantic formalism where nodes = concepts and edges = relations, the entire method is opaque.
  - **Quick check question:** Given the sentence "The boy wants the girl to believe him," can you sketch the AMR graph structure showing the want-01 concept with ARG0 (boy) and ARG1 (believe-01)?

- **Information Entropy and Perplexity:**
  - **Why needed here:** The scoring mechanism relies on entropy computed from model perplexity. Understanding why high perplexity → high entropy → higher information content is essential.
  - **Quick check question:** If a token has probability P = 0.01 in a model's distribution, what is its entropy E = exp(-log P)? Why would repeated low-probability tokens across a concept indicate high informational value?

- **Statistical Hypothesis Testing (t-test):**
  - **Why needed here:** The filtering step uses one-sample t-tests to identify significant concepts. Engineers must understand why comparing individual entropy to the sample mean with variance normalization matters.
  - **Quick check question:** In a document with 50 concepts having mean entropy H̄ = 2.3 and std dev s = 0.8, what t-statistic does a concept with H(v) = 3.9 achieve? Is it significant at α = 0.3 (roughly |t| > 1.0 for large n)?

## Architecture Onboarding

- **Component map:**
  Raw Documents (D) → AMR Graphs (G₁...Gₙ) → Concept Sets → Entropy-Scored Concepts → Significant Concepts → Compressed Context (C')

- **Critical path:** The entropy computation step (Eq. 2-3) is the bottleneck—it requires forward passes through the AMR parsing model for each concept's token sequence. The t-test filtering is O(n) per document and negligible.

- **Design tradeoffs:**
  - **α threshold:** Paper uses 0.3. Lower values (0.01, 0.05) cause severe degradation (-80 to -218 AUC). Higher values (0.5) retain noise. This is a compression-vs-preservation tradeoff.
  - **Discarding edges vs. preserving relations:** The method explicitly discards edge information Eᵢ, relying on LLMs to infer relationships. This reduces output size but may lose relational nuance.
  - **Controlled evaluation setting:** Paper filters to answer-containing documents only. Real-world deployment with noisy retrieval would require additional handling.

- **Failure signatures:**
  - **Small models (GPT-Neo-1.3B, Bloom-560m):** Show degraded performance relative to vanilla (Table 3: -3.62 to -17.53 AUC on EntityQuestions). Hypothesis: smaller models need richer linguistic context for scenario reconstruction.
  - **Long-context interval (Iₗ):** Performance gains shrink relative to standard interval. σ drops significantly, suggesting compression benefits plateau at higher K.
  - **Parser failures:** "Incomplete or noisy graphs" directly limit concept coverage (explicitly noted in Limitations).

- **First 3 experiments:**
  1. **Reproduce on PopQA subset with K ∈ {1, 5, 10}:** Implement the full pipeline using the referenced mBART parser. Compare AUC against vanilla and LLMLingua baselines. Verify ~50% compression ratio.
  2. **Ablate α threshold:** Test α ∈ {0.1, 0.3, 0.5} on held-out queries. Plot AUC vs. compression ratio to characterize the tradeoff curve for your target domain.
  3. **Test edge preservation:** Modify the pipeline to retain selected edge labels (e.g., ARG0, ARG1, time) alongside concepts. Measure whether relational hints improve accuracy on relation-heavy queries (e.g., "Who did X to Y?").

## Open Questions the Paper Calls Out
- **Adaptive compression strategies:** Future research includes exploring adaptive compression strategies based on query complexity, as the current static thresholding approach may not handle varying query difficulty optimally.
- **Cross-document concept relationships:** The paper suggests modeling cross-document concept relationships to enhance semantic scenario reconstruction, as the current methodology discards explicit AMR graph edges and relies on LLM's inherent reconstruction ability.
- **Realistic retrieval settings:** Performance in realistic retrieval settings containing irrelevant or conflicting documents remains unexplored, as the current evaluation uses a controlled setting with only answer-containing documents.

## Limitations
- The entropy computation mechanism relies on assumptions about parser perplexity correlating with informational importance that aren't empirically validated beyond the main experiments.
- The statistical filtering approach (α=0.3) appears somewhat arbitrary, with limited corpus evidence for this specific statistical approach in context compression.
- The controlled evaluation setting (filtering to answer-containing documents only) limits real-world applicability and doesn't test performance on noisy retrieval outputs.

## Confidence
- **High Confidence:** AMR-based semantic structure improves over token-level compression methods (well-supported by experimental results showing consistent gains across models and datasets).
- **Medium Confidence:** Information-theoretic scoring mechanism (entropy from parser perplexity) is mechanistically sound but relies on unvalidated assumptions about parser uncertainty correlating with relevance.
- **Low Confidence:** Statistical filtering approach (one-sample t-test with α=0.3) appears arbitrary, with poor performance at extreme values but no clear theoretical grounding for the optimal threshold.

## Next Checks
1. **Parser-Entropy Correlation Study:** Systematically evaluate whether AMR parser perplexity actually correlates with query relevance by annotating a sample of high/low entropy concepts for human relevance judgments.
2. **Edge Information Retention Experiment:** Implement a variant that preserves key edge labels (ARG0, ARG1, time) alongside concepts. Compare performance on relation-heavy queries to quantify information loss from discarding AMR edges.
3. **Noisy Retrieval Robustness Test:** Run the full pipeline on datasets with mixed relevant/irrelevant documents (not filtered to "hasanswer"=True). Measure performance degradation and characterize failure modes when the compression system encounters noisy, irrelevant contexts.