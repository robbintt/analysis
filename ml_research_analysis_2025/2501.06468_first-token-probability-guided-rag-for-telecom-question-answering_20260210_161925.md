---
ver: rpa2
title: First Token Probability Guided RAG for Telecom Question Answering
arxiv_id: '2501.06468'
source_url: https://arxiv.org/abs/2501.06468
tags:
- probability
- token
- chunk
- accuracy
- first
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving accuracy in domain-specific
  multiple choice question answering (MCQA) within telecommunications by combining
  Retrieval-Augmented Generation (RAG) with a novel first token probability guided
  approach. The proposed framework optimizes hyperparameters such as chunk number
  and chunk window size dynamically, guided by normalized first token probabilities
  from the model's output, to iteratively refine context and reduce hallucinations.
---

# First Token Probability Guided RAG for Telecom Question Answering

## Quick Facts
- arXiv ID: 2501.06468
- Source URL: https://arxiv.org/abs/2501.06468
- Reference count: 18
- Key result: 78.4% accuracy on telecom MCQA, 26.8% improvement over baseline

## Executive Summary
This paper introduces a first token probability guided retrieval-augmented generation (RAG) framework for domain-specific multiple choice question answering in telecommunications. The method iteratively optimizes retrieval hyperparameters (chunk number and chunk window size) by monitoring the normalized first token probability from the model's output. By using this probability as a confidence metric, the approach dynamically refines context to improve answer accuracy and reduce hallucinations. Experimental results on the 3GPP corpus demonstrate significant gains over baseline RAG and a commercial LLM, highlighting the effectiveness of token-level confidence signals in iterative retrieval.

## Method Summary
The proposed method combines RAG with an iterative retrieval optimization strategy guided by first token probability. For each question, the system starts with a baseline context window and iteratively retrieves additional chunks, adjusting the number and window size based on the normalized probability of the first token in the model's predicted answer. This dynamic refinement continues until the probability stabilizes or a stopping criterion is met. The approach is evaluated using the Small Language Model (SLM) Phi-2 on a telecom domain corpus, with accuracy measured as the primary performance metric.

## Key Results
- Best accuracy achieved: 78.4% on the test set
- Improvement over baseline (no RAG): 26.8%
- Outperformed a commercial LLM (ChatGPT-3.5) in the telecom domain

## Why This Works (Mechanism)
The method leverages the intuition that the first token probability in a multiple choice setting reflects the model's confidence in its answer. By normalizing probabilities across answer choices and using this as a signal for context quality, the framework can dynamically refine retrieval parameters to select the most relevant context chunks. This iterative process helps reduce hallucination and improve answer accuracy by ensuring the model has access to the most pertinent information.

## Foundational Learning
- **Retrieval-Augmented Generation (RAG):** Combines information retrieval with generative models to provide context for answering questions. Needed to ground model responses in relevant documents. Quick check: Does the retrieved context cover the answer?
- **Chunk Number and Window Size:** Hyperparameters controlling how many text chunks are retrieved and their size. Needed to balance context richness and noise. Quick check: Are the chunks large enough to contain answers but small enough to avoid irrelevant content?
- **First Token Probability:** The probability assigned by the model to the first token of its predicted answer. Used here as a confidence signal. Quick check: Does a higher first token probability correlate with answer correctness?
- **Iterative Retrieval:** Repeated refinement of retrieved context based on model feedback. Needed to progressively improve context quality. Quick check: Does accuracy improve with each iteration?
- **Normalization of Probabilities:** Adjusting token probabilities across answer choices to enable fair comparison. Needed for meaningful confidence assessment. Quick check: Are probabilities properly normalized before comparison?

## Architecture Onboarding
**Component Map:** Question -> Context Retrieval (chunk number, window size) -> SLM Phi-2 -> First Token Probability -> Feedback (adjust retrieval) -> Final Answer
**Critical Path:** Question → Context Retrieval → Model Prediction → Probability Check → (Iterate or Output)
**Design Tradeoffs:** Accuracy vs. computational overhead (iterative retrieval vs. single-pass), granularity of chunk selection vs. retrieval speed, confidence-based stopping vs. risk of premature termination.
**Failure Signatures:** Low first token probability across iterations (indicates poor context or ambiguous question), accuracy plateau (retrieval saturation), high computational cost (inefficient search).
**First Experiments:** 1) Compare accuracy with and without iterative retrieval; 2) Test different stopping criteria for iterations; 3) Vary chunk window size to find optimal context granularity.

## Open Questions the Paper Calls Out
### Open Question 1
- **Question:** Does the positive correlation between first token probability and answer accuracy hold for Large Language Models (LLMs) with significantly larger parameter counts?
- **Basis in paper:** [explicit] The authors currently validate their method using only the Small Language Model (SLM) Phi-2 (2.7B parameters) and explicitly state a plan to "investigate the impact of LLM-generated token probability on performance" in future work.
- **Why unresolved:** The probability distributions and calibration of SLMs often differ from those of larger models (e.g., 70B+ parameters), so the heuristic may not scale effectively without adjustment.
- **What evidence would resolve it:** Replicating the experiments on the same 3GPP corpus using larger models (e.g., Llama-3-70B) and analyzing the correlation between first token probability and accuracy.

### Open Question 2
- **Question:** Can the computational overhead of the "Best Probability Method" be reduced for real-time applications without sacrificing accuracy?
- **Basis in paper:** [inferred] While the "Best Probability Method" achieved the highest accuracy (78.4%), it requires an exhaustive grid search over all chunk numbers and window sizes (Algorithm 1), implying a high inference cost compared to the single-pass baseline.
- **Why unresolved:** The paper does not provide a latency analysis or comparison of inference times, leaving the practical trade-off between the exhaustive hyperparameter search and deployment feasibility unaddressed.
- **What evidence would resolve it:** A comparative study of accuracy versus inference latency, potentially testing early-stopping criteria or reinforcement learning to predict optimal hyperparameters without a full search.

### Open Question 3
- **Question:** Is the first token probability framework effective for generative tasks beyond Multiple Choice Question Answering (MCQA)?
- **Basis in paper:** [explicit] The conclusion notes the intention to "extend this approach to other specialized domains," while the methodology is explicitly restricted to MCQA where probabilities can be normalized across discrete options (A, B, C, D).
- **Why unresolved:** The mechanism relies on mapping tokens to specific choices; it is unclear how this confidence guidance would function when correct answers require generating complex, multi-token sentences rather than selecting a single token.
- **What evidence would resolve it:** Adapting the framework to an open-ended generation task (e.g., summarizing telecom standards) and defining a metric for "confidence" when multiple tokens are generated.

## Limitations
- The effectiveness of the first token probability heuristic is not validated across different model architectures or domains.
- The framework is restricted to MCQA and may not generalize to open-ended generative tasks.
- No explicit analysis of computational overhead or scalability for real-time applications.

## Confidence
- **Method novelty:** Medium
- **Empirical results:** Medium
- **Cross-domain generalization:** Low
- **Scalability and efficiency:** Low

## Next Checks
1. Replicate the framework on at least two non-telecom MCQA datasets to test cross-domain robustness and calibration stability.
2. Conduct ablation studies comparing first token probability against entropy-based or margin-based uncertainty measures for context selection.
3. Analyze computational overhead and iteration count to quantify scalability and efficiency relative to static RAG baselines.