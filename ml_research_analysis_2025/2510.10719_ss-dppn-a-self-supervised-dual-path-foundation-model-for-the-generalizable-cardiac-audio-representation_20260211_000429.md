---
ver: rpa2
title: 'SS-DPPN: A self-supervised dual-path foundation model for the generalizable
  cardiac audio representation'
arxiv_id: '2510.10719'
source_url: https://arxiv.org/abs/2510.10719
tags:
- learning
- dataset
- heart
- loss
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SS-DPPN, a self-supervised dual-path prototypical
  network for generalizable cardiac audio representation and classification. The method
  addresses the scarcity of expert-annotated data in medical AI by using self-supervised
  learning to create robust representations from unlabeled data.
---

# SS-DPPN: A self-supervised dual-path foundation model for the generalizable cardiac audio representation

## Quick Facts
- arXiv ID: 2510.10719
- Source URL: https://arxiv.org/abs/2510.10719
- Reference count: 40
- Primary result: Achieves 0.910 accuracy and 0.868 F1-score on CirCor 2022 with 25% labeled data efficiency

## Executive Summary
This paper introduces SS-DPPN, a self-supervised dual-path prototypical network designed for cardiac audio representation learning and classification. The method addresses the critical challenge of limited expert-annotated medical data by leveraging self-supervised learning to create robust representations from unlabeled data. SS-DPPN processes both 1D waveforms and 2D spectrograms through a dual-path architecture, using a hybrid loss function that combines NT-Xent and Wasserstein distance. The model demonstrates state-of-the-art performance across four cardiac audio benchmarks, achieving exceptional data efficiency by matching fully supervised performance with only 25% of labeled data. The learned representations successfully generalize to lung sound classification and heart rate estimation tasks, validating SS-DPPN as a robust foundation model for physiological signals.

## Method Summary
SS-DPPN employs a dual-path architecture where cardiac audio is processed through both 1D waveform and 2D spectrogram encoders, with representations fused for downstream classification. The self-supervised pre-training uses a hybrid loss combining NT-Xent contrastive loss and Wasserstein distance to learn discriminative representations from unlabeled data. A Dilated Temporal Convolutional Network processes the 1D waveform while a ResNet-50 processes the 2D log-mel spectrogram. The pre-trained backbone is then frozen and used with a Prototypical Network for downstream murmur classification. The model achieves significant data efficiency, matching fully supervised performance with only 25% of labeled data, while demonstrating strong generalization to non-cardiac tasks.

## Key Results
- Achieves 0.910 accuracy and 0.868 F1-score on CirCor 2022 dataset, surpassing supervised and self-supervised baselines
- Matches fully supervised performance using only 25% of labeled training data, demonstrating exceptional data efficiency
- Successfully generalizes to lung sound classification (82.2% accuracy) and heart rate estimation (8.3 bpm MAE) tasks

## Why This Works (Mechanism)
The dual-path architecture captures complementary information from both time-domain waveforms and frequency-domain spectrograms, while the hybrid loss function (NT-Xent + Wasserstein) encourages both similarity preservation and distributional alignment. The Wasserstein component improves calibration and distributional alignment, while the contrastive component ensures discriminative features. The Prototypical Network framework naturally handles class imbalance through episodic training, and the frozen backbone with episodic updates enables efficient adaptation to new tasks.

## Foundational Learning
- **Contrastive Learning (NT-Xent)**: Learn representations by attracting similar pairs and repelling dissimilar pairs in embedding space - needed for unsupervised feature learning without labels; quick check: verify embedding separation with t-SNE/UMAP
- **Wasserstein Distance**: Measure distributional discrepancy between positive and negative pairs using optimal transport - needed for improved calibration and distributional alignment; quick check: compare calibration curves with/without Wasserstein term
- **Prototypical Networks**: Episodic training where each batch forms support/query sets for few-shot classification - needed to handle class imbalance and enable efficient adaptation; quick check: monitor class-wise performance across episodes
- **Dual-Path Fusion**: Combine 1D temporal and 2D spectral representations through learned fusion - needed to capture complementary acoustic features; quick check: ablate each path individually to measure contribution
- **Self-Supervised Pre-training**: Learn general representations from unlabeled data before fine-tuning - needed to overcome scarcity of expert annotations; quick check: compare with supervised baseline at varying labeled data percentages
- **Geometric Losses**: Use Sinkhorn-based optimal transport for Wasserstein computation - needed for differentiable distributional alignment; quick check: verify gradient flow through Wasserstein term during training

## Architecture Onboarding

**Component map:** Raw PCG signal → 1D Dilated TCN + 2D ResNet-50 → Projection heads → Fusion MLP → Prototypical Network (downstream)

**Critical path:** Data preprocessing → Dual-path encoders → Hybrid loss optimization → Frozen backbone → Prototypical Network fine-tuning → Evaluation

**Design tradeoffs:** Dual-path architecture increases parameter count and computational cost versus single-path approaches, but captures complementary temporal and spectral information. The hybrid loss adds complexity over pure contrastive methods but improves calibration and distributional alignment. The frozen backbone approach trades some task-specific optimization for better generalization.

**Failure signatures:** Poor performance on minority classes suggests Prototypical Network not handling imbalance well; overconfident but poorly calibrated predictions indicate need for Wasserstein regularization; high train/validation gap suggests overfitting to small datasets; poor cross-domain transfer suggests pre-training failed to learn general features.

**Three first experiments:**
1. Train 1D path alone and 2D path alone on CirCor 2022 to establish baseline contributions
2. Train with NT-Xent loss only versus hybrid loss to quantify Wasserstein benefit
3. Vary labeled data percentage (5%, 25%, 50%, 100%) to validate data efficiency claims against supervised baseline

## Open Questions the Paper Calls Out
None identified in the paper.

## Limitations
- Evaluation primarily on single cardiac dataset with limited external validation on only one additional cardiac dataset
- Computational requirements for dual-path architecture with ResNet-50 backbone may limit accessibility
- 25% data efficiency claim benchmarked against specific supervised baseline without comparison to other self-supervised methods' efficiency curves

## Confidence

**High confidence:** Dual-path architecture design (1D waveform + 2D spectrogram processing) and hybrid loss function composition are well-specified and reproducible

**Medium confidence:** Reported state-of-the-art performance on CirCor 2022 benchmarks, given methodological rigor and appropriate evaluation metrics including calibration analysis

**Medium confidence:** Generalizability claims to lung sound classification and heart rate estimation, though validation is limited to only two tasks with modest performance metrics

## Next Checks

1. Reproduce the 25% data efficiency claim by systematically varying labeled data percentages (5%, 10%, 25%, 50%, 100%) on CirCor 2022 and comparing against supervised baselines and other self-supervised methods

2. Evaluate cross-domain generalization on at least 3-5 additional physiological signal datasets (ECG, EEG, EMG) to test the foundation model claims beyond the two validated tasks

3. Conduct ablation studies isolating the contribution of each component: 1D path alone, 2D path alone, hybrid loss versus NT-Xent-only, and dual-path versus single-path architectures on the same benchmarks