---
ver: rpa2
title: 'DeepSeek''s WEIRD Behavior: The cultural alignment of Large Language Models
  and the effects of prompt language and cultural prompting'
arxiv_id: '2512.09772'
source_url: https://arxiv.org/abs/2512.09772
tags:
- cultural
- alignment
- please
- dimensions
- importance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates cultural alignment of six large language models
  (GPT-4, GPT-4.1, GPT-4o, GPT-5, DeepSeek-V3, and DeepSeek-V3.1) with the United
  States and China using Hofstede's VSM13 survey framework. Models were tested using
  different prompt languages (English and Simplified Chinese) and cultural prompting
  strategies (US or China).
---

# DeepSeek's WEIRD Behavior: The cultural alignment of Large Language Models and the effects of prompt language and cultural prompting

## Quick Facts
- **arXiv ID:** 2512.09772
- **Source URL:** https://arxiv.org/abs/2512.09772
- **Reference count:** 40
- **Primary result:** All tested LLMs show stronger alignment with US cultural values than Chinese values, with DeepSeek models unexpectedly showing particularly strong US alignment despite being Chinese-developed.

## Executive Summary
This study evaluates cultural alignment of six large language models (GPT-4, GPT-4.1, GPT-4o, GPT-5, DeepSeek-V3, and DeepSeek-V3.1) with the United States and China using Hofstede's VSM13 survey framework. Models were tested using different prompt languages (English and Simplified Chinese) and cultural prompting strategies (US or China). Results show all models exhibited stronger alignment with the United States than China, with DeepSeek models showing particularly strong US alignment despite being Chinese-developed. GPT-4o and GPT-4.1 demonstrated the most flexibility, successfully aligning with both cultures when prompted appropriately. Cultural prompting proved highly effective in English but less so in Simplified Chinese. GPT-4 showed unexpected closer alignment to China when prompted in English without cultural prompting. The findings highlight significant cultural biases in LLMs and suggest that prompt language and cultural prompting can be effective tools for cultural adaptation in AI applications.

## Method Summary
The study used Hofstede's VSM13 framework (24 questions across 6 cultural dimensions) to evaluate LLM cultural alignment. Six models were tested across 6 conditions (English/Simplified Chinese × no cultural prompt/US prompt/China prompt), with 20 survey responses per condition. Responses were aggregated to compute mean values for each question, then dimension scores were calculated using specific equations with normalization constants. Total alignment distance to US and China reference values was measured as the sum of absolute differences across all 6 dimensions. Temperature=2.0 was used to introduce response variance.

## Key Results
- All models showed stronger alignment with US cultural values than Chinese values
- DeepSeek models showed particularly strong US alignment despite being Chinese-developed
- GPT-4o and GPT-4.1 demonstrated high cultural plasticity, successfully aligning with both cultures when prompted
- Cultural prompting in English reduced alignment distance by 18.7%, versus only 4.9% in Simplified Chinese
- GPT-4 showed unexpected closer alignment to China when prompted in English without cultural prompting

## Why This Works (Mechanism)

### Mechanism 1: Training Data Distribution Drives Persistent WEIRD Alignment
- **Claim:** LLMs exhibit durable US cultural alignment because their training corpora are dominated by English-language, Western-sourced content, which embeds cultural values into model weights.
- **Mechanism:** During pre-training, models internalize statistical patterns not just of language syntax but of culturally-bounded value expressions, reasoning styles, and preference structures. These patterns become encoded in hidden representations and persist through fine-tuning, creating a "cultural prior" that resists lightweight intervention.
- **Core assumption:** The cultural values expressed in training data are absorbed as implicit priors rather than surface-level patterns that can be easily overridden.
- **Evidence anchors:**
  - [abstract] "DeepSeek-V3, V3.1, and OpenAI's GPT-5 exhibit a close alignment with the survey responses of the United States and do not achieve a strong or soft alignment with China, even when using cultural prompts or changing the prompt language."
  - [results] "DeepSeek's unique training method (DeepSeek-AI et al., 2025) appears to limit its ability to shift its cultural alignment through the selected methods, and its models align very closely with the United States."
  - [corpus] Related work confirms persistent WEIRD bias across multiple LLM families (Atari et al., 2023; Cao et al., 2023), suggesting training data dominance is a systemic issue, not model-specific.
- **Break condition:** If a model were trained on genuinely balanced multilingual, multicultural corpora with equal representation across cultural contexts, baseline cultural alignment should be more neutral or context-dependent rather than consistently US-aligned.

### Mechanism 2: Language Channel Modulates Cultural Plasticity
- **Claim:** Cultural prompting is significantly more effective when delivered in English than in the target culture's native language, because models have richer, more nuanced semantic representations in English.
- **Mechanism:** English prompts access more developed embedding spaces and attention patterns, allowing cultural instructions to propagate more effectively through the model's reasoning pathways. Non-English prompts may activate shallower representations where cultural nuance is less precisely encoded.
- **Core assumption:** The effectiveness of prompt-based interventions depends on the semantic resolution available in the prompt language, not just the content of the instruction.
- **Evidence anchors:**
  - [abstract] "Cultural prompting in English was most effective at shifting alignment, reducing total distance by 18.7% compared to baseline, versus only 4.9% in Chinese."
  - [results] "When using English and culturally prompting for China, the alignment improved by 29.8% compared to English without cultural prompting. When using Simplified Chinese and prompting for the US, the alignment improved by only 1.5%."
  - [corpus] Limited direct corpus evidence on language-modulated plasticity; this mechanism is primarily inferred from within-paper results. Related work (Masoud et al., 2025; Zhong et al., 2024) shows promise for non-English prompts but with methodological limitations.
- **Break condition:** If a model had genuinely balanced multilingual training, cultural prompting effectiveness should be roughly equivalent across languages, or potentially stronger in the target culture's native language.

### Mechanism 3: Model Architecture and Training Approach Determine Cultural Rigidity
- **Claim:** Certain model architectures or training methodologies produce "culturally rigid" models that resist alignment shifts, while others exhibit higher cultural plasticity.
- **Mechanism:** Architectural choices (e.g., mixture-of-experts routing, reinforcement learning approaches, training data filtering) may create models that either lock in cultural priors or maintain flexibility. Reinforcement learning from human feedback (RLHF) may amplify certain cultural signals while suppressing others.
- **Core assumption:** Cultural alignment is not just a function of training data but of how that data is processed and consolidated through architectural choices.
- **Evidence anchors:**
  - [results] "DeepSeek's unique training method (DeepSeek-AI et al., 2025) appears to limit its ability to shift its cultural alignment through the selected methods."
  - [results] "GPT-4 showed no strong or soft alignments with either the United States or China... GPT-4 remains largely neutral and does not shift its alignment strongly to either country."
  - [results] "GPT-4o and GPT-4.1 respond to the prompt language used... and cultural prompting strategies to create acceptable alignments with both the United States and China."
  - [corpus] Related work on model-specific cultural behaviors exists (Wang et al., 2024; Tao et al., 2024) but does not directly test architectural explanations for rigidity differences.
- **Break condition:** Models with similar architectures should exhibit similar cultural plasticity profiles; observing divergent behavior in architecturally similar models would falsify this mechanism.

## Foundational Learning

- **Concept: Hofstede's Cultural Dimensions (VSM13 Framework)**
  - **Why needed here:** The entire evaluation methodology depends on understanding what the six dimensions (PDI, IDV, MAS, UAI, LTO, IVR) measure and how they quantify cultural differences between populations.
  - **Quick check question:** Can you explain why Power Distance Index (PDI) might differ between the US and China, and what dimension equation components contribute to its calculation?

- **Concept: WEIRD Bias in Psychological and AI Research**
  - **Why needed here:** The paper's central thesis is that LLMs exhibit WEIRD (Western, Educated, Industrialized, Rich, Democratic) bias; understanding this concept frames why alignment with non-Western cultures matters for global deployment.
  - **Quick check question:** What are two practical consequences of deploying a WEIRD-biased LLM in a non-Western cultural context?

- **Concept: Cultural Prompting as Alignment Intervention**
  - **Why needed here:** The paper tests cultural prompting as a low-cost mitigation strategy; understanding what it is and its limitations is essential for interpreting results and practical application.
  - **Quick check question:** Based on the paper's findings, would you recommend cultural prompting in Simplified Chinese as an effective strategy for aligning models to Chinese cultural values? Why or why not?

## Architecture Onboarding

- **Component Map:**
  - Survey Instrument (VSM13 24 questions) → Prompt Conditions (6 variants) → Model APIs (6 models) → Batch Processing (temperature=2.0, 20 responses) → Response Parsing → Dimension Calculation (Table 1 equations) → Alignment Metric (sum of absolute distances)

- **Critical Path:**
  1. Set up API access for all target models with consistent temperature settings
  2. Prepare survey prompts in both English and Simplified Chinese with cultural system prompts
  3. Execute batch prompting (5 surveys × 24 questions = 120 prompts per batch; 4 batches per condition)
  4. Parse and aggregate responses to compute mean values per question (m01-m24)
  5. Apply dimension equations with normalization constants
  6. Calculate distance metrics to US and China reference dimensions
  7. Compare across conditions to quantify alignment shift effectiveness

- **Design Tradeoffs:**
  - **Temperature=2.0:** High temperature introduces variance to simulate population diversity, but may reduce reproducibility; consider running multiple seeds
  - **Population size n=20:** Meets Hofstede's minimum but is undersampled compared to human surveys; larger samples would increase precision but multiply API costs
  - **Two-country comparison:** Isolates US-China axis but limits generalizability; extending to 10+ countries would enable dimension-level analysis
  - **System prompt simplicity:** Brief cultural prompts are low-cost but may be insufficient; richer cultural context descriptions could improve effectiveness at cost of token budget

- **Failure Signatures:**
  - **Non-responsive models:** Models that show >200 total distance to both countries regardless of prompting (like GPT-4 baseline) indicate either neutral positioning or fundamental misalignment with survey methodology
  - **Language-invariant responses:** Minimal difference between English and Simplified Chinese conditions suggests the model may not be processing cultural-linguistic nuance
  - **Rigid alignment:** Models where cultural prompting produces <5% improvement (like DeepSeek models) indicate architectural resistance to prompt-based cultural shifts
  - **High dimension variance:** Large spread within a single dimension across conditions (see box plots in Appendix) suggests unstable cultural representations

- **First 3 Experiments:**
  1. **Baseline verification:** Run English + no cultural prompt condition for all models to confirm US alignment finding; use identical temperature and batch structure; compare dimension-level results to paper's Table 3
  2. **Language-swap test:** For a single plastic model (GPT-4o or GPT-4.1), run the Chinese cultural prompt in English vs. Simplified Chinese to directly quantify the language-modulated effectiveness gap (target: ~25-30% vs ~5% improvement)
  3. **Rigidity confirmation:** Attempt to shift DeepSeek-V3.1 using more aggressive cultural prompting (extended cultural context, role-play framing) to test whether the rigidity is method-limited or architecture-locked; document any improvement or confirm <10% shift ceiling

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Why is cultural prompting significantly more effective in English (18.7% reduction in alignment distance) than in Simplified Chinese (only 4.9% reduction)?
- **Basis in paper:** [explicit] The paper reports that "English-language cultural prompts were able to achieve an 18.7% decrease in total alignment distance" versus "only 4.9%" for Simplified Chinese prompts, but offers no explanation for this asymmetry.
- **Why unresolved:** The paper documents this finding but does not investigate whether it stems from training data composition, tokenization differences, or the inherent structure of prompts in each language.
- **What evidence would resolve it:** Ablation studies controlling for prompt length and semantic content across languages, combined with analysis of the cultural representation in each model's training corpus.

### Open Question 2
- **Question:** What explains the strong United States cultural alignment of Chinese-developed models like DeepSeek-V3 and V3.1?
- **Basis in paper:** [explicit] The authors note that "DeepSeek's unique training method appears to limit its ability to shift its cultural alignment through the selected methods, and its models align very closely with the United States," but do not identify the cause.
- **Why unresolved:** The paper establishes this counterintuitive finding—a Chinese model aligning more closely with US than Chinese cultural dimensions—but the mechanism remains unknown.
- **What evidence would resolve it:** Comparative analysis of training data sources, fine-tuning procedures, and reinforcement learning from human feedback (RLHF) annotator demographics across Western and Eastern model developers.

### Open Question 3
- **Question:** How would results differ if the survey population exceeded the minimum threshold of 20 responses per condition?
- **Basis in paper:** [explicit] The authors acknowledge in the Limitations section that "this is far lower than the number of surveys collected from each participating country in the VSM13 International Survey" and that "inclusion of more surveys could provide more accurate results."
- **Why unresolved:** The minimum population size was used for all 36 experimental conditions, and it is unknown whether this sample adequately captures model response variance.
- **What evidence would resolve it:** Replication with larger sample sizes (e.g., 100+ responses per condition) to determine whether mean dimension values stabilize or shift significantly.

## Limitations
- Narrow focus on only two cultural contexts (US and China) limits generalizability of findings
- Small sample size (n=20 per condition) provides limited statistical power for detecting subtle cultural alignment differences
- Temperature=2.0 setting introduces significant variance that may affect reproducibility
- Does not account for regional variations, generational differences, or socioeconomic status within target cultures

## Confidence
- **High Confidence:** All tested models show stronger alignment with US cultural values than Chinese values
- **Medium Confidence:** Cultural prompting effectiveness differs significantly between English and Simplified Chinese
- **Low Confidence:** DeepSeek's "unique training method" causes cultural rigidity (lacks specific architectural evidence)

## Next Checks
1. **Multi-Country Validation:** Extend evaluation framework to include 8-10 countries across different cultural clusters to test whether US alignment is a general Western bias or specifically US-oriented.
2. **Dimension-Level Analysis:** Conduct granular analysis of alignment at individual dimension level to identify systematic bias patterns requiring targeted mitigation.
3. **Cultural Prompting Efficacy Test:** Compare simple cultural system prompts versus richer cultural context descriptions to determine whether limited effectiveness in Simplified Chinese reflects prompt insufficiency rather than fundamental model limitations.