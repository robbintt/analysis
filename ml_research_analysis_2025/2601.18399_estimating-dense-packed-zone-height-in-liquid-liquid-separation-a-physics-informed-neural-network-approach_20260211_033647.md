---
ver: rpa2
title: 'Estimating Dense-Packed Zone Height in Liquid-Liquid Separation: A Physics-Informed
  Neural Network Approach'
arxiv_id: '2601.18399'
source_url: https://arxiv.org/abs/2601.18399
tags:
- pinn
- height
- phase
- experimental
- flow
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: A physics-informed neural network (PINN) is used to estimate phase
  heights in liquid-liquid separation by pretraining on synthetic data from an approximate
  mechanistic model and then fine-tuning with scarce experimental data. The PINN is
  combined with an Extended Kalman Filter to estimate phase heights from flow rate
  measurements.
---

# Estimating Dense-Packed Zone Height in Liquid-Liquid Separation: A Physics-Informed Neural Network Approach

## Quick Facts
- arXiv ID: 2601.18399
- Source URL: https://arxiv.org/abs/2601.18399
- Reference count: 0
- Primary result: PINN ensemble outperforms purely data-driven models in estimating phase heights from flow rate measurements in liquid-liquid separation.

## Executive Summary
This work addresses the challenge of estimating phase heights in liquid-liquid separation processes, a critical task for process control and safety. The authors propose a Physics-Informed Neural Network (PINN) combined with an Extended Kalman Filter (EKF) to estimate heavy phase (hHP) and dense-packed zone (hDP) heights using only flow rate measurements. By pretraining on synthetic data from a mechanistic model and fine-tuning with scarce experimental data, the PINN leverages physical knowledge to overcome data limitations. The ensemble-based approach with adaptive noise covariance estimation demonstrates superior performance in tracking phase heights compared to purely data-driven models, especially in extrapolation scenarios.

## Method Summary
The method involves pretraining a PINN on synthetic data generated by a mechanistic model (volume balance equations) to establish a physical prior, then fine-tuning on scarce experimental data to capture real-world dynamics. The PINN architecture consists of 2 hidden layers with 32 nodes each, using tanh activation (hidden) and sigmoid (output). An ensemble of 40 PINNs is used within an Extended Kalman Filter framework, where the ensemble covariance estimates process noise adaptively. The model enforces volume balance constraints during pretraining but relies on data-driven learning for complex coalescence and sedimentation dynamics during fine-tuning.

## Key Results
- PINN ensemble outperforms both single-stage PINN ensembles and purely data-driven neural networks in tracking phase heights from flow rate measurements
- Two-stage training (pretraining + fine-tuning) is essential for performance, with pretraining on synthetic data providing a crucial physical prior
- PINN ensemble shows better extrapolation capability compared to purely data-driven models when flow rates exceed training bounds
- A simple neural network can predict the maximum dense-packed zone height at the separator end from the estimated average height

## Why This Works (Mechanism)

### Mechanism 1: Transfer Learning from Synthetic Data
Pretraining on synthetic data from a low-fidelity model establishes baseline network weights and aligns the PINN with the dynamics before scarce experimental data is introduced. This two-stage training approach enables the model to learn effective dynamics from limited real data by leveraging the approximate physical laws captured in the synthetic data.

### Mechanism 2: Physics Constraints in Loss Function
Embedding only volume balance equations in the loss function constrains the solution space sufficiently to improve extrapolation without incurring computational cost of complex submodels. The PINN minimizes a physics loss term that penalizes violations of mass conservation, preventing physically impossible state predictions that commonly occur in purely data-driven models.

### Mechanism 3: Ensemble-Based Process Noise Estimation
Deriving process noise covariance from the spread of a PINN ensemble provides a more robust state estimate than static noise assumptions. By calculating the covariance of 40 independently trained PINN predictions, the EKF dynamically adjusts trust in the model based on ensemble agreement, reducing model trust when uncertainty is high.

## Foundational Learning

- **Physics-Informed Neural Networks (PINNs)**
  - Why needed: Standard neural networks fail to respect conservation laws when data is scarce
  - Quick check: Can you write the MSE_physics term for a simple volume balance dh/dt = Qin - Qout?

- **Extended Kalman Filter (EKF) with Automatic Differentiation**
  - Why needed: EKF requires Jacobian of the model, which is computationally expensive for complex models but free for neural networks via automatic differentiation
  - Quick check: Why is the Jacobian F_k = ∂f/∂x required in the EKF prediction step?

- **Transfer Learning (Two-Stage Training)**
  - Why needed: Experimental data in chemical engineering is often scarce and noisy
  - Quick check: Why must the learning rate typically be reduced during the fine-tuning (second) stage compared to the pretraining stage?

## Architecture Onboarding

- **Component map:** Time (t), Initial States (hHP(0), hDP(0)), Control (Qin) -> Feedforward NN (2 hidden layers, 32 nodes, tanh activation) -> Predicted States (hHP, hDP), Measurements (Qbot, Qtop), Immeasurable Flows (Qc, Qs)

- **Critical path:**
  1. Generate synthetic data (Dsim) using the mechanistic model
  2. Pretrain PINN on synthetic data + Physics equations
  3. Fine-tune PINN on experimental data (reduced learning rate)
  4. Initialize EKF with ensemble of trained PINNs
  5. Run EKF loop: Predict → Calculate Ensemble Covariance (Wk) → Update State

- **Design tradeoffs:**
  - Architecture: Shallow network (2 layers) prevents overfitting on small datasets and ensures stable gradients for physics loss
  - Physics Inclusion: Complex submodels excluded to maintain computational tractability, trading physical completeness for training speed
  - Observation: Model assumes band-shaped zone rather than spatial distribution, requiring secondary NN for max height prediction

- **Failure signatures:**
  - Mechanistic Model: Predicts "flooding" prematurely; diverges from experimental trajectory
  - Vanilla NN (VNN): "Overshoots" DPZ height predictions; fails to extrapolate beyond training bounds
  - PINN (No Pretraining): Poor accuracy; highlights necessity of two-stage transfer learning

- **First 3 experiments:**
  1. Forward Simulation (Open Loop): Initialize PINN at t=0 with known experimental state; step forward in time without measurement corrections
  2. Interpolation Test (Closed Loop): Run PINN-EKF on trajectory with flow rates within training range
  3. Extrapolation Test (Stress Test): Run PINN-EKF on trajectory with flow rates (0.75-2.25) exceeding training bounds

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the framework be extended to model non-uniform, spatially distributed dense-packed zone (DPZ) geometries rather than relying on the current band-shaped (lumped) assumption?
- Basis: Conclusion states extending to handle non-uniform DPZ geometries is future work to improve predictive capability
- Why unresolved: Current PINN uses lumped model predicting average height, lacking spatial resolution to capture axial variations
- What evidence would resolve it: Modified PINN-EKF framework predicting phase heights as function of axial position, validated against experimental wedge-shaped DPZ profiles

### Open Question 2
- Question: How reliably can the PINN-based soft sensor detect and classify separator flooding behavior in absence of training data near flooding point?
- Basis: Conclusion notes experimental dataset includes no trajectories close to flooding
- Why unresolved: Safety constraints prevented training/testing on trajectories approaching critical flooding level
- What evidence would resolve it: Experimental validation of estimator performance on trajectories specifically designed to approach/reach flooding point without system damage

### Open Question 3
- Question: Can accuracy of phase height estimation be improved by modifying PINN architecture to enable filtering of immeasurable internal flows (coalescence and sedimentation rates)?
- Basis: Conclusion acknowledges filtering immeasurable internal flows could improve accuracy but is not possible with current PINN architecture
- Why unresolved: Current architecture prevents computation of Jacobians with respect to internal flows Qc and Qs
- What evidence would resolve it: Redesigned PINN architecture allowing joint estimation of phase heights and internal flows, resulting in statistically lower estimation errors

### Open Question 4
- Question: Can the modeling approach be extended to quantify incomplete separation by accounting for entrained droplets in outlet streams?
- Basis: Conclusion lists investigation of entrained droplets in outlets as future work
- Why unresolved: Current model assumes outlet streams are free of entrained droplets
- What evidence would resolve it: Integration of droplet entrainment physics into mechanistic model and PINN, validated by experimental droplet concentration measurements

## Limitations
- Core physics assumption (volume balance only) may be insufficient for regimes dominated by rapid coalescence or sedimentation dynamics
- Success of transfer learning depends on mechanistic model capturing essential system topology, but no quantitative similarity metric provided
- Model assumes lumped phase behavior and cannot resolve spatial gradients within separator, requiring secondary model for end-point heights

## Confidence

- **High Confidence:** PINN ensemble superiority over VNN in interpolation tasks; EKF convergence with adaptive noise covariance; basic volume balance constraints preventing unphysical states
- **Medium Confidence:** Extrapolation performance beyond training bounds; two-stage training necessity; PINN generalization to unseen flow rate ranges
- **Low Confidence:** Long-term stability of EKF with ensemble-based noise; robustness to significant model structural errors in pretraining; optimal ensemble size determination

## Next Checks
1. Perform sensitivity analysis on ensemble size (5, 10, 20, 40) to quantify trade-off between computational cost and state estimation accuracy
2. Test PINN-EKF performance on synthetic data with known structural model errors in pretraining mechanistic model to assess robustness
3. Compare EKF performance using ensemble covariance vs. static diagonal process noise matrices across multiple experimental trajectories