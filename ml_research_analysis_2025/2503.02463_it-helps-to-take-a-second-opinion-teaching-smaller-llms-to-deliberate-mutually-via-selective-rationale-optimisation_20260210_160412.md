---
ver: rpa2
title: 'It Helps to Take a Second Opinion: Teaching Smaller LLMs to Deliberate Mutually
  via Selective Rationale Optimisation'
arxiv_id: '2503.02463'
source_url: https://arxiv.org/abs/2503.02463
tags:
- rationale
- rationales
- coalition
- generate
- variants
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving smaller language
  models' (SLMs) reasoning abilities without relying on external large language models
  (LLMs) for supervision. The authors propose COALITION, a trainable framework that
  enables interaction between two distinct variants of the same SLM to generate and
  refine rationales optimized for end-task performance.
---

# It Helps to Take a Second Opinion: Teaching Smaller LLMs to Deliberate Mutually via Selective Rationale Optimisation

## Quick Facts
- arXiv ID: 2503.02463
- Source URL: https://arxiv.org/abs/2503.02463
- Reference count: 40
- Primary result: COALITION achieves up to 5% absolute gains on five datasets (GSM8K, PIQA, WinoGrande, CSQA, HellaSwag) using two variants of smaller LLMs to generate and refine rationales without external LLM supervision.

## Executive Summary
This paper addresses the challenge of improving smaller language models' (SLMs) reasoning abilities without relying on external large language models (LLMs) for supervision. The authors propose COALITION, a trainable framework that enables interaction between two distinct variants of the same SLM to generate and refine rationales optimized for end-task performance. The method uses selective rationale optimization to prefer generating candidates that maximize the likelihood of producing the ground-truth answer. During inference, a controller selects the appropriate variant for generating and refining rationales. COALITION demonstrates significant improvements over several baselines, achieving up to 5% absolute gains on five datasets covering mathematics problem-solving, natural language inference, and commonsense reasoning.

## Method Summary
COALITION creates two instruction-fine-tuned variants (LV1, LV2) of the same base SLM using different data splits. During training, each variant generates rationales that are refined by both itself and the other variant, creating cross- and self-refinement pairs. A utility score is assigned to each rationale candidate by estimating the likelihood of generating the ground-truth answer using an IFT model. Direct Preference Optimization (DPO) trains each variant to prefer rationales with higher utility scores. Sample filtration ensures only beneficial rationales are retained for training. During inference, a controller (deberta-v3-large) dynamically selects which variant to use for generation and refinement based on the input.

## Key Results
- COALITION achieves up to 5% absolute gains over baselines on five benchmark datasets
- Cross-communication between variants outperforms single-model self-refinement across all tasks
- Controller-based selection improves performance by ~1-2% over fixed routing strategies
- Consistent effectiveness across different model families and scales (4B to 14B parameters)

## Why This Works (Mechanism)

### Mechanism 1: Cross-Communication Between Diverse Variants
- Claim: Cross-communication between two SLM variants trained on separate data splits yields better rationales than single-model self-refinement.
- Mechanism: Two variants (LV1, LV2) are created by instruction fine-tuning the same base model on different data splits. During training, each variant generates rationales that are refined by both itself (self-refinement) and the other variant (cross-refinement). DPO trains each variant to prefer rationales with higher utility scores.
- Core assumption: Variants trained on different data splits develop sufficiently distinct reasoning patterns to provide complementary critiques.
- Evidence anchors:
  - [abstract] "Our ablation studies reveal that cross-communication between the two variants performs better than using the single model to self-refine the rationales."
  - [section] Table 4 shows cross-refinement (LV1→LV2, LV2→LV1) consistently outperforms self-refinement across all five datasets.
  - [corpus] Related work (Madaan et al., 2023a; Huang et al., 2024) shows LLMs struggle to revise their own outputs without external feedback—COALITION addresses this by creating internal diversity through split training.
- Break condition: If variants converge to similar behaviors (e.g., due to insufficient data split diversity or excessive DPO iterations), cross-communication loses advantage over self-refinement.

### Mechanism 2: Likelihood-Based Utility Scoring Without External Models
- Claim: Using the IFT model's conditional likelihood of generating the ground-truth answer as a rationale utility score enables effective preference optimization without external supervision.
- Mechanism: For each generated/refined rationale, compute πθIFT(AT | [instruction; rationale]). Rationales with higher likelihood are selected as "winners" for DPO training. Sample filtration retains only samples where the winning rationale improves likelihood over no-rationale baseline.
- Core assumption: Rationales that increase the model's confidence (likelihood) in generating the correct answer are genuinely helpful for reasoning, not just stylistically preferred.
- Evidence anchors:
  - [section] "A utility score is assigned to each rationale candidate by estimating the likelihood of generating the GT answer by the IFT model conditioned on the rationale in input" (§3.2).
  - [section] Table 5 shows removing likelihood-based selection (using LLM-as-a-judge instead) causes ~3-6% accuracy drops across tasks.
  - [corpus] Limited corpus evidence directly validates likelihood-as-utility; the approach builds on Wang et al. (2024c) but lacks independent validation for SLM-scale models.
- Break condition: If the IFT model's likelihood is poorly calibrated or systematically biased toward certain rationale styles, utility scores may not correlate with actual reasoning quality.

### Mechanism 3: Controller-Based Dynamic Variant Selection
- Claim: A trained controller that dynamically selects which variant to use for generation and refinement improves performance over fixed routing strategies.
- Mechanism: A small encoder-only model (deberta-v3-large) is trained on preference data from DPO to predict which variant should handle generation and refinement for each input. During inference, the controller routes samples to the appropriate variant at each stage.
- Core assumption: The controller can learn to identify input characteristics that predict which variant's reasoning style is better suited for the task.
- Evidence anchors:
  - [abstract] "During inference, COALITION employs a controller to select the suitable variant for generating and refining the rationales."
  - [section] Table 4 shows controller-based selection achieves 81.06% on GSM8K vs. 79.94% for best fixed cross-refinement strategy.
  - [section] Figure 4 shows controller routes ~65-75% of samples to cross-refinement, ~15-25% to self-refinement, and ~5-10% to generation-only.
- Break condition: If the controller overfits to training distribution or variant behaviors shift during DPO, selection accuracy degrades.

## Foundational Learning

- **Multi-Mode Instruction Fine-Tuning (IFT)**
  - Why needed here: The base model must learn four distinct modes: (1) instruction→rationale, (2) [instruction; rough rationale]→refined rationale, (3) [instruction; rationale]→answer, (4) instruction→answer. Without this, the variants cannot participate in the generate-refine loop.
  - Quick check question: Can you implement the four loss functions (LI→R, L[I;R′]→R, L[I;R]→A, LI→A) and explain why each is necessary?

- **Direct Preference Optimization (DPO)**
  - Why needed here: COALITION uses DPO (not RLHF) to train variants to prefer high-utility rationales. Understanding the DPO loss formulation (β parameter, reference model role) is essential for implementation.
  - Quick check question: Why does COALITION use the previous iteration's variant as the reference model instead of the original IFT model?

- **Likelihood as Quality Proxy**
  - Why needed here: The core innovation is using P(ground_truth_answer | rationale) as a quality metric without external models. This requires understanding how to efficiently compute conditional likelihood and interpret it as utility.
  - Quick check question: How would you compute the utility score for a candidate rationale, and what does a high score actually mean about the rationale's quality?

## Architecture Onboarding

- **Component map:**
  Base SLM (e.g., Llama3-8B) → Two IFT variants (LV1, LV2) via split data training → IFT model (MIFT) → Utility scorer → DPO training loop → Preference pairs → Controller (deberta-v3-large) → Variant selector for inference routing → Sample filter

- **Critical path:**
  1. Prepare IFT data (CoT-Collection + task samples), split into two partitions
  2. Train LV1 and LV2 on separate splits with four-mode IFT
  3. For each task sample: (a) generate rationales with both variants, (b) cross/self-refine, (c) score all candidates with MIFT, (d) create preference pairs, (e) apply sample filtration
  4. Run DPO for 2 iterations, 10 epochs each, updating variants separately
  5. Train controller (DeBERTa-v3-large) on preference labels from DPO phase
  6. Inference: controller selects variant for generate → controller selects variant for refine → answer from final rationale

- **Design tradeoffs:**
  - **Variant count**: 2 variants balance performance vs. complexity; 3 variants improve accuracy further (~2% gain per Table 11) but increase training cost
  - **DPO iterations**: Paper uses 2 iterations; more iterations risk overfitting to likelihood-based utility
  - **Sample filtration**: Skipping filtration (Table 5, row 3) causes ~5-6% drop; conservative approach recommended
  - **Controller vs. fixed routing**: Controller adds inference overhead (~15ms for deberta-v3-large) but provides ~1-2% gain over best fixed strategy

- **Failure signatures:**
  - **Low diversity between variants**: If LV1 and LV2 generate similar rationales (BLEU-diversity < 0.5), cross-communication provides no benefit. Monitor diversity metric during training.
  - **Controller overfitting**: If controller accuracy on validation set plateaus early or shows high variance, it may be overfitting to training distribution. Reduce controller training epochs.
  - **Likelihood calibration issues**: If winning rationales selected by likelihood score are consistently judged poor by humans (alignment < 70%), the utility metric may be miscalibrated. Consider hybrid scoring approaches.
  - **Self-refinement collapse**: If ablation shows self-refinement matching cross-refinement performance, variants have likely converged. Reduce DPO learning rate or increase data split randomness.

- **First 3 experiments:**
  1. **Variant creation ablation**: Train both variants on the *same* IFT data split (not separate splits). Compare cross-refinement performance to original setup. Expect: reduced diversity, lower accuracy gains.
  2. **Scoring method comparison**: Replace likelihood-based utility with GPT-4o-as-judge (paper's baseline) or human evaluation on 100-sample subset. Compare correlation with final task accuracy.
  3. **Controller routing analysis**: Log controller decisions during inference. Analyze: (a) which variant is selected for which task types, (b) whether controller prefers self vs. cross-refinement for specific samples, (c) correlation between controller confidence and accuracy. This validates whether the controller learns meaningful routing patterns or just defaults to cross-refinement.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of LLM variants for COALITION, and does performance scale predictably with additional variants?
- Basis in paper: [explicit] The conclusion states: "In future work, it is worthwhile to explore the impact of adding more variants on the performance." Additionally, Appendix A.9 shows preliminary results with 3 variants achieving ~2% average gains over 2 variants, but further exploration is deferred.
- Why unresolved: Only 2-3 variants have been tested; the relationship between number of variants, diversity of rationales, and final performance remains uncharacterized.
- What evidence would resolve it: Systematic experiments with 4, 5, and more variants across the same benchmark tasks, measuring both accuracy gains and computational overhead.

### Open Question 2
- Question: Can incorporating domain-specialized model variants further improve rationale quality and task performance?
- Basis in paper: [explicit] The conclusion proposes: "incorporating domain-specific models (specialized variants) to generate high-quality rationales" as future work.
- Why unresolved: Current variants are created by training the same base model on different data splits,