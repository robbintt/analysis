---
ver: rpa2
title: 'WiSE-OD: Benchmarking Robustness in Infrared Object Detection'
arxiv_id: '2507.18925'
source_url: https://arxiv.org/abs/2507.18925
tags:
- zero-shot
- corruption
- severity
- wise-odzs
- blur
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of robust object detection in
  infrared (IR) imagery under distribution shifts, which is critical for low-light
  and nighttime applications. The authors introduce LLVIP-C and FLIR-C, two cross-modality
  out-of-distribution benchmarks built by applying common corruptions to standard
  IR datasets, to evaluate cross-modality OOD performance.
---

# WiSE-OD: Benchmarking Robustness in Infrared Object Detection

## Quick Facts
- arXiv ID: 2507.18925
- Source URL: https://arxiv.org/abs/2507.18925
- Reference count: 27
- Primary result: WiSE-OD improves robustness in IR object detection under distribution shifts by combining zero-shot and fine-tuned weights without extra training or inference costs.

## Executive Summary
This paper addresses the challenge of robust object detection in infrared (IR) imagery under distribution shifts, which is critical for low-light and nighttime applications. The authors introduce LLVIP-C and FLIR-C, two cross-modality out-of-distribution benchmarks built by applying common corruptions to standard IR datasets, to evaluate cross-modality OOD performance. To improve robustness, they propose WiSE-OD, a weight-space ensembling method that combines zero-shot and fine-tuned weights without additional training or inference costs. Evaluated using four RGB-pretrained detectors (Faster R-CNN, FCOS, RetinaNet, YOLOv8) and two robust baselines, WiSE-OD significantly improves robustness across modalities and to corruption in synthetic and real-world distribution shifts. On the LLVIP-C benchmark, WiSE-OD ZS improved mPC by 18.68 over fine-tuning and by 4.12 over linear probing for Faster R-CNN. The method also performs well under real-world shifts such as night, fog, and rain in the M3FD dataset, demonstrating its effectiveness and generalizability.

## Method Summary
The paper proposes WiSE-OD, a weight-space ensembling method that improves robustness in infrared object detection without additional training or inference costs. The method interpolates weights between a zero-shot RGB-pretrained model and its IR fine-tuned counterpart using a convex combination: θ_ens = (1 - λ)θ_ZS + λθ_FT, with λ typically set to 0.5. The authors also introduce LLVIP-C and FLIR-C, two cross-modality OOD benchmarks created by applying 14 common corruptions from ImageNet-C/COCO-C to standard IR datasets. These benchmarks are used to evaluate the method alongside real-world shifts in the M3FD dataset. The approach is tested across four detector architectures (Faster R-CNN, FCOS, RetinaNet, YOLOv8) and shows consistent improvements in mean Performance under Corruption (mPC) while maintaining in-domain accuracy.

## Key Results
- WiSE-OD ZS improved mPC by 18.68 over fine-tuning and by 4.12 over linear probing for Faster R-CNN on LLVIP-C.
- The method achieves 75.08 mPC on LLVIP-C with in-domain AP50 of 96.06 for WiSE-OD ZS.
- Consistent improvements are observed on real-world M3FD dataset across fog, night, and rain conditions.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Weight-space ensembling between a zero-shot (ZS) RGB-pretrained model and its infrared (IR) fine-tuned (FT) counterpart improves robustness under distribution shifts while maintaining, or even improving, in-domain performance without additional training/inference cost.
- **Mechanism:** WiSE-OD interpolates the parameter vectors of a model trained on a large-scale RGB dataset (COCO) and the same architecture fine-tuned on IR data using a convex combination: θ_ens = (1 - λ)θ_ZS + λθ_FT, where λ ∈ [0, 1]. The ZS model retains generalization capabilities learned from diverse RGB data, while the FT model specializes for the IR modality. Their combination preserves complementary robustness features (e.g., to corruption types like noise, fog, blur) that might be lost or degraded during standard fine-tuning, which often overfits to the specifics of the IR training set and forgets broader features.
- **Core assumption:** The features learned by the RGB-pretrained model (zero-shot) and the IR-finetuned model lie in a region of the loss landscape where linearly interpolating their weights yields a model that performs well on both in-distribution (ID) IR data and out-of-distribution (OOD) corrupted IR data. This implies that the "robust features" from the ZS model and the "domain-specific features" from the FT model are not mutually exclusive and can be effectively combined linearly in weight space.
- **Evidence anchors:**
  - [abstract] "WiSE-OD... improves robustness across modalities and to corruption in synthetic and real-world distribution shifts without any additional training or inference costs."
  - [section 5] "The resulting interpolated model inherits both the broad generalization of large-scale COCO pre-training and the modality-specific accuracy of IR fine-tuning..."
  - [section 6.3] "The WiSE-OD ZS and WiSE-OD LP were able to outperform the others with in-domain of 96.06 and 96.24, respectively, and for the mPC, WiSE-OD LP achieves 75.83 and WiSE-OD ZS 75.08."
  - [corpus] No direct corpus evidence for this specific *weight-space ensembling* mechanism in cross-spectral detection.
- **Break condition:** The mechanism may fail if the training dynamics for the two models (ZS and FT) lead to their optima being in distinct basins of the loss landscape, where linear interpolation creates a model with degraded performance on all tasks. This is more likely if the FT model is trained for an excessive number of epochs, causing significant weight divergence.

### Mechanism 2
- **Claim:** The default interpolation coefficient λ = 0.5 provides a robust and effective trade-off for a wide range of corruption types and severities, eliminating the need for target-data hyperparameter tuning which is impractical for real-world deployment.
- **Mechanism:** The paper empirically demonstrates that a fixed λ = 0.5 consistently yields strong results across multiple detector architectures (Faster R-CNN, FCOS, RetinaNet) and corruption scenarios (synthetic corruptions, real-world fog/night/rain). This works because it balances the contribution from the robust ZS weights and the specialized FT weights roughly equally, capturing a broad set of features.
- **Core assumption:** The optimal balance between robustness (from ZS) and domain-specific accuracy (from FT) is relatively stable across different types of distribution shifts and does not require per-scenario adaptation to provide significant benefit.
- **Evidence anchors:**
  - [section 6.2] "...on average, WiSE-OD ZS with λ fixed at 0.5, i.e., equal weighting of Zero-Shot and FT, outperforms all other baselines without the need to tune any hyperparameters."
  - [section 6.7] "We observe that λ = 0.5 consistently provides a favorable trade-off... Notably, λ = 0.5 performs best under heavy corruptions such as Gaussian noise, Fog, and Brightness shifts..."
  - [corpus] No direct corpus evidence for this specific hyperparameter claim.
- **Break condition:** The fixed λ = 0.5 assumption may break in scenarios where one of the parent models is significantly inferior. For instance, if the fine-tuned model severely overfits and performs very poorly on OOD data, a 50-50 blend may inherit too much weakness.

### Mechanism 3
- **Claim:** Creating corrupted IR benchmarks (LLVIP-C, FLIR-C) by applying standard ImageNet-C/COCO-C style corruptions to existing IR datasets provides a necessary and reproducible way to evaluate detector robustness, revealing that standard fine-tuning severely degrades OOD performance.
- **Mechanism:** The authors generated new datasets by taking IR images from LLVIP and FLIR and applying a suite of 14 common corruptions (e.g., Gaussian noise, fog, frost, zoom blur) at varying severity levels. The results quantitatively demonstrate a fundamental tradeoff: while fine-tuning on clean IR data maximizes in-domain accuracy, it often leads to a collapse in performance under these corruptions, thus motivating the need for methods like WiSE-OD.
- **Core assumption:** The applied synthetic corruptions are representative enough of real-world distribution shifts (e.g., sensor noise, weather) to serve as a valid proxy for robustness testing. The paper validates this assumption by testing on the real-world M3FD dataset containing natural fog, night, and rain shifts.
- **Evidence anchors:**
  - [abstract] "To address this, we introduce LLVIP-C and FLIR-C, two cross-modality out-of-distribution (OOD) benchmarks built by applying corruptions to standard IR datasets."
  - [section 6.6] "...the consistent improvements on M3FD demonstrate that our approach is not limited to synthetic benchmarks (LLVIP-C, FLIR-C), but generalizes to real-world distribution shifts."
  - [corpus] The corpus paper "RASMD" similarly argues for the need for multi-spectral datasets to handle adverse conditions, supporting the general motivation.
- **Break condition:** This mechanism's utility breaks if the synthetic corruptions do not correlate with real-world failure modes, meaning a model performing well on LLVIP-C might still fail in actual foggy night conditions. A failure would be if a method like WiSE-OD showed large gains on LLVIP-C/FLIR-C but no improvement on real-world OOD datasets like M3FD.

## Foundational Learning

- **Concept: Weight-Space Ensembling (WiSE-FT/WiSE-OD)**
  - **Why needed here:** This is the core contribution of the paper. Understanding it is essential to grasping how the proposed method improves robustness without extra inference cost.
  - **Quick check question:** How does WiSE-OD combine the zero-shot and fine-tuned models, and what is the primary advantage over ensembling their predictions (output-space ensembling)?

- **Concept: Out-of-Distribution (OOD) Robustness vs. In-Domain (ID) Performance**
  - **Why needed here:** The paper's central problem is the tradeoff between ID accuracy and OOD robustness. Models fine-tuned on IR data excel at ID tasks but become brittle to distribution shifts.
  - **Quick check question:** Based on the paper, why does standard fine-tuning often improve in-domain AP50 but significantly reduce mean Performance under Corruption (mPC)?

- **Concept: Cross-Modality Transfer Learning (RGB-to-IR)**
  - **Why needed here:** The entire experimental setup is predicated on adapting models pre-trained on large-scale RGB datasets (e.g., COCO) to the IR modality where labeled data is scarce. This process introduces a "modality gap" that complicates robust fine-tuning.
  - **Quick check question:** Why do the authors start with RGB-pretrained weights for their IR object detectors, and what specific challenge does this create for robustness?

## Architecture Onboarding

- **Component map:**
  1. Base Object Detector: A standard architecture (e.g., Faster R-CNN, FCOS, RetinaNet, YOLOv8) initialized with weights pre-trained on the COCO dataset.
  2. Zero-Shot (ZS) Model: The base detector without any fine-tuning on IR data.
  3. Fine-Tuned (FT) Model: A copy of the base detector whose parameters are updated by training on a specific IR dataset (e.g., LLVIP, FLIR).
  4. Linear Probing (LP) Model: A copy of the base detector where the backbone is frozen and only the detection head is trained on the IR dataset.
  5. WiSE-OD Module: A simple operation that takes two sets of weights (e.g., θ_ZS and θ_FT) and a mixing coefficient λ, and produces new interpolated weights.

- **Critical path:**
  1. Initialization: Obtain a strong base detector pre-trained on RGB data.
  2. Fine-Tuning: Train the base detector on the target IR dataset to produce the θ_FT weights. Optionally, train the LP model.
  3. Weight Interpolation: Use the WiSE-OD formula to merge the pre-trained weights (θ_ZS) with the fine-tuned weights (θ_FT or θ_LP) using a chosen λ (default 0.5).
  4. Inference: Load the ensembled weights into the model architecture and run inference on IR images.

- **Design tradeoffs:**
  - Simplicity vs. Adaptivity: The method uses a fixed λ for simplicity and to avoid tuning on target data. The tradeoff is that a more complex, adaptive λ might yield marginally better results for specific corruption types but adds complexity.
  - ZS vs. LP Base: WiSE-OD_LP often shows slightly better results, but requires training the LP model. WiSE-OD_ZS is truly training-free on the target data side.
  - Storage Overhead: The method requires storing two full sets of model weights (ZS and FT), doubling the storage footprint, although inference still only requires one model.

- **Failure signatures:**
  - Collapse under extreme corruption: Both FT and ZS models may fail under very strong corruptions (e.g., severe snow, low brightness, contrast). The ensemble inherits these weaknesses.
  - Incompatibility: The method assumes the architectures of the ZS and FT models are identical. It will fail if applied to models with different layer structures or dimensions.

- **First 3 experiments:**
  1. Benchmark a single detector: Take a Faster R-CNN model pre-trained on COCO. Evaluate it on LLVIP-C at severity 5 for key corruptions (e.g., Gaussian Noise, Fog). Compare ZS, FT, and WiSE-OD_ZS (λ=0.5) performance to validate the core OOD robustness claim.
  2. Lambda ablation: For the same setup, vary λ from 0.0 to 1.0 and plot the AP50 curve. This demonstrates the tradeoff space and confirms that λ=0.5 is a reasonable default.
  3. Real-world validation: Fine-tune the base detector on the 'day' split of the M3FD dataset. Evaluate the ZS, FT, and WiSE-OD_ZS models on the 'fog', 'night', and 'rain' splits to test generalization beyond synthetic corruptions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can an adaptive weight-space ensembling strategy predict the optimal mixing coefficient λ per image or corruption type using lightweight signals (e.g., confidence or entropy)?
- Basis in paper: [explicit] The "Future work" section proposes "adaptive weight-space ensembling: predict λ per image or corruption using lightweight signals."
- Why unresolved: The current study fixes λ at 0.5 to avoid target-data tuning, which the authors note leaves corruption-specific gains unrealized and is not uniformly optimal.
- What evidence would resolve it: A mechanism that dynamically adjusts λ and achieves higher mean Performance under Corruption (mPC) than the fixed baseline on LLVIP-C or FLIR-C without validation set tuning.

### Open Question 2
- Question: Does integrating WiSE-OD with domain-generalization techniques improve performance on unseen IR conditions without target labels?
- Basis in paper: [explicit] The "Future work" section suggests integrating WiSE-OD "with domain-generalization to better handle unseen IR conditions without target labels."
- Why unresolved: While WiSE-OD handles synthetic corruptions and specific real-world shifts (fog, night), its synergy with explicit domain-generalization methods for entirely novel environments remains untested.
- What evidence would resolve it: Demonstrated improvements in detection accuracy on novel, unlabeled IR datasets when WiSE-OD is combined with domain-generalization algorithms compared to WiSE-OD alone.

### Open Question 3
- Question: Can the WiSE-OD framework be effectively generalized to other sensor modalities, such as depth or multispectral imaging?
- Basis in paper: [explicit] The "Future work" section advises evaluating "additional sensors (depth, multispectral) to assess generality beyond IR."
- Why unresolved: The paper focuses exclusively on the RGB-to-IR modality gap; it is unknown if the linear weight interpolation behaves similarly or maintains robustness when transferring learning across different spectral domains.
- What evidence would resolve it: Successful application of WiSE-OD on depth or multispectral object detection benchmarks, showing consistent robustness gains over standard fine-tuning.

## Limitations

- The paper does not provide full hyperparameter details (learning rates, batch sizes, optimizer settings) for the fine-tuning and linear probing procedures, which are critical for exact reproduction.
- The mechanism's effectiveness is heavily tied to the quality and representativeness of the synthetic corruptions. While validated on M3FD, the assumption that ImageNet-C/COCO-C style corruptions generalize to real-world IR degradation patterns remains untested across broader scenarios.
- The storage overhead of maintaining two full weight sets (ZS and FT) is acknowledged but not quantified in terms of practical deployment constraints.

## Confidence

- **High Confidence:** The empirical demonstration of WiSE-OD's effectiveness on synthetic benchmarks (LLVIP-C, FLIR-C) and real-world M3FD dataset is well-supported with specific AP50 and mPC metrics.
- **Medium Confidence:** The claim that λ=0.5 is a universally good default across corruption types and detector architectures is supported by ablation studies but may not hold for all edge cases.
- **Medium Confidence:** The weight-space ensembling mechanism's theoretical foundation is sound, but the paper does not provide evidence that the ZS and FT weight optima lie in the same basin of the loss landscape, which is a core assumption.

## Next Checks

1. Reproduce the LLVIP-C benchmark: Create the corrupted dataset with severity 5 and evaluate the base detectors (ZS, FT, LP) to confirm the reported degradation patterns and the effectiveness of WiSE-OD.
2. Test on a new real-world OOD dataset: Apply WiSE-OD to a different IR dataset (e.g., KAIST or VEDAI) with natural weather/lighting variations to assess generalization beyond the M3FD benchmark.
3. Storage and latency analysis: Quantify the memory overhead of storing dual weight sets and measure any inference latency impact compared to a single fine-tuned model.