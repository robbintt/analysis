---
ver: rpa2
title: 'Synthetic Forgetting without Access: A Few-shot Zero-glance Framework for
  Machine Unlearning'
arxiv_id: '2511.13116'
source_url: https://arxiv.org/abs/2511.13116
tags:
- data
- unlearning
- classes
- retained
- gfoes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GFOES addresses machine unlearning under the few-shot zero-glance
  setting, where only minimal retained data is available and forget data is entirely
  inaccessible. The method introduces Optimal Erasure Samples (OES) generated by a
  Generative Feedback Network (GFN) to induce forgetting without accessing the target
  data, and employs a two-phase fine-tuning strategy to balance aggressive forgetting
  with utility preservation.
---

# Synthetic Forgetting without Access: A Few-shot Zero-glance Framework for Machine Unlearning

## Quick Facts
- **arXiv ID:** 2511.13116
- **Source URL:** https://arxiv.org/abs/2511.13116
- **Reference count:** 37
- **Primary result:** Achieves complete forgetting (ADf = 0) on CIFAR-10/100 and Fashion-MNIST while maintaining high retained accuracy (ADr up to 89.23%) under few-shot zero-glance constraints.

## Executive Summary
GFOES addresses machine unlearning under the challenging few-shot zero-glance setting where forget data is entirely inaccessible and only minimal retained data is available. The method introduces Optimal Erasure Samples (OES) generated by a Generative Feedback Network (GFN) to induce forgetting without accessing the target data. A two-phase fine-tuning strategy enables aggressive forgetting in the first phase while preserving utility through selective recovery in the second phase. Experiments demonstrate GFOES outperforms state-of-the-art methods with significantly lower computational costs.

## Method Summary
GFOES employs a Generative Feedback Network to synthesize Optimal Erasure Samples that maximize cross-entropy loss on forgotten classes when labeled with target class labels. The method uses a stabilized reciprocal objective formulation to ensure stable optimization, replacing standard subtractive objectives with λ/G + (1-λ)F to bound gradient magnitudes. A two-phase fine-tuning approach first applies aggressive updates to erase forgotten class representations, then selectively recovers retained class boundaries using only the small retained data subset. The framework is trained on CIFAR-10, CIFAR-100, and Fashion-MNIST with AllCNN, ResNet-18, and ResNet-50 architectures respectively.

## Key Results
- Achieves complete forgetting (ADf = 0) on CIFAR-10, CIFAR-100, and Fashion-MNIST
- Maintains high retained accuracy (ADr up to 89.23%) despite only 5-20% retained data
- Outperforms state-of-the-art methods with significantly lower time costs
- Effectively removes class-specific representations at both logit and feature levels

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Synthetic adversarial samples can disrupt class-specific representations without accessing the original forget data.
- **Mechanism:** The Generative Feedback Network (GFN) optimizes synthetic "Optimal Erasure Samples" (OES) to maximize cross-entropy loss on forgotten classes when labeled with target class labels. These adversarial-like inputs, paired with the correct labels, mislead the model into associating irrelevant features with the forgotten classes, disrupting decision boundaries for target classes while preserving retained class structure.
- **Core assumption:** The model's response to adversarially constructed inputs can effectively overwrite learned feature representations for the forgotten classes.
- **Evidence anchors:**
  - [abstract]: "GFN synthesises Optimal Erasure Samples (OES), which induce high loss on target classes, enabling the model to forget class-specific knowledge without access to the original forget data."
  - [section 3.2]: "Instead of approximating the true data distribution, we deliberately generate synthetic samples that deviate from it. These adversarial-like inputs, paired with the target labels, mislead the model into associating irrelevant features with the forgotten classes."
  - [corpus]: Weak direct evidence; related papers (IMU, OPC) focus on influence functions or feature-level forgetting but do not address synthetic sample generation under zero-glance constraints.
- **Break condition:** If generated samples fail to produce sufficiently high loss on the original model, or if adversarial perturbations do not generalize across the target class distribution.

### Mechanism 2
- **Claim:** The reciprocal formulation of the forgetting loss stabilizes optimization and ensures convergence.
- **Mechanism:** Replacing the standard subtractive objective (F - G) with a stabilized form (λ/G + (1-λ)F) bounds the gradient magnitude while preserving direction. As the forgetting loss G increases, the gradient contribution from the reciprocal term (1/G²) decreases, preventing explosive updates while maintaining optimization pressure toward forgetting.
- **Core assumption:** Cross-entropy losses remain bounded away from zero (G(φ) ≥ ε > 0) during training.
- **Evidence anchors:**
  - [section 3.3]: "This formulation mitigates the influence of large values in L_max while retaining its gradient direction, as shown by its gradient: ∇_φ(1/L_max) = -1/L²_max · ∇_φL_max."
  - [appendix B.3-B.4]: Formal convergence theorem proves O(1/T) convergence rate under L-smoothness and bounded domain assumptions.
  - [corpus]: No comparable evidence in related papers; this stabilization technique appears unique to GFOES.
- **Break condition:** If loss values approach zero during training, the reciprocal term may become unstable; the bounded domain assumption must hold.

### Mechanism 3
- **Claim:** Temporal separation of aggressive erasure and gentle recovery prevents catastrophic damage to retained representations.
- **Mechanism:** Phase 1 applies a high learning rate (η_high = 4e-3) on both OES and retained data to broadly overwrite representations. Phase 2 applies a low learning rate (η_low = 4e-4) on retained data only to selectively repair retained class boundaries without reintroducing forgotten knowledge, since forgotten classes are excluded from this phase.
- **Core assumption:** The erasure phase damages but does not completely destroy retained class representations, allowing recovery with the limited retained subset.
- **Evidence anchors:**
  - [abstract]: "two-phase fine-tuning procedure enables aggressive forgetting in the first phase, followed by utility restoration in the second."
  - [section 3.4]: "This strategy is motivated by the observation that aggressive updates are needed to disrupt representations of forgotten classes, yet they risk damaging the structure of retained ones. The second stage serves to repair this collateral damage without reintroducing the erased knowledge."
  - [corpus: OPC paper]: "Existing unlearning methods tend to forget shallowly... while its internal representations remain intact" — supports the need for aggressive representation-level modification.
- **Break condition:** If erasure phase is too aggressive and destroys feature extractor representations beyond recovery with limited retained data; or if recovery phase trains too long and accidentally reintroduces forgotten patterns through unintended transfer.

## Foundational Learning

- **Gradient Ascent for Unlearning:**
  - Why needed here: GFOES relies on maximizing loss on forgotten classes (gradient ascent logic embedded in OES optimization) to remove learned representations, which is conceptually opposite to standard training.
  - Quick check question: Can you explain why maximizing cross-entropy loss on a class would reduce the model's ability to classify that class?

- **Representation vs. Logit-Level Forgetting:**
  - Why needed here: The paper explicitly distinguishes between surface-level accuracy drops (logit) and internal feature-space changes (representation), using t-SNE and GradCAM to verify that forgetting occurs at both levels.
  - Quick check question: If a model achieves 0% accuracy on forgotten classes but the feature extractor still clusters those samples together, has complete unlearning occurred?

- **Adversarial Sample Generation:**
  - Why needed here: OES are adversarial-like inputs optimized to maximize loss on target classes. Understanding how perturbations can be systematically optimized to exploit model vulnerabilities is essential for grasping why OES work.
  - Quick check question: How does optimizing a sample to maximize classification loss differ from standard adversarial attack generation?

## Architecture Onboarding

- **Component map:**
  - GFN Generator (G(z, φ)) -> OES -> Two-Phase Fine-Tuning Loop -> Target Model f(x; θ₀) -> Evaluation Metrics (ADf, ADr, t-SNE, GradCAM)
  - Retained Data Subset D_rs provides utility preservation signal

- **Critical path:**
  1. Pre-train original model f(x; θ₀) on full dataset D
  2. Partition dataset: select forget classes Y_f, sample retained subset D_rs (5-20%)
  3. Train GFN generator to produce OES using stabilized joint objective (Eqs. 8-11)
  4. Erasure phase: Fine-tune model on OES + D_rs with η_high (Eq. 12-13)
  5. Recovery phase: Fine-tune on D_rs only with η_low (Eq. 14)
  6. Evaluate: AD_f (should be 0), AD_r (should be high), feature-level changes via t-SNE/GradCAM

- **Design tradeoffs:**
  - Higher retained data ratio (e.g., 20% vs 5%): Better utility preservation (AD_r increases ~2-3%) but greater storage requirements and reduced privacy compliance margin
  - Larger erasure learning rate: More complete forgetting but higher risk of collateral damage to retained class representations
  - Longer GFN training: Higher-quality OES (more effective forgetting) but increased computational overhead (GFN dominates total time: ~85-95% of GFOES runtime)
  - Single-phase vs two-phase: Single-phase with uniform LR either fails to forget completely (low LR) or damages utility significantly (high LR only)

- **Failure signatures:**
  - AD_f > 0 after unlearning: OES generation ineffective; check GFN convergence, λ_t dynamics, or increase erasure phase epochs
  - AD_r drops significantly (>10-15% from original): Collateral damage too severe; reduce η_high, increase recovery phase epochs, or increase retained data ratio
  - Feature clusters for forgotten classes remain intact in t-SNE despite AD_f = 0: Shallow forgetting (only classification head modified); verify GradCAM shows attention shift away from semantically relevant regions for forgotten class
  - GKT achieves AD_f = 0 but AD_r drops drastically (e.g., to 30% on CIFAR-100): Distillation-based methods fail under complex multi-class scenarios; GFOES should outperform

- **First 3 experiments:**
  1. **Single-class unlearning on CIFAR-10 with 5% retained data:** Establish baseline for AD_f and AD_r; compare against Retrain, UNSIR, and GKT baselines from Table 1. Expected: AD_f = 0, AD_r ≈ 86-87%
  2. **Ablation study varying data composition and learning rate strategy:** Test OES+Dr+Rls (full GFOES) vs OES+Rl vs Dr+Rl to isolate contributions of OES generation and two-phase scheduling (replicate Table 3). Expected: Only full configuration achieves both AD_f = 0 and high AD_r
  3. **Representation-level verification via t-SNE and GradCAM:** Train on CIFAR-10 single-class setting, visualize feature extractor outputs before/after unlearning. Expected: Forgotten class cluster should disperse; GradCAM attention should shift away from semantically meaningful regions for forgotten class (matching Figure 3-4)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the GFOES framework scale efficiently to Large Language Models (LLMs) given the nested optimization requirements?
- **Basis in paper:** [inferred] The Generative Feedback Network (GFN) requires a "one-epoch fine-tuning" of the original model inside the training loop (Eq. 6) to compute the Minimise Branch loss.
- **Why unresolved:** The wall-clock time analysis (Table 2) shows non-linear scaling from CIFAR-10 to CIFAR-100; performing full model updates for every generator iteration would likely be prohibitive for massive LLMs.
- **What evidence would resolve it:** Benchmarking GFOES on a standard LLM unlearning task (e.g., TOFU) with memory and latency profiling.

### Open Question 2
- **Question:** How does class imbalance in the few-shot retained data affect the stability of the recovery phase?
- **Basis in paper:** [inferred] The experimental protocol (Sec C.1) explicitly ensures class balance by down-sampling, avoiding scenarios where retained data is naturally skewed.
- **Why unresolved:** The Recovery Phase relies entirely on the retained set to restore decision boundaries; severe imbalance could cause the model to overfit to majority classes while failing to recover minority class boundaries.
- **What evidence would resolve it:** Evaluation of GFOES on retained datasets with synthetically induced long-tailed distributions.

### Open Question 3
- **Question:** Can the gradient-based OES generation mechanism be adapted for discrete data modalities like text?
- **Basis in paper:** [inferred] The method optimizes synthetic samples via continuous gradients (Eq. 5), which works naturally for images but is not directly applicable to discrete token sequences.
- **Why unresolved:** The "zero-glance" strategy relies on optimizing input noise $z$ to maximize loss, a process that requires differentiable inputs or specialized discrete optimization techniques.
- **What evidence would resolve it:** Application of GFOES to a text classification dataset using discrete optimization or surrogate gradients for the generator.

## Limitations
- Architecture and hyperparameter sensitivity: Critical GFN architecture and hyperparameters are underspecified, likely impacting reproducibility and performance
- Convergence assumptions: Relies on bounded domain assumptions and positive loss conditions that are not empirically verified throughout training
- Generalization across architectures: Effectiveness on transformer-based models, recurrent networks, or other architectural families remains unverified

## Confidence
- **High Confidence:** The core mechanism of using synthetic adversarial samples for representation-level forgetting, the stabilized reciprocal objective formulation, and the two-phase fine-tuning strategy are all well-explained with theoretical grounding and experimental validation.
- **Medium Confidence:** The claim of complete forgetting (ADf = 0) is supported by CIFAR experiments, but the robustness of this result across different datasets, class distributions, and model architectures requires further verification.
- **Low Confidence:** The specific implementation details required for exact reproduction (GFN architecture, noise parameters, λ initialization) are insufficiently specified, limiting the ability to validate the method independently.

## Next Checks
1. **Parameter Sensitivity Analysis:** Systematically vary GFN learning rate, initial λ, and retained data ratio to identify thresholds where forgetting fails or utility preservation degrades significantly.
2. **Cross-Architecture Validation:** Apply GFOES to non-CNN architectures (e.g., ViT, LSTM) to verify the method generalizes beyond the tested convolutional networks.
3. **Robustness to Forget Set Characteristics:** Test the method's performance when forget classes have varying levels of semantic similarity to retained classes, or when forget sets are highly imbalanced.