---
ver: rpa2
title: The Resurgence of GCG Adversarial Attacks on Large Language Models
arxiv_id: '2509.00391'
source_url: https://arxiv.org/abs/2509.00391
tags:
- arxiv
- adversarial
- prompts
- attacks
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper evaluates the scalability of gradient-based adversarial\
  \ attacks on large language models using the Greedy Coordinate Gradient (GCG) algorithm\
  \ and its annealing-augmented variant, T-GCG. The study systematically tests GCG\
  \ attacks on three open-source LLMs\u2014Qwen2.5-0.5B, LLaMA-3.2-1B, and GPT-OSS-20B\u2014\
  using both safety-oriented prompts (AdvBench) and reasoning-intensive coding prompts."
---

# The Resurgence of GCG Adversarial Attacks on Large Language Models

## Quick Facts
- arXiv ID: 2509.00391
- Source URL: https://arxiv.org/abs/2509.00391
- Reference count: 40
- One-line primary result: Prefix-based evaluation substantially overestimates attack success compared to GPT-4o semantic judgment when attacking LLMs with GCG

## Executive Summary
This paper evaluates the scalability of gradient-based adversarial attacks on large language models using the Greedy Coordinate Gradient (GCG) algorithm and its annealing-augmented variant, T-GCG. The study systematically tests GCG attacks on three open-source LLMs—Qwen2.5-0.5B, LLaMA-3.2-1B, and GPT-OSS-20B—using both safety-oriented prompts (AdvBench) and reasoning-intensive coding prompts. Key findings include: (1) attack success rates decrease with model size, reflecting greater robustness of larger models due to more complex loss landscapes; (2) prefix-based heuristic evaluations substantially overestimate attack effectiveness compared to GPT-4o semantic judgments, which provide stricter and more realistic assessments; and (3) coding-related prompts are significantly more vulnerable than safety prompts, suggesting reasoning tasks expose alignment weaknesses. Preliminary T-GCG results show annealing-inspired exploration can improve prefix-based ASR, though benefits under semantic judgment remain limited. Overall, the work highlights the need for task-specific robustness testing and more rigorous evaluation protocols when assessing adversarial vulnerabilities in LLMs.

## Method Summary
The paper implements Greedy Coordinate Gradient (GCG) and its annealing-augmented variant T-GCG to generate adversarial suffixes that bypass LLM safety measures. The core method iteratively optimizes suffixes using gradients computed from the target LLM's loss function. GCG uses coordinate-wise stochastic gradient descent to select tokens that minimize refusal likelihood, while T-GCG introduces temperature-based stochastic sampling to escape local minima. The study tests these attacks on three model sizes (0.5B, 1B, and 20B parameters) using AdvBench safety prompts and 100 coding-related prompts. Attack success is measured via both prefix-based heuristics (absence of refusal tokens) and GPT-4o semantic judgment for harmfulness classification.

## Key Results
- Attack success rates decrease substantially as model size increases, from ~93% on 0.5B models to ~12% on 20B models
- Prefix-based evaluation overestimates attack effectiveness, with substantial gaps between prefix ASR and GPT-4o semantic ASR
- Coding-related prompts show significantly higher vulnerability (lower ASR) than safety prompts across all model sizes
- T-GCG shows promise in improving prefix-based ASR through temperature annealing, though benefits under semantic judgment are limited

## Why This Works (Mechanism)

### Mechanism 1: Gradient-Based Suffix Optimization (GCG)
The Greedy Coordinate Gradient algorithm induces harmful outputs by iteratively optimizing an adversarial suffix to minimize the likelihood of refusal. It treats suffix generation as a discrete optimization problem, computing gradients with respect to one-hot token identifiers at each position. Using coordinate descent, it identifies top-k token replacements that most effectively reduce refusal loss, samples a replacement, and evaluates the resulting string. The core assumption is that gradient information provides a reliable proxy for token utility, and the loss landscape is sufficiently smooth for greedy local steps to succeed. However, this mechanism degrades significantly as model size increases because the loss landscape becomes highly non-convex, causing greedy search to trap in local minima.

### Mechanism 2: Temperature-Based Exploration (T-GCG)
Standard GCG is strictly greedy, selecting the single best token replacement. T-GCG introduces stochasticity via temperature-based sampling, replacing deterministic selection with probability distributions based on loss improvements. This allows the algorithm to occasionally accept worse local suffixes to traverse the loss landscape more broadly, potentially finding global optima that greedy methods miss. The optimization landscape contains barriers that strictly greedy descent cannot cross but which can be traversed via probabilistic jumps. While T-GCG improves prefix-based ASR by exploring more candidates, benefits under semantic judgment remain limited.

### Mechanism 3: Evaluation Overestimation via Prefix Heuristics
Prefix-based evaluation systematically overestimates attack success compared to semantic judgment. Many aligned models are trained to output specific refusal tokens, so prefix heuristics assume absence of these tokens implies a successful attack. However, a model might fail to refuse while also failing to provide harmful instructions, instead outputting gibberish or unrelated text. Semantic judgment analyzes intent and content, ignoring superficial prefix markers. This mechanism highlights a failure in measurement rather than the attack itself—relying solely on prefix metrics creates a false picture of vulnerability.

## Foundational Learning

**Concept: White-box vs. Black-box Attacks**
*Why needed:* GCG requires access to model gradients, making it explicitly a "white-box" attack. Understanding this distinction is critical because one cannot run GCG on closed APIs without transferability workarounds.
*Quick check:* Can you run the T-GCG algorithm on a model where you only have input/output access (API)? Why or why not?

**Concept: Discrete Optimization in Continuous Space**
*Why needed:* Text is discrete while gradients are continuous. The paper replaces tokens based on gradients, requiring understanding that we cannot "apply gradient descent to text" directly but use gradients to inform discrete search.
*Quick check:* Why does the algorithm sample top-k tokens rather than directly updating the token embedding?

**Concept: Loss Landscapes and Non-Convexity**
*Why needed:* The paper attributes attack failures on larger models to "complex and non-convex loss landscapes." Learners need to grasp that as models get larger, the solution surface becomes more rugged, making it harder for greedy algorithms to find successful attacks.
*Quick check:* Why would a larger, more capable model be harder to optimize an attack for, despite having more capabilities to exploit?

## Architecture Onboarding

**Component map:** Input Module -> Gradient Engine -> Candidate Generator -> Batch Evaluator -> Selector -> Judgment Module

**Critical path:** The loop between Gradient Engine -> Candidate Generator -> Batch Evaluator runs for T epochs (e.g., 200). If loss plateaus or prefix check passes, the loop terminates.

**Design tradeoffs:**
- Batch Size (B) vs. Speed: Larger B provides better statistics but requires massive forward passes (VRAM intensive)
- Prefix vs. Semantic Eval: Prefix evaluation is free and fast; Semantic eval (GPT-4o) costs money and adds latency but is necessary for truthful reporting
- Deterministic (GCG) vs. Stochastic (T-GCG): T-GCG explores more but requires tuning α and T; improper tuning leads to unstable optimization

**Failure signatures:**
- High Prefix ASR, Low Semantic ASR: Model outputs confused text lacking "I'm sorry" but isn't actually harmful
- Zero Gradient/No Convergence: Suffix is stuck; gradients are uninformative
- T-GCG Divergence: If temperature T₂ is too high, suffix becomes random noise; if too low, it reverts to greedy GCG

**First 3 experiments:**
1. Baseline Verification: Run standard GCG on Qwen2.5-0.5B using AdvBench to reproduce ~93% prefix ASR
2. Evaluation Delta: Take successful outputs from Experiment 1, run through GPT-4o semantic judge, calculate "Overestimation Gap"
3. T-GCG Tuning: Implement temperature schedule on LLaMA-3.2-1B, run sweep on α (0.005 vs 0.01) to observe exploration vs stability tradeoff

## Open Questions the Paper Calls Out
None

## Limitations

- Model Identity Ambiguity: "GPT-OSS-20B" lacks a publicly documented HuggingFace identifier or explicit model card
- Evaluation Protocol Gaps: GPT-4o judge template from Figure 5 is not rendered in available text
- Coding Prompt Dataset Opacity: Only 2-3 concrete examples of the 100 coding prompts are visible

## Confidence

**High Confidence Claims:**
- Prefix-based heuristics overestimate attack success compared to semantic evaluation
- Larger models demonstrate increased robustness to GCG attacks

**Medium Confidence Claims:**
- T-GCG improves prefix-based ASR through temperature annealing
- Coding prompts are more vulnerable than safety prompts

**Low Confidence Claims:**
- Specific attribution of attack failures on larger models to "non-convex loss landscapes"

## Next Checks

**Validation Check 1: Evaluation Protocol Replication**
Run controlled experiments comparing prefix-based and semantic evaluation on known successful attacks using multiple semantic judgment prompts to quantify sensitivity to evaluation methodology.

**Validation Check 2: T-GCG Parameter Sensitivity Analysis**
Systematically sweep temperature parameters (T₁ decay rate and α values) on single model/prompt pair to map optimization landscape, tracking ASR, loss trajectories, suffix entropy, and prefix behavior.

**Validation Check 3: Model Architecture Ablation**
Test GCG attack on multiple variants of same base model architecture (different quantization levels or alignment protocols) to isolate whether robustness trends are due to scale, alignment strength, or other factors.