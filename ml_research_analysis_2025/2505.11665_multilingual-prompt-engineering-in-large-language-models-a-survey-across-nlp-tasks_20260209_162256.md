---
ver: rpa2
title: 'Multilingual Prompt Engineering in Large Language Models: A Survey Across
  NLP Tasks'
arxiv_id: '2505.11665'
source_url: https://arxiv.org/abs/2505.11665
tags:
- prompting
- language
- languages
- multilingual
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey comprehensively reviews 39 multilingual prompting techniques
  applied to 30 NLP tasks across 250 languages, identifying gaps in research coverage
  particularly for underrepresented language families and low-resource languages.
  The study reveals that while high-resource Indo-European and Sino-Tibetan languages
  dominate prompting research, low-resource languages remain significantly understudied
  despite comprising over 175 of the 249 languages examined.
---

# Multilingual Prompt Engineering in Large Language Models: A Survey Across NLP Tasks

## Quick Facts
- arXiv ID: 2505.11665
- Source URL: https://arxiv.org/abs/2505.11665
- Authors: Shubham Vatsal; Harsh Dubey; Aditi Singh
- Reference count: 40
- Primary result: Comprehensive survey of 39 multilingual prompting techniques across 30 NLP tasks and ~250 languages, revealing significant research gaps in low-resource language coverage

## Executive Summary
This survey systematically examines 39 multilingual prompting techniques applied to 30 NLP tasks across approximately 250 languages, revealing significant disparities in research coverage between high-resource and low-resource languages. The analysis shows that while high-resource Indo-European and Sino-Tibetan languages dominate prompting research, low-resource languages comprising over 175 of the studied languages remain significantly understudied. The survey establishes standardized categorizations for multilingual prompting methods and proposes potential state-of-the-art approaches for various datasets, providing a foundation for future research in this rapidly evolving field.

## Method Summary
The survey methodology involved two-step literature filtering using 11 Google Scholar queries, followed by standardized technique naming and consolidation. From 189 initial articles, 36 research papers were selected based on criteria focusing on hard prompts and studies involving at least two languages. The authors manually assigned datasets to single NLP tasks, aggregated coverage statistics by language family and resource level, and constructed a taxonomy mapping techniques to tasks. Evaluation metrics were noted as varying across studies and thus omitted from the analysis.

## Key Results
- 38 distinct prompting techniques tested on high-resource languages, but only 20 extended to low-resource settings
- Machine Translation shows broadest coverage (63 high-resource and 149 low-resource languages)
- Mathematical Problem Solving and Word Sense Disambiguation remain almost exclusively confined to high-resource languages
- Translation-to-English reasoning transfer improves mathematical problem-solving performance by approximately 37% over basic prompting
- Dictionary-based lexical injection improves machine translation by 1-9 BLEU points across low-resource language pairs

## Why This Works (Mechanism)

### Mechanism 1: Translation-to-English Reasoning Transfer
English-dominant pretraining creates stronger reasoning representations in English. Translate-En-CoT (Shi et al., 2022) translates inputs to English before applying chain-of-thought reasoning, achieving 37% better performance than basic prompting in mathematical problem solving. This works because English has the most robust internal representations due to training data imbalance, though it may degrade for tasks requiring deep cultural context or language-specific idioms.

### Mechanism 2: Cross-Lingual Alignment via Structured Decomposition
Multi-step prompting techniques like XLT (Huang et al., 2023) and CLP (Qin et al., 2023) explicitly guide LLMs through cross-lingual translation, reasoning chain generation, and structured output phases. This creates explicit representational bridges between native languages and English pivot languages, consistently outperforming direct prompting methods across multiple datasets in mathematical problem solving, causal reasoning, and machine translation.

### Mechanism 3: External Knowledge Injection via Dictionary Prompting
Dictionary-based techniques (CoD, DIPMT) append multilingual lexical mappings to prompts before translation requests, providing explicit cross-lingual knowledge weakly represented in model weights. CoD improves machine translation performance in 83.75% of translation directions across 200 languages, while DIPMT yields 1-9 BLEU point improvements for low-resource languages, though this breaks when dictionaries are unavailable or domain-specific terminology differs from dictionary definitions.

## Foundational Learning

- **Concept: High-resource vs. Low-resource Language Classification**
  - Why needed here: Central to interpreting which techniques apply to your target languages given the 38:20 technique extension gap
  - Quick check question: Does your target language have a Wikipedia with >100k articles, Universal Dependencies treebank, or FLORES benchmark? If not, treat as low-resource.

- **Concept: Zero-shot vs. Few-shot Prompting**
  - Why needed here: Determines data requirements for deployment across the 39 surveyed techniques
  - Quick check question: Do you have labeled examples in your target language for few-shot selection? If not, prioritize zero-shot techniques like En-Basic or Translate-En variants.

- **Concept: Language Family Typology**
  - Why needed here: Explains cross-family transfer challenges given Indo-European/Sino-Tibetan dominance in prompting research
  - Quick check question: Is your target language typologically similar to high-resource languages in the same family, or does it represent an underrepresented family like Austronesian or Afro-Asiatic?

## Architecture Onboarding

- **Component map:** Input layer (language identification, resource-level, task type) -> Prompt selection layer (technique matching via Figure 1 taxonomy) -> Pivot language decision (English, Native, or Hybrid) -> Optional external knowledge injection (dictionary, translation memory, post-editing)

- **Critical path:** 1) Classify target language as high/low-resource using corpus indicators 2) Map task to technique category using Figure 1 taxonomy 3) For low-resource languages, start with Translate-En or XLT; avoid techniques requiring native exemplars 4) For translation tasks, consider dictionary augmentation (CoD/DIPMT) as first enhancement 5) For reasoning tasks, prefer Translate-En-CoT or CLSP over Native-CoT for low-resource languages

- **Design tradeoffs:** English-centric prompting (higher performance for low-resource languages vs. cultural fidelity) vs. Native prompting; Few-shot (better performance vs. exemplar curation cost) vs. Zero-shot; Dictionary injection (+1-9 BLEU improvement vs. availability dependency) vs. Pure prompting; Multi-step decomposition (higher accuracy vs. latency and token costs)

- **Failure signatures:** Sudden accuracy drops switching from high-resource to low-resource languages → resource-level mismatch; Poor performance on culturally-knowledge tasks despite Translate-En → idiomatic meaning loss; Inconsistent few-shot outputs → exemplar selection bias; Translation quality degradation despite dictionary injection → domain mismatch

- **First 3 experiments:** 1) Test En-Basic and Native-Basic across one high-resource and one low-resource language to quantify resource gap 2) Compare Translate-En vs. direct native prompting for your lowest-resource language 3) Add exactly one enhancement (dictionary injection or CoT) to baseline and measure delta

## Open Questions the Paper Calls Out

### Open Question 1
Which prompting techniques are most effective for low-resource languages across diverse NLP tasks beyond Machine Translation? Current research disproportionately focuses on high-resource languages, leaving methodological experimentation sparse for low-resource contexts. Systematic comparative studies on standardized benchmarks across multiple low-resource languages and diverse task types would resolve this.

### Open Question 2
Can prompting techniques designed for Indo-European languages generalize effectively to typologically distant language families? Most prompting research is concentrated in Indo-European and Sino-Tibetan families, creating potential methodological bias toward their linguistic characteristics. Cross-family transfer experiments measuring performance when applying established techniques to typologically distinct languages would provide evidence.

### Open Question 3
How can evaluation metrics be standardized to enable fair comparison of multilingual prompting techniques across languages and tasks? The paper notes that "standardization of evaluation metrics persist" as a challenge, with metrics differing across studies. Development and community adoption of a unified multilingual evaluation framework with language-appropriate metrics across all surveyed tasks would resolve this.

## Limitations

- Focus on hard prompts only, excluding soft or automatic optimization techniques that could address limitations not covered by the 39 examined techniques
- Reliance on Google Scholar queries without systematic coverage guarantees may miss relevant work in non-English publications or gray literature
- Arbitrary cutoff of 39 techniques and 30 tasks may exclude emerging approaches or underrepresented task categories
- Undocumented thresholds for classifying languages as high-resource vs low-resource make verification difficult

## Confidence

- **High confidence**: Taxonomy construction and coverage statistics are reproducible and represent the field accurately based on sampled literature
- **Medium confidence**: Translate-En-CoT performance claims rely on single-source validation and may not generalize across different LLM architectures
- **Low confidence**: Claims about technique extension to low-resource settings depend on undocumented resource-level classification criteria

## Next Checks

1. **Resource-level verification**: Independently classify the top 3 datasets for each of the 30 tasks by language resource level using established criteria (Wikipedia article count, Universal Dependencies presence, FLORES benchmark availability) to validate the survey's low-resource language coverage claims.

2. **Technique generalizability test**: Apply 5 prompting techniques spanning different categories (zero-shot, few-shot, translation-pivot, dictionary-augmented) to a held-out language family (e.g., Austronesian or Niger-Congo) not prominently featured in the survey to test whether performance patterns hold beyond studied languages.

3. **Cross-survey triangulation**: Compare the identified SoTA methods for each task against findings from related surveys on Arabic content and African NLP to assess whether the English-centric prompting bias appears consistently across language-specific literature.