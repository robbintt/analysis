---
ver: rpa2
title: Group-Aware Reinforcement Learning for Output Diversity in Large Language Models
arxiv_id: '2511.12596'
source_url: https://arxiv.org/abs/2511.12596
tags:
- gapo
- diversity
- reward
- baseline
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses mode collapse in large language models, where
  models repeatedly generate the same limited set of responses despite many valid
  alternatives. The authors propose Group-Aware Policy Optimization (GAPO), an extension
  of Group Relative Policy Optimization (GRPO) that computes rewards over groups of
  completions rather than individual samples.
---

# Group-Aware Reinforcement Learning for Output Diversity in Large Language Models

## Quick Facts
- **arXiv ID:** 2511.12596
- **Source URL:** https://arxiv.org/abs/2511.12596
- **Reference count:** 40
- **Primary result:** GAPO achieves near-uniform sampling from lists of valid outputs, generating significantly more diverse responses while maintaining coherence on standard benchmarks

## Executive Summary
This paper addresses mode collapse in large language models, where models repeatedly generate the same limited set of responses despite many valid alternatives. The authors propose Group-Aware Policy Optimization (GAPO), an extension of Group Relative Policy Optimization (GRPO) that computes rewards over groups of completions rather than individual samples. This enables learning from group-level properties like diversity and coverage. The method is demonstrated using a frequency-aware reward function that encourages uniform sampling across valid outputs. GAPO-trained models achieve near-uniform sampling when selecting from lists, generating significantly more diverse responses on open-ended prompts while maintaining coherence on standard benchmarks including GSM8K, MATH, HumanEval, and MMLU-Pro.

## Method Summary
Group-Aware Policy Optimization (GAPO) extends GRPO by computing rewards over groups of completions rather than individual samples. The key innovation is partitioning generated samples into groups and calculating group-level statistics (e.g., frequency of occurrence, diversity metrics) to inform the reward signal. This group-level reward captures properties like coverage and uniformity that individual sample rewards miss. The method uses a frequency-aware reward function that encourages the model to sample uniformly from the space of valid outputs. During training, the model generates multiple completions for each prompt, these are grouped based on their content, and rewards are computed based on group properties rather than individual quality. This approach enables the model to learn strategies for maintaining diversity while preserving coherence on standard tasks.

## Key Results
- GAPO achieves near-uniform sampling from lists of valid outputs (e.g., countries in Asia), with 60% of generations containing all valid options compared to 25% for standard GRPO
- On open-ended prompts, GAPO generates significantly more diverse responses while maintaining benchmark performance on GSM8K, MATH, HumanEval, and MMLU-Pro
- The method demonstrates 70% accuracy on constrained counting tasks compared to 30% for standard GRPO, showing improved ability to generate valid diverse outputs

## Why This Works (Mechanism)
GAPO addresses mode collapse by shifting from individual sample rewards to group-level rewards. Standard RL methods evaluate each completion independently, which can lead to collapse when multiple valid outputs exist but share similar reward characteristics. By grouping similar completions and computing rewards based on group properties (like frequency distribution), GAPO encourages the model to explore and maintain coverage of the valid output space. The frequency-aware reward specifically penalizes over-representation of certain outputs and rewards uniform sampling across valid alternatives. This group-level optimization allows the model to learn strategies for balancing exploration (trying new outputs) with exploitation (generating high-quality outputs), which individual sample rewards cannot capture.

## Foundational Learning
- **Group Relative Policy Optimization (GRPO)**: A reinforcement learning method that computes rewards relative to a group of samples rather than absolute rewards. Why needed: Provides the foundation for group-level reward computation that GAPO builds upon.
- **Mode collapse**: The phenomenon where generative models repeatedly produce a limited subset of possible outputs despite many valid alternatives. Why needed: The core problem GAPO addresses.
- **Frequency-aware rewards**: Reward functions that encourage uniform sampling across output categories based on their occurrence frequencies. Why needed: The specific mechanism used in GAPO to promote diversity.
- **Reinforcement Learning from Human Feedback (RLHF)**: The broader framework of using RL to align model outputs with human preferences. Why needed: Context for understanding how GAPO fits into existing LLM training paradigms.
- **Policy gradient methods**: Optimization techniques that update model parameters based on gradient estimates of expected rewards. Why needed: The optimization framework underlying GAPO.
- **Diversity metrics**: Quantitative measures (e.g., distinct-n, entropy) used to evaluate output variety. Why needed: To assess whether GAPO successfully increases response diversity.

## Architecture Onboarding
- **Component map:** Language model (LLM) -> Completion generator -> Group partitioner -> Group-level reward calculator -> Policy optimizer -> Updated LLM
- **Critical path:** During training, the LLM generates multiple completions per prompt → completions are partitioned into groups based on content → group-level statistics are computed → rewards are calculated based on group properties → policy gradient updates are applied to the LLM
- **Design tradeoffs:** Group-level rewards capture diversity but may miss fine-grained quality differences between individual samples; more groups provide better coverage estimates but increase computational cost; frequency-based rewards promote uniformity but may conflict with quality optimization.
- **Failure signatures:** Mode collapse (over-representation of certain outputs), loss of coherence on standard tasks, failure to improve beyond baseline on constrained tasks, excessive computational overhead from large group sizes.
- **First experiments:** 1) Verify uniform sampling from simple lists (countries, colors) using GAPO vs GRPO, 2) Test GAPO on constrained counting tasks with known answer distributions, 3) Evaluate benchmark performance retention on GSM8K and HumanEval after GAPO training.

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focuses primarily on synthetic tasks with known ground truth distributions rather than real-world applications where mode collapse is most problematic
- Diversity improvements on open-ended prompts are based on limited qualitative examples rather than comprehensive quantitative diversity metrics
- Comparison with standard RLHF methods appears limited to constrained tasks rather than the full benchmark suite, making practical improvements difficult to assess

## Confidence
- **High confidence:** The theoretical framework of GAPO and its implementation details are well-documented and reproducible
- **Medium confidence:** The effectiveness of GAPO on synthetic list generation and constrained tasks is well-supported by experimental results
- **Medium confidence:** The claim about maintaining coherence on standard benchmarks is supported by scores, though the diversity improvements on open-ended tasks are less rigorously quantified

## Next Checks
1. Conduct a controlled experiment comparing GAPO vs GRPO on open-ended creative writing tasks using established diversity metrics (Self-BLEU, distinct-n-grams, entropy of response distributions)
2. Test GAPO with alternative reward functions beyond frequency awareness to determine if the diversity gains are specific to the reward design or inherent to the group-level optimization
3. Evaluate whether GAPO's diversity improvements persist under different sampling strategies (temperature scaling, top-k/p) and at different scale models (beyond the 7B model tested)