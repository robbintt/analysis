---
ver: rpa2
title: In-Context Learning can distort the relationship between sequence likelihoods
  and biological fitness
arxiv_id: '2504.17068'
source_url: https://arxiv.org/abs/2504.17068
tags:
- sequences
- sequence
- figure
- language
- protein
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study reveals that in-context learning in masked language models
  can distort the relationship between sequence likelihood scores and biological fitness,
  particularly for sequences with repeated motifs. The research demonstrates that
  transformer-based models, such as ESM2, exhibit an "uncertainty collapse" when presented
  with repeated sequences, leading to anomalously high likelihood scores that do not
  accurately reflect biological functionality.
---

# In-Context Learning can distort the relationship between sequence likelihoods and biological fitness

## Quick Facts
- arXiv ID: 2504.17068
- Source URL: https://arxiv.org/abs/2504.17068
- Authors: Pranav Kantroo; Günter P. Wagner; Benjamin B. Machta
- Reference count: 40
- Primary result: In-context learning in masked language models causes uncertainty collapse for repeated sequences, distorting likelihood-fitness relationships

## Executive Summary
Transformer-based masked language models can exploit repeated sequence motifs through in-context retrieval, leading to anomalously high likelihood scores that don't reflect biological functionality. This "uncertainty collapse" occurs when the model uses repeated copies as lookup tables to resolve masked positions, overriding learned priors about amino acid distributions. The effect persists for imperfectly repeated sequences and extends to other biological features like RNA hairpin structures, with larger models showing greater susceptibility. This finding challenges the reliability of likelihood-based fitness predictions, particularly for sequences with repeated or symmetrical features.

## Method Summary
The study analyzed 76,361 protein domains from SwissProt (20-1000 residues) using masked language models including ESM2, CARP, and RiNALMo. Researchers measured pseudo-perplexity (lower = higher likelihood) using one-at-a-time masking and One Fell Swoop methods. They tested doubling sequences, masking equivalent positions in repeated motifs, and examining prior override behavior. Additional experiments probed hairpin structure recognition and model behavior across different architectures and scales. MLP ensembles were trained on 80-20 splits for embedding regression tasks.

## Key Results
- Transformer models exhibit "uncertainty collapse" for doubled sequences, with pseudo-perplexity dropping to ~1 (complete certainty) regardless of biological plausibility
- Equivalent-position masking experiments confirm retrieval mechanism: masking position i in both copies restores uncertainty while non-equivalent masking maintains low entropy
- Model predictions can override learned biochemical priors when retrieved context contradicts them (e.g., predicting charged residues in hydrophobic positions)
- Different architectures show varying susceptibility: transformers collapse completely, CNNs collapse only for repeats <70 residues, and state-space models show progressive decline only

## Why This Works (Mechanism)

### Mechanism 1: In-Context Retrieval via Repeated Motifs
- **Claim**: Transformer-based masked language models use repeated copies of sequence regions as lookup tables, retrieving residue identities from equivalent positions rather than relying on learned priors.
- **Mechanism**: Bidirectional attention identifies equivalent positions across repeated units; when position i is masked, the model attends to position i in the duplicate copy and copies that residue identity directly, achieving near-certain predictions.
- **Core assumption**: The model can identify positional equivalence across repeated units and prioritizes retrieved context over learned amino acid distributions.
- **Evidence anchors**:
  - [abstract]: "This behavior is mediated by a look-up operation where the model seeks the identity of the masked position by using the other copy of the repeated motif as a reference."
  - [section]: Figure 3C–E shows that masking the equivalent position in the second copy restores uncertainty (high entropy), while masking a non-equivalent position maintains low entropy.
  - [corpus]: Related work on temporal retrieval biases (arXiv:2510.22752) confirms in-context retrieval is sensitive to positional relationships, though not specific to biological sequences.
- **Break condition**: When no equivalent position exists in context, or when repeated units share <50% similarity, the effect weakens but persists (Figure 4B).

### Mechanism 2: Uncertainty Collapse from Repeated Context
- **Claim**: Doubling any sequence (even random amino acids) causes pseudo-perplexity to collapse to ~1, the theoretical minimum, indicating complete certainty regardless of biological plausibility.
- **Mechanism**: The retrieval mechanism resolves all uncertainty by cross-referencing copies, overriding the model's learned distribution over amino acids.
- **Core assumption**: Bidirectional attention enables complete resolution of masked positions when equivalent information is present elsewhere.
- **Evidence anchors**:
  - [abstract]: "...transformer-based models, such as ESM2, exhibit an 'uncertainty collapse' when presented with repeated sequences, leading to anomalously high likelihood scores that do not accurately reflect biological functionality."
  - [section]: Figure 2A shows ESM2 pseudo-perplexity dropping from ~3–6 to ~1 for doubled natural domains; Figure 2B confirms this for random sequences.
  - [corpus]: No direct corpus evidence for uncertainty collapse in biological sequence models; this appears to be a novel finding.
- **Break condition**: Architecture-dependent. CNN-based CARP only collapses for repeats <70 residues (Figure 2C); BiMamba-S based LC-PLM shows only progressive decline without collapse (Figure 2D).

### Mechanism 3: Prior Override by Retrieved Context
- **Claim**: When retrieved context contradicts learned priors (e.g., a hydrophobic position showing charged residue in the duplicate), the model predicts the retrieved residue with high confidence, abandoning learned biochemical knowledge.
- **Mechanism**: In-context retrieval has higher effective weight than learned statistical associations during masked prediction, enabling biologically implausible predictions.
- **Core assumption**: The conflict resolution mechanism favors immediate context over training-accumulated knowledge.
- **Evidence anchors**:
  - [abstract]: "This retrieval behavior can override the model's learned priors."
  - [section]: Figure 3F shows that replacing tryptophan (hydrophobic, learned preference) with lysine (charged, biochemically inconsistent) in the second copy causes the model to predict lysine with near-certainty.
  - [section]: Figure 3I diagonal concentration shows predictions track the equivalent position's residue across thousands of domains.
  - [corpus]: Wei et al. (cited as arXiv:2303.03846 in paper) document similar prior override in natural language models with contradictory in-context exemplars.
- **Break condition**: Rare amino acids (W, C) show weaker flipping response (Figure 3I), suggesting priors retain partial influence; this weaker effect disappears for random sequences.

## Foundational Learning

- **Concept: Pseudo-perplexity as uncertainty metric**
  - **Why needed here**: All experiments use pseudo-perplexity to quantify model certainty; understanding that 1 = complete certainty and higher values = more uncertainty is essential for interpreting results.
  - **Quick check question**: If a doubled random sequence has pseudo-perplexity ≈ 1, what does this imply about the model's prediction confidence versus biological validity?

- **Concept: In-context learning without weight updates**
  - **Why needed here**: The paper shows models "learn" from repeated context at inference time without any fine-tuning; this emergent behavior differs fundamentally from training-based learning.
  - **Quick check question**: How does in-context retrieval differ from memorization, and why might surface-level patterns exploit this mechanism?

- **Concept: Bidirectional vs. autoregressive attention**
  - **Why needed here**: The paper compares ESM2 (bidirectional masked LM) and ProGen2 (autoregressive), showing bidirectional models exhibit complete collapse while autoregressive models only show perplexity decline.
  - **Quick check question**: Why would a bidirectional model achieve pseudo-perplexity ≈ 1 for doubled sequences while an autoregressive model cannot reach this minimum?

## Architecture Onboarding

- **Component map**: Input tokenization -> embedding layer -> context aggregation (architecture-dependent: attention/convolution/state-space) -> masked position prediction -> probability distribution over 20 amino acids -> likelihood scoring -> pseudo-perplexity via one-at-a-time masking or OFS method

- **Critical path**: Input tokenization → embedding layer → context aggregation (architecture-dependent: attention/convolution/state-space) → masked position prediction → probability distribution over 20 amino acids → likelihood scoring → pseudo-perplexity via one-at-a-time masking or OFS method

- **Design tradeoffs**:
  - Transformer: Captures long-range dependencies but vulnerable to repeated-pattern exploitation; larger models (650M) more susceptible than smaller (8M), both collapse
  - CNN: Receptive field limits exploitation to short repeats; operational memory ~70 residues provides natural defense
  - BiMamba-S: Long context without quadratic attention cost, but progressive decline suggests different retrieval dynamics
  - Model scale: RiNALMo 650M shows hairpin collapse; 33.5M only partial reduction (Figure 4H)

- **Failure signatures**:
  - Pseudo-perplexity ≈ 1 for any non-trivial sequence → likely repeated motif present
  - High likelihood scores for biologically implausible sequences (random doubled sequences) → in-context retrieval artifact
  - Contra-lateral retrieval preference at sequence ends (Figure 3J) → attention artifact from positional biases
  - Embedding quality degradation with multiplicity (Figure 5C) → representation dilution from repetition

- **First 3 experiments**:
  1. **Doubling test**: For any protein domain, compute pseudo-perplexity for original (1×) and doubled (2×) versions. Collapse to ~1 indicates susceptibility.
  2. **Equivalent-position masking test**: Create doubled sequence, mask position i in first copy, then mask equivalent position i in second copy. Measure entropy change.
  3. **Prior override test**: For a position with strong learned preference (e.g., conserved hydrophobic), double the sequence and substitute the equivalent position with a conflicting residue (e.g., charged). Model predicting the conflicting residue confirms retrieval overrides priors.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Does in-context retrieval distortion occur in models trained on biomolecular structures with symmetric subunits?
- **Basis in paper**: [explicit] The authors hypothesize that "a model trained on biomolecular structures could perhaps use symmetric subunits within a structure to perform in-context retrieval."
- **Why unresolved**: The current study limited experiments to sequence-based models (transformers and convolutions) for proteins and RNA, leaving the structural modality unexplored.
- **What evidence would resolve it**: Experiments applying masked modeling to structural inputs (e.g., 3D coordinates or density maps) containing symmetric subunits to observe if likelihood scores distort similarly.

### Open Question 2
- **Question**: Are the known clade-specific biases in protein language models caused by the in-context learning mechanisms described?
- **Basis in paper**: [explicit] The authors state that "a natural next step in this line of questioning is to investigate if the known biases exhibited by these models are connected to this phenomenology."
- **Why unresolved**: While the paper establishes that in-context retrieval can override priors, it does not empirically link this mechanism to specific, previously documented sampling biases (e.g., phylogenetic skews).
- **What evidence would resolve it**: A correlation analysis showing that sequences from over-represented clades trigger stronger in-context retrieval or that removing retrieval circuits mitigates these biases.

### Open Question 3
- **Question**: Can models decompose the composite skill of recognizing reverse complements into the constituent sub-skills of reversal and complementation?
- **Basis in paper**: [explicit] The authors note the failure to recognize reversed-only sequences "points towards the need for a systematic inquiry into their ability to recover simpler sub-skills from learned composite skills."
- **Why unresolved**: Models recognized "reversed complements" (hairpins) but not "reversed" or "complemented" sequences alone, suggesting the composite skill is learned holistically without accessible sub-components.
- **What evidence would resolve it**: Probing the internal representations of the model to determine if distinct subspaces encode "reversal" and "complement" operations independently.

## Limitations
- The study focuses primarily on transformer architectures, limiting generalizability to other model families
- Biological relevance of uncertainty collapse for real protein design tasks remains untested
- The threshold for problematic behavior in practical design constraints is not quantified
- Speculation about evolutionary implications lacks empirical testing

## Confidence
- **High Confidence**: Core observation that repeated motifs cause uncertainty collapse in transformer models is well-supported by multiple experimental approaches
- **Medium Confidence**: Claim that larger models are more susceptible has mixed support; assertion that this affects real protein design workflows lacks empirical validation
- **Low Confidence**: Speculation about evolutionary implications is intriguing but not tested; claim that this represents a fundamental limitation of likelihood-based fitness prediction may overstate the case

## Next Checks
1. **Real Design Task Validation**: Test whether uncertainty collapse actually leads to false positives in a protein design benchmark by generating sequences with subtle repeats and comparing model likelihoods against experimental fitness measurements
2. **Cross-Architecture Comparison**: Systematically evaluate uncertainty collapse across a broader range of architectures (RNNs, attention variants, different CNN configurations) with consistent experimental protocols
3. **Repeat Detection Threshold**: Characterize the minimum repeat similarity and length required to trigger uncertainty collapse to determine whether the effect is relevant for natural proteins with imperfect repeats