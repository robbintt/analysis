---
ver: rpa2
title: Improving Performance, Robustness, and Fairness of Radiographic AI Models with
  Finely-Controllable Synthetic Data
arxiv_id: '2508.16783'
source_url: https://arxiv.org/abs/2508.16783
tags:
- synthetic
- real
- data
- images
- cxrs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RoentGen-v2 is a text-to-image diffusion model for generating synthetic
  chest radiographs with fine-grained control over both radiographic findings and
  patient demographics (sex, age, race/ethnicity). The model was trained on 75k CXRs
  from MIMIC-CXR, producing a large, demographically balanced synthetic dataset of
  over 565k images.
---

# Improving Performance, Robustness, and Fairness of Radiographic AI Models with Finely-Controllable Synthetic Data

## Quick Facts
- arXiv ID: 2508.16783
- Source URL: https://arxiv.org/abs/2508.16783
- Reference count: 40
- Synthetic pretraining improves classification accuracy by 6.5%, reduces fairness gaps by 19.3%, and enhances OOD generalization.

## Executive Summary
This paper introduces RoentGen-v2, a text-to-image diffusion model for generating synthetic chest radiographs with fine-grained control over both radiographic findings and patient demographics. The model was trained on 75k CXRs from MIMIC-CXR, producing a large, demographically balanced synthetic dataset of over 565k images. When used for supervised pretraining of downstream disease classifiers—followed by fine-tuning on real data—it consistently outperformed alternative training strategies (real-only, synthetic-only, and naive synthetic+real mix) across five datasets. The approach yielded a 6.5% increase in classification accuracy, a 19.3% reduction in underdiagnosis fairness gaps, and improved generalization to out-of-distribution settings, all while being label-efficient.

## Method Summary
RoentGen-v2 fine-tunes Stable Diffusion 2.1 on 68k CXR-report-demographic triplets from MIMIC-CXR, using a text encoder and U-Net. The model generates synthetic CXRs conditioned on prompts encoding patient demographics and clinical findings. A quality control pipeline using XRV classifiers verifies generated images match the intended demographics and labels. For downstream tasks, a DenseNet-121 classifier is first pretrained on 565k synthetic images, then fine-tuned on real MIMIC-CXR data. Performance is evaluated using macro-averaged multi-label AUROC/AUPRC across 8 disease labels and fairness metrics across intersectional subgroups.

## Key Results
- Synthetic pretraining followed by real-data fine-tuning achieved 6.5% higher classification accuracy compared to naive synthetic+real mixing
- Underdiagnosis fairness gaps were reduced by 19.3% across intersectional subgroups
- Models pretrained on synthetic data required fewer real samples (under 10k vs 66k) to match baseline performance
- OOD generalization improved consistently across all five external datasets evaluated

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Synthetic pretraining followed by real-data fine-tuning outperforms naive data mixing for classifier performance and generalization
- Mechanism: Two-stage supervised training on demographically diverse synthetic images enables learning domain-specific visual representations that transfer more effectively than ImageNet pretraining, while fine-tuning on real data corrects distributional mismatches
- Core assumption: Synthetic images contain sufficient domain-relevant features to learn transferable representations despite not being real medical images
- Evidence anchors:
  - [abstract] Reports 6.5% accuracy increase with synthetic pretraining vs 2.7% with naive mixing
  - [section 2.3.2] Shows synthetic-pretrained models match baseline performance with <10k real samples (vs 66k without pretraining)
  - [corpus] Limited direct comparison; neighboring papers focus on fairness rather than pretraining strategies
- Break condition: If synthetic images systematically lack subtle but diagnostically critical features present in real CXRs, pretraining benefits may plateau and fail to close the performance gap further

### Mechanism 2
- Claim: Demographically controllable generation reduces classifier fairness gaps across intersectional subgroups
- Mechanism: Explicit conditioning on sex, age, and race/ethnicity during generation creates balanced representation across subgroups, reducing the correlation between protected attributes and disease labels that causes underdiagnosis in underrepresented populations
- Core assumption: Demographic features are learnable from CXR imagery and can be reliably controlled during diffusion-based generation
- Evidence anchors:
  - [abstract] States 19.3% reduction in underdiagnosis fairness gap
  - [section 2.3.3] Shows synthetic pretraining reduced underdiagnosis gap across all four evaluated datasets
  - [corpus] MedEqualizer (arXiv:2511.01054) investigates bias in synthetic medical data, supporting relevance but not replication
- Break condition: If demographic attributes are not visually encoded in CXRs (e.g., race lacks imaging correlates), conditioning may fail to translate into meaningful representation changes

### Mechanism 3
- Claim: Quality control filtering of synthetic images improves downstream utility by ensuring prompt-image alignment
- Mechanism: Classifier-based verification rejects images where generated demographics or findings mismatch prompts, reducing label noise in the synthetic training set
- Core assumption: Pretrained classifiers (XRV) are sufficiently accurate to serve as reliable quality filters
- Evidence anchors:
  - [section 2.2] Reports 9.4% rejection rate, primarily driven by race criterion failures
  - [section 4.3] Describes three-attempt retry policy before discarding prompts
  - [corpus] No direct corpus evidence on QC mechanisms for synthetic medical imaging
- Break condition: If QC classifiers themselves exhibit demographic biases, filtering may inadvertently reintroduce or amplify fairness issues

## Foundational Learning

- Concept: Latent Diffusion Models
  - Why needed here: RoentGen-v2 builds on Stable Diffusion 2.1; understanding how text conditioning controls generation is essential for modifying prompts or architecture
  - Quick check question: Can you explain how classifier-free guidance scaling affects prompt adherence vs diversity?

- Concept: Transfer Learning in Medical Imaging
  - Why needed here: The paper compares ImageNet pretraining vs synthetic pretraining; understanding why domain-specific pretraining helps is critical for interpreting results
  - Quick check question: Why might CXR-specific pretraining outperform ImageNet pretraining for disease classification?

- Concept: Fairness Metrics (AUROC gap, Underdiagnosis gap)
  - Why needed here: The paper uses multiple fairness metrics; understanding their definitions and tradeoffs is necessary for proper evaluation
  - Quick check question: How does the underdiagnosis gap differ from a simple performance gap, and why might it be more clinically relevant?

## Architecture Onboarding

- Component map:
  - Generator (RoentGen-v2) -> Quality Control (XRV classifiers) -> Downstream Classifier (DenseNet-121)

- Critical path:
  1. Format prompts as: `<AGE> year old <RACE> <SEX>. <IMPRESSION>.`
  2. Generate with classifier-free guidance scale 4.0, 75 denoising steps
  3. Run QC; reject if demographics mismatch (retry up to 3×)
  4. Pretrain classifier on 565k synthetic images from scratch
  5. Fine-tune on real MIMIC-CXR with reduced learning rate

- Design tradeoffs:
  - Larger synthetic datasets (up to 565k) improve OOD generalization but plateau at ~300k; diminishing returns beyond 5× real data size
  - Stricter QC reduces dataset size (9.4% rejected) but improves label fidelity
  - Race conditioning has highest failure rate (9.2% of rejections), potentially reflecting training data imbalance

- Failure signatures:
  - Race-based QC failures concentrate in Asian subgroup (40× upsampling needed vs 3.5× for White), suggesting training data imbalance propagates to generation
  - Synthetic-only models underperform real-only baseline in-distribution, indicating synthetic data alone cannot fully replace real data
  - VinDr dataset shows smallest gains, possibly due to AP vs PA view mismatch

- First 3 experiments:
  1. Reproduce synthetic pretraining vs naive mixing comparison on a single external dataset (e.g., CheXpert) to validate the 6.5% vs 2.7% gain claim
  2. Ablate QC: train classifiers on unfiltered synthetic data to measure performance degradation from label noise
  3. Test subgroup-specific generation: oversample underrepresented demographics (e.g., Asian patients) beyond balanced targets to assess whether further fairness gains are achievable

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does explicitly manipulating the synthetic dataset to oversample low-prevalence pathologies improve performance on long-tailed disease classification tasks?
- Basis in paper: [explicit] The authors note that they "replicated the training dataset label distribution without changing disease prevalence" and suggest that "In future studies, oversampling of specific diseases with the low prevalence should be investigated for long-tailed disease classification."
- Why unresolved: The current study prioritized creating a demographically balanced dataset regarding patient attributes (sex, age, race), but maintained the natural (imbalanced) disease distribution, leaving the efficacy of synthetic disease balancing untested.
- What evidence would resolve it: A comparative study where downstream classifiers are trained on synthetic datasets with uniform disease sampling versus natural prevalence, evaluated specifically on rare pathologies.

### Open Question 2
- Question: Can synthetic pretraining strategies remain effective while incorporating privacy-preserving mechanisms to prevent patient re-identification via data memorization?
- Basis in paper: [explicit] The authors state as a limitation: "Further research is needed to ensure privacy preservation and prevent patient re-identification that may occur through data memorization."
- Why unresolved: While the utility of synthetic data is demonstrated, the risk that the diffusion model memorizes and leaks specific training images (re-identification) was not mitigated or tested in the current pipeline.
- What evidence would resolve it: Quantification of membership inference attack success rates on the synthetic dataset, followed by the implementation of privacy guarantees (e.g., differential privacy) to see if classifier performance is maintained.

### Open Question 3
- Question: How does the reliance on a single-source dataset (MIMIC-CXR) and single view (PA) limit the model's expressive power and generalizability to diverse imaging protocols?
- Basis in paper: [explicit] The authors list as a limitation: "we trained RoentGen-v2 on one source dataset (MIMIC-CXR), and used only one radiographic view (PA), which limits the expressive power of the generative model."
- Why unresolved: The model's inability to generate certain subgroups (e.g., Asian patients) successfully was likely driven by the training data composition, suggesting the model may not generalize to unseen sites or views without multi-site training.
- What evidence would resolve it: Training a generative model on multi-institutional data and evaluating the quality and diversity of generated images across previously underrepresented demographics and imaging views.

## Limitations
- Limited to chest radiographs from a single dataset (MIMIC-CXR) and single view (PA), constraining generalizability
- Quality control classifiers may introduce demographic bias, particularly affecting race-based generation
- Synthetic data alone cannot fully replace real data for in-distribution performance

## Confidence
- **High**: Synthetic pretraining improves accuracy and generalization
- **Medium**: Synthetic pretraining reduces fairness gaps (underdiagnosis)
- **Low-Medium**: Demographically controllable generation is reliable

## Next Checks
1. **Cross-modal validation**: Test synthetic pretraining on non-CXR modalities (e.g., mammography, brain MRI) to assess broader applicability and identify modality-specific limitations
2. **QC classifier bias audit**: Conduct an independent audit of the XRV classifiers for demographic bias, especially in race classification, and assess the impact on downstream synthetic data quality
3. **Longitudinal performance and safety**: Evaluate synthetic-pretrained models on longitudinal cohorts and subtle disease detection tasks, measuring both performance and fairness over time