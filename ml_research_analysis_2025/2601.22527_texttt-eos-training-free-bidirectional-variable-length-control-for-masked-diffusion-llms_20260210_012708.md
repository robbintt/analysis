---
ver: rpa2
title: "$\u03C1$-$\\texttt{EOS}$: Training-free Bidirectional Variable-Length Control\
  \ for Masked Diffusion LLMs"
arxiv_id: '2601.22527'
source_url: https://arxiv.org/abs/2601.22527
tags:
- length
- masked
- arxiv
- density
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the fixed-length limitation in masked diffusion\
  \ large language models (dLLMs), where a predefined generation length forces a trade-off\
  \ between output quality and computational efficiency. The authors propose \u03C1\
  -EOS, a training-free, single-stage strategy that enables bidirectional variable-length\
  \ generation by leveraging the implicit density (\u03C1) of end-of-sequence (EOS)\
  \ tokens as a signal of generation sufficiency."
---

# $ρ$-$\texttt{EOS}$: Training-free Bidirectional Variable-Length Control for Masked Diffusion LLMs

## Quick Facts
- arXiv ID: 2601.22527
- Source URL: https://arxiv.org/abs/2601.22527
- Reference count: 14
- Primary result: Training-free, single-stage strategy enabling bidirectional variable-length generation in masked diffusion LLMs by leveraging implicit EOS density as a signal of generation sufficiency

## Executive Summary
This paper addresses the fixed-length limitation in masked diffusion large language models (dLLMs), where a predefined generation length forces a trade-off between output quality and computational efficiency. The authors propose ρ-EOS, a training-free, single-stage strategy that enables bidirectional variable-length generation by leveraging the implicit density (ρ) of end-of-sequence (EOS) tokens as a signal of generation sufficiency. During denoising, high ρ-EOS triggers MASK token contraction (indicating excessive length), while low ρ-EOS induces expansion (indicating insufficient length). Extensive experiments on mathematics and code benchmarks demonstrate that ρ-EOS achieves comparable performance to fixed-length baselines and two-stage methods while substantially improving inference efficiency and token utilization. The approach provides a flexible and effective solution for practical deployment scenarios requiring adaptive generation lengths.

## Method Summary
ρ-EOS operates by computing the implicit EOS density (ρEOS) at each denoising step, defined as the ratio of implicit EOS tokens to remaining MASK tokens. The method uses threshold-gated bidirectional length adjustment within a unified denoising process: when ρEOS falls below ρlow, MASK tokens are appended (expansion); when ρEOS exceeds ρhigh, trailing MASK tokens are removed (contraction); within the equilibrium region [ρlow, ρhigh], pure denoising proceeds without length changes. The approach is training-free and leverages the model's learned behavior from pretraining, where padding EOS was treated as part of the response during random masking. The best configuration uses Linit=64, asymmetric thresholds [ρlow=0.4, ρhigh=0.8], and constant expansion factors, achieving superior efficiency without sacrificing accuracy.

## Key Results
- ρ-EOS achieves comparable accuracy to fixed-length baselines (84.2% vs 84.6% on GSM8K) while reducing total tokens by 2.1x
- Single-stage unified denoising outperforms two-stage approaches on both GSM8K (84.2% vs 73.7%) and MBPP (40.6% vs 38.2%)
- Effective token ratio improves by 1.8x compared to fixed-length generation across benchmarks
- The method maintains stable performance across different initial lengths (64-1024 tokens) while fixed-length approaches degrade

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The implicit EOS density (ρEOS) serves as a reliable internal signal of generation length sufficiency during denoising.
- Mechanism: Masked dLLMs trained with padding EOS treated as part of the response learn to jointly predict semantic content and termination. The ratio of implicit EOS tokens to remaining MASK tokens reveals whether the current masked space is excessive (high ρEOS), insufficient (low ρEOS), or appropriate (intermediate equilibrium).
- Core assumption: The model's training paradigm—randomly masking padding EOS alongside semantic tokens—causes it to internalize length estimation as part of token prediction.
- Evidence anchors:
  - [abstract] "the evolving implicit EOS density during denoising reveals whether the current masked space is excessive or insufficient"
  - [Section 3.4] "when the generation length is excessively long... ρEOS converges toward near 1... when the length is too short... the density converges toward 0"
  - [Appendix A.1] "masked diffusion language models implicitly estimate how much generation space they need... This behavior is... deeply rooted in the training engineering implementation"

### Mechanism 2
- Claim: Bidirectional length adjustment via threshold-gated expansion/contraction enables adaptive generation without multi-stage pipelines.
- Mechanism: Define a length-equilibrium region [ρlow, ρhigh]. When ρEOS < ρlow, append MASK tokens proportional to the deviation (expansion). When ρEOS > ρhigh, remove trailing MASK tokens (contraction). Within the region, perform pure denoising without length changes.
- Core assumption: The equilibrium region exists and is stable enough that length adjustments converge rather than oscillate.
- Evidence anchors:
  - [abstract] "excessively high density triggers MASK token contraction, while insufficient density induces expansion"
  - [Section 3.4] Equations 6-7 define the conditional length adjustment logic
  - [Table 4] Ablation shows asymmetric thresholds [0.4, 0.8] work best for short initial lengths, suggesting the equilibrium region is task-dependent

### Mechanism 3
- Claim: Single-stage unified denoising outperforms two-stage approaches (e.g., DAEDAL) by interleaving length adjustment with token prediction in one forward pass.
- Mechanism: Each denoising step computes logits via forward pass, extracts ρEOS from implicit predictions, adjusts length if needed, then applies remasking—all within one loop. This avoids the latency of separate length-adjustment phases.
- Core assumption: The model can maintain coherent generation even when the sequence length changes mid-denoising.
- Evidence anchors:
  - [abstract] "achieves bidirectional length adjustment within a unified denoising process"
  - [Table 6] Single-stage with density signal outperforms both single-stage with confidence and two-stage methods on GSM8K (84.2% vs 84.6% and 73.7%) and MBPP (40.6% vs 38.2%)

## Foundational Learning

- Concept: **Masked Diffusion Language Models (dLLMs)**
  - Why needed here: ρ-EOS operates on masked dLLMs (e.g., LLaDA), which iteratively denoise from fully masked sequences. Understanding this parallel, bidirectional generation paradigm is essential.
  - Quick check question: Can you explain why masked dLLMs require a predefined generation length, unlike autoregressive models?

- Concept: **Implicit vs. Explicit Tokens**
  - Why needed here: ρ-EOS relies on "implicit tokens"—the model's predictions for still-masked positions—to compute density. These are distinct from "explicit tokens" already committed to the output.
  - Quick check question: At denoising step t, what is the difference between an implicit token and an explicit token in the sequence?

- Concept: **EOS Token Semantics in Training**
  - Why needed here: The mechanism depends on how EOS was treated during pretraining. If padding EOS was masked and predicted like any other token, the model learns length awareness.
  - Quick check question: Why might treating padding EOS as part of the response during training cause the model to develop implicit length estimation?

## Architecture Onboarding

- Component map: Prompt + Initial MASK sequence (Linit) -> Forward pass (logits) -> Implicit prediction (argmax) -> ρEOS computation -> Threshold comparison -> Length adjustment -> Remask + decode -> Loop until no MASKs remain

- Critical path: The forward pass is the computational bottleneck. ρEOS extraction adds minimal overhead (simple counting operation). Length adjustment modifies the sequence for the next iteration but does not require additional model calls.

- Design tradeoffs:
  - **Threshold width**: Wider [ρlow, ρhigh] (e.g., [0.2, 0.8]) → more conservative adjustment, higher effective token ratio, but may under-adjust for mismatched lengths. Narrower → more responsive but risk oscillation.
  - **Expansion factor function**: Constant (simple, stable), linear (proportional to deviation), or exponential (aggressive for large deviations). Exponential reduces adjustment steps but may overshoot.
  - **Initial length**: Shorter Linit (e.g., 64) requires more expansion but improves efficiency. Longer Linit may trigger early contraction.

- Failure signatures:
  - **Oscillation**: Length repeatedly expands and contracts without converging. Indicates thresholds too narrow or expansion factor too aggressive.
  - **Premature termination**: Output ends mid-sentence with high ρEOS early. May indicate model bias toward EOS (EOS Trap) or thresholds set too high.
  - **Excessive padding**: Final output has many trailing EOS tokens, low effective token ratio. Indicates contraction not triggered appropriately.
  - **Quality degradation**: Accuracy drops compared to fixed-length baseline. May indicate length instability disrupting denoising coherence.

- First 3 experiments:
  1. **Threshold sweep**: Fix Linit=64, expansion factor=constant. Grid search ρlow ∈ [0.2, 0.4], ρhigh ∈ [0.6, 0.8] on GSM8K. Measure accuracy, Eratio, and Truntime. Identify the Pareto-optimal threshold pair.
  2. **Expansion factor comparison**: Using best thresholds from (1), compare constant vs. linear vs. exponential expansion functions. Track average adjustment steps and final length distribution. Hypothesis: exponential should reduce steps but may increase variance.
  3. **Initial length robustness**: Run ρ-EOS with Linit ∈ [64, 128, 256, 512, 1024] on both GSM8K and MBPP. Compare against DAEDAL baseline. Expect ρ-EOS to show stable accuracy and Eratio across initializations, while DAEDAL degrades with longer Linit.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several important questions emerge from the methodology and results:

1. Can theoretically grounded methods for automatically determining optimal density thresholds [ρlow, ρhigh] be developed, rather than relying on empirical tuning?
2. Does ρ-EOS transfer effectively to other masked dLLM architectures trained with different EOS padding strategies, or is it specifically reliant on LLaDA's training paradigm?
3. How does ρ-EOS perform on open-ended generation tasks (dialogue, summarization, creative writing) beyond structured reasoning domains?
4. Can ρ-EOS be effectively integrated with reinforcement learning fine-tuning to enable efficient variable-length exploration during RL rollouts?

## Limitations

- The theoretical foundation connecting implicit EOS density to length sufficiency is not rigorously derived, relying instead on empirical observations
- Threshold sensitivity and optimal configuration depend heavily on initial length and task type, requiring empirical tuning rather than principled selection
- Generalization to open-ended generation tasks beyond structured domains (math, code) remains unproven and may exhibit different density dynamics

## Confidence

**High Confidence:**
- ρ-EOS successfully enables bidirectional variable-length generation in masked dLLMs
- The approach achieves comparable accuracy to fixed-length baselines while improving efficiency
- Single-stage unified denoising outperforms two-stage methods on evaluated benchmarks

**Medium Confidence:**
- ρEOS serves as a reliable internal signal of generation length sufficiency
- The asymmetric threshold configuration [0.4, 0.8] is near-optimal for Linit=64
- Bidirectional adjustment within one denoising loop is superior to multi-stage approaches

**Low Confidence:**
- ρ-EOS will generalize effectively to all masked dLLM architectures
- The method works equally well across all generation domains (not just math/code)
- The theoretical mechanism connecting EOS density to length sufficiency is fully understood

## Next Checks

**Validation Check 1: Theoretical Analysis of EOS Density Signal**
Conduct a formal analysis of the relationship between implicit EOS density and generation sufficiency across different dLLM training paradigms. Systematically vary the training procedure (e.g., whether padding EOS is masked, the proportion of EOS tokens in training data, the masking strategy) and measure how these factors affect the reliability of ρEOS as a length signal. This would involve training multiple dLLM variants with controlled differences and analyzing the correlation between ρEOS trajectories and generation quality across diverse task types.

**Validation Check 2: Comprehensive Threshold Sensitivity Study**
Perform an exhaustive grid search over threshold combinations [ρlow, ρhigh] across multiple initial lengths (64, 128, 256, 512) and task domains. For each configuration, measure not only accuracy and efficiency metrics but also stability indicators like oscillation frequency and convergence time. Additionally, conduct ablation studies varying the expansion factor function (constant, linear, exponential) to understand their interaction with threshold choices. This would provide principled guidance for threshold selection rather than the current empirical tuning.

**Validation Check 3: Cross-Domain Generalization Evaluation**
Evaluate ρ-EOS on a diverse set of generation tasks including open-ended text generation (story completion, summarization), dialogue response generation, and specialized domains like medical or legal text generation. For each domain, measure whether the standard [0.4, 0.8] thresholds remain effective or require domain-specific tuning. Additionally, test whether ρ-EOS maintains its efficiency advantages when applied to dLLMs with different architectural choices (different attention mechanisms, positional encoding schemes, or denoising objectives).