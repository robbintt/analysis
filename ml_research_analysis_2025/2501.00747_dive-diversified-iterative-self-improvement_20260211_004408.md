---
ver: rpa2
title: 'DIVE: Diversified Iterative Self-Improvement'
arxiv_id: '2501.00747'
source_url: https://arxiv.org/abs/2501.00747
tags:
- diversity
- selection
- pool
- arxiv
- global
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses diversity loss in iterative self-improvement
  (ISI) for large language models, particularly in reasoning tasks where diverse solution
  paths are crucial. The authors propose DIVE, a framework that enhances diversity
  while maintaining performance quality through two key components: Sample Pool Expansion,
  which incorporates responses from all previous iterations to broaden solution exploration,
  and Data Selection, which uses outlier detection and greedy selection to balance
  quality and diversity in preference pairs.'
---

# DIVE: Diversified Iterative Self-Improvement

## Quick Facts
- arXiv ID: 2501.00747
- Source URL: https://arxiv.org/abs/2501.00747
- Reference count: 24
- Primary result: DIVE achieves 10-45% relative increases in output diversity metrics compared to vanilla ISI while preserving accuracy on MATH and GSM8k datasets.

## Executive Summary
This paper addresses diversity loss in iterative self-improvement (ISI) for large language models, particularly in reasoning tasks where diverse solution paths are crucial. The authors propose DIVE, a framework that enhances diversity while maintaining performance quality through two key components: Sample Pool Expansion, which incorporates responses from all previous iterations to broaden solution exploration, and Data Selection, which uses outlier detection and greedy selection to balance quality and diversity in preference pairs. Experiments on MATH and GSM8k datasets demonstrate that DIVE achieves 10-45% relative increases in output diversity metrics compared to vanilla ISI while preserving accuracy. The ablation studies confirm both components' significance in driving these improvements.

## Method Summary
DIVE is a framework for iterative self-improvement that addresses diversity loss through two components: Sample Pool Expansion, which accumulates responses across all iterations into a global pool (Dt_pool = ∪Dt_pool from all iterations), and Data Selection, which filters outliers using Isolation Forest on Sentence-BERT embeddings and applies greedy selection to construct preference pairs maximizing diversity. The method uses Mistral-7B as the base model, performing SFT followed by 6 iterations of ISI. Each iteration samples K=50 responses per question (top_p=0.95, T=0.7), filters outliers, greedily selects up to P=5 diverse preference pairs per question, and trains with a combined DPO+NLL objective (α=0.5, β=0.4, lr=3e-8) using AdamW optimizer.

## Key Results
- DIVE achieves 10-45% relative increases in output diversity metrics compared to vanilla ISI
- DIVE maintains accuracy parity with vanilla ISI while significantly improving diversity
- Sample Pool Expansion and Data Selection components are both necessary for the observed improvements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Accumulating responses across iterations preserves diverse reasoning paths that would otherwise be permanently lost.
- Mechanism: Global data usage maintains Dt_pool = ∪Dt_pool from all iterations, creating a growing archive of solution strategies. When the model begins converging to narrow solution patterns in later iterations, previously generated diverse responses remain available for preference pair construction, counteracting the accumulation effect of diversity reduction.
- Core assumption: Assumes diverse solutions generated in early iterations remain valid for training later model versions (no distribution shift invalidates earlier solutions).
- Evidence anchors:
  - [abstract] "continuous training on self-generated data leads to reduced output diversity"
  - [section 4.1] "diversity lost in early iterations is difficult to recover through increased sampling alone... requiring only 60 total samples per question compared to 210 for Selection+Increased Sampling"
  - [corpus] Related work on dimensional collapse (arXiv:2504.20644) confirms diversity loss is a structural problem in iterative training, not just sampling insufficiency.
- Break condition: If early-iteration solutions become systematically incompatible with later model distributions (e.g., reasoning style drifts significantly), global data may introduce noise rather than diversity.

### Mechanism 2
- Claim: Outlier filtering before selection prevents diversity-maximizing choices from degrading model quality.
- Mechanism: Isolation Forest identifies embeddings that deviate significantly from the response distribution, removing "low-quality, outlier responses that harm the model's performance" before greedy selection. This creates a bounded diversity space where the selection algorithm operates only on plausible solutions.
- Core assumption: Assumes semantic embeddings (Sentence-BERT) reliably capture quality-relevant features; outliers in embedding space correspond to genuinely problematic responses rather than simply novel valid approaches.
- Evidence anchors:
  - [section 2.2.2] "we hypothesize that maximizing diversity may lead to selecting low-quality, outlier responses that harm the model's performance"
  - [section 2.2.2] "using distances in the embedding space, we identify and exclude extreme outliers... to maintain response quality"
  - [corpus] Weak direct evidence; no corpus papers validate this specific outlier-removal-for-quality hypothesis.
- Break condition: If the embedding model systematically penalizes genuinely novel but correct reasoning styles (e.g., unconventional notation), outlier filtering may reintroduce the diversity loss it aims to prevent.

### Mechanism 3
- Claim: Greedy selection over a filtered pool explicitly optimizes the diversity of training examples, not just their individual correctness.
- Mechanism: Starting from a random seed, each iteration adds the response maximizing the diversity metric of the growing selected set. This ensures preference pairs span the widest possible reasoning space within quality constraints, preventing the model from overfitting to a single solution pattern.
- Core assumption: Assumes the diversity metric (distinct N-grams or embedding similarity) correlates with meaningful reasoning diversity—not just surface variation.
- Evidence anchors:
  - [abstract] "Data Selection... uses outlier detection and greedy selection to balance quality and diversity in preference pairs"
  - [section 2.2.2] "preliminary experiments show that the diversity of the examples selected for preference learning... significantly impacts the model's ability to generate diverse outputs"
  - [section 4.3] "improvements generalize to more sophisticated metrics [NV-Embed, Stella], confirming the robustness of our approach"
  - [corpus] arXiv:2502.11027 supports that "meaningful response diversity" correlates with solution accuracy in scaling inference.
- Break condition: If the greedy diversity metric captures syntactic variation without semantic difference (e.g., paraphrasing without alternative solution paths), the mechanism may inflate diversity scores without improving reasoning breadth.

## Foundational Learning

- Concept: Direct Preference Optimization (DPO)
  - Why needed here: DIVE builds on DPO as its core preference learning method; understanding how DPO shapes policy via preference pairs is prerequisite to grasping why diverse pair selection matters.
  - Quick check question: Can you explain why DPO avoids training an explicit reward model and how it uses the reference model πref in its objective?

- Concept: Model Collapse in Self-Training
  - Why needed here: The paper's central problem—diversity loss in iterative self-improvement—is a manifestation of model collapse; recognizing this connection clarifies the stakes and failure modes.
  - Quick check question: What happens to a model's output distribution when it is repeatedly trained on its own generated data without external signal?

- Concept: Greedy Submodular Optimization
  - Why needed here: The data selection mechanism uses greedy selection to maximize diversity; understanding why greedy approaches work for diversity metrics (often submodular) helps diagnose when the method may fail.
  - Quick check question: For a set function measuring diversity, why does a greedy algorithm often achieve near-optimal solutions, and when might it fail?

## Architecture Onboarding

- Component map:
  - Response Generator (Mt-1) -> samples K responses per question
  - Pool Manager -> accumulates Dt_pool across iterations (global data)
  - Quality Filter (Isolation Forest) -> removes embedding-space outliers
  - Diversity Selector (Greedy) -> constructs preference pairs maximizing diversity
  - Preference Trainer (DPO + NLL) -> produces Mt from Mt-1

- Critical path: Pool expansion -> outlier filtering -> greedy selection -> preference pair construction -> DPO training. Errors in pool management (e.g., failing to accumulate global data) propagate downstream and cannot be recovered by selection alone.

- Design tradeoffs:
  - Larger K increases candidate diversity but raises sampling cost; paper shows K=50 outperforms K=10 only with proper selection.
  - Aggressive outlier filtering preserves quality but may discard genuinely novel solutions; Isolation Forest contamination parameter requires tuning.
  - Greedy selection is O(n²) in pool size; large pools become computationally expensive.

- Failure signatures:
  - Diversity metrics improve but accuracy drops -> outlier filtering too permissive or greedy selection ignoring quality.
  - No diversity improvement across iterations -> global data not being accumulated; check pool manager.
  - Diversity improves but only on training distribution, not test -> selection overfitting to training set artifacts.

- First 3 experiments:
  1. Baseline replication: Run vanilla ISI (K=10, no global data, no selection) for 3 iterations on GSM8K subset (500 questions); confirm diversity decline using Distinct-N and SentBERT metrics.
  2. Ablation component test: Add only global data (no selection), then only selection (no global data), measuring diversity vs. accuracy to isolate each component's contribution.
  3. Hyperparameter sensitivity: Vary Isolation Forest contamination (0.05, 0.1, 0.2) and K (10, 30, 50); identify settings where diversity gains persist without accuracy degradation.

## Open Questions the Paper Calls Out
- Can DIVE's diversity-preserving benefits generalize beyond mathematical reasoning to other domains such as code generation, creative writing, or multi-turn dialogue? The authors explicitly state in Section 7 (Limitations): "Our study focuses exclusively on mathematical reasoning tasks (MATH and GSM8k)... the generalization of our approach to other domains remains to be explored."

- Can adaptive sampling strategies based on question difficulty outperform the current fixed-sampling approach in balancing computational cost and diversity gains? Section 7 states: "Questions of different difficulty levels might benefit from adaptive sampling strategies to better balance computational cost and diversity gains." Section 4.2 shows harder questions maintain higher diversity and suffer less diversity loss.

## Limitations
- The effectiveness of outlier filtering hinges on the quality of Sentence-BERT embeddings for detecting low-quality responses; if the embedding space conflates unconventional-but-correct reasoning with genuinely flawed outputs, the diversity gains could be illusory.
- The greedy selection's computational cost scales quadratically with the pool size, creating practical limits for very long training runs.
- The claim that early-iteration diversity is preserved relies on the assumption that solution strategies remain valid across model versions, which may break down with significant reasoning style drift.

## Confidence
- High: The core empirical result that DIVE increases diversity metrics by 10-45% relative to vanilla ISI is well-supported by the experimental results.
- Medium: The claim that Sample Pool Expansion is essential (vs. Selection+Increased Sampling requiring 3.5× more samples) is based on the ablation results, but the exact mechanism by which global data prevents diversity collapse is not fully characterized.
- Medium: The claim that outlier filtering prevents quality degradation is supported by the ablation study, but the corpus lacks strong external validation of the specific outlier-detection approach.
- Low: The assertion that greedy selection explicitly optimizes meaningful reasoning diversity (not just surface variation) relies on indirect evidence; the diversity metrics used may not fully capture semantic solution space coverage.

## Next Checks
1. Outlier sensitivity test: Run the DIVE pipeline with Isolation Forest contamination rates at 0.05, 0.1, and 0.2; plot diversity and accuracy curves to identify the threshold where outlier filtering begins to harm performance.
2. Greedy selection ablation: Replace the greedy diversity maximization with random selection from the filtered pool; compare diversity and accuracy to isolate the effect of explicit diversity optimization.
3. Early-iteration diversity preservation: On a held-out set of questions, compare the diversity of responses generated by a 1-iteration DIVE model vs. a 6-iteration DIVE model; verify that the 6-iteration model's responses retain the breadth of solution strategies seen in early iterations.