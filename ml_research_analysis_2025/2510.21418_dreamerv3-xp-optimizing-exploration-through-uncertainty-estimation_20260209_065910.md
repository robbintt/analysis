---
ver: rpa2
title: 'DreamerV3-XP: Optimizing exploration through uncertainty estimation'
arxiv_id: '2510.21418'
source_url: https://arxiv.org/abs/2510.21418
tags:
- reward
- learning
- exploration
- replay
- latent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DreamerV3-XP improves exploration and learning efficiency by introducing
  a prioritized replay buffer that scores trajectories based on return, VAE reconstruction
  loss, and value error, and an intrinsic reward derived from reward prediction disagreement
  among an ensemble of world models. The authors reproduced key DreamerV3 results
  on Atari100k and DeepMind Control Visual Benchmark tasks, confirming the original
  performance.
---

# DreamerV3-XP: Optimizing exploration through uncertainty estimation

## Quick Facts
- arXiv ID: 2510.21418
- Source URL: https://arxiv.org/abs/2510.21418
- Reference count: 12
- Authors: Lukas Bierling; Davide Pasero; Jan-Henrik Bertrand; Kiki Van Gerwen
- Key outcome: Prioritized replay consistently reduced dynamics model loss and improved learning speed, especially in sparse-reward settings

## Executive Summary
DreamerV3-XP introduces two key innovations for improving exploration and learning efficiency in model-based reinforcement learning: a prioritized replay buffer that samples trajectories based on return, VAE reconstruction loss, and value error, and an intrinsic reward derived from reward prediction disagreement among an ensemble of world models. The authors reproduced key DreamerV3 results on Atari100k and DeepMind Control Visual Benchmark tasks, confirming the original performance. Their method demonstrates that targeted sampling of high-information transitions and uncertainty-driven exploration can accelerate world model learning and policy improvement, particularly in environments with sparse rewards.

## Method Summary
The method combines prioritized experience replay with ensemble-based intrinsic rewards. Trajectories are scored using a priority formula that weights return, VAE reconstruction error, and critic value error, then sampled proportionally to these scores to focus training on high-value, uncertain transitions. An ensemble of K world models generates reward predictions, and intrinsic rewards are computed from the variance across these predictions, encouraging exploration of states with high epistemic uncertainty. The extrinsic-intrinsic reward balance is dynamically adjusted using either exponential decay or EMA-based scheduling that responds to learning progress.

## Key Results
- Prioritized replay consistently reduced dynamics model loss and improved learning speed across all tested tasks
- Episode return curves showed steeper early learning, particularly in sparse-reward settings like Cup Catch
- Latent reward disagreement mechanism showed modest gains in learning speed in some tasks, with one EMA run for Cup Catch exhibiting a much steeper learning curve
- Baseline DreamerV3 performance was successfully reproduced on Atari100k and DeepMind Control tasks

## Why This Works (Mechanism)

### Mechanism 1: Prioritized Replay Buffer
- Claim: Scoring and sampling trajectories based on return, reconstruction error, and value error accelerates world model learning and policy improvement.
- Mechanism: Trajectory priority score s_i = (λ_r + λ_δ × δ_i) × R_i + λ_ε × ε_i combines task return (R_i), VAE reconstruction error (ε_i), and critic value error (δ_i). This focuses training on high-value transitions where the model is most uncertain.
- Core assumption: Uniform sampling wastes capacity on low-information transitions; prediction errors indicate underlearned dynamics that benefit from prioritized updates.
- Evidence anchors:
  - [abstract] "prioritized replay consistently reduced dynamics model loss and improved learning speed, especially in sparse-reward settings"
  - [Results] "Figure 3 shows that our optimized replay consistently reduces the dynamics loss across all tested tasks"
  - [Results] "the episode return curve rises more steeply early in training, indicating faster learning"
  - [corpus] "Uncertainty Prioritized Experience Replay" provides related evidence that TD-error prioritization improves sample efficiency, though uses different scoring metrics
- Break condition: If reconstruction and value errors don't correlate with learning progress in your domain (e.g., highly stochastic environments), prioritization may overfit to noisy signals without improving policy performance.

### Mechanism 2: Latent Reward Disagreement for Exploration
- Claim: Intrinsic reward computed from ensemble disagreement over predicted rewards encourages exploration of epistemically uncertain states.
- Mechanism: r_intr = (1/L) × Σ[mean_reward + (1/K) × Σ(ẑ_k - mean_reward)²] across ensemble members K and imagination horizon L. Total reward: r_total = λ × r_ext + (1-λ) × r_intr. High variance signals uncertain reward predictions worth exploring.
- Core assumption: Reward prediction variance approximates epistemic uncertainty; states with uncertain reward predictions contain actionable information for policy learning.
- Evidence anchors:
  - [abstract] "intrinsic reward derived from reward prediction disagreement among an ensemble of world models"
  - [Methods] "High variance indicates epistemic uncertainty over the predicted reward, and thus encourages exploration"
  - [Results] "latent reward disagreement mechanism showed modest gains in learning speed in some tasks" (Krull, Cup Catch with EMA-based λ)
  - [corpus] "InDRiVE" demonstrates similar disagreement-based intrinsic rewards for vehicle exploration, supporting the general approach
- Break condition: If ensemble members converge quickly or reward predictions are deterministic in your environment, disagreement signal diminishes early, providing no exploration benefit. May also fail if uncertain states aren't reward-relevant.

### Mechanism 3: Dynamic Extrinsic-Intrinsic Reward Weighting
- Claim: Adapting the λ weighting between extrinsic and intrinsic rewards based on learning progress improves exploration-exploitation balance.
- Mechanism: Two strategies tested: (1) exponential decay reducing intrinsic weight over time, (2) EMA-based adjustment where λ decreases when performance improves and increases during stagnation. This shifts from exploration to exploitation as the model matures.
- Core assumption: Early training benefits from uncertainty-driven exploration; later training should exploit learned knowledge. Learning progress can be approximated by episode return trends.
- Evidence anchors:
  - [Methods] "λ is decreased when performance tends to improve and increased when learning stagnates or regresses"
  - [Results] "one of the EMA runs for Cup Catch exhibited a much steeper learning curve"
  - [corpus] Weak corpus evidence—no direct comparisons to fixed-λ or alternative scheduling strategies found
- Break condition: If task requires sustained exploration throughout training (e.g., non-stationary environments), decaying intrinsic rewards may cause premature exploitation. EMA-based adjustment may oscillate if returns are highly variable.

## Foundational Learning

- **Variational Autoencoders (VAEs) for State Representation**
  - Why needed here: DreamerV3 encodes observations into latent states via VAE; reconstruction error is a key prioritization signal
  - Quick check question: Can you explain why reconstruction loss indicates model uncertainty about a transition?

- **Ensemble Methods and Epistemic Uncertainty**
  - Why needed here: Intrinsic reward depends on understanding how ensemble disagreement quantifies uncertainty
  - Quick check question: What's the difference between epistemic and aleatoric uncertainty, and which does ensemble disagreement capture?

- **World Models and Imagination-Based Planning**
  - Why needed here: DreamerV3 trains actor-critic entirely in latent space via imagined rollouts
  - Quick check question: How does training in imagination improve sample efficiency compared to environment interaction?

## Architecture Onboarding

- **Component map:**
  ```
  Environment → Encoder → RSSM (h_t, z_t) → Decoder, Reward Predictor, Value Predictor
                              ↓
                    Ensemble of K World Models (for disagreement)
                              ↓
                    Intrinsic Reward Calculator (variance over K predictions)
                              ↓
  Replay Buffer ← Trajectory Storage ← Priority Scorer (R_i, ε_i, δ_i)
        ↓
  Actor-Critic (trained via imagination)
  ```

- **Critical path:**
  1. Reproduce baseline DreamerV3 on 2-3 Atari/DMC tasks first
  2. Add prioritized replay buffer with initial weights λ_r=1.0, λ_ε=0.5, λ_δ=0.5
  3. Validate dynamics loss decreases relative to uniform sampling
  4. Add ensemble (K=3-5 world models) for reward disagreement
  5. Tune λ scheduling strategy (constant → exponential decay → EMA-based)

- **Design tradeoffs:**
  - Ensemble size K: Larger K improves uncertainty estimation but increases memory/compute linearly
  - Priority weights (λ_r, λ_ε, λ_δ): Task-dependent; sparse rewards may need higher λ_ε/λ_δ relative to λ_r
  - λ scheduling: Exponential decay is simpler but less adaptive than EMA-based; EMA requires tuning smoothing parameter

- **Failure signatures:**
  - Prioritized replay increases losses instead of decreasing: weight balance may over-prioritize low-return, high-error noise
  - Intrinsic reward shows zero disagreement early: ensemble initialized too similarly or learning rate too high causing fast convergence
  - EMA-based λ oscillates wildly: increase smoothing factor or use longer return window
  - No improvement over baseline on dense-reward tasks: expected—paper shows gains primarily in sparse-reward settings

- **First 3 experiments:**
  1. Ablate prioritized replay alone on Cup Catch (sparse reward): compare dynamics loss curves and episode returns vs uniform sampling
  2. Test ensemble sizes K∈{3,5,7} on Krull: measure intrinsic reward variance magnitude and correlation with learning speed
  3. Compare λ scheduling strategies (fixed λ=0.5, exponential decay, EMA-based) on Reacher (hard): track when intrinsic reward weight reaches near-zero and final performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the latent reward disagreement mechanism yield consistent and significant improvements across a broader range of environments beyond the specific subset tested?
- Basis in paper: [explicit] The Conclusion states that while gains were modest, they suggest "potential for further research, such as evaluating its impact across a broader range of tasks and environments."
- Why unresolved: The authors evaluated the method on only five tasks due to computational constraints, leaving its generalizability to the full benchmark suites unconfirmed.
- What evidence would resolve it: Evaluation of DreamerV3-XP on the complete Atari100k and DeepMind Control suites showing statistically significant gains.

### Open Question 2
- Question: Can dynamic adjustment of the intrinsic reward weight ($\lambda$) via EMA gradients provide statistically significant benefits over static or decaying strategies?
- Basis in paper: [explicit] The Results section notes that while one EMA run showed a steep learning curve, "resource constraints" prevented validating this with more seeds.
- Why unresolved: The promising result for the EMA strategy was observed in a single run, making it impossible to distinguish from noise.
- What evidence would resolve it: Experiments comparing dynamic vs. static $\lambda$ strategies across multiple random seeds (e.g., n > 5) to establish statistical significance.

### Open Question 3
- Question: How sensitive is the performance of the prioritized replay buffer to the specific weighting coefficients ($\lambda_r, \lambda_\delta, \lambda_\epsilon$) used in the trajectory scoring formula?
- Basis in paper: [inferred] The Methods section introduces a complex scoring formula $s_i$, but the paper does not provide an ablation study or sensitivity analysis for these hyperparameters.
- Why unresolved: It is unclear if the specific weights chosen were optimal or if the system is robust to different weightings of return, reconstruction loss, and value error.
- What evidence would resolve it: An ablation study demonstrating performance changes as the weighting coefficients $\lambda$ are varied independently.

## Limitations
- Missing hyperparameters for priority weights λ_r, λ_ε, λ_δ that could significantly affect results
- Limited task diversity—most gains observed in sparse-reward environments, with unclear benefits for dense-reward settings
- No comparison to alternative exploration methods like curiosity-based or count-based approaches

## Confidence
- Prioritized replay mechanism: High confidence based on consistent dynamics loss reduction and faster learning curves across multiple tasks
- Intrinsic reward disagreement: Medium confidence as results show only modest gains in some tasks without clear superiority over baseline methods
- Dynamic weighting mechanism: Low confidence due to limited ablation studies and weak corpus support for the EMA-based scheduling approach

## Next Checks
1. **Ablation study on priority weights**: Systematically vary λ_r, λ_ε, λ_δ in priority scoring to determine optimal balance and verify that reconstruction/value errors contribute meaningfully beyond return-based prioritization alone.

2. **Ensemble size sensitivity**: Test intrinsic reward disagreement across different ensemble sizes (K=3,5,7) on sparse-reward tasks to quantify uncertainty estimation quality versus computational overhead.

3. **Comparison with alternative exploration**: Implement and compare against a simple curiosity-driven exploration baseline (e.g., ICM) on the same tasks to establish whether disagreement-based intrinsic rewards offer advantages over established methods.