---
ver: rpa2
title: 'FormFactory: An Interactive Benchmarking Suite for Multimodal Form-Filling
  Agents'
arxiv_id: '2506.01520'
source_url: https://arxiv.org/abs/2506.01520
tags:
- form
- field
- agents
- form-filling
- click
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FormFactory, the first interactive benchmark
  for multimodal form-filling agents. It addresses the gap between existing GUI agents
  and the complex, real-world task of form completion, which requires fine-grained
  visual layout reasoning, semantic alignment, and interaction across diverse field
  types.
---

# FormFactory: An Interactive Benchmarking Suite for Multimodal Form-Filling Agents

## Quick Facts
- arXiv ID: 2506.01520
- Source URL: https://arxiv.org/abs/2506.01520
- Reference count: 40
- Zero-shot MLLM performance on form-filling: <5% episodic completion accuracy

## Executive Summary
This paper introduces FormFactory, the first interactive benchmark for multimodal form-filling agents. It addresses the gap between existing GUI agents and the complex, real-world task of form completion, which requires fine-grained visual layout reasoning, semantic alignment, and interaction across diverse field types. The authors develop a web-based platform with 25 realistic forms spanning 8 domains and a dataset of 13,800 field-value annotations from 1,250 instances. FormFactory simulates high-fidelity form interactions and enables both automated evaluation and scalable benchmarking. Zero-shot experiments with state-of-the-art MLLMs show that all models perform poorly—under 5% accuracy—revealing the task's difficulty and the limitations of current vision-language alignment.

## Method Summary
The FormFactory platform uses a Flask-based web interface paired with ground-truth key-value annotations. Agents observe visual states (screenshots), generate action sequences (Click, Type), and the backend scorer compares submitted values against annotations to produce field-level accuracy reports. The benchmark includes 25 web forms across 8 domains with 1,250 instances and 13,800 field-value annotations. Zero-shot evaluation is performed on MLLMs (GPT-4o, Gemini, Claude, Qwen-VL-Max) that generate GUI interaction commands executed via PyAutoGUI. A ruler-enhanced strategy overlays horizontal and vertical axes on form screenshots to provide pixel-scale spatial references for grounding.

## Key Results
- All tested MLLMs achieve <5% episodic form completion accuracy, demonstrating the task's inherent difficulty
- Click accuracy remains below 10% across models, indicating fundamental limitations in visual grounding
- Ruler-enhanced strategy provides only marginal improvements, especially on complex forms with many fields
- Performance degrades significantly with increasing field count and type diversity, revealing scalability bottlenecks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: An interactive web-based benchmark with automated backend scoring enables scalable, reproducible evaluation of form-filling agents under controlled conditions.
- Mechanism: The FormFactory platform uses a Flask-based web interface paired with ground-truth key-value annotations. Agents observe visual states (screenshots), generate action sequences (Click, Type), and the backend scorer compares submitted values against annotations to produce field-level accuracy reports.
- Core assumption: Simulated forms with realistic layouts and field diversity capture enough of the complexity of real-world platforms to make benchmark results predictive of actual performance.
- Evidence anchors:
  - [abstract]: "web-based platform with 25 realistic forms spanning 8 domains and a dataset of 13,800 field-value annotations from 1,250 instances"
  - [section 3.1]: "developed a dedicated simulation platform using Python and Flask... supports automatic backend scoring based on ground-truth key-value annotations"
  - [corpus]: FormGym (arXiv:2506.14079) addresses similar paperwork challenges in the pure-image domain, suggesting benchmark demand exists.
- Break condition: If agents overfit to simulated form templates or if the platform lacks real-world edge cases (dynamic validation, CAPTCHAs, multi-step authentication), benchmark performance may not transfer.

### Mechanism 2
- Claim: A ruler-enhanced visual grounding strategy can modestly improve click accuracy by providing pixel-scale spatial references, though gains are marginal on complex forms.
- Mechanism: Horizontal and vertical ruler axes are overlaid on form screenshots before feeding them to the MLLM. The model is instructed to use these markers when predicting Click(x, y) coordinates, giving explicit geometric context for spatial reasoning.
- Core assumption: MLLMs can learn to parse and utilize auxiliary visual reference information during inference without additional training.
- Evidence anchors:
  - [abstract]: "A simple ruler-enhanced strategy improves spatial grounding slightly, but click accuracy remains low, especially on complex forms"
  - [section 4.2]: "overlay ruler-like axes along the horizontal and vertical edges of the form screenshot... serve as visual references"
  - [corpus]: Weak—no direct corpus support for ruler-based grounding mechanisms in related work.
- Break condition: If forms have dense, overlapping elements or irregular layouts, ruler references provide insufficient disambiguation; also assumes models attend to the rulers rather than ignoring them.

### Mechanism 3
- Claim: Hierarchical evaluation (atomic field-level metrics vs. episodic end-to-end completion) isolates failure modes between spatial grounding and semantic alignment.
- Mechanism: Atomic evaluation measures Click accuracy (correct UI element selection) and Value accuracy (correct content) per field type (String, Dropdown, Checkbox, etc.). Episodic evaluation measures full form completion. This separation reveals whether failures stem from not knowing where to click or what to enter.
- Core assumption: Failures can be cleanly attributed to either spatial grounding or semantic understanding, rather than being fundamentally coupled.
- Evidence anchors:
  - [abstract]: "no model surpasses 5% accuracy, underscoring the inherent difficulty... reveal significant limitations in current models' visual layout reasoning and field-value alignment abilities"
  - [section 5.1.2]: "Atomic evaluation tests model performance on individual field types... Episodic evaluation measures end-to-end form completion"
  - [corpus]: WAREX (arXiv:2510.03285) discusses reliability evaluation in controlled vs. real environments, relevant to metric design.
- Break condition: If click and value errors compound non-linearly (e.g., a misclick cascades into wrong field population), isolated metrics may understate true task difficulty.

## Foundational Learning

- Concept: Visual grounding (pixel-to-element mapping)
  - Why needed here: Form-filling requires agents to translate visual perception of form layouts into precise click coordinates. The paper shows click accuracy <10% across models, indicating grounding is a core bottleneck.
  - Quick check question: Given a form screenshot with a text field at approximately (320, 450), can your model reliably output Click(320, 450) rather than an offset coordinate?

- Concept: Vision-language alignment (semantic field matching)
  - Why needed here: Agents must map user-provided content (e.g., resume text) to the correct form fields based on visual labels and context, not just spatial proximity.
  - Quick check question: If a resume lists "Software Engineer Intern" under experience, will the model correctly classify it as an internship rather than full-time employment when filling a job application?

- Concept: Sequential decision-making under partial observability
  - Why needed here: Multi-page forms with dropdowns, date pickers, and conditional fields require maintaining state across interaction steps. The paper formalizes this as a page-level sequential decision process.
  - Quick check question: After selecting "Other" from a dropdown, can your agent recognize and fill the newly revealed text field before proceeding?

## Architecture Onboarding

- Component map:
  - Web frontend -> Agent execution module -> Backend scorer
  - Form rendering -> Screenshot capture -> MLLM action generation -> PyAutoGUI execution -> Ground-truth comparison

- Critical path:
  1. Load form instance → render web page → capture screenshot
  2. Pass (screenshot + user input document) to MLLM with action-generation prompt
  3. Parse model output into Click(x, y) / Type(text) actions
  4. Execute actions via PyAutoGUI on live browser
  5. Submit form → trigger backend scoring → return atomic and episodic metrics

- Design tradeoffs:
  - **Simulated vs. real platforms**: Simulation enables controlled, reproducible evaluation but may lack real-world edge cases (network latency, dynamic validation)
  - **Greedy page-wise policy vs. step-by-step**: Generating all actions per page in one pass is efficient but may accumulate errors; step-by-step allows re-planning after each action
  - **Value scoring design**: Current design credits correct value generation regardless of whether it was entered into the correct field; stricter evaluation (value-in-correct-field) would yield lower scores

- Failure signatures:
  - **Click accuracy <10%**: Model cannot reliably ground visual elements to pixel coordinates
  - **Value accuracy high but episodic accuracy <5%**: Model understands content but cannot execute correct UI interactions
  - **Performance degradation with field count**: Indicates scalability bottleneck in handling dense layouts
  - **Near-zero accuracy on checkboxes/dropdowns**: Suggests complex multi-step interaction patterns are not captured

- First 3 experiments:
  1. **Baseline zero-shot evaluation**: Run GPT-4o, Gemini 2.5 Pro, Claude 3.7 Sonnet, Qwen-VL-Max on all 25 forms without ruler enhancement; record atomic (Click, Value) and episodic metrics to establish difficulty floor.
  2. **Ruler-enhanced ablation**: Re-run evaluation with ruler axes overlaid on screenshots; measure Click accuracy delta across form complexity levels (Groups 1-5) to quantify spatial grounding improvement.
  3. **Field complexity analysis**: Segment results by field count and field type diversity; plot 3D surface of Click accuracy vs. (field count, field types) to identify compounding failure modes and prioritize model improvements.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What advanced strategies beyond visual rulers can effectively improve pixel-level grounding for agents interacting with dense or complex form layouts?
- Basis in paper: [explicit] Section 5.3 concludes that the ruler-enhanced strategy is "insufficient for handling more challenging visual reasoning" and explicitly highlights the "need for more advanced strategies."
- Why unresolved: The paper demonstrates that while rulers help slightly, click accuracy remains critically low (<10%) as form complexity increases, indicating current models lack inherent spatial precision.
- What evidence would resolve it: A novel method (e.g., fine-tuning on coordinate data or hierarchical visual attention) that sustains high click accuracy (>80%) on the benchmark's most complex, field-dense forms.

### Open Question 2
- Question: How does model performance differ when evaluated strictly on successful field entry rather than just semantic value generation?
- Basis in paper: [explicit] Section 5.2 notes that current Value scores are high because they do not check if values are entered into the correct UI location, and explicitly states, "We plan to adopt this stricter evaluation protocol in future iterations."
- Why unresolved: The current metric inflates performance by decoupling reasoning from interaction; the true success rate of mapping semantic content to specific screen coordinates remains unknown.
- What evidence would resolve it: A re-evaluation of the benchmark using an interaction-dependent metric that awards points only if the correct value is found in the correct DOM element after execution.

### Open Question 3
- Question: Can model architectures be adapted to mitigate the compounding performance degradation caused by increasing field counts and type diversity?
- Basis in paper: [inferred] Section 5.4.3 analyzes the "joint effects" of field count and type, showing that performance plummets when both are high, identifying a "scalability bottleneck."
- Why unresolved: The results suggest current MLLMs struggle to maintain context and visual reasoning over long, complex sequences of heterogeneous input fields.
- What evidence would resolve it: An agent architecture that exhibits a flat or significantly flatter accuracy curve across the difficulty groups (1-5) defined in the paper's analysis.

## Limitations

- Simulated environment may not capture real-world edge cases like dynamic validation, CAPTCHAs, and multi-step authentication
- Ruler-enhanced strategy shows only marginal improvements, suggesting simple visual references are insufficient for complex forms
- Current value scoring decouples semantic correctness from correct field selection, potentially inflating performance metrics

## Confidence

- **High confidence**: The benchmark infrastructure design and zero-shot evaluation methodology are well-specified and reproducible; the reported <5% episodic completion is a robust finding
- **Medium confidence**: Hierarchical evaluation effectively isolates failure modes, though coupling between spatial and semantic errors remains uncertain
- **Low confidence**: Ruler-enhanced strategy effectiveness and real-world generalizability are the least certain claims due to limited empirical support

## Next Checks

1. **Cross-platform transferability test**: Deploy the same evaluation methodology on 3-5 real-world web forms (e.g., government forms, job applications) to measure performance degradation and identify simulation-to-reality gaps.

2. **Ruler mechanism ablation study**: Conduct controlled experiments varying ruler visibility, positioning, and instruction phrasing to quantify their impact on click accuracy across different form complexities and determine optimal configurations.

3. **Error propagation analysis**: Systematically track how click errors cascade into value entry failures across multi-field forms to validate whether atomic evaluation metrics adequately predict episodic completion rates and identify non-linear compounding effects.