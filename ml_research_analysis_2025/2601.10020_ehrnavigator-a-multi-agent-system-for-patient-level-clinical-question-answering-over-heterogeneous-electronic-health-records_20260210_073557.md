---
ver: rpa2
title: 'EHRNavigator: A Multi-Agent System for Patient-Level Clinical Question Answering
  over Heterogeneous Electronic Health Records'
arxiv_id: '2601.10020'
source_url: https://arxiv.org/abs/2601.10020
tags:
- clinical
- ehrnavigator
- question
- data
- structured
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces EHRNavigator, a multi-agent system for patient-level
  clinical question answering over heterogeneous EHR data. It addresses the challenge
  of efficiently retrieving and synthesizing information from structured and unstructured
  EHR sources to support clinical decision-making.
---

# EHRNavigator: A Multi-Agent System for Patient-Level Clinical Question Answering over Heterogeneous Electronic Health Records

## Quick Facts
- arXiv ID: 2601.10020
- Source URL: https://arxiv.org/abs/2601.10020
- Reference count: 0
- Achieved 86% accuracy on real-world clinical questions from Yale New Haven Hospital

## Executive Summary
EHRNavigator is a multi-agent system designed to answer clinical questions by retrieving and synthesizing information from heterogeneous electronic health records (EHRs). It addresses the challenge of efficiently querying both structured and unstructured EHR data to support clinical decision-making. The system employs specialized agents for schema exploration, structured querying, semantic retrieval, and multimodal answer synthesis, enabling natural language interaction without requiring schema-specific training. Evaluated on four datasets covering structured, unstructured, and multimodal queries, EHRNavigator achieved strong performance with median response times under 12 seconds, demonstrating practical feasibility in bridging the gap between benchmark evaluation and real-world clinical deployment.

## Method Summary
EHRNavigator uses a modular multi-agent architecture with specialized agents for different tasks: Table Reviewer (schema exploration), SQL Writer (structured querying), Note Retriever (semantic search over clinical notes), and Answer Synthesizer (evidence reconciliation). The system processes queries through a structured module (using SQL execution on MIMIC-III and OMOP CDM schemas), an unstructured module (semantic retrieval of clinical notes with temporal chunking), and a synthesis module that combines evidence across modalities. Backbones include GPT-4o, LLaMA3-70B-Instruct, and LLaMA3.1-70B-Instruct. Retrieval uses BGE-large-en embeddings, with optional LoRA fine-tuning (rank=16, alpha=32, lr=1e-5, batch=8, 1 epoch) on the answer synthesis module only. The system was evaluated on DrugEHRQA (multimodal), EHRSQL (structured), EHRNoteQA (unstructured), and YNHHQA (real-world OMOP) datasets.

## Key Results
- Achieved 86% accuracy on real-world clinical questions from Yale New Haven Hospital
- Outperformed direct LLM prompting across all query types: drug-only (95% vs 75.7%), lab-only (93.7% vs 73.7%), combination (95% vs 63.1%)
- Maintained median response times under 12 seconds across all query types

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing complex clinical QA into specialized sub-tasks improves accuracy over direct LLM prompting.
- Mechanism: A coordinator orchestrates specialized agents (Table Reviewer, SQL Writer, Note Retriever, Answer Synthesizer), each handling a bounded cognitive load. The Table Reviewer caches schema descriptions; SQL Writer generates queries with access to sampled rows; retrieval agents narrow search spaces before synthesis. This modular decomposition mirrors clinical diagnostic workflows.
- Core assumption: Errors are easier to isolate and correct in modular pipelines than in monolithic prompting.
- Evidence anchors:
  - [abstract] "The system uses specialized agents to decompose complex queries, retrieve relevant information from diverse EHR schemas, and synthesize answers through evidence-grounded reasoning."
  - [Page 6] "This consistent superiority validates the effectiveness of decomposing the complex structured query generation task into modular, manageable subtasks for LLMs."
  - [corpus] Weak corpus signal—neighboring multi-agent medical papers (MedAgentBoard, UniPACT) lack comparative FMR scores, so generalization to multi-agent efficacy remains under-validated.
- Break condition: If inter-agent communication overhead exceeds latency budgets, decomposition gains diminish.

### Mechanism 2
- Claim: Structure-guided retrieval improves unstructured evidence relevance over naive semantic search.
- Mechanism: After SQL execution returns structured evidence, the note retriever conditions its search on both the original question and the structured results. Relevance scores are computed jointly over (question, structured_evidence, note_chunk), narrowing the search space before LLM synthesis.
- Core assumption: Structured data provides temporal and contextual anchors that improve semantic alignment with clinical narratives.
- Evidence anchors:
  - [Page 14] "By incorporating structured query results into the retrieval context, this structure-guided retrieval strategy narrows the search space and improves alignment between clinical questions and unstructured documentation."
  - [Page 5] The manual review (Figure 5) showed minimal retrieval errors: 5 in structured querying, 3 in unstructured retrieval across 100 cases.
  - [corpus] No direct corpus evidence for structure-guided retrieval specifically; mechanism remains paper-specific.
- Break condition: If SQL queries return empty or misaligned results, conditioning degrades retrieval quality.

### Mechanism 3
- Claim: Targeted fine-tuning of only the answer synthesis module bridges the final "synthesis gap" without requiring full pipeline retraining.
- Mechanism: Only the Answer Synthesizer Agent is fine-tuned using LoRA on expert-curated multimodal QA pairs (DrugEHRQA). Upstream agents (Table Reviewer, SQL Writer) remain frozen in zero-shot mode. This isolates improvements to evidence reconciliation and formatting consistency.
- Core assumption: Retrieval quality is sufficiently robust that failures primarily occur at synthesis, not retrieval.
- Evidence anchors:
  - [Page 5] "The Multimodal Answer Synthesis stage further observed 6 error instances, where the models generated incorrect final answers despite being provided with the necessary evidence."
  - [Page 5] EHRNavigator (trained) improved LLaMA3-70B-Instruct from 10.21% to 79.78% on DrugEHRQA multimodal tasks.
  - [corpus] No corpus papers validate isolated module fine-tuning in multi-agent clinical systems.
- Break condition: If retrieval quality degrades on novel schemas, synthesis fine-tuning alone cannot compensate.

## Foundational Learning

- Concept: **Text-to-SQL over EHR schemas**
  - Why needed here: The structured data module requires generating executable SQL from natural language over heterogeneous schemas (MIMIC-III, OMOP CDM).
  - Quick check question: Can you explain why schema discovery and caching are necessary before SQL generation?

- Concept: **Retrieval-Augmented Generation (RAG) with semantic chunking**
  - Why needed here: Clinical notes exceed context windows; chunking with temporal metadata preserves clinically relevant boundaries.
  - Quick check question: How does adding timestamps to note chunks improve retrieval for temporal clinical queries?

- Concept: **Cross-modal evidence reconciliation**
  - Why needed here: Structured and unstructured sources may conflict (e.g., medication dose in table vs. narrative note); synthesis must resolve discrepancies.
  - Quick check question: What failure mode occurs when the model cannot reconcile drug concentration text with structured quantity fields?

## Architecture Onboarding

- Component map:
  - Structured Module: Table Reviewer Agent -> Table Retrieval Tool -> SQL Writer Agent -> SQL Execution Tool
  - Unstructured Module: Note Indexer (chunking/embedding) -> Note Retriever (structure-conditioned search)
  - Synthesis Module: Answer Synthesizer Agent (fine-tuned with LoRA)

- Critical path: SQL execution must complete before structure-guided note retrieval begins; both evidence streams feed Answer Synthesizer.

- Design tradeoffs:
  - Latency vs. accuracy: Combination queries (lab–drug) have highest accuracy (95%) but also highest latency (median 17.95s) due to high-cardinality JOINs.
  - Zero-shot generalization vs. performance ceiling: Frozen retrieval agents enable cross-schema deployment but leave synthesis gaps recoverable only via fine-tuning.

- Failure signatures:
  - **Encounter misalignment (57.1%)**: Query targets an encounter lacking data; agent drifts to adjacent temporal windows.
  - **Information displacement (21.4%)**: Critical facts embedded in semi-structured columns (e.g., medication instructions) not accessible via SQL-only retrieval.
  - **Synthesis gap (6%)**: Correct evidence retrieved but final answer incorrect due to reasoning failures.

- First 3 experiments:
  1. Run vanilla LLM baseline on DrugEHRQA multimodal split to establish accuracy floor before integrating EHRNavigator pipeline.
  2. Ablate structure-guided retrieval by conditioning note search on question only (excluding SQL results); compare retrieval relevance metrics.
  3. Profile latency breakdown across query types (lab-only, drug-only, combination) to identify JOIN bottlenecks and set realistic response time expectations.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can EHRNavigator be extended to support higher-level, concept-driven clinical reasoning rather than purely fact-based retrieval?
- Basis in paper: [explicit] Section 3.4 states the current system focuses on "fact-based queries" tied to specific entities, whereas real-world practice often requires reasoning about disease categories or treatment patterns (e.g., "Does the patient have structural lung disease?").
- Why unresolved: The current architecture lacks integration with medical ontologies like SNOMED-CT or UMLS, which is likely necessary to bridge the gap between raw data extraction and abstract clinical concepts.
- What evidence would resolve it: Evaluation results on a benchmark requiring logical inference or disease classification, showing improved performance after integrating semantic ontology layers into the reasoning agents.

### Open Question 2
- Question: Can explicit temporal relation modeling effectively reduce the high rate of "encounter misalignment" errors observed in real-world patient trajectories?
- Basis in paper: [inferred] In Section 2.6, the authors identify "Encounter misalignment" as the primary failure mode (57.1% of errors) when querying patient trajectories. In Section 3.4, they explicitly propose "enhancing the understanding of Temporal-Clinical Events" as a necessary future step.
- Why unresolved: The current framework struggles to align longitudinal data across multiple visits, often drifting to adjacent temporal windows when specific encounters lack data, indicating a lack of robust temporal anchoring.
- What evidence would resolve it: A comparative error analysis on the YNHHQA dataset showing a significant reduction in visit-scope mismatches after implementing a dedicated temporal logic module.

### Open Question 3
- Question: To what extent does a multi-turn dialogue capability improve diagnostic accuracy in cases requiring iterative hypothesis refinement?
- Basis in paper: [explicit] Section 3.4 notes that the current "single-round querying paradigm may be insufficient for complex cases" and suggests that multi-turn dialogue would enable "sophisticated clinical reasoning akin to physician thought processes."
- Why unresolved: The system currently operates in a single-shot mode, lacking the memory or state management required to refine queries based on previous answers or partial evidence.
- What evidence would resolve it: A user study measuring the success rate of complex diagnoses where clinicians interact with the system iteratively versus using the current single-prompt interface.

### Open Question 4
- Question: Can the multi-agent framework generalize to proprietary EHR systems (e.g., Epic, Cerner) without extensive schema-specific engineering?
- Basis in paper: [explicit] Section 3.4 states that while generalization across MIMIC and OMOP is established, "further validation across additional EHR systems (e.g., Epic, Cerner) and clinical specialties" is needed to prove broad applicability.
- Why unresolved: Proprietary systems often utilize vastly different data architectures and "documentation practices" compared to the research-grade MIMIC or OMOP databases used in the current study.
- What evidence would resolve it: Successful deployment and accuracy benchmarks on a hospital system running Epic or Cerner, specifically measuring the "zero-shot" adaptation capability of the Table Reviewer and SQL Writer agents.

## Limitations
- Real-world deployment readiness uncertain despite strong benchmark performance; YNHHQA validation limited to 100 cases
- Modular architecture creates potential failure modes where errors compound across agent boundaries
- Fine-tuning approach only addresses synthesis module, leaving retrieval agents frozen in zero-shot mode

## Confidence

- **High Confidence**: The modular decomposition approach showing consistent superiority over direct LLM prompting across multiple query types (drug-only: 95% vs 75.7%; lab-only: 93.7% vs 73.7%; combination: 95% vs 63.1%)
- **Medium Confidence**: Structure-guided retrieval effectiveness claims, as the mechanism is well-described but lacks comparative validation against alternative retrieval strategies
- **Medium Confidence**: Clinical deployment feasibility based on YNHHQA results, though the sample size (100 cases) and single institution context limit generalizability

## Next Checks

1. Conduct A/B testing comparing structure-guided retrieval against baseline semantic search without structured evidence conditioning, measuring retrieval precision and recall across 500 clinical questions
2. Perform latency profiling on combination queries under realistic clinical loads, measuring end-to-end response times with concurrent user sessions and varying database sizes
3. Test system robustness on out-of-distribution clinical scenarios (rare diseases, complex polypharmacy cases) using synthetic queries not present in any benchmark dataset