---
ver: rpa2
title: Mixture of Tunable Experts -- Behavior Modification of DeepSeek-R1 at Inference
  Time
arxiv_id: '2502.11096'
source_url: https://arxiv.org/abs/2502.11096
tags:
- experts
- expert
- behavior
- figure
- refused
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Mixture-of-Tunable-Experts (MoTE), a method
  that modifies the behavior of DeepSeek-R1 at inference time without additional training.
  By analyzing expert activations using a technique called functional Token Resonance
  Imaging (fTRI), the authors identified expert subgroups responsible for specific
  behaviors like refusal responses.
---

# Mixture of Tunable Experts -- Behavior Modification of DeepSeek-R1 at Inference Time

## Quick Facts
- arXiv ID: 2502.11096
- Source URL: https://arxiv.org/abs/2502.11096
- Reference count: 22
- Primary result: Inference-time behavior modification of DeepSeek-R1 via expert suppression/stimulation

## Executive Summary
This paper introduces Mixture-of-Tunable-Experts (MoTE), a method that modifies the behavior of DeepSeek-R1 at inference time without additional training. By analyzing expert activations using a technique called functional Token Resonance Imaging (fTRI), the authors identified expert subgroups responsible for specific behaviors like refusal responses. Deactivating the top 10 most refusal-relevant experts (0.07% of R1's 14,848 routed experts) achieved a 52% reduction in refusals on sensitive prompts without degrading performance on MT-Bench. Random expert deactivation had smaller effects with more noise, while forced expert activation increased refusals. The approach also successfully changed the model's reasoning language from English to Chinese in 10% of test prompts.

## Method Summary
The MoTE method combines functional Token Resonance Imaging (fTRI) analysis with inference-time expert routing modification. First, the system aggregates expert activation patterns for different behavioral classes (refused vs. aligned responses), then identifies "distinctive experts" by computing differential activations. The top-10 most relevant experts for refusal behaviors are then suppressed by modifying the router weights at inference time - setting their weights to zero and renormalizing. The same approach can stimulate behaviors by forcing specific experts into the top-k selection. The method requires no training and works by intercepting the router's expert selection process in DeepSeek-R1's MoE architecture.

## Key Results
- 52% reduction in refusals on sensitive prompts by suppressing top 10 refusal-relevant experts (0.07% of total experts)
- 10% success rate in changing reasoning language from English to Chinese through expert suppression
- No performance degradation on MT-Bench when modifying experts
- Random expert deactivation achieved only 9% behavioral change with more noise

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Specific high-level behaviors are functionally localized in small subsets of routed experts
- **Mechanism:** In DeepSeek-R1's 14,848 routed experts, specific experts consistently activate for specific behavioral classes. By calculating differential activation (resonance), a tiny cluster of experts (e.g., 10 experts, 0.07% of total) drives refusal behaviors.
- **Core assumption:** Functional specialization emerges naturally during pre-training of massive MoE models
- **Evidence:** Significant functional mechanisms can be localized in small expert subsets; DeepSeek's architecture designed for increased specialization

### Mechanism 2
- **Claim:** Modifying router weights at inference time directly alters model behavior without retraining
- **Mechanism:** The method intercepts the router's Top-K selection process, setting specific expert weights to zero for suppression or forcing them into Top-K for stimulation
- **Core assumption:** Routed experts act as behavioral modifiers that can be safely toggled while shared experts maintain general competency
- **Evidence:** Successful suppression/stimulation of specific behaviors without model retraining

### Mechanism 3
- **Claim:** Behavioral causality can be inferred via functional Token Resonance Imaging (fTRI)
- **Mechanism:** fTRI aggregates activation maps for tokens from distinct behavioral classes, then subtracts "aligned" patterns from "refusal" patterns to isolate experts unique to the target behavior
- **Core assumption:** High activation frequency in specific experts during specific behaviors implies causal role
- **Evidence:** Visual isolation of distinctive experts in activation maps; successful behavioral modification after expert suppression

## Foundational Learning

- **Concept: Sparse Mixture-of-Experts (MoE) Routing**
  - **Why needed:** MoTE relies on the distinction between shared experts (always on) and routed experts (sparse activation)
  - **Quick check:** If you suppressed a "shared" expert instead of a "routed" one, would the effect likely be subtle or catastrophic?

- **Concept: Model Steering vs. Fine-tuning**
  - **Why needed:** This paper uses inference-time modification, not training; no gradients are calculated
  - **Quick check:** Does this method require a forward pass to calculate gradients before altering behavior?

- **Concept: t-SNE (t-Distributed Stochastic Neighbor Embedding)**
  - **Why needed:** The authors use t-SNE to visualize activation clusters and verify that "refusal" prompts cluster differently from "aligned" prompts
  - **Quick check:** In Figure 4, what does the spatial proximity of points in cluster C1 represent regarding the input prompts?

## Architecture Onboarding

- **Component map:** DeepSeek-R1 Base -> 58 MoE Layers -> 1 Shared Expert + 256 Routed Experts per layer -> Router (selects Top-8) -> MoTE Wrapper (intercepts router, applies intervention)

- **Critical path:**
  1. fTRI Analysis: Generate prompts triggering target behavior -> Aggregate activation maps -> Subtract neutral maps to find "Distinctive Experts"
  2. vLLM Integration: Modify inference engine to allow dynamic weight overriding for specific expert indices
  3. Validation: Run modified inference on sensitive prompts + general capability benchmarks (MT-Bench)

- **Design tradeoffs:**
  - Precision vs. Distribution: Assumes behavior is localizable; distributed behaviors require massive deactivation
  - Interpretability vs. Training Cost: Zero training required but relies on pre-existing MoE structure

- **Failure signatures:**
  - Coherence Collapse: Too many experts suppressed causes gibberish output
  - Behavioral Overcorrection: Stimulation is noisier than suppression
  - False Positives: Random ablation shows 9% behavioral change, indicating some sensitivity is noise

- **First 3 experiments:**
  1. Implement top-10 distinctive expert suppression on sensitive prompts to replicate 52% reduction
  2. Deactivate 10 random experts as control to confirm specificity of fTRI-identified experts
  3. Attempt reasoning language switch by suppressing "English reasoning" experts

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does MoTE generalize to other MoE architectures beyond DeepSeek-R1, such as Mixtral or Grok?
- **Basis:** The authors note Mistral "did not find evidence for its experts specializing" and attribute this to "too-small expert count per layer"
- **Why unresolved:** Only DeepSeek-R1 was tested; method's dependence on expert count remains unknown
- **What evidence would resolve it:** Apply fTRI and expert suppression to Mixtral 8x22B or other MoE models and measure behavioral change magnitude

### Open Question 2
- **Question:** What explains the asymmetric effect sizes between expert suppression (~52% refusal reduction) and stimulation (~10% refusal increase)?
- **Basis:** Section 3.3 reports stimulating alignment-specific experts "had a less pronounced effect compared to suppression" without mechanistic explanation
- **Why unresolved:** Paper demonstrates asymmetry empirically but doesn't investigate routing dynamics or expert redundancy
- **What evidence would resolve it:** Analyze activation distributions during stimulation vs. suppression; test whether multiple experts compensate when one is stimulated

### Open Question 3
- **Question:** Does suppressing refusal-related experts cause subtle degradation in safety-aligned behaviors not captured by MT-Bench?
- **Basis:** Paper claims "no performance degradation on MT-Bench" but acknowledges need to verify across larger benchmark sets
- **Why unresolved:** General benchmark may miss nuanced safety regressions
- **What evidence would resolve it:** Evaluate modified model on safety-specific benchmarks (HarmBench, AdvBench) and adversarial red-teaming

## Limitations
- Assumes high-level behaviors are functionally localized in small expert subsets, which may not generalize to all prompt types
- 52% refusal reduction still leaves 48% of sensitive prompts unaffected, suggesting distributed behavior or shared expert involvement
- Lacks analysis of potential subtle side effects beyond MT-Bench performance
- Language switching experiment shows only 10% success rate, indicating limited effectiveness for some behavioral modifications

## Confidence
- **High Confidence:** Technical implementation of MoTE and its deployment on DeepSeek-R1
- **Medium Confidence:** fTRI methodology for identifying distinctive experts
- **Medium Confidence:** Behavioral modification results (52% refusal reduction, 10% language switching)

## Next Checks
1. Apply MoTE to behaviors known to be polysemantic to test limits of functional localization and measure degradation
2. Run the same sensitive prompts multiple times over extended periods to verify consistency and stability
3. Apply MoTE/fTRI approach to a different MoE model (e.g., Mixtral, LLaMA-Adapter) to determine generalizability