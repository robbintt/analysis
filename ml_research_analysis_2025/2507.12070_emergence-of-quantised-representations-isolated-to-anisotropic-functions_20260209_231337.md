---
ver: rpa2
title: Emergence of Quantised Representations Isolated to Anisotropic Functions
arxiv_id: '2507.12070'
source_url: https://arxiv.org/abs/2507.12070
tags:
- representations
- these
- which
- networks
- more
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a controlled ablation study investigating how
  activation function symmetries influence representational structure in autoencoders.
  The key innovation is the Privileged-Plane Projective (PPP) method, which visualizes
  high-dimensional representations by projecting them onto privileged planes defined
  by basis vectors.
---

# Emergence of Quantised Representations Isolated to Anisotropic Functions

## Quick Facts
- arXiv ID: 2507.12070
- Source URL: https://arxiv.org/abs/2507.12070
- Authors: George Bird
- Reference count: 40
- Primary result: Discrete-symmetry activation functions induce quantised, axis-aligned representations while continuous-symmetry functions produce smooth, isotropic distributions

## Executive Summary
This paper investigates how activation function symmetries influence representational structure in autoencoders through a controlled ablation study. The key innovation is the Privileged-Plane Projective (PPP) method, which visualizes high-dimensional representations by projecting them onto privileged planes defined by basis vectors. The study reveals that functional form choices carry unintended inductive biases that systematically shape representations, with discrete-symmetry activations producing quantised, axis-aligned patterns while continuous-symmetry functions yield smooth, isotropic distributions. The empirical findings show isotropic networks consistently outperform anisotropic counterparts in reconstruction tasks, suggesting these biases may be detrimental to performance.

## Method Summary
The study employs a controlled ablation framework comparing discrete-symmetry (permutation-equivariant) and continuous-symmetry (orthogonal-equivariant) activation functions in autoencoder architectures. The novel Privileged-Plane Projective (PPP) method projects high-dimensional representations onto privileged planes defined by basis vectors, enabling visualization of representational structure. The ablation design systematically varies activation function symmetry properties while holding other architectural parameters constant, allowing causal inference about how functional symmetries influence emergent representational patterns. Performance is evaluated through reconstruction accuracy metrics across different activation function classes.

## Key Results
- Discrete-symmetry activation functions induce quantised, axis-aligned representations in learned latent spaces
- Continuous-symmetry activation functions produce smooth, isotropic distribution patterns in representations
- Isotropic networks consistently outperform anisotropic counterparts in reconstruction tasks, suggesting representational biases affect performance

## Why This Works (Mechanism)
The mechanism underlying these findings relates to how activation function symmetries impose algebraic constraints on the representational space. Discrete-symmetry functions like permutation-equivariant activations preserve specific axis-aligned relationships, forcing representations to align with coordinate axes and creating quantised patterns. Continuous-symmetry functions like orthogonal-equivariant activations maintain rotational invariance, allowing representations to distribute smoothly across the latent space without preferential alignment to any particular axis. These algebraic properties directly translate into geometric constraints on the learned representations, with the activation function's symmetry group determining the structure of the emergent representational manifold.

## Foundational Learning

**Privileged-Plane Projective (PPP) Visualization**: A method for projecting high-dimensional representations onto privileged planes defined by basis vectors to visualize representational structure. Why needed: Enables interpretation of high-dimensional latent spaces through 2D projections. Quick check: Verify that PPP projections consistently reveal the same structural patterns across multiple random basis vector selections.

**Activation Function Symmetry Groups**: Algebraic properties of activation functions (discrete vs. continuous symmetry) that impose geometric constraints on learned representations. Why needed: Establishes the theoretical foundation linking functional form to representational structure. Quick check: Confirm that different symmetry groups produce distinct projection patterns in PPP visualizations.

**Representation Quantisation**: The emergence of discrete, axis-aligned patterns in latent space due to functional symmetries. Why needed: Demonstrates how architectural choices create unintended representational biases. Quick check: Measure the degree of quantisation using metrics like pairwise distance distributions in latent space.

## Architecture Onboarding

Component map: Input -> Encoder -> Activation Function -> Latent Space -> Decoder -> Output

Critical path: The activation function serves as the critical component determining representational structure, with symmetry properties cascading through the encoder to shape latent space geometry.

Design tradeoffs: Discrete-symmetry activations offer computational simplicity but induce potentially detrimental quantisation, while continuous-symmetry functions provide smoother representations at potential computational cost.

Failure signatures: Axis-aligned quantisation patterns in PPP projections indicate excessive inductive bias from discrete-symmetry activations; overly smooth distributions may suggest insufficient representational capacity.

First experiments:
1. Apply PPP projections to a simple autoencoder with ReLU activation to observe baseline quantised patterns
2. Replace ReLU with a continuous-symmetry activation (e.g., softplus) and compare PPP projections
3. Measure reconstruction performance differences between discrete and continuous symmetry activations on synthetic datasets

## Open Questions the Paper Calls Out
None

## Limitations
- Findings rely heavily on controlled synthetic data settings that may not capture real-world dataset complexity
- PPP projection method provides simplified 2D visualization that may obscure higher-dimensional structure
- Performance comparisons assume isotropic representations are universally preferable without considering task-specific benefits of quantisation

## Confidence

High: The correlation between activation function symmetry type and representational structure (quantised vs. smooth)

Medium: The causal relationship between functional symmetries and emergent properties

Medium: The performance advantage of isotropic over anisotropic representations

Low: Generalizability to complex real-world datasets and architectures beyond autoencoders

## Next Checks

1. Test PPP projections with multiple random basis vector selections to assess sensitivity and robustness of observed patterns

2. Evaluate the same activation function symmetries in transformer-based architectures and supervised learning tasks

3. Investigate whether quantised representations provide advantages in specific downstream tasks like feature selection or anomaly detection