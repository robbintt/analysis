---
ver: rpa2
title: 'BitMar: Low-Bit Multimodal Fusion with Episodic Memory for Edge Devices'
arxiv_id: '2510.10560'
source_url: https://arxiv.org/abs/2510.10560
tags:
- memory
- multimodal
- episodic
- vision
- bitmar
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: BitMar addresses the challenge of deploying large multimodal vision-language
  models on edge devices by introducing a quantized architecture that combines 1.58-bit
  text and vision encoders with an episodic memory system. The approach integrates
  a BitNet-style text encoder and a DiNOv2-based vision encoder to generate compact
  embeddings, which are fused and used to query a fixed-size episodic memory module.
---

# BitMar: Low-Bit Multimodal Fusion with Episodic Memory for Edge Devices

## Quick Facts
- arXiv ID: 2510.10560
- Source URL: https://arxiv.org/abs/2510.10560
- Authors: Euhid Aman; Esteban Carlin; Hsing-Kuo Pao; Giovanni Beltrame; Ghaluh Indah Permata Sari; Yie-Tarng Chen
- Reference count: 6
- One-line primary result: BitMar achieves competitive performance on language understanding tasks while maintaining low latency and memory usage suitable for edge deployment.

## Executive Summary
BitMar addresses the challenge of deploying large multimodal vision-language models on edge devices by introducing a quantized architecture that combines 1.58-bit text and vision encoders with an episodic memory system. The approach integrates a BitNet-style text encoder and a DiNOv2-based vision encoder to generate compact embeddings, which are fused and used to query a fixed-size episodic memory module. A BitNet decoder with per-layer conditioning and sliding-window attention processes the fused and retrieved context for efficient long-context generation. Experiments show BitMar achieves competitive performance on language understanding tasks while maintaining low latency and memory usage suitable for edge deployment.

## Method Summary
BitMar implements a 4-stage pipeline: (1) a 4-layer, 1.58-bit BitNet text encoder (d=128, h=4) that encodes text into 128-D embeddings; (2) a DiNOv2-based vision encoder with a 2-layer MLP bottleneck (768→384→128) that compresses visual features; (3) cross-modal attention fusion (text queries, vision keys/values) with 1.58-bit weights producing fused multimodal queries; (4) a 4-layer BitNet decoder with episodic memory (K=512, C=128) using sliding window + attention sinks and per-layer conditioning on retrieved memory vectors. Training uses 100M tokens (50M multimodal from CC3M + Localized Narratives with frozen DiNOv2 features; 50M text-only from BabyLM) with a mixed objective L_total = L_lm + 1.5*L_cm + 0.1*L_mem. The adaptive training controller monitors cross-modal similarity and intervenes when it drops >0.12.

## Key Results
- BitMar achieves 42.8% accuracy on BoolQ and 54.6% on WinoGrande with only 14M parameters.
- The episodic memory improves task accuracy by 3–4 percentage points on entity/property reasoning and multimodal QA.
- Performance degrades on morphology tasks (WUG) and psycholinguistic alignment despite gains on reasoning tasks.

## Why This Works (Mechanism)
BitMar combines aggressive 1.58-bit quantization for computational efficiency with episodic memory to overcome the context limitations of sliding-window attention. The ternary weight representation (-1, 0, +1) drastically reduces memory footprint while the episodic memory provides persistent storage for long-range information that would otherwise be lost. The per-layer conditioning mechanism allows the decoder to adaptively incorporate retrieved context at multiple levels of abstraction, improving reasoning capabilities without requiring full sequence attention.

## Foundational Learning
- **BitNet / 1.58-bit Quantization**: Core technique enabling BitMar's small footprint. Understand how weights are represented as ternary values {-1, 0, +1} and scaled, and how this differs from standard FP16/BF16 training. Quick check: How does the computational cost of a matrix multiplication change when both weights and activations are quantized to low bit-widths compared to full-precision computation?
- **Attention Sinks & Sliding Window Attention**: Critical for handling long sequences within a fixed memory budget. Grasp why "sink" tokens are preserved and how the window slides over new tokens. Quick check: In a standard transformer, what happens to the attention mechanism if you arbitrarily discard Key-Value pairs from the start of a sequence? How do attention sinks attempt to mitigate this?
- **External & Differentiable Neural Memory**: BitMar's episodic memory is a learned memory matrix with read/write operations, inspired by architectures like Neural Turing Machines. Understanding content-based addressing is key. Quick check: Explain the difference between a standard autoregressive transformer's KV cache and BitMar's episodic memory matrix M. What are the read and write operations for M?

## Architecture Onboarding
- **Component map**: Input Text + Image -> (Parallel Encoders) -> Fused Embedding -> (Memory Query) -> Retrieved Context -> (Conditioned Decoder) -> Output Text
- **Critical path**: Text and image are processed in parallel by quantized encoders, fused through cross-attention, the fused vector queries episodic memory, retrieved context conditions each decoder layer, and output is generated using sliding window attention with attention sinks.
- **Design tradeoffs**: Aggressive quantization drastically lowers memory/compute but hurts performance on knowledge-heavy tasks compared to larger models; fixed memory provides context beyond sliding window but capacity is limited; per-layer conditioning is more expressive than single injection but slightly more complex.
- **Failure signatures**: Modality collapse (cross-modal alignment fails), memory thrashing (memory slots change too rapidly), loss of long-range coherence (repetitive text or lost entities).
- **First 3 experiments**: (1) Quantization ablation: Train and evaluate a non-quantized (FP16) baseline with same architecture to isolate quantization impact; (2) Memory capacity analysis: Vary episodic memory size (K=128, 256, 512, 1024) on entity tracking and multimodal QA tasks; (3) Long-context stress test: Evaluate recall on synthetic dataset with key information at beginning, middle, and end of sequences exceeding 1020 tokens.

## Open Questions the Paper Calls Out
- Can alternative memory injection strategies or capacity tuning prevent the degradation of morphological productivity and psycholinguistic alignment observed in the current BitMar architecture? The authors note tuning memory capacity or injection strategy may mitigate this.
- To what extent does the 1.58-bit weight quantization inherently limit the factual knowledge capacity of BitMar compared to higher-precision small models? The paper reports challenging performance on knowledge-heavy tasks like CommonsenseQA and MMLU.
- Is the heuristic Adaptive Training Controller essential for preventing modality collapse in low-bit multimodal fusion, or can this stability be achieved through architectural regularization? The controller intervenes when cross-modal similarity drops, suggesting potential instability in the base architecture.

## Limitations
- The aggressive 1.58-bit quantization creates a fundamental performance gap on knowledge-intensive tasks that the episodic memory cannot fully overcome.
- The paper does not provide runtime measurements on actual edge hardware, only parameter counts and FLOPs comparisons.
- Performance degradation on morphology tasks and psycholinguistic alignment indicates the architecture actively harms specific linguistic capabilities despite helping reasoning tasks.

## Confidence
- **High Confidence**: BitMar successfully implements 1.58-bit quantized multimodal architecture that fits within edge device constraints (14M parameters, low memory usage).
- **Medium Confidence**: The episodic memory improves performance on entity/property reasoning and multimodal QA tasks by 3-4 percentage points.
- **Low Confidence**: BitMar achieves competitive performance on language understanding tasks relative to larger models.

## Next Checks
1. **Quantization Efficiency Validation**: Train a non-quantized (FP16) baseline with identical architecture and compare its performance against BitMar's 1.58-bit version across all benchmarks to isolate whether performance gains come from architecture or are lost due to quantization.
2. **Memory Capacity Trade-off Analysis**: Systematically vary the episodic memory size (K=128, 256, 512, 1024) and measure performance degradation/improvement on tasks where memory showed benefit to determine if K=512 is optimal.
3. **Long-context Coherence Evaluation**: Create synthetic evaluation dataset with sequences exceeding 1020 tokens where critical information is distributed across the sequence and compare BitMar's ability to maintain coherence against baselines without episodic memory.