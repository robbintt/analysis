---
ver: rpa2
title: 'Predicting the Road Ahead: A Knowledge Graph based Foundation Model for Scene
  Understanding in Autonomous Driving'
arxiv_id: '2503.18730'
source_url: https://arxiv.org/abs/2503.18730
tags:
- scene
- driving
- prediction
- autonomous
- understanding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes a knowledge graph (KG)-based foundation model\
  \ for scene understanding in autonomous driving. It transforms driving scenes into\
  \ a symbolic Bird\u2019s Eye View (BEV) representation via a KG that captures spatial-temporal\
  \ relationships, then serializes this into token sequences for training language\
  \ models."
---

# Predicting the Road Ahead: A Knowledge Graph based Foundation Model for Scene Understanding in Autonomous Driving

## Quick Facts
- arXiv ID: 2503.18730
- Source URL: https://arxiv.org/abs/2503.18730
- Reference count: 39
- Primary result: Knowledge graph-based foundation model achieves 88.7% masked scene object prediction accuracy on nuScenes dataset

## Executive Summary
This paper introduces FM4SU, a foundation model that learns scene understanding for autonomous driving using only symbolic representations. The approach transforms driving scenes into a knowledge graph, extracts a Bird's Eye View (BEV) symbolic representation, and serializes it into token sequences for training language models. Experiments on the nuScenes dataset demonstrate that pre-trained language models can effectively learn driving scene patterns and spatio-temporal evolution without additional visual modalities, achieving high accuracy on both masked scene object prediction (88.7%) and next-scene prediction (86.7%) tasks.

## Method Summary
The method constructs a knowledge graph (nuScenesKG) from the nuScenes autonomous driving dataset, containing 43 million triples with 42 classes and 10 object properties. For each driving scene, a BEV symbolic representation is extracted as a 20×11 cell matrix (40m×24m area around the ego vehicle), where each cell captures objects within its coordinates. This matrix is serialized row-by-row with positional delimiters and metadata tokens (country, displacement, orientation) prepended. A pre-trained T5 encoder-decoder transformer is then fine-tuned on two tasks: predicting masked scene objects and predicting the next scene, using cross-entropy loss. The approach demonstrates that language models can learn scene understanding from purely symbolic representations without visual encoders.

## Key Results
- FM4SU achieves 88.7% accuracy on masked scene object prediction task
- Next-scene prediction accuracy reaches 86.7% with precision of 61.8%, recall of 59.4%, and F1 of 60.3%
- Pre-trained language model knowledge provides significant advantage: without pre-training, scene object prediction accuracy drops from 88.7% to 37.4%
- Metadata tokens (country, displacement, orientation) contribute ~4% accuracy improvement for next-scene prediction
- Higher BEV resolution (2m cells vs 5m cells) improves accuracy from 39.6% to 88.7%

## Why This Works (Mechanism)

### Mechanism 1
Structuring driving scenes as knowledge graphs before tokenization preserves spatial-temporal semantics that enable language models to learn scene patterns without visual encoders. The KG captures entities and relations as triples, and a BEV area matrix is extracted via geo-SPARQL queries, grounding each cell to objects within its coordinates. This matrix is serialized row-by-row with positional delimiters preserving spatial structure as token sequences. Core assumption: The ontology's expressivity and cell-to-object mapping capture sufficient information for scene prediction without raw sensor data.

### Mechanism 2
Pre-trained language model knowledge transfers to symbolic scene understanding, dramatically improving prediction over random initialization. T5 is fine-tuned on serialized scene tokens using cross-entropy loss. Pre-training provides learned attention patterns and sequence modeling that transfer to the symbolic domain. The model predicts masked spans or full next-scene tokens. Core assumption: The attention mechanisms learned from natural language generalize to spatially-structured symbolic sequences.

### Mechanism 3
Metadata tokens (country, displacement, orientation) provide critical context for temporal scene evolution prediction. The serialization prepends tokens encoding driving-side, ego-vehicle motion, and orientation changes. This enables the model to infer relative object motion between frames. Core assumption: Temporal prediction requires explicit motion context; spatial patterns alone are insufficient.

## Foundational Learning

- **Knowledge Graphs and Ontologies (TBox/ABox)**: Understanding how nuScenesKG structures entities and relations is essential, as the entire pipeline depends on the ontology's expressivity. Quick check: Can you explain the difference between an ontology class (e.g., `Scene`) and a KG instance (e.g., `Scene_T_001`)?

- **BEV (Bird's Eye View) Representations**: The core innovation is extracting a BEV matrix from the KG. Understanding coordinate systems, cell-to-area mapping, and ego-vehicle-centric orientation is essential. Quick check: Given ego-vehicle at (0,0) facing +Y, which cells in a 20×11 matrix (40m×24m, 2m cells) cover the area 10m ahead and 4m to the left?

- **Encoder-Decoder Transformers (T5 Architecture)**: T5 frames all tasks as text-to-text generation. Understanding the encoder (context), decoder (generation), and masking strategy is critical for debugging prediction quality. Quick check: Why does T5 use sentinel tokens (`<M1>`, `<M2>`) for span corruption rather than single-token masking like BERT?

## Architecture Onboarding

- **Component map**: Perception layer (external) -> nuScenesKG (43M triples) -> BEV Extraction (geo-SPARQL queries) -> Serialization (row-major with metadata) -> T5 Model (fine-tuning) -> Evaluation (accuracy, precision, recall, F1)
- **Critical path**: KG → BEV Matrix → Serialization → T5 Fine-tuning → Prediction. Errors in KG extraction or coordinate mapping propagate directly to token sequences.
- **Design tradeoffs**: Resolution: 20×11 (2m cells) vs. 8×5 (5m cells). Higher resolution improves accuracy (88.7% vs. 39.6%) but increases token count. Model size: T5-base vs. T5-large. Large shows marginal improvement (86.7% vs. 86.1%) with significantly higher compute. Masking: T5's 100-token mask limit constrains full next-scene prediction to 98 central cells.
- **Failure signatures**: Low recall on dynamic objects (model may over-predict static objects due to class imbalance). High accuracy but low F1 (indicates dominant-class bias; check per-class metrics). Training loss plateaus early (check if metadata is missing or serialization delimiters are malformed).
- **First 3 experiments**: 1) Reproduce scene object prediction baseline: Fine-tune T5-base on 20×11 BEV data with 3 random masked cells per scene. Target: ~88% accuracy. Verify train/val loss curves match Fig. 7. 2) Ablate metadata: Remove `<country>`, `<dist>`, `<orientation_diff>` tokens and retrain. Confirm ~4% accuracy drop per Table 2, Exp. 4.2. 3) Test generalization to unseen locations: Train on Boston scenes only, evaluate on Singapore. Expect degradation due to driving-side differences; quantify gap.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can the learned scene understanding be effectively transferred to improve performance on downstream tasks such as 3D object detection and trajectory prediction? The authors explicitly state they aim to assess performance on these tasks, but the current work focuses solely on scene object prediction and next-scene prediction accuracy.

- **Open Question 2**: How does the framework perform when scaled using different Large Language Model architectures beyond T5? The authors list integrating other LLMs and investigating performance and scalability aspects as future work, but experiments are limited to T5 variants.

- **Open Question 3**: How can the significant performance disparity between static and dynamic object prediction in next-scene forecasting be mitigated? The paper acknowledges lower performance for dynamic objects but does not propose architectural or methodological changes to address the complexity of dynamic motion.

## Limitations

- **Dataset construction details**: The paper lacks precise specifications on how nuScenesKG was built from raw nuScenes annotations, including handling of object trajectories across frames and temporal alignment for next-scene prediction.

- **Model hyperparameters**: Training epochs, exact masking strategy details, and early stopping criteria are unspecified, making exact reproduction difficult.

- **Performance baselines**: The paper compares against zero-shot LLM baselines but doesn't specify which models or report their exact performance numbers, limiting quantitative comparison.

## Confidence

- **High confidence**: The BEV extraction and serialization pipeline is well-specified with clear equations and examples. The token sequence format and T5 fine-tuning procedure are reproducible.
- **Medium confidence**: The 88.7% accuracy and 86.7% accuracy results are believable given the methodology, but without exact training curves or epoch counts, minor performance variations are expected in reproduction.
- **Low confidence**: The contribution of metadata tokens is only weakly validated - the ablation shows performance drops but doesn't isolate whether country, displacement, or orientation individually matter.

## Next Checks

1. **Replicate ablation studies**: Systematically remove each metadata token type (`<country>`, `<dist>`, `<orientation_diff>`) individually and measure impact on both prediction tasks. This isolates which context is most critical.

2. **Test cross-location generalization**: Train on scenes from one city (Boston) and evaluate on another (Singapore) to quantify how well the model transfers across different driving conventions and layouts.

3. **Analyze per-class performance**: Generate confusion matrices and per-class precision/recall to identify if the model exhibits bias toward static objects (lanes, walkways) at the expense of dynamic objects (vehicles, pedestrians).