---
ver: rpa2
title: 'Fewer May Be Better: Enhancing Offline Reinforcement Learning with Reduced
  Dataset'
arxiv_id: '2502.18955'
source_url: https://arxiv.org/abs/2502.18955
tags:
- learning
- offline
- data
- dataset
- selection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ReDOR, a method for reducing offline reinforcement
  learning datasets by formulating data selection as a gradient approximation optimization
  problem. The key insight is that the actor-critic framework can be reformulated
  as a submodular optimization problem, enabling efficient subset selection via adapted
  orthogonal matching pursuit.
---

# Fewer May Be Better: Enhancing Offline Reinforcement Learning with Reduced Dataset

## Quick Facts
- **arXiv ID**: 2502.18955
- **Source URL**: https://arxiv.org/abs/2502.18955
- **Reference count**: 40
- **Primary result**: Introduces ReDOR, a method that formulates offline RL dataset subset selection as a gradient approximation optimization problem, achieving improved policy performance and training efficiency on D4RL benchmarks through submodular optimization and adapted orthogonal matching pursuit.

## Executive Summary
This paper addresses the challenge of selecting high-quality data subsets from large offline reinforcement learning datasets to improve policy performance and training efficiency. The authors propose ReDOR, which formulates data selection as a submodular optimization problem by approximating the gradient of the actor-critic objective. By leveraging the weak submodularity of this objective, ReDOR employs an adapted orthogonal matching pursuit algorithm for efficient subset selection. The method is evaluated on D4RL benchmark tasks, demonstrating significant improvements over baseline selection strategies and full-dataset training.

## Method Summary
ReDOR operates by reformulating the actor-critic framework as a submodular optimization problem, enabling efficient data subset selection via orthogonal matching pursuit (OMP). The key insight is that the gradient approximation objective exhibits weak submodularity, allowing for near-optimal greedy selection. The method stabilizes learning by using fixed empirical return targets instead of dynamic TD targets and performs trajectory-level selection with a top-m% constraint. Theoretical analysis establishes convergence properties and error bounds, while empirical results on D4RL benchmarks show ReDOR significantly improves both performance and training efficiency compared to baselines like Prioritized and Random selection methods.

## Key Results
- ReDOR significantly improves algorithm performance on D4RL benchmarks (Mujoco: hopper, walker2d, halfcheetah; Antmaze tasks) compared to baselines.
- Training efficiency is enhanced, with ReDOR achieving better or comparable performance using reduced dataset sizes.
- The method maintains low computational complexity while improving sample efficiency and policy performance.

## Why This Works (Mechanism)
ReDOR works by leveraging the mathematical structure of the actor-critic objective to formulate data selection as a submodular optimization problem. The weak submodularity of the gradient approximation objective ensures that greedy selection via OMP provides near-optimal subset selection. By focusing on trajectories that maximally approximate the gradient of the value function, ReDOR effectively prioritizes data that improves policy learning. The use of fixed empirical return targets stabilizes the selection process, preventing instability from changing targets during training. Trajectory-level selection with a top-m% constraint ensures diversity while maintaining computational efficiency.

## Foundational Learning
- **Submodularity**: A diminishing returns property where selecting additional elements yields decreasing marginal gains; needed for efficient greedy optimization, quick check: verify objective satisfies diminishing returns.
- **Orthogonal Matching Pursuit (OMP)**: A greedy algorithm for sparse approximation; needed for efficient subset selection under the submodular objective, quick check: confirm OMP selects trajectories with maximum gradient approximation error reduction.
- **Actor-Critic Framework**: A reinforcement learning paradigm with separate policy (actor) and value (critic) networks; needed as the backbone for gradient approximation, quick check: verify TD3+BC or IQL implementation matches reported hyperparameters.
- **Empirical Return Targets**: Fixed return estimates used instead of dynamic TD targets; needed to stabilize selection process, quick check: confirm target generation and loading per selection round.
- **Gradient Approximation Error**: The objective function measuring how well a subset approximates the full gradient; needed to guide subset selection, quick check: monitor Err(w, S, L, θ) during OMP iterations.
- **Trajectory-Level Selection**: Selecting entire trajectories rather than individual transitions; needed for computational efficiency and policy stability, quick check: verify trajectory aggregation and constraint enforcement.

## Architecture Onboarding

**Component Map**: TD3+BC/IQL Backbone -> Trajectory Gradient Computation -> Orthogonal Matching Pursuit Selector -> Reduced Dataset -> Policy Training

**Critical Path**: Warm-start policy training -> Snapshot generation (θ_t) -> Trajectory gradient computation (Eq. 14) -> OMP subset selection (Alg 2) -> Reduced dataset creation -> Policy fine-tuning on subset

**Design Tradeoffs**: Uses submodular optimization for computational efficiency versus potential loss of global optimality; employs fixed targets for stability versus theoretical guarantees based on TD loss; selects trajectories for efficiency versus potentially missing informative individual transitions.

**Failure Signatures**: 
- Performance degradation vs full dataset → likely overfitting to early snapshots or missing trajectory diversity
- Selection process stalls (Errλ never drops below ϵ) → residual threshold too strict or m% too low
- High variance in final performance → insufficient seeds or unstable snapshot generation

**First Experiments**:
1. Verify TD3+BC baseline returns match reported values on D4RL (hopper, walker2d) before implementing selection
2. Implement trajectory-level OMP with fixed targets and test on a small synthetic dataset to verify Err(w, S, L, θ) decreases monotonically
3. Run full ReDOR pipeline with T=50 rounds and m=50% on hopper-medium, compare returns to full-dataset TD3+BC baseline

## Open Questions the Paper Calls Out
- **Open Question 1**: How effectively does ReDOR generalize to real-world robotic tasks compared to the simulated D4RL benchmarks? The conclusion states future work will attempt application to real-world robot tasks, but current experiments are confined to D4RL, which may not capture the noise and complexities of physical robotics.
- **Open Question 2**: Can the theoretical analysis be extended to rigorously justify the use of fixed targets rather than the standard TD loss used in the derivation? The Discussion notes a discrepancy between theoretical analysis (relying on TD loss) and practical implementation (using fixed targets), with no rigorous proof that gradient bounds hold for the modified approach.
- **Open Question 3**: Is the weak submodularity of the gradient approximation objective preserved when applied to offline RL algorithms outside the standard actor-critic framework? Theorem 4.1 proves weak submodularity specifically for actor-critic gradients, but it's unclear if bounds hold for non-actor-critic methods like implicit Q-learning variants.

## Limitations
- Missing critical hyperparameters (λ regularization coefficient, exact θ_t snapshot generation protocol, precise OMP stopping criteria) significantly hinder faithful reproduction
- Theoretical analysis relies on TD loss and bounded gradients, but practical implementation uses fixed targets without rigorous proof of bound preservation
- Claims of computational efficiency are difficult to verify without explicit runtime measurements and clarity on selection overhead

## Confidence
- **Medium Confidence**: Theoretical framework is mathematically coherent and aligns with established optimization literature, but practical implementation details critical to realizing these properties are underspecified
- **Medium Confidence**: Empirical claims of performance improvement over baselines are plausible given the framework, but verification depends on faithful implementation of underspecified selection mechanisms
- **Low Confidence**: Claims regarding computational efficiency relative to full-dataset training are difficult to verify without explicit runtime measurements and clarity on the overhead of the selection process itself

## Next Checks
1. Perform hyperparameter sensitivity analysis by varying λ, ϵ, and m% to identify their impact on subset size, residual error, and final policy performance; report sensitivity curves
2. Clarify and implement the exact protocol for generating/loading θ_t snapshots (frequency, warm-start procedure) and verify that multi-round selection with updated targets is functioning as intended
3. Measure and report wall-clock time for selection (T=50 rounds) versus training on the full dataset; include memory usage and selection overhead to substantiate efficiency claims