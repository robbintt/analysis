---
ver: rpa2
title: 'FinCap: Topic-Aligned Captions for Short-Form Financial YouTube Videos'
arxiv_id: '2509.25745'
source_url: https://arxiv.org/abs/2509.25745
tags:
- video
- financial
- multimodal
- visual
- gemini
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper benchmarks multimodal large language models (MLLMs)
  for topic-aligned captioning of short-form financial YouTube videos, using transcripts,
  audio, and video across seven modality combinations. Experiments on 624 annotated
  videos and five captioning tasks reveal that video alone performs strongly on four
  of five topics, underscoring its importance for capturing visual context and cues
  such as emotions, gestures, and body language.
---

# FinCap: Topic-Aligned Captions for Short-Form Financial YouTube Videos

## Quick Facts
- **arXiv ID**: 2509.25745
- **Source URL**: https://arxiv.org/abs/2509.25745
- **Reference count**: 40
- **Primary result**: Video modality alone achieves high accuracy (87-92) on visual and financial entity recognition tasks; selective modality pairs often outperform tri-modal fusion.

## Executive Summary
This paper benchmarks multimodal large language models (MLLMs) for topic-aligned captioning of short-form financial YouTube videos, using transcripts, audio, and video across seven modality combinations. Experiments on 624 annotated videos and five captioning tasks reveal that video alone performs strongly on four of five topics, underscoring its importance for capturing visual context and cues such as emotions, gestures, and body language. Selective modality pairs (e.g., TV, AV) often outperform full tri-modal fusion, indicating that too many modalities may introduce noise. F1-score for ticker-action extraction peaks at 0.60 with AV, while G-VEval accuracy reaches 89 for visual analysis and 87 for financial entity recognition under video-only input. The results establish the first benchmarks for financial short-form video captioning and highlight the need for careful modality selection in multimodal reasoning.

## Method Summary
The study evaluates five MLLMs (Qwen2.5-Omni-7B, Gemini 2.0 Flash, Gemini 2.5 Flash, GPT-4o mini, GPT-5 mini) on five topic-aligned captioning tasks: main recommendation, sentiment analysis, video purpose, visual analysis, and financial entity recognition, plus structured ticker-action extraction. The VideoConviction dataset contains 624 YouTube videos (average 4 minutes, filtered from 687 clips to segments under 5 minutes). Three modalities are used: video frames (V) at 0.25 fps, audio (A), and transcripts (T). Seven modality combinations are tested: T, A, V, TA, TV, AV, TAV. GPT models are restricted to T, V, TV due to lack of audio support. G-VEval accuracy is used for topic-specific evaluation, employing GPT-4o with chain-of-thought prompting. Frame sampling follows the HourVideo approach. The minimum viable reproduction plan involves acquiring the dataset, preprocessing videos, implementing topic-specific prompts, and running inference across all modality combinations, with ticker-action parsing for F1 computation and G-VEval evaluation against ground-truth annotations.

## Key Results
- Video-only modality achieves 87-92 accuracy on visual analysis and financial entity recognition tasks.
- Selective modality pairs (e.g., TV, AV) often outperform full tri-modal fusion (TAV) across 3+ topics.
- F1-score for ticker-action extraction peaks at 0.60 with audio-video (AV) input.
- Full tri-modal fusion (TAV) sometimes underperforms dual-modality combinations, suggesting noise introduction.

## Why This Works (Mechanism)
None

## Foundational Learning
- **Multimodal Large Language Models (MLLMs)**: Models that process and reason across multiple input modalities (text, audio, video) to generate unified outputs. Why needed: Enables holistic understanding of short-form videos where visual, auditory, and textual cues jointly convey financial recommendations. Quick check: Confirm model supports the required modality combinations (e.g., GPT models exclude audio).
- **G-VEval (Grounded Visual Evaluation)**: GPT-4o-based automated evaluation using chain-of-thought prompting to score model outputs against ground-truth annotations for each topic. Why needed: Provides scalable, consistent evaluation for topic-aligned captioning tasks. Quick check: Review prompt templates and scoring rubrics for each topic.
- **Modality Selection and Fusion**: Choosing which input modalities to combine (e.g., TV, AV, TAV) and how to integrate their information. Why needed: Determines model performance and robustness; excessive fusion can introduce noise or hallucination. Quick check: Compare dual-modality vs. tri-modal results for each topic.

## Architecture Onboarding

**Component Map**
```
VideoFrames(0.25fps) -> MLLM
Audio -> MLLM
Transcripts -> MLLM
MLLM -> Topic-Aligned Captions (MR, SA, VP, VIA, FE) + Ticker-Action Pairs
```

**Critical Path**
1. Preprocess videos: extract frames at 0.25 fps, separate audio, align transcripts.
2. Prompt MLLMs with topic-specific instructions for each modality combination.
3. Evaluate outputs using G-VEval (for topics) and F1-score (for ticker-action extraction).

**Design Tradeoffs**
- Video frames at 0.25 fps balances computational cost and visual context capture.
- Selective modality pairs (e.g., TV, AV) may outperform tri-modal fusion due to reduced noise.
- GPT models restricted to T, V, TV due to lack of audio support, limiting cross-model comparison.

**Failure Signatures**
- Low VIA/FE scores with non-visual modalities—expected due to vision-dominant nature of these tasks.
- TAV underperforms TV or AV—consistent with noise introduction from excessive fusion.

**3 First Experiments**
1. Run video-only (V) inference for VIA and FE tasks; expect 87-92 accuracy.
2. Compare dual-modality (TA, TV, AV) vs. tri-modal (TAV) results for MR and SA topics.
3. Evaluate ticker-action extraction F1 for AV vs. other combinations; expect peak at 0.60.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Do the observed multimodal dynamics—where video dominates and selective modality pairs outperform tri-modal fusion—generalize to short-form video domains beyond finance?
- **Basis in paper**: [explicit] Discussion states: "Other short-form video domains may exhibit different multimodal dynamics, making cross-domain extensions a promising direction for future work."
- **Why unresolved**: The study is scoped exclusively to financial recommendation videos from VideoConviction; no experiments test other domains.
- **What evidence would resolve it**: Benchmark results on short-form video datasets from other domains (e.g., educational, news, lifestyle) using the same seven modality combinations and evaluation protocol.

### Open Question 2
- **Question**: What mechanisms cause full tri-modal fusion to underperform selective modality pairs, and can adaptive fusion mitigate this?
- **Basis in paper**: [explicit] Discussion notes: "adding modalities to V introduced contradictory signals that reduced accuracy or induced hallucination" and concludes "careful modality selection, rather than indiscriminate fusion, is essential."
- **Why unresolved**: The paper demonstrates the phenomenon but does not analyze why TAV introduces noise or how to predict which subset is optimal per task.
- **What evidence would resolve it**: Ablation studies analyzing cross-modal conflicts, plus experiments with learnable or task-conditioned modality weighting mechanisms.

### Open Question 3
- **Question**: How can prompting strategies be optimized to direct MLLMs toward task-relevant visual features in financial videos?
- **Basis in paper**: [explicit] Discussion states: "Effective prompting strategies are critical to ensure models attend to the right visual features."
- **Why unresolved**: The paper uses fixed prompts but observes that models sometimes default to scene-level descriptors rather than structured financial content; no prompt engineering is explored.
- **What evidence would resolve it**: Systematic evaluation of varied prompting approaches measuring whether models shift attention from generic visual elements to finance-specific cues.

### Open Question 4
- **Question**: What architectural or training improvements could raise the 0.60 F1 ceiling for structured ticker-action extraction?
- **Basis in paper**: [inferred] Table 1 shows ticker-action extraction F1 peaks at only 0.60 (Gemini 2.0 Flash with AV), leaving substantial room for improvement on this core financial task.
- **Why unresolved**: The paper benchmarks existing MLLMs but does not propose domain-specific enhancements; performance gaps may stem from limited financial pretraining or entity grounding.
- **What evidence would resolve it**: Experiments with finance-adapted MLLMs (e.g., fine-tuned on financial corpora, with specialized tokenizers for tickers) reporting F1 improvements on the same benchmark.

## Limitations
- Lack of publicly available prompt templates and G-VEval evaluation details, essential for reproducibility.
- Unspecified audio preprocessing parameters and transcript alignment methodology, introducing pipeline ambiguity.
- G-VEval accuracy metric construction and potential biases are not fully described, limiting assessment of true task performance.

## Confidence

**High confidence in**: Video modality is crucial for visual analysis and financial entity recognition tasks, given consistent high accuracy across models.

**Medium confidence in**: Relative performance of modality combinations, due to missing prompt and evaluation details that could influence results.

**Low confidence in**: Generalizability of the "noise from tri-modal fusion" claim without further ablation studies or analysis of modality interaction effects.

## Next Checks
1. Request and test the exact prompt templates for all five captioning tasks to verify reproducibility of the reported F1 and G-VEval scores.
2. Implement and run G-VEval with the described chain-of-thought prompting on a held-out subset to confirm consistency with published results.
3. Conduct ablation studies to quantify the impact of audio preprocessing choices and frame sampling rate on VIA and FE performance.