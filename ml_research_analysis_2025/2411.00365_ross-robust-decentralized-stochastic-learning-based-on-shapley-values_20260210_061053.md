---
ver: rpa2
title: 'ROSS: RObust decentralized Stochastic learning based on Shapley values'
arxiv_id: '2411.00365'
source_url: https://arxiv.org/abs/2411.00365
tags:
- ross
- data
- learn
- balance
- dmsgd
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ROSS, a novel robust decentralized stochastic
  learning algorithm based on Shapley values to address data heterogeneity challenges
  in decentralized learning. The algorithm leverages Shapley values to weight local
  gradients and cross-gradients from neighbors, enabling each agent to evaluate contributions
  from its neighbors effectively.
---

# ROSS: RObust decentralized Stochastic learning based on Shapley values

## Quick Facts
- arXiv ID: 2411.00365
- Source URL: https://arxiv.org/abs/2411.00365
- Reference count: 40
- Key outcome: ROSS achieves linear convergence speedup and outperforms 7 baselines in convergence speed and accuracy across non-IID, noise, and poisoning scenarios

## Executive Summary
ROSS is a novel decentralized stochastic learning algorithm that leverages Shapley values to weight local and cross-gradients from neighbors, enabling robust learning under data heterogeneity. The algorithm allows each agent to evaluate neighbor contributions without accessing raw data, achieving state-of-the-art convergence rates under both IID and non-IID conditions. Extensive experiments on MNIST and CIFAR-10 demonstrate ROSS consistently outperforms baselines across vulnerable scenarios, maintaining 1.1-2.57x higher accuracy while converging within 60-90 rounds.

## Method Summary
ROSS extends decentralized SGD by introducing Shapley value-based weighting of gradients and cross-gradients. Each agent shares its model parameters with neighbors, who compute cross-gradients on their local data to evaluate the received model. Agents maintain a small validation set to estimate Shapley values for neighbor gradients, weighting them by their marginal contribution to prediction accuracy. A momentum-based consensus step with Shapley-weighted gradients stabilizes convergence, particularly on non-IID data. The algorithm requires agents to exchange models and gradients but not raw data, making it privacy-preserving while robust to noise and poisoning attacks.

## Key Results
- ROSS achieves linear convergence speedup matching state-of-the-art rates under both IID and non-IID conditions
- Outperforms 7 state-of-the-art baselines with 1.1-2.57x higher test accuracy across vulnerable scenarios
- Converges within 60-90 rounds while maintaining robustness to non-IID data, long-tailed distributions, data noise, label noise, and gradient poisoning

## Why This Works (Mechanism)

### Mechanism 1
Agents can evaluate neighbor update quality using cross-gradients, mitigating blind spots from heterogeneous local data. Each agent computes gradients of its local loss function with respect to neighbors' received model parameters, enabling evaluation without accessing raw data. This works under the assumption that cross-gradient computation is efficient. Evidence shows Algorithm 1 details model exchange and cross-gradient computation, with related work supporting cross-gradients in non-IID settings. Break condition occurs if the communication graph is disconnected.

### Mechanism 2
Weighting gradients via Shapley values filters harmful updates better than averaging. Validation sets measure marginal contribution of each neighbor's gradient to prediction accuracy, with lower weights assigned to gradients reducing validation performance. This relies on having clean, representative validation datasets. Section IV defines Shapley value calculation based on validation accuracy, with experimental results showing ROSS maintains lower loss under poisoning and noise. Break condition occurs if the validation set itself is poisoned or severely skewed.

### Mechanism 3
Momentum-based consensus with Shapley-weighted gradients stabilizes convergence on non-IID data. Instead of direct averaging, Shapley-weighted gradients are aggregated into a momentum buffer, then mixed with neighbors via doubly stochastic matrix W. This decouples noisy gradients from model update trajectory. The approach assumes L-smooth objectives and mixable network topology. Theorem 1 proves linear convergence with specific bounds on learning rate and momentum, while the abstract claims matching state-of-the-art rates. Break condition occurs if momentum coefficient is too high, dampening corrective signals.

## Foundational Learning

- **Concept: Shapley Value (Cooperative Game Theory)**
  - Why needed here: To fairly quantify neighbor gradient "contribution" to global objective, accounting for marginal utility beyond simple magnitude
  - Quick check question: Can you explain why a high-magnitude gradient might receive a low Shapley weight in this system?

- **Concept: Cross-Gradient Aggregation**
  - Why needed here: Communication primitive enabling evaluation phase where Agent A tests Agent B's model on Agent A's data
  - Quick check question: In a 3-agent system (A, B, C), which gradients does Agent A need to receive to compute its update?

- **Concept: Gossip Averaging / Consensus**
  - Why needed here: Underlying network protocol where matrix W sparsity affects convergence speed
  - Quick check question: How does graph sparsity (Ring vs. Fully Connected) affect theoretical convergence bound (specifically √ρ)?

## Architecture Onboarding

- **Component map:** Agent Node -> Communication Interface -> Shapley Estimator -> Optimizer
- **Critical path:** 1. Share: Broadcast model x_i[t-1]. 2. Compute: Receive neighbor models -> Compute cross-gradients g_i,j on local data D_i -> Send back. 3. Evaluate: Receive gradients -> Run Monte Carlo Shapley estimation using validation set Q. 4. Update: Aggregate gradients using weights π -> Update momentum -> Consensus mixing.
- **Design tradeoffs:** Computational overhead from cross-gradient computation and Monte Carlo estimation vs. lighter weight than retraining. Validation set size tradeoff between representativeness and training data. Monte Carlo iterations R balance speed vs. valuation noise.
- **Failure signatures:** Divergence on disconnected graphs prevents consensus. Stalled convergence occurs if validation set is unrepresentative, suppressing useful gradients. High latency bottleneck from round-trip model exchange and computation.
- **First 3 experiments:** 1. Topology Stress Test: Run ROSS on Ring vs. Fully Connected graph with non-IID data to verify linear speedup under sparsity. 2. Shapley Estimator Calibration: Vary Monte Carlo iterations R under Gradient Poisoning to find minimum compute cost for robustness. 3. Validation Set Sensitivity: Reduce validation set size |Q| to near-zero and measure accuracy drop to validate sufficiency claim.

## Open Questions the Paper Calls Out
- Can communication overhead from cross-gradient information be reduced via compression without destabilizing Shapley value weighting? The authors plan to incorporate gradient/model compression to reduce overhead, but compression noise may distort gradient magnitudes and Shapley calculations.
- Can ROSS be extended to resist stronger adaptive model poisoning attacks where malicious neighbors mimic benign distributions? The current Shapley estimation relies on validation accuracy, which sophisticated attacks might optimize to appear beneficial while degrading the global model.
- How sensitive is ROSS to the assumption that every agent possesses validation data sampled from the global distribution? In truly decentralized scenarios, agents may lack representative global samples, potentially biasing Shapley calculations if validation data is skewed.

## Limitations
- Effectiveness heavily depends on validation set quality and representativeness; compromised or skewed validation data can cause Shapley mechanism failure
- Assumes IID sampling of validation set from test set, but single small validation set may not capture global distribution in highly non-IID scenarios
- Convergence proof relies on doubly stochastic mixing matrix W and connected graph, which real-world network dynamics may violate

## Confidence
- High Confidence: Theoretical convergence speedup (linear rate) under Assumptions 1-3 is well-established in decentralized optimization literature
- Medium Confidence: Empirical performance claims are robust across tested scenarios but rely on specific hyperparameter choices and data partitions
- Low Confidence: Claim that "small fraction of data samples is sufficient" for validation set is asserted but not rigorously validated against local dataset size or heterogeneity degree

## Next Checks
1. Validation Set Robustness Test: Systematically vary validation set size and composition under severe non-IID conditions and measure performance degradation compared to D-PSGD
2. Sparsity Stress Test: Evaluate ROSS convergence on sparse, time-varying communication graph to confirm theoretical bound dependence on network topology
3. Poisoning Scenario Sensitivity: Design targeted gradient poisoning attack to maximize Shapley value assignment and measure ROSS's ability to identify and downweight compared to naive averaging baseline