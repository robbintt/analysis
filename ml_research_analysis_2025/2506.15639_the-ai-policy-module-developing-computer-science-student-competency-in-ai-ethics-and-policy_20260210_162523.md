---
ver: rpa2
title: 'The AI Policy Module: Developing Computer Science Student Competency in AI
  Ethics and Policy'
arxiv_id: '2506.15639'
source_url: https://arxiv.org/abs/2506.15639
tags:
- policy
- module
- ethics
- students
- ethical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a new educational module that teaches computer
  science students how to understand and address ethical challenges in AI systems.
  The module was developed to bridge the gap between abstract AI ethics principles
  and their practical implementation through technical and policy interventions.
---

# The AI Policy Module: Developing Computer Science Student Competency in AI Ethics and Policy

## Quick Facts
- arXiv ID: 2506.15639
- Source URL: https://arxiv.org/abs/2506.15639
- Authors: James Weichert; Daniel Dunlap; Mohammed Farghally; Hoda Eldardiry
- Reference count: 40
- One-line primary result: Students showed increased awareness of AI ethical issues, greater confidence in discussing AI regulation, and stronger intentions to follow AI policy developments after completing the module.

## Executive Summary
This paper presents an educational module that teaches computer science students how to understand and address ethical challenges in AI systems. The module bridges the gap between abstract AI ethics principles and their practical implementation through technical and policy interventions. In a pilot study, students who completed the module demonstrated increased awareness of AI ethical issues, greater confidence in discussing AI regulation, and stronger intentions to follow developments in AI policy. The module's flexible structure makes it adaptable for various computing ethics courses.

## Method Summary
The AI Policy Module spans three lectures (approximately 4 hours total) covering AI ethics, AI policy/regulation, and a "jailbreak or align" assignment. The method involves pre-survey (n=44) and post-survey (n=30) administration with Likert-scale and free-response questions, plus 18 group assignment submissions. The module follows a three-part structure: (1) Ethical Impacts of AI (2 lectures), (2) Regulating AI (1 lecture), and (3) AI Regulation Assignment (take-home, group). Attitudinal changes are measured using Wilcoxon signed-rank test with 5% significance cutoff.

## Key Results
- Students showed statistically significant increases in awareness of AI ethical issues and confidence discussing AI regulation
- All 18 student groups chose the "jailbreak" option over "align" in the assignment, suggesting alignment is more technically demanding
- Post-survey showed the largest shift (+0.56 on 5-point scale) in students' intention to follow news about government regulation of technology and AI

## Why This Works (Mechanism)

### Mechanism 1: Policy as Translation Layer Between Abstract Principles and Technical Implementation
The module explicitly frames policy as an intermediary concept between high-level ethical principles and concrete code-level interventions. By defining policy broadly as "normative preferences that are enforced by an institution within a particular jurisdiction," students gain a conceptual tool for moving from "what should be" to "how to implement it."

### Mechanism 2: Active Learning Through Adversarial Technical Assignment
The AI Regulation Assignment forces students to directly probe the technical frontier—what current alignment mechanisms can and cannot prevent. By attempting to circumvent safety features, students discover concrete vulnerabilities and must propose technical mitigations, turning abstract concerns about "model safety" into tangible engineering problems.

### Mechanism 3: Reframing Policy as Power Structures Rather Than Partisan Politics
The module explicitly distinguishes policy from "party politics" and connects it to Winner's thesis that artifacts embed power structures. This framing makes policy feel like a systems-level engineering concern rather than ideological debate, reducing defensive disengagement while maintaining critical examination of who benefits from AI systems.

## Foundational Learning

- **Concept: Basic understanding of LLM behavior and alignment**
  - Why needed here: The AI Regulation Assignment requires students to interact with aligned models and understand what "alignment" means technically—content filters, refusal behaviors, instruction tuning.
  - Quick check question: Can you explain why an LLM might refuse to answer certain prompts, and name at least one technique used to align models with safety guidelines?

- **Concept: Familiarity with at least one ethical framework for AI**
  - Why needed here: The module references Floridi and Cowls' "Unified Framework" and expects students to connect specific harms (bias, hallucination) to broader principles.
  - Quick check question: Name two ethical principles commonly applied to AI systems and give one concrete example of how each might be violated.

- **Concept: Understanding of algorithmic bias and its sources**
  - Why needed here: The module's deep-dive on bias assumes baseline familiarity with how ML systems can perpetuate or amplify existing disparities.
  - Quick check question: Describe one way that a machine learning model trained on historical hiring data might produce biased outcomes, even if the training process itself appears neutral.

## Architecture Onboarding

- **Component map:** Part 1: Ethical Impacts (2 lectures) -> Part 2: AI Policy Landscape (1 lecture) -> Part 3: AI Regulation Assignment (take-home, group)

- **Critical path:**
  1. Establish that artifacts embed politics (Winner) → creates motivation for examining AI's social role
  2. Demonstrate concrete harms (bias case studies) → makes abstract concerns tangible
  3. Introduce policy as solution space → provides vocabulary and conceptual tools
  4. Technical assignment → forces translation from principle to practice
  5. Reflection and policy proposal → consolidates learning

  **If any step fails:** Students may view ethics as irrelevant (skip step 1), abstract (skip step 2), someone else's problem (skip step 3), or disconnected from their technical work (skip step 4).

- **Design tradeoffs:**
  - **Depth vs. breadth in ethics coverage:** Module 2.0 goes deep on bias; other topics (privacy, IP, trustworthiness) are covered elsewhere in the host course.
  - **Jailbreak vs. align options:** All 18 student groups chose jailbreak; align option was too technically demanding for an ethics course audience.
  - **Technical prerequisite requirements:** Assignment specifications were intentionally vague to accommodate varying skill levels.

- **Failure signatures:**
  - Low assignment submission quality or completion: Students may lack technical confidence or see assignment as irrelevant.
  - No significant pre/post survey changes: May indicate module is too short or content is not landing.
  - Student disengagement during policy lecture: May indicate "politics" framing is bleeding through despite redefinition.
  - All groups choose same assignment option: In this pilot, all chose jailbreak.

- **First 3 experiments:**
  1. Pilot in a technical ML course rather than standalone ethics course: Compare whether students in technical courses propose more concrete technical interventions in the assignment, and whether the align option becomes viable.
  2. Add explicit scaffolding for the "align" option: Provide a Jupyter notebook with a small open-source model and starter code for implementing content filters or refusal layers.
  3. Extend pre/post survey timing to 4-6 weeks post-module: Current design measures immediate attitude shifts. A delayed survey would test whether changes persist and whether students actually follow through on intentions to track AI policy news.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What technical scaffolding (e.g., interactive notebooks, open-source LLMs) would make the "align" option of the AI Regulation Assignment approachable enough for students to choose it over "jailbreak"?
- Basis in paper: Authors note all 18 groups chose jailbreak, suggesting alignment is "significantly more difficult" and requires "additional technical scaffolding."
- Why unresolved: The pilot did not test scaffolding approaches; the difficulty gap remains unaddressed.
- What evidence would resolve it: A follow-up study comparing completion rates and quality of work between scaffolded and unscaffolded "align" options.

### Open Question 2
- Question: Can the AI Policy Module produce statistically significant attitudinal changes beyond the 6 of 14 Likert statements that shifted in this pilot?
- Basis in paper: Authors state "most response distributions do not change significantly" and that attitudes are "not easily altered by small-scale interventions."
- Why unresolved: The pilot only tested one delivery format in one course context with limited sample size.
- What evidence would resolve it: Larger multi-institution studies with randomized control designs testing extended or more intensive module formats.

### Open Question 3
- Question: How does the AI Policy Module perform when embedded in technical AI courses versus standalone ethics courses?
- Basis in paper: The module was adapted from a machine learning course to an ethics course, but comparative efficacy data across contexts is not provided.
- Why unresolved: The two pilots used different student populations and assessment approaches, preventing direct comparison.
- What evidence would resolve it: A comparative study using consistent survey instruments across both technical and ethics course contexts.

### Open Question 4
- Question: Do students retain increased interest in AI policy and apply module concepts in their subsequent coursework or careers?
- Basis in paper: Post-survey showed increased intent to follow AI regulation news, but no longitudinal follow-up was conducted.
- Why unresolved: The study design captured only immediate post-module attitudes, not lasting behavioral change.
- What evidence would resolve it: Longitudinal surveys administered 6-12 months post-module assessing actual news-following behavior and career choices.

## Limitations
- The module's effectiveness depends heavily on implementation context—its impact in a standalone ethics course may differ substantially from embedding within a technical ML curriculum.
- Most critically, the correlation between attitudinal shifts and actual behavioral change (e.g., students following AI policy news or implementing ethical considerations in their work) remains unmeasured.
- The mechanism linking policy framing to technical engagement is plausible but not directly validated.

## Confidence

- **High confidence:** The module structure is clearly specified and reproducible; the jailbreak assignment is engaging and produces technically substantive work; students report finding the content relevant to their careers.
- **Medium confidence:** The attitudinal shifts are statistically significant but modest in magnitude; the policy-as-translation-mechanism is conceptually sound but not empirically validated; the framing of policy as distinct from partisan politics appears effective but relies on student self-reporting.
- **Low confidence:** The long-term impact on student behavior or career choices; whether the observed engagement translates to deeper understanding of AI governance; whether alternative framings (e.g., "responsible innovation" vs. "policy") might be equally or more effective.

## Next Checks
1. **Replication in technical ML course:** Test whether students in ML-focused courses propose more technically specific interventions and whether the "align" option becomes viable with reduced scaffolding requirements.
2. **Delayed post-survey (4-6 weeks):** Measure persistence of attitudinal changes and actual follow-through on intentions to track AI policy developments.
3. **Comparative framing experiment:** Test whether students taught with "responsible innovation" or "governance" terminology (rather than "policy") show similar engagement levels and attitudinal shifts, particularly among students who express political skepticism.