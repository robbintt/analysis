---
ver: rpa2
title: Towards On-Device Learning and Reconfigurable Hardware Implementation for Encoded
  Single-Photon Signal Processing
arxiv_id: '2504.09028'
source_url: https://arxiv.org/abs/2504.09028
tags:
- training
- hardware
- learning
- osos-elm
- fpga
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of implementing online learning
  for encoded single-photon signal processing using a hardware-efficient neural network
  algorithm. Specifically, it proposes an Online Sequential Extreme Learning Machine
  with One-Sided Jacobi rotation-based Singular Value Decomposition (OSOS-ELM) that
  enables real-time training and inference on FPGA hardware for applications like
  LiDAR, FLIM, and DCS.
---

# Towards On-Device Learning and Reconfigurable Hardware Implementation for Encoded Single-Photon Signal Processing

## Quick Facts
- arXiv ID: 2504.09028
- Source URL: https://arxiv.org/abs/2504.09028
- Reference count: 40
- Implements real-time online learning for encoded single-photon signal processing on FPGA with 1.05 ms training latency

## Executive Summary
This work proposes OSOS-ELM, a hardware-efficient neural network algorithm for online sequential learning of encoded single-photon signals. The method combines OS-ELM with One-Sided Jacobi rotation-based SVD (OJR-SVD) to enable real-time training and inference on FPGA hardware for applications including LiDAR, FLIM, and DCS. The implementation achieves sub-millisecond processing latencies while maintaining accuracy comparable to software implementations.

## Method Summary
The method implements OSOS-ELM by integrating OJR-SVD into OS-ELM for efficient matrix pseudo-inversion during initial training. The algorithm uses a two-phase approach: Initial Training (IT) on dual CPU cores computes P0 and η0 via parallel OJR-SVD operations, while One-Batch Training (OBT) processes sequential samples with batch size k=1 on FPGA programmable logic. The hardware implementation partitions IT onto the processing system and OBT onto the programmable logic, using AXI interfaces for data transfer and bare-metal pseudo-interruption for synchronization.

## Key Results
- Achieves real-time performance on ZCU104 Ultrascale+ MPSoC FPGA with 1.05 ms training and 0.18 ms inference latency
- Consumes 56 DSPs, 38,266 FFs, and 31,921 LUTs for network with 64 input nodes, 50 hidden nodes, and 1 output node
- FPGA implementation demonstrates comparable latency and power consumption to GPU implementations for compact network configurations
- Fixed 64-bit floating-point precision required for DCS regression accuracy; LiDAR and FLIM tolerate lower precision

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Replacing standard SVD with One-Sided Jacobi rotation-based SVD (OJR-SVD) enables hardware-amenable parallelization of matrix pseudo-inversion during initial training.
- Mechanism: OJR-SVD decomposes the Moore-Penrose pseudo-inverse computation into independent Jacobi rotation iterations across matrix columns. Two OJR-SVD operations (for PN0 and ηN0) have no data dependency and execute concurrently on separate CPU cores. The iteration count parameter (count) trades accuracy for latency, with the paper finding convergence at count ≥ 15.
- Core assumption: The initial training batch matrix H0 is sufficiently well-conditioned that Jacobi iterations converge within the fixed iteration budget.
- Evidence anchors:
  - [abstract] "proposes an Online Sequential Extreme Learning Machine with One-Sided Jacobi rotation-based Singular Value Decomposition (OSOS-ELM) that enables real-time training and inference on FPGA hardware"
  - [section III-C] "The two OJR SVD operations in IT can be executed independently, as there is no data dependency between them"
  - [corpus] Weak direct corpus support for OJR-SVD specifically; corpus focuses on general FPGA acceleration patterns.
- Break condition: Ill-conditioned matrices requiring >15 Jacobi iterations for convergence will degrade accuracy or require parameter tuning.

### Mechanism 2
- Claim: Setting sequential batch size k=1 converts matrix inversion to scalar inversion, eliminating the primary computational bottleneck during online training.
- Mechanism: In OS-ELM Equation (4), when k=1, the term I + HiPi-1HiT becomes a 1×1 scalar rather than a k×k matrix. This transforms the OBT phase from matrix inversion to scalar division, enabling efficient FPGA implementation with pipelined matrix-vector operations.
- Core assumption: Single-sample sequential updates provide sufficient gradient information for convergence; the initial training phase (N0 > L samples) establishes a stable baseline.
- Evidence anchors:
  - [section III-B] "the matrix inversion in Eq. (2) can be converted to a scalar number inversion by making the sequential batch size k = 1, since the size of I + HiPi-1HiT is 1 × 1"
  - [section IV-B] "OBT (from Lines 24 to 30 in Algorithm 1) mainly involves matrix-vector multiplication (MVM), matrix-matrix multiplication (MMM), and scalar division"
  - [corpus] No corpus papers specifically address this batch-size-1 simplification.
- Break condition: Applications requiring mini-batch updates (k>1) for stability will incur matrix inversion overhead.

### Mechanism 3
- Claim: Heterogeneous PS-PL partitioning with pseudo-interruption synchronization achieves lower latency than single-platform execution for the two-phase training pipeline.
- Mechanism: IT executes on dual ARM Cortex-A53 cores via memory-mapped trigger flags (addresses 0x0000_0000 and 0x0000_0004) without OS interruption overhead. OBT runs on PL with preloaded W/b parameters in BRAM, dispatched to register files for parallel access. Working mode switching (0=preload, 1=train, 2=infer) via AXI-lite enables runtime reconfiguration.
- Core assumption: IT latency (~4 seconds) is acceptable as a one-time overhead; OBT latency dominates real-time performance requirements.
- Evidence anchors:
  - [abstract] "processing latencies of 1.05 ms for training and 0.18 ms for inference"
  - [section IV-B] "IT is implemented on the PS using a bare-metal platform, which is more suitable for time-critical computing than PetaLinux"
  - [corpus] "Accelerated Digital Twin Learning for Edge AI" corroborates FPGA/GPU comparison methodology; "POLARON" supports precision-aware on-device learning patterns.
- Break condition: Applications requiring frequent re-initialization (new IT phases) will incur 4-second penalties.

## Foundational Learning

- Concept: **Extreme Learning Machine (ELM) basics**
  - Why needed here: OSOS-ELM inherits ELM's fixed random input-to-hidden weights (W, b), with only output weights (η) learned via pseudo-inverse. Understanding this explains why training is fast but model capacity is limited.
  - Quick check question: Can you explain why ELM avoids backpropagation and what trade-off this introduces?

- Concept: **Moore-Penrose pseudo-inverse and SVD relationship**
  - Why needed here: The core algorithmic contribution is computing (H0TH0)-1H0T via OJR-SVD. Understanding why SVD provides numerical stability for ill-conditioned matrices is essential.
  - Quick check question: Why might direct matrix inversion fail for H0TH0, and how does SVD truncation (Algorithm 1, lines 6-10) address this?

- Concept: **FPGA PS/PL architecture and AXI interfaces**
  - Why needed here: The implementation splits IT (CPU cores) from OBT (programmable logic) with AXI-full for data transfer and AXI-lite for configuration. Memory segmentation at 0x0100_0000, 0x0300_0000, 0x4000_0000 is critical to understand.
  - Quick check question: How does the pseudo-interruption mechanism differ from standard OS interrupts, and why does bare-metal execution matter?

## Architecture Onboarding

- Component map:
  - Processing System (PS): Quad ARM Cortex-A53; CPU#0/CPU#1 run parallel OJR-SVD; CPU#0 orchestrates mode switching
  - Programmable Logic (PL): Three IP cores—data loading module (BRAM), training module (pipelined MVM/MMM), inference module (separate logic)
  - Memory: DDR for dataset storage; segmented BRAM (0x4000_0000 shared, 0x4f00_0000 for η output); SD card for training data
  - Interfaces: AXI-full for matrix transfer, AXI-lite for scalar parameters and working mode

- Critical path:
  1. Preload W, b from PS to PL BRAM (mode=0)
  2. Load initial batch from SD → DDR → compute H0 on CPU
  3. Fork: CPU#0 runs OJR-SVD(A), CPU#1 runs OJR-SVD(H0) concurrently
  4. Join: CPU#0 receives ηN0, triggers OBT on PL (mode=1)
  5. Stream sequential samples: DDR → PL training module → update η
  6. Switch to inference (mode=2): xtest → PL inference module → ŷ

- Design tradeoffs:
  - **FLP vs. FXP**: Paper uses 64-bit floating-point despite higher resource cost; DCS requires >20 fractional bits for acceptable MSE, FLIM is more tolerant
  - **Separate training/inference modules**: Prevents simultaneous train+infer but reserves future capability
  - **Fixed Jacobi iteration count**: count=15 balances speed/accuracy; not adaptive to matrix conditioning

- Failure signatures:
  - **BRAM exhaustion at high L**: L=150 with #IN=256 uses 98% BRAM; place-and-route will fail on smaller FPGAs (PYNQ-Z2 noted as insufficient)
  - **Accuracy collapse with FXP**: DCS β reconstruction fails at <20 fractional bits; monitor MSE during quantization
  - **Training stall on trigger flag**: If CPU#1 doesn't detect flag flip at 0x0000_0004, check memory map alignment
  - **GPU outperforms FPGA at large L**: L≥150 shows GPU OBT latency lower; this is expected, not a bug

- First 3 experiments:
  1. **Validate IT/OBT handoff**: Load minimal dataset (N0=250, L=50), instrument timer around IT completion and first OBT iteration. Verify ηN0 transfer from CPU#1 to PL BRAM.
  2. **Latency scaling with L**: Measure per-sample OBT latency at L∈{50,100,150} with fixed #IN=128. Confirm FPGA latency grows faster than GPU (Table III, Fig. 8 pattern).
  3. **Precision sensitivity test**: Run DCS inference with FXP simulation (MATLAB quantizer) at fractional bits∈{8,16,20,24}. Confirm MSE convergence aligns with Fig. 3(b) before committing to FXP hardware.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can advanced quantization strategies enable the use of fixed-point arithmetic for complex regression tasks like DCS without compromising accuracy?
- Basis in paper: [inferred] The authors note that while fixed-point arithmetic saves resources, regression tasks (specifically DCS) are highly sensitive to rounding errors, forcing the use of floating-point arithmetic (Section IV.B).
- Why unresolved: The study identified the sensitivity but defaulted to floating-point precision rather than solving the quantization challenge for regression.
- What evidence would resolve it: A hardware implementation using optimized fixed-point or mixed-precision arithmetic that matches floating-point regression accuracy on the DCS dataset.

### Open Question 2
- Question: How does the system perform in terms of total end-to-end latency when the processing unit is fully integrated with the data acquisition front-end?
- Basis in paper: [explicit] The authors state, "hardware interfacing latency is not considered" in their performance evaluation, focusing only on the OSOS-ELM module's processing time (Section V).
- Why unresolved: The current benchmarks isolate the training/inference logic on the PL and PS but exclude the physical data transfer overhead from the sensor interface.
- What evidence would resolve it: Timing measurements from a holistic system including the sensor-to-memory and memory-to-PL data paths.

### Open Question 3
- Question: Can the OSOS-ELM FPGA architecture be optimized to maintain efficiency for larger network configurations (e.g., L > 150) to compete with GPU performance?
- Basis in paper: [inferred] Results indicate that FPGA latency increases significantly with the number of hidden nodes, leading the authors to conclude GPUs are more efficient for large networks while FPGAs are suited for compact ones (Section V).
- Why unresolved: The current implementation is optimized for smaller topologies; architectural changes are needed to prevent latency from scaling linearly with network size on hardware.
- What evidence would resolve it: FPGA utilization and latency metrics demonstrating superior or comparable performance to GPUs for large hidden layer sizes.

## Limitations
- Hardware interfacing latency not measured; current benchmarks only include OSOS-ELM processing time, not sensor-to-memory transfer overhead
- GPU efficiency advantage emerges at L≥150; FPGA design optimized for compact networks with L<150
- DCS regression requires 64-bit floating-point precision; fixed-point arithmetic insufficient for maintaining accuracy

## Confidence
- **High confidence**: OSOS-ELM algorithmic framework and FPGA PS/PL partitioning approach
- **Medium confidence**: Hardware implementation details and resource utilization estimates
- **Low confidence**: Claims about FPGA superiority for compact networks given limited comparison scope

## Next Checks
1. **Condition number sensitivity analysis**: Systematically vary Jacobi iteration count across matrices with known condition numbers to establish error bounds and identify numerical stability breaking points
2. **Scalability stress test**: Implement complete pipeline for L=200 and #IN=256 to verify hardware efficiency claims hold at network dimensions approaching practical limits
3. **Power consumption profiling**: Conduct comprehensive power measurements for FPGA and GPU implementations across all three applications (FLIM, DCS, LiDAR) at varying network sizes to validate claimed power efficiency advantages