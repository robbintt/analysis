---
ver: rpa2
title: 'One-Shot Federated Ridge Regression: Exact Recovery via Sufficient Statistic
  Aggregation'
arxiv_id: '2601.08216'
source_url: https://arxiv.org/abs/2601.08216
tags:
- one-shot
- privacy
- communication
- data
- fedavg
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper demonstrates that iterative communication is not necessary\
  \ for distributed linear regression in federated learning. The key insight is that\
  \ ridge regression admits a closed-form solution dependent only on sufficient statistics\u2014\
  the Gram matrix and moment vector\u2014which can be computed locally and transmitted\
  \ once to reconstruct the global solution exactly."
---

# One-Shot Federated Ridge Regression: Exact Recovery via Sufficient Statistic Aggregation

## Quick Facts
- arXiv ID: 2601.08216
- Source URL: https://arxiv.org/abs/2601.08216
- Reference count: 24
- One-line primary result: Exact recovery of ridge regression solution with single-round communication via sufficient statistic aggregation, achieving up to 38× less communication than iterative methods.

## Executive Summary
This paper introduces One-Shot σ-Fusion, a federated learning method that achieves exact recovery of ridge regression solutions without iterative communication. The key insight is that ridge regression admits sufficient statistics—the Gram matrix and moment vector—that decompose additively across data partitions. By computing these locally and transmitting them once to a central server, the global solution can be reconstructed exactly. The method demonstrates significant communication savings (O(d²) vs O(Rd)) and improved privacy-utility tradeoffs by avoiding differential privacy composition penalties inherent in iterative approaches.

## Method Summary
One-Shot σ-Fusion computes local sufficient statistics at each client: the Gram matrix G_k = A_k^T A_k and moment vector h_k = A_k^T b_k. These statistics are transmitted once to a central server, which aggregates them globally (G = ΣG_k, h = Σh_k) and solves for the ridge regression coefficients via w_σ = (G + σI)^(-1)h. The method achieves exact recovery under a coverage condition on client feature matrices, with non-asymptotic error bounds for heterogeneous cases. For high-dimensional settings where d ≫ R, random projections can reduce communication from O(d²) to O(m²) for m ≪ d. Differential privacy is achieved through single-round noise injection, avoiding the composition penalties that degrade privacy in multi-round protocols.

## Key Results
- One-Shot achieves exact recovery of centralized ridge solution (MSE 0.0100 vs 0.0102-0.0103 for FedAvg) with up to 38× less communication
- Single-round communication eliminates differential privacy composition penalties, improving privacy-utility tradeoffs at moderate privacy budgets (ε ≥ 1)
- Random projections reduce communication complexity from O(d²) to O(m²) while maintaining bounded approximation error
- The framework applies to kernel methods and random feature models but not general nonlinear architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Ridge regression admits sufficient statistics (Gram matrix G and moment vector h) that decompose additively across data partitions, enabling exact global solution reconstruction from local aggregates.
- Mechanism: Each client computes local statistics G_k = A_k^T A_k and h_k = A_k^T b_k locally. The server sums these to obtain global G = ΣG_k and h = Σh_k, then solves w_σ = (G + σI)^(-1)h. This algebraic decomposition is exact, not approximate.
- Core assumption: All clients transmit their complete local statistics; no intermediate model states are needed.
- Evidence anchors:
  - [abstract] "The optimal solution depends only on aggregated second-order statistics... these sufficient statistics decompose additively across data partitions"
  - [Section III-F, Theorem 1] Proves additive decomposition: G = ΣG_k, h = Σh_k
  - [corpus] Neighbor paper "Handling Covariate Mismatch in Federated Linear Prediction" addresses related linear prediction settings, suggesting broader applicability of statistical aggregation approaches.
- Break condition: Non-linear models lacking closed-form solutions; objectives without additively decomposable sufficient statistics.

### Mechanism 2
- Claim: Exact recovery of the centralized solution is guaranteed for any data distribution when the coverage condition (λ_min(G) ≥ α > 0) is satisfied.
- Mechanism: The closed-form solution w_σ = (G + σI)^(-1)h is mathematically identical whether computed on centralized data or reconstructed from federated statistics. Regularization parameter σ > 0 ensures (G + σI) is always positive definite and invertible.
- Core assumption: The regularization σ > 0 is non-zero; the feature matrices collectively span the feature space (α-coverage).
- Evidence anchors:
  - [abstract] "one-shot aggregation yields the centralized ridge solution, not an approximation"
  - [Section IV-A, Theorem 2] "wfed_σ = wcentral_σ. This equality is exact for any data distribution, any K, any partition"
  - [Section V-B, Table II] One-Shot achieves MSE 0.0100, identical to centralized oracle, versus FedAvg's 0.0102-0.0103
- Break condition: Zero regularization (σ = 0) with rank-deficient Gram matrix; clients failing to transmit.

### Mechanism 3
- Claim: Single-round communication eliminates differential privacy composition penalties, improving privacy-utility tradeoffs at moderate privacy budgets (ε ≥ 1).
- Mechanism: Noise is injected once per client into statistics: (G_k + E_k, h_k + e_k) where E_k ~ N(0, τ²I). Iterative methods incur ε_total ∝ √R under composition; one-shot incurs ε once.
- Core assumption: Assumption: Noise magnitude scaled to sensitivity; matrix inversion stability maintained after noise addition.
- Evidence anchors:
  - [abstract] "Differential privacy is achieved with single-round noise injection, avoiding composition penalties"
  - [Section IV-E, Theorem 7] Quantifies iterative composition: ε_total = √(2R ln(1/δ₀))ε₀ + Rε₀(e^ε₀ - 1)
  - [Section V-F, Table V] At ε ≥ 2, One-Shot outperforms DP-FedAvg (MSE 0.0102 vs 0.0105). At ε < 0.5, FedAvg wins due to noise amplification in matrix inversion.
  - [corpus] Limited direct corpus evidence on DP composition in one-shot FL; primarily paper-internal analysis.
- Break condition: Very high privacy regimes (ε < 0.5) where noise magnitude destabilizes matrix inversion; very large d where noise affects d² entries.

## Foundational Learning

- Concept: Ridge Regression (Tikhonov Regularization)
  - Why needed here: The entire method relies on the closed-form solution to ℓ₂-regularized least squares; without this, the sufficient statistics approach fails.
  - Quick check question: Can you derive why adding σI to A^T A guarantees invertibility?

- Concept: Sufficient Statistics
  - Why needed here: Understanding that G and h capture all information needed for the ridge solution enables the decomposition insight.
  - Quick check question: If you had only G and h (not raw data A, b), could you compute the ridge solution? What can you NOT compute?

- Concept: Differential Privacy Composition
  - Why needed here: The privacy advantage of one-shot methods is only meaningful relative to multi-round composition costs.
  - Quick check question: Why does privacy loss grow as O(√R) rather than O(R) under advanced composition?

## Architecture Onboarding

- Component map:
  - **Client Module**: Computes G_k (d×d Gram matrix), h_k (d-dim moment vector). Storage: O(d² + d). Computation: O(n_k × d²).
  - **Server Module**: Aggregates ΣG_k, Σh_k; performs Cholesky decomposition and matrix inversion. Computation: O(Kd² + d³).
  - **Random Projection Layer (optional)**: Projects features via R ∈ R^(d×m) where m ≪ d. Reduces communication from O(d²) to O(m²).

- Critical path:
  1. Clients receive σ (regularization) and optionally projection matrix R
  2. Each client computes local (G_k, h_k) or projected (G̃_k, h̃_k)
  3. Single transmission to server
  4. Server aggregates and inverts (G + σI)
  5. Broadcast w_σ to clients

- Design tradeoffs:
  - **Exact vs. compressed**: Full statistics (O(d²) comm) give exact recovery; random projection (O(m²)) gives bounded approximation error ‖w̃ - w_σ‖ ≤ O(√(d/m))‖w_σ‖
  - **Privacy vs. stability**: Higher noise improves privacy but can destabilize matrix inversion; crossover at ε ≈ 1
  - **Dimension vs. rounds**: One-Shot beats iterative when d < 4R; for d=1000, R=200, use m≈400-600 projection

- Failure signatures:
  - MSE significantly above centralized oracle → check if all clients transmitted; verify σ > 0
  - Matrix inversion fails → check if σ is too small relative to noise (privacy regime) or if G is rank-deficient
  - Communication exceeds iterative baseline → d too large; enable random projection with m ≈ 0.4d
  - Privacy-utility degradation at ε < 0.5 → expected; switch to iterative methods or secure aggregation

- First 3 experiments:
  1. **Exact recovery validation**: Run One-Shot on synthetic data (K=20, d=100, n_k=500) with varying heterogeneity γ ∈ [0,1]. Verify MSE matches centralized oracle (±10⁻⁴). Confirm heterogeneity invariance per Theorem 5.
  2. **Communication crossover analysis**: Vary d ∈ {50, 100, 200, 400, 800} with R=200 rounds. Identify crossover where One-Shot comm > FedAvg comm. Validate Corollary 2 prediction (d ≈ 4R).
  3. **Privacy-utility tradeoff**: Test DP variant at ε ∈ {0.1, 0.5, 1.0, 2.0, 5.0}. Confirm crossover at ε ≈ 1 where One-Shot begins outperforming DP-FedAvg. Monitor matrix condition number κ(G̃ + σI) for stability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can secure aggregation protocols improve the privacy-utility trade-off for One-Shot methods in the high-privacy regime (ε < 0.5)?
- Basis in paper: [explicit] The authors note in Section VI-D that One-Shot underperforms at very high privacy (ε < 0.5) due to noise amplification through matrix inversion, and propose secure aggregation as a potential solution that "would reduce total noise by factor √K while maintaining privacy."
- Why unresolved: Implementing secure aggregation for matrix-valued statistics requires cryptographic protocols for secure sum computation that were not developed or evaluated in this work.
- What evidence would resolve it: Empirical comparison of One-Shot with secure aggregation versus iterative private methods at ε < 0.5, measuring MSE and communication costs.

### Open Question 2
- Question: What are tighter theoretical bounds on random projection approximation error under heterogeneous (non-IID) data distributions?
- Basis in paper: [explicit] Section VI-D states: "Tighter characterization of the approximation error from random projection under heterogeneous data distributions remains open."
- Why unresolved: The current bound in Proposition 3 is worst-case and does not account for structure in heterogeneous feature distributions across clients.
- What evidence would resolve it: Refined error bounds depending on spectral properties of client-specific Gram matrices, validated empirically across controlled heterogeneity levels.

### Open Question 3
- Question: How can one-shot sufficient statistic aggregation be extended to vertically partitioned data where clients hold different features for the same samples?
- Basis in paper: [explicit] Section VI-C states: "Vertical partitioning (same samples, different features) requires secure inner product protocols."
- Why unresolved: Cross-client feature correlations require computing inner products between feature subsets held by different parties, which cannot be decomposed additively like horizontal partitions.
- What evidence would resolve it: A protocol using secure multiparty computation for cross-client inner products, with privacy guarantees and communication complexity analysis.

### Open Question 4
- Question: Do hybrid approaches combining one-shot initialization with few private iterative rounds achieve superior privacy-utility trade-offs across all privacy regimes?
- Basis in paper: [explicit] Section VI-D lists "hybrid approaches: Using One-Shot for initial model and refinement via few private iterative rounds could combine the advantages of both paradigms" as a research direction.
- Why unresolved: The optimal balance between one-shot and iterative components, and whether hybrids dominate pure approaches, is unexplored.
- What evidence would resolve it: Experiments varying the number of refinement rounds after one-shot initialization, measuring MSE versus total privacy budget across ε ∈ [0.1, 10].

## Limitations

- The framework only applies to linear models with closed-form solutions, not general nonlinear architectures requiring iterative optimization.
- High-dimensional settings (d > 4R) require random projections, introducing approximation error that degrades exact recovery guarantees.
- The method requires full client participation and complete statistic transmission, making it vulnerable to client dropouts or malicious clients.

## Confidence

- **Exact recovery claim**: High confidence. The additive decomposition of sufficient statistics and closed-form ridge solution are mathematically proven with empirical validation across heterogeneity levels.
- **Communication complexity advantage**: Medium-High confidence. The O(d²) vs O(Rd) comparison is rigorous, though real-world benefits depend on specific d, R, and bandwidth constraints.
- **Privacy composition advantage**: Medium confidence. The theoretical framework is sound, but empirical validation at extreme privacy regimes (ε < 0.5) shows method breakdown, suggesting practical limitations not fully addressed.

## Next Checks

1. **Robustness to client dropout**: Systematically measure MSE degradation when varying fractions of clients fail to transmit statistics. Quantify the relationship between participation rate and solution quality.
2. **Secure aggregation integration**: Implement secure aggregation protocols for statistic transmission and measure computational overhead and privacy-utility tradeoffs compared to plain additive aggregation.
3. **Generalization to kernel methods**: Extend the framework to kernel ridge regression with RBF kernels, validating whether sufficient statistics (kernel matrices) can be computed and transmitted efficiently in the federated setting.