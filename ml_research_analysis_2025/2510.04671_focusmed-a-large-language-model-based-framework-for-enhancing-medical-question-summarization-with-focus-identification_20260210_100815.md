---
ver: rpa2
title: 'FocusMed: A Large Language Model-based Framework for Enhancing Medical Question
  Summarization with Focus Identification'
arxiv_id: '2510.04671'
source_url: https://arxiv.org/abs/2510.04671
tags:
- focus
- question
- medical
- arxiv
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FocusMed tackles the problem of generating concise, accurate medical
  question summaries by leveraging large language models (LLMs). The framework first
  extracts core question focuses from consumer health questions using tailored prompts
  and faithfulness validation, then constructs an enhanced dataset for fine-tuning.
---

# FocusMed: A Large Language Model-based Framework for Enhancing Medical Question Summarization with Focus Identification

## Quick Facts
- arXiv ID: 2510.04671
- Source URL: https://arxiv.org/abs/2510.04671
- Reference count: 33
- Key outcome: Achieves 7.3% and 6.7% ROUGE-L improvement over previous methods on two medical summarization datasets

## Executive Summary
FocusMed addresses the challenge of generating accurate, concise medical question summaries by first extracting faithful question focuses using LLM-based techniques, then fine-tuning on enhanced datasets, and finally selecting the best output through multi-dimensional quality evaluation. The framework improves upon existing methods by mitigating hallucinations and better identifying key medical entities like medications and symptoms. Experiments show state-of-the-art performance on MEDIQA and MeqSum datasets with significant improvements in ROUGE-L scores while maintaining faithfulness to source content.

## Method Summary
FocusMed operates through a three-stage pipeline: (1) LLM-based focus extraction using JSON+CoT prompting and faithfulness verification with TextRank and semantic similarity thresholds, (2) fine-tuning Qwen2.5-7B and LLaMA3.1-8B models using QLoRA on an augmented dataset combining CHQs with verified focuses, and (3) multi-dimensional quality evaluation that selects the best summary from four model outputs based on faithfulness, conciseness, and coverage scores. The framework uses dataset-specific weight configurations for the selection mechanism and employs DeepSeek-R1 for atomic fact decomposition during evaluation.

## Key Results
- Achieves 7.3% and 6.7% improvement in ROUGE-L over previous best methods on MEDIQA and MeqSum datasets
- Significantly reduces hallucinations through faithfulness verification with semantic similarity thresholds (0.8-0.9)
- Improves question focus identification accuracy through structured extraction with JSON+CoT format
- Multi-dimensional selection mechanism outperforms individual models across all quality dimensions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structured prompting with JSON+CoT format improves focus extraction accuracy from CHQs
- Mechanism: Constrains LLM output to extract only medication and symptom entities (max 2 per category) in structured JSON format with justifications, channeling the model's pre-trained medical knowledge toward targeted extraction
- Core assumption: LLMs retain sufficient medical domain knowledge from pre-training to accurately identify relevant clinical entities without external knowledge bases
- Evidence anchors: Abstract states LLMs extract core focus faithful to original text; Section II-B describes the two-entity limit constraint; Limited corpus validation exists for related faithful summarization work

### Mechanism 2
- Claim: Semantic similarity-based faithfulness verification reduces hallucinated content in extracted focus points
- Mechanism: TextRank extracts key phrases from LLM output, then each phrase is validated against noun phrases from the original CHQ using semantic similarity; outputs below threshold (0.8-0.9) trigger re-extraction
- Core assumption: Hallucinated content exhibits measurably lower semantic similarity to source text than faithful extractions
- Evidence anchors: Abstract mentions prompt design for faithful extraction; Section II-B describes similarity threshold validation; Corpus shows related work but not this specific threshold approach

### Mechanism 3
- Claim: Multi-dimensional quality evaluation selects outputs that balance faithfulness, conciseness, and coverage better than any single model
- Mechanism: Four model combinations generate candidate outputs; DeepSeek-R1 decomposes each into atomic facts scored on three dimensions; weighted sum determines final selection with dataset-specific weights
- Core assumption: Combining complementary model strengths through systematic evaluation outperforms reliance on any single model
- Evidence anchors: Abstract describes multi-dimensional quality evaluation; Section II-D provides specific weight configurations (MEDIQA: 0.6/0.1/0.3, MeqSum: 0.3/0.4/0.3); Corpus lacks comparable multi-dimensional frameworks

## Foundational Learning

- Concept: Medical Question Summarization (MQS) task
  - Why needed here: Understanding that MQS converts verbose, non-professional CHQs into concise, standardized FAQs is essential for evaluating framework outputs
  - Quick check question: What makes a CHQ different from its target FAQ beyond just length reduction?

- Concept: LLM Hallucination in clinical contexts
  - Why needed here: The framework explicitly targets hallucination (fabricated medications, incorrect side effects) which poses patient safety risks
  - Quick check question: How would you distinguish between an inferential error and a factual hallucination in a medical summary?

- Concept: Chain-of-Thought (CoT) prompting with structured output
  - Why needed here: JSON+CoT format requires models to provide explicit justifications, improving extraction traceability and accuracy
  - Quick check question: Why would requiring justification improve extraction quality beyond just requesting the extraction?

## Architecture Onboarding

- Component map:
  Input CHQ → [Extraction LLM + JSON+CoT prompts] → Raw focus points → [TextRank + Semantic validation] → Verified focus → [Augmented dataset construction] → (CHQ + focus, FAQ) pairs → [Fine-tuning: Qwen2.5-7B or LLaMA3.1-8B with QLoRA] → 4 model variants → [Selection: Faithfulness/Conciseness/Coverage scoring] → Final summary

- Critical path:
  1. Focus extraction quality → determines augmented dataset quality
  2. Augmented dataset quality → determines fine-tuning effectiveness
  3. Model diversity in selection pool → determines selection mechanism value

- Design tradeoffs:
  - Similarity threshold (0.8-0.9): Higher = more faithful but more re-extraction cycles
  - Entity limit (max 2 per type): Constrains noise but may miss secondary concerns
  - Weight configurations differ by dataset (MEDIQA: faithfulness-dominant; MeqSum: balanced)
  - Model size in extraction: Larger models improve extraction but increase latency

- Failure signatures:
  - Over-extraction exceeding entity limits → model confusion during fine-tuning
  - Numerical details lost in summaries (paper notes "13 days" → "prolonged")
  - Similarity threshold too low → hallucinated entities pass verification
  - All four model candidates miss the same focus point → selection cannot recover

- First 3 experiments:
  1. Establish baseline ROUGE-L and SUMMAC ZS scores by fine-tuning Qwen2.5-7B directly on MEDIQA without focus extraction (expect ~0.334 ROUGE-L per Table III)
  2. Validate extraction pipeline by running focus extraction with faithfulness verification on 50 validation samples, manually checking precision/recall against gold standard focus annotations
  3. Ablate selection mechanism by comparing "FocusMed(w/o Selection)" (best single model = 0.364) vs full FocusMed (0.386) to isolate selection contribution on MEDIQA test set

## Open Questions the Paper Calls Out

- Question: How can medical question summarization systems be improved to accurately preserve and transfer numerical and temporal information (e.g., medication dosages, symptom durations, patient ages) from CHQs to summaries?
  - Basis in paper: [explicit] The Discussion section states: "due to the relatively low sensitivity of LLMs to numerical details, these models often struggle to focus on and accurately capture key numerical information within the text." The Conclusion explicitly identifies "insufficient sensitivity to critical data like numbers and time in certain scenarios" as a limitation requiring future optimization.
  - Why unresolved: Current LLM-based approaches tend to abstract numerical details into vague descriptors (e.g., "prolonged" instead of "13 days"), losing clinically actionable specificity. The paper's focus extraction mechanism targets semantic concepts but lacks dedicated handling of quantitative information.
  - What evidence would resolve it: A modified FocusMed framework with explicit numerical/temporal extraction modules, evaluated on a curated test set of CHQs containing critical numbers, showing statistically significant improvements in preserving exact values in summaries.

- Question: Would expanding the focus extraction beyond medications and symptoms to include other clinical entity types (e.g., procedures, lab values, demographics, temporal contexts) improve summary quality and clinical utility?
  - Basis in paper: [inferred] The Question Focus Extraction method (Section II-B) constrains extraction to "only these two types of entities, with a maximum limit of two entities per category," determined through experimentation. This design choice may omit clinically relevant information that doesn't fit these categories.
  - Why unresolved: The paper does not justify why only medications and symptoms were selected, nor does it report experiments with broader entity schemas. Medical questions may focus on other clinically important elements that current focus extraction misses.
  - What evidence would resolve it: Ablation studies comparing the current two-category schema against expanded schemas incorporating additional entity types, with evaluation on both automatic metrics and clinical relevance assessments.

- Question: Does FocusMed's performance generalize to real-world clinical settings and diverse patient populations beyond the MEDIQA and MeqSum benchmark datasets?
  - Basis in paper: [inferred] The evaluation relies exclusively on two relatively small datasets (1,000 and 500 samples respectively) from online medical platforms. The paper acknowledges no clinical validation beyond automated metrics, and the datasets may not represent the full diversity of patient question styles, medical conditions, or linguistic variations encountered in practice.
  - Why unresolved: The paper demonstrates improvements on benchmark datasets but does not address deployment in actual healthcare workflows, cross-institutional variation, or performance on questions from diverse demographic groups who may phrase concerns differently.
  - What evidence would resolve it: Multi-site clinical validation studies where FocusMed outputs are evaluated by practicing physicians across different specialties and institutions, along with analysis of performance stratified by question characteristics, patient demographics, and medical domains.

## Limitations
- Numerical and temporal information degradation: LLMs struggle to preserve specific values like "13 days" becoming vague descriptors like "prolonged"
- Focus extraction constraint: Limited to medications and symptoms only, potentially missing other clinically relevant entities
- Implementation uncertainty: Exact prompt templates and semantic similarity mechanisms not fully specified in text

## Confidence

- High confidence: Overall framework architecture and ROUGE improvement claims (7.3% and 6.7% gains)
- Medium confidence: Focus extraction mechanism effectiveness and hallucination mitigation claims
- Low confidence: Dataset-specific weight selection methodology and numerical precision preservation

## Next Checks

1. Implement and test the focus extraction pipeline with varying similarity thresholds (0.8, 0.85, 0.9) to determine optimal hallucination detection performance across different medical subdomains
2. Conduct ablation study comparing FocusMed with and without numerical entity preservation mechanisms to quantify information loss in temporal and dosage expressions
3. Validate the multi-dimensional selection mechanism by evaluating whether selected outputs consistently outperform individual model outputs across all three quality dimensions (faithfulness, conciseness, coverage)