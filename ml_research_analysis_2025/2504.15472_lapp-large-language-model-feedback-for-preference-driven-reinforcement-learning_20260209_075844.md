---
ver: rpa2
title: 'LAPP: Large Language Model Feedback for Preference-Driven Reinforcement Learning'
arxiv_id: '2504.15472'
source_url: https://arxiv.org/abs/2504.15472
tags:
- reward
- robot
- preference
- torch
- base
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LAPP, a framework that leverages large language
  models (LLMs) to generate preference feedback from raw state-action trajectories
  for reinforcement learning (RL). Instead of relying on handcrafted rewards or expensive
  human annotations, LAPP uses LLMs to automatically label trajectory pairs based
  on high-level behavior instructions.
---

# LAPP: Large Language Model Feedback for Preference-Driven Reinforcement Learning

## Quick Facts
- arXiv ID: 2504.15472
- Source URL: https://arxiv.org/abs/2504.15472
- Reference count: 40
- One-line result: LLM-generated preference feedback accelerates RL training and solves complex locomotion tasks like backflips

## Executive Summary
This paper introduces LAPP, a framework that leverages large language models (LLMs) to generate preference feedback from raw state-action trajectories for reinforcement learning (RL). Instead of relying on handcrafted rewards or expensive human annotations, LAPP uses LLMs to automatically label trajectory pairs based on high-level behavior instructions. These labels train an online transformer-based preference predictor, which guides policy optimization with adaptive, trajectory-informed rewards. Evaluated across challenging quadruped and dexterous manipulation tasks, LAPP accelerates training, achieves higher final performance, enables precise control over gait patterns and cadence, and successfully solves exploration-heavy tasks like quadruped backflips—previously infeasible with standard rewards or LLM-generated feedback. LAPP demonstrates strong real-world applicability and reduces reliance on manual reward engineering.

## Method Summary
LAPP integrates LLM-generated preference labels into reinforcement learning by prompting GPT-4o mini with numerical state-action trajectories and behavior instructions to compare trajectory pairs. The resulting preference labels train a transformer-based predictor using Bradley-Terry modeling with cross-entropy loss. This predictor provides dense rewards that combine with environment rewards during policy optimization via PPO. The framework employs online predictor updates using recent trajectory data, enabling dynamic preference shaping that adapts to different training stages. The system uses a 6-layer transformer with 8-headed attention processing sequences up to 8 timesteps, and employs an ensemble of 9 predictors selecting the top 3 for robustness.

## Key Results
- LAPP achieves faster training and higher final performance than PPO and LLM-Direct methods on quadruped locomotion tasks
- The framework enables precise control over gait patterns and cadence, with transformer predictors outperforming MLP alternatives on non-Markovian tasks
- LAPP successfully solves the quadruped backflip task, which was previously infeasible with standard rewards or direct LLM feedback alone

## Why This Works (Mechanism)

### Mechanism 1
LLM-generated preference labels over trajectory pairs serve as effective supervision for training a reward predictor in complex robotic control tasks. The system prompts an LLM with structured numerical state-action logs and behavior instructions, receiving preference labels that train a predictor via Bradley-Terry modeling with cross-entropy loss. The core assumption is that LLMs can reliably infer behavioral quality from numerical trajectory data without visual input, and their preferences align with human task objectives.

### Mechanism 2
A transformer-based preference predictor capturing temporal dependencies enables learning of non-Markovian rewards essential for tasks requiring sequence-level reasoning. The predictor uses a 6-layer GPT-style architecture with 8-headed attention, processing sequences of up to 8 timesteps to capture patterns like gait cadence and foot synchronization. The core assumption is that behaviors requiring temporal coherence cannot be evaluated adequately from single timesteps.

### Mechanism 3
Online predictor updates using only recent trajectory data enable dynamic preference shaping that adapts to different training stages. Every M epochs, newly collected trajectory pairs replace older data in the training set, allowing evaluation criteria to evolve as the policy improves. The core assumption is that optimal evaluation criteria change as policy improves, and stale trajectory data harms predictor accuracy.

## Foundational Learning

- **Bradley-Terry preference modeling**: Converts pairwise preference labels into probability distributions for training the reward predictor. Quick check: Can you derive P[A > B] from pairwise comparison data using the Bradley-Terry formula?

- **Markovian vs. non-Markovian reward functions**: Determines whether predictor needs temporal context; gait control and backflips require non-Markovian rewards. Quick check: Does the reward at time t depend only on (s_t, a_t) or on the trajectory history?

- **PPO (Proximal Policy Optimization)**: The underlying RL algorithm that optimizes the combined environment + preference reward. Quick check: How does PPO's clipped objective prevent excessive policy updates?

## Architecture Onboarding

- **Component map**: LLM (GPT-4o mini) -> Preference Predictor (Transformer ensemble) -> Policy Network (MLP) -> Environment -> Data buffers

- **Critical path**: 1) Policy rollout generates trajectory pairs 2) LLM labels pairs → D_p 3) Train/update preference predictor via Algorithm 1 4) Policy optimization with combined reward r = β·r_p + r_E 5) Repeat every M epochs

- **Design tradeoffs**: GPT-4o mini vs. GPT-4o costs ~$2.50 vs ~$40-50 per training run; transformer vs MLP predictor handles non-Markovian rewards but increases compute; sequence length 8 balances temporal coverage against computational cost

- **Failure signatures**: High "incomparable" (label=3) rate indicates prompts lack sufficient context; predictor overfitting triggers when validation loss exceeds 1.3× training loss; kettle task collapse occurs when using full trajectory pool or MLP predictor

- **First 3 experiments**: 1) Reproduce flat-plane locomotion with basic trot gait prompt 2) Ablation: Replace transformer with MLP predictor on cadence control task 3) Compare LAPP vs. PPO on stairs terrain

## Open Questions the Paper Calls Out

Can the framework be extended to incorporate visual inputs for tasks that require reactive motion planning or scene understanding? The paper notes that current multimodal models (VLMs) are more expensive and slower than text-only LLMs, making them difficult to integrate into the current framework which relies on numerical state-action logs.

Can the manual selection of state variables for the LLM prompt be automated to optimize the balance between prompt length and label accuracy? The paper suggests future work could warm up training with different variable selections, but currently requires human expert judgment to define the observation space for the LLM.

Can the frequency of LLM queries be reduced without compromising the performance gains achieved by online adaptive preference training? The method relies on continuous feedback to guide the policy, and reducing queries might reduce the granularity of the feedback, potentially leading to training instability or suboptimal convergence.

## Limitations

- Current framework does not handle visual inputs or multimodal trajectory data
- Manual curation of state variables for LLM prompts requires expert knowledge
- Frequent LLM queries create significant computational overhead
- Critical hyperparameters (predictor update interval M, pairs per epoch K) are unspecified in the main text

## Confidence

- Mechanism 1 (LLM labeling): Medium - Limited evidence on LLM consistency across diverse tasks
- Mechanism 2 (Transformer predictor): High - Well-justified architectural choices with clear ablation results
- Mechanism 3 (Online updates): Medium - Theoretical justification is sound but optimal update frequency remains empirically determined

## Next Checks

1. Replicate the flat-plane locomotion task with trot gait to verify the basic labeling-predictor loop functions correctly
2. Conduct ablation testing on predictor architecture (transformer vs MLP) for a non-Markovian task to confirm temporal dependency requirements
3. Measure LLM label consistency by computing inter-annotator agreement across multiple runs on identical trajectory pairs