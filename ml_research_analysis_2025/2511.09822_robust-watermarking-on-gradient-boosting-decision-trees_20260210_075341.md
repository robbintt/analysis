---
ver: rpa2
title: Robust Watermarking on Gradient Boosting Decision Trees
arxiv_id: '2511.09822'
source_url: https://arxiv.org/abs/2511.09822
tags:
- watermark
- conf
- dataset
- watermarking
- train
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work introduces the first robust watermarking framework for\
  \ Gradient Boosting Decision Trees (GBDTs), addressing their sequential and non-differentiable\
  \ structure. Four novel embedding strategies\u2014Wrong Prediction Flip, Outlier\
  \ Flip, Cluster Center Flip, and Confidence Flip\u2014are proposed, leveraging in-place\
  \ fine-tuning to embed resilient watermarks with minimal accuracy impact."
---

# Robust Watermarking on Gradient Boosting Decision Trees

## Quick Facts
- arXiv ID: 2511.09822
- Source URL: https://arxiv.org/abs/2511.09822
- Reference count: 22
- Introduces first robust watermarking framework for GBDTs with four novel embedding strategies

## Executive Summary
This work presents the first robust watermarking framework for Gradient Boosting Decision Trees (GBDTs), addressing their sequential and non-differentiable structure. The framework introduces four novel embedding strategies—Wrong Prediction Flip, Outlier Flip, Cluster Center Flip, and Confidence Flip—that leverage in-place fine-tuning to embed resilient watermarks with minimal accuracy impact. Experiments demonstrate high watermark embedding success (up to 100%), strong resistance to post-deployment fine-tuning, and competitive adjusted model accuracy across diverse datasets.

## Method Summary
The framework embeds watermarks through in-place fine-tuning rather than adding new trees, making watermarks integral to core decision logic. Four strategies select watermark candidates from training or separate datasets: Wrong Prediction Flip (targets misclassified samples), Outlier Flip (targets sparse region samples), Cluster Center Flip (targets cluster centroids with neighbor anchoring), and Confidence Flip (targets low-confidence boundary samples). The in-place engine modifies existing tree split points and leaf weights based on pseudo-residuals from watermark samples, while label modification assigns target watermark labels to selected candidates.

## Key Results
- Watermark embedding success rates up to 100% across all four strategies
- Confidence Flip achieves highest effectiveness (100%) when watermark dataset differs from training data
- Outlier Flip and Cluster Center Flip show best accuracy preservation with adjusted accuracy > 90%
- Watermarks demonstrate strong robustness, surviving post-deployment fine-tuning attacks

## Why This Works (Mechanism)

### Mechanism 1: In-Place Structural Integration
Embedding watermarks via in-place fine-tuning resists removal better than adding new trees because the watermark becomes integral to core decision logic rather than an appendage. The framework modifies split points and leaf weights of existing trees based on pseudo-residuals derived from watermark samples, rather than growing new trees that could be easily pruned.

### Mechanism 2: Boundary Plasticity via Confidence Targeting
Targeting low-confidence samples for label flipping minimizes global accuracy degradation because these points lie near decision boundaries where the model is most malleable. The Confidence Flip method selects samples where the model's prediction probability for the true class is lowest, requiring smaller shifts in tree structure compared to flipping high-confidence points deep within class regions.

### Mechanism 3: Localized Boundary Anchoring
Reinforcing neighbors of flipped cluster centroids prevents global boundary drift while maintaining accuracy. The Cluster Center Flip identifies a cluster centroid to flip but keeps its nearest neighbors at correct labels, forcing the model to create a small hole of wrong prediction without shifting the broader decision boundary for that class.

## Foundational Learning

- **Concept: Gradient Boosting Sequentiality**
  - Why needed: GBDTs build trees sequentially to correct errors from previous iterations. In-place updates modify these dependencies, making existing tree modifications necessary for robustness.
  - Quick check: If you modify a split in tree $t$, does it affect the pseudo-residuals calculated for tree $t+1$?

- **Concept: Decision Boundaries in Feature Space**
  - Why needed: Strategies (Outlier, Cluster, Confidence) rely on spatial geometry. You need to visualize how "outliers" or "cluster centers" map to regions where the model can be manipulated without breaking general logic.
  - Quick check: Why is flipping a sample in a sparse region (outlier) considered safer for accuracy than flipping a sample in a dense cluster?

- **Concept: Label Flipping vs. Data Poisoning**
  - Why needed: This method is a form of data manipulation (modifying labels $y_{wm}$ in Eq. 2) rather than direct weight manipulation. You need to distinguish between training with wrong data and editing model weights.
  - Quick check: According to Eq. 2, how is the target watermark label $y_{wm}$ chosen if the original prediction was correct?

## Architecture Onboarding

- **Component map:** Base Model (pre-trained GBDT) -> Candidate Generator (selects from training or separate datasets) -> Label Modifier (assigns watermark labels) -> In-Place Engine (Algorithm 1 implementation) -> Evaluator (measures effectiveness and accuracy)

- **Critical path:** The distinction between $D_{cand} = D_{train}$ vs. $D_{cand} \neq D_{train}$. If using training data for candidates, you must apply the duplication factor ($d_{cand=train}=5$) to ensure watermark gradients overpower the model's memory of original labels. If using separate data, standard fine-tuning applies.

- **Design tradeoffs:**
  - Cluster vs. Confidence: Cluster Center Flip offers higher accuracy preservation but requires clustering algorithm; Confidence Flip is computationally cheaper and very effective for separate watermark datasets.
  - Robustness vs. Accuracy: Increasing watermark ratio ($|W|/|D_{train}|$) generally increases robustness but degrades accuracy.

- **Failure signatures:**
  - Low Effectiveness ($A_{wm} < 0.5$): Gradients insufficient to flip prediction; likely cause: duplication factor too low or label conflict in Cluster Flip.
  - Accuracy Crash ($A_{model}$ drops > 10%): Candidates selected from high-confidence regions, shifting major boundaries.
  - Watermark Erasure: After fine-tuning, $A_{wm}$ drops to 0; likely cause: in-place update not used, allowing pruning of new trees.

- **First 3 experiments:**
  1. Feasibility Test: Train base model on `optdigits`, implement Confidence Flip with $k=10$, run Algorithm 1, verify trigger label prediction.
  2. Impact Analysis: Compare Random Flip vs. Cluster Center Flip on `Letter Recognition`, plot adjusted accuracy against watermark ratio (0.001, 0.01, 0.1).
  3. Robustness Verification: Fine-tune watermarked model from Experiment 2 on disjoint $D_{fine}$ set, measure fine-tuning robustness (Eq. 9).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can a theoretical guarantee be derived for convergence of gradient direction during in-place fine-tuning to ensure successful watermark embedding?
- **Basis:** Section 3.2 states gradient update cannot strictly guarantee negative gradient for watermark class, relying on empirical heuristics like sample duplication.
- **Why unresolved:** No formal guarantee exists; watermark signal relies on probability that gradient update favors target class, managed by duplication factor rather than closed-form solution.
- **What evidence would resolve it:** Theoretical proof establishing bounds for multiplier $r$ or model confidence $p$ required to ensure gradient directs prediction toward watermark label $y_{wm}$.

### Open Question 2
- **Question:** How do proposed strategies perform under active adversarial attacks like model pruning or evasion attacks versus standard fine-tuning?
- **Basis:** Paper evaluates robustness against fine-tuning but not structural attacks like pruning low-contribution nodes or leaves.
- **Why unresolved:** In-place updating mitigates tree addition problem, but nodes modified by watermark may be susceptible to pruning by efficiency-focused adversaries.
- **What evidence would resolve it:** Experimental results showing watermark survival rates under standard tree pruning algorithms or adversarial perturbations designed to minimize effectiveness.

### Open Question 3
- **Question:** Can Wrong Prediction Flip strategy be modified to be viable in high-accuracy scenarios with few or no misclassified samples?
- **Basis:** Table 7 notes strategy only viable when abundant misclassified samples exist; often invalid for $D_{cand} = D_{train}$ due to GBDT overfitting.
- **Why unresolved:** Method excluded from many internal watermarking contexts because high-performing models leave few wrong prediction candidates to flip.
- **What evidence would resolve it:** Modified algorithm that artificially generates or selects "near-miss" samples in high-confidence regions to simulate wrong prediction behavior without relying on naturally occurring errors.

## Limitations
- Framework's robustness assumes adversaries use standard pruning or fine-tuning rather than complete retraining from scratch, representing a fundamental vulnerability if original training data is accessible.
- Effectiveness metrics focus on watermark detection rates without fully exploring false positive rates or security implications of potential watermark collisions.
- Robustness guarantees against fine-tuning are primarily demonstrated through single scenario and may not capture full range of adversarial techniques.

## Confidence

**High Confidence:**
- In-place fine-tuning mechanism is technically sound with clear algorithmic descriptions
- Four watermark embedding strategies are distinct and implementable
- Experimental results showing high watermark embedding success rates (up to 100%) are verifiable

**Medium Confidence:**
- Accuracy preservation claims may not generalize across all dataset types and model complexities
- Comparative analysis showing Confidence Flip's superiority for separate watermark datasets could benefit from additional cross-dataset validation

**Low Confidence:**
- Robustness guarantees against fine-tuning lack comprehensive analysis under various attack scenarios
- Claim that in-place updates provide superior robustness compared to tree addition lacks comprehensive comparative analysis

## Next Checks

1. **Adversarial Retraining Experiment**: Implement complete retraining attack where adversary trains new model from scratch using original dataset and compare watermark persistence rates to quantify vulnerability to most powerful attack vector.

2. **Cross-Dataset Generalization Study**: Apply watermarking framework across 10-15 diverse datasets (different domains, sizes, and complexity levels) to validate whether claimed accuracy preservation and robustness patterns hold universally or are dataset-specific.

3. **False Positive and Collision Analysis**: Systematically measure false positive rates when applying watermark detection to unwatermarked models, and calculate probability of accidental watermark collisions across different datasets and embedding strategies.