---
ver: rpa2
title: 'VLM-KG: Multimodal Radiology Knowledge Graph Generation'
arxiv_id: '2505.17042'
source_url: https://arxiv.org/abs/2505.17042
tags:
- radiology
- knowledge
- graph
- generation
- vlm-kg
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first multimodal VLM-based framework
  for radiology knowledge graph generation, addressing the limitations of existing
  unimodal approaches by incorporating both radiology reports and images. The method
  uses a pretrained LLM with visual instruction tuning, integrating MedCLIP embeddings
  via a transformer-based projector.
---

# VLM-KG: Multimodal Radiology Knowledge Graph Generation

## Quick Facts
- arXiv ID: 2505.17042
- Source URL: https://arxiv.org/abs/2505.17042
- Reference count: 30
- Multimodal VLM-KG model achieves 54.98 BLEU-1 score on MIMIC-CXR, outperforming previous text-only approaches

## Executive Summary
This paper introduces the first multimodal VLM-based framework for radiology knowledge graph generation, addressing the limitations of existing unimodal approaches by incorporating both radiology reports and images. The method uses a pretrained LLM with visual instruction tuning, integrating MedCLIP embeddings via a transformer-based projector. The resulting VLM-KG model significantly outperforms previous methods, achieving BLEU-1 scores of 54.98 and 44.76 on MIMIC-CXR and IU-Xray datasets respectively, compared to 24.57 for the prior Dygiee++ model. The multimodal approach also generates more accurate and clinically relevant knowledge graphs with fewer hallucinated relations. The study demonstrates the value of combining visual and textual information for structured information extraction in radiology, particularly for long-form reports.

## Method Summary
VLM-KG is a two-stage framework that first fine-tunes a Qwen 1.5-0.5B LLM for instruction following using radiology reports, then jointly trains a transformer-based projector with the LLM to integrate MedCLIP visual embeddings. The projector uses 8 layers with 512→1024 dimensional transformation and learned prefix tokens to extract task-relevant visual features. Training occurs on 230K MIMIC-CXR and 2K IU-Xray reports with triplets generated by Dygiee++, using cross-entropy loss for token-by-token triplet generation. The model processes up to 32K tokens, enabling handling of long-form radiology reports that previous BERT-based models could not accommodate.

## Key Results
- VLM-KG achieves 54.98 BLEU-1 score on MIMIC-CXR dataset, compared to 40.52 for LLM-KG (text-only baseline) and 24.57 for Dygiee++
- On IU-Xray dataset, VLM-KG achieves 44.76 BLEU-1 score, outperforming Dygiee++ (24.57) and LLM-KG (26.42)
- Visual comparison shows VLM-KG generates cleaner knowledge graphs with fewer hallucinated relations compared to Dygiee++

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multimodal visual-textual input reduces hallucinated relations in knowledge graph generation compared to text-only approaches.
- Mechanism: Visual embeddings from MedCLIP provide disambiguating context that constrains the LLM's generation space. When textual descriptions are ambiguous (e.g., "possible consolidation"), image features help resolve uncertainty by providing visual evidence of anatomical relationships, reducing the model's tendency to generate spurious relations.
- Core assumption: Radiology images contain structured visual information that maps meaningfully to entity-relation triplets, and MedCLIP captures this information in its embeddings.
- Evidence anchors:
  - [abstract] "The multimodal approach also generates more accurate and clinically relevant knowledge graphs with fewer hallucinated relations."
  - [PAGE 6, Figure 2] Visual comparison shows Dygiee++ produces "dense but noisy graph with redundant and inaccurate relations" (red triplets) while VLM-KG generates "cleanest and most accurate graph."
  - [corpus] Neighbor paper "RADAR" similarly shows knowledge injection reduces hallucinations, suggesting cross-validation of the approach, though direct corpus evidence on visual disambiguation in KG generation is limited.
- Break condition: If image quality is severely degraded or MedCLIP embeddings fail to capture diagnostically relevant features, visual context may add noise rather than signal.

### Mechanism 2
- Claim: A transformer-based projector with learned prefix tokens effectively translates visual embeddings into the LLM's semantic space for structured output generation.
- Mechanism: The 8-layer transformer projector (512→1024 dimensions) uses multi-head attention with learned prefix tokens to extract task-relevant visual features. The prefix tokens attend over MedCLIP embeddings, learning to select and transform diagnostically salient information rather than attempting full visual-to-language translation via simple linear projection.
- Core assumption: A compact projector can learn the essential mapping without requiring full cross-modal pretraining, given sufficient instruction-tuning data.
- Evidence anchors:
  - [PAGE 3] "The prefix retrieves meaningful information from MedCLIP embedding through the multi-head attention, and it learns to adjust to the LLM-KG."
  - [PAGE 7, Table 2] Freezing the LLM (prefix tuning only) drops BLEU-1 from 54.98 to 10.00, indicating joint training of projector and LLM is critical for this specialized task.
  - [corpus] No direct corpus evidence on prefix-based projectors for medical VLMs; this appears to be a domain-specific adaptation.
- Break condition: If the projector architecture is insufficiently expressive (e.g., single MLP layer) or prefix length is misconfigured, visual information may not integrate properly with language embeddings.

### Mechanism 3
- Claim: Extended context length (32K tokens) enables processing of long-form radiology reports that previous BERT-based models could not handle.
- Mechanism: The Qwen 1.5-0.5B backbone's 32K context window allows the model to maintain coherence across multi-sentence reports with multiple findings, anatomical locations, and relations. This prevents information loss that occurs when reports exceed typical 512-token BERT limits.
- Core assumption: Long-range dependencies in radiology reports contain relation-relevant information that shorter context windows discard.
- Evidence anchors:
  - [PAGE 4] "Qwen 1.5-0.5B model is used... because it supports a context length of 32K tokens, which is much higher than the previous BERT-based models. This enables it to process long-form radiology reports, which was a limitation of previous models."
  - [PAGE 7, Table 4] Increasing generated token length from 200 to 300 raises BLEU-1 from 33.58 to 40.52, demonstrating that longer sequences improve triplet quality.
  - [corpus] No corpus papers directly address context length in radiology KG generation; this is a novel contribution.
- Break condition: If reports exceed even 32K tokens or if computational resources limit practical sequence lengths during inference, long-form reports may still be truncated.

## Foundational Learning

- Concept: **Knowledge Graph Triplets (Entity-Relation-Entity)**
  - Why needed here: The entire model outputs structured triplets like `['opacity', 'located_at', 'lobe']`. Understanding that entities are anatomical/observational spans and relations are `modify`, `located_at`, or `suggestive_of` is essential for interpreting outputs and debugging malformed triplets.
  - Quick check question: Given the triplet `['increased', 'modify', 'opacity']`, which token is the entity being modified and which is the modifier?

- Concept: **Visual Instruction Tuning**
  - Why needed here: This is the core training paradigm—unlike standard fine-tuning, visual instruction tuning teaches the model to follow prompts while conditioning on image embeddings. Understanding this clarifies why both the projector AND LLM must be trained jointly.
  - Quick check question: If you freeze the LLM and only train the projector, what component of the model cannot adapt to the visual modality? (Answer: the LLM's token prediction layers cannot learn to attend to visual embeddings.)

- Concept: **Cross-Entropy Loss for Structured Generation**
  - Why needed here: The model generates triplet strings token-by-token using cross-entropy loss. This means output format consistency depends on training data quality—if triplets in training data are malformed, the model will reproduce those errors.
  - Quick check question: Why might BLEU scores be higher than ROUGE-L scores for the same model, and what does this indicate about generation precision vs. recall?

## Architecture Onboarding

- Component map:
  Input: Radiology Image → MedCLIP (frozen) → 512-dim embeddings → Transformer Projector (8-layer, trainable) → 1024-dim visual embeddings → Concatenated [visual embeddings, token embeddings] → Qwen 1.5-0.5B (trainable) → Output: Knowledge graph triplets (text)

- Critical path:
  1. MedCLIP must receive correctly preprocessed chest X-ray images (same normalization as pretraining)
  2. Projector's `clip_length` and `prefix_length` must be set before training (Table 3 shows 64/64 works best)
  3. Instruction format in Figure 1 must match training data exactly—deviations cause format errors
  4. Post-processing with regex is required to convert string output to valid triplet lists

- Design tradeoffs:
  - **Small LLM (0.5B params)**: Enables deployment in resource-limited clinical settings but may lack deep medical knowledge
  - **Training both projector and LLM**: Superior performance (BLEU-1: 54.98) vs. prefix-tuning only (BLEU-1: 10.00), but requires more compute
  - **Beam search inference**: Better than top-p/top-k sampling for this task, but slower at inference time

- Failure signatures:
  - Malformed triplet output (missing brackets, wrong delimiters): Check instruction format match and post-processing regex
  - High hallucination rate: Verify image preprocessing matches MedCLIP pretraining; check if visual embeddings are being concatenated properly
  - Poor performance on long reports: Confirm 32K context is not being truncated; verify token generation length is set ≥300 (Table 4)

- First 3 experiments:
  1. **Reproduce baseline comparison**: Train LLM-KG (text-only) on MIMIC data and compare BLEU scores against reported 40.52 to validate pipeline
  2. **Ablate projector architecture**: Replace 8-layer transformer with simple MLP and measure performance drop to validate projector design choice
  3. **Test on out-of-distribution reports**: Evaluate on IU-Xray (smaller dataset) to verify cross-dataset generalization and identify domain shift issues

## Open Questions the Paper Calls Out

- **Generalization to other modalities**: The framework is currently limited to chest X-ray datasets (MIMIC-CXR and IU-Xray), with no validation on CT, MRI, or other radiology modalities.
- **Training data quality limitations**: The model is trained on noisy "silver standard" labels generated by Dygiee++, which may inherit systematic errors from the baseline model.
- **NLG metric limitations**: The paper uses BLEU/ROUGE scores for evaluation rather than strict triplet precision/recall, raising questions about whether high text similarity translates to accurate structured extraction.

## Limitations

- Limited dataset diversity with validation only on chest X-ray datasets (MIMIC-CXR and IU-Xray), raising concerns about generalization to other imaging modalities
- Underspecified projector architecture details beyond basic dimensions, making exact reproduction challenging
- Heavy reliance on MedCLIP's frozen embeddings without validation of whether they capture domain-specific medical imaging features

## Confidence

**High Confidence**: The multimodal approach outperforms text-only baselines (54.98 vs 40.52 BLEU-1 on MIMIC), and the ablation study clearly demonstrates the importance of joint projector-LLM training.

**Medium Confidence**: Claims about reduced hallucinations and improved accuracy compared to Dygiee++ are supported by qualitative examples but lack quantitative hallucination metrics.

**Low Confidence**: The paper makes strong claims about long-form report processing capabilities without providing ablation studies on context window effects.

## Next Checks

1. **Cross-dataset robustness validation**: Test VLM-KG on an independent radiology dataset (e.g., Open-i or RSNA datasets) with different report styles and image qualities to quantify generalization beyond MIMIC-CXR and IU-Xray.

2. **Quantitative hallucination analysis**: Implement automated hallucination detection by comparing generated relations against reference knowledge graphs and measuring precision-recall tradeoffs.

3. **Projector architecture ablation study**: Systematically vary projector depth (1-4-8 layers), attention head counts, and hidden dimensions while holding other factors constant to isolate architectural contributions.