---
ver: rpa2
title: Predicting life satisfaction using machine learning and explainable AI
arxiv_id: '2510.16547'
source_url: https://arxiv.org/abs/2510.16547
tags:
- life
- satisfaction
- learning
- data
- machine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study demonstrates that machine learning can accurately predict
  life satisfaction from survey data, achieving 93.80% accuracy and 73.00% macro F1-score.
  Using an ensemble of decision-tree and boosting models on a Danish government dataset
  of 19,000 people, the researchers identified 27 key survey questions as the most
  significant determinants of life satisfaction.
---

# Predicting life satisfaction using machine learning and explainable AI

## Quick Facts
- **arXiv ID:** 2510.16547
- **Source URL:** https://arxiv.org/abs/2510.16547
- **Reference count:** 40
- **Primary result:** 93.80% accuracy and 73.00% macro F1-score predicting life satisfaction from survey data

## Executive Summary
This study demonstrates that machine learning can accurately predict life satisfaction from survey data, achieving 93.80% accuracy and 73.00% macro F1-score. Using an ensemble of decision-tree and boosting models on a Danish government dataset of 19,000 people, the researchers identified 27 key survey questions as the most significant determinants of life satisfaction. Explainable AI methods were employed to make the model decisions interpretable. The approach outperformed traditional analog survey methods and enables a simple, reproducible assessment tool. The best-performing model was deployed in an interactive app. The results show machine learning can provide reliable, scalable predictions of subjective well-being from survey data, with important implications for mental health research and policy.

## Method Summary
The researchers used a Danish government survey (SHILD) with 243 variables from ~19,000 respondents. They applied iterative imputation, ordinal encoding, and outlier trimming, then used Recursive Feature Elimination with Cross-Validation (RFECV) to select 27 key features. The data was resampled using SMOTE followed by random undersampling to handle class imbalance. An ensemble of Random Forest, Gradient Boosting, and LightGBM models was trained and combined. The study also explored converting survey responses to natural language and using BioBERT for prediction. Explainable AI techniques were applied to visualize model decisions.

## Key Results
- 93.80% accuracy and 73.00% macro F1-score achieved on life satisfaction prediction
- 27 key survey questions identified as most significant determinants of life satisfaction
- Ensemble approach outperformed individual models and traditional analog methods
- Best-performing model deployed in interactive web application

## Why This Works (Mechanism)

### Mechanism 1
- Claim: High predictive accuracy is likely driven by aggressive feature distillation rather than model complexity alone.
- Mechanism: The study uses Recursive Feature Elimination with Cross-Validation (RFECV) to reduce 243 survey variables to 27 high-signal determinants. By removing noise and multicollinearity, the ensemble model focuses exclusively on the strongest correlates of life satisfaction, such as health status and emotional stability.
- Core assumption: The removed features contain negligible predictive information relative to the retained 27 features.
- Evidence anchors:
  - [abstract] "Using feature learning techniques, 27 significant questions... were extracted."
  - [section 3.4.7] "RFECV curve... represents the performance... as a function of the number of selected features."
  - [corpus] Weak direct evidence in the provided corpus for RFECV specifically in life satisfaction; related works focus on regression or deep learning.
- Break condition: If the 27 selected features fail to generalize to non-Danish populations (cultural drift), the distillation mechanism may overfit to the specific dataset's variance.

### Mechanism 2
- Claim: Reliable classification of the minority "discontent" class is contingent upon a dual resampling strategy.
- Mechanism: The dataset is imbalanced (more "content" respondents). The authors apply SMOTE (Synthetic Minority Over-sampling Technique) followed by undersampling. This forces the classifiers to learn decision boundaries for the minority class without simply memorizing the majority class, directly improving the Macro F1-score.
- Core assumption: Synthetic samples generated by SMOTE accurately approximate the true distribution of the minority "discontent" class.
- Evidence anchors:
  - [abstract] "...ablation studies were also conducted to understand the impact of data resampling..."
  - [section 4.5.1] "The ablation experiments reveal... Undersampling solely shows a marked decrease... The combined approach... underscores the efficacy of a dual balancing strategy."
  - [corpus] No direct corpus support for the specific "dual balancing" mechanism in this domain; general ML literature supports resampling for imbalance.
- Break condition: If SMOTE generates synthetic outliers in sparse regions of the feature space, the model may hallucinate decision boundaries that do not exist in reality.

### Mechanism 3
- Claim: Life satisfaction prediction appears semantically aligned with biomedical literature.
- Mechanism: When tabular data is converted to natural language, domain-specific LLMs (specifically BioBERT) outperform general-purpose and clinical models. This suggests the language used to describe well-being shares structural or semantic properties with biomedical text, allowing the model to leverage pre-trained biomedical knowledge.
- Core assumption: The mapping of categorical survey responses to natural language sentences preserves the semantic relationships necessary for the LLM to infer satisfaction.
- Evidence anchors:
  - [abstract] "It was found that life satisfaction prediction is more closely related to the biomedical domain than the clinical domain."
  - [section 5.2] "BioBERT achieves the highest macro-averaged F1-score... attributed to the breadth and diversity of its training corpus."
  - [corpus] No specific corpus evidence regarding BioBERT's application to life satisfaction; evidence is internal to the paper's experiments.
- Break condition: If the "biomedical" alignment is merely an artifact of the specific Danish translation/mapping style, the mechanism may fail in other linguistic contexts.

## Foundational Learning

- **Concept: Recursive Feature Elimination (RFE)**
  - Why needed here: The study relies on reducing a massive survey to a 27-question tool. Without RFE, the model would suffer from the curse of dimensionality, and the "simple, reproducible" goal would fail.
  - Quick check question: Can you explain why RFECV is preferred over Principal Component Analysis (PCA) when the goal is *interpretability* of specific questions?

- **Concept: Imbalanced Classification Metrics (Macro F1 vs. Accuracy)**
  - Why needed here: The paper reports 93.8% accuracy but emphasizes the 73.0% Macro F1-score. A naive model predicting "content" for everyone would have high accuracy but zero utility.
  - Quick check question: Why would a model with 93% accuracy be considered a failure if it misses the majority of "discontent" individuals in a mental health screening context?

- **Concept: Ensemble Learning (Bagging vs. Boosting)**
  - Why needed here: The final architecture combines Random Forest (bagging) with LightGBM and Gradient Boosting (boosting). Understanding how these reduce variance vs. bias is key to understanding the final performance.
  - Quick check question: How does the "dual balancing" strategy specifically aid the Boosting algorithms, which are typically sensitive to noise in the minority class?

## Architecture Onboarding

- **Component map:**
  1. **Input:** Danish SHILD Survey (243 variables).
  2. **Preprocessing:** Iterative Imputation -> Ordinal Encoding -> Outlier Trimming.
  3. **Feature Selection:** RFECV (Random Forest estimator) -> 27 Features.
  4. **Resampling:** SMOTE (to 40%) -> Random Undersampling (to 100% of minority).
  5. **Model Core:** Ensemble (Voting/Stacking of RF + GB + LGB).
  6. **Text Alt-Path:** Mapping to Sentences -> BioBERT.
  7. **Explainability:** Threshold-based XAI (Reward/Penalty visualization).

- **Critical path:** The **RFECV** step is the structural bottleneck. If this is removed or misconfigured, the 27-question "LifeWell survey" becomes invalid, and the model performance degrades to the levels seen in the ablation study (Table 8).

- **Design tradeoffs:**
  - **Interpretability vs. Nuance:** The 27-feature limit drastically improves interpretability but drops accuracy slightly compared to using all features (Table 8: Ensemble 93.2% -> 93.6% is comparable, but individual models vary).
  - **Tabular vs. LLM:** Tabular ensemble (73.00 F1) is slightly outperformed by BioBERT (73.21 F1), but the Tabular model is computationally cheaper and easier to deploy in a simple app.
  - **SVC Exclusion:** SVC failed entirely (37.5% F1) due to the heterogeneous nature of the data, serving as a "negative design" constraint—do not use kernel methods on this specific feature set without heavy scaling/normalization.

- **Failure signatures:**
  - **High Accuracy, Low F1:** Indicates the resampling strategy failed or was omitted; model is predicting the majority class only.
  - **SVC Collapse:** If using SVM-based models on the raw categorical data without proper scaling/kernel tuning.
  - **BioBERT Underperformance:** If the text mapping generates ambiguous or long sentences (exceeding 512 tokens), the LLM path will degrade.

- **First 3 experiments:**
  1. **Reproduce the RFECV:** Train a Random Forest on the full 243 features vs. the selected 27 to validate the claim that the performance drop is negligible.
  2. **Ablation on Resampling:** Run the Ensemble model with *only* SMOTE vs. *only* Undersampling to replicate the trade-off curve shown in Table 7.
  3. **XAI Validation:** Take a specific prediction (Case 1 or 2) and manually calculate the "rewards/penalties" to verify the XAI logic matches the model's probability output.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the 27-item questionnaire and trained models maintain robustness when applied to populations with diverse cultural and socioeconomic contexts outside of Denmark?
- **Basis:** [explicit] Section 6 states the intent to "validate and possibly enhance the models’ robustness by incorporating data from a diverse array of countries" to ensure a universally applicable framework.
- **Why unresolved:** The study relies exclusively on the SHILD dataset from Denmark, limiting the generalizability of the findings to other demographics.
- **What evidence would resolve it:** Deploying the model on cross-cultural datasets (e.g., global well-being surveys) and evaluating performance degradation or the need for retraining.

### Open Question 2
- **Question:** Can deep learning architectures, such as recurrent neural networks (RNNs), capture temporal dependencies to predict life satisfaction changes over time?
- **Basis:** [explicit] The conclusion notes that static models do not account for dynamic changes, proposing the evaluation of "deeper neural network architectures" for "longitudinal life satisfaction data."
- **Why unresolved:** The current models utilize cross-sectional data, failing to capture "inherent fluctuations and evolving circumstances" over time.
- **What evidence would resolve it:** Comparing the performance of temporal deep learning models against the current static ensemble on a longitudinal dataset.

### Open Question 3
- **Question:** Do the identified primary determinants (e.g., health, depression) have a causal relationship with life satisfaction, or are they merely correlational?
- **Basis:** [inferred] Section 5.4 highlights the limitation of relying on static, cross-sectional data, which prevents the model from encapsulating evolving circumstances or establishing causality.
- **Why unresolved:** Machine learning feature importance identifies correlation strength, not causal direction.
- **What evidence would resolve it:** Applying causal inference frameworks or conducting interventional studies to observe changes in life satisfaction following targeted changes to specific determinants.

## Limitations

- The 27-question LifeWell survey may not generalize beyond the Danish cultural context due to potential cultural drift in survey response patterns.
- SMOTE-generated synthetic samples for the minority class rely on the assumption that interpolated feature vectors accurately represent true "discontent" cases.
- The biomedical alignment of BioBERT could be an artifact of the specific Danish translation rather than a universal semantic property of life satisfaction language.

## Confidence

- **High Confidence:** The core methodology (RFECV feature selection + ensemble modeling) is well-specified and reproducible with the provided Dryad dataset.
- **Medium Confidence:** The 93.80% accuracy and 73.00% macro F1-score are credible, but exact reproduction requires clarifying the binarization threshold and ensemble aggregation method.
- **Low Confidence:** The claim that life satisfaction prediction is "more closely related to the biomedical domain" lacks external validation beyond the internal LLM experiments.

## Next Checks

1. **Cross-cultural validation:** Apply the 27-question LifeWell survey to a non-Danish dataset to test generalization.
2. **SMOTE distribution analysis:** Visualize the synthetic samples generated by SMOTE to verify they fall within realistic feature distributions.
3. **Linguistic robustness test:** Translate the survey responses to another language and rerun the BioBERT experiment to confirm the biomedical alignment persists.