---
ver: rpa2
title: AI Recommendation Systems for Lane-Changing Using Adherence-Aware Reinforcement
  Learning
arxiv_id: '2504.20187'
source_url: https://arxiv.org/abs/2504.20187
tags:
- human
- learning
- lane
- vehicle
- vehicles
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of lane-changing decision-making
  in semi-autonomous driving environments where human drivers may not fully comply
  with AI recommendations. The authors propose an adherence-aware deep Q-network (DQN)
  that models the probability of human driver compliance when following lane-changing
  suggestions.
---

# AI Recommendation Systems for Lane-Changing Using Adherence-Aware Reinforcement Learning

## Quick Facts
- **arXiv ID**: 2504.20187
- **Source URL**: https://arxiv.org/abs/2504.20187
- **Reference count**: 22
- **Primary result**: Adherence-aware DQN outperforms regular DQN and baseline human driving policies in CARLA, reducing travel time by 10.76% and increasing cumulative rewards by 31.33%

## Executive Summary
This paper addresses lane-changing decision-making in semi-autonomous driving where human drivers may not fully comply with AI recommendations. The authors propose an adherence-aware deep Q-network (DQN) that models the probability of human driver compliance when following lane-changing suggestions. The approach incorporates an adherence estimator within the Q-learning framework to account for situations where drivers ignore recommendations. The method is implemented and tested in the CARLA driving simulator using a 4-lane freeway scenario with merging traffic. Results show that accounting for partial compliance leads to more effective lane-changing recommendations that better align with human driver behavior.

## Method Summary
The authors formulate lane-changing as a Markov Decision Process with unknown human adherence levels. They develop an adherence-aware DQN that estimates the probability θ of human compliance and incorporates this into the Q-value update. The system uses a five-vehicle state representation (ego, left/right leader/follower) and trains online in CARLA Town 06 using experience replay. The adherence estimator tracks compliance indicators to update θ̂ recursively, while the Q-network outputs values for three actions (left, right, keep lane) weighted by the estimated adherence probability.

## Key Results
- Adherence-aware DQN reduces travel time by 10.76% compared to baseline human driving policy
- The method achieves 31.33% higher cumulative rewards compared to regular RL without adherence modeling
- System maintains safety performance while improving efficiency through better alignment with human compliance patterns

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Incorporating adherence probability directly into the Q-value update allows the policy to optimize for the distribution of actions that will actually be executed, rather than assuming perfect compliance.
- **Mechanism**: The adherence-aware Bellman update (Eq. 12) computes the target Q-value as a weighted combination: θ̂ weighting for the recommended action's future value plus (1-θ̂) weighting for the baseline action's future value. This prevents the policy from over-optimizing for recommendations that humans frequently ignore.
- **Core assumption**: Assumption 2 states that human drivers share the same long-term objective as the recommendation system, ensuring baseline actions remain goal-directed even when non-compliant.
- **Evidence anchors**:
  - [abstract]: "adherence-aware deep Q network, which takes into account the partial compliance of human drivers with the recommended actions"
  - [section III.A, Eq. 12]: Q(x,u) ← Q(x,u) + α · {θ̂ · [r + λ·max Q(x',u'r)] + (1-θ̂) · [r + λ·Q(x',u'b)] - Q(x,u)}
  - [corpus]: TGLD paper notes that overlooking human trust levels "limit[s] their ability to accurately predict human" behavior—partial adherence modeling addresses this gap.

### Mechanism 2
- **Claim**: An unbiased recursive estimator enables online learning of the unknown human adherence level from observed compliance events, allowing the system to adapt without prior behavioral modeling.
- **Mechanism**: The estimator tracks compliance indicators Yt ∈ {0,1} and updates θ̂ via θt+1 = (θt·n + I(Yt=1))/(n+1). By Eq. 11, the expected value converges to the true θ as sample size increases (citing Lehmann & Casella theory of point estimation).
- **Core assumption**: Assumption 1 asserts stationary adherence level θ—implying compliance probability does not shift dramatically within an episode.
- **Evidence anchors**:
  - [section III.A]: "An unbiased estimator is constructed to estimate the unknown θ, which accounts for the action of the human driver"
  - [abstract]: "incorporates an adherence estimator within the Q-learning framework to account for situations where drivers ignore recommendations"
  - [corpus]: Weak direct evidence—corpus papers address trust/dynamics but not this specific estimation approach.

### Mechanism 3
- **Claim**: Encoding the traffic state via five critical surrounding vehicles provides decision-relevant information while maintaining a tractable state dimension for neural network learning.
- **Mechanism**: The state vector (Eq. 3) captures only V = {Vego,l, Vleft,l, Vleft,f, Vright,l, Vright,f}. Boundary conditions use phantom vehicles with large distance offsets and matched speeds when adjacent lanes don't exist.
- **Core assumption**: Immediate leader/follower positions in current and adjacent lanes capture the dominant factors for lane-changing decisions; vehicles beyond this neighborhood have negligible impact.
- **Evidence anchors**:
  - [section II, Eq. 1-3]: State definitions for ego xe(t) and surrounding vehicles xV(t)
  - [section IV.B]: Boundary handling—"increase the ego car's position by a large distance to represent the position of surrounding vehicles that could have been on the left and right lane"
  - [corpus]: Neighboring papers (ramp planner, TGLD) use similar neighbor-focused encodings without systematic ablation.

## Foundational Learning

- **Concept: Markov Decision Process (MDP) Formulation**
  - **Why needed here**: The lane-changing problem is explicitly framed as an MDP with states X, actions U, rewards r, and transitions p. The Q-function derivation assumes this structure.
  - **Quick check question**: Given Eq. 5, what are the four penalty terms in the reward function and what behavior does each encourage/discourage?

- **Concept: Q-Learning and the Bellman Equation**
  - **Why needed here**: The adherence-aware update (Eq. 12) modifies the standard Bellman backup. Without understanding standard Q-learning, the weighting mechanism is opaque.
  - **Quick check question**: How does Eq. 12 differ from the standard Q-learning update? What happens to the update when θ̂ = 1.0?

- **Concept: Deep Q-Network (DQN) Architecture**
  - **Why needed here**: Continuous state variables (position, velocity) make tabular Q-learning infeasible. DQN approximates Q(s,a) via neural network trained on loss minimization (Eq. 13-14).
  - **Quick check question**: Why is the target value yt computed using the adherence-weighted formula rather than a single max operation? What role does the target network play in stability?

## Architecture Onboarding

- **Component map**: State Extractor -> Q-Network -> Action Selector -> Environment -> Reward Calculator -> Adherence Estimator -> Q-Network
- **Critical path**: CARLA state → Q-network forward pass → ε-greedy action selection → human compliance decision (θ probability) → environment step → reward computation → adherence estimator update → Q-network backward pass with adherence-weighted target
- **Design tradeoffs**:
  - **Sparse vs. dense state**: Five-vehicle encoding limits computation but may miss distant threats
  - **Stationary vs. dynamic θ**: Current model assumes constant adherence; dynamic adherence would require recurrent estimation
  - **Reward weight tuning**: α3 (safety) vs. α1 (speed) determines risk tolerance
  - **Online vs. offline training**: Paper trains online in CARLA; sim-to-real transfer not addressed
- **Failure signatures**:
  - **Loss divergence**: Learning rate too high or reward magnitude mismatch
  - **θ̂ stuck at extreme**: Insufficient exploration or compliance indicator not updating
  - **Unsafe lane changes**: α3 penalty too low relative to efficiency rewards
  - **Oscillating recommendations**: ε decay too slow or reward function inconsistent
- **First 3 experiments**:
  1. **Adherence estimation validation**: Inject known ground-truth θ (e.g., 0.5, 0.7, 0.3) and plot θ̂ convergence over 3000 episodes. Verify convergence within tolerance.
  2. **Ablation on adherence weighting**: Compare adherence-aware DQN vs. regular DQN at identical compliance levels. Measure gap in cumulative reward and travel time.
  3. **Robustness to dynamic adherence**: Introduce mid-episode θ shift (0.8→0.3 at step 150). Measure performance degradation and recovery time.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How can the framework be adapted to handle time-varying, dynamic compliance levels ($\theta$) rather than a stationary probability?
- **Basis in paper**: [explicit] The authors state in the concluding remarks, "Potential extensions of this work include modeling the problem with a dynamic compliance level throughout driving."
- **Why unresolved**: The current algorithm assumes a stationary adherence level, utilizing a running average estimator (Eq. 10) that converges to a single scalar value. This cannot capture fluctuations in human trust or attention over time.
- **What evidence would resolve it**: A modified estimation mechanism (e.g., a recurrent network or sliding window) demonstrating convergence to time-varying adherence states in a simulated environment where human compliance fluctuates.

### Open Question 2
- **Question**: Can the adherence-aware policy maintain robustness when the human driver's baseline policy deviates from the assumed heuristic?
- **Basis in paper**: [explicit] The conclusion suggests extending the work to "different types of baseline actions."
- **Why unresolved**: The current implementation assumes the baseline action $u_b$ is "explicitly known and arbitrarily defined" (Section III-A). It is untested whether the estimator remains accurate if the human driver's underlying behavior is stochastic or irrational rather than rule-based.
- **What evidence would resolve it**: Simulation results evaluating the trained DQN against a library of heterogeneous baseline policies (e.g., aggressive vs. conservative) without retraining the core adherence estimator.

### Open Question 3
- **Question**: Does the adherence-aware DQN scale effectively to complex, dense urban environments using advanced network architectures?
- **Basis in paper**: [explicit] The authors specify that future research should involve "training the DQN in more complex scenarios with a more advanced network architecture."
- **Why unresolved**: The current study is limited to a simplified 4-lane freeway scenario using a network with a single hidden layer (128 neurons). It is unclear if this architecture suffers from stability or convergence issues in higher-dimensional state spaces.
- **What evidence would resolve it**: Benchmarking the approach in CARLA's dense urban maps (e.g., Town 01 or Town 04) utilizing deeper architectures (e.g., Dueling DQN or LSTM layers) to show maintained stability and reward accumulation.

## Limitations

- Reward function hyperparameters (α₁-α₄ weights, velocity thresholds) are not specified, making exact reproduction difficult
- State representation details for handling missing surrounding vehicles lack precise specifications
- Baseline policy is described generically without detailing its lane-changing logic

## Confidence

- **High Confidence**: The adherence-aware Q-learning mechanism is theoretically sound and the implementation approach is clearly specified
- **Medium Confidence**: The unbiased estimator for adherence has solid theoretical foundation but depends on the stationary assumption
- **Low Confidence**: The five-vehicle state encoding is justified by computational tractability but lacks systematic validation

## Next Checks

1. Perform ablation studies varying the five-vehicle state encoding to determine if more comprehensive state representations improve performance
2. Test the system with dynamic adherence levels that change within episodes to evaluate robustness beyond the stationary assumption
3. Conduct transfer tests where the trained policy is evaluated on different traffic densities and driver types to assess generalization