---
ver: rpa2
title: 'Adapting Vision-Language Models Without Labels: A Comprehensive Survey'
arxiv_id: '2508.05547'
source_url: https://arxiv.org/abs/2508.05547
tags:
- adaptation
- arxiv
- data
- test-time
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This survey comprehensively categorizes and analyzes unsupervised
  adaptation methods for Vision-Language Models (VLMs), addressing the critical challenge
  of adapting pre-trained VLMs to downstream tasks without labeled data. The paper
  introduces a novel taxonomy based on unlabeled visual data availability, dividing
  methods into four paradigms: Data-Free Transfer (no data), Unsupervised Domain Transfer
  (abundant data), Episodic Test-Time Adaptation (batch data), and Online Test-Time
  Adaptation (streaming data).'
---

# Adapting Vision-Language Models Without Labels: A Comprehensive Survey

## Quick Facts
- **arXiv ID:** 2508.05547
- **Source URL:** https://arxiv.org/abs/2508.05547
- **Reference count:** 40
- **Primary result:** Introduces novel taxonomy categorizing unsupervised VLM adaptation into four paradigms based on unlabeled data availability: Data-Free Transfer (no data), Unsupervised Domain Transfer (abundant data), Episodic Test-Time Adaptation (batch data), and Online Test-Time Adaptation (streaming data)

## Executive Summary
This survey comprehensively categorizes and analyzes unsupervised adaptation methods for Vision-Language Models (VLMs), addressing the critical challenge of adapting pre-trained VLMs to downstream tasks without labeled data. The paper introduces a novel taxonomy based on unlabeled visual data availability, dividing methods into four paradigms: Data-Free Transfer (no data), Unsupervised Domain Transfer (abundant data), Episodic Test-Time Adaptation (batch data), and Online Test-Time Adaptation (streaming data). Within this framework, the survey systematically reviews core methodologies, adaptation strategies, and representative benchmarks across diverse applications including object classification, semantic segmentation, and out-of-distribution detection. By providing a structured overview of the field and identifying key research challenges and future directions, this work serves as a valuable resource for practitioners and researchers, facilitating fair comparisons and guiding the selection of suitable techniques for various unsupervised VLM adaptation scenarios.

## Method Summary
The survey builds on pre-trained Vision-Language Models like CLIP, which learn joint image-text embeddings through contrastive pre-training. The core adaptation framework uses cosine similarity between image embeddings and text embeddings of class names with prompt templates. Methods are categorized into four paradigms based on data availability: Data-Free Transfer uses LLM-generated semantic enrichment, Unsupervised Domain Transfer employs pseudo-labeling and distribution alignment, Episodic Test-Time Adaptation optimizes prompts on individual test batches via entropy minimization, and Online Test-Time Adaptation maintains dynamic memory banks of reliable feature-label pairs for streaming scenarios. Implementation requires pre-trained VLM (typically CLIP), unlabeled target images, class names, and paradigm-specific adaptation modules including learnable prompts, pseudo-label generators, and memory mechanisms.

## Key Results
- Introduces systematic taxonomy dividing unsupervised VLM adaptation into four paradigms based on unlabeled data availability
- Surveys adaptation strategies including text augmentation, pseudo-labeling, entropy minimization, and memory mechanisms across 40+ references
- Identifies key research challenges including hallucination risks in data-free transfer and feature corruption in online adaptation
- Provides comprehensive benchmark coverage across classification, segmentation, retrieval, and OOD detection tasks
- Highlights trade-offs between adaptation paradigms regarding compute efficiency, performance gains, and robustness to distribution shifts

## Why This Works (Mechanism)

### Mechanism 1: Textual Semantic Expansion (Data-Free Transfer)
Enriching class prompts with LLM-generated descriptors or hierarchical sub-classes can improve zero-shot classification if the generated text aligns with visual features present in the pre-training data. This approach replaces sparse class names with dense descriptions, leveraging the VLM's existing joint embedding space to bridge the modality gap by providing the text encoder with context similar to the internet-scale captions it was trained on. The core assumption is that the VLM has already encoded the specific visual attributes described by the LLM during its pre-training phase. This mechanism may introduce noise rather than signal if the downstream task involves domain-specific jargon or visual concepts that the LLM hallucinates about or the VLM has never "seen."

### Mechanism 2: Adaptive Confidence Tuning (Episodic Test-Time Adaptation)
Optimizing learnable prompts or model parameters to minimize the entropy of predictions on a single batch of test data can adapt the model to distribution shifts provided the initial predictions are directionally correct. Methods like TPT generate augmented views of a test image and optimize a prompt so that the model outputs a confident (low-entropy) prediction averaged across these views. It assumes that high-confidence predictions are more likely to be correct. This approach fails if the distribution shift is so severe that the model's initial predictions are uniformly wrong; minimizing entropy will merely make the model confidently incorrect.

### Mechanism 3: Accumulated Knowledge Retrieval (Online Test-Time Adaptation)
Storing reliable feature-label pairs in a dynamic memory bank allows the model to improve over time in streaming scenarios if the data stream exhibits stable class statistics and the memory update policy effectively filters noise. Methods like TDA cache keys (features) and values (pseudo-labels) from previous test samples. New predictions are refined by retrieving relevant entries from this cache, acting as a non-parametric adapter that "remembers" the test distribution. This mechanism fails in "open-world" scenarios where new classes appear continuously without a "reject" option, potentially polluting the memory bank with out-of-distribution features labeled as known classes.

## Foundational Learning

- **Concept: Contrastive Pre-training (CLIP)**
  - Why needed: This entire survey builds on the architecture where images and text are mapped to a shared latent space. Without understanding that CLIP learns by maximizing cosine similarity between matching image-text pairs, the adaptation strategies (which manipulate this similarity) won't make sense.
  - Quick check: If I embed an image of a dog and the text "a photo of a dog," will their vectors be orthogonal, parallel, or unrelated?

- **Concept: Prompt Tuning vs. Fine-Tuning**
  - Why needed: Most adaptation methods (DFT, UDT, TTA) rely on optimizing "soft prompts" (learnable continuous vectors) rather than weights. Distinguishing these discrete text inputs from learnable parameters is crucial for understanding what is being updated during adaptation.
  - Quick check: In "Prompt Tuning," do we update the weights of the text encoder, or the embedding vectors fed into the encoder?

- **Concept: Distribution Shift**
  - Why needed: The survey categorizes methods based on data availability, driven by the core problem that pre-trained models fail when $P_{test} \neq P_{train}$. Understanding covariate shift vs. label shift is necessary to select the right paradigm.
  - Quick check: If a model trained on photos of cars encounters sketches of cars, what type of shift is this, and which paradigm (Data-Free vs. Domain Transfer) might handle it better?

## Architecture Onboarding

- **Component map:**
  Image Encoder $f$ + Text Encoder $g$ -> Joint Embedding Space -> Cosine Similarity -> Classification/Adaptation Module

- **Critical path:**
  1. Input: Image $x$ + Class Names $C$
  2. Forward Pass: Generate image features $f(x)$ and text features $g(C + \text{Prompt})$
  3. Adaptation Step (e.g., TTA): Compute similarity $cos(f(x), g(...))$, calculate Entropy Loss, backprop to update only the Prompt/Adapter parameters (Backbone is frozen)
  4. Inference: Compute final similarity scores

- **Design tradeoffs:**
  - Compute vs. Performance: TTA (Episodic) requires backpropagation at inference time (slow) but handles shifts well. Data-Free Transfer (DFT) is inference-efficient but relies on static LLM knowledge
  - Stability vs. Plasticity: Online TTA with memory mechanisms is plastic (learns new data) but risks feature corruption (instability) over long streams

- **Failure signatures:**
  - Mode Collapse: During Entropy Minimization, the model collapses to predicting a single class for all inputs to minimize loss perfectly
  - Feature Corruption: In Online TTA, accumulating noisy pseudo-labels causes the model's accuracy to degrade below the zero-shot baseline over time
  - Hallucination: In DFT, the LLM generates descriptors that are semantically valid but visually absent in the VLM's training data, causing misalignment

- **First 3 experiments:**
  1. Baseline Validation (Zero-Shot): Run the frozen VLM on ImageNet and ImageNet-V2. Establish the performance drop due to distribution shift
  2. Paradigm Comparison (Data-Free vs. TTA): Implement CuPL (Data-Free) vs. TPT (Episodic TTA) on ImageNet-V2. Compare the trade-off between inference speed (TPT is slower due to optimization) and accuracy gains
  3. Robustness Check (Online TTA): Simulate a stream of data with a corrupted batch. Implement a simple cache-based OTTA method and observe if/when accuracy drops below the zero-shot baseline (testing for feature corruption)

## Open Questions the Paper Calls Out
None

## Limitations
- The survey provides a comprehensive taxonomy but does not specify exact hyperparameters for individual adaptation methods, which are critical for reproducible results
- Most cited works focus on few-shot or supervised adaptation rather than truly unsupervised settings, limiting direct validation of the proposed mechanisms
- The effectiveness of entropy minimization and pseudo-labeling strategies depends heavily on the initial quality of zero-shot predictions, which varies significantly across datasets and distribution shifts

## Confidence

- **High confidence:** The taxonomy framework dividing methods into four paradigms based on unlabeled data availability is logically sound and well-supported by the literature
- **Medium confidence:** The proposed mechanisms (textual semantic expansion, adaptive confidence tuning, accumulated knowledge retrieval) are theoretically plausible but require empirical validation with specific hyperparameter settings
- **Low confidence:** Claims about hallucination risks in data-free transfer and feature corruption in online TTA are supported by general ML literature but lack specific evidence from the surveyed VLM adaptation works

## Next Checks

1. **Baseline validation experiment:** Implement zero-shot CLIP inference on ImageNet and ImageNet-V2 to establish the distribution shift performance drop (0.8-3.3% accuracy decrease reported in Section VIII.A)

2. **Paradigm comparison experiment:** Implement CuPL (data-free) versus TPT (episodic TTA) on ImageNet-V2 to quantify the trade-off between inference speed and accuracy gains

3. **Robustness stress test:** Simulate a corrupted batch in an online TTA streaming scenario to observe when/where accuracy drops below the zero-shot baseline, testing for feature corruption as noted in Section IX.I