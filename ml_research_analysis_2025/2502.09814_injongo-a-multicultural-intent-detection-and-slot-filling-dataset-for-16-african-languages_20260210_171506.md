---
ver: rpa2
title: 'INJONGO: A Multicultural Intent Detection and Slot-filling Dataset for 16
  African Languages'
arxiv_id: '2502.09814'
source_url: https://arxiv.org/abs/2502.09814
tags:
- intent
- languages
- language
- dataset
- slot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces INJONGO, a multicultural benchmark dataset
  for intent detection and slot-filling tasks covering 16 African languages and English.
  The dataset was constructed by native speakers generating culturally relevant utterances
  across five domains, followed by detailed slot annotation.
---

# INJONGO: A Multicultural Intent Detection and Slot-filling Dataset for 16 African Languages

## Quick Facts
- arXiv ID: 2502.09814
- Source URL: https://arxiv.org/abs/2502.09814
- Reference count: 40
- Primary result: AfroXLMR-76L achieves 93.7% accuracy and 85.6 F1 for intent detection and slot-filling across 16 African languages

## Executive Summary
This paper introduces INJONGO, a multicultural benchmark dataset for intent detection and slot-filling tasks covering 16 African languages and English. The dataset was constructed by native speakers generating culturally relevant utterances across five domains, followed by detailed slot annotation. Experiments show that fine-tuned multilingual models like AfroXLMR-76L achieve strong performance (93.7% accuracy, 85.6 F1), while large language models struggle, especially on slot-filling (26 F1 average). Cross-lingual transfer from the multicultural INJONGO dataset outperforms Western-centric data, particularly in low-resource settings, highlighting the value of culturally adapted data.

## Method Summary
The INJONGO dataset consists of 3,200 utterances per African language and 1,779 for English, covering five domains (banking, travel, home, utility, kitchen & dining). Native speakers created utterances and performed slot annotation. The data is split 70/10/20 for train/dev/test per language. Models are fine-tuned using AdamW with 20 epochs, early stopping (patience=5), and 10% warmup. Learning rates vary by model type (encoder-only: 1e-5/3e-5, encoder-decoder: 5e-5/1e-4, NLLB-LLM2Vec: 1e-4/3e-4). Results are averaged over 5 runs with different seeds.

## Key Results
- AfroXLMR-76L achieves 93.7% accuracy and 85.6 F1 for intent detection and slot-filling
- Cross-lingual transfer from INJONGO outperforms Western-centric data, especially for low-resource languages
- Large language models struggle with slot-filling (26 F1 average), highlighting the advantage of fine-tuned encoders
- Multilingual training provides +4.5% improvement over English-only fine-tuning

## Why This Works (Mechanism)
The dataset's cultural relevance and native speaker curation enable models to learn language-specific intent and slot patterns. Multilingual training leverages shared linguistic structures across African languages, while the diverse domain coverage ensures broad applicability. The BIO tagging format for slots enables precise entity extraction, and the stratified splits maintain balanced representation across intents.

## Foundational Learning
- **BIO tagging**: Used for slot-filling to mark beginning, inside, and outside of entities. Why needed: Enables structured entity extraction. Quick check: Verify 23 merged entity types from Table 9.
- **Cross-lingual transfer**: Training on multiple languages improves performance on low-resource languages. Why needed: Addresses data scarcity for many African languages. Quick check: Compare performance of covered vs. uncovered languages in Table 11.
- **Multilingual fine-tuning**: Adapting multilingual models to specific tasks. Why needed: Leverages pre-trained knowledge across languages. Quick check: Confirm learning rate schedules and batch sizes.

## Architecture Onboarding
**Component map**: Data collection -> Tokenization -> Model fine-tuning -> Evaluation
**Critical path**: Dataset preparation → Tokenizer setup → Model loading → Fine-tuning → Evaluation
**Design tradeoffs**: Joint vs. separate classifiers for intent/slot tasks; multilingual vs. monolingual training
**Failure signatures**: Low slot-filling F1 for uncovered languages (~4-5% drop); overfitting on small English split
**First experiments**: 1) Fine-tune AfroXLMR-76L on English-only data; 2) Fine-tune on multilingual data; 3) Test cross-lingual transfer to low-resource languages

## Open Questions the Paper Calls Out
- Can expanding the dataset to include domains like healthcare and education improve the generalizability of African language models? The current scope misses essential real-world applications.
- What techniques are required to close the slot-filling performance gap between prompt-based LLMs and fine-tuned encoders? Current prompting strategies are insufficient for structured extraction tasks.
- Does scaling the dataset beyond 3,200 examples per language significantly improve zero-shot cross-lingual transfer for low-resource languages? It's unclear if performance ceilings are due to model capacity or limited training data.

## Limitations
- The dataset size is modest compared to high-resource benchmarks, potentially limiting model performance
- The current scope misses healthcare and education domains that are essential for real-world applications
- Significant gaps remain between LLM and fine-tuned model performance on slot-filling tasks

## Confidence
**High Confidence**: Core dataset construction methodology and multilingual training approach are well-documented
**Medium Confidence**: Experimental setup is mostly reproducible, though some hyperparameter details require interpretation
**Low Confidence**: Slot-filling implementation details and LLM baseline configurations have significant gaps

## Next Checks
1. Verify the exact AfroXLMR-76L checkpoint ID and confirm its compatibility with the INJONGO dataset tokenization requirements
2. Reconstruct the BIO tagging implementation for slot-filling and validate that the 23 merged entity types from Table 9 are correctly implemented
3. Test the multilingual training setup by comparing English-only fine-tuning performance against the reported +4.5% improvement from multilingual training, using the same seed-based averaging methodology