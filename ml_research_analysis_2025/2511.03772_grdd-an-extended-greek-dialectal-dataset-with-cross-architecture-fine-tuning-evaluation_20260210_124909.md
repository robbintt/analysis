---
ver: rpa2
title: 'GRDD+: An Extended Greek Dialectal Dataset with Cross-Architecture Fine-tuning
  Evaluation'
arxiv_id: '2511.03772'
source_url: https://arxiv.org/abs/2511.03772
tags:
- greek
- dialectal
- dialects
- cypriot
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents GRDD+, an extended Greek dialectal dataset
  that doubles the size of the original GRDD dataset to 6.4 million words across 10
  varieties, including six new dialects (Greco-Corsican, Griko, Maniot, Heptanesian,
  Tsakonian, and Katharevusa). The authors fine-tuned three model architectures (Llama-3-8B,
  Llama-3.1-8B, Krikri-8B) on four major dialects using LoRA and evaluated them against
  frontier models.
---

# GRDD+: An Extended Greek Dialectal Dataset with Cross-Architecture Fine-tuning Evaluation

## Quick Facts
- arXiv ID: 2511.03772
- Source URL: https://arxiv.org/abs/2511.03772
- Reference count: 0
- Key outcome: LoRA fine-tuning improved dialectal naturalness scores by 1.5-2 points, with frontier models (Claude-3.7-Sonnet) achieving highest scores (3.79-3.86) despite limited training data

## Executive Summary
This paper presents GRDD+, a major extension of the Greek Dialectal Dataset (GRDD) that doubles its size to 6.4 million words across 10 Greek dialect varieties. The authors fine-tuned three model architectures (Llama-3-8B, Llama-3.1-8B, Krikri-8B) on four major dialects using LoRA adapters and evaluated them against frontier models. Results show that fine-tuned models achieved 1.5-2 point improvements on a 5-point naturalness scale, with Claude-3.7-Sonnet achieving the highest scores. Notably, Northern Greek achieved strong performance with only 333 training examples, and fine-tuned models sometimes outperformed frontier models on specific dialects.

## Method Summary
The study extended GRDD from 4 to 10 Greek dialects (6.4M words total) and fine-tuned three 8B parameter models (Llama-3-8B, Llama-3.1-8B, Krikri-8B) using LoRA (rank=16, alpha=32) on four major dialects. Training used 3 epochs with batch size 16 (effective), learning rate 3e-4, and bfloat16 precision on A100 GPUs. Evaluation involved native speaker ratings on 1-5 naturalness scale using 7 prompts per dialect (short/medium/long stories, dialogue, creative writing). The sliding window approach created instruction-tuning data by chunking text into 100-word segments with dialect-specific prompts.

## Key Results
- Fine-tuned models achieved 1.5-2 point improvements on 5-point naturalness scale compared to base models
- Claude-3.7-Sonnet achieved highest scores (3.79-3.86) for Cretan and Northern Greek
- Northern Greek achieved strong performance (2.84-3.86) despite only 333 training examples
- Fine-tuned models sometimes outperformed frontier models on specific dialects
- Krikri-8B (Greek-specialized) underperformed compared to multilingual Llama models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dialectal fine-tuning improves naturalness scores by 1.5-2 points on a 5-point scale, even with limited training data.
- Mechanism: LoRA adaptation modifies attention and projection layers (q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj) to capture dialectal patterns while preserving base model knowledge.
- Core assumption: Dialectal features are learnable through low-rank parameter updates without catastrophic forgetting.
- Evidence anchors: [abstract] "Results show that fine-tuned models achieved 1.5-2 point improvements on a 5-point naturalness scale"; [section 5] "Northern Greek with only 333 examples (1.7%), manages to achieve strong scores (2.84-3.86)"
- Break condition: When training data is extremely limited (<100 examples) or when dialectal distance from base language is too large.

### Mechanism 2
- Claim: Training data quality and linguistic distance from the standard variety may matter more than quantity.
- Mechanism: Models leverage transfer from Standard Modern Greek (SMG) representations, making dialects linguistically closer to SMG easier to learn with fewer examples.
- Core assumption: Pre-trained models have SMG representations that can be adapted to related dialects.
- Evidence anchors: [abstract] "even with only 333 training examples, Northern Greek achieved strong performance (2.84-3.86)"; [section 5] "Northern outperforming Pontic despite having 12 times less data"
- Break condition: When linguistic distance exceeds a threshold (e.g., Pontic's greater distance from SMG), more data may not compensate.

### Mechanism 3
- Claim: Greek-specialized pre-training does not guarantee superior dialectal fine-tuning performance compared to multilingual models.
- Mechanism: Specialized models may overfit to standard variety patterns, reducing plasticity for dialectal adaptation.
- Core assumption: LoRA fine-tuning flexibility depends on base model's representational diversity.
- Evidence anchors: [abstract] "fine-tuned models sometimes outperformed frontier models on specific dialects"; [section 5] "Llama-Krikri which is the only model which is explicitly trained in Modern Greek, does not show the best performance out of the three"
- Break condition: Further investigation needed - this finding contradicts expectations and requires validation.

## Foundational Learning

- Concept: **LoRA (Low-Rank Adaptation)**
  - Why needed here: The paper uses LoRA to efficiently fine-tune 8B parameter models with only ~0.8% trainable parameters.
  - Quick check question: Can you explain why LoRA rank=16 with alpha=32 might be appropriate for dialectal adaptation?

- Concept: **Linguistic distance and dialectal variation**
  - Why needed here: Performance correlates (qualitatively) with dialectal distance from Standard Modern Greek.
  - Quick check question: Why might Pontic (with 4,190 examples) underperform Northern Greek (333 examples)?

- Concept: **Human evaluation methodology for language generation**
  - Why needed here: The study relies on native speaker ratings (5-point scale) with inter-rater reliability metrics (Krippendorff's α, ICC).
  - Quick check question: What does ICC(3,1)=0.87-0.96 tell us about evaluation validity despite moderate absolute agreement?

## Architecture Onboarding

- Component map:
  Raw text -> 100-word chunks -> prompt-completion pairs with dialect instructions -> JSONL format
  -> Llama-3-8B / Llama-3.1-8B / Krikri-8B with LoRA adapters
  -> Training (3 epochs, bfloat16, A100 GPUs)
  -> Generation (7 prompts per dialect)
  -> Human evaluation (5-point naturalness scale)

- Critical path:
  1. Data preprocessing: Chunk text, create prompts with dialect-specific instructions
  2. LoRA adapter initialization: Configure rank-16 updates for attention and MLP projections
  3. Fine-tuning: 3 epochs, batch size 16 (effective), learning rate 3e-4 with cosine scheduler
  4. Evaluation: Generate 7 samples per dialect using 7 prompts
  5. Human rating: Native speakers evaluate on 1-5 naturalness scale

- Design tradeoffs:
  - Dataset imbalance vs. natural distribution: Preserves natural imbalance (Cretan 44.8%, Northern 1.7%) rather than oversampling
  - Model size vs. specialization: 8B models chosen over larger models for efficiency
  - Evaluation scope vs. coverage: 7 prompts per dialect focusing on narrative tasks

- Failure signatures:
  - Near-zero base model performance: Base Llama models score 1.00-1.52, indicating no inherent dialectal knowledge
  - Krikri underperformance: Greek-specialized model scores lower than multilingual Llama on Cretan/Pontic
  - Pontic plateau: All fine-tuned models fail to reach 3.0 threshold despite 4,190 training examples
  - Low inter-rater agreement: Krippendorff's α=0.37 for Cypriot suggests evaluation subjectivity

- First 3 experiments:
  1. Reproduce baseline comparison: Fine-tune Llama-3.1-8B on Cretan dialect with reported LoRA settings, verify ~3.20 mean score
  2. Test data efficiency curve: Train on subsets of Northern Greek data (50, 100, 200, 333 examples) to validate performance with limited data
  3. Investigate Krikri anomaly: Compare LoRA adapter weights between Krikri and Llama-3.1 after fine-tuning on same dialect

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does the Greek-specialized Krikri-8B model underperform compared to multilingual Llama models when fine-tuned on Greek dialects?
- Basis in paper: [explicit] The authors state in the Conclusion and Limitations that the "unexpected underperformance of Krikri-8B, despite its Greek-specific training, merits further investigation."
- Why unresolved: The paper only speculates that other models might be more "flexible" but does not determine if Krikri's pre-training on Standard Modern Greek creates interference or overfitting that hinders dialectal adaptation.
- What evidence would resolve it: Ablation studies comparing Krikri's layer-wise adaptations against Llama's, or analysis of the embedding space overlap between SMG and dialectal varieties in Krikri versus base Llama models.

### Open Question 2
- Question: Can effective dialectal generation be achieved for the six newly added low-resource varieties (e.g., Griko, Tsakonian) given the success of fine-tuning on the original four?
- Basis in paper: [explicit] The authors list as an "immediate plan" the fine-tuning on the six newly added varieties (Greco-Corsican, Griko, Heptanesian, Tsakonian, Maniot, Katharevusa) to provide comprehensive coverage.
- Why unresolved: The current study only fine-tuned and evaluated the original four GRDD dialects. It is unknown if the data quality or volume for the new varieties (some extremely small, like Greco-Corsican) is sufficient for effective LoRA fine-tuning.
- What evidence would resolve it: Replicating the fine-tuning methodology used for Cretan and Pontic on the six new varieties and evaluating the resulting naturalness scores.

### Open Question 3
- Question: To what extent does the linguistic distance between a dialect and Standard Modern Greek (SMG) correlate with fine-tuning performance?
- Basis in paper: [explicit] The authors note that the performance cline (Northern/Cretan > Cypriot > Pontic) leads to an "interesting question" about whether results correlate with distance from the dominant variety, a relationship they state is "largely unexplored."
- Why unresolved: The paper relies on "impressionistic intuitions" about dialect distance because Greek dialectology lacks standardized distance metrics, preventing a systematic test of this hypothesis.
- What evidence would resolve it: Developing or utilizing a quantitative metric for linguistic distance between these dialects and SMG, then correlating these distances with the models' naturalness scores or loss curves.

## Limitations
- Dataset imbalance may create skewed performance patterns (44.8% Cretan vs 1.7% Northern)
- Evaluation methodology relies entirely on human ratings with moderate inter-rater reliability (Krippendorff's α=0.37 for Cypriot)
- Study does not provide direct comparisons between generated dialectal text and authentic native speaker examples
- Unexpected underperformance of Krikri-8B remains unexplained and could indicate issues with model architecture or fine-tuning methodology

## Confidence
**High Confidence (Level 1):** The core finding that LoRA fine-tuning improves dialectal naturalness scores by 1.5-2 points is well-supported by experimental data and human evaluation results.

**Medium Confidence (Level 2):** The claim that training data quality and linguistic distance from SMG matter more than quantity has supporting evidence but relies on qualitative assessments rather than quantitative metrics. The finding about Krikri-8B's unexpected underperformance is observed but unexplained.

**Low Confidence (Level 3):** The broader generalizability of these results to other language families or different types of linguistic variation cannot be established from this single case study of Greek dialects.

## Next Checks
1. Investigate Krikri-8B anomaly: Compare LoRA adapter weights between Krikri-8B and Llama-3.1-8B after fine-tuning on the same dialect to understand the architectural or representational differences causing the performance gap.

2. Test data efficiency curve: Conduct controlled experiments training on systematically varied subsets of Northern Greek data (50, 100, 200, 333 examples) to validate the claim that linguistic distance matters more than data quantity.

3. Validate against authentic examples: Generate dialectal text and compare it directly with authentic native speaker examples using both automated metrics and human evaluation to assess whether the "naturalness" scores reflect true dialectal authenticity.