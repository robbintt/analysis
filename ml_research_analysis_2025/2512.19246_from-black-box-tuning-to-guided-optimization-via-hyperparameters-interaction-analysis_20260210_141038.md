---
ver: rpa2
title: From Black-Box Tuning to Guided Optimization via Hyperparameters Interaction
  Analysis
arxiv_id: '2512.19246'
source_url: https://arxiv.org/abs/2512.19246
tags:
- hyperparameters
- optimization
- performance
- importance
- tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents MetaSHAP, a meta-learning and Shapley-based
  framework for estimating hyperparameter importance and interactions prior to optimization.
  The method leverages a large-scale knowledge base of over 09 million evaluated ML
  pipelines, retrieving similar tasks based on dataset meta-features and using surrogate
  models to estimate importance via Shapley values.
---

# From Black-Box Tuning to Guided Optimization via Hyperparameters Interaction Analysis

## Quick Facts
- arXiv ID: 2512.19246
- Source URL: https://arxiv.org/abs/2512.19246
- Authors: Moncef Garouani; Ayah Barhrhouj
- Reference count: 18
- Primary result: MetaSHAP-guided Bayesian optimization achieves faster convergence (3 vs 9 iterations) and reduces search space by 50-70% by focusing on 2-3 dominant hyperparameters per task.

## Executive Summary
This paper introduces MetaSHAP, a meta-learning framework that predicts hyperparameter importance and interactions before optimization begins. By leveraging a large-scale knowledge base of over 9 million evaluated ML pipelines, the method retrieves similar tasks based on dataset meta-features and uses surrogate models to estimate importance via Shapley values. The framework then guides Bayesian optimization by restricting the search space to 2-3 dominant hyperparameters with identified tuning ranges. Experiments demonstrate significant improvements in convergence speed and search efficiency compared to standard BO across 14 classifiers and 164 datasets.

## Method Summary
MetaSHAP estimates hyperparameter importance prior to optimization by extracting meta-features from a new dataset, retrieving similar datasets from a 9M+ pipeline knowledge base, and training surrogate models on the retrieved configurations. Shapley values computed on these surrogates identify the most influential hyperparameters and their optimal tuning ranges. The method then guides Bayesian optimization by restricting the search space to these identified hyperparameters and ranges, achieving faster convergence and substantial search space reduction while maintaining or improving final model performance.

## Key Results
- MetaSHAP-guided BO reaches optimal accuracy by iteration 3 versus iteration 9 for standard BO on the titanic dataset
- Search space reduced by 50-70% by focusing on 2-3 dominant hyperparameters per task
- Consistent performance improvements across 14 classifiers and 164 datasets
- Framework provides interpretable insights about tuning ranges and interactions

## Why This Works (Mechanism)

### Mechanism 1: Meta-Feature-Based Task Retrieval
Datasets with similar meta-characteristics exhibit similar hyperparameter importance landscapes. The framework extracts meta-features from a new dataset, computes Euclidean distance to datasets in the knowledge base, and retrieves the k-nearest neighbors. Surrogate models are trained only on configurations from these similar datasets, creating a localized performance approximation. Assumption: Meta-feature similarity correlates with hyperparameter sensitivity patterns across the 14 classifiers and 164 datasets.

### Mechanism 2: Surrogate Model as SHAP Proxy
A regression surrogate trained on historical (hyperparameter, performance) pairs can approximate the true performance function well enough for reliable importance attribution. For each retrieved dataset, MetaSHAP fits a surrogate model mapping hyperparameter configurations to observed performance. SHAP values are computed on this surrogate rather than requiring new model trainings, making importance estimation computationally tractable. Assumption: Surrogate fidelity is sufficient for SHAP's marginal contribution calculations to reflect true importance.

### Mechanism 3: Search Space Pruning via Actionable Range Extraction
SHAP value distributions over hyperparameter values identify "relevant tuning ranges" where influence on performance is most pronounced. For each hyperparameter, SHAP values are sorted by actual configuration values, smoothed via moving average, and sub-ranges with maximal absolute smoothed SHAP values are extracted. BO is then restricted to these 2-3 dominant hyperparameters within identified ranges. Assumption: High absolute SHAP values indicate tunable regions with consistent performance impact.

## Foundational Learning

- **Shapley Values (Cooperative Game Theory)**
  - Why needed here: Core attribution mechanism; quantifies each hyperparameter's marginal contribution across all possible coalitions. Without this, cannot interpret the surrogate's behavior.
  - Quick check question: Given 3 players with coalition values v({A})=2, v({B})=3, v({A,B})=6, compute player A's Shapley value.

- **Meta-Feature Engineering**
  - Why needed here: Determines retrieval quality. Must understand which dataset characteristics (skewness, entropy, landmarking accuracy) predict hyperparameter sensitivity.
  - Quick check question: Why might dataset size alone be insufficient for predicting whether regularization strength is important?

- **Gaussian Process Bayesian Optimization**
  - Why needed here: MetaSHAP guides BO by restricting its search space; understanding acquisition functions and GP surrogates clarifies what guidance modifies.
  - Quick check question: How does restricting the search space dimensionality affect GP surrogate modeling and acquisition function optimization?

## Architecture Onboarding

- **Component map:**
  Meta-feature Extractor (σ) -> Knowledge Base (K) -> Retrieval Module -> Surrogate Trainer -> SHAP Computer -> Range Extractor -> BO Wrapper

- **Critical path:** Meta-feature extraction → Retrieval quality → Surrogate adequacy → SHAP reliability → Range extraction accuracy → BO convergence speedup

- **Design tradeoffs:**
  - **k (neighbors):** Higher k increases training data but dilutes locality; lower k risks insufficient samples
  - **Surrogate model choice:** Paper doesn't specify; Random Forest vs. Gradient Boosting vs. Neural Net trade interpretability for accuracy
  - **Smoothing window size:** Affects range extraction—too aggressive smoothing merges distinct peaks, too little captures noise

- **Failure signatures:**
  - **Cold-start on novel domains:** If no similar datasets exist (e.g., the "cars" dataset exception where guidance underperformed), retrieval quality degrades and guidance becomes suboptimal
  - **Interaction misspecification:** Strong synergies (e.g., SVM's C+γ) may require joint range analysis not captured by marginal SHAP
  - **Surrogate underfitting:** Retrieved neighborhood has <100 configurations; importance rankings become unstable

- **First 3 experiments:**
  1. **Retrieval validity test:** On held-out datasets from KB, verify that retrieved neighbors' SHAP rankings correlate with ground-truth importance (computed via full fANOVA or ablation on target dataset)
  2. **Ablation on k:** Run MetaSHAP-guided BO with k∈{5,10,25,50} on 3 diverse datasets; plot convergence curves to identify optimal neighborhood size
  3. **Interaction diagnosis:** For SVM on a dataset where C and γ are known to interact strongly, compare single-hyperparameter range extraction vs. pairwise SHAP interaction indices; quantify performance gap

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effectively does MetaSHAP scale to deep neural networks with high-dimensional hyperparameter spaces and fundamentally different interaction structures?
- Basis in paper: [explicit] Conclusion states: "MetaSHAP opens promising directions for generalizing this approach to regression tasks, deep neural networks, and other forms of parameterized learning algorithms."
- Why unresolved: The current evaluation covers only 14 traditional classifiers; DNNs have orders of magnitude more hyperparameters with complex conditional dependencies that may challenge Shapley-based attribution.
- What evidence would resolve it: Empirical validation on DNN architectures (e.g., ResNet, Transformers) comparing MetaSHAP-guided vs. standard BO convergence rates and importance ranking stability.

### Open Question 2
- Question: What is the minimum knowledge base coverage required to guarantee reliable guidance when similar datasets are sparse or absent?
- Basis in paper: [inferred] Results note that the cars dataset exception "suggests that the quality of MetaSHAP guidance depends on the relevance of the retrieved analog datasets... If no sufficiently similar datasets are available, the risk of suboptimal guidance increases."
- Why unresolved: The paper provides no theoretical bounds or empirical characterization of KB density thresholds for safe guidance.
- What evidence would resolve it: Ablation studies varying KB size and measuring guidance quality degradation; analysis linking meta-feature neighborhood density to optimization regret.

### Open Question 3
- Question: Can learned dataset embeddings replace hand-crafted meta-features to improve retrieval accuracy and cross-domain transfer?
- Basis in paper: [inferred] MetaSHAP uses fixed meta-feature vectors; Related Work notes existing methods "lack mechanisms to reason about the causal or correlational impact of individual hyperparameters" and meta-features are static heuristics.
- Why unresolved: Learned representations could capture latent dataset characteristics beyond statistical moments, but this is unexplored.
- What evidence would resolve it: Comparing retrieval-based guidance quality using neural dataset embeddings versus current meta-features across heterogeneous domains.

### Open Question 4
- Question: Do hyperparameter importance patterns transfer across algorithm families for datasets with similar meta-characteristics?
- Basis in paper: [inferred] The framework estimates importance per algorithm-dataset pair independently; cross-algorithm transfer of interaction insights is not investigated despite the global HPO framing.
- Why unresolved: If patterns transfer, MetaSHAP could recommend algorithm-agnostic tuning priorities; current experiments test only within-algorithm transfer.
- What evidence would resolve it: Experiments measuring correlation of Shapley-derived importance rankings across algorithms (e.g., SVM vs. XGBoost) on similar datasets.

## Limitations

- The knowledge base contains 9+ million evaluated pipelines, but retrieval quality fundamentally depends on dataset similarity in meta-feature space. For datasets significantly outside the knowledge base distribution, guidance may degrade.
- Surrogate model specifications remain unspecified, which significantly impacts both fidelity and computational efficiency for SHAP computation.
- Range extraction methodology lacks precision in smoothing window size and algorithm for identifying optimal sub-ranges, affecting reproducibility.

## Confidence

- **High Confidence**: MetaSHAP framework architecture (retrieval → surrogate → SHAP → BO guidance) is well-defined and theoretically sound. The convergence speedup claims (3 vs 9 iterations on titanic) are specific and verifiable.
- **Medium Confidence**: SHAP-based importance estimation works as claimed for single hyperparameters, but the assumption that 2-3 dominant hyperparameters suffice may not hold for algorithms with strong interaction effects.
- **Low Confidence**: The 50-70% search space reduction claim lacks distributional context—this range may reflect specific datasets rather than typical performance across diverse ML tasks.

## Next Checks

1. **Retrieval Robustness Test:** For 5 datasets from the knowledge base, systematically remove them, run MetaSHAP retrieval, and measure correlation between retrieved importance rankings and ground-truth importance computed via full fANOVA on the held-out dataset.

2. **Interaction Detection Validation:** For SVM and XGBoost on datasets where C+γ or max_depth+learning_rate interactions are known to be strong, compare MetaSHAP's single-hyperparameter range extraction against pairwise SHAP interaction indices; measure performance gap when ignoring interactions.

3. **Cold-Start Analysis:** Identify datasets with minimum meta-feature similarity to the knowledge base (e.g., those with highest distance to nearest neighbor), run MetaSHAP-guided BO, and quantify convergence degradation relative to standard BO.