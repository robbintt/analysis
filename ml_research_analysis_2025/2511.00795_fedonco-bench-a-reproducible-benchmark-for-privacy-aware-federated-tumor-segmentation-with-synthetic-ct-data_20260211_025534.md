---
ver: rpa2
title: 'FedOnco-Bench: A Reproducible Benchmark for Privacy-Aware Federated Tumor
  Segmentation with Synthetic CT Data'
arxiv_id: '2511.00795'
source_url: https://arxiv.org/abs/2511.00795
tags:
- privacy
- data
- federated
- segmentation
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FedOnco-Bench introduces a reproducible benchmark for privacy-aware
  federated tumor segmentation using synthetic CT data. It evaluates FedAvg, FedProx,
  FedBN, and DP-SGD across segmentation accuracy (Dice) and privacy leakage (MIA AUC).
---

# FedOnco-Bench: A Reproducible Benchmark for Privacy-Aware Federated Tumor Segmentation with Synthetic CT Data

## Quick Facts
- arXiv ID: 2511.00795
- Source URL: https://arxiv.org/abs/2511.00795
- Reference count: 15
- Primary result: Introduces benchmark evaluating FedAvg, FedProx, FedBN, DP-SGD on synthetic CT segmentation with privacy-utility tradeoffs

## Executive Summary
FedOnco-Bench introduces a reproducible benchmark for privacy-aware federated tumor segmentation using synthetic CT data. It evaluates four federated learning methods (FedAvg, FedProx, FedBN, DP-SGD) across segmentation accuracy and privacy leakage through membership inference attacks. Results demonstrate clear privacy-utility tradeoffs: DP-SGD significantly reduces privacy risk but degrades accuracy, while FedAvg and FedBN achieve higher accuracy at greater privacy risk. The benchmark provides an open-source platform for future privacy-preserving FL development in medical imaging.

## Method Summary
The benchmark uses synthetic 256×256 CT slices with tumor annotations distributed non-IID across 5 simulated clients. A 2D U-Net serves as the segmentation backbone, and four FL algorithms are evaluated: FedAvg (baseline), FedProx (with proximal term μ=0.01), FedBN (excluding batch norm from aggregation), and DP-SGD (with gradient clipping C=1.0 and Gaussian noise σ=1.2). Training runs for 100 rounds with 1 epoch per round. Privacy is measured via membership inference attack AUC using shadow models, while accuracy is measured by Dice coefficient and cross-entropy loss. The benchmark includes synthetic data generation via diffusion models and comprehensive evaluation protocols.

## Key Results
- FedAvg and FedBN achieve highest segmentation accuracy (Dice ~0.85) but highest privacy risk (MIA AUC ~0.72)
- DP-SGD provides strongest privacy protection (MIA AUC ~0.25) at cost of accuracy (Dice ~0.79)
- FedProx and FedBN offer balanced performance under non-IID data conditions
- Privacy-utility tradeoff is clearly demonstrated across all evaluated FL methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Differential privacy via DP-SGD substantially reduces membership inference risk at measurable accuracy cost.
- Mechanism: Gradient clipping (ℓ₂ norm C=1.0) bounds maximum per-sample contribution; Gaussian noise (σ=1.2) masks individual gradient signals, making membership inference statistically indistinguishable from random guessing.
- Core assumption: Secure aggregation is enforced at server, so attacker only observes aggregated noisy updates or final model outputs.
- Evidence anchors:
  - [abstract] "DP-SGD provides a higher level of privacy (AUC around 0.25) at the cost of accuracy (Dice about 0.79)"
  - [section III.F] "Each gradient is clipped to ℓ₂ norm C=1.0... Add Gaussian noise: N(0, σ²C²I), with σ=1.2"
  - [corpus] No direct corpus validation of DP-SGD effectiveness for segmentation; this paper provides novel quantification.
- Break condition: If noise scale is set too low, MIA risk remains elevated; if too high, model fails to converge or accuracy degrades below usable threshold.

### Mechanism 2
- Claim: FedProx and FedBN provide implicit regularization against overfitting, modestly reducing MIA risk while maintaining accuracy under non-IID conditions.
- Mechanism: FedProx adds proximal term (μ/2)‖w−w^t‖² penalizing deviation from global model, reducing memorization capacity. FedBN excludes batch normalization parameters from aggregation, preserving local feature statistics and acting as implicit regularizer.
- Core assumption: Non-IID heterogeneity stems from feature distribution shifts (e.g., scanner noise, tumor size variation) rather than label distribution alone.
- Evidence anchors:
  - [abstract] "FedProx and FedBN offer balanced performance under heterogeneous data"
  - [section V.B] "FedProx lowers MI risk further to 0.68, suggesting its regularization discourages overfitting"
  - [corpus] FeTS Challenge 2024 (arXiv:2512.06206) similarly evaluates FL aggregation robustness but does not quantify privacy leakage.
- Break condition: If heterogeneity is extreme or architecture lacks batch norm, FedBN provides no benefit; FedProx may slow convergence excessively if μ is too large.

### Mechanism 3
- Claim: Membership inference attack AUC reliably quantifies privacy leakage in segmentation models.
- Mechanism: Shadow models trained on synthetic data learn to distinguish member vs. non-member outputs; attack classifier evaluates whether global model leaks information about training samples through prediction patterns.
- Core assumption: Black-box access reflects realistic threat model; white-box gradient access would increase leakage.
- Evidence anchors:
  - [abstract] Privacy leakage measured as "attack AUC about 0.72" for FedAvg
  - [section III.G] "Train a shadow model (same architecture) on a synthetic dataset... Train an attack classifier on shadow model outputs"
  - [corpus] Chobola et al. (arXiv:2212.01082, cited in paper) shows segmentation models are particularly susceptible to MIA.
- Break condition: If shadow dataset distribution diverges significantly from target data, attack classifier generalizes poorly and underestimates true risk.

## Foundational Learning

- Concept: **Federated Averaging (FedAvg)**
  - Why needed here: Baseline aggregation method; all other methods are modifications or comparisons to this.
  - Quick check question: Can you explain why FedAvg assumes IID data and what happens when this assumption is violated?

- Concept: **Differential Privacy (ε, δ)-guarantees**
  - Why needed here: Provides theoretical framework for DP-SGD; understanding noise calibration requires grasp of privacy budget concepts.
  - Quick check question: What is the relationship between noise scale σ, clipping threshold C, and privacy parameter ε?

- Concept: **U-Net Architecture for Segmentation**
  - Why needed here: The segmentation backbone used; skip connections and encoder-decoder structure are standard for medical imaging.
  - Quick check question: Why does U-Net use skip connections, and how do they affect gradient flow during federated training?

## Architecture Onboarding

- Component map:
  - Central Server -> Aggregates client updates via weighted averaging; broadcasts global model
  - Federated Clients (5 simulated) -> Each holds local synthetic CT data; trains local U-Net model
  - Segmentation Model -> 2D U-Net with ~1.2M parameters; two down-blocks, two up-blocks, batch norm, skip connections
  - Privacy Evaluation Pipeline -> Shadow model → attack classifier → MIA AUC computation
  - Synthetic Data Generator -> Diffusion-based model creates 256×256 CT slices with tumor annotations

- Critical path:
  1. Generate/distribute synthetic CT data across 5 clients with controlled heterogeneity (tumor size, noise)
  2. Initialize global U-Net with identical weights across all runs
  3. For each FL round (100 total): server broadcasts global model → clients train locally (1 epoch, batch size 16) → clients send updates → server aggregates
  4. Apply method-specific modifications: FedProx (add proximal term μ=0.01), FedBN (exclude BN from aggregation), DP-SGD (clip + noise gradients)
  5. Evaluate on held-out test set (1000 images): compute Dice, CE loss
  6. Run MIA: train shadow model → train attack classifier → compute AUC on 500 members + 500 non-members

- Design tradeoffs:
  - **FedAvg vs. FedProx**: FedAvg converges faster but less stable under non-IID; FedProx slower but more robust
  - **FedAvg vs. FedBN**: FedBN handles feature shift better but requires BN layers; FedAvg is architecture-agnostic
  - **Non-private FL vs. DP-SGD**: ~6 point Dice drop (0.85→0.79) for ~65% MIA AUC reduction (0.72→0.25)
  - **2D vs. 3D**: Paper uses 2D slices for simplicity; real clinical deployment would require volumetric models

- Failure signatures:
  - **Dice plateau < 0.75 with DP-SGD**: Noise scale too high; reduce σ or increase clipping threshold
  - **Training curves diverge across clients**: Non-IID severity exceeds FedAvg capacity; switch to FedProx or FedBN
  - **MIA AUC near 0.5 but Dice < 0.70**: Over-regularized model; privacy preserved but utility sacrificed beyond acceptable threshold
  - **FedBN underperforms FedAvg**: Heterogeneity may be label-based rather than feature-based; BN exclusion provides no benefit

- First 3 experiments:
  1. **Reproduce baseline results**: Run FedAvg for 100 rounds on provided synthetic data; verify Dice ≈ 0.85 ± 0.01 and MIA AUC ≈ 0.72. This validates environment setup.
  2. **Sweep DP noise scale**: Test σ ∈ {0.5, 1.0, 1.2, 1.5, 2.0} with fixed clipping C=1.0; plot privacy-utility curve (Dice vs. MIA AUC). Identify acceptable operating point for your deployment constraints.
  3. **Stress-test heterogeneity**: Increase non-IID severity (e.g., Client 1: 90% large tumors, Client 2: 90% small tumors) and compare FedAvg vs. FedProx vs. FedBN convergence rates and final Dice. Determine which method suits your expected data distribution.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the privacy-utility trade-off be improved through non-heuristic tuning of differential privacy parameters (e.g., noise scale, clipping norm) in federated tumor segmentation?
- Basis in paper: [explicit] Section VI.B states: "The DP-SGD parameters... were heuristically chosen. It is hypothesized that better accuracy could be achieved through careful tuning of these parameters... albeit at a higher $\epsilon$."
- Why unresolved: The benchmark only evaluates a single noise scale ($\sigma=1.2$) and clipping norm, without performing a grid search to identify optimal privacy budgets ($\epsilon$) for medical imaging.
- What evidence would resolve it: Experimental results displaying Dice scores and MIA AUCs across a gradient of $\epsilon$ values and clipping thresholds to identify an optimal operational curve.

### Open Question 2
- Question: How do FedOnco-Bench baselines perform under stronger, white-box threat models or reconstruction attacks?
- Basis in paper: [explicit] Section VI.E lists as a limitation: "This study assumes black-box MIA; stronger attacks with white-box access were not explored. Other privacy attacks, such as model inversion or attribute inference, were also not considered."
- Why unresolved: The reported privacy risks (AUC ~0.72) rely solely on black-box membership inference; defenses like DP-SGD might fail or require different calibration if the attacker has access to model weights (white-box) or reconstructs images.
- What evidence would resolve it: A study benchmarking the current FL models against gradient inversion attacks or white-box MIAs to measure data leakage more aggressively.

### Open Question 3
- Question: Do the privacy-utility findings generalize to volumetric 3D CT data and generative mechanisms like volumetric GANs?
- Basis in paper: [explicit] Section VI.E notes the reliance on 2D slices, while Section VII suggests: "An additional synthetic CT generation mechanism could be developed using... volumetric generative adversarial networks (GANs) or diffusion models."
- Why unresolved: 2D slices lack the spatial complexity of clinical 3D CT volumes, and it is unclear if the synthetic 2D data accurately replicates the heterogeneity that drives privacy leakage in 3D clinical workflows.
- What evidence would resolve it: Reproducing the FedOnco-Bench experiments using 3D U-Nets on synthetic volumetric datasets to compare the privacy leakage magnitude against the 2D baselines.

### Open Question 4
- Question: How does partial client participation impact the convergence speed and privacy leakage dynamics in this benchmark?
- Basis in paper: [explicit] Section VI.E states: "Unlike real-world FL systems with partial client availability, all clients participated in every round, which could affect convergence and privacy risks."
- Why unresolved: The current benchmark assumes full participation (cross-silo stability), whereas real-world cross-device or intermittent cross-silo settings involve varying client availability, which affects the aggregation of DP noise and model drift.
- What evidence would resolve it: Simulation results showing training curves and MIA AUCs under client sampling rates (e.g., 50% or 80% participation per round).

## Limitations
- Synthetic data generation methodology is incompletely specified (diffusion model architecture and weights not provided)
- MIA evaluation lacks critical details (shadow model training procedure, attack classifier architecture, training epochs not specified)
- Assumes secure aggregation at server without verification or exploration of compromised aggregation scenarios
- Only evaluates black-box membership inference attacks, not stronger white-box or reconstruction attacks
- Uses 2D slices rather than volumetric 3D CT data, limiting clinical applicability

## Confidence
- **High confidence** in the existence and general direction of the privacy-utility tradeoff curve
- **Medium confidence** in the exact quantitative tradeoffs due to missing implementation details
- **Low confidence** in the ability to reproduce identical synthetic data distributions and exact MIA results without the diffusion model weights and attack classifier specifications

## Next Checks
1. Contact authors for diffusion model weights and MIA attack classifier specifications, or implement closest available alternatives and document performance differences
2. Validate synthetic data quality by comparing tumor size distributions, noise characteristics, and anatomical plausibility against real CT benchmarks
3. Test sensitivity of MIA results to shadow dataset size and attack classifier architecture—determine if reported privacy metrics are robust to these methodological choices