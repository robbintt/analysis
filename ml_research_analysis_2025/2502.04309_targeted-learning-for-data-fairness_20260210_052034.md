---
ver: rpa2
title: Targeted Learning for Data Fairness
arxiv_id: '2502.04309'
source_url: https://arxiv.org/abs/2502.04309
tags:
- fairness
- data
- inference
- estimator
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a targeted learning approach to perform statistical
  inference on data fairness, moving beyond algorithmic fairness to assess the data-generating
  process itself. The authors derive estimators for demographic parity, equal opportunity,
  and conditional mutual information using the efficient influence function framework.
---

# Targeted Learning for Data Fairness

## Quick Facts
- arXiv ID: 2502.04309
- Source URL: https://arxiv.org/abs/2502.04309
- Reference count: 24
- Primary result: Introduces targeted learning estimators for demographic parity, equal opportunity, and conditional mutual information that achieve double robustness for probabilistic fairness metrics

## Executive Summary
This paper presents a targeted learning framework for statistical inference on data fairness metrics, moving beyond algorithmic fairness to assess the data-generating process itself. The authors derive estimators for demographic parity, equal opportunity, and conditional mutual information using the efficient influence function framework. They demonstrate that their estimators for probabilistic fairness metrics achieve double robustness, meaning they remain consistent even if one of the two required models is misspecified. The approach provides valid statistical inference for fairness metrics using flexible machine learning models, with applications demonstrated on both simulated and real-world datasets.

## Method Summary
The method uses targeted learning to construct unbiased estimators for fairness metrics by deriving the efficient influence function (EIF) and subtracting the plug-in bias. The framework requires estimating two nuisance models: the outcome model P(Y|X) and the propensity model P(G|X). Data is split into training and evaluation sets, with flexible machine learning models (SuperLearner ensembles) fitted on training data. The EIF-based estimator is then applied to the evaluation set to compute point estimates and confidence intervals. The approach achieves double robustness for probabilistic metrics, meaning consistency is maintained if either nuisance model is correctly specified.

## Key Results
- Estimators for probabilistic demographic parity and equal opportunity achieve double robustness, maintaining coverage even when one nuisance model is misspecified
- Conditional mutual information estimator shows significant bias near zero dependence but can still be informative about dependence structure
- Applied to Adult-Income dataset: demographic parity estimate of -0.13 (-0.19, -0.06) with relationship status contributing 0.08 to the disparity
- Law School dataset shows lower scores for Black students compared to White students across all LSAT score bins

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The estimators for probabilistic fairness metrics are consistent even if one of the two required nuisance models is misspecified.
- **Mechanism:** This works via the double robustness property inherent in the efficient influence function (EIF) derivation. The estimator combines an outcome model D(x) = P(Y=1|X=x) and a propensity model π(x) = P(G=1|X=x). As shown in the proof (Appendix C), the estimator's expectation cancels out errors if either the true outcome mechanism or the true group mechanism is captured, protecting against partial model failure.
- **Core assumption:** The data is independent and identically distributed (iid), and at least one of the nuisance models (D̂ or π̂) is consistently estimated.
- **Evidence anchors:**
  - [abstract]: "They demonstrate that their estimators for probabilistic fairness metrics achieve double robustness..."
  - [section 3.1.3]: "if either π̂(x) = π(x) or D̂(x) = D(x), then E[Ψ̄(π̂)] ≈ Ψ(P)."
  - [corpus]: Neighbor paper "Rescuing double robustness..." confirms the relevance of robust estimation under misspecification.
- **Break condition:** If both the outcome model and the propensity model are misspecified (e.g., using linear logistic models when the truth is quadratic), double robustness fails, though the estimator may still degrade gracefully depending on the error magnitude.

### Mechanism 2
- **Claim:** Targeted learning enables valid statistical inference (confidence intervals) for fairness metrics using flexible machine learning models, which typically lack inherent uncertainty quantification.
- **Mechanism:** This is achieved by plug-in bias correction. Standard ML plug-in estimators Ψ(π̂) are often biased. The framework analytically derives the bias term using the Efficient Influence Function (EIF) and subtracts it (specifically, by solving the estimating equation Σ φ(O, π̂) = 0). This allows the central limit theorem to apply, permitting the construction of Wald-type confidence intervals.
- **Core assumption:** The remainder term R in the expansion (Appendix D) must be oP(n^(-1/2)).
- **Evidence anchors:**
  - [section 2]: "Since we do not make any assumptions about the true form of P... Ψ(π̂) may be a biased estimate... the plug-in bias turns out to be exactly [the EIF mean]."
  - [section 4.1.1]: "TL estimates of variance are significantly larger than the t-test standard errors... TL estimates compensate for the estimation procedure."
- **Break condition:** If the sample size is too small to support the complexity of the nonparametric models (slow convergence rates), the remainder term dominates, and the confidence intervals may have incorrect coverage.

### Mechanism 3
- **Claim:** Assessing "data fairness" (fairness of the data-generating process) isolates systemic bias from the artifacts of a specific predictive algorithm.
- **Mechanism:** The estimand is defined using the Bayes optimal decision rule Dc(x) = 1{P(Y=1|X=x) ≥ c} rather than a specific trained model m(x). This evaluates the theoretical limit of fairness for any model trained on this data distribution, distinct from "model fairness" which evaluates a fixed function.
- **Core assumption:** The conditional distribution P(Y|X) is estimable from the available data features X.
- **Evidence anchors:**
  - [section 3.1]: "Notably, Dc(x) is a property of the data-generating process, not a specific model. Therefore, inferences we make... are about the data rather than a model."
  - [abstract]: "...moving beyond algorithmic fairness to assess the data-generating process itself."
  - [corpus]: Weak/Missing direct corpus anchors for the specific "data fairness" definition, though "Detecting Statistically Significant Fairness Violations" relates to the broader inferential goal.
- **Break condition:** This mechanism assumes X captures sufficient signal; if Y is effectively random given X, the "data fairness" measure reflects only noise, making it indistinguishable from fairness.

## Foundational Learning

- **Concept:** Efficient Influence Function (EIF)
  - **Why needed here:** The EIF is the engine of the paper. It is used both to derive the unbiased estimators (Mechanism 2) and to calculate the standard errors for confidence intervals. You cannot implement the proposed method without calculating/deriving the specific EIFs for fairness metrics.
  - **Quick check question:** Can you explain why the EIF for a simple population mean is just (X - μ)?

- **Concept:** Sample Splitting (Cross-fitting)
  - **Why needed here:** The paper relies on splitting data into training (to fit π̂) and evaluation (to calculate the estimator) sets. This decoupling is required to avoid overfitting bias (the "empirical process term") when using flexible machine learning models, ensuring the validity of the inference.
  - **Quick check question:** Why can't we train the ML model and evaluate the estimator on the exact same data points without risking anti-conservative confidence intervals?

- **Concept:** Double Robustness
  - **Why needed here:** This is the key theoretical guarantee for the probabilistic estimators. Understanding it guides model selection—you know you only need to get one of the two sub-models (outcome or propensity) right, which informs the ensemble strategy (SuperLearner).
  - **Quick check question:** In a doubly robust estimator for E[Y], if your outcome regression model is wrong, what must be true about your propensity model for the estimate to remain consistent?

## Architecture Onboarding

- **Component map:** Data with Y, X, G -> Splitter (Train/Test) -> Nuisance Estimators (Train: P(Y|X), P(G|X)) -> Targeted Estimator (Test: apply EIF formulas) -> Inference Module (compute mean, σ(EIF) for Wald CIs)

- **Critical path:** Accurately estimating the conditional probabilities P(Y|X) and P(G|X) is the bottleneck. If these models fail to converge or have high bias, the fairness estimates will be biased (unless both fail in a way that cancels out, which is unsafe to rely on).

- **Design tradeoffs:**
  - Probabilistic vs. Traditional Metrics: Probabilistic metrics (e.g., Probabilistic Parity) offer double robustness and lower variance but "soften" the fairness requirement. Traditional metrics (thresholded) are interpretable but lack double robustness guarantees in this framework.
  - Single vs. Separate Approach (for CMI): The paper notes that for Conditional Mutual Information, estimating the joint distribution (Single approach) performs uniformly better than estimating marginals separately (Separate approach).

- **Failure signatures:**
  - CMI Underflow: The CMI estimator produces negative values or zeros with extremely tight confidence intervals that exclude the true value (severe bias near zero dependence).
  - Variance Inflation: If the propensity model π̂(x) predicts probabilities very close to 0 or 1 (lack of overlap), the inverse weighting in the estimator causes variance to explode.
  - Coverage Collapse: If both nuisance models are misspecified, confidence interval coverage drops below the nominal 95% level.

- **First 3 experiments:**
  1. Setting 3 Validation (Double Robustness): Generate data with non-linear relationships (quadratic). Fit 4 configurations: (Correct Outcome/Wrong Propensity), (Wrong Outcome/Correct Propensity), (Both Correct), (Both Wrong). Verify coverage remains >90% for the first three.
  2. CMI Bias Check: Generate data with known conditional independence (Y ⊥ G | X). Run the CMI estimator. Check if the estimate is biased away from zero (the paper predicts it will be).
  3. Real Data Variable Importance: Apply the estimator to the Adult-Income dataset. Compute the variable importance (Shapley values) to identify which features drive the disparity, reproducing the finding that 'relationship' status dominates the fairness score.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can a hypothesis testing framework improve the utility of the Conditional Mutual Information (CMI) estimator for assessing conditional independence, given its bias and poor coverage in standard estimation?
- **Basis in paper:** [explicit] Section 6.2 states that hypothesis testing may be a more appropriate approach for the CMI estimator, as the targeted learning estimator exhibited bias and poor coverage.
- **Why unresolved:** The current study focused on estimation and confidence intervals, finding that the CMI estimator is unsuitable for inference, particularly when dependence is weak.
- **What evidence would resolve it:** A study developing a specific hypothesis test (e.g., using the influence function) for CMI and validating its Type I error and power in simulations.

### Open Question 2
- **Question:** How can targeted learning estimators for data fairness be generalized to accommodate categorical or continuous sensitive attributes and outcomes rather than being restricted to binary variables?
- **Basis in paper:** [explicit] Section 6.2 identifies the limitation to binary group status and binary outcomes, noting that many real-world attributes of interest have multiple categories or are continuous.
- **Why unresolved:** The current derivation of the efficient influence functions and estimators relies specifically on binary indicators for group membership and outcomes.
- **What evidence would resolve it:** The derivation of new efficient influence functions for non-binary fairness metrics (e.g., brier score differences for continuous outcomes) and simulations demonstrating their error properties.

### Open Question 3
- **Question:** What methodologies can be developed to intervene on or remediate data-generating processes that are identified as unfair using these targeted learning estimators?
- **Basis in paper:** [explicit] Section 6.2 notes that while the paper provides a method for assessing data fairness, it does not provide guidance on correcting for unfair decisions.
- **Why unresolved:** The current work is diagnostic and inferential, focusing on quantifying the extent of unfairness rather than providing a solution to rectify it.
- **What evidence would resolve it:** A proposed remediation technique (e.g., data re-sampling or augmentation) applied to a dataset, followed by a re-application of the proposed estimators to show a statistically significant reduction in unfairness.

## Limitations
- The CMI estimator shows severe bias near zero dependence, making it unreliable for detecting conditional independence
- Variance estimates can be highly unstable when propensity scores are close to 0 or 1 (lack of overlap)
- The framework is currently limited to binary sensitive attributes and binary outcomes, requiring generalization for real-world applications
- The concept of "data fairness" lacks strong theoretical grounding in the corpus, raising questions about its interpretability

## Confidence
- Double robustness mechanism for probabilistic metrics: High confidence
- Variance estimation for conditional mutual information: Medium confidence (significant bias observed)
- Interpretability of "data fairness" concept: Low confidence (weak theoretical anchoring)

## Next Checks
1. Test double robustness in simulation with both models misspecified (polynomial of degree 3 with linear models) to quantify coverage breakdown
2. Apply CMI estimator to synthetic data with known conditional independence structure to measure bias magnitude
3. Compare variable importance results from Adult-Income dataset with alternative interpretability methods (permutation importance) to validate Shapley value conclusions