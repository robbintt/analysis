---
ver: rpa2
title: Streaming Non-Autoregressive Model for Accent Conversion and Pronunciation
  Improvement
arxiv_id: '2506.16580'
source_url: https://arxiv.org/abs/2506.16580
tags:
- streaming
- speech
- accent
- audio
- native
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first streaming accent conversion (AC)
  model that transforms non-native speech into native-like accent while preserving
  speaker identity, prosody, and improving pronunciation. The authors modify a non-streaming
  AC architecture by incorporating an Emformer encoder for streaming capability and
  optimize inference with a Voice Activity Detector for chunk-based processing.
---

# Streaming Non-Autoregressive Model for Accent Conversion and Pronunciation Improvement

## Quick Facts
- arXiv ID: 2506.16580
- Source URL: https://arxiv.org/abs/2506.16580
- Reference count: 0
- Primary result: First streaming accent conversion model with 0.8s latency, achieving WER 14.1% vs baseline 14.3%

## Executive Summary
This paper introduces the first streaming accent conversion (AC) system that transforms non-native speech into native-like accent while preserving speaker identity and prosody. The authors modify a non-streaming AC architecture by incorporating an Emformer encoder for streaming capability and optimize inference with a Voice Activity Detector for chunk-based processing. They also integrate a native TTS model to generate ideal ground-truth data for training. The streaming model achieves comparable performance to the non-streaming baseline with WER of 14.1% versus 14.3%, SECS of 0.85 versus 0.84, and MOSNet scores around 4.1. Subjective tests show similar nativeness scores (3.78 vs 3.87) and speaker similarity MOS (3.92 vs 3.96). The streaming model maintains stable latency of 0.8 seconds, making it the first AC system capable of streaming applications like video conferencing.

## Method Summary
The method involves three main components: synthetic ground-truth generation, streaming AC model training, and chunk-based streaming inference. A native VITS TTS generates training targets from phoneme transcripts while preserving speaker identity via speaker embeddings and prosody via F0 extraction. The AC model uses an Emformer encoder with 12 layers, 1024 hidden size, 30 left context, and 8 right look-ahead frames. A WaveNet bottleneck removes accent, and HiFi-GAN V2 decodes the waveform. The model is pretrained on native speech then fine-tuned on non-native input paired with synthetic native ground-truth. Inference uses VAD to detect speech segments, buffers 10 chunks (0.8s) before processing, extracts speaker embedding once per utterance, and maintains cache across processing stages for smooth streaming output.

## Key Results
- Streaming AC model achieves WER of 14.1% vs non-streaming baseline 14.3%
- SECS (speaker similarity) scores 0.85 for streaming vs 0.84 for baseline
- MOSNet scores around 4.1 for both models
- Subjective nativeness scores: 3.78 (streaming) vs 3.87 (baseline)
- Speaker similarity MOS: 3.92 (streaming) vs 3.96 (baseline)
- Stable latency of 0.8 seconds maintained regardless of computational device

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Synthetic ground-truth from a native TTS model provides learnable targets for accent conversion and pronunciation correction while preserving speaker identity and prosody.
- Mechanism: A VITS-based TTS trained exclusively on native speech generates audio from phoneme transcripts (ensuring native pronunciation) conditioned on speaker embeddings and F0 extracted from non-native input (preserving identity and prosody). Montreal Forced Aligner provides temporal alignment. The AC model learns to map non-native acoustic features → native TTS output.
- Core assumption: Native TTS internal representations are accent-independent; the TTS will not introduce accent artifacts when conditioned on non-native speaker embeddings.
- Evidence anchors:
  - [abstract] "integrate a native text-to-speech (TTS) model to generate ideal ground-truth data for efficient training"
  - [section 2.1.2] Equations 1-4 describe the ground-truth generation pipeline using MFA alignment, F0, and speaker embedding g from non-native audio
  - [corpus] Related work (Nguyen et al. ICASSP 2025, referenced as [17]) validates the non-streaming version of this approach; corpus papers on accent generation (arXiv:2601.19786) suggest accent encoding remains an open problem in discrete representations
- Break condition: If the native TTS synthesizes artifacts or if speaker embeddings from accented speech carry accent information into the synthetic output, the AC model may learn incorrect mappings or fail to convert accent fully.

### Mechanism 2
- Claim: Emformer encoder enables streaming content extraction with bounded look-ahead, preserving conversion quality near non-streaming baselines.
- Mechanism: Emformer replaces standard Transformer attention with segment-wise processing using left context (30 frames), right look-ahead (8 frames), and segment size (4 frames). This provides ~0.64s look-ahead while enabling incremental inference. The content encoder remains frozen after pre-training with CTC + prosody losses.
- Core assumption: The 8-frame right context is sufficient for accurate phoneme disambiguation and pronunciation correction; the left context captures necessary coarticulation information.
- Evidence anchors:
  - [abstract] "modifying a previous AC architecture with an Emformer encoder"
  - [section 2.2.1] "Emformer is trained with 12 layers, a hidden size of 1024, a right look-ahead context of 8, a segment size of 4, and a left context of 30"
  - [corpus] Emformer is established for streaming ASR (TorchEmformer, cited as [26-29]); corpus lacks direct comparisons of Emformer vs. causal alternatives for AC specifically
- Break condition: If the look-ahead window is insufficient for disambiguating phonemes that require longer context (e.g., homophones resolved by later words), pronunciation accuracy will degrade.

### Mechanism 3
- Claim: Chunk-based inference with cached states and VAD-gated buffering produces consistent streaming output at ~0.8s latency.
- Mechanism: VAD identifies speech segments. The system buffers 10 chunks (0.8s) before processing, extracts speaker embedding once per utterance, and maintains cache across Emformer, WaveNet bottleneck, and HiFi-GAN decoder. Playback begins after the second output chunk to ensure smooth streaming.
- Core assumption: 0.8s latency is acceptable for target applications (video conferencing); speaker identity is consistent within a single utterance and does not require per-chunk updates.
- Evidence anchors:
  - [abstract] "maintaining stable latency"
  - [section 2.2.3, Algorithm 1] Detailed streaming inference logic; "latency stays around 0.8s, regardless of the computational device"
  - [corpus] StreamVC (arXiv:2504.20678 referenced as [39]) demonstrates real-time low-latency voice conversion but uses causal architecture; direct latency comparisons unavailable
- Break condition: If VAD produces false negatives (missed speech) or if cache invalidation is mishandled across chunk boundaries, output will have artifacts or dropped segments.

## Foundational Learning

- Concept: **Streaming vs. Non-Streaming Architectures**
  - Why needed here: The core contribution is adapting a non-streaming AC model to streaming; understanding the architectural constraints (look-ahead, causal vs. non-causal) is essential.
  - Quick check question: Can a causal model achieve comparable WER to a non-causal model for accent conversion? (Answer per paper: No—Table 2 shows causal streaming model WER 33.5% vs. streaming model 14.1%)

- Concept: **Knowledge Distillation via Synthetic Ground Truth**
  - Why needed here: The training pipeline depends on generating ideal targets from a native TTS rather than requiring parallel non-native/native recordings from the same speaker.
  - Quick check question: What three properties must the synthetic ground-truth preserve from the non-native input? (Answer: Speaker identity via embedding g, prosody via F0, duration via MFA alignment)

- Concept: **Non-Autoregressive Speech Generation**
  - Why needed here: The model uses a conventional autoencoder + HiFi-GAN rather than seq2seq with attention, enabling parallel frame-level generation suitable for streaming.
  - Quick check question: Why does the paper reject CVAE for this task? (Answer: AC/VC is one-to-one mapping from continuous content to audio, unlike TTS which is one-to-many; CVAE is not strictly necessary)

## Architecture Onboarding

- Component map:
  - **Content Encoder (Emformer)**: Frozen after pre-training; extracts linguistic content + F0 prosody from input audio; 12 layers, 1024 hidden, 30 left / 8 right context
  - **Bottleneck Extractor (WaveNet-based)**: Non-causal dilated convolutions; removes accent, corrects pronunciation
  - **HiFi-GAN V2 Decoder**: Generates waveform from bottleneck output + speaker embedding
  - **Speaker Encoder**: Pre-trained; extracts embedding g from first 0.8s, fixed per utterance
  - **VAD**: Gates chunk buffering; determines speech boundaries
  - **Native TTS (VITS)**: Offline; generates training ground truth (not used at inference)

- Critical path:
  1. Pre-train Emformer content encoder on LibriSpeech + L2Arctic with CTC + prosody losses (α=0.2 for prosody)
  2. Pre-train full AC model on native multi-speaker data (identity mapping)
  3. Fine-tune on non-native input → synthetic native ground truth (3:1 non-native:native ratio)
  4. At inference: VAD detects speech → buffer 10 chunks (0.8s) → extract speaker embedding → process with cached states → stream output

- Design tradeoffs:
  - Look-ahead vs. latency: 0.64s look-ahead enables accurate conversion but imposes 0.8s minimum latency; reducing look-ahead would lower latency but risks pronunciation errors
  - Non-causal vs. causal: Paper demonstrates causal model fails (WER 33.5%, MOSNet 3.01); non-causal with look-ahead is necessary for AC
  - Frozen content encoder: Reduces training complexity but may limit adaptation to domain-specific accents

- Failure signatures:
  - High WER on converted speech → insufficient look-ahead or poor content encoder pre-training
  - Speaker identity drift → speaker embedding extraction quality or HiFi-GAN over-smoothing
  - Chunk boundary artifacts → cache mishandling or VAD edge cases
  - Accent not fully converted → synthetic ground truth quality issues or accent leakage through prosody preservation (noted in ACC results)

- First 3 experiments:
  1. **Baseline replication**: Implement the non-streaming AC model from [17] on L2-ARCTIC, verify WER ~14.3% and SECS ~0.84 to establish equivalence
  2. **Ablation on look-ahead context**: Train streaming models with right context {4, 8, 12, 16} frames; measure WER and latency tradeoff curve
  3. **Speaker embedding extraction timing**: Compare embedding extraction at utterance start (0.8s buffered) vs. rolling per-chunk extraction; measure SECS stability and computational overhead

## Open Questions the Paper Calls Out

- **Question**: Can the look-ahead latency be reduced below 0.8 seconds without degrading the model's ability to correct pronunciation?
  - Basis in paper: [explicit] The authors explicitly state that future work will focus on "minimizing look-ahead latency while maintaining audio quality," noting the current latency is not yet "low-latency."
  - Why unresolved: The paper demonstrates that the causal streaming model (no look-ahead) fails completely (WER 33.5%), suggesting a strict dependency on the 0.64s look-ahead context for pronunciation accuracy.
  - What evidence would resolve it: A parametric study evaluating the performance drop (WER and MOS) as the look-ahead window is incrementally reduced from 0.64s toward zero.

- **Question**: How can the system be optimized for real-time performance on low-power consumer hardware, such as standard CPUs?
  - Basis in paper: [explicit] The conclusion lists "optimizing inference for low-power computing devices such as CPUs" as a specific goal for future research.
  - Why unresolved: The current efficiency benchmarks (RTF ~0.25) are reported on a GTX 1060 GPU; the computational feasibility of the Emformer and HiFi-GAN components on CPU-only devices remains unverified.
  - What evidence would resolve it: Benchmarks showing real-time factors (RTF < 1) on standard mobile or desktop CPUs, potentially achieved via model quantization or pruning.

- **Question**: To what extent does preserving the original non-native prosody limit the effectiveness of the accent conversion?
  - Basis in paper: [inferred] The authors note that their model's Accent Classifier Accuracy (ACC) is weaker than other metrics, hypothesizing that "prosody preservation may... reduce the effectiveness of accent conversion."
  - Why unresolved: There is a potential conflict between the goal of preserving the user's original prosody (F0 patterns) and the goal of sounding "native-like," as non-native prosody may signal a foreign accent even if pronunciation is corrected.
  - What evidence would resolve it: An ablation study comparing the current approach against a version where prosody is converted to a native style, measuring the trade-off between speaker similarity and perceived nativeness.

## Limitations

- The 0.8s minimum latency, while stable, may be prohibitive for ultra-low-latency applications like real-time gaming or certain telecommunication scenarios.
- The model's effectiveness depends on the assumption that accented speech speaker embeddings do not carry accent-correlated features into the synthetic ground-truth, which could limit pronunciation correction if violated.
- The frozen Emformer content encoder may restrict the model's ability to adapt to domain-specific accents or speaker variability beyond the pre-training corpus.

## Confidence

- **High confidence**: The streaming architecture achieves comparable quality to non-streaming baseline (WER 14.1% vs 14.3%, SECS 0.85 vs 0.84, MOSNet ~4.1) with stable latency of 0.8s.
- **Medium confidence**: The synthetic ground-truth generation effectively preserves speaker identity and prosody while providing learnable accent conversion targets.
- **Low confidence**: The streaming model maintains speaker identity consistently across varying utterance lengths and VAD conditions.

## Next Checks

1. **Look-ahead context ablation study**: Systematically evaluate streaming models with right contexts {4, 8, 12, 16} frames to quantify the WER-latency tradeoff curve and identify the minimum look-ahead for acceptable pronunciation quality.

2. **Speaker identity stability analysis**: Test the streaming model across utterances of varying lengths (1-30 seconds) and with different VAD configurations to measure SECS variance and identify failure modes in cache management or speaker embedding extraction.

3. **Accent encoding validation**: Conduct controlled experiments using accented TTS-generated ground-truth (e.g., non-native TTS trained on accented speech) to quantify how accent-correlated features in speaker embeddings affect the AC model's conversion capability.