---
ver: rpa2
title: Reduced-Basis Deep Operator Learning for Parametric PDEs with Independently
  Varying Boundary and Source Data
arxiv_id: '2511.18260'
source_url: https://arxiv.org/abs/2511.18260
tags:
- deeponet
- reduced
- trunk
- boundary
- basis
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RB-DeepONet, a hybrid operator-learning framework
  that combines reduced-basis (RB) numerical structure with the branch-trunk architecture
  of DeepONet. The method addresses the computational bottleneck in many-query parametric
  PDEs by fixing the trunk to a rigorously constructed RB space from Greedy selection,
  while the branch network predicts only RB coefficients trained label-free via projected
  variational residual.
---

# Reduced-Basis Deep Operator Learning for Parametric PDEs with Independently Varying Boundary and Source Data

## Quick Facts
- arXiv ID: 2511.18260
- Source URL: https://arxiv.org/abs/2511.18260
- Reference count: 40
- Primary result: RB-DeepONet achieves competitive accuracy with 2×10⁵ parameters vs 7×10⁵-1.3×10⁶ in baseline DeepONets

## Executive Summary
This paper introduces RB-DeepONet, a hybrid operator-learning framework that combines reduced-basis (RB) numerical structure with the branch-trunk architecture of DeepONet. The method addresses the computational bottleneck in many-query parametric PDEs by fixing the trunk to a rigorously constructed RB space from Greedy selection, while the branch network predicts only RB coefficients trained label-free via projected variational residual. The framework handles independently varying boundary and source data through boundary and source modal encodings, enabling strict offline-online separation with online cost scaling only with the RB dimension.

## Method Summary
RB-DeepONet fixes the trunk network to a reduced-basis basis constructed via Greedy selection, while the branch network learns to predict RB coefficients. The key innovation is training the branch network using a projected variational residual loss that targets the RB-Galerkin solution without requiring labeled solution data. For problems with independently varying boundary and source data, the method employs modal encoding via Empirical Interpolation Method to project these data onto reduced spaces constructed offline. This enables strict offline-online separation where online inference requires only evaluating the trained branch network and reconstructing the solution from the RB basis.

## Key Results
- Achieves mean relative L² errors around 1% with 95th-percentile errors below 6%
- Uses roughly 2×10⁵ trainable parameters versus 7×10⁵-1.3×10⁶ in baseline DeepONets
- Maintains accuracy competitive with intrusive RB-Galerkin, POD-DeepONet, and FEONet
- Enables strict offline-online separation with online cost scaling only with RB dimension
- Successfully handles independently varying boundary and source data through modal encodings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fixing the trunk to a rigorously constructed RB basis enables certified error control and dramatically reduces trainable parameters.
- Mechanism: The trunk basis is constructed offline via Greedy selection using a posteriori error estimators, guaranteeing $\|w(k) - w_{rb}(k)\|_V \leq \eta(k)$ for all $k \in D$. The branch network then predicts only $N$ RB coefficients instead of $N_0 \gg N$ FE coefficients.
- Core assumption: The solution manifold admits rapid Kolmogorov $N$-width decay, enabling accurate approximation with few modes.
- Evidence anchors:
  - [section 3.1]: "The space is grown iteratively, one truth snapshot per iteration, until a certified error indicator falls below a prescribed tolerance."
  - [section 5.1]: "With only $N=3$ modes, the RB–Galerkin reference already achieves mean rel-$L^2$ of order $10^{-7}$."
  - [corpus]: Related work ReBaNO similarly exploits reduced basis structure for discretization invariance.

### Mechanism 2
- Claim: Projected variational residual training eliminates the need for labeled solution data while guaranteeing convergence to RB–Galerkin solutions.
- Mechanism: The loss measures the departure from Galerkin equilibrium in the reduced space. If loss equals zero, then predicted coefficients coincide exactly with RB–Galerkin coefficients.
- Core assumption: The RB stiffness matrix has uniform spectral bounds for all parameters in the domain.
- Evidence anchors:
  - [abstract]: "The branch network predicts only RB coefficients and is trained label-free using a projected variational residual that targets the RB–Galerkin solution."
  - [section 3.3]: "This is exactly the intrusive Galerkin equilibrium condition projected onto the reduced basis."
  - [corpus]: PILNO similarly uses low-rank structure with physics-informed training.

### Mechanism 3
- Claim: Modal encoding of boundary and source data enables strict offline-online separation while controlling projection error.
- Mechanism: Boundary data is projected onto a Greedy-constructed space, yielding coordinates. Source functionals are mapped to Riesz representers and projected onto another space. Error bounds certify truncation.
- Core assumption: Boundary and source data lie in (or near) the span of the offline snapshot ensembles.
- Evidence anchors:
  - [section 3.4.1]: "For tolerances $\epsilon_g, \epsilon_f$, the boundary and source projections contribute at most $\mathcal{O}(\epsilon_g)$ and $\mathcal{O}(\epsilon_f)$ to the $V$-norm error."
  - [section 5.2]: "The RB trunk and the boundary/source modes are accurate for this Case II setting."
  - [corpus]: No directly comparable corpus work addresses independently varying data.

## Foundational Learning

- Concept: **Variational formulation and Galerkin projection**
  - Why needed here: The entire framework hinges on the weak form and projecting onto reduced spaces. Without this, residual-based training has no meaning.
  - Quick check question: Can you derive the weak form of $-\nabla \cdot (\kappa \nabla u) = f$ with Dirichlet/Neumann boundaries, and explain why Galerkin projection preserves well-posedness?

- Concept: **Reduced basis error estimation (a posteriori)**
  - Why needed here: Greedy selection uses residual-based error estimators to decide which snapshots enrich the basis. Understanding dual norms is essential.
  - Quick check question: Given a residual functional $r(v;k) = F(k)[v] - a(w_{rb}(k), v; k)$, how would you compute $\|r(\cdot;k)\|_{V_0'}$ using a reference inner product?

- Concept: **DeepONet branch-trunk decomposition**
  - Why needed here: RB–DeepONet modifies this architecture by fixing the trunk to RB basis and learning only branch weights.
  - Quick check question: Why does fixing the trunk reduce the learning problem from approximating $u(x;k)$ to approximating $\mathbf{c}(k) \in \mathbb{R}^N$?

## Architecture Onboarding

- Component map: FEM solver -> Greedy selection -> RB trunk basis -> Reduced operators -> Branch MLP -> Residual loss minimization
- Critical path:
  1. Implement Greedy selection (Algorithm 1) with residual-based error indicator.
  2. Assemble and store all reduced operators offline.
  3. Train branch network on residual loss.
  4. Validate on held-out parameters; monitor rel-$L^2$, rel-energy, rel-residual.

- Design tradeoffs:
  - **Greedy vs. POD trunk**: Greedy needs $N \ll N_k$ solves with certification; POD needs all $N_k$ snapshots but gives optimal mean-square projection.
  - **Trunk dimension $N$**: Larger $N$ reduces RB discretization error but increases branch network size and online cost.
  - **Modal truncation $(r_g, r_f)$**: Tighter tolerances reduce projection error but increase online coordinate dimension.

- Failure signatures:
  - Stagnant training loss: Check uniform coercivity; $A_{rb}(k)$ may be ill-conditioned for some $k$.
  - Large rel-residual but small rel-$L^2$: Network may be minimizing residual in wrong norm.
  - Case II generalization gap: Boundary/source projection error dominates.

- First 3 experiments:
  1. **Baseline elliptic problem**: Reproduce heat conduction case with $N=3$ modes. Verify mean rel-$L^2 < 5\times10^{-3}$ with $\sim$2×10⁵ parameters.
  2. **Ablation on trunk construction**: Compare Greedy vs. POD trunks at fixed $N$ for the same snapshot budget.
  3. **Independent data test**: Implement boundary/source modal encoding for diffusion-reaction with $(N, r_f, r_g) = (209, 128, 16)$.

## Open Questions the Paper Calls Out
None

## Limitations
- The method assumes the solution manifold has rapidly decaying Kolmogorov widths; performance degrades for convection-dominated or high-frequency problems
- Generalization to arbitrarily varying boundary and source data depends heavily on the quality of modal encoding and coverage of offline snapshots
- Exact implementation details of Greedy selection and EIM are not fully specified, creating reproducibility challenges

## Confidence

- **High confidence**: The theoretical framework connecting RB-Galerkin solutions to zero-residual training loss is sound. The demonstrated parameter reduction and accuracy preservation are well-supported.
- **Medium confidence**: The offline-online decomposition's computational benefits assume the RB dimension remains small relative to FE dimension.
- **Low confidence**: The generalization of Case II to arbitrarily varying boundary and source data depends heavily on modal encoding quality.

## Next Checks

1. **Trunk construction verification**: Implement the Greedy algorithm and verify that the residual error indicator decays by several orders of magnitude within the first few iterations.

2. **Modal encoding sensitivity**: For Example 5.2, systematically vary the truncation tolerances (ε_g, ε_f) and observe the trade-off between projection error and online computational cost.

3. **Generalization robustness**: Test the trained RB-DeepONet on boundary/source data that is adversarially chosen to be far from the span of offline snapshots, measuring the breakdown point where projection error dominates.