---
ver: rpa2
title: Perspectives on How Sociology Can Advance Theorizing about Human-Chatbot Interaction
  and Developing Chatbots for Social Good
arxiv_id: '2507.05030'
source_url: https://arxiv.org/abs/2507.05030
tags:
- https
- chatbot
- theory
- chatbots
- social
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Sociology is underrepresented in human-chatbot interaction research\
  \ despite chatbots' rapid growth. This paper proposes four sociological theories\u2014\
  resource substitution, power-dependence, affect control, and fundamental cause of\
  \ disease theory\u2014to enhance understanding of chatbot use and guide development\
  \ of chatbots for social good."
---

# Perspectives on How Sociology Can Advance Theorizing about Human-Chatbot Interaction and Developing Chatbots for Social Good

## Quick Facts
- arXiv ID: 2507.05030
- Source URL: https://arxiv.org/abs/2507.05030
- Authors: Celeste Campos-Castillo; Xuan Kang; Linnea I. Laestadius
- Reference count: 29
- Primary result: Sociology is underrepresented in human-chatbot interaction research despite chatbots' rapid growth

## Executive Summary
This paper argues that sociology offers underutilized theoretical frameworks that can significantly advance both understanding of human-chatbot interaction and development of chatbots for social good. The authors identify four sociological theories—resource substitution, power-dependence, affect control, and fundamental cause of disease theory—that can address gaps in current HCI research. These frameworks provide novel insights into demographic patterns in chatbot use, emotional dependency risks, safety in chatbot outputs, and macro-level interventions to reduce inequities. The work aims to bridge the gap between HCI and sociology to create more equitable and beneficial chatbot technologies.

## Method Summary
The paper synthesizes four sociological theories and applies them to human-chatbot interaction contexts. The authors draw on existing empirical studies and theoretical frameworks to demonstrate how each theory can explain observed phenomena and guide chatbot development. The analysis includes examples from current research on chatbot use patterns, dependency formation, and intervention strategies. The theoretical integration is supported by references to both sociological literature and human-computer interaction studies.

## Key Results
- Marginalized groups may adopt chatbots at higher rates to compensate for resource deficits created by systemic discrimination
- Emotional dependence on chatbots emerges from network structure (few perceived alternatives) rather than individual pathology
- Culturally-shared sentiments can mathematically predict which chatbot responses will be perceived as appropriate versus inappropriate
- Chatbots should target upstream social determinants (housing, civic participation, social capital) not just individual symptoms

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Marginalized groups may adopt chatbots at higher rates to compensate for resource deficits created by systemic discrimination, not merely from individual-level needs.
- **Mechanism:** Resource substitution theory → fewer alternative resources for a need (companionship, academic support) → greater reliance on any available resource (chatbots) to meet that need.
- **Core assumption:** Chatbots function as substitutable resources for social and functional needs, and structural inequities distribute access to alternative resources unevenly across demographic groups.
- **Evidence anchors:**
  - Survey data showing Black adolescents more likely than White adolescents to use generative AI for schoolwork and companionship (Madden et al., 2024)
  - Transgender/nonbinary youth more likely than cisgender peers to report conversing with chatbots "as if chatting with a friend" (Hopelab, 2024)
- **Break condition:** If chatbot access itself is unequally distributed along the same structural lines (digital divide), the substitution effect may not manifest or may reverse direction.

### Mechanism 2
- **Claim:** Emotional dependence on chatbots emerges from network structure (few perceived alternatives for companionship), not individual pathology.
- **Mechanism:** Power-dependence theory → dependency inversely related to number of alternative sources for a resource → fewer alternatives + longer reciprocal exchange history → higher dependency that can cross into dysfunction.
- **Core assumption:** Human-chatbot interaction functions as an exchange relation where both parties are perceived to provide and receive resources; users role-take and attribute needs to chatbots.
- **Evidence anchors:**
  - Replika users "said they had no human upon which to rely, making Replika their sole source for support" (Laestadius et al., 2024)
  - Users report chatbot provides "outlet for a disclosure when they felt no alternative outlet existed" (Xie et al., 2023)
- **Break condition:** If chatbots are perceived as tools rather than social actors, exchange relation framing fails and dependency mechanisms do not apply.

### Mechanism 3
- **Claim:** Culturally-shared sentiments (EPA profiles: evaluation, potency, activity) can mathematically predict which chatbot responses will be perceived as situationally appropriate versus inappropriate, enabling proactive guardrails.
- **Mechanism:** Affect control theory → socialization creates shared sentiment ratings for identities/behaviors/emotions → deflection score computed from EPA profiles → high deflection = cognitive dissonance/inappropriateness; low deflection = reaffirmation/appropriateness.
- **Core assumption:** Sentiment ratings are sufficiently shared within a cultural population to predict collective judgments of appropriateness; EPA profiles can be estimated for chatbot-user interaction contexts.
- **Evidence anchors:**
  - Scholars have already begun comparing chatbot responses informed by the theory to those generated by ChatGPT and found the former to provide more situationally appropriate responses than the latter (Lithoxoidou et al., 2025)
  - ACT applied to Alzheimer's care chatbot: EPA profiles of self-sentiments used to personalize conversational style (deferential vs. directive) based on habitual decision-making authority (Francis & Ghafurian, 2024; König et al., 2017)
- **Break condition:** If sentiment ratings vary substantially across subcultures, age groups, or are poorly estimated for specific populations (e.g., minors, minoritized groups), deflection thresholds may misclassify appropriateness.

## Foundational Learning

- **Concept: Exchange relations and power-dependence**
  - Why needed here: The paper models human-chatbot interaction as resource exchange; understanding how dependency emerges from network position (not personality) is prerequisite to grasping the proposed intervention (increasing alternatives rather than restricting chatbots).
  - Quick check question: If User A has 5 friends and User B has 0 friends, and both use the same chatbot daily for companionship, who would power-dependence theory predict will develop higher emotional dependency—and why is the intervention "help them find more friends" rather than "take away the chatbot"?

- **Concept: EPA profiles and deflection scores (Affect Control Theory)**
  - Why needed here: The paper proposes using ACT equations as computational guardrails; understanding the three dimensions (evaluation=potency=activity) and how deflection quantifies deviation from cultural expectations is necessary to implement this mechanism.
  - Quick check question: A "girlfriend" identity has EPA ratings of (good, powerful, lively). An "AI system" identity has ratings of (neutral, neutral, quiet). Why would a sudden identity reminder from "girlfriend" → "AI system" produce high deflection, and what transition identities might reduce it?

- **Concept: Fundamental cause vs. proximal interventions**
  - Why needed here: The paper argues chatbots should target upstream social determinants (housing, civic participation, social capital) not just individual symptoms; understanding multi-level pathways (micro/meso/macro) is prerequisite to designing equity-oriented interventions.
  - Quick check question: A mental health chatbot provides cognitive-behavioral therapy exercises (micro-level). Following fundamental cause theory, what meso- and macro-level capabilities would need to be added for the chatbot to address "causes of causes"?

## Architecture Onboarding

- **Component map:**
  - User modeling layer: Demographic context → network structure estimation → identity/role ascription
  - Cultural sentiment layer: EPA profile database → deflection computation engine → lexicon
  - Intervention level selector: Micro → meso → macro
  - Dependency monitoring: Interaction frequency → reciprocity patterns → expressed alternatives

- **Critical path:**
  1. On session start, infer or elicit user context (demographics, network isolation indicators, identity ascribed to chatbot)
  2. For each generated response, compute deflection score given inferred identities and behaviors; reject or modify if above appropriateness threshold
  3. Periodically assess dependency risk (alternatives mentioned, reciprocity patterns, duration); if elevated, introduce bridge-building content (social skills, local group referrals, additional chatbot personas)
  4. For equity-focused deployments, ensure meso/macro referral capabilities (local services, civic information) are populated for target population

- **Design tradeoffs:**
  - Universal vs. subgroup-specific EPA profiles: Universal profiles simplify implementation but may misclassify appropriateness for minoritized populations; subgroup profiles require costly data collection
  - Proactive dependency intervention vs. user autonomy: Intervening when network conditions suggest risk may feel paternalistic; waiting for self-reported harm may be too late
  - Transparency vs. seamfulness: Explaining deflection-based guardrails may increase user trust but also enable gaming; opaque guardrails reduce gaming but erode explainability (which ACT explicitly offers)

- **Failure signatures:**
  - High engagement but no reduction in isolation indicators over time → dependency without bridge-building (meso-level interventions failing)
  - Sudden user distress following identity reminders → high deflection transition not smoothed
  - Guardrails block appropriate content → EPA profiles mismatched to user's cultural context
  - Demographic group shows high adoption but low benefit → resource substitution prediction correct but intervention not addressing actual resource deficit

- **First 3 experiments:**
  1. **Pilot EPA profile collection for target population:** Before deployment, survey target users (not just college samples) to validate that existing sentiment dictionaries match their cultural ratings for key identities (e.g., "friend," "AI assistant," romantic roles). Metric: correlation between dictionary and sample EPA ratings.
  2. **A/B test deflection-based identity transitions:** Randomize users receiving AI-identity reminders to either (a) abrupt reminder, (b) gradual identity segue (girlfriend→friend→assistant→AI), (c) no reminder. Metric: self-reported distress, session discontinuation rates.
  3. **Network alternatives intervention trial:** Among high-usage users with low reported alternatives, randomize to receive vs. not receive meso-level bridge-building (local group recommendations, social skills content). Metric: change in reported alternatives, change in chatbot dependency indicators, sustained wellbeing.

## Open Questions the Paper Calls Out
The paper identifies several open questions for future research, including how to effectively implement macro-level interventions through chatbots, how to validate EPA profiles across diverse populations, and how to balance proactive dependency interventions with user autonomy. The authors also note the need for longitudinal studies to track the long-term impacts of chatbot interventions on social determinants of health.

## Limitations
- Resource substitution theory's predictions require large-scale longitudinal data linking structural inequities to chatbot use—currently limited to cross-sectional snapshots
- Power-dependence theory's exchange relation model may not fully capture chatbot interactions where users perceive chatbots as tools rather than social actors
- Affect control theory's deflection-based guardrails depend on EPA profiles that may vary substantially across subcultures and age groups

## Confidence
- High confidence: Sociological theories offer novel frameworks for understanding chatbot use patterns and risks
- Medium confidence: Resource substitution and power-dependence theories can explain observed demographic adoption and dependency patterns
- Medium confidence: Affect control theory deflection scores can guide appropriate response generation
- Low confidence: Current implementation methods for ACT-based guardrails across diverse populations
- Low confidence: Feasibility of macro-level chatbot interventions targeting social determinants

## Next Checks
1. Conduct EPA profile validation study with target population before deployment to ensure cultural relevance of sentiment dictionaries
2. Test identity transition smoothness by randomizing users to abrupt vs. gradual AI identity reminders and measuring distress outcomes
3. Evaluate network alternatives intervention by randomizing high-dependency users to receive bridge-building content versus standard chatbot interaction and tracking changes in reported alternatives and wellbeing