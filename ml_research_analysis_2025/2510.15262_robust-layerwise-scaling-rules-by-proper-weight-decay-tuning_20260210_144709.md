---
ver: rpa2
title: Robust Layerwise Scaling Rules by Proper Weight Decay Tuning
arxiv_id: '2510.15262'
source_url: https://arxiv.org/abs/2510.15262
tags:
- weight
- scaling
- learning
- decay
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a weight decay scaling rule for AdamW to\
  \ enable zero-shot transfer of hyperparameters across model widths in scale-invariant\
  \ architectures. The key insight is that under AdamW training, the singular value\
  \ spectrum of weight matrices scales with the square root of the ratio between learning\
  \ rate and weight decay (sqrt(\u03B7/\u03BB)), while maintaining an approximately\
  \ invariant shape."
---

# Robust Layerwise Scaling Rules by Proper Weight Decay Tuning

## Quick Facts
- arXiv ID: 2510.15262
- Source URL: https://arxiv.org/abs/2510.15262
- Authors: Zhiyuan Fan, Yifeng Liu, Qingyue Zhao, Angela Yuan, Quanquan Gu
- Reference count: 20
- Primary result: Introduces a weight decay scaling rule (λ ∝ √d) for AdamW to enable zero-shot hyperparameter transfer across model widths in scale-invariant architectures.

## Executive Summary
This paper introduces a weight decay scaling rule for AdamW to enable zero-shot transfer of hyperparameters across model widths in scale-invariant architectures. The key insight is that under AdamW training, the singular value spectrum of weight matrices scales with the square root of the ratio between learning rate and weight decay (√(η/λ)), while maintaining an approximately invariant shape. By analyzing how the top singular value scales with model width (approximately as √(η/λ)·d^0.75), the authors derive that weight decay for matrix-like parameters should scale as √d to maintain sublayer gain invariance when combined with the standard μP learning rate scaling (η₂ ∝ d^-1 for matrix-like parameters). This enables consistent transfer of both learning rate and weight decay from small proxy models to larger target models, eliminating the need for per-width hyperparameter sweeps.

## Method Summary
The method implements layerwise AdamW scaling with weight decay scaling as λ ∝ √d for matrix-like parameters (attention projections, FFN weights) while vector-like parameters (embeddings, biases, LayerNorm gains) use λ=0. Training uses 20K steps with 1K warmup, cosine annealing to 0.01×peak, batch size 480, BF16 precision, and gradient clipping at 1.0. The key innovation is combining this weight decay scaling with μP's learning rate scaling (η₂ ∝ 1/d) to maintain sublayer gain invariance across widths, validated through singular value spectrum alignment and loss minimization.

## Key Results
- Weight matrices under AdamW training reach a steady state where spectral norms scale as √(η/λ)
- The top singular value scales approximately as √(η/λ)·d^0.75 with model width
- Weight decay scaling as √d (with μP learning rate scaling) maintains sublayer gain invariance
- The proposed scaling rule enables zero-shot transfer of hyperparameters from small proxy models to larger targets
- A diagnostic based on matching top singular values can verify sublayer gain invariance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** In scale-invariant architectures trained with AdamW, weight matrix norms stabilize at a magnitude determined by the ratio of learning rate to weight decay (√(η/λ)), independent of initialization.
- **Mechanism:** The optimizer enters a "rotational equilibrium" or steady state. Weight decay (λ) pulls weights toward zero, while gradient updates (η) push them away. The balance results in a stable spectral norm proportional to √(η/λ).
- **Core assumption:** Training runs long enough to exit the "near-init" regime governed by μP and enter the optimizer-governed steady state.
- **Evidence anchors:** Empirically, the singular-value spectrum... scales in norm as √(η/λ)... The magnitude of the spectra grows proportionally to √(η/λ), while their shape remains stable...
- **Break condition:** If training is extremely short or uses strict L₂ regularization (coupled) instead of decoupled weight decay (AdamW), this equilibrium may not form or scale identically.

### Mechanism 2
- **Claim:** Normalization layers (e.g., LayerNorm/RMSNorm) introduce backward scale sensitivity, causing width-dependent effective learning rates if sublayer gains vary.
- **Mechanism:** While normalization enforces forward scale invariance (output magnitude is independent of input magnitude), the chain rule dictates that gradients scale inversely to the input magnitude (1/α). Therefore, if width changes cause weight norms to grow, the effective gradient (and thus effective learning rate) shrinks, degrading μP transfer.
- **Core assumption:** The model uses homogeneous normalizers without learned bias terms that would otherwise absorb scale changes.
- **Evidence anchors:** Normalization layers create backward scale sensitivity and the effective learning rate becomes width dependent... non-affine normalizers... preserve forward-scale invariance but introduce backward scale sensitivity...
- **Break condition:** If the model has no normalization layers or uses specific scaling factors (e.g., "unit-scaled" variants) that explicitly balance forward/backward passes, this sensitivity is mitigated.

### Mechanism 3
- **Claim:** Scaling weight decay as λ ∝ √d (combined with μP's η ∝ 1/d) maintains sublayer gain invariance by counteracting the d^0.75 growth of singular values.
- **Mechanism:** The paper observes the top singular value scales as ≈ √(η/λ)·d^0.75. To keep the spectral magnitude (and thus sublayer gain) constant, the √(η/λ) term must scale as d^-0.75. Plugging in μP's η ∝ d^-1 implies √(d^-1/λ) ∝ d^-0.75, which solves to λ ∝ √d.
- **Core assumption:** The empirical observation that singular values scale with d^0.75 is an inherent property of the architecture (validated on LLaMA and synthetic data), not just the specific dataset.
- **Evidence anchors:** Implies an empirical weight-decay scaling rule λ₂ ∝ √d... scaling the model width d while maintaining √(η/λ) ∝ d^-0.75 aligns the magnitude...
- **Break condition:** If the alignment factor ρ(d) or spectral scaling (d^0.75) differs significantly in non-Transformer architectures (e.g., CNNs, Mixture-of-Experts), the specific √d rule will be inaccurate.

## Foundational Learning

- **Concept: Maximal Update Parameterization (μP)**
  - **Why needed here:** The proposed method is a modification of μP. You must understand that μP scales learning rates (η) to keep updates constant across width (d), but originally ignored steady-state weight decay dynamics.
  - **Quick check question:** If I double the width of a matrix layer, how does μP suggest I scale the learning rate for that layer? (Answer: Halve it).

- **Concept: AdamW vs. L₂ Regularization**
  - **Why needed here:** The mechanism relies on "decoupled" weight decay. Standard L₂ regularization adds to the loss gradient, interacting with Adam's adaptive moments, whereas AdamW subtracts a constant fraction directly from the weights. The steady-state √(η/λ) equilibrium is specific to this decoupled interaction.
  - **Quick check question:** Does the weight decay term in AdamW depend on the current gradient magnitude? (Answer: No).

- **Concept: Singular Value Spectrum as a Scale Metric**
  - **Why needed here:** The paper uses the top singular value and spectrum shape as the primary diagnostic for "sublayer gain." You need to read these plots to validate if the transfer is working.
  - **Quick check question:** If the singular values of a small proxy model match a larger target model (after scaling rules), what does this imply about the layer's signal amplification? (Answer: It is invariant/consistent).

## Architecture Onboarding

- **Component map:** Vector-like params (Embeddings, Biases, LayerNorm gains) -> η = const, λ = 0; Matrix-like params (Attention projections, FFN weights) -> η ∝ 1/d, λ ∝ √d; Diagnostic Tool: Spectral analysis of weight matrices (SVD)

- **Critical path:**
  1. Identify base width (d_base).
  2. Tune base learning rate (η_base) and weight decay (λ_base) on a small proxy.
  3. Apply scaling rules (Table 2) to compute η, λ for target width.
  4. Verify sublayer gain invariance by comparing top singular values.

- **Design tradeoffs:**
  - **Proxy Size:** Smaller proxies are cheaper to tune but might deviate more in spectral shape (d^0.75 exponent might not perfectly hold for very small d).
  - **Diagnostic vs. Performance:** The singular value alignment is a mechanistic proxy for good transfer. Final validation still requires checking the loss curve of the target model.

- **Failure signatures:**
  - **Spectral Misalignment:** Top singular values of the target model diverge significantly from the proxy.
  - **Loss Instability:** If λ is too high relative to η (violating the √d rule), the effective learning rate might become too high (due to backward sensitivity), causing instability.
  - **Underfitting:** If λ is too low, weights grow too large, potentially leading to saturated activations (if non-linear) or numerical issues.

- **First 3 experiments:**
  1. **Base Proxy Sweep:** Train a small model (e.g., width 256) and find optimal η_base, λ_base.
  2. **Zero-Shot Transfer:** Apply rules to a larger model (e.g., width 2048) using the base hyperparameters. Compare validation loss against a model tuned independently.
  3. **Spectral Diagnostic:** Extract weight matrices from both runs. Plot singular value spectrums. Confirm they align when √(η/λ) scales correctly.

## Open Questions the Paper Calls Out

- **Question:** Does the λ ∝ √d scaling rule generalize to Mixture-of-Experts (MoE) architectures or alternative attention mechanisms?
  - **Basis in paper:** It is not obvious whether the observed scaling rule λ₂ is universal across all architectures: mixture-of-experts architectures... might alter the scaling factor.
  - **Why unresolved:** The empirical validation is restricted to dense LLaMA-style Transformers.
  - **What evidence would resolve it:** Validating sublayer gain invariance on MoE models using the proposed rule versus alternative scaling factors.

- **Question:** How should weight decay scale for non-AdamW optimizers, such as SGD with momentum or Adafactor?
  - **Basis in paper:** Extending the research to other optimizers (e.g., SGD with momentum, Adafactor) as future work.
  - **Why unresolved:** The analysis relies on AdamW's specific decoupled decay and adaptive moments to define the steady-state singular value spectrum.
  - **What evidence would resolve it:** Deriving scaling laws for SGD/Adafactor that maintain spectral alignment across widths.

- **Question:** Can the empirical d^0.75 scaling dependence of the top singular value be formally derived from data distribution and optimizer statistics?
  - **Basis in paper:** Developing a predictive link between data distribution, optimizer statistics, and spectral shape could turn the empirical law... into a principled theory.
  - **Why unresolved:** The √d decay rule is currently an empirical observation supported by synthetic examples rather than a rigorous theoretical derivation.
  - **What evidence would resolve it:** A theoretical framework linking gradient noise variance to the singular value spectrum shape.

- **Question:** How should learning rate and weight decay be scaled when increasing model depth?
  - **Basis in paper:** How to scale hyperparameters when increasing model depth as a promising direction.
  - **Why unresolved:** The study focuses exclusively on width scaling while keeping depth fixed.
  - **What evidence would resolve it:** Experiments identifying power-law scaling rules for depth that preserve sublayer gain invariance.

## Limitations

- The √d scaling rule is derived from Transformer-specific observations and validated on LLaMA-style architectures; its applicability to CNNs, ResNets, or other non-Transformer models is uncertain.
- The mechanism critically depends on reaching the optimizer-governed steady state, which may not form if training is too short or uses coupled L₂ regularization instead of AdamW.
- The paper uses singular value alignment as a mechanistic proxy for good transfer, but it is not proven that perfect spectral alignment always guarantees optimal validation loss.

## Confidence

- **High Confidence:** The existence of a steady-state equilibrium where weight norms scale as √(η/λ) under AdamW training.
- **Medium Confidence:** The specific √d scaling rule for weight decay, limited by empirical validation on specific architecture and dataset.
- **Low Confidence:** The claim that the d^0.75 exponent for top singular value scaling is a universal property of scale-invariant architectures.

## Next Checks

1. **Architecture Ablation:** Apply the √d weight decay scaling rule to a non-Transformer architecture (e.g., a ResNet or a CNN) and verify if spectral alignment and hyperparameter transfer still hold.

2. **Training Duration Analysis:** Conduct a systematic study varying the total number of training steps. Plot the evolution of the top singular value and the validation loss to identify when the steady-state equilibrium is reached.

3. **Loss-Based Transfer Benchmark:** For the LLaMA architecture, perform a full hyperparameter sweep (η, λ) on a small proxy and a large target model. Compare the optimal hyperparameters found via loss minimization to those predicted by the √d rule.