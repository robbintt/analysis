---
ver: rpa2
title: 'mFollowIR: a Multilingual Benchmark for Instruction Following in Retrieval'
arxiv_id: '2501.19264'
source_url: https://arxiv.org/abs/2501.19264
tags:
- retrieval
- instruction
- arxiv
- instructions
- multilingual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces mFollowIR, a multilingual benchmark for evaluating
  instruction-following ability in retrieval models across three languages (Russian,
  Chinese, and Persian). The authors adapt narratives from TREC NeuCLIR tracks to
  create paired instructions with small edits that predictably change relevance, then
  evaluate whether models can follow these nuanced changes.
---

# mFollowIR: a Multilingual Benchmark for Instruction Following in Retrieval

## Quick Facts
- arXiv ID: 2501.19264
- Source URL: https://arxiv.org/abs/2501.19264
- Reference count: 40
- Key outcome: mFollowIR introduces a multilingual benchmark for evaluating instruction-following in retrieval across Russian, Chinese, and Persian, revealing strong cross-lingual performance but notable gaps in pure multilingual settings.

## Executive Summary
This paper introduces mFollowIR, a multilingual benchmark designed to evaluate instruction-following ability in retrieval models across three languages: Russian, Chinese, and Persian. The authors adapt narratives from TREC NeuCLIR tracks to create paired instructions with small edits that predictably change relevance, then evaluate whether models can follow these nuanced changes. Results show that while cross-lingual models (En-XX) trained on English instructions perform well, multilingual models (XX-XX) struggle significantly, indicating a need for multilingual instruction-based training data. The benchmark provides both evaluation data and insights for developing multilingual instruction-following retrieval systems.

## Method Summary
The authors create mFollowIR by making small, targeted edits to retrieval narratives from TREC NeuCLIR tracks, creating paired instruction sets where the edits predictably change document relevance (making some previously relevant documents non-relevant). They evaluate models using both standard nDCG@20 and a novel p-MRR metric that measures whether models correctly demote newly non-relevant documents, isolating instruction-following ability from keyword-matching performance. The benchmark covers three languages (Russian, Chinese, Persian) and tests both cross-lingual (English instructions) and multilingual (native-language instructions) settings across multiple model architectures including bi-encoders and cross-encoders.

## Key Results
- Cross-lingual models (En→XX) achieve significantly higher p-MRR scores than multilingual models (XX→XX), with a ~5-point gap indicating limited transfer of instruction-following capabilities.
- Larger models (7B+ parameters) and cross-encoders consistently outperform smaller models and bi-encoders on instruction-following tasks.
- The p-MRR metric successfully isolates instruction-following ability, revealing that some models achieve high nDCG but fail to follow instruction constraints.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Paired instruction edits with p-MRR metric isolate instruction-following ability from keyword-matching retrieval.
- Mechanism: By making targeted narrative edits that make some relevant documents non-relevant, p-MRR measures whether models correctly demote newly irrelevant documents. This sidesteps the confound where models retrieve relevant documents via lexical overlap while ignoring instruction semantics.
- Core assumption: The edit affects only a subset of previously relevant documents predictably (e.g., splitting ~10 relevant docs into 5 remaining relevant, 5 newly non-relevant).
- Evidence anchors:
  - [abstract] "We make small changes to the narratives and isolate how well retrieval models can follow these nuanced changes."
  - [section 4.1] "p-MRR explicitly measures their ability to follow instructions by comparing their ranked lists before and after the alteration, sidestepping the issue of the impact of lexical overlap."
  - [corpus] Weak direct evidence—related benchmarks (M-IFEval, XIFBench) evaluate instruction following in LLMs generally but do not use this paired-edit methodology.
- Break condition: If models use instructions only for keyword extraction rather than semantic understanding, p-MRR will be near zero or negative despite high nDCG.

### Mechanism 2
- Claim: English instruction-training data transfers partially to multilingual instruction-following, but a performance gap remains in pure multilingual settings.
- Mechanism: Models trained on English instructions (e.g., Promptriever, GritLM) show strong cross-lingual (En→XX) performance because the underlying multilingual LM backbone transfers instruction-following patterns. However, without native-language instruction data, XX→XX performance drops ~5 p-MRR points.
- Core assumption: The pretrained LM has sufficient multilingual representations to enable cross-lingual transfer.
- Evidence anchors:
  - [abstract] "We see strong cross-lingual performance with English-based retrievers that trained using instructions, but find a notable drop in performance in the multilingual setting."
  - [section 5.1] "Promptriever-Llama3.1-Instruct with 10.4 [p-MRR]—all larger 7B models which have seen instructions (of some kind) during training."
  - [corpus] No direct corpus evidence on cross-lingual instruction transfer specifically in retrieval; adjacent work focuses on monolingual instruction evaluation.
- Break condition: For low-resource languages or when query/instruction languages diverge significantly from English, transfer may degrade further.

### Mechanism 3
- Claim: Larger model scale (7B+ parameters) and cross-encoder architecture confer measurable instruction-following advantages.
- Mechanism: Cross-encoders compute joint attention over query+instruction and document, enabling direct semantic comparison. Larger models have greater capacity to represent complex instruction semantics beyond keyword matching.
- Core assumption: Inference budget permits cross-encoder latency or 7B+ bi-encoder deployment.
- Evidence anchors:
  - [section 5.1] "Cross-encoders have much stronger instruction-following performance in general, enabled through their base model's instruction training and their attention between instruction and documents."
  - [section 6] "The best performance comes from larger models (such as 7B+ parameter models) and from cross-encoders."
  - [corpus] Weak—related work does not specifically analyze scale effects in multilingual instruction-following retrieval.
- Break condition: When latency constraints force smaller bi-encoders, expect lower p-MRR even if nDCG remains acceptable.

## Foundational Learning

- Concept: **Bi-encoder vs. Cross-encoder architectures in retrieval**
  - Why needed here: The paper shows cross-encoders consistently outperform bi-encoders on p-MRR; understanding this gap is essential for model selection.
  - Quick check question: Why can a cross-encoder capture instruction-document semantic interactions that a bi-encoder cannot?

- Concept: **Cross-lingual (En→XX) vs. multilingual (XX→XX) evaluation settings**
  - Why needed here: The performance gap between these settings (~5 p-MRR) is a key finding; conflating them obscures where improvements are needed.
  - Quick check question: In the multilingual setting, what input languages does the model receive for query and instruction?

- Concept: **Instruction-following vs. keyword-matching in retrieval**
  - Why needed here: A model can achieve high nDCG by matching keywords while ignoring instruction constraints; p-MRR is designed to detect this failure mode.
  - Quick check question: If a model has nDCG@20 = 0.48 but p-MRR = -2.0, what does this suggest about how it uses instructions?

## Architecture Onboarding

- Component map:
  - **Bi-encoders**: mContriever, mE5 (small/base/large), mDPR, GTE-multilingual, RepLLaMA, GritLM, Promptriever. Encode query+instruction and document separately; efficient but limited instruction interaction.
  - **Cross-encoders**: Jina-reranker-v2-multi, BGE-reranker-v2-m3, mT5-13B, Mistral-7B-Instruct, FollowIR-7B. Joint encoding enables richer instruction-document reasoning at higher compute cost.
  - **Evaluation metrics**: nDCG@20 (standard retrieval quality), p-MRR (instruction-following sensitivity to narrative edits).

- Critical path:
  1. Load pooled candidate set (top-1000 docs/query from NeuCLIR systems).
  2. Run reranking with original instruction → record ranks.
  3. Run reranking with altered instruction → compute p-MRR from rank changes of newly non-relevant docs.
  4. Report both nDCG@20 and p-MRR to separate retrieval quality from instruction-following.

- Design tradeoffs:
  - Bi-encoders: Faster inference, scalable to large corpora, but weaker p-MRR (avg -3.9 to 10.4 across settings).
  - Cross-encoders: Slower (up to 12 hours on A100 for mFollowIR), but stronger p-MRR (up to 7.7 multilingual, 10.4 cross-lingual).
  - Model size: <1B models generally negative p-MRR; 7B+ models positive. Assumption: instruction-tuning data quality matters as much as scale.

- Failure signatures:
  - High nDCG with negative p-MRR: Model retrieves via keywords, ignores instruction constraints.
  - Large gap between cross-lingual and multilingual p-MRR: Model relies on English training; needs native-language instruction data.
  - Cross-encoder nDCG much higher than bi-encoder but p-MRR similar: Architecture alone insufficient without instruction training.

- First 3 experiments:
  1. **Baseline profiling**: Run mE5-base and BGE-reranker-v2-m3 on one language (e.g., Persian) to establish bi-encoder vs. cross-encoder p-MRR gap.
  2. **Instruction-training ablation**: Compare Promptriever-Llama3.1 (instruction-tuned) vs. GTE-multilingual-base (not instruction-tuned) on both En→XX and XX→XX settings to quantify transfer benefit.
  3. **Error analysis**: For queries with negative p-MRR, inspect whether failure correlates with specific instruction types (negation, multi-condition constraints) to prioritize data collection.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we effectively create multilingual instruction-based training data to close the performance gap between cross-lingual (En-XX) and multilingual (XX→XX) settings?
- Basis in paper: [explicit] The authors state: "This implies that the main gaps are currently (1) a lack of multilingual instruction-based training data" and note a "wide gap of around 5 p-MRR" between cross-lingual and multilingual settings.
- Why unresolved: The paper demonstrates that English-based instruction training helps cross-lingually but shows "a notable drop in performance in the multilingual setting," indicating current approaches are insufficient.
- What evidence would resolve it: Development of multilingual instruction training datasets and demonstration that models trained on such data achieve comparable p-MRR scores in XX-XX settings to current En-XX performance.

### Open Question 2
- Question: Will instruction-following capabilities observed in Russian, Chinese, and Persian generalize to lower-resource languages?
- Basis in paper: [explicit] The authors note: "as our dataset does not include low-resource languages, it is unclear whether this will hold when moving to lower-resource languages."
- Why unresolved: The benchmark only covers three languages with substantial representation in training corpora; models may perform differently on languages with less pre-training data.
- What evidence would resolve it: Extension of mFollowIR methodology to low-resource languages and evaluation showing whether cross-lingual and multilingual instruction-following patterns remain consistent.

### Open Question 3
- Question: What approaches can bridge the performance gap between smaller models (<1B parameters) and larger models (7B+) on instruction-following retrieval?
- Basis in paper: [explicit] The authors identify "a large gap in performance between models under 1B parameter size and models larger than 1B" as a key unresolved challenge.
- Why unresolved: Current results show the best performance comes from 7B+ models and cross-encoders, which are computationally expensive and impractical for many deployment scenarios.
- What evidence would resolve it: Development of smaller models that achieve competitive p-MRR scores through novel architectures, training techniques, or knowledge distillation from larger models.

### Open Question 4
- Question: How would incorporating graded relevance judgments affect the evaluation and development of instruction-following retrieval models?
- Basis in paper: [explicit] The authors state: "We note that one could also look at graded relevance, however, we leave this as future work and leave the document relevance either unchanged or make it completely irrelevant due to the new edit."
- Why unresolved: The current binary relevance scheme may not capture nuanced instruction-following failures where documents partially satisfy modified instructions.
- What evidence would resolve it: An extended benchmark with graded relevance annotations showing whether this provides better discrimination between models' instruction-following capabilities.

## Limitations
- The paired-edit methodology assumes predictable, binary relevance shifts that may not reflect real-world instruction complexity.
- The benchmark relies on TREC NeuCLIR data, inheriting potential biases from that collection process.
- Three-language coverage (Russian, Chinese, Persian) may not generalize to other language families or lower-resource languages.

## Confidence
- **High confidence**: Cross-encoder architectures consistently outperform bi-encoders on instruction-following (p-MRR) across all tested languages and model scales. The ~5-point p-MRR gap between cross-lingual and multilingual settings is robust and reproducible.
- **Medium confidence**: The claim that larger models (7B+) show better instruction-following requires careful interpretation, as these models also have more extensive instruction-tuning data. The transfer gap between En→XX and XX→XX settings may be smaller or larger for languages not represented in the current benchmark.
- **Low confidence**: The assertion that cross-encoders' attention mechanism is the primary driver of their superior p-MRR performance lacks ablation studies isolating architectural effects from training data differences.

## Next Checks
1. **Instruction complexity ablation**: Create instruction pairs with varying semantic complexity (negation, multi-condition constraints, temporal reasoning) and measure whether p-MRR degradation correlates with instruction type rather than just language pair.

2. **Model scale vs. instruction data**: Train matched-sized bi-encoder and cross-encoder variants with identical instruction-tuning data to isolate whether architectural advantages persist independent of pretraining/instruction data differences.

3. **Zero-shot cross-lingual transfer**: Evaluate instruction-following performance on language pairs never seen during training (e.g., Spanish→Arabic) to test the limits of cross-lingual instruction transfer beyond the En→XX setting.