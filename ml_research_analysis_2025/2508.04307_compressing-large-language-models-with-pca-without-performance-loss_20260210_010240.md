---
ver: rpa2
title: Compressing Large Language Models with PCA Without Performance Loss
arxiv_id: '2508.04307'
source_url: https://arxiv.org/abs/2508.04307
tags:
- token
- input
- embeddings
- transformer
- compression
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Principal Component Analysis (PCA) as a universal
  compression method for neural models, applied either globally to structured inputs
  like polar-transformed images or locally to token embeddings in text. The approach
  reduces input dimensionality before training, enabling smaller, more efficient models
  without performance loss.
---

# Compressing Large Language Models with PCA Without Performance Loss

## Quick Facts
- arXiv ID: 2508.04307
- Source URL: https://arxiv.org/abs/2508.04307
- Authors: Magnus Bengtsson
- Reference count: 22
- Primary result: PCA compression achieves 98% MNIST accuracy with 840 parameters in one-layer classifier

## Executive Summary
This paper introduces Principal Component Analysis (PCA) as a universal compression technique for neural networks, demonstrating its effectiveness across vision and language tasks. The method reduces input dimensionality before training, allowing models to learn more efficiently from compressed representations without sacrificing performance. Key applications include polar-transformed image classification and transformer-based text classification, with results showing dramatic parameter reduction while maintaining or exceeding baseline accuracy.

## Method Summary
The approach applies PCA to reduce input dimensionality before model training, either globally to structured inputs like polar-transformed images or locally to token embeddings in text. For images, polar transformation followed by PCA compression enables efficient learning with minimal parameters. For text, PCA is applied to MiniLM embeddings, reducing dimensionality while preserving semantic information. The compressed representations are then used as inputs to lightweight neural architectures, including single-layer classifiers and two-layer transformers. This pre-processing step effectively aligns model capacity with intrinsic data complexity.

## Key Results
- One-layer classifier achieves 98% accuracy on polar MNIST with only 840 parameters
- Two-layer transformer on 20 Newsgroups reaches 76.62% accuracy with 81k parameters using 70-dimensional PCA-compressed MiniLM embeddings
- GPT-style decoder preserves over 97% cosine similarity while using less than 17% of GPT-2's parameters

## Why This Works (Mechanism)
PCA compression works by reducing input dimensionality to match the intrinsic dimensionality of the data, eliminating redundant information while preserving task-relevant features. This alignment between model capacity and data complexity allows smaller networks to achieve comparable performance to larger models. The method is particularly effective when applied to structured transformations (like polar coordinates for images) or semantic embeddings (like MiniLM for text), where the intrinsic dimensionality is already lower than the raw input space.

## Foundational Learning

### Principal Component Analysis
**Why needed:** Core mathematical technique for dimensionality reduction that identifies directions of maximum variance
**Quick check:** Can compute eigenvectors of covariance matrix and project data onto principal components

### Intrinsic Dimensionality
**Why needed:** Understanding the minimum number of parameters needed to represent data without information loss
**Quick check:** Can explain the gap between raw input dimensionality and actual information content

### Polar Coordinate Transformation
**Why needed:** Converts Cartesian image data to polar form, revealing lower intrinsic dimensionality for certain patterns
**Quick check:** Can implement polar transformation and understand its effect on data structure

## Architecture Onboarding

### Component Map
Input Data -> PCA Compression -> Lightweight Neural Network -> Output Prediction

### Critical Path
1. Data preprocessing (polar transformation or embedding extraction)
2. PCA dimensionality reduction
3. Neural network training on compressed representations
4. Evaluation on original task metrics

### Design Tradeoffs
- Higher compression ratios reduce parameters but may lose task-relevant information
- Single-layer architectures work for simple tasks but may need depth for complexity
- Global compression (images) vs. local compression (embeddings) affects applicability

### Failure Signatures
- Accuracy drops when compression ratio exceeds intrinsic dimensionality
- Training instability with aggressive dimensionality reduction
- Poor generalization when compressed representations lose semantic structure

### First Experiments
1. Apply PCA to MNIST images with varying compression ratios (50-95% reduction)
2. Train single-layer classifier on polar-transformed MNIST with PCA compression
3. Compress MiniLM embeddings with PCA and evaluate on 20 Newsgroups classification

## Open Questions the Paper Calls Out
None

## Limitations
- Results hinge on assumption that PCA-compressed embeddings preserve task-relevant information, not explicitly tested across all datasets
- 98% accuracy claim on polar MNIST comes from single-layer architecture that may not generalize to complex vision tasks
- Transformer results show strong performance with reduced parameters but baseline comparison uses deliberately compressed model rather than full-scale alternative

## Confidence
- Universal applicability across modalities: Medium (experimental scope limited to specific architectures and datasets)
- Parameter reduction without performance loss: High for presented scenarios, Medium for broader generalization
- Theoretical framing connecting PCA to intrinsic dimensionality: High (well-articulated but relies on empirical demonstration)

## Next Checks
1. Test PCA compression on larger-scale vision datasets (e.g., CIFAR-10/100 or ImageNet subsets) to verify generalization beyond MNIST
2. Compare against non-PCA compression methods (quantization, pruning, distillation) on identical architectures to establish relative effectiveness
3. Evaluate compressed models on end-to-end task performance metrics rather than just embedding similarity to confirm practical utility