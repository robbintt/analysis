---
ver: rpa2
title: Beat and Downbeat Tracking in Performance MIDI Using an End-to-End Transformer
  Architecture
arxiv_id: '2507.00466'
source_url: https://arxiv.org/abs/2507.00466
tags:
- beat
- tracking
- midi
- music
- downbeat
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an end-to-end transformer-based model for
  beat and downbeat tracking in MIDI performance data. The method uses an encoder-decoder
  architecture to translate MIDI input into beat annotations, incorporating dynamic
  augmentation and optimized tokenization.
---

# Beat and Downbeat Tracking in Performance MIDI Using an End-to-End Transformer Architecture

## Quick Facts
- **arXiv ID:** 2507.00466
- **Source URL:** https://arxiv.org/abs/2507.00466
- **Reference count:** 0
- **Primary result:** End-to-end transformer achieves up to 98.01% beat F1 and 80.00% downbeat F1 on MIDI performance datasets

## Executive Summary
This paper presents an end-to-end transformer architecture for beat and downbeat tracking in MIDI performance data. The method treats beat tracking as a sequence-to-sequence translation task, converting MIDI note events into beat annotations using a T5 encoder-decoder framework. Through extensive experimentation with different tokenization strategies, quantization levels, and data augmentation techniques, the model outperforms state-of-the-art HMM and deep learning baselines across four datasets (A-MAPS, ASAP, GuitarSet, Leduc), achieving F1-scores up to 98.01% for beats and 80.00% for downbeats.

## Method Summary
The approach uses a T5 encoder-decoder architecture to translate MIDI note events into beat annotations. MIDI files are processed using PrettyMIDI to extract note events and beat positions, then segmented into 10-second windows with 1-second hops. Dynamic augmentation includes pitch transposition, time shifting, and scaling. The model is trained from scratch with halved t5-small parameters (d_model=128, d_ff=1024, 3 layers, 4 heads) using Adafactor optimizer. Inference employs beam search with n-gram blocking. Five different tokenization encodings are explored, with optimized quantization (50-100ms) and velocity information providing the best results.

## Key Results
- Beat F1-score of 98.01% achieved with 50ms quantization and optimized tokenization
- Downbeat F1-score of 80.00% with 100ms quantization and beat counter removed
- Outperforms HMM baselines by 21.24% for beats and 30.22% for downbeats
- Dynamic augmentation improves beat tracking (98.10% vs 97.88%) but degrades downbeat tracking (52.25% vs 67.16%)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Framing beat tracking as sequence-to-sequence translation (MIDI → rhythm language) outperforms text-completion framing
- Core assumption: Mapping from MIDI performance tokens to beat positions follows learnable grammatical rules similar to language translation
- Evidence anchors: T5 achieves 96.03% beat F1 vs. GPT-2's 88.69%; beam search with 5 beams and no-repeat-ngram-size of 2 yields best results

### Mechanism 2
- Claim: Richer input encoding (note offsets + velocity) substantially improves beat detection, while removing explicit beat counters improves downbeat detection
- Core assumption: Downbeats are often performed with higher velocity and models implicitly track beat position without explicit counter supervision
- Evidence anchors: v3 encoding (+velocity +offset) achieves 96.03% beat F1 vs. v1's 81.06%; v4 (no counter) achieves 67.16% downbeat F1 vs. v3's 59.52%

### Mechanism 3
- Claim: Moderate temporal quantization (50-100ms) acts as noise reduction, improving both beat and downbeat accuracy
- Core assumption: Beat annotation noise exists at fine resolutions (5-10ms) and the 70ms tolerance window allows aggressive quantization without penalty
- Evidence anchors: 50ms quantization yields 97.88% beat F1 (best); 100ms yields 80.00% downbeat F1 (best); 200ms causes collapse

## Foundational Learning

- **Encoder-Decoder Transformer Architectures (T5-specific)**
  - Why needed here: The entire model builds on T5's text-to-text framework; understanding encoder self-attention, decoder cross-attention, and autoregressive generation is prerequisite
  - Quick check question: Can you explain why T5 frames all tasks as text-to-text, and how cross-attention differs from self-attention?

- **MIDI Representation and Symbolic Music Tokenization**
  - Why needed here: Input preprocessing converts MIDI (pitch, onset, offset, velocity) into token sequences; understanding token vocabulary design is critical
  - Quick check question: What information is lost when encoding MIDI as onset-only (v1) versus onset+offset+velocity (v3)?

- **Sequence-to-Sequence Evaluation Metrics for Temporal Events**
  - Why needed here: F1-scores with tolerance windows (70ms) and continuity metrics are domain-specific; standard classification metrics don't apply
  - Quick check question: Why does the 70ms tolerance window make 100ms quantization viable despite being coarser than the tolerance?

## Architecture Onboarding

- **Component map:** MIDI file → PrettyMIDI extraction → 10s overlapping segments (1s hop) → dynamic augmentation (transpose, shift, scale) → quantization (10-100ms) → tokenization (ON/OFF/VEL/T tokens) → T5-small halved (d_model=128, d_ff=1024, 3 layers, 4 heads) → 50 epochs, Adafactor optimizer → Autoregressive decoding with beam search (5 beams, n-gram blocking) → B/DB tokens with time positions

- **Critical path:** Tokenization design (v1-v5) directly determines what information the model can access; segment length (10s) balances context window vs. vocabulary size; quantization step (50-100ms) controls noise reduction vs. precision tradeoff

- **Design tradeoffs:** Beat vs. downbeat accuracy: Time augmentation (shift+scale) improves beat F1 (98.10%) but degrades downbeat F1 (52.25%); Velocity inclusion: Requires v3 encoding; unavailable for guitar datasets, forcing fallback to v2; Segment length: 10s optimal for beats; 5s better for downbeats; suggests different context requirements

- **Failure signatures:** Downbeat F1 drops sharply on guitar datasets (23.02% GuitarSet, 29.75% Leduc) vs. piano (76.56% A-MAPS)—likely due to lower note density and weaker downbeat emphasis; HMM baselines collapse on guitar (6.78-13.78% downbeat F1), suggesting symbolic-only methods struggle with sparse rhythmic cues; 200ms quantization causes catastrophic failure (71.85% beat F1)

- **First 3 experiments:** 1) Encoding ablation: Train identical models with v1-v5 encodings on A-MAPS; expect v3 best for beats, v4 best for downbeats; 2) Quantization sweep: Test 5/10/20/50/100/200ms on held-out set; expect peak at 50ms for beats, 100ms for downbeats; 3) Cross-dataset transfer: Train on A-MAPS+ASAP (piano), evaluate on GuitarSet+Leduc (guitar); expect downbeat degradation per Table 7 patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can integrating this MIDI beat tracker with an automatic music transcription (AMT) front-end achieve parity with state-of-the-art audio-based beat trackers like Beat This!?
- Basis in paper: [explicit] "In future works, this method could be combined with a AMT approach in order to have a more fair comparison with audio based beat-tracking methods."
- Why unresolved: The current comparison between symbolic and audio-based methods is inherently unfair because audio methods have access to expressive performance features (accent dynamics, timbre) that are lost or attenuated in MIDI transcription.

### Open Question 2
- Question: How does the model perform on multi-instrument performances, and what architectural modifications are needed to handle overlapping streams with divergent rhythmic roles?
- Basis in paper: [explicit] "Expanding the model to multi-instrument performances and applying self-supervised learning techniques could also increase its robustness and versatility in diverse musical settings."
- Why unresolved: All experiments used solo piano or solo guitar datasets; multi-instrument MIDI introduces competing onset streams, potential conflicting downbeat cues, and increased polyphony that may exceed the current 10-second context window's capacity.

### Open Question 3
- Question: Can self-supervised pre-training on large unlabeled MIDI corpora improve robustness to alignment noise and missing velocity information?
- Basis in paper: [explicit] Same quote as above; [inferred] The model's best encoding (v3) requires velocity, yet velocity is unavailable in guitar datasets, forcing fallback to v2 with degraded performance. Additionally, GuitarSet shows annotation misalignment issues.
- Why unresolved: The current model is trained from scratch solely on supervised data. Self-supervised objectives (e.g., masked note prediction, tempo-invariant contrastive learning) could teach the model to exploit structural regularities independent of velocity or precise alignment.

## Limitations

- The claimed superiority of translation framing over text-completion framing lacks ablation against other joint learning approaches
- Velocity as downbeat cue is assumed rather than empirically validated; no analysis of which encoding features drive downbeat accuracy
- Quantization benefits are dataset-specific; the 70ms tolerance window enables aggressive quantization that may not generalize to stricter evaluation regimes

## Confidence

- **High Confidence:** Encoder-decoder architecture design choices (T5 parameters, beam search settings) are well-specified and reproducible
- **Medium Confidence:** Input encoding improvements are supported by controlled experiments, though the underlying mechanisms (velocity encoding, beat counter removal) require further validation
- **Low Confidence:** Generalization claims across instrument types, particularly the sharp downbeat performance drop on guitar datasets, lack thorough investigation into domain adaptation strategies

## Next Checks

1. **Cross-dataset generalization test:** Train on piano-only datasets (A-MAPS, ASAP) and evaluate on guitar datasets (GuitarSet, Leduc) to quantify domain shift impact
2. **Encoding mechanism ablation:** Train models with velocity-only, offset-only, and beat-counter-only modifications to isolate which features drive accuracy improvements
3. **Quantization tolerance analysis:** Evaluate model performance across multiple tolerance windows (30ms, 70ms, 100ms) to determine if quantization benefits persist under stricter evaluation criteria