---
ver: rpa2
title: Evaluating the Effectiveness of Black-Box Prompt Optimization as the Scale
  of LLMs Continues to Grow
arxiv_id: '2505.08303'
source_url: https://arxiv.org/abs/2505.08303
tags:
- prompt
- optimization
- llms
- methods
- prompts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates the effectiveness of black-box prompt\
  \ optimization methods as large language models (LLMs) continue to scale up. The\
  \ authors evaluate three popular black-box optimization techniques\u2014EvoPrompt,\
  \ ProTeGi, and BPO\u2014on large-scale models including DeepSeek V3 (671B) and Gemini\
  \ 2.0 Flash across four NLU and NLG datasets."
---

# Evaluating the Effectiveness of Black-Box Prompt Optimization as the Scale of LLMs Continues to Grow

## Quick Facts
- arXiv ID: 2505.08303
- Source URL: https://arxiv.org/abs/2505.08303
- Authors: Ziyu Zhou; Yihang Wu; Jingyuan Yang; Zhan Xiao; Rongjun Li
- Reference count: 9
- Primary result: Black-box prompt optimization shows diminishing returns as model scale increases, with inverse scaling observed from 7B to 671B parameters.

## Executive Summary
This study investigates whether black-box prompt optimization methods remain effective as large language models scale up. The authors evaluate three popular black-box optimization techniques—EvoPrompt, ProTeGi, and BPO—on large-scale models including DeepSeek V3 (671B) and Gemini 2.0 Flash across four NLU and NLG datasets. Results show only limited performance improvements: for NLU tasks, average accuracy gains were 0.86% and 1.16% for DeepSeek V3 and Gemini 2.0 Flash respectively, while NLG tasks saw improvements of 1.04% and 2.03%. To understand this phenomenon, experiments were conducted on Qwen 2.5 models ranging from 7B to 72B parameters, revealing an inverse scaling law where optimization effectiveness diminishes as model size increases. The study suggests that larger models' superior inherent alignment and semantic understanding make them less sensitive to subtle prompt variations, rendering black-box optimization less beneficial at scale.

## Method Summary
The study evaluates three black-box prompt optimization methods (EvoPrompt, ProTeGi, BPO) on large-scale LLMs (DeepSeek V3 671B, Gemini 2.0 Flash) across four datasets (SST-5, AG's News, SAMSum, ASSET). For NLU tasks, 500 training and 500 test examples are randomly sampled, while NLG tasks use complete datasets. EvoPrompt runs 4 optimization cycles using evolutionary algorithms, ProTeGi uses 4 rounds of text language gradients, and BPO employs a sequence-to-sequence optimizer for 5 rounds. To investigate scaling effects, EvoPrompt is applied to Qwen 2.5 models (7B, 14B, 32B, 72B) on SST-5 and AG's News, with performance improvements measured against model size.

## Key Results
- NLU tasks showed minimal improvements: 0.86% accuracy gain for DeepSeek V3 and 1.16% for Gemini 2.0 Flash
- NLG tasks saw slightly better but still limited gains: 1.04% for DeepSeek V3 and 2.03% for Gemini 2.0 Flash
- Inverse scaling observed: Qwen-2.5 7B model gained 12% on SST-5, while Qwen-2.5 72B gained only 5.9%, and DeepSeek-V3 671B gained just 1.1%
- BPO optimization on AG's News with Gemini 2.0 Flash showed performance degradation (82.8% → 82.2%)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Black-box prompt optimization exhibits an inverse scaling relationship with model size—gains diminish as parameters increase.
- Mechanism: Smaller models (7B–14B) benefit from optimized prompts that inject domain-specific cues (e.g., explicitly mentioning "news article" for classification). Larger models (32B+) already encode this semantic context, so explicit cues become redundant and lexical refinements provide marginal utility.
- Core assumption: The observed pattern generalizes across model families beyond Qwen 2.5 and the tested datasets.
- Evidence anchors:
  - [abstract] "observed an inverse scaling law, wherein the effectiveness of black-box optimization methods diminished as the model size increased"
  - [section 4.2] "on the SST-5 benchmark, accuracy improvements diminish notably from 12% for the Qwen-2.5 7B model to 5.9% for the Qwen-2.5 72B model, ultimately reaching just 1.1% for the DeepSeek-V3 671B model"
  - [corpus] Related work on black-box optimization (FedPOB, Inference-Aware Prompt Optimization) focuses on smaller models; no corpus evidence contradicts the inverse scaling observation, but validation on intermediate scales (100B–500B) remains absent.
- Break condition: If a task requires highly specialized domain knowledge not captured in pre-training, larger models may regain sensitivity to prompt optimization—a hypothesis not tested in this study.

### Mechanism 2
- Claim: Larger models exhibit reduced sensitivity to superficial prompt variations (synonym substitution, minor rephrasing).
- Mechanism: Scaled-up LLMs develop more robust semantic representations and alignment, enabling them to converge on similar interpretations across lexical variants. The paper notes optimized prompts differ only slightly from initial prompts—primarily synonym swaps and clarity improvements.
- Core assumption: Robustness to prompt variation is a monotonic function of model scale across architectures.
- Evidence anchors:
  - [section 3.3] "the optimized prompts exhibit only slight modifications compared to the initial prompts for both datasets. The primary adjustments involve replacing synonyms and subtly rephrasing to improve clarity"
  - [section 3.3] "larger LLMs exhibit more refined alignment, making them less sensitive to such subtle variations in lexical choices"
  - [corpus] No direct corpus validation; related papers (OET, VERA) examine prompt sensitivity in adversarial contexts, not benign optimization.
- Break condition: If prompts contain structural or formatting changes (not just lexical), sensitivity may persist even at scale—this boundary was not explored.

### Mechanism 3
- Claim: The optimization methods tested (EvoPrompt, ProTeGi, BPO) converge to functionally equivalent prompts regardless of optimization algorithm.
- Mechanism: All three methods operate within a constrained search space of natural language refinements. Without access to gradients or internal representations, the methods can only explore surface-level variations, hitting a ceiling determined by the model's existing capability.
- Core assumption: The ceiling effect is inherent to black-box methods, not specific to the three algorithms chosen.
- Evidence anchors:
  - [section 3.2] describes all three methods as operating via API calls without internal access, using evolutionary algorithms or text feedback
  - [section 3.3, Table 1] shows consistent ~1% improvements across all three methods on large models, suggesting convergence
  - [corpus] Corpus papers on black-box optimization (e.g., Rethinking Prompt Optimization) propose diversification and migration strategies but do not report results at 100B+ scale.
- Break condition: If black-box methods incorporated multi-step reasoning traces or external tool use, the search space might expand beyond surface variations—untested in this work.

## Foundational Learning

- Concept: **Black-box vs. white-box prompt optimization**
  - Why needed here: The paper's central claim depends on understanding why gradient-free methods have inherent limits. White-box methods (AutoPrompt, prefix tuning) leverage internal gradients; black-box methods (EvoPrompt, ProTeGi, BPO) rely only on input-output observations.
  - Quick check question: Can you explain why accessing model gradients would enable different optimization trajectories than API-only refinement?

- Concept: **Inverse scaling laws**
  - Why needed here: The paper introduces an inverse scaling pattern specific to prompt optimization. This differs from standard scaling laws where larger models monotonically improve on task performance—here, the *marginal benefit of optimization* decreases.
  - Quick check question: If a 7B model gains 12% from prompt optimization and a 72B model gains 6%, what would you predict for a 200B model based on this paper's findings?

- Concept: **Prompt sensitivity and robustness**
  - Why needed here: The paper attributes diminished returns to larger models' reduced sensitivity to prompt variations. Understanding when models are prompt-sensitive vs. prompt-robust is essential for anticipating where optimization investments matter.
  - Quick check question: For a code generation task, would you expect higher or lower prompt sensitivity compared to sentiment classification—and why?

## Architecture Onboarding

- Component map:
  - EvoPrompt (evolutionary algorithm with crossover/mutation) -> ProTeGi (text language gradients) -> BPO (sequence-to-sequence optimizer) -> Evaluation pipeline (accuracy/ROUGE/SARI metrics)

- Critical path:
  1. Define initial prompt for task (baseline)
  2. Run selected optimization method on training split
  3. Evaluate optimized prompt on test split
  4. Compare metric delta (accuracy for NLU, ROUGE-L/SARI for NLG)
  5. Repeat across model scales to observe scaling trend

- Design tradeoffs:
  - **Training data size**: Paper uses 500 examples; larger training sets might enable more nuanced optimization but increase cost.
  - **Iteration count**: 4–5 rounds standardized across methods; more iterations could find better prompts but face diminishing returns.
  - **Model family coverage**: Qwen 2.5 provides controlled scale comparison but may not generalize to architectures with different training objectives (e.g., reasoning-focused models like DeepSeek R1).

- Failure signatures:
  - **Negative gains**: BPO on AG's News with Gemini 2.0 Flash showed 82.8% → 82.2% (performance degradation). Monitor for optimization paths that overfit to training distribution.
  - **Flat improvement curves**: If improvements plateau at <1% by iteration 2–3, continuing optimization wastes compute—early stopping warranted.
  - **Inconsistent cross-task signals**: If NLU shows gains but NLG degrades, the optimization may be overfitting to classification-specific patterns.

- First 3 experiments:
  1. **Baseline replication**: Run EvoPrompt on Qwen 2.5-7B for SST-5 with the paper's exact setup (500 train/500 test, 4 iterations). Target: ~12% improvement. If significantly different, investigate data sampling or prompt initialization.
  2. **Scale extrapolation test**: Apply the same optimization to a model in the 100B–200B range (e.g., Llama 3.1 70B or 405B if accessible) to fill the gap between Qwen 72B and DeepSeek 671B. Document whether the inverse scaling curve is smooth or exhibits phase transitions.
  3. **Structural prompt variation**: Instead of lexical refinement, test prompts with structural changes (e.g., adding chain-of-thought scaffolding, multi-step decomposition). This probes whether the ceiling is method-inherent or search-space-limited.

## Open Questions the Paper Calls Out
- Do reasoning-oriented LLMs exhibit different prompt-sensitivity characteristics compared to standard models under black-box optimization?
- Does the inverse scaling law persist for black-box optimization in multilingual or low-resource language contexts?
- Is black-box optimization effective for large-scale models when applied to multi-modal prompts (visual/audio)?

## Limitations
- The inverse scaling pattern may not generalize beyond Qwen 2.5 architecture to other model families or reasoning-focused LLMs.
- Only three specific black-box optimization methods were tested, leaving open whether more sophisticated approaches could overcome the observed ceiling effect.
- Results are primarily based on SST-5 and AG's News classification tasks, with uncertain generalizability to other NLU or NLG tasks.

## Confidence
- **High confidence**: The observation that black-box optimization yields diminishing returns on the specific models and tasks tested (DeepSeek V3, Gemini 2.0 Flash, Qwen 2.5 on SST-5/AG's News). The experimental methodology is clearly specified, and results are consistent across three independent optimization methods.
- **Medium confidence**: The generalization of inverse scaling to other model families and architectures beyond Qwen 2.5. While the pattern is compelling within the tested range, cross-architecture validation is absent.
- **Low confidence**: The mechanism attributing reduced sensitivity to "refined alignment" in larger models. This remains a hypothesis without direct experimental validation—alternative explanations (e.g., optimization methods hitting fundamental search space limits) are equally plausible.

## Next Checks
1. **Cross-architecture validation**: Test EvoPrompt on a different model family (e.g., Llama 3.1 series or GPT-4) spanning from 7B to 400B+ parameters to determine if inverse scaling is architecture-agnostic or specific to Qwen-style training.
2. **Structural prompt variation**: Instead of lexical refinement, test prompts with structural changes (e.g., chain-of-thought scaffolding, multi-step decomposition) to determine whether the optimization ceiling is method-inherent or search-space-limited.
3. **Reasoning task evaluation**: Apply the optimization pipeline to reasoning-intensive tasks (e.g., GSM8K, MATH) to test whether inverse scaling persists when semantic understanding alone is insufficient for optimal performance.