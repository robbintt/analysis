---
ver: rpa2
title: 'Depth Jitter: Seeing through the Depth'
arxiv_id: '2508.06227'
source_url: https://arxiv.org/abs/2508.06227
tags:
- depth
- underwater
- augmentation
- jitter
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Depth Jitter, a novel depth-aware data augmentation
  technique designed to improve underwater multi-label classification. The method
  leverages the underwater image formation model to simulate depth-based variations
  by applying depth offsets, generating synthetic training samples that reflect realistic
  underwater lighting and color distortions.
---

# Depth Jitter: Seeing through the Depth

## Quick Facts
- **arXiv ID:** 2508.06227
- **Source URL:** https://arxiv.org/abs/2508.06227
- **Reference count:** 23
- **Primary result:** Depth-aware augmentation improves underwater multi-label classification with mAP up to 0.97 and ROC AUC up to 0.99

## Executive Summary
Depth Jitter is a novel data augmentation technique designed to improve underwater image classification by leveraging the physics of underwater light propagation. The method simulates depth-based variations using the Underwater Image Formation Model (UIFM), applying depth offsets to generate physically plausible synthetic training samples. Evaluated on FathomNet and UTDAC2020 datasets using the Query2Label framework, Depth Jitter consistently outperforms traditional augmentations like ColorJitter and CLAHE. The approach demonstrates that depth-aware augmentation can enhance model robustness and generalization in underwater imaging tasks.

## Method Summary
The method estimates depth maps using Depth-Anything and precomputes UIFM parameters (absorption βc, backscatter γc, and background light Bc) via SeaThru. During training, it applies depth offsets through the UIFM equations to re-render images with physically realistic underwater lighting variations. Two strategies are used: adaptive offsets based on depth variance quantiles for UTDAC2020, and fixed uniform offsets for FathomNet. The augmented images are then used to train a Query2Label multi-label classifier with ResNeSt-101 backbone and Asymmetric Loss.

## Key Results
- On UTDAC2020: achieves mAP of 0.97, ROC AUC of 0.99, and validation loss of 0.042
- On FathomNet: achieves mAP@20 of 0.87, ROC AUC of 0.99, and recall of 0.79
- Consistently outperforms traditional augmentations across all metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Physics-grounded augmentation produces more semantically meaningful training variations than pixel-level transformations.
- Mechanism: Depth Jitter applies depth offsets through the Underwater Image Formation Model (UIFM), which explicitly models light absorption (βc) and backscatter (γc) coefficients. By solving for modified pixel intensities via I^mod_c,p = (I^orig_c,p - B_c(1-e^{-γc zp}))e^{-βc Δzm} + B_c(1-e^{-γc Δzm}), the method re-renders images with physically plausible depth-dependent color attenuation rather than arbitrary color shifts.
- Core assumption: Assumes estimated depth maps from monocular models (Depth-Anything) are sufficiently accurate to parameterize realistic re-rendering.
- Evidence anchors:
  - [abstract]: "leverages the underwater image formation model to simulate depth-based variations by applying depth offsets, generating synthetic training samples that reflect realistic underwater lighting and color distortions"
  - [Section III-A, Eq. 1-2]: Full UIFM equations with absorption/backscatter decomposition
  - [corpus]: Weak direct corpus support for this specific mechanism; neighboring papers (DeGuV, DeFM) address depth-guided learning but not physics-based augmentation
- Break condition: If depth estimation quality degrades significantly in turbid/low-light conditions (acknowledged in Limitations), the physical realism of augmentations collapses.

### Mechanism 2
- Claim: Selective augmentation based on depth variance prevents redundant transformations and focuses augmentation budget where it matters.
- Mechanism: A quantile-based threshold (τ = Q_0.25(V)) separates images into high-variance and low-variance sets. Only high-variance images receive depth offsets Δzm ~ U(-α·zmin, β·zmax), since low-variance images "converge towards depth boundaries" where augmentation has minimal effect.
- Core assumption: Depth variance correlates with augmentation utility—that high-variance scenes benefit more from depth perturbation.
- Evidence anchors:
  - [Section III-C, Eq. 8-10]: Formal threshold definition and conditional offset sampling
  - [Section IV-B]: Adaptive strategy employed for UTDAC2020; fixed offsets used for FathomNet after preliminary tuning
  - [corpus]: No direct corpus evidence; variance-based selective augmentation appears novel to this work
- Break condition: If depth variance distribution is unimodal with few high-variance samples, the threshold may either over-augment or under-augment.

### Mechanism 3
- Claim: Precomputed depth parameters enable efficient on-the-fly augmentation during training without runtime depth inference.
- Mechanism: UIFM parameters (Bc, βc, γc) and depth maps are computed once during preprocessing. During training, only Eq. 7 is evaluated—depth-aware re-rendering via scalar offsets—avoiding repeated Depth-Anything inference.
- Core assumption: Storage overhead for precomputed parameters is acceptable; depth maps remain static throughout training.
- Evidence anchors:
  - [Section III-B]: "since these estimates are computed during the pre-processing stage, the only operation performed during training is the re-rendering of images using adjusted depth offsets"
  - [Section IV-E]: "adding 40 to 60 seconds per epoch" overhead attributed primarily to preprocessing
  - [corpus]: DeFM similarly leverages precomputed depth representations for efficiency
- Break condition: For streaming/real-time systems requiring dynamic depth estimation, preprocessing advantage disappears.

## Foundational Learning

- **Underwater Image Formation Model (UIFM)**
  - Why needed here: The entire augmentation pipeline derives from UIFM equations; understanding absorption vs. backscatter contributions is essential for debugging augmentation artifacts.
  - Quick check question: Given observed intensity Ic,p, background light Bc, and depth zp, can you derive the restored intensity Jc,p?

- **Multi-label Classification with Asymmetric Loss**
  - Why needed here: The Query2Label framework with ASL handles label imbalance common in marine datasets; understanding why positive/negative examples require different loss weights prevents misdiagnosis of training dynamics.
  - Quick check question: Why would standard binary cross-entropy underperform on a dataset where most images contain 1-2 labels from a 50-class vocabulary?

- **Monocular Depth Estimation Limitations**
  - Why needed here: Depth Jitter inherits errors from Depth-Anything; understanding failure modes (textureless regions, reflective surfaces) helps anticipate where augmentation quality degrades.
  - Quick check question: What visual features does a monocular depth model lack when estimating distance in turbid water with uniform color?

## Architecture Onboarding

- **Component map**:
  1. Preprocessing Pipeline: Raw image → Depth-Anything inference → UIFM parameter estimation (Bc, βc, γc) → Store parameters alongside image
  2. Augmentation Module: Load precomputed parameters → Sample depth offset (adaptive or fixed) → Re-render via Eq. 7 → Return augmented image
  3. Classification Backbone: Query2Label transformer with ResNeSt-101 encoder → Asymmetric Loss → Multi-label predictions

- **Critical path**: Depth estimation quality → Parameter estimation accuracy → Augmentation realism → Model generalization. Errors propagate linearly; poor depth estimates produce physically implausible augmentations.

- **Design tradeoffs**:
  - **Adaptive vs. fixed offsets**: UTDAC2020 benefits from variance-aware adaptive offsets; FathomNet performs better with fixed [-4m, +15m] range. Trade-off between dataset-specific tuning effort and out-of-box performance.
  - **Storage vs. compute**: Precomputing parameters adds storage overhead but reduces training-time compute. For embedded deployment, may need to recompute on-device.
  - **Offset magnitude**: Larger offsets increase augmentation diversity but risk generating unrealistic deep-water images; α=0.5, β=0.2 were empirically tuned.

- **Failure signatures**:
  - **Over-darkened images**: Positive depth offsets too large; reduce β scaling factor or cap zmax
  - **No augmentation effect**: Depth variance threshold too conservative; inspect τ placement relative to V distribution
  - **Color artifacts in restored images**: UIFM parameter estimation diverged; check Bc initialization or add regularization

- **First 3 experiments**:
  1. **Baseline validation**: Run Query2Label on UTDAC2020/FathomNet with no augmentation to establish mAP/AUC floor; verify ASL loss converges on imbalanced labels.
  2. **Ablation on offset strategy**: Compare fixed vs. adaptive depth offsets on a held-out validation split; measure not just mAP but per-class recall to identify which species benefit most from depth augmentation.
  3. **Depth estimation quality audit**: Randomly sample 50 images, visualize Depth-Anything outputs vs. ground-truth depth (if available) or manual inspection; correlate estimation error with augmentation artifact severity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a learnable offset generation mechanism outperform the current quantile-based heuristic for Depth Jitter?
- Basis in paper: [explicit] The authors state in the Limitations section that "Incorporating a learnable offset generation mechanism, possibly guided by uncertainty or class distribution, could further enhance performance," as the current perturbations relies on statistical heuristics.
- Why unresolved: The current implementation applies global or region-specific perturbations based on fixed quantiles (25th percentile) and uniform distributions, which may not capture task-specific contextual nuances or dynamic environmental variations.
- What evidence would resolve it: A comparative study where a model trained using a learnable offset policy (e.g., via reinforcement learning or gradient-based optimization) outperforms the static quantile-based method on the FathomNet or UTDAC2020 benchmarks.

### Open Question 2
- Question: Does Depth Jitter generalize to non-optical underwater sensing modalities or distinct depth-sensitive domains?
- Basis in paper: [explicit] The authors note that their "evaluation is limited to two underwater image classification datasets" and suggest that "further validation across broader domains (e.g., sonar, medical, or aerial depth-sensitive imagery) would help assess the generalizability of the method."
- Why unresolved: The physics-based augmentation model (UIFM) is derived specifically for optical light absorption and scattering; it is untested whether simulating depth offsets provides similar robustness benefits for sonar or non-marine imaging tasks.
- What evidence would resolve it: Experimental results applying the Depth Jitter methodology (or an adapted variant) to sonar object detection or medical imaging datasets, demonstrating statistically significant improvements over non-augmented baselines.

### Open Question 3
- Question: To what extent does the accuracy of the pre-trained depth estimator impact the downstream classification performance?
- Basis in paper: [inferred] The paper relies on Depth-Anything for depth estimation but acknowledges in the Limitations that "depth prediction models may struggle" in turbid or low-light conditions, suggesting the augmentation's effectiveness is "partially dependent on the fidelity of these depth estimates."
- Why unresolved: It is unclear if the classification gains are robust to noise in the depth map or if they degrade significantly when the monocular depth estimation fails in complex underwater geometry.
- What evidence would resolve it: An ablation study comparing downstream classification metrics when using high-precision LiDAR depth maps versus estimated monocular depth maps on the same dataset.

### Open Question 4
- Question: Can self-supervised learning frameworks enhance the robustness of Depth Jitter in poor visibility conditions?
- Basis in paper: [explicit] The authors propose in Future Work that "Self-supervised learning or diffusion-based augmentation could further enhance model robustness by learning better depth representations," specifically to address dynamic marine conditions.
- Why unresolved: Current performance relies on supervised classification with standard augmentation; it is unknown if learning depth representations directly from unlabeled underwater video (self-supervised) would improve the stability of the depth offsets in turbid water.
- What evidence would resolve it: Implementation of a self-supervised pre-training step to refine depth estimation before applying Depth Jitter, resulting in higher mAP on subsets of data labeled as "low visibility" or "turbid."

## Limitations

- The method assumes monocular depth estimation quality is sufficient for realistic augmentation; no analysis of depth estimation error propagation is provided.
- No ablation study on UIFM parameter estimation accuracy; if SeaThru fails to converge on certain images, the entire augmentation pipeline breaks down without fallback mechanisms.
- The fixed offset strategy for FathomNet (-4m to +15m) was tuned without clear justification; the optimal range may be dataset-specific and require extensive validation.

## Confidence

- **High confidence** in the physics-based augmentation mechanism (Depth Jitter produces more realistic underwater variations than pixel-level transforms).
- **Medium confidence** in the adaptive offset strategy's benefit (selective augmentation based on depth variance is intuitive but lacks rigorous ablation evidence).
- **Low confidence** in out-of-distribution generalization (performance on FathomNet is strong, but no testing on other underwater datasets or real-world deployment scenarios).

## Next Checks

1. **Depth estimation error analysis**: Measure Depth-Anything estimation accuracy on a subset of images with available ground-truth depth, then quantify how depth estimation error correlates with augmentation realism.
2. **Ablation of UIFM parameters**: Train models with synthetic augmentations using ground-truth depth vs. estimated depth to isolate the impact of depth estimation quality on final performance.
3. **Cross-dataset generalization**: Apply the trained model to a third underwater dataset (e.g., OUC) without fine-tuning to assess whether depth-aware augmentation provides robustness beyond the training distribution.