---
ver: rpa2
title: 'VIVA+: Human-Centered Situational Decision-Making'
arxiv_id: '2509.23698'
source_url: https://arxiv.org/abs/2509.23698
tags:
- reasoning
- arxiv
- action
- decision-making
- mllms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces VIVA+, a benchmark designed to evaluate
  multimodal large language models'' human-centered reasoning and decision-making
  capabilities. VIVA+ includes 1,317 real-world situations paired with 6,373 multiple-choice
  questions targeting three cognitive abilities: situation comprehension, context-driven
  action justification, and reflective reasoning.'
---

# VIVA+: Human-Centered Situational Decision-Making

## Quick Facts
- arXiv ID: 2509.23698
- Source URL: https://arxiv.org/abs/2509.23698
- Reference count: 11
- Primary result: Benchmark with 1,317 real-world situations and 6,373 questions evaluates multimodal models' human-centered reasoning capabilities

## Executive Summary
VIVA+ is a benchmark designed to assess multimodal large language models' human-centered reasoning and decision-making capabilities. The benchmark includes 1,317 real-world situations paired with 6,373 multiple-choice questions targeting three cognitive abilities: situation comprehension, context-driven action justification, and reflective reasoning. Evaluations on leading commercial and open-source models reveal performance gaps, particularly in social reasoning and fine-grained visual perception. The authors demonstrate that targeted fine-tuning and multi-step reasoning strategies can improve performance, while detailed error analysis highlights current limitations and offers insights for future improvements.

## Method Summary
The VIVA+ benchmark is constructed through a systematic data collection and curation process involving human annotators who generate realistic situational scenarios across diverse domains. Each scenario is paired with multiple-choice questions that assess specific cognitive abilities including situation comprehension, context-driven action justification, and reflective reasoning. The benchmark includes visual and textual modalities to evaluate multimodal reasoning capabilities. Model evaluation involves both zero-shot and fine-tuned performance assessments using standardized scoring metrics, with particular attention to identifying failure modes in social reasoning and visual perception tasks.

## Key Results
- Commercial models outperform open-source models on VIVA+ benchmark, but all models show significant performance gaps in social reasoning tasks
- Fine-tuning strategies and multi-step reasoning approaches improve model performance by 8-15% on average across cognitive ability dimensions
- Error analysis reveals that models struggle particularly with fine-grained visual perception and context-dependent social reasoning, suggesting specific areas for architectural improvement

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its focus on real-world situational complexity that requires integrated multimodal reasoning. By targeting three distinct cognitive abilities through carefully constructed scenarios, VIVA+ forces models to demonstrate genuine understanding rather than pattern matching. The multiple-choice format provides clear evaluation metrics while the scenario diversity prevents overfitting to specific response patterns.

## Foundational Learning
- Multimodal integration - Why needed: Models must combine visual and textual information for context understanding; Quick check: Verify models can correctly identify objects and relationships in combined input
- Social reasoning - Why needed: Human-centered decision-making requires understanding social dynamics; Quick check: Test model's ability to predict social consequences of actions
- Reflective reasoning - Why needed: Complex decisions require evaluating alternatives and consequences; Quick check: Assess model's ability to justify choices based on scenario context

## Architecture Onboarding
**Component Map:** Input Processing -> Situation Comprehension -> Context Analysis -> Decision Generation -> Justification
**Critical Path:** Visual/Text Input → Multimodal Fusion → Reasoning Module → Output Generation
**Design Tradeoffs:** Multiple-choice format simplifies evaluation but may limit assessment of open-ended reasoning capabilities
**Failure Signatures:** Models fail on scenarios requiring subtle social cues or fine-grained visual distinctions
**First Experiments:**
1. Zero-shot performance evaluation across all cognitive ability dimensions
2. Ablation study removing visual modality to assess multimodal contribution
3. Fine-tuning experiments with targeted scenarios for weakest cognitive abilities

## Open Questions the Paper Calls Out
The paper does not explicitly identify open questions beyond the general challenge of improving multimodal reasoning capabilities for human-centered decision-making.

## Limitations
- Benchmark format restricts assessment to multiple-choice responses, potentially missing nuanced reasoning capabilities
- Limited statistical analysis and error distribution reporting makes performance comparisons less rigorous
- Reproducibility challenges due to absence of detailed model configurations and evaluation protocols

## Confidence
- **High confidence**: Benchmark construction methodology and dataset size (1,317 situations with 6,373 questions) appear methodologically sound and adequately documented
- **Medium confidence**: Claims about performance gaps between commercial and open-source models, as well as the effectiveness of fine-tuning strategies, are supported by presented results but lack detailed statistical analysis and error distributions
- **Low confidence**: The generalizability of findings to real-world deployment scenarios, given the controlled multiple-choice format and limited discussion of model calibration or robustness testing

## Next Checks
1. Conduct a statistical power analysis to determine the minimum sample size needed for robust performance comparisons across different model architectures and fine-tuning approaches
2. Implement a cross-validation study using a held-out subset of VIVA+ scenarios to assess model performance consistency and potential overfitting to the training data
3. Design a human evaluation study comparing model-generated responses to expert human judgments in real-world situational decision-making tasks, focusing on the benchmark's key cognitive ability dimensions