---
ver: rpa2
title: Chinese Toxic Language Mitigation via Sentiment Polarity Consistent Rewrites
arxiv_id: '2505.15297'
source_url: https://arxiv.org/abs/2505.15297
tags:
- toxic
- detoxification
- toxicity
- polarity
- sentiment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TOXIREWRITE CN, the first Chinese detoxification
  dataset designed to preserve sentiment polarity in toxic language rewrites. The
  dataset contains 1,556 annotated triplets of toxic sentences, sentiment-aligned
  non-toxic rewrites, and toxic word labels across five real-world scenarios including
  emoji-induced, homophonic, and conversational toxicity.
---

# Chinese Toxic Language Mitigation via Sentiment Polarity Consistent Rewrites

## Quick Facts
- **arXiv ID:** 2505.15297
- **Source URL:** https://arxiv.org/abs/2505.15297
- **Reference count:** 13
- **Primary result:** Introduces TOXIREWRITE CN, the first Chinese detoxification dataset preserving sentiment polarity, and evaluates 17 LLMs, revealing challenges in balancing safety with expressive fidelity.

## Executive Summary
This paper presents TOXIREWRITE CN, a novel Chinese dataset for toxic language detoxification that prioritizes preserving the original sentiment polarity during rewrites. Unlike standard detoxification that neutralizes emotion, this task requires models to maintain the user's intended tone (e.g., frustration) while removing toxic elements. The dataset, constructed via a three-stage human-in-the-loop pipeline, includes 1,556 annotated triplets across five real-world scenarios including emoji-induced, homophonic, and conversational toxicity. Evaluations across 17 LLMs reveal that while commercial and MoE models achieve the highest detoxification accuracy (up to 88.24%), all models struggle with over-polite rewrites that distort emotional tone, particularly in implicit toxicity and multi-turn dialogue settings.

## Method Summary
The method employs a three-stage human-in-the-loop pipeline: (1) Data filtering using toxicity revalidation and suitability screening, (2) Coarse-to-fine rewriting where Qwen-Max generates initial drafts refined by human annotators via Label Studio, and (3) Cross-verification by three annotators using 5-point Likert scales (retaining samples â‰¥4.0). Two fine-tuned Qwen3-32B classifiers evaluate detoxification accuracy and sentiment polarity (toxic/neutral/polite). The dataset covers five scenarios: standard sentences, emoji-induced toxicity, homophonic toxicity, single-turn dialogue, and multi-turn dialogue.

## Key Results
- Commercial models (GPT-4o, Qwen-Max) and MoE architectures achieve highest detoxification accuracy (up to 88.24% sentence classification).
- All models struggle with over-polite rewrites, especially smaller dense models like Llama3-8B (21.53% polite rate).
- Performance degrades significantly on implicit toxicity (emoji/homophone) and multi-turn dialogue, with S-CLS scores dropping below 56% even for top models.
- Sentiment preservation remains challenging, with models often neutralizing emotional tone rather than maintaining it in civil form.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A coarse-to-fine human-in-the-loop pipeline yields high-quality sentiment-preserving rewrites by letting LLMs draft and humans refine.
- **Mechanism:** A strong LLM (Qwen-Max) generates a draft rewrite attempting to remove toxicity while keeping tone. Human annotators then accept, edit, or discard this draft, focusing effort on the hardest cases. This reduces annotator burden compared to full manual rewriting and ensures final quality and emotional consistency via post-editing.
- **Core assumption:** Annotators can reliably judge and correct sentiment drift and over-politeness, and the LLM's drafts are good enough to serve as efficient starting points rather than requiring full rewrites.
- **Evidence anchors:**
  - [abstract]: Mentions 1,556 "carefully annotated triplets" and the dataset's design goal.
  - [Section 3.3]: Describes the coarse-to-fine approach, model-based rewriting (Task 4), and human correction (Task 5), stating the goal is to "reduce annotator burden."
  - [corpus]: No direct external validation found.
- **Break condition:** Fails if the LLM drafts are systematically poor (e.g., always over-polite), forcing annotators to rewrite from scratch and eliminating efficiency gains.

### Mechanism 2
- **Claim:** Superior detoxification performance from commercial and MoE models is linked to greater model scale and specialized architectural capacity.
- **Mechanism:** Models with more total parameters and larger active parameter budgets have greater capacity to learn complex mappings from toxic to non-toxic language while maintaining context. MoE architectures may activate specialized experts for different toxicity types, providing a representational advantage over dense models with similar active parameter counts.
- **Core assumption:** The observed performance correlation with scale and MoE architecture is causal and not an artifact of other variables like proprietary training data.
- **Evidence anchors:**
  - [Section 4.3]: Explicitly notes that "larger models tend to perform better" and compares MoE models (Llama4-Maverick vs. Scout) to show better performance from a larger expert pool with the same active budget.
  - [Section 4.6]: Notes performance degradation in multi-turn dialogue, highlighting the challenge for models with "limited capacity to manage long-range discourse."
  - [corpus]: No direct external papers confirm this specific MoE advantage.
- **Break condition:** The advantage may not hold for toxicity types or languages underrepresented in the models' pre-training data, regardless of scale.

### Mechanism 3
- **Claim:** Preserving negative sentiment requires models to overcome a strong default bias toward over-polite generation.
- **Mechanism:** Fine-tuning for safety (e.g., RLHF) penalizes toxicity but can inadvertently push models toward safe, polite outputs. Successful sentiment-preserving detoxification requires models to decouple the concept of "negative emotion" from "toxic content" and modulate only the latter, a nuanced ability that seems more prevalent in larger or more advanced models.
- **Core assumption:** A high "polite rate" is an undesirable failure mode of over-sanitization, and the goal is to maintain the original negative sentiment in a civil form.
- **Evidence anchors:**
  - [Section 4.4]: Directly states models struggle with "drifting into overly polite styles" and shows smaller dense models have higher polite rates (e.g., Llama3-8B at 21.53%) compared to larger MoE models.
  - [Section 1]: Introduces the problem of "overly polite rewrites, distorting the emotional tone."
  - [corpus]: No direct external evidence.
- **Break condition:** This framing depends on the task goal. If the desired outcome changes from "express frustration civilly" to "resolve a conflict," a polite tone may be preferred, and the bias becomes a feature.

## Foundational Learning

- **Concept: Sentiment Polarity Preservation**
  - **Why needed here:** This is the paper's core novelty. Standard detoxification often neutralizes all emotion. You must understand that this task evaluates a model's ability to retain the original emotional tone (e.g., anger, frustration) while only removing the toxic components.
  - **Quick check question:** Given an angry, toxic complaint, a successful rewrite should express the same complaint in a civil but still dissatisfied tone, not convert it into a polite suggestion. True or False?

- **Concept: Implicit & Contextual Toxicity**
  - **Why needed here:** The dataset includes emoji-induced, homophonic, and dialogue-based toxicity, which is harder to detect and rewrite than explicit insults. Understanding this is key to interpreting the performance degradation in these scenarios.
  - **Quick check question:** Why is detoxifying a multi-turn dialogue more challenging than a single sentence?

- **Concept: Over-Sanitization vs. Neutralization**
  - **Why needed here:** The paper distinguishes between a "neutral" rewrite (which preserves the original sentiment) and a "polite" rewrite (which over-sanitizes and distorts intent). Evaluating models requires checking for this specific failure mode.
  - **Quick check question:** A rewrite that changes "This delivery is trash!" to "The delivery could be improved" is an example of what kind of failure, according to the paper's framework?

## Architecture Onboarding

- **Component map:** Toxic sentence -> Model Under Test (LLM) -> Generated rewrite -> Detoxification Classifier (Qwen3-32B) -> S-CLS score -> Sentiment Classifier (Qwen3-32B) -> Polarity classification (toxic/neutral/polite) -> Additional metrics (BLEU, ChrF++, BERTScore, COMET, Text2Vec) -> Human reference comparison
- **Critical path:** The most critical and challenging path is the sentiment-preserving detoxification of multi-turn dialogue. This requires the model to process context across turns, identify cumulative or contextual toxicity, and generate a rewrite that maintains emotional continuity without introducing toxicity or excessive politeness.
- **Design tradeoffs:**
  - **Dataset Size vs. Quality:** The dataset is modest (1,556 samples) but high-quality due to intensive filtering and a multi-stage human-in-the-loop process.
  - **Model Scale vs. Cost:** Better performance requires larger commercial or MoE models, which have higher inference costs.
  - **Safety vs. Expressive Fidelity:** Stricter toxicity removal risks over-politeness; preserving tone risks leaving residual toxicity. The paper argues for the latter as a goal.
- **Failure signatures:**
  1. **High Polite Rate:** Model outputs sound unnaturally formal or customer-service-like ("I would appreciate it if..."), indicating a failure to preserve the original negative sentiment.
  2. **Emoji/Homophone Miss:** Model fails to recognize a toxic homophone (e.g., using a sound-alike character for an insult) and leaves it unchanged in the rewrite.
  3. **Context Drift in Dialogue:** Model loses track of the emotional tone across multiple turns, leading to a rewrite that is inconsistent with the dialogue's context.
- **First 3 experiments:**
  1. **Baseline Evaluation:** Run your chosen LLM on all five subsets of TOXIREWRITE CN. Report primary metrics: S-CLS (detox accuracy), Neutral Rate (sentiment preservation), and Polite Rate (over-sanitization). Compare against the provided benchmark tables.
  2. **Ablation on Prompting:** Test different prompts (e.g., "Rewrite to be civil but keep the frustration" vs. a standard "Make this non-toxic" prompt) to see if you can reduce the polite rate without hurting detox accuracy.
  3. **Error Analysis on Implicit Toxicity:** Manually review your model's failures on the emoji and homophone subsets. Classify errors into (a) failure to detect, (b) failure to rewrite contextually, or (c) over-polite rewrite. This will pinpoint specific model weaknesses.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can smaller, open-source models be optimized for targeted emotional style control to avoid the over-sanitization (high politeness rates) observed in dense architectures?
- **Basis in paper:** [explicit] Section 4.4 notes that sentiment-preserving detoxification remains a challenge for smaller models and explicitly calls for "future work on targeted emotional style control."
- **Why unresolved:** The experiments show that dense open-source models (e.g., Llama3-8B) produce polite rewrites in over 21% of cases, significantly distorting user intent compared to commercial models.
- **What evidence would resolve it:** Developing fine-tuning strategies or steering mechanisms that enable 7B-8B parameter models to maintain a neutral tone rate comparable to GPT-4o without increasing toxicity.

### Open Question 2
- **Question:** Can sentiment-aware detoxification frameworks be effectively extended to multilingual or code-mixed Chinese contexts?
- **Basis in paper:** [explicit] The Limitations section explicitly suggests that future work should "extend the dataset to multilingual and code-mixed Chinese contexts."
- **Why unresolved:** The current dataset focuses on standard Chinese expressions and specific perturbations (emojis/homophones), but does not cover the complexities of mixed-language input common in online discourse.
- **What evidence would resolve it:** Constructing a dataset extension with code-mixed toxic inputs and demonstrating that current or refined models can successfully rewrite them while preserving sentiment.

### Open Question 3
- **Question:** What computational approaches are required to model discourse-level reasoning for detoxifying multi-turn dialogues where toxicity emerges contextually?
- **Basis in paper:** [inferred] Section 4.6 concludes that "successful detoxification in dialogue requires discourse-level reasoning... beyond sentence rewriting," based on the finding that performance drops sharply in multi-turn settings.
- **Why unresolved:** Current models struggle to trace toxicity that accumulates over multiple turns, leading to a significant drop in Detoxification Accuracy (e.g., S-CLS scores falling below 56% for top models).
- **What evidence would resolve it:** A model architecture or context-aware mechanism that maintains high S-CLS scores in the multi-turn dialogue subset by tracking emotional continuity across turns.

## Limitations
- The dataset size (1,556 samples) is modest for training robust detoxification models, limiting generalizability.
- The human-in-the-loop efficiency claims are stated but not quantitatively validated through controlled experiments.
- Evaluation relies on automatic metrics for fluency and content preservation, which may not capture nuanced linguistic quality or semantic equivalence, particularly for culturally specific Chinese expressions.
- The performance advantage of commercial/MoE models may be due to proprietary training data rather than scale alone, as this is not controlled for in the study.

## Confidence

**High Confidence:**
- The dataset construction methodology (three-stage human-in-the-loop pipeline) is clearly specified and reproducible.
- The observation that models struggle with implicit toxicity (emoji/homophone) and multi-turn dialogue is well-supported by the empirical results.

**Medium Confidence:**
- The claim that commercial and MoE models achieve superior detoxification accuracy due to greater scale and architectural capacity. While results show correlation, causation is not established.
- The assertion that models exhibit a bias toward over-polite rewrites. This is observed but the underlying cause (RLHF, architecture, or other factors) is not definitively proven.

**Low Confidence:**
- The efficiency gains of the coarse-to-fine pipeline are stated but not quantitatively validated.
- The generalizability of findings to other languages or toxicity types beyond those in the dataset.

## Next Checks

1. **Quantify Human-in-the-Loop Efficiency:** Conduct a controlled experiment comparing the coarse-to-fine pipeline (LLM draft + human edit) against full manual rewriting. Measure time per sample, acceptance rates of LLM drafts, and inter-annotator agreement to validate claimed efficiency gains.

2. **Validate Classifier Reliability:** Perform an ablation study where a subset of rewrites is evaluated by both the fine-tuned Qwen3-32B classifiers and human raters. Compute inter-rater reliability (e.g., Cohen's Kappa) to assess the classifiers' accuracy in detecting detoxification and sentiment polarity.

3. **Test Cross-Lingual Transfer:** Apply the best-performing models (e.g., GPT-4o, Qwen-Max) to a small, manually translated subset of TOXIREWRITE CN into another language (e.g., English or Japanese). Evaluate if the observed patterns of over-politeness and implicit toxicity handling persist, indicating whether findings are language-specific or model-capacity driven.