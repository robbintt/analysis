---
ver: rpa2
title: Mitigating Spectral Bias in Neural Operators via High-Frequency Scaling for
  Physical Systems
arxiv_id: '2503.13695'
source_url: https://arxiv.org/abs/2503.13695
tags:
- neural
- spectral
- high-frequency
- prediction
- errors
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes high-frequency scaling (HFS) to mitigate spectral
  bias in convolutional-based neural operators, which tend to produce over-smoothed
  predictions for physical systems with high-frequency modes like turbulence and multiphase
  flows. HFS decomposes latent feature maps into low- and high-frequency components
  and learns separate scaling parameters for each, enhancing high-frequency information
  propagation without degrading low-frequency accuracy.
---

# Mitigating Spectral Bias in Neural Operators via High-Frequency Scaling for Physical Systems

## Quick Facts
- **arXiv ID**: 2503.13695
- **Source URL**: https://arxiv.org/abs/2503.13695
- **Reference count**: 40
- **Primary result**: HFS reduces prediction errors by 12.4% RMSE and 18.2% bubble RMSE on average across pool boiling datasets

## Executive Summary
This paper addresses spectral bias in convolutional neural operators for physical systems with high-frequency modes like turbulence and multiphase flows. The proposed high-frequency scaling (HFS) method decomposes latent feature maps into low- and high-frequency components, learning separate scaling parameters for each. When applied to UNet neural operators, HFS significantly reduces prediction errors on saturated and subcooled pool boiling datasets while also improving energy spectrum alignment at high wavenumbers. The method requires negligible additional memory and only modest computational overhead, making it practical for deployment.

## Method Summary
HFS operates by decomposing each convolutional layer's output into low-frequency (DC) and high-frequency (HFC) components through patch-based processing. For each feature map, the method divides the spatial domain into non-overlapping patches, computes the global mean across patches as DC, and defines HFC as the deviation of each patch from this mean. Separate learnable scaling parameters (λ_DC and λ_HFC) are applied to each component and then recombined. This approach enhances high-frequency information propagation without degrading low-frequency accuracy, addressing the fundamental spectral bias that causes neural operators to produce over-smoothed predictions for physical systems.

## Key Results
- HFS reduces prediction errors in saturated and subcooled pool boiling datasets by 12.4% (RMSE) and 18.2% (bubble RMSE) on average
- For 2D Kolmogorov flow, HFS reduces relative error from 5.3% to 4.7%
- When integrated with diffusion models, HFS-enhanced neural operators further reduce spectral bias while maintaining mean prediction accuracy
- HFS requires negligible additional memory (<0.1% parameter increase) and only modest computational overhead

## Why This Works (Mechanism)

### Mechanism 1: Latent Space Frequency Decomposition via Patch-based DC/HFC Separation
- Decomposes latent feature maps into low-frequency (DC) and high-frequency (HFC) components through patch-based processing without Fourier transforms
- Assumes patches exhibit meaningful DC/HFC separation where spatially localized features create distinguishable high-frequency content
- Breaks when all spatial patches contain similar frequency content (e.g., turbulent jet with globally distributed small-scale features)

### Mechanism 2: Learned Per-Channel Scaling with High-Frequency Emphasis
- Gradient descent learns λ_HFC > λ_DC on average, explicitly biasing the network toward preserving high-frequency components
- Assumes the loss gradient provides sufficient signal to distinguish useful high-frequency content from noise
- Breaks if base NO is severely underparameterized or produces extremely over-smoothed predictions

### Mechanism 3: Diffusion Model as Conditional High-Frequency Refinement
- Score-based diffusion models conditioned on NO predictions can further recover missing high-frequency modes
- Assumes conditional distribution p(X|F_θ) is well-defined and learnable, and DM training covers relevant noise spectrum
- Breaks if NO predictions have significant systematic errors in large-scale features or if DM training cost is prohibitive

## Foundational Learning

- **Spectral Bias in Neural Networks**:
  - Why needed here: HFS addresses the tendency of neural networks to learn low-frequency components faster, producing over-smoothed solutions
  - Quick check question: Given a neural operator trained on 2D turbulence, would you expect larger errors at low or high wavenumbers, and why?

- **UNet and ResUNet Architecture**:
  - Why needed here: HFS integrates into ResUNet encoder/decoder layers; understanding skip connections and feature map resolution hierarchy is essential
  - Quick check question: In a 5-level UNet encoder-decoder, at which spatial resolutions would you expect the highest vs. lowest frequency information to be represented?

- **Score-Based Diffusion Models and Langevin Dynamics**:
  - Why needed here: Diffusion model component uses denoising score matching with Langevin dynamics
  - Quick check question: In score-based diffusion, why does a smaller noise scale σ cause the score function to focus more on high-frequency details?

## Architecture Onboarding

- **Component map**: Base NO (ResUNet) -> HFS modules (after conv layers and skip connections) -> Optional diffusion model (score network)
- **Critical path**: Implement ResUnet baseline -> Add HFS modules -> Train jointly with same optimizer -> If using diffusion: pre-train HFS-enhanced NO first, then train DM
- **Design tradeoffs**: Patch size p (smaller increases localization but may introduce artifacts), HFS vs. frequency-domain scaling (HFS avoids FFT cost), DM integration (2-4× training cost)
- **Failure signatures**: Uniform gradient scaling indicates HFS not discriminative for this data type, λ_HFC ≈ λ_DC after training suggests model not learning frequency separation, DM produces artifacts if NO prior is too inaccurate
- **First 3 experiments**:
  1. Train ResUnet without HFS on subcooled pool boiling to establish baseline before HFS addition
  2. Add HFS modules to same ResUnet, compare RMSE and spectral errors, verify λ_HFC > λ_DC
  3. Apply trained HFS-enhanced NO to turbulent jet dataset, check gradient ratio map for uniform scaling indicating HFS ineffectiveness

## Open Questions the Paper Calls Out

- **Open Question 1**: Can alternative initialization strategies or dedicated optimization frameworks for the learnable scaling parameters accelerate convergence compared to standard gradient descent?
  - Basis: Authors state "It would be interesting to explore faster convergence by using different initializations and optimization frameworks for the scaling parameters in future work."
  - Why unresolved: Current work uses same learning rate as network without exploring specialized optimization routines
  - What evidence would resolve it: Comparative training curves showing reduced epochs to convergence with adaptive optimizers

- **Open Question 2**: Can scaling in the frequency domain be optimized to outperform spatial-domain HFS in scenarios where computational overhead of Fourier transforms is acceptable?
  - Basis: Authors note frequency-domain scaling improves accuracy but was discarded due to cost, adding "However, it may worth investigating this method in future work."
  - Why unresolved: Frequency-domain method was discarded primarily due to computational cost (iteration time increased from 34.5s to 52.6s)
  - What evidence would resolve it: Study demonstrating frequency-domain scaling achieves significantly lower spectral errors

- **Open Question 3**: How can the HFS patch-decomposition strategy be adapted to effectively mitigate spectral bias in systems with globally distributed small-scale features like turbulent jets?
  - Basis: Authors show HFS is ineffective for turbulent jet case because "the DC and HFC cannot be directly separated from spatial patching as all the patches may contain similar frequency components."
  - Why unresolved: Method relies on assumption that high-frequency information is localized, which fails for homogeneous turbulence
  - What evidence would resolve it: Modification of HFS mechanism that results in non-uniform gradient scaling and reduced error for turbulent jet dataset

## Limitations

- Patch size p is unspecified in the paper, creating ambiguity in DC/HFC decomposition granularity and potential reproducibility issues
- No ablation study isolating HFS effectiveness from general neural operator improvement across datasets
- Diffusion model integration lacks quantitative comparison of training cost vs. accuracy gain across different NO baselines
- Physical interpretability of λ_DC and λ_HFC values remains limited despite visual evidence of high-frequency emphasis

## Confidence

- **High confidence**: HFS reduces prediction errors (RMSE) on pool boiling and Kolmogorov flow datasets when baseline NO has reasonable accuracy
- **Medium confidence**: HFS achieves frequency-selective enhancement via learned per-channel scaling, based on observed λ_HFC > λ_DC patterns
- **Low confidence**: HFS effectiveness generalizes to all physical systems with high-frequency modes, given boundary case (turbulent jet) where HFS provides uniform scaling

## Next Checks

1. **Cross-dataset transferability test**: Apply HFS-enhanced NO to a new physical system (e.g., Rayleigh-Bénard convection) with localized high-frequency features; measure if bubble RMSE-style metrics improve by 10-20% as in pool boiling cases
2. **Patch size sensitivity analysis**: Systematically vary patch size p (2×2, 4×4, 8×8) on Kolmogorov flow; quantify impact on E_Fhigh and spectral error at different wavenumbers to identify optimal p
3. **Frequency decomposition verification**: Extract λ_DC and λ_HFC values from trained HFS-enhanced NO; compute correlation between λ_HFC magnitude and known high-frequency feature locations (bubble interfaces, sharp gradients) to validate learned frequency emphasis