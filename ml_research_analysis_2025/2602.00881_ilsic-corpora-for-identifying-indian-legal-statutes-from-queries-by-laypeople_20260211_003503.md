---
ver: rpa2
title: 'ILSIC: Corpora for Identifying Indian Legal Statutes from Queries by Laypeople'
arxiv_id: '2602.00881'
source_url: https://arxiv.org/abs/2602.00881
tags:
- statutes
- queries
- laypeople
- legal
- court
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The study introduces ILSIC, the first benchmark for identifying
  Indian legal statutes from laypeople queries. It contains two datasets: ILSIC-Lay
  with 8K real-world queries covering 567 statutes, and ILSIC-Multi with both laypeople
  and court judgment-based data for direct comparison.'
---

# ILSIC: Corpora for Identifying Indian Legal Statutes from Queries by Laypeople

## Quick Facts
- arXiv ID: 2602.00881
- Source URL: https://arxiv.org/abs/2602.00881
- Reference count: 40
- Primary result: Introduces first benchmark for identifying Indian legal statutes from laypeople queries, showing GPT-4.1 and fine-tuned Gemma-3 achieve up to 35% F1-score

## Executive Summary
The study introduces ILSIC, the first benchmark for identifying Indian legal statutes from laypeople queries. It contains two datasets: ILSIC-Lay with 8K real-world queries covering 567 statutes, and ILSIC-Multi with both laypeople and court judgment-based data for direct comparison. Experiments on ILSIC-Lay show GPT-4.1 and fine-tuned Gemma-3 outperform other models, achieving up to 35% F1-score. ILSIC-Multi reveals that models trained solely on court data perform poorly on laypeople queries, while transfer learning from court to laypeople data offers limited improvement. The findings highlight the importance of using real laypeople queries for training legal statute identification systems. All datasets, code, and trained models are publicly available.

## Method Summary
The paper introduces two datasets for multi-label legal statute identification: ILSIC-Lay with 8,127 real-world laypeople queries covering 567 statutes, and ILSIC-Multi with 399 common statutes shared between laypeople queries and court judgments. Models are evaluated using macro and micro F1-scores for multi-label classification. Fine-tuning uses QLoRA with 4-bit quantization, while zero-shot and two-shot inference use SBERT-based example selection. The verbalization algorithm normalizes predictions through fuzzy matching against known statute lists.

## Key Results
- GPT-4.1 and fine-tuned Gemma-3 achieve up to 35% F1-score on ILSIC-Lay test set
- Models trained solely on court data perform poorly on laypeople queries (μF1 ~20% vs ~33% for laypeople training)
- Two-shot inference consistently outperforms retrieval-augmented generation (TSI μF1 34.23% vs RAG 32.44%)
- All models show reduced performance on rare statutes, with systematic frequency effects across statute groups

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Domain-specific fine-tuning on laypeople queries substantially outperforms training on formal court documents when the target domain is informal legal queries
- Mechanism: The linguistic gap between formal court judgments (readability score ~46.81, longer sentences ~20.67 words) and laypeople queries (readability ~64.53, shorter sentences ~18.28 words) creates a distribution shift. Models trained on court data learn patterns specific to legal professionals' language that do not transfer to informal citizen queries
- Core assumption: The performance gap reflects genuine linguistic mismatch rather than simply data quantity differences
- Evidence anchors: [abstract] "Models trained solely on court data perform poorly on laypeople queries"; [section] Table 5 shows court-only training achieves ~19.95-23.10% μF1 vs. ~32.39-34.76% for laypeople training on the same statutes

### Mechanism 2
- Claim: Few-shot in-context examples provide more reliable guidance than retrieval-augmented statute definitions for this specific task
- Mechanism: TSI shows consistent improvements over RAG across models. The paper finds that SBERT retrieval precision is only ~31.99% at top-15, meaning the RAG pipeline often retrieves irrelevant statutes. Two-shot examples, selected via SBERT similarity from the same category, provide task framing that helps models generalize the statute identification pattern without relying on noisy retrieved content
- Core assumption: The SBERT retrieval quality is the bottleneck for RAG performance, not the LLM's reasoning capacity
- Evidence anchors: [section] Table 3: GPT-4.1 TSI achieves 34.23% μF1 vs. RAG's 32.44%; [section] Appendix Table 16: SBERT achieves only 31.99% macro precision in top-15 retrieval

### Mechanism 3
- Claim: Statute frequency and category overlap with training data strongly influence model performance, creating head-statute bias
- Mechanism: Performance drops systematically for rare statutes (Groups 6-10 in Figure 2) and for categories with more held-out statutes. The Taxation spike (μF1 ~0.6) is explained not by model capability but by having only 1 held-out statute among 6 test statutes
- Core assumption: The frequency correlation reflects memorization or pattern learning rather than genuine legal reasoning generalization
- Evidence anchors: [abstract] "All models have reduced performance over the rare statutes"; [section] Figure 2 shows clear performance decline across statute frequency groups

## Foundational Learning

- Concept: **Multi-label classification evaluation (Macro vs Micro F1)**
  - Why needed here: Each query can cite multiple statutes (avg 2.2-2.4 citations). Macro-F1 weights all statutes equally (sensitive to rare statutes), while Micro-F1 is dominated by frequent statutes. The paper reports both to show head-vs-tail performance trade-offs
  - Quick check question: If a model only predicts the most common statute for every query, would macro-F1 or micro-F1 be higher?

- Concept: **Domain adaptation vs transfer learning**
  - Why needed here: The paper tests whether sequential training (court→laypeople) helps. Results show limited benefit, suggesting the domains are sufficiently different that court knowledge doesn't provide a useful prior
  - Quick check question: What is the difference between domain adaptation (adapting a model to a new domain) and multi-domain training (training on both domains simultaneously)?

- Concept: **Held-out evaluation for generalization testing**
  - Why needed here: 8 statutes appear only in test data. Evaluating on these directly measures whether models can reason about unseen legal provisions rather than memorizing training associations
  - Quick check question: Why might held-out evaluation be particularly important for legal AI systems deployed in practice?

## Architecture Onboarding

- Component map:
  ILSIC-Lay (8K queries, 567 statutes)
  ├── Train: 6,465 | Val: 826 | Test: 836
  └── 8 held-out statutes in val/test

  ILSIC-Multi (common 399 statutes)
  ├── ILSIC-Multi_lay: 5,793 train
  ├── ILSIC-Multi_court: 12,930 train (formal judgments)
  └── Shared test set: 757 laypeople queries

  Evaluation Pipeline:
  Query → Model → Statute Predictions → Verbalization → Match against 567/399 statutes

- Critical path:
  1. Start with ILSIC-Multi (399 statutes) for controlled comparison experiments
  2. Use ILSIC-Lay (567 statutes) for full benchmarking
  3. Always evaluate on laypeople test set regardless of training source
  4. Report both macro and micro F1 to capture head/tail trade-offs

- Design tradeoffs:
  - **Training data source**: Court data is abundant (~2x size of laypeople) but ineffective for laypeople target
  - **Retrieval for RAG**: SBERT is fast but noisy (31.99% precision); specialized legal encoders (SAILER) fail to generalize to Indian law
  - **Few-shot vs fine-tuning**: TSI favors rare statutes (higher mF1); SFT favors frequent ones (higher μF1)

- Failure signatures:
  - Court-only training: μF1 ~20% vs ~33% for laypeople training (domain mismatch)
  - RAG underperforming TSI: Indicates retriever bottleneck
  - Hallucinated statutes: Non-existent statute numbers in zero-shot outputs (see Appendix Table 18)
  - Category-specific collapse: Constitutional and I.P. Law show ~50% lower F1 than Taxation (but check for held-out statute count)

- First 3 experiments:
  1. **Baseline comparison**: Run ZSI and TSI with GPT-4.1 on ILSIC-Lay test set to verify ~31.6% and ~34.2% μF1 benchmarks
  2. **Domain ablation**: Fine-tune Gemma-3 on ILSIC-Multi_court vs ILSIC-Multi_lay, evaluate on shared laypeople test, confirm ~10+ point μF1 gap
  3. **Retrieval bottleneck test**: Replace SBERT with exact statute text retrieval (or oracle retrieval) in RAG to isolate whether the ~2-point TSI > RAG gap is due to retriever quality

## Open Questions the Paper Calls Out

- **Question**: Can alternative data combination strategies (beyond sequential transfer learning) effectively leverage abundant court data to improve performance on laypeople LSI?
  - Basis in paper: [explicit] Authors state: "In future, we would like to try out other ways of combining the court and laypeople training data to further improve upon training purely on a small laypeople dataset."
  - Why unresolved: Transfer learning showed marginal gains only for Llama-3; synthetic laypeople queries failed to bridge the linguistic gap
  - What evidence would resolve it: Testing multi-task learning, curriculum learning, or mixture-of-experts approaches that leverage court data abundance while preserving laypeople-specific linguistic patterns

- **Question**: How can retrieval components be improved to make RAG competitive with few-shot inference for laypeople LSI?
  - Basis in paper: [inferred] RAG consistently underperformed Two-Shot Inference across all models; explicit analysis attributed this to poor SBERT retrieval (31.99% macro precision at top-15)
  - Why unresolved: The retrieval bottleneck was identified but not addressed; SAILER also failed due to US/Canadian legal pre-training mismatch with Indian law
  - What evidence would resolve it: Evaluating retrievers fine-tuned on Indian laypeople legal queries, or developing hybrid retrieval combining keyword matching with semantic similarity

- **Question**: How can models achieve better generalization to rare statutes and held-out statutes in laypeople LSI?
  - Basis in paper: [inferred] All models showed sharply reduced performance on rare statutes (Groups 6-10); fine-tuned models amplified frequency bias while GPT-4.1 maintained smoother degradation
  - Why unresolved: Current SFT approaches favor frequent statutes (higher μF1 vs mF1); no mechanism addresses the long-tail distribution problem
  - What evidence would resolve it: Testing data augmentation for rare statutes, few-shot learning with statute-specific examples, or prototype-based classification methods

- **Question**: Can multilingual laypeople LSI datasets effectively capture India's linguistic diversity for practical deployment?
  - Basis in paper: [explicit] Authors state: "It remains an important future work to develop a multilingual dataset of laypeople queries in various Indian languages."
  - Why unresolved: Current dataset is English-only; India has 22 official languages and significant code-mixing in real-world legal queries
  - What evidence would resolve it: Creating datasets in Hindi, Bengali, Tamil, etc., and evaluating cross-lingual transfer or multilingual models

## Limitations
- Limited linguistic diversity: Current dataset is English-only, not capturing India's multilingual legal query landscape
- Retrieval bottleneck: SBERT-based retrieval achieves only 31.99% precision at top-15, limiting RAG effectiveness
- Frequency bias: All models show systematic performance degradation on rare statutes, indicating memorization over reasoning

## Confidence
- **High**: RAG underperformance due to retrieval bottleneck; frequency effects on performance; court vs laypeople domain mismatch
- **Medium**: Linguistic gap as primary cause of domain transfer failure; verbalization algorithm effectiveness
- **Low**: Claims about legal reasoning vs memorization without held-out statute ablation

## Next Checks
1. **Held-out statute ablation**: Evaluate GPT-4.1 on held-out statutes only (Section 5.4) to distinguish memorization from reasoning - current data shows 40-54% F1 which is surprisingly high and may indicate frequency effects dominate
2. **Retriever quality isolation**: Replace SBERT with exact statute text matching or oracle retrieval in RAG pipeline to determine if the ~2-point TSI > RAG gap is truly due to retriever quality vs. prompting effects
3. **Annotation quality control**: Compare inter-annotator agreement or consistency metrics between court and laypeople datasets to rule out annotation artifacts as explanation for domain performance differences