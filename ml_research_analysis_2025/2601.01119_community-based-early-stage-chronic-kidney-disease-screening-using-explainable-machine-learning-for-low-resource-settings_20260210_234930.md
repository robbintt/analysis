---
ver: rpa2
title: Community-Based Early-Stage Chronic Kidney Disease Screening using Explainable
  Machine Learning for Low-Resource Settings
arxiv_id: '2601.01119'
source_url: https://arxiv.org/abs/2601.01119
tags:
- feature
- features
- disease
- kidney
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of early-stage chronic kidney
  disease (CKD) detection in low-resource settings by developing an explainable machine
  learning framework tailored to the Bangladeshi and South Asian context. The core
  method uses a community-based dataset from Bangladesh and evaluates twelve machine
  learning classifiers across multiple feature domains, employing ten feature selection
  techniques to identify robust predictors.
---

# Community-Based Early-Stage Chronic Kidney Disease Screening using Explainable Machine Learning for Low-Resource Settings

## Quick Facts
- arXiv ID: 2601.01119
- Source URL: https://arxiv.org/abs/2601.01119
- Reference count: 40
- Primary result: 90.40% balanced accuracy for CKD detection using non-pathology-test features

## Executive Summary
This study develops an explainable machine learning framework for early-stage chronic kidney disease (CKD) screening in low-resource settings, specifically targeting the Bangladeshi and South Asian context. The approach uses community-based data from Bangladesh and evaluates twelve machine learning classifiers across multiple feature domains, employing ten feature selection techniques to identify robust predictors. SHAP (SHapley Additive exPlanations) provides model explainability. The proposed models achieve high performance, with a balanced accuracy of 90.40% using minimal non-pathology-test features. External validation on three independent datasets confirms strong generalizability, with sensitivity ranging from 78% to 98%.

## Method Summary
The framework uses a primary dataset of 284 Bangladeshi participants with 24 categorical features across five domains (socio-demographic, lifestyle/habit, medical history, clinical exam, pathology). Twelve classifiers are evaluated with 10-fold stratified cross-validation, and feature selection employs ten methods including RFECV with various estimators. The S1 model uses all features (90.40% balanced accuracy with Decision Tree), while the S2 model uses only non-pathology-test features (89.23% balanced accuracy with CatBoost). SHAP provides interpretability by identifying clinically meaningful predictors like hypertension and age.

## Key Results
- High predictive accuracy (90.40% balanced accuracy) using only non-pathology-test features
- Strong external validation across three independent datasets with sensitivity ranging from 78% to 98%
- RFECV-based feature selection consistently outperforms other methods
- SHAP interpretation confirms clinical plausibility of model predictions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Non-pathological features can achieve high predictive parity with laboratory tests for early-stage CKD in specific populations.
- **Mechanism:** Risk markers like hypertension and age act as low-cost proxies for renal decline, forcing the model to learn latent risk profiles embedded in accessible clinical history rather than relying on direct biological measurements.
- **Core assumption:** The correlation between these simple risk markers and early-stage CKD is sufficiently stable in the target population to replace diagnostic biomarkers.
- **Evidence anchors:**
  - Minimal non-pathology-test features demonstrated excellent predictive capability at 89.23%.
  - The S2 common subset (Hypertension and Age 60+y) achieves 83.50% balanced accuracy while requiring only non-pathological features.
- **Break condition:** If the target population has atypical CKD etiologies that do not correlate strongly with standard risk factors, the proxy mechanism fails.

### Mechanism 2
- **Claim:** RFECV isolates a more generalizable signal than statistical filtering or full-feature approaches.
- **Mechanism:** RFECV wraps feature selection around the learning algorithm, iteratively pruning features that contribute noise rather than signal, reducing overfitting to the small primary dataset.
- **Core assumption:** The wrapper method successfully identifies features that capture underlying causal factors rather than dataset-specific spurious correlations.
- **Evidence anchors:**
  - RFECV-based feature selection consistently outperforms LASSO and the MWU test.
  - The common feature subset outperforms both the union and all-feature sets.
- **Break condition:** If the training data size is too small to support reliable cross-validation during elimination steps, selected features may be artifacts of sample variance.

### Mechanism 3
- **Claim:** Model explainability (SHAP) validates the clinical plausibility of the ML classifier.
- **Mechanism:** SHAP values decompose predictions into additive feature contributions, ensuring high-risk outputs are driven by established clinical factors like hypertension and age.
- **Core assumption:** Feature attributions that align with clinical theory imply a model will behave safely and predictably on new patients.
- **Evidence anchors:**
  - SHAP interpretation identified clinically meaningful predictors consistent with established CKD risk factors.
  - Age 60+y and Hypertension push predictions toward CKD (positive SHAP values).
- **Break condition:** If the model relies on complex feature interactions that SHAP's standard summary plots mask or misrepresent, the "clinical alignment" may be superficial.

## Foundational Learning

- **Concept:** **Recursive Feature Elimination (RFE)**
  - **Why needed here:** The paper emphasizes that smaller, selected feature sets outperformed larger ones. You must understand how RFE iteratively removes the weakest features to grasp why the "Minimal Non-Pathology" model works better than the full set.
  - **Quick check question:** Why would a model with fewer features (S2) outperform a model with all available features (S1) in a low-resource setting?

- **Concept:** **Class Imbalance & Balanced Accuracy**
  - **Why needed here:** The dataset is imbalanced (112 CKD vs 172 Non-CKD), and sensitivity is critical for screening. Standard accuracy is misleading here; balanced accuracy (average of sensitivity and specificity) is the primary metric.
  - **Quick check question:** If a model predicts "Non-CKD" for everyone, it achieves high standard accuracy but 0% sensitivity. How does "balanced accuracy" penalize this?

- **Concept:** **SHAP (SHapley Additive exPlanations)**
  - **Why needed here:** This is the "Explainable" component. You need to interpret waterfall plots to understand why a specific patient was flagged as high risk.
  - **Quick check question:** In a SHAP waterfall plot, does a red bar extending to the right indicate a feature pushing the prediction toward or away from the CKD diagnosis?

## Architecture Onboarding

- **Component map:** 24 categorical variables (SD, LH, MH, CE, Path) -> MICE imputation -> Discretization -> Harmonization -> One-hot encoding -> 10 parallel FS methods (dominant: RFECV with various estimators) -> 12 classifiers tested -> SHAP TreeExplainer or KernelExplainer

- **Critical path:** The **Feature Selection** phase is the bottleneck. The paper demonstrates that poor selection degrades performance. The transition from "All Features" to "RFECV-selected S2" is where the performance gain happens.

- **Design tradeoffs:**
  - S1 vs. S2 Sets: S1 includes pathology and hits 90.4% accuracy. S2 excludes all lab tests and hits 89.2%. The tradeoff is ~1% accuracy for significantly lower deployment cost (no lab required).
  - Specificity vs. Sensitivity: In screening, high sensitivity is prioritized to avoid missing cases.

- **Failure signatures:**
  - Negative Transfer: External validation on the TH dataset showed high sensitivity (98%) but couldn't calculate balanced accuracy due to lack of non-CKD controls.
  - Information Leakage: The paper warns against using features like eGFR/serum creatinine as inputs, as this constitutes circular reasoning for CKD diagnosis.

- **First 3 experiments:**
  1. **Replicate the S2 Baseline:** Train a CatBoost classifier using only the S2 feature set ("Hypertension", "Age 60+y", "Anemia", "Diabetes", "Daily sleep<7h") on the provided data splits to verify the ~89% balanced accuracy claim.
  2. **SHAP Consistency Check:** Run SHAP on the trained model. Verify that "Hypertension=Yes" yields a positive SHAP value. If it doesn't, the model is unlearning clinical logic.
  3. **External Stress Test:** Apply the trained model to the UCI-2015 dataset (India). Harmonize features (mapping "Age" bins correctly). Target the reported 73.6% sensitivity; if it falls below 65%, the generalizability claim is brittle.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the proposed minimal-feature framework effectively predict CKD progression or transitions between disease stages over time?
- **Basis in paper:** The authors state that "Longitudinal datasets are essential for developing models that not only detect CKD early but also forecast disease trajectories."
- **Why unresolved:** The current study utilizes cross-sectional datasets, which capture a single point in time and cannot model the temporal evolution of the disease.
- **What evidence would resolve it:** A study using longitudinal cohorts to validate if the selected non-pathology features can predict the rate of eGFR decline or progression to end-stage renal disease.

### Open Question 2
- **Question:** How well do the selected features and model weights generalize to populations outside of South Asia and the Middle East?
- **Basis in paper:** The authors note that "further evaluation on datasets from Africa, Europe, and Latin America would strengthen evidence of worldwide applicability."
- **Why unresolved:** External validation was limited to datasets from India, the UAE, and Bangladesh; the "Global South" is diverse, and risk profiles may differ significantly in other regions.
- **What evidence would resolve it:** Performance metrics derived from applying the model to community-based cohorts from the specified underrepresented regions.

### Open Question 3
- **Question:** Does the framework maintain its predictive performance and usability when deployed in real-world clinical or community workflows?
- **Basis in paper:** The paper states, "Prospective validation in real-world clinical or community settings will be a crucial next step to evaluate usability, acceptability, [and] workflow integration."
- **Why unresolved:** The current results are based on retrospective data analysis and have not yet faced the operational, behavioral, or socio-cultural constraints of a live screening environment.
- **What evidence would resolve it:** Results from prospective pilot studies that measure not just statistical accuracy, but also user acceptance, time-to-diagnosis, and actual early detection rates in the field.

## Limitations
- Primary dataset from ICDDR,B (n=284) is not publicly available, preventing exact replication
- Generalizability to other South Asian populations remains untested beyond three datasets
- Absence of longitudinal data limits assessment of temporal stability of model predictions
- Feature selection may overfit to specific dataset structure despite cross-validation efforts

## Confidence
- **High Confidence:** The framework's core approach using non-pathology-test features is well-supported by clinical evidence; high balanced accuracy (89.23% for S2) and external validation provide strong evidence
- **Medium Confidence:** Generalizability across different South Asian populations is supported but limited by small number of external validation datasets
- **Low Confidence:** Long-term clinical utility in real-world community settings remains unverified; impact of demographic shifts on model performance is unknown

## Next Checks
1. **Replication with Public Data:** Apply the S2 feature set and CatBoost classifier to the UCI-2015 dataset to verify the reported 73.6% sensitivity, ensuring feature harmonization and preprocessing match the original methodology.
2. **Feature Importance Stability:** Conduct SHAP analysis across multiple random seeds to confirm that "Hypertension" and "Age 60+y" consistently emerge as top predictors, validating the clinical plausibility of the model.
3. **Cross-Population Testing:** Evaluate the model on a fourth, independent South Asian dataset (if available) to assess its robustness beyond the three datasets used in the study, focusing on both balanced accuracy and sensitivity metrics.