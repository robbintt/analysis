---
ver: rpa2
title: 'Reliable Graph-RAG for Codebases: AST-Derived Graphs vs LLM-Extracted Knowledge
  Graphs'
arxiv_id: '2601.08773'
source_url: https://arxiv.org/abs/2601.08773
tags:
- graph
- llm-kb
- correct
- no-graph
- code
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper benchmarks three retrieval-augmented generation (RAG)
  strategies for Java codebases: vector-only, LLM-generated knowledge graph, and deterministic
  AST-derived graph. The deterministic AST approach builds a typed ontology graph
  in seconds using Tree-sitter, while the LLM approach requires much longer indexing
  and can skip files during extraction.'
---

# Reliable Graph-RAG for Codebases: AST-Derived Graphs vs LLM-Extracted Knowledge Graphs

## Quick Facts
- arXiv ID: 2601.08773
- Source URL: https://arxiv.org/abs/2601.08773
- Reference count: 40
- Deterministic AST-derived graphs achieve perfect correctness on architectural queries while being 19-46x cheaper than LLM-extracted graphs

## Executive Summary
This paper benchmarks three retrieval-augmented generation strategies for Java codebases: vector-only, LLM-generated knowledge graph, and deterministic AST-derived graph. The deterministic approach uses Tree-sitter to build a typed ontology graph in seconds, while the LLM method requires minutes and shows probabilistic file-skipping. On Shopizer, the deterministic method achieved perfect correctness on 15 architectural queries, slightly outperformed the vector baseline in build time, and embedded 902% of discovered chunks versus 641% for the LLM method. The deterministic graph approach offers a practical, lower-cost alternative that improves multi-hop architectural reasoning while preserving indexing completeness.

## Method Summary
The paper compares three RAG pipelines on three Java repositories (Shopizer, ThingsBoard, OpenMRS Core). The No-Graph baseline uses recursive chunking with vector similarity retrieval. The LLM-KB pipeline batches truncated files and prompts an LLM to extract structured JSON with class names and dependencies, but suffers from probabilistic skipping. The DKB pipeline uses Tree-sitter to mechanically parse ASTs and extract structural relationships (extends, implements, injects) into a typed graph. All methods use the same vector store for initial retrieval, but DKB and LLM-KB map retrieved chunks to graph nodes and perform bidirectional traversal with interface-consumer expansion for context assembly.

## Key Results
- Deterministic AST-derived graphs achieve perfect correctness (15/15) on Shopizer architectural queries versus 14/15 for vector-only
- LLM-generated graphs show 0.688-0.806 per-file success rates due to probabilistic skipping, embedding only 641% of discovered chunks
- DKB indexing completes in seconds versus minutes for LLM-KB, with 19-46x lower end-to-end cost

## Why This Works (Mechanism)

### Mechanism 1
Bidirectional graph expansion retrieves upstream dependencies (e.g., controllers calling a service) that vector similarity misses. Unlike standard retrieval which fetches textually similar chunks, this method maps retrieved entities to nodes in a code graph and traverses edges in both directions—successors and predecessors—to assemble context. Core assumption: relevant evidence for multi-hop questions resides in files structurally adjacent to initially retrieved code.

### Mechanism 2
Interface-consumer expansion resolves "hidden" upstream callers in decoupled architectures (e.g., Spring DI). When a retrieved class implements an interface, the system identifies other classes that inject or use that interface, effectively crossing abstraction boundaries that simple class-level traversal would miss. Core assumption: architecture follows standard dependency injection patterns where consumers depend on interfaces.

### Mechanism 3
Deterministic AST extraction guarantees higher corpus coverage than LLM-based extraction by eliminating probabilistic "skipping." LLM-based graph construction requires batching files into prompts, and if the LLM fails to emit valid JSON for a file or hits token limits, the file is skipped. AST parsing processes syntax trees mechanically, ensuring 100% parseable files are indexed.

## Foundational Learning

- Concept: **Abstract Syntax Trees (AST) vs. Knowledge Graphs (KG)** - Why needed: The paper leverages ASTs for deterministic structure (what the code is) versus KGs for semantic relationships (what the code means). Quick check: Does the DKB graph represent semantic "business logic" links or syntactic "extends/implements" links?

- Concept: **Predecessor vs. Successor Traversal** - Why needed: Standard code search looks "down" (what does this class use?). Architecture questions often look "up" (who uses this class?). Quick check: To find "which controllers use this service," would you traverse successors or predecessors of the service node?

- Concept: **Probabilistic Skipping in Tool-Use** - Why needed: LLM-KB fails not because of bad logic, but because LLMs are non-deterministic and context-limited. Quick check: Why does increasing the batch size in LLM-KB likely increase the "skip rate" of files?

## Architecture Onboarding

- Component map: Tree-sitter Parser -> Graph Store (NetworkX) -> Vector Store -> LLM -> Graph Traversal
- Critical path: Scan repo → Parse ASTs (DKB) or Prompt LLM (LLM-KB) → Build Graph → Chunk code → Embed → Vector Index → Query → Vector Search (Top-k) → Map Chunks to Graph Nodes → Traverse Graph → Assemble Context → Generate Answer
- Design tradeoffs: DKB offers ultra-low latency indexing, high correctness on structure, but misses dynamic/runtime relations. LLM-KB provides high semantic understanding but at 19x+ higher cost and incomplete coverage. Vector-only is fastest setup but fails on multi-hop reasoning.
- Failure signatures: Hallucinated Controllers (Vector-only) invents callers because it cannot see upstream files. Missing Files (LLM-KB) omits valid files from the index due to "SKIPPED/MISSED" errors. Broken Chains (DKB) stops prematurely if reflection or XML-based wiring is used instead of explicit Java injection.
- First 3 experiments: 1) Run LLM-KB extraction with logging to count "SKIPPED/MISSED" lines and quantify data loss. 2) Query "Who uses [UtilClass]?" using Vector-only vs. DKB to verify upstream caller retrieval. 3) Measure time and token cost for indexing Shopizer with DKB vs. LLM-KB to validate the 19x cost differential.

## Open Questions the Paper Calls Out

### Open Question 1
Can a hybrid graph combining deterministic AST-derived structural edges with LLM-extracted semantic edges outperform either pure approach on multi-hop reasoning? The paper only compares pure deterministic vs. pure LLM extraction; no hybrid architecture was tested.

### Open Question 2
Do the observed cost/coverage/correctness trade-offs generalize to dynamically-typed languages (Python, JavaScript) where AST-derived type graphs are less complete? All three repositories are Java codebases with explicit type declarations.

### Open Question 3
Can LLM-KB's probabilistic file skipping be eliminated through smaller batch sizes, retries, and static validation passes without negating its cost advantage? The current implementation uses batching with truncation, yielding 0.65-0.81 file success rates.

## Limitations
- LLM-KB per-file success rates (0.688-0.806) are measured only on Shopizer; generalization to other repositories remains uncertain
- Deterministic AST coverage is near-baseline for well-formed Java but untested on projects with heavy reflection or XML-based wiring
- Bidirectional traversal correctness assumes standard dependency injection; projects using alternative patterns may see degraded performance

## Confidence
- **High confidence**: Deterministic AST indexing is faster and cheaper than LLM-based indexing; coverage loss in LLM-KB is due to probabilistic skipping
- **Medium confidence**: DKB outperforms vector-only on multi-hop architectural queries due to bidirectional traversal; the interface-consumer expansion heuristic reliably resolves upstream dependencies
- **Low confidence**: Claims about correctness gains generalize beyond Shopizer; performance on non-Java languages or heavily dynamic codebases is unverified

## Next Checks
1. Measure per-file success rates of LLM-KB on ThingsBoard and OpenMRS to confirm generalizability of skipping behavior
2. Test DKB retrieval on a codebase with known reflection or XML-based wiring to assess traversal limitations
3. Benchmark indexing cost and time on a larger, more complex Java repository to validate scalability of deterministic AST extraction