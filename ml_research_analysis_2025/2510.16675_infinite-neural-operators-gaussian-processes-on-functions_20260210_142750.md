---
ver: rpa2
title: 'Infinite Neural Operators: Gaussian processes on functions'
arxiv_id: '2510.16675'
source_url: https://arxiv.org/abs/2510.16675
tags:
- neural
- function
- operator
- covariance
- kernel
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes a theoretical connection between neural
  operators and Gaussian processes by showing that infinitely wide neural operators
  converge to function-valued Gaussian processes under certain conditions. The authors
  prove this for arbitrary-depth neural operators with Gaussian-distributed convolution
  kernels, demonstrating that the limiting process is characterized by a covariance
  function that can be computed in closed form.
---

# Infinite Neural Operators: Gaussian processes on functions

## Quick Facts
- **arXiv ID**: 2510.16675
- **Source URL**: https://arxiv.org/abs/2510.16675
- **Reference count**: 40
- **Primary result**: Infinitely wide neural operators converge to function-valued Gaussian processes with computable covariance functions.

## Executive Summary
This paper establishes a theoretical connection between neural operators and Gaussian processes by showing that infinitely wide neural operators converge to function-valued Gaussian processes under certain conditions. The authors prove this for arbitrary-depth neural operators with Gaussian-distributed convolution kernels, demonstrating that the limiting process is characterized by a covariance function that can be computed in closed form. They derive these covariance functions for two parametrizations: the Fourier neural operator and a toroidal Matérn operator. Empirically, they validate their theoretical results by showing that finite-width neural operators with increasing width converge to the predicted Gaussian distribution. In regression experiments, they compare the performance of infinite-width neural operators against finite-width versions on synthetic data and the 1D Burgers' equation, finding that models with appropriate band-limits achieve better predictive accuracy. This work provides important theoretical foundations for understanding the inductive biases of current neural operator architectures and opens new possibilities for kernel-based operator learning methods.

## Method Summary
The paper proves that single-layer neural operators with Gaussian-distributed weights converge to Gaussian processes as width approaches infinity. The theoretical framework shows that the covariance function of the limiting GP can be computed in closed form using dual kernel formulations. For the Fourier Neural Operator, the covariance involves integrating the product of frequency components with the ReLU activation's dual kernel. The authors derive specific covariance expressions for both the Fourier and toroidal Matérn parametrizations. Empirically, they validate convergence by sampling random functions, passing them through finite-width FNOs with increasing width, and comparing the output distribution at a fixed point against the theoretical Gaussian prediction using total variation distance. They also compare FNO and ∞-FNO performance on regression tasks using mean test L2 error as the metric.

## Key Results
- Infinitely wide neural operators converge to Gaussian processes with computable covariance functions
- The limiting GP covariance for FNO can be expressed in closed form using dual kernel integration
- Finite-width FNOs with increasing width empirically converge to the theoretical Gaussian distribution (Fig 1)
- Regression experiments show ∞-FNO achieves comparable or better performance than finite FNOs on synthetic and Burgers' equation data (Fig 3)

## Why This Works (Mechanism)
The mechanism relies on the central limit theorem applied to function spaces: as the width J approaches infinity, the sum of independent Gaussian-distributed weights in the neural operator converges to a Gaussian process. The key insight is that the covariance structure is determined by the dual kernel of the activation function (ReLU) and the Fourier coefficients of the convolution kernels. For ReLU, this dual kernel has a specific analytic form that enables closed-form covariance computation. The toroidal domain assumption ensures periodic boundary conditions that simplify the Fourier analysis. The band-limit parameter B controls the frequency content and directly affects the expressivity and generalization of both finite and infinite-width operators.

## Foundational Learning
- **Function-valued Gaussian processes**: GPs where the output space consists of functions rather than scalars; needed to model operators mapping functions to functions; quick check: verify the covariance function outputs covariance between entire output functions.
- **Dual kernels**: Mathematical objects that characterize the covariance structure of non-linear activations in infinite-width networks; needed to compute the limiting GP covariance for ReLU activations; quick check: confirm the dual kernel integral converges to the correct variance.
- **Band-limited Fourier series**: Representation of functions with frequency content restricted to a maximum frequency B; needed to ensure the operators operate on smooth functions with finite Fourier modes; quick check: verify the Fourier coefficients decay appropriately beyond B.
- **Toroidal domains**: Periodic boundary conditions on the function space; needed to simplify the Fourier analysis and ensure well-defined convolution operations; quick check: confirm all operations respect periodicity.
- **Total variation distance**: Metric for comparing probability distributions; needed to quantify the convergence of finite FNO output distributions to the theoretical Gaussian; quick check: ensure TVD decreases monotonically with increasing width.

## Architecture Onboarding

### Component Map
Random function input -> Band-limited convolution -> ReLU activation -> Linear readout -> Output function

### Critical Path
1. Sampling random input functions with band-limit B
2. Forward pass through single-layer FNO with width J
3. Computing empirical output distribution at evaluation point
4. Comparing against theoretical Gaussian using TVD

### Design Tradeoffs
- Wider networks (larger J) provide better convergence to GP but increase computational cost
- Higher band-limits (larger B) increase expressivity but require more parameters and data
- Toroidal domain simplifies analysis but may not capture all real-world boundary conditions
- Dual kernel approximation accuracy affects the precision of covariance computation

### Failure Signatures
- Variance explosion/collapse in finite FNO outputs indicates incorrect weight initialization scaling
- Discretization mismatch between empirical and analytic covariance suggests numerical integration errors
- Poor regression performance may indicate inappropriate band-limit choice for the data

### First Experiments
1. Verify the $1/J$ scaling in weight/kernel initialization produces correct variance matching theoretical GP
2. Implement the dual kernel for ReLU and verify it produces the correct covariance structure
3. Test convergence of empirical covariance to analytic form as width increases

## Open Questions the Paper Calls Out
- Can the neural tangent kernel framework be rigorously extended to characterize the training dynamics of infinite-width neural operators in Hilbert spaces?
- Do closed-form covariance functions exist for the infinite-width limits of non-Fourier neural operator architectures, such as the graph neural operator?
- Can the computational complexity of infinite neural operator inference be reduced from cubic scaling to handle high-dimensional PDEs and high-resolution grids?

## Limitations
- The framework requires careful numerical integration of dual kernels, with no specified settings for implementation
- Computational complexity scales cubically with grid size and number of training functions, limiting scalability
- The analysis is restricted to single-layer operators and specific architectures (Fourier, Matérn), leaving other architectures unexplored

## Confidence
- **High Confidence**: Theoretical framework connecting infinitely wide neural operators to Gaussian processes
- **Medium Confidence**: Empirical validation of theoretical predictions, dependent on precise numerical implementation
- **Low Confidence**: Regression experiments comparing FNO and ∞-FNO performance, sensitive to hyperparameter optimization details

## Next Checks
1. Replicate the numerical integration scheme for the dual kernel (Eq 20) and verify computed ∞-FNO covariance matches theoretical prediction
2. Test sensitivity of TVD convergence curves to variations in numerical precision, sample size, and integration parameters
3. Conduct ablation studies on the Burgers' equation regression task varying only the band-limit parameter while holding other hyperparameters constant