---
ver: rpa2
title: 'RedStar: Does Scaling Long-CoT Data Unlock Better Slow-Reasoning Systems?'
arxiv_id: '2501.11284'
source_url: https://arxiv.org/abs/2501.11284
tags:
- reasoning
- long-cot
- performance
- data
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the potential of scaling Long Chain-of-Thought
  (Long-CoT) data for slow-thinking reasoning systems. The authors explore how increasing
  Long-CoT dataset sizes, model scales, and specialized training impact reasoning
  performance across mathematical, multimodal, and general tasks.
---

# RedStar: Does Scaling Long-CoT Data Unlock Better Slow-Reasoning Systems?

## Quick Facts
- **arXiv ID:** 2501.11284
- **Source URL:** https://arxiv.org/abs/2501.11284
- **Reference count:** 40
- **Primary result:** Scaling Long-CoT data improves slow-thinking reasoning systems, with larger models and RL further boosting performance on math, multimodal, and general benchmarks.

## Executive Summary
This paper investigates whether scaling Long Chain-of-Thought (Long-CoT) data can unlock better slow-thinking reasoning systems. Through extensive experiments across multiple model scales (7B, 14B, 32B), training regimes, and reasoning domains, the authors demonstrate that Long-CoT data scaling significantly improves mathematical and multimodal reasoning performance. The study introduces RedStar, a specialized reasoning system that leverages reinforcement learning and extended context to achieve state-of-the-art results on challenging benchmarks like MATH-Hard, AIME, and GeoQA. The findings highlight the transformative potential of Long-CoT for advancing reasoning systems, particularly when combined with appropriate model scale and specialized training.

## Method Summary
The method involves constructing Long-CoT datasets through difficulty scoring, multiple sampling with QwQ, and rule-based filtering. Models are fine-tuned using standard supervised fine-tuning (SFT) on datasets ranging from 4k to 1000k samples, with context extension via Position Interpolation for smaller models. Reinforcement learning (DPO, PPO, REINFORCE++) is applied using rule-based reward functions. The approach is evaluated across mathematical reasoning (Metamath-qwen2-math, TACO), multimodal geometric reasoning (GeoQA, MathVista-GEO, Geometry3K), and general capability benchmarks. The study systematically examines scaling effects across model sizes and training strategies.

## Key Results
- Scaling Long-CoT data from 4k to 1000k samples significantly improves performance on MATH-Hard and Olympiad benchmarks
- Larger models (14B, 32B) benefit more from Long-CoT scaling than smaller models (7B), which suffer catastrophic forgetting on general tasks
- Reinforcement learning with rule-based rewards further enhances reasoning performance beyond SFT
- The multimodal extension achieves competitive results on GeoQA and MathVista-GEO benchmarks

## Why This Works (Mechanism)
Long-CoT scaling works by providing models with extensive reasoning traces that capture step-by-step problem-solving processes. The increased data volume exposes models to diverse problem-solving patterns and difficulty levels, enabling them to develop more robust reasoning capabilities. Larger models can better leverage this additional information due to their increased capacity to store and process complex reasoning patterns. The rule-based verification system provides immediate feedback on solution correctness, while RL fine-tuning helps optimize the reasoning process itself rather than just the final answer.

## Foundational Learning
- **Difficulty scoring and filtering:** Needed to ensure high-quality training data by selecting appropriately challenging problems. Quick check: verify difficulty distribution matches target reasoning complexity.
- **Positive/negative pair construction:** Essential for contrastive learning and RL training. Quick check: ensure balanced ratio of correct/incorrect reasoning pairs.
- **Context extension via Position Interpolation:** Required for small models to handle Long-CoT sequences. Quick check: validate position embeddings maintain semantic coherence.
- **Rule-based verification systems:** Critical for automated evaluation across diverse benchmarks. Quick check: test verifier accuracy on known correct/incorrect answers.
- **Reinforcement learning optimization:** Enables fine-tuning of reasoning process beyond supervised learning. Quick check: monitor reward distribution during RL training.
- **Multimodal data processing:** Necessary for geometric reasoning tasks combining text and visual information. Quick check: validate multimodal input parsing correctness.

## Architecture Onboarding

**Component Map:** Data Collection -> Difficulty Scoring -> Sampling (QwQ) -> Filtering -> SFT Training -> RL Fine-tuning -> Evaluation

**Critical Path:** The core workflow follows: collect domain-specific datasets → score and filter by difficulty → generate multiple reasoning traces per problem → apply rule-based filtering → perform SFT → apply RL optimization → evaluate on benchmarks.

**Design Tradeoffs:** Larger models benefit more from scaling but require more computational resources. The rule-based verifier is simple but may miss nuanced errors. Position Interpolation enables smaller models to handle Long-CoT but may introduce artifacts. Multimodal processing adds complexity but enables geometric reasoning.

**Failure Signatures:** 
- Performance degradation on general tasks when scaling to 1000k samples on small models (7B drops from 83→38 on Hellobench)
- Step-by-step verification fails to improve results with complex error detection
- Context extension via Position Interpolation may cause position embedding misalignment

**First Experiments:**
1. Fine-tune Qwen2.5-14B-Instruct on 4k Long-CoT subset, evaluate on MATH-Hard and AIME24
2. Scale to 100k samples, compare performance across model sizes (7B vs 14B vs 32B)
3. Apply DPO fine-tuning on best-performing SFT model, evaluate improvement on Olympiad-Bench

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Catastrophic forgetting occurs in smaller models (7B) when scaling to 1000k samples, limiting scalability
- Rule-based verification may not capture complex reasoning errors, potentially inflating performance metrics
- The exact impact of Long-CoT scaling versus data quality filtering remains unclear

## Confidence
- High confidence: General scaling trends across multiple model sizes and benchmarks
- Medium confidence: Superiority of Long-CoT over other reasoning approaches
- Medium confidence: Multimodal extension results due to limited evaluation scope

## Next Checks
1. Implement ablation studies comparing Long-CoT scaling against pure data quality filtering to isolate scaling effects
2. Test catastrophic forgetting phenomenon on additional small model architectures to verify it's not model-specific
3. Develop and evaluate more sophisticated verification methods beyond rule-based to assess robustness of improvements