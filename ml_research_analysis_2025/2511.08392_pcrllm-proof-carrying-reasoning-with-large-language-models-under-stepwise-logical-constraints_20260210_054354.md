---
ver: rpa2
title: 'PCRLLM: Proof-Carrying Reasoning with Large Language Models under Stepwise
  Logical Constraints'
arxiv_id: '2511.08392'
source_url: https://arxiv.org/abs/2511.08392
tags:
- reasoning
- language
- llms
- rules
- natural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PCRLLM, a framework that improves LLM reasoning
  by constraining outputs to single-step logical inferences with explicit premises,
  rules, and conclusions. This "proof-carrying" format enables verification against
  formal logic, allowing chain-level validation even in black-box settings.
---

# PCRLLM: Proof-Carrying Reasoning with Large Language Models under Stepwise Logical Constraints

## Quick Facts
- arXiv ID: 2511.08392
- Source URL: https://arxiv.org/abs/2511.08392
- Reference count: 21
- Authors: Tangrui Li; Pei Wang; Hongzheng Wang Christian Hahm; Matteo Spatola; Justin Shi
- One-line primary result: Step-wise recomposition across three fine-tuned Qwen models outperforms single-model baselines on two-step NAL reasoning, with valid-answer ratios improving from ~0.15 to ~0.25 under loose thresholds

## Executive Summary
PCRLLM introduces a proof-carrying reasoning framework that constrains LLM outputs to single-step logical inferences with explicit premises, rules, and conclusions in structured JSON format. This enables verification against formal logic even in black-box settings and supports systematic multi-LLM collaboration by allowing intermediate steps to be compared and integrated under formal rules. The authors construct a benchmark schema for generating step-level reasoning data, combining natural language expressiveness with formal rigor.

Experiments with three fine-tuned Qwen models on two-step NAL reasoning tasks show that step-wise recomposition across models (mb9) outperforms single-model baselines, with performance improving with model size. However, even the best configurations struggle to exceed 0.2 valid-answer ratios under stricter evaluation, highlighting significant room for improvement in faithfully simulating formal reasoning.

## Method Summary
PCRLLM uses Non-Axiomatic Logic (NAL) as the target formal system, requiring models to output single-step inferences in structured JSON format with explicit premises, rules, and conclusions. The framework generates training data using a benchmark schema that combines natural language problem descriptions with formal logical structures, then fine-tunes three Qwen models on disjoint rule subsets. Evaluation uses a grading system based on bipartite matching between model outputs and all valid conclusions, with scores multiplied across single-step, inter-step, and ground-truth dimensions. The mb9 strategy decomposes multi-step reasoning across models and recomposes by selecting high-quality steps through exhaustive enumeration.

## Key Results
- Step-wise recomposition across three models (mb9) outperforms single-model baselines on two-step NAL reasoning
- Performance improves with model size: valid-answer ratios increase from ~0.15 to ~0.25 under loose thresholds
- Even best configurations struggle to exceed 0.2 valid-answer ratios under stricter evaluation thresholds
- Larger models (3B) show improved reasoning but produce more JSON syntax errors requiring external repair

## Why This Works (Mechanism)

### Mechanism 1: Single-Step Inference Constraint
Constraining LLM outputs to single-step logical inferences enables verification that would be impossible with free-form chain-of-thought. The framework requires each output to explicitly specify Premise 1, Premise 2, and Results in structured JSON format, allowing a reasoning engine to independently compute all valid conclusions and compare against LLM outputs via bipartite matching. This granular decomposition enables verification even when models are black boxes.

### Mechanism 2: Grading-Based Formal Conformity Scoring
Multiplying single-step grades, inter-step coherence grades, and ground-truth grades yields a content-agnostic measure of logical soundness. Single-step grading uses bipartite matching to compare LLM outputs against all valid conclusions, while inter-step grading verifies that conclusions from Step 1 appear as premises in Step 2. The product defines formal conformity grade that can be computed without understanding the natural language content.

### Mechanism 3: Cross-Model Step Recomposition
Decomposing multi-step reasoning across models and recomposing by selecting high-quality steps outperforms single-model baselines. Three models fine-tuned on disjoint rule subsets produce different reasoning paths, and the mb9 strategy enumerates all 3×3 combinations of first-step and second-step outputs, scores each, and selects the highest-scoring composed answer. This leverages specialization while avoiding combinatorial explosion for the two-step case.

## Foundational Learning

- **Non-Axiomatic Logic (NAL) Fundamentals**
  - Why needed here: PCRLLM uses NAL as its target logic because it handles uncertainty naturally (AIKR assumption) and supports non-monotonic reasoning for multi-LLM integration.
  - Quick check question: Can you explain why NAL's two-dimensional truth-value (frequency, confidence) differs from classical binary truth, and how this enables revision when multiple LLMs produce conflicting judgments?

- **Syllogistic Inference Structure**
  - Why needed here: All NAL rules require two premises sharing a common term M, with subject S and predicate P. Understanding this structure is prerequisite to generating valid training data and interpreting grading outputs.
  - Quick check question: Given premises "S → M" and "M → P", what inference rule applies, and what additional information does NAL require beyond the copula to compute the conclusion's truth-value?

- **Evidential Base Tracking**
  - Why needed here: Preventing double-counting of evidence is critical for valid multi-step chains. The evidential base (set of unique IDs) must propagate through inference steps.
  - Quick check question: If Step 1 derives a conclusion from premises with evidential bases {A, B} and {C, D}, and Step 2 uses this conclusion with a premise having evidential base {A, E}, why would NAL disallow this inference?

## Architecture Onboarding

- **Component map**:
  Data Generator -> Natural Language Translator -> Prompt Constructor -> Three fine-tuned Qwen models -> JSON parser -> Single-step grader -> Inter-step coherence checker -> Ground-truth comparator -> Step decomposition -> Cross-model enumeration -> Score-based selection

- **Critical path**:
  1. Generate 100 training instances per model using the benchmark schema
  2. Fine-tune with size-appropriate epochs (1 for 0.5B, 2 for 1.5B, 4 for 3B)
  3. Run inference on test set, optionally repair malformed JSON
  4. Score outputs using formal conformity grade (single-step × inter-step × ground-truth)
  5. If using multi-model setup, enumerate step combinations and select highest-scoring chain

- **Design tradeoffs**:
  - Model size vs. formatting reliability: Larger models (3B) show improved reasoning but more JSON syntax errors
  - Threshold strictness vs. valid-answer ratio: Under loose thresholds (~0.3), mb9 achieves ~0.25 valid-answer ratio; under strict thresholds (~0.7), drops to ~0.15
  - Disjoint training vs. coverage: Partitioning rules across models enables complementarity but leaves each model incomplete
  - Two-step simplicity vs. scalability: Current experiments use two-step reasoning; extending to multi-step triggers combinatorial explosion

- **Failure signatures**:
  - JSON parsing failure: Unclosed brackets, missing keys → triggers minimum grade (0.1), requires post-processing repair
  - Rule misapplication: LLM selects rule R that doesn't match premise structure → categorical indicator scores zero
  - Truth-value drift: Numerical indicators exceed 0.2 difference from reference → 0 points, indicates category-level mistake
  - Evidential base collision: Overlapping IDs across premises → inference would be disallowed by NAL engine
  - Inter-step incoherence: Step 1 conclusion doesn't match any Step 2 premise → inter-step grade drops, chain invalid

- **First 3 experiments**:
  1. Single-model baseline (m1/m2/m3): Fine-tune each model on its rule subset, evaluate on full test set, establish performance floor and identify rule-specific weaknesses.
  2. Multi-model priming (mb3): Run all three models, select highest-scoring answer per example without recomposition, quantify priming gain over best single model.
  3. Step-wise recomposition (mb9): Decompose outputs, enumerate all 9 combinations, score and select best chain, measure improvement over mb3 and analyze which step combinations yield gains.

## Open Questions the Paper Calls Out

- **Open Question 1**: How can the framework automate the attribution of generated premises to specific source text to mitigate hallucination risks? The authors state they cannot currently "attribute which premises originate from which problem description," necessitating manual intervention to reject false premises.

- **Open Question 2**: How can the exponential growth of the search space be managed when extending the method to complex, multi-step reasoning? The conclusion notes that extending to multi-step reasoning leads to a "combinatorial explosion" where the search space grows exponentially with depth.

- **Open Question 3**: Can strict logical formatting be maintained intrinsically by larger models without relying on external post-processing repair? Experiments showed larger models (3B) produced "fatal formatting errors" in JSON outputs, requiring a separate model (DeepSeek R1) for repair.

## Limitations
- Scalability bottleneck: Combinatorial explosion beyond two-step reasoning makes the approach impractical for deeper chains
- Heavy dependency on NAL inference engine correctness, which serves as ground truth for all grading
- JSON repair mechanism using DeepSeek R1 introduces additional black-box dependency without formal evaluation
- Experimental scope constrained to 100 training and test instances, raising statistical significance concerns

## Confidence
**High Confidence**: The proof-carrying framework's core mechanism—constraining outputs to single-step inferences with explicit premises, rules, and conclusions—is well-supported by the structured JSON format and bipartite matching grading system.

**Medium Confidence**: The NAL-based grading system's effectiveness relies on assumptions about truth-value tolerance thresholds (0.05 vs 0.2) that weren't empirically validated. The combinatorial explosion concern for deeper chains is logically sound but not experimentally verified.

**Low Confidence**: Claims about JSON formatting reliability and the necessity of DeepSeek R1 repair are based on observed failure patterns without systematic analysis of failure modes or repair success rates.

## Next Checks
1. **Statistical Power Validation**: Re-run experiments with 10× larger datasets (1000 training and test instances) to determine if performance gains persist and whether current conclusions are statistically significant.

2. **Deeper Chain Scalability Test**: Implement exact evaluation of three-step reasoning with all 27 possible model combinations (3³) to quantify the combinatorial explosion impact and assess whether approximation strategies could maintain benefits.

3. **Ground Truth Independence Check**: Validate NAL grading against human-annotated gold standards for a subset of instances to verify that the automated scoring system correctly captures logical soundness rather than artifacts of the NAL implementation.