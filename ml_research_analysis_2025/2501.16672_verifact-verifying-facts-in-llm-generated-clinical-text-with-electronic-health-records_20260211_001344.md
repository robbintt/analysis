---
ver: rpa2
title: 'VeriFact: Verifying Facts in LLM-Generated Clinical Text with Electronic Health
  Records'
arxiv_id: '2501.16672'
source_url: https://arxiv.org/abs/2501.16672
tags:
- text
- propositions
- patient
- facts
- hospital
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "VeriFact is an AI system that verifies whether LLM-generated clinical\
  \ text is factually supported by a patient\u2019s EHR. It decomposes text into propositions\
  \ and uses retrieval-augmented generation with an LLM-as-a-Judge to evaluate each\
  \ proposition against the patient\u2019s medical records."
---

# VeriFact: Verifying Facts in LLM-Generated Clinical Text with Electronic Health Records

## Quick Facts
- arXiv ID: 2501.16672
- Source URL: https://arxiv.org/abs/2501.16672
- Reference count: 40
- Primary result: VeriFact achieves up to 92.7% agreement with human clinician ground truth on verifying LLM-generated clinical text

## Executive Summary
VeriFact is an AI system designed to verify the factual accuracy of LLM-generated clinical text by checking each proposition against a patient's Electronic Health Record (EHR). The system decomposes complex clinical narratives into atomic propositions, retrieves relevant EHR facts through a hybrid retrieval pipeline, and uses an LLM-as-a-Judge to evaluate each proposition. VeriFact demonstrates high agreement with human clinicians (up to 92.7%) and outperforms existing fact-checking approaches, while being built on open-source models for transparency and reproducibility.

## Method Summary
VeriFact operates through a three-stage pipeline: (1) Text decomposition using an LLM to extract atomic propositions following Subject-Object-Predicate structure, (2) Retrieval-augmented generation where EHR notes are stored as facts in a vector database and retrieved using hybrid dense/sparse search with re-ranking, and (3) LLM-as-a-Judge evaluation comparing each proposition against retrieved EHR context to assign Supported, Not Supported, or Not Addressed verdicts. The system was evaluated on a new dataset (VeriFact-BHC) containing 13,290 propositions from Brief Hospital Course narratives with clinician annotations.

## Key Results
- Achieved 92.7% agreement with denoised human clinician ground truth
- Outperformed average clinician performance in verification tasks
- Demonstrated significant improvement over existing fact-checking approaches
- Showed monotonic performance increase with retrieval depth up to 50 facts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing complex clinical text into atomic propositions enables fine-grained, verifiable fact-checking
- Mechanism: VeriFact uses an LLM to parse long-form text into simple, declarative statements (atomic claims) adhering to a Subject-Object-Predicate structure, reducing ambiguity and allowing each unit to be independently evaluated
- Core assumption: The LLM accurately extracts all relevant atomic claims without introducing errors or hallucinations, and these claims can be evaluated in isolation
- Evidence anchors: [abstract] "It decomposes text into propositions..."; [section] "VeriFact adopts concepts from Bertrand Russell's logical atomism... by breaking down candidate input text for evaluation into a set of logical proposition statements that can be individually verified" (Page 2)
- Break condition: The mechanism fails if the LLM omits key claims or generates nonsensical ones (e.g., linking incompatible medical concepts)

### Mechanism 2
- Claim: A multi-stage retrieval pipeline creates a highly relevant, patient-specific evidence context for each proposition
- Mechanism: Patient EHR notes are decomposed into facts and stored in a vector database. For each proposition, VeriFact performs dense and sparse vector search, followed by a cross-encoder re-ranking step, to dynamically retrieve the `Top N` most relevant facts
- Core assumption: The hybrid retrieval pipeline effectively surfaces the specific evidence needed for verification, and the EHR representation is faithful and complete
- Evidence anchors: [abstract] "...uses retrieval-augmented generation... to evaluate each proposition against the patient's medical records."; [section] "The most important hyperparameter was the number of facts retrieved... The best VeriFact systems used hybrid retrieval with a re-ranker model to retrieve 50 facts..." (Page 8, Results)
- Break condition: Performance degrades significantly if the retrieval step fails to surface key evidence, particularly when `Top N` is set too low

### Mechanism 3
- Claim: An LLM acting as a judge can perform comparative evaluation to verify propositions with high clinician-level agreement
- Mechanism: The LLM-as-a-Judge is prompted to compare a single proposition against its retrieved EHR fact context, assigning one of three verdicts (Supported, Not Supported, Not Addressed) and generating a textual explanation
- Core assumption: LLMs are more reliable at comparison tasks than at unaided logical reasoning. The model can correctly interpret temporal and logical relationships within the provided context
- Evidence anchors: [abstract] "...uses retrieval-augmented generation with an LLM-as-a-Judge to evaluate each proposition..."; [section] "VeriFact achieves up to 92.7% agreement when compared to a denoised and adjudicated average human clinician ground truth..." (Abstract, Page 1)
- Break condition: The mechanism is less effective in scenarios of information asymmetry, where it shows a bias toward "Not Supported" over "Not Addressed" compared to human clinicians

## Foundational Learning

**Concept: Logical Atomism & Atomic Claims**
- Why needed here: This is the foundational design principle. Understanding that verification requires breaking complex text into indivisible units (Subject-Object-Predicate) is critical for debugging extraction failures
- Quick check question: Why is a compound sentence like "The patient received drugs X, Y, and Z, which alleviated symptoms A, B, and C" problematic for verification until decomposed?

**Concept: Retrieval-Augmented Generation (RAG) & Hybrid Search**
- Why needed here: This is the evidence-gathering engine. Understanding the roles of dense vectors, sparse vectors, and re-ranking is essential for tuning the system's primary performance lever (`Top N`)
- Quick check question: What is the role of the cross-encoder "Rerank" step after the initial vector search, and how does increasing `Top N` affect performance?

**Concept: LLM-as-a-Judge**
- Why needed here: This is the decision-making module. Understanding its 3-label classification schema (Supported, Not Supported, Not Addressed) and its inherent biases is key to interpreting system outputs correctly
- Quick check question: According to the paper, why is a 3-label classification used, and how does the system's performance change if you simplify it to a binary task?

## Architecture Onboarding

**Component map:**
Decomposition Engine (LLM) -> Fact Store (Vector Database) -> Retrieval Pipeline (BGE-M3 Embeddings + Reranker) -> Judging Module (LLM)

**Critical path:** The **Retrieval Pipeline** is the primary performance driver. The paper explicitly identifies the number of retrieved facts (`Top N`) as the most important hyperparameter, with agreement increasing monotonically up to the tested limit of 50.

**Design tradeoffs:**
- **Atomic vs. Sentence Propositions**: Atomic claims yield higher inter-clinician agreement but risk extraction errors. Sentences carry more context but are harder to adjudicate
- **3-Label vs. Binarized Output**: The 3-label schema preserves clinical nuance but achieves lower agreement. Binarizing improves agreement by 10-15% but loses the distinction between contradiction and omission
- **Information Symmetry**: The system is optimized for LLM-written text (symmetric info) and shows degraded performance on human-written text (asymmetric info)

**Failure signatures:**
- **"Not Supported" Bias**: A tendency to label propositions as "Not Supported" instead of "Not Addressed" when information is missing from the EHR
- **Nonsensical Claims**: Atomic claims that create illogical medical relationships during the decomposition step
- **Retrieval Starvation**: A significant drop in accuracy when `Top N` is set too low, leading to missing critical evidence

**First 3 experiments:**
1. **Retrieve `Top N` Scaling**: Test performance with `Top N` values of 5, 10, 25, and 50 to quantify the retrieval-performance relationship
2. **Decomposition Audit**: Manually review a sample of extracted atomic claims to identify and categorize extraction errors
3. **Asymmetry Stress Test**: Evaluate system performance on texts with known "hallucinations" (facts not in the EHR) to measure "Not Addressed" classification accuracy

## Open Questions the Paper Calls Out
None identified in the provided content

## Limitations
- Performance may degrade on clinical text types beyond Brief Hospital Course narratives due to information asymmetry
- Heavy reliance on retrieval pipeline with potential scalability issues for patients with extensive EHR histories
- Systematic bias toward "Not Supported" classifications when information is missing from EHR

## Confidence

**High Confidence (80-95%)**:
- The decomposition mechanism works as described for the tested text types
- The 3-label classification schema provides meaningful clinical differentiation
- Performance metrics (92.7% agreement) are reproducible on VeriFact-BHC dataset

**Medium Confidence (60-80%)**:
- Performance generalization to other clinical text types
- The LLM-as-a-Judge's comparative evaluation advantage over unaided reasoning
- The claim that binarizing output improves agreement by 10-15%

**Low Confidence (40-60%)**:
- Performance on EHR systems with different structures or terminologies
- Long-term stability of the system as EHR content scales
- The impact of varying clinical specialty contexts on verification accuracy

## Next Checks

1. **Cross-Dataset Generalization Test**: Evaluate VeriFact on clinical text types not represented in VeriFact-BHC (e.g., radiology reports, pathology notes, or clinical trial eligibility criteria) to assess performance across diverse clinical documentation styles and structures

2. **Human-in-the-Loop Validation**: Deploy the system in a controlled clinical setting where physicians can review and correct system outputs in real-time, measuring the rate of false positives (incorrect "Not Supported" classifications) and the system's impact on clinical workflow efficiency

3. **Information Asymmetry Benchmark**: Create a synthetic benchmark with controlled hallucinations at varying rates (5%, 15%, 30%) to precisely measure the system's "Not Addressed" classification accuracy and quantify the severity of its negative verification bias under different hallucination scenarios