---
ver: rpa2
title: Adversarial Training for Process Reward Models
arxiv_id: '2511.22888'
source_url: https://arxiv.org/abs/2511.22888
tags:
- training
- arxiv
- aprm
- reasoning
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Adversarially Trained Process Reward Models
  (APRM), a novel framework that formulates PRM training as a two-player adversarial
  game between a Generator and a Reward Model. The Generator learns to produce increasingly
  subtle reasoning errors to deceive the PRM, while the PRM concurrently learns to
  detect them, creating an adaptive curriculum of hard negatives without requiring
  manual step-level annotations.
---

# Adversarial Training for Process Reward Models

## Quick Facts
- arXiv ID: 2511.22888
- Source URL: https://arxiv.org/abs/2511.22888
- Authors: Gurusha Juneja; Deepak Nathani; William Yang Wang
- Reference count: 40
- Primary result: APRM improves solver accuracy by +3.4 percentage points across mathematical reasoning benchmarks

## Executive Summary
This paper introduces Adversarially Trained Process Reward Models (APRM), a novel framework that formulates PRM training as a two-player adversarial game between a Generator and a Reward Model. The Generator learns to produce increasingly subtle reasoning errors to deceive the PRM, while the PRM concurrently learns to detect them, creating an adaptive curriculum of hard negatives without requiring manual step-level annotations. The method is theoretically grounded with linear convergence guarantees under regularized game utilities using game-aware optimizers like Optimistic Gradient Descent-Ascent. Experiments show APRM improves solver accuracy by +3.4 percentage points averaged across diverse mathematical reasoning benchmarks compared to the strongest PRM baseline, with the largest gains of +5.3 pp on out-of-distribution tasks, demonstrating enhanced robustness and generalization across solver sizes and scientific domains.

## Method Summary
APRM frames PRM training as a general-sum game between a Generator and a Reward Model. The Generator receives a reward of +1 only if it produces an incorrect step that fools the PRM, forcing it to create subtle reasoning errors rather than trivial mistakes. Both models are trained simultaneously using PPO with OGDA optimizer, with regularization terms (KL-divergence and entropy) added to ensure linear convergence to a Nash Equilibrium. The framework requires no manual step-level annotations, as the Generator automatically creates hard negatives that adapt to the PRM's current weaknesses, creating a continuously evolving curriculum.

## Key Results
- APRM achieves +3.4 percentage points average improvement in solver accuracy across mathematical reasoning benchmarks
- Largest gains of +5.3 percentage points on out-of-distribution tasks, demonstrating enhanced generalization
- Ablation studies confirm the importance of entropy regularization and OGDA optimizer for training stability
- Performance improvements are consistent across different solver sizes and scientific domains

## Why This Works (Mechanism)

### Mechanism 1: Adaptive Hard Negative Mining
APRM improves PRM robustness by dynamically generating "hard negatives" (subtle errors) that specifically target the PRM's current weaknesses, rather than relying on static error distributions. A Generator and Reward Model play a general-sum game where the Generator receives +1 reward only if it generates an incorrect step that fools the Reward Model. This forces the Generator to move beyond trivial syntax errors and learn nuanced, logical fallacies that the Reward Model fails to catch. As the Reward Model improves, the Generator must generate harder errors to win, creating a continuously adapting curriculum.

### Mechanism 2: Convergence via Regularized Game Utilities
The inclusion of Symmetric KL-divergence and entropy regularization theoretically guarantees linear convergence to a unique Nash Equilibrium, preventing the oscillating dynamics common in adversarial training. Standard adversarial games often suffer from cycling or divergence. The paper adds regularization to the utility functions, forcing the players to stay close to the initial policy while exploring, rendering the utility functions strongly concave. This transforms the problem into a strongly monotone game, allowing the Optimistic Gradient Descent-Ascent optimizer to converge linearly.

### Mechanism 3: General-Sum Game Formulation
Modeling the interaction as a general-sum (non-zero-sum) game allows for a stationary-point Nash Equilibrium rather than a saddle point, enabling distinct, stable goals for the generator and the discriminator. Unlike GANs (zero-sum), where the Generator's loss is exactly -Reward Model's loss, APRM assigns asymmetric rewards. This decoupling allows the system to find a stable state where the PRM is robust against the specific error distribution generated by the Generator.

## Foundational Learning

- **Concept: Process Reward Models (PRMs)**
  - Why needed here: This paper specifically optimizes the step-level verification capability, distinguishing between outcome supervision (final answer) and process supervision (intermediate steps)
  - Quick check question: Can you explain why a correct final answer does not guarantee correct intermediate reasoning steps?

- **Concept: Nash Equilibrium vs. Saddle Points**
  - Why needed here: The paper claims convergence to a Nash Equilibrium, which is a "no regret" state where neither player can unilaterally improve
  - Quick check question: In a two-player game, if the Generator updates to a new strategy, does the Reward Model's previous best response remain optimal?

- **Concept: Optimistic Gradient Descent-Ascent (OGDA)**
  - Why needed here: The paper replaces standard PPO gradients with OGDA to prevent training oscillations
  - Quick check question: How does the "optimistic" update rule (z_{t+1} = z_t - η(2F(z_t) - F(z_{t-1}))) differ from standard gradient descent?

## Architecture Onboarding

- **Component map:** Correct step → Generator (Llama-3.1-8B) → Perturbed/incorrect step → Oracle → Reward Model (Llama-3.1-8B) → Binary classification
- **Critical path:** G perturbs a gold step → Oracle determines if perturbation is validly incorrect → R classifies perturbation → Calculate asymmetric rewards → OGDA updates
- **Design tradeoffs:** Oracle strictness (if too strict, rejects nuanced errors and curriculum fails; if too lenient, rewards nonsensical outputs); Using identical backbones for G and R ensures a "fair fight," but a larger G creates a stronger curriculum
- **Failure signatures:** Reward Hacking (Generator produces nonsensical steps that Oracle labels as "incorrect" but are trivially detectable); Mode Collapse (Generator produces same single error repeatedly); Oscillating Loss (Loss curves for G and R diverge and spike cyclically)
- **First 3 experiments:**
  1. Oracle Validation: Run Generator on hold-out set and manually inspect if Oracle correctly identifies "subtle" errors vs. "trivial" errors
  2. Ablation on Regularization: Train with τ=0 (no KL penalty) and visualize drop in stability compared to full setup
  3. Error Difficulty Analysis: Visualize "fooling rate" (R accuracy) over time, which should decrease initially and then stabilize/increase as R adapts

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical convergence guarantees depend heavily on regularization coefficients, but practical sensitivity to exact values of τ and c_H remains unclear
- Generator's dependence on a "correctness oracle" introduces a brittle bottleneck that could fail if oracle is too strict or too lenient
- General-sum game formulation's benefits over zero-sum alternatives are stated but not empirically validated through direct comparisons

## Confidence
- **High Confidence:** Core experimental results showing APRM outperforms baselines on mathematical reasoning benchmarks; ablation studies on OGDA and entropy regularization
- **Medium Confidence:** Theoretical claim of linear convergence under regularized game utilities; derivation is sound but practical conditions are not fully explored
- **Low Confidence:** Assertion that general-sum game formulation is primary driver of OOD robustness; requires head-to-head comparisons with zero-sum formulations

## Next Checks
1. Oracle Robustness Audit: Systematically vary oracle's strictness threshold and measure impact on Generator performance and PRM accuracy
2. Zero-Sum Game Baseline: Implement and train zero-sum variant of APRM on same tasks to isolate effect of game formulation
3. Generalization Stress Test: Evaluate APRM on broader set of out-of-distribution domains (physics problems, code reasoning) to test robustness generalization