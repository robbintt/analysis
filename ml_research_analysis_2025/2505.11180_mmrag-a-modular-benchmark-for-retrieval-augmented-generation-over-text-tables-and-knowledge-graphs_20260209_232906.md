---
ver: rpa2
title: 'mmRAG: A Modular Benchmark for Retrieval-Augmented Generation over Text, Tables,
  and Knowledge Graphs'
arxiv_id: '2505.11180'
source_url: https://arxiv.org/abs/2505.11180
tags:
- generation
- https
- query
- retrieval
- mmrag
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: mmRAG is a multi-modal benchmark designed to evaluate retrieval-augmented
  generation (RAG) systems over text, tables, and knowledge graphs. It addresses the
  limitations of existing benchmarks by providing a unified framework with cross-dataset
  relevance annotations and modular evaluation of query routing, retrieval, and generation.
---

# mmRAG: A Modular Benchmark for Retrieval-Augmented Generation over Text, Tables, and Knowledge Graphs

## Quick Facts
- arXiv ID: 2505.11180
- Source URL: https://arxiv.org/abs/2505.11180
- Reference count: 40
- Primary result: Multi-modal benchmark for RAG systems over text, tables, and knowledge graphs with 5,124 queries and 3.2 million chunks

## Executive Summary
mmRAG introduces a comprehensive multi-modal benchmark designed to evaluate retrieval-augmented generation systems across text, tables, and knowledge graphs. The benchmark addresses critical gaps in existing evaluation frameworks by providing a unified structure with cross-dataset relevance annotations and modular assessment of RAG components. It integrates six diverse question-answering datasets and offers extensive evaluation resources including 88,751 annotated query-chunk pairs, enabling systematic comparison of different retrieval strategies and generation models.

## Method Summary
The benchmark employs a modular architecture that separates query routing, retrieval, and generation components for independent evaluation. Six heterogeneous datasets were integrated and unified through a novel relevance annotation framework, enabling cross-dataset comparisons. The system processes 90,998 documents into 3.2 million chunks using a consistent 500-character size, with retrieval evaluated using NDCG@1 and Hits@1 metrics. Generation quality is assessed through ROUGE scores and human evaluations, while router performance is measured through classification accuracy. The modular design allows systematic ablation studies to identify bottlenecks and optimization opportunities in RAG pipelines.

## Key Results
- BGE model achieves highest retrieval accuracy with NDCG@1 of 0.617 and Hits@1 of 0.703
- LLM router outperforms semantic router in routing accuracy across all modalities
- Generation quality improves significantly when using dataset-specific chunks compared to cross-dataset retrieval
- Text modality shows superior performance compared to tables and knowledge graphs in both retrieval and generation tasks

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its modular decomposition of RAG systems into distinct, independently evaluable components. By providing unified relevance annotations across diverse datasets, it enables meaningful cross-dataset comparisons that were previously impossible. The standardized chunk processing and consistent evaluation metrics create reproducible conditions for comparing different retrieval strategies. The systematic ablation studies reveal how individual components contribute to overall system performance, while the cross-modal integration exposes unique challenges in handling heterogeneous data types within unified RAG pipelines.

## Foundational Learning
- **Retrieval-augmented generation (RAG)**: Combines information retrieval with text generation to produce more accurate responses by accessing external knowledge sources. Needed for systems requiring factual accuracy beyond training data. Quick check: Verify system can retrieve relevant documents before attempting generation.
- **NDCG@1 and Hits@1 metrics**: Evaluate retrieval quality by measuring ranking effectiveness and relevance of top results. Essential for quantifying retrieval performance objectively. Quick check: Compare metric scores across different retrieval models to identify performance gaps.
- **Query routing**: Determines which retrieval strategy or data source to use for a given query based on its characteristics. Critical for optimizing multi-modal RAG systems. Quick check: Test routing accuracy by measuring classification performance on labeled query sets.
- **Chunk-based processing**: Divides documents into uniform segments for consistent retrieval and generation. Necessary for managing document length variations across modalities. Quick check: Verify chunk boundaries preserve semantic coherence and context.
- **Cross-modal retrieval**: Enables unified search across heterogeneous data types including text, tables, and knowledge graphs. Important for comprehensive information access in multi-modal systems. Quick check: Test retrieval effectiveness when mixing document types in the same query.

## Architecture Onboarding

Component Map:
Document Preprocessing -> Chunk Processing -> Query Routing -> Retrieval Engine -> Generation Model

Critical Path:
Document Preprocessing -> Chunk Processing -> Query Routing -> Retrieval Engine -> Generation Model

Design Tradeoffs:
- Fixed chunk size (500 chars) vs. semantic coherence: Standardization simplifies processing but may split related content
- Automated vs. human relevance annotations: Scalability vs. annotation quality and nuance capture
- Multi-modal vs. modality-specific models: Unified architecture vs. specialized performance per data type
- Cross-dataset integration vs. domain specificity: Broad applicability vs. nuanced domain understanding

Failure Signatures:
- Low routing accuracy indicates inability to correctly classify query types or select appropriate retrieval strategies
- Poor NDCG scores suggest retrieval models fail to rank relevant chunks highly
- Inconsistent ROUGE scores across modalities reveal modality-specific weaknesses in generation models
- High variance in cross-dataset retrieval indicates dataset-specific retrieval challenges

First Experiments:
1. Evaluate retrieval performance of BGE vs. ColBERTv2 on each modality separately
2. Test LLM router vs. semantic router on a held-out validation set with known query types
3. Conduct ablation study removing query routing to measure impact on overall retrieval accuracy

## Open Questions the Paper Calls Out
The paper identifies several areas for future research including extending the benchmark to additional modalities beyond text, tables, and knowledge graphs. It suggests exploring cross-lingual capabilities and domain-specific extensions to improve real-world applicability. The authors also highlight opportunities for developing more sophisticated routing mechanisms that can better handle ambiguous queries and emerging multi-modal data types.

## Limitations
- English-language focus limits applicability to multilingual RAG systems and non-English document types
- Fixed chunk size of 500 characters may not be optimal for all document types, particularly long-form content or complex tables
- Automated metrics like NDCG and ROUGE may not fully capture semantic relevance or user satisfaction in practical applications

## Confidence
- **High Confidence**: Retrieval accuracy metrics (NDCG@1 of 0.617 and Hits@1 of 0.703 for BGE) and modular evaluation framework are well-supported
- **Medium Confidence**: LLM router superiority is demonstrated but needs testing with additional architectures and larger samples
- **Medium Confidence**: Dataset-specific chunk improvement claim is supported but needs more ablation studies across model families

## Next Checks
1. Conduct ablation studies testing impact of different chunk sizes (100-1000 characters) on retrieval and generation performance across all three modalities
2. Extend evaluation to include multilingual datasets and non-English document types to assess cross-lingual RAG capabilities
3. Implement human evaluation studies to validate automated metric results and assess real-world usability across different RAG system configurations