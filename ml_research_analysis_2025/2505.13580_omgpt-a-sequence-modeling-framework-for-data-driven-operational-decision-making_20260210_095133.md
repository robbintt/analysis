---
ver: rpa2
title: 'OMGPT: A Sequence Modeling Framework for Data-driven Operational Decision
  Making'
arxiv_id: '2505.13580'
source_url: https://arxiv.org/abs/2505.13580
tags:
- decision
- where
- demand
- optimal
- environment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces OMGPT, a transformer-based framework for
  solving sequential decision-making tasks in operations management and research.
  It reframes problems like dynamic pricing, inventory management, and queuing control
  as sequence modeling tasks, where the goal is to predict optimal actions based on
  historical data.
---

# OMGPT: A Sequence Modeling Framework for Data-driven Operational Decision Making

## Quick Facts
- arXiv ID: 2505.13580
- Source URL: https://arxiv.org/abs/2505.13580
- Authors: Hanzhao Wang; Guanting Chen; Kalyan Talluri; Xiaocheng Li
- Reference count: 40
- Key outcome: This paper introduces OMGPT, a transformer-based framework for solving sequential decision-making tasks in operations management and research. It reframes problems like dynamic pricing, inventory management, and queuing control as sequence modeling tasks, where the goal is to predict optimal actions based on historical data. The approach leverages generative AI techniques to train a transformer-based neural network, OMGPT, which does not assume any analytical model structure and can incorporate prior knowledge. Numerical experiments demonstrate that OMGPT consistently outperforms well-established benchmarks across multiple tasks. It shows strong generalization ability, handles non-stationary environments, and can provide valuable side information like demand forecasting. Analytically, the paper establishes a Bayesian perspective, showing that OMGPT behaves asymptotically as a Bayes-optimal decision maker. The framework offers a new paradigm for data-driven operational decision making, moving beyond traditional online learning methods.

## Executive Summary
OMGPT introduces a novel approach to sequential decision-making in operations management by reframing it as a supervised sequence modeling problem. Instead of relying on traditional online reinforcement learning, OMGPT pre-trains a transformer model on synthetic data generated from a prior distribution over environment parameters, learning to predict optimal actions given historical context. This allows the model to learn a direct mapping from history to optimal action without explicit online learning loops, enabling it to handle censored data, non-stationary environments, and provide valuable side information like demand forecasting. The framework demonstrates strong performance across multiple operational tasks including dynamic pricing, inventory management, and queuing control, consistently outperforming established benchmarks.

## Method Summary
OMGPT reframes sequential decision-making tasks as supervised sequence modeling problems where the model predicts optimal actions based on historical data. The method involves generating synthetic pre-training data by sampling environment parameters from a prior distribution and computing optimal actions via an oracle. A GPT-2 style transformer is then pre-trained to minimize prediction loss between its outputs and the oracle-computed optimal actions. The training uses a mixed approach that blends fixed data with data generated by the current model, incorporating curriculum learning to gradually increase sequence length. During testing, the model autoregressively feeds predicted actions back into the history, allowing it to handle non-stationary environments and censored data.

## Key Results
- OMGPT consistently outperforms well-established benchmarks across dynamic pricing, inventory management, and queuing control tasks
- The framework demonstrates strong generalization ability, handling non-stationary environments and providing valuable side information like demand forecasting
- Analytical results show OMGPT asymptotically behaves as a Bayes-optimal decision maker
- The model successfully handles censored data and can operate in environments where traditional online learning approaches struggle

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Reframing sequential decision-making as a supervised sequence modeling task allows the model to learn a policy without explicit online reinforcement learning loops.
- **Mechanism:** OMGPT generates pre-training data $(H_t, a^*_t)$ where $H_t$ is history and $a^*_t$ is the analytically computed optimal action for a sampled environment $\gamma$. By minimizing the prediction loss (e.g., MSE) between the model output and $a^*_t$, the model learns a direct mapping from history to optimal action.
- **Core assumption:** The environment parameter $\gamma$ of the testing task is sampled from the same prior distribution $P_\gamma$ used to generate the pre-training data, or the model has sufficient capacity to generalize to OOD (out-of-distribution) shifts.
- **Evidence anchors:**
  - [abstract] "reframes problems... as sequence modeling tasks, where the goal is to predict optimal actions based on historical data."
  - [section] Section 4.3.1 describes generating target actions $a^{(i)*}_t$ as the optimal action (Eq 3) to create the dataset $D_{PT}$.
  - [corpus] The paper "DecisionLLM" and "CORL" reinforce the paradigm of framing RL as sequence modeling/branch-and-bound, validating the general approach.
- **Break condition:** If the testing environment $\gamma$ falls outside the support of the pre-training prior $P_\gamma$ (extreme distribution shift), the model may fail to generalize or converge to suboptimal actions.

### Mechanism 2
- **Claim:** The pre-trained model asymptotically approximates a Bayes-optimal decision maker ($Alg^*$) by implicitly performing Bayesian inference over the environment parameter $\gamma$.
- **Mechanism:** Given the loss minimization objective over the prior $P_\gamma$, the model learns to predict the action that minimizes the expected loss under the posterior distribution $P(\gamma|H_t)$. For example, with squared loss, the model outputs the posterior averaging of optimal actions.
- **Core assumption:** The transformer architecture has sufficient capacity to model the complex mapping from history $H_t$ to the Bayes-optimal action.
- **Evidence anchors:**
  - [section] Section 5.1 establishes the Bayesian perspective, stating "asymptotically, it behaves as a Bayes-optimal decision maker."
  - [section] Corollary 5.2 details behavior under different losses (e.g., posterior averaging under squared loss).
  - [corpus] Corpus evidence is weak or missing for the specific theoretical link between GPT pre-training and Bayes-optimality in this context.
- **Break condition:** If the model capacity is too low (few layers/dimensions) or pre-training data is insufficient, the model converges to a suboptimal approximation of $Alg^*$.

### Mechanism 3
- **Claim:** Prediction errors resulting from finite pre-training data act as an inherent exploration mechanism, preventing the model from getting stuck in linear regret traps common in greedy algorithms.
- **Mechanism:** While a perfect Bayes-optimal function $Alg^*$ might suffer linear regret in certain continuous action spaces (due to lack of exploration), the finite-sample pre-trained model $\hat{f}$ has a deviation ($\Delta_{Exploit}$) from $Alg^*$. This deviation serves as random perturbation/exploration, aiding in the convergence of the posterior to the true environment.
- **Core assumption:** The deviation $\Delta_{Exploit}$ is neither zero (which would fail to explore) nor too large (which would fail to exploit).
- **Evidence anchors:**
  - [section] Section 5.2.3 and Theorem 5.9 discuss how prediction errors provide an "inherent exploration mechanism" ensuring sub-linear regret.
  - [section] Proposition 5.4 notes that the theoretical optimal function $Alg^*$ can incur linear regret, which the actual OMGPT avoids.
  - [corpus] Corpus evidence is weak or missing regarding this specific theoretical remedy for linear regret via pre-training error.
- **Break condition:** If pre-training data is infinite, the model might converge exactly to $Alg^*$, potentially causing the linear regret issue described in Proposition 5.4 (though the paper notes this is unlikely in practice).

## Foundational Learning

- **Concept: Sequential Decision Making under Uncertainty**
  - **Why needed here:** OMGPT targets OR/OM problems like dynamic pricing and inventory management where decisions influence future states and data is censored.
  - **Quick check question:** Can you explain the difference between "censored" and "uncensored" demand in the context of the Newsvendor problem?

- **Concept: Supervised Pre-training on Oracle Data**
  - **Why needed here:** Unlike standard RL which learns from interaction, this model requires generating a dataset of "optimal" actions ($a^*_t$) calculated via an oracle.
  - **Quick check question:** Why is access to the "optimal action" $a^*_t$ during training considered a paradigm shift compared to standard online learning?

- **Concept: Bayesian Posterior Inference**
  - **Why needed here:** The theoretical justification relies on the model approximating a Bayesian update $P(\gamma|H_t)$ given the history.
  - **Quick check question:** How does the choice of loss function (e.g., Squared Error vs. Cross-Entropy) dictate whether the model mimics Posterior Averaging or Posterior Sampling?

## Architecture Onboarding

- **Component map:** Feature elements (Context $X_\tau$ + Observation $O_{\tau-1}$) and Label elements (Action $a_\tau$) -> GPT-2 style Transformer (Encoder-decoder stack with causal masking) -> Linear layer predicting next optimal action $a^*_t$
- **Critical path:**
  1. Environment Sampling: Sample $\gamma \sim P_\gamma$ (e.g., demand parameters)
  2. Oracle Generation: Compute optimal $a^*_t$ for history $H_t$
  3. Pre-training: Supervised learning on $(H_t, a^*_t)$
  4. Testing: Autoregressively feed predicted actions back into history
- **Design tradeoffs:**
  - Loss Function: Use Squared Loss for regression tasks (leads to risk-averse averaging) vs. Cross-Entropy for discrete tasks (leads to posterior sampling)
  - Context Window: Truncating history to a fixed window allows generalization to longer horizons $T > 100$ but loses long-term memory
- **Failure signatures:**
  - Model Misspecification: If the testing environment's demand function is non-linear (e.g., square) but the model was only pre-trained on linear functions, performance degrades (unless a mixture was used)
  - Out-of-Distribution Priors: If the testing environment parameters fall outside the range of the pre-training prior $P_\gamma$, the model hallucinates suboptimal actions
- **First 3 experiments:**
  1. Dynamic Pricing (Linear): Train on contexts and optimal prices. Verify if the model learns to balance price vs. demand exploration.
  2. Ablation on History: Test the model with only the last $k$ steps of history to measure the importance of long-term context.
  3. Non-Stationarity Test: Introduce a sudden shift in the demand parameter $\gamma$ at time $t=T/2$ to see if the model adapts (as suggested in the "non-stationary" experiment).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretically optimal context window size for OMGPT when the testing horizon is random or extends beyond the pre-training horizon?
- Basis in paper: [explicit] The paper states in Section 4.5.1 that "theoretical analysis (e.g., determining the optimal window size for a random testing horizon) is an important and interesting future research direction."
- Why unresolved: While the paper demonstrates empirically that a context window allows generalization to longer horizons, it does not derive an analytical relationship between the window size, task complexity, and resulting regret or performance degradation.
- What evidence would resolve it: A theoretical derivation establishing the optimal window size as a function of the pre-training horizon length and the variance of the testing environment's horizon.

### Open Question 2
- Question: Under what specific conditions does the OMGPT pre-training phase converge and remain stable given the distribution shift between the data generation policy $\tilde{f}$ and the learned policy $TF_{\hat{\theta}}$?
- Basis in paper: [inferred] Appendix B.1 discusses the "performative prediction" nature of the training (where the model affects the data distribution) and notes that standard stability conditions (strong convexity) are not met, relying instead on a "claim" that richness of the function class prevents instability.
- Why unresolved: The paper provides an empirical algorithm (Algorithm 2) to mitigate this shift but acknowledges that theoretical guarantees for this "performative" stability in the absence of convexity remain unproven.
- What evidence would resolve it: A formal proof showing convergence conditions for the iterative training process defined in Algorithm 2, or a counter-example illustrating specific failure modes (instability/oscillation) in complex environments.

### Open Question 3
- Question: How can the "inherent exploration" mechanism of OMGPT be analytically characterized to ensure sub-linear regret without relying on the manual injection of random perturbations?
- Basis in paper: [inferred] Section 5.2.3 frames the regret analysis around a stylized model where regret depends on the balance between exploitation errors ($\Delta_{Exploit}$) and exploration intensity ($\Delta_{Explore}$). The paper notes that online algorithms usually make these time-dependent, but the exact automatic calibration of this balance by the transformer is not fully characterized theoretically.
- Why unresolved: The analysis relies on assumptions (Assumption 5.8) bounding the deviation of the transformer from the optimal decision, but does not rigorously prove that the finite-sample prediction errors naturally provide sufficient $\Delta_{Explore}$ to guarantee sub-linear regret in all non-stationary settings.
- What evidence would resolve it: Deriving a lower bound on the exploration term of the regret based solely on the statistical properties of the pre-training data and the transformer architecture, independent of the specific perturbation assumptions made in Example 5.10.

## Limitations
- The strength of benchmarks used to claim OMGPT "consistently outperforms" is not fully detailed in the main text
- The theoretical connection between finite-sample pre-training and asymptotic Bayes-optimality relies on assumptions about model capacity that are not empirically validated across all domains
- The mechanism by which pre-training errors provide "inherent exploration" lacks strong empirical validation in the paper

## Confidence
- **High Confidence:** The basic premise that sequential decision-making can be reframed as supervised sequence modeling is well-established and demonstrated
- **Medium Confidence:** The claim that OMGPT achieves strong generalization and handles non-stationary environments is supported by experiments, but the extent and robustness require further validation
- **Medium Confidence:** The theoretical claim of asymptotic Bayes-optimality is sound, but the practical implications and convergence rates in finite samples are not fully characterized

## Next Checks
1. **Benchmark Completeness:** Implement and test against a broader set of established online learning algorithms (e.g., UCB, Thompson Sampling, LinUCB) across all task domains to rigorously assess OMGPT's performance claims
2. **Finite-Sample Validation:** Conduct experiments to measure the gap between OMGPT's performance and the theoretical Bayes-optimal policy $Alg^*$ for various sample sizes, quantifying the impact of finite pre-training data on regret
3. **Distribution Shift Robustness:** Systematically test OMGPT's performance under controlled out-of-distribution shifts in the environment parameter $\gamma$ (beyond the testing range of $P_\gamma$) to quantify its failure modes and generalization limits