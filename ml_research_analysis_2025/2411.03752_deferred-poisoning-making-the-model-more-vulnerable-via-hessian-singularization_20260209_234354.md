---
ver: rpa2
title: 'Deferred Poisoning: Making the Model More Vulnerable via Hessian Singularization'
arxiv_id: '2411.03752'
source_url: https://arxiv.org/abs/2411.03752
tags:
- training
- poisoning
- poisoned
- attacks
- noise
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Deferred Poisoning Attack (DPA), a novel
  poisoning attack that maintains normal model performance during training and validation
  but significantly increases vulnerability to evasion attacks and natural noise.
  DPA achieves this by ensuring poisoned models have similar loss values to clean
  models while exhibiting large local curvature through Hessian singularization.
---

# Deferred Poisoning: Making the Model More Vulnerable via Hessian Singularization

## Quick Facts
- arXiv ID: 2411.03752
- Source URL: https://arxiv.org/abs/2411.03752
- Reference count: 11
- Key outcome: Novel poisoning attack maintains normal accuracy but significantly increases vulnerability to evasion attacks and natural noise via Hessian singularization

## Executive Summary
This paper introduces Deferred Poisoning Attack (DPA), a novel poisoning method that preserves clean training accuracy while dramatically increasing model vulnerability to evasion attacks. Unlike traditional poisoning that degrades accuracy, DPA manipulates the Hessian matrix's condition number during training, creating models with steep local curvature that amplify small input perturbations into large loss changes. The attack achieves this through a Singularization Regularization term that ensures poisoned models maintain similar loss values to clean models while exhibiting large local curvature. Experiments demonstrate DPA effectively reduces model robustness (e.g., FGSM robustness drops from 1.86% to 0.58% on CIFAR10) while maintaining accuracy, requires lower attack cost (ε=3/255), and demonstrates strong transferability across different model architectures.

## Method Summary
DPA uses a two-stage alternating optimization process. First, the model is trained on both clean and poisoned data using the loss L(f_θ(x), y) + L(f_θ(x̂), y), where x̂ represents the perturbed version of x. Second, the perturbation δ is updated to minimize L(f_θ(x̂), y) - Q(f_θ(x̂), y), where Q = tr(H^T H) is the trace of the Hessian squared, forcing the Hessian to become singular (ill-conditioned). The Hessian-vector product (HVP) approximation ||Hv||² is used instead of full Hessian computation for efficiency. Hyperparameters include learning rate 0.01 for model updates, 0.001 for perturbation updates, 20 epochs, batch size 64, and ε=3/255 constraint on perturbations. The optimization alternates between I_θ epochs of model training and I_δ iterations of perturbation updates.

## Key Results
- DPA achieves high adversarial vulnerability at ε = 3/255 (imperceptible perturbations), whereas traditional methods require ε ≥ 8/255
- Cross-architecture transfer: TinyImageNet poisoned via DenseNet121-derived perturbations reduces ResNet18 FGSM robustness from 0.74 → 0.48
- DPA remains effective against data augmentation defenses and pretrained models, posing significant threat to AI security
- The attack successfully reduces FGSM robustness from 1.86% to 0.58% on CIFAR10 while maintaining ~75% clean accuracy

## Why This Works (Mechanism)

### Mechanism 1: Hessian Singularization
The regularization term Q(f_θ(x+δ), y) = tr(H^T H) forces the loss landscape to develop steep local curvature around training samples. A large σ_max(H)/σ_min(H) ratio expands the range of possible loss changes under perturbation, enabling adversaries to find directions causing dramatic loss spikes. The optimization simultaneously minimizes L(f_θ(x), y) + L(f_θ(x+δ), y) to maintain clean performance. Core assumption: The loss function is strictly convex in a small neighborhood around each training sample.

### Mechanism 2: Stealth Through Decoupling
The perturbation update δ ← arg min_{||δ||∞≤ε} [L(f_θ(x+δ), y) - Q(f_θ(x+δ), y)] trades off maintaining low cross-entropy against maximizing curvature. With ε = 3/255, perturbations are imperceptible: even 5× magnification shows minimal residuals. This avoids the training-validation accuracy gap that flags traditional poisoning.

### Mechanism 3: Architecture-Independent Transferability
Perturbations generated on VGG16 successfully reduce robustness of ResNet18, ResNet50, and DenseNet121 in black-box settings. The curvature manipulation creates input-space structures that all models learn similarly, since convolutional architectures share inductive biases for edge/texture features where small perturbations concentrate.

## Foundational Learning

- **Concept:** Hessian matrices and condition numbers
  - **Why needed here:** Core mathematical object the attack manipulates; condition number κ(H) = σ_max/σ_min measures landscape "sharpness."
  - **Quick check question:** Can you explain why a high condition number implies sensitivity to small input changes?

- **Concept:** Adversarial robustness metrics (ρ̂)
  - **Why needed here:** The paper quantifies attack success via ρ̂(f) = 1/|D| Σ ||r̂(x)||_p / ||x||_p, the normalized minimum perturbation for misclassification.
  - **Quick check question:** What does a lower ρ̂ value indicate about model vulnerability?

- **Concept:** Hessian-Vector Product (HVP)
  - **Why needed here:** Computational optimization reducing Hessian computation from O(P²) to O(P) using PyTorch's autograd without materializing the full matrix.
  - **Quick check question:** Why does HVP avoid explicit Hessian construction while still providing curvature information?

## Architecture Onboarding

- **Component map:** Training Loop (I_θ epochs) → Phase 1: Model Training → Phase 2: Perturbation Update → Loss computation with HVP

- **Critical path:** Phase 2's HVP computation is the bottleneck. Use `torch.autograd.functional.hvp` for O(P) complexity. Full Hessian is O(P²) and impractical for image inputs.

- **Design tradeoffs:**
  - HVP vs. Full Hessian: HVP reduces computation from ~8s to ~0.008s per sample but provides lower-bound approximation of tr(H^T H).
  - Poisoning ratio vs. stealth: 100% poisoning yields strongest effect but 40% already shows meaningful degradation.
  - ε budget: Lower ε (3/255) maximizes stealth; higher ε increases effect but risks detection.

- **Failure signatures:**
  - Clean accuracy drops significantly (>5%): Perturbation optimization may be dominated by Q term; increase L weight or reduce I_δ iterations.
  - No robustness degradation: Check HVP implementation; verify gradient flows through the regularization term.
  - Poor transferability: Generate perturbations on ensemble of architectures rather than single source model.

- **First 3 experiments:**
  1. Reproduce CIFAR10 baseline: Train VGG16 with DPA (ε=3/255, 100% poisoning). Verify ACC ~75% and ρ̂_F drops from ~1.86 to ~0.58.
  2. Ablate HVP approximation: Compare full Hessian vs. HVP on subset (100 samples). Measure time and effectiveness gap.
  3. Test defense sensitivity: Apply adversarial training (PGD-AT, SAM, TRADES) to poisoned models. Expect limited defense.

## Open Questions the Paper Calls Out

### Open Question 1
Can the proposed curvature-minimizing defense be refined to prevent the severe degradation of clean accuracy observed in Table 10? The paper's proposed defense successfully restores robustness but causes clean accuracy to drop to 0.71, significantly lower than the clean baseline of 0.81.

### Open Question 2
Does the artificially induced high condition number of the Hessian matrix provide a detectable signature for identifying poisoned models before deployment? The attack is deemed "stealthy" based on standard validation metrics, but the core mechanism explicitly forces the Hessian matrix to become singular.

### Open Question 3
Does the Hessian-vector product (HVP) approximation maintain sufficient tightness to the true Hessian norm when scaling to high-resolution datasets like full ImageNet? The authors use the lower bound ||Hv||² to approximate the trace, and experiments are limited to low-resolution datasets.

## Limitations

- Transferability gaps: While the paper claims strong black-box transferability, experiments only show success across standard CNN architectures. The assumption that curvature manipulation transfers to non-CNN architectures (transformers, ViTs) remains untested.
- Defense landscape: The paper demonstrates that standard defenses only partially recover robustness, but the effectiveness against specialized poisoning defenses or anomaly detection methods is not evaluated.
- Scalability concerns: The HVP approximation still requires iterative perturbation updates. The attack's efficiency on larger datasets or with larger model architectures is unclear.

## Confidence

- **High:** The core mechanism of Hessian singularization and its implementation using HVP
- **Medium:** Transferability across CNN architectures, effectiveness against standard defenses
- **Medium:** Stealth claims based on visual inspection and low ε values

## Next Checks

1. **Cross-architecture transferability:** Test DPA-generated perturbations on Vision Transformer (ViT) and MLP-Mixer models to verify if the curvature-based attack transfers beyond CNNs. Measure FGSM and PGD robustness degradation.

2. **Defense robustness:** Implement spectral anomaly detection on training data (checking for unusual Hessian spectral properties) and evaluate whether this detects poisoned samples before model deployment.

3. **Computational scaling:** Measure wall-clock time for DPA on CIFAR10 (32×32) vs. TinyImageNet (64×64) to quantify how perturbation optimization scales with input resolution and model capacity.