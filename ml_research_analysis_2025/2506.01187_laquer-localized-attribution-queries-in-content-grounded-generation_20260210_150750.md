---
ver: rpa2
title: 'LAQuer: Localized Attribution Queries in Content-grounded Generation'
arxiv_id: '2506.01187'
source_url: https://arxiv.org/abs/2506.01187
tags:
- output
- attribution
- source
- spans
- laquer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Localized Attribution Queries (LAQuer), a
  new task that allows users to request fine-grained, user-directed attribution for
  specific spans of generated text. LAQuer improves upon existing sentence-level attribution
  methods by localizing both the output and source spans, enabling more precise fact-checking.
---

# LAQuer: Localized Attribution Queries in Content-grounded Generation

## Quick Facts
- arXiv ID: 2506.01187
- Source URL: https://arxiv.org/abs/2506.01187
- Reference count: 21
- Primary result: Localized Attribution Queries (LAQuer) reduce attributed text length by up to two orders of magnitude while maintaining accuracy in content-grounded generation

## Executive Summary
This paper introduces Localized Attribution Queries (LAQuer), a task enabling users to request fine-grained attribution for specific spans of generated text. Unlike sentence-level attribution methods, LAQuer localizes both output and source spans, allowing more precise fact-checking. The proposed framework consists of two stages: generation with optional attribution metadata, followed by decontextualization and query-focused attribution. Experiments on Multi-Document Summarization and Long-form Question Answering show LAQuer methods significantly reduce attributed text length while maintaining accuracy, with LLM prompting outperforming internal representation methods especially when combined with localized source attributions.

## Method Summary
LAQuer implements a two-stage framework where source-grounded text is first generated (with optional attribution metadata), then users can highlight specific spans to query for supporting evidence. The system decontextualizes highlighted spans into standalone claims, then performs query-focused attribution either through LLM prompting (GPT-4o with few-shot examples and fuzzy search fallback) or LLM internal representations (LLaMA-3.1-8B-Instruct with cosine similarity on layer 5 hidden states). The method is evaluated on SPARK MDS and Liu et al. LFQA datasets using AutoAIS (T5-XXL NLI model) for attribution quality, showing significant reductions in attributed text length while maintaining accuracy.

## Key Results
- LAQuer reduces attributed text length by up to two orders of magnitude compared to baseline methods
- LLM prompt method achieves 71.5% AutoAIS on contextualized facts versus 28.6% for LLM internals on Attr. First MDS
- Decontextualization improves AutoAIS by 15-20 points but remains challenging (42.4% vs 71.5% contextualized on Attr. First MDS)
- Attr. First generation reduces LAQuer query size by an order of magnitude (3K vs 58K chars for Vanilla LFQA)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Separating generation from attribution enables user-directed, post-hoc queries without requiring upfront prediction of user interests.
- **Mechanism:** The two-stage framework first produces grounded text (with optional metadata), then waits for user highlights before performing attribution. This allows the system to respond dynamically to arbitrary spans rather than pre-computing fixed attributions.
- **Core assumption:** Users can accurately identify which output spans express the fact they want to verify; the fact-checking intent is well-captured by the highlight selection.
- **Evidence anchors:** [abstract] "Our contributions include: (1) proposing the LAQuer task to enhance attribution usability"; [Section 3, Desiderata 1] "LAQuer requires developing methods that can dynamically provide attribution for any arbitrary fact of interest"

### Mechanism 2
- **Claim:** Decontextualizing highlighted spans before attribution improves alignment accuracy by resolving pronouns and implicit references.
- **Mechanism:** Step (A) reformulates contextualized highlights (e.g., "They deserve to know") into self-contained statements (e.g., "Consumers deserve to know"), enabling unambiguous matching against source text.
- **Core assumption:** The decontextualization model correctly infers the intended referents from surrounding output context.
- **Evidence anchors:** [Section 4.1] "This process may incorporate in the decontextualized statement additional phrases from the generated output text, beyond the user's initial highlights"; [Section 6.1] "LAQuer methods struggle with decontextualized facts... the model often omits the document's broader theme"

### Mechanism 3
- **Claim:** Pre-existing attribution metadata (sentence-level or span-level) drastically reduces the search space for LAQuer, improving both efficiency and accuracy.
- **Mechanism:** Methods like Attr. First provide localized source spans (~36 characters average). LAQuer then searches only within these spans rather than full documents, reducing prompt size by an order of magnitude (Table 7).
- **Core assumption:** The initial generation method's attribution metadata is reasonably accurate and covers the relevant content.
- **Evidence anchors:** [Section 4.2] "Sentence-level attribution approaches... significantly reduce the search space, facilitating the localization of supporting evidence"; [Section 6.1] "LAQuer methods can leverage initially localized attributions... ATTR. FIRST provides very concise sentence-level attribution, averaging only 36 characters"

## Foundational Learning

- **Concept:** Decontextualization (making contextualized claims stand-alone)
  - **Why needed here:** Core to Step (A); without it, pronouns and implicit references cause attribution failures
  - **Quick check question:** Given "He signed the bill yesterday" in a document about Biden, can you produce a decontextualized version?

- **Concept:** AutoAIS (attribution quality metric via NLI entailment)
  - **Why needed here:** Primary evaluation metric; distinguishes contextualized vs. decontextualized fact support
  - **Quick check question:** If a source says "The vote was 5-4" and the output says "It was close," does AutoAIS count this as supported?

- **Concept:** Hidden state similarity for token alignment
  - **Why needed here:** Underpins the LLM internals attribution method; requires understanding layer selection and thresholding
  - **Quick check question:** Why would cosine similarity between layer-5 hidden states of source and output tokens indicate attribution relationships?

## Architecture Onboarding

- **Component map:** Source-grounded generator (Vanilla/ALCE/Attr. First) → Decontextualization module (GPT-4o) → Query-focused attribution (LLM Prompt/Internals) → AutoAIS evaluation
- **Critical path:** 1. User highlights spans in generated output 2. Decontextualization prompt runs (GPT-4o) 3. Lexical alignment maps decontextualized fact back to output spans 4. Attribution method searches source (full docs or metadata-restricted spans) 5. Fuzzy search validates verbatim span extraction; retries on failure
- **Design tradeoffs:** LLM Prompt vs. Internals: Prompting achieves higher AutoAIS (71.5 vs. 28.6 on Attr. First MDS) but requires API calls and token costs; Internals requires GPU access and open weights but fails on non-localized sources. Generation method choice: Attr. First enables smallest LAQuer prompts (~3K chars vs. 58K for Vanilla LFQA) but increases generation-stage cost by ~90%. Decontextualization: Adds LLM call overhead but is necessary for pronoun-heavy outputs; skipping it causes systematic attribution gaps.
- **Failure signatures:** Low AutoAIS on decontextualized facts (42.4% vs. 71.5% contextualized on Attr. First MDS) → decontextualization omitting key entities; High non-attributed percentage (21.4% for LLM Internals + Attr. First) → internals method over-filtering extractive tokens; Long attribution lengths → search space not properly restricted by metadata.
- **First 3 experiments:** 1. Reproduce LLM Prompt baseline on SPARK MDS with Attr. First generation; verify AutoAIS contextualized score reaches ~71% and attribution length drops to ~15 content words 2. Ablate decontextualization: Run LAQuer skipping Step (A) on pronoun-containing highlights; measure AutoAIS degradation 3. Stress-test on out-of-domain sources: Apply Vanilla + LLM Prompt LAQuer to dataset without pre-existing attribution metadata; characterize where 32-character average attribution length breaks down

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can LAQuer methods be improved to accurately attribute decontextualized facts, which currently show substantially lower accuracy than contextualized facts?
- **Basis in paper:** [explicit] The authors state "LAQuer attribution remains a challenging task for decontextualized facts" and note that ATTR. FIRST yields "notably low AutoAIS scores" in the decontextualized setting.
- **Why unresolved:** Current methods fail to capture broader document context (e.g., omitting "Supreme Court" when attributing "issues"), and the decontextualization step doesn't adequately inform source attribution selection.
- **What evidence would resolve it:** A method achieving comparable AutoAIS scores for decontextualized and contextualized facts, with human evaluation confirming improved attribution completeness.

### Open Question 2
- **Question:** Can more efficient frameworks be developed to reduce the computational cost of LAQuer methods while maintaining attribution quality?
- **Basis in paper:** [explicit] The conclusion states "our methods are associated with a high cost of LLM calls, suggesting future research should focus on creating more efficient frameworks."
- **Why unresolved:** Current LAQuer methods require additional LLM calls for decontextualization and attribution; while ATTR. FIRST reduces query-time cost, it increases generation cost by 90%.
- **What evidence would resolve it:** A method achieving comparable attribution quality with significantly reduced latency or token usage, measured via cost analysis as in Table 7.

### Open Question 3
- **Question:** Can LAQuer be effectively applied to model outputs generated from parametric knowledge through post-hoc retrieval rather than pre-generation source grounding?
- **Basis in paper:** [explicit] The Limitations section states: "LAQuer could be applied to outputs generated by the model's parametric knowledge, by retrieving the documents after the generation rather than before. We leave such exploration for future work."
- **Why unresolved:** The current framework assumes source documents exist before generation; applying LAQuer to parametric outputs requires a retrieval step whose relevance to the original generation is unexplored.
- **What evidence would resolve it:** Evaluation of LAQuer on parametric-only generation with post-hoc retrieved documents, showing attribution quality comparable to content-grounded settings.

### Open Question 4
- **Question:** How can the LLM internals-based attribution method be improved to perform competitively with prompt-based methods for localized attribution?
- **Basis in paper:** [inferred] Results show LLM internals achieves only 10-46% AutoAIS compared to 42-83% for prompt-based methods, and the paper notes it "struggles with localization of document-level texts."
- **Why unresolved:** The internals method relies on hidden state similarity (cosine threshold θ=0.7, layer l=5), which may not capture cross-document attribution signals needed for LAQuer's fine-grained span localization.
- **What evidence would resolve it:** Modified representation-based methods achieving AutoAIS within 5-10% of prompt-based baselines across MDS and LFQA tasks.

## Limitations
- LLM internals method shows significant performance degradation on non-localized attribution sources (13.1% AutoAIS on Vanilla MDS vs. 28.6% with localized sources)
- Decontextualization step remains error-prone for facts requiring broader context, with AutoAIS scores 15-20 points lower for decontextualized facts
- High computational cost of repeated LLM calls (particularly for LLM Prompt method) and dependency on specific generation metadata may limit practical deployment

## Confidence
- High confidence: The core LAQuer framework architecture and its demonstrated ability to reduce attributed text length by up to two orders of magnitude while maintaining accuracy
- Medium confidence: The relative performance of LLM Prompt vs. LLM Internals methods, as results depend heavily on specific generation pipeline configurations
- Medium confidence: The claim that separating generation from attribution enables dynamic, user-directed queries, as this depends on users accurately identifying relevant spans

## Next Checks
1. Test LAQuer on out-of-domain datasets without pre-existing attribution metadata to characterize performance degradation when source localization is unavailable
2. Evaluate the impact of different decontextualization prompt variations on AutoAIS scores, particularly for pronoun-heavy outputs where current methods struggle
3. Measure computational cost trade-offs between LLM Prompt and LLM Internals methods across different document lengths and source complexity levels