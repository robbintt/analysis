---
ver: rpa2
title: 'A2D: Any-Order, Any-Step Safety Alignment for Diffusion Language Models'
arxiv_id: '2509.23286'
source_url: https://arxiv.org/abs/2509.23286
tags:
- harmful
- mask
- alignment
- arxiv
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses a new class of safety vulnerabilities in diffusion
  large language models (dLLMs) caused by their any-order, any-step decoding flexibility,
  which allows harmful content to emerge at arbitrary positions. The authors introduce
  A2D (Any-Order, Any-Step Defense), a token-level alignment method that trains dLLMs
  to emit an [EOS] refusal signal whenever harmful spans are encountered, enabling
  robust rejection regardless of decoding order or step.
---

# A2D: Any-Order, Any-Step Safety Alignment for Diffusion Language Models

## Quick Facts
- arXiv ID: 2509.23286
- Source URL: https://arxiv.org/abs/2509.23286
- Reference count: 40
- Primary result: A2D reduces DIJA attack success rates from over 80% to near-zero (1.3% on LLaDA-8B-Instruct, 0.0% on Dream-v0-Instruct-7B) while maintaining model capability and avoiding over-refusals

## Executive Summary
A2D addresses a critical safety vulnerability in diffusion language models (dLLMs) caused by their flexible any-order, any-step decoding, which allows harmful content to emerge at arbitrary positions. The authors introduce a token-level alignment method that trains dLLMs to emit an [EOS] refusal signal whenever harmful spans are encountered, enabling robust rejection regardless of decoding order or step. This approach significantly outperforms response-level safety alignment, cutting jailbreak success rates from over 80% to near-zero while maintaining model capability and avoiding over-refusals on benign prompts. Additionally, A2D supports real-time safety monitoring and early rejection, achieving up to 19.3× faster safe termination.

## Method Summary
A2D is a token-level safety alignment method for diffusion language models that trains models to predict an [EOS] token when encountering harmful content at any masked position, rather than only at the sequence start. The training uses a combined dataset of harmful samples (where masked tokens are supervised to predict [EOS]) and retain samples (where masked tokens predict original tokens). The method employs LoRA fine-tuning with specific hyperparameters (r=32, alpha=64, dropout=0.05) and monitors [EOS] probability at the leftmost masked position during inference for early rejection with configurable thresholds.

## Key Results
- Reduces DIJA attack success from over 80% to near-zero (1.3% on LLaDA-8B-Instruct, 0.0% on Dream-v0-Instruct-7B)
- Achieves 0% false positive rate on XSTest, indicating no over-refusals on benign prompts
- Maintains model capability on standard benchmarks (MMLU, GSM8K, HumanEval) without significant degradation
- Provides up to 19.3× faster safe termination through early rejection

## Why This Works (Mechanism)
A2D works by exploiting the token-level granularity of diffusion models' generation process. Unlike autoregressive models that generate left-to-right, dLLMs can predict any masked token at any step, creating vulnerabilities where harmful content can emerge at arbitrary positions. A2D trains the model to recognize harmful spans regardless of their position by supervising masked tokens in harmful contexts to predict [EOS]. This creates a robust refusal mechanism that operates at the token level rather than the response level, making it resistant to template-based attacks like DIJA that interleave harmful text with masked tokens. The [EOS] signal serves as both a refusal indicator and an early termination trigger.

## Foundational Learning

- **Concept: Masked Diffusion Models (dLLMs)**
  - Why needed here: A2D is designed specifically for masked diffusion LLMs (like LLaDA, Dream), which generate by iteratively predicting masked tokens rather than autoregressive left-to-right decoding.
  - Quick check question: Can you explain how a dLLM generates text differently from an autoregressive model, and why this creates different safety vulnerabilities?

- **Concept: Shallow Alignment Problem**
  - Why needed here: The paper identifies that standard safety alignment in dLLMs decays rapidly after initial tokens, leaving models vulnerable to attacks that target later generation steps.
  - Quick check question: What does "shallow alignment" mean in the context of dLLMs, and how does the KL divergence analysis in Figure 3 demonstrate this problem?

- **Concept: Template-Based Attacks (DIJA)**
  - Why needed here: DIJA exploits dLLM's any-order generation by interleaving harmful text with masked tokens, bypassing response-level refusals. A2D specifically targets this vulnerability.
  - Quick check question: How does DIJA attack differ from standard jailbreaks, and why does it require token-level rather than response-level defense?

## Architecture Onboarding

- **Component map:** BeaverTails dataset -> Harmful Set (1,733 samples) + Retain Set (1,288 samples) -> Training with LoRA (r=32, alpha=64) -> [EOS] monitoring at leftmost masked position -> Early rejection with threshold τ

- **Critical path:**
  1. Prepare combined dataset (harmful + retain samples)
  2. For each training step: sample (prompt, response), sample timestep t, compute mask ratio λ, apply masking
  3. If harmful sample: set masked targets to [EOS]; if retain: set targets to original tokens
  4. Train with cross-entropy loss on masked positions
  5. At inference: monitor [EOS] probability for early rejection

- **Design tradeoffs:**
  - Threshold τ selection: Higher τ (e.g., 0.99) minimizes false refusals but slower rejection; lower τ (e.g., 0.8) faster (19.3×) but more false positives
  - [EOS] vs alternative tokens: Paper shows OOD/high-freq/low-freq tokens provide defense but degrade MMLU; [EOS] preserves capability
  - Training data size: Uses 3,021 samples from BeaverTails; larger datasets may improve but not evaluated

- **Failure signatures:**
  - Over-refusal: Model rejects benign prompts that superficially resemble harmful ones (XSTest measures this; A2D achieves 0% false positives)
  - Alignment decay: If KL divergence from base model decreases rapidly across decoding steps, alignment is too shallow (A2D shows sustained divergence in Figure 4)
  - [EOS]-interference attacks: Explicit instructions to avoid [EOS] or produce long answers may attempt to bypass; Table 14 shows A2D remains robust

- **First 3 experiments:**
  1. Reproduce KL divergence analysis (Figure 3/4): Compare base vs. aligned models on harmful prompts across decoding steps to verify shallow alignment in baseline and deep alignment in A2D.
  2. DIJA attack evaluation: Apply DIJA to aligned model on HarmBench; verify attack success rate drops from >80% baseline to near-zero with A2D.
  3. Early rejection threshold sweep: Test τ ∈ {0.8, 0.9, 0.95, 0.99} on AdvBench (harmful) and XSTest (benign) to measure speedup vs. false positive tradeoff.

## Open Questions the Paper Calls Out

- **Can A2D effectively scale and adapt to future diffusion language model architectures with emerging decoding paradigms?**
  - Basis in paper: [explicit] The authors state in Appendix D that "extending A2D to future dLLMs as a promising direction" is necessary as "diffusion-based generation is evolving rapidly."
  - Why unresolved: The method was validated on current representative dLLMs (LLaDA, Dream), but the rapid development of new decoding strategies could introduce unforeseen vulnerabilities.
  - Evidence: Evaluation of A2D on next-generation dLLM architectures and decoding schedules to verify if token-level alignment persists.

- **How effectively does A2D generalize to autoregressive (AR) LLMs compared to diffusion models?**
  - Basis in paper: [explicit] Appendix C.5 notes that "a full architectural generalization study is beyond the scope of this work," despite preliminary findings suggesting the mechanism applies to AR models.
  - Why unresolved: A2D exploits the any-order nature of dLLMs; while a lightweight adaptation showed promise, a comprehensive study on standard AR architectures is missing.
  - Evidence: A rigorous benchmark of A2D on standard autoregressive models (e.g., LLaMA) against established AR-specific safety baselines.

- **Can a dedicated, purpose-built refusal token outperform the [EOS] token in balancing safety alignment with general capability?**
  - Basis in paper: [inferred] Section 6.4 investigates existing tokens (OOD, high-freq) and concludes [EOS] is best due to familiarity, but does not explore training a new specialized token.
  - Why unresolved: Using [EOS] conflates the "stop" signal with the "refusal" signal; a distinct token might theoretically allow for cleaner decision boundaries and lower alignment tax.
  - Evidence: An experiment training models with a novel special token for refusal and measuring the trade-off between Attack Success Rate (ASR) and capability benchmarks.

## Limitations

- Unknown Training Parameters: The minimum mask ratio ε in the mask ratio formula λ = (1-ε)t + ε is not explicitly specified in the appendix, which could affect the model's ability to learn robust token-level refusal behavior across different masking densities.

- Data Construction Ambiguity: The exact heuristics for identifying "harmful-looking prompts with benign completions" for the Retain Set are not fully specified, creating potential reproducibility challenges in balancing over-refusal and under-refusal rates.

- Attack Generalization: While A2D shows strong performance against DIJA and other template attacks, the paper does not extensively evaluate against adversarial strategies specifically designed to bypass token-level [EOS] monitoring.

## Confidence

**High Confidence:**
- A2D effectively reduces DIJA attack success rates from >80% to near-zero on both LLaDA-8B-Instruct (1.3%) and Dream-v0-Instruct-7B (0.0%)
- A2D maintains model capability on standard benchmarks (MMLU, GSM8K, HumanEval) without significant degradation
- A2D achieves 0% false positive rate on XSTest, indicating successful calibration against over-refusal

**Medium Confidence:**
- A2D provides sustained alignment throughout the decoding process (not just shallow alignment)
- The [EOS] token is the optimal choice for token-level refusal compared to OOD/high-freq/low-freq alternatives
- The 19.3× speedup for safe termination is achievable across diverse harmful prompts

**Low Confidence:**
- A2D's effectiveness against future, unknown attack variants specifically targeting token-level monitoring
- The generalizability of results to larger dLLM architectures beyond the tested 7B and 8B models
- The impact of different minimum mask ratio ε values on the model's robustness

## Next Checks

1. **Threshold Sensitivity Analysis**: Systematically test A2D with different [EOS] probability thresholds τ ∈ {0.7, 0.8, 0.9, 0.95, 0.99} on a comprehensive test suite including HarmBench, AdvBench, and XSTest to quantify the exact tradeoff between false positive rate and early rejection speed.

2. **Adversarial Bypass Testing**: Design and evaluate specific attack prompts that explicitly instruct the model to avoid [EOS] tokens, produce outputs longer than a specified threshold, or otherwise attempt to circumvent token-level monitoring, measuring any degradation in A2D's defense effectiveness.

3. **Cross-Model Generalization Study**: Apply A2D to a larger dLLM (e.g., 30B or 70B parameter models) and evaluate whether the same training methodology and hyperparameters produce comparable safety improvements and capability preservation, identifying any scaling challenges.