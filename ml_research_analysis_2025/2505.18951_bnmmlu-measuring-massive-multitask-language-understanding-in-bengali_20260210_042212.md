---
ver: rpa2
title: 'BnMMLU: Measuring Massive Multitask Language Understanding in Bengali'
arxiv_id: '2505.18951'
source_url: https://arxiv.org/abs/2505.18951
tags:
- shot
- bengali
- language
- question
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: BnMMLU is a new benchmark for measuring massive multitask language
  understanding in Bengali, consisting of 134,375 multiple-choice questions across
  41 academic and professional domains. The benchmark is designed to evaluate the
  factual knowledge, reasoning, and application skills of language models in Bengali.
---

# BnMMLU: Measuring Massive Multitask Language Understanding in Bengali

## Quick Facts
- arXiv ID: 2505.18951
- Source URL: https://arxiv.org/abs/2505.18951
- Reference count: 40
- Key outcome: BnMMLU is a new benchmark for measuring massive multitask language understanding in Bengali, consisting of 134,375 multiple-choice questions across 41 academic and professional domains.

## Executive Summary
BnMMLU is a comprehensive benchmark designed to evaluate the factual knowledge, reasoning, and application skills of language models in Bengali. It comprises 134,375 multiple-choice questions spanning 41 academic and professional domains. Evaluations on 24 model variants reveal that proprietary models currently lead, but the best open-weight models narrow the gap significantly, especially with reasoning enabled. The results highlight persistent performance gaps in reasoning and application skills, indicating that improvements beyond mid-compute scale depend heavily on data and training recipe quality.

## Method Summary
The BnMMLU benchmark is constructed by translating and adapting the Massive Multitask Language Understanding (MMLU) dataset into Bengali. It covers 41 domains, including STEM, humanities, and professional fields, with a total of 134,375 multiple-choice questions. The benchmark is designed to assess factual knowledge, reasoning, and application skills in Bengali. Evaluations were conducted on 24 model variants, including both proprietary and open-weight models, using both direct and Chain-of-Thought prompting strategies.

## Key Results
- Proprietary models currently outperform open-weight models on BnMMLU.
- The best open-weight models narrow the performance gap significantly when reasoning is enabled.
- Performance gaps persist in reasoning and application skills, indicating a need for improved data and training recipes beyond mid-compute scales.

## Why This Works (Mechanism)
BnMMLU leverages the established MMLU framework, adapting it to the Bengali language to provide a rigorous evaluation of language models' capabilities in a low-resource language. By covering a wide range of academic and professional domains, it ensures a comprehensive assessment of factual knowledge and reasoning skills. The use of multiple-choice questions allows for standardized evaluation across diverse models.

## Foundational Learning
- **Language Model Benchmarking**: Why needed: To measure and compare the performance of language models across different languages and domains. Quick check: Ensure the benchmark covers a representative set of tasks and domains.
- **Bengali Language Processing**: Why needed: To address the specific challenges and nuances of processing and understanding Bengali text. Quick check: Validate the quality and cultural relevance of the translated questions.
- **Multitask Learning**: Why needed: To evaluate models' ability to handle diverse tasks simultaneously. Quick check: Assess model performance across all domains and task types.

## Architecture Onboarding
- **Component Map**: Data Translation -> Question Generation -> Model Evaluation -> Result Analysis
- **Critical Path**: Data Translation and Question Generation must be accurate to ensure valid evaluations.
- **Design Tradeoffs**: Balancing the breadth of domains with the depth of questions per domain.
- **Failure Signatures**: Models may struggle with longer questions or those requiring deep domain-specific knowledge.
- **First Experiments**:
  1. Evaluate a subset of questions using human annotators to benchmark model performance.
  2. Conduct linguistic reviews of translated questions for quality and cultural appropriateness.
  3. Perform cross-lingual transfer experiments to quantify translation and domain adaptation effects.

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How does the performance of language models on BnMMLU change when vision-aided reasoning (multimodal inputs) is enabled compared to the text-only baselines?
- Basis in paper: [explicit] The Limitations section explicitly states the study evaluates "text-only capabilities and do[es] not cover multimodal settings (vision-aided reasoning)."
- Why unresolved: The current benchmark cannot assess whether the failure modes in visual-heavy domains (like geometry or physics diagrams) are due to language understanding or the lack of visual processing.
- What evidence would resolve it: A multimodal evaluation of frontier models on BnMMLU items containing diagrams, charts, or visual context to measure accuracy deltas against text-only baselines.

### Open Question 2
- Question: What are the underlying mechanisms causing Chain-of-Thought (CoT) prompting to degrade performance in specific mid-scale models (e.g., Llama-3.2-3B, Qwen-3-32B) on difficult items?
- Basis in paper: [explicit] Section 5.1 reports "heterogeneous" and "sometimes negative" results, specifically citing instances where 5-shot CoT underperformed 5-shot Direct prompting (e.g., QWEN 3-32B).
- Why unresolved: The paper identifies the anomaly where reasoning exemplars lower accuracy but does not isolate whether the cause is context window saturation, instruction confusion, or linguistic misalignment in the reasoning traces.
- What evidence would resolve it: Ablation studies analyzing attention patterns and token probabilities in affected models to determine if CoT distracts from Bengali-specific semantic cues.

### Open Question 3
- Question: To what extent can optimizing training data composition and recipe quality overcome the "sublinear returns" and early plateaus observed in Bengali-centric models?
- Basis in paper: [inferred] The Conclusion posits that improvements beyond mid-compute "depend heavily on data and training recipe quality," while Section 5 observes that Bengali-centric families "plateau earlier" than global families.
- Why unresolved: It is unclear if the performance ceiling for Bengali-centric models is due to a fundamental lack of high-quality native training data or architectural inefficiencies in current training regimes.
- What evidence would resolve it: Training experiments comparing Bengali-centric models trained on web-scraped data versus high-quality, manually curated native corpora at matched compute scales.

## Limitations
- The coverage and cultural specificity of the 41 domains included in BnMMLU are not fully detailed.
- The quality and diversity of the translation pipeline from English MMLU to Bengali are unclear, which could introduce bias or ambiguity in question interpretation.
- The evaluation framework focuses on multiple-choice format, potentially missing nuanced understanding or context-dependent reasoning that may be more natural in Bengali discourse.

## Confidence
- Confidence in the reported performance gaps between proprietary and open-weight models is **High**, as the evaluation methodology and results are presented with sufficient detail and reproducibility.
- Confidence in the claimed dependence of performance on data and training recipe quality at mid-compute scales is **Medium**, given that while the trend is observed, the analysis could benefit from more granular ablation studies isolating the impact of each factor.

## Next Checks
1. Conduct a linguistic review of a stratified sample of BnMMLU questions to assess translation quality and cultural appropriateness for Bengali speakers.
2. Evaluate a subset of questions using human annotators to benchmark model performance against human-level understanding and identify potential ambiguities.
3. Perform cross-lingual transfer experiments by evaluating the same model on both BnMMLU and English MMLU to quantify translation and domain adaptation effects.