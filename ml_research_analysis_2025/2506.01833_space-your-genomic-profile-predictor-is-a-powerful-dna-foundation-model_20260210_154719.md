---
ver: rpa2
title: 'SPACE: Your Genomic Profile Predictor is a Powerful DNA Foundation Model'
arxiv_id: '2506.01833'
source_url: https://arxiv.org/abs/2506.01833
tags:
- space
- genomic
- prediction
- species
- profile
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper revisits supervised genomic profile prediction as an
  effective alternative to unsupervised DNA pre-training for foundation models. It
  introduces SPACE, which leverages Mixture of Experts to capture cross-species and
  cross-profile relationships in genomic data.
---

# SPACE: Your Genomic Profile Predictor is a Powerful DNA Foundation Model

## Quick Facts
- arXiv ID: 2506.01833
- Source URL: https://arxiv.org/abs/2506.01833
- Reference count: 32
- Primary result: SPACE achieves state-of-the-art performance on nucleotide transformer downstream tasks with MCC: 0.764 average across 18 tasks

## Executive Summary
This paper challenges the prevailing paradigm of unsupervised DNA pre-training by demonstrating that supervised genomic profile prediction can serve as an effective foundation modeling approach. The authors introduce SPACE, a model that leverages Mixture of Experts to capture cross-species and cross-profile relationships in genomic data. Through extensive benchmarking, SPACE achieves superior performance on nucleotide transformer downstream tasks and demonstrates remarkable cross-species generalization capabilities, particularly outperforming Enformer on yeast tasks by 8.88% to 27.28%.

## Method Summary
SPACE is a foundation model that uses supervised learning to predict genomic profiles across species. The model employs a species-aware encoder to capture species-specific patterns and a profile-grouped enhancement decoder to learn relationships between different genomic profiles. By leveraging Mixture of Experts, SPACE can effectively model the complex relationships between DNA sequences and their associated genomic profiles across different species. The model is pre-trained on human and mouse genomic profiles and evaluated on both nucleotide transformer downstream tasks and the Genome Understanding Evaluation (GUE) benchmark.

## Key Results
- Achieves state-of-the-art performance on nucleotide transformer downstream tasks with MCC: 0.764 average across 18 tasks
- Demonstrates superior cross-species generalization on GUE benchmark, outperforming Enformer by 8.88% to 27.28% on yeast tasks
- Ablation studies confirm the effectiveness of both species-aware encoder and profile-grouped enhancement decoder modules

## Why This Works (Mechanism)
SPACE works by leveraging the inherent relationships between DNA sequences and their associated genomic profiles across species. The supervised learning objective forces the model to learn meaningful representations of DNA sequences that are predictive of real genomic measurements. The Mixture of Experts architecture allows the model to specialize in different aspects of genomic prediction, while the species-aware components ensure that species-specific patterns are properly captured. This approach proves more effective than unsupervised pre-training for capturing the functional relationships that matter for genomic prediction tasks.

## Foundational Learning
- **Genomic profiles**: Functional measurements of DNA sequences (chromatin accessibility, histone modifications, etc.) - needed to understand what the model predicts; quick check: know the difference between accessibility and modification profiles
- **Cross-species generalization**: Ability to apply knowledge from one species to another - needed to evaluate model's biological relevance; quick check: understand evolutionary conservation of genomic features
- **Mixture of Experts**: Parameter-efficient architecture where different experts handle different inputs - needed to understand model capacity; quick check: know how gating mechanisms route inputs
- **Nucleotide transformer architecture**: Standard transformer adapted for DNA sequences - needed to understand baseline comparisons; quick check: know how DNA tokens differ from language tokens
- **Downstream evaluation metrics**: MCC (Matthews Correlation Coefficient) for classification tasks - needed to interpret results; quick check: understand why MCC is preferred over accuracy for imbalanced genomic data
- **Species-aware encoding**: Mechanisms to capture species-specific patterns - needed to understand the model's cross-species capabilities; quick check: know how species information is incorporated into the model

## Architecture Onboarding

**Component Map:**
Input DNA sequences -> Species-aware encoder -> Profile-grouped enhancement decoder -> Output predictions

**Critical Path:**
The critical path for genomic profile prediction flows from DNA sequence input through the species-aware encoder, which captures both species-specific and universal patterns, then through the profile-grouped enhancement decoder that leverages relationships between different genomic profiles to produce final predictions.

**Design Tradeoffs:**
The model trades off between universal genomic knowledge and species-specific patterns by using a species-aware encoder. The profile-grouped enhancement decoder sacrifices some architectural simplicity for improved performance by explicitly modeling relationships between different genomic profiles. The Mixture of Experts approach balances model capacity with computational efficiency.

**Failure Signatures:**
Poor cross-species performance suggests inadequate species-aware encoding. Subpar performance on individual profile types indicates issues with the profile-grouped enhancement mechanism. General underperformance compared to baselines may indicate problems with the supervised learning objective or insufficient model capacity.

**3 First Experiments:**
1. Evaluate species-specific vs. universal encoding performance on cross-species tasks
2. Test profile-grouped enhancement with different numbers of profile groups
3. Compare Mixture of Experts with other parameter-efficient architectures

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Does the performance of SPACE scale effectively with model size and training data volume comparable to large unsupervised DNA foundation models?
- Basis in paper: [explicit] The authors state in Section 5 (Limitations) that their model (588M parameters) is significantly smaller than the largest Nucleotide Transformer variant (2.5B), and posit that "increasing our model scale would lead to improvements."
- Why unresolved: Computational resource constraints prevented the authors from training a larger variant to verify if the supervised paradigm holds its advantage over unsupervised models at the 2.5B parameter scale.
- What evidence would resolve it: Training and benchmarking a 2.5B parameter version of SPACE against NT-Multispecies (2.5B) on the Nucleotide Transformer downstream tasks.

### Open Question 2
- Question: To what extent does expanding pre-training beyond human and mouse genomes improve the model's generalization to evolutionarily distant species?
- Basis in paper: [explicit] Section 5 notes that SPACE has "only been trained on two species" and suggests that "extending to more species would yield benefits" as data becomes available.
- Why unresolved: The current pre-training dataset is restricted to human and mouse genomic profiles, limiting the analysis of cross-species generalization to the specific yeast and viral tasks in the GUE benchmark.
- What evidence would resolve it: Pre-training the model on a multi-species genomic profile corpus (e.g., including primates or vertebrates) and measuring performance shifts on the GUE yeast and viral classification tasks.

### Open Question 3
- Question: Is a hybrid objective combining masked language modeling (MLM) with genomic profile prediction necessary to achieve state-of-the-art performance on variant effect prediction tasks?
- Basis in paper: [explicit] Appendix E observes that SPACE shows limited effectiveness on variant effect tasks and hypothesizes that "masked language modeling tasks may be necessary to achieve good performance on variant effect prediction."
- Why unresolved: The pure supervised pre-training objective excels at chromatin accessibility and histone modification but fails to match specialized baselines on variant effects, suggesting a gap in learning single-nucleotide sensitivities.
- What evidence would resolve it: Implementing a multi-task loss function that includes both profile prediction and MLM, then evaluating on the BEND benchmark's variant effect tasks.

## Limitations
- Model scale limitations prevent comparison with largest unsupervised DNA foundation models
- Limited pre-training to only human and mouse genomes restricts cross-species generalization analysis
- Poor performance on variant effect prediction tasks suggests limitations of pure supervised approach

## Confidence
- **Supervised profile prediction as foundation modeling approach**: High
- **Cross-species generalization superiority**: Medium
- **Mixture of Experts architecture effectiveness**: Medium

## Next Checks
1. Evaluate SPACE on additional genomic foundation model benchmarks beyond nucleotide transformers and GUE, including variant effect prediction and chromatin accessibility tasks, to assess general applicability.

2. Conduct systematic comparisons with other parameter-efficient architectures (e.g., LoRA, sparse attention) to isolate the specific benefits of the Mixture of Experts approach.

3. Test the cross-species generalization claims on a broader range of organisms and prediction tasks to determine if the improvements extend beyond the current evaluation scope.