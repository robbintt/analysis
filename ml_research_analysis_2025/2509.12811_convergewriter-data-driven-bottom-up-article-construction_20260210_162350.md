---
ver: rpa2
title: 'ConvergeWriter: Data-Driven Bottom-Up Article Construction'
arxiv_id: '2509.12811'
source_url: https://arxiv.org/abs/2509.12811
tags:
- knowledge
- generation
- content
- article
- documents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes ConvergeWriter, a bottom-up, data-driven framework
  for generating long-form, factual documents grounded in external knowledge bases.
  Unlike existing top-down approaches that first generate outlines and then retrieve
  evidence, ConvergeWriter performs exhaustive retrieval followed by unsupervised
  clustering to organize documents into knowledge clusters.
---

# ConvergeWriter: Data-Driven Bottom-Up Article Construction

## Quick Facts
- arXiv ID: 2509.12811
- Source URL: https://arxiv.org/abs/2509.12811
- Reference count: 6
- Key outcome: Achieves 80.14% document coverage (14B) and 70.51% (32B) on Wikipedia tasks, outperforming baselines in relevance, breadth, depth, and novelty

## Executive Summary
ConvergeWriter introduces a bottom-up, data-driven framework for generating long-form, factual documents grounded in external knowledge bases. Unlike traditional top-down approaches that first generate outlines and then retrieve evidence, ConvergeWriter performs exhaustive retrieval followed by unsupervised clustering to organize documents into knowledge clusters. These clusters form a data-driven foundation that directly guides outline generation and content creation, ensuring strict adherence to source material and full traceability. Experiments on Wikipedia-based tasks using 14B and 32B parameter models show state-of-the-art performance, particularly excelling in knowledge-constrained scenarios demanding high fidelity and structural coherence.

## Method Summary
The framework inverts the standard generation pipeline by first performing iterative breadth-first and depth-expansion retrieval to establish "knowledge boundaries" from the source corpus. Retrieved documents are embedded and clustered using K-means with Silhouette optimization, creating semantic groupings that reflect the data's intrinsic structure. These clusters are summarized hierarchically, and the LLM generates an outline strictly based on these summaries, ensuring the structure maps directly to the retrieved knowledge. Each outline section is then mapped to a specific cluster, and content is generated using only documents from that cluster, followed by a global consistency pass. This approach maximizes coverage while minimizing hallucination by constraining generation to verifiable source material.

## Key Results
- Achieves 80.14% document coverage (14B) and 70.51% (32B) on Wikipedia-based tasks
- Outperforms baselines in LLM-based rubric grading across Relevance, Breadth, Depth, and Novelty metrics
- Ablation study shows clustering is critical, with removal causing Novelty to drop from 4.58 to 3.60 and Breadth from 4.95 to 4.87
- Demonstrates superior performance in knowledge-constrained scenarios requiring factual accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Inverting the standard pipeline to retrieve evidence before creating an outline reduces structural hallucination by strictly confining the planning phase to available data
- Mechanism: The system performs "Retrieval-First for Knowledge," executing iterative breadth-first and depth-expansion retrieval to define the "knowledge boundaries" prior to any generative planning
- Core assumption: The retrieved corpus is sufficiently comprehensive; if retrieval is sparse or biased, the "boundaries" will be artificially narrow, limiting the article's potential scope
- Evidence anchors:
  - [abstract] "first establishes the 'knowledge boundaries' of the source corpus before any generative planning occurs"
  - [section 1] "avoids reliance on the LLM's inherently open-ended imagination"
  - [corpus] Related work supports the general difficulty of faithfulness but doesn't specifically validate this "retrieval-first" ordering

### Mechanism 2
- Claim: Deriving article structure from unsupervised clusters rather than LLM priors improves content novelty and breadth by surfacing latent semantic relationships
- Mechanism: Documents are embedded and clustered (K-means with Silhouette optimization). The LLM generates the outline strictly based on summaries of these clusters, forcing the structure to reflect the data's intrinsic distribution
- Core assumption: Semantic similarity in vector space correlates with logical thematic grouping suitable for a document chapter
- Evidence anchors:
  - [section 4.5] Ablation study shows removing clustering causes "Novelty" to drop from 4.58 to 3.60 and Breadth from 4.95 to 4.87
  - [section 3.2] "clustering algorithm... organizes the retrieved documents into distinct 'knowledge clusters'"
  - [corpus] "StoryBox" suggests bottom-up simulation aids long-form generation

### Mechanism 3
- Claim: Jointly guiding generation by the outline and the specific mapped knowledge cluster ensures high document coverage and traceability
- Mechanism: Each outline section is explicitly mapped to a specific cluster. During generation, the model retrieves and re-ranks documents only from that specific cluster, anchoring the text generation
- Core assumption: The outline-to-cluster mapping is unambiguous; if a section concept falls between two clusters, the retrieval guidance may be incomplete
- Evidence anchors:
  - [section 3.4] "Outline-Knowledge Cluster Joint-Driven Section-Wise Retrieval"
  - [table 1] Achieves 80.14% (14B) and 70.51% (32B) Document Coverage
  - [corpus] "MegaRAG" and related RAG literature emphasize retrieval grounding

## Foundational Learning

- **Concept: K-Means Clustering with Silhouette Coefficient**
  - Why needed here: Used to automatically determine the optimal number of article sections based on data distribution rather than arbitrary assignment
  - Quick check question: How does the Silhouette coefficient prevent the model from creating too many or too few chapters?

- **Concept: Tree-Structured Hierarchical Summarization (RAPTOR)**
  - Why needed here: Compresses massive retrieved document sets into "cluster summaries" that fit within the LLM's context window while preserving core themes
  - Quick check question: Why is summarizing documents into cluster summaries better than just concatenating raw text for the outline generation step?

- **Concept: Retrieval-Augmented Generation (RAG)**
  - Why needed here: The fundamental paradigm ConvergeWriter modifies; understanding standard RAG clarifies why this paper's "retrieve-cluster-then-generate" is a structural innovation
  - Quick check question: In standard RAG, when does retrieval happen relative to generation? How does ConvergeWriter change this timing?

## Architecture Onboarding

- **Component map:** Retriever (Iterative breadth & depth search) -> Filter (LLM-based relevance check) -> Structurer (Embedding + K-Means Clustering) -> Compressor (Tree-based Summarizer) -> Planner (Outline Generator) -> Writer (Section Generator with Re-ranking) -> Polisher (Global consistency pass)

- **Critical path:** The **Retrieval â†’ Clustering** phase. If the clustering fails to create coherent semantic groups, the resulting outline will be structurally flawed, and the subsequent "jointly guided" generation will be anchored to the wrong documents

- **Design tradeoffs:**
  - **Latency vs. Coverage:** The "exhaustive iterative retrieval" maximizes coverage but introduces significant latency compared to single-shot retrieval
  - **Data-Driven vs. Creativity:** Strict adherence to cluster content reduces hallucination but may limit the LLM's ability to synthesize broad concepts that span across clusters

- **Failure signatures:**
  - **Outline Drift:** If the LLM ignores the cluster summaries during planning, sections may lack source evidence
  - **Cluster Fragmentation:** If k is too high, the outline becomes a list of minor details rather than structured chapters
  - **Retrieval Silos:** If depth expansion fails, the knowledge base may lack specific details, resulting in generic summaries

- **First 3 experiments:**
  1. **Retrieval Ablation:** Run the pipeline with only "Breadth-First" retrieval (disable depth expansion) to verify if document coverage drops significantly
  2. **Clustering Ablation:** Replace K-Means/Silhouette clustering with random document shuffling to measure the drop in "Novelty" and "Breadth" scores
  3. **Cluster-to-Section Mapping:** manually inspect if the generated outline sections map 1:1 to the cluster topics to verify the "strict adherence" claim

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does ConvergeWriter perform when deployed on heterogeneous or noisy knowledge bases compared to the clean, structured Wikipedia data used in the experiments?
- Basis in paper: [Explicit] The Introduction claims applicability to "high-stakes, knowledge-intensive domains" like finance and healthcare, but [Inferred] the experimental evaluation is restricted to the "WildSeek" dataset derived solely from Wikipedia
- Why unresolved: The "Retrieval-First" strategy assumes retrieved documents are largely relevant and factual. Noisy data could lead to incoherent clusters or propagate errors, a risk not assessed in the current results
- What evidence would resolve it: Benchmarking results on datasets containing conflicting information, unstructured raw text, or domain-specific noise

### Open Question 2
- Question: What is the computational latency and resource overhead of the exhaustive retrieval and clustering phase compared to top-down planning methods?
- Basis in paper: [Inferred] The Methodology section describes an "exhaustive iterative retrieval" process followed by embedding and K-means clustering for every topic, which implies a significantly higher pre-generation cost than single-pass retrieval
- Why unresolved: While the paper demonstrates superior quality, it does not report the wall-clock time or computational cost of the data-driven structuring phase
- What evidence would resolve it: A comparative efficiency analysis measuring latency and FLOPS against baselines like STORM or OmniThink

### Open Question 3
- Question: To what extent does the reliance on K-means clustering constrain the discovery of hierarchical or overlapping thematic structures in the generated outlines?
- Basis in paper: [Inferred] The method utilizes K-means with Silhouette Coefficient optimization, an algorithm that typically partitions data into spherical, non-overlapping clusters, potentially failing to capture complex nested topics
- Why unresolved: The paper asserts that clustering reveals "intrinsic structure," but K-means forces a flat partitioning which might not reflect the true hierarchical nature of complex subjects
- What evidence would resolve it: An ablation study comparing K-means against hierarchical or density-based clustering algorithms on topics with known nested sub-topics

## Limitations
- The approach relies heavily on the quality and comprehensiveness of initial retrieval; failure to capture key concepts compromises the entire knowledge foundation
- Strict adherence to cluster content may produce overly rigid structures that miss cross-cluster insights that would enhance article coherence
- The method requires significant computational resources for the exhaustive retrieval and clustering phases compared to simpler top-down approaches

## Confidence
- **High Confidence**: Document Coverage metric results and the general inversion of the generation pipeline are well-supported by experimental results and ablation studies
- **Medium Confidence**: Claims about novelty and breadth improvements from clustering are supported by ablation studies but could be influenced by specific implementation details
- **Low Confidence**: The assertion that this approach universally reduces hallucination across all long-form generation tasks lacks direct comparative evidence beyond the Wikipedia domain tested

## Next Checks
1. **Cross-Domain Generalization**: Test ConvergeWriter on a non-Wikipedia knowledge base (e.g., news articles or scientific papers) to verify if the retrieval-first approach maintains its effectiveness when the knowledge structure differs from encyclopedic content

2. **Cluster Quality Analysis**: Conduct a qualitative assessment of whether the automatically determined clusters actually represent coherent semantic themes that would naturally map to article sections, rather than arbitrary document groupings

3. **Hallucination Quantification**: Implement a fine-grained factuality analysis comparing ConvergeWriter outputs against both retrieved source documents and ground truth articles to measure the actual reduction in hallucinated content versus standard RAG approaches