---
ver: rpa2
title: The Primacy of Magnitude in Low-Rank Adaptation
arxiv_id: '2507.06558'
source_url: https://arxiv.org/abs/2507.06558
tags:
- lora
- initialization
- magnitude
- loram
- should
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the training dynamics of LoRA from the perspective
  of update magnitude. It proves that the low-rank structure inherently limits update
  magnitudes compared to full fine-tuning, and shows that spectral initialization
  methods work by amplifying these magnitudes.
---

# The Primacy of Magnitude in Low-Rank Adaptation

## Quick Facts
- **arXiv ID:** 2507.06558
- **Source URL:** https://arxiv.org/abs/2507.06558
- **Reference count:** 40
- **Primary result:** Update magnitude, rather than specific initialization method, is the primary driver of LoRA performance

## Executive Summary
This paper analyzes LoRA training dynamics through the lens of update magnitude, proving that low-rank structures inherently limit update magnitudes compared to full fine-tuning. The authors demonstrate that spectral initialization methods like PiSSA work by amplifying these magnitudes rather than preserving knowledge through singular vectors. Based on this insight, they propose LoRAM, a new initialization scheme using orthogonal bases scaled by pretrained weight magnitudes to achieve similar magnitude amplification without expensive SVD computation. Experiments across language and vision-language tasks show LoRAM matches or exceeds spectral initialization performance while maintaining LoRA's efficiency.

## Method Summary
LoRAM initializes adapter matrices using a Discrete Sine Transform (DST) basis scaled by a gain factor β. The gain factor is computed using a logarithmic approximation of spectral gain: β = (log(r)·ν[W]/ν[Φ])^{1/4}, where ν[W] is the Frobenius norm of pretrained weights and ν[Φ] is the norm of the DST basis. The initialization ensures zero initial update by modifying the frozen weights as W ← W - β²·Φ_n·Φ_m^T. This approach simulates the magnitude amplification of spectral initialization without requiring SVD computation, achieving O(mr+nr) complexity versus O(m²n) for SVD.

## Key Results
- LoRAM achieves comparable or superior performance to spectral initialization methods (PiSSA) across language and vision-language tasks
- Initialization choice has minimal impact on performance compared to magnitude scaling
- LoRAM initialization is 100-1000× faster than SVD-based methods while maintaining competitive task accuracy
- Magnitude amplification is the primary driver of LoRA performance rather than knowledge preservation

## Why This Works (Mechanism)

### Mechanism 1: The Magnitude Bottleneck in Low-Rank Adaptation
Standard LoRA converges slower than full fine-tuning because low-rank structure limits update magnitudes. The update magnitude grows at rate proportional to γt but scaled by small coefficient k₁ ≈ r/m. Since m is large in LLMs, weight magnitude evolves significantly slower than full fine-tuning, creating a "magnitude deficit." This assumes independent, zero-mean entries and small learning rates.

### Mechanism 2: Spectral Initialization as Magnitude Amplification
Spectral initialization amplifies update magnitude via singular value scaling rather than preserving knowledge through singular vectors. Methods like PiSSA inject a "Spectral Gain Factor" Q[r] derived from singular values into update dynamics, effectively pre-conditioning matrices A and B with higher variance. This accelerates magnitude growth rate to k₁ = Q[r](m+n)ν[W].

### Mechanism 3: LoRAM's Efficient Magnitude Simulation
LoRAM efficiently approximates spectral gain using logarithmic function of rank and deterministic orthogonal bases. It initializes matrices using DST basis scaled by factor β ≈ (log(r)·ν[W])^{1/4}, mimicking statistical properties of spectral gain without computing singular values. This assumes spectral gain's concave shape can be approximated by log_min(n,m)(r).

## Foundational Learning

- **Concept: Frobenius Norm as Magnitude (ν[W])**
  - Why needed here: Reframes LoRA tuning as dynamical system controlling "energy" of weight update. Essential for understanding "Magnitude Principle."
  - Quick check question: If you double all entries in matrix W, what happens to ν[W]? (Answer: It quadruples, since ν is proportional to squared norm)

- **Concept: Low-Rank Matrix Factorization (W = BA)**
  - Why needed here: Exploits relationship ν[BA] ≈ r·ν[B]·ν[A]. Understanding how rank r scales variance of product is essential for grasping why LoRA updates are smaller than full-rank updates.
  - Quick check question: If B ∈ ℝ^{n×r} and A ∈ ℝ^{r×m} with r ≪ m, how does increasing r affect "degrees of freedom" for magnitude growth?

- **Concept: Discrete Sine Transform (DST)**
  - Why needed here: LoRAM uses DST as "free" deterministic orthogonal basis. Unlike random noise, DST is fixed and reproducible, serving as cheap substitute for singular vectors of SVD.
  - Quick check question: Why is deterministic basis preferred over random Gaussian basis? (Hint: Think about reproducibility and potentially reducing variance in initialization step)

## Architecture Onboarding

- **Component map:** Pretrained weights W → get_dst_basis(n,r) → β computation → A^(0) = β·Φ_m^T, B^(0) = β·Φ_n → LoRA training

- **Critical path:**
  1. Retrieve Frobenius norm (magnitude) of pretrained weight matrix W
  2. Compute gain factor β using logarithmic approximation of spectral gain
  3. Construct orthogonal basis matrices Φ_n, Φ_m using DST (no SVD)
  4. Initialize adapter matrices A, B by scaling basis with β

- **Design tradeoffs:**
  - SVD vs. DST: DST is O(mr+nr) and fast, whereas SVD is O(m²n). Tradeoff is slight loss of "optimal" alignment (≈0.5-2% performance difference) for massive initialization speedup
  - Logarithmic vs. Exact Gain: Using log(r) is heuristic approximation of spectral gain factor Q[r]. Works well for standard ranks but is approximation of "true" spectral shape

- **Failure signatures:**
  - High-Rank Instability: Combining LoRAM with high ranks and high learning rates can lead to divergence or performance collapse
  - Magnitude Mismatch: Incorrect ν[W] calculation leads to off β, causing slow convergence (too small β) or instability (too large β)

- **First 3 experiments:**
  1. Magnitude Validation: Train small MLP on synthetic data, plot ν[W_LoRA] over time for different methods to verify magnitude growth curves
  2. Efficiency Benchmark: Measure initialization time for LLaMA-7B, compare LoRAM vs PiSSA specifically on CPU
  3. Ablation on Basis: Swap DST basis for Random Orthogonal basis in LoRAM, run on GLUE/MMLU to confirm basis choice has limited impact

## Open Questions the Paper Calls Out

- **Optimal magnitude scaling strategy:** Determining optimal magnitude bounds relative to model depth or dataset size remains challenging. LoRAM uses logarithmic approximation to match spectral gains heuristically rather than deriving mathematically optimal magnitude target for specific tasks.

- **Layer-wise tailored magnitude settings:** Different layers may benefit from tailored magnitude settings, motivating joint optimization with learning rate and rank. Current approach applies global gain factor across layers, potentially overlooking layer sensitivity variations.

- **Formal influence on optimization dynamics:** Work does not explicitly address optimization dynamics and convergence properties. While magnitude is established as performance driver, formal link between specific update magnitudes and non-convex loss landscape convergence remains unproven.

## Limitations
- Theoretical analysis assumes linear approximation that may break down at high learning rates or very large ranks
- Logarithmic approximation of spectral gain lacks rigorous bounds on approximation error across diverse architectures
- Claim that "magnitude is primary driver" may not fully account for task-specific feature alignment requirements

## Confidence

- **High confidence:** Empirical observation that LoRAM achieves competitive performance with spectral methods while being computationally efficient
- **Medium confidence:** Theoretical framework connecting update magnitude to LoRA performance (dependent on assumptions about gradient flow)
- **Medium confidence:** Claim that initialization choice has minimal impact compared to magnitude scaling (supported by ablation but limited task diversity)

## Next Checks

1. **High-LR Stress Test:** Evaluate LoRAM performance with learning rates 10× higher than standard (2×10⁻⁴) to verify whether magnitude-driven theory breaks down under aggressive optimization conditions

2. **Rank Scaling Study:** Systematically vary rank from 2 to 512 across multiple tasks to map boundary where logarithmic approximation becomes inaccurate and identify rank thresholds for optimal performance

3. **Task-Specific Feature Alignment:** Design experiments on tasks requiring precise feature preservation (e.g., style transfer in VLMs) to test whether magnitude-driven initialization sacrifices task-specific alignment compared to exact spectral methods