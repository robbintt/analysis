---
ver: rpa2
title: 'BikeScenes: Online LiDAR Semantic Segmentation for Bicycles'
arxiv_id: '2510.25901'
source_url: https://arxiv.org/abs/2510.25901
tags:
- bikescenes
- lidar
- segmentation
- dataset
- classes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work introduces BikeScenes-lidarseg, a novel dataset of 3021\
  \ consecutive LiDAR scans captured from a bicycle perspective around TU Delft\u2019\
  s campus, annotated for 29 classes. The dataset addresses the domain gap between\
  \ automotive LiDAR segmentation data and the unique viewpoint and sensor characteristics\
  \ of bicycle-mounted platforms."
---

# BikeScenes: Online LiDAR Semantic Segmentation for Bicycles

## Quick Facts
- arXiv ID: 2510.25901
- Source URL: https://arxiv.org/abs/2510.25901
- Reference count: 22
- Primary result: Fine-tuning FRNet on bicycle-perspective LiDAR data improves mIoU from 13.8% to 63.6% for semantic segmentation

## Executive Summary
This work introduces BikeScenes-lidarseg, a novel dataset of 3021 consecutive LiDAR scans captured from a bicycle perspective around TU Delft’s campus, annotated for 29 classes. The dataset addresses the domain gap between automotive LiDAR segmentation data and the unique viewpoint and sensor characteristics of bicycle-mounted platforms. Experiments with the FRNet architecture show that fine-tuning on BikeScenes significantly improves mean Intersection-over-Union (mIoU) from 13.8% (SemanticKITTI pre-training only) to 63.6%, demonstrating the necessity of domain-specific training. The best model achieves strong performance on static classes like road (87.2%) and building (91.6%), as well as dynamic classes such as person (83.2%) and bicyclist (87.5%). The model was deployed in a ROS 2 pipeline on the SenseBike platform, achieving real-time inference speeds of 1.31 FPS using mixed precision on an NVIDIA Jetson Orin NX. BikeScenes-lidarseg is released to support further research in cyclist-centric perception.

## Method Summary
The authors collect 3021 consecutive LiDAR scans from a RoboSense M1 Plus sensor mounted on a bicycle platform called SenseBike, recording around TU Delft campus at 10Hz. The dataset is annotated with 29 classes and split into training (subsequences 0,4,8), validation (subsequences 1,3,5,7), and test (subsequences 2,6) sets. They use the FRNet architecture, fine-tuning from SemanticKITTI pretrained weights with a learning rate of 3×10⁻⁴ for 30k iterations using a One-Cycle policy. The model is evaluated on 19 remapped classes (merging bike-path into road) and deployed on Jetson Orin NX with PyTorch AMP, achieving 1.31 FPS.

## Key Results
- mIoU improves from 13.8% (SemanticKITTI pre-training only) to 63.6% (fine-tuned on BikeScenes)
- Per-class IoU: road (87.2%), building (91.6%), person (83.2%), bicyclist (87.5%)
- Inference speed: 1.31 FPS on Jetson Orin NX with mixed precision
- Fine-tuning outperforms training from scratch (58.6% vs 51.5% validation mIoU)

## Why This Works (Mechanism)

### Mechanism 1: Domain Alignment via Fine-Tuning
Fine-tuning on bicycle-perspective data bridges the geometric and sensor domain gap from automotive datasets. Automotive datasets feature sensor perspectives from car rooftops (approx. 1.8m height) with specific scan patterns, while the SenseBike platform uses a lower-mounted sensor. Fine-tuning adapts the model's feature extractors to unique point density, occlusion patterns, and "ghost points" prevalent in bicycle-level LiDAR scans. The pre-trained weights contain transferable geometric features (ground planes, vertical structures) that are robust to the change in sensor resolution but require adjustment for the novel viewing angle.

### Mechanism 2: Hardware-Agnostic Efficiency Selection
Selecting architectures based on "normalized FPS" (FPS per TFLOPS) rather than raw FPS helps identify models that scale effectively to embedded hardware. Author-reported FPS is often measured on high-end desktop GPUs. By dividing FPS by the GPU's peak FP32 performance, the authors estimate the algorithm's computational efficiency relative to its accuracy. This metric predicted FRNet would maintain a viable speed on the lower-power Jetson Orin NX better than higher-performing but heavier Transformers.

### Mechanism 3: Precision-Compute Trade-off (AMP)
Automatic Mixed Precision (AMP) provides a significant speedup on edge devices with minimal accuracy degradation for this segmentation task. The NVIDIA Jetson Orin NX has Tensor Cores optimized for FP16/BF16 operations. By casting suitable operations to half-precision while keeping master weights or sensitive accumulations in FP32, the pipeline processes the 3D frustums faster without diverging the model's loss landscape.

## Foundational Learning

- **Concept: LiDAR Semantic Segmentation (Range View vs. Voxel)**
  - **Why needed here:** The paper uses FRNet, which relies on a "Frustum-Range" representation. Understanding that 3D points are projected onto a 2D range image (cylindrical projection) to enable efficient 2D convolutions is key to understanding how they achieve real-time speed on a Jetson.
  - **Quick check question:** Does the model process raw points directly, or does it project them into a 2D image-like format first?

- **Concept: Domain Gap (Transfer Learning)**
  - **Why needed here:** The central result (13.8% vs 63.6%) is driven by the domain gap. You must understand that a model trained on cars "sees" the world differently (mostly looking down on traffic) compared to a bike (looking up at traffic, different occlusion).
  - **Quick check question:** Why would a model trained on car-roof LiDAR struggle to identify a "bicyclist" from a bike-mounted LiDAR?

- **Concept: Label Remapping**
  - **Why needed here:** The authors map their 29 custom classes (including "bike-path") back to the standard 19 SemanticKITTI evaluation classes to measure performance.
  - **Quick check question:** Why is it necessary to merge the "bike-path" class into "road" before evaluating the model against the automotive baseline?

## Architecture Onboarding

- **Component map:**
  RoboSense M1 Plus LiDAR (3D Points) + IMU/GPS (for deskewing) -> Motion compensation (deskewing) -> Range Projection (Point to Frustum) -> FRNet (Backbone + Segmentation Head) -> Segmented Point Cloud (29 classes)

- **Critical path:**
  1. Data Collection: Ensure the 1.3km loop captures diverse dynamic objects (persons, bicyclists) as static objects are easier.
  2. Fine-Tuning: Run Schedule Cfg. 2d (Fine-Tune, LR=3e-4, 30k iters).
  3. Deployment: Enable AMP on the Jetson to cross the 1.0 FPS threshold.

- **Design tradeoffs:**
  - FRNet vs. Point Transformer V3: The paper selects FRNet (63.6% mIoU) over Point Transformer V3 (likely higher mIoU) because PTv3 has a "normalized FPS" of 0.27 vs FRNet's 2.05. They sacrifice ~2-5% accuracy for a 7x efficiency gain.
  - Scratch vs. Fine-tune: Fine-tuning is preferred (58.6% val) over scratch (51.5% val) on the small dataset (3021 scans) to prevent overfitting and leverage general geometric priors.

- **Failure signatures:**
  - Ghost Points: The model incorrectly classifies "ghost points" (mirrored reflections from glass facades) as `building` or `vegetation`.
  - Ground Confusion: Fine-grained distinction between `parking`, `sidewalk`, and `road` is poor (e.g., Parking IoU is 3.4%) due to LiDAR-only ambiguity.
  - Side-view Cyclists: Cyclists viewed from the side are split into `bicyclist` + `bicycle` classes instead of the unified `bicyclist` label.

- **First 3 experiments:**
  1. Baseline Validation: Run the SemanticKITTI pre-trained weights directly on BikeScenes test set to confirm the domain gap (Expectation: mIoU < 15%).
  2. Learning Rate Ablation: Compare fine-tuning with LR=0.01 vs. LR=3x10^-4 to verify that lower LRs prevent catastrophic forgetting on the small dataset (Expectation: Lower LR yields ~3-5% higher mIoU).
  3. Latency Profiling: Deploy the model on Jetson Orin NX with and without AMP to measure the speedup factor (Expectation: ~1.3x speedup).

## Open Questions the Paper Calls Out

### Open Question 1
Can model optimization techniques (e.g., quantization, pruning, or specialized accelerators) enable the FRNet architecture to achieve true real-time inference speeds (>10 Hz) on the Jetson Orin NX?
- Basis in paper: The authors note that the current deployment achieves only 1.31 FPS using mixed precision, stating, "While this does not reach full real-time performance yet, the improvement from AMP makes current on-bike deployment more practical."
- Why unresolved: The current computational budget on the embedded hardware limits the throughput, preventing the system from processing every LiDAR scan (recorded at 10Hz) in real-time.
- What evidence would resolve it: A demonstration of the system processing input scans at ≥10 FPS on the target hardware without significant degradation in mIoU.

### Open Question 2
How can the segmentation accuracy for rare and structurally ambiguous classes, specifically "parking" and "bicycle," be improved given the bicycle viewpoint?
- Basis in paper: The results section highlights that "The weakest classes remain bicycle (26.9%), motorcycle (32.4%), and parking (3.4%)," attributing this to the classes being rare and "often ambiguous in LiDAR-only views."
- Why unresolved: The current data volume and LiDAR-only approach struggle to distinguish these classes from similar ones (e.g., "parking" vs. "road" or "sidewalk").
- What evidence would resolve it: An improved model configuration or training strategy that significantly increases the IoU for these specific underperforming classes on the BikeScenes test set.

### Open Question 3
Does the integration of multi-modal sensor data (specifically the available camera imagery) improve segmentation performance over the LiDAR-only baseline?
- Basis in paper: The authors describe the SenseBike as a "multi-sensor" platform equipped with cameras, but the methodology notes these were only used "to aid in the labeling procedure," leaving their utility for the perception model unexplored.
- Why unresolved: The paper evaluates a strictly LiDAR-based approach (FRNet), leaving the potential benefits of sensor fusion for the bicycle domain unquantified.
- What evidence would resolve it: A comparative study showing mIoU performance of a multi-modal fusion model versus the LiDAR-only baseline on the BikeScenes dataset.

## Limitations
- The RoboSense M1 Plus's specific range, resolution, and noise characteristics relative to the SemanticKITTI sensor are not fully characterized, so the exact source of the domain gap is inferred, not measured.
- The "ghost points" error mode is described but not quantified, making it difficult to gauge severity.
- The study focuses on FRNet; while other architectures are compared on paper, no ablation is done on the bike dataset itself to prove FRNet is the optimal choice.

## Confidence
- Domain gap and fine-tuning efficacy: High confidence (large mIoU jump 13.8% → 63.6% and ablation on training from scratch)
- Efficiency selection metric: Medium confidence (assumes GPU compute scaling is dominant factor)
- AMP benefit: High confidence (measured on-device with no mIoU loss)

## Next Checks
1. Measure the statistical difference in point density, height distribution, and scan pattern between BikeScenes and SemanticKITTI data to directly quantify the domain gap.
2. Run the deployed model for 10+ minutes on the Jetson Orin NX to monitor for any FP16-related numerical errors or accuracy drift in a live stream.
3. Fine-tune a Point Transformer V3 on BikeScenes to verify if the ~2-5% mIoU loss predicted by the normalized FPS metric is accurate in practice.