---
ver: rpa2
title: Multilingual Knowledge Graph Completion via Efficient Multilingual Knowledge
  Sharing
arxiv_id: '2510.07736'
source_url: https://arxiv.org/abs/2510.07736
tags:
- knowledge
- entity
- hits
- mkgc
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a multilingual knowledge graph completion
  (MKGC) framework that leverages large language models' multilingual capabilities.
  The method uses Knowledge-level Grouped Mixture of Experts (KL-GMoE) to efficiently
  model shared knowledge across languages and Iterative Entity Reranking (IER) to
  improve entity ranking.
---

# Multilingual Knowledge Graph Completion via Efficient Multilingual Knowledge Sharing

## Quick Facts
- **arXiv ID:** 2510.07736
- **Source URL:** https://arxiv.org/abs/2510.07736
- **Reference count:** 35
- **Primary result:** 5.47%, 3.27%, and 1.01% gains in Hits@1, Hits@3, and Hits@10 metrics respectively over state-of-the-art MKGC methods

## Executive Summary
This paper introduces a multilingual knowledge graph completion (MKGC) framework that leverages large language models' multilingual capabilities through a novel Knowledge-level Grouped Mixture of Experts (KL-GMoE) architecture. The framework addresses the challenge of cross-lingual knowledge sharing in knowledge graphs by routing semantically related queries across languages to specialized expert groups, while maintaining efficient parameter usage. An Iterative Entity Reranking (IER) method refines the ranking of candidate entities beyond simple generation. Experiments on a 5-language dataset show significant improvements over state-of-the-art MKGC methods, with the framework demonstrating robustness to language imbalance and strong zero-shot performance on unseen languages.

## Method Summary
The framework processes MKGC queries by first retrieving top-$m$ candidate entities using a traditional KGE model (TransE/RotatE), then employing a multilingual LLM (Llama-2-7b) with KL-GMoE architecture for reranking. The KL-GMoE uses grouped LoRA experts with specialized routing mechanisms to capture shared knowledge across languages without fragmentation. During inference, IER performs $N_t$ rounds of iterative ranking refinement by removing predicted entities and reinserting them at their determined rank. The model is fine-tuned with LoRA rank 4 and learning rate 2e-5, using variable candidate list lengths during training.

## Key Results
- 5.47% improvement in Hits@1 metric compared to state-of-the-art MKGC methods
- 3.27% improvement in Hits@3 metric on multilingual test set
- 1.01% improvement in Hits@10 metric while maintaining strong performance on unseen languages
- Robust performance under language imbalance conditions
- Successful zero-shot transfer to Italian and Japanese languages

## Why This Works (Mechanism)

### Mechanism 1: Grouped Expert Routing for Semantic Atomicity
The KL-GMoE architecture mitigates knowledge fragmentation by routing complete triplet information to specialized expert groups rather than splitting components across channels. The routing decision is based on the aggregated representation of the input query $(h, r, t)$, ensuring semantically related cross-lingual knowledge is processed by the same expert parameters. This maintains the atomicity of knowledge facts while allowing parameter specialization. The core assumption is that routing weights can successfully cluster semantically equivalent queries from different languages into the same expert groups.

### Mechanism 2: Iterative Refinement for Ranking Alignment
IER aligns LLM generative pre-training with KGC ranking requirements by treating ranking as a sequential decision process. The model performs $N_t$ rounds of inference, extracting the highest-probability entity from candidates, removing it from the input pool, and inserting it at rank $t$ in the output list. This forces the model to look beyond the top-1 candidate and refine the ordering of lower-ranked entities. The assumption is that the LLM's probability distribution generalizes well enough to distinguish relative correctness among "second-best" candidates.

### Mechanism 3: Cross-Lingual Generalization via Shared Parameter Updates
Fine-tuning shared experts on multilingual data enables zero-shot generalization to unseen languages. By training experts on queries from multiple languages, the model updates parameters that capture underlying relation logic independent of language surface form. When an unseen language is queried, the router directs it to the most relevant "relation-expert" learned from other languages. The assumption is that relations in the KG are language-agnostic and the base LLM possesses sufficient multilingual embeddings.

## Foundational Learning

- **Concept: Mixture of Experts (MoE) & Sparse Activation**
  - **Why needed:** KL-GMoE is a variant of MoE; understanding router selection of parameter subsets is essential for grasping how the paper separates knowledge categories
  - **Quick check:** How does a router decide which expert to activate, and what is the computational benefit compared to a dense layer?

- **Concept: Knowledge Graph Embedding (KGE)**
  - **Why needed:** The framework relies on KGE models (TransE/RotatE) to generate initial candidate lists before LLM processing
  - **Quick check:** If the KGE model fails to rank the correct entity in the top-$m$, can LLM-based reranking recover it? (Answer: generally No)

- **Concept: Parameter-Efficient Fine-Tuning (PEFT) / LoRA**
  - **Why needed:** The experts are essentially LoRA matrices added to the FFN; understanding low-rank adaptation is key to implementing trainable components
  - **Quick check:** In LoRA, how are low-rank matrices $A$ and $B$ combined with frozen pre-trained weights $W_0$?

## Architecture Onboarding

- **Component map:** Query $(h, r, ?)$ + Entity Descriptions + Neighbor Facts -> Candidate Generator (KGE) -> LLM Backbone with KL-GMoE -> IER Loop (Iterative top-probability extraction and list re-insertion)

- **Critical path:**
  1. Prompt Construction: Embed query and $m$ candidates into context window
  2. Forward Pass: KL-GMoE routes input through specific expert groups to compute logits
  3. IER Inference: Run forward pass, pick top entity, remove from prompt, repeat $N_t$ times

- **Design tradeoffs:**
  - Router Complexity vs. Granularity: 3 routers used; simplifying might reduce precision while over-complicating might cause instability
  - IER Rounds ($N_t$): Higher rounds improve ranking metrics but increase latency linearly; paper uses 10
  - Candidate Size ($m$): Larger $m$ increases context length but provides higher recall potential

- **Failure signatures:**
  - Top-1 Bias: KGE baseline has very high top-1 accuracy during training, causing LLM to learn blind copying
  - Router Collapse: Metrics stall with >90% of inputs routed to single expert group
  - Language Silos: Performance on unseen languages drops to near-zero, indicating experts learned language-specific features

- **First 3 experiments:**
  1. Ablation on Routing: Replace KL-GMoE with standard LoRA to quantify grouped expert advantage
  2. Unseen Language Zero-Shot: Train only on English/French, test on Chinese/Japanese to validate shared knowledge transfer
  3. IER Step Analysis: Plot metric performance at each of 10 iterations to see when ranking stabilizes

## Open Questions the Paper Calls Out

1. **KGE Dependency Elimination:** Can the framework be redesigned to eliminate dependency on traditional KGE models for candidate generation, thereby mitigating "top-1 bias"? The authors state plans to investigate more stable fine-tuning instruction sets that don't rely on KGE models.

2. **Full KG Selection Capability:** How can the framework overcome context length limitations to allow entity selection based on the entire Knowledge Graph rather than restricted candidate subsets? The current approach is limited by LLM token length.

3. **Multimodal Extension:** Can KL-GMoE architecture be extended to process multimodal information (images, audio) alongside text for Knowledge Graph Completion? The current framework processes text exclusively, limiting application to multimodal KG datasets.

## Limitations

- **KGE Dependency:** Framework relies on traditional KGE models (TransE/RotatE) for candidate generation, creating a ceiling on LLM performance based on KGE recall
- **Context Window Constraints:** Unable to perform entity selection based on all entities in the KG due to LLM token length limitations
- **Text-Only Processing:** Framework processes text information exclusively, impeding application to multimodal KG datasets

## Confidence

- **High Confidence:** IER mechanism effectiveness is well-supported by ablation study and metric progression plots
- **Medium Confidence:** Cross-lingual generalization claims are supported by empirical results but rely on architectural assumptions about semantic routing not fully validated
- **Low Confidence:** Specific claims about "knowledge fragmentation" being solved and exact mechanisms of semantic atomicity preservation are based on architectural descriptions rather than direct experimental validation

## Next Checks

1. **Router Load Analysis:** Monitor and report distribution of routing weights during training to verify expert utilization and detect potential routing collapse

2. **Zero-Shot Language Transfer Validation:** Systematically test model on truly unseen languages with varying degrees of linguistic similarity to training languages to validate cross-lingual generalization

3. **Candidate Recall Impact Study:** Quantify relationship between KGE model recall (Hits@30) and LLM reranking performance by varying candidate generator quality and measuring resulting ceiling on LLM performance