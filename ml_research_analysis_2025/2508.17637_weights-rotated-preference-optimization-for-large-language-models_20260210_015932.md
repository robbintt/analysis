---
ver: rpa2
title: Weights-Rotated Preference Optimization for Large Language Models
arxiv_id: '2508.17637'
source_url: https://arxiv.org/abs/2508.17637
tags:
- ropo
- matrix
- wang
- preference
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses reward hacking in Direct Preference Optimization
  (DPO), where language models overly suppress rejected completions to maximize rewards,
  leading to verbose generations and knowledge forgetting. The authors propose Weights-Rotated
  Preference Optimization (RoPO), which combines implicit KL divergence regularization
  on output logits with explicit orthogonal regularization on intermediate hidden
  states using a multi-granularity rotation matrix.
---

# Weights-Rotated Preference Optimization for Large Language Models

## Quick Facts
- arXiv ID: 2508.17637
- Source URL: https://arxiv.org/abs/2508.17637
- Authors: Chenxu Yang; Ruipeng Jia; Mingyu Zheng; Naibin Gu; Zheng Lin; Siyuan Chen; Weichong Yin; Hua Wu; Weiping Wang
- Reference count: 40
- Key outcome: RoPO achieves up to 3.27-point improvement on AlpacaEval 2 and surpasses best baseline by 6.2-7.5 points on MT-Bench while using only 0.015% of trainable parameters, effectively mitigating reward hacking while maintaining strong alignment performance.

## Executive Summary
This paper addresses reward hacking in Direct Preference Optimization (DPO), where language models overly suppress rejected completions to maximize rewards, leading to verbose generations and knowledge forgetting. The authors propose Weights-Rotated Preference Optimization (RoPO), which combines implicit KL divergence regularization on output logits with explicit orthogonal regularization on intermediate hidden states using a multi-granularity rotation matrix. This design preserves angle-encoded knowledge while preventing excessive deviation from the reference model. Experiments show RoPO achieves significant performance gains while using minimal trainable parameters, effectively mitigating reward hacking while maintaining strong alignment performance.

## Method Summary
RoPO decomposes model weights into magnitude and direction components, freezing the normalized directional weights while making them trainable through orthogonal rotation. The rotation matrix is constructed from Householder reflections (global rotation) and Givens rotations (fine-grained 2D plane rotations), preserving relative angular distances between neurons. This multi-granularity approach maintains neuron distribution integrity while allowing preference learning through magnitude scaling. The method applies to query and value vectors in attention layers, with only magnitude vectors and rotation parameters being trainable (approximately 0.015% of total parameters).

## Key Results
- Achieves 3.27-point improvement on AlpacaEval 2 compared to best baseline
- Surpasses best baseline by 6.2-7.5 points on MT-Bench
- Uses only 0.015% of trainable parameters while maintaining strong performance
- Effectively mitigates reward hacking behaviors like verbosity and repetition
- Preserves knowledge retention across ID and OOD benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reward hacking in DPO correlates with neuron collapse in parameter space, which increases representation redundancy.
- Mechanism: DPO optimization causes neurons to collapse toward similar directions, increasing Hyperspherical Energy (HE). This disrupts the isotropic distribution that supports expressive capability. The model compensates by generating verbose, repetitive outputs to achieve high rewards without genuinely learning preference alignment.
- Core assumption: Knowledge is partially encoded in relative angles between neurons; preserving these angles retains pre-training/SFT knowledge.
- Evidence anchors: [Section 4.1] shows DPO causes neuron collapse and increases HE, while SFT reduces HE. Limited direct corpus support; related work on orthogonal fine-tuning supports angle preservation for knowledge retention.
- Break condition: If neuron arrangement is not the primary knowledge storage mechanism, or if collapse is symptom rather than cause of reward hacking.

### Mechanism 2
- Claim: Orthogonal rotation of weight directions preserves angle-encoded knowledge while allowing preference learning through magnitude scaling.
- Mechanism: RoPO decomposes weights into magnitude (trainable) and direction (frozen, then rotated via orthogonal matrix R). The multi-granularity rotation—combining global Householder reflections and fine-grained Givens rotations—maintains relative angular distances between neurons during updates. This explicitly constrains intermediate hidden states from deviating too far from the reference model.
- Core assumption: Orthogonal transformations preserve semantic structure encoded in neuron angles; magnitude adjustments are sufficient for preference adaptation.
- Evidence anchors: [Section 4.2] shows t-SNE visualization preserving neuron distribution integrity. Related work on OFT supports orthogonal fine-tuning for knowledge preservation.
- Break condition: If preference learning requires directional weight changes that orthogonal constraints prevent.

### Mechanism 3
- Claim: Dual constraints (implicit KL on logits + explicit orthogonal regularization on hidden states) provide complementary regularization that prevents excessive rejected-completion suppression.
- Mechanism: DPO's KL divergence implicitly constrains output distributions but operates only on final logits. RoPO adds explicit parameter-space constraints on intermediate layers. Together, these prevent the policy model from achieving high rewards primarily through suppressing rejected completions, forcing it to learn genuine preference features from chosen completions.
- Core assumption: Reward hacking manifests at multiple network levels; constraining only output logits is insufficient.
- Evidence anchors: [Section 4] shows dual constraints prevent deviation from reference model. Figure 6 demonstrates RoPO reduces rejected-suppression asymmetry while maintaining reward accuracy.
- Break condition: If single-level constraint is sufficient with proper loss formulation; if dual constraints over-regularize and limit alignment quality.

## Foundational Learning

- **Direct Preference Optimization (DPO)**: RoPO builds on DPO's framework; understanding DPO's objective (maximizing relative probability of chosen over rejected) reveals why reward hacking emerges.
  - Quick check question: Can you explain why DPO's objective creates incentive to suppress rejected completions rather than improve chosen completions?

- **Hyperspherical Energy**: The paper uses HE to measure neuron arrangement uniformity; higher HE indicates representation redundancy and collapse.
  - Quick check question: If HE increases after training, what does this suggest about the model's representational capacity?

- **Orthogonal Matrices (Householder + Givens)**: RoPO constructs rotation matrix R from these components; understanding their properties explains how angular relationships are preserved.
  - Quick check question: Why does the product of orthogonal matrices remain orthogonal, and why does this preserve vector norms and angles?

## Architecture Onboarding

- **Component map**:
  - Weight Decomposition: W -> m (magnitude, trainable) + W/||W||_c (direction, frozen)
  - Multi-Granularity Rotation Matrix R: R = G̃₁G̃₂H̃ (Givens + Householder)
  - Householder Reflection: H̃ = (I - 2u₁ᵀu₁)(I - u₂ᵀu₂) with 2d trainable params
  - Givens Matrices: G̃₁, G̃₂ with d-1 total trainable rotation angles θₖ
  - Forward Pass: z = m · (R · W/||W||_c)ᵀx

- **Critical path**:
  1. Initialize m = ||W||_c from SFT model weights
  2. Initialize u₁, u₂ as [1,0,...,0] and rotation angles θₖ = 0
  3. During training: only m, u₁, u₂, and θₖ update
  4. At inference: merge R and m into W' (no overhead)

- **Design tradeoffs**:
  - Parameter efficiency vs. expressiveness: Only ~0.015% params trainable; rotation angles may limit adaptation capacity for complex preference shifts
  - Global vs. fine-grained rotation: Householder provides coarse adjustments; Givens enables fine-grained 2D plane rotations. Ablation shows both contribute
  - Constraint strength: Stronger orthogonal constraints reduce reward hacking but may limit alignment quality; β in DPO loss provides additional control

- **Failure signatures**:
  - Over-regularization: If alignment performance degrades significantly vs. DPO, orthogonal constraints may be too restrictive
  - Insufficient rotation capacity: If preference learning fails, consider increasing rotation granularity or applying to more layers
  - Magnitude instability: If m grows unbounded, consider adding explicit magnitude regularization

- **First 3 experiments**:
  1. Reproduce HE analysis: Train DPO vs. RoPO on identical preference data; compute ΔHE across layers to verify collapse mitigation correlates with performance gains
  2. Ablation rotation components: Remove Householder (w/o GRM) and Givens (w/o FRM) separately; quantify contribution of each to AlpacaEval 2 and MT-Bench scores
  3. KL divergence trajectory: Log KL(chosen), KL(rejected), and reward accuracy during training; verify RoPO reduces rejected-suppression asymmetry while maintaining reward accuracy (replicate Figure 6 pattern)

## Open Questions the Paper Calls Out
None

## Limitations
- The paper's central claim about neuron collapse causing reward hacking rests on correlational evidence rather than causal intervention studies
- The orthogonal regularization mechanism assumes knowledge is encoded in relative angular relationships between neurons, which lacks direct experimental validation
- Claims about knowledge being "partially encoded in relative angles between neurons" lack direct experimental validation within this paper

## Confidence
- **High confidence**: Performance improvements (3.27-point AlpacaEval 2 gain, 6.2-7.5 point MT-Bench improvement) and parameter efficiency claims are well-supported
- **Medium confidence**: The mechanism explaining how neuron collapse leads to reward hacking is plausible but not definitively proven
- **Low confidence**: Claims about knowledge being "partially encoded in relative angles between neurons" lack direct experimental validation

## Next Checks
1. Causal intervention experiment: Apply targeted perturbations to neuron directions in trained DPO models and measure impact on reward hacking behaviors
2. Angle information content analysis: Perform ablation studies where specific angular relationships are destroyed while preserving magnitudes
3. Single-constraint isolation: Train models with only KL regularization on logits versus only orthogonal regularization on hidden states to isolate contribution of each constraint