---
ver: rpa2
title: 'VF-Eval: Evaluating Multimodal LLMs for Generating Feedback on AIGC Videos'
arxiv_id: '2505.23693'
source_url: https://arxiv.org/abs/2505.23693
tags:
- video
- reasoning
- videos
- arxiv
- mllms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "VF-Eval is a benchmark designed to evaluate the reasoning capabilities\
  \ of multimodal large language models (MLLMs) on AI-generated content (AIGC) videos.\
  \ The benchmark introduces four tasks\u2014coherence validation, error awareness,\
  \ error type detection, and reasoning evaluation\u2014to comprehensively assess\
  \ MLLM performance."
---

# VF-Eval: Evaluating Multimodal LLMs for Generating Feedback on AIGC Videos

## Quick Facts
- arXiv ID: 2505.23693
- Source URL: https://arxiv.org/abs/2505.23693
- Authors: Tingyu Song; Tongyan Hu; Guo Gan; Yilun Zhao
- Reference count: 34
- Primary result: VF-Eval benchmark reveals frontier MLLMs struggle with AIGC video error detection, with top model achieving only 51.6% overall accuracy.

## Executive Summary
VF-Eval is a novel benchmark designed to evaluate multimodal large language models' ability to generate detailed feedback on AI-generated content (AIGC) videos. Unlike traditional video quality scoring benchmarks, VF-Eval decomposes feedback generation into four distinct tasks: coherence validation, error awareness, error type detection, and reasoning evaluation. The benchmark includes 9,740 question-answer pairs derived from videos generated by eight different text-to-video models, covering diverse error scenarios specific to AIGC content.

Experiments with 13 state-of-the-art MLLMs reveal significant performance gaps, particularly in error detection tasks where models often exhibit "normalcy bias" - incorrectly rating flawed videos as error-free. The best-performing model, GPT-4.1, achieves only 51.6% overall accuracy, highlighting the challenge of AIGC video evaluation. The study also introduces REPROMPT, a human-in-the-loop mechanism showing that aligning MLLM feedback with human preferences can potentially improve video generation quality.

## Method Summary
VF-Eval evaluates MLLMs on four tasks using a dataset of 9,740 QA pairs from AIGC videos. The benchmark uses specific CoT prompts for each task and employs GPT-4.1-mini as an LLM-as-a-judge for open-ended tasks. Models without native video support use 2/4/8/16 frames per context window. The evaluation pipeline extracts structured answers using GPT-4o and scores them according to defined functions. A human-MLLM annotation pipeline creates the dataset, followed by rigorous human validation.

## Key Results
- Top model GPT-4.1 achieves only 51.6% overall accuracy across all tasks
- Error Awareness task shows worst performance, with models exhibiting normalcy bias (rating flawed videos as error-free)
- Information Synopsis task shows highest performance (84.1%), while Temporal Reasoning performs worst (24.8%)
- REPROMPT experiment shows improved video quality when MLLM feedback is aligned with human preferences

## Why This Works (Mechanism)

### Mechanism 1
- Claim: VF-Eval distinguishes AIGC video evaluation from natural video evaluation by emphasizing detailed error reasoning over simple quality scoring.
- Mechanism: The benchmark decomposes feedback generation into four measurable tasks: coherence validation (prompt-video alignment), error awareness (binary error detection), error type detection (categorizing errors), and reasoning evaluation (complex inference). This forces MLLMs to move from implicit scoring to explicit, structured feedback.
- Core assumption: The quality of feedback for AIGC videos can be effectively captured through these four sub-tasks, and performance on them correlates with practical utility in video generation pipelines.
- Evidence anchors:
  - [abstract] "...four tasks—coherence validation, error awareness, error type detection, and reasoning evaluation—to comprehensively evaluate..."
  - [section] Page 3, Section 3.1 defines the four tasks and their scoring functions (Eq. 1-4).
  - [corpus] Related benchmarks like QBench (Wu et al., 2024a) focus on scoring; VF-Eval is positioned as a reasoning-focused alternative.
- Break condition: If MLLMs can perform well on isolated tasks but cannot integrate them into coherent feedback for video generation, the benchmark's decomposition may not translate to practical improvement.

### Mechanism 2
- Claim: Aligning MLLM-generated feedback with human preferences through a RePrompt process can improve subsequent AIGC video generation quality.
- Mechanism: Human annotators revise an initial MLLM-generated prompt based on observed video artifacts. The revised prompt is fed back into the generation model. The resulting video shows improved metrics (e.g., subject consistency, aesthetic quality), suggesting the feedback captured meaningful flaws in the original generation.
- Core assumption: The improvements observed (e.g., 56.7% win rate for subject consistency on 300 videos) generalize to other video generation models and are not artifacts of the specific test set or generation models used.
- Evidence anchors:
  - [abstract] "...aligning MLLMs more closely with human feedback can benefit video generation."
  - [section] Table 4 and Section 5.3 detail the RePrompt experiment and its positive win rates.
  - [corpus] No direct corpus validation for this specific RePrompt approach was found; related work is limited.
- Break condition: If re-generated videos consistently fail to outperform originals across diverse generation models and prompt types, the feedback-to-prompt translation mechanism may be flawed.

### Mechanism 3
- Claim: The unique characteristics of AIGC videos (synthetic artifacts, temporal inconsistencies) expose a fundamental distribution shift that current MLLMs, trained primarily on natural video, cannot robustly handle.
- Mechanism: AIGC videos contain artifacts like sudden object appearances, unnatural motion continuity, and unrealistic physical interactions. MLLMs, which may rely on commonsense priors learned from natural data, struggle to detect these specific violations, leading to poor performance on Error Awareness and Reasoning Evaluation tasks.
- Core assumption: The error types identified in the VF-Eval dataset are representative of the general flaws produced by current and near-future text-to-video models.
- Evidence anchors:
  - [section] Page 7: "AIGC videos often exhibit temporal inconsistencies, such as abrupt changes in motion or unnatural continuity..."
  - [section] Figure 1(b) provides examples like "Athlete with 3 arms" and "Sudden appearance of ball."
  - [corpus] EGOILLUSION (arXiv:2508.12687) similarly notes MLLM hallucinations in video understanding tasks.
- Break condition: If future MLLMs fine-tuned on synthetic video data achieve near-human performance on VF-Eval, the challenge is primarily a data distribution problem, not a fundamental architectural limitation.

## Foundational Learning

- Concept: **AIGC Video Artifacts vs. Natural Video Flaws**
  - Why needed here: Understanding that AIGC errors (e.g., sudden object appearances, morphing textures) are distinct from natural video flaws (e.g., motion blur, compression artifacts) is critical for interpreting benchmark results and designing better evaluation metrics.
  - Quick check question: Name two AIGC-specific error types from Figure 1(b) that an MLLM might overlook if it relies primarily on commonsense reasoning from natural video training.

- Concept: **Evaluation Task Taxonomy (Detection vs. Reasoning)**
  - Why needed here: The benchmark separates error *detection* (a binary or categorical task) from *reasoning* (open-ended explanation). This distinction is crucial for diagnosing whether an MLLM fails to *see* the error or fails to *articulate* its cause.
  - Quick check question: According to the task definitions, what is the primary output format difference between "Error Awareness" and "Reasoning Evaluation"?

- Concept: **Human-in-the-Loop for Preference Alignment**
  - Why needed here: The RePrompt mechanism relies on human judgment to revise prompts, which serves as the ground truth for "better" feedback. Understanding this process is key to replicating the improvement pipeline.
  - Quick check question: In the RePrompt experiment, what are the human judges evaluating when comparing the re-generated video to the original?

## Architecture Onboarding

- Component map: Benchmark Core (VF-Eval) -> Four task modules (Coherence Validation, Error Awareness, Error Type Detection, Reasoning Evaluation) -> Dataset Layer (9,740 QA pairs from 8 T2V models) -> Evaluation Pipeline (inference harness with CoT prompting and LLM-as-judge scoring) -> Application Layer (RePrompt experiment)

- Critical path:
  1. **Data Curation:** Generate or collect AIGC videos from diverse T2V models.
  2. **Annotation:** Use the human-MLLM pipeline to create QA pairs for the four tasks, followed by rigorous human validation.
  3. **Model Evaluation:** Run the target MLLM on the benchmark, using specified CoT prompts, and collect raw responses.
  4. **Scoring & Analysis:** Use the defined scoring functions (exact match for multiple-choice, LLM-eval for open-ended) to compute task-specific and overall scores.

- Design tradeoffs:
  - **Automated vs. Human Scoring:** Using an LLM (GPT-4.1-mini) to score open-ended responses introduces scalability but creates a dependency on that model's judgment, potentially masking subtle errors or biases.
  - **Task-Specific vs. Generalist Models:** The benchmark evaluates generalist MLLMs. Specialized computer vision methods might outperform them on specific error detection (e.g., temporal flicker), which the paper notes as an auxiliary approach.
  - **Dataset Diversity:** Using multiple video generators ensures breadth but may conflate model-specific artifacts with general AIGC challenges.

- Failure signatures:
  - **"Normalcy Bias" in Error Awareness:** Performance worse than random guessing (Table 3, e.g., GPT-4.1 at 39.7%) indicates a tendency to rate AIGC videos as error-free.
  - **Prompt Expansion in Coherence Validation:** Simply adding adjectives to the original prompt rather than correcting core misalignments (Figure 14).
  - **Commonsense Hallucination:** Answering reasoning questions based on real-world logic (e.g., "a balloon pops") instead of observing the actual video content (e.g., "a balloon morphs").

- First 3 experiments:
  1. **Baseline Performance Check:** Evaluate a single frontier model (e.g., Gemini-2.0-Flash or InternVL3-38B) across all four tasks to reproduce key findings (e.g., strong performance on Information Synopsis vs. weak performance on Temporal Reasoning).
  2. **Ablation on Visual Input:** Run the Reasoning Evaluation task in a text-only mode (providing only the prompt/question, not the video) to quantify the performance drop and confirm visual reliance (similar to Figure 3).
  3. **Pilot RePrompt Test:** Select 10-20 videos with clear errors, manually revise their prompts based on MLLM feedback, and re-generate to see if subjective quality improves, validating the core application claim.

## Open Questions the Paper Calls Out

- **Cross-model generalization**: How does MLLM performance on error detection tasks change when evaluating videos generated from Image-to-Video (I2V) models compared to the Text-to-Video (T2V) models used in this study?
  - Basis: The authors explicitly state in the Limitations section that "only text-to-video models are considered, whereas videos generated from images may exhibit other types of error cases that are not addressed in this study."
  - Resolution: An extension of the VF-Eval dataset to include I2C outputs and a comparative evaluation of frontier MLLMs on these new samples.

- **Localized feedback impact**: Does incorporating fine-grained spatio-temporal localization (e.g., bounding boxes or timestamps) into MLLM feedback significantly improve video generation quality compared to the purely textual feedback used in the RePrompt experiment?
  - Basis: The authors note in the Limitations that the "design of the re-prompt pipeline is relatively simplistic, as it only incorporates textual feedback... The specific positions of error cases are not included."
  - Resolution: A comparative study where one group of video generators receives text-only prompts and another receives text plus error localization, measured by human evaluation metrics.

- **Hybrid CV-MLLM approach**: Can integrating auxiliary computer vision techniques (such as object tracking or physics simulation) correct the MLLM tendency to rely on commonsense assumptions rather than visual evidence when analyzing AIGC videos?
  - Basis: The error analysis identifies "Over-reliance on Commonsense Knowledge" as a major failure mode, and the Conclusion suggests that "integrating other methods, such as computer vision techniques... can further enhance feedback precision."
  - Resolution: Ablation studies showing performance improvements on the "Commonsense and Physics" Error Awareness tasks when CV-based grounding modules are added to the MLLM pipeline.

## Limitations

- The benchmark only evaluates text-to-video models, not image-to-video generation which may exhibit different error profiles
- The RePrompt experiment's improvement results are based on a limited set of 300 videos from specific T2V models
- LLM-as-a-judge scoring introduces potential bias from GPT-4.1-mini's judgment criteria, which are not fully transparent

## Confidence

- **High** for the core claim that MLLMs struggle with AIGC-specific error detection and reasoning (supported by strong statistical evidence across 13 models)
- **Medium** for the assertion that VF-Eval represents a novel decomposition of feedback generation into four measurable tasks (methodologically sound but similar concepts exist in other benchmarks)
- **Low** for the practical applicability of the RePrompt mechanism to improve video generation quality (limited experimental scope, no validation on diverse T2V models)

## Next Checks

1. **Cross-model validation**: Evaluate VF-Eval with additional T2V models beyond the original eight to test robustness of error type detection across different generation architectures.

2. **Alternative judge evaluation**: Re-score a subset of open-ended responses using multiple LLM judges (different models, prompting strategies) to quantify scoring consistency and potential bias.

3. **Temporal generalization test**: Re-run the benchmark after 6-12 months with updated frontier MLLMs to measure progress and determine if current performance gaps are closing.