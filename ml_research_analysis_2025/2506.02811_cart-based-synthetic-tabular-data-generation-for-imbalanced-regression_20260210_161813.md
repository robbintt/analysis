---
ver: rpa2
title: CART-based Synthetic Tabular Data Generation for Imbalanced Regression
arxiv_id: '2506.02811'
source_url: https://arxiv.org/abs/2506.02811
tags:
- regression
- data
- imbalanced
- synthetic
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CARTGen-IR, a method for handling imbalanced
  regression problems in tabular data. It adapts a CART-based synthetic data generation
  technique to generate synthetic data in sparse regions of the target space without
  requiring user-defined thresholds.
---

# CART-based Synthetic Tabular Data Generation for Imbalanced Regression

## Quick Facts
- arXiv ID: 2506.02811
- Source URL: https://arxiv.org/abs/2506.02811
- Authors: António Pedro Pinheiro; Rita P. Ribeiro
- Reference count: 34
- Key outcome: CARTGen-IR handles imbalanced regression by generating synthetic data in sparse target regions without user-defined thresholds, showing competitive performance with faster execution and superior extreme value prediction.

## Executive Summary
This paper introduces CARTGen-IR, a method for handling imbalanced regression problems in tabular data. It adapts a CART-based synthetic data generation technique to generate synthetic data in sparse regions of the target space without requiring user-defined thresholds. The method uses a relevance and density-based mechanism to guide sampling and employs a threshold-free, feature-driven generation process. Experimental results on 15 benchmark datasets show that CARTGen-IR is competitive with other resampling and generative strategies in terms of predictive performance (RW-RMSE, SERA, RMSE metrics) while offering faster execution and greater transparency. The method demonstrates superior performance particularly in predicting extreme target values and exhibits a favorable win-to-loss ratio compared to state-of-the-art approaches.

## Method Summary
CARTGen-IR addresses imbalanced regression by first computing rarity scores for each instance using density estimation methods (KDE, DenseWeight, or relevance functions), then resampling with replacement based on these scores. The method generates synthetic samples by fitting CART decision trees sequentially for each feature, using all other features (including the target) as predictors. Synthetic values are sampled from the conditional distributions at leaf nodes. The approach is threshold-free, avoiding arbitrary discretization of the continuous target domain, and includes optional noise injection to improve generalization. The entire process is designed to oversample sparse regions of the target space where extreme values are typically underrepresented.

## Key Results
- CARTGen-IR achieves competitive performance with state-of-the-art resampling methods (SMOTER, SMOGN) and generative models (RELASCO) on RW-RMSE, SERA, and RMSE metrics
- The method demonstrates superior performance in predicting extreme target values with favorable win-to-loss ratios in statistical comparisons
- CARTGen-IR offers faster execution times compared to GAN/VAE/diffusion-based approaches while maintaining predictive accuracy

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Relevance and density-based weighting enables targeted oversampling of sparse target regions without requiring arbitrary threshold definitions.
- **Mechanism:** A rarity score is computed for each instance using one of three methods (KDE, DenseWeight, or relevance function). The inverse of the estimated density (plus a small constant) determines the score, which is then adjusted by exponent α and normalized. Resampling with replacement according to these weights produces a dataset dominated by rare target values.
- **Core assumption:** The inverse density of target values correlates with domain relevance—rarer values are assumed to be more important to predict accurately.
- **Evidence anchors:**
  - [abstract] "integrates relevance and density-based mechanisms to guide sampling in sparse regions of the target space"
  - [section 3.2, Algorithm 1] Describes weighting techniques including KDE, DenseWeight, and relevance function with α exponent and normalization
  - [corpus] Weak direct corpus support; related work (Steininger et al. DenseWeight [28]) is cited as foundational but not independently validated in neighbors
- **Break condition:** If domain importance does not correlate with rarity (e.g., common values are actually critical), the weighting scheme will oversample the wrong regions.

### Mechanism 2
- **Claim:** CART-based sequential column-wise generation captures conditional dependencies between features while producing plausible synthetic samples.
- **Mechanism:** For each column, a CART decision tree is fitted using all other columns (including the target) as predictors. Synthetic samples are generated by traversing the tree and sampling from the conditional distribution at leaf nodes. This process repeats sequentially across columns.
- **Core assumption:** The CART tree structure adequately captures the true conditional distributions P(X_i | X_{-i}) for all features.
- **Evidence anchors:**
  - [abstract] "CART-based synthetic data generation technique"
  - [section 3.1, Fig. 1] Illustrates consecutive column-wise data generation using CART with leaf node sampling
  - [corpus] Limited corpus validation; neighbor papers focus on GAN/VAE/diffusion approaches rather than CART for imbalanced regression
- **Break condition:** If features have complex parametric distributions (e.g., multimodal continuous) that CART partitions approximate poorly, synthetic sample quality degrades.

### Mechanism 3
- **Claim:** Threshold-free formulation avoids arbitrary discretization of continuous target domains inherited from classification approaches.
- **Mechanism:** Rather than partitioning the target variable into "minority" and "majority" bins via user-specified thresholds (as SMOTER, SMOGN require), CARTGen-IR uses continuous weighting based on density/relevance. No crisp boundaries are introduced.
- **Core assumption:** Continuous relevance weighting is more principled and domain-appropriate than threshold-based binning.
- **Evidence anchors:**
  - [abstract] "employs a threshold-free, feature-driven generation process"
  - [section 3.2] "enables us to avoid crisp divisions over the continuous domain of the target variable into bins or partitions"
  - [corpus] No direct corpus comparison of threshold-free vs. threshold-based approaches
- **Break condition:** If domain experts have clear, meaningful thresholds that define critical operating ranges, the threshold-free approach may ignore useful prior knowledge.

## Foundational Learning

- **Concept: Imbalanced Regression Problem Formulation**
  - **Why needed here:** The entire method is motivated by the challenge that standard regression assumes uniform importance across target values, which fails when rare/extreme values are most critical.
  - **Quick check question:** Can you explain why MSE is ill-suited for evaluating predictions on extreme values in a skewed distribution?

- **Concept: Relevance Functions for Continuous Targets**
  - **Why needed here:** Understanding how ϕ(y) maps target values to [0,1] relevance scale is essential for interpreting the weighting mechanism and SERA metric.
  - **Quick check question:** How does the relevance function in [25] use adjusted boxplot statistics to automatically identify extreme values without user input?

- **Concept: Conditional Distribution Estimation via CART**
  - **Why needed here:** The core generation mechanism relies on CART trees estimating P(feature | other features); understanding this connects tree structure to synthetic sample validity.
  - **Quick check question:** Given a fitted CART tree for a continuous feature, how would you sample a new value from a leaf node?

## Architecture Onboarding

- **Component map:** Weighting module (KDE / DenseWeight / relevance function) → produces normalized rarity scores → Resampling module → draws instances weighted by rarity, with replacement → Optional noise injection (δ hyperparameter) → perturbs duplicate instances → CART fitting loop → fits one tree per column using other columns as predictors → Generation loop → samples synthetic values column-wise from tree leaf distributions → Output → augmented dataset (original + synthetic samples)

- **Critical path:**
  1. Compute rarity scores for all training instances
  2. Resample N × η instances (where N = original size, η controls proportion)
  3. For each selected instance, generate 5 synthetic samples (per common practice in imbalanced learning)
  4. Sequentially generate each feature via its CART tree

- **Design tradeoffs:**
  - α (rarity exponent): Higher values (1.5–2.0) improve performance on extremes but may reduce diversity; paper finds α=1.5–2.0 optimal
  - η (synthetic proportion): Controls augmentation scale; paper finds limited sensitivity
  - density method: DenseWeight and relevance outperform KDE in experiments
  - δ (noise): Small Gaussian noise (δ=0.001) on duplicates improves generalization

- **Failure signatures:**
  - Synthetic samples cluster near a few leaf nodes → CART trees may be too deep/overfit; consider limiting tree depth
  - No improvement on extreme value prediction → rarity exponent α may be too low; increase to 1.5+
  - Runtime degradation on high-dimensional data → column-wise CART fitting scales linearly with features; consider feature selection first
  - Poor performance on parametric distributions → partition boundaries may miss true relationships (acknowledged limitation in Section 5)

- **First 3 experiments:**
  1. **Baseline validation:** Apply CARTGen-IR to a dataset from the paper's benchmark (e.g., `housingBoston`) with default hyperparameters (α=1.5, η=0.5, density=DenseWeight, δ=0.001); compare SERA/RW-RMSE against no-augmentation baseline to confirm reproduction.
  2. **Ablation on weighting method:** Run CARTGen-IR with density={kde, DenseWeight, relevance} on 3 diverse datasets; measure impact on SERA and runtime to validate paper's claim that DenseWeight/relevance outperform KDE.
  3. **Noise sensitivity test:** Compare δ=0 vs. δ=0.001 vs. δ=0.01 on a dataset with many duplicate resampled instances; assess overfitting via train/validation gap on extreme target regions.

## Open Questions the Paper Calls Out

- **Open Question 1:** How does CARTGen-IR perform on regression tasks where the underrepresented domain intervals are located in the middle of the distribution rather than at the extremes?
  - **Basis in paper:** [explicit] Section 5 outlines future work to expand the study to "datasets containing target domain rare intervals that are not extremes."
  - **Why unresolved:** The current experimental study exclusively utilizes datasets where the rare and relevant cases are defined by extreme high or low target values.
  - **What evidence would resolve it:** Benchmarking performance on datasets curated to have sparse, high-relevance regions strictly within the central range of the target variable.

- **Open Question 2:** What modifications are required to improve the effectiveness of CARTGen-IR on data following parametric distributions?
  - **Basis in paper:** [explicit] Section 5 identifies the "unsuitability of CARTGen-IR for parametric distributions, as partition boundaries may underperform in effectively capturing the true data relationships."
  - **Why unresolved:** The recursive partitioning nature of CART trees may create boundaries that fail to align with the underlying structure of parametric data.
  - **What evidence would resolve it:** A comparative analysis on synthetic datasets with known parametric distributions showing improved fidelity or predictive accuracy after algorithmic adaptation.

- **Open Question 3:** Can the predictive performance of CARTGen-IR be further enhanced by integrating it with learners that use a cost-sensitive approach to optimization?
  - **Basis in paper:** [explicit] Section 5 proposes "incorporating learners with a cost-sensitive approach to imbalanced regression-specific metrics, such as SERA."
  - **Why unresolved:** The current experiments couple the data generation method with standard learners (RF, SVR, XGBoost) rather than models that directly optimize for the imbalanced metrics used in evaluation.
  - **What evidence would resolve it:** Experiments comparing standard learners against cost-sensitive variants trained on CARTGen-IR data, measuring improvements in SERA or RW-RMSE.

## Limitations
- The reliance on CART trees may struggle with highly parametric or multimodal feature distributions
- The threshold-free approach may discard valuable domain expertise encoded in meaningful rarity thresholds
- The method's scalability to very high-dimensional tabular data (100+ features) is untested

## Confidence
- **High Confidence:** The core mechanism of CART-based column-wise generation and the empirical results on benchmark datasets are well-supported
- **Medium Confidence:** The theoretical justification for threshold-free rarity weighting and the selection of hyperparameters through grid search is reasonable but could benefit from more rigorous sensitivity analysis
- **Low Confidence:** The generalizability of results beyond the 15 benchmark datasets used, particularly to real-world industrial applications with different data characteristics

## Next Checks
1. **Domain Transfer Validation:** Apply CARTGen-IR to a real-world imbalanced regression problem from a different domain (e.g., healthcare cost prediction for rare diseases) and compare against threshold-based approaches to test the threshold-free assumption.

2. **Distribution Complexity Test:** Generate synthetic tabular data with known parametric distributions (e.g., Gaussian mixtures, skewed distributions) and evaluate CARTGen-IR's ability to preserve these distributions versus methods using parametric density estimation.

3. **Scalability Benchmark:** Test CARTGen-IR on high-dimensional tabular datasets (50+ features) and measure both runtime performance and predictive accuracy degradation compared to baseline methods, documenting the computational complexity scaling.