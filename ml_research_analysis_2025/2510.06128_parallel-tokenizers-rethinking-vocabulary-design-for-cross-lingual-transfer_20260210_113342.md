---
ver: rpa2
title: 'Parallel Tokenizers: Rethinking Vocabulary Design for Cross-Lingual Transfer'
arxiv_id: '2510.06128'
source_url: https://arxiv.org/abs/2510.06128
tags:
- language
- languages
- tokenizer
- data
- single-13l
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces parallel tokenizers, a new approach for cross-lingual
  transfer that aligns semantically equivalent words across languages using word-level
  machine translation, ensuring shared vocabulary indices and improved cross-lingual
  representation. Instead of a single multilingual tokenizer, it trains monolingual
  tokenizers and aligns them into parallel vocabularies, supplemented with language
  identity embeddings for disambiguation.
---

# Parallel Tokenizers: Rethinking Vocabulary Design for Cross-Lingual Transfer

## Quick Facts
- **arXiv ID:** 2510.06128
- **Source URL:** https://arxiv.org/abs/2510.06128
- **Reference count:** 40
- **Primary result:** Parallel tokenizers improve cross-lingual transfer by aligning semantically equivalent words across languages using word-level machine translation, achieving 1.22% average F1 score improvement and better cross-lingual clustering

## Executive Summary
This paper introduces parallel tokenizers, a novel approach to cross-lingual representation learning that aligns semantically equivalent words across languages through word-level machine translation. Unlike traditional multilingual tokenizers that use a single vocabulary for all languages, parallel tokenizers train monolingual tokenizers and align them into parallel vocabularies with shared indices for equivalent words. The approach is evaluated on 13 low-resource languages across four task types, showing consistent improvements over multilingual baselines in both performance metrics and cross-lingual alignment quality.

## Method Summary
Parallel tokenizers address the vocabulary alignment challenge in cross-lingual transfer by training separate monolingual tokenizers and then aligning their vocabularies through word-level machine translation. The method identifies semantically equivalent words across language pairs and assigns them the same vocabulary index, creating parallel vocabularies that share representations for aligned concepts. To handle language-specific ambiguity, the approach incorporates language identity embeddings that provide disambiguation signals during representation learning. The system is evaluated across sentiment analysis, hate speech detection, emotion classification, and sentence similarity tasks in 13 low-resource languages, demonstrating improved cross-lingual transfer compared to traditional multilingual tokenizers.

## Key Results
- Achieved 1.22% average improvement in F1 scores compared to traditional multilingual baselines
- Demonstrated lower fertility and parity scores, indicating better vocabulary efficiency
- Showed stronger cross-lingual clustering in PCA visualizations, with better alignment of semantically similar sentences across languages

## Why This Works (Mechanism)
The parallel tokenizer approach works by creating shared vocabulary representations for semantically equivalent words across languages while maintaining language-specific tokenization flexibility. By using word-level machine translation to align vocabularies, the method ensures that conceptually similar terms receive the same representation regardless of language, facilitating cross-lingual transfer. The addition of language identity embeddings helps disambiguate context where direct translations might not capture full semantic nuance, particularly important for low-resource languages where translation quality may be limited.

## Foundational Learning
**Cross-lingual transfer learning** - The ability of models trained on one language to perform well on another language without direct supervision
*Why needed:* Essential for leveraging abundant high-resource language data to improve performance on low-resource languages
*Quick check:* Can you explain how parallel tokenizers improve cross-lingual transfer compared to traditional multilingual approaches?

**Vocabulary alignment** - The process of mapping semantically equivalent words across different language vocabularies to shared representations
*Why needed:* Critical for ensuring consistent representations of concepts across languages in multilingual models
*Quick check:* What challenges arise when aligning vocabularies across languages with different morphological structures?

**Language identity embeddings** - Additional embedding vectors that encode language information to disambiguate representations
*Why needed:* Helps resolve cases where shared vocabulary indices might create ambiguity between languages
*Quick check:* How might language identity embeddings affect the model's ability to generalize across language boundaries?

## Architecture Onboarding

**Component map:** Monolingual tokenizers -> Word-level MT alignment -> Parallel vocabulary creation -> Language identity embeddings -> Cross-lingual model training

**Critical path:** The alignment of semantically equivalent words through word-level machine translation is the core innovation that enables shared representations across languages, making this the critical path for achieving cross-lingual transfer improvements.

**Design tradeoffs:** The approach trades increased vocabulary complexity and alignment overhead for improved cross-lingual representation quality. Using word-level rather than subword alignment may miss morphological variations but ensures semantic equivalence at the word level.

**Failure signatures:** Poor machine translation quality between language pairs could lead to misaligned vocabulary indices, degrading rather than improving cross-lingual transfer. The method may struggle with languages that have significantly different word formation processes or where direct translations don't capture full semantic nuance.

**First experiments:**
1. Evaluate vocabulary alignment quality by measuring precision of aligned word pairs against human-annotated bilingual dictionaries
2. Compare performance of parallel tokenizers versus traditional multilingual tokenizers on a small set of language pairs with high-quality translation resources
3. Conduct ablation studies removing language identity embeddings to quantify their contribution to disambiguation

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to 13 low-resource languages and four specific task types, requiring broader validation across diverse language families and domains
- Relies on word-level machine translation for vocabulary alignment, which may introduce noise and errors, particularly for low-resource language pairs
- Supplementary use of language identity embeddings requires further investigation into optimal strategies and potential interference with task-specific representations

## Confidence
**High confidence:** The core methodology of training monolingual tokenizers and aligning them through parallel vocabularies is technically sound and well-explained
**Medium confidence:** The empirical improvements in cross-lingual transfer tasks, given the limited language coverage and task diversity
**Medium confidence:** The claimed benefits of reduced fertility and parity scores, as these metrics require careful interpretation in context

## Next Checks
1. Evaluate parallel tokenizers across a broader range of language families (including high-resource languages) and task types beyond the current focus areas
2. Conduct ablation studies to quantify the individual contributions of vocabulary alignment versus language identity embeddings
3. Compare performance against alternative cross-lingual alignment approaches using different granularities (subword, character-level) and alignment methods (contextual embeddings, dictionary-based approaches)