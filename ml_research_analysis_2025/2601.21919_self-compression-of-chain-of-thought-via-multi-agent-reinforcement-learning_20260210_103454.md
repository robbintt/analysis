---
ver: rpa2
title: Self-Compression of Chain-of-Thought via Multi-Agent Reinforcement Learning
arxiv_id: '2601.21919'
source_url: https://arxiv.org/abs/2601.21919
tags:
- reasoning
- length
- arxiv
- scma
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of redundant reasoning in Large
  Reasoning Models (LRMs), which increases inference latency and hampers deployment
  efficiency. The core method introduces Self-Compression via Multi-Agent Reinforcement
  Learning (SCMA), a framework that decomposes the reasoning process into logical
  chunks using specialized Segmentation and Scoring agents, which collaboratively
  define an importance-weighted length penalty to guide a Reasoning agent.
---

# Self-Compression of Chain-of-Thought via Multi-Agent Reinforcement Learning

## Quick Facts
- arXiv ID: 2601.21919
- Source URL: https://arxiv.org/abs/2601.21919
- Reference count: 25
- Self-Compression of Chain-of-Thought via Multi-Agent Reinforcement Learning reduces response length by 11.1%–39.0% while boosting accuracy by 4.33%–10.02% across multiple model scales and benchmarks.

## Executive Summary
This paper addresses the problem of redundant reasoning in Large Reasoning Models (LRMs), which increases inference latency and hampers deployment efficiency. The core method introduces Self-Compression via Multi-Agent Reinforcement Learning (SCMA), a framework that decomposes the reasoning process into logical chunks using specialized Segmentation and Scoring agents, which collaboratively define an importance-weighted length penalty to guide a Reasoning agent. This MARL-based approach selectively penalizes redundancy while preserving essential logic, without introducing inference overhead during deployment. Empirical results show that SCMA reduces response length by 11.1%–39.0% while boosting accuracy by 4.33%–10.02% across multiple model scales and benchmarks, outperforming length-penalized RL baselines. Ablation studies confirm that the multi-agent cooperative optimization is essential for achieving superior reasoning compression and accuracy.

## Method Summary
The SCMA framework uses three agents sharing parameters from a single base LLM: a Reasoning Agent that generates Chain-of-Thought responses, a Segmentation Agent that parses responses into logical chunks using `<seg>` tags, and a Scoring Agent that assigns importance scores (1-5) to each chunk using `<score>` tags. These agents are optimized jointly via Multi-Agent Group Relative Policy Optimization (GRPO) with a shared reward function that combines accuracy with an importance-weighted length penalty. During inference, only the Reasoning Agent is deployed, as the compression behavior is learned into the policy during training. The framework operates as a training-time auxiliary with zero inference overhead.

## Key Results
- SCMA achieves 11.1%-39.0% length reduction compared to baseline RL+LP
- Accuracy improves by 4.33%-10.02% across multiple model scales (1.5B-8B parameters)
- Multi-agent optimization is essential - freezing auxiliary agents causes 2.37% accuracy loss and 4.62% token increase
- The framework prevents "No Think" collapse seen in standard length-penalized RL approaches

## Why This Works (Mechanism)

### Mechanism 1: Importance-Weighted Length Penalty
The framework replaces scalar length penalties with segment-level importance weighting. The Scoring Agent assigns importance scores (1-5) to each logical chunk, and the weighting function φ(wᵢ) = 5 - wᵢ maps high importance → low penalty weight, and low importance → high penalty weight. This creates selective pressure that exempts essential logic from compression while penalizing redundant segments.

### Mechanism 2: Multi-Agent Co-Evolution via Shared Reward
Joint optimization of Reasoning, Segmentation, and Scoring agents through a shared global reward creates synergistic capability emergence. All three agents share parameters θ from a single base LLM and are optimized simultaneously via multi-agent GRPO. The shared reward couples their objectives: the Reasoning Agent learns to generate compressible chains, the Segmentation Agent learns to partition semantically, and the Scoring Agent learns accurate importance estimation.

### Mechanism 3: Training-Time-Only Auxiliary Architecture
The multi-agent framework induces compression behavior during training without inference-time overhead. All three agents are instantiated from the same base LLM with shared θ. During deployment, only the Reasoning Agent is used - the learned compression behavior is "compiled into" the policy rather than requiring auxiliary inference.

## Foundational Learning

- **Group Relative Policy Optimization (GRPO)**: Understanding baseline GRPO (advantage estimation via group reward normalization, no critic network) is prerequisite for grasping how SCMA extends single-agent GRPO to multi-agent settings.
- **Markov Games / Multi-Agent MDPs**: The paper formalizes SCMA as a Markov Game tuple ⟨G, V, O, A, R⟩ with sequential agent dependencies. Understanding how shared reward functions differ from individual agent rewards in cooperative MARL is essential.
- **Chain-of-Thought Redundancy Patterns**: Recognizing "over-thinking" behaviors (circular verification, meta-talk filler) helps debug compression failures and understand what patterns the framework targets.

## Architecture Onboarding

- **Component map**: Reasoning Agent → Segmentation Agent → Scoring Agent → Shared reward → Multi-Agent GRPO update
- **Critical path**: x → Reasoning → y → Segmentation → {sᵢ} → Scoring → {wᵢ} → Reward Computation → Multi-Agent GRPO Update
- **Design tradeoffs**: α (penalty strength): Higher α → more compression but risk of logic loss (0.1-0.4 range stable). Score granularity: 5-level scale balances expressiveness with learning signal. Agent prompt design: Segmentation must enforce lossless reconstruction (Σsᵢ = y); Scoring must output structured tags.
- **Failure signatures**: "No Think" collapse (response length drops to near-zero), Low Chunk Length Std (uniform segmentation rather than semantic-aware parsing), Low Average Score (Scoring Agent hasn't learned to distinguish importance).
- **First 3 experiments**: 1) Baseline comparison: Train vanilla GRPO + standard length penalty vs. SCMA on GSM8K; measure accuracy/length tradeoff curve. 2) Ablation freeze: Replace Segmentation/Scoring agents with frozen models; confirm joint optimization necessity. 3) Alpha sweep: Run SCMA with α ∈ {0.05, 0.1, 0.2, 0.3, 0.4}; plot accuracy vs. token reduction to identify stability boundary.

## Open Questions the Paper Calls Out

- **Open Question 1**: How does SCMA performance scale when applied to significantly larger models (70B+ parameters) and to heterogeneous multi-modal reasoning systems? The paper suggests future work will explore scaling towards large-scale, heterogeneous systems.
- **Open Question 2**: Can SCMA maintain its effectiveness when trained across multiple reasoning domains simultaneously, rather than on a single dataset? The framework's behavior under multi-domain training with potentially conflicting compression signals is unexplored.
- **Open Question 3**: What is the theoretical relationship between the discrete 1-5 importance score granularity and the optimal compression-accuracy trade-off? Whether this discrete granularity is optimal or whether continuous scoring would improve fine-grained control remains unaddressed.

## Limitations
- All experiments conducted on mathematics-focused reasoning tasks (GSM8K, MATH500, AMC23, AIME24/25), limiting generalizability to other domains
- Training duration and stopping criteria not fully specified, making it difficult to assess convergence and potential for further optimization
- Lack of qualitative analysis of Scoring Agent's output quality and edge case handling
- Single-dataset training approach may exploit domain-specific redundancy patterns rather than learning general compression principles

## Confidence

**High Confidence**: The empirical claim that SCMA achieves 11.1%-39.0% length reduction while improving accuracy by 4.33%-10.02% compared to baseline RL+LP is well-supported by experimental results and ablation studies.

**Medium Confidence**: The mechanism claim that importance-weighted penalties selectively preserve essential logic while pruning redundancy is plausible but lacks detailed qualitative analysis of specific reasoning patterns being compressed.

**Low Confidence**: The generalizability claim that SCMA's compression mechanism transfers across different model scales and diverse reasoning tasks is supported by reported results but would benefit from more extensive cross-domain validation.

## Next Checks
1. **Domain Generalization Test**: Evaluate SCMA on non-mathematical reasoning tasks including code generation (HumanEval), commonsense reasoning (StrategyQA), and scientific analysis to verify effectiveness beyond mathematics-focused benchmarks.

2. **Importance Scoring Analysis**: Conduct qualitative analysis of the Scoring Agent's output, examining cases where importance scores misalign with human judgment and identifying patterns in scoring errors.

3. **Training Stability Study**: Investigate impact of different α values on training dynamics across multiple random seeds, measure variance in performance outcomes, and identify stability boundary where "No Think" collapse becomes likely.