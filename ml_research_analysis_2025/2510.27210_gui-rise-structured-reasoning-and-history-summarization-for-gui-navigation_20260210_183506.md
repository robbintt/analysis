---
ver: rpa2
title: 'GUI-Rise: Structured Reasoning and History Summarization for GUI Navigation'
arxiv_id: '2510.27210'
source_url: https://arxiv.org/abs/2510.27210
tags:
- action
- reasoning
- history
- arxiv
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents GUI-Rise, a structured reasoning framework\
  \ for GUI navigation agents that addresses limitations in cross-domain generalization\
  \ and history utilization. The core method integrates three subtasks\u2014structured\
  \ reasoning (progress estimation and decision reasoning), action prediction, and\
  \ history summarization\u2014into a unified framework."
---

# GUI-Rise: Structured Reasoning and History Summarization for GUI Navigation

## Quick Facts
- arXiv ID: 2510.27210
- Source URL: https://arxiv.org/abs/2510.27210
- Reference count: 40
- State-of-the-art performance on Mind2Web, AITW, and MiniWob benchmarks with 39.7% step success rate on Mind2Web cross-domain split

## Executive Summary
GUI-Rise presents a structured reasoning framework for GUI navigation agents that addresses limitations in cross-domain generalization and history utilization. The method integrates three subtasks—structured reasoning (progress estimation and decision reasoning), action prediction, and history summarization—into a unified framework. By compressing interaction history into concise textual summaries and employing step-wise reasoning, the agent achieves state-of-the-art performance across multiple benchmarks. The approach demonstrates particular strength in out-of-domain generalization, with 50.7% improvement in action accuracy over prior methods on AITW.

## Method Summary
GUI-Rise employs a two-stage training procedure on a Qwen2-VL-2B/Qwen2.5-VL-3B backbone. First, supervised fine-tuning (SFT) uses pseudo-labeled trajectories generated by GPT-4o-mini through retrospective labeling, where the model receives ground-truth actions to produce structured Chain-of-Thought reasoning and history summaries. Second, reinforcement learning with Group Relative Policy Optimization (GRPO) refines the policy using three complementary rewards: action accuracy, format correctness, and history summary quality. The history summary reward performs k additional rollouts to evaluate summary utility for future actions, creating a feedback loop that improves summarization quality. This approach enables efficient long-horizon decision-making by replacing visual context with semantic descriptions while maintaining task-relevant information.

## Key Results
- Achieves 39.7% step success rate on Mind2Web's cross-domain split
- Improves action accuracy by 50.7% over prior methods on AITW
- Demonstrates strong out-of-domain generalization across Mind2Web, AITW, and MiniWob benchmarks
- Shows superior performance in tasks requiring multi-step reasoning compared to unstructured baselines

## Why This Works (Mechanism)

### Mechanism 1: Textual History Compression
- Compressing visual and action history into textual summaries improves long-horizon decision accuracy while reducing token costs compared to raw screenshots
- Replaces visual context with semantic descriptions at each step, avoiding fixed window limits and visual token overhead
- Assumes sufficient information for future decisions can be captured textually without fine-grained visual details

### Mechanism 2: Structured CoT with Progress Estimation
- Decomposing reasoning into explicit progress estimation followed by decision reasoning guides more coherent action selection
- Separates task completion status assessment from strategy formulation, mimicking human navigation patterns
- Assumes explicit progress awareness causally improves subsequent decision quality

### Mechanism 3: History-Aware Reinforcement Reward
- Rewarding history summaries based on their utility for future actions creates a feedback loop improving summarization quality
- Uses k additional rollouts to evaluate summary quality by testing if generated summaries enable accurate near-term predictions
- Assumes summaries enabling accurate single-step predictions generalize to longer horizons

## Foundational Learning

- **Group Relative Policy Optimization (GRPO)**: Replaces PPO's value network with group-level advantage normalization, reducing computational cost and training instability in RL phase
  - Quick check: Can you explain how GRPO computes advantages without a learned critic?

- **Retrospective Pseudo-labeling**: Cold-start requires labeled CoT and summaries generated by stronger models given ground-truth actions, not predicted prospectively
  - Quick check: Why does the pseudo-labeling prompt include the correct action rather than asking the model to predict it?

- **Multimodal Context Window Competition**: Screenshots consume many visual tokens while textual summaries compete for limited context capacity
  - Quick check: What happens to visual detail when screenshots are compressed (per UI-Hawk's 16x compression)?

## Architecture Onboarding

- **Component map**: Screenshot + instruction + previous summary → MLLM backbone (Qwen-VL) → Output: <Progress Estimation>, <Decision Reasoning>, <Action>, <History Summary>

- **Critical path**: 
  1. Pseudo-label generation: GPT-4o-mini labels trajectories with CoT and summaries
  2. SFT phase: Train on pseudo-labeled data to establish format compliance and basic reasoning
  3. RL phase: GRPO with format/action/history rewards; history reward requires k no-gradient rollouts per sample

- **Design tradeoffs**:
  - Cold-start necessity: Critical for visually complex domains (Mind2Web); less impact on simpler mobile tasks (AITW)
  - Rollout count (k): Higher k improves reward signal fidelity but increases compute linearly
  - Summary length: Shorter saves tokens; longer preserves more context

- **Failure signatures**:
  - Vanishing position reward without cold-start: Model never learns spatial grounding
  - Format reward stalls: Indicates structural tag learning failed
  - Summary drift: Without history reward, summaries can mislead policy

- **First 3 experiments**:
  1. Cold-start ablation on your domain: Compare SFT→RL vs. RL-only; check if position reward vanishes without SFT
  2. History representation comparison: Measure success rate vs. input token count for summary vs. screenshot approaches
  3. k-value sweep for history reward: Test k∈{1,2,4,8} to find compute/quality inflection point

## Open Questions the Paper Calls Out

- **Online Learning Adaptation**: The authors suggest exploring online learning methods as future work since the model is trained entirely offline and cannot adapt to new scenarios in real-time

- **Teacher Model Quality Dependency**: The performance dependence on GPT-4o-mini generated pseudo-labels is not analyzed, raising concerns about potential bias inheritance from teacher model errors

- **Computational Scalability**: The history summary reward's requirement for k additional inference rollouts per step significantly increases computational cost, with no complexity analysis provided

## Limitations

- Data dependency on high-quality pseudo-labels generated by GPT-4o-mini with incomplete prompt specifications
- Limited testing on truly novel UI paradigms beyond the training domains of web and mobile interfaces
- Token efficiency claims lack comprehensive comparative analysis across different input representations

## Confidence

- **Cross-domain generalization performance**: High confidence - results are comprehensive across three benchmarks with clear baselines
- **History summarization mechanism**: Medium confidence - effective in practice but underlying causal mechanism lacks ablation studies
- **Structured reasoning benefits**: Medium confidence - ablations show benefit, but individual contribution of progress estimation vs. decision reasoning is not isolated
- **RL training stability**: Medium confidence - GRPO implementation details are limited; cold-start necessity is demonstrated but optimal training schedule unclear

## Next Checks

1. **Generalization stress test**: Evaluate on held-out dataset with completely different UI paradigms (e.g., AR/VR interfaces or specialized enterprise software) to verify robust cross-domain transfer claims

2. **Token efficiency ablation**: Conduct controlled experiments varying input representation (full screenshots vs. compressed visual tokens vs. textual summaries) while holding model capacity constant to quantify precise tradeoff between token budget and performance

3. **Reasoning component isolation**: Design ablation study that removes progress estimation or decision reasoning individually to determine which component drives structured reasoning benefits