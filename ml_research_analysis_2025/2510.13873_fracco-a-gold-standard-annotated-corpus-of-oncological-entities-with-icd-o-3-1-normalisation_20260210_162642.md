---
ver: rpa2
title: 'FRACCO: A gold-standard annotated corpus of oncological entities with ICD-O-3.1
  normalisation'
arxiv_id: '2510.13873'
source_url: https://arxiv.org/abs/2510.13873
tags:
- were
- annotations
- corpus
- annotation
- expressions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces FRACCO, a gold-standard annotated corpus of
  French oncology clinical texts with ICD-O-3.1 normalisation. The dataset consists
  of 1,301 synthetic clinical cases translated from Spanish CANTEMIST corpus, containing
  71,127 expert-annotated entities across morphology, topography, differentiation,
  and composite expressions.
---

# FRACCO: A gold-standard annotated corpus of oncological entities with ICD-O-3.1 normalisation

## Quick Facts
- arXiv ID: 2510.13873
- Source URL: https://arxiv.org/abs/2510.13873
- Reference count: 20
- Primary result: Gold-standard annotated French oncology corpus with ICD-O-3.1 normalisation, containing 71,127 expert-annotated entities across 1,301 synthetic clinical cases

## Executive Summary
FRACCO is a French oncology corpus with ICD-O-3.1 normalisation, created by projecting Spanish CANTEMIST annotations onto machine-translated French text and manually validating the results. The corpus contains 71,127 annotations across four entity types: morphology, topography, differentiation, and composite expression_CIM codes. Annotations were produced through a hybrid approach combining automated projection with expert manual validation, achieving high inter-annotator agreement for span annotation (0.70-0.90 F1) but lower agreement for manual ICD-O normalisation (51.5%). The corpus was validated through NER model fine-tuning, achieving weighted F1 scores of approximately 89% across different BERT-based architectures, demonstrating its utility for machine learning applications in French oncology NLP.

## Method Summary
The corpus was created by translating 1,301 Spanish clinical cases from CANTEMIST into French using DeepL Pro, then projecting the original annotations onto the translated text through sentence segmentation and character-level alignment. This provided a baseline for manual correction, where annotators adjusted boundary shifts and rephrased medical terms. Annotations cover four entity types: morphology (histological type), topography (anatomical site), differentiation (tumour grade), and composite expression_CIM codes that combine multiple ICD-O elements. Normalisation was performed using a dictionary-based approach with manual review for unmatched expressions, and the dataset was validated through inter-annotator agreement studies and NER model fine-tuning experiments.

## Key Results
- Corpus contains 71,127 expert-annotated entities across 1,301 synthetic clinical cases
- Four entity types annotated: morphology, topography, differentiation, and composite expression_CIM codes
- Inter-annotator agreement: 0.70-0.90 F1 for span annotation, 80.65% concordance for automatic ICD-O matching, 51.5% for manual ICD-O assignment
- NER model fine-tuning achieved weighted F1 scores of approximately 89% across BERT-based architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hybrid annotation projection preserves semantic alignment across languages while enabling scalable corpus creation
- Mechanism: Spanish CANTEMIST annotations are projected onto French translations via sentence segmentation and character-level alignment, then manually corrected for boundary shifts and rephrased medical terms. This provides a pre-annotated baseline that reduces cold-start annotation effort.
- Core assumption: Machine translation preserves sufficient structural correspondence for annotation transfer; residual errors can be systematically corrected.
- Evidence anchors:
  - [abstract] "1,301 synthetic clinical cases, initially translated from the Spanish CANTEMIST corpus"
  - [section] "The projected annotations thus served as a baseline for the extended annotation work conducted in the subsequent phases" (Page 5)
  - [corpus] Neighbor papers show cross-lingual NLP methods achieve moderate success; no direct corpus evidence on projection accuracy rates.
- Break condition: Highly idiomatic or restructured translations where entity spans have no character-level correspondence; domain-specific abbreviations that translate unpredictably.

### Mechanism 2
- Claim: Composite expression normalisation enables higher-level semantic representation by combining atomic ICD-O components
- Mechanism: Individual annotations (morphologie, topographie, différenciation) are programmatically combined into expression_CIM codes when they meet simple composition criteria (one morphology, optionally one topography and/or differentiation). Complex or ambiguous expressions are flagged for manual review.
- Core assumption: Most clinical expressions follow compositional patterns that ICD-O can represent; edge cases are detectable and enumerable.
- Evidence anchors:
  - [abstract] "An additional annotation layer captures composite expression-level normalisations that combine multiple ICD-O elements"
  - [section] "Expressions meeting simple criteria... were automatically reconstructed from their components and assigned a composite annotation" (Page 6)
  - [corpus] No corpus evidence on proportion of automatically vs. manually constructed composites.
- Break condition: Expressions with multiple morphological components, context-dependent abbreviations, or concepts lacking direct ICD-O mappings (noted as ontology limitations in error analysis).

### Mechanism 3
- Claim: Dictionary-based normalisation with manual review achieves higher concordance than purely manual assignment
- Mechanism: Annotated spans are matched via exact string matching against an ICD-O terminology lexicon. Automatic matches achieve 80.65% inter-annotator concordance; manually assigned codes achieve only 51.5%, reflecting the difficulty of normalising expressions without direct lexical matches.
- Core assumption: Dictionary entries adequately cover common oncological terminology; unmatched expressions genuinely require expert interpretation.
- Evidence anchors:
  - [section] "In validating the automatic subset, the inter-annotator agreement on the sampled 1,060 reached an 80.65% concordance. By contrast, validation of the manually assigned annotations revealed lower agreement, at 51.5%" (Page 11)
  - [section] "approximately 55,000 annotations (≈78%) automatically [matched]. The remaining ~15,000 complex expressions were manually reviewed" (Page 11)
  - [corpus] Weak corpus support; neighbor papers do not directly address dictionary vs. manual normalisation tradeoffs.
- Break condition: Systematic terminology gaps where dictionary lacks clinically valid codes; expressions with legitimate multiple mappings.

## Foundational Learning

- Concept: **ICD-O-3 coding structure**
  - Why needed here: The entire corpus is normalised to ICD-O-3, which separates morphology (histology/behavior, e.g., 8140/3 for malignant adenocarcinoma) from topography (anatomical site, e.g., C34.9 for lung). Understanding this dual-axis system is essential for interpreting expression_CIM composite codes.
  - Quick check question: Given the code "C50.9 8500/3", which part indicates anatomy and which indicates histology?

- Concept: **BRAT standoff annotation format**
  - Why needed here: The corpus is distributed as .txt/.ann file pairs where annotations are stored separately from raw text with character offsets. Working with this format is prerequisite to any data access or conversion.
  - Quick check question: In standoff format, how would you modify an annotation if the underlying text changes length?

- Concept: **Span-level vs. token-level NER evaluation**
  - Why needed here: The paper reports both "soft" (partial overlap) and "hard" (exact match) F1 scores, with substantial differences (e.g., 0.82-0.90 soft vs. 0.70-0.90 hard). Understanding span matching is critical for interpreting benchmark results and reproducing experiments.
  - Quick check question: Why might a model achieve high soft F1 but lower hard F1 on clinical entity extraction?

## Architecture Onboarding

- Component map:
  CANTEMIST (Spanish) -> DeepL Pro translation -> FRASIMED French texts -> BRAT standoff annotations -> ICD-O-3.1 normalisation -> Zenodo dataset

- Critical path:
  1. Load .ann files using provided bratly-based scripts
  2. Extract entity spans and linked ICD-O codes via Note annotations
  3. For composite expressions, decompose to verify component consistency
  4. Convert to model-ready format (e.g., BIO tagging) for fine-tuning

- Design tradeoffs:
  - Synthetic clinical cases enable controlled annotation but may not capture real EHR variability (abbreviations, dictation errors)
  - Dictionary-first normalisation prioritises consistency but struggles with ontology gaps and multi-mappable expressions
  - Composite expression layer adds semantic richness but introduces normalisation complexity (51.5% manual concordance vs. 80.65% automatic)

- Failure signatures:
  - Low F1 on différenciation likely indicates span boundary inconsistency (e.g., "de grade 2" vs. "grade 2") rather than concept difficulty
  - Disagreement between annotators on manual normalisation often signals ICD-O lacks precise codes for the clinical concept
  - Mistranslated abbreviations left uncorrected will cause annotation misalignment (961 expressions were flagged and corrected)

- First 3 experiments:
  1. **Baseline reproduction**: Fine-tune CamemBERT-base on the provided train/test split using exact span+label matching; target ~89% weighted F1 to validate data pipeline.
  2. **Entity-type ablation**: Train separate models for morphologie, topographie, and différenciation to isolate performance drivers (différenciation has few examples but high regularity).
  3. **Normalisation error analysis**: Sample 100 manually normalised expressions with disagreement; categorise by ontology gap vs. semantic ambiguity to prioritise lexicon improvements.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Will models trained on FRACCO's synthetic clinical cases generalize effectively to authentic French clinical texts?
- Basis in paper: [explicit] The authors acknowledge that DeepL-translated expressions were sometimes "incorrectly rendered" and "not representative of authentic French clinical language," with 961 expressions requiring correction.
- Why unresolved: The corpus consists entirely of synthetic cases translated from Spanish; no evaluation was conducted on real clinical documents to assess transferability.
- What evidence would resolve it: Benchmark performance comparisons between models fine-tuned on FRACCO and tested on authentic French oncology EHR extracts.

### Open Question 2
- Question: Do the composite expression_CIM annotations meaningfully improve performance on downstream tasks such as relation extraction or document classification?
- Basis in paper: [explicit] The authors state these annotations "enable higher-level semantic analysis and facilitates use cases such as relation extraction and document classification" but only evaluated NER performance.
- Why unresolved: No experiments were conducted using the expression_CIM layer for any downstream tasks beyond entity recognition.
- What evidence would resolve it: Comparative experiments on relation extraction or document classification tasks, with and without leveraging the composite expression annotations.

### Open Question 3
- Question: How can manual ICD-O normalization agreement be improved beyond the observed 51.5% concordance?
- Basis in paper: [explicit] The validation revealed 80.65% agreement for automatically matched codes but only 51.5% for manually assigned codes, attributed to "ontology limitations," "semantic ambiguity," and "complex phrasing."
- Why unresolved: The paper documents the discrepancy but does not propose or test interventions to improve manual annotation consistency.
- What evidence would resolve it: Studies testing revised annotation guidelines, expanded code sets for edge cases, or annotator training interventions to measure agreement improvements.

### Open Question 4
- Question: Does the corpus's coverage of 399 morphology and 272 topography codes adequately represent real-world French oncological terminology?
- Basis in paper: [inferred] The paper notes that "resources specifically focused on oncology... remain scarce in the French language" but does not evaluate whether FRACCO's code coverage is sufficient for deployment in actual clinical settings.
- Why unresolved: No analysis compares the corpus's ICD-O code distribution against real French oncology report frequencies.
- What evidence would resolve it: Frequency analysis comparing FRACCO's code distribution to code occurrences in authentic French oncology departments.

## Limitations
- Synthetic clinical cases may not capture real-world EHR variability such as abbreviations, dictation errors, and varied clinical terminology
- Low inter-annotator concordance for manual ICD-O normalization (51.5%) indicates significant uncertainty in handling expressions without direct dictionary matches
- No empirical validation of annotation projection accuracy from Spanish to French, despite the methodology relying on cross-lingual alignment

## Confidence

**High Confidence**: The hybrid annotation approach combining automated projection with manual validation effectively reduces annotation effort while maintaining quality, as evidenced by the substantial annotation volume (71,127 entities) produced through this methodology. The BRAT standoff format specification and dataset availability through Zenodo are verifiable claims.

**Medium Confidence**: The NER model performance results (weighted F1 ~89.4%) are reproducible given the specified training methodology, though exact hyperparameter values and random seed selection remain unspecified. The inter-annotator agreement calculations appear methodologically sound based on the reported sampling procedures.

**Low Confidence**: The generalizability of findings to real clinical text environments is uncertain due to the synthetic nature of the dataset. The characterization of ontology gaps versus semantic ambiguity in manual normalization requires direct examination of the disagreed cases, which is not provided in the paper.

## Next Checks

1. **Annotation Projection Accuracy Assessment**: Sample 100 automatically projected annotations and measure the proportion requiring correction during manual validation. This would quantify the actual accuracy of the cross-lingual projection method beyond the reported 961 mistranslation corrections.

2. **Real-World Generalization Test**: Apply the trained NER models to a small sample of authentic French oncology clinical notes (from hospital databases) to measure performance degradation compared to synthetic test set results. This would quantify the impact of dataset synthetic nature on real-world applicability.

3. **Ontology Gap Analysis**: Categorize 100 disagreed manual normalization cases by type (ontology gap, semantic ambiguity, annotator error) to better understand the nature of the 51.5% concordance rate and inform lexicon improvement priorities.