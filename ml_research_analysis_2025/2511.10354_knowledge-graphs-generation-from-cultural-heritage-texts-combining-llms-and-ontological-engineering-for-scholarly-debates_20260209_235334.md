---
ver: rpa2
title: 'Knowledge Graphs Generation from Cultural Heritage Texts: Combining LLMs and
  Ontological Engineering for Scholarly Debates'
arxiv_id: '2511.10354'
source_url: https://arxiv.org/abs/2511.10354
tags:
- extraction
- knowledge
- scholarly
- entity
- sebi
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'ATR4CH provides a systematic methodology for converting complex
  scholarly discourse from Cultural Heritage texts into structured Knowledge Graphs
  using LLMs. The approach combines annotation models, ontological frameworks, and
  iterative development through five steps: foundational analysis, annotation schema
  development, pipeline architecture, integration refinement, and evaluation.'
---

# Knowledge Graphs Generation from Cultural Heritage Texts: Combining LLMs and Ontological Engineering for Scholarly Debates

## Quick Facts
- arXiv ID: 2511.10354
- Source URL: https://arxiv.org/abs/2511.10354
- Reference count: 30
- Primary result: ATR4CH achieves F1-scores of 0.96-0.99 for metadata extraction, 0.7-0.8 for entity recognition, 0.65-0.75 for hypothesis extraction, and 0.62 G-EVAL for discourse representation in Cultural Heritage knowledge extraction

## Executive Summary
ATR4CH presents a systematic methodology for converting complex scholarly discourse from Cultural Heritage texts into structured Knowledge Graphs using large language models. The approach combines annotation models, ontological frameworks, and iterative development through five steps: foundational analysis, annotation schema development, pipeline architecture, integration refinement, and evaluation. Validation on authenticity assessment debates achieved high performance across multiple extraction tasks, with smaller models demonstrating competitive results that enable cost-effective deployment for Cultural Heritage institutions.

## Method Summary
The ATR4CH methodology systematically converts scholarly debates from Cultural Heritage texts into Knowledge Graphs through a five-step process. It begins with foundational analysis of textual corpora and competency questions, followed by development of minimal working annotation schemas aligned with Core Ontological Patterns. The methodology implements a sequential pipeline with specialized components: metadata extraction, entity recognition using GliNER, entity linking via Wikidata, opinion extraction and classification, evidence mining, and hypothesis extraction. Each component produces structured JSON outputs conforming to predefined schemas, which are then mapped to RDF-star representations using the SEBI ontology. The approach was validated on Wikipedia articles covering authenticity disputes, with ground truth annotations providing performance benchmarks.

## Key Results
- Metadata extraction achieved F1-scores of 0.96-0.99 across Claude, GPT-4o-mini, and Llama models
- Entity recognition achieved F1-scores of 0.70-0.80, with GPT-4o-mini demonstrating superior recall (77.3%) over Claude (49.5%)
- Hypothesis extraction achieved F1-scores of 0.65-0.75, with evidence extraction accuracy of 0.95-0.97
- Discourse representation achieved 0.62 G-EVAL score, measuring alignment with ground truth interpretations
- Smaller models (GPT-4o-mini, Llama 3.3 70B) performed competitively with larger architectures, enabling cost-effective deployment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing extraction into a sequential pipeline with specialized components reduces error propagation compared to end-to-end approaches.
- Mechanism: ATR4CH implements a sequential pipeline (GliNER for NER → LLM for structured extraction → Rule-based entity linking) where each component produces validated JSON outputs. This modular design allows targeted optimization at each stage and constrains downstream tasks with structured intermediate representations.
- Core assumption: Intermediate JSON schemas provide sufficiently robust handoffs between pipeline stages to prevent catastrophic error cascades.
- Evidence anchors:
  - [abstract] "ATR4CH combines annotation models, ontological frameworks, and LLM-based extraction through iterative development... implementing a sequential pipeline with three LLMs."
  - [section 5.1] "This modular approach enables incremental extraction... facilitating debugging and targeted optimization while minimizing error propagation through robust intermediate representations."
  - [corpus] Weak/missing. Related papers discuss KG construction generally but lack comparative data on sequential vs. end-to-end architectures.
- Break condition: If intermediate JSON outputs become semantically inconsistent (e.g., entity IDs fail to propagate), the pipeline produces disconnected KGs.

### Mechanism 2
- Claim: Aligning annotation schemas directly with Core Ontological Patterns (COPs) before extraction improves KG fidelity.
- Mechanism: The methodology identifies essential KG patterns from Competency Questions and corpus analysis, then develops a Minimal Working Annotation (MWA) schema that maps directly to these patterns. LLM prompts are structured to produce JSON conforming to this schema.
- Core assumption: LLMs can reliably produce structured outputs (JSON) conforming to predefined schemas when provided with few-shot examples.
- Evidence anchors:
  - [abstract] "0.96-0.99 F1 for metadata extraction... 0.65-0.75 F1 for hypothesis extraction."
  - [section 3.2] "The identification process involves: (1) assessing alignment between Competency Questions, ontological structures, and available textual content... (4) selecting a manageable subset that forms the semantic backbone for KE."
  - [corpus] Weak/missing. No direct corpus evidence on annotation-ontology alignment efficacy.
- Break condition: If source texts lack sufficient information to populate the COPs, extraction fails regardless of schema alignment.

### Mechanism 3
- Claim: Smaller, cost-effective models can perform competitively with larger models within structured extraction pipelines.
- Mechanism: The structured task (producing specific JSON schemas) constrains the output space, reducing the need for broad generative capabilities of massive models. GPT-4o-mini achieves higher entity recognition recall than Claude Sonnet 3.7 despite fewer parameters.
- Core assumption: Extraction task complexity is bounded by schema structure, allowing smaller models to achieve sufficient performance with proper prompting.
- Evidence anchors:
  - [abstract] "Smaller models performed competitively with larger architectures, enabling cost-effective deployment."
  - [section 6.2] "GPT-4o-mini demonstrates superior entity recognition coverage, identifying 77.3% of scholarly agents present in the GT, significantly outperforming Claude (49.5%)."
  - [corpus] Weak/missing. Related papers focus on cultural awareness but don't compare model sizes for structured extraction.
- Break condition: If extraction requires deep semantic reasoning not captured by the schema, smaller models may underperform significantly.

## Foundational Learning
- Concept: RDF-star & Reification
  - Why needed here: SEBI ontology uses RDF-star to represent scholarly claims as quoted triples with contextual metadata.
  - Quick check question: Can you explain how RDF-star differs from standard RDF for representing statement-level metadata?
- Concept: Named Entity Recognition (NER)
  - Why needed here: Pipeline uses GliNER to identify scholarly agents ("Cognizers") with precise character-level spans.
  - Quick check question: What is the output format of an NER system and why does span precision matter for downstream linking?
- Concept: Competency Questions (CQs)
  - Why needed here: CQs drive ontology design and extraction prioritization throughout the methodology.
  - Quick check question: How do CQs differ from natural language queries in guiding KG construction?

## Architecture Onboarding
- Component map: GliNER → LLM Pipeline (Claude/GPT/Llama) → Rule-based Entity Linker → JSON-to-RDF Mapper
- Critical path: Raw Text → Metadata Extraction → Cognizer ID (GliNER) → Entity Linking → Opinion Extraction → Evidence Mining → Hypothesis Extraction → RDF-star Mapping
- Design tradeoffs:
  - **Precision vs. Recall (Models):** Claude (higher precision, F1=0.728) vs. GPT-4o-mini (higher recall, F1=0.803). Higher recall preferred when KGs undergo human review.
  - **Cost vs. Performance:** GPT-4o-mini/Llama achieve competitive results at ~5-20x lower cost than Claude.
  - **Automation vs. Human-in-the-loop:** Paper explicitly states "human oversight is necessary during post-processing"—full automation not yet viable.
- Failure signatures:
  - **Error Propagation:** Incorrect Cognizer identification (Step 2) produces empty/incorrect downstream outputs—most out-of-GT evidence extractions are empty or incorrect.
  - **Wikidata Dependency:** Failed entity linking reduces biographical context and disambiguation accuracy.
  - **Hallucination:** LLMs may extract non-existent opinions, especially with long-distance dependencies.
- First 3 experiments:
  1. **Single document end-to-end test:** Process one Wikipedia article through full pipeline with default model (GPT-4o-mini). Verify JSON outputs at each step match schemas in Section 5.1.
  2. **Cognizer ID bottleneck analysis:** Run GliNER + Cognizer classification prompt on 5 documents with both Claude and GPT-4o-mini. Measure precision/recall against manual annotations to identify error sources.
  3. **RDF mapping validation:** Take JSON output from Step 2, apply mapping algorithm (Listing 1), load into GraphDB. Execute SPARQL queries from Listing 22 to verify COP satisfaction and triple counts.

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How does the ATR4CH methodology perform when extracting knowledge from primary scholarly literature rather than the secondary Wikipedia sources used in validation?
- Basis in paper: [explicit] The authors note in Section 7.6 that "Performance on primary scholarly literature remains untested" and suggest working with primary works is a relevant future direction.
- Why unresolved: The study relied on Wikipedia articles which have a specific encyclopedic structure; primary sources (e.g., peer-reviewed papers, diplomatic editions) contain denser, more complex academic discourse.
- What evidence would resolve it: Applying the pipeline to a corpus of peer-reviewed journals or historical diplomatic editions and comparing the resulting F1-scores for evidence and hypothesis extraction against the Wikipedia baseline.

### Open Question 2
- Question: Can the ATR4CH pipeline be effectively adapted for multilingual Cultural Heritage texts without significant loss of accuracy?
- Basis in paper: [explicit] Section 7.6 identifies the "current focus on English Wikipedia sources" as a limitation that hinders "multilingual applicability," which is critical given the "glocal nature of CH scholarship."
- Why unresolved: The implemented pipeline utilizes English-centric tools (GliNER) and prompts, and has not been validated on non-English discourse structures.
- What evidence would resolve it: Testing the methodology on non-English Wikipedia articles covering the same authenticity debates and evaluating the cross-lingual alignment of the extracted Knowledge Graphs.

### Open Question 3
- Question: Can targeted fine-tuning resolve the "Cognizer classification" bottleneck that causes error propagation in downstream tasks?
- Basis in paper: [explicit] Section 7.6 lists "Cognizer classification difficulty" as a key bottleneck and states that future work will prioritize "targeted improvements for Cognizer identification through fine-tuning or hybrid approaches."
- Why unresolved: The current zero-shot approach forces a trade-off between precision and recall; misclassifying entities as Cognizers (False Positives) leads to the generation of incorrect evidence and hypotheses in later pipeline stages.
- What evidence would resolve it: A comparative study where a smaller model is fine-tuned specifically for the "is cognizer" classification task, measuring the reduction in error propagation rates and improvement in G-EVAL scores.

### Open Question 4
- Question: How robust is the extraction pipeline when processing Cultural Heritage debates with low coverage in Wikidata?
- Basis in paper: [inferred] Section 7.6 mentions a "dependency on Wikidata linking for optimal performance," while Section 5.1.3 describes the entity resolution strategy as relying heavily on the Wikibase API.
- Why unresolved: Historical figures involved in obscure authenticity debates often lack Wikidata entries, potentially forcing the system to rely on fragile string-matching heuristics or create isolated local nodes.
- What evidence would resolve it: Evaluating the pipeline on a dataset of "long-tail" historical forgeries with minimal Wikidata presence to measure the degradation in entity resolution and overall KG coherence.

## Limitations
- Methodological scope limitations: The ATR4CH methodology demonstrates effectiveness specifically for Cultural Heritage authenticity debates but lacks validation across other scholarly discourse domains.
- Ontology dependency: The methodology is tightly coupled with the SEBI ontology's RDF-star structure, creating dependencies that may limit adaptability to domains requiring different ontological patterns.
- Annotation burden: The iterative methodology requires substantial human annotation effort and domain expertise, potentially constraining scalability for institutions with limited curatorial staff.

## Confidence
- High confidence: The sequential pipeline architecture effectively reduces error propagation; smaller models can achieve competitive performance for structured extraction; the methodology successfully coordinates LLM-based extraction with the SEBI ontology.
- Medium confidence: The generalizability across different Cultural Heritage sub-domains; the long-term maintenance of annotation schemas; the specific performance trade-offs between models may shift with different prompts or datasets.
- Low confidence: Claims about "systematic" applicability across all Cultural Heritage text types without further validation; assertions about minimal human oversight requirements given observed error rates.

## Next Checks
1. **Cross-domain validation study:** Apply the ATR4CH methodology to a different scholarly discourse domain (e.g., art historical attribution debates or archaeological interpretation) with 10-15 new texts. Compare performance metrics and identify domain-specific adaptations needed.

2. **Pipeline ablation analysis:** Systematically disable each pipeline component (metadata, entity, opinion, evidence, hypothesis) and measure error propagation effects. Quantify the contribution of each stage to overall KG quality and identify critical failure points.

3. **Model architecture stress test:** Test the methodology with increasingly complex extraction schemas (adding nested relationships, temporal reasoning, or multi-document integration) using the same model families. Determine whether the observed size-performance relationship holds under schema complexity increases.