---
ver: rpa2
title: Deep Jump Gaussian Processes for Surrogate Modeling of High-Dimensional Piecewise
  Continuous Functions
arxiv_id: '2510.21974'
source_url: https://arxiv.org/abs/2510.21974
tags:
- local
- gaussian
- latent
- projection
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Deep Jump Gaussian Processes (DJGP) is introduced to model piecewise
  continuous, high-dimensional system responses. DJGP integrates region-specific locally
  linear projections into Jump Gaussian Processes, allowing adaptation to local subspace
  structures while capturing abrupt changes.
---

# Deep Jump Gaussian Processes for Surrogate Modeling of High-Dimensional Piecewise Continuous Functions

## Quick Facts
- **arXiv ID:** 2510.21974
- **Source URL:** https://arxiv.org/abs/2510.21974
- **Reference count:** 40
- **Primary result:** DJGP outperforms baselines in RMSE and CRPS for high-dimensional piecewise continuous functions while providing reliable uncertainty quantification.

## Executive Summary
This paper introduces Deep Jump Gaussian Processes (DJGP), a novel surrogate modeling framework for high-dimensional piecewise continuous functions with discontinuities. DJGP combines region-specific locally linear projections with Jump Gaussian Processes, enabling dimension reduction while preserving piecewise structure. A Gaussian Process prior on projection matrices ensures smooth evolution across input space, and scalable variational inference jointly optimizes projections and hyperparameters. The method demonstrates superior predictive performance and uncertainty quantification compared to existing baselines on both synthetic and real-world datasets.

## Method Summary
DJGP integrates local linear projections with Jump Gaussian Processes to model piecewise continuous, high-dimensional system responses. For each test location, DJGP learns a region-specific projection matrix W that maps D-dimensional inputs to K-dimensional latent space, where a GP prior ensures smooth evolution of projections. The JGP component handles discontinuities by partitioning local data into in-regime and out-of-regime groups using binary latent variables. Two-layer variational inference with inducing points enables scalable training, optimizing both projection matrices and JGP hyperparameters via ELBO maximization. The framework is transductive, fitting separate models at each test location using local neighborhoods.

## Key Results
- DJGP outperforms baseline methods (GP, JGP, Deep GP) in RMSE and CRPS on synthetic datasets (L2, LH) and UCI benchmarks (Wine Quality, Parkinsons, Appliances Energy)
- Performance remains stable as dimensionality increases, demonstrating robustness to the curse of dimensionality
- DJGP provides reliable uncertainty quantification, with prediction intervals capturing true values even in the presence of discontinuities
- Optimal hyperparameters identified: latent dimension Q ∈ [3,7], neighborhood size n ∈ [25,35], inducing points (L₁, L₂) = (4,40)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Local linear projections with GP priors enable dimension reduction while preserving piecewise structure.
- **Mechanism:** DJGP learns region-specific projection matrices W that map high-dimensional inputs to lower-dimensional latent space, with GP priors ensuring smooth evolution across input space.
- **Core assumption:** Unknown function has nonlinear sufficient dimension reduction: response depends on x only through z = g(x), where g is continuously differentiable.
- **Evidence anchors:** Abstract states projections evolve smoothly; Section 3.1 places GP prior on local projection matrices.
- **Break condition:** Fails if g is discontinuous or if local neighborhoods are too sparse to estimate W reliably.

### Mechanism 2
- **Claim:** Jump GP partitioning handles discontinuities by explicitly separating in-regime and out-of-regime local data.
- **Mechanism:** JGP introduces binary latent variables v_i indicating regime membership, with linear boundary approximation for local partitioning.
- **Core assumption:** Region boundaries are sufficiently smooth to be locally approximated by linear functions.
- **Evidence anchors:** Section 2.2 describes explicit division of local data by regime shifts; Section 3.1 discusses local linear boundary approximation.
- **Break condition:** Fails with highly irregular boundaries or when local neighborhoods span multiple discontinuities.

### Mechanism 3
- **Claim:** Two-layer variational inference jointly learns projections and JGP hyperparameters with computational tractability.
- **Mechanism:** Uses inducing points at both layers - local inducing variables for function values and global inducing variables for projection matrices.
- **Core assumption:** Variational posterior factorization adequately approximates true posterior; inducing points capture essential GP structure.
- **Evidence anchors:** Section 3.2 introduces two sets of inducing variables; Equation 21 shows closed-form ELBO for gradient-based optimization.
- **Break condition:** ELBO optimization may not correlate with RMSE; insufficient inducing points cause approximation error.

## Foundational Learning

- **Concept: Stationary Gaussian Processes**
  - Why needed here: DJGP builds on GP regression in latent space; understanding covariance kernels and predictive distributions is essential.
  - Quick check question: Can you derive the posterior predictive mean and variance for a GP with SE kernel given training data?

- **Concept: Variational Inference with Inducing Points**
  - Why needed here: Core scalability mechanism for DJGP; must understand ELBO, KL divergence, and sparse GP approximations.
  - Quick check question: How does adding inducing points reduce computational complexity from O(N³) in standard GPs?

- **Concept: Local vs Transductive Learning**
  - Why needed here: JGP/DJGP fit separate models at each test location (transductive), unlike standard GP (inductive).
  - Quick check question: Why does transductive learning require local data selection for each test point?

## Architecture Onboarding

- **Component map:** Input x → GP prior on W → Latent z = Wx → Gating ν → Partition D*/D° → Local GP → Prediction

- **Critical path:**
  1. Initialize inducing points (global near training mean, local randomly)
  2. Optimize ELBO jointly over variational parameters, inducing inputs, hyperparameters
  3. For prediction: sample W from q(W), fit local JGP, Monte Carlo aggregate

- **Design tradeoffs:**
  - Neighborhood size n: Too small → overfitting; too large → violates local linearity. Paper suggests n ∈ [25, 35].
  - Inducing points (L₁, L₂): Paper found (4, 40) works well; more points help but with diminishing returns.
  - Latent dimension Q: Paper suggests Q ∈ [3, 7] via cross-validation; need not match true K.

- **Failure signatures:**
  - ELBO improves but validation RMSE increases → overfitting to variational bound
  - High variance across MC samples → projection posterior too uncertain; increase L₂
  - Poor performance near boundaries → neighborhood spans discontinuity; reduce n

- **First 3 experiments:**
  1. Reproduce L2 dataset (2D latent) with Random Projection expansion to verify basic functionality and compare DJGP vs JGP.
  2. Ablation on (L₁, L₂): Grid search {2,4,6} × {20,40,60} on LH dataset; confirm (4,40) near-optimal.
  3. Sensitivity to latent dimension Q: Fix true K=5, vary Q ∈ {2,3,5,7} to understand mismatch tolerance.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can a robust stopping criterion for variational training be developed that correlates with predictive accuracy (RMSE) without relying on a held-out validation set?
- **Basis in paper:** The conclusion states that ELBO does not directly translate into RMSE improvements and using validation sets increases runtime.
- **Why unresolved:** Paper identifies lack of reliable stopping criterion as limitation, noting ELBO may improve while validation RMSE increases.
- **What evidence would resolve it:** Theoretical bound or heuristic metric derivable from ELBO trajectory that reliably signals convergence of predictive performance.

### Open Question 2
- **Question:** How can the optimal latent dimension (Q) be determined adaptively or theoretically, rather than empirically via cross-validation?
- **Basis in paper:** Section 5.4.2 notes optimal Q does not necessarily coincide with ground-truth dimension K and requires cross-validation.
- **Why unresolved:** Paper provides practical guidance for Q selection but lacks theoretical principle for automatic determination.
- **What evidence would resolve it:** Automated method for selecting Q that matches or exceeds performance of manually tuned cross-validation.

### Open Question 3
- **Question:** How does DJGP performance scale in extremely high-dimensional regimes (e.g., D ≈ 500) and massive datasets (e.g., N ≈ 10⁵)?
- **Basis in paper:** Conclusion acknowledges current evaluation does not fully cover extremely high-dimensional and massive-data regimes.
- **Why unresolved:** Experiments limited to moderate dimensions (D ≤ 50), leaving scalability in significantly higher dimensions unverified.
- **What evidence would resolve it:** Successful application on datasets with dimensionality and volume matching specified high-end regime.

## Limitations

- The effectiveness of two-layer variational inference lacks ablation studies on key design choices like inducing point numbers and neighborhood sizes.
- "ELBO hacking" phenomenon indicates variational objective may not reliably correlate with predictive performance, especially with discontinuities.
- The assumption of smooth regime boundaries for local linear approximation is not empirically validated for highly irregular discontinuities.

## Confidence

- **High Confidence:** Local linear projections with GP priors for dimension reduction mechanism is well-supported by mathematical formulation and established GP theory.
- **Medium Confidence:** Jump GP partitioning mechanism is theoretically sound but robustness to non-smooth boundaries requires further validation.
- **Medium Confidence:** Two-layer variational inference framework is novel and tractable, but sensitivity to hyperparameters and potential for "ELBO hacking" warrant caution.

## Next Checks

1. **Ablation Study on Inducing Points:** Conduct systematic ablation varying (L₁, L₂) across {2,4,6,8} × {20,40,60,80} on LH dataset to quantify impact on both ELBO and RMSE, explicitly testing for "ELBO hacking."

2. **Robustness to Boundary Irregularity:** Generate synthetic datasets with increasingly irregular boundaries (higher-order polynomial or fractal) and evaluate DJGP performance degradation compared to baselines.

3. **Cross-Validation on Real-World Data:** Apply DJGP to additional real-world datasets with known or suspected discontinuities (materials science with phase transitions) using nested cross-validation to compare predictive accuracy and uncertainty quantification.