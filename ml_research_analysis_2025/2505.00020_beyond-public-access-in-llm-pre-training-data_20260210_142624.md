---
ver: rpa2
title: Beyond Public Access in LLM Pre-Training Data
arxiv_id: '2505.00020'
source_url: https://arxiv.org/abs/2505.00020
tags:
- data
- auroc
- training
- book
- books
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates whether OpenAI\u2019s GPT models were\
  \ trained on non-public, copyrighted O\u2019Reilly Media books. Using the DE-COP\
  \ membership inference attack, we tested GPT-4o, GPT-4o Mini, and GPT-3.5 Turbo\
  \ on 13,962 paragraphs from 34 books, distinguishing between publicly accessible\
  \ preview content and paywalled content."
---

# Beyond Public Access in LLM Pre-Training Data

## Quick Facts
- arXiv ID: 2505.00020
- Source URL: https://arxiv.org/abs/2505.00020
- Authors: Sruly Rosenblat; Tim O'Reilly; Ilan Strauss
- Reference count: 40
- Key outcome: GPT-4o shows strong recognition of non-public O'Reilly books (82% AUROC) vs. public content (64% AUROC), suggesting non-public data in training

## Executive Summary
This study investigates whether OpenAI's GPT models were trained on non-public, copyrighted O'Reilly Media books. Using the DE-COP membership inference attack, the researchers tested GPT-4o, GPT-4o Mini, and GPT-3.5 Turbo on 13,962 paragraphs from 34 books, distinguishing between publicly accessible preview content and paywalled content. AUROC scores show GPT-4o strongly recognizes non-public book content (82% AUROC), far outperforming its recognition of public content (64% AUROC), while GPT-3.5 Turbo shows the opposite pattern. GPT-4o Mini shows no knowledge of either type of content (AUROC ≈ 50%). These findings suggest that non-public, paywalled data plays an increasingly important role in training more recent and capable models, highlighting the need for corporate transparency in AI training data sources.

## Method Summary
The study applies the DE-COP membership inference attack method using a dataset of 34 copyrighted O'Reilly Media books. Researchers extracted 13,962 paragraphs from these books, labeling them as public (preview content) or non-public (paywalled). They generated three machine paraphrases per paragraph using Claude 3.5 Sonnet. Models were tested with multiple-choice quizzes containing the original paragraph and three paraphrases, with AUROC scores measuring recognition ability between content published before and after each model's training cutoff date.

## Key Results
- GPT-4o shows 82% AUROC for non-public content vs. 64% for public content
- GPT-3.5 Turbo shows 46% AUROC for non-public vs. 55% for public content
- GPT-4o Mini shows no content recognition (≈50% AUROC for both types)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: If an LLM reliably identifies original human-authored text among paraphrased alternatives, it may have been trained on that text.
- Mechanism: The DE-COP attack presents models with a multiple-choice quiz containing one human-authored paragraph and three machine-generated paraphrases. Higher "guess rates" on books published before the training cutoff (vs. after) indicate prior exposure.
- Core assumption: Models that have seen text during training can better recognize it than text they have never encountered.
- Evidence anchors:
  - [abstract] "Using a legally obtained dataset of 34 copyrighted O'Reilly Media books, we apply the DE-COP membership inference attack method"
  - [section 2.2] "This works by quizzing an LLM with a multiple choice test to see whether it can identify original human-authored O'Reilly book paragraphs from machine-generated paraphrased alternatives"
  - [corpus] Related work (Duarte et al. DE-COP paper cited as [10]) establishes the foundational method
- Break condition: When models become so capable at distinguishing human vs. AI text generally (baseline guess rate >96%), the signal disappears.

### Mechanism 2
- Claim: AUROC scores above 50% indicate discriminatory ability between content the model was potentially trained on versus content it could not have seen.
- Mechanism: DE-COP guess rates are aggregated per book, then AUROC measures separability between the "potentially in-dataset" and "known out-of-dataset" distributions. Scores near 50% = random chance; higher = stronger evidence of training exposure.
- Core assumption: The temporal split (before/after training cutoff) correctly approximates in-sample vs. out-of-sample data.
- Evidence anchors:
  - [abstract] "AUROC scores show GPT-4o strongly recognizes non-public book content (82% AUROC)"
  - [section 2.2] "A high AUROC score, therefore, implies that the model was trained on many of the books in our dataset published prior to the model's cutoff date"
  - [corpus] Weak direct corpus evidence; related membership inference papers (cited [18], [34], [40]) provide methodological precedent but not validation of this specific AUROC threshold interpretation
- Break condition: Small sample sizes produce wide bootstrapped confidence intervals, reducing certainty (see Table 5 book-level CIs).

### Mechanism 3
- Claim: Stronger recognition of paywalled content than public content within the same books suggests access violations.
- Mechanism: O'Reilly books uniquely contain both publicly accessible preview content and paywalled content. Comparing AUROC scores across these subsets controls for book-specific factors while isolating access method.
- Core assumption: Public content would naturally appear more often in training corpora via legitimate scraping; non-public content requires circumventing access restrictions.
- Evidence anchors:
  - [abstract] "GPT-4o exhibits far stronger recognition of non-public O'Reilly book content compared to publicly accessible samples, with AUROC scores of 82% (non-public) vs 64% (public)"
  - [section 3] "We would expect the opposite, since public data is more easily accessible and repeated across the internet"
  - [corpus] No corpus papers validate this specific public/private differential approach
- Break condition: Public preview text may be more formulaic (lower perplexity), making it less memorable regardless of training exposure.

## Foundational Learning

- Concept: **Membership Inference Attacks (MIA)**
  - Why needed here: The entire detection method relies on understanding how MIAs probe whether specific data was in a training set.
  - Quick check question: Can you explain why comparing model confidence on known-member vs. known-non-member data reveals training exposure?

- Concept: **AUROC (Area Under Receiver Operating Characteristic)**
  - Why needed here: Interpreting results requires understanding what AUROC measures and what 50%, 64%, and 82% scores mean in terms of classification ability.
  - Quick check question: If a model achieves 50% AUROC on a dataset, what does that imply about its ability to distinguish between the two classes?

- Concept: **Temporal Bias in ML Evaluation**
  - Why needed here: The paper explicitly addresses how time-based data splits can confound membership inference by introducing distributional shifts unrelated to training exposure.
  - Quick check question: Why does testing GPT-4o and GPT-4o Mini (same cutoff, different results) help rule out temporal bias as an explanation?

## Architecture Onboarding

- Component map:
  Data layer: 34 O'Reilly books → paragraph extraction → public/non-public labeling → temporal split by model cutoff dates
  Paraphrase generation: Claude 3.5 Sonnet produces 3 paraphrases per paragraph (temperature=0.1)
  DE-COP testing: Target model receives 24 permutations per paragraph (4 options × 6 positions), outputs logprobs for A/B/C/D
  Aggregation: Paragraph-level guess rates → book-level means → AUROC calculation across books

- Critical path:
  1. Exclude books published during cutoff year (prevents label contamination)
  2. Generate paraphrases that preserve meaning while changing surface form
  3. Query models with forced-choice format (logit_bias restricts output to A/B/C/D)
  4. Calculate book-level AUROC using pre/post cutoff as positive/negative classes

- Design tradeoffs:
  - Book-level vs. paragraph-level AUROC: Book-level shows stronger signal but wider CIs due to small N=34; paragraph-level tighter CIs but noisier
  - 24 permutations vs. fewer: Eliminates position bias but increases API costs 24×
  - Claude 3.5 Sonnet vs. other paraphrasers: Higher-quality paraphrases make the task harder (conservative)

- Failure signatures:
  - AUROC ≈ 50% on all splits: Either no training exposure OR model too small to memorize (see GPT-4o Mini)
  - AUROC ≈ 100%: Ceiling effect; baseline guess rate approaching 100% means discrimination becomes impossible
  - Wide book-level CIs: Sample size insufficient; aggregate signal exists but individual book classification unreliable

- First 3 experiments:
  1. Replicate on a single book with known training status (if available) to validate AUROC threshold calibration
  2. Test whether paragraph length correlates with detection accuracy (shorter paragraphs may be less distinctive)
  3. Compare results when paraphrasing with different models (e.g., GPT-4 vs. Claude) to assess paraphrase-quality sensitivity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the patterns of training on non-public data observed in OpenAI's models generalize to other major foundation model developers (e.g., Anthropic, Meta)?
- Basis in paper: [explicit] The conclusion states the findings are specific to OpenAI but "likely a systemic issue," aiming to "provoke changes... across AI model developers."
- Why unresolved: The study only tested one provider (OpenAI) using one publisher (O'Reilly).
- What evidence would resolve it: Applying the DE-COP methodology to other leading closed-source and open-source models using diverse copyrighted datasets.

### Open Question 2
- Question: How can membership inference methods be adapted to accurately audit smaller models with lower memorization capacity or future highly capable models where baseline recognition rates approach ceiling?
- Basis in paper: [explicit] Section 3.1 notes "Smaller models are harder to test accurately" and that once baseline guess rates exceed 96%, the difference between member and non-member content "could become undetectable."
- Why unresolved: Current techniques rely on the model's ability to memorize and distinguish text, which degrades in smaller models and may saturate in future highly intelligent models.
- What evidence would resolve it: Development of new statistical techniques that do not rely solely on the model's ability to distinguish human vs. machine text.

### Open Question 3
- Question: Through what specific acquisition channels (e.g., LibGen, Books3, vs. benign user queries) did the paywalled O'Reilly content enter the training data?
- Basis in paper: [explicit] The introduction and findings speculate that violations "might have occurred via the LibGen database... or from Books3," but also note familiarity could be acquired via benign "user queries."
- Why unresolved: The study confirms the *presence* of knowledge but cannot definitively identify the *source* or ingestion method.
- What evidence would resolve it: Correlating specific dataset provenance records provided by OpenAI or identifying unique contamination patterns specific to known pirated databases.

## Limitations

- Temporal Cutoff Reliability: Method relies on accurate training cutoff dates, but exact dates cannot be verified
- Sample Size: Only 34 books tested, creating wide confidence intervals at book level
- Access Method Attribution: Cannot definitively prove access violations vs. alternative legitimate distribution channels

## Confidence

**Claim Cluster 1: GPT-4o was trained on non-public O'Reilly content (82% AUROC)** - **High confidence**
Evidence: Consistent AUROC across multiple validation approaches, AUROC substantially above chance (50%), and higher than public content recognition (64%).

**Claim Cluster 2: GPT-3.5 Turbo shows opposite pattern (46% vs 55% AUROC)** - **Medium confidence**
Evidence: Results exist but with wider confidence intervals; pattern is less pronounced than GPT-4o.

**Claim Cluster 3: GPT-4o Mini shows no content recognition (≈50% AUROC)** - **High confidence**
Evidence: Results consistently near random chance across all tests, suggesting either no training exposure or insufficient capacity to memorize.

## Next Checks

1. **Cross-Model Validation**: Apply the same DE-COP methodology to models with publicly documented training data (e.g., models trained exclusively on public datasets) to establish baseline AUROC values and verify that the method correctly identifies absence of training exposure.

2. **Temporal Robustness Test**: Conduct the same analysis on books where the publication date is definitively before and after training cutoffs (using books from different years with large temporal gaps) to validate that the temporal split effectively captures training exposure vs. absence.

3. **Access Method Ground Truth**: Identify books where the non-public content was legitimately available through alternative channels (e.g., academic partnerships, leaked datasets) and test whether these show similar recognition patterns to truly paywalled content, helping isolate the signal from the noise.