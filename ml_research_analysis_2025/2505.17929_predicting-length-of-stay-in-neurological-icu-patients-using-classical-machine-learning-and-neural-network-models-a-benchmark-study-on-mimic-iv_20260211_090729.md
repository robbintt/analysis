---
ver: rpa2
title: 'Predicting Length of Stay in Neurological ICU Patients Using Classical Machine
  Learning and Neural Network Models: A Benchmark Study on MIMIC-IV'
arxiv_id: '2505.17929'
source_url: https://arxiv.org/abs/2505.17929
tags:
- data
- patients
- which
- patient
- care
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated multiple machine learning approaches for predicting
  ICU length of stay (LOS) in neurological patients using the MIMIC-IV dataset. The
  research compared classic ML algorithms (K-Nearest Neighbors, Random Forest, XGBoost,
  and CatBoost) with neural network models (LSTM, BERT, and Temporal Fusion Transformer)
  on both static admission data and time-series patient monitoring data.
---

# Predicting Length of Stay in Neurological ICU Patients Using Classical Machine Learning and Neural Network Models: A Benchmark Study on MIMIC-IV

## Quick Facts
- **arXiv ID**: 2505.17929
- **Source URL**: https://arxiv.org/abs/2505.17929
- **Reference count**: 0
- **Primary Result**: Random Forest achieved 0.68 accuracy on static admission data; BERT achieved 0.80 accuracy on time-series data

## Executive Summary
This study benchmarks multiple machine learning approaches for predicting ICU length of stay (LOS) in neurological patients using the MIMIC-IV dataset. The research compares classical ML algorithms (K-Nearest Neighbors, Random Forest, XGBoost, and CatBoost) with neural network models (LSTM, BERT, and Temporal Fusion Transformer) on both static admission data and time-series patient monitoring data. Random Forest achieved the best performance on static data with an accuracy of 0.68, while BERT outperformed LSTM on time-series data with an accuracy of 0.80. The study provides insights into the applicability of ML techniques for improving ICU resource management and patient care in neurological settings.

## Method Summary
The study filtered MIMIC-IV patients using ICD codes (I61%, I63%, G41%) to identify neurological cases. Static features were extracted from admissions, patients, and diagnoses tables, while time-series features came from chartevents. The LOS was binned into three classes: short (<2 days), medium (3-7 days), and long (>7 days). Static models used SMOTE for class balancing, RFE for feature selection, and Optuna for hyperparameter tuning. Time-series models employed sliding windows (BERT: size 512, step 32) with linear interpolation for missing values. Models were evaluated using accuracy, precision, recall, and F1-score metrics.

## Key Results
- Random Forest achieved 0.68 accuracy, 0.68 precision, 0.68 recall, and 0.67 F1-score on static admission data
- BERT achieved 0.80 accuracy, 0.80 precision, 0.80 recall, and 0.80 F1-score on time-series data
- Top predictive features included ventilator modes, Richmond-RAS Scale, and CAM-ICU assessments

## Why This Works (Mechanism)

### Mechanism 1
- Tree ensemble methods outperform other classical ML approaches for static admission data in neurological ICU LOS prediction
- Random Forest aggregates multiple decision trees trained on bootstrap samples with feature randomization, reducing overfitting while capturing non-linear feature interactions common in clinical data
- Core assumption: Patient demographic and admission-time clinical features contain sufficient signal for coarse LOS stratification without temporal dynamics
- Evidence anchors: Random Forest achieved 0.68 accuracy; outperformed other models at every metric; similar performance in cardiac and diabetes LOS studies suggests cross-domain robustness

### Mechanism 2
- Transformer-based architectures capture longer-range temporal dependencies in clinical time-series better than sequential LSTM models
- BERT's bidirectional self-attention processes all time steps in parallel, allowing direct modeling of dependencies between distant clinical events without sequential bottleneck
- Core assumption: Clinical events separated by many hours/days contain predictive relationships that sequential models fail to propagate through their hidden states
- Evidence anchors: BERT outperformed LSTM with 0.80 F1-score vs 0.63; bidirectional attention learns different relationship types simultaneously

### Mechanism 3
- Domain-specific feature selection based on neurological clinical knowledge improves model interpretability and identifies actionable clinical factors
- Recursive Feature Elimination combined with permutation importance isolates clinically meaningful predictors (ventilator modes, sedation protocols, delirium assessments)
- Core assumption: Features important for prediction correlate with causal factors modifiable by clinical intervention
- Evidence anchors: Top predictors identified ventilator modes, Richmond-RAS Scale, and CAM-ICU assessments; these clinically validated by cited studies linking these to ICU outcomes

## Foundational Learning

- **Bootstrap aggregating (bagging) and feature randomization in Random Forest**
  - Why needed here: Explains why Random Forest generalizes better than single decision trees on high-dimensional clinical data with 332+ features
  - Quick check question: Can you explain why training trees on different feature subsets reduces correlation between individual tree predictions?

- **Self-attention mechanism and positional encoding**
  - Why needed here: Understanding BERT's architectural advantage over LSTM requires grasping how attention computes relationships between all sequence positions simultaneously
  - Quick check question: Why does self-attention require explicit positional encoding when LSTM does not?

- **Sliding window for time-series data augmentation**
  - Why needed here: Converting irregular clinical events into fixed-length training examples requires understanding window/step size tradeoffs between sample quantity and temporal coverage
  - Quick check question: If you increase step size from 32 to 256 with window size 1024, what happens to your training dataset size and why might model performance degrade?

## Architecture Onboarding

- **Component map**:
  Raw MIMIC-IV tables → Neurological patient filter (ICD codes) → Data marts → Static path: admissions + patients + diagnoses → SMOTE → RFE → Classical ML; Time-series path: chartevents → Test filtering → Windowing → LSTM/BERT

- **Critical path**: The ICD filtering step (I61%, I63%, G41%) defines your entire patient cohort; errors here propagate to all downstream models. Verify filtered patient counts against expected neurological admission rates before proceeding.

- **Design tradeoffs**:
  - Larger window sizes (1024) improve LSTM accuracy but reduce training samples; BERT optimal at 512 suggesting attention mechanism extracts signal from shorter contexts
  - SMOTE addresses class imbalance but may introduce synthetic artifacts near decision boundaries (patients with LOS ≈2 days misclassified more often)
  - TFT's theoretical advantages (static + temporal fusion, interpretability) failed due to computational constraints requiring 90× longer per epoch

- **Failure signatures**:
  - Poor performance on "medium" class (3-7 days): Indicates feature distributions near class boundaries lack discriminative power
  - anchor_year appearing in feature importance: Suggests data leakage or temporal artifacts rather than clinical signal
  - TFT at 0.37 accuracy with limited epochs: Model complexity exceeds available training budget

- **First 3 experiments**:
  1. Replicate static Random Forest pipeline on admission data with 5-fold cross-validation; verify accuracy within ±0.02 of reported 0.68 before attempting time-series approaches
  2. Ablation study on window sizes (256, 512, 1024) with fixed step=32 for LSTM to confirm reported performance trajectory before investing in BERT implementation
  3. Feature importance validation: Train models with top-10 vs. all features; if performance gap <5%, feature selection may be overfitting to validation set rather than capturing true signal

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would reformulating the LOS prediction task as a regression problem improve accuracy for patients whose stays fall near classification boundaries?
- Basis in paper: The authors state: "One way to address this issue is to revert problem back from classification to regression" when discussing class binning problems, noting that "a significant amount of patients in the used dataset had LOS near the border between classes, which resulted in worsened performance"
- Why unresolved: The current three-class classification scheme (0–2 days, 3–7 days, >7 days) creates artificial boundaries where patients with similar LOS values may be assigned different classes
- What evidence would resolve it: Training models on the same dataset using regression approaches and comparing prediction error distributions, particularly for patients near the 2-day and 7-day boundaries

### Open Question 2
- Question: Can Temporal Fusion Transformer models achieve competitive performance for neurological ICU LOS prediction when trained with sufficient computational resources?
- Basis in paper: The authors report TFT achieved only 37.6% accuracy and state: "lack of enough computing resources (only 2 GPUs per job allowed) led to the unsatisfactory results" and "Another issue is higher complexity of computations due to GRNs"
- Why unresolved: TFT's computational requirements prevented adequate training epochs compared to LSTM (30 epochs) and BERT (150 epochs); TFT required "almost 90 times more time to finish an epoch"
- What evidence would resolve it: Retraining TFT on equivalent hardware with sufficient epochs and comparing final metrics against BERT's 80% accuracy baseline

### Open Question 3
- Question: Would integrating data from multiple hospitals and healthcare systems improve model generalizability for neurological ICU LOS prediction?
- Basis in paper: The conclusions state: "only one dataset was used, MIMIC-IV, which included data only from one hospital. Next steps might include integration of data from different sources (and different countries as well). This might improve generalization of the developed models"
- Why unresolved: MIMIC-IV contains data solely from Beth Israel Deaconess Medical Center, limiting external validity
- What evidence would resolve it: Training models on multi-center datasets and evaluating performance on held-out hospitals not included in training data

## Limitations
- Performance metrics suggest moderate predictive power (0.68 accuracy for static, 0.80 for time-series) rather than clinical-grade accuracy
- Class imbalance handling through SMOTE may introduce synthetic data artifacts that don't fully represent real patient trajectories
- Small BERT architecture (4 hidden layers, 16 attention heads) may not capture all relevant temporal dependencies

## Confidence
- **High Confidence**: Random Forest outperforming other classical ML methods on static admission data
- **Medium Confidence**: BERT's superiority over LSTM for time-series data
- **Medium Confidence**: Clinical feature importance findings

## Next Checks
1. Conduct external validation on independent neurological ICU datasets to verify model generalizability beyond MIMIC-IV
2. Perform ablation studies removing top-performing features (ventilator modes, CAM-ICU assessments) to quantify their true predictive contribution
3. Test model performance on different LOS binning strategies (e.g., 1-day vs. 3-day thresholds) to assess robustness to classification scheme variations