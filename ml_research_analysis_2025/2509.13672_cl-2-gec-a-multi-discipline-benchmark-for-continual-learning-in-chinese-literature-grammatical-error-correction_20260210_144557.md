---
ver: rpa2
title: 'CL$^2$GEC: A Multi-Discipline Benchmark for Continual Learning in Chinese
  Literature Grammatical Error Correction'
arxiv_id: '2509.13672'
source_url: https://arxiv.org/abs/2509.13672
tags:
- learning
- arxiv
- error
- continual
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CL$^2$GEC is the first benchmark for continual learning in Chinese
  grammatical error correction across multiple academic disciplines. It contains 10,000
  human-annotated sentences from 10 disciplines, enabling evaluation of domain adaptation
  and catastrophic forgetting.
---

# CL$^2$GEC: A Multi-Discipline Benchmark for Continual Learning in Chinese Literature Grammatical Error Correction

## Quick Facts
- **arXiv ID**: 2509.13672
- **Source URL**: https://arxiv.org/abs/2509.13672
- **Reference count**: 13
- **Primary result**: First benchmark for continual learning in Chinese grammatical error correction across multiple academic disciplines

## Executive Summary
CL$^2$GEC introduces the first benchmark for continual learning in Chinese grammatical error correction across multiple academic disciplines. The benchmark comprises 10,000 human-annotated sentences from 10 distinct disciplines, enabling evaluation of domain adaptation and catastrophic forgetting. The study establishes task-specific continual learning metrics and evaluates both randomized and semantically ordered task sequences, revealing that regularization-based methods generally outperform replay-based and naive sequential approaches.

## Method Summary
The CL$^2$GEC benchmark is constructed from human-annotated sentences across 10 academic disciplines, with 1,000 sentences per domain. Two task sequences are defined: randomized and semantically ordered. The benchmark incorporates task-specific continual learning metrics and evaluates four CL algorithms (EWC, GEM, LwF, OGD) against baseline approaches using large language models. Experiments systematically compare performance across different task orders and model architectures.

## Key Results
- CL$^2$GEC is the first multi-discipline benchmark for continual learning in Chinese grammatical error correction
- Regularization-based methods (EWC, GEM, LwF, OGD) outperform replay-based and naive sequential approaches
- Semantic task ordering improves average performance but may reduce precision and backward transfer in some cases

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its comprehensive coverage of academic disciplines and its evaluation of both catastrophic forgetting and domain adaptation. By providing human-annotated data across diverse domains, it enables rigorous testing of continual learning algorithms' ability to maintain performance while adapting to new domains. The inclusion of both randomized and semantically ordered task sequences allows for analysis of how task ordering affects learning dynamics and model performance.

## Foundational Learning
- **Continual Learning**: Why needed - to enable models to learn new tasks without forgetting previous ones; Quick check - verify that model maintains performance on earlier domains
- **Catastrophic Forgetting**: Why needed - understanding when and why models lose previously acquired knowledge; Quick check - measure performance degradation on old tasks after learning new ones
- **Domain Adaptation**: Why needed - adapting models to different writing styles and terminology across disciplines; Quick check - compare performance across different academic domains
- **Regularization Methods**: Why needed - to constrain model parameters and prevent forgetting; Quick check - verify that regularization parameters are properly tuned
- **Semantic Task Ordering**: Why needed - to optimize learning sequence for better knowledge transfer; Quick check - analyze performance differences between task orders

## Architecture Onboarding

**Component Map**: Data Collection -> Annotation -> Benchmark Construction -> CL Algorithm Evaluation -> Performance Analysis

**Critical Path**: Human annotation of discipline-specific sentences → Benchmark construction with task sequences → Evaluation of CL algorithms → Performance analysis and comparison

**Design Tradeoffs**: 
- Sample size vs. annotation quality (10,000 total sentences)
- Task sequence randomization vs. semantic ordering
- Model complexity vs. computational efficiency

**Failure Signatures**:
- Significant performance drop on earlier domains indicates catastrophic forgetting
- Inconsistent performance across domains suggests poor domain adaptation
- Over-regularization may lead to underfitting on new tasks

**First Experiments**:
1. Verify annotation quality through inter-annotator agreement across disciplines
2. Establish baseline performance for each domain independently
3. Test CL algorithms on single task transitions before full sequence evaluation

## Open Questions the Paper Calls Out
None

## Limitations
- Relatively small sample size of 10,000 sentences across 10 disciplines may not capture full complexity
- Focus on Chinese academic texts limits generalizability to other languages and domains
- Occasional drops in precision and backward transfer with semantic ordering indicate context-dependent optimal strategies

## Confidence
- **Novelty claim**: High - first benchmark for this specific application
- **Method effectiveness**: Medium - results are model-dependent and vary with task sequence
- **Generalizability**: Medium - limited to Chinese academic domain

## Next Checks
1. Validate robustness of regularization-based methods across additional large language models beyond those tested
2. Conduct larger-scale annotation study to assess inter-annotator agreement and ensure consistency across disciplines
3. Test the benchmark on non-academic domains (e.g., technical manuals, news articles) to evaluate cross-domain adaptability