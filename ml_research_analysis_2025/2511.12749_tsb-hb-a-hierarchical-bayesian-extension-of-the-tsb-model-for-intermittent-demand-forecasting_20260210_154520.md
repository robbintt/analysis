---
ver: rpa2
title: 'TSB-HB: A Hierarchical Bayesian Extension of the TSB Model for Intermittent
  Demand Forecasting'
arxiv_id: '2511.12749'
source_url: https://arxiv.org/abs/2511.12749
tags:
- tsb-hb
- demand
- intermittent
- hierarchical
- shrinkage
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TSB-HB introduces a hierarchical Bayesian extension of the TSB
  model for intermittent demand forecasting. The method models demand occurrence with
  a Beta-Binomial distribution and nonzero demand sizes with a Log-Normal distribution,
  incorporating hierarchical priors that enable partial pooling across items.
---

# TSB-HB: A Hierarchical Bayesian Extension of the TSB Model for Intermittent Demand Forecasting

## Quick Facts
- arXiv ID: 2511.12749
- Source URL: https://arxiv.org/abs/2511.12749
- Reference count: 4
- Key outcome: Hierarchical Bayesian TSB model with Beta-Binomial occurrence and Log-Normal size components achieves lower RMSE and RMSSE than classical baselines on UCI Online Retail and M5 datasets.

## Executive Summary
TSB-HB extends the classical TSB model for intermittent demand by introducing hierarchical Bayesian components that enable partial pooling across items. The method models demand occurrence with a Beta-Binomial distribution and nonzero demand sizes with a Log-Normal distribution, incorporating hierarchical priors that enable partial pooling across items. This design provides shrinkage for sparse or cold-start series while preserving heterogeneity, yielding a fully generative and interpretable model that generalizes classical exponential smoothing.

## Method Summary
TSB-HB implements a three-layer structure: (1) Beta-Binomial for occurrence probability πi with posterior mean π̂i=(α+mi)/(α+β+ni), where (α,β) are estimated via marginal MLE using L-BFGS-B; (2) Log-Normal random intercept model for sizes ℓi,t=μi+εi,t where μi~N(μ0,τ²), with REML estimates for (μ0,τ²,σ²); (3) Final forecast Ŷi=π̂i·Ŝi where Ŝi=exp(μ̂i+½(σ̂²+v̂μ,i)). Training complexity is O(M+IN) and prediction is O(N) per epoch. The model uses Empirical Bayes to fit global hyperparameters and applies shrinkage formulas that combine item-specific statistics with global priors.

## Key Results
- On UCI Online Retail dataset, achieves RMSE of 17.777 and RMSSE of 1.179
- Outperforms Croston, SBA, TSB, ADIDA, IMAPA, ARIMA, and Theta baselines
- On M5 dataset subset, beats all classical baselines evaluated
- Provides calibrated probabilistic forecasts with improved accuracy on intermittent and lumpy items
- Maintains O(N) per-epoch forecasting complexity after one-shot hyperparameter fitting

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Partial Pooling (Shrinkage)
The model stabilizes forecasts for sparse or cold-start items by borrowing statistical strength across the panel of items. TSB-HB replaces per-item point estimates with posterior distributions derived from hierarchical priors. For occurrence, it uses a Beta-Binomial structure; for size, a Normal-Normal structure on the log scale. Item-level estimates are "shrunk" toward global means (μπ and μ0) with a force proportional to data sparsity. This prevents extreme estimates in low-data regimes while retaining heterogeneity for items with ample history. Core assumption: The panel of items shares enough latent similarity that global prior statistics provide a better "default" for sparse items than the item's own limited history. Evidence: Hierarchical priors enable partial pooling across items, stabilizing estimates for sparse or cold-start series while preserving heterogeneity. Break condition: If items exhibit multimodal heterogeneity not represented by a single global mean, shrinkage may systematically bias distinct clusters toward the center.

### Mechanism 2: Generative Multiplicative Decomposition
Modeling demand as the product of occurrence probability and conditional size provides a coherent probabilistic foundation that classical heuristics lack. The model factorizes the forecast Ŷi=π̂i·Ŝi. Unlike Croston or TSB which use overlapping exponential smoothers, TSB-HB treats this as a generative process: first, a Bernoulli trial determines if demand occurs; if yes, a Log-Normal draw determines size. This allows for explicit variance propagation and quantile generation. Core assumption: Demand occurrence and demand size are independent processes conditional on the latent parameters. Evidence: This framework yields a fully generative and interpretable model that generalizes classical exponential smoothing. Break condition: If occurrence and size are strongly correlated, the independence assumption biases the predictive distribution.

### Mechanism 3: Empirical Bayes (EB) Hyperparameter Fitting
Fitting global hyperparameters (μπ, φ, μ0, τ²) via maximum marginal likelihood allows for scalable closed-form inference without MCMC. Instead of treating priors as fixed constants or sampling them via full Bayesian inference, TSB-HB estimates the "shape" of the priors directly from the data statistics. This "one-shot" fitting turns the forecasting step into a deterministic calculation. Core assumption: The estimated hyperparameters are sufficiently close to the true generating parameters that treating them as fixed does not severely underestimate predictive variance. Evidence: Training is O(M+IN) and prediction is O(N) per epoch with Beta-Binomial hyper-parameters obtained by maximizing a two-parameter marginal likelihood. Break condition: In small panels, the estimated hyperparameters may have high variance themselves, leading to unstable shrinkage weights.

## Foundational Learning

- **Concept: Conjugate Priors (Beta-Binomial)**
  - Why needed: The occurrence model relies entirely on Beta-Binomial conjugacy to update beliefs about demand probability.
  - Quick check: If an item has 2 sales in 10 days, and the global Beta prior has parameters α,β, what is the formula for the posterior mean probability of sale?

- **Concept: Random Effects / Mixed Models**
  - Why needed: The size model is structurally a Linear Mixed Model with item-level random intercepts.
  - Quick check: If between-item variance (τ²) drops to zero, what happens to the weight wi assigned to the item's own sample mean?

- **Concept: Log-Normal Distribution**
  - Why needed: The paper models positive sizes on the log scale to handle right-skew.
  - Quick check: Why is the mean of a Log-Normal distribution strictly greater than the exponent of its log-scale mean (e^μ)?

## Architecture Onboarding

- **Component map:** Raw demand Y → Sufficient Statistics (mi, ni, ℓ̄i, s²ℓ,i) → Global Optimizer (Beta-Binomial MLE, REML) → Posterior Engine (shrinkage formulas) → Forecaster (Ŷi=π̂i·Ŝi)
- **Critical path:** Data → Sufficient Statistics → Global Hyperparameters → Item Posteriors → Forecasts. You cannot calculate item posteriors until the global hyperparameters are fitted.
- **Design tradeoffs:** The paper chooses Empirical Bayes over Full Bayes for O(N) speed but sacrifices proper uncertainty propagation. It defaults to Log-Normal for sizes but supports Gamma for different tail assumptions.
- **Failure signatures:** Cold-start collapse if N is too small to fit global priors, causing all forecasts to revert to a single global average. Log-scale explosion with extreme outliers producing numerically unstable forecasts.
- **First 3 experiments:** 1) Shrinkage Visualization: Plot MLE vs. Posterior estimates for sparse items to verify high-variance MLEs are pulled toward global mean. 2) Cold-Start Injection: Mask history of 10% of items and verify predictions fall back to global mean exactly. 3) Ablation of Size Law: Compare TSB-HB (Log-Normal) vs. TSB-HB-Gamma on lumpy items to see if tail assumption impacts heavy-tailed sparse items.

## Open Questions the Paper Calls Out

- Can TSB-HB be extended to model time-varying obsolescence, where demand occurrence probability decays dynamically over extended zero runs? Basis: "modeling time-varying obsolescence is outside our present scope and left for future work." Why unresolved: The current model assumes time-invariant per-period forecasting. What evidence: A state-space extension with decaying occurrence probability evaluated on datasets with documented item obsolescence patterns.

- Does propagating hyperparameter uncertainty through delta-method corrections or bootstrap improve calibration beyond the current plug-in empirical Bayes approach? Basis: "A first-order delta correction and a parametric bootstrap that propagate uncertainty in θ̂ are natural extensions but are not pursued in our experiments." Why unresolved: Plug-in EB systematically underestimates predictive variance. What evidence: Comparative experiments showing coverage closer to nominal levels with uncertainty propagation.

- How does incorporating covariates via logistic links for occurrence and linear mixed models for sizes affect both forecast accuracy and the magnitude of hierarchical shrinkage benefits? Basis: "Future work may... integrate covariates" and "Extensions with covariates can be obtained by (a) a logistic link on πi and (b) a linear mixed model on ℓi,t." Why unresolved: The intercept-only design was chosen to isolate shrinkage contributions. What evidence: Ablation studies on datasets with known demand drivers comparing shrinkage magnitudes with and without covariate adjustment.

## Limitations
- Empirical Bayes approach underestimates predictive uncertainty by treating estimated hyperparameters as fixed
- Independence assumption between occurrence and size processes may break down for items with strong demand patterns
- Shrinkage toward a single global mean may not capture multimodal heterogeneity across items

## Confidence

- **High confidence**: The hierarchical shrinkage mechanism is well-supported by the paper's equations and cross-validated by related work on global pooling strategies.
- **Medium confidence**: The generative multiplicative decomposition provides a coherent probabilistic foundation, though the independence assumption lacks direct empirical validation.
- **Low confidence**: The Empirical Bayes optimization lacks external validation in the corpus; effectiveness depends heavily on panel size and data quality.

## Next Checks

1. **Shrinkage Behavior Verification**: Replicate the shrinkage visualization to confirm that MLE estimates with high variance are systematically pulled toward global means, particularly for sparse items.

2. **Panel Size Sensitivity**: Test TSB-HB performance on varying panel sizes (N=50, N=500, N=5000) to quantify how shrinkage effectiveness scales with available data.

3. **Tail Assumption Comparison**: Implement the Gamma variant of TSB-HB and compare performance on heavy-tailed items to determine whether the Log-Normal assumption is optimal for all intermittent demand patterns.