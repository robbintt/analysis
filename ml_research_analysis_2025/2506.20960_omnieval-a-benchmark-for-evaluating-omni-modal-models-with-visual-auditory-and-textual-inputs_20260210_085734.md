---
ver: rpa2
title: 'OmniEval: A Benchmark for Evaluating Omni-modal Models with Visual, Auditory,
  and Textual Inputs'
arxiv_id: '2506.20960'
source_url: https://arxiv.org/abs/2506.20960
tags:
- arxiv
- video
- understanding
- omnieval
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: OmniEval is a benchmark for evaluating omni-modal models that process
  visual, auditory, and textual inputs. It addresses the gap in existing benchmarks
  that fail to capture deep multimodal coupling and synergy.
---

# OmniEval: A Benchmark for Evaluating Omni-modal Models with Visual, Auditory, and Textual Inputs

## Quick Facts
- **arXiv ID**: 2506.20960
- **Source URL**: https://arxiv.org/abs/2506.20960
- **Reference count**: 40
- **Primary result**: OmniEval is a benchmark for evaluating omni-modal models that process visual, auditory, and textual inputs, addressing the gap in existing benchmarks that fail to capture deep multimodal coupling and synergy.

## Executive Summary
OmniEval is a benchmark designed to evaluate omni-modal models that process visual, auditory, and textual inputs. It addresses the gap in existing benchmarks that fail to capture deep multimodal coupling and synergy. OmniEval includes 810 synchronized videos in Chinese and English, with 2617 question-answer pairs across 3 major task types and 12 sub-task types, including a novel fine-grained video localization task called Grounding. The benchmark emphasizes full-modal collaboration, requiring models to effectively integrate all modalities. Experiments with state-of-the-art omni-modal models show significant challenges in real-world understanding, demonstrating the benchmark's difficulty and the need for further model improvements. Codes and data are publicly available at https://omnieval-benchmark.github.io/.

## Method Summary
OmniEval is an inference-only evaluation benchmark with 810 synchronized videos (285 Chinese, 525 English) and 2617 QA pairs (1412 open-ended, 1205 multiple-choice) across 12 sub-task types. Models are evaluated on their ability to understand and reason across visual, auditory, and textual modalities. The benchmark includes a novel "Grounding" task for fine-grained temporal localization using adaptive timestamp tolerance thresholds. Performance is measured through multiple-choice accuracy, LLM-assisted scoring for open-ended questions, and temporal IoU for grounding tasks. Tested models include Qwen2.5-Omni-7B/3B, Baichuan-Omni-1.5, MiniCPM-O 2.6, VITA-1.5, and Gemini-2.5-pro-preview-05-06.

## Key Results
- Current omni-modal models show significant challenges in real-world understanding across all task types
- Models generally fail to effectively leverage raw video content compared to textual captions
- The novel Grounding task reveals particular difficulty with fine-grained temporal localization
- Language bias is evident, with models performing significantly better on Chinese versus English questions

## Why This Works (Mechanism)

### Mechanism 1: Collaborative Modality Coupling
- **Claim:** If a benchmark enforces "strong coupling" between audio and video streams, it purportedly prevents models from relying on single-modality shortcuts (e.g., answering based solely on subtitles).
- **Mechanism:** OmniEval constructs tasks where the visual and auditory streams provide complementary, non-redundant information required for the correct answer. The evaluation logic implicitly measures the "synergy" by penalizing models that fail to cross-reference modalities.
- **Core assumption:** The correct answers in the dataset genuinely require simultaneous audio-visual processing and cannot be solved via language priors or static visual features alone.
- **Evidence anchors:**
  - [abstract] "design evaluation tasks that highlight the strong coupling between audio and video, requiring models to effectively leverage the collaborative perception..."
  - [section 3.1] "This transcends evaluation approaches that merely sum individual unimodal understanding capabilities."
  - [corpus] WorldSense (arXiv:2502.04326) similarly emphasizes "collaboration of omni-modality," suggesting this coupling is a recognized, albeit difficult, evaluation standard.
- **Break Condition:** If questions are linguistically solvable (e.g., "What did the man say?"), the mechanism fails to test omni-modal fusion.

### Mechanism 2: Adaptive Temporal Grounding Thresholds
- **Claim:** Precise temporal localization (Grounding) is enabled by an adaptive error threshold that scales with frame sampling rates.
- **Mechanism:** The evaluation uses a dynamic threshold ($\tau_{ts}$) defined as $\min(1/FPS, \text{duration}/\text{max\_frame})$. This adjusts the "margin of error" allowed for a correct timestamp prediction based on the video's temporal resolution. This allows fair evaluation across models with different frame extraction strategies.
- **Core assumption:** The granularity of the model's temporal prediction correlates with its internal frame-level alignment capabilities.
- **Evidence anchors:**
  - [section 4.1.1] Eq. 1 defines $\tau_{ts}$ and states: "When the number of extracted frames is low... we use a larger threshold... allowing for a more lenient assessment."
  - [abstract] Mentions "a novel fine-grained video localization task named Grounding."
  - [corpus] OmniDPO (arXiv:2509.00723) discusses hallucinations in omni-modal contexts, which this grounding mechanism aims to penalize by requiring precise temporal evidence.
- **Break Condition:** If the threshold is too lenient (e.g., extremely low FPS), a model could guess randomly within a large time window and succeed, invalidating the precision metric.

### Mechanism 3: Modality Substitution Sensitivity (Text-Bias Detection)
- **Claim:** Removing or adding raw modalities (Video/Audio) while keeping textual surrogates (Captions/Subtitles) exposes a model's over-reliance on text processing vs. true sensory fusion.
- **Mechanism:** The paper ablates inputs (e.g., Audio+Caption vs. Audio+Caption+Video). If performance degrades or stagnates when raw video is added, it indicates the model fails to fuse the raw signal effectively and prefers the textual tokenization.
- **Core assumption:** Textual descriptions (captions/subtitles) are accurate enough proxies for raw data that a capable model should either match or exceed text-only performance when raw data is added.
- **Evidence anchors:**
  - [section 4.3.1] "The subsequent addition of raw video frames generally did not lead to further improvements... these MLLMs more effectively leverage textual captions than raw video content."
  - [section 4.3.1] Table 6 shows MiniCPM-O 2.6 performance dropping from 56.54 (Audio+Caption) to 27.79 (Audio+Caption+Video).
  - [corpus] "Is Extending Modality The Right Path..." (arXiv:2506.01872) supports this by noting models "struggle to generalize" even when modalities are extended, aligning with the observed difficulty in utilizing raw video.
- **Break Condition:** If the captions used in the ablation are of poor quality, the drop in performance might be due to data noise rather than model fusion failure.

## Foundational Learning

- **Concept:** **Intersection over Union (IoU) for Time Spans**
  - **Why needed here:** The "Grounding" task evaluates time-span predictions (start/end times). Understanding IoU is critical to interpreting why a model might fail (e.g., predicting the right event but wrong duration).
  - **Quick check question:** If a ground truth event is 10s-20s and a model predicts 15s-25s, is the IoU 5/15 (overlap/union)?

- **Concept:** **Tokenization of Multimodal Inputs**
  - **Why needed here:** The paper references models like Qwen2.5-Omni using "1fps" or "64 frames." You must understand how raw video/audio is converted to tokens to diagnose context length errors (like VITA-1.5's tensor size issues).
  - **Quick check question:** Does increasing the frame rate (e.g., 1fps to 10fps) linearly increase the input token count?

- **Concept:** **ASR (Automated Speech Recognition) Integration**
  - **Why needed here:** The dataset relies on ASR transcripts (via Volcano Engine) for the "Auditory" textual component. Failures in the benchmark could stem from ASR errors rather than model reasoning errors.
  - **Quick check question:** If the audio contains heavy background noise, will the ASR transcript likely contain hallucinations that mislead the LLM?

## Architecture Onboarding

- **Component map:**
  - Video Loader (FPS sampling) -> Visual Encoder (e.g., SigLip/ViT) -> MLP/Q-Former projection -> LLM Backbone (e.g., Qwen2.5) -> Classification/Generation Head
  - Audio Splitter -> Audio Encoder (e.g., Whisper) -> MLP/Q-Former projection -> LLM Backbone -> Classification/Generation Head
  - ASR pipeline -> Text tokens -> LLM Backbone -> Classification/Generation Head

- **Critical path:**
  1. **Preprocessing:** Standardize video to 1fps (or model specific limit)
  2. **ASR Check:** Ensure subtitle track exists; if not, run ASR pipeline
  3. **Inference:** Concatenate Visual tokens + Audio tokens + Text tokens (Question)
  4. **Post-processing:** Extract timestamps from text response using Regex or LLM-parsing for Grounding metric calculation

- **Design tradeoffs:**
  - **Context Window vs. Granularity:** High frame rates improve Grounding accuracy (Mechanism 2) but explode token count, potentially causing the "tensor size out of range" errors seen in VITA-1.5 (Section 4.2)
  - **Text-Reliance vs. Raw Fusion:** Relying on subtitles (high performance, low fusion) vs. raw audio (low performance, high fusion potential)

- **Failure signatures:**
  - **Modality Drowning:** Model ignores video tokens if text context is too rich (Section 4.3.1)
  - **Timestamp Hallucination:** Model generates timestamps that are out of bounds (e.g., "150s" for a 60s video)
  - **Language Bias:** Models like MiniCPM-O 2.6 score 58.34 on CN MC but only 13.31 on EN MC, indicating severe training data imbalance (Table 5)

- **First 3 experiments:**
  1. **Zero-shot Baseline:** Run Qwen2.5-Omni-7B on OmniEval "Grounding" subset to verify the evaluation script correctly parses Eq. 1 (adaptive thresholds)
  2. **Ablation Study (Text-Only):** Input only the subtitles and questions (no video/audio) to establish a "text-cheating" baseline
  3. **Language Skew Test:** Evaluate performance on English vs. Chinese videos specifically to check for tokenizer/vocabulary bias in the model

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can model architectures be redesigned to prevent performance degradation when integrating raw video frames with existing audio and textual inputs?
- **Basis in paper:** [explicit] Section 4.3.1 and Table 6 show that adding raw video frames to audio+caption inputs caused performance to drop significantly for MiniCPM-O 2.6 (56.54 to 27.79), suggesting models fail to effectively leverage raw visual data.
- **Why unresolved:** The paper identifies that current paradigms favor textual captions over raw video, but does not propose a solution for the observed "negative collaboration" or interference between modalities.
- **What evidence would resolve it:** Development of a model architecture that demonstrates monotonically increasing performance (positive synergy) as raw video frames are added to audio-text inputs on the OmniEval benchmark.

### Open Question 2
- **Question:** What specific training methodologies are required to enable omni-modal models to extract semantic meaning from raw audio waveforms as effectively as from textual transcripts?
- **Basis in paper:** [explicit] Section 4.3.2 notes that while adding subtitles consistently improves performance, adding raw audio yields mixed results (improving Baichuan-Omni but degrading MiniCPM-O), indicating a failure in raw audio utilization.
- **Why unresolved:** The authors conclude that "multimodal understanding of raw audio... still requires significant advancement," implying current training does not sufficiently align raw acoustic features with semantic reasoning.
- **What evidence would resolve it:** A model achieving comparable performance on audio-only tasks (without subtitles) versus subtitle-only tasks, or showing statistically significant gains when raw audio is added to video inputs.

### Open Question 3
- **Question:** How can temporal reasoning mechanisms be improved to handle fine-grained event grounding in dynamic, multi-modal contexts?
- **Basis in paper:** [explicit] The paper introduces a novel "Grounding" task (Section 3.3.1) to test fine-grained localization, but the experimental results (Table 4) reveal generally low performance across all evaluated open-source models.
- **Why unresolved:** The low scores suggest that current models struggle to pinpoint precise moments or time spans based on synchronized audio-visual cues, a capability distinct from general captioning.
- **What evidence would resolve it:** A significant increase in IoU scores for time-span questions and accuracy for moment-based questions, achieved through new temporal attention mechanisms or specialized pre-training objectives.

## Limitations
- The benchmark's evaluation heavily depends on the quality and consistency of the proprietary LLM used for open-ended question scoring, which is not specified in the paper.
- The grounding task's adaptive threshold mechanism, while theoretically sound, may not fully account for varying frame extraction strategies across different models.
- The dataset construction process, particularly the ASR quality for non-Chinese videos and the selection criteria for question types, lacks detailed specification.

## Confidence
- **High Confidence**: The benchmark's basic structure (810 videos, 2617 QA pairs, 12 sub-task types) and the general finding that current models struggle with omni-modal fusion are well-supported by the data presented.
- **Medium Confidence**: The specific performance numbers (e.g., Qwen2.5-Omni-7B at 39.14% accuracy) are internally consistent but depend on the unspecified LLM judge for OE scoring.
- **Low Confidence**: The claim that Grounding represents a "novel" task type is questionable given similar temporal localization tasks exist in other benchmarks, though the adaptive threshold mechanism may be novel.

## Next Checks
1. **LLM Judge Consistency Test**: Run the same set of OE questions through multiple different LLM judges (e.g., GPT-4, Claude, Gemini) to establish scoring variance and determine if the original results are judge-dependent.

2. **Text-Only Ablation Baseline**: Evaluate all models on OmniEval using only subtitles/captions (no video/audio) to establish a "text-cheating" baseline, verifying whether the reported performance drops when raw modalities are added are statistically significant.

3. **Language Balance Verification**: Analyze the dataset's language distribution by running a subset of questions through a language detection model to verify the claimed 285 Chinese/525 English split and investigate whether performance differences correlate with question complexity rather than language per se.