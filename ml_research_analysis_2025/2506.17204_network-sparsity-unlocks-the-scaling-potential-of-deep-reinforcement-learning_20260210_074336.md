---
ver: rpa2
title: Network Sparsity Unlocks the Scaling Potential of Deep Reinforcement Learning
arxiv_id: '2506.17204'
source_url: https://arxiv.org/abs/2506.17204
tags:
- network
- sparsity
- scaling
- learning
- sparse
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that introducing static network sparsity
  through one-shot random pruning can unlock further scaling potential in deep reinforcement
  learning (DRL) models beyond current state-of-the-art architectures. Unlike previous
  dynamic approaches, this simple pre-training sparsity enables larger models to achieve
  both higher parameter efficiency and better performance by preventing optimization
  pathologies like capacity collapse, plasticity loss, and gradient interference.
---

# Network Sparsity Unlocks the Scaling Potential of Deep Reinforcement Learning

## Quick Facts
- arXiv ID: 2506.17204
- Source URL: https://arxiv.org/abs/2506.17204
- Authors: Guozheng Ma; Lu Li; Zilin Wang; Li Shen; Pierre-Luc Bacon; Dacheng Tao
- Reference count: 40
- One-line primary result: Static network sparsity through one-shot random pruning unlocks scaling potential in deep RL, outperforming dense models on hard tasks

## Executive Summary
This paper demonstrates that introducing static network sparsity through one-shot random pruning can unlock further scaling potential in deep reinforcement learning (DRL) models beyond current state-of-the-art architectures. Unlike previous dynamic approaches, this simple pre-training sparsity enables larger models to achieve both higher parameter efficiency and better performance by preventing optimization pathologies like capacity collapse, plasticity loss, and gradient interference. Experiments across multiple DRL scenarios (including visual and streaming RL) show that appropriately sparse networks consistently outperform their dense counterparts when scaled, with performance improvements of up to 40% on challenging tasks.

## Method Summary
The method employs Erdős-Rényi (ER) layer-wise sparsity ratios applied once at initialization through binary masks, creating static sparse networks that are trained normally without mask updates. The approach uses SimBa architecture with residual blocks, layer normalization, and observation normalization, trained with SAC/DDPG. The sparsity ratios are computed per layer based on input/output dimensions, with optimal results at 0.8-0.9 sparsity for large networks (~109M parameters). The technique is evaluated across multiple DRL scenarios including DMC hard tasks, visual RL, and streaming RL, with comparisons between dense and sparse networks at various scales.

## Key Results
- Large sparse networks (109M parameters, 0.8-0.9 sparsity) outperform large dense networks on challenging DRL tasks
- Sparse networks prevent capacity collapse and plasticity loss that plague dense networks when scaled
- Performance improvements of up to 40% on hard DMC tasks like Humanoid Run and Dog Run
- Parameter-equivalent comparisons show sparse networks achieve higher returns than dense networks

## Why This Works (Mechanism)

### Mechanism 1: Representational Capacity Preservation via Srank
Static sparsity prevents capacity collapse that occurs when naively scaling dense DRL networks by constraining the optimization landscape and preventing representational degradation (measured via Stable-rank). This maintains effective rank of learned representations that correlates with task performance.

### Mechanism 2: Plasticity Maintenance via Dormant Neuron Suppression
Sparsity preserves learning capability by preventing neuron dormancy and gradient collapse. Sparse connectivity limits the parameter space that gradients can modify, reducing the rate at which neurons become permanently inactive and maintaining stronger gradient signals throughout training.

### Mechanism 3: Gradient Interference Reduction
Sparsity promotes gradient orthogonality across training samples by limiting gradient overlap between different data points. Fixed sparse topology maintains more independent update directions compared to dense networks where correlations strengthen during training.

## Foundational Learning

- Concept: Static Sparse Training (SST) vs. Dynamic Sparse Training (DST)
  - Why needed here: The paper specifically isolates static sparsity as the mechanism; conflating with DST methods (RigL, SET, magnitude pruning) would misinterpret the contribution
  - Quick check question: Can you explain why fixed binary masks applied once at initialization differ fundamentally from gradual magnitude pruning during training?

- Concept: Optimization Pathologies in DRL (plasticity loss, capacity collapse, primacy bias)
  - Why needed here: Understanding what sparsity mitigates requires knowing these distinct but related failure modes
  - Quick check question: Name two observable symptoms of plasticity loss and one metric used to detect each

- Concept: Erdős-Rényi vs. Uniform Sparsity Distribution
  - Why needed here: The paper adopts ER-based layer-wise ratios rather than uniform sparsity; implementation requires this distinction
  - Quick check question: How does ER-based sparsity allocate connectivity differently across layers compared to uniform sparsity?

## Architecture Onboarding

- Component map:
  Dense backbone: SimBa architecture (residual blocks + layer norm + observation normalization)
  → Sparsification layer: Binary masks M_l per layer, applied as W_eff = M ⊙ W
  → Layer-wise sparsity: Erdős-Rényi kernel (s_l ∝ 1 - (n_{l-1} + n_l) / (n_{l-1} × n_l))
  → Training: Standard SAC/DDPG with masked forward/backward passes

- Critical path:
  1. Initialize SimBa network at target (larger) scale
  2. Compute ER-based sparsity ratio per layer from overall target sparsity
  3. Generate random binary masks and apply once before training begins
  4. Maintain fixed masks throughout—all gradients flow only through active connections

- Design tradeoffs:
  - Large networks (>50M params) + high sparsity (0.8-0.9) → best scaling gains
  - Default networks (~17M params) + high sparsity → performance degradation (insufficient learnable parameters)
  - Width scaling + sparsity generally outperforms depth scaling + sparsity
  - Sparsity eliminates need for Reset operations, avoiding training disruption

- Failure signatures:
  - Small networks with sparsity >0.6: Performance drops below dense baseline
  - Large dense networks without sparsity: Rising dormant ratio, collapsing gradient norm, declining Srank after ~500k steps
  - Sparse networks don't benefit from Reset (expected, not a failure)

- First 3 experiments:
  1. Replicate Figure 3 on Humanoid Run: Compare default (4.5M) vs. large (109M) networks across sparsity 0.1-0.9
  2. Diagnostic measurement: Track dormant ratio, gradient norm, and Srank during training for dense vs. sparse large networks
  3. Parameter-equivalent comparison: Match learnable parameter count between small dense and large sparse networks; verify sparse achieves higher return

## Open Questions the Paper Calls Out

### Open Question 1
Can dynamic sparsity methods, which update the network topology during training, outperform the static one-shot random pruning demonstrated in this study?
- Basis in paper: [explicit] The conclusion states that this work "opens new avenues for research into... dynamic sparsity methods" beyond the static approach isolated here.
- Why unresolved: The paper intentionally decouples sparsity from dynamicity to isolate its effects, showing static sparsity is sufficient but leaving dynamic approaches unexplored in this specific scaling context.
- What evidence would resolve it: Comparative scaling curves on DMC tasks pitting static random pruning against dynamic sparse training (DST) methods like RigL or SET.

### Open Question 2
Do the scaling benefits of random static sparsity persist in high-data regimes or longer training horizons not covered by the Atari-100k benchmark?
- Basis in paper: [explicit] Appendix C.3 notes that "Atari-100k low-data regime may not fully demonstrate the benefits of scaling, and more comprehensive studies with longer training would be valuable."
- Why unresolved: The Atari experiments were restricted to 100k steps, potentially failing to reveal performance divergence or pathologies that might emerge with extended training.
- What evidence would resolve it: Experiments extending the training duration to 200M frames (standard Atari regime) comparing dense and sparse scaling trends.

### Open Question 3
What are the formal theoretical mechanisms linking random static sparsity to the mitigation of capacity collapse and plasticity loss?
- Basis in paper: [explicit] The conclusion identifies "theoretical frameworks" as a necessary new avenue for research to understand why this architectural feature aids scaling.
- Why unresolved: The current paper relies on empirical diagnostics (e.g., Srank, dormant ratios) to explain the benefits but does not provide a formal mathematical derivation.
- What evidence would resolve it: A theoretical proof connecting the spectral properties of randomly pruned initialization to the stability of gradient flow in non-stationary RL environments.

## Limitations

- The core uncertainty is whether static sparsity truly enables fundamental scaling improvements versus merely delaying existing pathologies
- Network size boundaries are unclear—transition points where sparsity becomes harmful or unnecessary aren't well-characterized
- The claim that static sparsity "unlocks scaling potential" overstates the contribution without theoretical analysis explaining why dense networks fail to scale while sparse networks succeed

## Confidence

**High confidence**: The empirical demonstration that large sparse networks outperform large dense networks on standard benchmarks. The controlled experiments across multiple tasks and scales provide strong evidence for this claim.

**Medium confidence**: The proposed mechanisms (plasticity preservation, gradient interference reduction) are plausible given the diagnostic measurements, but the causal relationships aren't definitively established. The metrics used (dormant ratio, gradient norms, Srank) are reasonable proxies but not direct measurements of the claimed phenomena.

**Low confidence**: The claim that static sparsity "unlocks scaling potential" suggests a fundamental breakthrough rather than an incremental optimization. Without theoretical analysis explaining why dense networks fail to scale while sparse networks succeed, this framing overstates the contribution.

## Next Checks

1. **Ablation of learning dynamics**: Compare learning curves (return vs. training steps) between dense and sparse networks at the same parameter count. If sparse networks learn faster or reach higher asymptotic performance, this would support the claim that sparsity provides fundamental advantages beyond parameter efficiency.

2. **Transfer to non-visual domains**: Validate whether the scaling benefits extend to state-based continuous control tasks (like MuJoCo locomotion) where visual complexity isn't the bottleneck. This would test whether the findings generalize beyond the visual RL experiments presented.

3. **Temporal scaling analysis**: Extend training beyond 1M steps to test whether sparse networks maintain their advantages over very long timescales. If capacity collapse and plasticity loss eventually affect sparse networks too, this would suggest sparsity delays rather than prevents scaling pathologies.