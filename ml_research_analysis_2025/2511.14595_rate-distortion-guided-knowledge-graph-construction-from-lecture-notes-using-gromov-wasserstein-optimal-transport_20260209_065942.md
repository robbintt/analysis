---
ver: rpa2
title: Rate-Distortion Guided Knowledge Graph Construction from Lecture Notes Using
  Gromov-Wasserstein Optimal Transport
arxiv_id: '2511.14595'
source_url: https://arxiv.org/abs/2511.14595
tags:
- knowledge
- graph
- lecture
- distortion
- notes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a principled framework for constructing and
  refining knowledge graphs (KGs) from lecture notes, addressing the challenge of
  balancing completeness and complexity in educational KG extraction. The method leverages
  rate-distortion theory and fused Gromov-Wasserstein optimal transport to quantify
  semantic and relational alignment between lecture content and candidate KGs.
---

# Rate-Distortion Guided Knowledge Graph Construction from Lecture Notes Using Gromov-Wasserstein Optimal Transport

## Quick Facts
- **arXiv ID:** 2511.14595
- **Source URL:** https://arxiv.org/abs/2511.14595
- **Reference count:** 38
- **One-line primary result:** Refined KGs improve MCQ quality by 15% over raw lecture notes, with 30.4% higher coverage.

## Executive Summary
This paper introduces a principled framework for constructing and refining knowledge graphs (KGs) from lecture notes, addressing the challenge of balancing completeness and complexity in educational KG extraction. The method leverages rate-distortion theory and fused Gromov-Wasserstein optimal transport to quantify semantic and relational alignment between lecture content and candidate KGs. By modeling lecture notes and KGs as metric-measure spaces, the framework iteratively refines KGs using operations such as adding, merging, splitting, and removing nodes, guided by a Lagrangian objective combining rate (complexity) and distortion (information loss). Applied to data science lecture notes, the refined KGs showed a 30.4% improvement in coverage and generated MCQs that scored on average 15% higher on 15 quality criteria compared to those from raw lecture notes. The approach demonstrates that rate-distortion guided optimization yields compact, information-rich KGs, supporting better AI-assisted educational content generation and personalized learning.

## Method Summary
The framework parses Markdown lecture notes into flattened segments with content and path metadata, embeds them using a pre-trained sentence model, and computes a fused distance matrix combining chronological, logical, and semantic components. An initial KG is bootstrapped via LLM using a predefined ontology of 12 relations. Both lecture content and KG are modeled as metric-measure spaces, and their alignment is optimized using fused Gromov-Wasserstein (FGW) optimal transport. The Lagrangian objective L = R + βD is iteratively minimized by applying graph edit operations (add, merge, split, remove, rewire) guided by analysis of the FGW coupling matrix. Refinement converges at the "knee point" of the rate-distortion curve, balancing KG compactness with information fidelity. The resulting KG is used to generate higher-quality MCQs compared to raw lecture notes.

## Key Results
- Refined KGs achieved 30.4% higher coverage than raw lecture notes.
- MCQs generated from refined KGs scored on average 15% higher across 15 quality criteria.
- The rate-distortion framework successfully identified an optimal trade-off point (knee) balancing KG complexity and information retention.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A quantifiable trade-off function (Lagrangian objective) enables principled KG refinement, rather than relying on heuristics.
- **Mechanism:** The framework defines Rate (R) as KG size (complexity) and Distortion (D) as the Fused Gromov-Wasserstein (FGW) distance between the source lecture content (a metric-measure space) and the candidate KG. The combined objective, L = R + βD, is iteratively minimized using local refinement operators (add, merge, split, remove, rewire). An operation is accepted if it improves L, creating a data-driven, quantifiable signal for KG edits.
- **Core assumption:** The FGW distance is a valid proxy for the "information loss" or semantic fidelity between a lecture's content and a knowledge graph representation.
- **Evidence anchors:**
  - [abstract] Mentions the framework is "grounded in rate-distortion (RD) theory" and minimizes a "rate-distortion Lagrangian."
  - [Section III.A] Formally defines L = R + βD and states, "This formalism gives us a target for optimization and a way to quantitatively evaluate any candidate knowledge graph."
  - [corpus] Weak direct evidence. Neighbor papers focus on KG construction for RAG or domain-specific tasks but do not directly validate an FGW-based rate-distortion objective.
- **Break condition:** The FGW distance fails to capture critical semantic or structural nuances (e.g., fine-grained pedagogical flow) and provides a noisy or misleading optimization signal, leading to degraded KG quality.

### Mechanism 2
- **Claim:** Jointly aligning semantic and structural information improves the quality of KGs extracted from unstructured text.
- **Mechanism:** The method models both the source lecture notes and the candidate KG as metric-measure spaces. It then uses Fused Gromov-Wasserstein (FGW) optimal transport to compute a coupling matrix (π). This coupling minimizes costs from both feature/semantic mismatch (e.g., embedding distance) and structural/relational mismatch (e.g., difference in internal distances), guiding refinement operations to fix misalignments.
- **Core assumption:** Lecture content and KG concepts can be meaningfully embedded in a shared or comparable metric space where optimal transport can be computed efficiently.
- **Evidence anchors:**
  - [abstract] States the framework "quantifies semantic and relational alignment" using FGW optimal transport.
  - [Section III.A, Figure 3] Explains that FGW combines structural and feature costs, with Figure 3 visualizing the coupling.
  - [corpus] Paper 75182 and 34075 discuss KG construction and relation extraction but do not provide evidence for FGW-based alignment specifically.
- **Break condition:** The assumptions about metric spaces break down, for instance, if lecture segments are not semantically coherent units or if the defined distances (chronological, logical, semantic) do not capture the true pedagogical structure, making the computed coupling meaningless.

### Mechanism 3
- **Claim:** A compact, information-rich KG produces higher-quality educational content (e.g., MCQs) than raw lecture notes.
- **Mechanism:** The iterative refinement process, guided by the rate-distortion objective, converges to a "knee point" on the RD curve. This point represents an optimal trade-off, yielding a KG that is compact (low R) yet covers key concepts with high fidelity (low D). This structured, non-redundant, and semantically aligned representation provides a superior prompt for an LLM, reducing hallucinations and improving the factual accuracy and pedagogical quality of generated MCQs.
- **Core assumption:** The "knee point" on the empirical RD curve corresponds to an optimal state for the downstream task (MCQ generation), and an LLM can effectively leverage the structured KG to produce better questions.
- **Evidence anchors:**
  - [abstract] Reports "MCQs that scored on average 15% higher on 15 quality criteria."
  - [Section V.D, Table I] Shows KG-based MCQs had a mean score of 4.70 vs. 4.08 for lecture-based MCQs.
  - [Section V.D, Table II] Details per-criterion improvements, notably in "no_answer_hints," "hard_to_guess," and "plausible distractors."
  - [corpus] Weak evidence. No corpus papers evaluate MCQ generation quality from rate-distortion optimized KGs.
- **Break condition:** The "knee point" does not generalize to the downstream task, or the LLM fails to interpret the KG structure correctly, resulting in MCQs that are no better (or potentially worse) than those from raw text.

## Foundational Learning

- **Concept: Rate-Distortion Theory**
  - **Why needed here:** This is the core theoretical framework. It provides the formal, information-theoretic basis for the entire paper's objective function.
  - **Quick check question:** In rate-distortion theory, what does the "knee" of the curve represent?

- **Concept: Optimal Transport (specifically Wasserstein and Gromov-Wasserstein distances)**
  - **Why needed here:** This is the mathematical tool used to quantify "distortion." Understanding FGW is essential to grasp how the framework aligns two different metric spaces (lecture notes and KG).
  - **Quick check question:** How does the Fused Gromov-Wasserstein distance differ from the standard Wasserstein distance?

- **Concept: Metric-Measure Spaces (mm-spaces)**
  - **Why needed here:** The paper formalizes both lecture content and KGs as metric-measure spaces (a set, a distance metric, and a probability measure). This abstraction is critical for applying FGW optimal transport.
  - **Quick check question:** What are the three components of a metric-measure space `(Z, d_Z, μ_Z)`?

## Architecture Onboarding

- **Component map:**
  1. **Source Space Constructor:** Parses Markdown notes, flattens structure, computes embeddings, and constructs the lecture's metric-measure space `(Z, d_Z, μ_Z)`.
  2. **KG Bootstrapper:** Uses an LLM to extract an initial KG from the notes based on a predefined ontology.
  3. **KG Space Constructor:** Takes the KG graph and constructs its metric-measure space `(V, d_V, μ_V)`.
  4. **FGW Aligner:** Computes the optimal coupling `π` and the distortion `D` using the Python Optimal Transport (POT) library's FGW solver.
  5. **Refinement Engine:** Iteratively proposes and applies graph edit operations (Add, Merge, Split, Remove, Rewire) based on analysis of the coupling `π`.
  6. **Optimization Controller:** Evaluates proposed edits against the Lagrangian `L = R + βD`, accepts beneficial changes, and detects convergence/knee point.

- **Critical path:**
  1. **Correct Metric Definition:** The definitions of distance metrics for both source (chronological, logical, semantic) and KG (structural, semantic) are paramount. Flawed metrics → flawed FGW alignment → flawed refinement.
  2. **Coupling Analysis:** The entire refinement logic hinges on interpreting the coupling matrix `π` (row/column mass, entropy) to trigger operations.

- **Design tradeoffs:**
  - **Computational cost vs. precision:** The exact FGW problem is hard; entropic regularization (Sinkhorn algorithm) makes it tractable but is an approximation.
  - **Cold-start quality:** The system bootstraps with an LLM. A poor initial KG could make alignment and convergence slower or suboptimal.
  - **Hyperparameter sensitivity:** The system has many hyperparameters (`β`, `λ`, `θ` thresholds). Performance likely depends on careful tuning.

- **Failure signatures:**
  - **Oscillating Refinement:** The KG oscillates between adding and removing the same node/edge, failing to converge. This suggests `β` is poorly tuned or the objective landscape is too flat/noisy.
  - **Trivial or Bloated KG:** The process converges to a single node (ignoring distortion) or a massive graph (ignoring rate), indicating an extreme `β` value.
  - **No Coverage Improvement:** The refinement runs, but the coverage metric fails to increase, indicating that the operations are not effectively addressing uncovered lecture content.

- **First 3 experiments:**
  1. **Reproduce RD Curve on Single Lecture:** Take one provided notebook, run the full pipeline, and plot the RD curve. Verify that it exhibits the characteristic "knee" shape as seen in Figure 7.
  2. **Ablation on Distance Components:** Create a variant where only the semantic distance is used for the source space (ignoring chronological and logical). Compare the resulting KG and MCQ quality to the full model to test the importance of structural signals.
  3. **Vary β Sensitivity:** Run the pipeline with different values of the Lagrange multiplier `β` (e.g., low, medium, high). Inspect the resulting KGs—expect a small, lossy graph for low `β` and a large, detailed graph for high `β`. Verify this matches theoretical expectations.

## Open Questions the Paper Calls Out

- **Question:** How can the semantic richness of refined KGs be enhanced to improve performance on hallucination prevention and option unambiguity in MCQ generation?
  - **Basis in paper:** [explicit] The authors state: "In future research, we will investigate strategies to enhance the semantic richness and contextual grounding of KGs to better support high-quality, unambiguous, and factually consistent MCQ generation."
  - **Why unresolved:** KG-based MCQs scored lower than lecture-based MCQs on no_hallucinations (-0.150) and unambiguous_options (-0.185), suggesting the concise, abstract KG representation lacks sufficient contextual cues for reliable MCQ generation.
  - **What evidence would resolve it:** A comparative study where KG nodes are enriched with additional context (e.g., example snippets, extended definitions) before MCQ generation, showing improved scores on hallucination and ambiguity criteria without sacrificing the gains in distractor quality.

- **Question:** How does the framework generalize to domains beyond introductory data science, particularly those with different pedagogical structures?
  - **Basis in paper:** [inferred] The evaluation is limited to eight weeks of data science lecture notes; no experiments or discussion address applicability to other disciplines or more advanced content.
  - **Why unresolved:** Domain characteristics (e.g., sequential dependency in programming vs. conceptual breadth in humanities) may affect the suitability of the chosen relations (prerequisiteOf, contrastsWith, etc.) and the behavior of the RD curve.
  - **What evidence would resolve it:** Application of the framework to at least two additional domains (e.g., history and physics) with domain-appropriate relation sets, reporting coverage improvements and MCQ quality comparisons.

- **Question:** What is the sensitivity of the RD-optimized KG to the choice of hyperparameters, particularly the Lagrangian trade-off coefficient β and distance fusion weights?
  - **Basis in paper:** [inferred] The paper sets β=100 and distance weights (α_chron, α_logic, α_sem) = (0.2, 0.3, 0.5) empirically, but provides no analysis of how changes affect the identified knee point or final KG quality.
  - **Why unresolved:** Small changes in β could shift the optimal operating point along the RD curve, potentially yielding different trade-offs between compactness and fidelity.
  - **What evidence would resolve it:** A systematic ablation study varying β across a range (e.g., 10–500) and reporting resulting coverage, distortion, and MCQ quality to identify stable operating regions.

- **Question:** Can scalable approximations (e.g., minibatch OT) maintain alignment quality when applied to larger lecture corpora with thousands of segments?
  - **Basis in paper:** [explicit] The authors note: "If scaling up, one might need more efficient heuristics or minibatch OT approximations."
  - **Why unresolved:** The current Sinkhorn-based FGW solver is manageable for hundreds of nodes/segments; computational cost may become prohibitive for full courses or multi-course curricula.
  - **What evidence would resolve it:** Benchmarking minibatch OT approximations against exact FGW on synthetic and real larger datasets, comparing distortion accuracy, runtime, and downstream MCQ quality.

## Limitations

- The FGW distance's effectiveness as a proxy for semantic fidelity depends on the quality of the distance metrics and embeddings; noisy or unrepresentative distances could lead to misleading refinement signals.
- The method assumes lecture notes can be cleanly segmented and embedded in a shared metric space, which may not hold for all educational content.
- The approach is sensitive to hyperparameters (β, θ thresholds) and the initial KG quality from LLM bootstrapping, which are not fully specified.

## Confidence

- **High confidence:** The core theoretical framework (rate-distortion theory, FGW optimal transport) is well-established. The formal definitions of the Lagrangian objective and the iterative refinement operators are explicitly stated.
- **Medium confidence:** The improvement in MCQ quality (15% higher scores) is supported by the reported experiment, but the robustness of this result to different LLM models, prompts, or lecture topics is unclear. The "knee point" heuristic for convergence is plausible but not rigorously validated as optimal for downstream tasks.
- **Low confidence:** The sensitivity of the method to the choice of embedding model, distance metric weights, and the specific LLM used for bootstrapping is not explored. The paper does not provide evidence that the FGW-based alignment is superior to simpler baselines.

## Next Checks

1. **Reproduce RD Curve:** Take a single provided lecture, run the full pipeline, and plot the RD curve. Verify it exhibits the characteristic "knee" shape as claimed.
2. **Ablate Distance Components:** Create a variant using only semantic distance for the source space (ignoring chronological and logical). Compare resulting KG and MCQ quality to the full model to test the importance of structural signals.
3. **Vary β Sensitivity:** Run the pipeline with different β values (e.g., low, medium, high). Inspect resulting KGs—expect a small, lossy graph for low β and a large, detailed graph for high β—and verify this matches theoretical expectations.