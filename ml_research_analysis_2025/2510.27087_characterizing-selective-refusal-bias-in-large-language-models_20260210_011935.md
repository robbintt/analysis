---
ver: rpa2
title: Characterizing Selective Refusal Bias in Large Language Models
arxiv_id: '2510.27087'
source_url: https://arxiv.org/abs/2510.27087
tags:
- refusal
- groups
- across
- language
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines selective refusal bias in LLM safety guardrails,
  revealing that models systematically refuse to generate toxic content targeting
  certain demographic groups while complying with similar requests targeting others.
  Through analysis of individual and intersectional groups across gender, nationality,
  religion, and sexual orientation, the study demonstrates statistically significant
  disparities in refusal rates and response lengths.
---

# Characterizing Selective Refusal Bias in Large Language Models

## Quick Facts
- **arXiv ID:** 2510.27087
- **Source URL:** https://arxiv.org/abs/2510.27087
- **Reference count:** 21
- **Primary result:** Systematic disparities in LLM refusal rates across demographic groups, with successful circumvention via indirect attacks

## Executive Summary
This study investigates selective refusal bias in large language model safety guardrails, revealing that models demonstrate systematic disparities in refusing to generate toxic content targeting different demographic groups. The research shows that models are more likely to comply with requests for harmful content targeting certain groups while refusing similar requests for others, with statistically significant differences in both refusal rates and response lengths. The findings highlight fundamental inequities in current safety systems that evaluate requests based on demographic identifiers rather than inherent content toxicity.

The research further demonstrates a critical vulnerability in these guardrails through an indirect attack methodology that successfully circumvents safety refusals, suggesting that current protective measures are insufficient. The study's comprehensive analysis spans individual demographic groups (gender, nationality, religion, sexual orientation) and intersectional combinations, providing robust evidence of bias that cannot be explained by random variation. These findings call for fundamental rethinking of how safety systems should evaluate and respond to potentially harmful requests.

## Method Summary
The study employed a systematic experimental design using controlled prompt templates to test LLM safety guardrails across multiple demographic groups. Researchers created standardized prompts requesting toxic content targeting specific groups, including individual categories (gender, nationality, religion, sexual orientation) and intersectional combinations. The methodology measured two key metrics: refusal rates (whether the model declined to generate the requested content) and response lengths (the extent of content produced when not refused). Statistical analysis was performed to identify significant disparities across groups, and an indirect attack methodology was developed to test guardrail vulnerabilities by rephrasing harmful requests in ways that bypassed safety mechanisms.

## Key Results
- Statistically significant disparities in refusal rates across demographic groups (p-values ranging from 1.26e-7 to 2.39e-16)
- Models produce longer responses for toxic content targeting certain demographic groups compared to others
- Indirect attack methodology successfully circumvents safety refusals, demonstrating guardrail vulnerabilities

## Why This Works (Mechanism)
The selective refusal bias operates through the model's training data and fine-tuning processes, where safety guardrails appear to make decisions based on demographic identifiers rather than the inherent toxicity of requested content. The mechanism likely involves learned associations between certain demographic groups and content moderation policies, leading to inconsistent application of safety measures. The longer responses for certain groups suggest the model may be "facilitating" rather than refusing harmful content in some cases, indicating that guardrails are not uniformly applied.

## Foundational Learning
- **Statistical significance testing:** Why needed - to distinguish real bias patterns from random variation; Quick check - verify p-values and confidence intervals
- **Demographic group analysis:** Why needed - to identify intersectional biases and their compound effects; Quick check - ensure comprehensive coverage of relevant groups
- **Prompt engineering methodology:** Why needed - to create controlled experiments that isolate specific variables; Quick check - validate prompt templates produce consistent responses
- **Guardrail evaluation metrics:** Why needed - to measure both refusal behavior and content generation; Quick check - confirm metrics capture meaningful safety differences
- **Adversarial attack techniques:** Why needed - to test the robustness of safety mechanisms; Quick check - verify attack success rate across multiple models

## Architecture Onboarding

**Component Map:** Prompt Generator -> LLM with Safety Guardrails -> Response Analyzer -> Statistical Validator

**Critical Path:** User request → Safety evaluation → Decision (refuse/allow) → Content generation → Output delivery

**Design Tradeoffs:** The study reveals that current safety systems trade off between false positives (over-refusing benign requests) and false negatives (under-refusing harmful requests), with the tradeoff being applied unevenly across demographic groups. This suggests a need for content-based rather than identity-based safety evaluation.

**Failure Signatures:** Guardrails fail when requests are phrased indirectly, when demographic identifiers are embedded in complex contexts, and when intersectional identities are involved. Failures manifest as either unnecessary refusals or dangerous compliance with harmful requests.

**3 First Experiments:**
1. Test whether rephrasing toxic requests using euphemisms or indirect language consistently bypasses safety guardrails across multiple demographic groups
2. Compare refusal rates for identical toxic content requests with different demographic identifiers swapped
3. Measure response length differences when guardrails are bypassed versus when they function correctly

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- The study relies on controlled prompt templates that may not reflect real-world usage complexity
- Analysis focuses on specific demographic groups and may not generalize to all intersectional identities
- Indirect attack methodology represents only one approach to circumventing guardrails

## Confidence

**High confidence:** The core finding of systematic disparities in refusal rates across demographic groups is well-supported by statistical analysis (p-values ranging from 1.26e-7 to 2.39e-16). The methodological approach for measuring refusal rates and response lengths is sound and reproducible.

**Medium confidence:** The interpretation of longer responses for toxic content targeting certain groups as a "facilitative" behavior is plausible but requires additional context to fully validate. The study's conclusions about guardrail vulnerability through indirect attacks are supported but represent a single attack vector.

**Low confidence:** The generalizability of findings to production environments and other safety systems beyond those tested requires further validation. The relative severity of biases across different demographic groups may vary in real-world applications.

## Next Checks

1. Conduct a field study using naturalistic prompts from actual user interactions to validate whether observed biases persist outside controlled experimental conditions.

2. Test the indirect attack methodology across additional language models and safety systems to assess the universality of the identified vulnerability.

3. Perform longitudinal analysis to determine whether refusal biases evolve over time as models are updated and fine-tuned.