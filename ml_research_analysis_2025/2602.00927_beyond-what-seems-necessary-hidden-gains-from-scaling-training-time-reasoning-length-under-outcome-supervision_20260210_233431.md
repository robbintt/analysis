---
ver: rpa2
title: 'Beyond What Seems Necessary: Hidden Gains from Scaling Training-Time Reasoning
  Length under Outcome Supervision'
arxiv_id: '2602.00927'
source_url: https://arxiv.org/abs/2602.00927
tags:
- reasoning
- length
- arxiv
- frac
- scaling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper investigates how scaling reasoning length during training\u2014\
  via token budget in RL fine-tuning or loop count in looped Transformers\u2014affects\
  \ out-of-distribution (OOD) generalization under outcome-only supervision. Theoretically,\
  \ it shows that increasing self-iteration changes the effective hypothesis class,\
  \ enabling better OOD performance even after in-distribution (ID) accuracy saturates,\
  \ via mechanisms like stronger inductive bias or regularization suppressing shortcut\
  \ solutions."
---

# Beyond What Seems Necessary: Hidden Gains from Scaling Training-Time Reasoning Length under Outcome Supervision

## Quick Facts
- arXiv ID: 2602.00927
- Source URL: https://arxiv.org/abs/2602.00927
- Reference count: 40
- Primary result: Increasing reasoning length during training under outcome-only supervision yields OOD gains even after ID accuracy plateaus.

## Executive Summary
This paper investigates how scaling reasoning length during training—via token budget in RL fine-tuning or loop count in looped Transformers—affects out-of-distribution (OOD) generalization under outcome-only supervision. Theoretically, it shows that increasing self-iteration changes the effective hypothesis class, enabling better OOD performance even after in-distribution (ID) accuracy saturates, via mechanisms like stronger inductive bias or regularization suppressing shortcut solutions. Empirically, increasing loop counts in looped Transformers on synthetic tasks and token budgets in RL fine-tuning on math problems both show ID performance plateauing while OOD accuracy continues improving. This suggests training with longer reasoning lengths than what appears sufficient for ID validation can yield substantial OOD gains.

## Method Summary
The paper employs two main experimental setups: (1) Looped Transformers on a 4-hop induction synthetic task, where a single Transformer block is applied k times with shared weights, and (2) RL fine-tuning of Qwen2.5-1.5B-Instruct on math problems using GRPO with varying token budgets. The theoretical analysis examines how compositional iteration (F∘ᵏ) changes the hypothesis class and how regularization interacts with shortcut solutions. The key insight is that training with longer reasoning lengths than what appears sufficient for ID validation can yield substantial OOD gains, even when ID performance plateaus.

## Key Results
- Increasing loop counts in looped Transformers on synthetic tasks shows ID accuracy plateauing while OOD accuracy continues improving.
- Increasing token budgets in RL fine-tuning on math problems shows similar ID-OOD divergence, with ID saturating but OOD improving.
- Theoretical analysis shows that self-iteration can induce stronger inductive bias, reshape ID-optimal solutions, and enable better OOD generalization under outcome supervision.

## Why This Works (Mechanism)

### Mechanism 1: Inductive Bias Strengthening Through Compositional Constraints
Increasing self-iteration imposes structural constraints (e.g., requiring functions to admit k-th compositional roots) that shrink or reshape the hypothesis class toward more generalizable solutions. When the base hypothesis class F is sufficiently overparameterized, F_k := F∘ᵏ doesn't expand capacity—it restricts solutions to those representable as k-fold compositions. This can eliminate spurious interpolants that achieve zero ID error but fail OOD, forcing uniqueness toward the true target.

### Mechanism 2: Regularization Suppressing Shortcut Reliance
When shortcut solutions persist in F_k, appropriate regularization can increasingly penalize them relative to structured solutions as k grows, yielding OOD gains without ID gains. The hypothesis class decomposes as F_k = {g + h : g ∈ G_k, h ∈ H_k} where G_k contains transferable mechanisms and H_k contains shortcuts. Regularization (implicit via gradient descent, or explicit via weight decay/KL constraints in RL) penalizes H_k more strongly.

### Mechanism 3: Convergence Toward Algorithmic Solvers (In-Context GD)
For certain architectures (linear transformers), increasing self-iterations steers ID minimizers toward parameter configurations that implement generic algorithms (gradient descent) rather than dataset-specific heuristics. The infinite-context loss landscape over (V,W) has multiple global minima at small k, but as k increases, the distance between any ID minimizer and the GD-implementing family Θ_GD decreases exponentially.

## Foundational Learning

- **Concept: Hypothesis class and compositional iteration**
  - Why needed here: The paper's core argument hinges on how F_k = F∘ᵏ changes the set of learnable functions.
  - Quick check question: For a function class F containing all endomorphisms on S, does F∘² contain more or fewer functions than F?

- **Concept: Shortcut learning and distribution shift**
  - Why needed here: Mechanism 2 relies on distinguishing transferable patterns (valid on Q) from shortcuts (valid on P only).
  - Quick check question: If a model achieves 100% ID accuracy by memorizing training examples, would you expect zero OOD accuracy? Under what conditions might this fail?

- **Concept: Looped Transformers and weight-tying**
  - Why needed here: The empirical validation uses looped Transformers—understanding that the same block is applied k times with shared weights is essential for connecting theory to practice.
  - Quick check question: How does parameter count change when increasing loops from k=2 to k=10 in a looped Transformer?

## Architecture Onboarding

- **Component map:**
  - Looped Transformer: embedding layer → [shared Transformer block × k loops] → classification head
  - RL fine-tuning pipeline: base LLM → GRPO with token budget → LoRA adapter updates → correctness reward

- **Critical path:**
  1. Define k (loop count or token budget) before training
  2. Train with outcome-only supervision (no process labels)
  3. Evaluate ID and OOD separately; expect ID saturation before OOD plateau

- **Design tradeoffs:**
  - Larger k → longer training time, potential overfitting to ID if shortcuts persist
  - Smaller k → faster training, but may select spurious solutions with high OOD error
  - Regularization strength: too weak → shortcuts dominate; too strong → structured component underfits

- **Failure signatures:**
  - ID accuracy saturates at very small k but OOD accuracy remains flat → shortcuts may be k-invariant or regularizer is ineffective
  - OOD accuracy degrades with larger k → overfitting or target not in F_k for larger k
  - Response length collapses early in RL training (Fig. 3) → model learned non-compositional shortcut

- **First 3 experiments:**
  1. Replicate p-hop induction task with looped Transformers, sweeping k ∈ {1, 2, 4, 8, 16, 32}; plot ID vs OOD accuracy curves to confirm divergence pattern.
  2. On a math reasoning dataset, train with GRPO at token budgets {128, 256, 512, 1024, 4096}; fix evaluation budget to 4096 to isolate training-time effect.
  3. Ablate regularization: train with and without KL penalty/weight decay; measure whether OOD gains at large k disappear without regularization (tests Mechanism 2).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the benefits of scaling training-time reasoning length be replicated under explicit process supervision?
- **Basis in paper:** The paper explicitly isolates "outcome-only supervision" as the setting where intermediate computation is learned implicitly.
- **Why unresolved:** It is unknown if the mechanism of "hidden gains" via regularization or inductive bias is necessary only when supervision is sparse, or if it complements dense process-level rewards.
- **What evidence would resolve it:** Experiments comparing OOD generalization curves for models trained with Process Reward Models (PRMs) versus Outcome Reward Models (ORMs) across varying training budgets.

### Open Question 2
- **Question:** How can we characterize "seemingly natural" shortcuts that persist despite self-iteration and regularization?
- **Basis in paper:** The conclusion notes the work "does not rule out 'seemingly natural' shortcuts that persist during self-iteration yet are not strongly discouraged by the regularizer."
- **Why unresolved:** The theory proves gains occur *if* regularization suppresses shortcuts, but offers no guarantees for shortcuts that align well with the training objective or regularizer.
- **What evidence would resolve it:** Identification of specific failure modes or synthetic tasks where increasing reasoning length fails to improve OOD performance due to specific persistent shortcuts.

### Open Question 3
- **Question:** Do these hidden gains scale to larger foundation models (>1.5B parameters) with different implicit biases?
- **Basis in paper:** Empirical validation relies on a 1.5B parameter model (Qwen2.5-1.5B-Instruct) and synthetic looped transformers.
- **Why unresolved:** Larger models may possess stronger inherent reasoning capabilities or different shortcut structures, potentially altering the trade-off where ID performance plateaus but OOD continues to improve.
- **What evidence would resolve it:** Replicating the RL token-budget scaling experiments on significantly larger models (e.g., 7B or 70B) to observe if the divergence between ID and OOD performance persists.

## Limitations

- The theoretical framework rests on strong assumptions about shortcut structures and regularization behavior across compositional iterations that may not hold in realistic training scenarios.
- Empirical validation is limited to one synthetic task and one math reasoning dataset, with minimal ablation studies on regularization effects.
- Claims about the exact nature of shortcuts in LLM reasoning are not directly tested and remain hypotheses pending further investigation.

## Confidence

**High Confidence:** The core empirical observation—that increasing reasoning length (loops or token budget) can improve OOD accuracy even after ID accuracy plateaus—is well-supported by the presented experiments and aligns with prior work on shortcut learning.

**Medium Confidence:** The theoretical mechanisms (inductive bias strengthening, regularization suppression, algorithmic convergence) are logically coherent and mathematically grounded, but their applicability to realistic training scenarios is less certain due to simplifying assumptions.

**Low Confidence:** Claims about the exact nature of shortcuts in LLM reasoning (e.g., that they are k-invariant or easily penalized) are not directly tested; these remain hypotheses pending further investigation.

## Next Checks

1. **Ablation of Regularization Strength:** Train RL models with and without KL penalty/weight decay across token budgets; measure whether OOD gains at large budgets disappear without regularization. This would directly test whether Mechanism 2 (regularization suppressing shortcuts) is necessary.

2. **Controlled Shortcut Injection:** On the 4-hop task, artificially inject a shortcut solution (e.g., always output the last symbol) and measure whether increasing k + regularization can recover the structured solution. This would validate the assumptions behind Mechanism 2 in a controlled setting.

3. **Broader Task Suite:** Replicate the looped Transformer experiments on a second synthetic task (e.g., sorting or arithmetic) and a second math dataset (e.g., GSM8K or MATH). Compare ID vs OOD scaling curves to assess generalizability of the "hidden gains" phenomenon.