---
ver: rpa2
title: 'Landmarks, Monuments, and Beacons: Understanding Generative Calls to Action'
arxiv_id: '2509.19030'
source_url: https://arxiv.org/abs/2509.19030
tags:
- game
- player
- they
- landmarks
- also
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of evaluating complex procedurally
  generated content in games by introducing the concepts of Landmarks, Monuments,
  and Beacons. These nested concepts focus on the artefact's perceivability, evocativeness,
  and Call to Action from a player-centric perspective.
---

# Landmarks, Monuments, and Beacons: Understanding Generative Calls to Action

## Quick Facts
- arXiv ID: 2509.19030
- Source URL: https://arxiv.org/abs/2509.19030
- Authors: Victoire Hervé; Henrik Warpefelt; Christoph Salge
- Reference count: 40
- One-line primary result: Proposes a framework of nested salience categories (Landmarks, Monuments, Beacons) for evaluating complex procedurally generated game content

## Executive Summary
This paper addresses the challenge of evaluating complex procedurally generated content in games by introducing the concepts of Landmarks, Monuments, and Beacons. These nested concepts focus on the artefact's perceivability, evocativeness, and Call to Action from a player-centric perspective. The authors propose that these concepts can be automatically identified and evaluated using existing techniques, enabling better decomposition and assessment of PCG components. The framework aims to bridge the gap between humanities and technical game research, facilitating more embodied and experience-driven PCG evaluation.

## Method Summary
The paper introduces a nested framework for evaluating composite procedurally generated artefacts by decomposing them into three categories: Landmarks (perceivable + salient features), Monuments (evocative elements that trigger associations/memories), and Beacons (action-invoking elements). The method proposes computational detection through existing techniques including isovist analysis for visibility, player trace analysis for behavior prediction, and interpretation tools for evocativeness. The framework is designed to replace uniform metric application with targeted evaluation of experience-critical sub-components.

## Key Results
- Proposes hierarchical decomposition of PCG artefacts via nested salience categories (Landmarks, Monuments, Beacons)
- Framework bridges humanities concepts with implementable evaluation methods
- Claims existing techniques (isovists, salience metrics, player modeling) can be repurposed for LMB detection
- Aims to enable more embodied, experience-driven PCG evaluation through automated decomposition

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hierarchical decomposition of composite artefacts via nested salience categories enables targeted evaluation of experience-critical sub-components
- Mechanism: The framework replaces uniform metric application (which treats all parts equally) with a three-tier filter: Landmarks (perceivable + salient), Monuments (evocative), Beacons (action-invoking). This allows automated systems to focus computational resources on the subset of content most likely to determine player experience, rather than averaging across undifferentiated components
- Core assumption: Players experience composite artefacts through selective attention to salient features, not holistic uniform perception; what stands out drives experience disproportionately
- Evidence anchors:
  - [abstract] "automatic decomposition as a possible solution requires concepts that meet a range of properties... nested concepts of Landmarks, Monuments, and Beacons"
  - [section 1, page 1-2] "Players might focus, and have their experience determined by a small, meaningful subset of the artefact, but an automatic metric might treat all parts of the overall artefact the same... entropy measure was mostly dominated by the large extent of the still unaltered map"
- Break condition: If player experience is driven by accumulated micro-interactions rather than salient peaks, decomposition may miss emergent experience properties

### Mechanism 2
- Claim: Evocativeness and Call-to-Action provide player-centric grounding for metrics that otherwise correlate poorly with human judgement
- Mechanism: By requiring computational detection to assess (a) whether content triggers associations/memories and (b) whether it intrinsically motivates action, the framework forces metrics to incorporate interpretive and motivational layers absent from purely structural measures
- Core assumption: Evocation and intrinsic motivation are measurable or approximable via existing techniques (player traces, agent behavior, LLM-based interpretation) with sufficient accuracy for PCG optimization
- Evidence anchors:
  - [section 2.1, page 2] Draws on indicators (Warpefelt), indexes (Fernandez-Vara), and perceived affordances (Cardona-Rivera) as theoretical antecedents
  - [section 5.2-5.3, page 6-7] Proposes empirical recall tests, reaction recording, player modeling, and trace analysis for Monument/Beacon detection
- Break condition: If evocation/action detection requires player-specific cultural/background knowledge that cannot be reliably modeled, automated evaluation will produce false positives/negatives

### Mechanism 3
- Claim: Existing PCG metrics and techniques (isovists, salience, player traces) can be repurposed for LMB detection without requiring novel fundamental methods
- Mechanism: Landmarks via visibility/salience tests (referencing Canossa et al. and Hervé et al. isovist work); Monuments via interpretation/memorability proxies (LLMs, recall studies); Beacons via player modeling and trace-based behavior prediction
- Core assumption: Techniques validated in narrow domains (e.g., Minecraft isovists) generalize across genres and artefact types (narrative, soundscapes, items)
- Evidence anchors:
  - [section 5.1, page 6] "similar approach to existing work [17, 21], which could be tuned for Landmarks detection"
  - [section 5.3, page 7] "existing work on predicting player behaviour, through agent-based evaluation [31], and player modelling [36]"
- Break condition: If generalization across domains fails, per-game bespoke implementations become necessary, reducing framework value

## Foundational Learning

- Concept: Expressive Range Analysis (ERA)
  - Why needed here: The paper positions LMB as a response to ERA's limitations—ERA reduces artefacts to scalar metrics without embodiment; understanding what ERA does helps grasp what LMB tries to fix
  - Quick check question: Can you explain why entropy-based ERA might show high variety while players perceive generated content as indistinguishable "oatmeal"?

- Concept: Affordances (perceived vs. actual)
  - Why needed here: Beacons build on perceived affordances—understanding the gap between what an object supports and what a player believes is possible is essential for Call-to-Action design
  - Quick check question: Given a game object, can you distinguish its designed affordances from what a novice player might perceive as possible actions?

- Concept: Composite Artefacts and Hierarchical Composition
  - Why needed here: The paper's core claim is that composite artefacts (settlements made of houses, houses of rooms) resist uniform evaluation; recognizing hierarchical structure is prerequisite to applying LMB decomposition
  - Quick check question: For a generated game narrative, can you identify at least two hierarchical levels and explain why uniform metrics might fail?

## Architecture Onboarding

- Component map: Landmark Detector -> Monument Classifier -> Beacon Predictor -> Aggregation Layer
- Critical path:
  1. Implement Landmark detection first—it requires only existing geometry/perception metrics
  2. Validate Landmark detection correlates with player attention (eye-tracking or self-report)
  3. Layer Monument classification with domain-specific interpretation rules
  4. Layer Beacon prediction with player modeling calibrated to target audience
  5. Integrate into PCG fitness function or quality filter

- Design tradeoffs:
  - **Generic vs. tailored implementations**: Generic detectors reduce engineering but risk false positives; tailored implementations increase accuracy at integration cost
  - **Automated vs. human-in-loop**: Fully automated evaluation scales but may miss cultural nuance; human validation improves grounding but limits throughput
  - **Binary vs. spectrum classification**: Strict thresholds simplify logic but misrepresent gradations; spectrum scores better match reality but complicate downstream decision-making

- Failure signatures:
  - **Landmark over-detection**: Too many flagged features dilute salience, making the concept meaningless
  - **Monument false positives**: Content labeled evocative but players report no emotional/cognitive response
  - **Beacon misalignment**: Predicted actions do not match observed player behavior (e.g., players ignore "obvious" attractors)
  - **Cross-domain transfer failure**: Techniques calibrated on spatial artefacts perform poorly on narrative or soundscape domains

- First 3 experiments:
  1. **Landmark attention validation**: In an existing game with known landmarks (e.g., Assassin's Creed towers), compare computed salience/visibility scores against player gaze data or retrospective recall. Confirm correlation before extending to generated content
  2. **Monument recall test**: Generate variants of a game scene with and without theoretically evocative elements (e.g., wreckage vs. pristine ship). Measure player recall and reported emotional response after brief exposure. Test whether proposed interpretation proxies predict outcomes
  3. **Beacon behavior trace analysis**: In a PCG environment (e.g., Minecraft settlement generator), instrument player sessions to record approach/interaction with candidate beacons. Compare trace-predicted actions against observed behavior. Iterate on Call-to-Action model weights

## Open Questions the Paper Calls Out

- **Open Question 1**: Can the density or distribution of Landmarks serve as a reliable predictor for player fatigue or disorientation?
  - Basis in paper: [explicit] The authors propose in the Future Work section that Landmark detection could be investigated as a predictor for player fatigue
  - Why unresolved: The paper currently focuses on defining the theoretical framework and implementation possibilities; no empirical studies have been conducted to link Landmark distribution to physiological or cognitive player states
  - What evidence would resolve it: Empirical data correlating specific Landmark spatial frequencies or absences with measurable player fatigue or lostness metrics

- **Open Question 2**: How does the framework accommodate the social construction of Landmarks in multiplayer environments?
  - Basis in paper: [explicit] The Future Work section notes that the authors have not yet addressed the social aspect of games, where player groups create their own Landmarks
  - Why unresolved: The current definitions rely on individual perception and game design intent, failing to account for emergent, community-agreed significance of game objects
  - What evidence would resolve it: An extension of the theory that incorporates multiplayer dynamics, validated by studies comparing designer-intended Landmarks with those recognized by player communities

- **Open Question 3**: To what degree can the subjective "evocativeness" of Monuments be reliably automated using current computational techniques?
  - Basis in paper: [inferred] While the authors argue that these entities can be found using existing techniques, they admit that identifying "evocativeness" (Monuments) relies on player background and memory, making it "difficult to automatise"
  - Why unresolved: The paper suggests potential tools (e.g., LLMs, thematic analysis) but provides no proof that algorithms can successfully model the complex, culturally dependent associations required to identify a Monument
  - What evidence would resolve it: A functional implementation where an AI agent successfully identifies Monuments with a high correlation to human player interpretations across diverse cultural backgrounds

## Limitations

- No empirical validation provided for LMB framework's effectiveness in actual PCG evaluation scenarios
- Critical thresholds and classification boundaries for Landmark/Monument/Beacon detection remain undefined and subjective
- Assumes existing techniques (isovists, player modeling, LLMs) can reliably detect evocation and Call-to-Action without player-specific knowledge
- No reference implementation, code, or annotated dataset is provided for reproduction

## Confidence

- **High confidence**: The conceptual framework for nested salience categories is well-grounded in game studies literature and addresses a real problem in PCG evaluation
- **Medium confidence**: The proposed computational detection methods (isovists, salience metrics, player modeling) are technically feasible but their accuracy for LMB detection is unverified
- **Low confidence**: Claims about cross-domain generalization and superiority over existing evaluation methods lack empirical support

## Next Checks

1. **Cross-domain transferability test**: Implement LMB detection on at least two distinct game types (e.g., spatial exploration game and narrative-driven game) and measure detection accuracy and correlation with player experience in each
2. **Ablation study against baselines**: Compare PCG quality predictions using LMB-based evaluation versus traditional metrics (entropy, diversity, player preference surveys) on the same content set
3. **Player perception validation**: Conduct user studies to verify that automatically detected Landmarks, Monuments, and Beacons correspond to what players actually notice, remember, and act upon in generated content