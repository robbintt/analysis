---
ver: rpa2
title: LLMs Can Assist with Proposal Selection at Large User Facilities
arxiv_id: '2512.10895'
source_url: https://arxiv.org/abs/2512.10895
tags:
- human
- proposal
- proposals
- llms
- ranking
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explores the use of large language models (LLMs) to
  automate and improve proposal selection at large user facilities. Traditional human
  scoring is inconsistent and labor-intensive; the authors propose using LLMs for
  pairwise comparisons between proposals, enabling a Bradley-Terry model ranking that
  is both rigorous and scalable.
---

# LLMs Can Assist with Proposal Selection at Large User Facilities
## Quick Facts
- arXiv ID: 2512.10895
- Source URL: https://arxiv.org/abs/2512.10895
- Reference count: 0
- LLMs can automate proposal selection at user facilities with high correlation to human rankings and significant cost savings

## Executive Summary
This study demonstrates that large language models can effectively assist with proposal selection at large user facilities by performing pairwise comparisons between proposals. The authors show that LLM rankings correlate strongly with human rankings (Spearman ρ ≈ 0.2–0.8) and perform as well as humans in identifying high-publication-potential proposals while reducing costs by over two orders of magnitude. The methodology enables scalable, consistent evaluation that addresses the labor-intensive and inconsistent nature of traditional human scoring processes.

## Method Summary
The authors developed a methodology using LLMs to perform pairwise comparisons between proposals, which feeds into a Bradley-Terry model for ranking. They applied this approach to proposals from three SNS beamlines, comparing LLM rankings against human rankings. The study also utilized LLM embeddings for quantitative similarity analysis between proposals, enabling duplicate detection and advanced review insights. The computational framework was designed to handle large volumes of proposals while maintaining rigorous evaluation standards.

## Key Results
- LLM rankings showed strong correlation with human rankings (Spearman ρ ≈ 0.2–0.8), improving to ≥0.5 after outlier removal
- LLMs performed as well as humans in identifying high-publication-potential proposals
- Implementation reduced costs by over two orders of magnitude compared to human-only review

## Why This Works (Mechanism)
LLMs excel at processing and comparing complex textual information across multiple proposals simultaneously. By using pairwise comparisons rather than absolute scoring, the approach reduces cognitive load and enables more consistent evaluation. The Bradley-Terry model provides a statistically rigorous framework for aggregating pairwise judgments into overall rankings. LLM embeddings capture semantic similarity between proposals, enabling automated duplicate detection and clustering analysis that would be prohibitively time-consuming for human reviewers.

## Foundational Learning
1. **Bradley-Terry Model** - Why needed: Converts pairwise comparisons into global rankings; Quick check: Verify model convergence and ranking stability across different proposal subsets
2. **Pairwise Comparison Methodology** - Why needed: Reduces cognitive load compared to absolute scoring; Quick check: Ensure consistent pairwise judgment patterns across different proposal types
3. **LLM Embeddings for Similarity Analysis** - Why needed: Enables automated duplicate detection and clustering; Quick check: Validate embedding-based similarity against human-identified duplicates
4. **Spearman Correlation Analysis** - Why needed: Measures rank correlation between LLM and human evaluations; Quick check: Calculate confidence intervals for correlation coefficients
5. **Cost Analysis Framework** - Why needed: Quantifies computational versus human review costs; Quick check: Account for infrastructure and validation costs in total cost calculations
6. **Publication Potential Assessment** - Why needed: Validates LLM effectiveness in predicting research impact; Quick check: Track actual publication outcomes for both LLM-selected and human-selected proposals

## Architecture Onboarding
**Component Map**: Proposal Database -> LLM Pairwise Comparator -> Bradley-Terry Ranker -> Spearman Correlation Calculator -> Cost Analyzer -> Similarity Embeddings Generator

**Critical Path**: Proposal input → Pairwise LLM comparisons → Bradley-Terry ranking → Correlation validation → Cost analysis → Similarity clustering

**Design Tradeoffs**: 
- Precision vs. computational cost: More pairwise comparisons increase accuracy but also runtime
- Generalizability vs. specialization: Models trained on diverse proposals may be less precise for specific domains
- Automation vs. oversight: Fully automated systems require robust validation mechanisms

**Failure Signatures**: 
- Inconsistent pairwise judgments indicating model confusion or proposal ambiguity
- Poor correlation with human rankings suggesting model bias or training gaps
- High computational costs exceeding anticipated savings
- False duplicate detection leading to proposal rejection errors

**3 First Experiments**:
1. Compare LLM pairwise judgments against human pairwise judgments on a subset of proposals
2. Test Bradley-Terry ranking stability when different proposal subsets are evaluated
3. Validate embedding-based similarity detection against manually identified duplicates

## Open Questions the Paper Calls Out
None

## Limitations
- Performance inconsistency across different proposal types and beamlines, with correlation improving only after outlier removal
- Reliance on pairwise comparisons may oversimplify complex scientific merit assessment
- Similarity analysis using embeddings has not been validated against ground truth duplicate identification

## Confidence
- Strong correlation with human rankings: Medium confidence (facility-specific validation needed)
- Cost reduction claims: High confidence (based on computational analysis)
- Performance matching human reviewers: Medium confidence (single facility scope limits generalizability)

## Next Checks
1. Conduct blind validation comparing LLM rankings against final outcomes (awards, publications) rather than intermediate human rankings
2. Test methodology across multiple facilities with different scientific domains and review criteria to assess generalizability
3. Perform A/B testing where some proposals are reviewed by LLMs and others by humans, then compare downstream outcomes including scientific productivity and reviewer satisfaction