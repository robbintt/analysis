---
ver: rpa2
title: RL for Reasoning by Adaptively Revealing Rationales
arxiv_id: '2506.18110'
source_url: https://arxiv.org/abs/2506.18110
tags:
- learning
- reward
- reasoning
- adaback
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces adaptive backtracking (AdaBack), a per-sample
  curriculum learning algorithm that dynamically adjusts the amount of supervision
  during RL training based on reward feedback. The key insight is that revealing partial
  prefixes of target outputs allows models to incrementally learn reasoning chains
  that are otherwise intractable under full supervision or sparse RL rewards.
---

# RL for Reasoning by Adaptively Revealing Rationales

## Quick Facts
- arXiv ID: 2506.18110
- Source URL: https://arxiv.org/abs/2506.18110
- Reference count: 40
- Key outcome: Adaptive backtracking (AdaBack) uses per-sample curriculum learning to dynamically adjust supervision during RL training, enabling models to learn long reasoning chains by incrementally completing partial solutions rather than requiring full target sequences.

## Executive Summary
This paper introduces AdaBack, a per-sample curriculum learning algorithm for RL that dynamically adjusts supervision length based on reward feedback. The method addresses the challenge of learning long reasoning chains where both sparse RL rewards and full supervision are problematic. By adaptively revealing only prefixes of target outputs and gradually reducing this supervision, AdaBack transforms intractable sparse-reward problems into a sequence of simpler sub-problems. The approach is demonstrated on both synthetic parity tasks (where it shows clear separation from SFT and standard RL) and mathematical reasoning benchmarks, with particular strength in out-of-distribution settings.

## Method Summary
AdaBack operates by maintaining per-sample intervals [ρ_min, ρ_max] that control the fraction of target output revealed as supervision. During training, it samples ρ from this interval to determine how much of the target sequence to reveal as a prefix. The model then generates rollouts conditioned on this prefix using GRPO, and the average reward across rollouts determines whether to increase or decrease supervision for that sample. If reward ≥ threshold τ, supervision decreases (making the task harder); if reward < τ, supervision increases. To ensure train-test distribution matching, 10% of samples have ρ=0 (no supervision). The method uses global moving averages to bootstrap new samples and operates without requiring SFT initialization, though it can be applied to both base and SFT-initialized models.

## Key Results
- Solves synthetic chain-of-parities task where both SFT and standard RL fail, achieving >0.8 reward within 1000 iterations
- On GSM8k and MATH benchmarks, AdaBack improves performance over standard RL and SFT+RL pipelines
- Applied to base models, AdaBack often matches or exceeds SFT-initialized counterparts, suggesting acquisition of new reasoning capabilities
- Shows strong performance on out-of-distribution settings: Base-7 GSM8k (unfamiliar numerical format) and Tensor-2 GSM8k (longer reasoning chains)
- Pass@k evaluation indicates AdaBack expands the model's solution space rather than just reweighting existing answers

## Why This Works (Mechanism)

### Mechanism 1: Per-Sample Curriculum Reduces Effective Search Depth
AdaBack transforms a sparse-reward problem with success probability p^n into n sub-problems each with probability Θ(p), making exploration tractable. By revealing a prefix Y_1:k of the target output, the model conditions on correct partial solutions and only needs to complete the suffix. The supervision ratio ρ is adjusted per-sample via stochastic binary search using reward feedback against threshold τ.

### Mechanism 2: Adaptive Difficulty Estimation via Multi-Rollout Rewards
Using average reward across multiple rollouts per sample provides a reliable estimate of sample-specific difficulty, enabling automatic curriculum progression without manual staging. GRPO generates k rollouts per question; the average reward r_t^(i) indicates whether the model can reliably complete sample i at current supervision level.

### Mechanism 3: SFT-Free Learning from Base Models
AdaBack applied to base models can match or exceed SFT-initialized RL, suggesting adaptive partial supervision may be a more effective prior than SFT initialization for some tasks. By starting with high supervision (ρ ≈ 1) and gradually reducing it, the model receives structured learning signal without ever requiring full imitation learning on complete rationales.

## Foundational Learning

- Concept: **Curriculum Learning in RL**
  - Why needed here: AdaBack is fundamentally a curriculum method applied to RL; understanding how progressive difficulty scheduling enables learning in sparse-reward settings is prerequisite.
  - Quick check question: Can you explain why starting rollouts near the goal state and moving backward helps in sparse-reward environments like Montezuma's Revenge?

- Concept: **Sparse Reward Exploration Problem**
  - Why needed here: The paper's central motivation is that standard RL fails when correct solutions are exponentially rare; understanding this failure mode is essential.
  - Quick check question: For a 16-step reasoning chain where each step has 50% chance of being correct, what is the probability of randomly sampling a fully correct solution?

- Concept: **Statistical Query Learning and Parity Complexity**
  - Why needed here: The synthetic chain-of-parities task is grounded in SQ learning theory; the separation result relies on degree-k parity sample complexity bounds.
  - Quick check question: Why does learning a degree-3 parity require Ω(d^2) samples while degree-2 requires only Õ(d) in the SQ model?

## Architecture Onboarding

- Component map:
  - Supervision Ratio Tracker [per-sample intervals] -> Reward Aggregator [average across rollouts] -> Binary Search Controller [update ρ bounds] -> Prefix Revealer [slice target at k=⌊ρ·m⌋] -> Global Average Bootstrapper [EMA for new samples] -> Zero-Injection Sampler [10% ρ=0 probability]

- Critical path:
  1. Load sample (X, Y) with current ρ^(i) from tracker
  2. Reveal prefix Y_1:k, generate k rollouts of suffix via GRPO
  3. Compute average reward r_t^(i) across rollouts
  4. Update [ρ_min^(i), ρ_max^(i)] based on r_t^(i) vs. τ
  5. Sample new ρ^(i)_(t+1) ~ U(ρ_min, ρ_max) or inject ρ=0 with 10% probability

- Design tradeoffs:
  - Threshold τ: Higher values slow curriculum progression but ensure competence before advancing
  - Number of rollouts: More rollouts improve difficulty estimation but increase compute (paper uses 8)
  - EMA momentum α: Controls how fast global averages adapt; affects cold-start behavior
  - Zero-injection probability: Higher values improve train-test matching but reduce curriculum effect

- Failure signatures:
  - Rewards plateau at ~0.1 (format-only): model learned output structure but not reasoning
  - ρ never decreases across training: task may be too hard at current model scale
  - Large train-test gap: insufficient zero-injection
  - No improvement on instruct-tuned models: pretraining already saturated task exposure

- First 3 experiments:
  1. Sanity check on chain-of-parities (L=8, n=512): Train with AdaBack vs. SFT+RL. Expected: AdaBack reaches >0.8 reward within 1000 iterations; SFT+RL plateaus at ~0.1.
  2. Ablation on threshold τ (GSM8k base model): Test τ ∈ {0.3, 0.5, 0.7}. Monitor convergence speed and final accuracy.
  3. Zero-injection ablation: Compare 0%, 10%, 20% ρ=0 injection on Tensor-2 GSM8k. Measure train-test gap and convergence.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can supervision schedules based on embedding-space regions preserve AdaBack's effectiveness in large dataset regimes where per-sample adaptation becomes impractical?
- Basis in paper: Authors state: "When the number of unique training samples is high, most examples are seen infrequently and therefore rely heavily on global moving averages for supervision scheduling. This may reduce the effectiveness of per-sample adaptation."
- Why unresolved: Current per-sample scheduling works when samples are revisited frequently, but scalability to data-rich settings remains untested.
- What evidence would resolve it: Experiments comparing per-sample vs. k-nearest-neighbor-based supervision scheduling on datasets with significantly more unique samples.

### Open Question 2
- Question: Under what precise conditions does AdaBack fail to provide benefits for instruct-tuned models or models with extensive pretraining exposure?
- Basis in paper: "For instruct-tuned models or models where pretraining has already exposed the model to most problem types, AdaBack together with standard RL training provide no benefits, underscoring their limitation in aiding exploration where uncertainty is low."
- Why unresolved: The paper observes this phenomenon but does not characterize the threshold of prior exposure or uncertainty levels at which AdaBack ceases to help.
- What evidence would resolve it: Systematic evaluation across models with varying pretraining exposure levels, measuring uncertainty and correlating with AdaBack's marginal gains.

### Open Question 3
- Question: Does AdaBack primarily expand the solution space, or does it also reweight existing solutions in ways that standard RL does not?
- Basis in paper: While pass@k results suggest solution space expansion, the authors note prior work claiming RL merely reweights distributions. The mechanism by which curriculum learning induces novel capabilities versus amplification remains incompletely characterized.
- Why unresolved: Pass@k improvements could arise from either mechanism or both; disentangling them requires more targeted analysis.
- What evidence would resolve it: Analysis of solution diversity metrics beyond pass@k, and comparison of solution trajectories between AdaBack-trained and standard RL models on held-out problems.

## Limitations
- The distinction between "learning new reasoning patterns" versus "optimizing existing ones" remains largely qualitative, relying on observed performance gaps rather than mechanistic interpretability.
- Claims about AdaBack's superiority over SFT+RL on instruct-tuned models are supported by data, but the interpretation that this reflects "discovery of novel solutions" requires further validation.
- The assertion that AdaBack enables base models to "match or exceed" SFT-initialized counterparts across all settings is overstated—results show this occurs in some cases but not universally.

## Confidence
- **High confidence**: The adaptive curriculum mechanism works as described. The per-sample binary search over supervision ratios is technically sound and experimental results on synthetic tasks are reproducible.
- **Medium confidence**: Claims about AdaBack's superiority over SFT+RL on instruct-tuned models are supported by data, but interpretation as "discovery of novel solutions" requires further validation.
- **Low confidence**: The assertion that AdaBack enables base models to "match or exceed" SFT-initialized counterparts across all settings is overstated.

## Next Checks
1. **Mechanistic validation**: Apply activation patching or attention pattern analysis to compare reasoning structures learned by AdaBack versus SFT+RL on the same base model, directly testing whether new solution paths are discovered.
2. **Curriculum sensitivity analysis**: Systematically vary τ and zero-injection probability across the full range (0.1-0.9, 0-0.5) to determine optimal hyperparameter settings and identify regimes where AdaBack fails versus succeeds.
3. **Pretraining data overlap audit**: Analyze the overlap between pretraining data and GSM8k/MATH test sets to quantify how much observed performance gains stem from memorization versus genuine reasoning capability acquisition.