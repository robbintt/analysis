---
ver: rpa2
title: Weight Space Representation Learning with Neural Fields
arxiv_id: '2512.01759'
source_url: https://arxiv.org/abs/2512.01759
tags:
- weight
- lora
- neural
- weights
- space
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether neural network weights can serve
  as effective data representations, focusing on implicit neural representations (INRs).
  The authors propose using low-rank adaptation (LoRA) within a pre-trained base neural
  field to create structured weight space representations.
---

# Weight Space Representation Learning with Neural Fields

## Quick Facts
- arXiv ID: 2512.01759
- Source URL: https://arxiv.org/abs/2512.01759
- Authors: Zhuoqian Yang; Mathieu Salzmann; Sabine Süsstrunk
- Reference count: 9
- Primary result: mLoRA achieves 0.073 Fréchet Distance on FFHQ compared to 0.241 for HyperDiffusion, the previous state-of-the-art

## Executive Summary
This paper demonstrates that neural network weights, when properly constrained, can serve as effective data representations for implicit neural representations (INRs). The authors introduce multiplicative LoRA (mLoRA), where weight updates are applied through element-wise multiplication rather than addition, and show it provides superior representation quality compared to standard additive LoRA. The method is evaluated across reconstruction, generation, and discriminative tasks on 2D and 3D data, establishing that neural network weights can capture semantic structure when appropriate constraints are applied.

## Method Summary
The method trains a base neural field via variational autodecoding, then fits low-rank adaptation weights per instance using multiplicative parameterization (W' = W ⊙ BA). Asymmetric masking eliminates permutation symmetry by freezing √d_out entries per row. For generation, a diffusion transformer with hierarchical layer encoder processes the weight representations to generate new instances. The approach constrains weight space through a pre-trained base model and low-rank adaptation, inducing structure that enables effective downstream tasks.

## Key Results
- mLoRA achieves 35.65 PSNR on FFHQ reconstruction vs 35.11 for standalone MLPs
- mLoRA-Asym shows superior linear mode connectivity compared to other weight representations
- Diffusion models using mLoRA weights generate higher-quality samples (0.073 FD on FFHQ) than previous weight-space methods (0.241 FD for HyperDiffusion)
- Classification and clustering confirm mLoRA weight representations capture semantic structure (90% accuracy with linear classifier on ShapeNet)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Constraining weight updates to a low-rank subspace via a pre-trained base model induces structure in weight space.
- **Mechanism:** A base neural field trained via variational autodecoding captures transferable features across instances. Fine-tuning via LoRA restricts adaptations to a shared low-dimensional subspace, reducing the curse of dimensionality and aligning weight configurations across instances.
- **Core assumption:** The base model's learned feature space contains semantically meaningful structure transferable to individual instances.
- **Evidence anchors:** [abstract] "constraining the optimization space through a pre-trained base model and low-rank adaptation (LoRA) can induce structure in weight space"; [Section 3.1] references Hu et al. (2022) showing LoRA weights converge to a subspace; [corpus] Related work "Ensuring Semantics in Weights of Implicit Neural Representations" (FMR=0.53) confirms INR weights as structured data modality
- **Break condition:** If base model is poorly trained or dataset lacks shared structure, the subspace constraint becomes misaligned and representations degrade.

### Mechanism 2
- **Claim:** Multiplicative LoRA preserves channel alignment better than additive LoRA, reducing weight entanglement.
- **Mechanism:** In mLoRA (W' = W ⊙ BA), each rank component applies per-channel scaling via diagonal matrices: W' = Σᵢ diag(bᵢ)W diag(aᵢ). This preserves the base network's channel structure. Additive LoRA mixes features across channels, reintroducing entanglement when channels must compensate for fixed large-magnitude weights.
- **Core assumption:** Neural field channels learn disentangled features when trained in a generative regime; multiplicative scaling respects this structure.
- **Evidence anchors:** [abstract] "multiplicative LoRA weights achieve high representation quality while exhibiting distinctiveness and semantic structure"; [Section 3.4] "Multiplicative LoRA avoids this issue by zeroing out frozen entries, effectively removing certain signal components rather than forcing compensation"; [Section 4.2] mLoRA-Asym shows exceptional linear mode connectivity while additive LoRA-Asym does not; [corpus] Weak direct evidence; corpus papers focus on weight generation rather than multiplicative vs additive parameterization
- **Break condition:** If base model channels are already entangled, multiplicative formulation cannot recover disentanglement.

### Mechanism 3
- **Claim:** Asymmetric masking eliminates permutation symmetry, enabling weights to converge to a linear mode.
- **Mechanism:** Freezing √d_out entries per row with shared positions across runs breaks the r!-fold permutation symmetry in LoRA's rank dimensions. For mLoRA, frozen entries are zeroed, naturally removing signal components. This forces unique identification of each rank component.
- **Core assumption:** Permutation symmetry is the primary obstacle to linear mode connectivity in LoRA weight spaces.
- **Evidence anchors:** [Section 3.4] formal proof of permutation symmetry in both additive and multiplicative LoRA; [Section 4.2, Figure 3] mLoRA-Asym maintains high weight similarity and low barrier even with large initialization perturbations; [Corollary 2.2] "multiplicative LoRA weights are aligned with base network channels once permutation symmetry is eliminated"; [corpus] "Structure Is Not Enough" (FMR=0.56) emphasizes behavior matching, not just weight structure—consistent with symmetry breaking being insufficient alone
- **Break condition:** For additive LoRA, asymmetric masking with large variance κ causes entanglement, counteracting benefits.

## Foundational Learning

- **Concept: Implicit Neural Representations (INRs)**
  - Why needed here: The entire method operates on INRs—networks that map coordinates to signal values. Understanding how INRs encode signals as continuous functions is prerequisite to understanding why their weights could be representations.
  - Quick check question: Can you explain why an INR fitting a single image can be stored as ~27K parameters instead of 128×128×3 pixels?

- **Concept: Low-Rank Adaptation (LoRA)**
  - Why needed here: The paper builds on LoRA's property that adaptations converge to a shared subspace. Understanding the standard additive formulation W' = W + BA is necessary to grasp why multiplicative modification matters.
  - Quick check question: For a weight matrix W ∈ ℝ^(256×512) and rank r=4, how many parameters does standard LoRA add vs full fine-tuning?

- **Concept: Permutation Symmetry in Neural Networks**
  - Why needed here: The paper's core motivation is that weight ambiguity from neuron reordering makes weights poor representations. Understanding this symmetry is essential to appreciate why asymmetric masking matters.
  - Quick check question: If you swap hidden neurons 3 and 7 in an MLP layer along with their incoming/outgoing weights, does the function change? How many equivalent configurations exist for a layer with r hidden units?

## Architecture Onboarding

- **Component map:**
  1. Base Model (frozen): Modulated neural field with mapping network → synthesis blocks with weight modulation per layer
  2. mLoRA Adapter (trained per instance): Low-rank matrices {A_l, B_l} for each synthesis layer, applied via element-wise multiplication
  3. Asymmetric Mask (fixed): Pre-determined frozen entries per layer, zeroed in mLoRA
  4. Diffusion Transformer: Hierarchical encoder treating (a^(i)_l, b^(i)_l) pairs as tokens → within-layer attention → cross-layer processing

- **Critical path:**
  1. Train base model via variational autodecoding (jointly optimize θ and latent codes {z_i})
  2. Freeze base, fit mLoRA weights per instance with shared initialization
  3. Collect weight representations, train diffusion model with hierarchical encoder
  4. Generate via DDIM sampling, instantiate neural fields from sampled weights

- **Design tradeoffs:**
  - Rank r: Higher r → more expressive but less constrained weight space (paper uses r=4)
  - Asymmetric mask density: More frozen entries → stronger symmetry breaking but reduced capacity
  - Base model training: Multi-stage progressive training needed for stability but computationally expensive

- **Failure signatures:**
  - LoRA-Asym showing 24.63 PSNR on FFHQ (vs 35.65 for mLoRA): Indicates additive masking with large κ causing entanglement
  - Poor 1-NNA near 100% in generation: Model collapsing to single mode, likely insufficient weight space structure
  - Classification accuracy near random: Weight representations not capturing semantics—check base model quality or parameterization mismatch

- **First 3 experiments:**
  1. **Reproduction sanity check**: Fit mLoRA-Asym to 10 instances from ShapeNet airplane; verify Chamfer Distance matches ~0.0241 range
  2. **Ablation on operation type**: Compare mLoRA vs LoRA (additive) on same instances without asymmetric masking; expect ~0.07 PSNR gap
  3. **Linear mode connectivity test**: Take two mLoRA-Asym fittings from different initializations, interpolate weights, plot reconstruction error along path—should see flat/low barrier per Figure 3

## Open Questions the Paper Calls Out
None

## Limitations
- The asymmetric masking mechanism requires careful hyperparameter tuning - variance κ and frozen entry density are not fully explored
- Method depends heavily on base model quality - poor base model training propagates to all downstream tasks
- Paper doesn't thoroughly examine generalization to datasets with weaker shared structure than FFHQ and ShapeNet

## Confidence
- **High Confidence:** mLoRA achieving better reconstruction quality than standard parameterization (directly measured on held-out data)
- **Medium Confidence:** Claims about semantic structure capture in weight space (clustering/linear classification results depend on base model quality)
- **Low Confidence:** The asymmetric masking mechanism's superiority - only one comparison point (LoRA-Asym failing) is provided, and the mechanism's sensitivity to hyperparameters is unclear

## Next Checks
1. Test asymmetric masking sensitivity: Systematically vary κ and frozen entry density on ShapeNet; plot reconstruction quality vs. mask parameters to identify optimal ranges
2. Cross-dataset generalization: Apply the complete pipeline to a dataset with weaker shared structure (e.g., LSUN churches vs. bedrooms) to test base model transfer limits
3. Additive vs multiplicative capacity: Fix the number of trainable parameters in both formulations and compare reconstruction quality on FFHQ to isolate the effect of parameterization choice