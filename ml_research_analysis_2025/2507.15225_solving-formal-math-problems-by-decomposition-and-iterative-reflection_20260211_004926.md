---
ver: rpa2
title: Solving Formal Math Problems by Decomposition and Iterative Reflection
arxiv_id: '2507.15225'
source_url: https://arxiv.org/abs/2507.15225
tags:
- proof
- formal
- lean
- arxiv
- prover
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Delta Prover is an agent-based framework that uses a general-purpose
  LLM (Gemini 2.5 Pro) to generate formal proofs in Lean 4 without requiring model
  fine-tuning or specialized training data. It introduces a novel reflective decomposition
  process, where the LLM first generates an informal proof plan and then translates
  it into a formal proof sketch using a custom DSL.
---

# Solving Formal Math Problems by Decomposition and Iterative Reflection

## Quick Facts
- arXiv ID: 2507.15225
- Source URL: https://arxiv.org/abs/2507.15225
- Reference count: 40
- Delta Prover achieves 95.9% success rate on miniF2F-test using general-purpose LLM without fine-tuning

## Executive Summary
Delta Prover is an agent-based framework that uses a general-purpose LLM (Gemini 2.5 Pro) to generate formal proofs in Lean 4 without requiring model fine-tuning or specialized training data. It introduces a novel reflective decomposition process, where the LLM first generates an informal proof plan and then translates it into a formal proof sketch using a custom DSL. This sketch is automatically decomposed into sub-problems, each solved via iterative proof repair—where the LLM refines its output based on Lean 4 verification feedback. The framework achieves a state-of-the-art 95.9% success rate on the miniF2F-test benchmark, surpassing models that require specialized training, and shows stronger test-time scaling than standard best-of-N sampling. Key ablation studies confirm that both iterative proof repair and reflective decomposition significantly contribute to this performance. Delta Prover demonstrates that general-purpose LLMs, guided by effective agentic structures, can effectively tackle complex formal theorem proving tasks.

## Method Summary
Delta Prover combines two core components: Iterative Proof Repair and Reflective Decomposition. The Iterative Proof Repair component uses an LLM to generate proof candidates that are verified by Lean 4, with the LLM receiving detailed error feedback and retrieved theorems to iteratively improve the proof. The Reflective Decomposition component first generates an informal proof plan, then translates it into a formal DSL sketch, extracts sub-problems, solves them independently via iterative repair, and consolidates the results. The framework automatically switches between these modes based on problem complexity, using the decomposition strategy when direct proof attempts fail. A custom DSL with four tactics (suppose, define, show_by, conclude) enables automatic sub-problem extraction and proof consolidation through Lean 4 metaprogramming.

## Key Results
- Achieves 95.9% success rate on miniF2F-test benchmark, surpassing state-of-the-art models requiring specialized training
- Outperforms standard best-of-N sampling with stronger test-time scaling as sample budget increases
- Iterative proof repair with feedback consistently outperforms parallel sampling approaches
- Reflective decomposition enables solving problems that direct proof attempts cannot, demonstrated on IMO 2019 P1

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sequential error-correction with Lean 4 feedback outperforms parallel sampling for proof generation.
- Mechanism: The LLM generates an initial proof candidate; if verification fails, the system augments the next prompt with (a) the failed proof, (b) Lean 4 error messages and tactic state, and (c) retrieved theorems/definitions matching errored identifiers. This creates a tightly coupled feedback loop where the LLM's reflection capabilities are leveraged to iteratively refine proofs rather than explore independently.
- Core assumption: The base LLM possesses sufficient reflection capability to correctly interpret compiler errors and translate them into meaningful proof repairs.
- Evidence anchors:
  - [abstract] "Delta Prover leverages the reflection and reasoning capabilities of general-purpose LLMs to interactively construct formal proofs in Lean 4"
  - [section 4.3] Ablation shows iterative repair (n > 1) consistently outperforms Best-of-N (n = 1) across budget ranges, with performance gap widening as budget increases
  - [corpus] Limited direct corpus comparison; related work (Hilbert, DeepSeek-Prover-V2) uses different repair/feedback mechanisms
- Break condition: If the LLM cannot correctly interpret Lean 4 error messages or repeatedly makes the same syntactic error, the repair loop stalls without convergence.

### Mechanism 2
- Claim: Problem decomposition with decomposition-level reflection enables solving problems that direct proof attempts cannot.
- Mechanism: For complex problems, the LLM first generates an informal proof plan, then translates it into a formal sketch using the custom DSL. Sub-problems are extracted and solved independently via iterative repair. Critically, if sub-problems remain unsolved, the LLM receives this failure information and regenerates the formal sketch with revised decomposition strategy.
- Core assumption: The LLM can estimate appropriate sub-problem granularity and revise decomposition strategy based on which sub-problems failed.
- Evidence anchors:
  - [section 2.1.2] "if any sub-problems remain unsolved, prompting the LLM to regenerate the formal sketch, informed by the list of unsolved sub-problems"
  - [section 4.3] On IMO 2019 P1, baseline iterative repair failed after 1024 API calls; Delta Prover with decomposition succeeded in ~332 calls by breaking into 83 sub-problems
  - [corpus] DeepSeek-Prover-V2 uses recursive subgoal decomposition via RL; DSP (Draft, Sketch, Prove) pioneered informal-to-formal decomposition but lacks decomposition-level reflection
- Break condition: If the LLM generates consistently poor decompositions (too fine-grained or too coarse) and cannot improve based on failure feedback, the decomposition repair loop exhausts without progress.

### Mechanism 3
- Claim: A custom DSL enables automatic extraction and re-integration of sub-problems that native Lean 4 cannot support.
- Mechanism: The DSL introduces four tactics (`suppose`, `define`, `show...by`, `conclude`) implemented via Lean 4 metaprogramming. The `show...by` tactic records proof states in a custom monad (PlayM); these are extracted as formal sub-problem statements, solved independently, and automatically consolidated via `conclude` using Lean's delaborator.
- Core assumption: The DSL syntax is simple enough for general-purpose LLMs to learn from in-context examples without pre-training exposure.
- Evidence anchors:
  - [section 2.2] "native Lean 4 syntax...is less suited for high-level proof sketching, isolating subproblem contexts, and automatically reintegrating subproofs"
  - [figure 3] Demonstrates how DSL scaffolds maintain context across nested sub-problems and automate proof consolidation
  - [corpus] No direct corpus comparison; this appears to be a novel contribution
- Break condition: If LLM-generated DSL code is syntactically invalid or semantically incoherent, sub-problem extraction fails at the kernel level before solving begins.

## Foundational Learning

- Concept: Lean 4 tactic proofs and goal states
  - Why needed here: Understanding how Lean 4 represents proof obligations as "goals" and how tactics transform goal states is essential for interpreting error messages and debugging failed proof attempts.
  - Quick check question: Given a Lean goal `⊢ n + 0 = n`, what tactic would complete this proof?

- Concept: Auto-formalization (informal to formal translation)
  - Why needed here: The reflective decomposition process requires translating natural language proof sketches into formal DSL code; errors in quantifier scope or implicit assumptions cause downstream failures.
  - Quick check question: How would you formalize "for all even integers n, n² is even" in Lean 4 syntax?

- Concept: LLM reflection and self-correction
  - Why needed here: The entire framework relies on the LLM's ability to reason about its own failed outputs; without this, iterative repair degrades to random search.
  - Quick check question: Given an error message "unknown identifier 'mul_comm'", what information would help an LLM repair this?

## Architecture Onboarding

- Component map:
  - **LLM (Gemini 2.5 Pro)**: Generates informal proofs, formal sketches (DSL), and proof repairs
  - **Retrieval model (Ψ)**: Retrieves relevant Mathlib theorems/definitions when identifiers error
  - **Lean 4 Kernel + DSL Environment (L)**: Verifies proofs, extracts sub-problems, consolidates proofs
  - **Orchestrator**: Routes between direct proof (Algorithm 1) and decomposition (Algorithm 2) based on failure

- Critical path:
  1. Attempt direct proof via Iterative Proof Repair (m rounds × n repairs)
  2. If failed, generate informal proof plan → formal DSL sketch
  3. Extract sub-problems; solve each via Iterative Proof Repair
  4. If any sub-problem fails, regenerate sketch with failure feedback (up to l_max attempts)
  5. If all sub-problems solved, consolidate via `conclude` tactic

- Design tradeoffs:
  - Higher n (repairs per round) exploits LLM self-correction; higher m (rounds) explores more initial hypotheses. Ablation suggests higher n is better for fixed budget.
  - Granular decomposition reduces individual sub-problem difficulty but increases orchestration overhead and DSL complexity.
  - Retrieval augmentation helps with unknown identifiers but adds latency per repair iteration.

- Failure signatures:
  - **Stall pattern**: Same error repeated across repair iterations → LLM not correctly interpreting feedback
  - **Decomposition explosion**: Very high sub-problem count (e.g., 83 for IMO problem) with many unsolved → decomposition too fine-grained
  - **DSL syntax errors**: Kernel-level errors during sketch parsing → LLM not following DSL format guidelines

- First 3 experiments:
  1. Reproduce the iterative repair vs. Best-of-N ablation on a subset of miniF2F-test (e.g., 50 problems) to validate the test-time scaling claim before full deployment.
  2. Test decomposition on 5 problems that fail direct proof; inspect whether regenerated sketches show meaningful strategy revision vs. superficial changes.
  3. Evaluate retrieval component in isolation: measure how often retrieved theorems are actually used in successful repairs vs. retrieved but ignored.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can reinforcement learning (RL) be integrated into the Delta Prover framework to enable the agent to continuously refine its proof strategies?
- Basis in paper: [explicit] The conclusion lists "Implementing reinforcement learning (RL) based on the agentic workflow proposed in this paper" as a specific future direction.
- Why unresolved: The current framework is "training-free" and relies on the frozen reasoning capabilities of Gemini 2.5 Pro; it lacks a mechanism for the agent to learn from successful or failed trajectories over time to update its policy.
- What evidence would resolve it: A study demonstrating that an RL-enhanced version of Delta Prover improves its success rate or sample efficiency on miniF2F or other benchmarks compared to the static agent.

### Open Question 2
- Question: Can evolutionary algorithms (e.g., FunSearch, AlphaEvolve) replace the predefined workflows to achieve dynamic proof search optimization?
- Basis in paper: [explicit] The conclusion suggests "Exploring more elaborate test-time techniques... to move beyond predefined workflows towards dynamic proof search optimization."
- Why unresolved: The current system relies on a fixed algorithmic structure of "Reflective Decomposition" and "Iterative Proof Repair"; it does not employ evolutionary search to dynamically mutate or evolve proof strategies during test time.
- What evidence would resolve it: An implementation of an evolutionary search mechanism within Delta Prover that outperforms the current fixed decomposition strategy on complex theorem proving tasks.

### Open Question 3
- Question: Does the effectiveness of the reflective decomposition framework transfer to smaller, open-source models, or is it dependent on the specific reasoning capabilities of Gemini 2.5 Pro?
- Basis in paper: [inferred] The paper exclusively evaluates the framework using Gemini 2.5 Pro 05-06, leaving the performance on smaller or different model architectures unexplored.
- Why unresolved: While the authors claim general-purpose LLMs possess "untapped theorem-proving capabilities," it is unclear if the agentic scaffolding is sufficient to bridge the gap for models with less inherent reasoning power or different training distributions.
- What evidence would resolve it: Experimental results applying the Delta Prover framework to smaller open-source models (e.g., Llama 3, Mistral) to see if they achieve comparable success rates or relative improvements.

## Limitations
- Missing specific hyperparameter values (m, n, lmax, m̃, ñ) for rounds, repairs per round, and decomposition attempts
- Retrieval model Ψ details not fully specified (embedding method, retrieval mechanism)
- Full prompt templates in Appendix are truncated
- Implementation details of DSL's PlayM monad and automatic sub-problem extraction not provided

## Confidence
- **High confidence** in the overall methodology and architectural framework, as the paper provides sufficient detail to understand the two core components (Iterative Proof Repair and Reflective Decomposition) and their integration
- **Medium confidence** in the reported results, as the paper claims state-of-the-art performance on miniF2F-test (95.9%) and IMO subset (85%), but the exact implementation details and hyperparameter settings that achieved these results are not fully disclosed
- **Low confidence** in the reproducibility of the exact results without additional experimentation, due to missing hyperparameter values and incomplete prompt templates

## Next Checks
1. Reproduce the iterative repair vs. Best-of-N ablation on a subset of miniF2F-test (e.g., 50 problems) to validate the test-time scaling claim before full deployment, using reasonable hyperparameter defaults (e.g., m=5, n=10, lmax=3)
2. Test decomposition on 5 problems that fail direct proof and inspect whether regenerated sketches show meaningful strategy revision vs. superficial changes, focusing on the decomposition-level reflection mechanism
3. Evaluate the retrieval component in isolation by measuring how often retrieved theorems are actually used in successful repairs vs. retrieved but ignored, to quantify the contribution of the retrieval augmentation