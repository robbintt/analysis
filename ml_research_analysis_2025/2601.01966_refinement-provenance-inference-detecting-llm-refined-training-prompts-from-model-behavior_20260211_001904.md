---
ver: rpa2
title: 'Refinement Provenance Inference: Detecting LLM-Refined Training Prompts from
  Model Behavior'
arxiv_id: '2601.01966'
source_url: https://arxiv.org/abs/2601.01966
tags:
- training
- refinement
- arxiv
- victim
- provenance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Refinement Provenence Inference (RPI), which
  aims to determine whether a fine-tuned model was trained on raw prompts or prompts
  rewritten by an external refiner LLM. The authors propose RePro, a logit-based framework
  that extracts complementary teacher-forced logit cues (including NLL statistics,
  ranking patterns, and margin features) and learns a transferable embedding via shadow
  fine-tuning and supervised contrastive learning.
---

# Refinement Provenance Inference: Detecting LLM-Refined Training Prompts from Model Behavior

## Quick Facts
- **arXiv ID:** 2601.01966
- **Source URL:** https://arxiv.org/abs/2601.01966
- **Reference count:** 21
- **Primary result:** RPI uses teacher-forced logit statistics and contrastive learning to detect whether fine-tuned models were trained on raw or LLM-refined prompts, achieving consistent AUC gains of 0.04-0.12 across datasets and refiners.

## Executive Summary
This paper introduces Refinement Provenance Inference (RPI), which aims to determine whether a fine-tuned model was trained on raw prompts or prompts rewritten by an external refiner LLM. The authors propose RePro, a logit-based framework that extracts complementary teacher-forced logit cues (including NLL statistics, ranking patterns, and margin features) and learns a transferable embedding via shadow fine-tuning and supervised contrastive learning. During inference, a lightweight linear classifier predicts refined-versus-raw provenance scores. Across GSM8K and HumanEval datasets with multiple victim families and refiners (GPT-4o, Llama-3.3-70B-Instruct), RePro consistently outperforms learning-free baselines (AUC gains of 0.04-0.12; TPR@1%FPR gains of 0.07-0.12). Cross-refiner transfer remains strong (AUC degradation ≤ 0.03), indicating refiner-agnostic distribution-level preference shifts rather than style artifacts. Ablation shows uplift features contribute most to transfer, and contrastive training improves robustness. The approach is stable under different refinement instruction templates and scales with refined data fraction and fine-tuning intensity. Overall, RePro demonstrates that prompt refinement leaves a detectable and transferable footprint in fine-tuned models, enabling provenance inference from token-level behavior.

## Method Summary
RePro extracts teacher-forced features from fine-tuned models, including NLL mean/quantiles, top-k inclusion for k∈{1,5,10}, confidence margin, and uplift features (differences from base model). These features are encoded via a 2-layer MLP trained with supervised contrastive learning on shadow fine-tuned models. A linear classifier trained on frozen embeddings predicts refined-vs-raw provenance. Shadow fine-tuning uses LoRA (r=16, α=32, dropout=0.05, lr=2e-4, 500 steps) on instance-disjoint mixtures sampled with Bernoulli(ρ). The method assumes access to the base model for shadow fine-tuning and reference outputs for teacher forcing.

## Key Results
- Across GSM8K and HumanEval with multiple victim families and refiners, RePro achieves AUC gains of 0.04-0.12 over learning-free baselines.
- Cross-refiner transfer remains strong with AUC degradation ≤ 0.03, indicating refiner-agnostic distribution-level shifts.
- Uplift features contribute most to transfer, with ablation showing AUC drops of 0.04-0.09 when removed.
- Contrastive training improves robustness, outperforming linear probe only (AUC gain of 0.05) and no adaptation (AUC gain of 0.07).
- Performance scales with refined data fraction and fine-tuning intensity, remaining stable under different refinement instruction templates.

## Why This Works (Mechanism)

### Mechanism 1: Distribution-Level Preference Shift from Refined Prompt Training
- **Claim:** Training on LLM-refined prompts induces systematic, detectable shifts in a model's token-level predictive distribution that persist beyond surface-form differences.
- **Mechanism:** Refined prompts are more canonical and better aligned with instruction-following conventions. When a model is fine-tuned on these refined prompts, the gradient updates bias the model's internal preferences toward this refined distribution. Under teacher forcing, these preferences manifest as measurable differences in likelihood patterns, ranking behavior, and logit margins—even when semantic content is preserved.
- **Core assumption:** The refiner's standardization (not just style) alters the effective training distribution in a way that generalizes across refiners.
- **Evidence anchors:** [abstract] "prompt refinement yields stable, detectable shifts in teacher-forced token distributions, even when semantic differences are not obvious"; [page 2] "training on refined prompts induces distribution-level preference shifts that persist beyond surface realizations"
- **Break condition:** If refined and raw prompts produced indistinguishable gradient trajectories (e.g., refinement only changes surface syntax without affecting alignment), the distribution shift would vanish and detection would fail.

### Mechanism 2: Uplift Features Capture Fine-Tuning-Induced Behavioral Changes
- **Claim:** Contrasting a model's behavior before and after fine-tuning (uplift features) provides the strongest transferable provenance signal.
- **Mechanism:** Uplift features compute the difference in logit-derived statistics (NLL, top-k inclusion, confidence margin) between the base model M₀ and the fine-tuned model M. This isolates the change attributable to training, removing baseline confounds. The ablation shows uplift contributes the largest gain (AUC drop of 0.04–0.09 when removed).
- **Core assumption:** The base model M₀ is available to the auditor; the uplift signal generalizes across victim families.
- **Evidence anchors:** [page 4, Eq. 9] ∆S(i) = S_{M₀}(i) − S_M(i) defines uplift features; [page 7, Table 3] "w/o uplift" causes AUC drop of 0.04 (GSM8K) and 0.03 (HumanEval)
- **Break condition:** If the base model is unavailable, uplift features cannot be computed, and the method must rely on weaker victim-only statistics.

### Mechanism 3: Supervised Contrastive Learning Shapes Transferable Embedding Geometry
- **Claim:** Contrastive training on shadow model features produces an embedding space where provenance separation transfers to unseen victim models.
- **Mechanism:** The encoder h_ψ is trained to pull same-provenance embeddings together and push different-provenance embeddings apart. This shapes a representation that captures provenance-relevant structure rather than overfitting to shadow-specific artifacts. The linear classifier trained on frozen embeddings then generalizes to victims.
- **Core assumption:** Shadow models fine-tuned from the same base initialization produce similar provenance-relevant feature distributions as victim models.
- **Evidence anchors:** [page 5, Eq. 12] Supervised contrastive loss formulation; [page 7, Figure 5a] Contrastive training (0.69 AUC) outperforms linear probe only (0.64) and no adaptation (0.62)
- **Break condition:** If shadow and victim fine-tuning distributions diverge significantly (e.g., different tasks, domains, or mixture ratios), the learned embedding may not transfer.

## Foundational Learning

- **Concept: Membership Inference Attacks**
  - **Why needed here:** RPI extends the membership inference paradigm from "was this instance in training?" to "which prompt variant was used during training?" Understanding loss-based and confidence-based auditing is prerequisite.
  - **Quick check question:** Can you explain why a model typically assigns lower loss to training examples than non-training examples, and how this asymmetry is exploited for auditing?

- **Concept: Teacher Forcing in Language Models**
  - **Why needed here:** All provenance features are extracted under teacher forcing (conditioning on the reference output token-by-token). This is not stochastic decoding—it requires understanding log-probability extraction along a fixed sequence.
  - **Quick check question:** Given a model M, an input x, and a reference output y = [y₁, y₂, ..., yₙ], what does teacher forcing compute at each step t?

- **Concept: Supervised Contrastive Learning**
  - **Why needed here:** The encoder is trained with a supervised contrastive objective that uses provenance labels to structure the embedding space. Understanding positive/negative set construction and the temperature-scaled similarity is required.
  - **Quick check question:** In a batch with 4 examples where 2 are refined and 2 are raw, how would you construct the positive set P(i) for a refined example i?

## Architecture Onboarding

- **Component map:** Feature Extractor ϕ(·) -> Encoder h_ψ (2-layer MLP: 256→128) -> Linear Classifier g
- **Critical path:**
  1. Prepare raw/refined prompt pairs with cached refinements (single rewrite per raw prompt).
  2. Sample mixture indicator z_i ∼ Bernoulli(ρ) to construct shadow training set.
  3. Fine-tune shadow model M_c from base M₀.
  4. Extract features ϕ(M_c; x_i, y_i) for all shadow instances (including base model features for uplift).
  5. Train encoder h_ψ with supervised contrastive loss.
  6. Freeze h_ψ, train linear classifier g.
  7. At inference: extract features from victim M_a, embed with h_ψ, classify with g.

- **Design tradeoffs:**
  - **Feature complexity vs. transfer:** More features (e.g., full logit vectors) may overfit to shadow artifacts; the paper uses aggregated statistics for robustness.
  - **Uplift computation requires base model access:** If M₀ is unavailable, fallback to victim-only features (lower AUC).
  - **LoRA-only fine-tuning assumption:** Full fine-tuning may produce different gradient magnitudes; transfer may degrade.

- **Failure signatures:**
  - AUC near 0.5 with high variance across runs: shadow/victim distribution mismatch; check mixture ratios and instance disjointness.
  - Large performance drop in cross-refiner setting (AUC degradation > 0.1): encoder overfitting to refiner-specific style; increase contrastive temperature or reduce model capacity.
  - TPR@1%FPR near 0: classifier threshold calibration failed; re-calibrate on shadow validation split.

- **First 3 experiments:**
  1. **Reproduce matched-refiner baseline:** Fine-tune shadow and victim with same refiner (GPT-4o), extract features, train contrastive encoder, report AUC and TPR@1%FPR on held-out instances. Compare against learning-free baselines (s_NLL, s_ΔNLL, s_pair).
  2. **Ablate uplift features:** Remove uplift features from ϕ(·), retrain encoder, and quantify AUC drop. This validates the primary contribution of pre/post fine-tuning contrast.
  3. **Cross-refiner transfer test:** Train shadow attacker with GPT-4o-refined data; evaluate on victim fine-tuned with Llama-3.3-70B-Instruct-refined data. Confirm AUC degradation ≤ 0.03 as reported.

## Open Questions the Paper Calls Out

- **Can RPI be extended to detect provenance when refinement jointly edits both prompts and response labels, rather than only rewriting prompts?**
  - **Basis in paper:** [explicit] From Limitations: "settings where refinement jointly edits prompts and labels... may exhibit different leakage characteristics and require modified features or protocols."
  - **Why unresolved:** Current RePro assumes fixed reference outputs; joint editing changes both input and target distributions, potentially creating new signature types or masking existing ones.
  - **What evidence would resolve it:** Empirical evaluation on datasets with joint prompt-response refinement, measuring AUC and identifying which features remain predictive.

- **How can refinement provenance inference operate under weaker interfaces without access to reference outputs or full logit information?**
  - **Basis in paper:** [explicit] From Future Work: "relax the reliance on teacher-forced statistics with a known reference output, enabling auditing with weaker interfaces such as sampled generations or score-only APIs."
  - **Why unresolved:** Current method requires teacher forcing on known references; many real-world auditing scenarios provide only model outputs without gold labels.
  - **What evidence would resolve it:** Development and evaluation of sampling-based or generation-based features that achieve comparable AUC without gold references.

- **What mitigation strategies can effectively reduce refinement provenance detectability while preserving training quality?**
  - **Basis in paper:** [explicit] From Conclusion: "motivating future work on mitigation and refinement-aware privacy evaluation" and Future Work: "study adaptive obfuscation and mitigation, including mixing refiners or style randomization."
  - **Why unresolved:** The paper demonstrates detectability but does not explore countermeasures; practical deployment requires understanding how to hide refinement traces.
  - **What evidence would resolve it:** Evaluation of refiner mixing, style randomization, or adversarial techniques, measuring AUC reduction and downstream task performance impact.

## Limitations

- **Uplift features require base model access:** The method critically depends on comparing fine-tuned models to their base models; without M₀, detection degrades significantly.
- **Limited domain evaluation:** Experiments are restricted to GSM8K and HumanEval; generalization to other domains (e.g., long-form generation, structured reasoning) is untested.
- **Assumed LoRA fine-tuning:** The method assumes LoRA-only fine-tuning; full fine-tuning may produce different gradient magnitudes affecting transferability.

## Confidence

- **High confidence:** Cross-refiner transfer remains strong (AUC degradation ≤ 0.03) and ablation shows uplift features are the dominant contributor to detection performance.
- **Medium confidence:** The contrastive training improves robustness, but the encoder's ability to generalize to vastly different tasks/domains is untested.
- **Low confidence:** The paper claims refinement induces "distribution-level preference shifts" but does not directly measure or validate this causal mechanism; the claim is primarily supported by detection success rather than direct distributional analysis.

## Next Checks

1. **Base model ablation test:** Repeat the main experiments without access to M₀; report AUC and TPR@1%FPR using only victim-only statistics. This validates the method's robustness when uplift features are unavailable.
2. **Cross-task transfer evaluation:** Apply the shadow-trained encoder to a victim fine-tuned on a completely different task (e.g., summarization or translation). Measure AUC degradation to test task-agnostic generalization.
3. **Refiner diversity test:** Extend evaluation to include a third refiner with a different architecture or instruction-following style (e.g., Claude-3-5-Sonnet). Confirm AUC degradation remains ≤ 0.03 to validate refiner-agnostic distribution-level shifts.