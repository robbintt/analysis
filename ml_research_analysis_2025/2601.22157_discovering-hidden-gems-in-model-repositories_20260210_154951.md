---
ver: rpa2
title: Discovering Hidden Gems in Model Repositories
arxiv_id: '2601.22157'
source_url: https://arxiv.org/abs/2601.22157
tags:
- best
- arxiv
- downloads
- tree
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "We demonstrate that public model repositories contain \u201Chidden\
  \ gems,\u201D high-performing fine-tuned models that are overlooked despite significantly\
  \ outperforming popular checkpoints. Across four major model families, we identify\
  \ models that improve performance by up to 12.8% on math tasks without increased\
  \ inference costs, yet receive orders of magnitude fewer downloads than popular\
  \ versions."
---

# Discovering Hidden Gems in Model Repositories

## Quick Facts
- arXiv ID: 2601.22157
- Source URL: https://arxiv.org/abs/2601.22157
- Authors: Jonathan Kahana; Eliahu Horwitz; Yedid Hoshen
- Reference count: 40
- We demonstrate that public model repositories contain "hidden gems," high-performing fine-tuned models that are overlooked despite significantly outperforming popular checkpoints.

## Executive Summary
Public model repositories contain high-performing fine-tuned models that outperform popular checkpoints yet receive orders of magnitude fewer downloads. These "hidden gems" improve performance by up to 12.8% on math tasks without increased inference costs, but most lack performance documentation making them nearly impossible to discover through text search. We formulate the discovery process as a Multi-Armed Bandit problem and accelerate Sequential Halving by using shared query sets and aggressive elimination schedules. Our method consistently retrieves top-3 models with as few as 50 queries per candidate, accelerating discovery by over 50x compared to exhaustive evaluation.

## Method Summary
The method formulates model discovery as a pure exploration Multi-Armed Bandit problem with fixed budget best-arm identification. It modifies Sequential Halving by implementing correlated sampling (identical query sets across candidates to minimize variance) and an aggressive elimination scheduler that reduces to 100 models after the first round using 60% of the total budget. The algorithm tests candidates on subsampled benchmarks (2,500 queries from RouterBench) and iteratively eliminates the bottom 50% each round while doubling queries per model. Custom schedulers and query allocation strategies enable efficient discovery of top-performing models while minimizing inference costs.

## Key Results
- Identified models improving performance by up to 12.8% on math tasks without increased inference costs
- Discovered models receive orders of magnitude fewer downloads than popular versions despite superior performance
- Consistently retrieves top-3 models with as few as 50 queries per candidate
- Accelerates discovery by over 50x compared to exhaustive evaluation

## Why This Works (Mechanism)

### Mechanism 1: Correlated Sampling for Variance Reduction
- Claim: Using identical query sets across all candidate models reduces ranking noise compared to independent sampling
- Mechanism: Benchmarks contain a mix of trivial and difficult problems. By enforcing that all surviving models are tested on the exact same queries each round, the variance of the difference estimator between model pairs is minimized, making relative rankings more stable
- Core assumption: Query difficulty variance is a primary source of ranking noise; models tested on different query subsets may appear artificially better or worse
- Evidence anchors: Section 3 states that correlated sampling minimizes variance of the difference estimator; Table 5 ablation shows clear improvements at N=50 budget; no direct corpus validation for this specific technique

### Mechanism 2: Aggressive Early Elimination via Skewed Distribution Exploitation
- Claim: Rapidly eliminating the majority of low-quality models in the first round reallocates compute toward distinguishing elite candidates
- Mechanism: The paper observes that most uploaded models are significantly worse than the best available (often >10% lower accuracy). A custom scheduler allocates 60% of the budget to round 1 and eliminates down to a fixed 100 models immediately, rather than the standard 50% elimination rate
- Core assumption: Model quality distribution is heavily skewed; low performers can be identified with very few queries
- Evidence anchors: Section 3 presents distribution of model quality showing vast majority of uploads are low-quality or broken; Figure 4 shows 60-95% of models are >10% worse than best model; Table 5 ablation shows retrieval rank degradation of 30+ positions when using standard scheduling

### Mechanism 3: Multi-Armed Bandit Formulation with Fixed-Budget Best-Arm Identification
- Claim: Model discovery can be framed as a pure exploration bandit problem minimizing simple regret
- Mechanism: Models are arms; each query is an action; rewards are binary correctness. Unlike reward-maximizing MAB, the goal is identifying the single best arm after a fixed budget B is exhausted
- Core assumption: The objective is finding the best model post-hoc, not accumulating reward during evaluation
- Evidence anchors: Section 3 formulates model discovery as MAB problem with pure exploration goal; Table 2 outperforms 8 baselines including UCB, TTTS, Successive Rejects, and standard Sequential Halving

## Foundational Learning

- **Concept: Sequential Halving Algorithm**
  - Why needed here: This is the base algorithm being accelerated; understanding its round-based elimination structure is prerequisite to appreciating the modifications
  - Quick check question: Can you explain why Sequential Halving eliminates 50% per round rather than a fixed number of candidates?

- **Concept: Best-Arm Identification (BAI) vs. Cumulative Reward MAB**
  - Why needed here: The paper explicitly contrasts pure exploration (BAI) from standard MAB reward maximization; confusing these leads to wrong baseline comparisons
  - Quick check question: In a fixed-budget setting, would an epsilon-greedy strategy be appropriate for BAI? Why or why not?

- **Concept: Variance Reduction via Paired/Correlated Sampling**
  - Why needed here: The core technical contribution exploits correlated sampling to reduce estimator variance; this requires understanding how paired comparisons affect the variance of differences
  - Quick check question: If two models A and B are tested on independent query sets vs. the same query set, which comparison has lower variance in the estimated accuracy gap?

## Architecture Onboarding

- **Component map:**
  - Candidate Pool Generator -> Query Selector -> Scheduler -> Evaluation Engine -> Ranking & Elimination Module

- **Critical path:**
  1. Load candidate models from target model tree (handle download failures, missing weights)
  2. Initialize shared query set for round 1 (6-120 queries depending on budget)
  3. Evaluate all candidates on identical queries
  4. Rank by accuracy; retain top 100
  5. For rounds 2-5: double queries per model, eliminate 50% each round
  6. Return final surviving model

- **Design tradeoffs:**
  - Aggressive vs. conservative early elimination: Aggressive elimination (100 survivors) speeds discovery but risks losing strong models if round-1 queries are unrepresentative; conservative (50% elimination) is safer but slower
  - Budget allocation front-loading vs. even distribution: Front-loading (60% in round 1) enables confident early cuts but leaves less for fine-grained later discrimination
  - System prompt handling: Evaluating with and without system prompts doubles compute but ensures fair comparison across models with different prompt sensitivities

- **Failure signatures:**
  - High variance in retrieved model rank across repetitions: Indicates insufficient query budget or overly aggressive elimination; increase N or soften first-round elimination
  - Retrieved model fails to beat base model: Suggests query set is not representative of target task; verify subsampling stratification
  - Many models fail to load/run: Expected; Tab. 3 shows substantial attrition. Pre-filter for compatibility before running search

- **First 3 experiments:**
  1. Ablate correlated sampling: Run with independent query sets per model vs. shared sets; measure rank variance reduction at N=50 on a single tree
  2. Ablate scheduler: Compare custom scheduler (Table 4) vs. standard SH 50% elimination; measure mean retrieved rank and accuracy gap
  3. Budget sensitivity test: Sweep N âˆˆ {10, 25, 50, 100} on two model trees; plot retrieved model rank vs. budget to characterize convergence behavior

## Open Questions the Paper Calls Out

- **Open Question 1:** Can weight-space learning methods bypass inference entirely to identify hidden gems directly from model weights?
  - Basis: Appendix A states that weight-space learning has potential but is currently limited to small-scale networks and benchmarks
  - Why unresolved: Current weight-space methods cannot handle the scale of LLMs evaluated here (up to 8B parameters)
  - What evidence would resolve it: Demonstration of weight-space predictors achieving comparable retrieval quality to the proposed MAB approach on LLM-scale model trees

- **Open Question 2:** Do hidden gems identified for one task generalize to other tasks without requiring re-evaluation?
  - Basis: Appendix A states that finding gems for new tasks would require reevaluation on that task
  - Why unresolved: The paper evaluates each task independently; it remains unknown whether gems maintain superiority across different benchmarks
  - What evidence would resolve it: Correlation analysis showing whether gems identified on GSM8K transfer to other tasks

## Limitations
- Evaluation confined to four specific model families with curated candidate pools, limiting generalizability
- Results depend on specific distribution of model quality observed in studied trees
- Aggressive elimination strategy risks discarding potentially competitive models if early query sets are unrepresentative

## Confidence
- **Claim confidence: High** - Statistically significant improvements demonstrated with well-controlled ablation experiments; core mechanism clearly described and tested against strong baselines
- **Claim confidence: Medium** - Statistical methodology sound but limited exploration of how results vary with different query distributions or model quality skews; 50x acceleration claim assumes specific repository structure
- **Claim confidence: Low** - No analysis of query selection strategy impact; limited exploration of recall vs. precision trade-offs; no validation of discovered gems generalizing beyond evaluation benchmarks

## Next Checks
1. **Query Set Representativeness Test**: Run discovery algorithm on a single tree using multiple independently sampled 2,500-query subsets. Compare retrieved model ranks and accuracy gaps to quantify sensitivity to query selection.

2. **Distribution Sensitivity Analysis**: Artificially manipulate the model quality distribution in a controlled subset of candidates and measure how aggressive elimination performs vs. standard SH.

3. **Cross-Domain Transferability**: Apply the method to a different model family (e.g., text-to-image or multimodal models) with known quality variance. Compare discovery efficiency and retrieved model quality against exhaustive evaluation.