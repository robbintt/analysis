---
ver: rpa2
title: 'Robust Probabilistic Load Forecasting for a Single Household: A Comparative
  Study from SARIMA to Transformers on the REFIT Dataset'
arxiv_id: '2512.00856'
source_url: https://arxiv.org/abs/2512.00856
tags:
- data
- learning
- lstm
- time
- instead
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper tackles probabilistic load forecasting for a single\
  \ household using the REFIT dataset, which is complicated by a large structural\
  \ data gap. The authors first conduct a rigorous comparative experiment to select\
  \ a Seasonal Imputation method, demonstrating its superiority over linear interpolation\
  \ in preserving the data\u2019s underlying distribution."
---

# Robust Probabilistic Load Forecasting for a Single Household: A Comparative Study from SARIMA to Transformers on the REFIT Dataset

## Quick Facts
- arXiv ID: 2512.00856
- Source URL: https://arxiv.org/abs/2512.00856
- Authors: Midhun Manoj
- Reference count: 15
- Primary result: Temporal Fusion Transformer (TFT) achieves best point forecast accuracy (RMSE 481.94) and produces safer, more cautious prediction intervals that effectively capture extreme volatility in single-household load data.

## Executive Summary
This paper addresses probabilistic load forecasting for a single household using the REFIT dataset, complicated by a large structural data gap. The authors conduct a rigorous comparative experiment to select Seasonal Imputation over linear interpolation for preserving distributional structure, then systematically evaluate models from classical baselines (SARIMA, Prophet) through machine learning (XGBoost) to advanced deep learning (LSTM, TFT). Classical models fail to capture non-linear, regime-switching behavior, while LSTM achieves the best-calibrated probabilistic forecast. The Temporal Fusion Transformer emerges as the superior all-round model, balancing point accuracy with well-calibrated, conservative prediction intervals that capture extreme volatility.

## Method Summary
The study uses the REFIT Electrical Load Measurements dataset for House 1, resampled to hourly averages (~15K entries) with 8-second raw resolution. A large structural data gap is addressed using Seasonal Imputation, demonstrated superior to linear interpolation for preserving bimodality. Features include datetime-derived features (hour, dayofweek, month, is_weekend) and lag features (1hr, 24hr, 168hr). Models progress from classical (SARIMAX) through tree-based (XGBoost/LightGBM) to deep learning (LSTM, TFT), all configured for quantile regression at 5th, 50th, and 95th percentiles. Evaluation uses RMSE/MAE for point forecasts and PICP/Average Quantile Score for probabilistic calibration on an 80/20 temporal split.

## Key Results
- Seasonal Imputation preserves bimodal load distribution better than linear interpolation, maintaining key two-peak patterns in household load
- Classical models (SARIMA) fail catastrophically with RMSE >2800, producing flat daily cycles regardless of weekday/weekend regime switches
- LSTM achieves best-calibrated probabilistic forecast (PICP 88.81%) but misses extreme spikes with narrow intervals
- Temporal Fusion Transformer emerges as superior all-round model: best point accuracy (RMSE 481.94) and safest prediction intervals that capture extreme volatility

## Why This Works (Mechanism)

### Mechanism 1: Seasonal Imputation Preserves Distributional Structure
Seasonal imputation estimates missing values by averaging known observations from equivalent seasonal positions (e.g., prior Mondays at 9 AM), reconstructing recurring patterns rather than assuming smooth continuity across gaps. Core assumption: periodic structure (daily/weekly cycles) remains stable across the gap period. Evidence: Fig.2 shows Linear Imputation missed the two-peak pattern seen in real values, while Seasonal Imputation kept key features intact. Break condition: regime shifts during gap (changed occupancy, new appliances) invalidate seasonal patterns.

### Mechanism 2: Quantile Regression with Pinball Loss
The pinball loss function applies different penalties for over- vs. under-prediction at each quantile (τ), forcing models to learn separate outputs for 5th, 50th, and 95th percentiles that bracket uncertainty. Core assumption: future load distribution can be approximated by discrete quantile estimates. Evidence: PICP measures calibration - values <90% indicate overconfidence. Break condition: true distribution is multi-modal or has fat tails beyond sampled quantiles.

### Mechanism 3: TFT's Self-Attention Captures Long-Range Dependencies
Self-attention allows direct reference to any historical time step without sequential compression, while TFT architecture combines this with local RNN processing and explicit handling of variable types. Core assumption: key predictive signals may occur at irregular, distant intervals rather than only recent history. Evidence: TFT intervals widen dramatically during high volatility periods (late April), successfully catching spikes LSTM missed. Break condition: insufficient training data for attention to learn meaningful long-range patterns.

## Foundational Learning

- **Pinball Loss / Quantile Regression**: Needed because standard MSE produces point estimates only while household load requires uncertainty bounds for grid management. Quick check: Can you explain why the loss function uses different coefficients (τ vs. 1-τ) for over- vs. under-predictions?

- **Prediction Interval Coverage Probability (PICP)**: Needed to quantify calibration - whether stated confidence levels match empirical coverage (e.g., 90% interval should contain 90% of actuals). Quick check: If PICP is 95.85% for a nominal 90% interval, is the model over- or under-confident?

- **Attention Mechanisms for Sequences**: Explains why TFT can reference distant events (e.g., same hour last week) directly while LSTM must compress all history through fixed-size hidden state. Quick check: What is the computational complexity difference between self-attention (O(n²)) and LSTM (O(n)) for sequence length n?

## Architecture Onboarding

- **Component map**: REFIT dataset → hourly resampling → kNN imputation for minor gaps → Seasonal Imputation for structural gaps → Min-Max scaling → Feature engineering (datetime + lags) → Model hierarchy (Seasonal Naïve → SARIMAX → XGBoost/LightGBM → LSTM → TFT) → Evaluation (RMSE/MAE + PICP/AQS)

- **Critical path**: 1) Imputation method selection (Seasonal > Linear for distributional fidelity), 2) Feature engineering (lags + calendar features essential for tree models), 3) Quantile output configuration (5th, 50th, 95th percentiles), 4) Early stopping on validation loss

- **Design tradeoffs**: XGBoost achieves better point accuracy (RMSE 452 vs. 481) but provides no uncertainty estimates; TFT trades some point accuracy for probabilistic robustness; TFT's conservatism (PICP > nominal) may be preferable for grid operations where missing spikes is costly

- **Failure signatures**: SARIMAX failure mode - outputs flat daily cycle regardless of weekday/weekend regime (RMSE 2815); LSTM failure mode - narrow prediction intervals miss extreme peaks; Imputation failure - linear interpolation produces unimodal distribution, losing true bimodal load pattern

- **First 3 experiments**: 1) Validate imputation method on held-out data: create artificial gap in complete data segment, compare Linear vs. Seasonal reconstruction using KS-distance, 2) Establish classical baseline: train SARIMAX(1,1,1)(1,1,0,24) with hour/dayofweek exogenous variables, confirm expected failure on weekend regime switches, 3) Calibrate probabilistic models: train LSTM and TFT with identical quantile configuration (0.05, 0.50, 0.95), compare PICP against 90% target and visualize interval width during volatile periods

## Open Questions the Paper Calls Out

- **Unified Multi-Household Extension**: Can TFT and LSTM models maintain performance superiority when extended to forecast across all 20 homes in REFIT using a unified multi-horizon approach? The study only examined House 1; results may not generalize due to different gadgets, routines, or habits across households.

- **Bayesian vs. Quantile Approaches**: How does quantile regression compare to Bayesian neural network approaches (e.g., DeepAR) in calibration and sharpness for household load probabilistic forecasting? The paper only implemented quantile loss; distributional approaches remain untested on this noisy dataset.

- **End-to-End Missing Value Handling**: Can unified models that handle missing values internally (e.g., GRU-D) outperform the two-stage seasonal imputation plus forecasting pipeline? The paper used separate seasonal imputation step; end-to-end learning of imputation and prediction was not evaluated.

- **Weather Exogenous Variables**: Does incorporating weather exogenous variables (temperature, solar radiation) significantly improve probabilistic load forecasting for households with high renewable energy usage? REFIT contains no weather covariates; models rely solely on calendar features and lagged load.

## Limitations

- Seasonal Imputer selection relies on visual inspection and distributional plots rather than quantitative gap reconstruction error; household usage patterns may shift during gaps
- TFT's higher PICP (95.85%) is presented as strength without quantifying economic cost of overly wide intervals that reduce forecast value
- Evaluation metrics are appropriate but narrow - fixed 90% nominal interval without testing sensitivity to other coverage levels or asymmetric loss penalties

## Confidence

- **High Confidence**: Seasonal Imputer superiority over linear interpolation for preserving bimodality (visually evident in Fig.2, backed by known failure of linear methods on periodic gaps)
- **Medium Confidence**: TFT's overall superiority (RMSE 481.94, cautious intervals) - ranking is clear but operational trade-off of over-conservatism unexamined
- **Low Confidence**: Exact mechanism by which TFT captures "extreme volatility" - paper shows interval widening but not which features or attention patterns drive it

## Next Checks

1. **Imputation Ablation**: Create synthetic gaps in complete REFIT segment, impute with Seasonal vs. Linear methods, compute KS-distance and RMSE against ground truth to quantify distributional fidelity loss

2. **Interval Cost Analysis**: Define cost function penalizing both missed spikes (under-coverage) and overly wide intervals (over-conservatism), compare LSTM vs. TFT under this metric rather than raw PICP

3. **TFT Interpretability Probe**: Extract attention weights from TFT's encoder to identify which lagged features (e.g., 168hr lag vs. 24hr lag) are most activated during extreme volatility periods identified in results