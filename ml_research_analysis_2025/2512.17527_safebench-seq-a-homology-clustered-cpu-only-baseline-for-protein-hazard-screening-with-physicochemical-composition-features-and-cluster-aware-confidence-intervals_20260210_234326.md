---
ver: rpa2
title: 'SafeBench-Seq: A Homology-Clustered, CPU-Only Baseline for Protein Hazard
  Screening with Physicochemical/Composition Features and Cluster-Aware Confidence
  Intervals'
arxiv_id: '2512.17527'
source_url: https://arxiv.org/abs/2512.17527
tags:
- cluster
- split
- protein
- sequences
- random
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces SafeBench-Seq, a CPU-only, homology-clustered\
  \ baseline for protein hazard screening. It uses interpretable physicochemical and\
  \ composition features with calibrated classifiers (Logistic Regression, SVM, Random\
  \ Forest) evaluated under cluster-level holdouts (\u226440% identity)."
---

# SafeBench-Seq: A Homology-Clustered, CPU-Only Baseline for Protein Hazard Screening with Physicochemical/Composition Features and Cluster-Aware Confidence Intervals

## Quick Facts
- **arXiv ID**: 2512.17527
- **Source URL**: https://arxiv.org/abs/2512.17527
- **Reference count**: 22
- **Primary result**: AUROC drops from ~0.95 (random split) to ~0.87 (cluster split), showing random splits overestimate robustness

## Executive Summary
SafeBench-Seq introduces a CPU-only baseline for protein hazard screening using interpretable physicochemical and composition features with calibrated classifiers. Evaluated under cluster-level holdouts (≤40% sequence identity), the benchmark reveals that random splits substantially overestimate screening performance—AUROC drops from ~0.95 to ~0.87 when homology control is applied. Linear models (Logistic Regression, SVM) show better calibration than Random Forest, while spurious-signal probes demonstrate models capture motif-level signals beyond trivial cues. The metadata-only, reproducible benchmark provides a realistic baseline for future biohazard detection.

## Method Summary
SafeBench-Seq uses 854 protein sequences (427 hazards from SafeProtein, 427 benigns from UniProt) with CD-HIT clustering at ≤40% identity yielding 597 clusters. Features include 20 amino acid compositions plus 8 ProtParam descriptors (length, MW, pI, GRAVY, aromaticity, instability index, aliphatic index, net charge pH 7). Three calibrated classifiers (L2-LogReg, Linear SVM, Random Forest with 400 trees) are evaluated under cluster-level 80/20 splits with bootstrap confidence intervals (n=200). The benchmark emphasizes interpretability and reproducibility through CPU-only feature extraction and metadata-only release.

## Key Results
- Homology-clustered evaluation shows substantial performance drop: AUROC from ~0.95 (random split) to ~0.87 (cluster split)
- Linear models exhibit superior calibration: LogReg Brier=0.140, ECE=0.118 vs RF Brier=0.156, ECE=0.138
- Spurious-signal probes confirm models capture motif-level signals: length-only and composition-only features yield near-random AUROC (~0.50-0.55), but residue shuffling degrades performance further

## Why This Works (Mechanism)

### Mechanism 1: Homology-Clustered Split Prevents Overoptimistic Estimates
- Claim: Cluster-level holdouts at ≤40% sequence identity yield realistic performance estimates for screening novel sequences
- Mechanism: CD-HIT clusters sequences so no test sequence shares >40% identity with any training sequence; this breaks the implicit memorization shortcut that inflates random-split metrics
- Core assumption: Novel hazardous proteins will share limited sequence identity with known toxins in training data
- Evidence anchors: [abstract]: "random splits substantially overestimate robustness relative to homology-clustered evaluation"; Table 1 shows AUROC drops from ~0.95 to ~0.87–0.92; TPR@1%FPR drops from ~0.27 to ~0.12

### Mechanism 2: Physicochemical Descriptors Capture Functional Hazard Signatures
- Claim: Global physicochemical and composition features encode sufficient signal to distinguish toxins from benign proteins
- Mechanism: Toxins exhibit distinct biophysical properties (e.g., higher hydrophobicity for membrane disruption, specific charge patterns, instability indices) that aggregate into discriminative feature vectors even without positional information
- Core assumption: Hazardous proteins possess conserved biophysical properties correlated with toxic function
- Evidence anchors: [abstract]: Models use "interpretable features (global physicochemical descriptors and amino-acid composition)"; Spurious-signal probes show length-only and composition-only features yield near-random AUROC (~0.50–0.55), but residue shuffling degrades performance further—indicating models capture motif/ordering signals beyond bulk properties

### Mechanism 3: Post-hoc Calibration Aligns Probabilities with Screening Requirements
- Claim: Isotonic/Platt calibration produces reliable probability estimates suitable for threshold-based decision-making
- Mechanism: CalibratedClassifierCV maps raw classifier scores to empirically calibrated probabilities via 5-fold internal cross-validation; linear models benefit more due to their already-near-linear score distributions
- Core assumption: Screening workflows require well-calibrated probabilities to set risk-aware thresholds (e.g., TPR@1%FPR)
- Evidence anchors: [abstract]: "calibrated linear models exhibit comparatively good calibration, while tree ensembles retain slightly higher Brier/ECE"; Table 2 shows LogReg Brier=0.140, ECE=0.118 vs RF Brier=0.156, ECE=0.138 on cluster split

## Foundational Learning

- Concept: Homology Clustering with CD-HIT
  - Why needed here: Creates train/test splits that simulate "never-before-seen" sequences by enforcing maximum identity thresholds
  - Quick check question: Why does a random 80/20 split give AUROC ~0.95 while a cluster 80/20 split gives ~0.87?

- Concept: Probability Calibration (Brier Score, ECE, Isotonic/Platt)
  - Why needed here: Raw classifier scores often don't reflect true probabilities; calibration enables threshold-setting based on actual risk
  - Quick check question: If a model outputs 0.80 probability, what does Brier score tell you about whether 80% of such predictions are actually positive?

- Concept: Operating-Point Metrics for Asymmetric Costs
  - Why needed here: Biosecurity screening has highly asymmetric error costs—false negatives (missed hazards) are far more consequential than false positives
  - Quick check question: Why is TPR@1%FPR more informative than overall AUROC for this application?

## Architecture Onboarding

- Component map: Data ingestion (SafeProtein hazards + UniProt benigns) -> Clustering (CD-HIT ≤40% identity) -> Feature extraction (ProtParam + 20-aa composition) -> Models (L2-LogReg, Linear SVM, Random Forest) -> Evaluation (bootstrap CI with AUROC, AUPRC, TPR@1%FPR, FPR@95%TPR, Brier, ECE)

- Critical path: 1) Fetch sequences via accession IDs from public databases (UniProt, SafeProtein) 2) Extract features using Biopython's ProtParam (no GPU needed) 3) Load pre-computed cluster assignments and split labels from metadata CSV 4) Fit classifier on train set, apply CalibratedClassifierCV internally 5) Evaluate on held-out test clusters with bootstrap CI

- Design tradeoffs:
  - CPU-only vs GPU embeddings: Sacrifices potential accuracy gains from learned representations for accessibility and reproducibility
  - Metadata-only release: Enhances safety but requires users to fetch sequences externally
  - Linear models vs RF: LogReg offers better calibration; RF offers slightly higher AUROC—choose based on whether probability quality or discrimination is primary concern
  - 40% identity threshold: Stricter thresholds (e.g., 25%) would be more stringent but reduce dataset size

- Failure signatures:
  - Large AUROC gap between random and cluster splits indicates memorization/homology leakage
  - Length-only ablation achieving high AUROC signals trivial cue exploitation (should fail per design)
  - Very wide bootstrap CIs at TPR@1%FPR indicates insufficient support at extreme operating points
  - ECE >> 0 with calibration applied suggests calibration fold mismatch or model misspecification

- First 3 experiments:
  1. Reproduce Table 1 cluster-split results to validate pipeline correctness (target: RF AUROC 0.919 [0.866–0.961])
  2. Run composition-preserving residue shuffle on test sequences; expect degradation confirming motif-level signal capture
  3. Train a new classifier (e.g., gradient-boosted trees) and compare Brier/ECE against LogReg to assess calibration tradeoffs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would incorporating explicit signal peptide or N-terminal heuristics improve hazard discrimination, particularly for toxin families where secretion pathways are functionally critical?
- Basis in paper: [explicit] Authors state "signal peptides are left for future work" (p. 2) and list "no explicit signal-peptide covariate (left for future work)" and "explicit SP/N-terminal heuristics" as future directions (p. 7–8)
- Why unresolved: Current features capture only global physicochemical properties and composition; secretion signals are known correlates of toxicity but were deliberately excluded from this baseline
- What evidence would resolve it: Ablation experiments adding signal peptide prediction scores (e.g., from SignalP) or N-terminal motif features, evaluated under the same homology-clustered protocol with comparisons to base feature performance

### Open Question 2
- Question: Can lightweight structure-aware proxies (e.g., predicted secondary structure, solvent accessibility, or contact maps) improve generalization to novel homology groups under cluster holdouts?
- Basis in paper: [explicit] Limitations section lists "sequence-only features (no structure/context)" (p. 7–8), and future directions include "integrating lightweight structure proxies" while maintaining CPU-only execution (p. 8)
- Why unresolved: Structure may encode functional constraints preserved across distant homologs that sequence composition alone misses; however, structure prediction adds computational overhead
- What evidence would resolve it: Experiments augmenting the current feature set with predicted structural descriptors from tools like NetSurfP or ESM-fold, measuring AUROC gaps between random and cluster splits

### Open Question 3
- Question: How does SafeBench-Seq performance scale with dataset size and taxonomic coverage, particularly for underrepresented toxin families with insufficient sample sizes?
- Basis in paper: [inferred] Authors note "moderate dataset size (854 sequences) may limit statistical power for rare toxin families" (p. 8) and show heterogeneous family-level performance with some clusters near AUROC ≈ 0.80 (p. 6)
- Why unresolved: The current benchmark balances classes but may lack depth for rare toxin categories; whether model performance plateaus or improves with more data per family remains unknown
- What evidence would resolve it: Systematic subsampling experiments and evaluation on expanded toxin datasets (e.g., adding venoms, bacterial toxins) with per-family power analyses and confidence interval narrowing assessments

## Limitations
- CPU-only design sacrifices potential accuracy gains from modern protein language models
- Dataset size (854 sequences) may limit statistical power for rare toxin families
- 30-1000 amino acid length filter excludes very short functional peptides and large multidomain proteins

## Confidence

- Homology-clustered evaluation prevents overestimation: High confidence
- Physicochemical features capture hazard signatures: Medium confidence
- Linear models offer superior calibration: High confidence

## Next Checks

1. **Cross-validation on emerging toxin families**: Apply the trained baseline to recent toxin discovery papers or databases not represented in SafeProtein to assess generalization to novel hazard classes and validate the cluster-split assumption.

2. **Local motif ablation study**: Systematically mask or shuffle known functional motifs (catalytic triads, binding sites) in test sequences to quantify the contribution of local vs global features to model performance and identify potential failure modes.

3. **Deployment simulation with calibration drift**: Simulate realistic deployment by training calibration on a subset of clusters, then testing on held-out clusters from different phylogenetic distributions to measure calibration robustness and identify when recalibration would be necessary.