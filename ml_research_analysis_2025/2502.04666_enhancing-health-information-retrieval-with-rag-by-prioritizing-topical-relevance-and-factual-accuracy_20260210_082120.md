---
ver: rpa2
title: Enhancing Health Information Retrieval with RAG by Prioritizing Topical Relevance
  and Factual Accuracy
arxiv_id: '2502.04666'
source_url: https://arxiv.org/abs/2502.04666
tags:
- health
- information
- retrieval
- arxiv
- factual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a Retrieval-Augmented Generation (RAG) model
  to enhance Health Information Retrieval by addressing both topical relevance and
  factual accuracy. The model leverages scientific literature from PubMed Central
  (PMC) to retrieve relevant passages, generates a contextually relevant text (GenText)
  using Large Language Models (LLMs), and ranks documents based on topicality and
  factual accuracy.
---

# Enhancing Health Information Retrieval with RAG by Prioritizing Topical Relevance and Factual Accuracy

## Quick Facts
- arXiv ID: 2502.04666
- Source URL: https://arxiv.org/abs/2502.04666
- Reference count: 40
- Key outcome: RAG model combining topical relevance and factual accuracy significantly improves health information retrieval, achieving CAM MAP up to 0.1602 and CAM NDCG up to 0.2723

## Executive Summary
This paper proposes a novel RAG-based system for health information retrieval that addresses both topical relevance and factual accuracy. The model retrieves scientific passages from PubMed Central, generates a concise "GenText" using LLMs, and ranks documents by combining topicality (BM25) with factual accuracy (stance detection + semantic similarity). Experimental results on CLEF eHealth and TREC Health Misinformation datasets show significant improvements over baseline models. The GenText generation also provides explainability by referencing scientific sources.

## Method Summary
The proposed RAG model operates in two stages: (1) retrieval of relevant scientific passages from PubMed Central using BM25 and NER-based filtering, and (2) generation of a concise, citation-rich GenText using LLMs (GPT-3, Llama 3, or Falcon) with strict constraints. Documents are ranked by combining topical relevance scores with factual accuracy scores computed via stance detection (SCIFIVE) and semantic similarity against the GenText. The system explicitly separates topicality from accuracy scoring, allowing users to understand the reasoning behind rankings.

## Key Results
- CAM MAP scores reached 0.1602 on TREC Health Misinformation dataset
- CAM NDCG scores reached 0.2723 on the same dataset
- Significant improvements over baseline models that only consider topical relevance
- GenText generation provides explainability by referencing scientific sources

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Generating scientific "GenText" via RAG allows scoring external documents for factual accuracy relative to trusted literature
- **Mechanism:** PMC passages are retrieved and fed to LLM with strict prompt to generate 64-word GenText. Documents are compared to this GenText rather than raw query
- **Core assumption:** GenText acts as bias-free proxy for scientific truth, reducing LLM hallucinations
- **Evidence anchors:** Abstract states passages are "processed by LLMs to generate a contextually relevant rich text (GenText)"; Section 3.2 mentions prompt instruction to "Do not use extra knowledge"
- **Break condition:** If initial PMC retrieval extracts irrelevant passages or LLM ignores prompt constraints and hallucinates

### Mechanism 2
- **Claim:** Combining semantic similarity with stance detection provides more robust factual accuracy than keyword overlap alone
- **Mechanism:** Factual Accuracy score combines cosine similarity (semantic overlap) with stance detection logits (agreement with GenText)
- **Core assumption:** Stance detection models can reliably quantify agreement between web documents and generated scientific text
- **Evidence anchors:** Abstract mentions "ranking documents by combining topicality and factual accuracy scores computed via stance detection and semantic similarity"
- **Break condition:** If document uses highly similar vocabulary but contradicts scientific consensus, weighting factor α must be precisely tuned

### Mechanism 3
- **Claim:** Discounting passages lacking specific medical entities improves precision of generation context
- **Mechanism:** During PMC retrieval, passages are penalized if they don't contain Named Entities matching the query
- **Core assumption:** Query-passage relevance in medical domain strictly depends on shared medical entities
- **Evidence anchors:** Section 3.1 mentions "passages receive lower weights if they do not contain Named Entities"
- **Break condition:** If queries contain layman terms not recognized by NER system, relevant passages using medical synonyms might be incorrectly discounted

## Foundational Learning

- **Concept: Retrieval-Augmented Generation (RAG)**
  - **Why needed here:** Architectural core that injects fresh, specific medical data into context window rather than relying on LLM's internal weights
  - **Quick check question:** What is the specific source of truth used to ground the LLM? (Answer: PubMed Central)

- **Concept: Stance Detection**
  - **Why needed here:** Standard semantic similarity cannot distinguish between documents that quote myths vs. debunk them
  - **Quick check question:** Why is cosine similarity alone insufficient for factual accuracy? (Answer: High similarity can exist between refuting and promoting text)

- **Concept: Multi-dimensional Relevance**
  - **Why needed here:** Explicitly separates "is this about the topic?" from "is this true?" and combines them via weighted parameter β
  - **Quick check question:** In final ranking formula RSV = β·T + (1-β)·F, what happens if β = 1? (Answer: System ranks solely by topical relevance, ignoring factual accuracy)

## Architecture Onboarding

- **Component map:** User Query → PMC Retriever → LLM GenText Generator → Scoring Module (Topicality + Accuracy) → Ranking Aggregator
- **Critical path:** Generation of GenText - if this contains hallucinations or irrelevant context, subsequent document comparison will rank valid sources incorrectly
- **Design tradeoffs:** Latency vs. Accuracy (requires multiple inference steps), Strict Context vs. LLM Utility (64-word constraint trades fluency for factuality)
- **Failure signatures:** Poisoned Context (GenText asserts falsehood from irrelevant PMC papers), Stance Ambiguity (SCIFIVE returns neutral for satirical documents)
- **First 3 experiments:**
  1. Manually review 50 random GenText outputs for hallucinations not present in retrieved passages
  2. Grid search on weighting factors α and β across different health topics
  3. Disable NER discounting to quantify its contribution to final CAM MAP score

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How much do domain-specific LLMs (Med-PaLM, MedMT5) improve performance compared to general-purpose models tested?
- Basis in paper: [explicit] Conclusion states "it would be beneficial to evaluate the application of domain-specific LLMs, such as Med-PaLM... [and] MedMT5"
- Why unresolved: Current study used general-purpose LLMs to establish baseline, leaving specialized medical pre-training gains unquantified
- What evidence would resolve it: Comparative ablation study on same datasets substituting general LLMs with domain-specific alternatives

### Open Question 2
- Question: Can alternative methods for calculating factual accuracy outperform weighted combination of stance detection and cosine similarity?
- Basis in paper: [explicit] Authors acknowledge "assessment of factual accuracy provides an approximation rather than definitive measure of truth"
- Why unresolved: Current methodology relies on specific linear combination that may not capture nuanced contradictions
- What evidence would resolve it: Experiments integrating NLI-based entailment checks or external fact-checking APIs

### Open Question 3
- Question: How does relying on only first highly relevant article for GenText generation affect robustness vs. synthesizing multiple top-ranked articles?
- Basis in paper: [inferred] Section 4.4 mentions "excellent results are obtained by considering only first highly relevant scientific article"
- Why unresolved: Using single source risks anchoring to retrieval noise of top-1 result, missing consensus views across top-k results
- What evidence would resolve it: Comparison of GenText quality and final retrieval accuracy when context includes top-3 or top-5 articles

## Limitations
- Reliance on PubMed Central may limit coverage of emerging health information not yet in peer-reviewed literature
- Strict 64-word constraint may prevent capturing necessary context in complex medical topics
- Stance detection component may not generalize perfectly to all types of health misinformation

## Confidence

**High Confidence:** Core retrieval architecture is well-described and follows established RAG principles; improvement in CAM MAP and CAMNDCG scores is clearly demonstrated

**Medium Confidence:** Mechanism by which stance detection improves factual accuracy scoring is plausible but depends on SCIFIVE model quality and generalizability

**Low Confidence:** Assumption that restricting LLM generation to 64 words and disallowing "extra knowledge" will consistently produce hallucination-free GenText is not empirically validated

## Next Checks

1. **Hallucination Audit:** Manually examine 100 randomly selected GenText outputs to quantify hallucination rates, checking whether facts appear in neither query nor retrieved PMC passages

2. **Temporal Validity Test:** Measure retrieval performance on time-sensitive health information (e.g., "COVID-19 vaccine effectiveness 2024") to assess whether PMC's publication lag creates accuracy gaps

3. **Stance Detection Robustness:** Create adversarial test cases where documents contain scientifically accurate but unpopular positions to verify stance detection doesn't penalize legitimate scientific discourse