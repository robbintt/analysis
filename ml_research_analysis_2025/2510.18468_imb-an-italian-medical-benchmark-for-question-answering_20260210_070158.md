---
ver: rpa2
title: 'IMB: An Italian Medical Benchmark for Question Answering'
arxiv_id: '2510.18468'
source_url: https://arxiv.org/abs/2510.18468
tags:
- medical
- question
- questions
- https
- italian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces IMB, the first large-scale Italian medical
  benchmark for question answering, consisting of two datasets: IMB-QA with 782,644
  patient-doctor conversations across 77 medical categories, and IMB-MCQA with 25,862
  multiple-choice questions from medical specialty exams. The authors address the
  challenge of limited medical QA resources for non-English languages by leveraging
  Large Language Models to improve data quality and anonymization while preserving
  conversational authenticity.'
---

# IMB: An Italian Medical Benchmark for Question Answering

## Quick Facts
- **arXiv ID**: 2510.18468
- **Source URL**: https://arxiv.org/abs/2510.18468
- **Reference count**: 40
- **Primary result**: IMB is the first large-scale Italian medical benchmark for question answering, demonstrating that specialized adaptation strategies can outperform larger general-purpose models in medical QA tasks.

## Executive Summary
This paper introduces IMB, the first large-scale Italian medical benchmark for question answering, consisting of two datasets: IMB-QA with 782,644 patient-doctor conversations across 77 medical categories, and IMB-MCQA with 25,862 multiple-choice questions from medical specialty exams. The authors address the challenge of limited medical QA resources for non-English languages by leveraging Large Language Models to improve data quality and anonymization while preserving conversational authenticity. Through experiments with Retrieval-Augmented Generation (RAG) and domain-specific fine-tuning, they demonstrate that specialized adaptation strategies can outperform larger general-purpose models in medical QA tasks. Their results show BERTScore Precision improvements up to 0.638 and accuracy gains through RAG and fine-tuning, challenging the assumption that model scale is the primary factor for medical QA success. The datasets and evaluation frameworks are publicly released to support multilingual medical QA research.

## Method Summary
The authors created IMB by collecting patient-doctor conversations from Italian medical forums (MedicItalia, Dica33) and multiple-choice questions from medical specialty exams (CompitoInClasse.org). They employed a preprocessing pipeline using Llama3-Med42-8B to reformulate answers for clarity and anonymize PII, reducing PII presence from 27% to 1%. The open-ended QA task (IMB-QA) was evaluated using BERTScore Precision with bert-base-multilingual-cased, while the multiple-choice task (IMB-MCQA) used standard accuracy metrics. They implemented RAG using all-MiniLM-L6-v2 embeddings with FAISS indexing (top-5 retrieval from 100k answer knowledge base) and fine-tuned small language models using the Unsloth library with 6 epochs, cross-entropy loss, learning rate 2.97e-4, and 80/20 train/eval split.

## Key Results
- IMB-QA dataset contains 782,644 QA pairs across 77 medical categories with PII reduced from 27% to 1% through LLM-based anonymization
- BERTScore Precision improvements up to 0.638 achieved through RAG and fine-tuning strategies
- Fine-tuned Llama-3.2-1B improved BERTScore F1 from 0.6423 to 0.6976, outperforming larger general-purpose models
- RAG consistently improved BERTScore Precision across all medical categories (e.g., Neurology saw an 8.12% increase)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Reformulating colloquial medical text using domain-specific LLMs improves data consistency and anonymization while preserving semantic fidelity.
- **Mechanism:** A medical-specialized LLM (Llama3-Med42-8B) processes raw forum responses to remove redundancies and colloquialisms while a NER model identifies PII. The LLM then semantically masks these entities, converting informal patient-doctor exchanges into standardized training data.
- **Core assumption:** The LLM-based reformulation does not introduce factual hallucinations or significantly alter the clinical meaning during the rewriting process.
- **Evidence anchors:** Reports that PII presence dropped from 27% to 1% after the LLM-based anonymization and reformulation pipeline; States LLMs were leveraged to "improve the clarity and consistency of medical forum data while retaining their original meaning."

### Mechanism 2
- **Claim:** Retrieval-Augmented Generation (RAG) enhances semantic precision in open-ended medical QA by grounding responses in verified domain knowledge.
- **Mechanism:** A dense vector index (FAISS) of anonymized answers is searched using query embeddings (all-MiniLM-L6-v2). The top-5 retrieved contexts are injected into the prompt, providing the generator with specific medical facts that may not be present in its parametric memory.
- **Core assumption:** The retrieval mechanism successfully identifies contextually relevant and factually correct passages from the dataset, and the generator can effectively synthesize this context.
- **Evidence anchors:** Shows RAG consistently improved BERTScore Precision across all medical categories (e.g., Neurology saw an 8.12% increase); Mentions that RAG and domain-specific fine-tuning "can outperform larger, general-purpose models."

### Mechanism 3
- **Claim:** Fine-tuning Small Language Models (SLMs) on domain-specific conversational data provides a parameter-efficient path to competitive performance.
- **Mechanism:** Models like Llama-3.2-1B are trained on the IMB-QA dataset using techniques like Curriculum Learning. This shifts the model's prior from general text to specific medical dialogue, reducing the parameter count required to encode this knowledge from scratch.
- **Core assumption:** The dataset size (782k samples) is sufficient to overwrite or adapt the general priors of the SLM without causing catastrophic forgetting.
- **Evidence anchors:** Demonstrates that fine-tuned Llama-3.2-1B improved BERTScore F1 from 0.6423 to 0.6976; Suggests systems "benefit more from domain expertise... than from increased model scale."

## Foundational Learning

- **Concept:** **BERTScore for Semantic Evaluation**
  - **Why needed here:** Traditional metrics like BLEU or ROUGE measure n-gram overlap, which fails to capture semantic equivalence in medical answers (e.g., synonyms like "hypertension" vs. "high blood pressure"). This paper relies on BERTScore to assess if the model's answer is clinically semantically similar to the reference.
  - **Quick check question:** If a model answers "heart attack" instead of "myocardial infarction," would BLEU or BERTScore be more appropriate to capture correctness?

- **Concept:** **PII Anonymization & Redaction**
  - **Why needed here:** Medical data is inherently sensitive. The paper employs a pipeline involving Named Entity Recognition (NER) to identify sensitive entities and LLMs to rewrite sentences, ensuring GDPR compliance before data release.
  - **Quick check question:** Does the anonymization strategy simply mask entities (e.g., "[NAME]") or semantically rewrite the sentence to remove the need for the identifier entirely?

- **Concept:** **RAG (Retrieval-Augmented Generation)**
  - **Why needed here:** The paper uses RAG to mitigate hallucinations by grounding the model in a retrieved knowledge base (IMB-QA answers). This is critical in medicine where factual accuracy is non-negotiable.
  - **Quick check question:** What is the specific retrieval function $R(Q, D)$ used in the paper to fetch context for the query?

## Architecture Onboarding

- **Component map:** Source Ingest (MedicItalia/Dica33 forums + CompitoInClasse exams) -> Preprocessing Pipeline (NER Anonymizer + LLM Re-writer) -> Knowledge Store (FAISS index of 100k anonymized answers) -> Evaluator (LLMs with Standard/RAG modes)

- **Critical path:** The Data Preprocessing Pipeline is the most critical step. The utility of the benchmark depends entirely on the Llama3-Med42-8B's ability to successfully anonymize PII (reducing it from 27% to 1%) while retaining medical logic.

- **Design tradeoffs:**
  - Synthetic vs. Raw Data: The authors chose to reformulate answers using LLMs to improve clarity and anonymize. The tradeoff is the potential introduction of synthetic artifacts or loss of nuance present in the original raw doctor responses.
  - Scale vs. Specialization: The paper argues for fine-tuning smaller models (SLMs) rather than relying on 70B+ parameter general models. The tradeoff is reduced general world knowledge for efficient domain specialization.

- **Failure signatures:**
  - Category Imbalance: Models may overfit on "General Medicine" or "Gastroenterology" (high frequency) and fail on "Sleep Medicine" or "Pediatric Surgery" (low frequency).
  - Metric Discrepancy: A model may show improved BERTScore (semantic similarity) but reduced METEOR score (fluency/overlap) after fine-tuning, indicating outputs that are accurate but stylistically distinct from the reference.

- **First 3 experiments:**
  1. **RAG Ablation:** Reproduce the retrieval setup by indexing the provided 100k subset of IMB-QA and comparing Gemma-2-9b-it performance with and without context retrieval (measuring BERTScore Precision delta).
  2. **SLM Fine-Tuning:** Fine-tune Llama-3.2-1B on the IMB-QA training split using the Unsloth library (FP16, 6 epochs) and verify the reported jump in ROUGE/BERTScore.
  3. **Specialty Gap Analysis:** Evaluate a baseline model (e.g., Mistral-7B) specifically on the "Thermal Medicine" or "Neurology" categories in IMB-MCQA to confirm the difficulty spikes reported in Figures 2 and 3.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the LLM-based reformulation of ground-truth answers introduce factual hallucinations or stylistic bias into the dataset?
- **Basis in paper:** The authors used Llama3-Med42-8B to rewrite answers for clarity (Section 3.2.1) without reporting a validation metric for factual preservation during this preprocessing step.
- **Why unresolved:** Automated rewriting can smooth over subtle medical nuances or introduce distinct model biases that become part of the "gold standard" for benchmarking.
- **What evidence would resolve it:** A human expert evaluation comparing the original forum answers against the reformulated versions to measure factual consistency and drift.

### Open Question 2
- **Question:** What is the relationship between high BERTScore values and clinical factuality in Italian medical QA?
- **Basis in paper:** The authors explicitly state that "formal hallucination metrics are not reported" despite relying on semantic similarity scores like BERTScore for evaluation (Section 4.3).
- **Why unresolved:** A model could achieve high semantic overlap with a reference answer while generating plausible but clinically dangerous misinformation.
- **What evidence would resolve it:** A study correlating BERTScore/ROUGE results with expert-annotated hallucination rates on the IMB-QA test set.

### Open Question 3
- **Question:** How does the choice of general-purpose embeddings impact the retrieval efficacy of Italian medical terminology in RAG systems?
- **Basis in paper:** The authors state they "did not perform a separate retriever evaluation" while using the `all-MiniLM-L6-v2` model for the retrieval index (Section 4.2).
- **Why unresolved:** It is unclear if the bottleneck in RAG performance is the retriever's limited understanding of Italian medical vernacular or the generator's reasoning capabilities.
- **What evidence would resolve it:** A comparative analysis of retrieval accuracy using general multilingual embeddings versus domain-specific Italian medical embeddings.

### Open Question 4
- **Question:** Can data balancing techniques effectively generalize model performance to underrepresented medical specialties?
- **Basis in paper:** The authors identify "imbalance in specialty representation" (e.g., overrepresentation of Gastroenterology) and list "improving category balancing" as a goal for future work (Section 6).
- **Why unresolved:** Current models may overfit to high-frequency categories while failing to learn meaningful representations for low-resource specialties like Pediatric Surgery.
- **What evidence would resolve it:** Fine-tuning experiments using oversampling or weighted loss functions on low-frequency categories to measure accuracy improvements.

## Limitations
- The exact prompt templates used for LLM-based anonymization and reformulation are unspecified, making it difficult to verify reported PII reduction and assess semantic drift risk
- Category imbalance in the dataset (overrepresentation of General Medicine/Gastroenterology) may bias model performance and evaluation
- Claims about fine-tuning small models outperforming larger ones assume sufficient dataset size for effective adaptation, but curriculum learning details remain undocumented

## Confidence
- **High confidence**: The dataset statistics (782k QA pairs, 25k MCQA questions) and the basic RAG implementation using FAISS and all-MiniLM-L6-v2 embeddings are clearly specified and reproducible
- **Medium confidence**: Claims about PII anonymization success and semantic preservation during LLM reformulation are plausible given reported numbers, but depend on opaque prompt engineering
- **Medium confidence**: The reported performance gains from fine-tuning small models (e.g., Llama-3.2-1B) are well-supported by reported metrics, but hinge on undocumented curriculum learning details
- **Low confidence**: Claims about category-level performance differences and the robustness of models on rare medical specialties lack granular validation due to dataset imbalance

## Next Checks
1. **PII Anonymization Audit**: Manually sample 100 anonymized answers from IMB-QA and check for residual personally identifiable information (names, addresses, IDs) to verify the claimed 1% PII rate
2. **Category-Level Robustness Test**: Train and evaluate a baseline model (e.g., Mistral-7B) on each of the 77 medical categories separately, focusing on low-frequency specialties (e.g., "Sleep Medicine", "Pediatric Surgery") to confirm or refute the claimed difficulty gaps
3. **RAG Retrieval Quality Check**: For a random sample of 50 queries, manually inspect the top-5 retrieved contexts for semantic relevance and factual accuracy, and measure if retrieval actually improves BERTScore versus random context injection