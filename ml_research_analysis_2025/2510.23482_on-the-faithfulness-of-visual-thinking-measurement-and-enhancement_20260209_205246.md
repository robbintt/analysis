---
ver: rpa2
title: 'On the Faithfulness of Visual Thinking: Measurement and Enhancement'
arxiv_id: '2510.23482'
source_url: https://arxiv.org/abs/2510.23482
tags:
- visual
- image
- mcot
- information
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the unfaithfulness of visual information in
  multimodal chain-of-thought (MCoT) reasoning, where visual cues are often ignored
  or inaccurate. The authors attribute this issue to reward designs that incentivize
  visual information presence without ensuring its correctness.
---

# On the Faithfulness of Visual Thinking: Measurement and Enhancement

## Quick Facts
- arXiv ID: 2510.23482
- Source URL: https://arxiv.org/abs/2510.23482
- Reference count: 40
- The authors introduce Sufficient-Component Cause Model (SCCM) learning strategy and LVLM-based evaluation pipeline to improve faithfulness of visual information in multimodal chain-of-thought reasoning.

## Executive Summary
This paper addresses the unfaithfulness problem in multimodal chain-of-thought (MCoT) reasoning where visual components are often ignored or inaccurate. The authors attribute this issue to reinforcement learning reward designs that incentivize visual information presence without ensuring correctness. They introduce an LVLM-based evaluation pipeline assessing visual reliability and sufficiency, and propose SCCM learning strategy that requires visual components to be both sufficient (independently capable of leading to correct answers) and minimal (no extra irrelevant information). SCCM is annotation-free and compatible with existing RFT frameworks. Empirical results show SCCM consistently improves visual faithfulness across multiple benchmarks, achieving up to 89.56% sufficiency and 82.61% reliability on V*Bench.

## Method Summary
The authors propose the Sufficient-Component Cause Model (SCCM) learning strategy that modifies the reward function in reinforcement fine-tuning to ensure visual components are both sufficient and minimal. The approach uses a two-stage training process: first SFT fine-tuning on the Pixel-Reasoner dataset, then GRPO training with modified rewards. The key innovation is the SCCM reward that multiplies sufficiency and minimality rewards, forcing visual components to independently lead to correct answers while remaining information-efficient. The method is annotation-free, requiring only ground-truth answers rather than detailed visual annotations.

## Key Results
- SCCM achieves 89.56% sufficiency and 82.61% reliability on V*Bench, significantly outperforming baselines
- Intervention experiments show SCCM models use visual evidence more effectively, with higher Average Treatment Effect (ATE) for visual corruption
- SCCM prevents reward hacking by ensuring visual components are necessary and sufficient for correct answers
- The method works across multiple benchmarks including V*Bench, HR-Bench 4K, and HR-Bench 8K

## Why This Works (Mechanism)

### Mechanism 1: Reward Hacking via Visual Presence Incentives
Standard RFT reward designs incentivize the format of interleaved vision-text cues without verifying visual correctness or relevance, enabling models to include arbitrary visual information while relying primarily on textual reasoning. The reward signals `r_acc(y)` and `r_format(y)` encourage tool usage and answer accuracy but do not enforce that visual evidence causally supports the conclusion. Models learn to generate visual operations that satisfy format constraints while bypassing visual reasoning pathways.

### Mechanism 2: SCCM Reward Composition (Sufficiency × Minimality)
Multiplying sufficiency and minimality rewards (`r_s · r_m`) creates a sparse reward landscape that forces visual components to be independently capable while remaining information-efficient. The sufficiency reward `r_s ∈ {0,1}` requires that visual components alone predict the correct answer (evaluated by a judge model). The minimality reward `r_m ∈ (0,2]` amplifies this only when visual information is below average in token count. This multiplication ensures no positive reward is given unless sufficiency is satisfied, preventing gaming through large crops.

### Mechanism 3: Causal Intervention for Faithfulness Diagnosis
Intervention experiments (corrupting visual vs. textual components) reveal the causal contribution of each modality to final predictions, exposing whether visual evidence is actually used. Replace visual components `V` with random noise (`do(V)`) or inject textual errors (`do(T)`), then measure accuracy drop via Average Treatment Effect (ATE). Low ATE for visual intervention indicates the model ignored visual evidence; high ATE for textual intervention indicates heavy text reliance.

## Foundational Learning

- **Concept: Multimodal Chain-of-Thought (MCoT)**
  - Why needed here: The entire framework operates on MCoT traces where visual and textual reasoning steps are interleaved. Understanding that visual components are observation tokens from tool calls (not generated images) is critical.
  - Quick check question: Can you explain why visual components in this paradigm are treated as observations rather than generations?

- **Concept: Reinforcement Fine-Tuning (RFT) with GRPO**
  - Why needed here: SCCM is a reward modification within RFT. Understanding group-relative policy optimization and how rewards shape behavior is essential for diagnosis.
  - Quick check question: How does the group-relative formulation of GRIM (Group Relative Information Minimization) differ from absolute penalty terms?

- **Concept: Structural Causal Models (SCM) and Intervention**
  - Why needed here: The faithfulness evaluation uses Pearl's SCM framework. You need to understand `do(X)` notation and how ATE quantifies causal effects.
  - Quick check question: Why is McNemar's test appropriate for binary accuracy changes under intervention?

## Architecture Onboarding

- **Component map:**
  Input (I, Q) → Policy Model (Qwen2.5-VL-7B) → MCoT Trace [(T,V)...] → Answer A
  ↓
  Reward Computation:
  r_final = r_acc + r_format + α·r_s·r_m
  ↓        ↓          ↓
  Accuracy  Format    Sufficiency × Minimality
  ↓              ↓
  Judge Model (Qwen2.5-VL-72B)

- **Critical path:** The sufficiency judge model's reliability is the linchpin. If `r_s` evaluation is noisy, training collapses. Verify judge consistency on held-out samples before training.

- **Design tradeoffs:**
  - Using a separate 72B judge adds inference cost but enables annotation-free training.
  - Minimality reward prevents trivial solutions but adds hyperparameter (α) and requires careful clipping.
  - GRPO with 8 rollouts provides variance reduction but increases compute.

- **Failure signatures:**
  - **Reward hacking:** Models output entire image as crop (detected by high CRZ metric).
  - **Judge collapse:** `r_s` becomes always-1 or always-0 (check judge calibration).
  - **Textual shortcut:** Accuracy improves but visual intervention ATE remains low (faithfulness didn't improve).
  - **Tool overuse:** Tool call count keeps increasing (minimality not working).

- **First 3 experiments:**
  1. **Baseline faithfulness audit:** Run intervention experiments on your warm-start model. Confirm visual ATE is low before applying SCCM.
  2. **Judge calibration check:** Evaluate sufficiency judge (`J_S`) on 100 examples with manual labels. Verify >90% agreement before training.
  3. **Minimality ablation:** Train with `α=0` (no minimality) and measure CRZ and TCC. Confirm cropped regions become excessively large, then enable minimality and observe stabilization.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the SCCM framework effectively transfer to intrinsic visual reasoning paradigms (e.g., models generating mental images) as opposed to the extrinsic tool-based cropping evaluated in this study?
- Basis in paper: [explicit] The authors explicitly define their scope in the Introduction and Related Work as "agentic 'thinking with images'" using tool calls, distinguishing it from "intrinsic generation or imagination," and do not evaluate SCCM on the latter.
- Why unresolved: The "visual information" in this paper is treated as "observation tokens" from external tools. Intrinsic generation involves creating visual tokens from internal states, which may present different challenges for establishing faithfulness and minimality.
- What evidence would resolve it: Applying the SCCM reward mechanism to a model capable of visual generation (like a text-to-image integrated reasoning model) and measuring reliability/sufficiency scores.

### Open Question 2
- Question: Can the visual components in SCCM be strengthened to the point where they are statistically *necessary* causes of the prediction across all benchmarks?
- Basis in paper: [inferred] In Table 1(c), while SCCM improves performance, the p-values for visual intervention (Hypothesis 2) remain > 0.05 (e.g., 0.8318 on HR-Bench 4K), indicating that visual evidence is still not statistically guaranteed to be a cause of the prediction in high-resolution settings.
- Why unresolved: The current method encourages sufficiency (visuals *can* yield the answer) but the intervention experiments show the model can still often ignore them (lack of causal necessity) without significantly dropping accuracy.
- What evidence would resolve it: A modification of the training objective or architecture that results in p-values < 0.05 for visual intervention across V*Bench and HR-Bench simultaneously.

### Open Question 3
- Question: Does the "Minimality" constraint impede performance on complex reasoning tasks requiring simultaneous observation of multiple distant objects?
- Basis in paper: [inferred] The GRIM (Group Relative Information Minimization) reward explicitly penalizes large visual tokens (Figure 3, Eq 8) to avoid trivial solutions, but complex spatial reasoning (e.g., "left of X and right of Y") often requires a larger field of view.
- Why unresolved: The paper demonstrates that Minimality prevents "information inefficient" solutions (Figure 4c), but does not analyze if this aggressive cropping forces the model to discard necessary context for multi-object relational queries.
- What evidence would resolve it: A comparative analysis on a benchmark specifically designed for multi-object spatial relations, comparing "Sufficiency-only" rewards versus the full SCCM (Sufficiency + Minimality).

## Limitations

- The method relies heavily on a separate 72B judge model for sufficiency evaluation, which adds inference cost and creates a single point of failure if the judge is unreliable
- The "image tokens" metric for minimality calculation is not precisely defined, which could affect reproducibility across different implementations
- The approach may struggle with complex reasoning tasks requiring simultaneous observation of multiple distant objects due to the minimality constraint

## Confidence

- **High confidence**: The mechanism that standard RFT rewards create format incentives without enforcing visual correctness (supported by clear textual evidence of reward hacking potential)
- **Medium confidence**: The multiplication reward design (`r_s · r_m`) will effectively prevent trivial solutions, as minimality constraints are theoretically sound but depend heavily on the judge's reliability
- **Medium confidence**: The intervention-based faithfulness evaluation using ATE and McNemar's test will accurately diagnose visual unfaithfulness, though this assumes interventions cleanly isolate modality contributions without unintended side effects

## Next Checks

1. **Judge calibration validation**: Before training, evaluate the sufficiency judge on 100 held-out examples with manual labels to verify >90% agreement. If agreement falls below this threshold, the sufficiency reward becomes unreliable and training may collapse.

2. **Faithfulness baseline audit**: Run the causal intervention experiments on your warm-start model to confirm that visual ATE is low (indicating visual evidence is ignored) before applying SCCM. This establishes that unfaithfulness exists and that the intervention methodology works as intended.

3. **Minimality effect measurement**: Train with `α=0` (no minimality) and measure cropped region size (CRZ) and tool call count (TCC). Confirm that crops become excessively large (entire image) without minimality, then enable minimality and verify that CRZ stabilizes at reasonable values, confirming the minimality constraint prevents trivial solutions.