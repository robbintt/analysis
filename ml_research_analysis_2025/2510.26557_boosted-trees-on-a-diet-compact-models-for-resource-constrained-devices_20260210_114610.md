---
ver: rpa2
title: 'Boosted Trees on a Diet: Compact Models for Resource-Constrained Devices'
arxiv_id: '2510.26557'
source_url: https://arxiv.org/abs/2510.26557
tags:
- penalty
- threshold
- feature
- uni00000013
- uni00000011
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Trees on a Diet (ToaD), a compression scheme
  for boosted decision trees aimed at resource-constrained IoT devices. The method
  combines training-time regularization that penalizes feature and threshold reuse
  with a specialized memory layout that leverages global lookup tables for shared
  features and thresholds.
---

# Boosted Trees on a Diet: Compact Models for Resource-Constrained Devices

## Quick Facts
- arXiv ID: 2510.26557
- Source URL: https://arxiv.org/abs/2510.26557
- Authors: Jan Stenkamp; Nina Herrmann; Benjamin Karic; Stefan Oehmcke; Fabian Gieseke
- Reference count: 40
- Primary result: 4-16x compression ratios compared to LightGBM while maintaining model performance

## Executive Summary
This paper introduces Trees on a Diet (ToaD), a compression scheme for boosted decision trees aimed at resource-constrained IoT devices. The method combines training-time regularization that penalizes feature and threshold reuse with a specialized memory layout that leverages global lookup tables for shared features and thresholds. Models are stored without pointers using bit-wise encoding, significantly reducing memory footprint. Experimental results show that ToaD achieves 4-16x compression ratios compared to LightGBM while maintaining equivalent model performance.

## Method Summary
The Trees on a Diet (ToaD) method introduces a novel compression approach for boosted decision trees through two main innovations. First, it employs training-time regularization that penalizes the reuse of features and thresholds during tree construction, encouraging more diverse splits and reducing redundancy. Second, it implements a specialized memory layout that stores models without pointers, instead using bit-wise encoding and global lookup tables for shared features and thresholds. This approach transforms the traditional pointer-based tree structure into a more compact representation suitable for resource-constrained devices.

## Key Results
- Achieves 4-16x compression ratios compared to LightGBM baselines
- Maintains equivalent model performance despite significant size reduction
- Enables deployment of high-quality boosted trees on devices with limited memory and computing power
- Supports autonomous operation in edge computing scenarios

## Why This Works (Mechanism)
ToaD works by addressing the inherent redundancy in boosted tree models. Traditional boosted trees often reuse the same features and similar thresholds across different trees, creating opportunities for compression. The training-time regularization explicitly discourages this reuse, forcing the model to explore more diverse feature-threshold combinations. The specialized memory layout then exploits this diversity by creating global lookup tables for all features and thresholds used across the ensemble, eliminating the need for repeated storage. The bit-wise encoding further reduces overhead by removing pointer structures typically required for tree navigation.

## Foundational Learning
- **Boosted Decision Trees**: Ensemble methods that build trees sequentially, each correcting errors of the previous one. Needed to understand the baseline approach being compressed.
- **Feature and Threshold Reuse**: Common in boosted trees where similar splits appear across multiple trees. Understanding this redundancy is crucial for appreciating the compression opportunity.
- **Memory Layout Optimization**: Techniques for storing data structures efficiently. Essential for understanding how ToaD achieves its compression ratios.
- **Regularization in Tree Learning**: Methods to prevent overfitting and encourage generalization. Critical for understanding how ToaD modifies the training process.

## Architecture Onboarding

**Component Map**
Feature Threshold Pool -> Regularized Tree Learner -> Bit-wise Encoder -> Compressed Model Storage

**Critical Path**
Training -> Regularization -> Tree Construction -> Encoding -> Storage

**Design Tradeoffs**
- Memory vs. Inference Speed: Compression reduces memory but may increase lookup overhead
- Regularization Strength: Stronger regularization increases compression but may reduce model accuracy
- Encoding Complexity: More sophisticated encoding yields better compression but increases implementation complexity

**Failure Signatures**
- Excessive regularization leading to significant performance degradation
- Encoding/decoding errors causing incorrect predictions
- Lookup table collisions or mismanagement causing memory corruption

**3 First Experiments**
1. Compare compression ratios on datasets with varying feature dimensionalities
2. Measure inference latency on actual IoT hardware vs. traditional boosted trees
3. Test model accuracy sensitivity to different regularization strength parameters

## Open Questions the Paper Calls Out
None

## Limitations
- Generalization of compression ratios across diverse problem domains remains untested beyond reported datasets
- Computational overhead during inference for lookup table operations is not thoroughly characterized
- Scalability with very deep trees or extremely high-dimensional features needs further validation

## Confidence
- **High Confidence**: Memory compression effectiveness and bit-wise encoding implementation
- **Medium Confidence**: Performance preservation claims across all tested datasets
- **Medium Confidence**: Applicability to general IoT deployment scenarios

## Next Checks
1. Test compression and performance metrics on high-dimensional sparse datasets (e.g., click-through prediction) to evaluate scalability
2. Measure inference latency and energy consumption on actual resource-constrained hardware (not just simulation)
3. Validate model robustness when applying the regularization approach to extremely deep trees (>1000 leaves) to assess training stability