---
ver: rpa2
title: Client Selection in Federated Learning with Data Heterogeneity and Network
  Latencies
arxiv_id: '2504.01921'
source_url: https://arxiv.org/abs/2504.01921
tags:
- heterogeneity
- clients
- client
- data
- selection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes two novel client selection schemes for federated
  learning that jointly handle data heterogeneity and network latency heterogeneity
  among clients. The first scheme, DelayHetSubmodular, selects a fixed subset of clients
  via submodular optimization, while the second, DelayHetSampling, samples clients
  according to a learned probability distribution.
---

# Client Selection in Federated Learning with Data Heterogeneity and Network Latencies

## Quick Facts
- arXiv ID: 2504.01921
- Source URL: https://arxiv.org/abs/2504.01921
- Reference count: 40
- One-line primary result: Two novel client selection schemes (DelayHetSubmodular and DelayHetSampling) outperform existing baselines by up to 20× in total runtime to reach target accuracy by jointly handling data and network latency heterogeneity.

## Executive Summary
This paper addresses the challenge of client selection in federated learning (FL) when clients have both non-IID data distributions and heterogeneous network latencies. The authors propose two novel schemes, DelayHetSubmodular and DelayHetSampling, which optimize the selection of clients to minimize the total wall-clock runtime to reach a target test accuracy. These schemes balance the tradeoff between selecting fast clients (to reduce delay) and diverse clients (to reduce data heterogeneity). Theoretical analysis proves the optimality of both schemes under bounded heterogeneity assumptions. Empirical results on 9 datasets with non-IID distributions and 2 delay distributions demonstrate that the proposed algorithms significantly outperform existing baselines, including random sampling, heterogeneity-aware methods (DivFL, Power-of-Choice), and delay-aware methods (FLANP).

## Method Summary
The method introduces two client selection schemes that integrate into the standard FedAvg loop as a drop-in replacement for the `ClientSelection()` subroutine. Both schemes are derived by minimizing a theoretical runtime to convergence objective that jointly accounts for client delays and data heterogeneity. The first scheme, DelayHetSubmodular, selects a fixed subset of clients via submodular optimization, while the second, DelayHetSampling, samples clients according to a learned probability distribution. The core of both methods is a heterogeneity estimator that computes a pairwise feature heterogeneity matrix $B$ using last-layer activations from a warm-up round. The server then solves an optimization problem (submodular minimization or constrained non-linear optimization) to determine the optimal client set or sampling distribution for each round, considering the pre-computed heterogeneity matrix and known client delays.

## Key Results
- The proposed algorithms (DelayHetSubmodular and DelayHetSampling) outperform existing baselines by up to 20× in total runtime to reach target test accuracy on 9 datasets with non-IID distributions and 2 delay distributions.
- The methods demonstrate robustness to varying levels of data and latency heterogeneity, with DelayHetSubmodular performing best for high data heterogeneity with high latency variance, and DelayHetSampling being competitive for lower heterogeneity scenarios.
- Computational overhead of the selection algorithms is manageable compared to network delays, making them practical for real-world deployment.

## Why This Works (Mechanism)
The proposed schemes work by minimizing the theoretical runtime to convergence, which is a function of both client delays and data heterogeneity. By explicitly modeling the tradeoff between these two factors, the methods can select clients that are both fast and diverse, leading to faster convergence. The heterogeneity estimator provides a proxy for gradient dissimilarity, allowing the server to quantify the impact of data heterogeneity on convergence. The optimization problems solved by the `Runtime Minimizer` component then select clients that minimize the expected runtime given these estimates.

## Foundational Learning
- **Heterogeneity Estimation**: Computing pairwise feature heterogeneity $B_{ij}$ using last-layer activations as a proxy for gradient dissimilarity. *Why needed*: To quantify the impact of data heterogeneity on convergence without requiring expensive gradient computations. *Quick check*: Verify that the estimated heterogeneity values satisfy the bounded heterogeneity assumption (max_i avg_j B_ij < 1/√2).
- **Submodular Optimization**: Minimizing the runtime objective function over a fixed subset of clients. *Why needed*: To find a diverse and fast subset of clients that minimizes the expected runtime. *Quick check*: Ensure the MinNorm algorithm for submodular minimization converges and returns a valid subset.
- **Constrained Non-linear Optimization**: Solving for a sampling distribution $p^*$ that minimizes the runtime objective. *Why needed*: To find a probability distribution over clients that, when sampled, minimizes the expected runtime. *Quick check*: Verify that the solver returns a valid probability distribution (∑p_i=1) and that the optimization converges.

## Architecture Onboarding

### Component Map
1. **Heterogeneity Estimator** -> **Runtime Minimizer** -> **Selection Engine**
2. **FedAvg Loop** -> **ClientSelection() Subroutine** (replaced by proposed method)

### Critical Path
1. **Initialization**: Warm-up round with full client participation to estimate feature covariances and measure delays.
2. **Per-Round Optimization**: At the start of each round, the `Runtime Minimizer` runs to determine the optimal client set or sampling distribution.
3. **Selection & Aggregation**: Clients are selected and perform local training. Upon receiving updates, the server aggregates them using coefficients derived from the `Minimizer`.

### Design Tradeoffs
- **DelayHetSubmodular vs. DelayHetSampling**: Submodular offers lower variance and more stable selection but may exclude useful clients. Sampling explores the client population more broadly but has higher variance.
- **Estimation Granularity vs. Overhead**: Using feature covariance as a proxy is computationally cheaper than using gradients directly but is an approximation.
- **Assumption of Static Delays**: The method assumes client delays are relatively stable, which may not hold in highly dynamic networks.

### Failure Signatures
- **Convergence Stagnation**: If test loss plateaus, the estimated heterogeneity bounds may be too loose. Increase the size of the selected set or re-examine the feature representation.
- **Runtime Worse than Random**: If total wall-clock time is high, the computational overhead of the selection algorithms may be too large. Profile the solver time and optimize if necessary.
- **Non-Convergence with Sampling**: If the algorithm fails to converge, the assumption B_p < 1 may be violated. Fall back to the Submodular method.

### First 3 Experiments
1. **Baseline Comparison on Real Data**: Run FedAvg with `DelayHetSubmodular`, `DelayHetSampling`, Random, DivFL, and FLANP on FEMNIST with simulated NYCMesh delays. Measure total wall-clock time to reach 80% accuracy.
2. **Heterogeneity Sensitivity**: Run `DelayHetSampling` on Rotated MNIST with synthetic delays, varying the maximum rotation angle (30°, 45°, 60°). Plot total runtime.
3. **Ablation on Selection Method**: Compare the two proposed schemes on MNIST with Dirichlet α=2 and low latency heterogeneity. Expect `DelayHetSampling` to be competitive due to its ability to explore.

## Open Questions the Paper Calls Out
- **Open Question 1**: Can the proposed client selection schemes be adapted to variance-reduced FL algorithms like SCAFFOLD to further improve convergence speed? *Basis*: The Conclusion states, "Extending our methods to FL algorithms other than FedAvg, like SCAFFOLD [24], is a promising direction for future work." *Why unresolved*: The theoretical derivation currently relies on the convergence properties of FedAvg. *What evidence would resolve it*: A new theoretical derivation of the runtime objective specific to SCAFFOLD's control variates and empirical validation.
- **Open Question 2**: How does the performance of DelayHetSubmodular and DelayHetSampling degrade when client latencies are time-varying or stochastic rather than static? *Basis*: Section 4 states delays are assumed constant, but Figure 1 implies real-world delays fluctuate. *Why unresolved*: The optimization objective minimizes runtime based on fixed τ_i values; dynamic delays could invalidate the optimal selection. *What evidence would resolve it*: Empirical tests using delay distributions that change stochastically between rounds.
- **Open Question 3**: Can the O(m^2 d^2) computational complexity of estimating pairwise heterogeneity be reduced to scale for cross-device FL with millions of clients? *Basis*: Section 7.5 notes the complexity is manageable for m ≤ 100 but becomes prohibitive for larger m. *Why unresolved*: Calculating B_ij for all pairs is computationally expensive for large-scale deployments. *What evidence would resolve it*: An approximation method (e.g., locality-sensitive hashing) that preserves optimality while reducing complexity to near-linear time.

## Limitations
- The analysis assumes bounded heterogeneity and makes approximations in the heterogeneity estimation process, which may not hold in all scenarios.
- The computational overhead of the selection algorithms, while manageable, adds to the server's workload and may become significant in extremely large-scale deployments.
- The method relies on feature covariance as a proxy for gradient dissimilarity, which may not capture all aspects of data heterogeneity across different model architectures.

## Confidence
- **High Confidence**: The empirical results demonstrating runtime improvements over baselines are robust and consistent across multiple datasets and heterogeneity levels. The architectural description of the system components and their integration with FedAvg is clear and implementable.
- **Medium Confidence**: The theoretical optimality proofs under bounded heterogeneity assumptions are sound within their stated scope, but their practical applicability depends on the accuracy of the heterogeneity estimation. The specific performance gains are context-dependent and may vary with different hyperparameter choices.
- **Low Confidence**: The generalizability of the feature covariance proxy across different model architectures and data types is uncertain. The paper does not extensively test scenarios with highly dynamic client delays or explore the sensitivity to the warm-up round design.

## Next Checks
1. **Assumption Validation**: Log the estimated heterogeneity values (max_i avg_j B_ij) across all datasets to verify Assumption 3 holds. If violated, test the effect of clipping values or increasing the batch size for covariance estimation.
2. **Solver Overhead Profiling**: Measure the wall-clock time of the MinNorm submodular solver and the scipy 'trust-constr' optimizer per round. Compare this to the client delays to quantify the overhead and determine if it becomes prohibitive for larger client populations.
3. **Architecture Sensitivity**: Replace the last-layer feature covariance with an alternative proxy (e.g., gradient cosine similarity on a small batch) and re-run experiments on one or two datasets to assess the robustness of the selection to the heterogeneity estimation method.