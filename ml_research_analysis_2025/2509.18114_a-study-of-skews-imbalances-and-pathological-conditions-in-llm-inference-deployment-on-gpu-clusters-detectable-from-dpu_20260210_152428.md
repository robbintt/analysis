---
ver: rpa2
title: A Study of Skews, Imbalances, and Pathological Conditions in LLM Inference
  Deployment on GPU Clusters detectable from DPU
arxiv_id: '2509.18114'
source_url: https://arxiv.org/abs/2509.18114
tags:
- inference
- pcie
- kent
- dpus
- memory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a framework for detecting and mitigating load
  imbalance in LLM inference deployments using DPU-assisted telemetry. The core idea
  is to leverage BlueField-3 DPUs to monitor GPU telemetry and inter-node communication
  patterns, enabling real-time detection of skews and pathological conditions.
---

# A Study of Skews, Imbalances, and Pathological Conditions in LLM Inference Deployment on GPU Clusters detectable from DPU

## Quick Facts
- arXiv ID: 2509.18114
- Source URL: https://arxiv.org/abs/2509.18114
- Authors: Javed I. Khan and Henry Uwabor Moye
- Reference count: 0
- One-line primary result: A framework using BlueField-3 DPUs to monitor GPU telemetry and inter-node communication patterns, enabling real-time detection of skews and pathological conditions in LLM inference deployments.

## Executive Summary
This paper proposes a DPU-assisted telemetry framework for detecting and mitigating load imbalance in multi-node tensor-parallel LLM inference. The core insight is that BlueField-3 DPUs can observe network and PCIe-level anomalies invisible to CPU-based monitoring, enabling fine-grained observability into distributed inference workloads. By correlating software signals (request metadata, scheduling queues) with hardware signals (DMA bursts, NIC buffer buildup), the system achieves multi-layer root-cause attribution. The primary deliverable is a comprehensive runbook mapping skew conditions to root causes and mitigation strategies, enabling more precise workload rebalancing and improved latency predictability.

## Method Summary
The method employs a hybrid observability approach combining software telemetry from inference engines (vLLM, TGI, DeepSpeed) with hardware telemetry from BlueField-3 DPUs. The DPU monitors network traffic (packet timestamps, microbursts, retransmits) and PCIe transactions (DMA bursts, doorbell writes) inline with the NIC. Software signals include request arrival, sequence length, decode progress, KV-cache state, and queue depth. These are correlated with DPU-observed hardware signals to attribute skews to specific system components (CPU preprocessing, PCIe congestion, GPU scheduling, or network instability). The framework produces a runbook mapping observable skew conditions to root causes and mitigation strategies.

## Key Results
- DPU can detect three types of imbalances: computational skew, network skew, and PCIe-level skew in LLM inference
- Prefill/decode phase boundaries can be inferred from DMA transaction patterns without application instrumentation
- Hybrid software-hardware signal correlation enables multi-layer root-cause attribution for inference bottlenecks

## Why This Works (Mechanism)

### Mechanism 1: DPU Inline Network Observability
- Claim: DPUs positioned inline with the NIC can detect network-level anomalies invisible to CPU-based monitoring.
- Mechanism: The DPU intercepts all ingress/egress traffic before it reaches the host, enabling sub-microsecond timestamping of packets, queue depth measurement, and burst rate analysis without CPU overhead.
- Core assumption: Network pathologies (microbursts, congestion, retransmits) correlate meaningfully with inference latency degradation.

### Mechanism 2: PCIe-Level Phase Inference via DMA Pattern Recognition
- Claim: DPUs can infer LLM inference phase boundaries (prefill vs. decode) by observing DMA transaction patterns across PCIe.
- Mechanism: Prefill generates large clustered H2D DMA bursts for embeddings; decode produces many small repetitive D2H reads. Correlating these patterns with ingress request flows enables phase-level tracing without application instrumentation.
- Core assumption: DMA burst sizes and timing signatures map reliably to prefill/decode phases across model architectures and batch sizes.

### Mechanism 3: Hybrid Software-Hardware Signal Correlation
- Claim: Combining software telemetry (request metadata, scheduling queues, KV-cache state) with DPU hardware telemetry (PCIe bursts, NIC buffer buildup) enables multi-layer root-cause attribution.
- Mechanism: Software signals identify what request/batch is affected; hardware signals identify where in the system (CPU, PCIe, network) the bottleneck occurs. Correlation distinguishes CPU tokenization delays from PCIe saturation from network congestion.
- Core assumption: Timestamps and identifiers can be aligned across software and hardware telemetry streams with sufficient precision.

## Foundational Learning

- Concept: Tensor Parallelism (TP) and Pipeline Parallelism (PP) in distributed LLM inference
  - Why needed here: Skews manifest differently under TP (stragglers stall collectives) vs. PP (stage bubbles propagate downstream). Understanding these patterns is prerequisite to interpreting runbook signals.
  - Quick check question: Can you explain why a TP straggler affects all ranks in an all-reduce, while a PP stage stall primarily affects downstream stages?

- Concept: KV-cache management and PagedAttention
  - Why needed here: Table-2(b) and runbooks reference KV-cache occupancy as a key software signal; PCIe bottlenecks often stem from KV-cache transfers during decode.
  - Quick check question: What triggers a KV-cache page eviction in vLLM, and what PCIe signal would indicate contention from this mechanism?

- Concept: RDMA/NCCL collectives and RoCE congestion control
  - Why needed here: East-West runbook (Table-3c) relies on detecting head-of-line blocking, credit starvation, and PFC-related issues in RDMA flows.
  - Quick check question: How would you distinguish credit starvation from network congestion based on DPU-observable packet timing patterns?

## Architecture Onboarding

- Component map: Inference Engine (vLLM/TGI/DeepSpeed) -> GPU Cluster with TP/PP -> BlueField-3 DPU (inline with NIC) -> Telemetry Correlation Layer -> Feedback Controller
- Critical path:
  1. Request arrives → Inference engine logs metadata (SW signal)
  2. Prefill begins → Large H2D DMA bursts → DPU captures PCIe pattern (HW signal)
  3. TP collective issued → NCCL traffic traverses NIC → DPU timestamps inter-node packets
  4. Decode phase → Small D2H DMAs + egress token streams → DPU detects phase boundary
  5. Skew detected → Correlate SW+HW signals → Map to runbook → Trigger mitigation

- Design tradeoffs:
  - DPU polling interval vs. detection latency: Finer granularity improves skew detection but increases DPU CPU load
  - Telemetry export overhead vs. observability precision: Sub-microsecond timestamps require high-bandwidth telemetry channels
  - Closed-loop vs. advisory feedback: Automatic rebalancing reduces operator burden but risks oscillation if root-cause attribution is wrong

- Failure signatures:
  - TP straggler: Wide spread in collective burst arrivals; one node shows thin/irregular PCIe DMA pattern
  - PCIe saturation: Sustained near-peak throughput; burstiness in inter-node traffic waves
  - Network congestion: Periodic latency spikes across many links; retransmit storms visible in DPU counters
  - Early-stop skew: Some nodes stop D2H transfers mid-iteration while peers continue; collectives hang waiting

- First 3 experiments:
  1. Baseline telemetry capture: Deploy BlueField-3 DPU in passive monitoring mode; capture PCIe DMA and network packet timestamps during single-node LLM inference; validate prefill/decode phase detection against ground truth from CUDA events
  2. Synthetic skew injection: In a 2-node TP setup, artificially delay one GPU's compute (e.g., via CUDA sleep); verify DPU detects TP straggler pattern (wide collective arrival spread) and maps to correct runbook entry
  3. Mitigation validation: Configure feedback controller to trigger batch-size reduction on PCIe saturation detection; measure impact on token latency and GPU idle time under high-concurrency workload

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the latency overhead introduced by DPU-based telemetry collection on the inference critical path?
- Basis in paper: [inferred] The paper claims "lightweight, real-time observability" (Section 1) but provides no measurements of the monitoring overhead itself.
- Why unresolved: No experimental validation is presented; the framework remains a design proposal without quantification of DPU processing costs.
- What evidence would resolve it: End-to-end latency measurements comparing inference with and without DPU telemetry enabled, across varying request rates.

### Open Question 2
- Question: How can software-level signals (e.g., KV-cache occupancy, queue depth) be precisely time-correlated with DPU-observed hardware signals (e.g., DMA bursts, NIC buffer buildup) for real-time decision making?
- Basis in paper: [explicit] Section 5 states the system must correlate "software signals... with hardware signals" to achieve "multi-layer observability," but no synchronization mechanism is described.
- Why unresolved: Clock domains differ between host software and DPU hardware; synchronization jitter could obscure causal relationships.
- What evidence would resolve it: A demonstrated correlation algorithm with sub-millisecond alignment accuracy and its impact on mitigation latency.

### Open Question 3
- Question: What inference accuracy or performance penalty arises when automated mitigation actions (e.g., dynamic remapping, early eviction) are triggered based on DPU-detected skews?
- Basis in paper: [inferred] The runbook (Tables 3a–c) prescribes mitigation directives, but none are validated against actual LLM inference workloads.
- Why unresolved: Aggressive load rebalancing may disrupt KV-cache locality or introduce scheduling churn.
- What evidence would resolve it: Benchmark results showing throughput and latency before/after automated mitigation across representative inference scenarios.

## Limitations
- The framework cannot directly observe intra-GPU computation and NVLink traffic, limiting root-cause attribution in GPU-bound scenarios
- DMA pattern recognition for phase inference assumes consistent burst signatures across models and batch sizes, which may not hold under CUDA Graphs or kernel fusion
- The runbook mapping is comprehensive but relies on controlled skew injection rather than real-world adversarial workloads

## Confidence
- High confidence: DPU inline network observability for microburst detection and queue depth measurement (well-established DPU capabilities, clear correlation with network congestion)
- Medium confidence: PCIe DMA pattern recognition for prefill/decode phase inference (mechanistically sound but dependent on stable burst signatures across model variants)
- Medium confidence: Hybrid SW-HW correlation for multi-layer root-cause attribution (valid in principle but alignment precision and cardinality handling are open questions)

## Next Checks
1. Validate DMA phase inference robustness across different model architectures (LLaMA, Mistral, Falcon) and batch sizes by comparing DPU-inferred phases against CUDA event timestamps under varying CUDA Graph configurations
2. Test DPU telemetry overhead and alignment accuracy under high-concurrency multi-client workloads using a traffic generator that varies request arrival patterns, sequence lengths, and inter-arrival times
3. Evaluate runbook accuracy in a production-like multi-node deployment with adversarial skew injection (e.g., stage-level compute delays, network microbursts, PCIe congestion) and measure false positive/negative rates in skew classification