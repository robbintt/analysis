---
ver: rpa2
title: 'CXRAgent: Director-Orchestrated Multi-Stage Reasoning for Chest X-Ray Interpretation'
arxiv_id: '2510.21324'
source_url: https://arxiv.org/abs/2510.21324
tags:
- diagnostic
- reasoning
- tools
- medical
- evidence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CXRAgent addresses the challenge of automated chest X-ray interpretation
  by introducing a director-orchestrated, multi-stage reasoning agent. It integrates
  specialized diagnostic tools, an Evidence-driven Validator for visual-grounded reliability
  assessment, and adaptive team-based collaborative decision-making.
---

# CXRAgent: Director-Orchestrated Multi-Stage Reasoning for Chest X-Ray Interpretation

## Quick Facts
- arXiv ID: 2510.21324
- Source URL: https://arxiv.org/abs/2510.21324
- Authors: Jinhui Lou; Yan Yang; Zhou Yu; Zhenqi Fu; Weidong Han; Qingming Huang; Jun Yu
- Reference count: 40
- Primary result: Achieves 67.0% overall accuracy on CheXbench, outperforming existing models

## Executive Summary
CXRAgent addresses the challenge of automated chest X-ray interpretation by introducing a director-orchestrated, multi-stage reasoning agent. It integrates specialized diagnostic tools, an Evidence-driven Validator for visual-grounded reliability assessment, and adaptive team-based collaborative decision-making. The framework demonstrates strong performance across comprehensive benchmarks including CheXbench (67.0% overall accuracy), Medical-CXR-VQA (75.6%), and MIMIC-CXR report generation (RaTEScore 0.513). The ablation study confirms that combining tools, visual evidence validation, and team collaboration yields optimal performance, demonstrating strong adaptability to varying clinical tasks and complexities.

## Method Summary
CXRAgent employs a three-stage pipeline: (1) ReAct-based iterative tool invocation with Evidence-driven Validator (EDV) for visual grounding, (2) Diagnostic Planning selecting from four collaboration modes (Skip/Relay/Dispatch/Probe), and (3) Team-based collaborative decision-making. The director LLM (GPT-4o or Qwen-VL-Max) orchestrates tool invocation from a pool of specialized CXR models (MedGemma-4B, LLaVA-Rad, CheXagent, LLaVA-Med, MAIRA-2, MedVLM-R1), validates outputs through visual evidence grounding, and coordinates expert teams for complex cases. The system is deployed on 2x NVIDIA RTX A6000 GPUs and demonstrates improved accuracy through structured reasoning and collaborative diagnosis.

## Key Results
- Achieves 67.0% overall accuracy on CheXbench benchmark, outperforming existing models
- Demonstrates 75.6% accuracy on Medical-CXR-VQA for presence/abnormality questions
- Generates chest X-ray reports with RaTEScore 0.513 on MIMIC-CXR dataset
- Ablation confirms combined approach (tools + EDV + team) outperforms individual components

## Why This Works (Mechanism)

### Mechanism 1: Evidence-driven Validator (EDV) for Tool Output Reliability Assessment
EDV improves diagnostic reliability by grounding tool outputs in visual evidence rather than treating all tool outputs as equally trustworthy. For each tool output, EDV generates four structured components: reformatted conclusion, supportive visual evidence, refuting visual evidence, and qualitative confidence assessment. This bidirectional validation identifies unsupported claims before downstream reasoning. Evidence shows EDV alone improves CheXbench accuracy from 60.3% (tools only) to 65.4% (tools+EDV) with Qwen director. However, if tool outputs contain claims that cannot be visually verified (e.g., historical clinical context), EDV's grounding mechanism may incorrectly downgrade valid assertions.

### Mechanism 2: Adaptive Strategy Selection Based on Task Complexity
The director LLM dynamically selects among four collaboration modes (Skip, Relay, Dispatch, Probe) to improve efficiency for simple cases while enabling thorough analysis for complex cases. The director analyzes query complexity and intermediate findings to select appropriate strategy: Skip for straightforward findings, Relay for sequential refinement, Dispatch for parallel subtask analysis, Probe for ambiguous cases requiring targeted investigation. Evidence shows the full model (tools + EDV + team) achieves 67.0% vs. 65.4% (tools + EDV only), confirming collaboration adds value. Incorrect complexity assessment leads to under-processing (missed findings) or over-processing (inefficiency).

### Mechanism 3: Multi-Agent Team Collaboration with Role Specialization
Simulating a multidisciplinary team through specialized agent roles improves diagnostic accuracy by enabling focused expertise application. Team recruitment formulates agents as tuples (agent_id, role, mission). In Dispatch mode, parallel experts analyze distinct anatomical regions; in Probe mode, experts answer targeted questions; Relay chains sequential refinements. The full model achieves 67.0% vs. 65.4% (tools + EDV only), confirming collaboration adds value. Team coordination overhead may exceed diagnostic benefit for simple cases, or synthesized outputs may introduce inconsistencies.

## Foundational Learning

- **ReAct reasoning pattern (Thought-Action-Observation cycles)**
  - Why needed here: Tool invocation uses iterative ReAct loops to decide which tools to call, execute them, and reflect on results before proceeding
  - Quick check question: Given a CXR with suspected consolidation, can you trace how ReAct would iteratively invoke classification then localization tools?

- **Visual grounding in multimodal models**
  - Why needed here: EDV's core function requires the model to identify which image regions support or refute textual diagnostic claims
  - Quick check question: If a tool reports "right lower lobe opacity," can you identify what visual features the EDV should look for as supportive evidence?

- **Multi-agent orchestration patterns**
  - Why needed here: Understanding when to use sequential (Relay) vs. parallel (Dispatch) vs. interrogative (Probe) collaboration is essential for diagnostic planning
  - Quick check question: For a CXR with potential cardiac AND skeletal abnormalities, which collaboration strategy is most appropriate and why?

## Architecture Onboarding

- **Component map:**
  Director LLM (GPT-4o/Qwen-VL-Max) -> Tool Pool (MedGemma-4B, LLaVA-Rad, CheXagent, LLaVA-Med, MAIRA-2, MedVLM-R1) -> Evidence-driven Validator -> Team Recruitment -> Memory Log -> Final Synthesis

- **Critical path:**
  1. User query + CXR image → Director
  2. ReAct loop: Thought → Tool Invocation → Observation → EDV validation → Log update
  3. Repeat until sufficient information gathered
  4. Diagnostic Planning: Select strategy s ∈ {Skip, Relay, Dispatch, Probe}
  5. Team Recruitment: Generate Team = {(agent_i, role_i, mission_i)}
  6. Execute collaboration per strategy
  7. Final synthesis: Diagnose(q, I, s, M, TeamOutput)

- **Design tradeoffs:**
  - Director choice: GPT-4o vs. Qwen-VL-Max—Table I shows GPT better at SDI-MIMIC (39.3% vs. 42.9% with Qwen), but Qwen better overall (67.0% vs. 66.3%)
  - Team size (k): Larger teams increase compute but provide more diverse analysis; paper does not specify fixed k
  - Tool redundancy: Multiple overlapping tools increases conflict potential but EDV mitigates

- **Failure signatures:**
  - Tool conflict without resolution: Without EDV, Table IV shows tools alone can degrade performance (VC accuracy drops from 86.6% to 83.6%)
  - Over-sensitive tools: Case 1 shows MedGemma produces unsupported findings like "right upper lobe hyperinflation"—EDV flags as "not well-supported"
  - Strategy mismatch: Simple cases using Probe waste compute; complex cases using Skip miss findings

- **First 3 experiments:**
  1. EDV validation on synthetic conflicts: Create paired tool outputs with known conflicts, verify EDV correctly identifies supporting/refuting evidence and assigns appropriate confidence
  2. Strategy ablation by case complexity: Manually classify test cases as simple/medium/complex, run with forced Skip/Relay/Dispatch/Probe strategies, measure accuracy vs. latency tradeoffs
  3. Team composition sensitivity: For Dispatch strategy, vary team size (k=1, 2, 4, 8) and role granularity, measure diagnostic accuracy on multi-disease cases

## Open Questions the Paper Calls Out

- **Adaptation to 3D volumetric data**: Can CXRAgent maintain performance when adapted to 3D volumetric medical imaging data (CT/MRI), or does increased spatial complexity disrupt the Evidence-driven Validator?
- **Computational efficiency**: What is the computational latency and cost overhead introduced by the multi-stage reasoning process compared to single-pass foundation models, and does it permit real-time clinical application?
- **Smaller director models**: How robust is the system if the Director LLM is replaced with a smaller, open-source model suitable for on-premise hospital deployment?
- **EDV robustness to hallucinations**: To what extent does EDV prevent "hallucination" when a specialized tool provides a plausible but incorrect diagnosis supported by misleading visual features?

## Limitations

- The dynamic team recruitment mechanism lacks complete specification regarding how agent roles and missions are determined from case complexity
- The effectiveness of EDV depends on the multimodal LLM's ability to accurately assess visual support, which may degrade for subtle or ambiguous findings
- The scalability and efficiency of multi-agent collaboration for routine clinical workflows remains incompletely validated

## Confidence

- **High confidence**: The ablation results showing improved performance with tools+EDV+team versus tools alone (67.0% vs. 65.4% on CheXbench)
- **Medium confidence**: The visual grounding mechanism's reliability across diverse clinical scenarios
- **Medium confidence**: The scalability and efficiency of multi-agent collaboration for routine clinical workflows

## Next Checks

1. **EDV robustness testing**: Create synthetic tool outputs with controlled visual evidence patterns to validate EDV's accuracy in identifying supporting/refuting evidence across varying confidence levels
2. **Strategy selection validation**: Manually classify test cases by complexity and verify that the diagnostic planning module correctly maps cases to appropriate collaboration strategies
3. **Clinical workflow integration**: Evaluate CXRAgent's latency and resource requirements in a simulated clinical environment to assess practical deployment feasibility