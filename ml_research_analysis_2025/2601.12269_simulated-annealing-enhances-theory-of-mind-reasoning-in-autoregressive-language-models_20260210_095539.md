---
ver: rpa2
title: Simulated Annealing Enhances Theory-of-Mind Reasoning in Autoregressive Language
  Models
arxiv_id: '2601.12269'
source_url: https://arxiv.org/abs/2601.12269
tags:
- language
- belief
- autoregressive
- sampling
- annealing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The authors propose using simulated annealing\u2014a temperature-controlled\
  \ MCMC sampling method\u2014to improve theory-of-mind (ToM) reasoning in autoregressive\
  \ language models. Unlike standard decoding or Chain-of-Thought prompting, this\
  \ approach samples from a power-sharpened sequence-level distribution, enabling\
  \ exploration of globally coherent completions that align with underlying latent\
  \ belief states."
---

# Simulated Annealing Enhances Theory-of-Mind Reasoning in Autoregressive Language Models

## Quick Facts
- arXiv ID: 2601.12269
- Source URL: https://arxiv.org/abs/2601.12269
- Authors: Xucong Hu; Jian-Qiao Zhu
- Reference count: 10
- Primary result: Simulated annealing improves theory-of-mind accuracy by up to 16 percentage points on false-belief tasks in small language models

## Executive Summary
This paper proposes using simulated annealing—a temperature-controlled MCMC sampling method—to improve theory-of-mind (ToM) reasoning in autoregressive language models. Unlike standard decoding or Chain-of-Thought prompting, this approach samples from a power-sharpened sequence-level distribution, enabling exploration of globally coherent completions that align with underlying latent belief states. Tested on the BigToM benchmark using three small language models (Phi-3.5-Mini-Instruct, LLaMA-3.2-3B-Instruct, and Qwen3-1.7B), the method achieved accuracy improvements of up to 16 percentage points on false-belief tasks compared to direct decoding.

## Method Summary
The authors implement simulated annealing over MCMC sampling to optimize sequence-level coherence in autoregressive language models. The method uses an exponential temperature schedule (τ_start=0.90 to τ_end=0.25) over K=10 iterations, with block resampling (num_blocks=16) to efficiently explore the sequence space. At each step, a random position is selected, the suffix is resampled, and proposals are accepted/rejected via Metropolis-Hastings using a power-sharpened distribution p^α. The approach was tested on BigToM benchmark backward-inference tasks with three small language models, comparing against greedy decoding, Chain-of-Thought prompting, and fixed-temperature power sampling baselines.

## Key Results
- Simulated annealing improved ToM accuracy by up to 16 percentage points on false-belief tasks
- The method showed strongest gains on harder false-belief conditions compared to true-belief tasks
- Generated reasoning demonstrated structured belief hypothesis testing rather than simple reality-belief conflation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Sequence-level distortion via power sampling prioritizes global coherence over local plausibility, addressing the inherent myopia of autoregressive decoding.
- **Mechanism:** By sampling from a sharpened joint distribution p(x)^α (where α > 1) rather than the product of conditionals, the system up-weights sequences that maintain high probability across the entire causal chain (e.g., consistent belief states) rather than just high-probability next tokens. This corrects the tendency of standard autoregressive models to drift into "locally typical" but globally inconsistent text.
- **Core assumption:** The pretrained model possesses latent representations of consistent world states (competence), but the standard decoding objective (next-token prediction) fails to elicit them (performance gap).
- **Evidence anchors:**
  - [Methods]: Equation 4 demonstrates that naive local sharpening ignores the marginal future Σp(x₁|x₀)^α, whereas sequence-level sharpening accounts for "how many high-probability future continuations a given token admits."
  - [Corpus]: The corpus context (e.g., "Reasoning Promotes Robustness in Theory of Mind Tasks") supports the broader difficulty of ToM in LLMs but does not offer direct evidence for this specific power-sampling mechanism.
- **Break condition:** If the base model lacks the latent capability to represent the state (e.g., deeply nested beliefs), sharpening the distribution will only amplify noise or hallucinations rather than correct reasoning.

### Mechanism 2
- **Claim:** Simulated annealing (temperature decay) converts the MCMC sampler into an optimizer, allowing escape from local probability traps.
- **Mechanism:** The system initializes sampling at high temperature (τ=0.9) to explore the broad probability landscape of reasoning paths. It then decays temperature (τ → 0.25) to "anneal" the distribution, concentrating mass onto the highest-probability mode (the most coherent solution). This dynamic allows the model to accept "worse" intermediate steps to eventually find a globally better reasoning chain, unlike greedy search.
- **Core assumption:** Correct reasoning paths correspond to high-probability modes in the sequence distribution that are separated by low-probability "valleys" which greedy decoding cannot cross.
- **Evidence anchors:**
  - [Abstract]: "Incorporating annealing... substantially improves ToM performance over fixed-temperature power sampling."
  - [Results]: Figure 3 shows Simulated Annealing outperforming both high-temp and low-temp fixed power sampling, suggesting the schedule (movement) is crucial.
- **Break condition:** If the temperature schedule cools too fast, the chain may quench into a sub-optimal local mode (incorrect reasoning); if too slow, computational cost becomes prohibitive without accuracy gain.

### Mechanism 3
- **Claim:** Block-resampling proposals enable efficient traversal of the sequence space without evaluating the entire context.
- **Mechanism:** Instead of modifying one token at a time, the algorithm selects a random split point t and resamples the entire suffix using the base model's autoregressive capabilities. This leverages the LLM's own predictive power to propose syntactically and semantically coherent "chunks" of reasoning for the Metropolis-Hastings acceptance step.
- **Core assumption:** The base model's conditional generation is sufficiently high-quality that a random suffix resampling has a non-trivial probability of landing on a coherent alternative.
- **Evidence anchors:**
  - [Methods]: "We exploit the structure of autoregressive generation to accelerate MCMC by generating sequences block by block... number of blocks = 16."
  - [Corpus]: Neighbor papers (e.g., "DEL-ToM") discuss inference-time scaling but do not verify this specific block-resampling efficiency.
- **Break condition:** If the context window is very long or the "critical" reasoning step is early in the sequence, random block resampling may inefficiently propose changes that ruin prefix consistency or fail to correct early errors.

## Foundational Learning

- **Concept:** Markov Chain Monte Carlo (MCMC) & Metropolis-Hastings
  - **Why needed here:** The paper relies on MCMC to sample from an intractable sequence-level distribution. You cannot understand the proposal/acceptance loop or why "burn-in" and temperature matter without this basis.
  - **Quick check question:** If a proposed sequence has lower probability than the current sequence, is it always rejected in this system? (Check: No, depends on the acceptance ratio and temperature).

- **Concept:** Theory of Mind (False Belief Tasks)
  - **Why needed here:** The evaluation uses the BigToM benchmark, specifically "False Belief" scenarios. Understanding that the model must represent "Agent believes X" (where X is false) distinct from "World is Y" is critical to diagnosing the "Reality-Belief Conflation" failure mode.
  - **Quick check question:** Why is "Backward Inference" (inferring belief from action) harder for standard LLMs than "Forward Inference" (predicting action from belief)?

- **Concept:** Temperature (τ) vs. Sharpness (α)
  - **Why needed here:** The core manipulation involves an exponential temperature schedule (τ = 1/α). Confusing sampling temperature (randomness) with distribution sharpening (peakiness) will lead to misunderstanding the optimization dynamic.
  - **Quick check question:** In this paper, does lowering the temperature make the model more "creative" or more "deterministic/peaky"?

## Architecture Onboarding

- **Component map:** Base LLM (Frozen) -> Proposal Engine (Random suffix resampling) -> Scorer (Power-sharpened probability calculation) -> Annealer (Temperature schedule control) -> Acceptor (Metropolis-Hastings logic)

- **Critical path:** The acceptance step is the bottleneck. It requires forward passes through the Base LLM to compute log-probabilities of the entire proposed sequence to evaluate p(x)^α. You are trading inference latency for reasoning accuracy.

- **Design tradeoffs:**
  - **Compute vs. Consistency:** Simulated annealing requires multiple forward passes (K=10 steps minimum, multiple proposals per step) compared to 1 pass for greedy.
  - **Exploration vs. Stability:** High start temp (τ=0.9) is required to find the mode, but risks drifting off-distribution; low end temp (τ=0.25) is required to commit, but risks "representational collapse" if cooled too fast.

- **Failure signatures:**
  - **Reality-Belief Conflation:** The model outputs what is true rather than what the agent believes is true (common in fixed low-temp sampling).
  - **Representational Collapse:** The model contradicts its own premises within a generated block (common in greedy decoding).
  - **Looping/Non-convergence:** The MCMC chain oscillates between two distinct belief states without settling (indicates temperature is too high or schedule is too short).

- **First 3 experiments:**
  1. **Sanity Check (Determinism):** Run a simple True Belief prompt through Direct Decoding vs. Low-Temp Power Sampling (τ=0.25). Verify that Power Sampling improves coherence.
  2. **Ablation (Schedule):** Compare Fixed High (τ=0.9) vs. Fixed Low (τ=0.25) vs. Annealing on a False Belief task. Confirm that only Annealing bridges the gap.
  3. **Efficiency Profiling:** Measure the latency multiplier of the MCMC loop. Is the accuracy gain on the BigToM benchmark worth the 10x+ inference cost for your application?

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does sequence-level distortion with simulated annealing generalize to other reasoning domains beyond Theory-of-Mind, such as moral reasoning, strategic interaction, multi-agent coordination, or long-horizon planning?
- **Basis in paper:** [explicit] "Finally, it remains an open question whether the benefits of sequence-level distortion generalize to other reasoning domains, such as moral reasoning, strategic interaction, multi-agent coordination, or long-horizon planning."
- **Why unresolved:** The study only evaluated ToM tasks using the BigToM benchmark; other reasoning domains involve different latent structures and consistency requirements.
- **What evidence would resolve it:** Applying the simulated annealing method to established benchmarks in moral reasoning, game-theoretic tasks, and multi-agent coordination, comparing against direct decoding and CoT baselines.

### Open Question 2
- **Question:** Can more efficient proposal mechanisms or adaptive stopping criteria be developed to reduce the computational cost of simulated annealing at test time?
- **Basis in paper:** [explicit] "Future work should investigate more efficient proposal mechanisms, adaptive stopping criteria, or hybrid approaches that selectively apply sequence-level optimization."
- **Why unresolved:** Current implementation incurs significant inference-time cost compared to standard autoregressive decoding, raising scalability concerns for longer sequences and real-time applications.
- **What evidence would resolve it:** Development and empirical comparison of alternative proposal distributions (e.g., guided by uncertainty estimates) and adaptive stopping rules that maintain accuracy while reducing MCMC iterations.

### Open Question 3
- **Question:** Do other optimization-based decoding methods beyond simulated annealing exist that more effectively traverse the sequence-level probabilistic landscape?
- **Basis in paper:** [inferred] The authors state "simulated annealing is an effective strategy... but it is unlikely to be unique" and suggest "a broader family of optimization-based decoding methods."
- **Why unresolved:** Only simulated annealing and fixed-temperature power sampling were tested; the space of possible sequence-level optimization algorithms remains unexplored.
- **What evidence would resolve it:** Systematic comparison of alternative MCMC variants (e.g., parallel tempering, Hamiltonian Monte Carlo) or gradient-based sequence optimization on the same ToM benchmarks.

## Limitations
- **Computational efficiency:** The MCMC-based approach requires 10+ forward passes per generation, creating 10-100x inference latency compared to greedy decoding.
- **Small model focus:** Results are limited to 3-4B parameter models; effectiveness on larger models with stronger native ToM capabilities remains untested.
- **Task specificity:** Method validated only on BigToM false-belief tasks; generalization to broader ToM domains requires further validation.

## Confidence
- **High Confidence (4/4 evidence anchors):**
  - Power sampling enables exploration of globally coherent sequences rather than locally typical ones
  - Simulated annealing schedule outperforms fixed-temperature sampling
  - Method shows improved accuracy on false-belief tasks compared to baselines
  - Generated reasoning exhibits structured belief hypothesis testing

- **Medium Confidence (2-3 evidence anchors):**
  - The 16-percentage-point improvement is specific to the tested model sizes and tasks
  - Block resampling efficiency claims are supported but not extensively validated
  - Representational collapse failure mode is observed but not systematically quantified

- **Low Confidence (0-1 evidence anchor):**
  - Claims about superiority over Chain-of-Thought prompting are based on single comparison
  - Scalability to production workloads remains speculative
  - Generalizability to non-BigToM ToM tasks is untested

## Next Checks
1. **Efficiency Benchmark:** Measure actual inference latency multiplier across different hardware (GPU/CPU) and batch sizes to quantify real-world deployment costs.

2. **Large Model Validation:** Apply the method to 70B+ parameter models to determine if smaller models benefit more from inference-time optimization than larger, more capable models.

3. **Cross-Benchmark Testing:** Evaluate performance on alternative ToM benchmarks (e.g., SocialIQA, ToMi, Mind-OP) to assess generalization beyond BigToM's specific false-belief paradigm.