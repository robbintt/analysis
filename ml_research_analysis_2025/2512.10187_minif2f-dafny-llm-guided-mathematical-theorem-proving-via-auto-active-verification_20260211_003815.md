---
ver: rpa2
title: 'MINIF2F-DAFNY: LLM-Guided Mathematical Theorem Proving via Auto-Active Verification'
arxiv_id: '2512.10187'
source_url: https://arxiv.org/abs/2512.10187
tags:
- proof
- dafny
- verification
- mathematical
- assert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MINIF2F-DAFNY, the first translation of the
  mathematical benchmark miniF2F to the auto-active verifier Dafny. The translation
  enables evaluation of AI-assisted theorem proving in a verification paradigm where
  SMT solvers handle low-level reasoning while LLMs provide high-level proof guidance.
---

# MINIF2F-DAFNY: LLM-Guided Mathematical Theorem Proving via Auto-Active Verification

## Quick Facts
- arXiv ID: 2512.10187
- Source URL: https://arxiv.org/abs/2512.10187
- Reference count: 33
- Baseline Dafny solves 39-44% of miniF2F problems with empty proofs; Claude Sonnet 4.5 achieves 55.7% success rate with LLM guidance

## Executive Summary
This paper introduces MINIF2F-DAFNY, the first translation of the miniF2F mathematical benchmark to the Dafny auto-active verifier. The work enables evaluation of AI-assisted theorem proving in a verification paradigm where SMT solvers handle low-level reasoning while LLMs provide high-level proof guidance. Baseline results show Dafny's automation alone solves 39-44% of problems with empty proofs, outperforming Lean's automation on this benchmark. Evaluating 7 off-the-shelf LLMs on providing proof hints, Claude Sonnet 4.5 achieves 55.7% success rate on remaining problems. These results demonstrate effective division of labor: automated solvers handle routine deductions while LLMs focus on proof structure and strategies. The work opens new directions for accessible formal verification of mathematically complex software.

## Method Summary
The paper translates 488 miniF2F Lean problems into Dafny, creating the MINIF2F-DAFNY benchmark with supporting axiomatized libraries (definitions.dfy: 283 lines, 81 definitions; library.dfy: 938 lines, 174 lemmas). The evaluation uses off-the-shelf LLMs via AWS Bedrock API with system prompts and iterative refinement (N=4 trajectories, E=3 correction iterations, temperature T=0.5). The verification pipeline checks requires clauses match exactly and ensures clauses are supersets. Dafny compiles to Boogie IR and uses Z3 SMT solver with 30-second timeout. The approach tests whether general-purpose LLMs can effectively guide Dafny's SMT automation for mathematical theorem proving.

## Key Results
- Dafny baseline automation solves 38.9% (test) and 43.4% (validation) of problems with empty proofs
- Claude Sonnet 4.5 achieves 55.7% Pass@4 success rate on test set with LLM guidance
- Performance gap between Dafny and Lean suggests miniF2F favors Dafny's automated reasoning
- 7 evaluated LLMs show varying performance, with GPT-4 showing promise but facing API limitations

## Why This Works (Mechanism)
The approach succeeds by exploiting Dafny's auto-active verification paradigm, which divides proof tasks between SMT automation and high-level guidance. Dafny's weakest precondition calculus generates verification conditions that Z3 can discharge automatically for routine mathematical steps, while LLMs focus on proof structure through ghost code (assertions, calc statements, lemma invocations). This division allows LLMs to avoid generating low-level proof terms while still providing effective guidance. The axiomatized library approach keeps the problem focused on proof synthesis rather than library retrieval, and iterative refinement enables correction of initial LLM errors.

## Foundational Learning
- **Concept: Weakest Precondition Calculus & Verification Conditions**
  - Why needed here: This is the core logic by which Dafny transforms code/specs into a solvable formula. Understanding it is essential for debugging why a proof fails.
  - Quick check question: Given a postcondition `Q` and a statement `s`, what does `wp(s, Q)` represent?

- **Concept: Ghost Code & Proof Hints in Auto-Active Verification**
  - Why needed here: LLMs do not generate full proof terms; they generate ghost code (e.g., `calc`, `assert`) to guide Z3. Knowing these idioms is critical for prompt design and error analysis.
  - Quick check question: What is the purpose of a `calc` statement in Dafny, and how does it differ from an `assert`?

- **Concept: SMT Solver Capabilities & Brittleness**
  - Why needed here: The pipeline's success hinges on Z3's ability to discharge subgoals. Its reasoning is powerful but opaque and sensitive to formulation, leading to the "verification brittleness" error mode.
  - Quick check question: Why might reordering a sequence of `assert` statements cause a previously verified proof to fail?

## Architecture Onboarding
- **Component map:** MINIF2F-DAFNY benchmark -> definitions.dfy + library.dfy -> LLM proof synthesis -> Dafny/Z3 verification -> (if fail) Error feedback -> LLM refinement -> Validation
- **Critical path:** Problem + Context → LLM generates proof hints → Dafny/Z3 verifies → (if fail) Error feedback → LLM refines → Validation
- **Design tradeoffs:** Axiomatized vs. Proven Library (compact set of axioms to focus on proof synthesis vs. comprehensive proven facts), Off-the-shelf vs. Specialized Models (accessibility vs. performance), SMT Automation vs. Explicit Proof Terms (automation on routine steps vs. transparent proof states)
- **Failure signatures:** Verification brittleness (minor syntactic reordering causes failure), LLM syntax confusion (generates Lean tactics or misuses Dafny idioms), Missing theory (requires facts absent from library)
- **First 3 experiments:** Baseline Automation Test (empty proofs measure automation floor), Single-Model Guidance Test (Claude Sonnet 4.5 raw capability with N=1, E=0), Iterative Refinement Test (full loop N=4, E=3 analyze error trajectories)

## Open Questions the Paper Calls Out
- Can Dafny-specific fine-tuning teach LLMs to better predict Z3's capacity to automatically discharge subgoals?
- Can Dafny-specific agentic frameworks significantly outperform single-pass off-the-shelf LLMs on MINIF2F-DAFNY?
- How does the completeness of the mathematical library affect the division of labor between SMT automation and LLM reasoning?

## Limitations
- The axiomatized library approach limits completeness and requires trusting axioms rather than proven facts
- Off-the-shelf LLMs struggle with predicting Z3's automation capacity, often producing unnecessary proof steps
- Performance depends critically on prompt engineering and is sensitive to minor formulation changes (verification brittleness)

## Confidence
- High confidence: Translation methodology and baseline automation results are reproducible
- Medium confidence: LLM guidance results depend on correct implementation of validation and iterative refinement
- Medium confidence: Division of labor claims supported but need more detailed error analysis

## Next Checks
1. Implement and verify the requires/ensures checking logic independently to ensure correct specification validation
2. Manually examine verification conditions and error messages for failed proofs to classify failure modes
3. Test LLM evaluation sensitivity to temperature settings (0.1, 0.5, 1.0) on a subset of problems