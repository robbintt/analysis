---
ver: rpa2
title: 'Policy to Assist Iteratively Local Segmentation: Optimising Modality and Location
  Selection for Prostate Cancer Localisation'
arxiv_id: '2508.03953'
source_url: https://arxiv.org/abs/2508.03953
tags:
- segmentation
- policy
- image
- cancer
- modality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes a reinforcement learning-based system to assist
  in prostate cancer segmentation by iteratively selecting optimal imaging modalities
  and local image regions for review. The approach trains a policy network that guides
  a segmentation model to inspect portions of multiparametric MRI scans in a manner
  similar to radiologists, aiming to maximize segmentation accuracy.
---

# Policy to Assist Iteratively Local Segmentation: Optimising Modality and Location Selection for Prostate Cancer Localisation

## Quick Facts
- arXiv ID: 2508.03953
- Source URL: https://arxiv.org/abs/2508.03953
- Reference count: 19
- One-line primary result: RL-based system that iteratively selects optimal imaging modalities and local image regions to improve prostate cancer segmentation accuracy compared to standard models.

## Executive Summary
This work proposes a reinforcement learning-based system to assist in prostate cancer segmentation by iteratively selecting optimal imaging modalities and local image regions for review. The approach trains a policy network that guides a segmentation model to inspect portions of multiparametric MRI scans in a manner similar to radiologists, aiming to maximize segmentation accuracy. During training, the policy network selects portions and modalities, which are then processed by a pre-trained segmentation network to generate local segmentations. This iterative process continues until all cancerous regions are localized. Validation on a dataset of 1325 multiparametric MRI images demonstrates improved segmentation accuracy compared to standard models.

## Method Summary
The method employs reinforcement learning to iteratively improve prostate cancer segmentation by selecting optimal imaging modalities (T2-weighted, diffusion-weighted, or both) and anatomical portions of the scan. A policy network learns to make these selections based on the current segmentation state, while a pre-trained SwinUNETR-v2 segmentation network acts as a "simulated radiologist" environment. The policy selects a specific portion and modality, the segmentation network processes only this local input, and the resulting local segmentation updates the global mask. The process iterates until convergence, with rewards based on improvements in Dice score. The system was validated on 1325 multiparametric MRI volumes.

## Key Results
- Improved segmentation accuracy compared to standard models on a dataset of 1325 multiparametric MRI images
- Trained agent developed its own optimal strategy, sometimes deviating from standard radiologist guidelines like PI-RADS
- Demonstrated potential for improving annotation efficiency and accuracy, especially for challenging cases
- Showed promise for interactive human-machine collaboration in radiology workflows

## Why This Works (Mechanism)

### Mechanism 1: Partial Mask Refinement via Delta-Reward Signaling
The system improves segmentation accuracy by iteratively updating local regions based on a reward signal derived from the reduction in segmentation loss. At each timestep, the policy selects a specific location and modality. A pre-trained segmentation network processes only this local portion. The resulting local segmentation replaces the corresponding region in the global mask to create an updated prediction. The reward is calculated as the difference in loss, incentivizing the policy to search for "high-yield" regions where processing improves the global metric.

### Mechanism 2: Surrogate "Simulated Radiologist" Environment
Using a pre-trained segmentation network as a fixed environment allows the reinforcement learning agent to decouple strategy learning from feature extraction. The system utilizes a SwinUNETR-v2 model trained on full volumes to act as the environment dynamics. When the policy selects an action, the environment simulates the radiologist's perception by masking out non-selected modalities and cropping non-selected regions. This stability allows the RL policy to optimize the sequence of views without the instability of training the segmentation backbone simultaneously.

### Mechanism 3: Action-Space Discretization for Strategy Discovery
Restricting the action space to discrete modality and location selections forces the model to learn interpretable diagnostic strategies that can deviate from clinical heuristics. The policy outputs a discrete action (Portion × Modality). This bottleneck prevents the model from relying on soft-attention over all data simultaneously. To maximize reward, the agent must learn conditional logic (e.g., "If unclear in T2, check DWI"). The results show this leads to data-driven strategies that occasionally contradict standard PI-RADS guidelines.

## Foundational Learning

- **Concept: Markov Decision Processes (MDPs)**
  - Why needed here: The entire framework relies on formulating the segmentation task as a state-transition problem rather than a mapping problem. Without understanding states, actions, and rewards, the training loop is unintelligible.
  - Quick check question: What constitutes the "State" in this architecture? (Answer: The multi-modal image and the cumulative segmentation mask).

- **Concept: Policy Gradients (REINFORCE)**
  - Why needed here: The paper utilizes REINFORCE and GRPO. One must understand that the policy is optimized by sampling trajectories and reinforcing actions that led to high cumulative rewards, rather than using backpropagation through the segmentation loss directly to the policy weights.
  - Quick check question: How does the policy network receive the gradient signal if the segmentation network is frozen? (Answer: Via the reward, which acts as a scalar objective for the policy update).

- **Concept: Medical Imaging Modalities (MRI)**
  - Why needed here: The agent's primary action is selecting between T2-weighted and Diffusion-Weighted imaging. Understanding that these provide different tissue contrasts (anatomical vs. functional/cellular density) is required to interpret why an agent might prefer one over the other for specific zones.
  - Quick check question: Why might an agent choose DW over T2 for a specific lesion? (Answer: DW is often more sensitive to cellular density in cancerous tissue, though T2 provides better anatomical context).

## Architecture Onboarding

- **Component map:** Input MRI volume → Policy Network → Action (Portion, Modality) → Masked/Cropped Input → SwinUNETR-v2 → Local Segmentation → Updated Global Mask → Reward Calculator

- **Critical path:**
  1. State initialization (Zero mask)
  2. Policy selects Action (e.g., "Top slice, T2 only")
  3. Input volume is masked and cropped
  4. Seg Network predicts local mask
  5. Global mask is updated (Paste local mask into previous mask)
  6. Reward calculated -> Policy Update

- **Design tradeoffs:**
  - Fixed vs. Learnable Environment: The paper uses a fixed pre-trained segmentation network. This stabilizes RL training but prevents the segmentation features from adapting to the specific needs of the policy.
  - Discrete vs. Continuous Actions: The system uses discrete portions (8 slices per portion). This reduces action space complexity but may split anatomical structures across boundaries.

- **Failure signatures:**
  - Cycling Behavior: The agent repeatedly selects the same patch/modality because the reward is noisy or the segmentation network is inconsistent on that patch.
  - Reward Hacking: The agent finds a region where the initialization (zeros) yields a high loss drop simply by predicting "background," maximizing reward without finding cancer.
  - Early Exit: The agent stops updating prematurely if the discount factor is too low, failing to localize small lesions.

- **First 3 experiments:**
  1. **Sanity Check (Frozen Seg Net):** Run the pre-trained SwinUNETR on the hold-out set using only T2, only DW, and Both (static inference) to establish baseline upper/lower bounds the RL agent must beat.
  2. **Reward Shaping Analysis:** Train the agent with γ=0 (myopic/greedy) vs. γ=0.8 (far-sighted) to verify if multi-step planning is actually required or if greedy selection suffices.
  3. **Trajectory Visualization:** For a known "hard" case (e.g., PZ lesion), visualize the sequence of actions (Modality/Location choices) to confirm the agent is switching modalities rationally rather than randomly wandering before convergence.

## Open Questions the Paper Calls Out

- **Can the proposed policy network effectively assist human radiologists in an interactive annotation workflow to improve efficiency and accuracy?**
  - The conclusion states that "future research will investigate the feasibility to use the proposed policy networks working with radiologists, to test the capability in assisting a human-machine interactive workflow." The current work only validates the agent using a "simulated radiologist" rather than human subjects.

- **What is the clinical validity and utility of the agent's learned strategy when it contradicts established guidelines like PI-RADS v2.1?**
  - The paper notes that the "trained agent independently developed its own optimal strategy, which may or may not be consistent with current radiologist guidelines" and cites examples where the agent chose non-standard modalities for specific zones. While the paper reports improved Dice scores, it does not explain why the agent deviates or if these deviations represent clinically superior biomarkers or simply dataset-specific artifacts.

## Limitations
- The policy network architecture details are not specified, requiring assumptions that may affect reproducibility
- The discrete action space with rigid 8-slice portions may cause anatomical structures to be split across boundaries, leading to jagged segmentations
- The fixed pre-trained segmentation network limits the system's ability to adapt to the specific decision-making patterns of the RL agent

## Confidence

- **High Confidence:** The core RL framework (policy network selecting actions, frozen segmentation network providing environment dynamics, Dice-based reward) is well-specified and theoretically sound. The use of a pre-trained SwinUNETR-v2 as a "simulated radiologist" is clearly described.

- **Medium Confidence:** The overall training procedure (REINFORCE/GRPO, 60-step episodes, action space definition) is specified, but implementation details like policy network architecture and exact reward normalization are missing.

- **Low Confidence:** The exact handling of prostate gland cropping and the availability of ground truth prostate masks for preprocessing are unclear, which could significantly impact data preparation.

## Next Checks

1. **Sanity Check on Simulated Radiologist:** Evaluate the pre-trained SwinUNETR-v2 on the validation set using only T2-weighted and only DW MRI inputs. Confirm that the Dice scores are above 0.2 to ensure the fixed environment provides meaningful feedback to the RL agent.

2. **Action Space Boundary Analysis:** Analyze segmentation performance across the 8 defined portions to identify if specific slices (especially boundaries between portions) consistently show lower Dice improvement. This will indicate if the rigid action space is causing suboptimal coverage.

3. **Policy Strategy Visualization:** For a challenging case (e.g., a peripheral zone lesion), visualize the sequence of modality and location selections made by the trained policy. Verify that the agent's strategy (e.g., switching from T2 to DW) is rational and not random wandering, and compare it against standard PI-RADS guidelines to confirm the deviation mentioned in the results.