---
ver: rpa2
title: Symmetric Pruning of Large Language Models
arxiv_id: '2501.18980'
source_url: https://arxiv.org/abs/2501.18980
tags:
- pruning
- performance
- relative
- wanda
- weight
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Symmetric Weight And Activation (SymWanda),
  a theoretical framework that unifies post-training pruning methods by minimizing
  reconstruction error through both input activations and output influences. Building
  on this foundation, the authors propose several practical pruning strategies, including
  a novel stochastic relative importance approach (stochRIA) that samples subsets
  of weights to reduce computational cost while maintaining performance.
---

# Symmetric Pruning of Large Language Models

## Quick Facts
- arXiv ID: 2501.18980
- Source URL: https://arxiv.org/abs/2501.18980
- Reference count: 40
- Primary result: R2-DSnoT achieves 96.5% perplexity improvement over magnitude pruning in training-free fine-tuning

## Executive Summary
This paper introduces Symmetric Weight And Activation (SymWanda), a theoretical framework that unifies post-training pruning methods by minimizing reconstruction error through both input activations and output influences. Building on this foundation, the authors propose several practical pruning strategies, including a novel stochastic relative importance approach (stochRIA) that samples subsets of weights to reduce computational cost while maintaining performance. Experiments across multiple LLMs show stochRIA achieves perplexity scores within 0.66 of full-sampling RIA using only 10% of samples. The paper also introduces R2-DSnoT, a training-free fine-tuning method that incorporates relative weight importance and regularization within a dynamic sparse framework. R2-DSnoT significantly outperforms existing baselines in both perplexity (up to 96.5% improvement over magnitude pruning) and zero-shot accuracy across seven tasks, establishing a new state-of-the-art for training-free sparse LLM fine-tuning.

## Method Summary
The paper proposes SymWanda, a theoretical framework for post-training pruning that minimizes reconstruction error through both input activations and output influences. The framework introduces stochastic relative importance sampling (stochRIA) for efficient weight selection by sampling only 10% of weights, and R2-DSnoT, a training-free dynamic sparse refinement method that iteratively adjusts sparse masks using relative importance scores and regularization. The method operates on pre-trained models using calibration data (128 samples from C4 dataset) and evaluates performance on Wikitext-2 (perplexity) and seven zero-shot tasks.

## Key Results
- StochRIA achieves perplexity scores within 0.66 of full-sampling RIA using only 10% of samples
- R2-DSnoT reduces perplexity from 6.9e3 to 240 for magnitude-pruned LLaMA2-7b at 60% sparsity
- R2-DSnoT achieves up to 96.5% perplexity improvement over magnitude pruning and sets new state-of-the-art for training-free sparse LLM fine-tuning

## Why This Works (Mechanism)

### Mechanism 1: Symmetric Reconstruction Error Minimization
- **Claim:** Minimizing reconstruction error through both input activations and output influences provides theoretical grounding for why methods like Wanda and RIA work.
- **Mechanism:** The framework defines a symmetric objective g(fW) := ∥X(fW − W)∥F + ∥(fW − W)Y∥F that jointly considers how pruning affects both forward propagation (via input activations X) and downstream layers (via output calibration Y). For a single weight Wjk, this simplifies to Sjk = |Wjk|(∥X:j∥2 + ∥Yk:∥2), providing a principled importance score.
- **Core assumption:** The Frobenius norm of reconstruction error meaningfully correlates with downstream task performance (perplexity, zero-shot accuracy).
- **Evidence anchors:**
  - [abstract] "introduces Symmetric Weight And Activation (SymWanda), a theoretical framework that unifies post-training pruning methods by minimizing reconstruction error through both input activations and output influences"
  - [section 3.2] Lemma 3.1 shows the simplified expression g(fW) = |Wjk|(∥X:j∥2 + ∥Yk:∥2)
  - [corpus] Limited corpus support; related work (Wanda++, STADE) focuses on activation-aware pruning but doesn't directly validate the symmetric formulation.
- **Break condition:** If reconstruction error no longer correlates with downstream performance (e.g., at extreme sparsity >80%), or if output calibration Y cannot be efficiently computed.

### Mechanism 2: Stochastic Relative Importance Sampling
- **Claim:** Sampling only 10% of weights to estimate relative importance achieves performance within 0.66 perplexity of full-sampling RIA.
- **Mechanism:** Instead of computing relative importance across all rows and columns (O(b + c) complexity), stochRIA samples τ indices from each row and column, computing Sjk = |Wjk|(∥Wj:Sj∥−1_1 + ∥WSk:k∥−1_1). This preserves diversity in lower-importance weights while reducing noise from uniform averaging.
- **Core assumption:** Relative importance scores are approximately preserved under random subsampling when τ ≥ 0.05 × min(b, c).
- **Evidence anchors:**
  - [abstract] "stochRIA achieves perplexity scores within 0.66 of full-sampling RIA using only 10% of samples"
  - [section 4.1] Table 2 shows stochRIA with 10% sampling matches or exceeds RIA across LLaMA2-7b/13b, LLaMA3-8b, OPT-1.3b
  - [section 4.1] "stochRIA exhibits stable and competitive performance relative to RIA, particularly when the sampling ratio τ ≥ 0.05"
  - [corpus] No direct corpus validation of stochastic sampling for pruning efficiency.
- **Break condition:** If sampling ratio drops below 5% (τ < 0.05), performance degrades significantly (Table 9 shows perplexity increases at β = 0.01).

### Mechanism 3: Training-Free Dynamic Sparse Refinement
- **Claim:** Incorporating relative importance and regularization into a prune-and-grow cycle improves perplexity by up to 96.5% over magnitude pruning without any gradient-based training.
- **Mechanism:** R2-DSnoT iteratively adjusts sparse masks by (1) identifying weights to regrow based on sign(E[ϵq]) · Dq,r · E[Xq]/Var(Xq) + γ1 regularization, and (2) pruning weights minimizing |fWq,r| · Dq,r · ∥Xq∥α_2 + γ2 regularization, where Dq,r = ∥fWq,:∥−1_1 + ∥fW:,r∥−1_1 captures relative importance.
- **Core assumption:** The reconstruction error gradient direction (sign(E[ϵq])) combined with relative importance provides sufficient signal for mask adaptation without backpropagation.
- **Evidence anchors:**
  - [abstract] "R2-DSnoT significantly outperforms existing baselines in both perplexity (up to 96.5% improvement over magnitude pruning)"
  - [section 3.6] Equations (5)-(6) define the growing and pruning indices with relative importance and regularization
  - [section 4.3] Table 5 shows R2-DSnoT reduces perplexity from 6.9e3 to 240 for magnitude-pruned LLaMA2-7b at 60% sparsity
  - [corpus] Corpus papers (e.g., "A Free Lunch in LLM Compression") discuss retraining after pruning but don't validate training-free approaches.
- **Break condition:** If the initial pruned model has extreme reconstruction error (e.g., magnitude pruning on LLaMA3-8b causing 4.05e5 perplexity), R2-DSnoT may not fully recover performance.

## Foundational Learning

- **Concept: Post-training pruning (PTP)**
  - Why needed here: The entire paper operates on compressing pre-trained LLMs without gradient-based retraining. Understanding that PTP relies on calibration data (not training data) to estimate importance is critical.
  - Quick check question: Given a 7B parameter model, how many forward passes on calibration data are needed to compute Wanda-style importance scores?

- **Concept: Reconstruction error vs. task performance**
  - Why needed here: The theoretical justification hinges on reconstruction error as a proxy for perplexity/accuracy. This is not guaranteed—high reconstruction error in early layers may not harm output quality as much as errors in attention heads.
  - Quick check question: If layer A has reconstruction error 0.1 and layer B has error 0.5, which should you prioritize for careful pruning?

- **Concept: Relative vs. absolute importance**
  - Why needed here: RIA's key insight is normalizing by ∥Wj:∥_1 and ∥W:k∥_1, making importance scores comparable across rows/columns with different magnitude distributions. Without this, outlier rows dominate pruning decisions.
  - Quick check question: In a weight row [0.1, 0.2, 5.0], which element has higher relative importance under RIA scoring?

## Architecture Onboarding

- **Component map:**
  ```
  SymWanda Framework
  ├── Input Calibration (X): Calibration data activations from forward pass
  ├── Output Calibration (Y): Can be W^T (self-referential) or explicit downstream signals
  ├── Score Matrix Computation: Sjk = |Wjk|(∥X:j∥2 + ∥Yk:∥2)
  ├── Stochastic Sampling (stochRIA): Random subset Sj, Sk of size τ
  ├── Pruning Decision: Zero out lowest Sjk scores to meet sparsity target
  └── R2-DSnoT (optional)
      ├── Growing Index: Maximize Eq. (5) with relative importance Dq,r
      ├── Pruning Index: Minimize Eq. (6) with regularization γ
      └── Iteration: Repeat until convergence or max cycles
  ```

- **Critical path:**
  1. Load pre-trained model and calibration data (128 samples, 2048 tokens each)
  2. Forward pass to collect activations X for each layer
  3. Compute score matrix Sjk (full RIA or stochRIA with τ = 0.1)
  4. Prune weights with lowest Sjk to target sparsity
  5. (Optional) Run R2-DSnoT for 50 cycles with γ1, γ2 tuned

- **Design tradeoffs:**
  - **Full RIA vs. stochRIA:** Full RIA has O(b + c) cost per layer; stochRIA reduces to O(β·min(b, c)) with β = 0.1. Trade-off is ~0.03-0.1 perplexity increase on some models.
  - **α = 0.5 vs. α = 1.0:** Paper finds α = 0.5 (square root activation) optimal for mitigating outlier activations, but α = 1.0 used in some tables for fair comparison with baselines.
  - **Pruning-only vs. R2-DSnoT:** R2-DSnoT adds ~50 iterations of mask adaptation but requires no backpropagation. Critical for models with poor initial pruning (e.g., magnitude pruning).

- **Failure signatures:**
  - **Perplexity explosion (>100):** Likely using magnitude pruning at high sparsity (>50%) without R2-DSnoT recovery.
  - **StochRIA variance too high:** Sampling ratio β < 0.05 causing unstable importance estimates (Table 9 shows this at β = 0.01).
  - **R2-DSnoT not converging:** Regularization γ too high, or applying relative importance to both grow and prune phases simultaneously (Table 10 shows 2.1 perplexity drop).

- **First 3 experiments:**
  1. **Baseline validation:** Implement Wanda on LLaMA2-7b at 50% sparsity; verify perplexity ~7.79 matches paper. This confirms calibration data and activation extraction are correct.
  2. **StochRIA efficiency test:** Compare full RIA vs. stochRIA (β = 0.1) on a single layer, measuring both perplexity impact and wall-clock time. Expect <5% perplexity difference with 10× speedup.
  3. **R2-DSnoT recovery test:** Start with magnitude-pruned LLaMA2-7b at 60% sparsity (perplexity ~6900), apply R2-DSnoT for 50 cycles, verify perplexity drops to ~240. This validates the prune-and-grow mechanism.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the SymWanda formulation be effectively extended to post-training quantization or training-aware compression techniques?
- Basis in paper: [explicit] The Discussion section "Beyond pruning" explicitly states that extending the symmetric objective to quantization and training-aware compression are promising directions for future research.
- Why unresolved: The current study restricted its scope to validating the theoretical framework and pruning strategies (StochRIA) for post-training pruning only.
- What evidence would resolve it: Experimental results applying the symmetric reconstruction objective to quantization baselines (e.g., OPTQ) that demonstrate maintained or improved accuracy-compression trade-offs.

### Open Question 2
- Question: Does incorporating output activations into the R2-DSnoT fine-tuning criterion minimize reconstruction error better than the input-only approach?
- Basis in paper: [explicit] Section 3.6 notes that for simplicity, output activations are not considered in DSnoT, stating this "may provide an interesting avenue for future exploration."
- Why unresolved: The current growing and pruning rules rely solely on input activations and weight properties, leaving the influence of output dependencies untested.
- What evidence would resolve it: Ablation studies modifying the R2-DSnoT equations to include the $\|Y_{k:}\|$ term, showing a reduction in perplexity or improved zero-shot accuracy.

### Open Question 3
- Question: Do asymmetric or non-uniform sampling strategies improve the efficiency and accuracy of StochRIA compared to the current random uniform sampling?
- Basis in paper: [explicit] The Discussion section "Better sampling" suggests exploring asymmetric or non-uniform sampling within the symmetric framework to optimize performance.
- Why unresolved: StochRIA currently utilizes random uniform sampling of rows and columns; the paper speculates but does not test if structured sampling better handles weight outliers.
- What evidence would resolve it: Benchmarks showing that importance-based or clustered sampling yields lower perplexity than the random baseline at identical sampling ratios ($\beta$).

## Limitations
- The SymWanda framework assumes reconstruction error correlates with downstream performance, but this relationship may break down at extreme sparsities (>80%) or in attention-heavy architectures where weight interactions are non-linear
- Stochastic sampling's theoretical justification relies on empirical validation only; no formal convergence proof that τ=0.1 samples preserve relative importance rankings
- R2-DSnoT's training-free claim assumes calibration data is representative; performance may degrade significantly if calibration distribution differs from test data

## Confidence
- **High Confidence:** Symmetrization objective provides cleaner theoretical foundation for Wanda/RIA (direct mathematical derivation, consistent experimental validation)
- **Medium Confidence:** Stochastic sampling preserves relative importance at τ≥0.05 (strong empirical results but limited theoretical backing)
- **Medium Confidence:** R2-DSnoT achieves 96.5% perplexity improvement over magnitude pruning (extensive ablation studies but complex hyperparameter sensitivity)

## Next Checks
1. **Convergence validation:** Run stochRIA with varying τ (0.01, 0.05, 0.1, 0.2) on LLaMA2-7b, measure perplexity and variance across 5 random seeds to confirm τ≥0.05 stability
2. **Extreme sparsity test:** Apply R2-DSnoT to 80% and 90% sparsity levels, measure if perplexity explosion occurs or if framework breaks down
3. **Calibration data sensitivity:** Compare R2-DSnoT performance using C4 vs. Wikitext-2 calibration data on same model, quantifying distribution shift impact on reconstruction-based pruning