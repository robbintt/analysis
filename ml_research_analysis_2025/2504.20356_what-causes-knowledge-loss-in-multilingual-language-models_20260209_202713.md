---
ver: rpa2
title: What Causes Knowledge Loss in Multilingual Language Models?
arxiv_id: '2504.20356'
source_url: https://arxiv.org/abs/2504.20356
tags:
- languages
- performance
- language
- learning
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates knowledge loss in multilingual language
  models, focusing on catastrophic forgetting when fine-tuning on new languages. The
  authors experiment with LoRA adapters of varying ranks to evaluate non-shared, partially
  shared, and fully shared parameter strategies across 52 languages using the MASSIVE
  dataset.
---

# What Causes Knowledge Loss in Multilingual Language Models?

## Quick Facts
- arXiv ID: 2504.20356
- Source URL: https://arxiv.org/abs/2504.20356
- Reference count: 14
- Primary result: NON-SHARED LoRA strategy achieves 72-73% F1 while mitigating catastrophic forgetting in multilingual continual learning

## Executive Summary
This paper investigates catastrophic forgetting in multilingual language models during sequential fine-tuning on 52 languages from the MASSIVE dataset. The authors systematically evaluate parameter sharing strategies using LoRA adapters of varying ranks, comparing NON-SHARED (per-language adapters) against SHARED (single adapter) approaches. They demonstrate that non-Latin script languages suffer disproportionately from forgetting due to tokenization mismatches and representational interference, while NON-SHARED LoRA effectively isolates language-specific knowledge. The study reveals that higher LoRA ranks do not improve performance, and that script similarity significantly influences cross-lingual transfer effectiveness in continual learning scenarios.

## Method Summary
The authors experiment with XLM-R_BASE using LoRA adapters to evaluate parameter sharing strategies across 52 languages. They test NON-SHARED LoRA (separate adapter per language), SHARED LoRA (single adapter across all languages), VANILLA (full fine-tuning), MONO (per-language models), and MULTI (joint training). LoRA settings include rank ∈ {32, 64, 256} with learning rate 5×10⁻⁶, 100 epochs, and dropout 0.1. The MASSIVE dataset provides 11.5K training examples per language for slot-filling tasks. Performance is measured using F1 score, Cross-lingual Forward/Backward Transfer (CFT/CBT), and Multi-Hop Forward/Backward Transfer (MFT/MBT) metrics.

## Key Results
- NON-SHARED LoRA achieves 72-73% F1 while maintaining stable performance across training steps
- SHARED LoRA performance drops 15-30 F1 points, indicating severe catastrophic forgetting
- Non-Latin script languages (zh-CN, ja-JP, th-TH) consistently show highest forgetting rates
- LoRA rank has minimal impact on performance (r=32 vs r=256 differ by <1% F1)
- MULTI method achieves best overall performance (75.03% F1) but requires joint training of all languages

## Why This Works (Mechanism)

### Mechanism 1: Script-Based Representational Interference
- Claim: Non-Latin scripts experience higher forgetting due to tokenization mismatches and lower subword overlap with pre-trained vocabulary
- Mechanism: XLM-R's SentencePiece tokenizer is optimized for Latin scripts, creating representational hierarchies. Non-Latin languages must allocate capacity to under-represented token sequences, which get overwritten during subsequent Latin-script training
- Core assumption: Tokenizer vocabulary distribution reflects pre-training corpus composition
- Evidence anchors: zh-CN, ja-JP, th-TH show most pronounced forgetting; subword tokenizer aggravates imbalance

### Mechanism 2: Parameter Isolation via Non-Shared LoRA Adapters
- Claim: Per-language LoRA adapters mitigate forgetting by isolating gradient updates to language-specific low-rank matrices
- Mechanism: LoRA decomposes updates as ΔW = BA where B and A are language-specific. Freezing base model θ₀ preserves pre-trained knowledge while adapters serve as modular "plugins"
- Core assumption: Pre-trained base model contains sufficient multilingual representations activatable via low-rank perturbations
- Evidence anchors: NON-SHARED LoRA maintains 70-73% F1 vs SHARED dropping to 60-61%; adapter stability across training steps

### Mechanism 3: Donor-Receiver Dynamics in Sequential Training
- Claim: Languages exhibit asymmetric transfer properties—some act as effective knowledge donors while others are vulnerable receivers
- Mechanism: Languages with high token overlap with many future languages (Latin scripts) act as donors, reinforcing shared representations. Languages with idiosyncratic token distributions receive interference without reciprocating benefits
- Core assumption: Transfer asymmetry driven by linguistic similarity rather than training order randomness
- Evidence anchors: German shows beneficial performance drop but low MBT ranking; destructive languages affect sequential training differently based on position

## Foundational Learning

- **Concept: Catastrophic Forgetting in Neural Networks**
  - Why needed here: The paper frames multilingual knowledge loss through this lens. Understanding sequential gradient updates overwrite previously learned weights is essential for motivating adapter-based isolation
  - Quick check question: If you fine-tune a model on task A, then task B, what happens to task A performance and why?

- **Concept: Low-Rank Adaptation (LoRA)**
  - Why needed here: Experimental methodology depends entirely on LoRA mechanics. Understanding ΔW = BA, rank parameter r, and why freezing W₀ matters is prerequisite
  - Quick check question: Why does LoRA reduce trainable parameters without changing inference latency?

- **Concept: Cross-Lingual Transfer Metrics (CFT/CBT)**
  - Why needed here: Paper introduces MFT/MBT as extensions. Understanding forward vs. backward transfer is prerequisite to interpreting multi-hop analysis
  - Quick check question: What does a negative CBT score indicate about a model's continual learning behavior?

## Architecture Onboarding

- **Component map:**
  Base Model (XLM-R_BASE, θ₀) [FROZEN] -> LoRA Adapters (ϕ) per layer -> Classification Head (slot-filling)

- **Critical path:**
  1. Load pre-trained XLM-R_BASE (270M parameters, frozen)
  2. Initialize LoRA adapters with rank r ∈ {32, 64, 256} targeting attention weights
  3. For each language in sequence D = {D₁, D₂, ..., D₅₂}:
     - SHARED: Update same ϕ incrementally
     - NON-SHARED: Train new ϕₜ, keep previous adapters frozen
  4. Evaluate using F1 on slot-filling task, compute CFT/CBT/MFT/MBT

- **Design tradeoffs:**
  | Strategy | Parameters | Performance | Forgetting Risk | Parallelizable |
  |----------|------------|-------------|-----------------|---------------|
  | MULTI | 278M (best) | 75.03% F1 | N/A (joint training) | No |
  | NON-SHARED LoRA | 278-2228M | 72-73% F1 | Low | Yes |
  | SHARED LoRA | 5-43M | 60-61% F1 | High | No |
  | VANILLA | 278M | 66% F1 | Moderate | No |

  Higher LoRA rank does NOT substantially improve performance (r=32 vs r=256 differ by <1% F1), suggesting capacity is not the bottleneck—interference is.

- **Failure signatures:**
  - SHARED LoRA degrades 15-30 F1 points across tasks → indicates adapter is being overwritten
  - zh-CN, ja-JP cause sharp drops in MBT → indicates these languages are destructive receivers
  - High-vitality languages (zh-CN, hi-IN) show negative MBT despite abundant resources → indicates structural mismatch, not data scarcity

- **First 3 experiments:**
  1. **Baseline replication:** Train SHARED LoRA (r=64) on first 5 languages from Order 1. Measure CBT after each step. Expect negative CBT accumulating to ~-0.1 by step 5.
  2. **Ablation on script families:** Compare NON-SHARED LoRA training on [Latin-only languages] vs. [mixed scripts]. Hypothesis: Latin-only should show less variance in per-language F1.
  3. **Order sensitivity test:** Train SHARED LoRA with destructive languages (zh-CN, ja-JP, th-TH) at positions 1-3 vs. positions 48-52. Measure final average F1. Hypothesis: Late placement of destructive languages preserves earlier knowledge better.

## Open Questions the Paper Calls Out
None

## Limitations
- Experimental design assumes MASSIVE dataset represents realistic multilingual continual learning scenarios, but specific construction and sampling strategy could bias results toward certain language families
- Analysis attributes catastrophic forgetting primarily to script-based representational interference without systematically ruling out alternative explanations like frequency-based token dominance
- Study focuses on LoRA-based adaptation but does not compare against other parameter-efficient methods like prefix tuning or adapter fusion

## Confidence

**High Confidence** (supported by direct experimental evidence):
- Non-Latin script languages consistently show higher catastrophic forgetting rates than Latin script languages
- NON-SHARED LoRA strategy significantly outperforms SHARED LoRA in preserving knowledge
- Increasing LoRA rank from 32 to 256 provides negligible performance improvement

**Medium Confidence** (mechanistic interpretation supported by data but with alternative explanations possible):
- Script-based tokenization imbalance is the primary driver of representational interference
- Donor-receiver asymmetry in sequential training is primarily determined by linguistic similarity
- MULTI method's superior performance stems from learning shared representations across all languages

**Low Confidence** (interpretation-heavy claims with limited direct evidence):
- Tokenizer's vocabulary distribution systematically marginalizes non-Latin scripts
- Adapter-only updates are insufficient for languages with poor pre-training coverage
- MFT/MBT metrics capture meaningful aspects of cross-lingual transfer dynamics

## Next Checks

**Check 1: Ablation on Tokenizer Configuration**
Test whether script-based forgetting patterns persist when using a balanced tokenizer that allocates equal vocabulary capacity to each script family. Train custom tokenizer with uniform subword distribution across major scripts, then repeat SHARED LoRA experiment to isolate effect of tokenization bias.

**Check 2: Order Sensitivity Analysis**
Systematically test whether placing destructive languages (zh-CN, ja-JP, th-TH) at different positions in training sequence affects overall performance differently than predicted by donor-receiver model. Train SHARED LoRA with these languages in positions 1-3, 25-27, and 50-52, then measure variance in final F1 scores.

**Check 3: Alternative Parameter-Efficient Methods**
Compare LoRA against prefix tuning and adapter fusion on same sequential training setup to determine whether forgetting patterns are specific to LoRA's low-rank decomposition or represent general challenge for parameter-efficient multilingual adaptation.