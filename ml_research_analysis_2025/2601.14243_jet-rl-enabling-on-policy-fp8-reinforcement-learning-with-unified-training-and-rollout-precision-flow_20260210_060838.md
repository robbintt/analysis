---
ver: rpa2
title: 'Jet-RL: Enabling On-Policy FP8 Reinforcement Learning with Unified Training
  and Rollout Precision Flow'
arxiv_id: '2601.14243'
source_url: https://arxiv.org/abs/2601.14243
tags:
- training
- rollout
- uni00000013
- bf16
- jet-rl
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the inefficiency and instability of existing
  reinforcement learning (RL) pipelines, where the rollout phase consumes over 70%
  of total training time. The authors identify that the commonly used BF16-train +
  FP8-rollout strategy suffers from severe training instability and accuracy collapse,
  especially under long-horizon rollouts and challenging tasks.
---

# Jet-RL: Enabling On-Policy FP8 Reinforcement Learning with Unified Training and Rollout Precision Flow

## Quick Facts
- arXiv ID: 2601.14243
- Source URL: https://arxiv.org/abs/2601.14243
- Reference count: 40
- Primary result: Unified FP8 precision flow for training and rollout achieves 16% end-to-end speedup over BF16 baseline while maintaining stable convergence.

## Executive Summary
This paper addresses the inefficiency and instability of existing reinforcement learning (RL) pipelines, where the rollout phase consumes over 70% of total training time. The authors identify that the commonly used BF16-train + FP8-rollout strategy suffers from severe training instability and accuracy collapse, especially under long-horizon rollouts and challenging tasks. This is due to numerical mismatches between training and inference precision, which introduce off-policy effects. To resolve this, the authors propose Jet-RL, a unified FP8 precision flow for both training and rollout. By enforcing on-policy consistency, Jet-RL eliminates the need for inter-step calibration and minimizes numerical discrepancies. Extensive experiments show that Jet-RL achieves up to 33% speedup in the rollout phase, 41% in the training phase, and 16% end-to-end speedup over BF16 training, while maintaining stable convergence and negligible accuracy degradation.

## Method Summary
Jet-RL introduces a unified FP8 precision flow that enforces identical quantization granularity between training forward passes and rollout inference. The approach uses E4M3 FP8 format with per-group (1Ã—128) and per-block (128Ã—128) quantization granularity for activations and weights respectively. Training uses BF16 master weights with FP8 compute for forward and weight gradient passes, while activations are stored in FP8 for the backward pass and gradients remain in BF16. The method integrates with vLLM for rollout and VeRL for training, using DeepGEMM kernels optimized for FP8 operations. After each training step, weights are quantized to FP8 and synchronized to the rollout engine, ensuring on-policy consistency throughout the training process.

## Key Results
- Achieves 33% speedup in rollout phase, 41% in training phase, and 16% end-to-end speedup over BF16 baseline
- Maintains stable convergence on long rollouts (>8K tokens) where BF16-train-FP8-rollout strategy fails
- Shows negligible accuracy degradation (<1.5%) compared to full BF16 training across GSM8K, MATH500, and other reasoning tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Enforcing a unified FP8 precision flow between training and rollout restores on-policy consistency and stabilizes RL optimization.
- Mechanism: The paper models precision propagation as a directed graph ð’¢ = (ð’±, â„°). For BF16-train-FP8-rollout, training graph ð’¢_train and inference graph ð’¢_infer have different edge precisions, creating distributional mismatch. Jet-RL forces ð’¢_infer to be a strict subgraph of ð’¢_fwd_train with identical quantization granularity, so rollout behavior matches training forward pass behavior.
- Core assumption: The on-policy assumptionâ€”that the agent learns from data collected under its current policyâ€”is necessary for stable RL convergence in LLM training.
- Evidence anchors:
  - [abstract] "the widely used BF16-training + FP8-rollout strategy suffers from severe training instability and catastrophic accuracy collapse under long-horizon rollouts"
  - [Section 4] "BF16-train-FP8-rollout have two distinct forward quantization graphs... This leads to a mismatch between training and rollout, making the forward process deviate from what the actor will actually generate"
  - [corpus] Related work FP8-RL (arXiv:2601.18150) addresses similar low-precision RL stability issues; corpus evidence on this specific mechanism is nascent but directionally aligned.
- Break condition: If the model architecture uses non-linear operations that cannot be stably quantized to FP8, or if gradient accumulation requires BF16 precision for numerical stability, the unified flow may not hold.

### Mechanism 2
- Claim: Mixed per-group (1Ã—128) and per-block (128Ã—128) quantization granularity stabilizes FP8 GEMM operations across forward and backward passes.
- Mechanism: FProp uses 1Ã—128 per-group quantization for activations (fused with prior operator) and 128Ã—128 per-block for weights. WGrad uses asymmetric granularity (1Ã—128 Ã— 128Ã—1) following DeepSeek-V3. DGrad reuses FProp kernel. This avoids per-tensor quantization instability while enabling FP8 TensorCore acceleration.
- Core assumption: Finer-grained quantization (per-group/per-block) provides sufficient dynamic range to avoid underflow/overflow that would degrade convergence.
- Evidence anchors:
  - [Section 4.2] "Per-tensor quantization of FP8 has been shown to be unstable in training large language models. Therefore, we adopt a finer-grained quantization granularity"
  - [Section 4.2] "For WGrad... we quantize the first matrix in 1Ã—128 while quantizing the second one in 128Ã—1. This finer-grained design helps stabilize training"
  - [corpus] QeRL (arXiv:2510.11696) combines NVFP4 quantization with LoRA for memory efficiency, but Jet-RL's granularity strategy for FP8 is not directly validated in corpus.
- Break condition: If hardware does not support the required matrix layouts (row-wise Ã— column-wise for TensorCore), or if the 128-block size causes edge-case precision loss for very small tensors.

### Mechanism 3
- Claim: Storing FP8 activations for backward pass and retaining BF16 gradients maintains training stability while achieving FP8 compute acceleration.
- Mechanism: Since quantization is fused with the previous operator, FP8 activations from forward pass are stored for backward pass directly. Gradients transported between operators remain BF16 to avoid underflow/noise. Master weights stay in BF16; only compute uses FP8.
- Core assumption: BF16 precision is sufficient for gradient propagation to avoid numerical degradation, while FP8 precision suffices for stored activations.
- Evidence anchors:
  - [Section 4.1] "we elect to store the activations for the backward pass also in FP8 precision. This strategy has been shown to maintain training stability in prior work on pretraining and supervised fine-tuning"
  - [Section 4.1] "We retain the gradients transported between operators during the backward pass in BF16 precision to preserve model accuracy"
  - [corpus] No direct corpus validation for this specific activation/gradient split; evidence is primarily internal to the paper.
- Break condition: If gradient magnitudes fall below BF16 representable range during long training runs, or if activation quantization error accumulates across many layers.

## Foundational Learning

- Concept: **On-Policy vs. Off-Policy RL**
  - Why needed here: The paper's core argument is that BF16-train-FP8-rollout creates an implicit off-policy condition due to precision mismatch. Understanding why on-policy consistency matters for PPO/GRPO convergence is essential to evaluate Jet-RL's contribution.
  - Quick check question: Can you explain why importance sampling is needed when training data comes from a different policy than the one being updated?

- Concept: **Quantization Granularity (Per-Tensor vs. Per-Group vs. Per-Block)**
  - Why needed here: The paper selects specific granularities (1Ã—128, 128Ã—128) for different GEMM operations. Understanding trade-offs between granularity (precision) and overhead (scaling factor storage/computation) is necessary to assess whether this is a reasonable design.
  - Quick check question: Why would per-tensor quantization be unstable for LLM training but acceptable for post-training quantization?

- Concept: **FP8 E4M3 Format**
  - Why needed here: The paper uses E4M3 (4 exponent bits, 3 mantissa bits) with Î”_max = 448. Understanding the representable range and precision characteristics helps evaluate where FP8 may fail.
  - Quick check question: What is the maximum representable value in FP8 E4M3, and why might this be problematic for activations with outliers?

## Architecture Onboarding

- Component map:
  - vLLM (Rollout Engine) -> generates responses using FP8 weights
  - VeRL (Training Framework) -> handles Update phase with FP8 GEMMs
  - DeepGEMM (FP8-optimized kernels) -> row-wise/column-wise layout requirements
  - Triton (Quantization Operators) -> fused activation/RMSNorm quantization

- Critical path:
  1. Rollout phase: Actor (FP8) generates responses â†’ responses stored for training
  2. Evaluation phase: Reference/Reward/Critic (can be FP8 or BF16) score responses
  3. Update phase: Actor forward (FP8 GEMM) â†’ compute loss â†’ backward pass (FP8 WGrad/DGrad, BF16 gradients) â†’ update BF16 master weights â†’ quantize to FP8 â†’ sync to rollout engine

- Design tradeoffs:
  - Tensor Parallelism vs. Speedup: Higher TP reduces FP8 speedup due to communication overhead (32B: TP=2 gives 1.33Ã—, TP=4 gives only 1.08Ã—)
  - Precision vs. Speed: Storing activations in FP8 saves memory but may accumulate error; keeping gradients BF16 preserves stability
  - Model size vs. Benefit: Larger models (32B) see greater speedup than smaller models (8B) due to compute-bound vs. memory-bound profiles

- Failure signatures:
  - Training divergence on long rollouts (>8K tokens): Indicates precision mismatch accumulating over sequence length
  - Failure on challenging tasks with weak base models: Indicates quantization error distorting trajectories when model confidence is low
  - No convergence at all (as seen with Qwen2.5-7B + BF16-train-FP8-rollout): Severe off-policy breakdown

- First 3 experiments:
  1. Validate baseline instability: Reproduce BF16-train-FP8-rollout failure on Qwen2.5-7B with 8K rollout on GSM8K+MATH; verify it diverges while BF16 baseline converges.
  2. Confirm unified precision restores stability: Run Jet-RL on same configuration; measure if convergence is restored and quantify accuracy gap vs. BF16 (expect <1.5% degradation).
  3. Profile end-to-end speedup: On Qwen3-8B-Base with 8K rollout, measure per-phase latency (rollout, evaluation, update) for BF16 vs. Jet-RL; verify ~1.16Ã— end-to-end speedup and identify remaining bottlenecks.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the end-to-end throughput and convergence stability of Jet-RL scale with model sizes significantly larger than the tested 8Bâ€“32B range?
- **Basis in paper:** [explicit] The paper states, "A full scaling study on 14â€“32B models is left to future work given resource constraints," noting only that speedups are expected to be more significant for larger models.
- **Why unresolved:** The authors could not empirically verify if the 16% end-to-end speedup holds or improves for models like Llama-70B or larger, where memory bandwidth and communication patterns differ.
- **What evidence would resolve it:** Benchmarking results of Jet-RL on 70B+ parameter models, reporting step-time speedup and final task accuracy compared to BF16 baselines.

### Open Question 2
- **Question:** Can the diminishing returns of FP8 acceleration under high tensor parallelism (TP) be recovered through communication-overlapping optimizations?
- **Basis in paper:** [inferred] In Table 4 and Section 5.3, the authors observe that speedup drops from 1.33Ã— (TP=2) to 1.07Ã— (TP=4) for the 32B model because "communication overhead becomes more pronounced."
- **Why unresolved:** The paper identifies the communication bottleneck in high-TP settings but does not propose or evaluate methods (such as overlapping) to mitigate this specific efficiency loss.
- **What evidence would resolve it:** A system implementation that overlaps FP8 communication with computation in high-TP settings, demonstrating a restoration of speedup ratios closer to the theoretical maximum.

### Open Question 3
- **Question:** Can the unified precision flow strategy be extended to lower precisions (e.g., FP4) without causing training divergence?
- **Basis in paper:** [inferred] The method relies on FP8 (E4M3) to maintain stability, explicitly retaining BF16 for the backward pass to avoid gradient underflow.
- **Why unresolved:** While the paper proves FP8 viable for "on-policy" flows, it remains untested if the accumulated numerical mismatch would return or worsen if the precision were dropped to 4-bit.
- **What evidence would resolve it:** An ablation study applying the Jet-RL logic to FP4 quantization, measuring if convergence remains stable or if it reverts to the "catastrophic collapse" seen in naive FP8 methods.

### Open Question 4
- **Question:** Does Jet-RL generalize to non-reasoning reinforcement learning tasks (e.g., standard RLHF alignment)?
- **Basis in paper:** [inferred] The evaluation focuses exclusively on "reasoning-oriented RL" (MATH, GSM8K) where correctness is verifiable, leaving performance on preference-based alignment tasks unstated.
- **Why unresolved:** The benefits of unified precision are demonstrated on long-context reasoning, but it is unclear if the "negligible accuracy degradation" holds for tasks with shorter rollouts or subjective reward models.
- **What evidence would resolve it:** Training runs on standard alignment datasets (e.g., HH-RLHF) comparing Jet-RL against BF16 baselines using win-rate metrics.

## Limitations

- **Communication bottleneck in high tensor parallelism**: Speedup diminishes from 1.33Ã— to 1.07Ã— when increasing TP from 2 to 4 for 32B models due to communication overhead
- **Unverified scaling to larger models**: The paper only tested 8B-32B models and states that scaling study is left to future work
- **Reasoning task focus**: All evaluations are on reasoning-oriented RL tasks; generalization to standard RLHF alignment remains unproven

## Confidence

- **High**: End-to-end speedup measurements (rollout 33%, training 41%, total 16%) are straightforward empirical results that can be independently verified.
- **Medium**: Claims about precision mismatch causing off-policy effects are well-supported by the controlled experiments (BF16-train-FP8-rollout fails on long rollouts while Jet-RL succeeds), though the theoretical mechanism could be more rigorously formalized.
- **Medium**: Claims about quantization granularity stability (per-group/per-block vs per-tensor) are partially supported by the experiments but rely on undocumented kernel implementations and lack corpus validation.

## Next Checks

1. **Baseline instability verification**: Reproduce the BF16-train-FP8-rollout failure on Qwen2.5-7B with 8K rollout on GSM8K+MATH; verify divergence within first 20 steps while BF16 baseline converges.
2. **Precision mismatch isolation**: Run ablation study comparing BF16-train-FP8-rollout vs Jet-RL vs BF16-train-BF16-rollout to quantify how much speedup comes from FP8 compute vs precision consistency.
3. **Cross-model generalization**: Test Jet-RL on a third model architecture (e.g., Mistral-7B or DeepSeek-Coder) to verify the unified precision approach generalizes beyond the two tested models.