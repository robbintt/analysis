---
ver: rpa2
title: 'FedCF: Fair Federated Conformal Prediction'
arxiv_id: '2509.22907'
source_url: https://arxiv.org/abs/2509.22907
tags:
- fairness
- pred
- coverage
- parity
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper extends Conformal Fairness (CF) to a federated setting
  (FedCF), addressing fairness gaps in decentralized models. It provides a theoretical
  framework to bound conditional coverage disparities across groups by reformulating
  fairness-specific coverage in terms of client-server terms.
---

# FedCF: Fair Federated Conformal Prediction

## Quick Facts
- **arXiv ID:** 2509.22907
- **Source URL:** https://arxiv.org/abs/2509.22907
- **Reference count:** 40
- **Primary result:** Extends CF to federated setting (FedCF), controlling fairness disparities while maintaining CP validity across tabular, graph, and image datasets.

## Executive Summary
FedCF addresses fairness gaps in decentralized models by extending Conformal Fairness to federated learning. It provides a theoretical framework to bound conditional coverage disparities across groups without centralizing calibration data. The method decomposes fairness-specific coverage into client-server terms, enabling two communication protocols that balance overhead and data protection. Experiments demonstrate FedCF's ability to control fairness disparities within specified closeness criteria while maintaining coverage validity across multiple data modalities.

## Method Summary
FedCF finds a score threshold λ that satisfies user-specified closeness criteria for coverage gaps between sensitive groups while maintaining valid coverage. It trains base models (XGBoost, GraphSAGE, ResNet-18) via FedAvg, then uses a descent-based optimization with momentum to find optimal λ. Clients compute local terms involving non-conformity scores and counts, which are aggregated by the server to compute coverage gaps. Two communication protocols offer trade-offs between efficiency and privacy, with clients sending either group-level statistics or pairwise differences.

## Key Results
- Controls fairness disparities within specified closeness criteria across tabular, graph, and image datasets
- Maintains conformal prediction validity while reducing efficiency overhead compared to baseline FCP
- Offers communication-efficient (O(|G||Y⁺|)) and enhanced privacy (O(|G|²|Y⁺|)) protocols
- Demonstrates robustness across varying client counts (K ∈ {2, 4, 8}) and supports intersectional fairness

## Why This Works (Mechanism)

### Mechanism 1: Federated Coverage Decomposition
FedCF estimates fairness-specific coverage disparities across groups without centralizing calibration data by decomposing coverage probability into four separable terms: client-local conditional coverage, filter probability given client, client exchangeability probability, and global filter probability. Terms I-II are computed locally; III-IV aggregated server-side via Lemmas B.1-B.3. Partial exchangeability assumption holds: within each client k, calibration scores and test scores are exchangeable with probability γk.

### Mechanism 2: Descent-Based Threshold Optimization
FedCF efficiently finds λ_opt satisfying fairness closeness criterion c while maintaining CP validity by replacing grid search with momentum-based gradient descent. Initializes λ₀ = FCP quantile, then iteratively updates λ based on coverage gap with adaptive learning rate and momentum. Continues past first valid λ to seek smaller solutions. Assumes coverage gap varies smoothly enough for descent; requires sufficient calibration data per (g,ỹ) pair.

### Mechanism 3: Privacy-Utility Tradeoff via Aggregation Protocol Choice
Clients choose between communication efficiency and enhanced privacy based on local constraints. Communication-efficient protocol: clients send O(2·|G||Y⁺|) values; server computes max/min separately. Enhanced privacy protocol: clients send O(|G|²|Y⁺|) pairwise differences, preventing server from reconstructing individual group distributions. Assumes server is honest-but-curious; adversarial server could infer distributions from multiple λ queries over time.

## Foundational Learning

- **Concept: Conformal Prediction Coverage Guarantee**
  - Why needed: FedCF builds on CP's guarantee that Pr[y_test ∈ C(x_test)] ≥ 1-α. Without understanding this, the fairness extension (conditional coverage) lacks context.
  - Quick check: Given calibration scores [0.1, 0.3, 0.5, 0.7, 0.9] and α=0.2, what quantile gives valid coverage? (Answer: ⌈(5+1)(0.8)⌉/5 = 5/5 = 1.0, use 0.9)

- **Concept: Federated Partial Exchangeability**
  - Why needed: FedCF relies on Lu et al. (2023)'s result that test points are exchangeable with client k's data with probability γk. This justifies weighted aggregation.
  - Quick check: With n₁=100, n₂=50 calibration points, what is γ₁? (Answer: (100+1)/(150+2) ≈ 0.664)

- **Concept: Group Fairness Metrics (Demographic Parity, Equal Opportunity, Predictive Equality)**
  - Why needed: FedCF adapts these to set-valued predictions. Understanding original definitions clarifies how coverage replaces point prediction events.
  - Quick check: For Equal Opportunity with Y⁺={1}, group A has Pr[ỹ ∈ Cλ | Y=1, g=A]=0.95, group B has 0.85. What is the disparity? (Answer: 0.10)

## Architecture Onboarding

- **Component map:** Client-side computes local terms α^(λ)_(g,ỹ),k and n^(g,ỹ)_k → Server-side aggregates γk weights, computes Lcov/Ucov → Communication layer handles protocol selection

- **Critical path:** 1) Pre-training: Train federated model via FedAvg → obtain held-out calibration split per client 2) Prior computation: Server aggregates n^(g,ỹ)_k counts to compute L(g,ỹ)/U(g,ỹ) 3) Threshold search: For each descent iteration, clients send local terms → server computes coverage gap → updates λ_t 4) Deployment: Final λ_opt used for test-time prediction sets

- **Design tradeoffs:** Interval bounds vs. Point estimates (MLE): Bounds guarantee finite-sample coverage but yield larger prediction sets (lower efficiency). MLE tightens estimates but assumes IID and may violate guarantees. Communication-efficient vs. Enhanced privacy: Former gives O(|G||Y⁺|) messages with better efficiency; latter gives O(|G|²|Y⁺|) with stronger privacy. Closeness criterion c: Smaller c enforces stricter fairness but increases efficiency cost.

- **Failure signatures:** Excessive efficiency (>80% of classes): Likely interval bounds too wide due to sparse (g,ỹ) pairs → increase calibration data or switch to MLE. Coverage gap consistently exceeds c: Check data heterogeneity across clients; may need client-specific λ values. Privacy protocol degrades performance significantly: Verify client has sufficient n^(g,ỹ)_k for stable estimates.

- **First 3 experiments:** 1) Baseline validation: Run FCP on dataset without fairness constraints → measure baseline coverage gap disparity 2) Ablation on estimation method: Compare interval bounds vs. MLE on validation set → quantify efficiency-fairness tradeoff 3) Protocol comparison: Test communication-efficient vs. enhanced privacy with varying client counts (K ∈ {2, 4, 8}) → verify overhead scales as O(|G||Y⁺|) vs. O(|G|²|Y⁺|)

## Open Questions the Paper Calls Out

### Open Question 1
How can FedCF be effectively extended to split learning architectures where model layers are divided between client and server? The conclusion states this will be explored in the future. Split learning involves sharing partial computations rather than full model updates, potentially requiring new aggregation protocols for fairness terms that differ from the current federated averaging approach.

### Open Question 2
Can utilizing an exponential mechanism for noise injection in differentially private FedCF restore strict fairness guarantees without significant utility loss? Appendix F notes that Gaussian noise results in a PAC-style guarantee, whereas strictly positive noise via an exponential mechanism could restore the strict guarantee. It's unstated if the overestimation of the coverage gap caused by the exponential mechanism leads to impractically large prediction sets.

### Open Question 3
Can alternative Binomial Proportion Confidence Intervals provide a superior trade-off between efficiency and fairness compared to the current bounds? Section 5 suggests that several results provide tighter or looser bounds which can be used to get better efficiency vs. fairness trade-offs. The current implementation uses conservative bounds to ensure validity, which may inflate prediction set sizes unnecessarily.

## Limitations

- Empirical validation covers only three datasets with fixed sensitivity group definitions, leaving questions about performance on highly imbalanced groups or non-tabular modalities
- Paper does not report statistical significance testing for fairness disparities, making it unclear whether observed improvements are robust to sampling variation
- Descent algorithm's convergence properties lack formal guarantees beyond empirical momentum stabilization

## Confidence

- **High confidence:** The decomposition mechanism is mathematically sound and grounded in established exchangeability theory; the fairness coverage bounds follow directly from lemmas
- **Medium confidence:** The descent-based optimization appears practical and well-motivated, though lack of hyperparameter specification and convergence proofs limits full verification
- **Medium confidence:** The privacy-utility tradeoff is conceptually valid with clear protocol differences, but empirical comparisons are limited to efficiency metrics without formal privacy leakage quantification

## Next Checks

1. Run statistical significance tests (bootstrap confidence intervals) on coverage gap disparities between FedCF and baseline FCP across multiple runs to verify observed fairness improvements are not due to sampling noise
2. Test FedCF with heterogeneous client calibration distributions (e.g., simulated drift) to evaluate bound validity when exchangeability assumption weakens
3. Implement and evaluate the hybrid protocol (Algorithm 6) on a dataset with mixed privacy requirements to quantify real-world performance trade-offs