---
ver: rpa2
title: Block removal for large language models through constrained binary optimization
arxiv_id: '2602.00161'
source_url: https://arxiv.org/abs/2602.00161
tags:
- blocks
- block
- arxiv
- optimization
- large
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of compressing large language
  models (LLMs) by removing entire transformer blocks, a problem complicated by the
  exponential complexity of selecting the optimal set of blocks to remove. The authors
  formulate this as a constrained binary optimization (CBO) problem, mapping it to
  an Ising model where low-energy solutions correspond to high-performing pruned models.
---

# Block removal for large language models through constrained binary optimization

## Quick Facts
- arXiv ID: 2602.00161
- Source URL: https://arxiv.org/abs/2602.00161
- Reference count: 40
- Block removal for LLMs via constrained binary optimization outperforms state-of-the-art methods, with MMLU scores improving by up to 6 percentage points at high compression rates

## Executive Summary
This paper introduces a novel approach to compressing large language models by removing entire transformer blocks through constrained binary optimization (CBO). The authors formulate block removal as an Ising model where low-energy solutions correspond to high-performing pruned models, enabling efficient ranking of exponentially many configurations. By leveraging a second-order Taylor expansion of the loss function, the method captures block-to-block interactions and consistently outperforms state-of-the-art block-removal techniques across multiple benchmarks. The approach also generalizes to heterogeneous architectures like mixture-of-experts models, successfully removing blocks from NVIDIA-Nemotron-3-Nano-30B-A3B-FP8 while retaining strong performance without retraining.

## Method Summary
The method formulates transformer block removal as a constrained binary optimization problem by mapping it to an Ising model where binary variables indicate block removal. A second-order Taylor expansion of the loss with respect to block coupling variables captures interactions between blocks, enabling globally informed pruning decisions. The Hessian matrix, approximated as H_0 ≃ (1/m)A^TA from per-sample gradients, serves as the energy function. Solving for low-energy states yields candidate configurations ranked by predicted degradation, which can be evaluated without benchmarking each configuration individually. The approach uses knowledge distillation for retraining after block removal.

## Key Results
- MMLU scores improved by up to 6 percentage points compared to state-of-the-art methods at high compression rates (40%+ parameter reduction)
- Low-energy excited states often outperform the ground state, with the 17th excited state removing block 2 and achieving superior performance across multiple benchmarks
- Successfully applied to heterogeneous mixture-of-experts architectures (NVIDIA-Nemotron-3-Nano-30B-A3B-FP8), removing MoE-only blocks without retraining while maintaining performance
- Outperformed baselines including Sliding Window Merging (SWM) and consecutive block removal heuristics across MMLU, Winogrande, GSM8K, BBH, Hellaswag, and ARC Challenge

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Second-order Taylor expansion of the loss captures block-to-block interactions, enabling globally informed pruning decisions.
- **Mechanism:** Introduce coupling variables α_i for each block, compute per-sample gradients over calibration data, and approximate the Hessian as H_0 ≃ (1/m)A^TA. This Hessian encodes pairwise interaction strengths between blocks—positive entries indicate pruning both together increases loss more than expected from individual effects.
- **Core assumption:** The loss landscape near the trained model is approximately quadratic with respect to block coupling variables, and first-order gradients are negligible (∇L(α_0) ≈ 0).
- **Evidence anchors:**
  - [Section 3]: "We construct a second-order approximation of the loss by Taylor-expanding it with respect to these variables and computing an approximate Hessian."
  - [Section 3]: "We further approximate the Hessian as H_0 ≃ 1/m A^TA, where A is a matrix of per-sample gradients."
  - [corpus]: Weak—no corpus papers address Hessian-based block pruning in LLMs.
- **Break condition:** If blocks have highly non-additive effects beyond second order (e.g., three-way interactions dominate), the quadratic approximation degrades. May occur with very aggressive compression (>50%) or highly non-uniform architectures not represented in calibration data.

### Mechanism 2
- **Claim:** The Ising energy strongly proxies downstream model performance, allowing efficient ranking of exponentially many configurations without evaluation.
- **Mechanism:** Map the CBO to an Ising model: binary variables x_i ∈ {0,1} indicate removal; the Hamiltonian x^T H_0 x represents predicted loss increase. Solving for low-energy states yields candidate configurations ranked by predicted degradation, enabling selection without benchmarking each.
- **Core assumption:** The energy computed from the Hessian approximation correlates with actual downstream performance across diverse benchmarks.
- **Evidence anchors:**
  - [Abstract]: "...formulate block removal as a constrained binary optimization problem that can be mapped to a physical system (Ising model), whose energies are a strong proxy for downstream model performance."
  - [Section 4.1]: Fig. 3-4 show low-energy states consistently produce high-performing configurations.
  - [corpus]: Corpus neighbor on "Self-Adaptive Ising Machines" discusses Ising formulations for constrained optimization but not LLM pruning.
- **Break condition:** If calibration data doesn't represent benchmark domains, energy-performance correlation weakens. Paper uses OpenHermes-2.5; different domains may require recalibration.

### Mechanism 3
- **Claim:** Low-energy excited states often outperform the ground state, making approximate solvers viable and optimal consecutive-block heuristics invalid.
- **Mechanism:** The optimization landscape contains near-degenerate solutions with similar energies but different structural properties (e.g., removing early vs. late blocks). The 17th excited state removed block 2 and outperformed the ground state across MMLU, Winogrande, BBH, GSM8K. This suggests the Hessian proxy is approximate and downstream performance depends on factors not fully captured.
- **Core assumption:** The energy function is an imperfect proxy—multiple structurally distinct solutions lie within a low-energy band where actual performance varies non-monotonically.
- **Evidence anchors:**
  - [Section 4.1]: "after short retraining, this excited-state configuration outperforms the ground state across several benchmarks. This observation disproves the common heuristic assumption that high-quality pruning solutions consist of consecutive blocks."
  - [Section 4.1]: "the first 16 low-energy states are quite similar... the 17th excited state... is the first configuration that proposes removing a block close to the beginning."
  - [corpus]: No corpus papers address excited-state selection in pruning.
- **Break condition:** If the energy landscape has large gaps (few near-degenerate states), excited-state exploration yields little benefit. Observed for models with O(10²) blocks; unclear for deeper models.

## Foundational Learning

- **Concept: Hessian matrix and second-order Taylor expansion**
  - **Why needed here:** The method approximates how loss changes when multiple blocks are removed simultaneously by computing curvature (Hessian) rather than just individual sensitivities.
  - **Quick check question:** Given f(x), what does the term (1/2)δα^T H_0 δα represent in a Taylor expansion around α_0?

- **Concept: Ising model and energy minimization**
  - **Why needed here:** The binary pruning problem is mapped to finding low-energy spin configurations, where block removal decisions are spins and Hessian entries are couplings.
  - **Quick check question:** In an Ising model with Hamiltonian E = Σ J_ij s_i s_j, what configuration minimizes energy if all J_ij > 0?

- **Concept: Quadratic Unconstrained Binary Optimization (QUBO)**
  - **Why needed here:** The constrained problem (remove exactly M blocks) can be reformulated as QUBO via penalty terms, enabling use of classical/quantum solvers.
  - **Quick check question:** How would you convert a constraint Σx_i = M into a penalty term added to the objective?

## Architecture Onboarding

- **Component map:** Hessian computation → CBO formulation → Solver execution → State inspection → Block removal → Distillation retraining
- **Critical path:** Hessian computation → CBO formulation → Solver execution → State inspection → Block removal → Distillation retraining
- **Design tradeoffs:**
  - Calibration data size vs. compute: 2048 samples used; larger may improve Hessian quality but increases gradient-pass cost.
  - Exact vs. approximate solver: Brute-force exact for N ~ 40, M ~ 16 is tractable (C(40,16) ~ 6B, but with fixed magnetization and pruning, minutes to hours on CPU). Approximate solvers (Gurobi, quantum) scale but may miss excited states.
  - Ground state vs. excited-state selection: Ground state is default; excited states require manual inspection or automated diversity scoring (not implemented).
- **Failure signatures:**
  - **Near-degenerate energy rankings:** Numerical rounding may reorder states; treat similar-energy configurations as equivalent.
  - **Catastrophic benchmark drops:** If calibration data underrepresents benchmark domains (e.g., math for GSM8K), energy-performance correlation fails. Expand calibration diversity.
  - **Non-convergent retraining:** After aggressive pruning (>40%), distillation may not recover performance—reduce compression or extend retraining epochs.
- **First 3 experiments:**
  1. **Validation on small model:** Take Llama-3.1-8B-Instruct, compute Hessian with 2048 OpenHermes samples, solve CBO for M=8, compare ground state vs. top-5 excited states on MMLU. Verify excited-state advantage.
  2. **Calibration sensitivity:** Repeat M=16 removal with different calibration datasets (e.g., mixed domain vs. single domain). Measure correlation between energy rank and benchmark score.
  3. **Architecture transfer:** Apply to a non-dense model (e.g., MoE with 20+ blocks), remove 2-3 MoE-only layers without retraining, evaluate on GPQA/AIME. Test generality claim from NVIDIA-Nemotron results.

## Open Questions the Paper Calls Out

- **Question:** Can the CBO framework be effectively integrated with layer-merging techniques to improve performance retention over simple block removal?
  - **Basis in paper:** [explicit] The authors state in Section 4.2 that the method is "in principle, compatible with layer-merging approaches," but "exploring how these techniques can be effectively combined is left for future work."
  - **Why unresolved:** The current study evaluates CBO against merging methods like Sliding Window Merging (SWM) as separate baselines, but does not test a hybrid approach where CBO selects blocks specifically for merging rather than removal.
  - **What evidence would resolve it:** Experiments applying CBO to identify optimal blocks for subsequent merging, showing benchmark performance superior to removal-only or merging-only strategies.

- **Question:** Can automated metrics (e.g., Pearson coefficient) reliably identify the optimal pruning configurations from the low-energy spectrum without manual inspection?
  - **Basis in paper:** [explicit] The Conclusion notes that practitioners currently must "manually inspect" solutions and suggests introducing "similarity or diversity scores... to automatically select promising candidate configurations" as future work.
  - **Why unresolved:** The paper demonstrates that excited states (e.g., the 17th state) often outperform the ground state, but identifying these states currently requires manual analysis of the specific blocks removed.
  - **What evidence would resolve it:** A study demonstrating a statistical metric that correlates strongly with downstream task accuracy, successfully automating the selection of high-performing excited states across different model architectures.

- **Question:** Does the CBO method retain its performance advantages when applied in conjunction with orthogonal compression techniques such as quantization or width pruning?
  - **Basis in paper:** [explicit] The Conclusion states the method is "largely orthogonal to existing pruning approaches" and could be combined with other schemes, which the authors "leave for future work."
  - **Why unresolved:** The presented experiments isolate block removal to compare against state-of-the-art depth-pruning baselines, leaving the interaction with width-pruning or quantization unexplored.
  - **What evidence would resolve it:** Evaluations of models compressed using CBO followed by 4-bit quantization or structured width pruning, confirming that the knowledge retention benefits of CBO persist through additional compression steps.

## Limitations
- Hessian approximation may break down for extremely aggressive pruning (>50%) or architectures with strong non-linear interactions between blocks
- Performance depends on calibration data representativeness; poor domain coverage may weaken energy-performance correlation
- Excited-state selection currently requires manual inspection, limiting scalability to larger models or automated deployment
- Computational scaling challenges for models with hundreds of blocks due to brute-force solver limitations

## Confidence
- **High confidence:** The core CBO formulation, Hessian computation methodology, and basic energy-performance correlation are well-established and validated across multiple models and benchmarks
- **Medium confidence:** The excited-state observation and the claim that low-energy states are strong proxies for downstream performance are supported by experiments but rely on approximations
- **Low confidence:** The generalizability claim to heterogeneous architectures like mixture-of-experts models is based on a single example without systematic testing across different MoE architectures

## Next Checks
1. **Calibration data sensitivity study:** Systematically vary calibration dataset size (128, 512, 2048, 8192 samples) and domain diversity, then measure correlation between energy rank and benchmark performance across MMLU, GSM8K, and Winogrande
2. **Excited-state selection automation:** Implement an automated diversity metric (e.g., block coverage, early/late block distribution) to rank excited states without manual inspection and test whether this automated selection matches or exceeds the manual approach
3. **Extreme compression boundary:** Apply the method to Llama-3.1-8B with M=24-28 (75-87.5% parameter reduction) and evaluate whether the Hessian approximation remains predictive compared to random and consecutive block removal baselines