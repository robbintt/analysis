---
ver: rpa2
title: Uncovering Cross-Linguistic Disparities in LLMs using Sparse Autoencoders
arxiv_id: '2507.18918'
source_url: https://arxiv.org/abs/2507.18918
tags:
- activation
- languages
- across
- layer
- layers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study reveals that multilingual large language models exhibit
  systematic activation disparities across languages, with medium-to-low resource
  languages receiving significantly lower activations compared to high-resource languages.
  Using Sparse Autoencoders (SAEs) on Gemma-2-2B, the research analyzes activation
  patterns across 26 layers and 10 languages, showing up to 26.27% lower activations
  in early layers for medium-to-low resource languages, with a persistent gap of 19.89%
  in deeper layers.
---

# Uncovering Cross-Linguistic Disparities in LLMs using Sparse Autoencoders
## Quick Facts
- **arXiv ID:** 2507.18918
- **Source URL:** https://arxiv.org/abs/2507.18918
- **Reference count:** 29
- **Primary result:** Multilingual LLMs show systematic activation disparities, with medium-to-low resource languages receiving up to 26.27% lower activations in early layers compared to high-resource languages.

## Executive Summary
This study reveals systematic cross-linguistic activation disparities in multilingual large language models, where medium-to-low resource languages receive significantly lower activations compared to high-resource languages. Using Sparse Autoencoders on Gemma-2-2B across 26 layers and 10 languages, the research demonstrates up to 26.27% lower activations in early layers for medium-to-low resource languages, with a persistent gap of 19.89% in deeper layers. To address this, activation-aware fine-tuning via LoRA was applied, leading to substantial activation gains (e.g., 87.69% for Malayalam, 86.32% for Hindi) while maintaining English retention at approximately 91%. Post-fine-tuning benchmark results showed modest but consistent improvements, highlighting activation alignment as a key factor in enhancing multilingual LLM performance.

## Method Summary
The study uses Sparse Autoencoders (SAEs) to analyze activation patterns across 26 layers of Gemma-2-2B for 10 languages. Researchers extracted English phrases from Neuronpedia with high activations (≥80% of maximum), translated them using Helsinki-NLP models into 9 target languages, and computed mean activations per feature index. Activation disparities were quantified as percentage gaps between high-resource and medium-to-low resource languages. To mitigate disparities, activation-aware fine-tuning was performed using LoRA with a loss function combining L1 distance between language activations and L2 regularization to preserve English retention. The fine-tuning targeted layer 20 and all layers below, using 4,000 samples across 2 iterations.

## Key Results
- Medium-to-low resource languages showed up to 26.27% lower activations in early layers (Layer 6) compared to high-resource languages
- Activation gap persisted in deeper layers, with 19.89% difference observed at Layer 25
- LoRA fine-tuning achieved substantial activation gains: 87.69% for Malayalam, 86.32% for Hindi, 75.49% for Marathi
- English retention remained high at ~91% post-fine-tuning
- Benchmark improvements were modest: ARC-C +0.16%, HellaSwag +1.62%, MMLU -0.22% for Malayalam post-fine-tuning

## Why This Works (Mechanism)
The activation disparity mechanism operates through layer-wise differential encoding where high-resource languages receive stronger foundational representations in early layers. Sparse Autoencoders decompose dense activations into interpretable features, revealing that medium-to-low resource languages have systematically weaker feature activations. LoRA fine-tuning addresses this by aligning activation distributions between languages through parameter-efficient adaptation, specifically targeting the layers where disparities originate (layers 0-20) while preserving higher-level generation capabilities in later layers.

## Foundational Learning
- **Concept: Sparse Autoencoders (SAEs) and Mechanistic Interpretability**
  - Why needed here: The core contribution relies on SAEs to decompose dense, polysemantic vectors into sparse, interpretable features, making "activation disparity" measurable
  - Quick check question: Can you explain why looking at an SAE's latent features provides more insight than simply measuring the cosine similarity of the model's raw residual stream embeddings?

- **Concept: Residual Stream and Layer-wise Processing**
  - Why needed here: The 26-layer analysis shows different activation gaps across depths, requiring understanding that early layers handle foundational encoding while later layers build higher-level representations
  - Quick check question: Why might a larger activation gap in early layers (e.g., layer 6) suggest a problem with foundational encoding for a language, compared to a gap in a much deeper layer?

- **Concept: Parameter-Efficient Fine-Tuning (PEFT) with LoRA**
  - Why needed here: The mitigation strategy uses LoRA to efficiently align activations without updating all model weights, enabling English retention while improving low-resource language performance
  - Quick check question: How does the use of LoRA align with the paper's goal of improving multilingual performance while maintaining English retention (~91%)?

## Architecture Onboarding
- **Component map:** Neuronpedia dataset (English phrases) -> Helsinki-NLP translation (to 9 languages) -> Gemma Scope (SAE activation extraction) -> LoRA fine-tuning loop (activation alignment) -> Okapi framework (benchmark evaluation)

- **Critical path:** The most critical path is the fine-tuning data preparation and loss definition. The effectiveness of the entire mitigation strategy hinges on the quality of the translated phrases and the precise formulation of the alignment loss function (`|ul - vl| + α · ||ul - uorig,l||²`). An error here could align activations to noise or cause catastrophic forgetting of English.

- **Design tradeoffs:**
  - Translation Quality vs. Scale: Using Helsinki-NLP models for large-scale translation may introduce errors, especially for low-resource languages, potentially misaligning activation patterns during fine-tuning
  - Activation Gain vs. Benchmark Improvement: The paper shows massive gains in activation (e.g., 87% for Malayalam) but only "modest" gains in downstream benchmark accuracy
  - Layer Targeting: Fine-tuning targets layer 20 and all layers below to avoid disrupting text generation in final layers while attempting to fix foundational representations

- **Failure signatures:**
  - Catastrophic Forgetting: If regularization term (α) is too weak, English performance could collapse below 90% retention
  - Superficial Alignment: The model could learn to align activations without learning meaningful semantic equivalence
  - SAE Artifacts: If SAEs learn composed features rather than "true" features, the entire disparity signal could be an artifact of the analysis tool

- **First 3 experiments:**
  1. Correlation Analysis: Compute Pearson correlation coefficient between "activation gap" and "benchmark accuracy" across all tested languages
  2. Ablation on α: Run fine-tuning with varying values for the English retention regularization weight (α) to find optimal balance
  3. Layer-Specific Fine-Tuning: Fine-tune only layer with highest disparity (Layer 6) vs. only a deep layer (Layer 25) to test early-layer alignment importance

## Open Questions the Paper Calls Out
- How can activation-aware fine-tuning be refined to ensure activation gains translate into consistent performance improvements across all downstream benchmarks?
- Do systematic cross-linguistic activation disparities exist in other LLM architectures, or are they unique to Gemma-2-2B?
- To what extent are the observed activation disparities intrinsic to the model versus artifacts of translation errors in dataset construction?

## Limitations
- Translation quality uncertainty from Helsinki-NLP models may confound activation disparity measurements and fine-tuning effectiveness
- SAE feature interpretability not validated, raising questions about whether identified features are truly "atomic" or composed representations
- Substantial activation alignment gains did not translate to proportional benchmark improvements, with MMLU score dropping for Malayalam post-fine-tuning

## Confidence
- **High Confidence:** Activation disparity findings are robust and well-demonstrated across multiple layers and languages
- **Medium Confidence:** LoRA fine-tuning methodology is reasonably documented but missing hyperparameters create reproducibility uncertainty
- **Low Confidence:** Claim that activation alignment translates to improved multilingual reasoning is weakly supported by modest and inconsistent benchmark improvements

## Next Checks
1. Conduct human evaluation of translated phrases (particularly for low-resource languages) to quantify translation errors and their impact on activation disparity measurements
2. Design experiment to decouple activation alignment from performance improvement by testing whether artificially aligning activations produces similar benchmark gains
3. Apply the same SAE analysis and LoRA fine-tuning approach to a different multilingual LLM (e.g., LLaMA 2 or Mistral) to test generalization beyond Gemma-2-2B