---
ver: rpa2
title: 'LongPerceptualThoughts: Distilling System-2 Reasoning for System-1 Perception'
arxiv_id: '2504.15362'
source_url: https://arxiv.org/abs/2504.15362
tags:
- reasoning
- answer
- tasks
- wang
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LongPerceptualThoughts, a synthetic dataset
  of 30K long-chain-of-thought (CoT) traces for perceptual tasks. The authors propose
  a three-stage framework that first generates verifiable multiple-choice questions
  from dense image descriptions, then extracts simple CoTs from vision-language models,
  and finally expands these into elaborate long CoTs using frontier reasoning models.
---

# LongPerceptualThoughts: Distilling System-2 Reasoning for System-1 Perception

## Quick Facts
- **arXiv ID:** 2504.15362
- **Source URL:** https://arxiv.org/abs/2504.15362
- **Reference count:** 40
- **Primary result:** +3.4 average point improvement on 5 vision benchmarks; +11.8 points on V*Bench

## Executive Summary
This paper introduces LongPerceptualThoughts, a synthetic dataset of 30K long-chain-of-thought traces for vision-centric perceptual tasks. The authors propose a three-stage framework that first generates verifiable multiple-choice questions from dense image descriptions, then extracts simple CoTs from vision-language models, and finally expands these into elaborate long CoTs using frontier reasoning models. Fine-tuning a 7B vision-language model on this data yields significant improvements on vision benchmarks and surprisingly also improves text reasoning performance on MMLU-Pro by +2 points.

## Method Summary
The approach synthesizes long CoTs through a three-stage pipeline: (1) gpt-4o-mini generates multiple-choice questions from dense image captions, (2) a 7B VLM produces simple CoTs for these questions, and (3) a 32B reasoning model expands these into elaborate traces using "Wait," cues. The resulting dataset is used to fine-tune a 7B VLM with both supervised fine-tuning (SFT) and direct preference optimization (DPO). The DPO component uses pairwise preference data constructed from correct/incorrect reasoning paths to improve credit assignment and self-correction capabilities.

## Key Results
- Average +3.4 points improvement across 5 vision-centric benchmarks
- +11.8 points improvement on V*Bench specifically
- +2 points improvement on MMLU-Pro text reasoning benchmark
- DPO provides +1.9 points additional gain over SFT alone

## Why This Works (Mechanism)

### Mechanism 1: Distribution-Aligned Thought Expansion
Preconditioning a reasoning model with VLM-generated reasoning traces before expansion keeps outputs within the student VLM's output distribution, improving learnability. Naive distillation from large reasoning models creates outputs that deviate from the student's distribution, degrading performance.

### Mechanism 2: Preference Data for Credit Assignment
Pairwise preference pairs (z⁻₁ ⊕ z⁺₂, a⁺₂) ≻ (z⁻₁, a⁻₁) teach the model to recover from incorrect initial reasoning, improving credit assignment for self-correction behaviors. This addresses noisy token issues in long CoT traces.

### Mechanism 3: Verifiable MCQ Scaffolding Removes Process Verification Requirement
Generating multiple-choice questions from dense captions enables automatic correctness verification without building perceptual process verifiers. The MCQ format allows binary correctness determination by comparing predicted answers to ground truth.

## Foundational Learning

- **Chain-of-Thought (CoT) Reasoning**:
  - Why needed here: The entire method builds on generating, expanding, and fine-tuning with CoT traces
  - Quick check question: Can you explain why "Wait," cues might elicit longer reasoning traces?

- **Direct Preference Optimization (DPO)**:
  - Why needed here: The +1.9 point gain from adding DPO to SFT is central to the method's effectiveness
  - Quick check question: How does DPO differ from reinforcement learning with a reward model?

- **Distillation and the Learnability Gap**:
  - Why needed here: Understanding why Virgo and VLAA-thinking hurt performance despite being distilled from stronger models is essential context
  - Quick check question: Why might smaller models struggle to learn from much larger teacher models?

## Architecture Onboarding

- **Component map**: Dense captions → gpt-4o-mini → MCQs → Qwen2.5-VL-7B-Instruct → Simple CoTs → DeepSeek-R1-Distill-Qwen-32B → Long CoTs → SFT/DPO → Fine-tuned 7B VLM

- **Critical path**: Stage 2 simple CoT quality directly limits Stage 3 expansion quality. If MVLM produces incoherent traces, MReason has poor scaffolding.

- **Design tradeoffs**:
  - Using same model family for MVLM and MReason (both Qwen-derived) improves distribution alignment but limits reasoning capability
  - Dense caption dependency (DOCCI/DCI) enables MCQ verifiability but constrains image domain coverage
  - Filtering "bad words" (e.g., "description") reduces hallucination leakage but may discard valid reasoning

- **Failure signatures**:
  - SFT-only shows marginal gains → DPO likely needed for noisy token mitigation
  - Performance drops on non-vision benchmarks → likely distribution shift from training data domain
  - Model overthinks (excessive tokens without accuracy gain) → check if compactness preference pairs are over-weighted

- **First 3 experiments**:
  1. Reproduce Table 1 with held-out images from DOCCI to verify dataset quality dependency
  2. Ablate the "Wait," cue set to measure expansion trigger sensitivity
  3. Replace MReason with a smaller reasoning model to quantify the teacher-student capacity gap tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
Can long CoT reasoning for perceptual tasks be generated effectively without relying on frontier reasoning models for thought expansion? The dependency on large reasoning models (32B+) raises cost and accessibility concerns; no smaller-scale alternatives are explored.

### Open Question 2
Why does fine-tuning on vision-centric long CoT data improve performance on text-only reasoning benchmarks? The authors offer two hypotheses but neither is empirically tested; the cross-modal transfer mechanism remains unexplained.

### Open Question 3
How can reliable process verification be constructed for perceptual tasks in long CoT reasoning? The authors identify this as a key challenge and position their data-centric method as "a practical alternative" that sidesteps verification entirely.

### Open Question 4
Does the LongPerceptualThoughts approach scale to open-ended perceptual tasks beyond multiple-choice questions? The MCQ format enables easy correctness identification but limits applicability to real-world perceptual reasoning.

## Limitations
- Heavy dependency on DOCCI dense captions - if these contain hallucinations, the entire pipeline propagates errors
- Three-stage pipeline effectiveness hinges on unverified assumption that expanding from simple CoTs preserves distributional alignment
- Potential overfitting concerns as training set uses DOCCI data, which CVBench also uses

## Confidence

- **High confidence**: The three-stage synthesis pipeline works as described; DPO provides measurable benefits over SFT
- **Medium confidence**: Distribution alignment mechanism is the primary driver of effectiveness; improvements generalize beyond training distribution
- **Low confidence**: MMLU-Pro improvements indicate genuine cross-domain transfer; the specific choice of markers and hyperparameters is optimal

## Next Checks

1. **Dataset Dependency Test**: Train the model on a held-out subset of DOCCI images not used in synthesis, then evaluate on CVBench and V*Bench. This would validate whether improvements are due to overfitting versus genuine capability gains.

2. **Distribution Alignment Ablation**: Compare fine-tuning results using (a) Stage 2 simple CoTs expanded by MReason, (b) direct distillation from MReason without preconditioning, and (c) synthetic CoTs generated entirely by MVLM. This would quantify the actual benefit of the distribution alignment approach.

3. **Cross-Domain Transfer Validation**: Test the model on additional text-only reasoning benchmarks beyond MMLU-Pro (such as GSM8K or MATH) to determine if the vision-focused training genuinely improves general reasoning or if MMLU-Pro improvements are dataset-specific.