---
ver: rpa2
title: 'GThinker: Towards General Multimodal Reasoning via Cue-Guided Rethinking'
arxiv_id: '2506.01078'
source_url: https://arxiv.org/abs/2506.01078
tags:
- reasoning
- multimodal
- visual
- arxiv
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of advancing multimodal reasoning
  in large language models (MLLMs) beyond domain-specific tasks like math and science,
  extending toward more general scenarios. The authors introduce GThinker, a novel
  reasoning framework that excels across diverse multimodal tasks, including general,
  mathematical, and scientific domains.
---

# GThinker: Towards General Multimodal Reasoning via Cue-Guided Rethinking

## Quick Facts
- arXiv ID: 2506.01078
- Source URL: https://arxiv.org/abs/2506.01078
- Authors: Yufei Zhan; Ziheng Wu; Yousong Zhu; Rongkun Xue; Ruipu Luo; Zhenghao Chen; Can Zhang; Yifan Li; Zhentao He; Zheming Yang; Ming Tang; Minghui Qiu; Jinqiao Wang
- Reference count: 40
- Primary result: Achieves 81.5% on M³CoT benchmark, surpassing O4-mini, with 2.1% average improvement on general multimodal reasoning tasks

## Executive Summary
GThinker introduces a novel Cue-Rethinking Pattern that enables flexible multimodal reasoning by decoupling initial reasoning from structured constraints and deferring visual verification to a triggered rethinking phase. The framework addresses the challenge of advancing multimodal reasoning beyond domain-specific tasks through a two-stage pipeline combining Pattern-Guided Cold Start and Incentive Reinforcement Learning. Extensive experiments demonstrate GThinker's superiority over existing reasoning MLLMs across diverse benchmarks, achieving strong performance in general, mathematical, and scientific reasoning domains while maintaining cross-domain adaptability.

## Method Summary
GThinker employs a two-stage training pipeline to teach flexible multimodal reasoning. The first stage, Pattern-Guided Cold Start, uses selective formatting during supervised fine-tuning to teach the Cue-Rethinking Pattern: samples with flawed visual cues receive complete three-stage sequences (initial reasoning → trigger → rethinking), while others use free-form reasoning. The second stage, Incentive Reinforcement Learning, employs DAPO with hybrid rewards to encourage diverse reasoning strategies beyond the supervised distribution. The approach builds on Qwen2.5-VL-7B and is trained on GThinker-11K, a dataset constructed through multimodal iterative annotation involving multiple frontier models.

## Key Results
- Achieves 81.5% accuracy on comprehensive M³CoT benchmark, surpassing O4-mini
- Shows 2.1% average improvement on general scenario multimodal reasoning benchmarks
- Maintains on-par mathematical reasoning performance compared to advanced reasoning models
- Demonstrates strong cross-domain adaptability across general, math, and science domains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Cue-Rethinking Pattern enables flexible, visually-grounded reasoning by decoupling initial reasoning from structured constraints and deferring visual verification to a triggered rethinking phase.
- Mechanism: The model first generates free-form reasoning while tagging visual cues (`<vcues_*>...</vcues_*>`). After completing initial reasoning, a prompt triggers cue-based rethinking: "Let's check each visual cue and corresponding reasoning before reaching the final answer." The model revisits tagged cues, identifies inconsistencies, and revises reasoning before concluding.
- Core assumption: Deferring rethinking until after initial reasoning completes preserves natural reasoning flow and allows holistic context assessment, rather than interrupting with immediate verification.
- Evidence anchors: [abstract] "GThinker introduces Cue-Rethinking, a flexible reasoning pattern that grounds inferences in visual cues and iteratively reinterprets these cues to resolve inconsistencies." [section 3.1] "This flexibility allows the model to apply learned reasoning strategies, such as step-by-step deduction or logical and knowledge reflection... depending on the task."

### Mechanism 2
- Claim: Pattern-Guided Cold Start with selective formatting teaches domain-adaptive reasoning strategies, where samples with flawed visual cues receive full rethinking sequences while others use free-form reasoning.
- Mechanism: The base model runs on training questions; outputs are compared to annotations. Samples with incorrect visual cues are formatted with complete three-stage Cue-Rethinking sequences. Remaining samples use free-form reasoning only. This selective formatting prevents over-constraining the model to rigid patterns while ensuring it learns rethinking when needed.
- Core assumption: Mixing rethinking-enabled and rethinking-disabled formats during SFT improves robustness compared to enforcing a single format across all samples.
- Evidence anchors: [section 3.2] "Samples with flawed visual cues are selected to form full Cue-Rethinking sequences, covering all three stages. Remaining examples are formatted as free-form reasoning paths." [table 4] Ablation shows removing PGS Formatting drops overall performance from 81.5% to 68.4%.

### Mechanism 3
- Claim: Incentive Reinforcement Learning with DAPO and hybrid rewards generalizes reasoning capabilities by encouraging exploration of diverse reasoning strategies beyond the supervised cold-start distribution.
- Mechanism: After cold start, DAPO with clip-higher and token-level loss encourages sampling diverse reasoning paths. A hybrid reward supports multiple-choice, math, and open-ended questions. This enables the model to discover strategies that weren't explicitly in the SFT data.
- Core assumption: Outcome-based rewards alone are sufficient to reinforce multi-strategy reasoning without process-level supervision.
- Evidence anchors: [section 3.3] "By incorporating DAPO, especially its clip-higher mechanism and token-level loss, the model is better equipped to sample diverse reasoning paths." [table 4] Ablation shows removing Incentive RL drops overall performance from 81.5% to 73.6%.

## Foundational Learning

- Concept: Chain-of-Thought (CoT) Reasoning
  - Why needed here: The Cue-Rethinking Pattern extends CoT by adding visual cue tagging and deferred verification. Understanding basic CoT is prerequisite to appreciating why free-form reasoning with optional rethinking improves over fixed-structure CoT.
  - Quick check question: Can you explain why enforcing a rigid CoT structure might fail on diverse general-scenario questions compared to math/science tasks?

- Concept: Outcome-Reward Reinforcement Learning (GRPO/DAPO family)
  - Why needed here: The Incentive RL stage uses DAPO, which builds on GRPO with clip-higher, dynamic sampling, and token-level loss. Understanding outcome-based rewards vs. process rewards is essential.
  - Quick check question: What is the key difference between GRPO and PPO in how they handle group-based reward estimation?

- Concept: Visual Grounding in MLLMs
  - Why needed here: The Cue-Rethinking Pattern explicitly requires tagging and revisiting visual cues. Understanding visual grounding is critical for implementing the `<vcues_*>` mechanism.
  - Quick check question: How would you detect if an MLLM's visual cue references are hallucinated vs. grounded in actual image content?

## Architecture Onboarding

- Component map: Qwen2.5-VL-7B -> Pattern-Guided Cold Start (SFT) -> Incentive RL (DAPO) -> GThinker
- Critical path: 1) Data preparation: Filter M³CoT-derived data for visual dependency and reasoning complexity (7,358 samples) 2) Annotation: Run iterative pipeline with O-series models to generate reasoning paths with visual cue tags 3) Cold start SFT: Train with selective formatting 4) RL data curation: Cluster and select 4K diverse samples 5) Incentive RL: Train with DAPO, hybrid reward, monitor format and accuracy rewards
- Design tradeoffs: Free-form vs. structured reasoning (flexibility vs. simplicity), SFT data scale vs. quality (smaller high-quality vs. larger noisier), RL exploration vs. stability (DAPO's clip-higher encourages exploration but may increase variance)
- Failure signatures: Repeated content generation (reward hacking), missing rethinking trigger (cold-start formatting error), visual cue hallucination (insufficient grounding capability)
- First 3 experiments: 1) Validate cold start alone: Train on 7K SFT data without RL, evaluate on M³CoT 2) Ablate iterative annotation: Compare using single-model vs. full iterative pipeline 3) Test reward sensitivity: Run RL stage with different reward weights

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would the Cue-Rethinking pattern generalize effectively to larger model scales (e.g., 70B+) or different architectures beyond Qwen2.5-VL?
- Basis in paper: The paper only implements GThinker on Qwen2.5-VL-7B (Section 4.1), without testing on other architectures or scales.
- Why unresolved: No experiments or discussion on whether the two-stage training pipeline and Cue-Rethinking pattern transfer to larger models or different MLLM families.
- What evidence would resolve it: Implementing GThinker on alternative base models (e.g., InternVL, LLaVA) and at larger scales, then evaluating on M³CoT and other benchmarks.

### Open Question 2
- Question: What causes the 2.7% performance drop in commonsense reasoning when Pattern-Guided Selection (PGS) Formatting is applied, and can this trade-off be mitigated?
- Basis in paper: Table 4 ablation shows that adding PGS Formatting improves science (+5.1%) and math (+4.2%) but decreases commonsense performance by 2.7%.
- Why unresolved: The authors attribute this to "variability due to the diversity and ambiguity of the cues" but do not propose or test solutions to recover commonsense performance.
- What evidence would resolve it: Ablation studies varying the PGS formatting threshold or incorporating domain-specific formatting rules, with separate reporting of commonsense metrics.

### Open Question 3
- Question: Can the hybrid reward design be extended to support complex open-ended visual reasoning tasks beyond multiple-choice, math, and simple short-form answers?
- Basis in paper: Section 3.3 states the hybrid reward "supports three main question types" and uses exact matching or Math-Verify, acknowledging limitations for "open-ended questions that yield concise responses."
- Why unresolved: General multimodal reasoning often involves longer, free-form explanations where verifiable rewards are difficult to define.
- What evidence would resolve it: Developing and evaluating model-based or human-aligned reward functions for open-ended reasoning tasks on benchmarks like Sherlock or other explanation-focused datasets.

## Limitations
- Limited testing on real-world unstructured multimodal inputs where visual cues may be ambiguous or conflicting
- Computationally intensive data construction process requiring access to multiple frontier models
- Lack of ablation studies on actual reasoning quality versus traditional CoT approaches
- Performance focus on curated benchmarks rather than completely unstructured multimodal reasoning tasks

## Confidence
- **High Confidence** in two-stage training pipeline effectiveness: Ablations clearly demonstrate significant performance drops when removing either stage
- **Medium Confidence** in Cue-Rethinking Pattern's mechanism: Performance gains shown but reasoning quality verification lacking
- **Low Confidence** in generalizability beyond curated benchmarks: Experiments focus on controlled benchmarks without real-world deployment testing

## Next Checks
1. **Real-world deployment test**: Evaluate GThinker on real-world images with open-ended questions collected outside curated benchmarks, measuring both accuracy and reasoning quality through human evaluation
2. **Ablation on reasoning quality**: Compare actual reasoning paths generated by GThinker versus traditional CoT approaches on a subset of problems, using automated logic verification and human expert review
3. **Scalability stress test**: Evaluate performance degradation as visual cues increase from 1-2 to 5-10, identifying the breaking point where Cue-Rethinking Pattern becomes ineffective