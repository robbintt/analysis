---
ver: rpa2
title: 'FedSAUC: A Similarity-Aware Update Control for Communication-Efficient Federated
  Learning in Edge Computing'
arxiv_id: '2504.04867'
source_url: https://arxiv.org/abs/2504.04867
tags:
- devices
- learning
- round
- training
- update
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FedSAUC, a similarity-aware update control
  mechanism for federated learning that reduces communication and energy costs in
  edge computing environments. The method uses clustering algorithms (K-means, Agglomerative,
  and Spectral Clustering) to group devices with similar model parameters based on
  their gradient updates.
---

# FedSAUC: A Similarity-Aware Update Control for Communication-Efficient Federated Learning in Edge Computing

## Quick Facts
- arXiv ID: 2504.04867
- Source URL: https://arxiv.org/abs/2504.04867
- Reference count: 17
- One-line primary result: FedSAUC achieves up to 50% communication reduction in federated learning while maintaining comparable model accuracy through gradient-based clustering and selective device participation.

## Executive Summary
FedSAUC introduces a similarity-aware update control mechanism that reduces communication and energy costs in federated learning by clustering devices based on their gradient updates and probabilistically selecting representatives for model aggregation. The method groups devices with similar model parameters using K-means, Agglomerative, or Spectral Clustering algorithms, then randomly selects a subset of devices from each cluster to update their models to the server while others enter power-saving mode. Experiments on a testbed using Raspberry Pi 4 and NVIDIA Jetson Nano devices demonstrate that gradient-based clustering achieves higher accuracy than coefficient-based clustering, with up to 50% reduction in message sizes while maintaining similar model performance. The approach shows particular effectiveness when activated after initial training rounds, allowing the model to converge before participation reduction.

## Method Summary
The FedSAUC method implements a similarity-aware update control mechanism for federated learning that uses clustering algorithms to group devices with similar model parameters based on their gradient updates. For each cluster, only a random subset of devices (selected with probability τ) is chosen to update their models to the server, while others enter power-saving mode. The approach was implemented on a testbed using Raspberry Pi 4 and NVIDIA Jetson Nano devices, testing three clustering algorithms (K-means, Agglomerative, and Spectral Clustering) with four different update types (WW, WBW, WG, WBG). The method processes MNIST data partitioned non-IID across 10 clients, using a linear regression model with 7840 parameters. The system achieves communication efficiency by reducing the number of model updates transmitted while maintaining model accuracy through strategic clustering and selective participation.

## Key Results
- Gradient-based clustering (WG/WBG) achieves 72.6-83.7% clustering accuracy compared to 62.6-78.6% for coefficient-based clustering
- Update control mechanism reduces message sizes by up to 50% (from 17.50MB to 4.38MB on server side, 3.50MB to 0.88MB on client side)
- Model accuracy decreases by approximately 10% when applying FedSAUC after 10 rounds but converges to baseline levels by round 50
- Spectral clustering achieves the highest accuracy (83.3-83.7%) among the three clustering algorithms tested

## Why This Works (Mechanism)

### Mechanism 1: Gradient-Based Similarity Clustering
Clustering devices using gradient updates achieves higher grouping accuracy than clustering on model coefficients because gradient updates capture the optimization direction informed by each device's local data distribution. Devices with similar data produce similar gradient vectors, enabling clustering algorithms to partition devices into groups where members share redundant information content. This works under the assumption that devices training on similar data distributions compute similar gradient updates under the same optimization process. The evidence shows WG (gradient updates of coefficients) achieves 72.6-83.3% clustering accuracy across algorithms versus WW (coefficients only) at 62.6-78.6%.

### Mechanism 2: Probabilistic Representative Selection
Random selection of representatives within each cluster preserves long-term model accuracy while reducing communication overhead. Each device independently determines representative status with probability τ per round, allowing non-representatives to enter power-saving mode and skip local training and transmission. This distributes energy burden across cluster members over time while maintaining coverage of the data distribution space. The core assumption is that devices within a cluster provide approximately redundant gradient information, so partial participation does not systematically bias the global model. Experimental results show models starting FedSAUC at round 10 show ~10% accuracy drop but converge to baseline levels by round 50.

### Mechanism 3: Delayed Activation for Convergence Preservation
Activating update control after initial training rounds reduces early-phase accuracy degradation. Early training rounds establish the model's optimization trajectory, and by delaying FedSAUC activation (e.g., after round 10 vs. round 40), the model reaches a more mature state before participation reduction, limiting the divergence caused by partial updates. The core assumption is that early-round gradient contributions are more critical for establishing model direction than later-round refinements. Evidence shows starting at round 10 causes largest accuracy dip, while starting at round 40 shows minimal deviation from baseline.

## Foundational Learning

- **Federated Learning Aggregation (FedAvg)**: Understanding that global model θ is a weighted average of local models θi clarifies what information is preserved when selecting subset κ. Quick check: If only 50% of devices in a cluster upload their models, how does the server compute the global model for that cluster's contribution?

- **Gradient vs. Model Coefficient Clustering**: The paper's key finding is that gradients (optimization direction) cluster better than coefficients (current state). This distinction matters for implementation choices. Quick check: Why would two devices with different initial models but similar data produce similar gradients but different coefficients?

- **Spectral Clustering on Graph-Structured Data**: Spectral clustering achieved the highest accuracy (83.7% on Training Set 1). Understanding its graph-partition formulation helps debug clustering failures. Quick check: What does it mean when spectral clustering fails to separate devices that train on different digit classes?

## Architecture Onboarding

- **Component map**: Client-side: Local training loop → gradient computation → coefficient/gradient extraction → probabilistic participation decision (coin flip with probability τ) → conditional model upload. Server-side: Gradient collection G={g1,...,gn} → clustering algorithm (K-means/Agglomerative/Spectral) → cluster assignment broadcast → selective aggregation from representatives only → global model broadcast

- **Critical path**: 1) Verify gradient extraction matches clustering input format (flattened vectors). 2) Confirm clustering runs on server before selection decisions per round. 3) Validate that non-selected clients correctly skip training and transmission. 4) Check that aggregation weights account for reduced participation count

- **Design tradeoffs**: K-means is fastest but lowest accuracy (72.6-72.9%); Spectral is slowest but highest accuracy (83.3-83.7%). Earlier start = more communication savings but larger accuracy dip. Lower τ = more savings but higher information loss risk. Gradients (WG/WBG) cluster better but require storing previous-round model state

- **Failure signatures**: Accuracy divergence after round 50 suggests τ is too low or clusters are not capturing true similarity. Cluster instability across rounds indicates gradient noise or insufficient data separation. Uneven cluster sizes suggest clustering parameters (e.g., m) are misconfigured. Training set 1 vs. set 2 performance gap requires checking data partitioning

- **First 3 experiments**: 1) Baseline replication: Run standard FedAvg (no update control) for 50 rounds on MNIST partition to establish accuracy baseline. 2) Clustering accuracy validation: Implement all three clustering algorithms with WG update type; measure clustering accuracy against known device-data assignments. 3) Ablation on activation timing: Run FedSAUC starting at rounds 10, 20, 30, 40 with τ=0.5 and Spectral clustering; plot accuracy curves to verify Figure 6 patterns

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several important research directions emerge from the work.

## Limitations
- Limited hyperparameter specification (learning rate, optimizer choice) may affect reproducibility
- Unclear clustering frequency (per round vs. periodic) introduces ambiguity in implementation
- Real-world energy savings validation limited to synthetic testbed rather than deployed edge environments
- No analysis of scaling behavior beyond 10 devices or to different dataset characteristics

## Confidence
- **High confidence**: Core clustering accuracy results (72.6-83.7% for gradient-based clustering vs. 62.6-78.6% for coefficient-based)
- **Medium confidence**: Communication reduction claims (50% message size reduction) due to controlled testbed conditions
- **Medium confidence**: Accuracy preservation claims given convergence to baseline by round 50, though initial degradation is significant (~10%)

## Next Checks
1. Implement baseline FedAvg with identical hyperparameters to verify testbed functionality and establish control accuracy baseline
2. Run clustering accuracy validation on both Training Set 1 and Set 2 to confirm digit-class separation patterns
3. Conduct ablation study varying τ (selection probability) and activation round to identify optimal configuration boundaries