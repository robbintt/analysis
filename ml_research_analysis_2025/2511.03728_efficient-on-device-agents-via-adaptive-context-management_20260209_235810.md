---
ver: rpa2
title: Efficient On-Device Agents via Adaptive Context Management
arxiv_id: '2511.03728'
source_url: https://arxiv.org/abs/2511.03728
tags:
- tool
- user
- context
- agent
- on-device
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a context-efficient framework for on-device
  AI agents that addresses the challenge of limited memory capacity by integrating
  a dual-adapter memory system and a just-in-time tool management protocol. The dual-adapter
  system uses a lightweight State-Tracker to maintain a compressed, structured Context
  State Object that preserves essential conversational details while minimizing token
  overhead.
---

# Efficient On-Device Agents via Adaptive Context Management

## Quick Facts
- **arXiv ID:** 2511.03728
- **Source URL:** https://arxiv.org/abs/2511.03728
- **Reference count:** 40
- **One-line result:** Context-efficient on-device agents achieving >6× prompt size reduction and 10-25× context growth reduction via dual-adapter system and just-in-time schema loading

## Executive Summary
This work introduces a context-efficient framework for on-device AI agents that addresses the challenge of limited memory capacity by integrating a dual-adapter memory system and a just-in-time tool management protocol. The dual-adapter system uses a lightweight State-Tracker to maintain a compressed, structured Context State Object that preserves essential conversational details while minimizing token overhead. Combined with token-efficient tool schemas and selective schema passing, the approach dramatically reduces context growth. Evaluated on a 3B-parameter SLM, the agent achieved more than a 6-fold reduction in initial prompt size and a 10- to 25-fold reduction in context growth rate, while matching or exceeding baseline performance on complex multi-turn tasks.

## Method Summary
The method employs a dual-adapter LoRA system (119M parameters each) on a 3B SLM: an Executor adapter for task execution and a State-Tracker adapter for maintaining a compressed Context State Object (CSO). The State-Tracker uses supervised distillation to generate concise key-value updates that preserve task-critical information while discarding conversational filler. Tool schemas are minified to reduce token costs by ~40%, and a just-in-time loading mechanism passes only tool names/descriptions initially, loading full schemas only when needed. Training uses 150-200K samples from an adapted APIGen-MT pipeline with 5,000 unique tools, and evaluation measures tool calling F1/precision/recall alongside context efficiency metrics.

## Key Results
- More than 6-fold reduction in initial system prompt context size (baseline: ~3,200 tokens vs. tool-efficient: ~400 tokens)
- 10- to 25-fold reduction in context growth rate during multi-turn interactions
- F1 scores of 0.91-0.92 matching or exceeding baseline (0.88-0.89) on 406-test set tasks
- 5× reduction in Time-To-First-Token (TTFT) due to smaller initial prompts

## Why This Works (Mechanism)

### Mechanism 1: Semantic Compression via Context State Object (CSO)
A lightweight State-Tracker adapter generates concise key-value updates after each turn, appending only new information to a structured Context State Object. Verbose turns are discarded after CSO update, preventing KV cache bloat. The Executor adapter is co-trained to read this compressed state. Critical assumption: State-Tracker can reliably identify and preserve task-critical information while discarding filler. Evidence: Achieved 10-25× context growth reduction while maintaining F1 scores around 0.92.

### Mechanism 2: Just-In-Time Schema Loading
A two-stage tool-calling process (selection → execution) reduces initial prompt size by ~6×. The agent first sees only tool names and concise descriptions (~25% of baseline token cost). Upon selecting a tool, the full token-optimized schema is injected into the next observation. Critical assumption: Agent can reliably select correct tool from minimal descriptions without seeing full parameter schemas upfront. Evidence: Tool-efficient model requires ~400 tokens vs. 3,200 for baseline.

### Mechanism 3: Token-Efficient Schema Serialization
Removing non-essential whitespace and fields from OpenAPI-style schemas reduces per-tool token cost by ~40% without harming invocation accuracy. The model is fine-tuned directly on compact schemas containing only essential fields in minified JSON. Critical assumption: SLMs can parse and reason about dense JSON without formatting cues humans rely on. Evidence: Direct comparison shows 121 tokens (OpenAPI) vs. 72 tokens (efficient format).

## Foundational Learning

- **Concept: KV Cache Mechanics in Autoregressive Models** - Understanding prefill vs. decode phases explains why smaller prompts dramatically reduce TTFT. Quick check: Why does discarding ephemeral tokens after CSO update reduce memory usage without losing conversation state?
- **Concept: Parameter-Efficient Fine-Tuning (LoRA)** - Both adapters use LoRA (rank=64, α=128) on shared 3B base model, enabling simultaneous loading in limited on-device memory (~80MB overhead per adapter). Quick check: How does LoRA enable hot-swapping between Executor and State-Tracker adapters without reloading full base model?
- **Concept: Context Window vs. Usable Context in SLMs** - A 3B model may have nominal 8K-32K context window, but KV cache memory and attention degradation make large contexts impractical on-device. Quick check: Why does linear KV cache growth (O(n)) create tighter constraint than nominal context window length suggests?

## Architecture Onboarding

- **Component map:** User Input → [Executor LoRA + CSO] → Tool Selection (JIT) → [Schema Injection] → Tool Call
  - ↓
  - [State-Tracker LoRA] → CSO Update (Δ append)
  - ↓
  - KV Cache Manager (rewinds ephemeral tokens)

- **Critical path:** On startup, both adapters are primed with system prompts + initial empty CSO. Per turn: Executor processes user query + CSO → selects tool (from lightweight list) → receives full schema → generates tool call. After tool response: State-Tracker generates CSO delta → append to CSO → KV caches are rewound to permanent context + new CSO length.

- **Design tradeoffs:**
  - **Precision vs. Recall:** JIT selection enforces deliberate choice (higher precision) but may reduce recall if first choice is wrong. Memory-Efficient model is more flexible but may call irrelevant tools initially.
  - **Compression vs. Fidelity:** CSO is lossy; complex multi-step reasoning chains may not survive compression. Paper acknowledges this and recommends richer logs for error recovery scenarios.
  - **Latency overhead:** CSO update adds ~500ms per turn on Galaxy S25 CPU; optimized inference could reduce this.

- **Failure signatures:**
  - **Repetitive error loops:** Baseline models ignore tool error feedback in long contexts (attention dilution). CSO should log errors explicitly.
  - **Missing CSO entries:** If State-Tracker fails to log key detail (e.g., ticket ID), Executor will hallucinate or ask again.
  - **Selection-Execution mismatch:** JIT process may select correct tool but fail to generate correct arguments if schema is complex.

- **First 3 experiments:**
  1. **Ablate CSO compression:** Run Memory-Efficient model with full history vs. CSO on 20-turn tasks. Measure token usage, F1, and error recovery rate.
  2. **Stress-test JIT selection:** Provide tool set with highly similar descriptions (e.g., "send message" vs. "send email"). Measure selection accuracy and fallback behavior.
  3. **Profile KV cache memory:** On target device, measure peak memory for baseline vs. dual-adapter across 10, 20, 30 turns. Validate 10-25× growth reduction claim.

## Open Questions the Paper Calls Out
- Can the context-efficient framework generalize effectively to open-domain tasks beyond the on-device orchestration scenarios evaluated?
- Can Reinforcement Learning (RL) improve the adaptive decision-making capabilities of the State-Tracker compared to the current Supervised Fine-Tuning (SFT) approach?
- Does the "lossy" compression of the Context State Object (CSO) result in critical information loss in ultra-long horizons (e.g., 100+ turns) compared to the 20-30 turns evaluated?

## Limitations
- The lossy CSO compression may discard critical information in ultra-long interactions (100+ turns) where early details become important later
- Qualitative judgment scores (3.63 for memory-efficient vs. 4.00 for baseline) suggest some degradation in response quality not captured by F1 metrics
- Just-in-time tool selection introduces two-stage overhead that could frustrate users in rapid interaction scenarios

## Confidence

- **High confidence:** 6-fold reduction in initial prompt size and 10-25× reduction in context growth rate (directly measured from token-count data)
- **Medium confidence:** F1 performance matching or exceeding baseline (0.91-0.92 vs 0.88-0.89), though qualitative scores show lower quality
- **Low confidence:** Claims about handling "complex, multi-turn tasks" lack detailed breakdowns of failure modes for scenarios where lossy CSO compression might fail

## Next Checks

1. **Ablate CSO compression on 20-turn tasks:** Run Memory-Efficient model with full conversation history vs. CSO compression on complex multi-turn scenarios. Measure both token usage and F1, but also track qualitative scores and error recovery rates to identify where lossy compression fails.

2. **Stress-test JIT tool selection with ambiguous descriptions:** Create a tool set with highly similar descriptions (e.g., "send message" vs. "send email" vs. "send SMS") and measure selection accuracy, fallback behavior, and user-perceived latency from the two-stage process.

3. **Profile KV cache memory growth on target device:** On the actual Galaxy S25 or equivalent mobile device, measure peak memory usage for baseline vs. dual-adapter across 10, 20, 30 turns. Validate that the claimed 10-25× reduction in context growth rate translates to meaningful on-device memory savings under real-world constraints.