---
ver: rpa2
title: 'Beyond Top Activations: Efficient and Reliable Crowdsourced Evaluation of
  Automated Interpretability'
arxiv_id: '2506.07985'
source_url: https://arxiv.org/abs/2506.07985
tags:
- methods
- evaluation
- neuron
- siglip
- explanations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of reliably and efficiently evaluating
  automated interpretability methods for neurons in deep learning models. The core
  issue is that current crowdsourced evaluations are noisy, costly, and typically
  assess only the highest-activating inputs, leading to unreliable results.
---

# Beyond Top Activations: Efficient and Reliable Crowdsourced Evaluation of Automated Interpretability

## Quick Facts
- arXiv ID: 2506.07985
- Source URL: https://arxiv.org/abs/2506.07985
- Authors: Tuomas Oikarinen; Ge Yan; Akshay Kulkarni; Tsui-Wei Weng
- Reference count: 40
- Primary result: Reduces crowdsourced evaluation cost by ~40x using MG-IS and BRAgg

## Executive Summary
This paper addresses the challenge of efficiently and reliably evaluating automated interpretability methods for neurons in deep learning models. Current crowdsourced evaluations are noisy, expensive, and typically assess only the highest-activating inputs, leading to unreliable results. The authors introduce two key techniques: Model-Guided Importance Sampling (MG-IS) to select the most informative inputs to show raters, reducing labeling costs by approximately 13x, and Bayes Rating Aggregation (BRAgg) to handle label noise in crowd-sourced ratings, reducing the number of ratings per input needed by approximately 3x. Together, these techniques reduce the evaluation cost by approximately 40x, making large-scale evaluation feasible. The authors then use these methods to conduct a large-scale crowdsourced study comparing recent automated interpretability methods for vision networks, finding that Linear Explanations (LE) overall produces the best vision neuron explanations.

## Method Summary
The authors propose a two-pronged approach to reduce the cost of crowdsourced evaluation of neuron explanations. First, Model-Guided Importance Sampling (MG-IS) selects a small subset of highly informative images for human raters by combining SigLIP-guided sampling (80%) with uniform sampling (20%, γ=0.2). Second, Bayes Rating Aggregation (BRAgg) uses Bayesian inference to aggregate noisy crowd ratings, computing posterior probabilities of concept presence given multiple ratings and an estimated error rate of η≈23%. The final evaluation metric is Pearson correlation between neuron activations and concept presence vectors, estimated using importance sampling corrections.

## Key Results
- MG-IS reduces the number of inputs needed to reach the same evaluation accuracy by ~13x
- BRAgg reduces the number of ratings per input required to overcome noise by ~3x
- Together, these techniques reduce the evaluation cost by ~40x
- Linear Explanations (LE) produces the best vision neuron explanations, outperforming recent LLM-based methods like MAIA

## Why This Works (Mechanism)

### Mechanism 1
Selecting inputs based on model-predicted informativeness reduces labeling cost by ~13x while preserving correlation estimation accuracy. Model-Guided Importance Sampling (MG-IS) constructs a proposal distribution q(x) that prioritizes inputs where the product of normalized neuron activation and predicted concept presence is high. The sampling distribution combines SigLIP-guided sampling (80%) with uniform sampling (20%, γ=0.2) to ensure unbiased estimation. Variance minimization theory proves the optimal q*(x) ∝ |h(x)|p(x).

### Mechanism 2
Bayesian aggregation of noisy crowd ratings reduces required ratings per input by ~3x compared to majority voting. Bayes Rating Aggregation (BRAgg) computes the posterior probability P(c*|R) that a concept is truly present given multiple noisy ratings R, assuming a uniform error rate η≈23% (estimated from MTurk validation). The method supports uniform prior (β=0.05) or SigLIP-based prior.

### Mechanism 3
Pearson correlation between neuron activations and concept presence captures explanation quality more reliably than top-activation-only metrics. The correlation ρ(ak, ct) measures alignment across the full activation distribution, penalizing both missed activations (low recall) and false positive predictions (low precision). This addresses the failure mode where explanations match only high activations but are overly broad.

## Foundational Learning

- **Concept: Importance Sampling**
  - Why needed here: MG-IS relies on importance sampling theory to select informative inputs while maintaining unbiased estimation of E[h(x)].
  - Quick check question: If h(x) has high variance under uniform sampling, why does sampling from Q ∝ |h(x)|P reduce estimator variance?

- **Concept: Bayesian Inference for Latent Variables**
  - Why needed here: BRAgg requires computing posteriors over unknown concept presence given noisy observations.
  - Quick check question: Given ratings [1,1,0] with prior P(c=1)=0.1 and error rate η=0.25, why might the posterior for c=1 remain below 0.5 despite majority vote suggesting 1?

- **Concept: Correlation vs. Recall-based Metrics**
  - Why needed here: Understanding why top-activation evaluation fails requires distinguishing precision from recall.
  - Quick check question: Why would "objects" as an explanation score high on recall but low on correlation for a dog-detector neuron?

## Architecture Onboarding

- **Component map**: 1. Forward pass → neuron activations ak over dataset D
  2. SigLIP forward pass → concept predictions csiglip
  3. MG-IS sampling → subset S (180 inputs/neuron recommended)
  4. Crowd labeling → m=3 ratings per input
  5. BRAgg aggregation → soft concept labels ct
  6. Correlation estimator → final ρS score

- **Critical path**: 1. Forward pass → neuron activations ak over dataset D
  2. SigLIP forward pass → concept predictions csiglip
  3. MG-IS sampling → subset S (180 inputs/neuron recommended)
  4. Crowd labeling → m=3 ratings per input
  5. BRAgg aggregation → soft concept labels ct
  6. Correlation estimator → final ρS score

- **Design tradeoffs**:
  - γ (default 0.2): Higher = more uniform = less bias, more variance
  - Raters per input vs. inputs per neuron: Small budgets favor fewer raters; Appendix A.3 provides cost-error curves
  - Prior choice: SigLIP prior improves accuracy (~20% RCE) vs uniform prior (~25% RCE) but introduces model dependency
  - Error model: Uniform error model slightly outperforms per-input error modeling (Appendix B.4)

- **Failure signatures**:
  - High RCE with MG-IS: γ too small or SigLIP uncorrelated with ground truth → increase γ or check model quality
  - Low inter-rater agreement (Fleiss's Kappa <0.2): Rater quality issues → use BRAgg with informative prior or recruit better raters
  - Extremely low correlations for all methods: Probing dataset mismatch or polysemantic neurons → try different layers or SAE latents
  - Importance weights exploding: q(x) near zero for some samples → increase γ

- **First 3 experiments**:
  1. Validate MG-IS on ImageNet labels: Use class labels as ground truth; compare RCE of MG-IS vs uniform sampling at |S|∈{50,100,200,500}; verify ~15x efficiency gain
  2. Estimate MTurk error rate: Pilot 10 neurons with 600 inputs, 9 raters each; compute η from disagreements with ImageNet labels; verify η≈0.23
  3. Compare explanation methods: Run full pipeline on 100 neurons each for ResNet-50 layer4 and ViT-B-16 layer11; compare LE vs CLIP-Dissect vs MAIA; confirm LE superiority is statistically significant (t-test p<0.01)

## Open Questions the Paper Calls Out

### Open Question 1
How can the proposed evaluation methodology be adapted to rigorously assess "output-based" neuron explanations (neuron activation → model output) rather than the current focus on input-based explanations? The authors state in the Limitations that "Rigorously evaluating these output-based explanations will require different methodology and is an interesting problem for future work." This remains unresolved because the current study designs MG-IS and BRAgg specifically for the "input → activation" function using probing datasets, which differs fundamentally from measuring a neuron's causal effect on outputs.

### Open Question 2
Do recent LLM-based explanation methods (e.g., MAIA) outperform Linear Explanations when applied to the monosemantic latents of Sparse Autoencoders (SAEs) rather than polysemantic neurons? The authors hypothesize in the Limitations that "LLM based methods... might perform better when describing neurons of a sparse autoencoder [6] as these are more monosemantic." This is unresolved because the current study focuses on standard polysemantic neurons where LLM methods underperformed Linear Explanations, potentially because highly activating inputs were insufficient to describe complex polysemantic behavior.

### Open Question 3
To what extent does the use of non-expert AMT raters bias evaluation results toward simpler descriptions, and would domain experts yield different rankings for complex concepts? The paper notes that crowdsourced evaluation "might favor simpler descriptions over more complex concepts requiring domain knowledge" and states "we cannot improve their domain knowledge." This remains unresolved because the current study establishes correlations based on crowd workers but does not validate if "complex" explanations are penalized simply because non-expert raters fail to identify them correctly.

## Limitations

- The scalability of MG-IS to extremely rare concepts is unknown, with potential performance degradation when target concepts appear in <1% of high-importance samples
- The 23% rater error rate may not generalize across different concept types and datasets, potentially violating BRAgg's independence assumptions
- The correlation metric may not fully capture explanation quality for polysemantic neurons or when activation-concept relationships are highly non-linear

## Confidence

- High confidence: MG-IS efficiency claims (15x reduction verified in simulation and ImageNet label validation); BRAgg noise reduction (3x fewer ratings verified through ablation studies)
- Medium confidence: Overall 40x cost reduction (combines two mechanisms with separate validations but no integrated experiment); Pearson correlation as optimal metric (supported by prior work but not rigorously validated against alternatives in this paper)
- Low confidence: SigLIP prior superiority (20% vs 25% RCE difference is statistically significant but may not generalize to different concepts or models)

## Next Checks

1. Test MG-IS efficiency on ImageNet labels as ground truth: compare RCE of MG-IS vs uniform sampling at varying |S| values to verify ~15x efficiency gain holds beyond simulated conditions
2. Validate error rate estimation: pilot study with 10 neurons, 600 inputs, 9 raters each; compute empirical η from disagreements with ImageNet labels and compare to assumed 0.23 value
3. Test correlation metric robustness: run full pipeline on 100 neurons from different layers/models; compare Pearson correlation against top-k activation recall and precision metrics to verify correlation captures explanation quality more reliably