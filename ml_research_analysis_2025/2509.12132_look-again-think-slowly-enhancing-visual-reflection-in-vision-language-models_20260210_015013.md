---
ver: rpa2
title: 'Look Again, Think Slowly: Enhancing Visual Reflection in Vision-Language Models'
arxiv_id: '2509.12132'
source_url: https://arxiv.org/abs/2509.12132
tags:
- visual
- reasoning
- arxiv
- reflection
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates the problem of visual reflection in vision-language
  reasoning models (VRMs), which refers to the ability of models to check and refine
  their reasoning process based on visual information. Current VRMs struggle with
  visual reflection, as their attention to visual tokens diminishes rapidly during
  long reasoning chains, leading to visual neglect and hallucinations.
---

# Look Again, Think Slowly: Enhancing Visual Reflection in Vision-Language Models

## Quick Facts
- **arXiv ID:** 2509.12132
- **Source URL:** https://arxiv.org/abs/2509.12132
- **Reference count:** 36
- **Primary result:** Proposes Reflection-V, a vision-language model that maintains visual attention throughout reasoning chains, outperforming baselines on MathVision, MMMU, and M3CoT.

## Executive Summary
This paper addresses the problem of visual reflection in vision-language reasoning models (VRMs), where attention to visual tokens rapidly diminishes during long reasoning chains, leading to visual neglect and hallucinations. The authors propose a two-stage training strategy: first, constructing vision-centered reasoning data through LLM-VLM interaction to embed visual reflection patterns, then applying reinforcement learning with a visual attention-based reward to encourage sustained focus on visual tokens. The resulting model, Reflection-V, significantly outperforms existing VRMs on multiple benchmarks while maintaining stronger and more consistent reliance on visual information during reasoning, demonstrating fewer hallucinations compared to baseline models.

## Method Summary
The approach consists of two training stages. First, cold-start supervised fine-tuning uses LLM-VLM interaction to generate reasoning data where visual information is continuously accessed and utilized, creating chains that structurally require visual verification. Second, GRPO reinforcement learning optimizes the model using a combined reward function that includes accuracy, format, and a novel visual attention reward. This visual attention reward measures the ratio of attention to visual tokens in the second half versus first half of generation, encouraging the model to "look back" at images during later reasoning steps. The method is evaluated on benchmarks including MathVision, MMMU, M3CoT, and HallBench.

## Key Results
- Reflection-V achieves state-of-the-art performance on MathVision (76.1%), MMMU (81.5%), and M3CoT (82.5%) benchmarks
- The model maintains near-constant Visual Dependency Measure across generation length, while baselines show sharp decline
- Ablation studies show visual reflection data provides 4-5 point improvements over caption-based data, while visual attention reward adds 1-2 points
- Visual attention spikes correlate with textual "aha moments" like "Let's check the image again"

## Why This Works (Mechanism)

### Mechanism 1: LLM-VLM Interaction for Visual Reflection Pattern Injection
A three-role agent system creates reasoning chains where visual verification is structurally required. The Visual Requester (LLM) determines what visual information is needed, the Visual Expert (VLM) provides targeted visual descriptions, and the Summarizer (LLM) integrates information toward answers. Failed summaries trigger new interaction rounds, filtering out shortcuts and ensuring visual information is continuously accessed during reasoning.

### Mechanism 2: Visual Attention-Based Reward Shaping
The reward function $r_v = \frac{\sum_{n>|T_{res}|/2} Attn(n, T_{vis})}{\sum_{n<|T_{res}|/2} Attn(n, T_{vis})}$ creates a ratio comparing visual attention in the second half versus first half of generation. Higher ratios earn higher rewards when answers are correct. This encourages sustained focus on visual tokens during long reasoning chains and prevents visual neglect.

### Mechanism 3: Visual Dependency Preservation Through Training
The combination of visual reflection data and attention-based reward creates models that maintain consistent dependency on visual information throughout generation. Visual Dependency Measure (VDM) quantifies this as Hellinger distance between next-token distributions with and without visual tokens. Reflection-V maintains near-constant upper-bound VDM while baselines decline sharply with generation length.

## Foundational Learning

- **Concept: Attention Weight Analysis in Transformers**
  - **Why needed here:** Essential for both diagnostic analysis and reward computation, which uses last-layer attention to measure visual reliance.
  - **Quick check question:** Given a transformer's attention matrix $A \in \mathbb{R}^{n \times n}$ where $A_{ij}$ represents attention from token $i$ to token $j$, how would you compute the total attention from all response tokens to all visual tokens?

- **Concept: Hellinger Distance for Distribution Comparison**
  - **Why needed here:** The Visual Dependency Measure uses Hellinger distance to quantify how much predictions change when visual tokens are removed.
  - **Quick check question:** If two probability distributions $p$ and $q$ have Hellinger distance $H(p,q) = 0.3$, what does this imply about their similarity? Why might this be preferred over KL divergence for this application?

- **Concept: GRPO (Group Relative Policy Optimization)**
  - **Why needed here:** The RL stage uses GRPO, which compares multiple rollout samples from the same prompt to compute relative advantages.
  - **Quick check question:** In GRPO, why are multiple rollouts generated for each prompt? How does the visual attention reward integrate with the group-based advantage estimation?

## Architecture Onboarding

- **Component map:** Input: Image + Question -> Base VLM: Qwen2.5-VL-7B-Instruct -> Stage 1: SFT on Cold-Start Data -> Stage 2: GRPO Training -> Reflection-V Model

- **Critical path:** Cold-start data quality is the critical dependency. Switching from visual reflection data to caption-based reasoning data causes a ~4-5 point drop across benchmarks.

- **Design tradeoffs:**
  - Data construction cost vs. quality: LLM-VLM interaction requires multiple model calls per sample vs. cheaper caption-then-reason approach
  - Reward coefficient tuning: $\lambda_v=0.5$ and $\lambda_f=0.1$ are empirically set, with higher visual attention reward potentially conflicting with accuracy on text-heavy tasks
  - Layer selection for attention: Uses last layer where visual attention is "most significant," but different architectures may require different layer choices

- **Failure signatures:**
  1. Visual reflection data too sparse: If non-reflection filtering removes >90% of samples, model won't learn the pattern
  2. Reward hacking: Model may increase attention weights superficially without functional improvement
  3. Catastrophic forgetting on text-only tasks: Excessive visual emphasis may degrade performance on tasks where visual information is distractor

- **First 3 experiments:**
  1. Replicate diagnostic analysis (Figure 2): Measure attention decay and VDM on your base model using the MMMU validation set
  2. Ablate cold-start data composition: Train with (a) visual reflection data, (b) caption-then-reason data, (c) mixed 50/50 on held-out MathVision subset (500 samples)
  3. Reward coefficient sweep: With cold-start model, run GRPO with $\lambda_v \in \{0.0, 0.25, 0.5, 1.0\}$ while monitoring attention ratio trend, VDM trend, and accuracy

## Open Questions the Paper Calls Out
None

## Limitations

- **Attention Weight Interpretation:** The paper assumes attention weights directly correlate with functional visual reliance, but attention mechanisms can be sparse or diffuse without clear semantic meaning.
- **Data Quality Dependency:** The cold-start data construction requires perfect coordination between LLM requester and VLM expert, with no specification of failure rates or interaction rounds needed.
- **Reward Design Assumptions:** The visual attention reward assumes attention in the second half should exceed the first half, potentially missing legitimate reasoning patterns that require initial visual verification followed by pure textual reasoning.

## Confidence

**High Confidence:**
- Visual neglect occurs in baseline VRMs (supported by Figure 2 showing attention decay)
- Visual reflection data improves performance vs caption-based data (supported by Table 3)
- Attention-based reward improves performance vs no reward (supported by Table 2)
- Reflection-V outperforms baselines on standard benchmarks (supported by Table 1)

**Medium Confidence:**
- Visual attention reward causally improves visual grounding (inferred from correlation between attention and VDM)
- Cold-start data quality is the dominant factor (inferred from ablation showing data composition matters more than reward)
- Visual reflection patterns are learnable behaviors (assumed rather than proven)

**Low Confidence:**
- Attention weights meaningfully measure visual reliance (assumed without validation against alternative measures)
- The specific reward formulation is optimal (no comparison to alternative attention-based rewards)
- Visual neglect is the primary cause of hallucinations (correlation observed but not causally proven)

## Next Checks

1. **Attention Weight Validation Experiment:** Replace attention-based visual reward with a simpler binary reward that activates when the model explicitly references visual tokens in its generation (e.g., contains phrases like "looking at the image"). Compare performance and VDM trends to determine if attention weights are the right proxy for visual grounding.

2. **Data Construction Robustness Test:** During data generation, instrument the LLM-VLM interaction to log: (a) number of interaction rounds needed per sample, (b) success rate of different prompt formulations, (c) VLM response quality scores. If >70% of samples require >3 interaction rounds or fail entirely, the data construction pipeline may not be reliable.

3. **Ablation of Visual Attention Reward Components:** Instead of the ratio-based reward, test: (a) absolute attention threshold in second half, (b) attention increase relative to first half minimum, (c) binary reward for any attention above baseline. Track whether performance correlates with the magnitude of attention increase or just its presence.