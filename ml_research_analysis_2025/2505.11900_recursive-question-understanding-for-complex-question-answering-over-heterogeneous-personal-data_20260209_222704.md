---
ver: rpa2
title: Recursive Question Understanding for Complex Question Answering over Heterogeneous
  Personal Data
arxiv_id: '2505.11900'
source_url: https://arxiv.org/abs/2505.11900
tags:
- data
- user
- questions
- events
- attr
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "R EQAP is a novel QA system for heterogeneous personal data that\
  \ recursively decomposes questions into executable operator trees. It combines retrieval-based\
  \ and structured query approaches using two novel operators\u2014RETRIEVE for efficient\
  \ event retrieval with deduplication, and EXTRACT for on-the-fly information extraction\
  \ from unstructured text."
---

# Recursive Question Understanding for Complex Question Answering over Heterogeneous Personal Data

## Quick Facts
- arXiv ID: 2505.11900
- Source URL: https://arxiv.org/abs/2505.11900
- Authors: Philipp Christmann; Gerhard Weikum
- Reference count: 39
- Key outcome: 38% Hit@1 with 1B model on PERQA benchmark, outperforming verbalization and translation baselines

## Executive Summary
This paper introduces REQAP, a novel QA system for heterogeneous personal data that recursively decomposes complex questions into executable operator trees. The system combines retrieval-based and structured query approaches using two novel operators - RETRIEVE for efficient event retrieval with deduplication, and EXTRACT for on-the-fly information extraction from unstructured text. Trained via in-context learning and model distillation to run on end-user devices, REQAP demonstrates substantial improvements over baseline approaches on the new PERQA benchmark, achieving 38% Hit@1 and 53% relaxed Hit@1 with its small 1B variant.

## Method Summary
REQAP employs a two-stage pipeline for QA over heterogeneous personal data. First, the QUD (Question Understanding) module recursively decomposes questions into operator trees using a 1B decoder-only LLM trained via in-context learning and model distillation. Second, the OTX (Operator Execution) engine executes these trees using two key operators: RETRIEVE (combining SPLADE retrieval with cross-encoder reranking and deduplication) and EXTRACT (fine-tuned BART-base for on-the-fly information extraction). The system is trained on the PERQA benchmark with 40 handcrafted ICL examples and distilled from larger models to achieve on-device viability while maintaining high accuracy.

## Key Results
- 1B SFT model achieves 38% Hit@1 and 53% relaxed Hit@1 on PERQA benchmark
- Outperforms verbalization (RAG) and translation (CODEGEN) baselines
- Ablation studies show recursive decomposition reduces semantic mapping errors
- User study confirms 69% of answers are correct or nearly correct on actual personal data
- On-device deployment viable with minimal performance loss compared to GPT-4o baseline

## Why This Works (Mechanism)

### Mechanism 1: Recursive Decomposition for Semantic Parsing
The system treats question understanding as a recursive process rather than generating complete SQL-like queries at once. This reduces the hypothesis space for the language model at each step, preventing logic errors common in one-shot translation. Ablation studies show performance drops from 0.396 to 0.356 Hit@1 when using one-shot approaches.

### Mechanism 2: Just-In-Time Schema Alignment via EXTRACT
The EXTRACT operator functions as a schema-on-read mechanism, transforming heterogeneous unstructured text into uniform key-value format just before logical operations are applied. Ablation confirms this is necessary - performance collapses to 0.138 Hit@1 when relying only on exact key matches.

### Mechanism 3: High-Recall Retrieval with Pattern Pruning
The RETRIEVE operator prioritizes recall first using SPLADE, then prunes entire sources and patterns using cross-encoders. This preserves accuracy for downstream aggregators while avoiding computational costs of scoring every event. Ablation shows performance drops to 0.269 Hit@1 when using SPLADE-only without pruning.

## Foundational Learning

- **Semantic Parsing & Program Synthesis**: REQAP is essentially a semantic parser translating natural language into a DSL of operators (RETRIEVE, JOIN, etc.). Quick check: Can you explain why generating a program is often more robust than generating natural language text for multi-step reasoning tasks?

- **Knowledge Distillation (Teacher-Student)**: The system uses a large model to generate reasoning traces which are then used to train a small student model. Quick check: Why might a small model trained on the outputs of a larger model outperform the small model trained directly on raw data?

- **Sparse vs. Dense Retrieval (SPLADE)**: The RETRIEVE operator uses SPLADE. Quick check: How does query expansion in sparse models like SPLADE help in finding specific entities compared to standard dense embeddings?

## Architecture Onboarding

- **Component map**: Query -> QUD Model (Generates Tree) -> OTX Engine (Executes) -> RETRIEVE (Gets candidates) -> EXTRACT (Adds keys) -> JOIN/GROUP (Logic) -> Answer

- **Critical path**: The latency bottleneck is likely the EXTRACT operator (seq-to-seq on hundreds of events) and the RETRIEVE classifier. The system biases towards high recall in the first retrieval step and pays the cost of heavier filtering later.

- **Design tradeoffs**: Recall vs. Noise (biases toward high recall), On-Device vs. Accuracy (1B model retains ~98% relative performance)

- **Failure signatures**: "Mailing List" Error (promotional emails skewing aggregates), Tree Hallucination (syntactically valid but logically incorrect trees), 50% of failures stem from incorrect operator trees

- **First 3 experiments**:
  1. Ablate the Extractor: Swap fine-tuned BART model for standard LLM prompt or regex
  2. Stress Test Retrieval: Inject noise into synthetic data to measure RETRIEVE operator's precision/recall balance
  3. One-Shot vs. Recursive: Implement single-shot prompt and compare syntax error rate against recursive method

## Open Questions the Paper Calls Out
- The current reliance on materialized data exports is a restriction and pursuing a federated architecture would be important for future research
- The complexity of non-equi joins and temporal merging over heterogeneous sources results in lower accuracy for join-centric questions
- The trade-off between on-device viability and performance suggests small models may be reaching performance ceilings on complex extraction tasks

## Limitations
- PERQA benchmark representativeness for real-world complexity is inferred from user study rather than independent evaluation
- Recursive decomposition benefits are clear but optimal recursion depth and termination criteria remain underspecified
- Relaxed Hit@1 metric (±10% for numbers) may overstate practical utility
- 50% of failures stem from incorrect operator trees, suggesting significant quality gaps in distillation

## Confidence
- **High confidence**: Recursive decomposition reduces semantic mapping errors (0.396→0.356 Hit@1 drop when disabled)
- **Medium confidence**: EXTRACT operator successfully bridges structured/unstructured data (performance collapses without it)
- **Low confidence**: 1B model retains ~98% relative performance compared to GPT-4o baseline (0.386→0.396 to 0.38 extrapolation)

## Next Checks
1. **Schema generalization test**: Create synthetic personal dataset with novel attribute combinations and measure EXTRACT's precision on keys not seen during training
2. **Recall threshold stress test**: Systematically vary SPLADE threshold (0.05→0.2) and cross-encoder pruning aggressiveness to map precision-recall tradeoff curve
3. **Error attribution analysis**: Classify the 50% of failures due to incorrect operator trees into (a) incomplete recursion, (b) hallucinated constraints, or (c) incorrect operator selection