---
ver: rpa2
title: Assessing the performance of correlation-based multi-fidelity neural emulators
arxiv_id: '2512.02868'
source_url: https://arxiv.org/abs/2512.02868
tags:
- network
- error
- samples
- encoding
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluates multi-fidelity neural emulators that combine\
  \ scarce high-fidelity data with abundant low-fidelity information to improve predictive\
  \ accuracy. Three neural network architectures\u2014MLPs, SIRENs, and KANs\u2014\
  are compared for learning input-to-output mappings, with and without coordinate\
  \ encoding strategies to handle dissimilar parameterizations."
---

# Assessing the performance of correlation-based multi-fidelity neural emulators

## Quick Facts
- arXiv ID: 2512.02868
- Source URL: https://arxiv.org/abs/2512.02868
- Authors: Cristian J. Villatoro; Gianluca Geraci; Daniele E. Schiavazzi
- Reference count: 40
- Primary result: Multi-fidelity neural emulators combining scarce high-fidelity data with abundant low-fidelity information consistently outperform single-fidelity networks across diverse scenarios, with KAN architectures showing superior performance overall.

## Executive Summary
This study systematically evaluates multi-fidelity neural emulators that combine scarce high-fidelity data with abundant low-fidelity information to improve predictive accuracy. Three neural network architectures—MLPs, SIRENs, and KANs—are compared for learning input-to-output mappings, with and without coordinate encoding strategies to handle dissimilar parameterizations. The research tests these approaches on diverse scenarios including oscillatory functions, discontinuities, high-dimensional inputs, and multiple noisy low-fidelity sources. Results demonstrate that multi-fidelity approaches consistently outperform single-fidelity networks, particularly in data-scarce settings, with KAN networks showing superior performance overall. Coordinate encoding provides significant benefits for problems with phase shifts or misaligned parameterizations, though it can introduce unnecessary complexity in simpler cases.

## Method Summary
The study evaluates three neural architectures (MLP, SIREN, KAN) for multi-fidelity emulation, combining scarce high-fidelity data with abundant low-fidelity information through linear and nonlinear correlation components. Networks are tested with optional coordinate encoding to handle dissimilar parameterizations between fidelity levels. The framework uses analytical test functions (K1-K5, 2DE, 2DU, GJG9, RD PDE) with HF sample sizes of {8, 16, 32} for 1D and {64, 128, 256} for multi-dimensional cases. LF-to-HF oversampling ratio is 8×. Networks are optimized using Adam with hyperparameter search via Hyperopt, and performance is measured using normalized HF MSE with 4 independent repetitions.

## Key Results
- Multi-fidelity approaches consistently outperform single-fidelity networks across all tested scenarios and sample regimes
- KAN architectures show superior overall performance, particularly in high-dimensional and complex correlation cases
- Coordinate encoding provides significant benefits for phase-shifted and misaligned parameterization problems but can introduce unnecessary complexity in simpler cases
- SIREN networks exhibit significant underfitting on oscillatory and phase-shifted problems, while nonlinear coordinate encoding can overfit discontinuity locations

## Why This Works (Mechanism)

### Mechanism 1
Decomposing high-fidelity predictions into linear and nonlinear correlation components from low-fidelity data improves sample efficiency. The network learns ŷ_H = ŷ_H^l + ŷ_H^nl, where the linear component captures the dominant correlation structure (e.g., affine transformations between LF and HF), and the nonlinear component learns residual corrections. This decomposition reduces the burden on the nonlinear network, as shown in test case 2DE where linear encoding captured the general shape while nonlinear only learned small corrective features.

### Mechanism 2
Learnable coordinate encodings align dissimilar parameterizations between LF and HF models, enabling fusion when input spaces differ. A coordinate map T_i: X_H → X_L,i transforms HF inputs to LF input space before evaluating LF surrogates. Initialized as identity, it learns to correct phase shifts (K4), relocate discontinuities (K2 shift), or handle dimensional mismatches (2DU). The interval score loss term limits extrapolation.

### Mechanism 3
KAN architectures achieve superior performance for complex correlations due to their compositional univariate structure. KANs use learnable B-splines + SiLU activations on edges (univariate functions), composed across layers. This matches the Kolmogorov-Arnold representation theorem, which expresses multivariate functions as compositions of univariate functions. KANs consistently outperformed MLP and Siren across most test cases, especially in high-dimensional K5 and complex correlation scenarios.

## Foundational Learning

- Concept: Multi-fidelity surrogate modeling fundamentals
  - Why needed here: The entire framework assumes understanding that LF models provide inexpensive but biased approximations, while HF models are accurate but scarce.
  - Quick check question: Can you explain why adding more LF samples doesn't eliminate the need for any HF data?

- Concept: Spectral bias in neural networks
  - Why needed here: The paper compares MLPs (low-frequency bias), SIRENs (high-frequency capacity via sinusoidal activations), and KANs (flexible frequency via splines). Understanding this helps interpret why SIRENs underfit oscillatory K3 but MLPs succeeded.
  - Quick check question: Why might a Siren network overfit small datasets while underfitting larger ones?

- Concept: Coordinate transformations and basis functions
  - Why needed here: Coordinate encoding requires understanding how transformations between input spaces affect learned correlations. B-spline basis functions in KAN require understanding how spline grid resolution affects approximation capacity.
  - Quick check question: What happens to the interval score loss when the coordinate encoder maps HF inputs outside the LF training domain?

## Architecture Onboarding

- Component map:
  Input (x_H) → [Coordinate Encoder T_i] → LF Surrogate ŷ_L,i → [Linear Correlation F^l] ─┐
                                                                                          ├→ ŷ_H
       └─────────────────────────────────────────────────→ [Nonlinear Correlation F^nl] ─┘
  Each LF source has its own encoder (optional) and surrogate. Linear correlation has bias; nonlinear does not.

- Critical path: Start with standard MF architecture (no encoding), exact LF functions, and MLP baseline. Verify on K1 (linear correlation) before adding complexity.

- Design tradeoffs:
  - Encoding: Adds flexibility for misaligned parameterizations but introduces optimization complexity and can overfit (K2 with nonlinear encoding moved discontinuities incorrectly)
  - Learned vs. exact LF: Learned LF is more realistic but limits achievable accuracy (normalized MSE couldn't drop below 10^-6 in K1)
  - Network complexity: 3×16 with exact LF is upper bound; 1×8 with learned LF is practical scenario

- Failure signatures:
  - SIREN underfitting on K3: Normalized MSE 10^2 despite other architectures achieving 10^-5
  - Nonlinear encoding overfitting discontinuities in K2: Moved discontinuity to fit training data, poor test performance
  - High-dimensional plateau in K5: All architectures showed high MSE (~10^1) regardless of sample size, suggesting insufficient samples for curse of dimensionality

- First 3 experiments:
  1. Reproduce K1 (linear correlation) with MLP, no encoding, exact LF, 16 HF samples to verify implementation matches Figure 2 baseline (~10^-4 MSE)
  2. Add nonlinear encoding to K4 (phase shift) and verify encoder learns T(x) ≈ x + 1/80, comparing against no-encoding failure mode (Figure 15)
  3. Test KAN vs. MLP on K5 (20D) with 256 samples to reproduce KAN's superior scaling (Figure 40)

## Open Questions the Paper Calls Out

### Open Question 1
How can adaptive selection algorithms be developed to automatically match neural architectures (MLP, SIREN, KAN) to specific multi-fidelity correlation structures?
- Basis in paper: [explicit] The conclusion explicitly calls for "developing adaptive architecture selection methods" as a key direction for future research.
- Why unresolved: The study manually benchmarks architectures against problem types, but provides no automated mechanism to detect these characteristics a priori.
- What evidence would resolve it: A meta-learning framework or heuristic that successfully predicts the optimal architecture based on initial low-fidelity data statistics.

### Open Question 2
Does the multi-fidelity advantage persist in dimensions significantly higher than 20 without requiring exponential increases in high-fidelity samples?
- Basis in paper: [inferred] In the 20-dimensional K5 test case, results showed poor generalization, leading the authors to suggest "the number of HF samples required... has not been reached."
- Why unresolved: The study only tests up to 20 dimensions and 256 samples; it remains unclear if the observed efficiency gains scale to truly high-dimensional engineering problems.
- What evidence would resolve it: Successful emulation of a 100+ dimensional problem using sample sizes comparable to the lower-dimensional cases tested here.

### Open Question 3
Can specific regularization strategies mitigate the non-monotonic convergence behaviors observed in KANs when fusing multiple noisy data sources?
- Basis in paper: [inferred] In the reaction-diffusion and GJG9 cases, KANs exhibited "intriguing non-monotonic behavior" where performance degraded at intermediate sample counts.
- Why unresolved: The paper identifies this instability in the optimization landscape but does not propose a method to smooth the convergence path.
- What evidence would resolve it: A modification to the KAN training loop that results in strictly monotonic error reduction as sample size increases.

## Limitations

- High-dimensional scenarios (K5) reveal fundamental scaling limitations where all architectures plateau regardless of sample size
- Nonlinear coordinate encoding can overfit discontinuity locations rather than learning true transformations
- SIREN networks exhibit significant underfitting on oscillatory and phase-shifted problems due to spectral bias

## Confidence

- **High confidence**: Multi-fidelity approaches consistently outperform single-fidelity networks across all tested scenarios and sample regimes
- **Medium confidence**: KAN architectures show superior overall performance, particularly in high-dimensional and complex correlation cases
- **Medium confidence**: Coordinate encoding provides significant benefits for phase-shifted and misaligned parameterization problems
- **Low confidence**: Specific hyperparameter choices (learning rates, regularization weights) and their sensitivity to problem characteristics

## Next Checks

1. Test KAN vs. MLP scaling on K5 with 512-2048 HF samples to determine if performance plateaus are due to architecture limitations or fundamental curse of dimensionality
2. Implement theoretical analysis of interval score loss to establish when encoding extrapolation is bounded versus when it fails
3. Compare multi-fidelity MF networks against transfer learning approaches where a single network is pre-trained on LF data before HF fine-tuning