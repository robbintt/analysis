---
ver: rpa2
title: 'PROPEL: Supervised and Reinforcement Learning for Large-Scale Supply Chain
  Planning'
arxiv_id: '2504.07383'
source_url: https://arxiv.org/abs/2504.07383
tags:
- learning
- propel
- variables
- primal
- prop
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PROPEL integrates supervised and deep reinforcement learning to
  solve large-scale supply chain planning problems formulated as Mixed Integer Linear
  Programs (MIPs) with millions of variables. The supervised component identifies
  integer variables fixed at zero in optimal solutions using reduced costs, while
  the DRL component selectively relaxes fixed variables when optimality gaps exceed
  target thresholds.
---

# PROPEL: Supervised and Reinforcement Learning for Large-Scale Supply Chain Planning

## Quick Facts
- arXiv ID: 2504.07383
- Source URL: https://arxiv.org/abs/2504.07383
- Reference count: 16
- Primary result: PROPEL reduces primal integral by 60% and primal gap by 88% compared to state-of-the-art MIP solvers on industrial supply chain instances.

## Executive Summary
PROPEL is a hybrid machine learning framework that combines supervised learning and deep reinforcement learning to solve large-scale supply chain planning problems formulated as Mixed Integer Linear Programs (MIPs) with millions of variables. The framework uses supervised learning to predict which integer variables should be fixed at zero in the optimal solution, dramatically reducing the search space. When the initial solution does not meet optimality targets, a DRL component iteratively relaxes fixed variables to restore solution quality. PROPEL was evaluated on industrial supply chain instances, demonstrating significant improvements in solution time and solution quality compared to state-of-the-art MIP solvers.

## Method Summary
PROPEL integrates supervised and deep reinforcement learning to solve large-scale supply chain planning problems. The supervised component trains a neural network classifier to predict which integer variables are zero in optimal solutions, using reduced costs from LP relaxations as additional features. Variables predicted to be zero are fixed, reducing the MIP size. The DRL component serves as a corrective mechanism, learning to selectively relax fixed variables when the optimality gap exceeds target thresholds. The framework models this unfixing process as a Markov Decision Process with macro-actions, allowing the agent to efficiently navigate the trade-off between problem size reduction and solution quality.

## Key Results
- PROPEL achieves 60% reduction in primal integral and 88% reduction in primal gap compared to state-of-the-art MIP solvers
- Solution time improvements of up to 13.57x and primal gap reductions of up to 15.92x on industrial instances
- Consistent performance across 60 test instances, demonstrating robustness
- Effectively handles non-binary MIPs at industrial scale, addressing feasibility and scalability challenges

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Restricting the MIP search space by predicting and fixing zero-valued variables reduces solve times while maintaining feasibility.
- **Mechanism**: PROPEL trains a supervised classifier to predict which integer variables are zero in the optimal solution. By fixing these variables, the framework drastically reduces the number of branching decisions the solver must explore. This avoids the infeasibility risks associated with predicting non-zero values, as adding constraints $x_i=0$ is generally safer in supply chain problems with slack variables (e.g., unmet demand) than assigning arbitrary non-zero integers.
- **Core assumption**: The optimal solution to the supply chain MIP is sparse, containing a large number of integer variables fixed at zero, and the problem structure allows for "easy" feasibility even with aggressive fixing.
- **Evidence anchors**:
  - [abstract] "PROPEL uses supervised learning... to identify the variables that are fixed to zero... significantly reducing the search space."
  - [section 5] "By correctly identifying and fixing these zero-valued variables, PROPEL simplifies the optimization task... and avoids making mistakes in predicting integer values."
  - [corpus] Weak/General. Neighbor papers apply RL for inventory control but do not focus on MIP search space reduction via variable fixing.
- **Break condition**: If the underlying optimization problem changes such that the optimal solution becomes dense (few zeros), the recall of the supervised component will drop, failing to sufficiently reduce the problem size.

### Mechanism 2
- **Claim**: Integrating Linear Programming (LP) reduced costs with machine learning probabilities mitigates prediction errors in the variable fixing phase.
- **Mechanism**: The framework does not rely solely on the neural network's softmax output. It calculates the reduced cost $rc_i$ from the LP relaxation. A normalized reduced cost is added to the prediction probability. Since reduced costs indicate the marginal improvement potential of a variable, combining this mathematical signal with the learned signal creates a more robust criterion for deciding whether to fix a variable.
- **Core assumption**: The LP relaxation of the supply chain MIP provides a useful geometric signal (reduced costs) that correlates with the variable's status in the optimal integer solution.
- **Evidence anchors**:
  - [abstract] "...leverages the structure of SCP applications [and] includes a DRL component that selects which fixed-at-zero variables must be relaxed..." (Note: Abstract mentions structure, Section 5 details the reduced cost mechanism).
  - [section 5] "PROPEL refines this approximation by using the reduced costs... PROPEL then approximates $S_0(d)$ as follows: $\hat{\psi}^0_i(d) + r_i(d) \geq \tau$."
  - [corpus] N/A (Specific hybrid mechanism not detailed in general corpus summaries).
- **Break condition**: If the LP relaxation is a poor approximation of the integer polytothpe (large LP gap), reduced costs may be misleading, causing the model to fix variables that should be active.

### Mechanism 3
- **Claim**: Deep Reinforcement Learning (DRL) serves as a correctives mechanism to restore solution quality by iteratively relaxing variables fixed too aggressively by the supervised phase.
- **Mechanism**: The supervised phase may over-constrain the problem, leading to suboptimal solutions (high optimality gap). PROPEL models the "unfixing" process as a Markov Decision Process (MDP). It partitions fixed variables into subsets (macro-actions). The RL agent learns a policy to re-insert these subsets into the problem to maximize the objective value (reward), effectively navigating the trade-off between problem size reduction and solution quality.
- **Core assumption**: The variables incorrectly fixed by the supervised learner are localized or correlated in a way that allows the RL agent to identify and relax them efficiently via partition-based actions.
- **Evidence anchors**:
  - [abstract] "PROPEL includes a DRL component that selects which fixed-at-zero variables must be relaxed... when the supervised learning step does not produce a solution with the desired optimality tolerance."
  - [section 6] "The goal of the DRL component is to find a subset $J \subseteq \{1, \dots, m\}$... meeting the optimality target."
  - [corpus] Weak/Distinct. Neighbor papers use RL for direct policy learning (e.g., inventory control), whereas PROPEL uses RL as a heuristic to modify a MIP formulation.
- **Break condition**: If the RL agent requires too many episodes or iterations to converge, the cumulative solve time may exceed the original solver time, negating the benefits of the framework.

## Foundational Learning

- **Concept**: Mixed-Integer Linear Programming (MILP) Sparsity
  - **Why needed here**: The entire PROPEL strategy hinges on the observation that industrial SCP problems have many zero-value variables at optimality. Without understanding this sparsity, the "predict-then-fix" approach seems counter-intuitive compared to end-to-end solution prediction.
  - **Quick check question**: Can you explain why fixing a variable to 0 is generally safer for feasibility than predicting a non-zero integer value in a flow-balance constraint?

- **Concept**: Reduced Costs in Linear Programming
  - **Why needed here**: This is the specific "structure" signal PROPEL leverages to augment the ML classifier. Understanding how the reduced cost $rc_i$ reflects the opportunity cost of a variable is necessary to implement the scoring function in Section 5.
  - **Quick check question**: If the reduced cost of a variable is strictly positive in a minimization problem's LP relaxation, what does that imply about the variable's value in that specific LP solution?

- **Concept**: Macro-Actions in Reinforcement Learning
  - **Why needed here**: The DRL component does not unfix variables one by one (which is too slow) but uses macro-actions (partitioning). Understanding this abstraction is key to grasping how the agent navigates the massive state space of a MIP with millions of variables.
  - **Quick check question**: Why would an RL agent operating on a variable-partition space (macro-actions) converge faster than one acting on individual variables in a problem with 1M+ variables?

## Architecture Onboarding

- **Component map**: Data Generator -> Feature Extractor -> Supervised Model (PROP) -> Solver Wrapper -> RL Agent (ENLARGE) -> Iterative Re-solving

- **Critical path**:
  1. Generating representative training data (Perturbation logic)
  2. Feature extraction mapping demands to specific variable-constraint paths
  3. Thresholding logic ($\tau$) combined with reduced costs to balance reduction vs. accuracy

- **Design tradeoffs**:
  - **Threshold ($\tau$)**: A high threshold fixes fewer variables (safer but slower); a low threshold is aggressive (faster but risks optimality loss)
  - **Partition Size**: Small partitions allow fine-grained correction by RL but increase the action space; large partitions are coarse but faster to train
  - **Time Limits**: Allocating too much time to the initial PROP solve leaves insufficient budget for the RL ENLARGE phase

- **Failure signatures**:
  - **Infeasibility**: The supervised model fixes critical variables, and the solver returns "Infeasible" immediately
  - **Stagnation**: The RL agent iterates through ENLARGE steps without improving the primal gap
  - **Slow Convergence**: The overhead of prediction + RL inference exceeds the time it would take Gurobi to solve the original MIP naively

- **First 3 experiments**:
  1. **Baseline Comparison**: Run standard Gurobi (OPT) vs. PROPEL on a held-out test set of 10 instances with a 600s time limit. Measure Primal Integral
  2. **Ablation on Reduced Costs**: Run PROP (Supervised only) with and without the reduced cost scoring term to quantify the value of the LP-based signal (replicate Table 1 logic)
  3. **RL Scaling Test**: Vary the RL partition size (e.g., 4, 8, 16) on the "hard" instances (where gap > 1%) to determine the optimal granularity for the corrective phase

## Open Questions the Paper Calls Out
None

## Limitations
- Performance depends heavily on the sparsity pattern of optimal solutions, which may vary across different problem types
- Computational overhead of ML and RL components could become prohibitive for problems much larger than those tested
- Framework relies on Gurobi solver, and performance may vary with different MIP solvers

## Confidence

**High Confidence**: The claim that PROPEL reduces primal integral and primal gap compared to baseline MIP solvers (60% reduction, 88% reduction) is supported by experimental results across 60 test instances. The framework's ability to handle non-binary MIPs at industrial scale is well-demonstrated.

**Medium Confidence**: The effectiveness of combining reduced costs with ML predictions for variable fixing is reasonable but requires more ablation studies to quantify the exact contribution of each component. The DRL component's role as a corrective mechanism is supported but the conditions under which it succeeds or fails are not fully characterized.

**Low Confidence**: The generalizability of PROPEL's performance across different types of supply chain problems and optimization formulations remains unclear. The paper focuses on specific SCP instances without extensive testing on diverse problem structures.

## Next Checks
1. **Cross-Problem Generalization**: Test PROPEL on a diverse set of supply chain optimization problems beyond the industrial instances used in the paper, including different objective functions (cost minimization vs. profit maximization) and constraint structures. Measure performance degradation as problem characteristics vary.

2. **Solver Independence Validation**: Implement PROPEL with an open-source MIP solver (e.g., SCIP or CBC) to verify that the performance gains are not solver-specific. Compare solution times and primal gaps across different solvers to assess robustness.

3. **Ablation Study on Component Contributions**: Conduct a comprehensive ablation study isolating the contributions of the supervised component, the reduced cost signal, and the DRL corrective phase. Measure the marginal improvement from each component and identify scenarios where certain components become bottlenecks or provide diminishing returns.