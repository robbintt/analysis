---
ver: rpa2
title: 'TeroSeek: An AI-Powered Knowledge Base and Retrieval Generation Platform for
  Terpenoid Research'
arxiv_id: '2505.20663'
source_url: https://arxiv.org/abs/2505.20663
tags:
- knowledge
- base
- research
- teroseek
- terpenoid
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of integrating interdisciplinary
  terpenoid knowledge across chemistry, pharmacology, and biology. The authors developed
  TeroSeek, a knowledge base and retrieval-augmented generation (RAG) platform that
  extracts data from 20 years of terpenoid literature.
---

# TeroSeek: An AI-Powered Knowledge Base and Retrieval Generation Platform for Terpenoid Research

## Quick Facts
- arXiv ID: 2505.20663
- Source URL: https://arxiv.org/abs/2505.20663
- Reference count: 0
- TeroSeek-normal achieved 0.78 accuracy vs. general LLMs (0.44-0.61)

## Executive Summary
This study addresses the challenge of integrating interdisciplinary terpenoid knowledge across chemistry, pharmacology, and biology. The authors developed TeroSeek, a knowledge base and retrieval-augmented generation (RAG) platform that extracts data from 20 years of terpenoid literature. Using an AI-powered RAG framework with a dedicated knowledge base, TeroSeek significantly outperforms general LLMs on terpenoid-specific queries. In benchmark testing, the system improved accuracy from 0.37 to 0.66 (DeepSeek-V3), 0.50 to 0.78 (DeepSeek-R1), and 0.46 to 0.74 (Qwen2-235B-A22B).

## Method Summary
The researchers built TeroSeek by processing 47,731 terpenoid-related academic documents into a knowledge base. They converted PDFs to structured Markdown, segmented text into chunks, and generated up to 4 hypothetical questions per chunk using LLMs. These questions and text were embedded as 2048-dimensional vectors and stored in a vector database. The system employs a two-layer hierarchical retrieval: first searching abstracts to identify relevant documents (top 400), then sub-chunks within those documents (top 20 with cosine similarity > 0.7). The retrieved context is combined with user queries and passed to a backbone LLM (DeepSeek-R1) for final answer generation.

## Key Results
- TeroSeek-normal achieved 0.78 accuracy compared to 0.61 (Gemini 2.5 Pro), 0.51 (Claude 3.7 Sonnet), 0.46 (o3), and 0.44 (Grok 3 Beta)
- DeepSeek-R1 with TeroSeek KB improved from 0.50 to 0.78 accuracy on terpenoid benchmark
- Hierarchical retrieval strategy successfully filtered noise while maintaining recall

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Generating hypothetical questions for text chunks improves retrieval recall and semantic matching compared to direct text embedding.
- **Mechanism:** The system uses an LLM to generate up to 4 potential questions for each text chunk. These questions are vectorized and stored. When a user queries the system, the search occurs in the "question vector space" rather than the "document vector space," potentially bridging the lexical and semantic gap between a user's specific inquiry and the formal language of academic text.
- **Core assumption:** LLM-generated hypothetical questions accurately represent the types of queries users will ask and the semantic content of the source text better than direct chunk embeddings.
- **Evidence anchors:**
  - [section] (Materials and Methods): "All the Chunks will generate up to 4 hypothetical questions through LLMs... stored in the vector database."
  - [abstract] "...outperforms general-purpose large language models (LLMs)..."
  - [corpus] Limited direct evidence in the provided corpus for "hypothetical question indexing" specifically, though OmniBench-RAG discusses general RAG evaluation challenges.
- **Break condition:** If user queries are structurally or semantically distinct from the LLM-generated hypotheticals (e.g., highly specialized slang or novel concepts not predicted during indexing), retrieval may fail.

### Mechanism 2
- **Claim:** A hierarchical retrieval strategy (Summary Index â†’ Sub-chunk) filters noise and improves precision.
- **Mechanism:** The system first searches a high-level index of literature abstracts (Summary Index Layer) to identify relevant documents (Top 400). It then narrows the search to sub-chunks within those documents (Top 20, Cosine Similarity > 0.7). This prevents the LLM context window from being flooded with low-relevance details from irrelevant documents.
- **Core assumption:** Relevant documents can be successfully identified via their abstracts, and high similarity scores (>0.7) correlate directly with answer relevance.
- **Evidence anchors:**
  - [section] (Materials and Methods): "...system first searches the summary index layer... to quickly identify key documents... Next, candidate knowledge is further matched at the sub-chunk level."
  - [section] (Results): "TeroSeek-normal demonstrates superior capability in extracting information from tables and figures..."
  - [corpus] Weak corpus evidence for this specific hierarchical abstract-to-chunk architecture.
- **Break condition:** If the crucial answer lies in a document where the abstract is misleading or semantically distant from the specific query (false negative in the first layer), the system will discard the correct answer before reaching the sub-chunk retrieval phase.

### Mechanism 3
- **Claim:** Decoupling the knowledge base from the model parameters via RAG allows for domain-specific accuracy without fine-tuning.
- **Mechanism:** The system stores terpenoid knowledge in an external vector database (TeroSeek-KB). The LLM acts as a reasoning engine and text generator but relies on injected context. This separates the "knowledge" (which needs frequent updates) from the "reasoning" (model weights), enabling domain adaptation and citations without retraining.
- **Core assumption:** The general reasoning capabilities of the backbone LLM are sufficient to synthesize correct answers if provided with the right context, and that RAG is more efficient than fine-tuning for this domain.
- **Evidence anchors:**
  - [abstract] "...curated knowledge base (KB) built from two decades of terpenoid literature... outperforms general-purpose large language models..."
  - [section] (Discussion): "Fine-tuning strategies are tightly coupled with the underlying backbone model... In contrast, the RAG strategy provides greater flexibility."
  - [corpus] N/A
- **Break condition:** If the retrieval mechanism fails to fetch the relevant context, the model reverts to its baseline behavior (hallucination or "I don't know"), potentially with higher confidence due to the RAG wrapper.

## Foundational Learning

- **Concept: Retrieval-Augmented Generation (RAG)**
  - **Why needed here:** TeroSeek is essentially a specialized RAG implementation. Understanding the difference between "parametric knowledge" (what the model was born with) and "contextual knowledge" (what the model is shown in the prompt) is critical.
  - **Quick check question:** If you update a fact in the TeroSeek vector database, does the underlying LLM's parametric memory change? (Answer: No).

- **Concept: Vector Embeddings & Cosine Similarity**
  - **Why needed here:** The "Knowledge Base" is queried using vector math. You must understand that text is converted to numbers (vectors) and "relevance" is calculated as the angle (similarity) between these vectors.
  - **Quick check question:** Why does the paper specify a cosine similarity threshold of 0.7?

- **Concept: Markdown Structuring for LLMs**
  - **Why needed here:** The paper explicitly converts PDFs to Markdown to preserve structure (headers like `#`, `##`). LLMs process structured text more effectively than unstructured raw PDF data.
  - **Quick check question:** Why would preserving `#` and `##` headers help an LLM understand a scientific paper better than a raw text dump?

## Architecture Onboarding

- **Component map:** PDFs -> Marker (Parser) -> Markdown -> Chunks -> LLM (Generate Hypothetical Questions) -> Embedding Model -> Vector DB (TeroSeek-KB) -> Abstract Index -> Sub-chunk Index -> Context + Query -> LLM (DeepSeek-R1) -> Response

- **Critical path:** The quality of the **Hypothetical Question Generation**. If the generated questions do not match user intent, the retrieval fails, and the LLM has no context to answer.

- **Design tradeoffs:**
  - **RAG vs. Fine-Tuning:** Authors chose RAG to allow "real-time" knowledge updates and avoid the "catastrophic forgetting" or high cost of fine-tuning. The tradeoff is higher latency (retrieval time) and token costs per query compared to a purely parametric model.
  - **Recall vs. Noise:** The hierarchical retrieval (400 docs -> 20 chunks) is a precision-focused tradeoff. It risks missing data in the first filter to ensure the final context is highly relevant.

- **Failure signatures:**
  - **Ref6 Error (as seen in paper):** Retrieval returning a document about "terpenoids in breast cancer" when asked about "paclitaxel mechanism," which was a retrieval error likely due to semantic overlap in the vector space but lack of specific entity precision.
  - **Knowledge Latency:** If the KB is not updated, the system will confidently cite old (potentially retracted) papers.
  - **Context Window Saturation:** If the "Review" mode generates too many sub-questions, the context window might overflow or the synthesis step might lose fine-grained details.

- **First 3 experiments:**
  1. **Retrieval Ablation:** Turn off the "Hypothetical Question" indexing and query raw chunks. Compare accuracy on the 41-entry test set to validate the paper's claim about the indexing strategy.
  2. **Threshold Tuning:** Vary the cosine similarity threshold (e.g., 0.5 vs 0.7 vs 0.9) to find the optimal balance between "No Answer" (threshold too high) and "Hallucination/Irrelevant Answer" (threshold too low).
  3. **Backbone Swap:** The paper claims modularity. Swap DeepSeek-R1 for a smaller, cheaper model (e.g., Llama-3-8B) to see if the *retrieval* quality drives performance, or if the *reasoning* capability of the specific LLM is the bottleneck for synthesizing the final answer.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the integration of heterogeneous, unstructured sources (books, forums) impact the hallucination rate and retrieval precision compared to the current academic-only knowledge base?
- **Basis in paper:** [explicit] The Conclusion states plans to expand the knowledge base to incorporate books, online forums, and encyclopedic content.
- **Why unresolved:** The current high performance relies on rigorously peer-reviewed, structured literature; informal sources introduce noise that may degrade the RAG pipeline's accuracy.
- **What evidence would resolve it:** A comparative benchmark evaluating TeroSeek's hallucination frequency and factual accuracy before and after ingesting non-peer-reviewed datasets.

### Open Question 2
- **Question:** Can the hierarchical retrieval strategy be refined to eliminate "false positive" matches where documents are topically similar but contextually irrelevant?
- **Basis in paper:** [inferred] In the case study (Figure 3), the system retrieved "Ref6" (terpenoids in breast cancer) for a query on Paclitaxel's mechanism, despite the paper containing no mention of Paclitaxel.
- **Why unresolved:** The current two-layer strategy with a 0.7 cosine similarity threshold failed to distinguish between general topic relevance and specific factual content.
- **What evidence would resolve it:** Failure analysis of retrieval precision using test cases specifically designed to trap semantic similarity without factual overlap.

### Open Question 3
- **Question:** Is the reported performance boost robust across a larger, more diverse benchmark than the 41-question test set utilized in the study?
- **Basis in paper:** [inferred] The authors refined their test set to 41 entries after excluding cases where models produced identical responses.
- **Why unresolved:** A sample size of 41 questions may be insufficient to fully represent the complexity of 20 years of terpenoid literature across chemistry, pharmacology, and biology.
- **What evidence would resolve it:** Validation of TeroSeek-normal against a standardized, expanded dataset (e.g., >500 questions) with statistical significance testing.

## Limitations
- The specific embedding model and its compatibility with question generation remain unspecified
- The test set (41 questions) is relatively small for claiming general superiority
- No comparison to fine-tuned models that could serve as a stronger baseline

## Confidence
- **High:** TeroSeek's superior performance on the terpenoid benchmark compared to general LLMs (accuracy improvements of 0.29-0.32 points are statistically robust)
- **Medium:** The architectural choices (hierarchical retrieval, hypothetical questions) are reasonable given the domain but lack ablation studies proving their necessity
- **Low:** Claims about real-time knowledge updates and citation accuracy depend on KB maintenance practices not detailed in the paper

## Next Checks
1. **Retrieval Ablation Test:** Remove the hypothetical question generation step and query raw text chunks directly. Compare accuracy on the 41-question test set to quantify the indexing strategy's contribution.

2. **Threshold Sensitivity Analysis:** Systematically vary the cosine similarity threshold (0.5, 0.6, 0.7, 0.8, 0.9) and measure the trade-off between "No Answer" responses and retrieval precision on a sample of 20 test queries.

3. **Cross-Domain Transfer:** Apply the identical TeroSeek architecture to a different specialized domain (e.g., protein engineering) using domain-appropriate literature. If accuracy drops significantly without domain-specific tuning, it suggests the system's performance relies heavily on terpenoid-specific knowledge patterns rather than generalizable RAG principles.