---
ver: rpa2
title: 'MoTE: Mixture of Task-specific Experts for Pre-Trained ModelBased Class-incremental
  Learning'
arxiv_id: '2506.11038'
source_url: https://arxiv.org/abs/2506.11038
tags:
- learning
- task
- expert
- mote
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MoTE (Mixture of Task-specific Experts),
  a novel approach for exemplar-free class-incremental learning using pre-trained
  models. The key challenge addressed is catastrophic forgetting and dimensional inconsistency
  when learning new tasks sequentially.
---

# MoTE: Mixture of Task-specific Experts for Pre-Trained ModelBased Class-incremental Learning

## Quick Facts
- arXiv ID: 2506.11038
- Source URL: https://arxiv.org/abs/2506.11038
- Authors: Linjie Li; Zhenyu Wu; Yang Ji
- Reference count: 40
- Achieves state-of-the-art average accuracy while being approximately 30% faster in inference

## Executive Summary
MoTE (Mixture of Task-specific Experts) introduces a novel approach for exemplar-free class-incremental learning using pre-trained models. The method addresses catastrophic forgetting and dimensional inconsistency when learning new tasks sequentially by assigning lightweight adapters as task-specific experts. Through task-aware expert filtering and weighted multi-expert joint inference, MoTE maintains performance on old tasks while enabling full optimization of new tasks, achieving superior results across five benchmark datasets.

## Method Summary
MoTE assigns a lightweight adapter as a task-specific expert for each incoming task, preserving old task representations while enabling full optimization of new tasks. During inference, task-aware expert filtering discards unreliable experts whose predictions fall outside their task scope, followed by weighted multi-expert joint inference using confidence and self-confidence scores to handle ambiguous samples. The approach is evaluated on five benchmark datasets (CIFAR100, CUB200, ImageNet-A, ImageNet-R, VTAB) and demonstrates state-of-the-art average accuracy with approximately 30% faster inference compared to existing methods.

## Key Results
- Achieves state-of-the-art average accuracy across five benchmark datasets
- Demonstrates approximately 30% faster inference compared to existing methods
- Introduces Adapter-Limited MoTE to explore the trade-off between adapter expansion and model performance

## Why This Works (Mechanism)
The paper addresses the fundamental challenge of catastrophic forgetting in class-incremental learning by using task-specific adapters that preserve representations of old tasks while allowing full optimization of new tasks. The task-aware expert filtering mechanism ensures that only relevant experts contribute to predictions for each sample, while the weighted multi-expert inference handles ambiguous samples through confidence-based aggregation. This approach maintains a balance between stability (preserving old knowledge) and plasticity (learning new tasks) without requiring stored exemplars.

## Foundational Learning
- **Catastrophic forgetting**: Neural networks tend to overwrite previously learned information when trained on new tasks, requiring mechanisms to preserve old knowledge while learning new tasks
- **Class-incremental learning**: Learning new classes sequentially without access to previous data, demanding strategies to maintain performance across all learned classes
- **Adapter-based fine-tuning**: Lightweight modules added to pre-trained models that enable task-specific adaptation without modifying the base model parameters
- **Confidence-based inference**: Using model confidence scores to weight contributions from multiple experts, improving robustness for ambiguous samples
- **Exemplar-free learning**: Learning without storing any examples from previous tasks, which is more realistic for many real-world applications

## Architecture Onboarding
**Component map**: Pre-trained model -> Task-specific adapters -> Task-aware filtering -> Weighted multi-expert inference
**Critical path**: New task arrives → Adapter created → Old task representations preserved → Inference uses filtering + weighted aggregation
**Design tradeoffs**: Memory vs performance (more adapters improve accuracy but increase memory), inference speed vs accuracy (filtering speeds up inference but may discard useful information)
**Failure signatures**: Performance degradation on old tasks indicates catastrophic forgetting, poor confidence calibration suggests filtering mechanism issues
**Three first experiments**: 1) Ablation study removing task-aware filtering to measure its contribution, 2) Comparison with baseline methods on CIFAR100 with varying number of tasks, 3) Memory usage analysis across different adapter configurations

## Open Questions the Paper Calls Out
None

## Limitations
- The 30% faster inference claim requires verification of baseline methods and measurement conditions
- Long-term stability of adapter-based representation preservation across many incremental tasks needs extended validation
- The Adapter-Limited MoTE analysis appears limited to a specific subset of experiments without comprehensive scaling studies

## Confidence
High confidence: The core methodology of using task-specific adapters for class-incremental learning is well-established in the literature
Medium confidence: State-of-the-art performance claims across five benchmark datasets are promising but require full experimental verification
Low confidence: The 30% inference speed improvement needs independent verification across different hardware configurations

## Next Checks
1. Conduct ablation studies isolating the contribution of each component (task-aware filtering, weighted inference, adapter architecture) to verify claimed improvements
2. Test MoTE on datasets with significantly more classes and tasks to evaluate scalability limits and long-term stability
3. Perform cross-dataset generalization experiments where adapters trained on one dataset are applied to related but different datasets to assess robustness