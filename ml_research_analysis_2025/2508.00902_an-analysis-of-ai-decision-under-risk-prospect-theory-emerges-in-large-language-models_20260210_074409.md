---
ver: rpa2
title: 'An analysis of AI Decision under Risk: Prospect theory emerges in Large Language
  Models'
arxiv_id: '2508.00902'
source_url: https://arxiv.org/abs/2508.00902
tags:
- chance
- risk
- points
- scenarios
- framing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study tested prospect theory\u2014which predicts humans take\
  \ more risk when feeling losses versus gains\u2014in large language models (LLMs)\
  \ by presenting them with scenarios having identical expected values but framed\
  \ as losses or gains. Across civilian and military contexts, LLMs exhibited classic\
  \ prospect theory effects, especially in military scenarios, but also showed context-dependent\
  \ variations (e.g., reverse framing in career decisions)."
---

# An analysis of AI Decision under Risk: Prospect theory emerges in Large Language Models

## Quick Facts
- **arXiv ID:** 2508.00902
- **Source URL:** https://arxiv.org/abs/2508.00902
- **Reference count:** 26
- **Primary result:** LLMs exhibit classic prospect theory effects, taking more risk when decisions are framed as losses rather than gains, but these effects disappear in purely mathematical contexts

## Executive Summary
This study demonstrates that large language models (LLMs) exhibit human-like decision-making biases described by prospect theory, taking more risk when scenarios are framed as losses versus gains. Testing across seven scenarios (business, career, sports, and military contexts) with 25 iterations per condition per model, the research found consistent framing effects that vary by semantic domain. Crucially, these biases vanished entirely when scenarios were presented as abstract mathematical problems, indicating that LLMs acquire human cognitive heuristics through language patterns rather than formal reasoning. Different LLMs displayed distinct "cognitive personalities" in their risk profiles, suggesting stable model-specific decision tendencies.

## Method Summary
The study tested prospect theory by presenting LLMs with three-option decision scenarios having identical expected values but framed as gains or losses. Seven scenarios (3 civilian, 4 geopolitical) were tested with 25 iterations per condition per model across five different LLMs (GPT-4o, o3, Claude Sonnet 4, Claude Sonnet 4 Thinking, Gemini 1.5 Pro). Responses were coded on a 0-2 risk scale and framing effects measured as the difference in average risk scores between loss and gain frames. A mathematical control condition used symbolic notation to test if framing effects persisted without natural language context. Mann-Whitney U tests assessed statistical significance.

## Key Results
- LLMs consistently took more risk in loss-framed scenarios across all domains
- Military scenarios generated the largest framing effects, while business scenarios showed minimal effects
- Framing effects disappeared entirely when scenarios were presented as abstract mathematical problems
- Different LLMs displayed distinct risk profiles ("cognitive personalities") that persisted across scenarios
- The "Conflit Frontalier" French scenario showed higher risk tolerance than its English equivalent

## Why This Works (Mechanism)

### Mechanism 1: Linguistic Embedding of Decision Heuristics
LLMs acquire human-like risk heuristics through statistical patterns in language, not explicit instruction. The semantic content of scenarios activates domain-specific associations from training data that encode implicit risk norms (e.g., "desperate times call for desperate measures"). These associations override formal probability calculations. Core assumption: Language statistically encodes human cognitive biases; models learn these patterns through next-token prediction. Evidence: Framing effects disappear entirely in pure mathematical conditions.

### Mechanism 2: Domain-Specific Semantic Activation
Framing effects are not uniform but depend on the semantic context (military, business, sports, career). Different semantic domains activate distinct "cognitive scripts" with pre-encoded risk attitudes. Military and sports contexts trigger competitive/combative frameworks; business contexts activate prudential frameworks; career contexts may trigger opportunity-seeking frameworks. Core assumption: Training data contains sufficiently distinct linguistic patterns across domains to create differentiated heuristic activation. Evidence: Military scenarios generate far larger framing effects than civilian settings.

### Mechanism 3: Emergent Model-Specific "Cognitive Personalities"
Different LLM architectures/training produce stable, reproducible differences in risk profiles. Training data composition, RLHF procedures, and architectural choices create distinctive patterns of heuristic activation that persist across scenarios. Core assumption: Model differences are systematic rather than noise; they reflect stable properties of model "psychology." Evidence: GPT-4o exhibits the most extreme application of semantic heuristics; Claude is consistently hawkish across military scenarios.

## Foundational Learning

- **Prospect Theory (Kahneman & Tversky)**: Humans take more risk in loss domains than gain domains, even with mathematically equivalent options. Quick check: Given a certain $50 vs. a 50% chance at $100, would you choose differently if you were "losing" vs. "gaining"?
- **Framing Effects**: How information presentation (not content) changes decisions. Quick check: Rewrite "preventing 0.5 points of loss" as a gain-frame equivalent.
- **Expected Value Equivalence**: Critical experimental control—all options must have equal EV so any preference differences are attributable to framing. Quick check: Verify that 100%×0.5 = 50%×1.0 + 50%×0 = 25%×2.0 + 75%×0.

## Architecture Onboarding

- **Component map:** Scenario templates (Appendix B) -> Multi-provider API layer -> Single prompt collection (decision + rationale) -> Rationale coder classifier -> Statistical analyzer
- **Critical path:** Generate gain/loss frame pairs with identical EVs → Collect 25+ iterations per condition → Parse decisions + rationales; code for EV calculation → Compare risk scores across frames; validate with mathematical control
- **Design tradeoffs:** Three-option vs. two-option design (more granular risk measurement but diverges from classic paradigm); Original scenarios vs. replications (reduces memorization risk but introduces authorship confounds); Free-choice vs. forced-choice mathematical condition (free-choice reveals indifference; forced-choice reveals tie-breaking heuristics)
- **Failure signatures:** Safety refusals (Gemini 2.5-Pro declined military scenarios); Non-parsing responses (2.3% excluded; recoverable with robust regex); Language-specific divergence (French "Conflit Frontalier" showed higher ESCALATE rates than English equivalents)
- **First 3 experiments:** 1) Add 2-3 new semantic domains (medical, legal, romantic) to map generalization of domain-specific effects; 2) Test semi-formal language conditions between natural language and pure math to identify transition point where framing collapses; 3) Run 50+ iterations per model on subset of scenarios to quantify within-model variance vs. between-model variance

## Open Questions the Paper Calls Out

### Open Question 1
Do framing effects in LLMs vary systematically across natural languages due to embedded cultural heuristics, or are they artifacts of specific token distributions? The study only tested one non-English language (French) in a single scenario, leaving it unclear if this "cultural context" finding generalizes to other languages or is a specific artifact of the translation. Evidence would require replicating the experimental scenarios across diverse languages (e.g., Mandarin, Arabic, Spanish).

### Open Question 2
To what degree do model safety guard-rails account for the "semantic dampening" of risk-seeking behavior in geopolitical scenarios? The author attributes dampening to "prudential heuristics" and "responsible actor" frameworks learned from language, but cannot disentangle this from the influence of safety alignment or refusal training. Evidence would require testing "base" models against their aligned counterparts to isolate the effect of safety training on risk appetite.

### Open Question 3
Can explicit prompting for mathematical calculation override the susceptibility to framing effects in natural language contexts? The paper establishes a dichotomy between pure math and pure language, but it is unresolved whether models can be prompted to maintain mathematical rationality when semantic frames are present. Evidence would require an intervention study where models are explicitly instructed to calculate expected values before selecting an option in natural language scenarios.

## Limitations

- Small sample size (25 iterations per condition) may not fully capture model variability
- Use of original scenario templates rather than validated experimental paradigms introduces potential authorship confounds
- Exclusion of ~2.3% of responses due to parsing difficulties suggests methodology may not generalize cleanly to all LLM outputs

## Confidence

**High Confidence:**
- Framing effects emerge in LLMs across multiple domains
- These effects disappear when scenarios are presented as abstract mathematical problems
- Different LLMs exhibit distinct risk profiles ("cognitive personalities")

**Medium Confidence:**
- Military scenarios produce larger framing effects than civilian contexts
- The "reverse framing" in career scenarios represents a genuine domain-specific effect rather than statistical noise
- The three-option design captures meaningful risk preference gradients

**Low Confidence:**
- The specific cognitive mechanisms underlying domain-dependent framing effects
- Whether the observed patterns would replicate with larger sample sizes
- The stability of model-specific "personalities" across different testing conditions

## Next Checks

1. **Domain boundary extension**: Add 2-3 new semantic domains (medical, legal, romantic) to test whether domain-specific framing effects generalize beyond the current seven scenarios, helping establish the breadth of semantic activation patterns.

2. **Abstraction gradient test**: Implement semi-formal language conditions between natural language and pure math (e.g., "prevent 0.5 points of loss" vs. symbolic notation) to identify the precise transition point where framing effects collapse.

3. **Cross-model variance quantification**: Run 50+ iterations per model on a subset of scenarios to determine whether observed model-specific "personalities" represent stable properties or are artifacts of small sample sizes.