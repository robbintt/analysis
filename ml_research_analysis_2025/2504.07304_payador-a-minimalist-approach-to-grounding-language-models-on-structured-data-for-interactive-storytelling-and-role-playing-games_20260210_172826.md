---
ver: rpa2
title: 'PAYADOR: A Minimalist Approach to Grounding Language Models on Structured
  Data for Interactive Storytelling and Role-playing Games'
arxiv_id: '2504.07304'
source_url: https://arxiv.org/abs/2504.07304
tags:
- world
- state
- games
- player
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the world-update problem in Interactive Storytelling,\
  \ where systems must handle player actions that weren't preprogrammed. The authors\
  \ propose PAYADOR, a method that grounds a Large Language Model (LLM) to a minimal\
  \ structured representation of the fictional world\u2014consisting of items, locations,\
  \ and characters with key attributes and descriptive text."
---

# PAYADOR: A Minimalist Approach to Grounding Language Models on Structured Data for Interactive Storytelling and Role-playing Games

## Quick Facts
- arXiv ID: 2504.07304
- Source URL: https://arxiv.org/abs/2504.07304
- Reference count: 13
- One-line primary result: Grounding LLM outputs to minimal structured world state prevents hallucination and maintains coherence in interactive storytelling

## Executive Summary
PAYADOR addresses the world-update problem in Interactive Storytelling by grounding a Large Language Model to a minimal structured representation of the fictional world. Instead of mapping player actions to preprogrammed responses, PAYADOR predicts world changes, validates them against structured state, and updates only when consistent. Experiments demonstrate improved coherence, better dialogue state tracking, and prevention of some LLM inconsistencies. The method is open-source and adaptable for co-creative RPG systems.

## Method Summary
PAYADOR uses a minimal structured world representation with three components: Items (name, descriptions list, gettable boolean), Locations (name, descriptions, items list, connecting/blocked locations), and Characters (name, descriptions, location, inventory). The system renders visible components, constructs prompts with few-shot examples, calls Gemini API to predict structured changes, parses outputs, runs consistency checks against current state, and updates only valid changes. This approach prevents hallucination of non-existent entities while maintaining narrative flexibility through free-text descriptions.

## Key Results
- Grounding approach maintains world coherence and prevents some LLM inconsistencies
- Better tracking of dialogue state through structured world representation
- Local rendering keeps prompt length bounded regardless of world size
- System successfully rejects invalid actions (e.g., dropping items not in inventory)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structured state grounding constrains LLM outputs to valid world modifications
- Mechanism: System maintains authoritative structured representation; LLM proposes changes in structured format; changes parsed and validated against current state before update; invalid proposals rejected
- Core assumption: LLM reliably outputs parseable structured change descriptions; validation logic covers relevant constraints
- Evidence anchors: Abstract mentions consistency checking; Figure 5 demonstrates rejection of bazooka placement when item absent; SNAP corpus mentions similar consistency concerns

### Mechanism 2
- Claim: Rendering only locally-visible components keeps prompts bounded regardless of world size
- Mechanism: Each LLM call receives text rendering of components accessible from player's current location, not entire world graph
- Core assumption: Local visibility sufficient for coherent narrative; off-screen state changes handled separately
- Evidence anchors: Page 3 states prompt built using only visible components; length does not drastically grow; WhatELSE corpus discusses abstraction levels but not this technique

### Mechanism 3
- Claim: Descriptions as free-text strings preserve narrative richness while structured attributes enforce consistency-critical data
- Mechanism: Each component stores typed attributes for mechanical consistency plus free-text descriptions for narrative flexibility
- Core assumption: Consistency-critical attributes enumerable in advance; description text doesn't require structural enforcement
- Evidence anchors: Table 1 shows component schema with Descriptions: List[String]; Figure 4 shows LLM correctly matching "toy hammer" to green hammer via description

## Foundational Learning

- **Dialogue State Tracking (DST)**: PAYADOR uses structured world state as dialogue state representation rather than conversation history. Understanding DST clarifies why resetting state from structured data after each turn prevents cascading hallucinations.
  - Quick check: Can you explain why resetting state from structured data after each turn prevents cascading hallucinations?

- **Few-shot Prompting**: LLM module uses few-shot examples to enforce structured output format for predicted changes.
  - Quick check: What failure mode occurs if few-shot examples don't cover edge cases like "no changes detected"?

- **Knowledge Representation Trade-offs (expressiveness vs. tractability)**: Minimalist representation deliberately excludes complex relations to keep validation tractable.
  - Quick check: If you needed to add a "locked by" relation between doors and keys, which component would you modify and what validation would you add?

## Architecture Onboarding

- **Component map**: World State Store -> State Renderer -> Prompt Builder -> LLM Oracle -> Change Parser -> Consistency Validator -> State Updater -> (loop)
- **Critical path**: User input → State Renderer → Prompt Builder → LLM Oracle → Change Parser → Consistency Validator → State Updater → (loop)
- **Design tradeoffs**:
  - Minimal representation vs. expressiveness: Only location, inventory, blocked passages tracked; complex relations require schema extension
  - LLM-dependent validation vs. rule-based: Consistency checks are symbolic, but semantic validation relies entirely on LLM
  - Local visibility vs. global coherence: Prompt efficiency gained at cost of off-screen entity tracking
- **Failure signatures**:
  - LLM outputs unparseable change format → Parser exception, no state update
  - LLM hallucinates entity not in descriptions → Consistency check fails, change rejected
  - Validation rules miss a constraint → Invalid state committed
  - Description ambiguity → LLM maps "hammer" to wrong item when multiple exist
- **First 3 experiments**:
  1. Baseline consistency test: Run Figure 5 bazooka scenario; confirm rejection when item absent, confirm acceptance when previously picked up
  2. Prompt length scaling test: Generate worlds of 10, 50, 100 locations; measure prompt token count per turn
  3. Parser robustness test: Manually inject malformed LLM outputs; verify graceful degradation

## Open Questions the Paper Calls Out

- Can prompt engineering alone resolve common-sense reasoning failures (such as allowing access to locked locations without keys), or is additional logical machinery required?
  - Basis: Paper demonstrates failure case where LLM allows player to enter locked kitchen without key, but doesn't determine solution
  - Evidence needed: Comparative evaluation of optimized prompts versus hybrid neuro-symbolic architectures on logic-constrained scenarios

- Does the grounding approach transfer effectively to non-English languages, particularly those with fewer NLP resources?
  - Basis: Paper states planning to test approach for other languages
  - Evidence needed: Benchmarking world-update consistency and narrative quality in non-English languages

- How does the minimalist representation impact system's ability to support complex, long-term narrative generation and automated content creation?
  - Basis: Paper presents "playable proof of concept" and suggests representation is minimal enough for "more complex IS systems"
  - Evidence needed: Testing system in full-length RPG campaign to measure consistency and narrative depth over time

## Limitations
- Lacks systematic quantitative metrics for measuring coherence maintenance and hallucination prevention rates
- Parser and validation robustness details are unclear, with limited evidence of comprehensive edge case coverage
- Minimalist representation may not scale to complex domains requiring temporal constraints or rich relational data

## Confidence

**High Confidence (8-10/10)**:
- Core mechanism of grounding LLM outputs to structured state representations is sound and implementable
- Local rendering approach effectively bounds prompt length regardless of world size
- Hybrid representation (structured attributes + free-text descriptions) is a practical design choice

**Medium Confidence (4-7/10)**:
- Method successfully prevents all LLM hallucinations - evidence shows specific cases work but comprehensive coverage unproven
- System maintains coherence over extended narrative sessions - only short examples demonstrated
- Approach scales to more complex fictional worlds - only tested on toy examples

**Low Confidence (0-3/10)**:
- No significant confidence gaps identified beyond those already noted

## Next Checks

1. **Comprehensive Consistency Testing**: Design test suite of 100+ player actions across diverse scenarios and measure true positive/false positive rate of consistency checker in rejecting hallucinated entities.

2. **Extended Session Coherence Analysis**: Implement play session simulator running 1000+ turns with random player actions, logging every state update. Analyze frequency and types of coherence failures over time compared to ungrounded LLM baseline.

3. **Schema Extension Validation**: Extend world representation to include item properties (weight, durability) and character relationships. Implement corresponding validation rules and test whether PAYADOR maintains coherence while handling richer domain.