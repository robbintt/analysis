---
ver: rpa2
title: Investigating cybersecurity incidents using large language models in latest-generation
  wireless networks
arxiv_id: '2504.13196'
source_url: https://arxiv.org/abs/2504.13196
tags:
- data
- adversarial
- language
- signal
- large
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates the use of large language models (LLMs)
  to detect and explain cybersecurity incidents in next-generation wireless networks.
  The authors emulate wireless network data using the DeepMIMO tool and perform adversarial
  data poisoning attacks on regression models.
---

# Investigating cybersecurity incidents using large language models in latest-generation wireless networks

## Quick Facts
- arXiv ID: 2504.13196
- Source URL: https://arxiv.org/abs/2504.13196
- Reference count: 28
- Primary result: Gemma-7b LLM achieves Precision=0.89, Recall=0.89, F1-Score=0.89 on adversarial data poisoning detection in wireless networks

## Executive Summary
This paper investigates using large language models (LLMs) to detect and explain cybersecurity incidents in next-generation wireless networks. The authors emulate wireless network data using the DeepMIMO tool and perform adversarial data poisoning attacks on regression models using the Fast Gradient Sign Method (FGSM). They fine-tune six LLMs to classify poisoned versus clean data, achieving the best performance with Gemma-7b (Precision=0.89, Recall=0.89, F1-Score=0.89). The study also explores LLM explainability capabilities, showing that Gemma-7b can identify inconsistencies in compromised data, analyze feature importance, and provide recommendations for mitigating adversarial attacks.

## Method Summary
The methodology involves generating wireless network data using the DeepMIMO emulator (Boston5G_28 scenario) with 12 features per user, then applying FGSM adversarial attacks to create poisoned samples. The dataset (1:1 ratio of benign to malicious) is split into training (41,837 records) and test sets (500 records). Tabular data is converted to descriptive text templates, and six LLMs are fine-tuned using the Unsloth library with Supervised Fine-Tuning Trainer (200 iterations, AdamW-8bit optimizer, max_seq_length=2048). Classification performance is evaluated using precision, recall, and F1-score metrics, while explainability is assessed through prompt engineering that extracts chain-of-thought reasoning about classification decisions.

## Key Results
- Gemma-7b LLM achieves the highest classification performance: Precision=0.89, Recall=0.89, F1-Score=0.89
- Ensemble classifiers outperform fine-tuned LLMs (0.96 F1-score) but lack explainability
- LLM explainability analysis correctly identifies feature importance (distance, power, line of sight) and provides mitigation recommendations
- Test set limited to 500 samples due to multi-second token generation times per inference

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuned LLMs can detect gradient-based adversarial poisoning in wireless network data by learning statistical signatures of perturbations.
- Mechanism: FGSM computes loss gradients with respect to input data and perturbs features in the direction that maximizes model error. The fine-tuned LLM learns to recognize these perturbation patterns when tabular data is transformed to text representations, classifying samples as Benign or Malicious.
- Core assumption: FGSM perturbations create detectable statistical anomalies that persist through tabular-to-text transformation.
- Evidence anchors:
  - [abstract] "The authors emulate wireless network data using the DeepMIMO tool and perform adversarial data poisoning attacks on regression models."
  - [section] Page 3: "A basic approach for generating adversarial samples... is the Fast Gradient Sign Method (FGSM)... computes the gradients of the loss function with respect to the original data, and then uses the sign of the gradients to generate a new data sample that maximizes the loss."
  - [corpus] "Adversarial Attacks Against Deep Learning-Based Radio Frequency Fingerprint Identification" confirms wireless ML vulnerability to gradient-based attacks.
- Break condition: If attackers use adaptive methods that minimize detectable perturbation signatures (not just maximize loss), classification may fail.

### Mechanism 2
- Claim: Transforming numerical network features into descriptive text enables LLMs to apply pre-trained language understanding to domain-specific classification.
- Mechanism: Twelve wireless signal features (coordinates, pathloss, angles, power, etc.) are converted to text templates following instruction-input-output format. Supervised fine-tuning updates model parameters to adapt the LLM to map feature descriptions to binary labels.
- Core assumption: Semantic reasoning capabilities from pre-training transfer to numerical pattern recognition in wireless domain data.
- Evidence anchors:
  - [abstract] "Fine-tuning of large language models was performed on the prepared data of the emulated wireless network segment."
  - [section] Page 5: "Large language models work with text data in the form of a sequence of input tokens, so tabular data must be presented in a descriptive form."
  - [corpus] "The Dark Side of Digital Twins" shows related LSTM vulnerability to adversarial manipulation in infrastructure systems.
- Break condition: If text transformation loses numerical precision or feature relationships, classification accuracy degrades.

### Mechanism 3
- Claim: Prompt engineering can extract interpretable chain-of-thought reasoning from fine-tuned LLMs about their classification decisions.
- Mechanism: Three prompt variants (chain-of-thought, feature importance, comparative analysis) query the fine-tuned model. The LLM generates textual explanations identifying statistical inconsistencies, feature contributions, and mitigation recommendations.
- Core assumption: Generated explanations reflect actual decision factors, not post-hoc rationalizations.
- Evidence anchors:
  - [abstract] "Gemma-7b can identify inconsistencies in compromised data, analyze feature importance, and provide recommendations for mitigating adversarial attacks."
  - [section] Page 6: Table 3 shows model outputs including "Statistical Analysis," "Domain Knowledge," and feature importance identification (distance, power, line of sight).
  - [corpus] Limited direct corpus evidence for explainability mechanism effectiveness; related papers focus on attack methods rather than explanation extraction.
- Break condition: If explanations are confabulations uncorrelated with actual model reasoning, operational trust may be misplaced.

## Foundational Learning

- Concept: **Fast Gradient Sign Method (FGSM)**
  - Why needed here: Understanding gradient-based perturbation generation is essential to understanding what patterns LLMs detect.
  - Quick check question: If ε = 5 and the gradient of loss w.r.t. input is [0.1, -0.3, 0.2], what perturbation does FGSM add?

- Concept: **Supervised Fine-Tuning vs. In-Context Learning**
  - Why needed here: The paper uses parameter updates via fine-tuning; understanding trade-offs informs architecture decisions.
  - Quick check question: What are the key differences between updating model parameters via fine-tuning versus providing examples in the prompt context?

- Concept: **Wireless Signal Features (Pathloss, DoA/DoD, Line of Sight)**
  - Why needed here: The 12 DeepMIMO features have domain-specific meaning; understanding relationships helps validate LLM explanations.
  - Quick check question: If a user moves outside line of sight of the base station, how should pathloss and signal power change?

## Architecture Onboarding

- Component map:
  DeepMIMO emulator (Boston5G_28) -> 12 wireless features per user -> FGSM adversarial attack (ε=1-10, fract=0.99) -> poisoned dataset (MSE +33%, R² -10%) -> Tabular-to-text transformation -> Fine-tuning (Unsloth, AdamW-8bit, 200 iterations) -> Binary classification (Benign/Malicious) -> Explainability prompts

- Critical path:
  1. DeepMIMO generates clean signal features
  2. FGSM creates poisoned variants
  3. Combined dataset (1:1 ratio) split: training (41,837) / test (500)
  4. Tabular-to-text transformation
  5. Supervised fine-tuning (200 iterations)
  6. Inference with optional explainability prompts

- Design tradeoffs:
  - Accuracy vs. Explainability: LLMs (89% F1) lower than ensemble classifiers but offer transparency
  - Test set size vs. Inference time: Only 500 test samples due to multi-second token generation
  - Model size vs. Deployment: Lightweight models (<8B parameters) for practical infrastructure integration

- Failure signatures:
  - Low classification metrics -> insufficient training data or poor text transformation
  - Inconsistent explanations -> LLM features don't align with SHAP ground truth (distance, power, LoS)
  - High training loss -> learning rate issues or insufficient iterations

- First 3 experiments:
  1. Replicate FGSM attack and Gemma-7b fine-tuning on Boston5G_28 to verify F1 ≈ 0.89
  2. Test fine-tuned model against different adversarial methods (e.g., PGD, C&W) to evaluate robustness beyond FGSM
  3. Compare LLM-identified important features against SHAP analysis to measure explanation fidelity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can fine-tuned LLMs be effectively integrated with ensemble classifiers to leverage the high accuracy of the latter and the explainability of the former?
- Basis in paper: [explicit] The conclusion states that integrating LLMs with binary classifiers is necessary for "faster and more accurate response" and to overcome the explainability barrier.
- Why unresolved: The study demonstrates that LLMs are currently inferior to ensemble classifiers in accuracy and resource intensity but does not implement the proposed hybrid architecture.
- What evidence would resolve it: A proposed system architecture where an ensemble classifier triggers an LLM explainer, achieving both high F1-scores and interpretability.

### Open Question 2
- Question: Does the detection capability of the Gemma-7b model generalize to diverse adversarial attack vectors beyond data poisoning?
- Basis in paper: [inferred] The methodology relies exclusively on synthesizing adversarial examples using the Fast Gradient Sign Method (FGSM).
- Why unresolved: Real-world wireless networks face varied threats (e.g., jamming, evasion) that may not exhibit the same feature inconsistencies as the gradient-based poisoning attacks tested.
- What evidence would resolve it: Performance metrics of the fine-tuned model against a test set containing non-FGSM attack types.

### Open Question 3
- Question: Can the inference latency of LLMs be reduced to facilitate real-time investigation of cybersecurity incidents?
- Basis in paper: [explicit] The authors note that the test sample size was limited to 500 records because "token generation can take up to several seconds."
- Why unresolved: While accuracy is acceptable, the time required for text generation currently hinders the model's deployment in live, high-speed network monitoring.
- What evidence would resolve it: Benchmarks showing sub-second inference times or successful deployment in a high-throughput 5G/6G testbed.

## Limitations
- Limited test set size (500 samples) may not adequately capture performance across diverse attack scenarios
- Adversarial attack restricted to single FGSM method with fixed parameters, leaving robustness to other attacks unknown
- Text transformation process may introduce information loss affecting both classification and explanation quality
- Explainability mechanism lacks ground truth validation to confirm explanations reflect actual model reasoning

## Confidence
- **High Confidence**: Classification performance claims (F1=0.89 for Gemma-7b) - clear methodology though limited by small test set size
- **Medium Confidence**: Explainability claims - demonstrates coherent explanations but limited validation against actual decision-making
- **Low Confidence**: Real-world deployment readiness - emulated data without operational network validation, computational overhead concerns

## Next Checks
1. **Cross-Attack Robustness Test**: Evaluate fine-tuned models against multiple adversarial attack methods (PGD, C&W, HopSkipJump) to assess whether classification performance generalizes beyond FGSM.
2. **Explainability Fidelity Validation**: Compare LLM-generated explanations against SHAP feature importance scores on a held-out validation set to measure correlation between identified features.
3. **Real Network Deployment Simulation**: Replace DeepMIMO-generated data with telemetry from an operational wireless network to validate performance on real-world data distributions.