---
ver: rpa2
title: Trainable Reference-Based Evaluation Metric for Identifying Quality of English-Gujarati
  Machine Translation System
arxiv_id: '2510.05113'
source_url: https://arxiv.org/abs/2510.05113
tags:
- evaluation
- translation
- system
- were
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces MATRA, a supervised-learning-based reference
  MT evaluation metric for English-Gujarati translation. The authors extract 25 features
  from MT outputs and reference translations, then train two deep neural network variants
  (6 and 10 hidden layers, 500 epochs each) to predict human evaluation scores.
---

# Trainable Reference-Based Evaluation Metric for Identifying Quality of English-Gujarati Machine Translation System

## Quick Facts
- **arXiv ID**: 2510.05113
- **Source URL**: https://arxiv.org/abs/2510.05113
- **Reference count**: 18
- **Key outcome**: MATRA DNN variants (6 and 10 hidden layers) trained on 24 features outperform BLEU, METEOR, LEPOR, chrF++, COMET on 1,000 Gujarati MT test sentences, showing strong correlation with human judgments

## Executive Summary
This study introduces MATRA, a supervised-learning-based reference MT evaluation metric for English-Gujarati translation. The authors extract 25 features from MT outputs and reference translations, then train two deep neural network variants (6 and 10 hidden layers, 500 epochs each) to predict human evaluation scores. Tested on 1,000 sentences from agriculture and education domains across seven MT systems, MATRA variants outperform existing metrics (BLEU, METEOR, LEPOR, chrF++, COMET) and show strong positive correlation with human judgments (HEval). The results demonstrate MATRA's ability to produce evaluation scores comparable to human assessments, offering a reliable automatic evaluation method for Gujarati MT systems.

## Method Summary
The authors developed a trainable reference-based evaluation metric by extracting 24 features from English-Gujarati MT outputs and reference translations, including lexical, POS, and embedding cosine similarities, n-gram probability quartiles, REKHA scores, and BLEU. These features were fed into two deep neural network variants (6 and 10 hidden layers) trained to minimize MSE between predicted and averaged human evaluation scores. The model was trained on 80,000 samples from 16 MT systems across administration, health, and tourism domains, then tested on 1,000 sentences from agriculture and education domains using seven MT systems.

## Key Results
- MATRA-1 (6 layers) and MATRA-2 (10 layers) achieved higher correlation with human judgments than baseline metrics across all test systems
- System-level scores showed MATRA variants closely matched human evaluation rankings for systems 1-7
- MaTrA-2 generally outperformed MaTrA-1, though both exceeded BLEU, METEOR, LEPOR, chrF++, and COMET performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A multi-feature fusion approach capturing lexical, syntactic, and semantic signals improves correlation with human judgment over single-metric baselines.
- Mechanism: The architecture extracts 24 features from candidate-reference pairs (lexical cosine, POS cosine, stem cosine, Word2Vec, sentence embeddings, n-gram probability quartiles, REKHA scores, BLEU) and trains a DNN to regress onto averaged human evaluation scores. By combining orthogonal similarity signals, the model compensates for individual metric blind spots.
- Core assumption: Human annotators implicitly weight multiple quality dimensions; a supervised model can approximate this weighting if features span those dimensions.
- Evidence anchors:
  - Table 2 lists 24 features including Lexical Cosine Similarity, POS Cosine Similarity, Word2Vec Cosine Similarity, Sentence Embedding Cosine Similarity, and n-gram probability bins.
  - "24 features were extracted which were used as input for the deep learning model and the averaged human evaluation was considered as the target."
  - Related work (PEAR, TASER) similarly leverages multi-dimensional or reasoning-based evaluation, but corpus evidence specific to Gujarati is limited.
- Break condition: If feature set lacks coverage for phenomena critical to Gujarati (e.g., morphological richness, agglutination), model may underfit human preferences.

### Mechanism 2
- Claim: Supervised learning with human-grounded labels (HEval) enables the metric to internalize annotator preferences beyond surface n-gram overlap.
- Mechanism: HEval provides 11 parameter scores (gender/number translation, tense, voice, proper noun handling, lexical choice, phrase sequence, punctuation, fluency, semantics, overall syntax/meaning) on a 0-4 Likert scale; these are averaged to produce the training target. The DNN minimizes MSE between predicted and average HEval scores.
- Core assumption: Averaged Likert scores are a stable proxy for translation quality; inter-annotator variability is either low or smoothed by averaging.
- Evidence anchors:
  - Table 1 lists 11 HEval parameters; "A Likert scale of 0-4 was used to judge these parameters. Finally, all these 11 parameters were averaged to get one final objective score."
  - "The translations were then manually evaluated by human annotators based on 11 parameters and their average score was registered."
  - No direct corpus validation of HEval reliability for Gujarati; related metrics (BLEUBERI, ContrastScore) use alternative supervision signals.
- Break condition: If HEval annotations are noisy, inconsistent, or systematically biased (e.g., annotator fatigue), model learns spurious patterns.

### Mechanism 3
- Claim: Deeper regularization-heavy DNNs (6-10 hidden layers with dropout, L1, batch normalization) can learn non-linear feature-to-quality mappings for Gujarati MT evaluation.
- Mechanism: Two variants (6 and 10 hidden layers) use tanh activation, Adam optimizer, MSE loss, and regularization to prevent overfitting on 80,000 training samples. The 10-layer variant (MaTrA-2) generally yields higher correlation scores.
- Core assumption: The feature-label relationship is sufficiently non-linear to benefit from depth; regularization prevents overfitting given feature count (24) and sample size (80,000).
- Evidence anchors:
  - "one model is trained using 6 hidden layers with 500 epochs while the other model is trained using 10 hidden layers with 500 epochs."
  - "MaTrA-2 and MaTrA-1 showed better results as compared to other metrics" (Table 3, Table 4).
  - Corpus does not provide comparative architecture studies for Gujarati evaluation; depth benefit is not independently validated.
- Break condition: If training data is insufficiently diverse (domain-limited, system-limited), deeper models may memorize rather than generalize.

## Foundational Learning

- **Concept**: Reference-based MT evaluation
  - Why needed here: MaTrA compares candidate translations to human references to extract similarity features; understanding reference-based vs. reference-free paradigms clarifies design constraints.
  - Quick check question: Can you explain why reference-based metrics may fail when multiple valid translations exist?

- **Concept**: Cosine similarity across representation levels (lexical, POS, embedding)
  - Why needed here: 5 of 24 features are cosine similarities at different abstraction levels; understanding what each captures is essential for feature debugging.
  - Quick check question: What does POS cosine similarity capture that lexical cosine does not?

- **Concept**: Supervised regression for metric learning
  - Why needed here: MaTrA frames evaluation as regression to human scores, not ranking; loss function choice (MSE) directly shapes optimization behavior.
  - Quick check question: Why might MSE be suboptimal if human scores are ordinal rather than interval-scaled?

## Architecture Onboarding

- **Component map**: Candidate translation + reference translation → Feature extraction (24 features) → DNN (6 or 10 hidden layers) → Scalar quality score

- **Critical path**:
  1. Ensure HEval annotations are averaged correctly (11 parameters, 0-4 scale).
  2. Validate feature extraction pipeline (cosine similarities, LM probabilities, REKHA/BLEU scores).
  3. Confirm train/test split prevents leakage (different domains and MT systems).

- **Design tradeoffs**:
  - 6-layer vs. 10-layer: MaTrA-2 (10-layer) shows slightly higher system-level scores and correlations, but increased training time and overfitting risk.
  - Feature count: 24 features balance expressiveness and dimensionality; reducing features may speed training but lose signal.
  - Domain coverage: Training on administration/health/tourism; testing on agriculture/education tests generalization.

- **Failure signatures**:
  - Low correlation with HEval on new domains → feature set may lack domain-relevant signals.
  - MaTrA-1 and MaTrA-2 scores converge → depth may not be necessary; check regularization strength.
  - High training loss plateau → learning rate, feature scaling, or label noise issues.

- **First 3 experiments**:
  1. Reproduce correlation results (HEval vs. MaTrA-1/MaTrA-2) on held-out agriculture/education test set; verify Table 4 values.
  2. Ablate feature groups (remove embedding-based features, remove n-gram probability features) to identify critical feature subsets.
  3. Test on out-of-distribution MT systems (e.g., newer LLM-based translators) to assess generalization beyond the 7 tested systems.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can the MaTrA architecture and its specific feature set be effectively adapted for other Indian languages?
- **Basis in paper**: The conclusion states, "In the future, we would like to extend these trained metrics to other Indian languages as well."
- **Why unresolved**: The current feature set (Table 2) includes specific components like REKHA scores and language model probabilities which were tailored specifically for the English-Gujarati linguistic pair.
- **What evidence would resolve it**: Successful training of the model on other language pairs (e.g., English-Hindi) demonstrating comparable correlation with human judgments without significant feature re-engineering.

### Open Question 2
- **Question**: To what extent does the choice of optimizer and regularization impact the final correlation with human evaluation?
- **Basis in paper**: The authors note they "wish to experiment with more DNN models... using different sets of optimizers, different combinations of regularizes and different dropout values."
- **Why unresolved**: The current study fixed the optimizer (Adam), activation (tanh), and regularization (L1), only varying the number of hidden layers (6 vs. 10).
- **What evidence would resolve it**: Ablation studies showing performance variance when switching optimizers (e.g., Adam vs. SGD) or altering dropout rates.

### Open Question 3
- **Question**: Is the reliance on 25 hand-crafted features necessary, or could a raw sequence-based model achieve similar results?
- **Basis in paper**: The methodology relies entirely on extracting 25 specific features (lexical, semantic, syntactic) before feeding them into the Dense Neural Network.
- **Why unresolved**: Modern metrics like COMET often utilize end-to-end transformer embeddings; it is unclear if the manual feature engineering is the key to beating baselines or a potential bottleneck.
- **What evidence would resolve it**: A comparative analysis between the current feature-based MaTrA model and an ablated version trained directly on token embeddings.

## Limitations

- The study provides no inter-annotator agreement statistics for HEval annotations, raising concerns about label reliability and potential biases in the training data.
- Feature set coverage of Gujarati-specific linguistic phenomena (morphological richness, agglutination) is not validated, limiting confidence in model generalization.
- Training domains (administration, health, tourism) differ from testing domains (agriculture, education), but the study doesn't test generalization to completely different MT systems or domains.

## Confidence

- **High confidence**: Experimental methodology and correlation results given clear experimental setup and comparative results with baseline metrics.
- **Medium confidence**: Claim that multi-feature approach improves over single metrics, as this relies on assumptions about feature complementarity without explicit feature ablation validation.
- **Low confidence**: Model's ability to generalize beyond tested MT systems and domains without additional out-of-distribution testing.

## Next Checks

1. Conduct inter-annotator agreement analysis on HEval annotations to quantify label reliability and potential biases.
2. Perform feature ablation studies to identify which feature subsets contribute most to performance and whether all 24 features are necessary.
3. Test MaTrA on newer MT systems (particularly LLM-based translators) and additional Gujarati domains to assess generalization beyond the 7 systems and 2 domains evaluated in the current study.