---
ver: rpa2
title: Examining Reject Relations in Stimulus Equivalence Simulations
arxiv_id: '2507.00265'
source_url: https://arxiv.org/abs/2507.00265
tags:
- reject
- relations
- equivalence
- comparison
- stimulus
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigated the role of reject relations in stimulus
  equivalence (SE) formation using computational models. The researchers tested feedforward
  neural networks (FFNs), BERT, and GPT under 18 experimental conditions varying training
  structure (linear series, one-to-many, many-to-one), relation type (select-only,
  reject-only, select-reject), and negative comparison selection (standard vs biased).
---

# Examining Reject Relations in Stimulus Equivalence Simulations

## Quick Facts
- arXiv ID: 2507.00265
- Source URL: https://arxiv.org/abs/2507.00265
- Reference count: 40
- Primary result: Reject relations significantly influence stimulus equivalence formation in computational models, with performance suggesting reliance on associative learning rather than true equivalence.

## Executive Summary
This study investigates how reject relations affect stimulus equivalence (SE) formation using computational models including feedforward neural networks (FFNs), BERT, and GPT. The researchers tested these models across 18 experimental conditions varying training structure, relation type, and negative comparison selection. A probabilistic agent served as a benchmark for comparison. The findings reveal that while reject relations significantly impact agent performance, the artificial neural networks demonstrate patterns consistent with associative learning strategies rather than true stimulus equivalence formation. This suggests current ANN architectures may not fully capture human-like equivalence formation, highlighting the need for careful consideration of reject relations in computational modeling of SE.

## Method Summary
The study employed computational simulations using three types of models: feedforward neural networks, BERT, and GPT. These were tested across 18 experimental conditions created by varying three factors: training structure (linear series, one-to-many, many-to-one), relation type (select-only, reject-only, select-reject), and negative comparison selection (standard vs biased). A probabilistic agent served as a benchmark for comparison. The researchers measured performance on equivalence tests and analyzed the influence of reject relations on learning outcomes.

## Key Results
- Reject relations significantly influenced agent performance across all experimental conditions
- Accurate equivalence test performance was observed primarily in conditions with biased negative comparisons and linear series or many-to-one structures
- Computational agents' performance was comparable to the probabilistic agent, suggesting reliance on associative learning rather than true stimulus equivalence
- Current ANN architectures failed to demonstrate human-like equivalence formation despite varying training conditions

## Why This Works (Mechanism)
The study demonstrates that computational models rely on associative learning strategies when processing stimulus equivalence tasks. When trained with specific combinations of reject relations and training structures, these models can achieve high performance on equivalence tests, but this performance appears to stem from exclusion-based strategies rather than genuine formation of equivalence classes. The biased negative comparison selection particularly enhances this associative pattern, suggesting that models learn to exclude incorrect options rather than forming true relational understanding between stimuli.

## Foundational Learning
- Stimulus Equivalence: A fundamental learning phenomenon where stimuli become functionally equivalent through training with some relations, leading to derived relations. Why needed: Forms the theoretical basis for understanding how computational models should learn relational patterns.
- Reject Relations: The use of incorrect or non-matching stimuli during training to establish exclusion patterns. Why needed: Central to the study's investigation of how negative feedback influences learning outcomes.
- Associative Learning: Learning through forming connections between stimuli based on their co-occurrence or exclusion. Why needed: The mechanism by which the computational models appear to be solving equivalence tasks rather than demonstrating true equivalence.
- Quick check: Compare the model's behavior when trained with only correct pairings versus correct pairings with reject relations.

## Architecture Onboarding
Component Map: Input Stimuli -> Neural Network Layers -> Output Classification -> Performance Evaluation
Critical Path: Training Phase -> Equivalence Testing -> Performance Analysis
Design Tradeoffs: Supervised learning provides clear labels but may not capture the dynamic emergence seen in human learning; offline training misses trial-order effects that influence human behavior.
Failure Signatures: High performance on equivalence tests without corresponding internal representation of equivalence classes; reliance on exclusion strategies rather than relational understanding.
First Experiments:
1. Test a simple feedforward network on a basic equivalence task to establish baseline performance
2. Compare performance between models trained with select-only versus reject-only conditions
3. Analyze hidden layer representations using dimensionality reduction to check for stimulus clustering

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do human participants perform under the specific "select-only" and "reject-only" experimental conditions using dummy stimuli, and does their performance differ from the associative patterns observed in the computational agents?
- Basis in paper: The authors state that the direct manipulation of reject relations "represents a novel experimental approach that, to our knowledge, has not yet been replicated with human participants" and suggest their conditions can serve as a template for human studies.
- Why unresolved: While the computational results suggest agents rely on associative learning, it is unknown if humans would utilize similar exclusion-based strategies or demonstrate true equivalence when trained with these specific isolations of select/reject relations.
- What evidence would resolve it: Data from human subjects undergoing the same 18 experimental conditions (specifically the select-only and reject-only variations) to see if they pass equivalence tests where ANNs failed or relied on association.

### Open Question 2
- Question: Can reinforcement learning (RL) algorithms or online learning architectures succeed in demonstrating stimulus equivalence where supervised, offline-trained ANNs failed?
- Basis in paper: The discussion notes that "Incorporating online learning mechanisms into future simulations is crucial" and explicitly suggests that "reinforcement learning-based algorithms... may more accurately model the dynamic nature of human equivalence learning."
- Why unresolved: The current study was limited to feedforward and transformer models trained via supervised learning (offline/batch processing), which lacks the trial-order sensitivity and gradual emergence seen in human behavior.
- What evidence would resolve it: Testing RL-based agents (e.g., Projective Simulation) under the same 18 conditions to see if they demonstrate emergent derived relations (transitivity/symmetry) rather than simple associative exclusion.

### Open Question 3
- Question: Does the analysis of feature space representations in the hidden layers of these ANNs reveal stimulus clustering based on class membership, even when output behavior indicates associative learning?
- Basis in paper: The authors note a limitation that "the study did not analyze the networks' internal representations," while simultaneously suggesting that analyzing feature space representations is a valuable methodology to determine if organization is based on "acquired relational responding or surface feature similarities."
- Why unresolved: It is unclear if the networks failed to form equivalence classes entirely or if they formed internal representations of classes but utilized an associative strategy for the output due to the training parameters.
- What evidence would resolve it: Dimensionality reduction (e.g., t-SNE, PCA) of the hidden layer activations for sample and comparison stimuli to visualize if stimuli from the same class cluster together in the feature space.

## Limitations
- Simulation-based approach may not fully capture the complexity of human cognitive processes in stimulus equivalence formation
- Small human sample size (five participants) limits generalizability of findings
- Focus on specific training structures and relation types may not encompass full range of real-world equivalence conditions

## Confidence

| Claim | Confidence |
|-------|------------|
| Impact of reject relations on agent performance | High |
| Comparison between ANNs and human-like equivalence formation | Medium |
| Generalizability to real-world stimulus equivalence formation | Low |

## Next Checks
1. Conduct a larger-scale human study with diverse participants to validate the computational model findings and assess generalizability
2. Investigate computational model performance under a wider range of training structures and relation types
3. Compare computational models' performance to other established models of stimulus equivalence formation