---
ver: rpa2
title: Do Political Opinions Transfer Between Western Languages? An Analysis of Unaligned
  and Aligned Multilingual LLMs
arxiv_id: '2508.05553'
source_url: https://arxiv.org/abs/2508.05553
tags:
- political
- languages
- language
- opinions
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether political opinions in multilingual
  large language models (MLLMs) transfer between Western languages or remain language-specific.
  The authors first evaluate 15 unaligned MLLMs across five Western languages (German,
  English, French, Spanish, Italian) using political statements from voting advice
  applications.
---

# Do Political Opinions Transfer Between Western Languages? An Analysis of Unaligned and Aligned Multilingual LLMs

## Quick Facts
- **arXiv ID**: 2508.05553
- **Source URL**: https://arxiv.org/abs/2508.05553
- **Reference count**: 27
- **Primary result**: Political opinions transfer between Western languages in multilingual LLMs, with minimal cross-lingual differences both before and after alignment

## Executive Summary
This paper investigates whether political opinions in multilingual large language models transfer between Western languages or remain language-specific. The authors evaluate 15 unaligned MLLMs across five Western languages (German, English, French, Spanish, Italian) using political statements from voting advice applications. They then align two MLLMs using direct preference optimization with English political manifestos and re-evaluate across all languages. The study finds minimal cross-lingual differences in political opinions before alignment and uniform opinion shifts across all languages after alignment, demonstrating strong cross-lingual transfer of political opinions.

## Method Summary
The study employs a two-phase approach: first evaluating unaligned MLLMs on political opinion tasks across five Western languages using the ProbVAA dataset, then aligning two models (Phi3.5-3B and Llama3.1-8B) with DPO using English political manifestos and re-evaluating. The evaluation uses closed-ended agreement/disagreement responses with a robustness framework testing response consistency across different prompt formulations. Political opinion scores are computed via beta regression, and language differences are assessed using Kruskal-Wallis tests.

## Key Results
- Minimal cross-lingual differences in political opinions were found in unaligned MLLMs, with only a few significant differences at the policy issue level
- After alignment with English manifestos, opinion shifts occurred uniformly across all five languages without systematic cross-lingual differences
- An open-ended evaluation task validated the closed-ended findings, confirming strong opinion transfer between Western languages

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** English-centric concept representations in MLLMs cause political opinions to transfer from English to other Western languages.
- **Mechanism:** The predominance of English in pre- and post-training data causes the model to ground political concepts primarily in English. Aligning these English-grounded representations via DPO modifies a shared conceptual core, which is then accessed regardless of the prompt language.
- **Core assumption:** Political concepts are not stored in a fully language-agnostic manner but are instead anchored to English representations.
- **Evidence anchors:** [abstract] "The political alignment shifts opinions almost uniformly across all five languages." [Section 5, Discussion] "The transfer of information out of English likely reflects the English-focused alignment of conceptual representations within the MLLM itself, as observed by Wendler et al. (2024)."
- **Break condition:** If aligning the model using political data in a non-English language (e.g., German or French) does *not* produce uniform opinion shifts across all target languages, this mechanism would be insufficient as the sole explanation.

### Mechanism 2
- **Claim:** Strong implicit cross-lingual alignment in MLLMs enables uniform opinion transfer across languages.
- **Mechanism:** MLLMs are trained with objectives (e.g., on parallel or massive multilingual corpora) that align the internal latent spaces of different languages. A change in the model's "opinion" (a state in the network's weights) propagates through this aligned space, affecting outputs in all languages uniformly.
- **Core assumption:** The model's architecture and training have created a shared multilingual representation space where high-level concepts like political stances are language-independent.
- **Evidence anchors:** [abstract] "We conclude that in Western language contexts, political opinions transfer between languages." [Section 1, Introduction] References "implicit and explicit training for cross-lingual concept space alignment of MLLMs."
- **Break condition:** If significant opinion differences were found between typologically or culturally distant language pairs (e.g., English vs. non-Western languages) after alignment, it would suggest the cross-lingual alignment is not uniformly strong across all language pairs.

### Mechanism 3
- **Claim:** Homogenization of political opinions in training data masks potential language-specific differences.
- **Mechanism:** The pre-training data for all Western languages may come from similar, globally-connected sources (e.g., translated news, Western-centric internet), leading to a convergence of represented political opinions. The models thus start with a uniform opinion baseline, and subsequent alignment modifies this baseline uniformly.
- **Core assumption:** The lack of initial cross-lingual differences in unaligned models (RQ1 results) is due to training data characteristics rather than a fundamental architectural transfer mechanism.
- **Evidence anchors:** [Section 3.2, Results] "Analysis reveals minimal cross-lingual differences in political opinions before alignment... with only a few significant differences at the policy issue level."
- **Break condition:** If a model trained on carefully curated, culturally distinct political corpora for each language showed strong, consistent cross-lingual opinion differences in its unaligned state, this mechanism would be less plausible as a primary driver.

## Foundational Learning

- **Concept: Direct Preference Optimization (DPO)**
  - **Why needed here:** The paper uses DPO as the method to align models with left- or right-leaning political views. Understanding DPO is critical to grasping how the authors introduced the opinion shift in RQ2.
  - **Quick check question:** Can you explain how DPO uses a preference dataset (preferred vs. dispreferred outputs) to directly update a model's weights without training a separate reward model?

- **Concept: Cross-Lingual Transfer**
  - **Why needed here:** This is the central phenomenon the paper investigatesâ€”whether a modification (alignment) applied in one language affects the model's behavior in others. Grasping this concept is essential for interpreting the study's core findings.
  - **Quick check question:** If a model shows strong cross-lingual transfer for political opinions, what would you expect to happen if you DPO-align it to be more right-leaning using only German data?

- **Concept: Robustness-Aware Evaluation**
  - **Why needed here:** The paper emphasizes that opinion measurements are sensitive to prompt wording. The authors' robustness framework is what allows them to filter out unstable models and claim their findings are reliable.
  - **Quick check question:** Why is testing a model's opinion consistency across multiple prompt templates and paraphrases crucial before drawing conclusions about its inherent political bias?

## Architecture Onboarding

- **Component map:** MLLM (Phi3.5, Llama3.1) -> LoRA adapter -> DPO training loop -> multilingual evaluation pipeline
- **Critical path:** 1. Filter candidate models using robustness tests. 2. Prepare left/right alignment datasets from English political manifestos. 3. Train separate LoRA adapters via DPO for each political leaning. 4. Re-evaluate the aligned models (base + adapter) on the political opinion task across all five target languages.
- **Design tradeoffs:**
  - **Alignment Data Source:** Using English manifestos vs. broader or multilingual sources. Targeted, policy-issue-annotated data proved more effective for strong alignment.
  - **Alignment Language:** Aligning solely in English to test for transfer vs. multilingual alignment data.
  - **Evaluation Task:** Closed-ended (agree/disagree) for simplicity vs. ecological validity addressed partially with open-ended generation.
- **Failure signatures:**
  - **Alignment Collapse:** Over-aggressive DPO training degrades the model's fluency or general reasoning capabilities.
  - **No Transfer:** Alignment in English changes opinions only in English, indicating a failure of cross-lingual transfer.
  - **Over-Uniformity:** Alignment causes opinions to become uniformly neutral, possibly by pushing the model towards a refusal or "safe" state.
- **First 3 experiments:**
  1.  **Replicate with a non-Western language:** Extend the methodology to a language like Arabic or Chinese to test if the transfer effect holds across a greater linguistic and cultural divide.
  2.  **Non-English alignment:** Perform the DPO alignment using German or French manifestos to test if the transfer mechanism is bidirectional or if English is special.
  3.  **Probe internal activations:** Use techniques like linear probing to compare the internal representations of politically charged statements across languages in both aligned and unaligned models to directly visualize the cross-lingual alignment of concepts.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does political alignment using non-English data transfer opinions to English and other Western languages?
- **Basis in paper:** [explicit] The discussion notes the study aligned with English data only and suggests "future work could test this by aligning in other languages."
- **Why unresolved:** The current results could be attributed to the dominance of English training data rather than a general capability for cross-lingual transfer.
- **What evidence would resolve it:** Repeating the alignment experiments using German or French political manifestos and measuring the resulting opinion shifts in English and other target languages.

### Open Question 2
- **Question:** Does the observed cross-lingual transfer of political opinions extend to non-Western languages and cultural contexts?
- **Basis in paper:** [explicit] The limitations section states that extending the setup to non-Western languages is "a topic for future work" and notes potential mismatches in political context.
- **Why unresolved:** The study focused on Western languages to control for political systems, and the evaluation data (ProbVAA) was derived from European sources.
- **What evidence would resolve it:** Evaluating MLLMs using culturally relevant political statements in non-Western languages (e.g., Arabic, Chinese) and assessing alignment transfer.

### Open Question 3
- **Question:** Does political alignment via Direct Preference Optimization (DPO) degrade the general language generation abilities or performance on downstream tasks?
- **Basis in paper:** [explicit] The authors state in the limitations that they "do not further test whether the alignment affected the general language generation abilities or performance on other downstream tasks."
- **Why unresolved:** While opinion shifts were validated, the potential negative impact on the model's general utility (the "alignment tax") was not measured.
- **What evidence would resolve it:** Benchmarking the aligned models on standard NLP tasks (e.g., translation, summarization) across all target languages.

## Limitations

- The study exclusively focuses on Western languages, limiting generalizability to non-Western contexts
- English-centric alignment data creates potential circularity in transfer claims
- Cannot distinguish between true cross-lingual concept alignment and English-centric representation dominance
- Robustness framework may over-filter models, potentially excluding linguistically nuanced cases

## Confidence

**High confidence**: Claims about minimal cross-lingual differences in unaligned models (RQ1 results), and the observation that English-centric alignment shifts opinions uniformly across all five languages.

**Medium confidence**: The mechanism explanation attributing transfer to English-centric conceptual representations, as the paper acknowledges insufficient knowledge about training processes to definitively establish causality.

**Low confidence**: Generalization beyond Western languages, as the paper explicitly acknowledges this limitation and provides no evidence for transfer between Western and non-Western languages.

## Next Checks

1. **Non-English alignment test**: Repeat the DPO alignment process using German or French political manifestos to determine if English is uniquely privileged in the transfer mechanism or if cross-lingual transfer operates bidirectionally.

2. **Typologically distant language pair**: Extend the methodology to include a non-Western language (e.g., Arabic or Chinese) to test whether the strong cross-lingual transfer observed between Western languages persists across greater linguistic and cultural distances.

3. **Internal representation analysis**: Apply linear probing or similar techniques to compare internal activations for politically charged statements across languages in both aligned and unaligned models, providing direct evidence of cross-lingual concept space alignment.