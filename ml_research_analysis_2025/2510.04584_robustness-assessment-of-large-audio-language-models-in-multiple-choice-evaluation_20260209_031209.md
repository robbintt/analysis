---
ver: rpa2
title: Robustness assessment of large audio language models in multiple-choice evaluation
arxiv_id: '2510.04584'
source_url: https://arxiv.org/abs/2510.04584
tags:
- audio
- flamingo
- evaluation
- answer
- choices
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large audio language models (LALMs) are evaluated using multiple-choice
  question answering (MCQA), but their performance varies significantly with minor
  changes in question or choice wording. We systematically studied the impact of choice
  ordering, question rephrasing, ground truth answer rephrasing, and distractor rephrasing
  on three benchmarks (MMAU, MMAR, MMSU) and four models (Audio Flamingo 2, Audio
  Flamingo 3, Qwen2.5-Omni-7B-Instruct, Kimi-Audio-7B-Instruct).
---

# Robustness assessment of large audio language models in multiple-choice evaluation

## Quick Facts
- **arXiv ID:** 2510.04584
- **Source URL:** https://arxiv.org/abs/2510.04584
- **Reference count:** 0
- **Key outcome:** Large audio language models show significant performance variation (up to 13.7% std) when question or choice wording is minimally changed, with distractor rephrasing causing the largest drops.

## Executive Summary
Large audio language models (LALMs) evaluated on multiple-choice question answering (MCQA) show significant performance instability when questions or choices are rephrased. This study systematically tested four perturbation types across three benchmarks and four LALM architectures, finding that distractor rephrasing causes the largest accuracy variance (up to 13.7% standard deviation). Models exhibit a strong bias toward selecting longer answer choices, which can compromise reliability. The authors propose a robust evaluation framework combining perturbations and introduce the Correctness Rate (CoR) metric as a more reliable assessment of LALM robustness than single-pass accuracy.

## Method Summary
The study evaluates LALMs on three audio MCQA benchmarks (MMAU, MMAR, MMSU) using systematic perturbations: choice ordering (24 permutations), question rephrasing (6 variants), ground truth rephrasing (6 variants), distractor rephrasing (6 variants), and probabilistic mix of all types. Rephrasing is performed using gemini-2.5-flash and gemma-3-12b-it models. The evaluation pipeline includes standardized prompting, greedy decoding, and output parsing. Key metrics include accuracy (mean, std, min-max), Consistency Rate (CR), and Correctness Rate (CoR). The study also quantifies length bias by measuring longest-choice selection frequency.

## Key Results
- Distractor rephrasing causes the largest performance drops, with accuracy standard deviation up to 13.7%
- LALMs show a strong bias toward selecting longer answer choices (52.38% of the time vs. 45.05% baseline in data)
- Correctness Rate (CoR) provides a more reliable robustness assessment than single-pass accuracy
- MMAU benchmark shows most severe sensitivity to perturbations across all models tested

## Why This Works (Mechanism)

### Mechanism 1
LALMs use distractor wording as a decision signal rather than semantic understanding. When distractors are paraphrased (typically becoming longer), models redistribute attention across choices differently, breaking the original decision boundary. This surface-level reliance causes significant accuracy variance.

### Mechanism 2
Models conflate choice length with correctness probability, exploiting spurious correlations between answer length and correctness from training data. This length bias is amplified when rephrasings increase choice length, with models selecting the longest choice 52.38% of the time versus 45.05% baseline.

### Mechanism 3
The CoR metric across perturbation mixes provides reliable robustness assessment by aggregating correctness across multiple semantically-equivalent variants. Combined with probabilistic perturbation application (0.5 probability per modification), this reduces variance from any single perturbation type while exposing overall brittleness.

## Foundational Learning

- **Multiple-Choice Question Answering (MCQA) Evaluation**
  - Why needed here: The entire paper critiques standard MCQA evaluation practices; understanding MCQA is essential context
  - Quick check question: Why might MCQA evaluation be preferred over open-ended generation for benchmarking, and what are the tradeoffs?

- **Perturbation-Based Robustness Testing**
  - Why needed here: The methodology relies on systematically applying controlled modifications to measure model sensitivity
  - Quick check question: What is the difference between a perturbation that tests robustness vs. one that changes the underlying task difficulty?

- **Consistency vs. Correctness Metrics**
  - Why needed here: The paper introduces CR (internal consistency) and CoR (external correctness) as complementary measures
  - Quick check question: Can a model have high consistency but low correctness? What would this indicate about the model's behavior?

## Architecture Onboarding

**Component map:**
Benchmarks (MMAU, MMAR, MMSU) -> Perturbation Engine (4 types + mix) -> LALM Inference -> Output Parsing -> Metric Computation

**Critical path:**
1. Benchmark loading and audio-text alignment verification
2. Perturbation generation with semantic preservation validation
3. Standardized prompt construction across all four LALMs
4. Output parsing (especially Kimi-Audio which only outputs letters)
5. CoR calculation as primary robustness metric

**Design tradeoffs:**
- Perturbation count vs. compute cost: 24 orderings × questions is expensive; mix approach reduces cost while maintaining signal
- Rephrasing model choice: gemini-2.5-flash and gemma-3-12b-it introduce dependency on external models' paraphrase quality
- Manual validation scope: Only "a few randomly selected samples" were manually validated for ground truth rephrasing—scales poorly

**Failure signatures:**
- High accuracy variance (std > 5%) under choice ordering → positional bias
- Large accuracy drop under distractor rephrasing → surface-level decision heuristics
- CoR significantly lower than mean accuracy → inconsistent reasoning across formulations
- Longest-choice selection rate > 50% → length bias exploitation

**First 3 experiments:**
1. Baseline replication: Run default setup on all three benchmarks to verify alignment with reported numbers (±2% drift expected)
2. Distractor-only perturbation: Isolate distractor rephrasing impact—highest-variance perturbation and most diagnostic of shallow reasoning
3. Length-balanced control: Create version where rephrased choices match original lengths to disentangle length bias from semantic rephrasing effects

## Open Questions the Paper Calls Out

**Open Question 1**
How can the identified bias of LALMs toward selecting longer answer choices be effectively mitigated during training or inference? The paper quantifies this bias (52.38% longest-choice selection) but doesn't propose correction methods.

**Open Question 2**
Do the observed sensitivity patterns generalize to proprietary or closed-source Large Audio Language Models? The study is restricted to four specific open-source models, leaving uncertainty about fundamental architectural issues versus training data effects.

**Open Question 3**
To what extent does the choice of the auxiliary LLM used for generating rephrasings influence the robustness scores of the evaluated LALMs? If rephrasing models introduce semantic shifts or stylistic biases, performance drops might reflect generator errors rather than LALM brittleness.

## Limitations

- Reliance on paraphrase models (gemini-2.5-flash and gemma-3-12b-it) whose quality and potential biases directly impact perturbation validity
- Manual validation performed only on a subset of samples, raising concerns about systematic semantic drift across all benchmarks
- Length bias finding suggests surface feature exploitation but correlation doesn't prove causation—other correlated features could explain the pattern

## Confidence

- **High confidence:** Methodology for perturbation-based robustness testing and CoR metric introduction is sound and well-documented
- **Medium confidence:** Specific magnitude of accuracy drops (up to 13.7% std) depends on paraphrase models and manual validation scope
- **Low confidence:** Interpretation that length bias indicates shallow reasoning is plausible but not definitively proven

## Next Checks

1. **Length-balanced control experiment:** Create perturbations where rephrased choices match original lengths to isolate semantic effects from length bias
2. **Cross-paraphrase validation:** Re-run key experiments using different paraphrase models (e.g., GPT-4, Claude) to verify robustness of findings
3. **Semantic drift quantification:** Systematically measure semantic similarity between original and paraphrased questions/answers using established metrics (e.g., SBERT similarity scores) to quantify potential meaning changes