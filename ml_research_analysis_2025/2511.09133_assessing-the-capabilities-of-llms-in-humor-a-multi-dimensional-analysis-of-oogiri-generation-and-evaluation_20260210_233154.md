---
ver: rpa2
title: Assessing the Capabilities of LLMs in Humor:A Multi-dimensional Analysis of
  Oogiri Generation and Evaluation
arxiv_id: '2511.09133'
source_url: https://arxiv.org/abs/2511.09133
tags:
- humor
- llms
- responses
- human
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The study conducted a multi-dimensional analysis of humor generation
  and evaluation capabilities of large language models (LLMs) using the Japanese comedic
  format Oogiri. The researchers expanded existing Oogiri datasets with new data and
  LLM-generated responses, then manually annotated the collection with 5-point ratings
  across six dimensions: Novelty, Clarity, Relevance, Intelligence, Empathy, and Overall
  Funniness.'
---

# Assessing the Capabilities of LLMs in Humor:A Multi-dimensional Analysis of Oogiri Generation and Evaluation

## Quick Facts
- arXiv ID: 2511.09133
- Source URL: https://arxiv.org/abs/2511.09133
- Reference count: 5
- LLMs generate Oogiri responses at mid-tier human level, but humans prioritize Empathy while LLMs prioritize Novelty in evaluation

## Executive Summary
This study evaluates large language models' capabilities in generating and evaluating humor using the Japanese comedic format Oogiri. Researchers created a comprehensive dataset combining human responses with LLM-generated content, then annotated responses across six dimensions: Novelty, Clarity, Relevance, Intelligence, Empathy, and Overall Funniness. Three state-of-the-art models (GPT-4.1, Gemini 2.5 Pro, Claude Sonnet 4) were tested on both generation and evaluation tasks. Results reveal that while LLMs can produce creative responses, they systematically undervalue Empathy in evaluation and exhibit positivity bias, leading to misalignment with human humor preferences.

## Method Summary
The researchers constructed a dataset from Oogiri-GO (Bokete) and Oogiri-Chaya, filtering 200 popular topics (100 text, 100 image). They generated 8 response types per topic using three LLMs and human contributions. Four native Japanese annotators rated each response on 6 dimensions (0-4 scale) via crowdsourcing, with each annotator seeing only one response type per topic to prevent bias. Analysis focused on Spearman correlation between human and LLM scores, mean comparisons across response tiers, and identifying evaluation criteria divergences between humans and models.

## Key Results
- LLMs generated responses at mid-tier human performance level, with Gemini 2.5 Pro most proficient
- Humans prioritize Empathy in humor evaluation, while LLMs prioritize Novelty
- LLMs exhibit systematic positivity bias, rating unrelated responses highly while humans rate them poorly
- Correlation between human and LLM funniness scores was limited (ρ = 0.17–0.27), indicating fundamental evaluation misalignment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Human humor appreciation is driven primarily by Empathy, while LLM evaluation prioritizes Novelty.
- Mechanism: For humans, the correlation between Empathy and Overall Funniness is the strongest among all dimensions. For LLMs, Novelty shows the highest correlation with Funniness. This creates divergent evaluation criteria.
- Core assumption: The six evaluation dimensions adequately capture the humor assessment space.
- Evidence anchors:
  - [abstract] "Correlation analyses of human and model evaluation data further reveal a fundamental divergence in evaluation criteria: LLMs prioritize Novelty, whereas humans prioritize Empathy."
  - [section: RQ1 Results] "As revealed by the heatmap of human evaluation dimensions (lower triangle of Figure 3), Empathy is the dimension most strongly correlated with Overall Funniness for humans."
- Break condition: If Empathy were removed or collapsed into a single "relatability" metric, the divergence pattern may not be observable.

### Mechanism 2
- Claim: LLMs can generate incongruity (high Novelty) but fail to resolve it through Empathy, limiting humor quality.
- Mechanism: According to Suls' two-stage incongruity-resolution model, humor requires (1) detecting incongruity and (2) resolving it via cognitive rules. LLMs succeed at stage 1 but fail at stage 2 because resolution requires grounding punchlines in shared human context (Empathy).
- Core assumption: Suls' cognitive model applies to Oogiri-style improvisational comedy.
- Evidence anchors:
  - [section: RQ1 Discussion] "The LLMs' notable deficit in Empathy prevents them from achieving this humorous resolution, even when their responses are novel and relevant."
  - [section: Related Work] References Suls (1972) two-stage model explicitly.
- Break condition: If a different humor theory better explains Oogiri, this mechanism may not hold.

### Mechanism 3
- Claim: LLMs exhibit systematic evaluation biases (positivity bias, self-preference bias) that inflate scores for low-quality responses.
- Mechanism: Training objectives (instruction tuning, preference optimization) encourage cooperative, positive outputs. This causes LLMs to assign high scores even to irrelevant or low-quality responses, and to prefer AI-generated content over human content.
- Core assumption: Instruction/preference tuning causes evaluative leniency rather than other factors.
- Evidence anchors:
  - [section: RQ2 Discussion] "Through processes like instruction and preference tuning, they are optimized to be cooperative and to generate positive responses, inadvertently predisposing them to assign favorable scores."
  - [section: RQ2 Results] "While humans consistently rated [Unrelated responses] as not funny (0.681), LLMs rated these same responses very highly (GPT-4.1: 2.411)."
- Break condition: If bias stems from prompt design rather than training objectives, different prompting strategies could mitigate it.

## Foundational Learning

- Concept: **Incongruity-Resolution Theory (Suls 1972)**
  - Why needed here: Provides the theoretical framework explaining why LLMs can create novelty but fail at resolution without Empathy.
  - Quick check question: Can you explain why a response might be "novel but not funny" using this two-stage model?

- Concept: **Multi-Dimensional Annotation with Inter-Annotator Agreement**
  - Why needed here: The study relies on 4 human annotators per response; understanding annotation reliability is critical for trusting ground truth labels.
  - Quick check question: Why might Spearman correlation be preferred over Pearson for comparing human vs. LLM ordinal ratings?

- Concept: **Correlation vs. Causation in Evaluation Alignment**
  - Why needed here: The paper identifies correlation patterns but does not claim causal mechanisms. Distinguishing descriptive from causal claims prevents overreach.
  - Quick check question: If Empathy and Funniness are highly correlated for humans, does increasing Empathy in a response *cause* higher Funniness?

## Architecture Onboarding

- Component map: Oogiri-GO + Oogiri-Chaya → filtered topics → generation layer (3 LLMs) → 8 response types → annotation layer (4 humans + LLM self-eval) → analysis layer (Spearman correlation, mean comparison)
- Critical path: Topic filtering → Response generation → Human annotation → Correlation computation
- Design tradeoffs:
  - Bokete vs. Chaya: Bokete has first-mover bias; Chaya has cleaner signals but smaller scale
  - 4 annotators vs. more: Balances cost against reliability; no inter-annotator agreement reported
  - 5-point vs. continuous scale: Integer ratings simplify annotation but reduce granularity
- Failure signatures:
  - Positivity bias: LLMs rate Unrelated responses ~2.4/4 while humans rate ~0.7/4
  - Self-preference bias: LLMs rate AI-generated responses comparably to human High-tier; humans rate them below Mid-tier
  - Low correlation: Spearman ρ = 0.17–0.27 indicates fundamental misalignment
- First 3 experiments:
  1. Ablate Empathy from evaluation rubric and re-run correlations to test whether Novelty becomes dominant for humans
  2. Apply negative prompting to LLM evaluators to test whether positivity bias can be mitigated
  3. Fine-tune a smaller model on high-Empathy human responses to evaluate Empathy score improvements

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can specific fine-tuning or prompting strategies targeted at "Empathy" close the performance gap between LLMs and high-tier human humor generation?
- Basis in paper: The Discussion section states, "To generate responses that humans consistently find funny, future development must focus not only on creativity (Novelty) but critically on improving the models' capacity for empathetic reasoning."
- Why unresolved: The study evaluated existing state-of-the-art models using default settings but did not attempt to alter their training or prompting to specifically address the identified Empathy deficit.
- What evidence would resolve it: A comparative study where models are fine-tuned on high-empathy humor datasets or prompted with specific empathy-related constraints, resulting in human evaluation scores that match or exceed the "High" tier baseline.

### Open Question 2
- Question: Can the "LLM-as-a-Judge" evaluation logic be realigned to prioritize Empathy over Novelty, thereby reducing the positivity and self-preference biases observed in current models?
- Basis in paper: The authors identify a "fundamental divergence in evaluation criteria" where LLMs prioritize Novelty while humans prioritize Empathy, causing LLMs to rate irrelevant or AI-generated content higher than humans do.
- Why unresolved: The paper quantifies the misalignment but does not propose or test a method to force LLMs to adopt human-weighted evaluation criteria.
- What evidence would resolve it: Development of a prompting framework or reward model that weights the "Empathy" dimension higher than "Novelty," leading to a statistically significant increase in Spearman correlation with human "Overall Funniness" ratings.

### Open Question 3
- Question: Do the specific evaluation criteria divergences (Human: Empathy vs. LLM: Novelty) persist across other humor forms or languages, or are they unique to the high-context, improvisational nature of Japanese Oogiri?
- Basis in paper: The study is limited to Japanese "Oogiri" format; the authors note that humor requires "shared context" and "cultural awareness," implying that the Empathy deficit might manifest differently in other cultural or linguistic contexts.
- Why unresolved: The methodology relied exclusively on Japanese datasets, leaving the cross-cultural generalizability of the "Empathy vs. Novelty" trade-off untested.
- What evidence would resolve it: Replicating the six-dimensional annotation and analysis methodology on a distinct humor dataset (e.g., English stand-up comedy or Western cartoon captions) to see if LLMs still exhibit the same Novelty bias and Empathy deficit.

## Limitations

- The divergent evaluation criteria may not generalize beyond Oogiri-style humor or Japanese cultural contexts
- The paper does not provide inter-annotator agreement statistics, leaving uncertainty about human ground truth reliability
- Specific Japanese prompt templates for generation and evaluation are referenced but not fully specified

## Confidence

- **High confidence**: LLMs can generate creative Oogiri responses at mid-tier human level; Empathy is strongest human correlate with funniness; LLMs exhibit systematic positivity bias in evaluation
- **Medium confidence**: The claim that LLMs fail at empathy-based humor resolution due to training objectives; broader generalizability of Empathy-Novelty divergence to other humor formats
- **Low confidence**: Exact mechanisms causing LLM evaluation biases (training vs. prompting) and whether these biases are inherent or can be mitigated through prompt engineering

## Next Checks

1. Replicate the study with English-language humor datasets to test whether the Empathy-Novelty divergence holds across languages and cultural contexts
2. Conduct ablation studies removing Empathy from the evaluation rubric to determine if Novelty becomes the dominant factor for human evaluators as well
3. Implement constrained scoring prompts for LLM evaluators (e.g., "be critical" or "penalize irrelevant responses") to test whether positivity bias can be reduced at inference time