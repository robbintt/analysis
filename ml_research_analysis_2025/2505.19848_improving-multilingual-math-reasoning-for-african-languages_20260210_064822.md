---
ver: rpa2
title: Improving Multilingual Math Reasoning for African Languages
arxiv_id: '2505.19848'
source_url: https://arxiv.org/abs/2505.19848
tags:
- languages
- data
- language
- african
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper systematically evaluates strategies for adapting large
  language models to African languages, focusing on mathematical reasoning tasks.
  The authors compare synthetic data generation (via persona-driven prompts) against
  machine translation of English math datasets, examining prompt masking, scaling,
  monolingual vs multilingual training, and continual pretraining.
---

# Improving Multilingual Math Reasoning for African Languages

## Quick Facts
- arXiv ID: 2505.19848
- Source URL: https://arxiv.org/abs/2505.19848
- Authors: Odunayo Ogundepo; Akintunde Oladipo; Kelechi Ogueji; Esther Adenuga; David Ifeoluwa Adelani; Jimmy Lin
- Reference count: 33
- Key outcome: Synthetic native-language data generation combined with high-quality translated instruction data achieves strong mathematical reasoning performance across African languages, with multilingual training outperforming monolingual approaches.

## Executive Summary
This paper addresses the challenge of mathematical reasoning in African languages by systematically evaluating synthetic data generation versus machine translation approaches for fine-tuning large language models. Using Llama 3.1 8B as the base model, the authors generate native-language synthetic data through persona-driven prompts (AfriPersona-Instruct) and compare it against high-quality translations of English math datasets. The study examines prompt masking, scaling, monolingual vs multilingual training, and continual pretraining. Results show that synthetic native data performs comparably to translated data, both outperforming narrow-domain translation approaches, with multilingual training yielding the best results. Notably, the approach approaches or exceeds GPT-4 performance on several African languages.

## Method Summary
The authors fine-tune Llama 3.1 8B using supervised fine-tuning with two main data strategies: synthetic native-language generation via persona-driven prompts (AfriPersona-Instruct) and high-quality translation of English math datasets. They evaluate prompt masking during loss computation, scaling of synthetic data, monolingual versus multilingual training approaches, and continual pretraining effects. Training uses DeepSpeed ZeRO-3 with 2 epochs and learning rate 5e-5. Evaluation employs LLM-as-Judge (GPT-4o) on the AfriMGSM benchmark, which contains 3,277 mathematical reasoning samples across 9 African languages. The study systematically compares different data sources and training configurations to identify optimal approaches for mathematical reasoning in low-resource languages.

## Key Results
- Synthetic native-language data (AfriPersona-Instruct) performs comparably to high-quality translated data (OpenMathInstruct)
- Multilingual joint training significantly outperforms monolingual approaches across all languages tested
- Masking prompts during loss computation degrades performance, contrary to standard practice
- The 30k-sample synthetic dataset fine-tuned model approaches or exceeds GPT-4 performance on several African languages
- Combined approach of synthetic and translated data yields best overall results

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Computing loss over instruction tokens yields better results than masking prompts during training.
- **Mechanism:** In low-resource settings, including prompt tokens in loss calculation provides denser learning signal by forcing the model to learn linguistic patterns of instructions, not just response patterns.
- **Core assumption:** Instruction text contains high-value linguistic signal that aids in grounding model's understanding of low-resource language syntax.
- **Evidence anchors:** Abstract states "computing loss over instruction tokens yields better results than masking prompts during training"; section 5.2 confirms masking leads to slightly worse performance.
- **Break condition:** If instruction prompts are excessively long or noisy compared to response length, gradient signal might be diluted.

### Mechanism 2
- **Claim:** Multilingual joint training yields superior performance compared to monolingual experts.
- **Mechanism:** Pooling data from multiple languages enables positive cross-lingual transfer, leveraging data scarcity in one language by utilizing signals from related or high-resource counterparts in the batch.
- **Core assumption:** Reasoning capabilities (math logic) are largely language-agnostic and can be transferred across multilingual representations.
- **Evidence anchors:** Abstract states "multilingual training outperforms monolingual approaches"; section 5.6 shows multilingual model outperforms monolingual models by large margin.
- **Break condition:** If languages in batch have catastrophic interference (e.g., vastly different tokenization efficiency), transfer might degrade.

### Mechanism 3
- **Claim:** Hybrid data strategy (synthetic native data + translated instruction data) maximizes performance.
- **Mechanism:** Translated data provides high-quality reasoning chains from established datasets while native synthetic data improves cultural alignment and linguistic naturalness, compensating for each other's weaknesses.
- **Core assumption:** GPT-4o can generate sufficiently accurate native-language synthetic data, and translation artifacts can be mitigated by high-quality source datasets.
- **Evidence anchors:** Abstract states "synthetic data generation... combined with high-quality translated instruction data, achieves strong performance"; section 5.3 shows training on all available data yields best results.
- **Break condition:** If synthetic generation model (GPT-4o) has severe capability degradation in specific target language, native data may introduce more noise than signal.

## Foundational Learning

- **Concept: Supervised Fine-Tuning (SFT) Loss Masking**
  - **Why needed here:** Standard practice often masks user prompt to focus learning on assistant's response, but this paper challenges that for low-resource languages.
  - **Quick check question:** Should you include the "User: ..." tokens in your loss calculation when fine-tuning on a low-resource language? (Answer: Evidence suggests yes).

- **Concept: Cross-lingual Transfer**
  - **Why needed here:** Decision to build one multilingual model instead of nine separate models relies on principle that learning math in Yoruba helps model learn math in Igbo.
  - **Quick check question:** Why might training on Swahili math problems improve performance on Hausa math problems in the same model?

- **Concept: Persona-Driven Synthesis**
  - **Why needed here:** "AfriPersona" dataset is generated using personas to ensure diversity.
  - **Quick check question:** How does prompting a model with specific persona (e.g., "A chef from Kumasi") improve resulting training data compared to generic prompting?

## Architecture Onboarding

- **Component map:** AfriPersonaHub (Personas) -> GPT-4o -> AfriPersona-Instruct (Native Synthetic); OpenMathInstruct -> GPT-4o Translation -> Translated Data -> Llama 3.1 8B -> DeepSpeed ZeRO-3 -> SFT (2 epochs, LR 5e-5) -> LLM-as-Judge (GPT-4o) -> Evaluation

- **Critical path:** 1. Generate/Collect diverse personas (text-to-persona) 2. Synthesize math problems in target languages using personas 3. Translate high-quality English math datasets 4. Mix data and train with unmasked loss on instruction tokens

- **Design tradeoffs:** Native Synthetic vs. Translated: Native data is culturally aligned but risks factual hallucination; Translated data is logically sound but risks "translationese" artifacts. Architecture favors hybrid mix. Evaluation: Exact Match (EM) is insufficient for multilingual math due to formatting variance; LLM-as-Judge is preferred but adds inference cost.

- **Failure signatures:** Linguistic Unnaturalness: Section 5.1 notes synthetic data contained "unnatural phrasing" and "lexical inaccuracies" despite persona usage. Pure synthetic approach might bake in grammar errors. CPT Degradation: Section 5.5 shows Lugha-Llama (continually pre-trained) underperformed base model on math SFT, suggesting domain mismatch.

- **First 3 experiments:** 1. Masking Ablation: Train identical models on same data, one with prompt masking and one without, to verify loss signal hypothesis. 2. Scaling Study: Train with 10k, 20k, and 30k synthetic samples to establish performance curve relative to data volume. 3. Zero-Shot Probe: Evaluate fine-tuned model on held-out African languages (e.g., those in AfriMGSM not in training set) to test cross-lingual generalization.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does translating high-quality English mathematical instruction datasets yield better model performance compared to generating synthetic data directly in target African languages?
- Basis in paper: Section 4.3.1 explicitly poses this comparative question as core ablation.
- Why unresolved: Paper finds translated data outperforms synthetic when source is diverse (OpenMathInstruct), but synthetic is competitive with GPT-4 at modest scale; optimal combination remains unclear.
- What evidence would resolve it: Controlled experiments varying source dataset diversity, translation quality, and synthetic data scale across more domains and languages.

### Open Question 2
- Question: Do observed effects of scaling data size and instruction tuning across languages generalize to domains beyond mathematics, such as commonsense reasoning, reading comprehension, or scientific QA?
- Basis in paper: Section 7 (Limitations) states: "It is unclear whether patterns we observe... would generalise to other domains."
- Why unresolved: Study focuses exclusively on mathematical reasoning; cross-domain validation was not performed.
- What evidence would resolve it: Replication of synthetic vs. translated data ablations on established benchmarks for commonsense, reading comprehension, and scientific QA in African languages.

### Open Question 3
- Question: Would alternative prompt templates, source tasks, or instruction styles yield more stable and transferable results under synthetic data generation pipeline?
- Basis in paper: Section 7 (Limitations) notes reliance on single synthetic data generation setup limits insight into stability and transferability.
- Why unresolved: Only persona-driven pipeline was evaluated; sensitivity to prompt design and task framing remains unknown.
- What evidence would resolve it: Systematic ablations varying prompt templates, source tasks, and instruction styles, measuring downstream performance variance.

### Open Question 4
- Question: Why does continual pre-training on general African language corpora not improve mathematical reasoning under limited supervision, and would domain-aligned pre-training resolve this?
- Basis in paper: Section 5.5 hypothesizes distributional shift from literary/news pretraining to math-focused fine-tuning, but this is not empirically tested.
- Why unresolved: Experiment shows Lugha-LLaMA underperforms baseline, but mechanism (domain mismatch vs. other factors) is unverified.
- What evidence would resolve it: Continual pre-training experiments using math-aligned corpora versus general corpora, with controlled supervision budgets.

## Limitations

- Data quality constraints: Study relies on GPT-4o for synthetic data generation and translation, creating potential cascading quality issues with acknowledged "unnatural phrasing" and "lexical inaccuracies" in synthetic data.
- Evaluation methodology concerns: Primary evaluation uses LLM-as-Judge approach (GPT-4o), introducing potential bias and circularity when assessing models trained on GPT-4o-generated data.
- Generalization boundaries: Results based on Llama 3.1 8B fine-tuning experiments; effectiveness of persona-driven synthesis remains uncertain without comparison to alternative generation approaches.

## Confidence

**High Confidence Claims:**
- Multilingual joint training outperforms monolingual approaches (supported by direct experimental comparison showing "large margin" improvements)
- Loss masking during fine-tuning degrades performance (clearly demonstrated through ablation study)
- Synthetic native data combined with translated data yields strong results (empirically validated across multiple languages)

**Medium Confidence Claims:**
- AfriPersona-Instruct approaches or exceeds GPT-4 performance on several languages (based on limited benchmark evaluation)
- Persona-driven synthesis provides meaningful cultural alignment (inferred from methodology, but not directly validated)
- 30k-sample threshold represents optimal balance (scaling study limited to three points)

**Low Confidence Claims:**
- Synthetic data quality is "sufficient" despite acknowledged imperfections (not directly measured against human-annotated alternatives)
- Cross-lingual transfer mechanisms are well-understood (mechanism proposed but not explicitly tested)
- GPT-4o evaluations are more reliable than alternatives (subjective assessment without rigorous validation)

## Next Checks

1. **Human Evaluation Validation**: Conduct expert human evaluation of model outputs across target languages to validate LLM-as-Judge reliability and identify systematic errors GPT-4o evaluations may miss, particularly focusing on cultural appropriateness and mathematical correctness in context.

2. **Generalization Stress Test**: Evaluate the approach on truly low-resource African languages not included in original training set (e.g., languages from same families but not explicitly trained on) to assess cross-lingual generalization beyond studied languages.

3. **Alternative Generation Comparison**: Compare persona-driven synthetic data generation against other approaches including human-annotated data, template-based generation, and few-shot prompting to quantify specific contribution of persona methodology to performance improvements.