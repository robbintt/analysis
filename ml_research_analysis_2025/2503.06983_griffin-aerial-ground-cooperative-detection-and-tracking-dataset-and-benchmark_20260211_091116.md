---
ver: rpa2
title: 'Griffin: Aerial-Ground Cooperative Detection and Tracking Dataset and Benchmark'
arxiv_id: '2503.06983'
source_url: https://arxiv.org/abs/2503.06983
tags:
- uni00000013
- fusion
- uni00000011
- perception
- uni00000003
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Griffin, a novel dataset and benchmark for
  aerial-ground cooperative 3D perception, addressing the lack of high-quality public
  datasets in this domain. Griffin features over 250 dynamic scenes (37k+ frames)
  with varied drone altitudes (20-60m), diverse weather conditions, realistic drone
  dynamics via CARLA-AirSim co-simulation, and occlusion-aware 3D annotations.
---

# Griffin: Aerial-Ground Cooperative Detection and Tracking Dataset and Benchmark

## Quick Facts
- **arXiv ID**: 2503.06983
- **Source URL**: https://arxiv.org/abs/2503.06983
- **Reference count**: 30
- **Primary result**: Introduces Griffin, a novel dataset and benchmark for aerial-ground cooperative 3D perception with 250+ dynamic scenes, supporting evaluation of communication efficiency, altitude adaptability, and robustness to various impairments.

## Executive Summary
The paper introduces Griffin, a novel dataset and benchmark for aerial-ground cooperative 3D perception, addressing the lack of high-quality public datasets in this domain. Griffin features over 250 dynamic scenes (37k+ frames) with varied drone altitudes (20-60m), diverse weather conditions, realistic drone dynamics via CARLA-AirSim co-simulation, and occlusion-aware 3D annotations. The dataset supports comprehensive evaluation of cooperative detection and tracking, including metrics for communication efficiency, altitude adaptability, and robustness to communication latency, data loss, and localization noise. Experiments demonstrate that instance-level fusion methods are more resilient to varying altitudes and localization errors compared to BEV-level fusion. The dataset and benchmarking framework provide crucial insights for future research in aerial-ground cooperative perception.

## Method Summary
Griffin is created through a co-simulation framework combining CARLA (for ground vehicle and environment) with AirSim (for drone dynamics and aerial perception). The dataset includes synchronized data from a ground vehicle equipped with 4 RGB cameras and 1 LiDAR, plus an aerial drone with 5 RGB cameras. The framework generates dynamic scenes with diverse weather conditions, traffic densities, and drone altitudes ranging from 20-60m. Key innovations include occlusion-aware 3D annotations using instance segmentation to filter invisible targets, and a comprehensive evaluation framework that tests not only detection accuracy but also communication efficiency and robustness to various impairments like latency, packet loss, and localization noise.

## Key Results
- Instance-level fusion methods like CoopTrack demonstrate superior robustness to altitude changes and localization noise compared to BEV-level methods like V2X-ViT
- Communication latency has more severe impact on performance than packet loss, particularly for dense data transmission methods
- Most fusion methods fail to maintain positive performance gains over single-agent baselines on the Griffin-Random altitude dataset due to perspective distortions
- The dataset reveals that current cooperative perception methods are highly vulnerable to localization errors, with performance collapsing below single-agent baselines when translation noise exceeds 1.5m

## Why This Works (Mechanism)

### Mechanism 1: Geometry-Semantics Decoupling
Instance-level fusion maintains performance better than BEV-level fusion under dynamic altitude changes and localization noise by separating geometric alignment from feature representation. Instance-level methods fuse sparse object queries containing semantic features and 3D reference points, making them inherently more resilient to perspective distortions caused by altitude shifts. In contrast, BEV-level fusion relies on dense feature grids that require rigid geometric alignment, making them sensitive to these distortions.

### Mechanism 2: Occlusion-Aware Labeling Efficiency
Filtering ground truth annotations based on visibility improves model accuracy by preventing conflicting supervision signals. Standard datasets may label objects invisible to the ego agent, forcing the model to hallucinate or over-rely on map priors. By using instance segmentation to calculate visibility rates and filtering invisible targets, Griffin provides "clean" supervision that aligns with the sensor's actual information capacity.

### Mechanism 3: Sparse Selective Fusion for Communication Resilience
Communication latency degrades performance more significantly than packet loss because latency creates temporal misalignment between agents, destroying the geometric consistency required for fusion. Packet loss merely reduces information gain. Methods with lower bandwidth requirements (sparse queries) inherently suffer less from latency bottlenecks compared to raw image transmission.

## Foundational Learning

- **Coordinate Systems & Extrinsics (FLU, ENU, NED)**: Required for integrating ground vehicles (FLU - Forward-Left-Up) and drones (often NED - North-East-Down or ENU). Fusing perception requires precise transformation matrices between these moving frames. *Quick check*: If a drone rotates 5 degrees but the ground vehicle is not updated on this rotation, will the projected BEV features align? (Answer: No, severe misalignment will occur, particularly for BEV-level fusion).

- **Fusion Paradigms (Early vs. Intermediate vs. Late)**: The paper benchmarks these three strategies. Understanding the trade-off between bandwidth (raw images vs. features vs. boxes) and information density is crucial for interpreting the results. *Quick check*: Why is Late Fusion bandwidth-efficient but performance-poor in occluded scenarios? (Answer: It transmits only final boxes, losing the raw feature data needed to resolve occlusions).

- **Bird's-Eye View (BEV) Representation**: The baseline backbone (BEVFormer) transforms multi-view images into a top-down BEV grid. This is the standard "common language" for fusing aerial and ground perspectives. *Quick check*: What happens to a ground object's feature size in a BEV grid when the observing drone doubles its altitude? (Answer: The object occupies fewer pixels in the image, potentially leading to lower resolution features in the BEV grid).

## Architecture Onboarding

- **Component map**: CARLA Server (Traffic/Environment) + AirSim Server (Drone Physics) -> Co-simulation Bridge -> Ground Vehicle (4 Cameras + LiDAR) + Aerial Drone (5 Cameras) -> BEVFormer Backbone (ResNet-50) -> Encoder -> Decoder (Object Queries) -> Fusion Module (Variant-dependent)

- **Critical path**: Calibration (precise extrinsics between Drone and Vehicle) -> BEV Generation (multi-camera features lifted to 3D space) -> Feature Transmission (aerial features sent to Ground with optional noise injection) -> Fusion & Detection (Ground combines local + aerial features to produce 3D boxes)

- **Design tradeoffs**: Realism vs. Scale (co-simulation provides realistic drone dynamics but is computationally heavier than simple joint simulation); BEV vs. Query (BEV fusion offers high performance in ideal conditions but collapses under localization noise, while query fusion is robust to noise but may miss small objects)

- **Failure signatures**: Altitude Collapse (performance drops significantly on Griffin-55m or Griffin-Random if model is not scale-adaptive); Ghost Tracks (high localization noise causes BEV features to be stamped in wrong location, resulting in duplicate or offset boxes); Sparsity Loss (methods relying on spatial confidence maps may filter out valid features if drone's view is too sparse)

- **First 3 experiments**: 1) Baseline Establishment: Run No Fusion and Early Fusion models on Griffin-25m to quantify potential gain of cooperation; 2) Robustness Stress Test: Evaluate V2X-ViT vs. CoopTrack on Griffin-Random with 1.0m localization noise added; 3) Altitude Generalization: Train on Griffin-25m and test directly on Griffin-55m to measure drop in performance due to scale variance

## Open Questions the Paper Calls Out

- **How can fusion mechanisms be designed to be altitude-adaptive and scale-aware to handle dynamic UAV viewpoints?**: The authors state that future efforts should focus on developing altitude-adaptive and scale-aware fusion mechanisms capable of handling dynamic aerial viewpoints. This remains unresolved as experiments show current methods fail to generalize across varying altitudes.

- **How can dynamic trust mechanisms be implemented to discern and filter erroneous signals caused by localization noise?**: The Conclusion posits that successful cooperation requires discerning which data to trust and fuse and calls for creating dynamic trust mechanisms to weigh or filter erroneous signals. This is unresolved as current methods are highly vulnerable to localization errors.

- **Can perception models specifically designed for the aerial domain provide more robust features for cooperative fusion than generic backbones?**: The Discussion notes value in benchmarking perception models specifically designed for the aerial domain, which may provide more robust features for fusion. This remains unquantified as all current baselines utilize BEVFormer, a model designed for ground vehicles.

## Limitations
- Simulation-based approach may not fully capture real-world complexity including true sensor noise characteristics and unexpected object behaviors
- Dataset focus on urban scenes with structured traffic may limit generalization to rural or highway scenarios
- Controlled environment provides precise annotations but may not reflect annotation challenges in real-world data collection

## Confidence
- **Dataset creation methodology and annotation quality**: High confidence (controlled simulation environment with detailed documentation)
- **Baseline performance results**: Medium confidence (dependent on specific implementation details and hyperparameters not fully specified)
- **Communication robustness findings**: Medium confidence (real-world network conditions may differ from synthetic models used)

## Next Checks
1. **Real-world transfer validation**: Test the trained models on actual aerial-ground cooperative data to assess performance degradation compared to simulation results

2. **Cross-dataset generalization**: Evaluate models trained on Griffin when applied to existing datasets like RCooper or nuScenes to measure domain adaptation capabilities

3. **Communication model stress testing**: Implement and test the proposed methods under more realistic network conditions including variable bandwidth, packet reordering, and intermittent connectivity patterns