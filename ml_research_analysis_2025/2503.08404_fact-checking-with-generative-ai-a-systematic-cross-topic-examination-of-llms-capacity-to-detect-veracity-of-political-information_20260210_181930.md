---
ver: rpa2
title: 'Fact-checking with Generative AI: A Systematic Cross-Topic Examination of
  LLMs Capacity to Detect Veracity of Political Information'
arxiv_id: '2503.08404'
source_url: https://arxiv.org/abs/2503.08404
tags:
- statements
- llms
- fact-checking
- 'false'
- topic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically evaluates the performance of five large
  language models (ChatGPT-4, Llama 3 models, Claude 3.5 Sonnet, and Google Gemini)
  for fact-checking political information using a dataset of 16,513 statements verified
  by professional journalists. Using AI auditing methodology, the research examines
  how model accuracy varies by statement veracity (true, false, mixed) and topic.
---

# Fact-checking with Generative AI: A Systematic Cross-Topic Examination of LLMs Capacity to Detect Veracity of Political Information

## Quick Facts
- arXiv ID: 2503.08404
- Source URL: https://arxiv.org/abs/2503.08404
- Reference count: 0
- Primary result: ChatGPT-4 and Google Gemini achieved highest accuracy in detecting veracity of political statements, but overall performance remained modest with significant topic-level variation.

## Executive Summary
This study systematically evaluates five large language models (ChatGPT-4, Llama 3 models, Claude 3.5 Sonnet, and Google Gemini) for fact-checking political information using a dataset of 16,513 statements verified by professional journalists. Using AI auditing methodology, the research examines how model accuracy varies by statement veracity (true, false, mixed) and topic. Results show ChatGPT-4 and Google Gemini achieved the highest accuracy, though overall performance remained modest. Models performed best on false statements, particularly those related to COVID-19, American political controversies, and social issues, suggesting potential guardrails that enhance accuracy on sensitive topics. The study finds significant variation in performance across different LLMs and unequal quality of outputs for specific topics, highlighting challenges for automated fact-checking including training data limitations.

## Method Summary
The study used AI auditing methodology to evaluate 5 LLMs on fact-checking political statements with 4-class labels (TRUE, FALSE, MIXTURE, N/A). Using the ClaimsKG dataset of 16,513 filtered statements from 2000-2023, models were prompted via API with temperature=0.1 to classify each statement and provide ≤20-word justification. BERTopic clustering was applied to statements to identify 10 topic clusters. Performance was measured using precision, recall, and macro F1 scores per veracity class, with logistic regression analyzing topic effects on agreement with human fact-checkers.

## Key Results
- ChatGPT-4 and Google Gemini achieved the highest accuracy across all models tested
- Models performed best on false statements (F1: 0.69-0.77) compared to true statements (F1: 0.25-0.36)
- COVID-19 and American political controversies showed 100-151% higher odds of agreement for false statements across models
- U.S. Economic Analysis and U.S. Fiscal Impact topics showed 31-70% lower odds of agreement versus reference topics

## Why This Works (Mechanism)

### Mechanism 1
Pre-built guardrails on sensitive topics (COVID-19, political controversies) enhance detection accuracy for false statements. Models may have additional fine-tuning or rule-based overlays for high-risk topics, increasing the probability of correctly flagging false claims in these domains. Core assumption: Guardrails exist and are topic-targeted; improved accuracy is not solely due to richer training data on those topics. Evidence: statements related to COVID-19 and American politics showed higher agreement in all LLMs for the false category, potentially pointing to the possibility that all models have set up guardrails surrounding sensitive topics. Break condition: If accuracy improvements disappear when controlling for topic prevalence in training data, guardrails are not the primary driver.

### Mechanism 2
Training data distribution bias causes models to over-predict certain labels, particularly under-representing "true" statements. Models learn statistical priors from training corpora; if false/mixed claims are more prevalent or more prominently fact-checked in pre-training, models default toward those labels. Core assumption: Label distribution in training data is imbalanced toward false/mixed; models generalize this prior to new inputs. Evidence: F1 for true statements ranged 0.25-0.36 across models, vs. 0.69-0.77 for false. One possible explanation for why ChatGPT-4 struggles with the true information category could be that the training data contains more fact-checked statements labeled as false. Break condition: If balanced fine-tuning on true/false/mixed statements equalizes F1 scores across categories, training distribution is confirmed as a key factor.

### Mechanism 3
Topic-specific knowledge density in pre-training determines veracity detection accuracy. Topics with denser, higher-quality coverage in pre-training data (e.g., U.S. social issues, COVID-19) yield better semantic understanding and contextual judgment, enabling higher agreement with human fact-checkers. Core assumption: Knowledge density correlates with accuracy; sparse or under-represented topics (e.g., U.S. fiscal policy) lack sufficient context for reliable inference. Evidence: For false statements, topics like "U.S. Economic Analysis" and "U.S. Fiscal Impact" showed 31-70% lower odds of agreement vs. reference topic across all models. COVID-19 and American political controversies showed 100-151% higher odds of agreement for false statements across models. Break condition: If retrieval-augmented generation (RAG) with external evidence eliminates topic-level accuracy gaps, pre-training density is not the binding constraint.

## Foundational Learning

- **AI Auditing Methodology**: The study uses systematic, repeatable prompting of LLMs against a ground-truth dataset to quantify performance; understanding audit design is essential to interpret results and design your own evaluations. Quick check: Can you list three components of an AI audit for fact-checking (e.g., dataset, prompt, metrics)?

- **Precision, Recall, and F1 in Multi-Class Settings**: The paper reports macro F1 scores for TRUE, FALSE, MIXTURE categories; these metrics capture trade-offs between false positives and false negatives in imbalanced label spaces. Quick check: If a model labels 90% of statements as FALSE and only 10% as TRUE, which metric (precision or recall) would likely be most affected for the TRUE class?

- **Topic Modeling for Bias Detection**: BERTopic clustering was used to group 16,513 statements into 10 topic clusters, enabling topic-level regression analysis of model performance. Quick check: Why is unsupervised topic clustering (vs. manual labeling) useful for detecting systematic performance gaps across content areas?

## Architecture Onboarding

- **Component map**: ClaimsKG dataset (16,513 statements) → Filtering (language, format, veracity type) → BERTopic clustering (143 → 10 topic clusters) → LLM APIs (ChatGPT-4, Llama 3/3.1, Claude 3.5, Gemini) with temperature=0.1 → Structured output parsing → Agreement metrics (F1, precision, recall) → Logistic regression (topic × veracity interaction).

- **Critical path**: Prompt design and temperature control directly constrain model behavior and output consistency. Topic clustering quality determines validity of topic-level performance claims. Choice of ground truth (ClaimsKG journalist labels) sets the ceiling for measured agreement.

- **Design tradeoffs**: Binary vs. multi-class labeling (TRUE/FALSE/MIXTURE): Multi-class better reflects reality but lowers accuracy. Temperature setting: Low (0.1) for reproducibility vs. higher for exploratory diversity. API vs. open-weight models: API models (ChatGPT, Gemini) offer higher performance but less transparency; open-weight (Llama) enables inspection but underperforms.

- **Failure signatures**: Instruction-following drift (e.g., Llama outputting labels at end of justification instead of first token). High N/A rates or refusal to classify (e.g., Gemini error messages on 94 statements). Topic-level accuracy collapse (e.g., fiscal/economic topics showing systematically lower agreement).

- **First 3 experiments**: Replicate the prompt at temperature 0.1 on a 500-statement subset from ClaimsKG, measuring F1 by veracity class to establish a baseline for your API setup. Add a retrieval-augmented step (e.g., provide model with top-3 search results for each claim) and measure delta in F1 for low-performing topics like "U.S. Fiscal Impact." Test prompt variations (e.g., include explicit definitions, chain-of-thought instructions) on a held-out 200-statement set to quantify sensitivity to prompt design vs. model capability.

## Open Questions the Paper Calls Out

### Open Question 1
How does LLM fact-checking performance vary across non-English languages and non-US socio-political contexts compared to the US-centric results observed? Basis: The authors list the US-centric nature of the dataset as a limitation, noting that "performance will most likely be different in different socio-political contexts and languages." Why unresolved: The study was restricted to English-language statements, likely underreporting the models' capabilities in low-resource languages. What evidence would resolve it: A replication of the audit methodology using a multilingual dataset of claims from non-Western fact-checking organizations.

### Open Question 2
To what extent is the high accuracy on sensitive topics (e.g., COVID-19, politics) driven by explicit model guardrails versus the composition of training data? Basis: The authors discuss whether better performance on sensitive topics indicates "possible guardrails" or simply reflects "the training data," leaving the causal mechanism unidentified. Why unresolved: The "black box" nature of the proprietary models makes it difficult to distinguish between safety alignment interventions and data distribution patterns. What evidence would resolve it: Comparative analysis of model outputs with safety layers disabled, or testing on novel sensitive claims not present in training corpora.

### Open Question 3
Can targeted fine-tuning or domain-specific pre-training correct the observed tendency of models to struggle with "true" and "mixture" veracity labels? Basis: The authors suggest that "performance of LLMs might potentially be further improved with targeted pre-training and fine-tuning" to address specific deficits in identifying true statements. Why unresolved: The study evaluated baseline model capabilities via API without intervening in the model weights or providing external context. What evidence would resolve it: Experiments applying domain-specific fine-tuning to the tested models to measure improvements in F1 scores for non-false categories.

## Limitations

- The study's dataset is US-centric and English-language, limiting generalizability to other socio-political contexts and languages
- The "black box" nature of proprietary models makes it impossible to definitively distinguish between guardrail mechanisms and training data effects
- Topic modeling results depend heavily on clustering hyperparameters and random seeds, which are not fully specified

## Confidence

- **High**: Overall ranking of model performance (ChatGPT-4 and Gemini highest); systematic differences between true/false/mixture class F1 scores; observation that false statements are classified more accurately than true ones
- **Medium**: Claims about topic-specific accuracy gaps (e.g., U.S. fiscal policy underperforming); attribution of performance differences to training data distribution bias rather than model architecture
- **Low**: Causal attribution of accuracy improvements to "guardrails" for sensitive topics; claims about pre-training knowledge density driving topic-level performance without direct corpus validation

## Next Checks

1. **Prompt sensitivity test**: Run a 200-statement subset with 3 prompt variations (explicit definitions, chain-of-thought, label-first) to quantify whether prompt design or model capability drives performance differences

2. **Retrieval augmentation experiment**: Apply RAG with top-3 web search results to low-performing topics (e.g., "U.S. Fiscal Impact") and measure F1 improvements to test if knowledge density is the binding constraint

3. **Guardrail isolation test**: Create a balanced set of 100 false statements spanning sensitive/non-sensitive topics and measure accuracy delta when explicitly prompting models to "ignore topical guardrails"