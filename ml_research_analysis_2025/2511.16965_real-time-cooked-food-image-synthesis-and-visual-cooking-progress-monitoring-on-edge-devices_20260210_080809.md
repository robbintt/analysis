---
ver: rpa2
title: Real-Time Cooked Food Image Synthesis and Visual Cooking Progress Monitoring
  on Edge Devices
arxiv_id: '2511.16965'
source_url: https://arxiv.org/abs/2511.16965
tags:
- cooking
- image
- images
- similarity
- food
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of generating realistic cooked
  food images on edge devices, enabling real-time cooking progress monitoring. The
  authors introduce a novel dataset of oven-based cooking progressions with chef-annotated
  doneness levels and propose an edge-efficient U-Net generator conditioned on raw
  images, recipe names, and cooking states.
---

# Real-Time Cooked Food Image Synthesis and Visual Cooking Progress Monitoring on Edge Devices

## Quick Facts
- arXiv ID: 2511.16965
- Source URL: https://arxiv.org/abs/2511.16965
- Reference count: 40
- Key outcome: Edge-efficient cooking progress monitoring with real-time image synthesis achieving 30% FID improvement

## Executive Summary
This paper introduces a novel system for real-time cooked food image synthesis and visual cooking progress monitoring on edge devices. The approach uses a text-guided U-Net generator with FiLM-based conditioning to synthesize realistic cooked food images from raw inputs, recipe names, and desired cooking states. A domain-specific Culinary Image Similarity (CIS) metric is introduced to capture gradual cooking transformations and serve as a progress-monitoring signal. The system achieves real-time performance (1.2s/image generation, 0.3s similarity computation) on a 5 TOPS NPU while significantly outperforming existing baselines in image quality metrics.

## Method Summary
The system generates cooked food images using a U-Net generator conditioned on raw images, recipe names, and cooking states through FiLM modulation with sinusoidal embeddings. Training uses a composite loss combining GAN, LPIPS, and CIS losses. A Siamese network (EfficientNet-B1 backbone) learns the CIS metric from temporal cooking sequences to capture gradual culinary transformations. During inference, users select desired doneness from generated images, and the system monitors live camera frames against the target using CIS, stopping cooking when peak similarity is detected.

## Key Results
- 30% improvement in FID scores on custom dataset (52.18 vs baseline)
- 60% improvement in FID on public datasets
- 90% accuracy in stopping cooking at user-desired doneness
- Real-time performance: 1.2s/image generation, 0.3s similarity computation on 5 TOPS NPU
- 45MB quantized model size suitable for edge deployment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sinusoidal embeddings combined with FiLM-based feature modulation enable a single compact generator to handle multiple recipe-specific cooking transformations.
- Mechanism: Recipe-state pairs are mapped to sinusoidal positional embeddings, transformed via MLP into scale (γ) and shift (β) parameters, then modulate intermediate features at every U-Net layer through FiLM: `FiLM(z; γ, β) = γ ⊙ z + β`.
- Core assumption: Cooking transformations share underlying structural operations that can be dynamically weighted by context.
- Evidence anchors: [abstract] "edge-efficient text-guided conditional image generator", [section 3.1.1] "This mechanism allows to dynamically alter the statistical properties of the feature maps"
- Break condition: If recipe-specific transformations require fundamentally different architectures rather than shared weights with different modulations

### Mechanism 2
- Claim: Training a Siamese similarity metric on temporal cooking sequences produces a domain-aware embedding that captures culinary transformations better than generic metrics.
- Mechanism: Siamese network (EfficientNet-B1) embeds image pairs into shared space, trained with MSE loss on cosine similarity vs temporal proximity scores to reflect culinary distance.
- Core assumption: Visual cooking progression follows learnable, temporally-smooth trajectories.
- Evidence anchors: [abstract] "domain-specific Culinary Image Similarity (CIS) metric", [section 4.2.2] "CIS exhibits a wide dynamic range, from 1.0 to ≈0.1"
- Break condition: If cooking transformations vary too dramatically across recipes, a single similarity embedding may not generalize

### Mechanism 3
- Claim: Peak similarity detection between live camera frames and user-selected generated targets provides reliable cooking termination without time/temperature presets.
- Mechanism: User selects desired endpoint from generated images; during cooking, each frame is compared to target using CIS; cooking stops when similarity peaks.
- Core assumption: Generated target images accurately represent achievable cooked states, and CIS similarity reliably correlates with visual doneness.
- Evidence anchors: [abstract] "CIS... serves both as a training loss and a progress-monitoring signal", [section 3.3] "Cooking stops when peak similarity is detected"
- Break condition: If oven door opening, food repositioning, or lighting changes disrupt visual similarity measurements

## Foundational Learning

- **Feature-wise Linear Modulation (FiLM)**
  - Why needed here: Enables text conditions to modulate visual features at every network layer, allowing a single model to handle 30+ recipe-specific transformations
  - Quick check question: Can you explain how γ (scale) and β (shift) parameters would differ for "medium-rare" vs. "well-done" steak conditioning?

- **Siamese Networks with Contrastive Learning**
  - Why needed here: Training CIS requires learning an embedding space where temporal cooking distance is preserved
  - Quick check question: Why use MSE loss with continuous temporal scores rather than binary contrastive loss for training CIS?

- **U-Net Skip Connections**
  - Why needed here: Cooking transformations preserve spatial structure while changing texture/color
  - Quick check question: What would happen to food shape preservation if skip connections were removed from the generator?

## Architecture Onboarding

- **Component map:**
  - Raw image + (recipe, cooking_state) → sinusoidal embedding → MLP → (γ, β) per layer → FiLM-modulated U-Net → generated cooked image
  - Generated image → composite loss (GAN + LPIPS + CIS) → discriminator (70×70 PatchGAN)
  - Live frames → CIS comparison with target → peak detection → stop signal

- **Critical path:**
  1. Raw image + (recipe, cooking_state) → sinusoidal embedding → MLP → (γ, β) per layer
  2. FiLM-modulated U-Net forward pass → generated cooked image
  3. During training: generated image → composite loss (GAN + LPIPS + CIS)
  4. During inference: live frames → CIS comparison with target → peak detection → stop signal

- **Design tradeoffs:**
  - Single model vs. per-state models: Pix2Pix requires 3 separate models (163M total); this approach uses 8.68M unified model with conditioning
  - One-shot generation vs. diffusion: Diffusion models (>1GB, iterative sampling) are too slow for real-time edge
  - Generic vs. domain-specific similarity: SSIM/LPIPS show narrow dynamic range (1.0-0.7); CIS shows wide range (1.0-0.1)

- **Failure signatures:**
  - Blurry outputs: CIS loss weight too low; model reverts to adversarial-only training
  - Identical cooking states: Sinusoidal embedding not propagating to FiLM layers
  - No peak detection: CIS range compressed; retrain similarity network with augmentation
  - Edge OOM: Quantization failed; verify ONNX export matches target precision

- **First 3 experiments:**
  1. Ablate FiLM conditioning: Remove recipe-state modulation, train baseline U-Net. Expected: FID degradation from 52 to ~59
  2. Validate CIS on held-out recipes: Train CIS on 25 recipes, test on 5 unseen. Check if temporal trajectories remain smooth
  3. Stress test progress monitoring: Simulate lighting changes and food repositioning. Verify peak detection robustness

## Open Questions the Paper Calls Out
- How does the system handle dynamic environmental changes like lighting shifts from oven door opening or in-session food manipulation?
- To what extent does the CIS metric generalize to unseen food categories or cooking styles not present in the 30-recipe dataset?
- Does the peak visual similarity detected by CIS consistently align with optimal internal thermodynamic properties for complex items like meats?

## Limitations
- Relies on several domain-specific assumptions that may not generalize beyond oven-based cooking
- CIS metric assumes temporally-smooth cooking trajectories, which may break down for recipes with discrete cooking phases
- Peak similarity detection mechanism is untested against real-world perturbations like oven door opening
- 90% success rate comes from a relatively small sample and may not capture edge cases

## Confidence
- **High Confidence**: Edge deployment performance, FID/LPIPS improvements, and general framework of using generated targets for progress monitoring
- **Medium Confidence**: Effectiveness of FiLM-based conditioning for recipe-specific transformations
- **Medium Confidence**: CIS metric performance and its ability to generalize across unseen recipes
- **Low Confidence**: Robustness of peak similarity detection in real-world conditions with environmental perturbations

## Next Checks
1. Ablate FiLM conditioning: Remove recipe-state modulation and train baseline U-Net to verify the 7-point FID improvement
2. Validate CIS generalization: Train CIS on 25 recipes and test on 5 unseen recipes to verify the learned similarity embedding generalizes
3. Stress test progress monitoring: Simulate lighting changes, food repositioning, and oven door opening during cooking sessions to verify peak similarity detection remains robust