---
ver: rpa2
title: 'Towards mechanistic understanding in a data-driven weather model: internal
  activations reveal interpretable physical features'
arxiv_id: '2512.24440'
source_url: https://arxiv.org/abs/2512.24440
tags:
- features
- feature
- graphcast
- interpretable
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors apply sparse autoencoders to uncover interpretable
  physical features in GraphCast, a state-of-the-art deep learning weather model.
  By analyzing intermediate layer activations, they discover distinct features corresponding
  to tropical cyclones, atmospheric rivers, diurnal/seasonal patterns, large-scale
  precipitation, and sea-ice extent.
---

# Towards mechanistic understanding in a data-driven weather model: internal activations reveal interpretable physical features

## Quick Facts
- arXiv ID: 2512.24440
- Source URL: https://arxiv.org/abs/2512.24440
- Reference count: 40
- Authors: Theodore MacMillan; Nicholas T. Ouellette
- Primary result: Sparse autoencoders applied to GraphCast uncover interpretable physical features corresponding to tropical cyclones, atmospheric rivers, and other atmospheric phenomena

## Executive Summary
This work applies sparse autoencoders (SAEs) to analyze the internal representations of GraphCast, a state-of-the-art deep learning weather model. By examining intermediate layer activations, the authors discover distinct features corresponding to various atmospheric phenomena including tropical cyclones, atmospheric rivers, diurnal/seasonal patterns, large-scale precipitation, and sea-ice extent. Using logistic probes, they demonstrate that these features encode relevant physical information with high accuracy. The key innovation lies in validating that manipulating these features leads to physically plausible and predictable changes in the model's predictions, suggesting that GraphCast has learned meaningful physical representations beyond its training inputs.

## Method Summary
The authors apply sparse autoencoders to analyze intermediate layer activations in GraphCast, a state-of-the-art deep learning weather model. They train logistic regression probes on these features to identify those encoding specific atmospheric phenomena. For validation, they manipulate the activations of identified features (particularly tropical cyclones) and observe the resulting changes in model predictions. They track hurricane strength metrics, verify conservation laws, and check force balances to ensure physical consistency. The methodology combines SAE analysis for feature discovery with causal manipulation experiments for validation of physical interpretability.

## Key Results
- Identified distinct SAE features corresponding to tropical cyclones, atmospheric rivers, diurnal/seasonal patterns, large-scale precipitation, and sea-ice extent
- Achieved high F1-scores (>0.9) for tropical cyclone and atmospheric river classification using logistic probes
- Demonstrated that modifying tropical cyclone feature activations leads to physically plausible changes in hurricane strength predictions while maintaining conservation laws and force balances

## Why This Works (Mechanism)
The approach works by leveraging the hypothesis that deep learning models trained on physical data implicitly learn interpretable representations of the underlying phenomena. Sparse autoencoders can isolate these representations by identifying patterns of neuron activations that correspond to specific physical features. The logistic probes then provide a quantitative measure of what each feature encodes. The causal manipulation experiments validate that these features are not merely correlated with but causally related to the model's predictions about atmospheric phenomena.

## Foundational Learning

**Sparse Autoencoders (SAEs)**
- Why needed: To discover interpretable, sparse representations in high-dimensional activation spaces
- Quick check: Verify reconstruction error and sparsity constraints are satisfied

**Logistic Regression Probes**
- Why needed: To quantify what physical phenomena each SAE feature encodes
- Quick check: Evaluate classification accuracy and F1-scores for known phenomena

**Causal Manipulation Experiments**
- Why needed: To validate that identified features causally influence model predictions
- Quick check: Observe predictable changes in predictions when features are modified

## Architecture Onboarding

**Component Map**
GraphCast model -> SAE analysis -> Logistic probes -> Feature identification -> Activation manipulation -> Prediction validation

**Critical Path**
SAE feature discovery → Logistic probe classification → Activation manipulation → Physical validation

**Design Tradeoffs**
- Sparsity vs. reconstruction accuracy in SAE training
- Feature dimensionality vs. interpretability
- Granularity of activation manipulation vs. physical plausibility

**Failure Signatures**
- Features that don't correspond to physical phenomena despite high classification scores
- Activation manipulations that violate conservation laws or produce unrealistic predictions
- Inconsistent feature behavior across different weather conditions

**First Experiments**
1. Apply SAE to different intermediate layers to compare feature interpretability
2. Test multiple sparsity levels to assess feature stability
3. Validate identified features across different geographic regions and weather regimes

## Open Questions the Paper Calls Out
None provided

## Limitations
- Feature stability across different SAE architectures and hyperparameters is unclear
- Validation limited to hurricane strength predictions, not comprehensive across all variables
- Claims about conservation laws need more rigorous mathematical verification

## Confidence
- **High**: SAE methodology for identifying interpretable features is sound
- **Medium**: Identified features correspond to real atmospheric phenomena with high accuracy
- **Medium**: Activation manipulation demonstrates causal relationships with predictions
- **Low**: Broader claims about mechanistic understanding and scientific discovery

## Next Checks
1. Test SAE feature stability across different sparsity levels, architectures, and random seeds
2. Systematically verify conservation of momentum, energy, and mass across all affected variables when manipulating features
3. Extend activation manipulation experiments to include longer forecast horizons and validate against multiple atmospheric variables simultaneously