---
ver: rpa2
title: 'Redefining Experts: Interpretable Decomposition of Language Models for Toxicity
  Mitigation'
arxiv_id: '2509.16660'
source_url: https://arxiv.org/abs/2509.16660
tags:
- toxicity
- language
- toxic
- semantic
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of toxic content generation in
  large language models (LLMs) by proposing a novel intervention method called EigenShift.
  Existing approaches that manipulate individual neuron activations suffer from instability
  and often compromise the model's language abilities.
---

# Redefining Experts: Interpretable Decomposition of Language Models for Toxicity Mitigation

## Quick Facts
- arXiv ID: 2509.16660
- Source URL: https://arxiv.org/abs/2509.16660
- Authors: Zuhair Hasan Shaik; Abdullah Mazhar; Aseem Srivastava; Md Shad Akhtar
- Reference count: 40
- Primary result: 58% toxicity reduction with minimal perplexity increase on RealToxicPrompts

## Executive Summary
This paper addresses toxic content generation in large language models by proposing EigenShift, an intervention method based on eigen-decomposition of the final output layer. Unlike neuron-level interventions that are unstable and often compromise language abilities, EigenShift identifies interpretable semantic directions ("eigen choices") associated with toxic content and selectively dampens their influence during generation. The method requires no additional training or fine-tuning and incurs minimal computational cost while achieving significant toxicity reduction with preserved language fluency.

## Method Summary
EigenShift performs singular value decomposition on the language model's final output layer (lm_head) to identify interpretable semantic directions. The method computes activation differences between toxic and non-toxic samples for each eigenvector, selects top-k directions most associated with toxicity, and dampens their singular values during generation. This approach targets "generation experts" (semantic directions linked to toxic content) while preserving "detection experts" and overall linguistic competence. The intervention is applied during inference by reconstructing the weight matrix with attenuated toxic eigenvectors.

## Key Results
- Achieves 58% reduction in toxicity on RealToxicPrompts benchmark while keeping perplexity low (+3.62)
- Outperforms all baselines with a TPH score of 60.37%, the highest reported metric
- Layer-wise representations outperform neuron-based methods in toxicity detection across Jigsaw and ToxiCN datasets
- Maintains semantic intent while steering toxic phrases toward neutral alternatives

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SVD of the output projection layer exposes interpretable semantic axes ("eigen choices") that influence token selection.
- Mechanism: The lm_head weight matrix W is factorized as W = UΣV^T, where V^T defines an orthonormal basis of semantic directions. Each eigenvector corresponds to a latent decision axis. Hidden states projected onto these axes reveal the model's implicit "choices" before vocabulary mapping.
- Core assumption: Semantically meaningful directions are linearly decodable from the final projection layer.
- Evidence anchors: Abstract states EigenShift targets generation-aligned components; section 2 hypothesizes certain eigenvectors are systematically associated with toxic content; related work (GloSS over Toxicity) similarly identifies toxic subspaces in FFN layers.

### Mechanism 2
- Claim: Toxicity-associated eigenvectors can be identified by comparing mean activations on toxic vs. non-toxic hidden states.
- Mechanism: For each eigenvector v_i, compute activation a_i = v_i^T · h for hidden states from toxic and non-toxic samples. The directional influence Δ_i = E[h_Φ v_i^T] - E[h_Ψ v_i^T] quantifies how much more the eigenvector activates for toxic content.
- Core assumption: Toxic generation causally correlates with elevated activation along specific eigen directions.
- Evidence anchors: Section 2 describes Δ_i capturing eigenvector activation differences; section 4 demonstrates steering toxic phrases toward neutral alternatives; SGM (neuron-level detoxification) uses similar activation difference approach.

### Mechanism 3
- Claim: Damping singular values (σ'_i = α·σ_i, α < 1) for toxicity-aligned directions reduces toxic generation while preserving fluency.
- Mechanism: Rather than zeroing activations, EigenShift attenuates the contribution of toxic eigen-directions by scaling their singular values. This reduces the model's capacity to amplify those directions in logit computation while leaving other semantic axes intact.
- Core assumption: Generation experts and detection experts occupy orthogonal subspaces; dampening generation-aligned components does not destroy detection capability.
- Evidence anchors: Section 2 states this reduces amplification of semantic directions correlated with toxic content; table 2 shows LLaMA-2 achieving 58% toxicity reduction with minimal perplexity increase; Projecting Out the Malice uses similar subspace projection approach.

## Foundational Learning

- Concept: Singular Value Decomposition (SVD) and low-rank matrix structure
  - Why needed here: The entire method hinges on interpreting W = UΣV^T as a factorization into semantic axes and vocabulary mappings.
  - Quick check question: Given W = UΣV^T, which matrix would you modify to attenuate a specific semantic direction without changing the vocabulary mapping?

- Concept: AUROC and classification reliability at scale
  - Why needed here: The paper critiques prior work for using AUROC ≥ 0.5 as "expert" threshold; you must understand why near-0.5 scores indicate near-random behavior.
  - Quick check question: If 22% of neurons have AUROC > 0.50 but only 9% > 0.55, what does this imply about neuron-level "expertise" stability?

- Concept: Trade-offs in controlled generation (perplexity vs. safety)
  - Why needed here: TPH score formalizes the balance; you need to internalize why 100% toxicity removal with 1000× perplexity increase is not a success.
  - Quick check question: Given T = 50% toxicity reduction and P = 50% perplexity increase, compute the TPH score using Equation 3.

## Architecture Onboarding

- Component map: Input hidden states h → SVD Module (factors W → U, Σ, V^T) → Activation Scorer (projects onto V^T, computes Δ_i) → Intervention Engine (selects top-k eigenvectors, applies damping α) → Inference (replaces original lm_head with W')
- Critical path: 1) Sample generations from base LLM on RealToxicPrompts 2) Classify tokens as toxic/non-toxic 3) Extract hidden states h_{n-1} preceding toxic tokens 4) Compute Δ_i for all eigenvectors 5) Select top-k eigenvectors 6) Apply α = 0.9 damping, reconstruct W' 7) Evaluate toxicity reduction, perplexity change, TPH score
- Design tradeoffs:
  - α value: Higher preserves fluency but reduces mitigation; lower increases intervention but risks incoherence. Paper recommends α = 0.9.
  - top_k / percentile: Smaller targets only highest-Δ_i directions (more precise); larger covers more directions but risks over-suppression. Ablation shows top_k=1024 with α=0.9 maintains PPL.
  - Layer selection: Current method only intervenes on lm_head; layer-wise decomposition is explicitly noted as future work.
- Failure signatures:
  - Catastrophic perplexity (>100× baseline): Check if α is negative or top_k is excessively large with low α.
  - No toxicity reduction despite intervention: Δ_i scores may be near-zero; verify toxic/non-toxic hidden state separation.
  - Semantic drift (generations lose meaning): Over-damping of non-toxic axes; reduce top_k or increase α.
  - Language-specific failure: ToxiCN results show smaller gains than Jigsaw; may need language-specific calibration.
- First 3 experiments:
  1. **Reconstruction sanity check**: Apply full SVD reconstruction (no damping) on GPT-2 XL. Verify perplexity change is <1%.
  2. **Sweep α at fixed top_k**: On LLaMA-7B, fix top_k percentile at 95%, vary α ∈ {0.1, 0.5, 0.9, 1.0}. Plot toxicity vs. perplexity to locate Pareto frontier.
  3. **Cross-dataset generalization**: Identify toxic eigenvectors using Jigsaw (English), then apply intervention to ToxiCN (Chinese) generations without re-computing Δ_i.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do semantic eigen-directions evolve or emerge across different layers of LLMs, rather than only the final output layer?
- Basis in paper: Explicit statement that current work does not explore layer-wise decomposition of LLMs.
- Why unresolved: EigenShift currently applies SVD only to the final linear layer, leaving unexplored whether semantic directions emerge earlier in the network or are distributed across layers.
- What evidence would resolve it: Apply EigenShift decomposition to intermediate transformer layers and analyze whether toxicity-aligned eigenvectors appear earlier, correlate across layers, or are uniquely localized to the output layer.

### Open Question 2
- Question: Can the eigen-decomposition approach generalize to control other semantic concepts (e.g., bias, formality, sentiment) beyond toxicity?
- Basis in paper: Explicit claim that the eigen-choice framework generalizes to any semantic concept, but not empirically validated beyond toxicity.
- Why unresolved: The paper demonstrates EigenShift only for toxicity; no experiments verify that the same method isolates and suppresses other semantic axes without unintended side effects.
- What evidence would resolve it: Apply EigenShift to datasets labeled for bias, sentiment, or formality; measure whether targeted concepts are suppressed while fluency is preserved.

### Open Question 3
- Question: What is the optimal parameter selection (α, Top-k) for EigenShift across diverse model architectures and scales?
- Basis in paper: Ablation study tests α and Top-k on LLaMA-2-7B only, finding α=0.9 and Top-k=1024 effective, but lacks systematic tuning across other architectures.
- Why unresolved: Parameter sensitivity is architecture-dependent; a single configuration may not transfer, especially to non-LLaMA models or larger scales.
- What evidence would resolve it: Conduct ablation studies of α and Top-k across multiple architectures and sizes, reporting TPH scores to identify architecture-specific optima.

## Limitations
- The method's reliance on linear SVD of the lm_head layer may not capture all toxic generation mechanisms, particularly non-linear interactions across layers.
- Cross-lingual robustness remains unproven for languages with different toxicity expression patterns beyond English and Chinese.
- The offline computation of Δ_i scores requires substantial toxic/non-toxic sample collections, which may be impractical or biased.

## Confidence

- **High Confidence**: The core mathematical framework of SVD-based decomposition and reconstruction validity are well-established. Superiority of layer-wise representations over single neurons is supported by systematic comparison.
- **Medium Confidence**: The claim of achieving 58% toxicity reduction while maintaining fluency is supported by results, but sensitivity to parameter choices and potential dataset-specific overfitting require further validation.
- **Low Confidence**: Interpretability claims regarding "generation experts" vs "detection experts" occupying orthogonal subspaces are primarily theoretical with limited empirical validation.

## Next Checks

1. **Layer-wise Sensitivity Analysis**: Extend the intervention beyond lm_head to intermediate layers (e.g., layer 12, 18, 24 in LLaMA-2-7B) and measure how toxicity reduction and perplexity preservation vary across architectural positions.

2. **Cross-domain Transferability Test**: Train Δ_i scores on Jigsaw (Wikipedia/Reddit toxicity) and apply the same eigenvector damping to generations from domain-specific corpora (medical forums, legal documents, academic writing).

3. **Long-range Dependency Stress Test**: Generate extended sequences (2000+ tokens) with EigenShift intervention and analyze whether toxic content suppression degrades over sequence length or causes accumulation of suppression artifacts.