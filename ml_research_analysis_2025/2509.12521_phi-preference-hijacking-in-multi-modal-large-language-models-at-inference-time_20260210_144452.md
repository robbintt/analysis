---
ver: rpa2
title: 'Phi: Preference Hijacking in Multi-modal Large Language Models at Inference
  Time'
arxiv_id: '2509.12521'
source_url: https://arxiv.org/abs/2509.12521
tags:
- response
- image
- preference
- score
- hijacking
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Preference Hijacking (Phi), a novel inference-time
  attack that manipulates the output preferences of multimodal large language models
  (MLLMs) by embedding carefully optimized images. Unlike prior methods that force
  fixed responses or conceal instructions, Phi preserves contextual relevance between
  the user's query and the image while steering the model toward attacker-specified
  preferences.
---

# Phi: Preference Hijacking in Multi-modal Large Language Models at Inference Time

## Quick Facts
- arXiv ID: 2509.12521
- Source URL: https://arxiv.org/abs/2509.12521
- Authors: Yifan Lan; Yuanpu Cao; Weitong Zhang; Lu Lin; Jinghui Chen
- Reference count: 23
- This paper introduces Preference Hijacking (Phi), a novel inference-time attack that manipulates the output preferences of multimodal large language models (MLLMs) by embedding carefully optimized images.

## Executive Summary
This paper introduces Preference Hijacking (Phi), a novel inference-time attack that manipulates the output preferences of multimodal large language models (MLLMs) by embedding carefully optimized images. Unlike prior methods that force fixed responses or conceal instructions, Phi preserves contextual relevance between the user's query and the image while steering the model toward attacker-specified preferences. The approach leverages preference learning to optimize adversarial perturbations and introduces universal hijacking perturbations that can be transferred across different images without retraining. Experimental results across multiple MLLMs, tasks, and preferences demonstrate Phi's effectiveness in manipulating both text-only and multi-modal responses while maintaining visual similarity to clean images. The method raises significant safety concerns for the deployment of open-sourced MLLMs.

## Method Summary
Phi optimizes adversarial perturbations using a preference-contrastive objective derived from Direct Preference Optimization (DPO). The method optimizes perturbations to maximize the log-probability ratio between target-preference responses and opposite-preference responses while maintaining visual stealth through ℓ∞ constraints. Three perturbation approaches are investigated: additive noise, patch-based, and border-based perturbations. The patch-based and border-based methods enable universal hijacking perturbations that transfer across different images without retraining. The optimization uses Projected Gradient Descent (PGD) with a perturbation budget of Δ = 16/255. The approach is evaluated on both text-only and multi-modal tasks across multiple MLLMs including LLaVA-1.5-7B, Llama-3.2-11B, and Qwen2.5-VL-7B.

## Key Results
- Phi successfully manipulates MLLM output preferences on both text-only and multi-modal tasks while maintaining contextual relevance
- Universal hijacking perturbations (patch and border-based) can transfer across different images without retraining
- Perturbations remain visually similar to clean images while effectively shifting model preferences
- Basic preprocessing defenses like JPEG compression and noise injection can mitigate Phi's effectiveness

## Why This Works (Mechanism)

### Mechanism 1: Preference-Contrastive Perturbation Optimization
- Claim: Adversarial perturbations can systematically shift MLLM output preferences toward attacker-specified targets.
- Mechanism: Phi optimizes perturbations by maximizing the log-probability ratio between target-preference responses and opposite-preference responses, derived from Direct Preference Optimization principles (Equation 1). The optimization uses PGD with an ℓ∞ constraint (Δ = 16/255) to maintain visual stealth while amplifying the gradient direction that increases target-response likelihood.
- Core assumption: The MLLM's visual encoder representations admit gradients that can be systematically shifted to bias downstream preference outputs.

### Mechanism 2: Visual-Preference Coupling via Contrastive Data
- Claim: Preference manipulation succeeds when perturbations are trained on contrastive response pairs that explicitly represent the target preference.
- Mechanism: A dataset D = {(x, q, rt, ro)} is constructed where rt reflects the target preference (e.g., power-seeking) and ro reflects the opposite preference (e.g., humility). The perturbation h is optimized to shift the model's likelihood ratio toward rt and away from ro, regardless of the specific query content.
- Core assumption: The visual modality can encode preference-relevant signals that generalize across diverse textual queries.

### Mechanism 3: Universal Perturbation Transfer via Spatially-Constrained Patterns
- Claim: A single perturbation pattern (patch or border) can transfer across unseen images for a given preference.
- Mechanism: Instead of optimizing image-specific perturbations, Phi-Border and Phi-Patch optimize a fixed spatial pattern that is applied across a distribution of images during training. The pattern is constrained to a small region (168×168 patch or ~250×250 border), forcing the optimization to learn preference features independent of image content.
- Core assumption: The MLLM's visual encoder aggregates information from local regions that can influence global preference outputs.

## Foundational Learning

- Concept: **Direct Preference Optimization (DPO) Objective**
  - Why needed here: Phi's core loss function (Equation 1) is derived from DPO, which replaces explicit reward modeling with a closed-form objective over preference pairs.
  - Quick check question: Can you explain why DPO uses a single model (no reference model) in Phi's formulation, unlike the original DPO which requires both?

- Concept: **Projected Gradient Descent (PGD) for ℓ∞-Constrained Optimization**
  - Why needed here: The perturbation h must remain imperceptible (‖h‖∞ ≤ Δ), and PGD with sign gradients is the standard method for this.
  - Quick check question: After each gradient step, how should the perturbation be projected back into the ℓ∞ ball?

- Concept: **Vision Encoder Patch Structure in MLLMs**
  - Why needed here: Ablation results (Tables 7-8) show that patch/border sizes smaller than the encoder's patch size (14 for LLaVA) are ineffective.
  - Quick check question: If you were targeting a model with 32×32 vision encoder patches, what minimum perturbation size would you test first?

## Architecture Onboarding

- Component map:
  - Input: Clean image x, text query q (unknown at attack time), preference dataset D with contrastive pairs
  - Perturbation Optimizer: PGD loop computing gradients of Equation 1, projecting to ℓ∞ constraint
  - Perturbation Types: Image-specific h, Phi-Patch (fixed-location patch), Phi-Border (peripheral border), or scattered patches
  - Output: Hijacked image xh = x + h (or x with patch/border overlay)

- Critical path:
  1. Construct D: For each preference, collect contrastive response pairs (rt, ro) using unaligned models or human annotations
  2. Optimize h: Run PGD for 10,000 iterations with batch size 2, gradient accumulation 8, Δ = 16/255
  3. Validate: Test on held-out queries and images (training/test sets use different questions)
  4. Deploy: Embed h into target images; model processes xh and q at inference time

- Design tradeoffs:
  - **Additive vs. Patch/Border**: Additive noise is more imperceptible but requires clipping on new images, reducing transfer. Patch/border avoids clipping but is more visible
  - **Patch size vs. Stealth**: Larger patches (168×168) are more effective (Table 8) but less stealthy. Scattered patches (Figure 4) offer a middle ground
  - **Image-specific vs. Universal**: Image-specific h is more effective (Table 3 vs. Table 4) but requires per-image optimization. Universal patterns are cheaper but slightly less performant

- Failure signatures:
  - **Low P-Score with high MC**: Model is selecting the target option mechanically but not generating detailed, preference-aligned text (check response naturalness)
  - **Defense degradation**: JPEG compression with quality ≤ 30, rescaling with RF ≤ 0.5, or Gaussian noise with σ ≥ 40 significantly reduces MC (Table 9-10). Consider adaptive attacks that include these as data augmentations
  - **Cross-model transfer failure**: Perturbations optimized on LLaVA may not transfer to Llama or Qwen without re-optimization (Tables 2-5 show model-specific results)

- First 3 experiments:
  1. **Reproduce text-only task results** (Table 2): Start with Wealth-seeking on LLaVA-1.5-7B. Verify that P-Score increases from ~1.84 (Clean) to ~2.89 (Phi) with Δ = 16/255. If P-Score remains low, check contrastive response quality
  2. **Ablate perturbation magnitude**: Sweep Δ ∈ {8, 16, 32, 64}/255 on the City dataset. Confirm the threshold at Δ = 16/255 per Table 6. If no improvement, verify gradient flow through the vision encoder
  3. **Test universal perturbation transfer**: Optimize Phi-Border on the Landscape training set (60 images), then evaluate on held-out images (Table 4). If MC drops below 45%, check if border size is at least 2× the vision encoder patch size

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does Preference Hijacking (Phi) maintain effectiveness in multi-turn dialogue scenarios where the model accumulates context over several exchanges?
- Basis in paper: [explicit] Section 6 (Limitations) states that the study focuses on single-turn dialogues and notes that "the ability of Phi to consistently maintain preference manipulation over extended interactions remains unexplored."
- Why unresolved: The current experimental design is restricted to single-query responses to establish a baseline for hijacking efficacy, leaving the temporal dynamics of preference retention in conversations unknown.
- What evidence would resolve it: Measuring the Attack Success Rate (ASR) and Preference Score (P-Score) over extended multi-turn dialogue benchmarks (e.g., MT-Bench) with Phi applied to the initial image input.

### Open Question 2
- Question: Can adaptive attacks be developed to bypass standard preprocessing defenses like JPEG compression or noise injection?
- Basis in paper: [explicit] Section 4.5 (Defense Analysis) suggests that while basic preprocessing helps, "More sophisticated adaptive attacks could potentially be developed to bypass such preprocessing defense, for example, by incorporating these preprocessing methods as data augmentations during training."
- Why unresolved: The paper evaluates static defenses but does not implement an adaptive optimization loop (e.g., EOT - Expectation Over Transformation) to harden the perturbation against these specific transformations.
- What evidence would resolve it: A new experiment where the Phi optimization process includes the defense transformation (e.g., differentiable JPEG or noise layers) in the gradient calculation to test if robustness persists.

### Open Question 3
- Question: Can hijacking perturbations optimized on open-source models transfer effectively to closed-source, black-box MLLMs (e.g., GPT-4o)?
- Basis in paper: [inferred] The Threat Model in Section 3.1 explicitly assumes "attackers have white-box access to the target MLLM." While the paper demonstrates transferability across images, it doesn't explore transferability across model architectures or to proprietary black-box systems.
- Why unresolved: The experimental evaluation is limited to open-weights models (LLaVA, Llama, Qwen), and the gradient-based optimization relies on accessing internal model parameters.
- What evidence would resolve it: Optimizing perturbations using surrogate open-source models and testing their effectiveness when querying the API of a black-box commercial model.

## Limitations

- Limited Cross-Architecture Transfer: Effectiveness degrades when transferring perturbations between different MLLM architectures, suggesting architectural differences in visual encoder receptive fields
- Sensitivity to Defense Mechanisms: Basic preprocessing defenses like JPEG compression and noise injection can significantly reduce hijacking effectiveness
- Dataset Construction Transparency: The paper doesn't fully specify how contrastive preference pairs are generated, making it difficult to assess data quality and potential biases

## Confidence

- **High Confidence**: The core mechanism of using DPO-style contrastive optimization with PGD is well-established and mathematically sound
- **Medium Confidence**: The effectiveness metrics are reasonable but rely on GPT-4o preference alignment without human validation
- **Low Confidence**: Universal perturbation transfer claims are limited, particularly regarding transfer across different visual encoders and domains

## Next Checks

1. **Architecture Transfer Study**: Optimize Phi perturbations on LLaVA-1.5-7B and systematically evaluate transfer effectiveness across a diverse set of MLLMs including Llama-3.2-11B, Qwen2.5-VL-7B, and models with different vision backbones (SigLIP vs CLIP). Measure both MC and P-Score to assess preference hijacking quality.

2. **Adaptive Attack Against Defenses**: Implement an adaptive version of Phi that incorporates JPEG compression, rescaling, and Gaussian noise as data augmentations during the PGD optimization. Compare effectiveness against static Phi to quantify the potential for defensive countermeasures.

3. **Human Preference Validation**: Conduct a human study where participants rate responses from hijacked vs clean images on multiple preference dimensions (coherence, relevance, preference alignment). Compare human ratings with GPT-4o Preference Score to validate the automated metric's alignment with human judgment.