---
ver: rpa2
title: Adaptive Identification and Modeling of Clinical Pathways with Process Mining
arxiv_id: '2512.03787'
source_url: https://arxiv.org/abs/2512.03787
tags:
- process
- clinical
- pathways
- mining
- healthcare
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a two-phase process mining method for adaptive
  identification and modeling of clinical pathways in healthcare. The method addresses
  the challenge of evolving patient treatment procedures by combining historical data
  modeling with online conformance checking and clustering.
---

# Adaptive Identification and Modeling of Clinical Pathways with Process Mining

## Quick Facts
- arXiv ID: 2512.03787
- Source URL: https://arxiv.org/abs/2512.03787
- Reference count: 40
- Primary result: Adaptive process mining method achieves up to 95.62% AUC and 67.11% arc-degree simplicity for clinical pathway modeling

## Executive Summary
This paper presents a two-phase process mining method for adaptive identification and modeling of clinical pathways in healthcare. The method addresses the challenge of evolving patient treatment procedures by combining historical data modeling with online conformance checking and clustering. Historical treatment data is first modeled into a baseline process model, then new treatment data is monitored for conformance against this model. When deviations exceed a threshold, new clinical pathways are identified through clustering and added to the knowledge base. Experiments using the Synthea COVID-19 dataset demonstrate the method's effectiveness in expanding clinical pathway knowledge while maintaining high precision and manageable model complexity.

## Method Summary
The method uses a two-phase approach to adaptively identify and model clinical pathways. In Phase 1, historical event logs are collected and transformed into a baseline process model using process discovery algorithms (ILP, IM, or HM). In Phase 2, new event logs are compared against the knowledge base via conformance checking. When fitness scores fall below a threshold, non-conforming traces are clustered based on their conformance diagnostics, and new specialized models are mined and added to the knowledge base. The approach uses alignment-based conformance checking to generate diagnosis matrices that capture per-activity mismatch patterns, which serve as features for clustering. This prevents the knowledge base from becoming a single monolithic "spaghetti" model by maintaining multiple simpler specialized models instead.

## Key Results
- Achieves up to 95.62% AUC in distinguishing clinical pathways from different disease complications
- Maintains arc-degree simplicity of 67.11% even as the knowledge base expands
- Successfully identifies distinct treatment pathways for different disease variants and complications
- Outperforms single monolithic model approaches that suffer from "spaghetti" effect (17.67% simplicity)

## Why This Works (Mechanism)

### Mechanism 1: Two-Phase Knowledge Base Expansion
- Claim: Separating offline baseline modeling from online adaptive expansion enables detection of clinically distinct pathways while preserving interpretability.
- Mechanism: Phase 1 creates reference process model from historical data. Phase 2 monitors new traces via conformance checking; when fitness drops below threshold, non-conforming traces are clustered and new specialized models are mined rather than merging into increasingly complex single model.
- Core assumption: Distinct clinical pathways produce detectable deviations that cluster into coherent groups.
- Evidence anchors: [abstract] describes two-phase approach; [Section 3.2] states new models are mined when conformance is unsatisfying.
- Break condition: If deviations are random noise rather than systematic pathway differences, clustering produces spurious models.

### Mechanism 2: Conformance Diagnostics as Feature Vectors for Clustering
- Claim: Using alignment-based conformance diagnostics (per-activity mismatch counts) as clustering features groups traces by structural deviation patterns.
- Mechanism: Conformance checking produces diagnosis matrix D capturing mismatches per activity per trace. Traces with similar diagnosis vectors are clustered, revealing structurally related deviations that may indicate shared clinical pathways.
- Core assumption: Traces from same emerging pathway exhibit similar patterns of missing, duplicated, or reordered activities relative to reference model.
- Evidence anchors: [Section 3.2, Table 1] defines diagnosis matrix D; [corpus] neighbor paper 15446 uses conformance checking for anomaly detection.
- Break condition: If reference model has low fitness even for "normal" traces, diagnosis vectors become noisy and clustering degrades.

### Mechanism 3: Model Complexity Control via Simplicity-Precision Tradeoff
- Claim: Fragmenting pathways into multiple specialized models maintains higher arc-degree simplicity than expanding single monolithic model.
- Mechanism: Creates separate models for each detected pathway cluster rather than incrementally adding all new behavior to one model.
- Core assumption: Users can manage and interpret collection of simpler models better than one complex "spaghetti" model.
- Evidence anchors: [Section 4.2, Table 3] shows baseline ILP drops to 17.67% simplicity while ILP+DBScan maintains 67.11%.
- Break condition: If number of models grows unboundedly (up to 22 in experiments), knowledge base management becomes challenge.

## Foundational Learning

- Concept: Process mining fundamentals (event logs, traces, process discovery)
  - Why needed here: The entire method builds on converting clinical event data into process models. You must understand what an event log, trace, and activity sequence are to follow the architecture.
  - Quick check question: Given a patient with events [admission, X-ray, medication, discharge], what is the trace and what is the activity set?

- Concept: Conformance checking and fitness
  - Why needed here: Phase 2 relies entirely on fitness scores to trigger adaptation. You need to understand alignment-based conformance to interpret diagnosis vectors.
  - Quick check question: If a trace has fitness 0.8 against a model, what does the remaining 0.2 represent in terms of deviations?

- Concept: Density-based vs. probabilistic clustering
  - Why needed here: The method compares DBScan, OPTICS, and DPGMM—each makes different assumptions about cluster shape and noise handling.
  - Quick check question: Why might DPGMM outperform DBScan when pathway clusters have overlapping regions in diagnosis space?

## Architecture Onboarding

- Component map:
  - Phase 1: EHR data → Event log extraction → Variant filtering → Process discovery (ILP/IM/HM) → Baseline model N
  - Phase 2: New event log → Conformance checking vs. all N → Diagnosis matrix D → Threshold check → Clustering (DBScan/OPTICS/DPGMM) → Sub-log generation → Process discovery → Update knowledge base

- Critical path: Event log quality → Conformance threshold setting → Clustering algorithm choice → Model count explosion risk. The threshold parameter th most directly controls adaptation sensitivity.

- Design tradeoffs:
  - Higher threshold → more conservative adaptation, fewer models, potentially missing emerging pathways
  - Lower threshold → aggressive expansion, model proliferation, higher maintenance burden
  - Simplicity vs. coverage: Specialized models are simpler individually but create larger collection to manage

- Failure signatures:
  - Rapid model count growth with low AUC improvement: clustering is capturing noise, not systematic pathways
  - High AUC but very low simplicity: single-model baseline may be preferable despite complexity
  - Fitness never exceeding threshold even for training data: baseline model is too restrictive or data quality is poor

- First 3 experiments:
  1. Reproduce single-iteration behavior: Train on L_PE only, test conformance checking against held-out L_PE traces. Verify baseline fitness distribution before adding adaptation complexity.
  2. Threshold sensitivity analysis: Run Phase 2 with thresholds from 0.5 to 0.95 in 0.05 increments. Plot model count vs. AUC to identify operating range.
  3. Clustering algorithm ablation: Fix process discovery algorithm (e.g., HM), run all three clustering methods on same L_ARD deviation data. Compare resulting model semantics—do clusters map to clinically meaningful pathway distinctions?

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the method prevent the uncontrolled proliferation of process models in the knowledge base as the number of online adaptation iterations increases?
- Basis in paper: [explicit] The authors state that future work involves "improving the adaptation cycle... by reducing the number of models, which can become significantly higher as more iterations are performed."
- Why unresolved: The current approach creates new models for non-conforming clusters, which risks fragmentation and reduced interpretability over time.
- What evidence would resolve it: An algorithmic enhancement that merges or prunes redundant models while maintaining the reported high AUC and simplicity scores.

### Open Question 2
- Question: Does the method retain high precision and simplicity when applied to real-world electronic medical records rather than synthetic data?
- Basis in paper: [explicit] The "Discussion and limitations" section notes that "validation with actual, real-life electronic medical records would strengthen the external validity of the results."
- Why unresolved: The study relies entirely on the Synthea benchmark, which may not capture the noise and variability of actual clinical environments.
- What evidence would resolve it: Successful replication of the experimental results using noisy, real-life hospital event logs.

### Open Question 3
- Question: Can an explanation layer be integrated to determine the root causes of clinical pathway deviations?
- Basis in paper: [explicit] The Conclusion lists adding "an explanation layer to our method to investigate the root causes of clinical pathway deviations" as a goal.
- Why unresolved: The current framework detects non-conformance and clusters traces but does not provide semantic reasoning for why a deviation occurred.
- What evidence would resolve it: A module that correlates specific conformance diagnostics with clinical or operational variables to output human-readable explanations.

## Limitations

- Critical hyperparameter unknown: The conformance threshold value (th) triggering new pathway discovery is not specified, requiring experimentation for reproduction.
- Clustering parameter sensitivity: Specific hyperparameters for DBScan, OPTICS, and DPGMM are not provided, making results potentially sensitive to parameter choices.
- Knowledge base scalability: The method can produce up to 22 models, creating potential management challenges not addressed in the paper.

## Confidence

- **High confidence** in two-phase mechanism architecture and general effectiveness, supported by clear AUC (up to 95.62%) and simplicity (67.11%) results.
- **Medium confidence** in clustering-based pathway identification mechanism, as approach is novel and while results are promising, clustering hyperparameters are unspecified.
- **Low confidence** in long-term scalability and knowledge base management, as paper doesn't address what happens when model count grows beyond observed 22 models.

## Next Checks

1. **Baseline fitness verification**: Before implementing adaptation complexity, reproduce single-iteration behavior by training on L_PE only and testing conformance checking against held-out L_PE traces. Verify baseline fitness distribution and identify if reference model already has low fitness even for "normal" traces.

2. **Threshold sensitivity analysis**: Systematically run Phase 2 with thresholds from 0.5 to 0.95 in 0.05 increments. Plot model count versus AUC to identify optimal operating range where adaptation provides meaningful improvement without excessive fragmentation.

3. **Clustering algorithm ablation study**: Fix process discovery algorithm (e.g., HM), run all three clustering methods (DBScan, OPTICS, DPGMM) on same L_ARD deviation data. Compare resulting model semantics to determine if clusters map to clinically meaningful pathway distinctions and identify which algorithm best captures systematic deviations.