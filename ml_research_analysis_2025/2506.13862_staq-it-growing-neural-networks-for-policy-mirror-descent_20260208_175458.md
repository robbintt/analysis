---
ver: rpa2
title: StaQ it! Growing neural networks for Policy Mirror Descent
arxiv_id: '2506.13862'
source_url: https://arxiv.org/abs/2506.13862
tags:
- policy
- uni00000014
- eval
- staq
- step
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: StaQ is a new deep RL algorithm that uses entropy regularization
  with KL constraints between successive policies, following Policy Mirror Descent
  theory. It stores up to M past Q-functions and combines them into a policy by stacking
  neural networks, with a closed-form, optimization-free update.
---

# StaQ it! Growing neural networks for Policy Mirror Descent

## Quick Facts
- arXiv ID: 2506.13862
- Source URL: https://arxiv.org/abs/2506.13862
- Reference count: 40
- Primary result: StaQ is a new deep RL algorithm that uses entropy regularization with KL constraints between successive policies, following Policy Mirror Descent theory, achieving competitive performance with reduced oscillation.

## Executive Summary
StaQ is a novel deep reinforcement learning algorithm that addresses the challenge of stable policy optimization by storing and combining up to M past Q-functions into a policy through a "stacked neural network" architecture. The key innovation is a weight-corrected finite memory mechanism that enables theoretical convergence to the optimal policy without the irreducible error found in naive truncation methods. Empirically, StaQ demonstrates competitive performance with state-of-the-art deep RL baselines on MuJoCo and MinAtar tasks while showing significantly reduced performance oscillation, suggesting greater stability in learning.

## Method Summary
StaQ implements Policy Mirror Descent theory in deep RL by maintaining a finite memory of past Q-functions (up to M) and combining them into a policy through stacked neural networks. Unlike traditional methods that train a policy network via gradient descent, StaQ freezes the weights of past Q-networks and computes the policy logits exactly by passing states through all M networks and summing their weighted outputs. The algorithm uses entropy regularization with KL constraints between successive policies, and employs an epsilon-softmax behavior policy for exploration. Policy evaluation is performed using standard TD-learning on a single active Q-network, while the policy update is optimization-free, relying solely on the weighted combination of frozen network outputs.

## Key Results
- StaQ achieves competitive performance with deep RL baselines on MuJoCo and MinAtar tasks
- The algorithm shows reduced performance oscillation compared to baseline methods, indicating greater stability
- Theoretical analysis proves convergence for sufficiently large M, eliminating the irreducible error found in naive finite-memory approaches

## Why This Works (Mechanism)

### Mechanism 1: Weight-Corrected Finite Memory
The algorithm converges to the optimal policy using a finite memory of M past Q-functions by introducing a rescaling factor of 1/(1-β^M) to the weights. This correction ensures the Q-function weights sum to 1, approximating the infinite series' cumulative weight without requiring infinite memory storage.

### Mechanism 2: Optimization-Free Policy Update
StaQ creates a stable policy by physically stacking neural networks to compute the exact softmax of the weighted Q-sum, avoiding approximation errors from gradient-based policy updates. The policy logits are computed exactly by passing the state through all M networks and summing their weighted outputs.

### Mechanism 3: Implicit Ensemble Stabilization
Averaging predictions over multiple historical Q-functions reduces performance oscillation caused by variance in individual Q-estimates. By aggregating M Q-functions, the policy acts as a temporal ensemble that smooths out catastrophic forgetting and sudden value shifts common in single-network deep RL.

## Foundational Learning

- **Concept: Policy Mirror Descent (PMD)**
  - Why needed here: PMD frames RL as a regularized convex optimization problem where the update involves a sum over all past Q-functions
  - Quick check question: Can you explain why the standard PMD update is intractable for neural networks without approximation?

- **Concept: Entropy & KL Regularization**
  - Why needed here: StaQ relies on specific definitions of temperature α and decay β derived from entropy weight τ and KL step-size η
  - Quick check question: How does increasing the KL constraint weight η affect the decay factor β and the required memory size M?

- **Concept: Fitted Q Iteration (FQI)**
  - Why needed here: StaQ uses FQI for the policy evaluation step (training Q_k) before stacking it
  - Quick check question: In StaQ, is the policy learned via gradient ascent on a policy objective, or derived directly from the value function?

## Architecture Onboarding

- **Component map:** Q-Network (Trainable) -> Stack (SNN) -> Logits Computation -> Behavior Policy
- **Critical path:** Collect data using current Behavior Policy → Train the single Q-Network on this data → Freeze the Q-Network weights and push them to the Stack → Compute new logits ξ_k+1 using the updated Stack
- **Design tradeoffs:** Larger M guarantees theoretical convergence and stability but increases memory usage and forward-pass latency; exploration requires epsilon-softmax mixing; currently limited to discrete action spaces
- **Failure signatures:** Oscillating performance if M is too small; exploration stalls on sparse reward tasks; slow inference if stack forward pass isn't batched efficiently
- **First 3 experiments:**
  1. Ablation on M: Run StaQ on Acrobot with M=[1, 10, 50, 300] to verify stability improvements with larger M
  2. Weight Correction Validation: Compare "Vanilla" finite memory update vs. "Weight Corrected" update to check for irreducible error
  3. Behavior Policy Test: Compare pure softmax (ε=0) vs. epsilon-softmax (ε=0.05) to observe Q-function stability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can extending StaQ to continuous action domains retain stability benefits despite losing the optimization-free update?
- Basis in paper: [explicit] The authors state extending StaQ to continuous action domains could be done as in SAC, but this will lose the optimization-free and exact nature
- Why unresolved: The method currently relies on finite action spaces to compute the exact closed-form update via stacked neural networks
- What evidence would resolve it: A continuous-action variant demonstrating reduced oscillation compared to standard actor-critic methods like SAC

### Open Question 2
- Question: Do sophisticated policy evaluation techniques (e.g., normalization in PQN/CrossQ) resolve the remaining instabilities in StaQ's Q-function learning?
- Basis in paper: [explicit] "StaQ provides a promising setting for testing more sophisticated forms of policy evaluation" and "the catastrophic forgetting in the Q-function itself should be addressed in the future work"
- Why unresolved: Despite a stable policy update, the authors note that "some instability in learning the Q-function remains"
- What evidence would resolve it: Empirical results showing stable, monotonic convergence when combining StaQ with advanced critic regularization or normalization techniques

### Open Question 3
- Question: Can the theoretical lower bound for the memory size M be refined to be less pessimistic?
- Basis in paper: [inferred] The authors note that theoretical values for M are "generally pessimistic" compared to smaller values empirically found sufficient for convergence
- Why unresolved: The current analysis requires large M to satisfy strict contraction inequalities, potentially over-estimating requirements
- What evidence would resolve it: A tighter theoretical bound or systematic empirical analysis showing the minimum viable M scales differently than the current logarithmic bound

## Limitations
- The method is restricted to discrete action spaces, significantly limiting applicability to continuous control problems
- Computational overhead of maintaining and evaluating M neural networks could become prohibitive for larger models
- Theoretical convergence guarantees depend on problem-specific constants that aren't characterized, creating tuning challenges

## Confidence
- **High:** The empirical demonstration that StaQ reduces performance oscillation compared to baselines is well-supported by Figure 3
- **Medium:** The theoretical convergence proof is rigorous, but its practical implications depend on problem-specific constants
- **Medium:** The claim of competitive performance with state-of-the-art algorithms is supported by results, though comparison is limited to specific benchmark suites

## Next Checks
1. **Memory Size Sensitivity:** Systematically vary M on a simple control task (e.g., Acrobot) to identify the minimum M required for stable learning and characterize the trade-off between stability and computational cost
2. **Continuous Action Extension:** Investigate whether the core insights (weighted ensemble of value functions) can be adapted to continuous action spaces using actor-critic architectures or distributional RL approaches
3. **Computational Overhead Analysis:** Measure wall-clock time per update and memory consumption as a function of M and network size to quantify practical scalability limits