---
ver: rpa2
title: 'AFed: Algorithmic Fair Federated Learning'
arxiv_id: '2501.02732'
source_url: https://arxiv.org/abs/2501.02732
tags:
- data
- fairness
- learning
- distribution
- local
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes AFed, a framework for training fair models
  in federated learning without centralized data access. The core idea is to learn
  the global data distribution using conditional generators or GANs trained collectively
  by clients, then use the generated samples to augment local data for debiasing.
---

# AFed: Algorithmic Fair Federated Learning

## Quick Facts
- **arXiv ID:** 2501.02732
- **Source URL:** https://arxiv.org/abs/2501.02732
- **Reference count:** 40
- **Primary result:** AFed-GAN achieves state-of-the-art fairness (reduced ΔDP) while maintaining or improving accuracy compared to FedAvg, FedReg, and FairFed baselines in federated learning settings.

## Executive Summary
This paper proposes AFed, a framework for training fair models in federated learning without centralized data access. The core idea is to learn the global data distribution using conditional generators or GANs trained collectively by clients, then use the generated samples to augment local data for debiasing. Two approaches are proposed: AFed-G, which uses a conditional generator on the server side, and AFed-GAN, which uses a conditional GAN on the client side with an auxiliary classification head to extract informative features. Theoretical analysis and empirical results on four real-world datasets show substantial improvements in fairness (measured by demographic parity) while maintaining or improving accuracy compared to baselines like FedAvg, FedReg, and FairFed. AFed-GAN outperforms AFed-G on more complex tasks due to better sample diversity, and the auxiliary classification head significantly improves performance.

## Method Summary
AFed addresses fair federated learning by learning global data distributions through collective generator training without centralized data access. The framework consists of two variants: AFed-G (server-side conditional generator) and AFed-GAN (client-side conditional GAN). Both approaches use Mixup-based debiasing where real samples are mixed with counterfactual generated samples to enforce prediction invariance. The conditional generators are trained to synthesize feature vectors conditioned on sensitive attributes, enabling local clients to augment their data with samples from other demographic groups. An auxiliary classification head is added in AFed-GAN to force the feature extractor to retain attribute information, improving generator training. The framework is theoretically analyzed and empirically validated on four datasets, showing significant fairness improvements while maintaining accuracy.

## Key Results
- AFed-GAN outperforms AFed-G on complex tasks due to better sample diversity
- AFed-GAN achieves state-of-the-art fairness while maintaining or improving accuracy compared to baselines
- Auxiliary classification head significantly improves performance by forcing feature disentanglement
- AFed-G performance scales with participant ratio, while AFed-GAN remains stable across ratios

## Why This Works (Mechanism)

### Mechanism 1: Global Distribution Learning via Collective Generator Training
A conditional generator/GAN trained collectively by federated clients can approximate the global data distribution without centralized data access, enabling effective local debiasing. Clients train components that capture their local distribution (either classification heads encoding p(a|z) for AFed-G, or discriminator D for AFed-GAN). These are aggregated to form a global generator G that synthesizes latent features conditioned on sensitive attributes. Generated samples augment local data, giving each client visibility into the global distribution. The core assumption is that aggregated local distribution knowledge can approximate the true global conditional distribution p(Z|A) sufficiently for debiasing purposes.

### Mechanism 2: Auxiliary Classification Head Preserves Attribute Information
Adding an auxiliary classification head ha for sensitive attributes forces the feature extractor to retain information about both labels and sensitive attributes, enabling the generator to produce attribute-conditional samples. The feature extractor E receives gradient signals from both hy (label classifier) and ha (attribute classifier). Without ha, E compresses attribute information. With ha, samples are separated into distinct regions by (y, a) pairs, allowing the generator to learn meaningful conditional mappings. The core assumption is that the attribute classification task provides useful feedback that doesn't conflict with the label classification objective.

### Mechanism 3: Mixup-Based Debiasing Enforces Prediction Invariance
Mixing real latent features with counterfactual generated features (different attribute) and penalizing prediction variance achieves demographic parity. For real sample zt with attribute a0, generate zf with attribute a1. Create z̄ = t·zt + (1-t)·zf where t ~ Beta(α,α). The fairness loss penalizes d/dt E[f(z̄)], forcing predictions to be invariant to the interpolation—and thus to attribute changes. The core assumption is that linear interpolation in latent space corresponds to meaningful semantic interpolation, and invariance on mixed samples generalizes to invariance on real demographic groups.

## Foundational Learning

- **Concept: Demographic Parity (ΔDP)**
  - Why needed here: This is the paper's primary fairness metric. It measures |Pr(Ŷ=1|A=0) - Pr(Ŷ=1|A=1)|. The goal is minimizing ΔDP while maintaining accuracy.
  - Quick check question: Why might a model with ΔDP = 0 still be problematic for fairness? (Hint: consider individual-level fairness or other metrics.)

- **Concept: H-Divergence and Distribution Shift in FL**
  - Why needed here: Theorem 2 shows that local debiasing fails because d_HΔH(D_k, D_g) can be large due to non-IID data. This formalizes why single-client debiasing doesn't generalize.
  - Quick check question: If H-divergence between a client's data and global data is large, what does Remark 1 suggest about the benefit of augmentation?

- **Concept: Conditional GAN Training**
  - Why needed here: AFed-GAN trains a conditional GAN in latent space. The generator G(ε, a) produces features conditioned on attribute a; discriminator D(z, a) distinguishes real from fake conditional samples.
  - Quick check question: How does the training objective of conditional GAN (Eq. 8) differ from unconditional GAN (Eq. 7)?

## Architecture Onboarding

- **Component map**:
  ```
  Server:
    - Aggregates: θE, θy, θa, wG, wD
    - (AFed-G only) Trains G using aggregated ha models
  
  Client k:
    - Feature Extractor E: X → Z
    - Label Head hy: Z → Ŷ
    - Attribute Head ha: Z → Â
    - Generator G: (ε, a) → z_fake
    - Discriminator D: (z, a) → {real, fake}
  ```

- **Critical path**:
  1. **Initialization**: All clients receive initial θE, θy, θa, wG, wD
  2. **Feature Learning Phase** (T1 epochs): Train E, hy, ha with J1 (classification) + J5 (debiasing) losses
  3. **Distribution Learning Phase** (T2 epochs): Train G, D with J3 (discriminator) + J4 (generator) losses
  4. **Aggregation**: Server aggregates all client updates
  5. **(AFed-G only)** Server trains G with J2 using aggregated ha knowledge
  6. Repeat until convergence

- **Design tradeoffs**:
  | Choice | Pros | Cons |
  |--------|------|------|
  | AFed-G | Lower client compute/communication; server-side generator | Poor sample diversity; fails at low participant ratios |
  | AFed-GAN | Better diversity; robust to participant ratio | Higher client compute; discriminator training instability |
  | Latent space augmentation | Compact; privacy-preserving | May miss input-level bias patterns |
  | Higher λ | Better fairness | Potential accuracy drop |

- **Failure signatures**:
  - **High std(ΔDP) across runs**: Indicates generator lacks diversity (AFed-G symptom)
  - **Generated samples overlap across attributes in UMAP**: ha is not providing useful feedback; check gradient flow
  - **AFed-G degrades at r < 0.4**: Insufficient clients to capture decision boundary; switch to AFed-GAN
  - **Accuracy collapses with debiasing**: λ too high; reduce tradeoff weight
  - **Discriminator never converges**: GAN training instability common in FL; consider spectral normalization or gradient penalty

- **First 3 experiments**:
  1. **Validate on Adult (tabular) dataset**: Simple task where AFed-G ≈ AFed-GAN. Verify both methods reduce ΔDP vs FedAvg while maintaining accuracy. Set λ ∈ {0.1, 0.5, 1.0} to trace accuracy-fairness tradeoff curve.
  2. **Ablate auxiliary head on UTKFace**: Run AFed-GAN with and without ha. Expect performance drop without ha. Visualize latent space with UMAP—should see 4 distinct clusters for (y, a) combinations with ha, overlapping clusters without.
  3. **Participant ratio sensitivity on UTKFace**: Test r ∈ {0.2, 0.4, 0.6, 0.8}. Expect AFed-G performance to scale with r; AFed-GAN should be stable across ratios. This validates AFed-GAN's robustness claim.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the AFed framework be extended to effectively handle multiple sensitive attributes simultaneously without causing performance degradation or combinatorial explosion in the generated samples?
- Basis in paper: [explicit] The conclusion states, "One potential future application of our methods is the case of multiple sensitive attributes, where the data is divided into several groups."
- Why unresolved: The current implementation conditions the generator on a single binary attribute A. Handling intersectional groups requires a generator that models complex dependencies between multiple attributes in the latent space.
- What evidence would resolve it: Demonstration of AFed on a dataset with multiple protected attributes (e.g., race and gender), reporting fairness metrics for intersectional subgroups.

### Open Question 2
- Question: Does AFed maintain its theoretical guarantees and empirical performance when applied to Federated Learning systems with heterogeneous client model architectures?
- Basis in paper: [explicit] The conclusion proposes the framework "could be extended to FL scenarios where each client uses a different model architecture, provided they share the same latent feature dimension."
- Why unresolved: The current algorithm aggregates parameters (e.g., θE, θa) directly, which assumes identical model structures across all clients.
- What evidence would resolve it: An empirical evaluation or theoretical proof where clients use different feature extractor backbones (e.g., ResNet vs. MobileNet) but align on a shared latent space.

### Open Question 3
- Question: How can AFed be adapted to provide formal differential privacy guarantees given the risk that shared generators might memorize and leak sensitive client training data?
- Basis in paper: [explicit] The paper explicitly defers this challenge on Page 4, stating, "We leave both private and fair FL to our future work."
- Why unresolved: Generative Adversarial Networks (GANs) are known to be susceptible to membership inference attacks; simply training in an FL setting does not guarantee privacy against the server or other clients.
- What evidence would resolve it: Integration of Differential Privacy (DP) mechanisms (e.g., DP-SGD) into the generator training loop, with an analysis of the privacy-utility-fairness trade-off.

### Open Question 4
- Question: Is the mix-up debiasing strategy employed by AFed effective for other fairness metrics beyond Demographic Parity, such as Equalized Odds?
- Basis in paper: [inferred] The theoretical analysis (Theorem 3) and experimental setup focus exclusively on Demographic Parity (DP), relying on the independence of prediction Ŷ and attribute A.
- Why unresolved: Enforcing independence (DP) is mathematically distinct from enforcing equal error rates (Equalized Odds). The gradient penalty derived in Equation 21 optimizes specifically for DP and may not converge for constraints conditional on the label Y.
- What evidence would resolve it: Experimental results showing the change in Equalized Odds difference as the hyperparameter λ is varied, compared to baselines that explicitly optimize for that metric.

## Limitations

- The paper relies heavily on synthetic data augmentation without validating whether generated samples truly represent real demographic distributions, which could limit generalization to real-world scenarios
- Non-IID partitioning parameters (Dirichlet concentration) and Mixup hyperparameters (Beta distribution parameters) are unspecified, making exact reproduction challenging
- The theoretical analysis assumes perfect generator performance, but practical GAN training in federated settings may suffer from mode collapse and instability

## Confidence

- **High**: AFed-GAN outperforms AFed-G on complex tasks due to better sample diversity (supported by UTKFace and FairFace results)
- **High**: Auxiliary classification head significantly improves performance by forcing feature disentanglement (validated through UMAP visualizations)
- **Medium**: Theoretical guarantees hold under ideal conditions; practical performance may vary with data complexity and FL participant ratios
- **Medium**: Mixup-based debiasing generalizes across datasets, though effectiveness depends on latent space interpolation quality

## Next Checks

1. Test AFed-GAN's robustness to extreme non-IID conditions by varying Dirichlet concentration parameters and measuring fairness-accuracy tradeoff
2. Validate generated sample quality by comparing latent space distributions of real vs. synthetic data across sensitive attributes using statistical tests
3. Implement ablation studies on Mixup interpolation strength (Beta parameters) to identify optimal balance between fairness improvement and accuracy preservation