---
ver: rpa2
title: 'TimeRewarder: Learning Dense Reward from Passive Videos via Frame-wise Temporal
  Distance'
arxiv_id: '2509.26627'
source_url: https://arxiv.org/abs/2509.26627
tags:
- reward
- learning
- frame
- progress
- videos
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents TimeRewarder, a method that learns dense rewards
  from passive videos by modeling frame-wise temporal distances between video frames.
  The core idea is to predict how far apart two frames are in terms of task progression,
  using this temporal distance as a proxy reward for reinforcement learning.
---

# TimeRewarder: Learning Dense Reward from Passive Videos via Frame-wise Temporal Distance

## Quick Facts
- arXiv ID: 2509.26627
- Source URL: https://arxiv.org/abs/2509.26627
- Authors: Yuyang Liu, Chuan Wen, Yihang Hu, Dinesh Jayaraman, Yang Gao
- Reference count: 40
- Primary result: Achieves nearly perfect success on 9 out of 10 Meta-World tasks using only 200k environment interactions, outperforming all baselines and manual dense reward designs.

## Executive Summary
This paper presents TimeRewarder, a method that learns dense rewards from passive videos by modeling frame-wise temporal distances between video frames. The core idea is to predict how far apart two frames are in terms of task progression, using this temporal distance as a proxy reward for reinforcement learning. By treating temporal ordering as an implicit measure of progress and employing techniques like implicit negative sampling, weighted frame pair selection, and discretization, the method generates step-wise rewards that guide agents toward task completion. Experiments on ten Meta-World manipulation tasks show that TimeRewarder achieves nearly perfect success on 9 out of 10 tasks with only 200,000 environment interactions, outperforming all baselines and even manual dense reward designs in both success rate and sample efficiency. Additionally, it generalizes well to cross-domain settings, leveraging human demonstration videos alongside robot data.

## Method Summary
TimeRewarder learns dense rewards from passive videos by predicting the temporal distance between frame pairs. The method first pre-trains a transformer-based encoder to predict normalized temporal distance between frames using curated demonstration videos. During RL, this temporal distance is converted into step-wise dense rewards using a discretized function. Key innovations include weighted frame pair selection to focus on informative transitions, implicit negative sampling to handle task-agnostic videos, and an inference-time reward conversion that balances progress and consistency. The approach requires no manual reward engineering and can leverage diverse video sources for training.

## Key Results
- Achieves nearly perfect success on 9 out of 10 Meta-World tasks using only 200k environment interactions
- Outperforms all baselines and manual dense reward designs in both success rate and sample efficiency
- Demonstrates successful cross-domain generalization by leveraging human demonstration videos alongside robot data
- Shows superior performance particularly on long-horizon tasks like book stacking and door opening

## Why This Works (Mechanism)
TimeRewarder leverages the inherent temporal structure in successful demonstration videos to create dense rewards. By predicting how far apart frames are in terms of task progression, it provides a continuous signal that guides the agent toward task completion. The weighted sampling strategy ensures the model focuses on informative transitions between task stages, while the discretization process converts the continuous temporal distance into discrete reward levels that are more stable during RL training. The use of implicit negative sampling allows the model to learn from uncurated video data by treating non-matching frame pairs as negative examples.

## Foundational Learning
- Temporal distance modeling: Why needed - provides a continuous measure of task progress; Quick check - verify temporal distance correlates with actual task completion across different tasks
- Frame pair sampling: Why needed - focuses learning on informative transitions between task stages; Quick check - ensure sampling weights emphasize critical task transitions
- Discretization: Why needed - converts continuous temporal distance to stable discrete rewards; Quick check - validate that reward discretization preserves the monotonic relationship with progress
- Cross-domain transfer: Why needed - enables use of diverse video sources including human demonstrations; Quick check - test performance with videos from different domains and perspectives
- Negative sampling: Why needed - handles uncurated video data by treating non-matching pairs as negative examples; Quick check - confirm negative samples improve temporal distance prediction accuracy

## Architecture Onboarding

Component map: Video frames -> Temporal Distance Encoder -> Frame Pair Network -> Temporal Distance Prediction -> Discretized Reward

Critical path: Demonstration videos → Pre-training (temporal distance prediction) → RL agent training (using dense rewards) → Task success

Design tradeoffs: The method trades potential reward sparsity issues for the complexity of temporal distance modeling, requiring careful balancing of pre-training data quality and RL stability.

Failure signatures: Poor performance on tasks with frequent back-and-forth motions, degraded results with noisy or misaligned demonstration videos, and potential instability when scaling to very long task horizons.

First experiments:
1. Validate temporal distance prediction accuracy on held-out demonstration videos
2. Test reward discretization sensitivity by varying the number of reward levels
3. Evaluate performance degradation with increasing proportions of noisy demonstration data

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How can the temporal distance modeling be adapted to effectively handle tasks requiring non-monotonic or back-and-forth motions?
- Basis in paper: The authors state in the Conclusion that "current limitations emerge on tasks with frequent back-and-forth motions."
- Why unresolved: The current method assumes a monotonic correlation between normalized temporal distance and task progress (Equation 8), meaning it may penalize necessary regressive actions (e.g., withdrawing a tool) that are essential to the task but visually resemble "going backward."
- What evidence would resolve it: Successful application of TimeRewarder to tasks like hammering or assembly where optimal trajectories require oscillating movements, potentially using a hierarchical architecture as suggested by the authors.

### Open Question 2
- Question: Does the reliance on "expert optimality" in the theoretical justification limit the robustness of TimeRewarder when learning from noisy, real-world passive videos?
- Basis in paper: The theoretical justification (Section 4.3, Equation 9) explicitly assumes "expert optimality" to map temporal distance to progress. While the method claims to learn from "passive videos," the definition of progress relies on the video representing a successful trajectory.
- Why unresolved: Real-world passive data often contains failed attempts or sub-optimal behaviors. If the model strictly learns temporal distance as progress, it risks assigning high rewards to trajectories that move forward in time but fail to complete the task.
- What evidence would resolve it: Experiments measuring performance degradation when the pre-training dataset is contaminated with varying ratios of failed or sub-optimal demonstration videos.

### Open Question 3
- Question: Can TimeRewarder maintain its sample efficiency and performance advantages when scaled to uncurated, internet-scale video datasets?
- Basis in paper: The conclusion posits that the method provides a path to "scalable 'watch-to-act' skill acquisition from in-the-wild video," but experiments were limited to 100 curated demos or small sets of human videos.
- Why unresolved: The method uses a ViT-B backbone and specific weighted sampling (Equation 4). It is unclear if the "temporal distance" signal remains distinct and learnable amidst the high variance of visual appearances and task definitions found in uncurated web video.
- What evidence would resolve it: Evaluating the transfer learning capabilities of a model pre-trained on a large, diverse dataset (e.g., Ego4D or YouTube-8M) on downstream robotic tasks.

## Limitations
- Current limitations emerge on tasks with frequent back-and-forth motions where the monotonic temporal distance assumption breaks down
- Performance may degrade when learning from noisy or uncurated passive videos containing failed attempts or sub-optimal behaviors
- Scalability to very long task horizons and complex state spaces remains untested beyond the relatively short Meta-World manipulation sequences

## Confidence

**High**: Temporal distance as dense reward improves sample efficiency and success rates on tested Meta-World tasks.

**Medium**: Generalization capability to cross-domain settings, given limited diversity in video sources.

**Medium**: Outperformance over manual reward design, but only within the scope of the tested tasks.

## Next Checks
1. Test scalability and robustness on tasks with longer horizons and more complex state spaces
2. Evaluate performance using noisy or partially misaligned passive video data, including occlusions and viewpoint changes
3. Extend cross-domain experiments to additional domains (e.g., different robot platforms, simulation-to-real) and with varied video qualities