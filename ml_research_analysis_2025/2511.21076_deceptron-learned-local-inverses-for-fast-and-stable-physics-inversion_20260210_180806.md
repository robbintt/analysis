---
ver: rpa2
title: 'Deceptron: Learned Local Inverses for Fast and Stable Physics Inversion'
arxiv_id: '2511.21076'
source_url: https://arxiv.org/abs/2511.21076
tags:
- d-ipg
- inverse
- rjcp
- heat-1d
- iterations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses ill-conditioned inverse problems in physics
  by learning a local inverse of a differentiable forward model. The proposed Deceptron
  combines a lightweight bidirectional module with a Jacobian Composition Penalty
  (JCP) that enforces near-inverse behavior via JVP/VJP probes.
---

# Deceptron: Learned Local Inverses for Fast and Stable Physics Inversion

## Quick Facts
- arXiv ID: 2511.21076
- Source URL: https://arxiv.org/abs/2511.21076
- Reference count: 40
- This work addresses ill-conditioned inverse problems in physics by learning a local inverse of a differentiable forward model, achieving 2-3 iterations to tolerance versus ~60 for projected gradient descent.

## Executive Summary
This paper introduces Deceptron, a framework that learns a local inverse of a differentiable forward model to accelerate and stabilize physics inversion. The key innovation is the Jacobian Composition Penalty (JCP), which trains the reverse map to act as an approximate left inverse via JVP/VJP probes. At inference, Deceptron Inverse-Preconditioned Gradient (D-IPG) takes output-space descent steps and pulls them back through the learned inverse, achieving Gauss-Newton-like convergence at lower per-iteration cost. Experiments on 1D heat conduction and damped oscillator problems show D-IPG converges in 2-3 iterations on easy settings versus ~60 for projected gradient descent, with comparable final RMSE to Gauss-Newton but lower per-iteration cost.

## Method Summary
Deceptron learns a lightweight forward surrogate f_W and a learned inverse g_V with a Jacobian Composition Penalty that enforces near-inverse behavior via JVP/VJP probes. The D-IPG algorithm computes descent steps in well-conditioned output space, pulls them back through g_V, and projects under backtracking rules. Training combines supervised fit, reconstruction, cycle consistency, spectral penalty, bias tie, and composition penalty. The approach explicitly avoids tying V=W^T to maintain degrees of freedom for conditioning.

## Key Results
- D-IPG achieves 2.8±1.0 iterations to tolerance on Heat-1D (hard) versus 58.2±28.9 for projected gradient descent
- DeceptronNet v0 (2D image variant) reaches tolerance in 6 steps with strong stability under real-data noise
- JCP reduces RJCP by several orders of magnitude, yielding fewer iterations and improved conditioning diagnostics

## Why This Works (Mechanism)

### Mechanism 1: Jacobian Composition Penalty (JCP) for Local Inverse Learning
Enforcing J_g(f(x))J_f(x) ≈ I via JVP/VJP probes trains the reverse map g to act as an approximate left inverse of the forward map, improving conditioning without explicit Jacobian computation. The JCP term uses stochastic probes to estimate composition error, encouraging g to undo the local linearization of f.

### Mechanism 2: Output-Space Descent with Learned Pullback (D-IPG)
Taking descent steps in well-conditioned output space and pulling back through a learned inverse provides Gauss-Newton-like updates at lower per-iteration cost. D-IPG computes y^{prop} = y_t - αr_t in output space, then pulls back via x^{prop} = g_V(y^{prop}). When J_g ≈ J_f^+, this matches GN direction scaled by α.

### Mechanism 3: RJCP as Runtime Conditioning Diagnostic
The runtime Jacobian composition error RJCP(x) = E_ξ∥J_g(f(x))J_f(x)ξ - ξ∥² provides an unbiased estimate of local inverse fidelity and correlates with convergence speed. Lower values indicate well-scaled, stable steps; high values signal mis-scaling or axis mixing.

## Foundational Learning

- **Jacobian-Vector Products (JVP) and Vector-Jacobian Products (VJP)**: JCP and RJCP rely on efficient JVP/VJP probes to estimate composition error without materializing Jacobians. Quick check: Given f: R^n → R^m and vector v, can you compute J_f(x)v without forming the full Jacobian matrix?

- **Ill-conditioning in inverse problems**: The paper's core motivation is that physics inverse problems have poorly-scaled gradients in input space. Quick check: If ||J_f|| is large but ||J_f^T r|| is small, what happens to gradient descent step sizes?

- **Gauss-Newton vs. gradient descent for least-squares**: D-IPG is explicitly compared to GN; the paper claims near-GN behavior when J_g ≈ J_f^+. Quick check: Why does GN converge faster than gradient descent on ill-conditioned least-squares, and what is the per-iteration cost difference?

## Architecture Onboarding

- **Component map**: Input x → [f_W: σ(Wx+b)] → Output y (forward surrogate) → [g_V: σ̃(Vy+c)] → Estimated x̂ (learned inverse) → JCP Module → D-IPG Loop

- **Critical path**: Train f_W as forward surrogate, train g_V jointly with JCP to minimize composition error, at inference compute residual r = f_W(x_t) - y*, step in y-space, pull back through g_V, project

- **Design tradeoffs**: Untied V vs. W^T (tying removes degrees of freedom and degrades conditioning), JCP weight scheduling (must warm up after forward fit stabilizes), probe count k (2-4 suffice), network expressivity (deliberately lightweight)

- **Failure signatures**: RJCP remains high across epochs (surrogate fidelity issue), low acceptance rate with backtracking (proposed steps too aggressive), convergence stalls but RJCP low (projection may be dominating)

- **First 3 experiments**: 
  1. Sanity check: Train Deceptron on a linear forward model A where A^+ exists. Verify RJCP → 0 and D-IPG matches analytical GN solution exactly.
  2. Ablation sweep: Disable JCP, tie V=W^T, remove rec/cycle terms separately. Measure RJCP and iterations-to-tolerance.
  3. Robustness test: Train with one noise level σ, evaluate at different σ. Monitor RJCP and final RMSE to characterize surrogate fidelity boundaries.

## Open Questions the Paper Calls Out

- Can the Deceptron framework be extended to multi-scale operators and higher-dimensional tasks while maintaining the lightweight inference cost observed in the single-scale prototypes? The paper currently validates only a single-scale 2D variant and acknowledges limitations regarding expressivity in problems with long-range dependencies.

- How does D-IPG convergence degrade when the differentiable forward surrogate exhibits significant systematic bias or fidelity errors compared to the true physics? Experiments assume a reasonable surrogate fit, but the authors warn that misuse outside validity domains yields overconfidence without quantifying the breaking point.

- Can the method resolve global ambiguities in highly non-identifiable problems, or is it strictly limited to improving local conditioning? The Jacobian Composition Penalty enforces a local left-inverse, which cannot distinguish between distinct global solutions that map to similar outputs.

## Limitations

- Surrogate fidelity requirements: performance degrades if learned forward model poorly approximates true physics, particularly under distribution shift
- Local linearity assumption: JCP enforces local inverse behavior, but non-identifiable or highly nonlinear regimes can cause RJCP to remain high and convergence to stall
- Computational overhead: JVP/VJP probes add ~4× Jacobian-vector multiply cost per training step

## Confidence

- **High**: D-IPG iteration reduction over X-GD (Table 1, Heat-1D hard), JCP-RJCP correlation (Figure 5b), ablation showing JCP's dominant effect on iteration count
- **Medium**: Real-data performance on DeceptronNet v0 (no direct comparisons provided), GN-equivalence claim under ideal J_g conditioning
- **Low**: Claims about broader applicability beyond tested physics problems, runtime overhead in large-scale settings

## Next Checks

1. **Linear verification test**: Apply Deceptron to a known linear forward model (e.g., small dense matrix A) where A^+ exists analytically. Verify RJCP → 0 and D-IPG matches exact GN solution (up to α scaling) across the full input domain.

2. **Surrogate fidelity boundary**: Train with clean data, evaluate on progressively noisier/perturbed inputs. Track RJCP and RMSE to quantify the domain of validity and identify when JCP stops stabilizing.

3. **Constraint set stress test**: Apply D-IPG to problems with active inequality constraints (e.g., positivity, bounds). Measure acceptance rate and final accuracy vs. unconstrained case to assess projection-induced degradation.