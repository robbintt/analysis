---
ver: rpa2
title: It's Not a Walk in the Park! Challenges of Idiom Translation in Speech-to-text
  Systems
arxiv_id: '2506.02995'
source_url: https://arxiv.org/abs/2506.02995
tags:
- translation
- systems
- idioms
- whisper
- news
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper compares idiom translation performance across speech-to-text\
  \ (SLT), text-to-text machine translation (MT), and Large Language Models (LLMs)\
  \ for German\u2192English and Russian\u2192English. The study evaluates four end-to-end\
  \ SLT systems (SeamlessM4T SLT-to-text, Whisper Large v3) against four MT systems\
  \ (SeamlessM4T SLT-to-text, No Language Left Behind), four LLMs (DeepSeek, LLaMA),\
  \ and cascaded alternatives using a newly created idiomatic corpus and conventional\
  \ news data."
---

# It's Not a Walk in the Park! Challenges of Idiom Translation in Speech-to-text Systems

## Quick Facts
- arXiv ID: 2506.02995
- Source URL: https://arxiv.org/abs/2506.02995
- Reference count: 15
- Primary result: SLT systems significantly underperform MT and LLMs on idiom translation, with 24.2% and 22.6% COMET score drops for German→English and Russian→English respectively.

## Executive Summary
This study investigates idiom translation performance across speech-to-text (SLT), text-to-text machine translation (MT), and Large Language Models (LLMs). The research evaluates four end-to-end SLT systems against four MT systems, four LLMs, and cascaded alternatives using a newly created idiomatic corpus and conventional news data. Results demonstrate that SLT systems significantly underperform MT and LLMs on idiomatic data, with COMET scores dropping 24.2% (German) and 22.6% (Russian) when translating idioms versus news. Layer-wise DecoderLens analysis reveals SLT systems only begin refining translations in higher encoder layers and frequently revert to literal translations even in later layers, whereas MT systems show smoother semantic evolution.

## Method Summary
The study creates a new idiomatic corpus for German→English and Russian→English translation by annotating idiomatic phrases in news data. Four end-to-end SLT systems (SeamlessM4T SLT-to-text, Whisper Large v3) are evaluated against four MT systems (SeamlessM4T MT, No Language Left Behind), four LLMs (DeepSeek, LLaMA), and cascaded alternatives. Performance is measured using COMET metric on both idiomatic and news data. DecoderLens analysis is applied to examine layer-wise evolution of translations in encoder-decoder architectures.

## Key Results
- SLT systems significantly underperform MT and LLMs on idiom translation, with COMET scores dropping 24.2% (German) and 22.6% (Russian) for idioms versus news
- LLMs (particularly DeepSeek) outperform dedicated MT models and SLT systems on idiom translation
- Cascaded systems (ASR→MT/LLM) outperform direct end-to-end SLT systems for idiom translation
- DecoderLens analysis reveals SLT systems delay meaningful semantic processing until very late encoder layers, leading to literal translations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cascaded SLT systems outperform direct end-to-end SLT systems for idiom translation.
- Mechanism: A cascaded system separates acoustic-to-text task (handled by ASR) from cross-lingual semantic transfer (handled by text-based MT or LLM). Text-based translation components possess stronger semantic processing capabilities honed on large-scale text data, allowing them to interpret non-compositional language without acoustic interference.
- Core assumption: ASR transcription errors on idiomatic phrases are less detrimental than direct SLT model's inability to integrate acoustic and semantic information.
- Evidence anchors: [abstract] "cascaded systems outperform direct SLT"; [section 4.1] "cascaded systems... mostly outperform end-to-end SLT systems."
- Break condition: Fails if ASR critically mis-transcribes key words of the idiom.

### Mechanism 2
- Claim: LLMs outperform MT models and SLT systems on idiom translation.
- Mechanism: LLMs acquire vast world knowledge and contextual understanding from massive pre-training corpora, allowing them to "memorize" or "reason" about idioms as holistic semantic units.
- Core assumption: The idiom and its correct translation are well-represented in the LLM's pre-training data.
- Evidence anchors: [abstract] "MT systems and Large Language Models demonstrate better handling of idioms"; [section 4.1] "DeepSeek model largely outperforms all other models, especially on idiom translation."
- Break condition: Fails if LLM hallucinates incorrect figurative meaning or reasons too literally.

### Mechanism 3
- Claim: Direct SLT systems fail on idioms because they delay meaningful semantic processing until very late encoder layers.
- Mechanism: DecoderLens analysis shows early encoder layers in SLT systems produce empty or hallucinated output. Meaningful text emerges late, leaving insufficient network depth to refine literal interpretations into correct figurative meanings.
- Core assumption: Decoder output from intermediate encoder layers is a valid proxy for the model's internal semantic state.
- Evidence anchors: [abstract] "Layer-wise DecoderLens analysis reveals SLT systems only begin refining translations in higher encoder layers"; [section 5.1] Shows Whisper layer-by-layer output with correct layer outputting literal translation.
- Break condition: Increasing layers may not solve problem if fundamental issue is how model integrates acoustic vs. semantic information.

## Foundational Learning

- Concept: **End-to-End vs. Cascaded Architectures for SLT**
  - Why needed here: Core architectural comparison understanding trade-off between single direct model and specialized sub-components.
  - Quick check question: In a cascaded SLT system, which component is primarily responsible for the semantic interpretation of an idiom?

- Concept: **Idioms as Non-Compositional Language**
  - Why needed here: Central challenge - idioms cannot be translated by summing individual word meanings.
  - Quick check question: If a model translates "in den Kinderschuhen" literally as "in children's shoes," what core property of idioms has it failed to handle?

- Concept: **Interpretability via Layer-wise Probing (DecoderLens)**
  - Why needed here: Diagnostic tool revealing evolution of model's "thought process."
  - Quick check question: Why is the late emergence of meaningful output in SLT layers problematic specifically for idiom translation?

## Architecture Onboarding

- Component map:
  Direct SLT System: Audio Input → Encoder → Decoder → Text Output
  Cascaded System: Audio Input → ASR → Text MT/LLM → Text Output
  Evaluation: COMET Metric → Semantic similarity score
  Diagnostics: DecoderLens → Layer-wise translation evolution analysis

- Critical path: The path from Audio Input → [Encoder Layer 0...31] → [Decoder] → Text Output is where translation forms. Critical failure point is in late encoder layers where model fails to transition from literal to figurative representation.

- Design tradeoffs:
  - Direct SLT vs. Cascaded: Direct offers simpler, potentially lower-latency system but suffers on idioms due to integrated processing bottlenecks. Cascaded is more complex with higher latency but achieves superior idiom performance.
  - LLM vs. MT: LLMs provide best idiom performance but are larger and slower. MT models are faster but less robust to figurative language.

- Failure signatures:
  - Literal Translation: Grammatically correct, word-for-word translation missing figurative meaning
  - Hallucination: Fluent but irrelevant text in lower layers, indicating lack of proper semantic grounding
  - Late Semantic Refinement: Output becomes relevant only in last few layers, often culminating in literal translation

- First 3 experiments:
  1. Baseline the Failure: Run idiomatic audio samples through direct SLT model (Whisper-large-v3) and cascaded system (Whisper ASR → NLLB), score with COMET to confirm performance gap.
  2. Apply DecoderLens Diagnostics: Use DecoderLens to extract and compare translations from final 5-10 encoder layers of failing direct SLT model and high-performing text MT model, document evolutionary path.
  3. Isolate the ASR Contribution: Introduce controlled noise into ASR transcript of idioms and measure downstream MT performance degradation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do performance drops in SLT systems generalize to languages with morphological structures or idiom compositionality different from German and Russian?
- Basis in paper: [explicit] Authors state in Limitations that "approach focuses only on translating German and Russian to English, while idiom usage varies widely across languages."
- Why unresolved: Current experimental scope restricted to two Germanic/Slavic language pairs.
- What evidence would resolve it: Extending evaluation to diverse language pairs (e.g., Japanese, Arabic) using same methodology.

### Open Question 2
- Question: Does synthetic speech in evaluation affect literal translation errors compared to natural, spontaneous speech?
- Basis in paper: [explicit] Paper acknowledges limitation that "synthetic speech may differ from real-world spontaneous speech though prior work suggests minimal impact on core translation errors."
- Why unresolved: Study relied on synthesized audio (Microsoft Edge TTS) which may lack prosodic variability of natural speech.
- What evidence would resolve it: Comparative study evaluating same SLT models on idiomatic corpora using both synthetic and human-recorded audio.

### Open Question 3
- Question: What training interventions or architectural modifications can encourage end-to-end SLT models to resolve semantic meaning in earlier encoder layers?
- Basis in paper: [inferred] Authors conclude with need for "idiom-specific strategies and improved internal representations" as analysis showed SLT systems only refine translations in higher layers.
- Why unresolved: Paper identifies locus of problem but doesn't propose or test methods to alter layer-wise behavior.
- What evidence would resolve it: Experiments testing techniques like curriculum learning or multi-task training on SLT models, analyzed via DecoderLens.

### Open Question 4
- Question: How does layer-wise evolution of idiom processing in decoder-only LLMs compare to encoder-decoder architectures?
- Basis in paper: [explicit] Authors note in Limitations that "DecoderLens analysis is limited to encoder-decoder architectures and may not capture idiom handling in purely decoder-based systems like LLaMA."
- Why unresolved: While LLMs were evaluated for performance, their internal processing wasn't analyzed due to architectural incompatibility.
- What evidence would resolve it: Applying interpretability methods suitable for decoder-only models (e.g., Logit Lens) to same idiomatic dataset.

## Limitations
- Reliance on automatic COMET evaluation without human validation to capture idiom translation nuances
- DecoderLens interpretability findings depend on assumption that decoding intermediate layer outputs provides valid proxy for model's semantic state
- Cascaded system performance may be sensitive to intermediate ASR transcription quality under varying noise conditions
- Use of synthetic speech in evaluation rather than natural, spontaneous speech

## Confidence
- **High Confidence**: SLT systems underperform MT and LLMs on idiom translation (24.2% and 22.6% COMET score drops)
- **Medium Confidence**: Layer-wise DecoderLens analysis revealing SLT's late semantic refinement and tendency toward literal translations
- **Medium Confidence**: Cascaded systems outperform direct SLT, though performance gap is smaller than with text-based systems

## Next Checks
1. **Human Evaluation Validation**: Conduct human evaluation study where professional translators rate idiomatic quality of translations from SLT, MT, and LLM systems on same test set to validate COMET scores.
2. **ASR Error Sensitivity Analysis**: Systematically corrupt ASR transcriptions of idioms with controlled noise levels and measure degradation in cascaded system performance to quantify robustness of two-stage approach.
3. **Expanded DecoderLens Analysis**: Apply DecoderLens to broader range of SLT models (including smaller models and different architectures) to determine whether late semantic refinement pattern is universal or model-specific, and whether it correlates with model size or training data composition.