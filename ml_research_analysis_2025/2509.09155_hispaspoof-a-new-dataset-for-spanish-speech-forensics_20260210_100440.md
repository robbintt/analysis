---
ver: rpa2
title: 'HISPASpoof: A New Dataset For Spanish Speech Forensics'
arxiv_id: '2509.09155'
source_url: https://arxiv.org/abs/2509.09155
tags:
- speech
- spanish
- synthetic
- detection
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the underrepresentation of Spanish in synthetic
  speech detection research by introducing HISPASpoof, the first large-scale Spanish
  dataset for synthetic speech detection and attribution. The dataset contains 535,687
  speech signals across six Spanish accents and six zero-shot TTS systems.
---

# HISPASpoof: A New Dataset For Spanish Speech Forensics

## Quick Facts
- **arXiv ID:** 2509.09155
- **Source URL:** https://arxiv.org/abs/2509.09155
- **Reference count:** 40
- **Primary result:** HISPASpoof dataset enables reliable synthetic speech detection and attribution for Spanish, addressing language underrepresentation in speech forensics

## Executive Summary
This paper introduces HISPASpoof, the first large-scale dataset specifically designed for synthetic speech detection and attribution in Spanish. The dataset contains 535,687 speech signals spanning six Spanish accents and six zero-shot TTS systems, addressing a critical gap in speech forensics research where Spanish has been significantly underrepresented. The authors demonstrate that existing English-trained detectors fail to generalize to Spanish, while models trained on HISPASpoof achieve substantially improved performance with EERs as low as 1.57%. The work establishes a foundation for developing robust synthetic speech detection methods for one of the world's most widely spoken languages.

## Method Summary
The authors constructed HISPASpoof by generating synthetic speech across six Spanish accents (Argentina, Chile, Colombia, Mexico, Peru, Spain) using six zero-shot TTS systems (Bark, Coqui, DDSP, Fal, Tortoise, VITS). The dataset contains 535,687 total speech signals designed to enable both detection (binary classification of real vs. synthetic speech) and attribution (multi-class classification identifying the specific generator). Five detection methods were evaluated: CREPE, ResNet, Spec-ResNet, PaSST, and WavLM. The authors conducted experiments comparing English-trained detectors on Spanish data versus HISPASpoof-trained models, and performed open-set attribution experiments where models were tested on unseen generators.

## Key Results
- English-trained detectors fail to generalize to Spanish speech, while HISPASpoof training substantially improves detection performance
- Best-performing method achieves EER as low as 1.57% on Spanish synthetic speech detection
- PaSST and Spec-ResNet show superior generalizability in open-set attribution scenarios with unseen generators
- HISPASpoof enables reliable evaluation of synthetic speech detection and attribution methods for Spanish

## Why This Works (Mechanism)
The paper demonstrates that language-specific characteristics significantly impact synthetic speech detection performance. By creating a large-scale, diverse Spanish dataset with multiple accents and generators, the authors provide the necessary training data for models to learn language-specific artifacts and patterns in synthetic speech. The zero-shot TTS systems used ensure that the synthetic speech captures various generation techniques, while the multiple Spanish accents provide dialectal diversity. This comprehensive coverage allows detection models to learn robust features that generalize across different Spanish-speaking populations and synthetic generation methods.

## Foundational Learning

**Synthetic Speech Detection**: Identifying whether speech is real or artificially generated - needed to combat misinformation and fraud; quick check: binary classification performance metrics

**Zero-shot TTS Systems**: Text-to-speech models that can generate speech in languages they weren't specifically trained on - needed to ensure diverse synthetic speech generation; quick check: generator diversity across accent regions

**Speaker Attribution**: Determining which specific TTS system generated a given speech sample - needed for tracing synthetic speech origins; quick check: multi-class classification accuracy across generators

**Language-specific Acoustic Features**: Distinctive patterns in speech signals that vary across languages - needed for effective cross-lingual detection; quick check: feature importance analysis across language pairs

**Open-set Recognition**: Testing models on data from classes not seen during training - needed for real-world robustness; quick check: performance degradation when encountering unseen generators

## Architecture Onboarding

**Component Map:** HISPASpoof Dataset -> Detection Models (CREPE, ResNet, Spec-ResNet, PaSST, WavLM) -> Evaluation Metrics (EER, Accuracy) -> Attribution Analysis

**Critical Path:** Dataset Generation -> Model Training -> Cross-lingual Evaluation -> Open-set Attribution Testing

**Design Tradeoffs:** Language-specific vs. multilingual training (HISPASpoof shows superior performance but lacks multilingual generalization) vs. dataset size vs. accent/generator diversity

**Failure Signatures:** English-trained models show high EER on Spanish data (>20%) vs. HISPASpoof-trained models (EER ~1.57%); poor attribution performance on unseen generators indicates limited generalization

**First Experiments:** 1) Train English models on Spanish data and measure EER; 2) Train models on HISPASpoof and measure improvement; 3) Test attribution performance on seen vs. unseen generators

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset covers only six zero-shot TTS systems and six Spanish accents, potentially missing broader synthetic speech generation diversity
- Performance improvements are specific to the five evaluated detection methods and may not generalize to other architectures
- Claims about English dataset failure to generalize are based on specific English-Spanish language pairs tested
- Attribution performance on truly unseen generators beyond the six included may differ from reported results

## Confidence
- **High confidence**: HISPASpoof enables reliable evaluation for Spanish speech forensics under tested conditions
- **Medium confidence**: English-trained detectors fail to generalize to Spanish (based on specific tested methods)
- **Medium confidence**: PaSST and Spec-ResNet show better generalizability in open-set scenarios (based on six included generators)

## Next Checks
1. Evaluate detector performance on Spanish speech generated by TTS systems not included in HISPASpoof to test true open-set generalizability
2. Test detection performance across additional Spanish accents and dialects not represented in the current dataset
3. Compare HISPASpoof-trained detectors against models trained on multilingual datasets that include Spanish to determine if language-specific training provides advantages beyond multilingual approaches