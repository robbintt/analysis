---
ver: rpa2
title: 'PQS (Prune, Quantize, and Sort): Low-Bitwidth Accumulation of Dot Products
  in Neural Network Computations'
arxiv_id: '2504.09064'
source_url: https://arxiv.org/abs/2504.09064
tags:
- overflows
- pruning
- products
- quantization
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of reducing accumulator bitwidth
  in quantized neural networks (QNNs) without sacrificing accuracy. The authors observe
  that wide accumulators are typically needed to prevent overflows when summing partial
  products, but these increase memory bandwidth usage and reduce energy efficiency.
---

# PQS (Prune, Quantize, and Sort): Low-Bitwidth Accumulation of Dot Products in Neural Network Computations

## Quick Facts
- arXiv ID: 2504.09064
- Source URL: https://arxiv.org/abs/2504.09064
- Reference count: 40
- Primary result: Achieves 2.5x reduction in accumulator bitwidth (32→13 bits) while maintaining accuracy on CIFAR-10 with MobileNetV2 and ResNet-18.

## Executive Summary
This paper addresses the challenge of reducing accumulator bitwidth in quantized neural networks (QNNs) without sacrificing accuracy. The authors identify that wide accumulators are typically needed to prevent overflows when summing partial products, but these increase memory bandwidth usage and reduce energy efficiency. They propose a three-pronged approach: Prune, Quantize, and Sort (PQS). By combining N:M structured pruning to reduce the number of partial products, performing pruning in floating-point before quantization, and sorting partial products to avoid transient overflows, PQS enables narrow accumulators while maintaining model accuracy comparable to floating-point baselines.

## Method Summary
PQS is a three-stage pipeline: First, iterative N:M structured pruning is applied in floating-point (P→Q), reducing the number of partial products per dot product and limiting persistent overflows. Second, quantization-aware training (QAT) converts weights and activations to 8-bit integers. Third, at inference, partial products are accumulated in sorted order—splitting by sign, sorting positives descending and negatives ascending, then pairwise adding—to eliminate transient overflows. This allows accurate computation with narrow accumulators (e.g., 13 bits) instead of wide accumulators (e.g., 32 bits).

## Key Results
- 2.5x reduction in accumulator bitwidth (32→13 bits) while maintaining accuracy on CIFAR-10 with MobileNetV2 and ResNet-18
- Single sorting round resolves 99.8% of transient overflows in MobileNetV2 inference
- P→Q pruning schedule outperforms Q→P by 0.5-1.5% accuracy at high sparsity (>70%)
- Sorted accumulation maintains accuracy ~4 bits lower than clipped accumulation across bitwidths

## Why This Works (Mechanism)

### Mechanism 1: N:M Structured Pruning Reduces Persistent Overflows
- **Claim**: Structured pruning reduces persistent overflows by limiting dot product length, which allows narrower accumulators.
- **Mechanism**: N:M pruning forces the smallest N of every M weights to zero, reducing partial product count per dot product. Fewer terms → smaller maximum sum → fits in fewer bits.
- **Core assumption**: Persistent overflows dominate when dot products are long; reducing term count proportionally reduces overflow probability.
- **Evidence anchors**:
  - [abstract]: "iterative N:M pruning in floating point followed by quantization... allows for accurate, compressed models with short dot product lengths that do not require wide accumulators"
  - [section 3.1]: "N:M weight pruning can restrict dot product lengths sufficiently to avoid most persistent overflows when using narrow accumulators"
  - [corpus]: Limited direct corpus support; MGS addresses low-bitwidth accumulation but targets floating-point precision rather than overflow reduction via pruning.

### Mechanism 2: Pruning Before Quantization (P→Q) Preserves More Information Than Q→P
- **Claim**: Pruning in FP32 before quantization yields higher accuracy at high sparsity than quantizing first then pruning.
- **Mechanism**: FP32 weights retain fine-grained magnitude information; quantized weights lose precision and provide degraded importance signals for pruning decisions.
- **Core assumption**: Weight magnitude correlates with importance; this correlation is stronger in floating-point than in coarse quantized bins.
- **Evidence anchors**:
  - [abstract]: "iterative N:M pruning in floating point followed by quantization to 8 (or fewer) bits"
  - [section 4, Figure 3]: P→Q models maintain higher accuracy than Q→P under low-rank weight approximations, especially at higher sparsities.
  - [corpus]: No directly comparable P→Q vs Q→P evaluation in neighbor papers; corpus evidence weak for this specific claim.

### Mechanism 3: Sorted Dot Product Eliminates Transient Overflows via Early Cancellation
- **Claim**: Accumulating partial products in sorted order (large positives with large negatives) eliminates nearly all transient overflows.
- **Mechanism**: Pairwise multiply → split by sign → sort positives descending, negatives ascending → pairwise add. Large values cancel early, keeping intermediate partial sums monotonic and within range.
- **Core assumption**: Neural network weights are approximately zero-mean Gaussian; ReLU activations produce half-normal positive distributions, yielding roughly symmetric positive/negative partial product distributions.
- **Evidence anchors**:
  - [abstract]: "accumulation of partial products in a sorted order ('small to large') allows for accurate, compressed models"
  - [section 3.2]: Single sorting round resolves 99.8% of transient overflows in MobileNetV2 inference.
  - [corpus]: MGS addresses accumulation accuracy via different greedy-sum approach; corpus supports importance of accumulation ordering but not this specific algorithm.

## Foundational Learning

### Concept: Uniform quantization with scale and offset
- **Why needed here**: PQS operates on quantized weights/activations; understanding how FP32 maps to integers (via scale factor s and offset o) is essential for debugging overflow behavior.
- **Quick check question**: Given an activation range [0, 255] and 8-bit quantization, what is the scale factor and how would you map FP32 value 127.5 to its quantized integer?

### Concept: Persistent vs transient overflow
- **Why needed here**: PQS addresses these differently—pruning targets persistent overflows; sorting targets transient overflows. Misdiagnosing overflow type leads to wrong mitigation.
- **Quick check question**: If a dot product's final result fits in 14 bits but an intermediate partial sum exceeds 14 bits, is this persistent or transient overflow?

### Concept: N:M structured sparsity
- **Why needed here**: PQS uses N:M pruning (e.g., 2:4 or 4:16) which balances hardware amenability with accuracy; unstructured sparsity would not guarantee reduced dot product length per group.
- **Quick check question**: In 2:4 sparsity with M=4 weights [0.5, -0.1, 0.9, -0.3], which weights are pruned?

## Architecture Onboarding

### Component map
Dense FP32 weights → N:M pruning (iterative during FP32 training) → Sparse FP32 weights → Quantization-aware training (QAT) → Sparse quantized weights → Load sparse quantized weights and activations → Pairwise multiply (2b-bit partial products) → Split by sign → Sort (positives descending, negatives ascending) → Pairwise add → Accumulate into narrow p-bit register

### Critical path
The sorted dot product (split → sort → pairwise add) is the inference-time bottleneck. Sorting overhead grows with dot product length K; tiling (K=256) reduces sort complexity while still eliminating ~99% of transient overflows per the paper's analysis.

### Design tradeoffs
- Sparsity vs accuracy: Higher N:M sparsity (e.g., 4:16 vs 2:4) reduces accumulator requirements but may drop accuracy.
- Accumulator bitwidth vs sorting overhead: Narrower accumulators require more aggressive sparsity and sorting; wider accumulators tolerate simpler unsorted accumulation.
- Tiling vs overflow resolution: Tiling breaks long dot products into shorter sorted segments, reducing sort cost but potentially leaving ~1% of transient overflows unresolved.

### Failure signatures
- **Clipping transient overflows**: Accuracy drops sharply (e.g., 10% → 40% on MNIST MLP) when using narrow accumulators without sorting—Figure 2b.
- **Q→P training at high sparsity**: Accuracy degrades faster than P→Q—Figure 4 shows up to 1.5% gap on CIFAR10.
- **Untiled sorting on long dot products**: For K≈4000 (transformers), full-dot-product sorting introduces latency spikes; tile to K≤256.

### First 3 experiments
1. **Baseline overflow profiling**: Take an existing quantized model (e.g., MobileNetV2 INT8), run inference on CIFAR10, and log overflow counts per layer at various accumulator bitwidths (12, 14, 16, 18 bits). Categorize as persistent vs transient using the paper's definition.
2. **P→Q vs Q→P accuracy comparison**: Implement N:M pruning (e.g., 2:16, 4:16) with both schedules on ResNet-18 CIFAR10. Compare top-1 accuracy at 70%, 80%, 90% sparsity. Expect P→Q to outperform Q→P by ~0.5-1.5% at high sparsity.
3. **Sorted vs clipped accumulation**: For a P→Q pruned-then-quantized model, compare inference accuracy when using sorted accumulation vs simple clipping, sweeping accumulator bitwidth from 12 to 20 bits. Expect sorted accumulation to maintain accuracy ~4 bits lower than clipped.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: What are the specific energy and area trade-offs of implementing the sorting network in hardware versus the energy savings gained from using narrow accumulators?
- **Basis in paper**: [explicit] The authors state, "We plan on investigating the hardware implications of PQS in future work."
- **Why unresolved**: The paper validates the algorithmic accuracy and theoretical reduction in bitwidth but does not quantify the physical overhead (latency, area, power) of the sorting logic required to achieve it.
- **What evidence would resolve it**: Post-synthesis area and power analysis (e.g., ASIC or FPGA) comparing a baseline MAC unit against a PQS-enhanced unit.

### Open Question 2
- **Question**: How can the PQS algorithm be adapted for matrix tiling strategies without negating the benefits of reduced memory bandwidth or introducing excessive sorting latency?
- **Basis in paper**: [explicit] The authors note, "our algorithm as presented is not compatible with tiling and would introduce significant overhead when sorting longer dot products."
- **Why unresolved**: Tiling is standard for cache optimization, but PQS theoretically requires full dot product visibility for optimal sorting; the paper only briefly explores a fixed tile size ($k=256$) without a generalized solution.
- **What evidence would resolve it**: A software scheduler or hardware architecture that demonstrates high cache reuse (tiling) while maintaining the overflow guarantees of the sorted accumulation.

### Open Question 3
- **Question**: Can PQS maintain its efficiency and accuracy advantages when applied to Transformer architectures characterized by significantly longer dot product lengths?
- **Basis in paper**: [inferred] The paper mentions that "sorting longer dot products e.g., 4000 partial sums in certain transformer models" poses a challenge for the method.
- **Why unresolved**: The evaluation is restricted to CNNs (MobileNetV2, ResNet-18) on CIFAR-10, leaving the scalability of the sorting mechanism for the large sequence dimensions found in Transformers untested.
- **What evidence would resolve it**: Evaluation of PQS on standard Transformer benchmarks (e.g., BERT or ViT) showing accumulator bitwidth reduction without accuracy loss or prohibitive sorting overhead.

## Limitations
- **Model scope**: Evaluation limited to CIFAR-10 with MobileNetV2 and ResNet-18; results may not generalize to ImageNet-scale models or language models with longer dot products (K~4000).
- **Training resource assumptions**: Assumes availability of FP32 weights for pruning before quantization; less applicable to post-training quantization scenarios.
- **Hardware implications**: While PQS enables narrower accumulators, the paper doesn't fully explore sorting overhead on different hardware (e.g., systolic arrays vs CPUs).

## Confidence
- **High confidence**: The mechanism of sorted dot products eliminating transient overflows is well-supported (99.8% resolution in MobileNetV2). The claim that P→Q pruning preserves more accuracy than Q→P at high sparsity is also well-evidenced by Figure 4.
- **Medium confidence**: The 2.5× bitwidth reduction claim (32→13 bits) is demonstrated but may not hold across all model architectures and datasets.
- **Low confidence**: The assertion that MGS provides only marginal improvements over sorted accumulation is based on limited comparative analysis in the paper.

## Next Checks
1. **Cross-architecture validation**: Apply PQS to ResNet-50 on ImageNet and a transformer-based language model to verify accumulator reduction benefits scale beyond CIFAR-10.
2. **Hardware overhead measurement**: Implement sorted dot product on a systolic array simulator and measure the sorting overhead versus theoretical bandwidth savings from narrower accumulators.
3. **Post-training quantization compatibility**: Test whether PQS principles can be adapted for post-training quantization by analyzing pruning signal quality from quantized vs FP32 weights in pre-trained models.