---
ver: rpa2
title: Have Multimodal Large Language Models (MLLMs) Really Learned to Tell the Time
  on Analog Clocks?
arxiv_id: '2505.10862'
source_url: https://arxiv.org/abs/2505.10862
tags:
- clocks
- clock
- time
- mllms
- hands
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the ability of multimodal large language
  models (MLLMs) to tell time on analog clocks, revealing that despite advances in
  image understanding, these models struggle with this seemingly simple task. The
  study focuses on GPT-4.1, demonstrating that its success on standard clock images
  stems from memorization of patterns in training data rather than genuine understanding.
---

# Have Multimodal Large Language Models (MLLMs) Really Learned to Tell the Time on Analog Clocks?

## Quick Facts
- arXiv ID: 2505.10862
- Source URL: https://arxiv.org/abs/2505.10862
- Reference count: 8
- Primary result: GPT-4.1's analog clock reading relies on pattern memorization, with MAE increasing from 232.5s to 3,726.9s on modified hand clocks

## Executive Summary
This study investigates whether MLLMs truly understand analog clock reading or merely memorize training patterns. Testing GPT-4.1 on normal and deformed clock variants reveals significant performance degradation when visual features deviate from training data. The model's errors stem from confusion about hand functions (especially when hand thickness cues are removed) and impaired directional perception under interference. Fine-tuning improves performance but shows limited generalization to novel clock variants, suggesting current approaches don't enable abstract understanding.

## Method Summary
The researchers created synthetic clock datasets and tested GPT-4.1 on three variants: normal clocks, distorted dials, and modified hand designs. They fine-tuned the model on 300 synthetic clock samples and evaluated performance on 150 test samples per variant using Mean Absolute Error (MAE) in seconds. Error analysis categorized failures into directional perception errors and hand function confusion (hour/minute/second hand swaps).

## Key Results
- MAE increased from 232.5 seconds on normal clocks to 1,380.7 seconds on distorted dials and 3,726.9 seconds on modified hand clocks
- Fine-tuning improved normal clock performance (85.5% error reduction) but showed limited generalization (32.5% reduction on modified hands)
- Hand function confusion caused multi-hour errors, with MAE reaching 10,088.9 seconds when hands were confused
- Directional perception errors were minimal in isolation (6.5° vs 8.1° single-hand difference) but degraded under interference

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MLLM performance on analog clock reading is driven primarily by pattern memorization from training data, not abstract understanding of clock mechanics.
- Mechanism: The model learns statistical correlations between clock image features and time labels present in training data. When visual features deviate from training distribution, the model cannot generalize because it lacks a conceptual model of angular position → time mapping.
- Core assumption: The sharp performance drop on modified clocks (vs. humans who easily adapt) indicates memorization rather than learned rules.
- Evidence anchors:
  - [abstract]: "the model relies on memorized patterns rather than true understanding"
  - [section 2]: MAE increased from 232.48s to 3726.93s when hands were modified; model "has not learned to tell the time but rather memorized patterns in the training set"
  - [corpus]: "Do MLLMs Really Understand the Charts?" similarly argues MLLMs rely on visual recognition rather than reasoning.
- Break condition: If future models trained on diverse clocks successfully generalize to novel hand styles/shapes without explicit exposure, this mechanism would not fully explain performance.

### Mechanism 2
- Claim: Model over-relies on hand thickness as a heuristic for distinguishing hour/minute/second hands, causing functional confusion when this cue is removed.
- Mechanism: Training data likely contains clocks with consistently thicker hour hands and thinner minute/second hands. The model learns this correlational shortcut rather than learning to track which hand is which based on angular velocity or logical constraints.
- Core assumption: The disproportionate error increase from hand modification vs. dial distortion suggests shape features are more critical to the model's inference than the model's own directional perception.
- Evidence anchors:
  - [section 3]: "overfitting, probably due to insufficient diversity in the training data, leading it to overly rely on the thickness of the clock hands to determine their types"
  - [section 4, Table 1]: When hands were confused (45 samples), MAE was 10088.9s vs. 882.2s when not confused—indicating hand function confusion dominates error.
  - [corpus]: No direct corpus evidence on hand-specific overfitting; related work on attention failures is adjacent but not directly applicable.
- Break condition: If models trained with explicitly varied hand styles from the start still fail on new variations, thickness bias alone would not explain failure.

### Mechanism 3
- Claim: Local interference creates cascading degradation—when one visual element confuses the model, performance on otherwise-tractable subtasks also deteriorates.
- Mechanism: MLLMs appear to lack modular processing; confusion about hand identity increases uncertainty that propagates to directional perception, even though single-hand tests show minimal directional error difference (6.5° vs 8.1°).
- Core assumption: The gap between isolated subtask performance and integrated task performance suggests interference effects, not independent error accumulation.
- Evidence anchors:
  - [section 4]: "when the model is confused by one aspect of the input, its performance in other areas—where it would otherwise excel—can also deteriorate"
  - [section 4]: Fine-tuning reduced errors by only 32.5% on modified-hand subset (vs. 85.5% on normal clocks), showing fine-tuning is less effective under interference.
  - [corpus]: "Do Video Language Models Really Know Where to Look?" diagnoses attention failures in complex scenes, aligning with interference-based degradation.
- Break condition: If architecture changes enabling modular/structured reasoning eliminate this cascade, the mechanism would point to architectural limitations rather than training data issues.

## Foundational Learning

- Concept: **Overfitting to visual shortcuts**
  - Why needed here: The paper's central finding is that models learn surface correlations (hand thickness → hand type) rather than task-relevant abstractions. Understanding this helps diagnose why fine-tuning alone may not generalize.
  - Quick check question: When you modify a non-semantic visual feature (e.g., line thickness, color), does model performance collapse? If yes, suspect shortcut learning.

- Concept: **Generalization gap (in-distribution vs. out-of-distribution)**
  - Why needed here: The study shows dramatic performance differences between normal clocks (in-distribution-like) and modified clocks (OOD). Evaluating only on familiar inputs masks brittleness.
  - Quick check question: Is your evaluation set systematically different from training data along any axis the model might exploit?

- Concept: **Compositional failure modes**
  - Why needed here: The interference cascade shows models don't independently solve subtasks. When one component fails, others degrade unexpectedly.
  - Quick check question: Does your model perform well on isolated subtasks but fail when they're combined? If so, interference effects may be at play.

## Architecture Onboarding

- Component map: Vision encoder -> tokenizes image into patch/region representations -> Projection layer -> maps visual tokens to LLM embedding space -> LLM backbone -> integrates visual and textual tokens, generates time predictions -> Fine-tuning adapter (if used) -> adjusts weights on synthetic clock data

- Critical path:
  1. Generate synthetic clock dataset with full time coverage (all minute/second combinations)
  2. Create variant datasets: normal, distorted dial, modified hands
  3. Baseline evaluation on all variants before any fine-tuning
  4. Fine-tune on subset of normal clocks; re-evaluate on all variants
  5. Analyze errors by category: directional vs. hand-function confusion

- Design tradeoffs:
  - Synthetic data ensures coverage but may not match real-world clock diversity
  - Fine-tuning improves in-distribution performance but shows limited transfer to variants
  - Error analysis is manual and time-intensive but reveals specific failure modes

- Failure signatures:
  - Large MAE increase when visual features deviate from training distribution (>10x in this study)
  - Hand function confusion: hour/minute hand swap errors causing multi-hour mistakes
  - Fine-tuning gains that don't transfer across variants (32.5% vs 85.5% error reduction)

- First 3 experiments:
  1. Replicate the single-hand directional perception test: show model images with only one hand, measure angular error. This isolates directional perception from hand-identification confusion.
  2. Fine-tune separate models on normal-only vs. mixed variants; compare transfer performance. This tests whether training diversity improves generalization or just multiplies shortcuts.
  3. Add explicit hand-type prompts (e.g., "the thicker hand is the hour hand") and measure whether this reduces confusion errors. This tests whether the bottleneck is visual identification or conceptual integration.

## Open Questions the Paper Calls Out
None

## Limitations
- Results are specific to GPT-4.1 and may not generalize to other MLLM architectures or training regimes
- Fine-tuning data was synthetic, which may not capture the full diversity of real-world clocks
- The study focuses on a single, relatively constrained task rather than broader generalization capabilities

## Confidence
- High confidence in the core finding that performance degrades significantly on modified clocks (MAE increase from 232.5s to 3,726.9s on hand-modified variants)
- Medium confidence in the mechanism of hand thickness overfitting, as this relies on inference about training data patterns
- Medium confidence in the interference cascade mechanism, as error analysis is primarily correlational

## Next Checks
1. Test additional MLLM architectures (GPT-4o, Claude-3, Gemini) on the same clock variants to assess whether results generalize across models
2. Conduct ablation studies varying hand thickness systematically to quantify the exact contribution of this visual feature to model performance
3. Evaluate models on a more diverse set of real-world clock images (not synthetic) to test generalization to naturally occurring variations