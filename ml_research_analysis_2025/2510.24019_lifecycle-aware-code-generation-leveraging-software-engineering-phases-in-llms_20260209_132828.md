---
ver: rpa2
title: 'Lifecycle-Aware code generation: Leveraging Software Engineering Phases in
  LLMs'
arxiv_id: '2510.24019'
source_url: https://arxiv.org/abs/2510.24019
tags:
- code
- generation
- design
- arxiv
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a lifecycle-aware code generation framework\
  \ that integrates software engineering phases\u2014requirements analysis, architectural\
  \ design (SCXML-based state machine modeling), detailed design (pseudocode), and\
  \ code generation\u2014into LLM-based workflows. By structuring code generation\
  \ into these intermediate stages, the approach enhances traceability, reduces ambiguity,\
  \ and improves overall code quality."
---

# Lifecycle-Aware code generation: Leveraging Software Engineering Phases in LLMs

## Quick Facts
- arXiv ID: 2510.24019
- Source URL: https://arxiv.org/abs/2510.24019
- Authors: Xing Xing; Wei Wang; Lipeng Ma; Weidong Yang; Junjie Zheng
- Reference count: 40
- Primary result: Multi-step lifecycle-aware fine-tuning improves code correctness by up to 75% over single-step generation

## Executive Summary
This paper introduces a lifecycle-aware code generation framework that integrates software engineering phases—requirements analysis, architectural design (SCXML-based state machine modeling), detailed design (pseudocode), and code generation—into LLM-based workflows. By structuring code generation into these intermediate stages, the approach enhances traceability, reduces ambiguity, and improves overall code quality. Experiments demonstrate that lifecycle-aware fine-tuning improves code correctness by up to 75% compared to the same model without fine-tuning. Multi-step inference consistently outperforms single-step generation, with open-source LLMs achieving relative CodeBLEU improvements of up to 34.3% over leading commercial models. The framework remains robust even with up to 80% less training data and shows that each intermediate artifact contributes to the final code quality, with architectural design having the most significant impact.

## Method Summary
The lifecycle-aware code generation framework maps user intent through four sequential stages: Requirements Analysis (structured text), Architectural Design (SCXML state machine XML), Detailed Design (pseudocode), and Code Generation (Python). The model is fine-tuned end-to-end using Low-Rank Adaptation (LoRA) on a corpus constructed from RTCA/DO-185B avionics standards, XState, Simulink, and OPNET. The training process learns to generate each intermediate artifact sequentially, with the output of one stage serving as input to the next. The framework is evaluated using CodeBLEU as the primary metric, with ablation studies and data efficiency tests demonstrating the effectiveness of the multi-stage approach.

## Key Results
- Multi-step inference consistently outperforms single-step generation, with open-source models achieving up to 34.3% relative CodeBLEU improvement over commercial models
- Lifecycle-aware fine-tuning improves code correctness by up to 75% compared to models without fine-tuning
- The framework remains robust with up to 80% less training data while maintaining stable CodeBLEU scores
- Architectural design (SCXML) contributes most significantly to final code quality, with a 7.1% CodeBLEU drop when removed

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Decomposing code generation into sequential software engineering artifacts reduces semantic ambiguity and error accumulation compared to direct translation.
- **Mechanism:** The framework enforces a "scaffolding" effect where intermediate representations (requirements, SCXML, pseudocode) act as explicit reasoning steps. This grounds the LLM in a structured context, preventing the model from having to infer both architecture and syntax simultaneously.
- **Core assumption:** Assumes that the model can successfully generate valid intermediate artifacts and that these artifacts faithfully represent the solution logic.
- **Evidence anchors:** [Abstract] "Multi-step inference consistently outperforms single-step generation." [Section 5.2] Shows single-step inference causes a CodeBLEU drop (e.g., -0.0560 for DSK-C-1.3B-F), indicating weaker inductive structure without intermediates.
- **Break condition:** If intermediate artifact generation introduces hallucinations or logical errors, this error will propagate and amplify through subsequent stages, degrading final code quality.

### Mechanism 2
- **Claim:** Formal architectural modeling (via SCXML state machines) contributes more significantly to final code correctness than requirements or pseudocode stages.
- **Mechanism:** SCXML forces the explicit definition of control logic, transitions, and states. This structured constraint likely acts as a "logical scaffold," reducing the complexity the LLM must manage during the final code synthesis phase.
- **Core assumption:** Assumes the problem domain maps effectively to finite state machine logic and that SCXML provides a superior inductive bias compared to purely textual design descriptions.
- **Evidence anchors:** [Abstract] "Ablation studies further reveal that each intermediate artifact contributes distinctly to final code quality, with architectural design having the most significant impact." [Section 5.5] Removing architectural design results in the largest performance drop (CodeBLEF -7.1%), exceeding the impact of removing requirements or pseudocode.
- **Break condition:** If the target software requires architectural paradigms incompatible with state machines (e.g., continuous signal processing or purely functional transformations without discrete states), this mechanism may fail to provide useful guidance.

### Mechanism 3
- **Claim:** Lifecycle-aware fine-tuning provides data efficiency, enabling smaller models (1.3B parameters) to match or outperform larger commercial models.
- **Mechanism:** By training on structured tuples (Intent -> Reqs -> SCXML -> Code), the model learns the "process" of software engineering rather than just probabilistic text-to-code mapping. This strong structural prior may reduce the data volume required to achieve convergence.
- **Core assumption:** Assumes that the training distribution aligns closely with the inference-time structure and that the base model has sufficient capacity to learn the hierarchical dependencies.
- **Evidence anchors:** [Abstract] "The framework remains robust even with up to 80% less training data." [Section 5.4] Shows Qwen-7B retains stable CodeBLEU scores even when training data is reduced to 20%.
- **Break condition:** If the training data lacks diversity or contains misaligned artifacts, the model will overfit to the specific lifecycle structure without generalizing to new intents.

## Foundational Learning

- **Concept:** **SCXML (State Chart XML)**
  - **Why needed here:** This is the core intermediate representation used in the architectural design phase. Understanding how XML represents states, transitions, and events is critical to debugging the pipeline.
  - **Quick check question:** Can you identify the `<state>` and `<transition>` tags in an XML file and map them to a specific behavior in a loop?

- **Concept:** **LoRA (Low-Rank Adaptation)**
  - **Why needed here:** The paper uses LoRA to fine-tune large models efficiently. Understanding this helps explain how the framework adapts base models without retraining all weights.
  - **Quick check question:** Does LoRA update the pre-trained weights of the LLM directly, or does it inject trainable rank-decomposition matrices?

- **Concept:** **CodeBLEU**
  - **Why needed here:** This is the primary metric used to validate the "correctness" of the generated code. It differs from standard BLEU by incorporating abstract syntax trees (AST) and data flow.
  - **Quick check question:** Why would a syntactically correct code snippet score poorly on standard BLEU but well on CodeBLEU (or vice versa)?

## Architecture Onboarding

- **Component map:** Intent -> Requirements Analysis -> Architectural Design (SCXML) -> Detailed Design (Pseudocode) -> Code (Python)
- **Critical path:** The Architectural Design component is the most sensitive. The paper identifies this as the highest-impact stage; failures here (invalid SCXML) correlate strongly with final code failure.
- **Design tradeoffs:**
  - **Latency vs. Quality:** This is a 4-step sequential pipeline. Inference time is roughly 4x higher than a single-step generation, trading speed for the 75% improvement in correctness.
  - **Rigidity vs. Flexibility:** The fixed lifecycle enforces structure (good for safety-critical code) but may be overkill for simple snippet generation.
- **Failure signatures:**
  - **Syntactic hallucination in SCXML:** The model generates XML that looks valid but fails W3C schema validation.
  - **Error Propagation:** A vague requirement in Stage 1 leads to an incomplete state machine in Stage 2, resulting in missing logic in Stage 4.
- **First 3 experiments:**
  1. **Ablation Run:** Run the pipeline skipping Stage 2 (SCXML) and compare CodeBLEU scores against the full pipeline to verify the claimed 7.1% performance drop.
  2. **Data Scarcity Test:** Fine-tune a small model (e.g., 1.5B params) on only 20% of the dataset and verify if CodeBLEU remains stable (checking the "robustness" claim).
  3. **Single-Step Baseline:** Prompt the fine-tuned model to generate code directly from intent (skipping all intermediates) to measure the performance delta of the scaffolding mechanism.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can automated verification tools be integrated into intermediate stages to detect specification mismatches early?
- Basis in paper: [explicit] Future work suggests exploring automated verification integration within each generation stage.
- Why unresolved: The current framework relies on sequential generation without formal automated validation at intermediate steps, risking uncaught errors.
- What evidence would resolve it: Comparative error rates in final code with and without intermediate stage formal verifiers.

### Open Question 2
- Question: Does the framework maintain performance when applied to domains structurally different from the RTCA/DO-185B standard?
- Basis in paper: [inferred] The authors acknowledge limited external validity and uncertain generalizability to domains like video games.
- Why unresolved: The training corpus focuses on formal avionics standards; success in less formal or non-safety-critical domains is unproven.
- What evidence would resolve it: Evaluation of CodeBLEU scores on generated code for web applications or game logic.

### Open Question 3
- Question: How can the multi-stage architecture be optimized to reduce latency for real-time or interactive use cases?
- Basis in paper: [explicit] The authors list latency introduced by the multi-stage architecture as a limitation.
- Why unresolved: Sequential generation across four stages is inherently slower than single-step methods, potentially hindering adoption.
- What evidence would resolve it: Latency benchmarks comparing single-step vs. optimized multi-step inference in interactive IDEs.

## Limitations

- **Dataset construction and generalizability:** The corpus is built from domain-specific sources (RTCA/DO-185B avionics, XState, Simulink, OPNET) with chains extracted using GPT-4o. The methodology for chain creation is not fully detailed, raising concerns about reproducibility and whether the lifecycle-aware approach generalizes beyond structured control-flow-heavy domains to general-purpose code generation.

- **Ablation granularity:** While the paper claims architectural design (SCXML) has the largest impact, the ablation removes entire stages rather than isolating the impact of the structured artifact itself versus the multi-step process. It is unclear if SCXML's superiority stems from its formalism or simply from being a multi-step intermediate.

- **Hyperparameter transparency:** Critical training and inference hyperparameters (LoRA rank, alpha, learning rate, prompt templates for stages) are referenced as in an Appendix but are not included in the provided text, blocking faithful reproduction and independent verification.

## Confidence

- **High confidence:** The core experimental result that multi-step inference outperforms single-step (CodeBLEU delta up to -0.0560 in reverse) and that lifecycle-aware fine-tuning improves correctness by up to 75%. These are directly supported by ablation and comparison results in the paper.

- **Medium confidence:** The claim that architectural design (SCXML) contributes most significantly (7.1% CodeBLEU drop when removed). This is stated and supported by one ablation figure, but lacks comparison against alternative intermediate representations or deeper analysis of *why* SCXML is superior.

- **Low confidence:** The 80% data efficiency claim. While stated, the experimental setup for data reduction (e.g., random sampling, stratified splits) is not detailed, and the stability of CodeBLEU under such reduction is asserted but not deeply analyzed for variance or failure modes.

## Next Checks

1. **Ablation granularity test:** Reproduce the pipeline but create two variants: (a) remove the SCXML stage entirely, and (b) replace SCXML with an equivalent textual architectural description (e.g., structured English). Compare the performance drop in each case to isolate whether the benefit is from formality/structure versus the multi-step process.

2. **Robustness under synthetic noise:** Introduce controlled errors (e.g., 10%, 30%, 50% of SCXML files with invalid transitions) into the training data and measure if the model learns to self-correct or if error propagation breaks the pipeline. This tests the "grounding" assumption of Mechanism 1.

3. **Cross-domain transferability:** Apply the fine-tuned lifecycle-aware model to a non-state-machine domain (e.g., numerical algorithms or data transformation scripts) and measure if CodeBLEU degrades sharply, indicating overfitting to the SCXML formalism.