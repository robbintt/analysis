---
ver: rpa2
title: 'Pay (Cross) Attention to the Melody: Curriculum Masking for Single-Encoder
  Melodic Harmonization'
arxiv_id: '2601.16150'
source_url: https://arxiv.org/abs/2601.16150
tags:
- melody
- harmony
- training
- tokens
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of weak cross-attention between
  melody and harmony in single-encoder transformer architectures for melodic harmonization.
  The authors propose a training curriculum called FF (full-to-full) that initially
  masks all harmony tokens and progressively unmasks them during training to strengthen
  melody-harmony interactions.
---

# Pay (Cross) Attention to the Melody: Curriculum Masking for Single-Encoder Melodic Harmonization

## Quick Facts
- arXiv ID: 2601.16150
- Source URL: https://arxiv.org/abs/2601.16150
- Authors: Maximos Kaliakatsos-Papakostas; Dimos Makris; Konstantinos Soiledis; Konstantinos-Theodoros Tsamis; Vassilis Katsouros; Emilios Cambouropoulos
- Reference count: 40
- Key outcome: FF curriculum outperforms baselines in melodic harmonization, especially out-of-domain

## Executive Summary
This paper addresses the problem of weak cross-attention between melody and harmony in single-encoder transformer architectures for melodic harmonization. The authors propose a training curriculum called FF (full-to-full) that initially masks all harmony tokens and progressively unmasks them during training to strengthen melody-harmony interactions. The method is evaluated across multiple experimental axes including temporal quantization, bar-level vs. time-signature conditioning, melody representation, and inference-time unmasking strategies. Models are trained on the HookTheory dataset and evaluated both in-domain and on jazz standards using metrics that assess chord progression structure, harmony-melody alignment, and rhythmic coherence. Results show that the FF curriculum consistently outperforms baseline methods in nearly all metrics, with particularly strong gains in out-of-domain evaluations. The study finds that quarter-note quantization, intertwining bar tokens, and pitch-class melody representations are advantageous in the FF setting.

## Method Summary
The paper addresses weak cross-attention in single-encoder transformers for melodic harmonization by introducing a curriculum learning approach. The FF (full-to-full) method starts training with all harmony tokens masked, forcing the model to rely on melody for reconstruction. During training, harmony tokens are progressively unmasked according to a fixed schedule. The model uses a single transformer encoder with concatenated melody (pianoroll) and harmony (chord tokens) inputs, creating a functional partition in the attention map: lower-left for cross-attention (harmony→melody), lower-right for self-attention (harmony→harmony). The approach is evaluated on the HookTheory dataset with both in-domain and out-of-domain (jazz standards) testing across multiple experimental conditions including quantization level, bar encoding, melody representation, and inference strategies.

## Key Results
- FF curriculum consistently outperforms baseline MD and R10% methods in both in-domain and out-of-domain evaluations
- Quarter-note quantization (q4) and pitch-class melody representation show consistent advantages across FF variants
- Out-of-domain jazz evaluation reveals FF's superior ability to generalize melody-harmony relationships beyond training data
- Bar intertwining encoding outperforms time-signature conditioning, particularly for out-of-domain generalization
- FF models achieve better alignment between melody notes and chord tones (CTnCTR) while maintaining smoother progressions (CTD)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Starting training with fully masked harmony tokens forces cross-attention pathways to melody that persist through later training phases.
- **Mechanism:** When all harmony tokens are masked, the model has no visible harmony context to exploit and must attend to melody for reconstruction. This strengthens weights in the lower-left quadrant of the attention map (harmony→melody). As training progresses and harmony tokens become visible, these cross-attention pathways remain functional while self-attention gradually develops.
- **Core assumption:** Attention patterns established early in training persist and integrate with later-learned patterns rather than being overwritten.
- **Evidence anchors:**
  - [abstract]: "keeps all harmony tokens masked for several training steps before progressively unmasking entire sequences during training to strengthen melody–harmony interactions"
  - [Section II-A]: Figure 2(c) shows FF method recovers expected diagonal cross-attention pattern while baseline methods (a, b) do not
  - [Section III-A]: "This design choice aligns with the goal of forcing early reliance on melody before gradually introducing harmony self-attention"
  - [corpus]: Limited direct corpus evidence; related work [38, 39] document cross-modal attention underutilization in multimodal transformers

### Mechanism 2
- **Claim:** Random stage sampling in prior curricula creates an optimization shortcut where harmony-harmony correlations are learned at the expense of melody-harmony dependencies.
- **Mechanism:** In MD and R10%, each batch contains samples across all masking levels regardless of training progress. Early training thus includes many samples with substantial visible harmony. Since harmony-token relations are direct (same embedding space) while melody supervision is indirect (through pianoroll projection), the model learns to reconstruct harmony from adjacent harmony tokens—ignoring melody entirely.
- **Core assumption:** Models optimize along the gradient of least resistance; direct token-token correlations are easier to learn than cross-modal projections.
- **Evidence anchors:**
  - [Section II-A]: "existing training curricula inspired by discrete diffusion often result in weak ('cross') attention between melody and harmony"
  - [Section III-A]: "Since many targets can be reconstructed from local harmonic correlations alone, the model faces little incentive to attend to the melody"
  - [Section III-A]: Explicitly links to multimodal shortcut behavior in [38, 39]
  - [corpus]: Neighbor paper "Incorporating Structure and Chord Constraints" addresses related constraint integration issues but not curriculum effects

### Mechanism 3
- **Claim:** Pitch-class melody representation simplifies the cross-modal mapping by aligning melody feature space with chord vocabulary structure.
- **Mechanism:** Chord symbols are pitch-class based (root ∈ {0–11} + quality). Using 12-dimensional pitch-class pianoroll rather than 88-dimensional full-range representation reduces the melody-to-harmony mapping complexity. The model learns harmonic relationships (chroma) rather than octave-specific patterns that are irrelevant to chord selection.
- **Core assumption:** Harmonic compatibility is determined primarily by pitch class (chroma), not absolute register.
- **Evidence anchors:**
  - [Section III]: "The pc-roll encodes chroma information at each timestep, similar to the approach in [7], supporting reasoning over harmonic context"
  - [Section IV-B, Q4]: "using only pitch classes (PC) is not only sufficient but in fact advantageous...in Table III, all best-performing FF variants rely solely on PC"
  - [corpus]: Reference [7] uses similar chroma-based approach; limited additional corpus validation

## Foundational Learning

- **Concept: Self-attention vs. cross-attention in single-encoder architectures**
  - Why needed here: Understanding how one attention map partitions into functional roles based on query/key positions
  - Quick check question: In a concatenated [melody; harmony] input, which quadrant of the attention matrix represents harmony tokens attending to melody tokens?

- **Concept: Curriculum learning and training dynamics**
  - Why needed here: The core contribution is a curriculum design; understanding why ordering matters for what gets learned
  - Quick check question: Why does random difficulty sampling produce different learned behaviors than progressive difficulty increase, even with identical data?

- **Concept: Masked language modeling for conditional generation**
  - Why needed here: The model generates via iterative unmasking, not autoregressive decoding
  - Quick check question: What advantage does bidirectional context provide for harmonization specifically, compared to left-to-right generation?

## Architecture Onboarding

- **Component map:**
  Input Layer: Melody branch (Piano-roll: PC or FRPC) → Linear projection Wm → d_model
  Harmony branch: Chord tokens + <bar> tokens → Embedding Ey → d_model
  Concatenation: [Wm(m); Ey(y_in)] → 2L tokens
  Positional Encoding: Fixed repeating embeddings p → concat[p, p] → added to input
  Transformer Encoder: BERT-style bidirectional encoder
  Output: Linear head → |V| = 348 chord vocabulary → Predict at unmasked positions

- **Critical path:**
  1. Prepare melody pianoroll with bar-row indicator
  2. Initialize harmony sequence with <mask> tokens at target positions
  3. Concatenate and add positional embeddings
  4. Forward pass through transformer encoder
  5. Sample predictions at unmasked positions (training) or masked positions (inference)
  6. Update harmony sequence with predicted tokens
  7. Repeat steps 2–6 for iterative unmasking (inference) or advance curriculum step (training)

- **Design tradeoffs:**
  | Decision | Options | Paper finding |
  |----------|---------|---------------|
  | Quantization | q4 vs q16 | q4 consistently better for FF |
  | Bar encoding | intertwined vs time-signature condition | intertwined preferred, especially out-of-domain |
  | Melody representation | PC vs FRPC | PC alone sufficient and often superior |
  | Positional embeddings | fixed vs learned | Fixed chosen for simplicity |
  | Inference strategy | uR10%, uMD, Seq | All viable; uR10% and uMD more efficient |

- **Failure signatures:**
  - **Attention inspection:** Absence of diagonal pattern in lower-left quadrant → melody underutilization
  - **In-domain vs out-of-domain gap:** Large CHE/CC degradation on jazz set → model memorizing training patterns rather than learning melody conditioning
  - **High CTD (Chord Tonal Distance):** Disjunct progressions → model not learning smooth voice-leading
  - **Low CTnCTR:** Melody notes poorly aligned with chord tones → weak melody-harmony coupling

- **First 3 experiments:**
  1. **Diagnostic attention probe:** Train MD and FF curricula on the artificial dataset (chord root = melody note). Visualize average attention maps to confirm FF recovers diagonal cross-attention while MD does not.
  2. **Curriculum ablation:** Train FF with exponents {3, 5, 7, 10} and measure cross-attention strength (lower-left quadrant activation) vs. final metrics on both in-domain and jazz sets.
  3. **Representation ablation:** Train FF-q4-bar with PC vs. FRPC melody representations. Compare convergence speed, attention patterns, and out-of-domain generalization to validate pitch-class sufficiency claim.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can dynamically monitoring cross-attention and self-attention during training to adaptively adjust harmony token visibility schedules improve harmonization quality beyond the fixed FF curriculum?
- Basis in paper: [explicit] The authors state: "further improvements might be achieved by dynamically monitoring the behavior of cross and self attention during training and adapting the percentage of visible harmony tokens accordingly."
- Why unresolved: The FF curriculum uses a fixed exponent schedule (Equation 2); adaptive scheduling based on real-time attention behavior has not been explored.
- What evidence would resolve it: Experiments comparing fixed vs. attention-adaptive masking schedules across the same datasets and metrics (CHE, CTD, CTnCTR, etc.).

### Open Question 2
- Question: Can piece-specific or style-specific visibility schedules for harmony tokens be systematically derived to optimize harmonization for different musical genres?
- Basis in paper: [explicit] The authors note: "It would therefore be valuable to explore whether such interactions can be identified and quantified to compute optimal, piece- or style-specific visibility schedules."
- Why unresolved: Current work uses a single curriculum across all training examples regardless of style (HookTheory vs. jazz showed different metric profiles in Table I).
- What evidence would resolve it: Per-style or per-piece curriculum optimization experiments showing measurable gains over uniform FF training.

### Open Question 3
- Question: How do FF-trained models perform under perceptual evaluation by human listeners compared to baseline curricula?
- Basis in paper: [explicit] The authors plan "to supplement quantitative metrics with qualitative listening studies and curated harmonization examples, enabling a more perceptual assessment of the FF curriculum's impact."
- Why unresolved: All current evaluation relies on algorithmic metrics (CHE, CTD, PCS, etc.); no human listening tests have been conducted.
- What evidence would resolve it: Controlled listening studies with human ratings comparing FF vs. MD/R10% outputs.

### Open Question 4
- Question: Does the FF curriculum transfer effectively to richer transformer architectures with learned positional embeddings, stage embeddings, and explicit time-signature conditioning?
- Basis in paper: [explicit] "Future work will extend the method to richer and more expressive model variants" beyond the intentionally minimal architecture used.
- Why unresolved: The simplified model uses fixed positional embeddings and omits stage/time-signature inputs present in prior work [33].
- What evidence would resolve it: Comparative experiments applying FF to the full architecture from [33] with all conditioning inputs.

## Limitations

- Attention pattern attribution: Direct causal link between training masking order and attention pattern formation remains largely inferred rather than experimentally isolated
- Generalization across domains: Strong in-domain performance but more modest out-of-domain gains; limited exploration of other musical domains
- Curriculum mechanism specificity: Limited systematic exploration of alternative curriculum designs beyond the FF approach

## Confidence

**High confidence**: 
- FF curriculum outperforms baseline curricula in controlled in-domain evaluations
- Quarter-note quantization (q4) provides consistent advantages across FF variants
- Pitch-class representation (PC) is sufficient and often superior to full-range representations

**Medium confidence**:
- FF curriculum's cross-attention strengthening mechanism is the primary driver of performance gains
- The identified architectural choices (bar intertwining, PC representation) generalize to other single-encoder harmonization settings
- Out-of-domain performance improvements are robust across different inference-time unmasking strategies

**Low confidence**:
- The causal relationship between early full-masking and persistent cross-attention is definitively established
- The observed attention patterns are solely attributable to curriculum design rather than architectural constraints
- The methodology scales to substantially larger or more complex harmonization tasks

## Next Checks

1. **Curriculum ablation with artificial data**: Create a synthetic dataset where chord roots are deterministically derived from melody pitches. Train MD and FF curricula on this dataset and directly visualize attention maps to isolate the effect of masking order on cross-attention pattern formation. Measure whether FF consistently recovers the expected diagonal pattern while MD fails, controlling for other variables.

2. **Alternative curriculum design exploration**: Systematically test alternative curriculum strategies including staged masking (different harmony token subsets unmasked at different stages), difficulty-based ordering (prioritizing harmony-harmony reconstruction before cross-modal learning), and hybrid approaches that combine progressive and random elements. Compare these against FF and baseline curricula on both in-domain and out-of-domain metrics.

3. **Attention intervention experiment**: During FF training, periodically freeze and manipulate attention weights in the lower-left quadrant to test whether maintaining strong cross-attention is necessary for sustained performance. Compare models where cross-attention is preserved, degraded, or allowed to evolve normally, measuring the impact on both training convergence and final harmonization quality.