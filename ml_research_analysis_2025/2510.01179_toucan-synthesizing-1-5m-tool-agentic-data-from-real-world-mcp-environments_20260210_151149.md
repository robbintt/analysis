---
ver: rpa2
title: 'TOUCAN: Synthesizing 1.5M Tool-Agentic Data from Real-World MCP Environments'
arxiv_id: '2510.01179'
source_url: https://arxiv.org/abs/2510.01179
tags:
- tool
- tools
- question
- server
- servers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TOUCAN addresses the lack of high-quality, permissively licensed
  tool-agentic training data by synthesizing 1.5 million trajectories from nearly
  500 real-world MCP servers. The dataset is generated through a pipeline that synthesizes
  diverse tasks using five distinct models, filters them for quality, and generates
  agentic trajectories with three teacher models and two frameworks, ensuring realistic
  tool execution and multi-turn conversations.
---

# TOUCAN: Synthesizing 1.5M Tool-Agentic Data from Real-World MCP Environments

## Quick Facts
- arXiv ID: 2510.01179
- Source URL: https://arxiv.org/abs/2510.01179
- Reference count: 40
- Synthesizes 1.5 million trajectories from nearly 500 real-world MCP servers

## Executive Summary
TOUCAN addresses the lack of high-quality, permissively licensed tool-agentic training data by synthesizing 1.5 million trajectories from nearly 500 real-world MCP servers. The dataset is generated through a pipeline that synthesizes diverse tasks using five distinct models, filters them for quality, and generates agentic trajectories with three teacher models and two frameworks, ensuring realistic tool execution and multi-turn conversations. Rigorous rule-based and model-based validation maintains output quality, and three extensions add edge-case handling, persona-based diversification, and multi-turn dialogues. Models fine-tuned on TOUCAN outperform larger closed-source counterparts on BFCL V3 and MCP-Universe benchmarks, advancing the Pareto frontier in agentic task performance.

## Method Summary
TOUCAN synthesizes tool-agentic data through a five-stage pipeline. First, it crawls and filters MCP servers to select 495 accessible servers. Second, five LLMs generate diverse tasks, which are then filtered by a judge model for quality and difficulty. Third, three teacher models execute these tasks using two agent frameworks to generate trajectories with real tool responses. Fourth, rule-based and model-based validation ensures trajectory quality. Finally, three extensions add edge-case handling, persona-based diversification, and multi-turn dialogues to the dataset.

## Key Results
- Models fine-tuned on TOUCAN outperform larger closed-source counterparts on BFCL V3 and MCP-Universe benchmarks
- Advances the Pareto frontier in agentic task performance
- Demonstrates superior performance compared to state-of-the-art models on real-world MCP environments

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Real-world tool execution within MCP environments provides higher fidelity training signals than simulated or synthetic tool responses.
- **Mechanism:** By interfacing with 495 live MCP servers, the pipeline captures ground-truth tool outputs, latency, and failure modes. This forces the model to learn robust error handling and parsing of complex JSON structures rather than hallucinating idealized responses.
- **Core assumption:** Assumes that the variance and complexity inherent in live API responses (e.g., nested data, edge-case errors) are critical for generalization, and that simulated responses (used in prior work like ToolAlpaca) fail to capture this distribution.
- **Evidence anchors:**
  - [abstract] "generates agentic trajectories... ensuring realistic tool execution"
  - [section] "Stage 1... retaining only remote MCP servers accessible via streamable HTTP... filtering out servers with problematic tools that returned error messages."
  - [corpus] *ToolMind* and *ToolACE* datasets focus on synthesis; TOUCAN explicitly contrasts its "Real Executed" tool responses against "Synthetic Simulated" ones in Table 1.
- **Break condition:** If the MCP servers used for training differ fundamentally in structure or behavior from those in the deployment environment, this transfer of "realism" may degrade.

### Mechanism 2
- **Claim:** Multi-model synthesis followed by rigorous multi-dimensional filtering creates a "curriculum" of high-difficulty, verifiable tasks.
- **Mechanism:** The pipeline uses five diverse LLMs to generate candidate tasks to avoid model-specific bias. It then employs a judge model to score tasks on difficulty, uniqueness, and realism (Stage 3). This filters out trivial or redundant tasks, forcing the student model to learn nuanced tool selection.
- **Core assumption:** Assumes that LLM-as-a-judge correlates sufficiently with human notions of task quality and that filtering for "Tool Selection Difficulty" directly translates to improved downstream agentic reasoning.
- **Evidence anchors:**
  - [abstract] "pipeline first produces a broad spectrum... using five distinct models, applies model-based quality filtering"
  - [section] "Stage 3... annotating tasks across six dimensions... [including] Tool Selection Difficulty... Tool Selection Uniqueness."
  - [corpus] *Unlocking Implicit Experience* synthesizes trajectories from text; TOUCAN focuses on generating *new* diverse tasks via multi-model prompts.
- **Break condition:** If the judge model systematically prefers a specific style of task that doesn't reflect real user needs, the dataset may become artificially difficult but practically less useful.

### Mechanism 3
- **Claim:** Targeted dataset extensions for irrelevance and multi-turn interactions instill specific agentic behaviors (refusal and context retention).
- **Mechanism:** The "Irrelevance" extension (Ext.1) shuffles tool contexts to teach the model to recognize when *not* to act. The "Multi-turn" extension (Ext.3) chains tasks, forcing the model to maintain state across turns. This addresses the brittleness of single-turn function calling.
- **Core assumption:** Assumes that explicit negative sampling (irrelevance) is required to reduce hallucination and that simple concatenation of single-turn data is insufficient for multi-turn reasoning.
- **Evidence anchors:**
  - [abstract] "three extensions add edge-case handling, persona-based diversification, and multi-turn dialogues."
  - [section] "Ext.1: Irrelevance... systematically generate queries unsolvable with the current toolset... to reduce hallucination."
  - [corpus] *FunReason-MT* and *Magnet* also highlight the necessity of multi-turn structures, validating TOUCAN's design focus here.
- **Break condition:** If the multi-turn synthesis logic produces disjointed or unnatural conversation flows, the model may overfit to specific transition patterns.

## Foundational Learning

- **Concept: Model Context Protocol (MCP)**
  - **Why needed here:** MCP is the standardized interface (the "USB-C for AI") used to connect LLMs to the tools. Understanding the distinction between the protocol, the server, and the tool definition is required to navigate the dataset schema.
  - **Quick check question:** Can you distinguish between an *MCP Server* (a collection of capabilities) and a *Tool* (a specific function within that server) in the dataset metadata?

- **Concept: Agentic Trajectory**
  - **Why needed here:** The dataset is not just (query, response) pairs. It is a sequence of *planning, tool calls, tool responses, and final synthesis*. You must understand this temporal structure to effectively utilize the data for SFT (Supervised Fine-Tuning).
  - **Quick check question:** In a TOUCAN instance, where would you look to find the *reasoning* step versus the raw *tool output*?

- **Concept: Negative Sampling / Irrelevance Detection**
  - **Why needed here:** A core contribution of TOUCAN is training models to *refuse* tasks they cannot solve. This requires understanding how the "Irrelevance" extension (Ext.1) modifies the training distribution.
  - **Quick check question:** If a user asks for stock prices but only weather tools are available, what output should the model generate, and which subset of TOUCAN teaches this?

## Architecture Onboarding

- **Component map:** Source (500 MCP Servers) -> Task Synthesis (5 LLMs) -> Filtering (Judge Model) -> Execution (3 Teacher Models + 2 Frameworks) -> Validation (Rule-based + LLM Judge) -> Extensions (Irrelevance, Persona, Multi-turn)
- **Critical path:** The **Task Filtering (Stage 3)** is the bottleneck for data quality. If the judge (Kimi-K2) approves low-quality tasks, the trajectory generation (Stage 4) wastes compute on meaningless tool calls.
- **Design tradeoffs:**
  - **Real vs. Simulated:** TOUCAN uses *real* execution, which ensures high fidelity but increases cost and latency compared to the simulated responses used in datasets like ToolAlpaca.
  - **Diversity vs. Coherence:** Using 5 different generator models increases task diversity but risks stylistic inconsistency in the questions.
- **Failure signatures:**
  - **Low Tool-Use Percentage:** If post-filtering shows <100% "desired tool use," it implies the teacher model failed to follow the plan.
  - **High Hallucination:** If the model attempts to answer the question without using tools (on the main dataset), it suggests the "Irrelevance" extension data was improperly mixed.
- **First 3 experiments:**
  1.  **Server Onboarding Validation:** Select 5 diverse MCP servers from the Smithery registry and attempt to run the Stage 1 "test question" filter to verify connectivity and tool validity.
  2.  **Ablation Replication:** Fine-tune a small base model (e.g., Qwen-7B) on *only* the single-turn data vs. *only* the multi-turn data to replicate the ablation study results in Figure 9.
  3.  **Judge Correlation Check:** Manually score 50 random synthesized tasks and compare your ratings against the automated "Tool Selection Difficulty" scores to verify the alignment of the LLM-as-a-judge.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can specialized expert models accurately simulate tool execution for high-latency or costly tools without compromising trajectory fidelity?
- **Basis in paper:** [explicit] Section 5 states the intent to "develop an expert LLM capable of simulating tool execution" to reduce costs compared to real execution.
- **Why unresolved:** Real execution ensures high-quality results but is slow; simulation is faster but risks hallucinations or incorrect state modeling.
- **What evidence would resolve it:** A comparative study showing expert model simulations achieving trajectory equivalence scores â‰¥ 95% relative to ground-truth execution.

### Open Question 2
- **Question:** How does the exclusion of servers requiring third-party credentials (e.g., Notion, GitHub) limit the transferability of TOUCAN-trained agents to secure, authenticated real-world workflows?
- **Basis in paper:** [explicit] Section 5 acknowledges the dataset excluded servers requiring API keys to maintain accessibility, potentially missing "widely-used scenarios."
- **Why unresolved:** The current dataset focuses on publicly accessible tools, leaving a gap in handling authentication flows and permission errors common in production.
- **What evidence would resolve it:** Benchmarks on secured enterprise environments showing comparable performance between TOUCAN-trained models and those trained on authenticated data.

### Open Question 3
- **Question:** What automated mechanisms can scalably onboard and validate new MCP servers with complex configurations to prevent dataset stagnation?
- **Basis in paper:** [explicit] Section 5 notes that the static collection (June 2025) may miss emerging servers and suggests "developing automated onboarding agents."
- **Why unresolved:** Manual curation is the current bottleneck for keeping the dataset current with the rapidly expanding MCP ecosystem.
- **What evidence would resolve it:** An automated pipeline that successfully integrates and generates valid trajectories from previously unseen MCP servers with minimal human intervention.

## Limitations
- The dataset's reliance on real-time MCP server execution introduces significant reproducibility challenges
- The "GPT-OSS" and "GPT-OSS-120B" models are not clearly specified, potentially limiting faithful replication
- The evaluation methodology conflates dataset quality with model performance, making it difficult to isolate the dataset's contribution to observed improvements

## Confidence
- **High confidence:** The dataset synthesis pipeline (Stages 1-5) is technically sound and well-documented
- **Medium confidence:** The claimed performance improvements on BFCL V3 and MCP-Universe benchmarks are difficult to fully attribute to dataset quality versus model architecture choices
- **Low confidence:** The generalization claims across diverse MCP environments, given the relatively small number (495) of curated servers compared to the total MCP ecosystem

## Next Checks
1. **Server Stability Audit:** Conduct a longitudinal study tracking the availability and response consistency of the 495 MCP servers used in TOUCAN across a 30-day period, documenting any changes in tool specifications or failure rates
2. **Human Evaluation Correlation:** Recruit human annotators to score 200 random TOUCAN trajectories on task difficulty, tool appropriateness, and conversation naturalness, then compute correlation coefficients with the automated LLM-as-a-judge scores reported in the paper
3. **Zero-Shot Transfer Test:** Fine-tune a model exclusively on TOUCAN data, then evaluate its performance on a completely disjoint set of MCP servers not present in the original dataset, measuring both success rates and hallucination frequency